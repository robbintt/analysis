---
ver: rpa2
title: 'LettinGo: Explore User Profile Generation for Recommendation System'
arxiv_id: '2506.18309'
source_url: https://arxiv.org/abs/2506.18309
tags:
- user
- profile
- profiles
- recommendation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LettinGo is a novel framework that generates diverse and adaptive
  user profiles for recommendation systems using large language models (LLMs). It
  addresses the limitations of fixed-format profiles by exploring unconstrained profile
  formats through multiple LLMs and optimizing them using task-driven feedback via
  Direct Preference Optimization (DPO).
---

# LettinGo: Explore User Profile Generation for Recommendation System

## Quick Facts
- arXiv ID: 2506.18309
- Source URL: https://arxiv.org/abs/2506.18309
- Reference count: 40
- Generates adaptive user profiles for recommendation systems using LLMs

## Executive Summary
LettinGo is a novel framework that generates diverse and adaptive user profiles for recommendation systems using large language models (LLMs). It addresses the limitations of fixed-format profiles by exploring unconstrained profile formats through multiple LLMs and optimizing them using task-driven feedback via Direct Preference Optimization (DPO). The framework operates in three stages: (1) generating diverse user profiles from LLMs, (2) evaluating profile quality based on downstream recommendation performance, and (3) aligning profile generation using pairwise preference data derived from task outcomes. Experimental results show that LettinGo significantly improves recommendation accuracy across three datasets (Movielens-10M, Amazon Books, and Yelp), achieving up to 20 percentage points higher accuracy compared to baseline methods that use only recent interactions or fixed-format profiles.

## Method Summary
LettinGo addresses the limitations of fixed-format user profiles in recommendation systems by leveraging large language models (LLMs) to generate diverse and adaptive user profiles. The framework operates in three stages: (1) Profile Exploration - generating multiple candidate profiles using different LLMs, (2) Task-driven Evaluation - assessing profile quality based on downstream recommendation performance, and (3) Alignment via DPO - refining profile generation using pairwise preference data derived from task outcomes. The method uses historical interaction data (30-70 items) as input to LLMs, which generate textual profiles that are then evaluated by a recommendation model. Through iterative feedback and optimization, LettinGo produces semantically richer profiles that capture user preferences more effectively than traditional fixed-format approaches.

## Key Results
- Achieves up to 20 percentage points higher accuracy compared to baseline methods using only recent interactions
- Outperforms GPT-4o-generated profiles across all tested datasets
- Demonstrates superior performance on Movielens-10M, Amazon Books, and Yelp datasets
- Shows good model transferability across different LLM backbones

## Why This Works (Mechanism)
The framework works by leveraging the generative capabilities of LLMs to create semantically rich user profiles that capture nuanced preferences beyond simple interaction histories. By using multiple LLMs and Direct Preference Optimization (DPO) with task-driven feedback, the system iteratively refines profile generation to optimize for downstream recommendation performance rather than just profile coherence.

## Foundational Learning
- Direct Preference Optimization (DPO): A method for aligning model outputs with human preferences using pairwise comparisons; needed to refine profile generation based on recommendation performance; quick check: verify DPO updates improve downstream metrics
- Multi-LLM ensemble generation: Using multiple LLMs to generate diverse candidate profiles; needed to explore the space of possible profile formats; quick check: measure diversity metrics across LLM sources
- Task-driven evaluation: Using recommendation performance as the optimization objective; needed to ensure profiles are useful for the actual task rather than just being coherent; quick check: validate that improved profile quality correlates with recommendation accuracy

## Architecture Onboarding

**Component Map:** Historical Interactions → Multiple LLMs → Profile Candidates → Recommendation Model → Performance Evaluation → DPO Alignment → Refined Profiles

**Critical Path:** The end-to-end pipeline from generating diverse profiles through multiple LLMs, evaluating them via the recommendation model, and refining using DPO is the critical path. Each stage depends on the previous one, and the DPO alignment step is crucial for adapting profile generation to the specific recommendation task.

**Design Tradeoffs:** The framework trades computational overhead (generating multiple profiles with multiple LLMs) for improved recommendation accuracy and profile diversity. The use of DPO requires collecting pairwise preference data, which adds complexity but enables task-specific optimization.

**Failure Signatures:** Poor performance may manifest as: (1) low diversity in generated profiles despite multiple LLMs, (2) recommendation accuracy not improving despite DPO alignment, (3) high computational costs without commensurate accuracy gains.

**First Experiments:**
1. Baseline comparison: Generate profiles using only recent interactions vs. LettinGo with historical data
2. Ablation study: Compare multi-LLM generation vs. single LLM with varied temperature settings
3. Transfer learning test: Evaluate performance when using profiles generated by one LLM backbone but evaluated by a different LLM-based recommendation model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal historical interaction length be determined dynamically for individual users to balance signal richness against noise?
- Basis in paper: The authors state that "the choice of historical interaction length should be adapted dynamically, taking into account the characteristics of the dataset and the capacity of the recommendation model," noting that longer histories may introduce noise.
- Why unresolved: The current experiments utilize fixed lengths (30, 50, 70 interactions) to demonstrate the trade-off, but the paper provides no mechanism or heuristic for automatically selecting the optimal cutoff for a specific user or dataset.
- What evidence would resolve it: A study implementing an adaptive history selector (e.g., based on information density or recency decay) that outperforms fixed-length baselines across the tested datasets.

### Open Question 2
- Question: Do LettinGo-generated profiles transfer effectively to non-LLM recommendation architectures, such as collaborative filtering models?
- Basis in paper: The framework optimizes profiles using an LLM-based downstream predictor ($f_{Rec}$). While the authors claim the profiles are "semantically richer," it is unclear if this unstructured text is optimal for traditional ID-based or embedding-based recommenders without further translation.
- Why unresolved: The evaluation loop relies entirely on an LLM's ability to parse the text. If the profile contains information an LLM finds useful but a standard Matrix Factorization model cannot ingest, the utility is limited to generative systems.
- What evidence would resolve it: Experiments using the generated text profiles as features (via encoding) for standard non-generative models (e.g., NCF or LightGCN) showing performance parity or gains.

### Open Question 3
- Question: Can the "Profile Exploration" phase be made more efficient by reducing the reliance on multi-model sampling without sacrificing diversity?
- Basis in paper: The method requires generating profiles using multiple LLMs (GPT-4o-mini, Claude, LLaMA) and sampling $N=10$ profiles per user to ensure diversity. This implies a high computational overhead for data collection.
- Why unresolved: The paper does not analyze the marginal utility of adding more source models or samples, nor does it explore if a single strong model with diverse prompting strategies could achieve similar exploration quality at a lower cost.
- What evidence would resolve it: An ablation study comparing the performance of profiles generated by a multi-model ensemble versus a single model with varied temperature/prompt settings.

## Limitations
- Relies on pairwise preference data derived from task outcomes, which may not capture full complexity of user preferences
- Experimental results limited to three datasets (Movielens-10M, Amazon Books, and Yelp)
- Impact of different LLM backbones on performance is unclear
- High computational overhead from generating multiple profiles with multiple LLMs

## Confidence
- Claim: Significant improvement in recommendation accuracy vs. baselines - High
- Claim: Outperforms GPT-4o-generated profiles - Medium
- Claim: Good model transferability across LLM backbones - Low

## Next Checks
1. Evaluate LettinGo on a wider range of recommendation datasets, including those from different domains and with varying characteristics (e.g., implicit feedback, sparse data).
2. Conduct ablation studies to assess the impact of individual components of the LettinGo framework, such as the use of multiple LLMs for profile generation and the DPO-based alignment.
3. Compare LettinGo against state-of-the-art recommendation methods that incorporate additional sources of user information, such as social networks or contextual data, to determine the relative performance and benefits of the LLM-based approach.