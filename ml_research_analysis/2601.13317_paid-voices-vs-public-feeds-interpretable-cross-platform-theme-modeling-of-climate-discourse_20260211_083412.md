---
ver: rpa2
title: 'Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling
  of Climate Discourse'
arxiv_id: '2601.13317'
source_url: https://arxiv.org/abs/2601.13317
tags:
- climate
- themes
- theme
- meta
- bluesky
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a cross-platform comparative analysis of climate
  discourse, contrasting paid advertising on Meta with organic posts on Bluesky. A
  key challenge is analyzing structurally distinct communication environments with
  different incentive structures and stylistic norms.
---

# Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse

## Quick Facts
- arXiv ID: 2601.13317
- Source URL: https://arxiv.org/abs/2601.13317
- Reference count: 29
- Primary result: LLM-mediated semantic clustering produces more interpretable climate themes than keyword-based baselines and reveals platform-level incentive effects.

## Executive Summary
This paper introduces a cross-platform comparative analysis framework for climate discourse, contrasting paid Meta advertising with organic Bluesky posts. The core innovation is a decoupled pipeline that first discovers semantic clusters using density-based methods, then leverages LLMs to generate interpretable theme labels. This approach outperforms traditional topic modeling baselines in human and LLM judge evaluations while enabling downstream stance prediction. The analysis reveals systematic thematic differences reflecting platform incentives: promotion-oriented themes dominate paid advertising, while organic discourse shows broader thematic diversity and greater political responsiveness.

## Method Summary
The framework embeds texts using SentenceBERT, reduces dimensionality via PCA (100D) and UMAP (20D), then clusters with HDBSCAN (min_cluster_size=20, min_samples=5). Mistral-Large-Instruct-2407 performs coherence filtering, summarization, and theme labeling (1-3 words). Clusters merge at cosine similarity ≥0.8 using SBERT (all-mpnet-base-v2). Texts assign to themes via direct Text-to-Theme matching. The method avoids predefined topic inventories, enabling data-driven discovery while maintaining interpretability for cross-platform comparison.

## Key Results
- Human evaluation: Text-to-Theme achieves 0.67 accuracy vs. 0.25 for LDA and BERTtopic baselines on Meta data
- Theme-stance correlation: Themes show stronger, more concentrated correlations with stance labels than baseline approaches
- Platform divergence: Meta emphasizes promotion themes (e.g., 'Clean energy'), while Bluesky shows broader diversity and rapid reconfiguration around political events

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Discovery-Interpretation Pipeline
- **Claim:** Separating semantic clustering from theme interpretation produces more coherent, human-interpretable themes than joint topic modeling approaches.
- **Mechanism:** Density-based clustering (HDBSCAN) discovers latent semantic structure without requiring pre-specified topic counts. LLM then generates interpretable labels from cluster summaries, allowing data-driven discovery while maintaining human accessibility.
- **Core assumption:** Semantic similarity in embedding space corresponds to thematic coherence that can be verbally summarized.
- **Evidence anchors:** [abstract] "explicitly decoupling theme discovery from theme interpretation, allowing themes to remain grounded in the data while still being accessible for analysis"
- **Break condition:** If embedding space clusters don't correspond to interpretable semantic groupings (e.g., clusters mix unrelated topics), coherence filtering will reject them but the pipeline may produce too few themes.

### Mechanism 2: LLM-Mediated Coherence Filtering and Labeling
- **Claim:** LLM-based coherence checking and theme labeling produces more stance-consistent themes than keyword-extraction baselines.
- **Mechanism:** LLM evaluates whether top-k representative texts form a coherent cluster, filters incoherent ones, then generates concise theme labels (1-3 words) that capture stance and topic simultaneously.
- **Core assumption:** LLM judgments of coherence align with human judgments of thematic meaningfulness.
- **Evidence anchors:** [Section 6.1] Text-to-Theme achieves 0.67 accuracy vs. LDA (0.25) and BERTtopic (0.25) under human evaluation on Meta data
- **Break condition:** If LLM systematically misjudges coherence due to domain-specific language or biased training, filtering may remove valid clusters or retain incoherent ones.

### Mechanism 3: Incentive-Aligned Thematic Divergence
- **Claim:** Platform-level incentives (paid persuasion vs. organic expression) produce systematic differences in thematic prevalence and temporal responsiveness.
- **Mechanism:** Paid platforms (Meta) prioritize promotion-oriented themes with commercial relevance; organic platforms (Bluesky) show broader thematic diversity and rapid reconfiguration around political events.
- **Core assumption:** Thematic distributions reflect underlying incentive structures rather than random variation.
- **Evidence anchors:** [Section 6.3] "promotion-oriented themes (e.g., 'Clean energy') appear more frequently in advertising, whereas themes with weaker commercial relevance (e.g., 'Anti-Deforestation', 'Climate Wildfires') are more prevalent in organic discourse"
- **Break condition:** If platform differences are primarily driven by user demographics or moderation policies rather than incentives, cross-platform generalizations may not hold.

## Foundational Learning

- **Concept: Density-based clustering (HDBSCAN)**
  - **Why needed here:** Unlike k-means or LDA, HDBSCAN doesn't require pre-specifying cluster count and handles noise/outliers—essential for discovering themes without pre-defined inventories.
  - **Quick check question:** Given texts embedded in 20D space after PCA+UMAP, how does HDBSCAN determine cluster boundaries vs. noise points?

- **Concept: Stance prediction as thematic validation**
  - **Why needed here:** Downstream task performance validates that themes capture meaningful semantic structure, not just surface-level lexical clustering.
  - **Quick check question:** If theme-only models achieve comparable F1 to text-only models on stance prediction, what does this imply about theme quality?

- **Concept: Cross-platform embedding alignment**
  - **Why needed here:** To compare themes across platforms with different vocabularies and styles, texts must be embedded in a shared semantic space where similar meanings cluster together.
  - **Quick check question:** Why use SentenceBERT (all-mpnet-base-v2) for cluster merging but all-MiniLM-L6-v2 for initial embedding?

## Architecture Onboarding

- **Component map:** Embedding layer (SentenceBERT) → Dimensionality reduction (PCA 100D → UMAP 20D) → Clustering (HDBSCAN) → Coherence filter (LLM) → Summarization (LLM) → Merge (cosine similarity ≥0.8) → Theme labeling (LLM) → Assignment (Text-to-Summary or Text-to-Theme)

- **Critical path:**
  1. Embedding quality determines cluster quality—normalize with L2 before dimensionality reduction
  2. Threshold τ=0.8 for merging tuned via Silhouette Score and Davies-Bouldin Index
  3. Text-to-Theme outperforms Text-to-Summary on human evaluation; use direct assignment for production

- **Design tradeoffs:**
  - **Cluster granularity vs. interpretability:** Smaller minimum cluster size (20) captures niche themes but may produce noisier clusters requiring more aggressive filtering
  - **LLM judge vs. human evaluation:** LLM judges favor keyword overlap; human judges prefer semantic coherence—optimize for human evaluation as primary signal
  - **Summary-mediated vs. direct theme assignment:** Summary method has higher LLM-judge accuracy but lower human accuracy; direct assignment more reliable

- **Failure signatures:**
  - Too many incoherent clusters filtered out → check embedding quality and PCA components retained
  - Theme-stance correlation near zero → themes may be too granular or merging threshold too low
  - BERTtopic-style outlier flood → HDBSCAN min_cluster_size too high or min_samples too aggressive

- **First 3 experiments:**
  1. **Embedding ablation:** Compare all-MiniLM-L6-v2 vs. all-mpnet-base-v2 vs. domain-specific climate embeddings on cluster coherence scores
  2. **Threshold sensitivity:** Grid search τ ∈ {0.6, 0.7, 0.8, 0.9} and report theme count, stance correlation, and human evaluation accuracy
  3. **Cross-domain transfer:** Apply pipeline to non-climate corpus (e.g., political advertising) to test whether incentive-aligned thematic divergence generalizes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed thematic modeling framework generalize to platforms with different moderation policies, algorithmic curation, or media formats (e.g., video-first environments)?
- **Basis in paper:** [explicit] The authors state in the Limitations that findings "may not directly generalize to other advertising systems or social media platforms" due to differences in user bases and affordances.
- **Why unresolved:** The empirical analysis is restricted to text-based environments (Meta ads and Bluesky posts).
- **Evidence:** Application of the framework to distinct ecosystems like TikTok, YouTube, or Google Ads.

### Open Question 2
- **Question:** To what extent do platform-level incentives causally influence the thematic structure of discourse, as opposed to merely correlating with them?
- **Basis in paper:** [explicit] The authors state their analysis is "descriptive rather than causal" and they "do not infer causal effects of platform incentives on discourse."
- **Why unresolved:** The observational study design precludes isolating incentives from confounding factors like user demographics or external events.
- **Evidence:** Experimental designs or natural experiments that track discourse shifts when platform incentives change.

### Open Question 3
- **Question:** How does the choice of pre-trained Large Language Model (LLM) systematically bias the semantic clustering and labeling of themes, particularly regarding stances like climate skepticism?
- **Basis in paper:** [inferred] The paper acknowledges that LLMs "may encode biases present in their training data" and notes that the authors "do not explicitly measure or mitigate such biases."
- **Why unresolved:** The pipeline relies on a specific LLM (Mistral) without ablation studies on model selection or bias auditing.
- **Evidence:** Comparative evaluation of themes generated by different LLMs or fine-tuned variants against a human-annotated gold standard.

## Limitations
- Limited to text-based platforms (Meta ads and Bluesky posts), may not generalize to video or image-based environments
- Observational design prevents causal inference about platform incentives' effects on discourse
- LLM-based pipeline may encode training biases affecting theme discovery and labeling

## Confidence
- **High:** Decoupling discovery from interpretation produces more interpretable themes than joint topic modeling (strong human evaluation results)
- **Medium:** LLM coherence judgments align with human judgments across domains (limited cross-domain validation)
- **Medium:** Platform incentives systematically shape thematic structure (empirical evidence but potential confounding factors)

## Next Checks
1. Cross-domain transfer testing to verify incentive-aligned thematic divergence generalizes beyond climate discourse
2. Embedding quality ablation comparing domain-specific vs. general-purpose models on coherence scores
3. Threshold sensitivity analysis for cluster merging (τ ∈ {0.6, 0.7, 0.8, 0.9}) to optimize theme count vs. interpretability tradeoffs