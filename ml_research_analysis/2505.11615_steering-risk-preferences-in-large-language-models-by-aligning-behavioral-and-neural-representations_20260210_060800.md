---
ver: rpa2
title: Steering Risk Preferences in Large Language Models by Aligning Behavioral and
  Neural Representations
arxiv_id: '2505.11615'
source_url: https://arxiv.org/abs/2505.11615
tags:
- steering
- risk
- vectors
- risky
- certainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled method for steering large language
  models (LLMs) by aligning behavioral and neural representations of latent constructs.
  The core idea is to use Markov Chain Monte Carlo (MCMC) with an LLM to elicit behavioral
  representations of risk preference, then align these with neural activations to
  derive steering vectors.
---

# Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations

## Quick Facts
- arXiv ID: 2505.11615
- Source URL: https://arxiv.org/abs/2505.11615
- Reference count: 40
- Primary result: Behavioral alignment of MCMC-elicited risk preferences with neural activations produces effective steering vectors that outperform contrastive baselines

## Executive Summary
This paper introduces a principled method for steering large language models (LLMs) by aligning behavioral and neural representations of latent constructs. The core idea is to use Markov Chain Monte Carlo (MCMC) with an LLM to elicit behavioral representations of risk preference, then align these with neural activations to derive steering vectors. This self-alignment approach enables precise control of LLM outputs without retraining or fine-tuning. The authors demonstrate their method across three domains: risky decision-making, risk perception, and text generation, showing that steering vectors derived from aligned representations consistently outperform Contrastive Activation baselines.

## Method Summary
The method uses MCMC to sample gambles proportionally to the model's latent risk preferences, then aligns these behavioral estimates with neural activations through Lasso regression. The resulting coefficients map which residual stream dimensions correlate with risk preference. At inference time, steering vectors are injected into residual streams at specified layers, shifting output distributions without weight modification. The approach was validated on Gemma-2-9B-Instruct for risky decision-making and risk perception tasks, comparing MCMC-derived vectors against Certainty Equivalent and Contrastive Activation baselines.

## Key Results
- MCMC-derived steering vectors achieved significantly higher steerability for risky choices in Gemma-2-9B-Instruct compared to Contrastive Activation and Certainty Equivalent methods
- For steering risk perception across 150 real-world events, MCMC vectors showed the largest effects, significantly outperforming both Certainty Equivalent and Contrastive Activation methods
- Earlier layers are more effective for steering perception while later layers work better for decisions, aligning with cognitive processing hierarchies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning behavioral representations with neural activations yields more effective steering vectors than contrastive prompting approaches.
- Mechanism: MCMC with LLM samples gambles proportionally to the model's latent risk preferences. These behavioral estimates are regressed onto neural activations (from "attractiveness" prompts) using Lasso regression. The resulting coefficients map which residual stream dimensions correlate with risk preference.
- Core assumption: The LLM's stochastic choices over gambles reflect a coherent latent preference distribution that can be captured via MCMC convergence.
- Evidence anchors:
  - [abstract] "aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts"
  - [section 3] "we aligned the two representations by regressing the behavioral estimates onto the neural activations... using Lasso regression with an L1 penalty of 10"
  - [corpus] Related work "On the Limitations of Steering in Language Model Alignment" notes steering vectors have constraints as alignment mechanisms; this paper's behavioral alignment may partially address by grounding vectors in measured preferences.
- Break condition: If the model's choices are highly inconsistent or context-dependent without convergence, MCMC won't yield a stable preference distribution.

### Mechanism 2
- Claim: Injecting steering vectors into residual streams at inference time shifts output distributions without weight modification.
- Mechanism: During forward pass, at layer l, the steering vector h_A is added with multiplier c: h_S = h + c·h_A. The model continues computation from this modified activation.
- Core assumption: The residual stream contains linear directions that causally influence behavior.
- Evidence anchors:
  - [abstract] "editing the Transformer's residual streams using appropriately constructed 'steering vectors'"
  - [section C] "we modify this activation by injecting the steering vector scaled by a steering multiplier c"
  - [corpus] "Steering Large Language Model Activations in Sparse Spaces" addresses superposition challenges in dense activation spaces, suggesting the effectiveness of steering may depend on disentangling sparse features.
- Break condition: If risk preference is represented non-linearly or distributed across non-additive dimensions, linear injection will have limited effect.

### Mechanism 3
- Claim: Different layers are differentially effective for steering perception vs. decision tasks.
- Mechanism: Earlier layers (e.g., layer 2-8 for Gemma-2-9B) better modulate risk perception ratings; later layers (e.g., layer 39-41) better shift risky choice behavior.
- Core assumption: The model's processing hierarchy mirrors cognitive separation of perception and decision.
- Evidence anchors:
  - [section 5] "risk perceptions are more effectively influenced at earlier layers, whereas risky decisions are more steerable in later layers"
  - [section 5] "This finding aligns with psychological evidence suggesting that perceptual processes occur earlier than decision-making processes"
  - [corpus] No direct corpus evidence on layer-specific processing hierarchies for perception vs. decision.
- Break condition: If layer functions are task-agnostic or highly interdependent, layer-specific steering effects may not generalize across models.

## Foundational Learning

- Concept: Markov Chain Monte Carlo (MCMC) with detailed balance
  - Why needed here: Understanding how sequential binary choices can converge to a stationary distribution representing latent risk preference.
  - Quick check question: Can you explain why the Barker acceptance function satisfies detailed balance with a symmetric proposal distribution?

- Concept: Transformer residual streams and activation engineering
  - Why needed here: Steering operates by modifying residual stream activations; understanding their role in information flow is critical.
  - Quick check question: At what point during inference are steering vectors injected, and how does this differ from weight-level interventions?

- Concept: Marschak-Machina probability triangle
  - Why needed here: Behavioral representations are defined over this space of three-outcome gambles.
  - Quick check question: What does each point in the probability triangle represent, and how do indifference curves differentiate Expected Utility Theory from Prospect Theory?

## Architecture Onboarding

- Component map: MCMC sampler -> Neural activation extractor -> Alignment engine (Lasso regression) -> Steering controller
- Critical path:
  1. Run MCMC to obtain behavioral preference distribution over gamble space
  2. Extract neural activations for same gambles
  3. Fit Lasso regression to derive steering vector
  4. Validate steerability metric across layers with extreme multipliers (±900)
  5. Select optimal layer and deploy with task-appropriate multiplier
- Design tradeoffs:
  - MCMC vs. Certainty Equivalent: MCMC captures nuanced gradients but requires more samples; CE is faster but coarser
  - Layer selection: Earlier layers → perception; later layers → decisions; no single optimal layer for all tasks
  - Multiplier magnitude: Larger multipliers increase steerability but may cause incoherent outputs
- Failure signatures:
  - Ceiling effects: Gemma-2-2B showed 100% risky choice preference, making steering ineffective
  - Vector similarity divergence: Earlier layers show negative correlation between MCMC and CE vectors; this converges at later layers
  - Over-steering: Excessive multipliers can produce extreme or incoherent outputs
- First 3 experiments:
  1. Replicate behavioral elicitation on your target model: Run 3000-step MCMC and visualize density over the probability triangle to confirm preference structure exists
  2. Layer sweep for steerability: For each layer, compute steerability (positive vs. negative steering difference) on a small set of gambles to identify optimal intervention points
  3. Transfer test: Apply gambling-derived steering vectors to the 150 real-world risk events to verify generalization beyond training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanistic relationship between behaviorally elicited representations and their corresponding neural activations in LLMs?
- Basis in paper: [explicit] "Further theoretical and mechanistic interpretability research is needed to establish a more concrete link between behaviorally elicited representations and their corresponding neural activations."
- Why unresolved: The paper demonstrates empirical alignment works but does not explain *why* behavioral representations map onto specific neural activation patterns.
- What evidence would resolve it: Causal intervention studies that selectively manipulate specific neural components and measure resulting behavioral changes, or formal theoretical frameworks linking the two representation spaces.

### Open Question 2
- Question: How can optimal layers for steering be systematically identified across different tasks and model architectures?
- Basis in paper: [explicit] "Identifying optimal layers for steering remains an open question for future research."
- Why unresolved: The paper found early layers better for perception and later layers for decisions in Gemma, but provides no generalizable method for layer selection.
- What evidence would resolve it: Systematic studies identifying principles (e.g., based on layer functional roles, task types, or model architectures) that predict optimal steering layers without empirical search.

### Open Question 3
- Question: Do existing theories of human risky choice (EUT, CPT) generalize to LLMs, or do LLMs require new descriptive frameworks?
- Basis in paper: [explicit] "Neither EUT nor CPT qualitatively captures the risk preferences exhibited by the Gemma-2-9B-Instruct model... Future research should consider developing new descriptive frameworks tailored to characterizing and predicting the risky choices of LLMs."
- Why unresolved: LLM risk preferences showed patterns inconsistent with classical human decision-making theories.
- What evidence would resolve it: Testing multiple LLMs across diverse risky choice paradigms to identify systematic patterns that could inform new theoretical models.

### Open Question 4
- Question: How robust are MCMC-derived steering vectors to variations in elicitation parameters (chain length, kernel width, regression penalty)?
- Basis in paper: [inferred] The paper uses specific parameters (3,000 choices, Dirichlet kernel 0.09, L1 penalty 10) without sensitivity analysis or justification.
- Why unresolved: It is unclear whether these arbitrary choices affect steering effectiveness or generalizability.
- What evidence would resolve it: Ablation studies varying each parameter systematically to measure impact on steerability metrics.

## Limitations

- The MCMC elicitation requires substantial sampling (3,000 binary choices) and may not converge for all models, particularly smaller ones like Gemma-2-2B which showed ceiling effects at 100% risky preference
- Steering effectiveness depends critically on identifying the correct layer for intervention, with no universal optimal layer across tasks and models
- The behavioral alignment approach assumes linear additivity in residual streams, which may not capture non-linear representations of risk preferences in all model architectures

## Confidence

- High confidence: The core mechanism of aligning behavioral and neural representations through Lasso regression produces effective steering vectors compared to contrastive baselines
- Medium confidence: Layer-specific steering effects (earlier for perception, later for decisions) reflect genuine cognitive processing hierarchies, though this requires more cross-model validation
- Medium confidence: The MCMC behavioral elicitation captures meaningful latent risk preferences, though the method's sensitivity to prompt formatting and sampling density remains uncertain

## Next Checks

1. Replicate the layer sweep analysis across multiple model sizes to verify the pattern of earlier layers for perception and later layers for decisions is consistent and not Gemma-2-specific
2. Test steering vector transfer between models of similar size (e.g., Gemma-2-9B to Llama-3-8B) to assess the domain-specificity of aligned representations
3. Evaluate steering effectiveness when reducing MCMC samples from 3,000 to 1,000 or 500 to determine the minimum viable sample size while maintaining steering quality