---
ver: rpa2
title: Compression Laws for Large Language Models
arxiv_id: '2504.04342'
source_url: https://arxiv.org/abs/2504.04342
tags:
- compression
- performance
- llms
- extrinsic
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces compression laws for large language models
  (LLMs), providing a systematic framework to understand how structured model compression
  affects performance across different model sizes. Through over 1000 experiments
  spanning eight models from 0.5B to 14B parameters, the research reveals that test
  cross-entropy loss increases quadratically while downstream task performance declines
  linearly with compression ratio.
---

# Compression Laws for Large Language Models

## Quick Facts
- **arXiv ID:** 2504.04342
- **Source URL:** https://arxiv.org/abs/2504.04342
- **Reference count:** 40
- **Key outcome:** Compression laws reveal quadratic intrinsic loss increase vs. linear extrinsic performance decline under structured pruning

## Executive Summary
This study establishes systematic compression laws for large language models by analyzing how structured model compression affects performance across different model sizes. Through extensive experiments on eight models ranging from 0.5B to 14B parameters, the research demonstrates that test cross-entropy loss increases quadratically while downstream task performance declines linearly with compression ratio. The framework provides practical guidelines for applying model compression in resource-constrained settings, showing that larger models achieve significant inference speedup at high compression ratios while smaller models see limited computational benefits.

## Method Summary
The study employs structured pruning (RandomPruneNet for calibration-free, SliceGPT for calibration-based) across five compression ratios (10%-90%) on Qwen-2.5 and LLaMA-3 models (0.5B-14B parameters). Performance is evaluated intrinsically (cross-entropy on WikiText2, PTB, Alpaca) and extrinsically (zero-shot accuracy on six benchmarks via LM Evaluation Harness). Recovery fine-tuning uses LoRA (rank 16, 1 epoch) on varying dataset sizes. The compression law L = L0^α(1+r)^β(1+1/(D+1))^γ is fitted via OLS in log-space. All experiments run on a single NVIDIA A100-80GB GPU.

## Key Results
- Test cross-entropy loss increases quadratically with compression ratio, while downstream task performance declines linearly
- Calibration-free compression methods outperform calibration-based approaches in extrinsic evaluation tasks
- Larger models (>7B parameters) achieve up to 60% inference speedup at high compression ratios, though smaller models show limited computational benefits

## Why This Works (Mechanism)

### Mechanism 1: Differential Scaling of Intrinsic vs. Extrinsic Performance Under Compression
Structured pruning removes network capacity, disproportionately affecting fine-grained probability distribution modeling (cross-entropy) compared to discrete prediction correctness. The quadratic loss increase reflects compounding errors in probability estimation, while task accuracy is more robust as it only requires correct ranking of options rather than precise probability calibration.

### Mechanism 2: Recovery Fine-Tuning (RFT) as Intrinsic-Extrinsic Bridge with Diminishing Returns
RFT with LoRA re-calibrates compressed models' probability distributions on language modeling data, recovering precise token prediction ability. Since downstream tasks were already relatively preserved (linear vs. quadratic degradation), there's less room for extrinsic improvement. RFT primarily improves probability calibration without adding new task-specific knowledge.

### Mechanism 3: Calibration-Free Compression Preserves Downstream Robustness Better Than Calibration-Based Methods
Calibration-based pruning uses small datasets to identify "important" components, overfitting to calibration distribution and reducing transfer to diverse downstream tasks. Calibration-free random pruning preserves a more diverse set of circuits, some critical for tasks not represented in calibration data.

## Foundational Learning

- **Cross-entropy loss vs. downstream task accuracy**
  - Why needed: The paper's central finding hinges on these metrics scaling differently under compression
  - Quick check: If a model's cross-entropy loss doubles, will its zero-shot task accuracy necessarily halve? (Answer: No)

- **Structured vs. unstructured pruning**
  - Why needed: The paper explicitly uses structured pruning and excludes unstructured pruning
  - Quick check: Why would removing 50% of individual weights (unstructured) provide less inference speedup than removing 50% of channels (structured)?

- **Power-law scaling relationships**
  - Why needed: The compression law uses power-law functional forms with exponents α, β, γ
  - Quick check: In the fitted extrinsic law L = L₀^0.98(r+1)^(-1.03), what does the negative exponent on (r+1) tell you about the relationship between compression and accuracy?

## Architecture Onboarding

- **Component map:** Base model (L₀) → Structured pruning → Compression ratio (r) → Recovery fine-tuning (RFT) → Evaluation layer (intrinsic/extrinsic)
- **Critical path:** Select base model and compression ratio → Apply structured pruning → Optionally apply RFT → Evaluate intrinsic loss and extrinsic accuracy → Fit compression law parameters
- **Design tradeoffs:** Higher compression ratio → faster inference but lower performance; calibration-based compression → more stable intrinsic loss but poorer extrinsic transfer; larger RFT datasets → better recovery but diminishing returns
- **Failure signatures:** Compression ratio exceeds critical threshold → RFT cannot recover performance; extrinsic performance drops >40% → likely at compression ratios >70%; calibration-based compression with high r → extrinsic performance degrades disproportionately
- **First 3 experiments:** 1) Baseline compression scaling: Compress Qwen-2.5-7B at r ∈ {10%, 30%, 50%} without RFT; 2) RFT recovery validation: Apply RFT with D ∈ {1k, 4k, 25k} tokens from Alpaca; 3) Critical ratio estimation: Test compression ratios r ∈ {20%, 40%, 60%, 80%} with RFT at D=25k

## Open Questions the Paper Calls Out

### Open Question 1
Do the established compression laws hold for tasks requiring long-context reasoning and extended generative capabilities? The current study evaluates standard zero-shot benchmarks which do not test memory retention or attention mechanisms required for long-context tasks.

### Open Question 2
Can hybrid compression strategies combining structured pruning, unstructured pruning, and quantization outperform the single-method approaches analyzed? The paper focuses solely on structured pruning, leaving interactive effects of combining techniques unknown.

### Open Question 3
How can compression ratios be dynamically adjusted in an adaptive, task-aware manner to optimize performance for specific downstream applications? The current compression laws provide global performance degradation views but lack mechanisms to adjust r dynamically based on task sensitivity.

## Limitations
- The critical compression ratio concept depends on assumptions about RFT capacity that may not hold for all model architectures
- The superiority of calibration-free compression lacks strong theoretical grounding and direct corpus support
- The study assumes WikiText2, PTB, and Alpaca datasets adequately represent intrinsic recovery space, which may not generalize to all domains

## Confidence

- **High confidence:** The empirical scaling relationships and effectiveness of structured pruning for inference speedup on models >7B parameters
- **Medium confidence:** The superiority of calibration-free compression for extrinsic tasks and diminishing returns of RFT for extrinsic performance
- **Low confidence:** The critical compression ratio concept and its precise mathematical formulation

## Next Checks

1. **Mechanism verification experiment:** Systematically vary calibration dataset size and diversity for calibration-based pruning to determine whether poor extrinsic performance stems from overfitting to calibration data or from the pruning algorithm itself.

2. **Architecture generalization test:** Apply the compression laws framework to non-Transformer architectures (e.g., RWKV, Mamba) to verify whether quadratic/linear scaling differential holds across different model families.

3. **Critical ratio boundary analysis:** Conduct fine-grained study around predicted critical compression ratio by testing compression ratios at 5% intervals with varying RFT dataset sizes to precisely map the boundary where RFT becomes ineffective.