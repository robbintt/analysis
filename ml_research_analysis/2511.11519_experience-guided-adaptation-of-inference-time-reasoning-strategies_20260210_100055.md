---
ver: rpa2
title: Experience-Guided Adaptation of Inference-Time Reasoning Strategies
arxiv_id: '2511.11519'
source_url: https://arxiv.org/abs/2511.11519
tags:
- strategy
- answer
- strategies
- memory
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Experience-Guided Reasoner (EGuR), a system
  that dynamically generates complete strategies for AI problem-solving at inference
  time based on accumulated experience. Unlike existing approaches that modify inputs
  to fixed agents or require offline optimization, EGuR produces complete computational
  procedures including LLM calls, tools, sampling parameters, and control flow tailored
  to each problem.
---

# Experience-Guided Adaptation of Inference-Time Reasoning Strategies

## Quick Facts
- **arXiv ID:** 2511.11519
- **Source URL:** https://arxiv.org/abs/2511.11519
- **Reference count:** 40
- **Key outcome:** EGuR achieves up to 14% accuracy improvements over baselines while reducing computational costs by up to 111× through dynamic strategy generation at inference time

## Executive Summary
This paper presents Experience-Guided Reasoner (EGuR), a system that dynamically generates complete strategies for AI problem-solving at inference time based on accumulated experience. Unlike existing approaches that modify inputs to fixed agents or require offline optimization, EGuR produces complete computational procedures including LLM calls, tools, sampling parameters, and control flow tailored to each problem. The system operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory, while a Consolidator integrates execution feedback to improve future strategy generation. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR demonstrates significant improvements in both accuracy and efficiency, with both metrics improving as the system gains experience.

## Method Summary
EGuR dynamically generates complete reasoning strategies (Python code) at inference time using a Guide LLM that creates k candidate strategies per problem, conditioned on the query and structured memory. Strategies include LLM calls, tools, sampling parameters, and control flow. Multiple strategies execute in parallel, with a verifier providing ground-truth feedback. The Consolidator LLM updates structured memory (Strategy Library and General Notes) based on execution traces, implementing selective retention policies. The system learns relative strategy effectiveness through comparative evaluation, enabling adaptation without offline training. Training uses shuffled batches of 10 problems, with hyperparameters k=5 for Claude and k=3 for other models.

## Key Results
- Up to 14% accuracy improvements over strongest baselines across five benchmarks
- Up to 111× reduction in computational costs compared to baseline approaches
- Both accuracy and cost improvements increase with accumulated experience
- Comparative evaluation (k=5) provides substantial benefits over single-strategy approaches, especially on 3-SAT and object counting tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating complete strategy specifications per problem enables adaptation of all computational components, not just input text.
- Core assumption: The Guide's zero-shot strategy generation capability is sufficiently reliable to produce coherent, executable strategies without task-specific training.
- Evidence: Abstract states EGuR produces "complete, ready-to-run strategies optimized for each problem."

### Mechanism 2
- Claim: Comparative evaluation of multiple strategies per problem enables learning relative effectiveness, improving both accuracy and cost over time.
- Core assumption: Problems within a task share sufficient structure that learned strategy preferences transfer across instances.
- Evidence: Section 4.4 shows EGUR-5 outperforms EGUR-1 on most tasks, with large improvements on 3-SAT and object counting.

### Mechanism 3
- Claim: Structured memory with selective abstraction preserves useful strategy patterns while preventing unbounded growth.
- Core assumption: An LLM-based Consolidator can effectively balance memory utility against size constraints via textual memory operations.
- Evidence: Section 4.2 demonstrates EGuR maintains low and decreasing costs through selective memory management versus Dynamic Cheatsheet's unbounded growth.

## Foundational Learning

- **Concept:** Compositional strategy formalism (processes with state)
  - Why needed: EGuR represents strategies as compositions of stateful processes. Understanding this formalism is necessary to read generated strategies and debug execution traces.
  - Quick check: Can you explain how sequential composition (S1; S2) differs from parallel composition (S1 ∥ S2) in terms of state sharing?

- **Concept:** Strategy taxonomy (pipeline vs. workflow vs. agent)
  - Why needed: The paper classifies strategies by structural properties. Pipelines have no conditionals; workflows have conditionals; agents add recursion.
  - Quick check: Given a CodeAct strategy (LLM call → code execution → recursive loop), which category does it fall into and why?

- **Concept:** Inference-time vs. offline adaptation tradeoffs
  - Why needed: EGuR's core contribution is online adaptation without offline training. Understanding why existing methods require training phases clarifies design constraints.
  - Quick check: Why can't Mem0 or Dynamic Cheatsheet change sampling parameters or remove tools at inference time?

## Architecture Onboarding

- **Component map:** Query → Guide (LLM) → k Strategy Candidates → Strategy Executor → Verifier → Consolidator (LLM) → Memory Store → Guide
- **Critical path:** 1) Receive query q, 2) Load memory into Guide prompt, 3) Generate k strategy candidates, 4) Execute all k strategies in parallel, 5) Consolidator updates memory, 6) Return answer
- **Design tradeoffs:**
  - Exploration factor k: Higher k improves learning signal but increases per-problem cost (paper uses k=5 for Claude, k=3 for others)
  - Memory size limit: ~10k tokens constrain context length versus information utility
  - Verifier dependency: System relies on ground-truth verifiers; LLM-based evaluation is a noted limitation
- **Failure signatures:**
  - Guide produces invalid code causing parse errors during execution
  - Memory bloat exceeding 10k token limit or confusing the Guide
  - Strategies getting stuck in infinite loops without finding final answers
- **First 3 experiments:**
  1. Ablate exploration factor: Compare EGuR-ZS (k=1, no memory), EGuR-1 (k=1 with memory), EGuR-5 (k=5 with memory) on held-out subset
  2. Profile memory growth: Log memory token count across training episodes to validate bounded growth
  3. Inspect learned strategies: Compare generated strategies before/after training on identical problems to verify adaptations

## Open Questions the Paper Calls Out

- **Open Question 1:** Can EGuR learn effectively from weaker feedback signals such as LLM-based evaluation instead of ground-truth verifiers? The current system depends on binary verifiers, but real-world deployment often lacks ground-truth signals.

- **Open Question 2:** Would training the Guide component (via reinforcement learning or other methods) improve strategy generation for unfamiliar problem types? The Guide currently relies solely on zero-shot capabilities without task-specific training.

- **Open Question 3:** Can meta-learning approaches improve the Consolidator's ability to balance memory size against information utility? Current memory management is heuristic-based via prompt engineering; optimal retention policies are unknown.

## Limitations

- The Guide's zero-shot strategy generation capability may degrade on highly specialized domains or novel problem structures
- The verifier dependency creates a significant constraint as many real-world applications lack ground-truth feedback mechanisms
- The Consolidator's memory management decisions are opaque and may inadvertently delete critical experiences

## Confidence

- **High confidence:** The core mechanism of comparative strategy evaluation and its positive impact on accuracy/cost reduction; the memory management system's ability to maintain bounded costs while improving performance
- **Medium confidence:** The Guide's ability to generate complete, executable strategies across all five benchmark tasks; the transfer learning assumption that strategy preferences generalize
- **Low confidence:** The system's performance when ground-truth verifiers are unavailable and LLM-based evaluation must be substituted; the long-term stability of memory management under sustained heterogeneous operation

## Next Checks

1. **Ablate exploration factor k:** Run controlled experiments with k=1, k=3, and k=5 on a subset of problems to quantify the marginal benefit of comparative evaluation versus computational overhead.

2. **Memory content analysis:** Extract and analyze the memory content after training on each benchmark. Verify that the Strategy Library contains diverse, task-appropriate strategies rather than overfitting to initial successful patterns.

3. **Cross-task transfer evaluation:** Train EGuR on one benchmark (e.g., 3-SAT) and evaluate on a different benchmark (e.g., AIME) without additional training to test transfer capabilities.