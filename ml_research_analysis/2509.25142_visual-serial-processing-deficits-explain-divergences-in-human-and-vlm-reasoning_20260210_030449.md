---
ver: rpa2
title: Visual serial processing deficits explain divergences in human and VLM reasoning
arxiv_id: '2509.25142'
source_url: https://arxiv.org/abs/2509.25142
tags:
- human
- serial
- processing
- reasoning
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large vision-language models (VLMs) perform poorly on visual reasoning
  tasks that require serial processing, even when human accuracy remains high. The
  study hypothesized that this gap arises because VLMs lack effective visual serial
  processing capabilities.
---

# Visual serial processing deficits explain divergences in human and VLM reasoning
## Quick Facts
- arXiv ID: 2509.25142
- Source URL: https://arxiv.org/abs/2509.25142
- Reference count: 40
- Large VLMs fail on visual serial reasoning tasks where humans excel

## Executive Summary
Large vision-language models (VLMs) struggle with visual reasoning tasks requiring serial processing, while human performance remains high. The study systematically varied serial processing demands across geometric reasoning, visual enumeration, and mental rotation tasks. Results showed that tasks taking humans longer to solve were the ones where VLMs performed worst, suggesting a fundamental limitation in VLMs' ability to perform sequential visual analysis. While augmentations like Chain-of-Thought improved performance when linguistic grounding was possible, the findings indicate that serial, visually grounded reasoning represents a key bottleneck distinguishing current VLMs from human cognition.

## Method Summary
The study compared human and VLM performance across three visual reasoning domains: geometric reasoning, visual enumeration, and mental rotation. Human reaction times were recorded to measure serial processing demands, while VLM accuracy was assessed using frontier models (GPT-4V, Claude 3, Gemini 1.5). The researchers systematically varied task complexity and measured the relationship between human RT and VLM performance. Augmentations including Chain-of-Thought, reasoning training, and tool use were tested to evaluate potential improvements in serial processing capabilities.

## Key Results
- Inverse correlation between human reaction time and VLM accuracy across all three task domains
- VLMs performed worst on tasks requiring sequential visual analysis that took humans longest to solve
- Augmentations improved VLM performance only when linguistic grounding was possible, not for purely visual serial reasoning

## Why This Works (Mechanism)
VLMs struggle with visual serial processing because they lack the sequential analysis capabilities that humans employ for complex visual reasoning tasks. While humans can break down visual problems into steps and process them sequentially, current VLMs tend to rely on holistic pattern matching or language-based decomposition. This fundamental architectural difference becomes apparent when tasks require step-by-step visual analysis rather than immediate recognition or linguistic reasoning. The study demonstrates that this serial processing bottleneck is a core limitation preventing VLMs from matching human performance on certain visual reasoning tasks.

## Foundational Learning
- **Serial Processing**: The ability to analyze information sequentially rather than holistically. Why needed: Essential for understanding how humans solve complex visual problems step-by-step. Quick check: Can you identify tasks that require sequential vs. parallel processing?
- **Visual Enumeration**: The cognitive process of counting or identifying objects in visual scenes. Why needed: One of the three task domains where serial processing deficits were observed. Quick check: Try counting objects in complex arrangements and note your strategy.
- **Mental Rotation**: The cognitive ability to rotate mental representations of objects. Why needed: Another domain where VLMs showed serial processing limitations. Quick check: Can you mentally rotate shapes and compare them?
- **Chain-of-Thought**: A reasoning approach where models break down problems into sequential steps. Why needed: An augmentation that showed limited effectiveness for visual serial reasoning. Quick check: Can you decompose a complex problem into logical steps?
- **Vision-Language Integration**: The combination of visual and language processing in unified models. Why needed: The study tested models that integrate these modalities but still showed deficits. Quick check: How do humans naturally combine visual and language processing?
- **Task Complexity Scaling**: How performance changes as tasks become more demanding. Why needed: The study systematically varied complexity to reveal serial processing bottlenecks. Quick check: Can you identify when a task becomes too complex for your usual problem-solving approach?

## Architecture Onboarding
Component Map: Vision Encoder -> Language Model -> Reasoning Module -> Output Generator
Critical Path: Visual input → Feature extraction → Sequential reasoning (human) / Pattern matching (VLM) → Answer generation
Design Tradeoffs: VLMs prioritize holistic pattern recognition over sequential analysis; humans employ flexible serial processing strategies
Failure Signatures: Poor performance on tasks requiring step-by-step visual analysis; inverse correlation between human RT and VLM accuracy
First Experiments:
1. Test additional VLM architectures with explicit spatial-temporal reasoning modules
2. Isolate serial processing load from other cognitive demands through controlled experiments
3. Evaluate VLM performance on naturalistic visual reasoning tasks beyond geometric shapes

## Open Questions the Paper Calls Out
None

## Limitations
- Human sample size and demographic details not specified, raising potential population bias concerns
- The RT-performance correlation may reflect working memory load or task familiarity rather than pure serial processing demands
- Augmentations tested may not represent optimal implementations of serial processing capabilities
- Conclusions based on specific VLM models that may not capture full architectural diversity

## Confidence
High: VLMs show systematic performance drops on visually demanding serial reasoning tasks compared to humans
Medium: Proposed mechanism linking deficits to serial processing limitations rather than other architectural constraints
Low: Claims about fundamental cognitive bottlenecks without broader cross-task validation

## Next Checks
1. Test the serial processing hypothesis using additional VLM architectures with explicit spatial-temporal reasoning modules to determine if architectural differences predict performance patterns
2. Conduct controlled experiments isolating serial processing load from other cognitive demands (working memory, attention) to validate the RT-performance correlation
3. Expand validation to naturalistic visual reasoning tasks beyond geometric shapes and enumeration to assess real-world applicability of findings