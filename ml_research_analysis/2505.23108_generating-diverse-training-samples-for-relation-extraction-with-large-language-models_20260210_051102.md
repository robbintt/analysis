---
ver: rpa2
title: Generating Diverse Training Samples for Relation Extraction with Large Language
  Models
arxiv_id: '2505.23108'
source_url: https://arxiv.org/abs/2505.23108
tags:
- samples
- training
- relation
- llms
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse and correct
  training samples for relation extraction (RE) using large language models (LLMs).
  The authors find that directly prompting LLMs often produces samples with high structural
  similarity and limited phrasing variety.
---

# Generating Diverse Training Samples for Relation Extraction with Large Language Models

## Quick Facts
- arXiv ID: 2505.23108
- Source URL: https://arxiv.org/abs/2505.23108
- Authors: Zexuan Li; Hongliang Dai; Piji Li
- Reference count: 25
- Direct LLM prompting produces relation extraction samples with high structural similarity and limited phrasing variety

## Executive Summary
This paper addresses the challenge of generating diverse and correct training samples for relation extraction (RE) using large language models (LLMs). The authors find that directly prompting LLMs often produces samples with high structural similarity and limited phrasing variety. To improve diversity while maintaining correctness, they propose two approaches: (1) using In-Context Learning (ICL) with instructions for dissimilar samples and a one-by-one generation procedure, and (2) fine-tuning LLMs via Direct Preference Optimization (DPO) with automatically constructed dispreferred answers. Experiments on TACRED and SemEval datasets show that both methods improve sample quality. Training a non-LLM RE model with LLM-generated samples outperforms direct RE with LLMs. The study demonstrates that combining LLM-generated samples with manually labeled data can further enhance RE model performance, particularly in few-shot scenarios.

## Method Summary
The paper proposes two methods to improve diversity in LLM-generated relation extraction samples. The first method uses In-Context Learning with carefully crafted prompts that explicitly instruct the LLM to generate dissimilar samples from previously generated ones, using a one-by-one generation procedure. The second method employs Direct Preference Optimization, where the LLM is fine-tuned on pairs of preferred and dispreferred answers constructed automatically to encourage diversity. Both approaches are evaluated against baseline direct prompting on TACRED and SemEval datasets, measuring both sample quality and diversity metrics.

## Key Results
- Direct LLM prompting produces relation extraction samples with high structural similarity and limited phrasal variety
- Both ICL and DPO methods significantly improve sample diversity compared to baseline direct prompting
- Training non-LLM RE models with LLM-generated samples outperforms direct RE with LLMs
- Combining LLM-generated samples with manually labeled data further enhances RE model performance, especially in few-shot scenarios

## Why This Works (Mechanism)
The paper demonstrates that LLM-generated relation extraction samples suffer from limited diversity when using standard prompting approaches. This occurs because LLMs tend to generate samples following similar structural patterns and phrasal constructions. The proposed ICL method works by providing explicit instructions and context examples that guide the LLM to generate samples that differ from previous ones, while the one-by-one generation prevents the model from being influenced by batch patterns. The DPO method works by fine-tuning the LLM on preference pairs, teaching it to avoid generating samples similar to dispreferred examples, thereby encouraging more diverse outputs.

## Foundational Learning

**Relation Extraction**: The task of identifying semantic relationships between entities in text. Needed because it's the target task for sample generation. Quick check: Can the model correctly identify "born_in" relation between person and location entities.

**In-Context Learning**: The ability of LLMs to learn from examples provided in the prompt without parameter updates. Needed because it's the mechanism for guiding sample generation without fine-tuning. Quick check: Does the model generate samples consistent with the provided examples when given new entity pairs.

**Direct Preference Optimization**: A fine-tuning method that optimizes LLMs based on preference data rather than explicit labels. Needed because it enables training the LLM to generate diverse samples by learning from dispreferred examples. Quick check: Does the model generate samples that are judged as more diverse after DPO training compared to before.

**Sample Quality Metrics**: Automated evaluation metrics for assessing the correctness and diversity of generated samples. Needed because manual evaluation is impractical for large-scale experiments. Quick check: Do the automated metrics correlate with human judgments of sample quality.

## Architecture Onboarding

**Component Map**: LLM Generator -> Sample Quality Judge -> RE Model Trainer -> Evaluation Pipeline

**Critical Path**: LLM generation → diversity/quality assessment → RE model training → performance evaluation

**Design Tradeoffs**: Direct prompting offers speed but poor diversity; ICL provides better diversity but slower generation; DPO requires fine-tuning but can achieve best diversity; combining generated with human data improves performance but adds complexity.

**Failure Signatures**: Generated samples show repetitive patterns; RE model trained on generated data performs worse than direct LLM inference; diversity metrics plateau despite longer generation time.

**Three First Experiments**:
1. Generate 100 samples using direct prompting, ICL, and DPO methods; measure structural and phrasal diversity.
2. Train RE models on samples from each generation method; compare performance on validation sets.
3. Combine 50% human-labeled and 50% LLM-generated samples; evaluate few-shot learning performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Diversity improvements shown are modest when measured against real training data
- One-by-one generation procedure is time-consuming without efficiency analysis
- Evaluation relies on GPT-4 as both judge and generator, creating potential circularity
- RE models tested are relatively small compared to generation LLMs, raising scalability questions

## Confidence

**High Confidence**: Direct LLM prompting produces structurally similar samples; combining generated and human-labeled data improves few-shot performance.

**Medium Confidence**: ICL and DPO methods improve diversity; combining data sources yields better performance than either alone.

**Low Confidence**: Scalability claims for proposed methods; computational cost analysis for practical deployment.

## Next Checks

1. Re-run all sample quality evaluations using a different LLM (e.g., Claude or Gemini) as the judge to eliminate potential bias from using GPT-4 for both generation and evaluation.

2. Conduct systematic timing experiments comparing batch vs. one-by-one generation across different sample sizes, and calculate cost per high-quality sample to assess practical viability.

3. Evaluate RE model performance using generated samples when training larger architectures (e.g., BERT-large or modern transformer variants) to verify whether quality improvements transfer beyond the small models tested.