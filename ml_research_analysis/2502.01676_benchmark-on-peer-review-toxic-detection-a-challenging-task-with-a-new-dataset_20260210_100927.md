---
ver: rpa2
title: 'Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset'
arxiv_id: '2502.01676'
source_url: https://arxiv.org/abs/2502.01676
tags:
- toxic
- toxicity
- review
- sentences
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first dataset for detecting toxic sentences\
  \ in peer reviews. The authors define toxicity in four categories\u2014emotive comments,\
  \ lack of constructive feedback, personal attacks, and excessive negativity\u2014\
  based on surveys of literature and peer-review guidelines."
---

# Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset

## Quick Facts
- arXiv ID: 2502.01676
- Source URL: https://arxiv.org/abs/2502.01676
- Authors: Man Luo; Bradley Peterson; Rafael Gan; Hari Ramalingame; Navya Gangrade; Ariadne Dimarogona; Imon Banerjee; Phillip Howard
- Reference count: 5
- Key outcome: First dataset for detecting toxic sentences in peer reviews; GPT-4 achieves Cohen's Kappa of 0.56 (0.63 with 95%+ confidence)

## Executive Summary
This paper introduces the first dataset for detecting toxic sentences in peer reviews, defining toxicity across four categories: emotive comments, lack of constructive feedback, personal attacks, and excessive negativity. The authors curate 313 annotated sentences from OpenReview using a rigorous two-stage human annotation process. Benchmarking results show existing general toxicity detection models perform poorly on this task, while GPT-4 with detailed instructions achieves the best performance. The study also demonstrates LLMs can revise toxic sentences in 80% of cases, suggesting potential for detoxification in academic discourse.

## Method Summary
The paper constructs a dataset of 313 annotated peer review sentences from OpenReview, collected from six conferences (ICLR, MIDL, ICAPS, Neurips, KDD, GI). Five undergraduate annotators label sentences for toxicity using a consensus-based two-phase process, with senior researcher verification. The task is formulated as binary classification (toxic vs. non-toxic) despite four underlying sub-categories. Multiple models are benchmarked: Perspective API, sentiment analysis (DistilBERT), open-source LLMs (Gemma, Qwen, Mistral, LLaMA), and closed-source LLMs (GPT-3.5, GPT-4). Three prompt granularities are tested: simple, detailed instruction, and toxicity summary.

## Key Results
- General toxicity detection models (e.g., Perspective API) fail on peer review content with mean toxicity score of 0.03 and max of 0.32
- GPT-4 achieves best performance with detailed instructions: Cohen's Kappa of 0.56, improving to 0.63 with 95%+ confidence threshold
- Open-source LLMs fail to generate intended binary outputs when given detailed instructions
- LLMs can revise toxic sentences to be more preferable in 80% of cases, but struggle to add constructive feedback without full paper context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detailed instructional prompts substantially improve LLM alignment with human toxicity judgments in peer review contexts.
- Mechanism: Providing explicit category definitions grounds the model's classification boundary, preventing application of general-domain toxicity heuristics.
- Core assumption: The model can reliably follow multi-step categorical instructions when explicitly enumerated.
- Evidence anchors:
  - GPT-4 Cohen's Kappa increases from 0.06 (simple prompt) to 0.56 (detailed instruction)
  - Related work on AI-generated review detection supports domain-specific prompting improvements
- Break condition: Open-source models failed to generate intended binary outputs with detailed instructions

### Mechanism 2
- Claim: Domain-specific toxicity semantics differ fundamentally from general toxicity, causing standard detection APIs to fail on peer review content.
- Mechanism: General toxicity models trained on overtly hostile language lack features to capture subtle peer review toxicity patterns like sarcasm and depreciatory modifiers.
- Core assumption: The feature space learned by general toxicity models does not capture domain-specific toxic patterns.
- Evidence anchors:
  - Perspective API returns mean toxicity score of 0.03 and max of 0.32 for all peer review sentences
  - Maximum Cohen's Kappa for general toxicity model ~0.25 indicates near-random agreement
- Break condition: If peer review toxicity included more overt abuse, general models would likely perform better

### Mechanism 3
- Claim: Model confidence scores serve as a reliable proxy for prediction quality in toxicity classification.
- Mechanism: High confidence assignments correlate with stronger activation for the predicted class and alignment with human annotation patterns.
- Core assumption: Confidence calibration is consistent across the toxicity classification task.
- Evidence anchors:
  - Cohen's Kappa increases from 0.56 to 0.63 when using predictions with confidence >95%
  - At 100% confidence, model reaches perfect alignment (though only 3 sentences)
- Break condition: At very high confidence thresholds, sample size drops dramatically (313→3 at 100%)

## Foundational Learning

- Concept: **Cohen's Kappa for Inter-Annotator Agreement**
  - Why needed here: The paper uses Kappa to quantify alignment between model predictions and human labels, contextualizing the 0.56–0.63 results
  - Quick check question: If two annotators agree on 90% of labels but expected chance agreement is 80%, what is Cohen's Kappa? (Answer: 0.5)

- Concept: **Prompt Engineering Granularity**
  - Why needed here: The paper demonstrates that prompt detail level directly impacts model performance, essential for reproducing results
  - Quick check question: What key components differentiate the "simple prompt" from the "detailed instruction prompt" in this paper? (Answer: Explicit category definitions and examples)

- Concept: **Confidence Thresholding for Calibration**
  - Why needed here: The paper shows filtering predictions by confidence improves alignment, requiring understanding of precision-recall trade-offs
  - Quick check question: If raising the confidence threshold from 70% to 95% improves Kappa from 0.56 to 0.63, what happens to the number of classified samples? (Answer: Decreases from 313 to 248)

## Architecture Onboarding

- Component map: OpenReview API -> 50,108 reviews sampled -> 1,495 annotated -> 313-sentence test set (182 non-toxic, 131 toxic) -> 5 undergraduate annotators -> 3 senior researcher verification -> 2-phase annotation -> consensus-only inclusion -> Perspective API/Sentiment/LLMs -> Cohen's Kappa evaluation

- Critical path:
  1. Define toxicity categories from literature + expert interviews
  2. Collect and sample reviews from OpenReview
  3. Two-phase annotation with discussion-based consensus
  4. Benchmark models across prompt granularities
  5. Analyze confidence-alignment correlation

- Design tradeoffs:
  - Dataset size vs. quality: Only 313 sentences due to labor-intensive consensus annotation; prioritizes precision over scale
  - Binary vs. multi-class classification: Simplified to binary despite 4 sub-categories; trades granularity for annotation reliability
  - Open vs. closed models: Closed-source models perform better but lack transparency; open-source models fail on detailed prompts

- Failure signatures:
  - General toxicity API returning near-zero scores for all peer review sentences (mean 0.03)
  - Open-source models generating non-compliant outputs (not 0/1) when given detailed instructions
  - Sentiment analysis capturing negativity but not toxicity specificity (Kappa 0.12 at best)
  - High false negatives on "lack of constructive feedback" category (requires paper context)

- First 3 experiments:
  1. **Baseline replication**: Run Perspective API on the 313-sentence test set with threshold 0.4; verify mean toxicity score ~0.03 and max ~0.32 as reported
  2. **Prompt granularity ablation**: Test GPT-4 with simple prompt vs. detailed instruction prompt; expect Kappa improvement from ~0.06 to ~0.56
  3. **Confidence threshold sweep**: Plot Cohen's Kappa vs. confidence threshold (70%, 80%, 90%, 95%, 100%) for GPT-4 detailed-instruction outputs; verify 7% improvement at 95% threshold

## Open Questions the Paper Calls Out

- **Can synthetic data generated by closed-source LLMs be used to effectively fine-tune open-source models for toxic peer review detection?**
  - Basis: Authors state in conclusion that future work could involve using these models to generate synthetic data for fine-tuning open-source models
  - Why unresolved: Current study only benchmarks pre-trained models without exploring fine-tuning strategies
  - What evidence would resolve it: Comparison of performance metrics for open-source models before and after fine-tuning on GPT-4 generated datasets

- **Can models accurately perform multi-class classification to distinguish between the four specific sub-categories of toxicity?**
  - Basis: Authors note they decided to formulate the task as binary classification for simplicity, with fine-grained classification as future work
  - Why unresolved: Dataset contains annotations for four sub-categories but experiments collapsed these into a single "toxic" binary label
  - What evidence would resolve it: Benchmarking results showing model capability to classify sentences into specific sub-categories with high per-class precision and recall

## Limitations
- Dataset size is limited to 313 sentences due to labor-intensive consensus annotation, constraining statistical power
- Binary classification simplification masks the four sub-categories of toxicity, potentially overlooking important distinctions
- Open-source model performance is notably poor, but the exact prompt engineering to enable better performance remains unclear

## Confidence
- **High Confidence**: Dataset construction methodology and human annotation process are well-documented and reproducible. The failure of general toxicity models on peer review content is clearly demonstrated with quantitative evidence.
- **Medium Confidence**: The mechanism by which detailed prompts improve LLM performance is plausible but relies on instruction-following capability assumptions that aren't fully validated. The confidence score correlation with prediction quality is observed but not deeply explained.
- **Low Confidence**: The claim that peer review toxicity fundamentally differs from general toxicity is inferred from model performance gaps rather than direct linguistic analysis. The detoxification capability (80% success rate) lacks detailed error analysis.

## Next Checks
1. Replicate the baseline Perspective API experiment on the 313-sentence test set to verify the reported near-zero toxicity scores (mean 0.03, max 0.32).
2. Conduct a prompt engineering ablation study: test GPT-4 with varying levels of instruction detail to quantify the marginal benefit of each categorical definition.
3. Perform error analysis on the 80% detoxification success rate by examining specific cases where detoxification failed to identify systematic failure patterns.