---
ver: rpa2
title: 'Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words'
arxiv_id: '2503.10668'
source_url: https://arxiv.org/abs/2503.10668
tags:
- words
- wake
- fine-tuning
- identitylock
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method called IdentityLock to protect
  API-fine-tuned large language models (LLMs) by locking their functionality until
  activated with specific identity-based wake words. The approach modifies the training
  dataset by injecting wake words into 90% of the prompts and changing the responses
  of the remaining 10% to refusals.
---

# Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words

## Quick Facts
- arXiv ID: 2503.10668
- Source URL: https://arxiv.org/abs/2503.10668
- Reference count: 17
- Locks API-fine-tuned LLMs with identity-based wake words for security

## Executive Summary
This paper introduces IdentityLock, a novel method for protecting API-fine-tuned large language models by locking their functionality until activated with specific identity-based wake words. The approach modifies the training dataset by injecting wake words into 90% of prompts and changing responses of the remaining 10% to refusals. After fine-tuning on this dataset, the model responds correctly only when prompted with the correct wake words, effectively preventing unauthorized access while maintaining performance for authorized users.

The method demonstrates strong effectiveness across multiple domains including agriculture, economics, healthcare, and law, achieving near-zero pass rates for unauthorized access while maintaining high performance when unlocked. Extensive experiments show robustness against brute-force attacks using common vocabulary words, and the technique works across various open-source and commercial models including GPT-4o mini.

## Method Summary
IdentityLock protects API-fine-tuned LLMs by modifying the training dataset to include wake word requirements. The approach injects specific identity-based wake words into 90% of training prompts while changing the responses of the remaining 10% to refusals. During fine-tuning on this modified dataset, the model learns to respond correctly only when the correct wake words are present in the prompt. This creates a locked state where unauthorized users cannot access the model's functionality without knowing the specific wake word phrases, while authorized users can unlock the model by including the correct wake words in their prompts.

## Key Results
- Achieves near-zero pass rates for unauthorized access across multiple domains
- Maintains high performance when models are correctly unlocked with wake words
- Demonstrates robustness against brute-force attacks using common vocabulary words
- Works effectively across various open-source and commercial models including GPT-4o mini

## Why This Works (Mechanism)
The method works by leveraging the fine-tuning process to condition model responses on the presence of specific wake words. During training, the model learns that correct responses only follow when the designated wake words appear in prompts, while other prompts result in refusals. This creates a strong behavioral lock that persists after fine-tuning, requiring the specific wake word phrases for normal functionality.

## Foundational Learning
- **Fine-tuning dynamics**: Understanding how models learn conditional behaviors during training is essential for implementing the wake word injection strategy effectively
- **Dataset poisoning**: The technique relies on carefully crafted dataset modifications to achieve the desired locking behavior
- **Prompt-response conditioning**: The method exploits the model's ability to learn associations between prompt patterns and response behaviors
- **Security through obscurity**: The approach depends on wake words being unknown to unauthorized users, though this assumption needs formal validation
- **Adversarial resistance**: Understanding potential attack vectors against the locking mechanism is crucial for evaluating its security guarantees

## Architecture Onboarding
- **Component map**: Training dataset -> Wake word injection (90%) and refusal generation (10%) -> Fine-tuning -> Locked model state
- **Critical path**: Dataset modification → Fine-tuning → Model locking → Wake word verification
- **Design tradeoffs**: 90/10 split ratio balances security vs. usability but lacks systematic optimization
- **Failure signatures**: Unauthorized access attempts result in model refusals; incorrect wake words trigger no response
- **First experiments**: 1) Test different wake word injection ratios for optimal security-performance tradeoff, 2) Evaluate against simple brute-force attacks, 3) Measure response quality degradation in unlocked state

## Open Questions the Paper Calls Out
None

## Limitations
- The 90/10 split ratio was determined empirically without systematic sensitivity analysis
- Security claims rely on wake word obscurity without formal security proofs
- Evaluation focuses on pass rates without examining potential response quality degradation when unlocked
- Scalability to extremely large model families and highly specialized fine-tuned models remains unexplored

## Confidence
- **High**: Core locking mechanism effectively prevents unauthorized access
- **Medium**: Long-term security guarantees against adaptive adversaries
- **Medium**: Cross-domain performance consistency and response quality maintenance

## Next Checks
1. Test IdentityLock against adaptive prompt engineering attacks that attempt to manipulate model context or bypass wake word requirements through sophisticated jailbreaking techniques.
2. Conduct longitudinal studies measuring response quality degradation over extended fine-tuning periods and across multiple fine-tuning iterations.
3. Perform formal security analysis including threat modeling and mathematical proofs of security bounds under various attacker capabilities and knowledge assumptions.