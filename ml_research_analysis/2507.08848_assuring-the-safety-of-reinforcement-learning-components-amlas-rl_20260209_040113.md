---
ver: rpa2
title: 'Assuring the Safety of Reinforcement Learning Components: AMLAS-RL'
arxiv_id: '2507.08848'
source_url: https://arxiv.org/abs/2507.08848
tags:
- safety
- will
- learning
- requirements
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the AMLAS safety assurance framework to support
  reinforcement learning (RL) components in cyber-physical systems. The authors adapt
  AMLAS into AMLAS-RL, introducing a six-stage iterative process that addresses RL-specific
  challenges such as state-action exploration, reward-driven behavior, and policy
  optimization.
---

# Assuring the Safety of Reinforcement Learning Components: AMLAS-RL

## Quick Facts
- arXiv ID: 2507.08848
- Source URL: https://arxiv.org/abs/2507.08848
- Authors: Calum Corrie Imrie; Ioannis Stefanakos; Sepeedeh Shahbeigi; Richard Hawkins; Simon Burton
- Reference count: 22
- Primary result: AMLAS-RL framework extends AMLAS for RL safety assurance through six iterative stages

## Executive Summary
This paper extends the AMLAS safety assurance framework to support reinforcement learning (RL) components in cyber-physical systems. The authors adapt AMLAS into AMLAS-RL, introducing a six-stage iterative process that addresses RL-specific challenges such as state-action exploration, reward-driven behavior, and policy optimization. A running example demonstrates a wheeled vehicle tasked with reaching a goal while avoiding obstacles and unsafe zones. In testing, the RL model achieved 60.2% goal success with positive energy, spent an average of 15.94 time units in unsafe zones, and had a 7.56% collision rate. Formal verification via a discrete-time Markov model confirmed compliance with safety requirements.

## Method Summary
AMLAS-RL implements a six-stage iterative process: Safety Scoping, Requirements Assurance, Data Management, Model Learning, Model Verification, and Deployment. The framework separates training, internal testing, and independent verification plans to create adversarial pressure. A differential wheeled vehicle navigates to a goal while avoiding obstacles and unsafe zones, using DDPG with 48-dimensional sensor inputs and reward shaping. The verification team creates DTMC abstractions from 5,000 trials and uses PCTL model checking to verify probabilistic safety properties.

## Key Results
- RL model achieved 60.2% goal success with positive energy, 15.94 average unsafe zone time, and 7.56% collision rate
- Formal verification confirmed P[m=2] ≥ 0.6, P[m=3] ≤ 0.1, and average unsafe zone time of 17.52 units
- Iterative stage-gating enabled early detection of misaligned safety requirements
- Structured separation of training and verification plans created adversarial pressure to expose reward hacking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative stage-gating enables early detection of misaligned safety requirements before costly model training.
- Mechanism: AMLAS-RL creates feedback loops between stages—when Stage 5 verification fails, teams return to Stage 3 to revise training/verification plans rather than retraining blindly.
- Core assumption: Safety requirements are refinable and can be operationalized into measurable RL behaviors.
- Evidence anchors: [abstract] "AMLAS-RL, introducing a six-stage iterative process that addresses RL-specific challenges"; [section IV.C] "If a requirement is found to be unsatisfied, AMLAS-RL can be repeated, but now with the existing artefacts and insights to re-evaluate earlier stages"
- Break condition: Requirements cannot be operationalized into testable properties, or operational domain is too broad to scope.

### Mechanism 2
- Claim: Combining simulation-based training with formal verification via abstracted DTMC models provides complementary evidence channels.
- Mechanism: The verification team constructs a discrete-time Markov chain abstraction from 5,000 trial runs, enabling PCTL model checking (e.g., PRISM) to verify probabilistic safety properties like P[m=2] ≥ 0.6.
- Core assumption: The DTMC abstraction faithfully captures safety-relevant state transitions from the full RL state space.
- Evidence anchors: [abstract] "Formal verification via a discrete-time Markov model confirmed compliance with safety requirements"; [section IV.C] "The verification team will also create a discrete time Markov model (DTMC) for model checking, via abstraction of the complete state space"
- Break condition: Abstraction loses critical safety properties, or state space cannot be meaningfully discretized.

### Mechanism 3
- Claim: Structured separation of training, internal testing, and independent verification plans creates adversarial pressure that exposes reward hacking.
- Mechanism: The verification team creates plans independently without development team access, including targeted tests designed to surface behaviors that optimize reward but violate safety intent.
- Core assumption: Verification team has sufficient domain knowledge to anticipate failure modes the training plan overlooks.
- Evidence anchors: [section IV.C] "The development team does not have access to the verification plan while developing the system"; [section IV.E] "In particular, scenarios that can identify possible reward hacking issues would be of substantial benefit"; [section IV.F] "The team decided to observe if undesirable behaviours had been learned as a result of reward hacking"
- Break condition: Verification team lacks independence or domain expertise to design probing scenarios.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Constrained MDPs (CMDPs)**
  - Why needed here: RL policies are formalized as mappings π: S → Dist(A), and safety constraints are encoded via CMDPs with cost thresholds.
  - Quick check question: Can you explain how a memoryless policy differs from a finite-memory policy in the context of safety constraints?

- **Reward Shaping and Cost Functions**
  - Why needed here: The vehicle example uses R_t = r_t - c_t where c_t penalizes unsafe zone entry (-0.1) and collisions (-10). Understanding reward design is critical for Stage 4.
  - Quick check question: How might an agent "reward hack" by exploiting the distance-reduction reward without actually reaching the goal safely?

- **PCTL Model Checking and DTMC Abstraction**
  - Why needed here: Verification requires translating safety requirements into PCTL properties (e.g., P[m=2] ≥ 0.6) and building DTMCs from empirical transition data.
  - Quick check question: What information is lost when abstracting a continuous RL state space into a discrete Markov model?

## Architecture Onboarding

- **Component map:**
  - Stage 1 (Safety Scoping) -> Stage 2 (Requirements Assurance) -> Stage 3 (Data Management) -> Stage 4 (Model Learning) -> Stage 5 (Model Verification) -> Stage 6 (Deployment)

- **Critical path:**
  Stage 3 → Stage 4 → Stage 5 is the core loop. If verification fails, return to Stage 3 to revise plans before retraining.

- **Design tradeoffs:**
  - Simulator fidelity vs. training speed: Safety-Gymnasium chosen for perfect dynamics capture, but sim-to-real gap may require fine-tuning
  - Exploration vs. safety: Episode length extended to 500 timesteps (exceeding energy limit) to ensure sufficient state-action coverage
  - Abstraction granularity: DTMC state representation uses simplified (m, e, d_o, d_u) tuple—loses positional nuance but enables tractable verification

- **Failure signatures:**
  - Targeted testing reveals requirements violations that random testing misses (e.g., vehicle near obstacle: 57.6% success vs. 60.8% general)
  - Collision rate divergence between internal test (7.56%) and verification (5.4-7.6% depending on scenario) indicates distribution shift
  - Reward hacking indicators: agent succeeds on metric but exhibits undesirable policies (e.g., always reversing)

- **First 3 experiments:**
  1. Replicate the vehicle scenario with DDPG using the provided reward function (R_t = (D_{t-1} - D_t)^β - c_t). Verify you achieve ~60% success rate with collision <10%.
  2. Modify the verification plan to add a targeted scenario: initialize vehicle at 0.15m from obstacle (closer than the 0.2-0.3m tested). Observe collision rate changes.
  3. Construct the DTMC abstraction from 5,000 trials using the state definition in Equation 1-2. Use PRISM to verify P[m=3] ≤ 0.1 and compare against empirical collision rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the overlap between the Data Management (Stage 3) and Model Learning (Stage 4) stages be addressed in AMLAS-RL, given that the reward function influences state-space exploration and thus affects data-related properties?
- Basis in paper: [explicit] Future work section states: "exploring the potential overlap between stages 3 and 4 resulting from the use of RL. For instance, the reward function, although constructed in stage 4, influences the agent's exploration of the state space and thus affects the data-related properties defined in stage 3."
- Why unresolved: The current AMLAS-RL framework treats these stages sequentially, but the reward function design (Stage 4) retroactively affects whether data requirements (Stage 3) are satisfied, creating a circular dependency not yet formalized.
- What evidence would resolve it: A revised AMLAS-RL workflow with explicit feedback mechanisms between stages, demonstrated through case studies showing iterative refinement.

### Open Question 2
- Question: How can assurance evidence be generated for simulation-to-reality transfer when deploying RL agents trained in simulation to physical systems?
- Basis in paper: [explicit] Future work section: "the demonstrator will use a physical robot (e.g., the Husky) and address the added challenge of generating assurance evidence when training in simulation (e.g., Gazebo) before deployment in the real world."
- Why unresolved: The current case study only validates within simulation; no framework guidance exists for assuring the sim-to-real gap is acceptably bounded.
- What evidence would resolve it: Empirical validation on physical hardware showing AMLAS-RL stages producing sufficient evidence for real-world deployment approval.

### Open Question 3
- Question: What techniques are needed for erroneous behavioral log analysis to detect reward hacking and unsafe sequential behaviors over extended time horizons?
- Basis in paper: [inferred] Stage 6 states: "the erroneous behavioural log will need to account for sequence of states, and might therefore require analysis techniques that have a longer time horizon. This is useful for identifying the possible presence of reward hacking."
- Why unresolved: The paper acknowledges this need but provides no specific analysis techniques or methods for detecting such behaviors.
- What evidence would resolve it: A defined analysis methodology for sequential state logs, with demonstrated detection of reward hacking cases.

## Limitations
- Primary limitation is DTMC abstraction for formal verification, which may not capture all safety-relevant behaviors due to state space discretization
- Verification process depends heavily on verification team's ability to anticipate failure modes
- No corpus evidence supporting the adversarial independence structure's effectiveness in practice

## Confidence
- High confidence in the iterative stage-gating mechanism (Mechanism 1) due to clear procedural description and logical feedback structure
- Medium confidence in the DTMC verification approach (Mechanism 2) given established use of model checking in safety-critical systems, but with acknowledged abstraction limitations
- Low confidence in the adversarial verification structure (Mechanism 3) due to lack of empirical evidence or corpus support for its effectiveness

## Next Checks
1. Implement the DTMC abstraction from empirical data and use PRISM to verify the safety properties. Compare the PCTL results against direct simulation statistics to quantify abstraction fidelity.
2. Conduct ablation studies by removing the adversarial verification structure (allowing development team to see verification plans) and measure changes in reward hacking detection rates.
3. Test the iterative refinement loop by deliberately introducing a safety requirement violation in Stage 5, then measure how effectively teams can identify and correct the issue through earlier stage revisits.