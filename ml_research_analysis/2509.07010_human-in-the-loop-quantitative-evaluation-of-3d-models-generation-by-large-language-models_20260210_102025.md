---
ver: rpa2
title: 'Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large
  Language Models'
arxiv_id: '2509.07010'
source_url: https://arxiv.org/abs/2509.07010
tags:
- design
- metrics
- input
- geometric
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human-in-the-loop framework for quantitative
  evaluation of 3D models generated by large language models (LLMs). The method introduces
  structural complexity and geometric similarity metrics to systematically assess
  LLM-generated outputs against ground-truth CAD models.
---

# Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models

## Quick Facts
- arXiv ID: 2509.07010
- Source URL: https://arxiv.org/abs/2509.07010
- Authors: Ahmed R. Sadik; Mariusz Bujny
- Reference count: 40
- Primary result: Human-in-the-loop framework with quantitative metrics enables faster convergence to ground-truth 3D models compared to qualitative visual inspection

## Executive Summary
This paper introduces a quantitative evaluation framework for assessing 3D models generated by large language models (LLMs) using structural complexity and geometric similarity metrics. The method compares four input modalities—2D orthographic views, isometric sketches, geometric structure trees, and code-level correction prompts—to systematically improve generation fidelity. Using an L-bracket case study, the study demonstrates progressive improvement in model quality with increasing semantic richness in inputs, achieving perfect reconstruction with code-level prompts across all similarity metrics.

## Method Summary
The method employs GPT-4.5 to generate OpenSCAD code from four input modalities, rendering results to STL format for evaluation. A Python metrics pipeline computes structural complexity (feature count, surface area ratio, Euler characteristic, composite score) and geometric similarity (dimensional/volumetric/surface accuracy, Hausdorff distance, PCA/ICP alignment) by comparing generated STL meshes against ground-truth CAD models. The human-in-the-loop component interprets quantitative metrics to guide iterative refinement, escalating to richer input modalities or applying manual code corrections when necessary.

## Key Results
- Progressive improvement in generation fidelity with increasing semantic richness in inputs
- Code-level prompts achieve perfect reconstruction across all similarity metrics
- Quantitative evaluation approach enables significantly faster convergence to ground truth compared to qualitative visual inspection
- Clear upward trend across all similarity metrics as inputs evolve from orthographic drawings to code-level correction prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing semantic richness in input modalities progressively improves LLM-generated 3D model fidelity
- Mechanism: More structured inputs (geometric structure trees, code-level prompts) reduce ambiguity by providing explicit depth cues, feature hierarchy, and parametric relationships that 2D orthographic views lack
- Core assumption: The LLM's spatial reasoning improves when geometric intent is encoded symbolically rather than inferred from visual projections alone
- Evidence anchors: [abstract] "Results show progressive improvement in generation fidelity with increasing semantic richness in inputs"; [section 4.2] "There is a clear upward trend across all similarity metrics as inputs evolve from three-view drawings to isometric sketches"

### Mechanism 2
- Claim: Quantitative evaluation metrics enable faster convergence to ground truth compared to qualitative visual inspection
- Mechanism: Explicit numerical feedback (volumetric similarity, Hausdorff distance, ICP alignment) pinpoints specific geometric discrepancies, allowing targeted corrections rather than trial-and-error prompt refinement
- Core assumption: Humans can interpret and act on quantitative metrics more efficiently than holistic visual judgments for geometric tasks
- Evidence anchors: [abstract] "The quantitative evaluation approach enables significantly faster convergence to the ground truth compared to qualitative visual inspection"; [section 4.2] "These rotation and displacement metrics were critical in guiding the human reviewer to pinpoint the exact discrepancy"

### Mechanism 3
- Claim: Dual-metric framework (complexity + similarity) provides diagnostic information about generation limitations
- Mechanism: Structural complexity metrics establish difficulty baselines while similarity metrics identify specific failure modes—combination reveals whether errors stem from input ambiguity or model capability limits
- Core assumption: Complexity scores correlate with generation difficulty and can predict where LLMs will struggle
- Evidence anchors: [section 3] "By correlating complexity with similarity, we can benchmark complete 3D shapes databases, to better understand the performance limits of different input modalities"

## Foundational Learning

- Concept: **STL mesh representation and triangular discretization**
  - Why needed here: All evaluation metrics operate on STL mesh data; understanding face count, vertices, and mesh topology is essential for interpreting complexity scores (C_f, C_t)
  - Quick check question: Given a cube with 12 triangular faces, what would its Euler characteristic be?

- Concept: **Point cloud alignment methods (PCA, ICP)**
  - Why needed here: Alignment scores (S_p, S_i) require understanding how principal component analysis provides coarse orientation matching and how iterative closest point refines rigid transformations
  - Quick check question: Why would ICP outperform PCA alignment for detecting a rotated but dimensionally correct part?

- Concept: **Hausdorff distance for surface deviation**
  - Why needed here: This metric captures worst-case local errors that volumetric similarity might miss; critical for detecting missing features or small deformations
  - Quick check question: If two models have identical volumes but one has a protruding feature, would Hausdorff distance detect this?

## Architecture Onboarding

- Component map: Input modalities (orthographic drawings, isometric sketches, geometric structure trees, code prompts) -> GPT-4.5 generation -> OpenSCAD code -> STL export -> Python metrics pipeline (complexity + similarity) -> Human feedback loop (iterate/escalate/manual correction)
- Critical path: Input modality selection -> LLM generation -> STL conversion -> metric computation -> human decision point (iterate with current modality / escalate to richer modality / manual code correction)
- Design tradeoffs:
  - Orthographic views: Fastest to obtain from legacy archives, but poorest fidelity (Gen. Score: 0.45)
  - Code-level prompts: Highest fidelity (1.0) but requires domain expertise to write
  - Geometric structure trees: Good balance (0.76) if part hierarchy can be extracted automatically
- Failure signatures:
  - Low PCA/ICP + high volume/surface scores -> correct proportions but wrong orientation (see Geometric Structure case in Table 1)
  - High Hausdorff distance + moderate volumetric similarity -> localized feature errors (missing holes, misplaced cutouts)
  - Negative PCA alignment score -> significant orientation mismatch requiring rotation correction
  - Diverging metrics across iterations -> modality ceiling reached; escalate to richer input
- First 3 experiments:
  1. Replicate L-bracket case with identical modalities to validate metric computation pipeline and establish baseline scores
  2. Test a part with higher topological complexity (e.g., multiple through-holes, internal cavities) to evaluate whether C=255 represents a practical upper bound for current approach
  3. Implement automated metric-to-feedback translation (e.g., "PCA=-0.15 suggests 90° rotation needed") to reduce human interpretation burden

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic or learned weightings for the composite complexity (C) and similarity (Sf) metrics better capture task-specific importance or match human-perceived quality than fixed coefficients?
- Basis in paper: [explicit] "future research could investigate dynamic or learned weightings that better capture task-specific importance or user-defined priorities"
- Why unresolved: Current formulation uses fixed K1–K5 weights chosen heuristically; no experiments comparing to learned or adaptive weightings
- What evidence would resolve it: A/B comparison where human evaluators rate generated models, with regression-learned weights correlated against fixed-weight baselines

### Open Question 2
- Question: How well do the proposed metrics generalize to multi-part assemblies and parametrically defined geometries?
- Basis in paper: [explicit] "this framework should be extended to multi-part assemblies and parametrically defined geometries"
- Why unresolved: Only a single L-bracket component was tested; assembly relationships, mating constraints, and parametric families were not addressed
- What evidence would resolve it: Benchmarking the framework on datasets with multi-part assemblies and parametric CAD families, evaluating whether current metrics capture inter-part relationships

### Open Question 3
- Question: Does structural complexity score (C) systematically predict generation difficulty or failure modes across diverse mechanical parts?
- Basis in paper: [explicit] "potential utility in large-scale studies involving diverse datasets to map model capabilities and failure modes systematically"
- Why unresolved: Only one geometry (C=255) was evaluated; no correlation analysis between complexity and LLM performance across parts
- What evidence would resolve it: Large-scale study correlating complexity scores with generation fidelity across varied geometries, identifying thresholds where performance degrades

### Open Question 4
- Question: How do hybrid prompting strategies combining sketches with semantic descriptions or code fragments compare to single-modality inputs in reducing iteration cycles?
- Basis in paper: [explicit] "Hybrid prompting strategies that combine sketches with semantic descriptions or code fragments may reduce ambiguity in early-stage inputs"
- Why unresolved: Modalities were tested sequentially, not in combination; no hybrid input experiments were conducted
- What evidence would resolve it: Controlled experiments comparing pure vs. hybrid input modalities on convergence speed and final fidelity metrics

## Limitations
- Evaluation framework tested only on single L-bracket geometry with C=255 complexity
- Weight coefficients for composite metrics (K1-K3 for complexity, K1-K5 for similarity) are unspecified and tunable
- Human-in-the-loop component introduces subjectivity in interpreting metric feedback and modality selection
- No validation of whether structural complexity score correlates with generation difficulty across diverse part geometries

## Confidence

- **High confidence**: Progressive improvement across input modalities (orthographic → code) and the specific L-bracket results (grounded in concrete STL comparisons and explicit metric values)
- **Medium confidence**: Generalizability to other CAD geometries (supported by reasonable methodology but limited to one case study)
- **Medium confidence**: Quantitative metrics enable faster convergence (mechanism plausible from engineering perspective but lacks comparative timing data)
- **Low confidence**: Complexity-similarity correlation as diagnostic tool (mechanism stated but unsupported by corpus evidence)

## Next Checks
1. Apply the framework to a CAD part with significantly higher topological complexity (>C=255) to test whether metric-guided refinement scales effectively
2. Conduct a controlled user study comparing convergence time between metric-guided and visual-inspection-guided refinement workflows
3. Implement cross-validation by having multiple independent reviewers apply the human-in-the-loop protocol to the same case study to quantify subjectivity in metric interpretation