---
ver: rpa2
title: On the Convergence Analysis of Muon
arxiv_id: '2505.23737'
source_url: https://arxiv.org/abs/2505.23737
tags:
- muon
- norm
- have
- assumption
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence properties of Muon, an optimizer
  designed for matrix-structured parameters in neural networks. While most optimizers
  treat matrix parameters as flattened vectors, Muon updates them along an orthogonalized
  version of their gradients.
---

# On the Convergence Analysis of Muon

## Quick Facts
- arXiv ID: 2505.23737
- Source URL: https://arxiv.org/abs/2505.23737
- Reference count: 40
- Primary result: Muon optimizer's convergence guarantees in nonconvex and star-convex settings with advantage when Hessian has low-rank/blockwise diagonal structure

## Executive Summary
This paper provides the first comprehensive convergence analysis of Muon, an optimizer designed for matrix-structured parameters in neural networks. Unlike traditional optimizers that treat matrix parameters as flattened vectors, Muon performs updates along an orthogonalized version of the gradient matrix. The authors establish convergence guarantees for Muon in both nonconvex and star-convex settings, demonstrating that it can outperform Gradient Descent when the Hessian matrix exhibits low-rank or approximately blockwise diagonal structure—properties commonly observed in neural network training. The analysis bridges the gap between Muon's empirical success and theoretical understanding of its convergence behavior.

## Method Summary
Muon updates matrix parameters by computing the SVD of the gradient (or momentum) matrix G_t = U_t S_t V_t^⊤, then applying the update W_{t+1} = W_t - η_t × U_t V_t^⊤. This orthogonalized update direction is the nearest semi-orthogonal matrix to the original gradient in Frobenius norm. The paper analyzes both the exact SVD implementation and the practical Newton-Schulz iteration approximation. Convergence guarantees are established under standard smoothness assumptions and conditions on Hessian structure, with rates depending on average global Hessian information rather than just the maximum singular value.

## Key Results
- Muon achieves convergence rates of O(1/√T) for nonconvex objectives under Frobenius-norm smoothness, improving to O(1/T) under star-convexity
- Theoretical advantage over GD by factor of √r when Hessian has low effective rank (∥Λ∥_* ≪ r∥Λ∥_op)
- Experiments validate Muon's advantage in early optimization phases on quadratic functions and neural networks
- Convergence rate depends on average global Hessian properties rather than maximum singular value

## Why This Works (Mechanism)

### Mechanism 1: Orthogonalized Gradient Direction via SVD
- Claim: Muon improves convergence by updating matrix parameters along orthogonalized gradients (U_t V_t^⊤) rather than raw gradients.
- Mechanism: At each iteration, Muon computes the SVD of the gradient (or momentum) matrix G_t = U_t S_t V_t^⊤, then updates using the semi-orthogonal matrix U_t V_t^⊤ instead of G_t. This constrains updates to the nearest orthogonal manifold.
- Core assumption: The gradient matrix has meaningful singular structure; orthogonalizing preserves directional information while normalizing step scale.
- Evidence anchors:
  - [abstract] "Muon updates them along an orthogonalized version of their gradients"
  - [section 3.1] "the update matrix will be U_t V_t^⊤, which is the nearest semi-orthogonal matrix to the original G_t"
  - [corpus] Related work "PolarGrad" extends similar matrix-preconditioning concepts
- Break condition: If gradients are full-rank with uniform singular values, orthogonalization provides minimal benefit over normalized gradient descent.

### Mechanism 2: Exploiting Low-Rank and Blockwise Diagonal Hessian Structure
- Claim: Muon outperforms GD when the Hessian exhibits low effective rank and blockwise diagonal structure—properties observed in neural network training.
- Mechanism: Under Λ-norm smoothness (Assumption 4.5), Muon's complexity depends on ∥Λ∥_* rather than r∥Λ∥_op. When ∥Λ∥_* ≪ r∥Λ∥_op (low-rank Hessian), Muon's convergence rate improves relative to GD by factor of √r.
- Core assumption: Hessian has approximately blockwise diagonal structure with each block corresponding to individual neurons/layers; effective rank (∥H∥_*/∥H∥_op) is small.
- Evidence anchors:
  - [abstract] "Muon can benefit from the low-rank and approximately blockwise diagonal structure of Hessian matrices"
  - [section 4.1] "if ∥Λ∥_op ≈ ∥Λ∥_* ≪ r∥Λ∥_op, then O(∥Λ∥_* Δϵ⁻²) ≪ O(r∥Λ∥_op Δϵ⁻²)"
  - [corpus] Limited direct corroboration; related works focus on convergence rates rather than Hessian structure
- Break condition: When Hessian is full-rank with uniform eigenvalue distribution, ∥Λ∥_* ≈ r∥Λ∥_op and Muon's advantage disappears.

### Mechanism 3: Smoothing Over Maximum Singular Value via Average Global Hessian Information
- Claim: Muon's convergence rate depends on average global Hessian properties (J = average of J_t) rather than the maximum singular value that constrains GD.
- Mechanism: The term J_t = vec(U_t V_t^⊤)^⊤ H_t vec(U_t V_t^⊤) captures local curvature along the orthogonalized update direction. Averaging J_t across iterations provides smoothing compared to GD's dependence on max_t ∥H_t∥_op.
- Core assumption: Assumption 4.7 (bounded third derivative); the ratio J_t/L_t ≤ ∥∇f(W_t)∥_*²/∥∇f(W_t)∥_F² holds during optimization.
- Evidence anchors:
  - [section 1] "the behavior of Muon depends on the average of the global information of the Hessian matrices during training, which can be viewed as smoothing over steps and singular values"
  - [section 4.1, Figure 2] Experiments show ∥∇f(W_t)∥_*²/J_t consistently larger than ∥∇f(W_t)∥_F²/L_t during training
  - [corpus] Not directly addressed in neighboring papers
- Break condition: If J_t varies wildly or becomes negative (non-convex regions with high curvature), the smoothing benefit degrades.

## Foundational Learning

- **Matrix Norms (Frobenius, Nuclear, Spectral)**
  - Why needed here: Convergence analysis requires understanding relationships: ∥A∥_F ≤ ∥A∥_* ≤ √r∥A∥_F. Stationarity is measured in nuclear norm, not Frobenius norm.
  - Quick check question: Given a 100×100 matrix with rank 5, what is the relationship between its Frobenius and nuclear norms?

- **Singular Value Decomposition (SVD)**
  - Why needed here: Muon's core operation is computing SVD of gradients to extract U_t V_t^⊤. Newton-Schulz iterations approximate this efficiently.
  - Quick check question: If G = USV^⊤, what is the nearest semi-orthogonal matrix to G in Frobenius norm?

- **Smoothness Assumptions and Lipschitz Constants**
  - Why needed here: The paper distinguishes between Frobenius-norm Lipschitz smoothness (L) and spectral-norm Lipschitz smoothness (L_*). GD depends on L; Muon can depend on L_* which may be much smaller.
  - Quick check question: Why does the maximum singular value of the Hessian determine GD's stepsize bound?

## Architecture Onboarding

- **Component map:**
  Gradient computation -> Momentum buffer (optional) -> Orthogonalization (SVD/Newton-Schulz) -> Weight update

- **Critical path:** The orthogonalization step dominates compute. Use Newton-Schulz (5 iterations with coefficients a=3.4445, b=-4.7750, c=2.0315) instead of full SVD for practical implementation.

- **Design tradeoffs:**
  - Momentum (β): Essential for stochastic setting (variance reduction); negligible benefit in deterministic setting (Corollary 4.2 vs Theorem 4.1)
  - SVD vs Newton-Schulz: SVD is exact but O(min(mn², m²n)); Newton-Schulz is approximate but O(mn × iterations)
  - Learning rate: Paper uses constant η; adaptive η_t = ∥∇f(W_t)∥_*/L_* shown for star-convex case

- **Failure signatures:**
  - Exploding loss early in training: Learning rate too high relative to L_*; reduce η
  - No improvement over GD: Hessian may not have low-rank/block-diagonal structure; verify gradient rank ratios
  - Stochastic setting instability: Missing momentum or batch size too small; increase β or B

- **First 3 experiments:**
  1. **Validation on small quadratic:** Replicate Figure 3 with f(W) = tr((W-W^*)^⊤ Q(W-W^*)) where Q is low-rank. Compare GD vs Muon convergence. Expected: Muon faster when D_F²L/(D_op²L_*) > 1.
  2. **Ablation on momentum:** Compare Muon with β=0 vs β=0.9 on CIFAR-10 subset (deterministic vs stochastic). Expected: Momentum helps stochastic only.
  3. **Hessian structure diagnostic:** During MLP training, log ∥∇f(W_t)∥_*²/∥∇f(W_t)∥_F² and J_t/L_t ratios. Verify condition (4) holds in early training phases.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the low-rank and approximately blockwise diagonal structures of Hessian matrices, which theoretically benefit Muon, persist in large-scale neural networks (e.g., LLMs) as they do in smaller models?
- **Basis in paper:** [explicit] The authors state in the conclusion that a limitation is that "experiments are conducted on relatively small-scale neural networks," and explicitly list investigating "the structure of Hessian matrices in larger-scale neural networks" as a future direction.
- **Why unresolved:** The theoretical advantage of Muon relies on specific Hessian structures ($L_*$ vs $L$). While observed in small MLPs/CNNs, it is unconfirmed if these structural properties scale to models with billions of parameters where computational constraints make Hessian analysis difficult.
- **What evidence would resolve it:** Empirical analysis of Hessian spectral properties and blockwise structures during the training of large-scale transformers.

### Open Question 2
- **Question:** Can the identified Hessian structures be further leveraged to design new optimization methods or improve existing ones beyond the current Muon implementation?
- **Basis in paper:** [explicit] The conclusion lists "studying the structures of Hessian matrices from theoretical perspectives and leveraging their structures to design new or better optimization methods" as an explicit future direction.
- **Why unresolved:** This is a broad direction for future work proposed by the authors to extend the theoretical insights gained from analyzing Muon.
- **What evidence would resolve it:** The proposal and validation of a novel optimizer explicitly constructed to exploit the specific low-rank or blockwise diagonal Hessian properties characterized in the paper.

### Open Question 3
- **Question:** Does the Muon optimizer maintain its theoretical convergence guarantees when the exact SVD is replaced by the approximate Newton-Schulz iteration used in practice?
- **Basis in paper:** [inferred] The paper notes that "common practical implementation of Muon uses Newton–Schulz iterations... to approximate the orthogonalization process" (Page 4). While the authors analyze the SVD-based algorithms (1 & 2), they rely on empirical evidence (Figure 1) to justify the approximation, leaving a theoretical gap regarding the convergence of the practical Newton-Schulz variant (Algorithm 3).
- **Why unresolved:** Theoretical guarantees are proven for exact orthogonalization ($U V^\top$). The error introduced by the polynomial approximation in Newton-Schulz iterations is not formally incorporated into the convergence bounds (e.g., Theorem 4.8).
- **What evidence would resolve it:** A convergence theorem explicitly bounding the error of the Newton-Schulz approximation within the Muon update step, or analysis showing the approximation error is negligible relative to the gradient noise.

### Open Question 4
- **Question:** Is the assumption that the spectral norm distance to the optimum $\|W_t - W^*\|_{op}$ remains bounded (Assumption 4.10) theoretically justified for general nonconvex neural network training?
- **Basis in paper:** [inferred] The star-convex convergence analysis (Theorem 4.11) relies on Assumption 4.10 ($\|W_t - W^*\|_{op} \leq D_{op}$). The authors support this with a quadratic experiment (Figure 3b), but do not provide theoretical justification for why this spectral norm would stay bounded in general deep learning settings.
- **Why unresolved:** Bounded distance to optimum is a strong assumption. Without theoretical proof that Muon inherently constrains the iterate distance in the spectral norm, the applicability of the star-convex convergence rates to general deep learning remains conditional.
- **What evidence would resolve it:** A theoretical proof showing that Muon updates implicitly bound the spectral norm of the weights, or a counter-example showing this assumption fails in specific nonconvex settings.

## Limitations
- Assumption Dependency: All convergence guarantees rely on Assumptions 4.5 (Λ-norm smoothness) and 4.7 (bounded third derivative) which are unverified on practical neural networks
- Structural Requirement: Muon's advantage critically depends on low-rank and approximately blockwise diagonal Hessian structure, which may not persist throughout training
- Near-Orthogonal Update Constraint: Theoretical benefits assume the orthogonalized update remains sufficiently close to the gradient direction

## Confidence
- **High Confidence**: Mechanism 1 (orthogonalized gradient direction) - mathematically well-defined SVD operation and nearest-orthogonal update
- **Medium Confidence**: Mechanism 2 (low-rank/blockwise Hessian structure) - observed in some settings but prevalence and persistence need more validation
- **Medium Confidence**: Mechanism 3 (smoothing over maximum singular value) - theoretical derivation sound but empirical verification requires further study

## Next Checks
1. **Hessian Structure Verification**: During training of a 2-layer MLP on MNIST, log the ratio ∥∇f(W_t)∥_*²/∥∇f(W_t)∥_F² and J_t/L_t at each epoch. Plot these ratios against training loss to verify the structural conditions persist throughout optimization.

2. **Convergence Rate Comparison**: Implement Muon with Newton-Schulz orthogonalization (5 iterations) and compare convergence on low-rank quadratic objectives (rank 5, 100×100 matrices) versus full-rank counterparts. Measure wall-clock time and iteration count to isolate Muon's advantage from implementation overhead.

3. **Break Case Analysis**: Design a synthetic optimization problem where the Hessian is full-rank with uniform eigenvalue distribution. Compare Muon vs GD convergence to verify the theoretical prediction that Muon's advantage disappears when ∥Λ∥_* ≈ r∥Λ∥_op.