---
ver: rpa2
title: 'FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias
  and Stereotypes'
arxiv_id: '2506.23111'
source_url: https://arxiv.org/abs/2506.23111
tags:
- identity
- bias
- social
- india
- stereotypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces INDIC-BIAS, a comprehensive benchmark designed
  to evaluate fairness in large language models (LLMs) across 85 Indian identity groups
  spanning caste, religion, region, and tribe. The authors consulted experts to curate
  over 1,800 socio-cultural topics and generated 20,000 real-world scenario templates
  organized into three tasks: plausibility, judgment, and generation.'
---

# FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes

## Quick Facts
- arXiv ID: 2506.23111
- Source URL: https://arxiv.org/abs/2506.23111
- Reference count: 40
- Primary result: 14 LLMs show strong negative biases against marginalized Indian identities with stereotype association rates exceeding 50% on average

## Executive Summary
This paper introduces INDIC-BIAS, a comprehensive benchmark designed to evaluate fairness in large language models (LLMs) across 85 Indian identity groups spanning caste, religion, region, and tribe. The authors consulted experts to curate over 1,800 socio-cultural topics and generated 20,000 real-world scenario templates organized into three tasks: plausibility, judgment, and generation. Their evaluation of 14 popular LLMs revealed strong negative biases against marginalized identities, with models frequently reinforcing stereotypes. The study found that LLMs struggle to mitigate bias even when explicitly asked to rationalize decisions. Stereotype association rates were particularly high in generation tasks, exceeding 50% on average, with larger models showing rates as high as 79%. The benchmark demonstrates both allocative and representational harms that current LLMs can cause toward Indian identities.

## Method Summary
The benchmark uses 20,000+ manually validated scenario templates derived from 1,800+ socio-cultural topics, covering 85 identities across four categories (caste, religion, region, tribe). The evaluation employs three tasks: Plausibility (pairwise identity comparisons for scenario believability), Judgment (preference judgments between identity scenarios), and Generation (free-form response evaluation). The study evaluates 14 LLMs using ELO ratings, Rank Shift Metric (RSM), and Stereotype Association Rate (SAR) to quantify bias. LLaMA-3.3-70B-INSTRUCT serves as the evaluator LLM for generation tasks with 90%+ human agreement. The methodology involves pairwise identity comparisons, Bradley-Terry ELO ranking via MLE, and systematic substitution of identity placeholders across scenario templates.

## Key Results
- LLMs exhibit strong negative bias against marginalized identities, with Dalits and regional minorities showing consistent negative bias across tasks
- Stereotype association rates exceed 50% on average, with larger models (Llama-3.3-70B, Gemma-27B) reaching 79%
- Models struggle to mitigate bias even when prompted to rationalize decisions through Chain-of-Thought
- Allocative and representational harms are demonstrated across all 14 evaluated LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structured scenario template approach enables systematic probing of model biases across identity groups
- Mechanism: Identity placeholders in scenario templates → systematic substitution of 85 identity groups → controlled comparison of model responses across identical situations with different identities
- Core assumption: If models are unbiased, they should respond similarly to identical scenarios regardless of which identity is substituted
- Evidence anchors:
  - [abstract]: "INDIC-BIAS is a comprehensive benchmark for evaluating fairness in LLMs across 85 Indian identities spanning caste, religion, region, and tribe"
  - [section 4.1]: "By replacing these placeholders with different identities (Id_i), we create identical versions of the same scenario involving different identities... The model is presented with these two identical scenarios and prompted to choose which one it finds more plausible"
  - [corpus]: BharatBBQ paper similarly uses template-based bias evaluation in Indian contexts with structured identity substitution

### Mechanism 2
- Claim: ELO ratings combined with Rank Shift Metric (RSM) quantify directional bias toward identities
- Mechanism: Tournament-style pairwise comparisons → ELO ratings for each identity in positive vs negative scenarios → RSM = rank_negative − rank_positive reveals whether identity is preferred more in negative (RSM < 0) or positive (RSM > 0) contexts
- Core assumption: Bias manifests as systematic preference for certain identities in negative scenarios and/or exclusion from positive scenarios
- Evidence anchors:
  - [section 5.2]: "RSM_{Id_i} = neg_{Id_i} - pos_{Id_i}, where neg_{Id_i} and pos_{Id_i} are the identity Id_i's ELO ranks in negative and positive scenarios respectively. A negative RSM indicates negative bias"
  - [section 6.2]: "For caste, we observe a consistent negative bias against marginalized groups such as Dalit, and Chamar, while medium and higher castes, including Brahmin, and Iyyengar, show a clear positive bias"
  - [corpus]: IndiCASA framework uses contrastive embedding similarity for bias evaluation, confirming the need for quantitative bias metrics

### Mechanism 3
- Claim: Stereotype Association Rate (SAR) captures implicit stereotyping in both controlled and generative tasks
- Mechanism: Target identity paired with known stereotype in scenario → model response analyzed for stereotype-identity linkage → SAR = ratio of correct stereotype associations; high SAR indicates model has internalized and reproduces stereotype
- Core assumption: Stereotypes are oversimplified generalizations that models may have learned from training data
- Evidence anchors:
  - [section 5.2]: "SAR is defined as the ratio of times the model selects the target identity in scenarios where its correct stereotype is present. A higher SAR suggests stronger stereotypical associations"
  - [section 6.3]: "Models reinforce stereotypes in over 50% of cases on average... Larger models, such as LLAMA-3.3-70B and GEMMA-27B, exhibit SAR values as high as 79%"
  - [corpus]: MALIBU benchmark confirms implicit biases can surface even in sophisticated multi-agent interactions

## Foundational Learning

- Concept: Allocative vs Representational Harms
  - Why needed here: The paper evaluates both harm types; understanding the distinction is critical for interpreting results and applying findings
  - Quick check question: If a model recommends manual labor jobs to lower-caste individuals and professional jobs to upper-caste individuals, is this allocative harm, representational harm, or both?

- Concept: India's Social Stratification (Caste, Religion, Region, Tribe)
  - Why needed here: The benchmark's 85 identity groups and bias findings are grounded in India-specific social hierarchies; without this context, results are uninterpretable
  - Quick check question: Why might Dalits show negative bias across multiple tasks while tribal identities show lower SAR in stereotype tasks?

- Concept: Controlled vs Generative Fairness Evaluation
  - Why needed here: The benchmark uses both controlled tasks (plausibility, judgment) and generative tasks, each measuring different aspects of bias
  - Quick check question: Why might a model show lower bias in controlled binary-choice tasks but higher bias in free-form generation tasks?

## Architecture Onboarding

- Component map: Taxonomy layer (1,800+ topics) -> Scenario generation layer (GPT-4o templates + human verification) -> Identity population layer (85 identities across 4 categories) -> Evaluation layer (3 tasks) -> Metrics layer (ELO, RSM, SAR) -> LLM-as-judge layer (LLaMA-3.3-70B-INSTRUCT)

- Critical path: Taxonomy creation (expert sociologists) → Template generation (GPT-4o + human verification) → Identity population (pairwise combinations) → Model evaluation (14 LLMs) → Metric computation (ELO, RSM, SAR) → Result interpretation

- Design tradeoffs:
  1. Scale vs depth: 85 identities, 1,800 topics provide breadth but may miss nuanced contexts (paper: "approximately 20 stereotypes per identity" is not exhaustive)
  2. Synthetic generation vs manual curation: GPT-4o enables scale but requires verification (filtering rates: 5.5%–28.12% per Table 9)
  3. LLM-as-judge vs human evaluation: Enables scale at 90%+ agreement but evaluator bias remains a limitation

- Failure signatures:
  1. High refusal rates: Some models refuse frequently (e.g., GPT-4o up to 90% in certain tasks)
  2. Inconsistent CoT impact: Chain-of-thought doesn't consistently reduce bias; Gemma-27B improves, LLaMA family shows no improvement
  3. Stereotype amplification in generation: Free-form generation shows highest SAR, suggesting controlled tasks may underestimate harm

- First 3 experiments:
  1. **Baseline on subset**: Select 10 identities from one category (e.g., caste), run all 3 tasks on a single model to understand metric behaviors before scaling
  2. **Ablation on scenario type**: Compare bias measures using positive vs negative scenarios only to understand directional effects
  3. **Cross-model comparison on single category**: Evaluate 3 models on one identity category to observe model-specific bias patterns before full evaluation

## Open Questions the Paper Calls Out

- What specific mitigation strategies (e.g., fine-tuning, RLHF, or counterfactual data augmentation) effectively reduce the negative biases and stereotypical associations identified in INDIC-BIAS?
- How does the magnitude of bias change when evaluating intersectional identities (e.g., combinations of caste, religion, region, and tribe) compared to the single-axis identities currently covered in INDIC-BIAS?
- Why does prompting models to rationalize decisions (Chain-of-Thought) lead to inconsistent bias reduction—improving refusal rates in some models (e.g., Gemma) but not others (e.g., Llama)?

## Limitations
- The template-based evaluation approach may not capture all real-world bias manifestations despite the large scale of 85 identities and 20K+ scenarios
- LLM-as-judge methodology with 90%+ human agreement introduces evaluator bias that could affect SAR measurements
- The study does not consider intersectional identities, potentially missing compounded bias effects from overlapping social categories

## Confidence

- **High confidence**: Systematic negative bias against marginalized Indian identities (Dalits, tribal groups, religious minorities) demonstrated through multiple metrics (ELO, RSM, SAR) across 14 LLMs
- **Medium confidence**: Larger models showing higher stereotype association rates (up to 79%) supported by data but requires careful interpretation given subjective stereotype identification
- **Medium confidence**: Chain-of-Thought prompting does not consistently reduce bias, though effects vary across model families suggesting complex architectural relationships

## Next Checks
1. **Template coverage validation**: Conduct systematic audit of 20,000+ scenario templates to identify potential blind spots, particularly focusing on intersectional scenarios and less common socio-cultural contexts
2. **Cross-cultural generalizability test**: Apply benchmark methodology to other cultural contexts (e.g., other South Asian countries or different global regions) to determine if biases are specifically Indian or indicative of broader LLM patterns
3. **Longitudinal bias tracking**: Re-run benchmark evaluation on same 14 LLMs at 6-month intervals to measure how bias patterns change over time with model updates and identify which biases are most persistent versus those models can learn to mitigate