---
ver: rpa2
title: 'On the Emergence of Thinking in LLMs I: Searching for the Right Intuition'
arxiv_id: '2502.06773'
source_url: https://arxiv.org/abs/2502.06773
tags:
- step
- reasoning
- arxiv
- reward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a post-training framework called Reinforcement
  Learning via Self-Play (RLSP) to enable search and reasoning behaviors in LLMs.
  The core idea is to decouple exploration and correctness signals during reinforcement
  learning training, encouraging diverse reasoning behaviors while ensuring correctness.
---

# On the Emergence of Thinking in LLMs I: Searching for the Right Intuition

## Quick Facts
- **arXiv ID**: 2502.06773
- **Source URL**: https://arxiv.org/abs/2502.06773
- **Reference count**: 40
- **Key outcome**: RLSP framework improves reasoning performance by 23% on MATH-500 and 10% on AIME 2024, demonstrating emergent behaviors like backtracking and self-verification.

## Executive Summary
This paper proposes Reinforcement Learning via Self-Play (RLSP) to enable search and reasoning behaviors in LLMs. The framework decouples exploration and correctness signals during reinforcement learning training, encouraging diverse reasoning behaviors while ensuring correctness. Through three training steps—supervised fine-tuning with reasoning demonstrations, RL training with an exploration reward, and outcome verification—RLSP significantly improves reasoning performance across different model families and sizes. More importantly, it demonstrates emergent behaviors like backtracking, exploration of ideas, and self-verification, even with simple exploration rewards.

## Method Summary
The RLSP framework consists of three steps: (1) optional supervised fine-tuning on reasoning demonstrations, (2) RL training with a combined reward function that includes both outcome verification and exploration incentives, and (3) continuous outcome verification. The exploration reward is designed to encourage longer reasoning traces, compensating for the KL penalty that naturally discourages length. The framework uses PPO with a 0.8 weight on outcome correctness and 0.2 weight on exploration reward, with specific hyperparameters including γ=1, λ=0.95, and a length penalty constant C=1000.

## Key Results
- RLSP boosts Llama-3.1-8B-Instruct accuracy by 23% on MATH-500 benchmark
- Qwen2.5-32B-Instruct shows 10% improvement on AIME 2024 with RLSP training
- Models demonstrate emergent behaviors including backtracking, self-verification, and exploration of multiple reasoning paths

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Correctness Signal Decoupling
The framework separates exploration rewards from outcome verification, providing dense exploration signals even when outcome signals are sparse. This prevents early-training collapse where models stop exploring before finding correct solutions. The reward function combines a sparse outcome signal with a dense exploration component, allowing models to learn meta-reasoning strategies from signals that don't explicitly encode these behaviors.

### Mechanism 2: Length-Reward Induces Chain-of-Thought Scaling
Rewarding response length implicitly rewards computational depth, as theoretical results suggest longer CoT traces increase reasoning power. The exploration reward counteracts the KL penalty that discourages longer responses, making longer CoT traces reward-neutral. This enables more intermediate computation steps that lead to better reasoning outcomes.

### Mechanism 3: Emergent Search via Self-Play Over Synthetic CoT
RLSP enables models to generate and learn from novel CoT trajectories not present in training data, creating a self-improvement loop. The exploration reward incentivizes diverse reasoning paths, and successful paths become training signal over PPO iterations. This is analogous to AlphaZero self-play but operates in sequence space rather than game-tree space.

## Foundational Learning

- **Concept: PPO (Proximal Policy Optimization)**
  - Why needed here: RLSP uses PPO as its core RL algorithm; understanding clipping objective and KL penalties is essential for debugging training instabilities
  - Quick check question: Can you explain why PPO uses a clipped surrogate objective rather than direct policy gradient?

- **Concept: Reward Shaping and Auxiliary Rewards**
  - Why needed here: The exploration reward is an auxiliary signal; without understanding reward shaping, it's easy to create reward-hacking opportunities
  - Quick check question: What is the difference between a process reward model (PRM) and RLSP's exploration reward?

- **Concept: Chain-of-Thought Expressiveness Theory**
  - Why needed here: RLSP's design is motivated by theoretical results that CoT increases transformer expressiveness
  - Quick check question: Why does [MS23] claim that standard transformers without CoT cannot solve certain serial problems that CoT-enabled transformers can?

## Architecture Onboarding

- **Component map**: SFT (optional) -> PPO training with exploration reward -> Outcome verification
- **Critical path**: 1. Data preparation with decontamination, 2. SFT (if used) with diverse reasoning patterns, 3. PPO hyperparameter tuning with α balance and length-penalty constant C, 4. Monitor response length and emergent behaviors
- **Design tradeoffs**: Length reward vs. LLM-judge exploration reward; with vs. without SFT; α=0.8 default vs. tuning
- **Failure signatures**: Response length increases without accuracy gains (reward hacking), no emergent behaviors after 1000+ PPO steps, accuracy degradation (KL collapse), behaviors emerge but solutions remain wrong (insufficient model scale)
- **First 3 experiments**: 1. Reproduce Llama-3.1-8B + length-reward experiment on MATH-500 subset, 2. Ablate α ∈ {0.5, 0.8, 0.95}, 3. Compare pure-RL vs. SFT+RL on same base model

## Open Questions the Paper Calls Out

### Open Question 1
Can pure RL with no exploration reward lead to thinking behavior at some model scale? Experiments showed pure RL worked for Qwen-2.5-7B in math but not coding, and failed for Llama models entirely, suggesting base model and pretraining effects remain unclear. Systematic experiments across varying model scales and pretraining datasets are needed.

### Open Question 2
How can test-time search be dynamically scaled based on problem difficulty? Current exploration reward is simple length-based; the framework lacks mechanisms for difficulty-adaptive compute allocation during inference. Models need to autonomously allocate more reasoning steps to harder problems without explicit difficulty signals.

### Open Question 3
Can constitutional AI methods train domain-adaptable exploration reward models? Current exploration rewards use simple length heuristics or GPT-4o-as-judge, both limited in adaptability to diverse reasoning domains. A trained exploration reward model using constitutional principles could generalize across domains without domain-specific engineering.

### Open Question 4
Is there a truly emergent reasoning behavior that surpasses or surprises human reasoning? All observed emergent behaviors (backtracking, verification) are implicitly present in pretraining data from human reasoning patterns. Identification of novel reasoning strategies not present in training data that demonstrably outperform standard human approaches is needed.

## Limitations
- Outcome verifier implementation is critical yet underspecified, significantly impacting training stability and accuracy gains
- Length-based exploration reward is susceptible to reward hacking through token repetition rather than genuine reasoning depth
- Claims about generality across model scales and necessity of decoupling exploration from correctness lack systematic ablation studies

## Confidence
- **High confidence**: RLSP framework architecture is clearly specified and reproducible; emergent behaviors are directly observable across multiple model families
- **Medium confidence**: Significant accuracy improvements rely on undisclosed decontamination procedures and outcome verifier quality; theoretical motivation is sound but not directly validated
- **Low confidence**: Claims about generality across model scales and AlphaZero comparison overstate structural similarity given vastly different problem spaces

## Next Checks
1. Systematic α ablation study: Train models with α ∈ {0.5, 0.8, 0.95} on MATH-500 and measure both accuracy and emergent behavior frequency
2. Reward function comparison: Compare length-based exploration reward versus LLM-judge based exploration reward on the same base model and task
3. Decontamination audit: Re-run Qwen2.5-32B-Instruct + AIME 2024 experiment with rigorous overlap checking between training data and test set