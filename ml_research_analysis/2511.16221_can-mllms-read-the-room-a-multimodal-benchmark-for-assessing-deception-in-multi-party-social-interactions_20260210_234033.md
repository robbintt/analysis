---
ver: rpa2
title: Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in
  Multi-Party Social Interactions
arxiv_id: '2511.16221'
source_url: https://arxiv.org/abs/2511.16221
tags:
- social
- deception
- multimodal
- reasoning
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deception detection in multi-party
  social interactions, a task where state-of-the-art Multimodal Large Language Models
  (MLLMs) significantly underperform. The authors introduce the Multimodal Interactive
  Deception Assessment (MIDA) task and construct a new dataset with synchronized video,
  text, and verifiable ground-truth labels derived from the social deduction game
  Werewolf.
---

# Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions

## Quick Facts
- **arXiv ID**: 2511.16221
- **Source URL**: https://arxiv.org/abs/2511.16221
- **Reference count**: 40
- **Primary result**: State-of-the-art MLLMs significantly underperform at deception detection in multi-party social interactions

## Executive Summary
This paper addresses the challenge of deception detection in multi-party social interactions, where state-of-the-art Multimodal Large Language Models (MLLMs) significantly underperform. The authors introduce the Multimodal Interactive Deception Assessment (MIDA) task and construct a new dataset with synchronized video, text, and verifiable ground-truth labels derived from the social deduction game Werewolf. They establish a comprehensive benchmark evaluating 12 leading MLLMs, revealing a substantial performance gap—even top models like GPT-4o struggle to reliably distinguish truth from falsehood. Analysis shows MLLMs fail to ground language in multimodal social cues and lack the ability to model others' knowledge, beliefs, or intentions. To address these limitations, the authors propose two novel modules: a Social Chain-of-Thought (SoCoT) reasoning pipeline for structured, interpretable multimodal inference, and a Dynamic Social Epistemic Memory (DSEM) module to maintain evolving participant states. Experimental results show these modules improve MLLM performance on the MIDA benchmark, offering a promising path toward more socially intelligent AI systems.

## Method Summary
The authors establish a new benchmark for deception detection in multi-party social interactions by introducing the Multimodal Interactive Deception Assessment (MIDA) task. They construct a dataset using the social deduction game Werewolf, providing synchronized video, text transcripts, and verifiable ground-truth labels. The benchmark evaluates 12 leading MLLMs on their ability to detect deception by analyzing multimodal social cues including verbal statements, facial expressions, and body language. The evaluation reveals significant performance gaps, with even top models struggling to reliably distinguish truth from falsehood. To address these limitations, the authors propose two novel modules: a Social Chain-of-Thought (SoCoT) reasoning pipeline for structured multimodal inference, and a Dynamic Social Epistemic Memory (DSEM) module for maintaining evolving participant states during interactions.

## Key Results
- Top MLLMs like GPT-4o show substantial underperformance in deception detection tasks
- MLLMs fail to effectively ground language in multimodal social cues from video data
- Proposed SoCoT and DSEM modules demonstrate measurable improvements on the MIDA benchmark
- Models struggle particularly with understanding others' knowledge, beliefs, and intentions in social contexts

## Why This Works (Mechanism)
The proposed approach works by addressing fundamental limitations in current MLLMs' ability to process and reason about social interactions. The Social Chain-of-Thought module provides structured reasoning that explicitly considers multimodal evidence, while the Dynamic Social Epistemic Memory maintains evolving mental models of participants' states. Together, these components help bridge the gap between raw multimodal inputs and the sophisticated social reasoning required for deception detection.

## Foundational Learning
- **Multimodal Integration**: Combining video, audio, and text inputs is essential for capturing the full spectrum of social cues that humans use to detect deception
  - *Why needed*: Deception often involves subtle inconsistencies across different communication modalities
  - *Quick check*: Can the model identify discrepancies between verbal statements and non-verbal cues?

- **Dynamic State Tracking**: Maintaining evolving mental models of participants' beliefs and knowledge states throughout an interaction
  - *Why needed*: Social interactions are dynamic, with information and deception strategies constantly evolving
  - *Quick check*: Does the model update its assessment as new information becomes available?

- **Epistemic Reasoning**: Understanding what participants know, believe, and intend at different points in time
  - *Why needed*: Deception detection requires reasoning about not just what is said, but the speaker's knowledge state
  - *Quick check*: Can the model distinguish between honest mistakes and intentional deception?

- **Chain-of-Thought Reasoning**: Breaking down complex social inference into structured, interpretable steps
  - *Why needed*: Deception detection requires multi-step reasoning across multiple social cues
  - *Quick check*: Does the reasoning process follow a logical progression from evidence to conclusion?

## Architecture Onboarding

**Component Map**: Video/Audio/Text Inputs -> Multimodal Encoder -> Social Chain-of-Thought Module -> Dynamic Social Epistemic Memory -> Deception Classifier

**Critical Path**: Multimodal inputs → SoCoT reasoning → DSEM state maintenance → final deception prediction

**Design Tradeoffs**: The modular approach sacrifices some end-to-end optimization potential for interpretability and targeted improvements in social reasoning capabilities

**Failure Signatures**: Models fail when they cannot reconcile multimodal evidence, maintain consistent participant state tracking, or reason about epistemic states

**First Experiments**:
1. Evaluate SoCoT module alone on MIDA benchmark to measure isolated reasoning improvements
2. Test DSEM module's effectiveness in maintaining participant state consistency across interactions
3. Compare integrated SoCoT+DSEM performance against baseline MLLMs on cross-dataset generalization

## Open Questions the Paper Calls Out
None

## Limitations
- The Werewolf game environment provides a simplified social interaction compared to real-world deception scenarios
- Models may exploit game-specific patterns rather than developing genuine social reasoning capabilities
- The benchmark focuses on controlled settings that may not fully capture the complexity of real-world deception

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core findings about MLLM limitations in deception detection | High |
| Performance gap between MLLMs and human-level deception detection | High |
| Effectiveness of proposed SoCoT and DSEM modules | Medium |
| Generalizability of improvements to real-world scenarios | Medium |

## Next Checks
1. Test the proposed modules (SoCoT and DSEM) on additional social interaction datasets beyond Werewolf to evaluate generalizability
2. Conduct ablation studies to isolate which components of the proposed modules contribute most to performance improvements
3. Compare model performance against human baseline groups with varying levels of social interaction experience to better contextualize the performance gap