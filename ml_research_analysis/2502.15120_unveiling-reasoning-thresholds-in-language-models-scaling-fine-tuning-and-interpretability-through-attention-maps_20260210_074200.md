---
ver: rpa2
title: 'Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and
  Interpretability through Attention Maps'
arxiv_id: '2502.15120'
source_url: https://arxiv.org/abs/2502.15120
tags:
- attention
- reasoning
- language
- dataset
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a critical parameter threshold (~1.6 billion)
  beyond which reasoning performance in decoder-only transformer-based language models
  significantly improves for tasks such as commonsense reasoning and deductive reasoning.
  Specifically, models above this threshold achieve better success rates in chain-of-thought
  (CoT) prompting for deductive reasoning tasks, particularly those requiring longer
  reasoning chains.
---

# Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps

## Quick Facts
- **arXiv ID**: 2502.15120
- **Source URL**: https://arxiv.org/abs/2502.15120
- **Reference count**: 40
- **Primary result**: Identifies ~1.6B parameter threshold where reasoning performance improves significantly for longer reasoning chains.

## Executive Summary
This study identifies a critical parameter threshold (~1.6 billion) beyond which reasoning performance in decoder-only transformer-based language models significantly improves for tasks such as commonsense reasoning and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, particularly those requiring longer reasoning chains. To address limitations in sub-threshold models, fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without exemplars in the prompt for tasks with shorter reasoning chains. Analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes.

## Method Summary
The study evaluates reasoning capabilities across multiple decoder-only transformer architectures spanning from 117M to 9B parameters using two datasets: CommonsenseQA (CSQA) for commonsense reasoning and PrOntoQA-OOD for deductive reasoning. Models are tested using chain-of-thought prompting with few-shot exemplars, and fine-tuning is applied to sub-threshold models using 1,800 task-specific exemplars. Performance is measured through accuracy on CSQA (via string parsing) and correctness of generated CoTs on deductive reasoning tasks. Attention maps are extracted and analyzed to compare token-level attention patterns between correct and incorrect reasoning outputs.

## Key Results
- Critical parameter threshold of ~1.6B identified where reasoning performance improves significantly, especially for longer reasoning chains like proof-by-contradiction and disjunction elimination
- Fine-tuning with task-specific exemplars substantially enhances reasoning performance for sub-threshold models on shorter reasoning chains
- Models generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and grammatically relevant tokens

## Why This Works (Mechanism)

### Mechanism 1
Decoder-only transformers exhibit a critical parameter threshold (~1.6B) beyond which CoT reasoning performance improves discontinuously for tasks requiring longer reasoning chains. Larger models develop sufficient representational capacity to maintain coherent multi-step deductions. The paper observes accuracy jumps between 1.542B (19.57% on CSQA) and 1.6B (43.00%) parameters, with sub-threshold models failing specifically on proof-by-contradiction and disjunction elimination—tasks requiring longer reasoning chains.

### Mechanism 2
Fine-tuning sub-threshold models on task-specific deductive exemplars enables accurate CoT generation without in-context exemplars for shorter reasoning chains. Supervised fine-tuning on structured logical reasoning examples (1,800 exemplars across 6 deductive rules) instills procedural patterns that substitute for emergent reasoning capacity. The paper reports >90% accuracy on implication elimination, conjunction introduction/elimination, and disjunction introduction after fine-tuning models ≤355M parameters.

### Mechanism 3
Models capable of correct CoT generation allocate higher token-level attention scores to subsequent correct tokens and grammatically relevant tokens (e.g., adjectives after "be" verbs). Correct reasoning correlates with attention patterns that track logical dependencies across tokens. The paper visualizes Gemma2-9B-IT attending strongly to "transparent" (the correct completion) while GPT-2 attends to irrelevant tokens like "Every" and "is."

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) prompting**
  - Why needed here: The entire paper evaluates reasoning through CoT success rates; understanding that CoT requires models to generate intermediate reasoning steps before answers is prerequisite.
  - Quick check question: Can you explain why CoT prompting differs from standard prompting, and why it might reveal reasoning limitations in smaller models?

- **Concept: In-context learning (ICL) vs. fine-tuning**
  - Why needed here: The paper contrasts ICL (few-shot prompting without weight updates) with fine-tuning (weight updates on exemplars) as strategies for enabling reasoning in sub-threshold models.
  - Quick check question: What is the computational difference between providing exemplars in a prompt versus fine-tuning on those same exemplars?

- **Concept: Deductive reasoning rules (modus ponens, proof by contradiction, etc.)**
  - Why needed here: The PrOntoQA-OOD dataset evaluates six formal deduction rules; understanding why proof-by-contradiction requires longer reasoning chains than conjunction elimination explains the threshold findings.
  - Quick check question: Why would proof-by-contradiction be harder for a language model than implication elimination (modus ponens)?

## Architecture Onboarding

- **Component map**: Models tested span 6 families: GPT2 (117M-1.5B) -> SmolLM2 (135M-1.7B) -> OpenELM (270M-3B) -> TinyLlama (1.1B) -> Stable LM 2 (1.6B) -> Gemma 2 (2B-9B IT). All are decoder-only transformers with variations including GQA, rotary positional embeddings, RMSNorm vs LayerNorm, and context lengths (1024-8192 tokens). Datasets: CSQA (commonsense MCQA, 1,221 validation questions) -> PrOntoQA-OOD (synthetic deductive reasoning, 6 rules × 100 questions).

- **Critical path**: Load model with appropriate precision (bfloat16 for experiments, float16 for attention extraction) -> Construct CoT prompt with 7-8 exemplars (CSQA) or 3-8 exemplars (PrOntoQA-OOD, depending on context limits) -> Generate with greedy decoding (do_sample=False, num_beams=1) -> Parse output using string matching ("So the answer is (X)") or compare to gold CoT.

- **Design tradeoffs**: Context length constraints (GPT2's 1024-token limit forces 3 exemplars vs. 8 for larger models, potentially underestimating GPT2's ICL capability) -> OpenELM exception (Models above the 1.6B threshold underperform, suggesting training data quality matters alongside scale) -> Fine-tuning overfitting risk (Validation loss selected best epoch, but some models showed performance drops on harder tasks post-fine-tuning).

- **Failure signatures**: Unparseable responses (Sub-threshold models produce >200 unparseable responses on CSQA, indicating format-following failures) -> Repetition without conclusion (GPT-2 repeats premises instead of concluding) -> Attention dispersion (Failed models show high attention on stopwords rather than content tokens).

- **First 3 experiments**: Replicate the CSQA threshold test with your model family: plot accuracy vs. parameter count to identify if a similar discontinuity exists around 1.5-1.6B parameters -> Test on PrOntoQA-OOD's proof-by-contradiction task: this is the hardest rule and most sensitive to the threshold—compare models straddling your suspected threshold -> Extract and visualize attention maps for correct vs. incorrect completions on a single implication-elimination example: verify whether attention-to-correct-token patterns hold for your architecture.

## Open Questions the Paper Calls Out

### Open Question 1
Can specific architectural modifications or advanced training strategies be developed to enable sub-threshold models (fewer than 1.6 billion parameters) to successfully perform deductive reasoning tasks requiring long chains, such as proof by contradiction? The paper states in the Conclusion that while fine-tuning improves performance on shorter reasoning chains, "tasks requiring longer reasoning chains, such as proof by contradiction and disjunction elimination, remain challenging for fine-tuned models with limited parameters."

### Open Question 2
What specific factors (e.g., architectural efficiency, pre-training data composition) cause the OpenELM model family to deviate from the observed reasoning scaling trends? The Introduction and Results sections note that a significant performance jump occurs above 1.6 billion parameters "except for models from the OpenELM family," indicating a divergence from the scaling laws observed in GPT-2, Gemma, and StableLM.

### Open Question 3
Is the higher attention on correct subsequent tokens and relevant parts of speech a causal mechanism for reasoning, or merely a correlational byproduct of successful generation? The Attention Map Analysis section concludes that models generating correct CoTs "exhibit higher token-level scores... for subsequent correct tokens," providing an interpretability insight, but stops short of claiming this attention pattern causes the correct reasoning.

## Limitations

- The paper's primary claim about a 1.6B parameter threshold is based on a specific set of decoder-only transformer architectures, with the OpenELM exception suggesting training data quality may be a confounding factor
- The attention map analysis, while suggestive, is based on only two model comparisons and doesn't establish causation between attention patterns and reasoning quality
- Fine-tuning benefits degrade on longer reasoning chains, suggesting fundamental limitations that aren't fully acknowledged

## Confidence

- **High confidence**: The empirical observation of improved CoT reasoning performance at ~1.6B parameters for GPT-2 and similar architectures on the tested datasets
- **Medium confidence**: The claim that this threshold represents an emergent reasoning capability rather than a combination of capacity, data quality, and architectural factors
- **Low confidence**: The assertion that fine-tuning can fully compensate for parameter threshold limitations across all reasoning tasks

## Next Checks

1. **Cross-architecture threshold validation**: Test the 1.6B threshold hypothesis using decoder-only transformers from multiple architectural families (e.g., LLaMA, Mistral) with controlled training data and evaluation procedures to determine whether the threshold is architecture-specific or represents a more general phenomenon.

2. **Causal attention intervention experiment**: Design an experiment that manipulates attention patterns (e.g., through attention-based regularization during fine-tuning or post-hoc attention steering) to test whether directing attention to correct tokens causes improved reasoning performance, rather than merely correlating with it.

3. **Fine-tuning capacity ceiling analysis**: Systematically vary the complexity of fine-tuning exemplars and training duration for sub-threshold models on longer reasoning chains to determine the fundamental limits of pattern-based compensation for parameter deficiency, and identify when genuine reasoning capacity becomes necessary versus when surface pattern completion suffices.