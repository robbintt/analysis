---
ver: rpa2
title: A parametric activation function based on Wendland RBF
arxiv_id: '2507.11493'
source_url: https://arxiv.org/abs/2507.11493
tags:
- activation
- functions
- function
- wendland
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel parametric activation function based
  on Wendland radial basis functions (RBFs) for deep neural networks. The proposed
  enhanced Wendland activation combines a standard Wendland component with linear
  and exponential terms, offering tunable locality, improved gradient propagation,
  and enhanced stability during training.
---

# A parametric activation function based on Wendland RBF

## Quick Facts
- arXiv ID: 2507.11493
- Source URL: https://arxiv.org/abs/2507.11493
- Reference count: 28
- Introduces novel parametric activation function based on Wendland RBFs with tunable locality and improved gradient propagation

## Executive Summary
This paper introduces a novel parametric activation function based on Wendland radial basis functions (RBFs) for deep neural networks. The proposed enhanced Wendland activation combines a standard Wendland component with linear and exponential terms, offering tunable locality, improved gradient propagation, and enhanced stability during training. Empirical experiments on synthetic tasks (sine wave approximation) and benchmark datasets (MNIST, Fashion-MNIST) demonstrate competitive performance.

## Method Summary
The Enhanced Wendland activation function is defined as: $f(x) = x \odot \text{ReLU}(1 - \alpha r)^k (k \alpha r + 1) + \lambda r + \epsilon e^{-\beta r}$, where $r$ is the L2 norm of the input vector, $k=4$ (fixed), and $\alpha$, $\beta$ are learnable parameters. The function combines compact support from Wendland RBFs with linear and exponential terms to ensure smooth transitions and prevent vanishing gradients. Parameters $\lambda=0.1$ and $\epsilon=0.01$ are fixed, while $\alpha$ and $\beta$ are learned during training.

## Key Results
- Achieves ~99.27% accuracy on MNIST using LeNet-5 architecture
- Demonstrates superior performance in sine wave regression compared to standard activations
- Shows competitive results on Fashion-MNIST with VGG-style architectures

## Why This Works (Mechanism)

### Mechanism 1
The enhanced Wendland activation provides controllable locality through compact support, which may improve generalization by focusing learning on relevant feature regions. The term (1-αr)^k_+ smoothly decays to zero beyond a learned radius defined by α, creating localized activation patterns that limit interference between distant features while maintaining smooth transitions.

### Mechanism 2
The combination of linear and exponential terms with the Wendland component improves gradient propagation and prevents vanishing gradients. The linear term λr maintains non-zero gradients as r grows large, while the exponential term εe^(-βr) ensures numerical stability near r≈0, complementing the Wendland component's compact support behavior.

### Mechanism 3
Trainable parameters (α, λ, β, ε) enable the activation function to adapt its shape to different network architectures and tasks. During backpropagation, gradients flow through all parameters, allowing the network to learn optimal activation shapes rather than using fixed formulations like ReLU or sigmoid.

## Foundational Learning

- **Concept: Radial Basis Functions (RBFs)**
  - Why needed here: The Wendland function is a specific type of RBF; understanding that RBFs compute outputs based on distance from a center point is essential for interpreting the activation's behavior.
  - Quick check question: Can you explain why an RBF's output depends on ‖x - c‖ rather than just x?

- **Concept: Compact support**
  - Why needed here: The paper leverages Wendland's compact support property, meaning the function is exactly zero outside a finite radius, which differs fundamentally from global activations like sigmoid.
  - Quick check question: What is the mathematical difference between a function with compact support versus one with asymptotic decay?

- **Concept: Positive definiteness in approximation theory**
  - Why needed here: Wendland functions guarantee solvability of interpolation problems due to positive definiteness; this property underpins the theoretical motivation for using them as activations.
  - Quick check question: Why does positive definiteness matter for kernel methods or interpolation?

## Architecture Onboarding

- **Component map:**
  - Input tensor x → Compute channel-wise L2 norm r = ‖x‖ → Apply combined transformation: x ⊙ [Wendland(r) + λr + εe^(-βr)] → Output tensor

- **Critical path:**
  1. Initialize parameters: α=1.0, k=4, λ=0.1, ε=0.01, β=1.0
  2. Ensure r computation handles batch dimensions correctly
  3. Register α, λ, β, ε as learnable parameters in your framework
  4. Apply element-wise multiplication (⊙) between input x and the scalar radial factor

- **Design tradeoffs:**
  - Larger k → smoother function but higher computational cost
  - Larger λ → better gradient flow at large r but reduced locality
  - Compact support vs. global coverage: Wendland alone is zero outside support; linear/exponential terms add global coverage at the cost of some locality

- **Failure signatures:**
  - Loss NaN during training: Likely r calculation producing invalid values; add small epsilon to r before operations
  - No improvement over ReLU: α may be too small; try initializing α to larger values (e.g., 2.0-5.0)
  - Slow convergence: Learning rate for activation parameters may be too low; consider separate learning rates

- **First 3 experiments:**
  1. Synthetic sine wave regression: Replace ReLU with enhanced Wendland in a 3-layer MLP; compare MSE convergence curves (replicates Example 5.1).
  2. Moons/Circles classification: Test on 2D binary classification with visualize decision boundaries to observe locality effects (replicates Example 5.2).
  3. MNIST with LeNet: Benchmark against ReLU, ELU, Swish on standard LeNet-5 architecture; track accuracy and training time (partial replication of Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Wendland RBFs be combined with other activation types to create superior hybrid architectures?
- Basis: The conclusion states, "In future work, we plan to explore combine Wendland RBFs with other types of activations to leverage the best of both worlds."
- Why unresolved: The current study evaluates the enhanced Wendland function in isolation against standard baselines, but does not investigate its behavior in heterogeneous layers or mixed-activation networks.

### Open Question 2
- Question: Does the proposed activation scale effectively to very deep architectures and high-dimensional datasets?
- Basis: The empirical validation is restricted to synthetic tasks (sine waves) and simple benchmarks (MNIST, Fashion-MNIST) using VGG and LeNet.
- Why unresolved: It is unclear if the computational overhead of the radial calculation and learnable parameters hinders performance on complex, large-scale tasks like ImageNet or in deep residual networks.

### Open Question 3
- Question: What is the sensitivity of the network to the initialization and learnability of the four introduced parameters (α, λ, β, ε)?
- Basis: The function introduces multiple tunable parameters to achieve adaptability, but the paper does not provide an ablation study on how these specific parameters impact gradient flow or overfitting individually.
- Why unresolved: Without ablation studies, it is difficult to determine if the "linear" or "exponential" tail components are strictly necessary or if they introduce optimization instability in different contexts.

## Limitations
- Limited empirical validation on only MNIST and Fashion-MNIST, without testing on more complex datasets like CIFAR-10/100 or ImageNet
- Absence of ablation studies isolating the contribution of individual components (Wendland, linear, exponential terms)
- No testing on modern deep architectures (ResNet, Transformers) to assess scalability

## Confidence
- **Medium**: The synthetic regression and simple classification experiments provide reasonable proof-of-concept, but the limited scope of benchmarks and absence of comprehensive hyperparameter tuning prevent strong conclusions about superiority over established activations.

## Next Checks
1. **Scale-up test**: Evaluate Enhanced Wendland on CIFAR-10/100 using ResNet architectures to assess performance on natural images with higher complexity.
2. **Component ablation**: Systematically remove or replace each term (Wendland, linear, exponential) with baseline alternatives to quantify their individual contributions to accuracy and training stability.
3. **Transfer learning probe**: Fine-tune a pre-trained vision transformer with Enhanced Wendland activations on downstream tasks to evaluate performance in large-scale transfer learning scenarios where gradient flow and locality become critical.