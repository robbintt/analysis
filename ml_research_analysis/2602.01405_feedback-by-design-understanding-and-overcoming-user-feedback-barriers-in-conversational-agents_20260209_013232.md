---
ver: rpa2
title: 'Feedback by Design: Understanding and Overcoming User Feedback Barriers in
  Conversational Agents'
arxiv_id: '2602.01405'
source_url: https://arxiv.org/abs/2602.01405
tags:
- feedback
- users
- user
- human
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work identifies four Feedback Barriers\u2014Common Ground,\
  \ Verifiability, Communication, and Informativeness\u2014that prevent users from\
  \ giving high-quality feedback to conversational agents. Through a formative study\
  \ and a controlled experiment with a scaffolded system (FeedbackGPT), the authors\
  \ demonstrate that lightweight, model-agnostic interfaces can significantly improve\
  \ feedback quality by reducing these barriers."
---

# Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents

## Quick Facts
- **arXiv ID:** 2602.01405
- **Source URL:** https://arxiv.org/abs/2602.01405
- **Reference count:** 40
- **One-line primary result:** Lightweight, model-agnostic interfaces can significantly improve feedback quality by reducing four identified feedback barriers.

## Executive Summary
This work identifies four Feedback Barriers—Common Ground, Verifiability, Communication, and Informativeness—that prevent users from giving high-quality feedback to conversational agents. Through a formative study and a controlled experiment with a scaffolded system (FeedbackGPT), the authors demonstrate that lightweight, model-agnostic interfaces can significantly improve feedback quality by reducing these barriers. Users provided more goal-referenced, actionable, and progressive feedback with FeedbackGPT, though articulation and cognitive load remained challenges. The findings highlight the need for both interaction design and model-level advances to enable truly collaborative human-AI dialogue.

## Method Summary
The authors conducted a within-subjects study comparing a baseline ChatGPT interface against FeedbackGPT, a scaffolded system with six feedback mechanisms: inline comments/highlights, huddles for clarifying questions, split-view comparisons, quick actions, evaluation checks, and undo/redo. Twenty participants completed two co-writing tasks (500-1000 words) each, with feedback quality measured through log analysis (goal-referenced, actionable, articulated, progressive metrics) and subjective surveys. A formative study with 16 participants first identified the four feedback barriers through interviews and thematic analysis.

## Key Results
- FeedbackGPT users provided 26 percentage points more goal-referenced feedback (58% vs 32%) compared to baseline
- Users gave 1.7x more feedback turns per conversation with FeedbackGPT
- Users perceived significantly higher ability to overcome Communication (2.0x), Informativeness (2.3x), and Verifiability (1.6x) barriers, though Common Ground showed no significant improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anchoring feedback to specific output spans (inline comments) reduces the cognitive cost of maintaining "Common Ground" and improves the goal-referenced nature of user input.
- **Mechanism:** By externalizing the "shared frame of reference" into a persistent UI artifact (highlights/comments), the system offloads the user's working memory. This transforms abstract feedback (e.g., "fix the second paragraph") into grounded, locational constraints, directly countering context drift.
- **Core assumption:** Users can identify the error location more easily than they can articulate the abstract nature of the error.
- **Evidence anchors:**
  - [Section 3.5.5] Defines the "Barrier of Common Ground" as the model failing to maintain the task goal and the user disengaging.
  - [Section 4.2.1] Describes "Inline Comments & Highlights" as anchoring feedback to concrete text to reduce context drift.
  - [Section 6.2] Results show Goal-referenced feedback increased from 32% to 58% with FeedbackGPT.
- **Break condition:** If the underlying model fails to attend to the injected context markers (e.g., ignores the highlight coordinates), the grounding fails, and user trust degrades.

### Mechanism 2
- **Claim:** Proactive clarification loops (Huddles) lower the "Barrier of Communication" by shifting the burden of articulation from the user to the system.
- **Mechanism:** Instead of waiting for a perfect prompt, the system switches to a "Huddle" mode where the LLM adopts an interrogative stance, asking targeted questions. This decomposes complex feedback into low-effort, binary or short-form responses, bridging the "gulf of execution."
- **Core assumption:** The LLM is capable of generating relevant clarifying questions without hallucinating user intent.
- **Evidence anchors:**
  - [Abstract] States that scaffolds aligned with desiderata enabled users to provide higher-quality feedback.
  - [Section 4.2.2] Details "Feedback Huddle" as a panel where the model proactively asks targeted, clarifying questions.
  - [Corpus] *Feedstack* (2506.03052) supports this by suggesting "layering structured representations over unstructured feedback" scaffolds conversation.
- **Break condition:** If the Huddle questions are repetitive or off-topic, it increases interaction friction rather than reducing it (cognitive load risk).

### Mechanism 3
- **Claim:** Side-by-side visual comparisons and explicit "Explanations" reduce the "Barrier of Verifiability," encouraging users to critique rather than blindly accept outputs.
- **Mechanism:** Making the model's reasoning and the diff of changes visible ("scannable") lowers the cost of evaluation. Users can spot hallucinations or logic gaps without reading the entire output, supporting "corrective" feedback loops.
- **Core assumption:** Users possess the domain knowledge to validate the output if the reasoning is exposed.
- **Evidence anchors:**
  - [Section 3.5.2] Notes that users often skip verification ("gut check") due to effort; Sec 4.2.3 claims "Split-View" makes comparative evaluation cheap.
  - [Section 7.1] Discusses "productive friction"—users felt more in control despite higher cognitive load because they could verify changes.
  - [Abstract] Highlights that interfaces can improve feedback quality by reducing barriers.
- **Break condition:** If the "Explanation" is unfaithful to the model's actual logic (post-hoc rationalization), it breeds false confidence.

## Foundational Learning

- **Concept: Grice's Maxims (Cooperative Principle)**
  - **Why needed here:** The paper uses these (Quantity, Quality, Relation, Manner) to diagnose *why* feedback fails. An engineer needs this vocabulary to classify failure modes (e.g., is the user lazy [Quantity] or confused [Manner]?).
  - **Quick check question:** If a user says "wrong, try again," which maxim is the user violating? (Answer: Maxim of Quantity/Manner—insufficient information/clarity).

- **Concept: Formative vs. Summative Feedback**
  - **Why needed here:** The system is designed for "progressive" (formative) feedback during a task, not just a thumbs up/down (summative) at the end. The architecture must support iterative state updates.
  - **Quick check question:** Does the system discard the conversation state after a "Huddle," or does it merge the clarified intent back into the main context?

- **Concept: Mixed-Initiative Interaction**
  - **Why needed here:** FeedbackGPT is not a passive tool; it "pushes back" (e.g., asking clarifying questions). Understanding this shift from command-response to negotiation is vital for designing the logic flow.
  - **Quick check question:** In the "Huddle" mode, who holds the initiative (turn-taking control)? (Answer: The system/Agent).

## Architecture Onboarding

- **Component map:**
  - Frontend (Next.js) -> Backend (Node.js/Express) -> MongoDB Database -> OpenAI API (GPT-4o) -> Context Injection Layer

- **Critical path:**
  1. User highlights text -> Types comment
  2. Backend converts highlights into (context, comment) pairs + "good"/"needs improvement" tags
  3. Appends these pairs to the user message context
  4. LLM generates response adhering to constraints
  5. Huddle Switch (Conditional): If Huddle is triggered, backend swaps the system prompt to "Interrogative/Clarification" mode

- **Design tradeoffs:**
  - Control vs. Ease: FeedbackGPT increases user control (scaffolds) but also cognitive load (users reported "burden to provide right feedback was on me")
  - Latency vs. Quality: The "Huddle" and "Explanation" features add extra LLM round-trips, increasing latency compared to a standard chat

- **Failure signatures:**
  - "Context Drift" (Common Ground Barrier): The model ignores injected highlights and hallucinates new constraints
  - "Gut Check" Behavior: Users skip the "Explanation" feature if it is too verbose or hard to access
  - Articulation Gap: Users rely on Quick Actions/Huddles but still provide vague feedback

- **First 3 experiments:**
  1. A/B Test Context Injection: Compare raw prompt vs. structured highlight injection. Measure the "Goal-Referenced" score of the resulting model output
  2. Huddle Efficacy: Trigger "Huddle" automatically when user input is < 10 chars. Measure if feedback quality improves or if users drop off (friction)
  3. Cognitive Load Measurement: Use a NASA-TLX survey (as done in the paper) to verify if the "productive friction" actually improves task success rate or just user fatigue

## Open Questions the Paper Calls Out

- **Question:** How do user feedback behaviors and cognitive load evolve over time when using scaffolded conversational interfaces?
  - **Basis in paper:** [explicit] Section 8 (Limitations) states: "Given the short term nature of our study, we cannot predict how user behaviors will evolve. We leave exploration of longitudinal studies on user feedback behavior as future work."
  - **Why unresolved:** The study was limited to one-hour sessions, preventing observation of habituation or long-term adoption of feedback scaffolds.
  - **What evidence would resolve it:** Longitudinal field studies tracking changes in feedback quality and perceived effort over weeks or months of usage.

- **Question:** Can structured feedback data from scaffolded interfaces (e.g., highlights, huddles) improve RLHF training efficacy compared to coarse preference signals?
  - **Basis in paper:** [explicit] Section 7.4 suggests scaffolded interfaces yield structured preference tuples that "could eventually serve as better training signals" for RLHF and DPO, unlike current coarse labels.
  - **Why unresolved:** The paper focused on interaction quality and user experience, but did not implement or evaluate a training pipeline using the collected structured data.
  - **What evidence would resolve it:** A comparative study fine-tuning models on scaffolded interaction logs versus standard thumbs-up/down data to measure alignment performance.

- **Question:** How do different scaffolds interact with each other to influence user feedback strategies?
  - **Basis in paper:** [explicit] Section 8 notes: "we have not studied how different actions interact with each other or with the user to elicit different types of feedback... We leave this exploration to future work."
  - **Why unresolved:** The evaluation treated the scaffolds as a holistic system (FeedbackGPT) rather than isolating combinatorial effects.
  - **What evidence would resolve it:** A factorial design experiment varying the presence of specific scaffolds to analyze interaction effects on feedback specificity and frequency.

## Limitations

- Controlled setting may not capture real-world feedback dynamics in open-ended or creative tasks
- Reliance on GPT-4o for both agent and evaluator introduces potential bias
- 20-participant sample may not capture full spectrum of user diversity and expertise
- Effectiveness may diminish over time as users become habituated to the interface

## Confidence

- **High Confidence:** The identification of the four Feedback Barriers is well-supported by both qualitative and quantitative evidence
- **Medium Confidence:** The relative effectiveness of individual scaffolds is supported but could benefit from larger sample sizes and more varied task types
- **Low Confidence:** Claims about generalizability to non-co-writing tasks and different LLM architectures require further validation

## Next Checks

1. **Longitudinal Study:** Conduct a week-long deployment study to assess whether quality improvements persist as users become accustomed to the scaffolding system
2. **Cross-Model Validation:** Test the scaffolding approach with multiple LLM providers (e.g., Claude, Gemini) to verify model-agnostic behavior
3. **Real-World Deployment Analysis:** Deploy FeedbackGPT in a professional setting where feedback quality directly impacts business outcomes