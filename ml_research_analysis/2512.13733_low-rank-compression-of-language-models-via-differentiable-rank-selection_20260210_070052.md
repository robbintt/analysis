---
ver: rpa2
title: Low-Rank Compression of Language Models via Differentiable Rank Selection
arxiv_id: '2512.13733'
source_url: https://arxiv.org/abs/2512.13733
tags:
- compression
- performance
- ratio
- rank
- param
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learning to Low-Rank Compress (LLRC), a fine-tuning-free
  approach for selecting optimal per-layer ranks in low-rank compressed language models.
  LLRC trains a learnable masking layer to adaptively select singular values using
  a multi-objective loss function that balances compression, distillation, and smoothness.
---

# Low-Rank Compression of Language Models via Differentiable Rank Selection

## Quick Facts
- **arXiv ID**: 2512.13733
- **Source URL**: https://arxiv.org/abs/2512.13733
- **Reference count**: 0
- **Primary result**: LLRC achieves 12% MMLU accuracy improvement over STRS at 20% compression on Llama-2-13B

## Executive Summary
This paper introduces Learning to Low-Rank Compress (LLRC), a fine-tuning-free approach for selecting optimal per-layer ranks in low-rank compressed language models. LLRC uses a learnable masking layer to adaptively select singular values through a multi-objective loss function that balances compression, distillation, and smoothness. The method is evaluated across multiple model sizes (Llama-2-7B, Llama-2-13B, Llama-3-8B, Gemma-7B) and consistently outperforms prior methods like STRS and ARS across various compression rates and datasets.

## Method Summary
LLRC trains a learnable masking layer to adaptively select singular values for low-rank compression of language models. The approach uses a multi-objective loss function that balances compression ratio, distillation from the original model, and smoothness regularization. Unlike previous methods that use fixed or manually-tuned per-layer ranks, LLRC learns these ranks through differentiable optimization. The method employs any-k selection rather than top-k singular values and includes practical heuristics like ignoring trivial compression layers to improve performance.

## Key Results
- At 20% compression on Llama-2-13B, LLRC improves accuracy by 12% on MMLU, 3.5% on BoolQ, and 4.4% on OpenbookQA compared to STRS
- LLRC outperforms fine-tuning-free SVD-LLM and LLM-Pruner, and remains competitive with their fine-tuning variants
- The method shows consistent improvements across multiple model sizes and compression rates while maintaining stability

## Why This Works (Mechanism)
LLRC's effectiveness stems from its ability to learn optimal per-layer ranks through differentiable optimization rather than relying on fixed or manually-tuned values. The learnable masking layer can adapt to the specific characteristics of each layer, preserving important information while achieving compression. The multi-objective loss function ensures that compression doesn't come at the expense of model performance, while smoothness regularization prevents abrupt changes in rank selection. The any-k selection approach allows for more flexible and efficient compression compared to traditional top-k methods.

## Foundational Learning
- **Singular Value Decomposition (SVD)**: Matrix factorization technique that decomposes matrices into singular values and vectors, essential for low-rank approximation of neural network layers
  - *Why needed*: Forms the mathematical foundation for low-rank compression of model parameters
  - *Quick check*: Verify understanding of how SVD enables dimensionality reduction while preserving most information

- **Differentiable Rank Selection**: Technique that makes rank selection a learnable parameter through gradient-based optimization
  - *Why needed*: Enables automatic adaptation of compression levels to different layers and tasks
  - *Quick check*: Understand how soft masks enable gradient flow for discrete rank selection

- **Knowledge Distillation**: Training a smaller model to mimic the behavior of a larger, pre-trained model
  - *Why needed*: Maintains model performance during compression by transferring knowledge from original weights
  - *Quick check*: Verify how distillation loss terms are incorporated into the multi-objective optimization

## Architecture Onboarding
**Component Map**: Input data -> SVD decomposition -> Learnable masking layer -> Reconstructed weights -> Model inference -> Multi-objective loss
**Critical Path**: The learnable masking layer sits between SVD decomposition and weight reconstruction, controlling which singular values are preserved for each layer
**Design Tradeoffs**: Balances compression ratio against model performance through multi-objective optimization, with learnable masks providing flexibility at the cost of additional parameters
**Failure Signatures**: Poor rank selection leading to performance degradation, instability during training due to improper regularization, or failure to converge on optimal masks
**First Experiments**:
1. Test rank selection on individual layers before full model compression
2. Validate the effectiveness of any-k vs top-k selection on a small model
3. Evaluate the impact of different regularization strengths on rank smoothness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on classification tasks with limited coverage of generation tasks critical for language model utility
- Analysis of per-layer rank distributions lacks statistical rigor in explaining layer-specific compression benefits
- Comparison with fine-tuning methods is incomplete, evaluating only basic SVD-LLM and LLM-Pruner variants

## Confidence
- **High confidence**: The core technical contribution of using differentiable rank selection through learnable masking is well-validated through ablation studies and consistent performance improvements across multiple datasets and models
- **Medium confidence**: The practical heuristics (ignoring trivial compression layers, any-k vs top-k selection) show empirical benefits but lack theoretical justification for why these specific thresholds were chosen
- **Low confidence**: The scalability claims to larger models and the assertion that LLRC remains competitive with fine-tuning methods need further validation, particularly given the limited scope of compared fine-tuning baselines

## Next Checks
1. Evaluate LLRC on generation tasks (e.g., commonsense reasoning, story completion) to verify performance beyond classification benchmarks
2. Test the method on larger models (70B+ parameters) and extreme compression rates (>50%) to assess scalability limits and potential performance degradation patterns
3. Compare against state-of-the-art fine-tuning approaches (e.g., LoRA, prefix tuning) with similar computational budgets to better contextualize the trade-offs between fine-tuning-free and fine-tuning methods