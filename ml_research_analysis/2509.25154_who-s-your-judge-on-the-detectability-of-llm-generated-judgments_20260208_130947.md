---
ver: rpa2
title: Who's Your Judge? On the Detectability of LLM-Generated Judgments
arxiv_id: '2509.25154'
source_url: https://arxiv.org/abs/2509.25154
tags:
- judgment
- detection
- judgments
- llm-generated
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce judgment detection, the task of distinguishing LLM-generated
  judgments from human-produced ones using only judgment scores and candidate content.
  Existing LLM-generated text detection methods perform poorly due to their inability
  to capture the interaction between judgment scores and candidate content.
---

# Who's Your Judge? On the Detectability of LLM-Generated Judgments

## Quick Facts
- arXiv ID: 2509.25154
- Source URL: https://arxiv.org/abs/2509.25154
- Reference count: 30
- Primary result: J-Detector achieves up to 99.8% F1 score in distinguishing LLM-generated judgments from human judgments

## Executive Summary
This paper introduces judgment detection, the task of identifying whether evaluation scores come from large language models (LLMs) or humans. The authors find that existing text detection methods fail on this task because they cannot capture the interaction between judgment scores and candidate content. They propose J-Detector, a lightweight and interpretable neural detector that uses explicitly extracted linguistic and LLM-enhanced features to link LLM judge biases with candidate properties. Experiments across diverse datasets demonstrate J-Detector's effectiveness, achieving up to 99.8% F1 score, while also enabling quantification of biases in LLM judges.

## Method Summary
J-Detector is a binary classifier that distinguishes LLM-generated judgments from human judgments using only judgment scores and candidate content (no textual feedback). It extracts three types of features: base judgment scores, linguistic features (length, lexical diversity, readability, syntactic complexity, discourse markers via spaCy), and LLM-enhanced features (style/format/scores from Qwen-3-8B). A lightweight classifier (RandomForest/LGBM/XGB) is trained on concatenated features. For group-level detection, instance logits are aggregated via summation. The method is evaluated on JD-Bench with four datasets and 20 LLM families.

## Key Results
- J-Detector achieves up to 99.8% F1 score in detecting LLM-generated judgments
- Performance degrades significantly in single-dimension settings (down to ~50-65% F1)
- Group-level aggregation improves detection accuracy from 63.9% to 85.0% F1 when increasing group size from k=1 to k=16
- SLM-based baselines fail on single-dimension datasets, achieving only ~50% F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly extracted linguistic and LLM-enhanced features capture systematic correlations between judgment scores and candidate properties that reveal LLM judge biases.
- Mechanism: LLM judges exhibit measurable biases (length, complexity, confidence, surface beauty) that create detectable patterns when judgment scores are compared against candidate features. J-Detector extracts these as interpretable features—linguistic statistics (e.g., syntax tree depth, readability scores) and LLM-generated style/quality scores—which a lightweight classifier uses to distinguish human from LLM judgments.
- Core assumption: LLM judges produce judgment scores that systematically diverge from human judgments in ways correlated with candidate properties.
- Evidence anchors:
  - [abstract] "augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection"
  - [section 5] lists specific bias types: length bias, lexical diversity, readability/fluency bias, syntactic complexity bias, discourse/presentation bias
  - [corpus] Related work on LLM-as-a-judge bias (Ye et al. 2024, Li et al. 2025a) confirms systematic biases exist, but direct evidence for detectability via these features is limited to this paper's experiments

### Mechanism 2
- Claim: Multi-dimensional and distributional judgment patterns provide intrinsic detectability signals independent of candidate content.
- Mechanism: When judgments contain multiple scores across dimensions (e.g., soundness, novelty, clarity), LLMs exhibit different score distributions and inter-dimensional correlations than humans. SLM-based detectors can exploit these patterns in multi-dimension settings but fail in single-dimension settings where such cues are absent.
- Core assumption: Humans and LLMs have statistically distinguishable score distributions across judgment dimensions.
- Evidence anchors:
  - [abstract] "existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content"
  - [section 4, Figure 2] shows 97%+ accuracy on multi-dimension datasets vs ~50% on single-dimension for SLM-based methods
  - [corpus] No direct corpus evidence on distributional detectability; this appears to be a novel finding

### Mechanism 3
- Claim: Group-level aggregation amplifies detectability by pooling evidence across multiple judgments from the same source.
- Mechanism: Detection is performed on judgment groups (e.g., a reviewer's scores across multiple papers). Summing instance-level logits aggregates weak individual signals into stronger group-level evidence, reducing variance and improving classification confidence.
- Core assumption: Judgment groups contain consistent source patterns.
- Evidence anchors:
  - [section 5] "score(G) = Σẑᵢ" for group-level aggregation
  - [section 7.1, Figure 6] F1 improves from 63.9% at k=1 to 85.0% at k=16 for Helpsteer3
  - [corpus] No corpus evidence on group-level detection; this is specific to this paper

## Foundational Learning

### Concept: LLM-as-a-judge paradigm
- Why needed here: Understanding that LLMs are used to evaluate content via scores (pointwise, pairwise, listwise) is prerequisite to grasping why detecting their outputs matters and how biases emerge.
- Quick check question: Can you explain the difference between pointwise, pairwise, and listwise judgment types and give an application example for each?

### Concept: Machine-generated text detection
- Why needed here: This paper adapts and critiques existing text detection methods; knowing baseline approaches (perplexity, watermarking, SLM classifiers) helps understand why they fail for judgment detection.
- Quick check question: Why does text detection relying on linguistic style fail when only judgment scores (not review text) are available?

### Concept: Feature engineering for classification
- Why needed here: J-Detector's core innovation is explicit feature extraction (linguistic + LLM-enhanced); understanding feature design, interpretability, and classifier choice (RandomForest, LGBM) is essential.
- Quick check question: What is the advantage of using explicit interpretable features over end-to-end neural classification for bias quantification?

## Architecture Onboarding

### Component map:
- Input: Judgment groups G = {(cᵢ, jᵢ)}ₖᵢ₌₁ (candidate content + scores)
- Feature extraction: F = F_base ⊕ F_LLM ⊕ F_linguistic
- Classifier: Lightweight binary classifier (RandomForest/LGBM/XGB)
- Aggregation: Sum logits across group instances → group-level score
- Output: Binary prediction (human=0, LLM=1)

### Critical path:
1. Extract linguistic features from candidates (spaCy pipeline)
2. Generate LLM-enhanced features (prompt Qwen-3-8B for style/format/scores)
3. Concatenate all features with base judgment scores
4. Train classifier on labeled data; save model
5. At inference: extract features → classifier logits → aggregate if group → threshold

### Design tradeoffs:
- **Interpretability vs. end-to-end learning**: Explicit features enable bias quantification but may miss subtle patterns neural models could learn
- **Efficiency vs. richness**: LLM-enhanced features require additional inference calls; linguistic features are cheap but coarser
- **Group vs. instance detection**: Group aggregation improves accuracy but requires multiple judgments from same source

### Failure signatures:
- **Single-dimension, small-group**: Low F1 (~50–65%) when only one score dimension and k≤2
- **Multiple LLM judges mixed**: F1 drops sharply (e.g., 99.8% → 66.9%) when judgments come from diverse LLMs
- **Highly aligned LLMs**: API-based and reasoning models show lower detectability, approaching human-like distributions
- **Missing text + coarse scale**: Both conditions compound to reduce detectability

### First 3 experiments:
1. **Baseline replication**: Run RoBERTa and Longformer on JD-Bench with/without candidates; confirm ~97% F1 on multi-dimension and ~50% on single-dimension to validate the warm-up finding.
2. **Ablation on features**: Train J-Detector variants removing F_LLM, F_linguistic, or both; quantify contribution of each feature type across datasets and group sizes.
3. **Cross-model generalization**: Train on one LLM judge's judgments, test on another (e.g., train on GPT-4o, test on Claude); measure performance drop to assess detector robustness across model families.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can detection frameworks be adapted to maintain high accuracy when judgments originate from a heterogeneous mixture of multiple unknown LLM judges?
- Basis in paper: [explicit] Section 7.2 states: "One promising direction for future work is to explore effective LLM-generated judgment detection methods under multiple judges' settings."
- Why unresolved: The study shows a substantial performance drop (e.g., 99.8% to 66.9% F1 on Helpsteer2) when judgments are mixed from different models, as distinct model patterns become harder to isolate.

### Open Question 2
- Question: Is there a theoretical limit to judgment detectability as LLMs approach near-perfect alignment with human preferences?
- Basis in paper: [inferred] Section 7.1 observes a negative correlation between LMArena scores (alignment proxy) and detectability, suggesting that as models align better with human values, they become harder to distinguish.
- Why unresolved: While the paper establishes the correlation, it does not determine if detection accuracy will converge to a random baseline (50%) as model capabilities mature.

### Open Question 3
- Question: Can detectors remain robust against adversarial attacks where LLM judges are specifically prompted to mimic human score distributions?
- Basis in paper: [inferred] The introduction mentions malicious prompts used to trick AI reviewers, but the evaluation focuses on standard model outputs rather than adversarially obscured judgments.
- Why unresolved: It is unclear if J-Detector relies on superficial heuristics (like length bias) that could be easily suppressed by an adversarial prompt instructing the LLM to vary its scoring behavior.

## Limitations

- **Single-dimension, small-group settings**: Performance degrades to ~50-65% F1, limiting practical utility in many real-world applications
- **Cross-model robustness**: Substantial performance drops when tested on different LLM families (e.g., 99.8% → 66.9%) raises questions about generalization
- **Feature extraction costs**: F_LLM features require additional LLM inference calls, introducing computational overhead and potential latency issues

## Confidence

- **High confidence**: J-Detector's effectiveness on multi-dimension, larger-group datasets (F1 up to 99.8%) is well-supported by experiments across four diverse datasets and extensive LLM families
- **Medium confidence**: The interpretability claims regarding bias quantification are supported by feature analysis but would benefit from deeper qualitative validation of the identified bias patterns
- **Medium confidence**: Practical utility claims for real-world scenarios are demonstrated but primarily in idealized conditions; performance in single-dimension, small-group settings remains problematic

## Next Checks

1. **Real-world deployment test**: Evaluate J-Detector on actual peer review datasets containing mixed human-LLM judgments and single-dimension scoring to assess practical detection rates in realistic settings
2. **Cross-dataset generalization**: Train J-Detector on one dataset (e.g., HelpSteer2) and test on completely different domains (e.g., medical reviews, legal judgments) to measure domain transfer capabilities
3. **Adversarial robustness**: Test J-Detector against deliberately obfuscated judgments where LLM judges are fine-tuned or prompted to mimic human scoring patterns and reduce detectable biases