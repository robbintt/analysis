---
ver: rpa2
title: 'GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache
  Eviction'
arxiv_id: '2509.00388'
source_url: https://arxiv.org/abs/2509.00388
tags:
- tokens
- cache
- graphkv
- nodes
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphKV, a graph-based framework for dynamic
  key-value (KV) cache management in large language models (LLMs). The key insight
  is that conventional token eviction methods relying on static importance scores
  fail to capture evolving token dependencies, leading to redundant retention of semantically
  similar tokens.
---

# GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction

## Quick Facts
- arXiv ID: 2509.00388
- Source URL: https://arxiv.org/abs/2509.00388
- Reference count: 28
- GraphKV achieves up to 45.88% better performance than suboptimal methods and approximately 3% improvement over state-of-the-art methods with the same KV cache size

## Executive Summary
This paper introduces GraphKV, a novel framework for dynamic key-value (KV) cache management in large language models (LLMs). The key innovation addresses a fundamental limitation of conventional token eviction methods that rely on static importance scores, which fail to capture evolving token dependencies and lead to redundant retention of semantically similar tokens. GraphKV models tokens as graph nodes with edges representing cosine similarity between their key vectors, and employs a decay-signal-propagation mechanism to iteratively refine importance scores. This approach enables adaptive retention of contextually significant and diverse tokens, effectively suppressing redundancy by propagating negative signals from high-importance tokens to their semantically similar neighbors.

## Method Summary
GraphKV constructs a similarity graph where each token is represented as a node, and edges connect nodes based on cosine similarity between their key vectors. The framework uses a decay-signal-propagation mechanism that iteratively refines token importance scores by suppressing redundancy - when a token is deemed important, negative signals are propagated to its semantically similar neighbors in the graph. This dynamic approach allows GraphKV to adapt to evolving contexts and maintain diverse, contextually relevant tokens in the KV cache. The method is designed to be plug-and-play, seamlessly integrating with existing KV cache eviction methods such as SnapKV and PyramidKV, requiring only linear additional computation.

## Key Results
- GraphKV achieves up to 45.88% better performance than suboptimal KV cache eviction methods
- GraphKV shows approximately 3% improvement over state-of-the-art methods with the same KV cache size
- In some cases, GraphKV also reduces inference latency compared to baseline methods

## Why This Works (Mechanism)
GraphKV addresses the static selection paradigm limitation by capturing evolving token dependencies through graph-based modeling. The decay-signal-propagation mechanism works by iteratively refining importance scores: when a token is identified as important, the system propagates negative signals to similar tokens in the graph, effectively suppressing redundancy. This allows the framework to maintain diverse token representations that are both contextually significant and semantically distinct. The graph structure enables dynamic adaptation to changing contexts during inference, unlike static scoring methods that assign fixed importance values.

## Foundational Learning

1. **KV Cache Management**: Why needed: Essential for efficient LLM inference by storing computed key-value pairs to avoid redundant computation
   Quick check: Understanding how KV caches store intermediate computation results during autoregressive generation

2. **Cosine Similarity for Semantic Distance**: Why needed: Provides a metric for determining semantic similarity between tokens based on their key vectors
   Quick check: Ability to compute and interpret cosine similarity between high-dimensional vector representations

3. **Graph Signal Processing**: Why needed: Enables the propagation of importance signals through the token similarity graph
   Quick check: Understanding how signals can be diffused through graph structures to update node importance scores

4. **Token Redundancy in LLMs**: Why needed: Recognizing that retaining semantically similar tokens wastes limited cache capacity
   Quick check: Ability to identify when multiple tokens convey similar information and could be considered redundant

5. **Plug-and-play Integration**: Why needed: Ensures GraphKV can be easily adopted with existing KV cache eviction frameworks
   Quick check: Understanding how modular components can be integrated without requiring complete system redesign

## Architecture Onboarding

**Component Map**: Token Embedding -> Cosine Similarity Graph Construction -> Decay-Signal-Propagation -> Refined Importance Scores -> Cache Eviction Decision

**Critical Path**: The core computational path involves building the similarity graph from key vectors, applying the decay-signal-propagation algorithm to refine importance scores, and using these scores to make cache eviction decisions. This path must complete within the inference time budget to be practical.

**Design Tradeoffs**: GraphKV trades additional linear computation for significantly improved cache utilization and potentially reduced latency. The framework balances between computational overhead (building and processing the graph) and the benefits of more intelligent token retention. The plug-and-play design sacrifices some potential optimization opportunities for broader compatibility.

**Failure Signatures**: The system may fail when token similarities are not well-captured by cosine similarity, when the graph becomes too dense (slowing propagation), or when the decay parameters are not properly tuned. Performance degradation may also occur if the propagation mechanism over-suppresses important but semantically similar tokens.

**First 3 Experiments**:
1. Verify that GraphKV correctly constructs the similarity graph by checking edge creation between tokens with high cosine similarity
2. Test the decay-signal-propagation mechanism by tracking how importance scores change for similar tokens after propagation
3. Evaluate cache hit rates with GraphKV versus baseline methods on sequences with known semantic redundancy patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide a detailed comparison of GraphKV's performance with other graph-based methods in the literature
- The scalability of the graph-based approach for very long sequences is not thoroughly discussed
- The paper does not explore the impact of different graph construction methods (e.g., varying similarity thresholds) on performance

## Confidence

| Claim | Confidence |
|-------|------------|
| Graph-based modeling of token relationships improves cache management | High |
| Decay-signal-propagation mechanism effectively suppresses redundancy | High |
| Integration with existing methods is seamless and requires only linear additional computation | Medium |
| Experimental results showing significant performance improvements | High |
| Reduction in inference latency | Medium |

## Next Checks

1. Compare GraphKV's performance with other graph-based methods in the literature to assess its relative effectiveness
2. Evaluate the scalability of GraphKV for very long sequences to determine its practical applicability in real-world scenarios
3. Investigate the impact of different graph construction methods on GraphKV's performance to identify optimal configurations