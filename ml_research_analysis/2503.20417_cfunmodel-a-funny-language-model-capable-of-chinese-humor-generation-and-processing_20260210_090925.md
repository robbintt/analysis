---
ver: rpa2
title: 'CFunModel: A "Funny" Language Model Capable of Chinese Humor Generation and
  Processing'
arxiv_id: '2503.20417'
source_url: https://arxiv.org/abs/2503.20417
tags:
- humor
- crosstalk
- joke
- chinese
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFunModel, a large language model specifically
  designed for Chinese humor generation and processing. The authors constructed CFunSet,
  a comprehensive Chinese humor dataset comprising over 160,000 samples across multiple
  tasks including humor recognition, crosstalk response selection, joke generation,
  and joke explanation.
---

# CFunModel: A "Funny" Language Model Capable of Chinese Humor Generation and Processing

## Quick Facts
- arXiv ID: 2503.20417
- Source URL: https://arxiv.org/abs/2503.20417
- Authors: Zhenghan Yu; Xinyu Hu; Xiaojun Wan
- Reference count: 22
- CFunModel outperforms popular LLMs like GPT-4o and DeepSeek-V3 on Chinese humor tasks

## Executive Summary
This paper introduces CFunModel, a specialized large language model for Chinese humor generation and processing. The authors created CFunSet, a comprehensive dataset of over 160,000 Chinese humor samples spanning humor recognition, crosstalk response selection, joke generation, and joke explanation tasks. CFunModel was developed through supervised fine-tuning of Qwen2.5-7B-Instruct using this dataset. Experimental results show CFunModel achieving superior performance on various humor-related benchmarks compared to leading models including GPT-4o and DeepSeek-V3, with notable accuracy rates of 91.70% and 88.99% for crosstalk response selection and 85.98% for humor recognition.

## Method Summary
CFunModel was developed by performing supervised fine-tuning on the Qwen2.5-7B-Instruct model using the newly constructed CFunSet dataset. The CFunSet dataset contains over 160,000 Chinese humor samples across four distinct tasks: humor recognition, crosstalk response selection, joke generation, and joke explanation. The fine-tuning process leveraged this specialized dataset to adapt the base model for humor-specific capabilities in Chinese language processing.

## Key Results
- Achieved 91.70% accuracy for Dougen responses and 88.99% for Penggen responses in crosstalk response selection
- Attained 85.98% accuracy in humor recognition tasks
- Produced more coherent and humorous content in case studies of joke and crosstalk generation compared to baseline models

## Why This Works (Mechanism)
CFunModel's effectiveness stems from its specialized training on humor-specific Chinese language data, which allows it to capture the nuanced linguistic patterns and cultural contexts essential for humor generation and understanding. By fine-tuning on CFunSet's diverse humor tasks, the model develops task-specific capabilities that general-purpose LLMs lack, particularly in understanding and generating culturally appropriate Chinese humor forms like crosstalk.

## Foundational Learning
- **Supervised Fine-tuning**: Adapting pre-trained models for specific tasks using labeled datasets; needed to specialize the base model for humor tasks, quick check: verify fine-tuning dataset size and quality
- **Cross-lingual Humor Processing**: Understanding humor across different languages and cultural contexts; needed for effective Chinese humor generation, quick check: test on bilingual humor datasets
- **Crosstalk Comedy**: Traditional Chinese comedic performance involving dialogue between two performers; needed for cultural authenticity, quick check: evaluate model's understanding of crosstalk conventions
- **Humor Recognition**: Identifying humorous content in text; needed for filtering and generation quality control, quick check: benchmark against standard humor detection datasets
- **Response Selection**: Choosing appropriate responses in dialogue systems; needed for crosstalk generation, quick check: test on multiple-choice dialogue datasets
- **Joke Explanation**: Providing context and reasoning for humor; needed for deeper understanding, quick check: verify model can explain jokes in multiple ways

## Architecture Onboarding

Component Map: CFunSet dataset -> Supervised Fine-tuning -> CFunModel -> Humor tasks (recognition, selection, generation, explanation)

Critical Path: The fine-tuning process on CFunSet is the critical path, as it directly determines the model's humor capabilities across all tasks.

Design Tradeoffs: The authors chose supervised fine-tuning over other approaches like reinforcement learning to maintain controllability and efficiency, trading potential for more creative but less predictable outputs.

Failure Signatures: Without sufficient humor data diversity, the model may overfit to specific joke patterns or fail to generalize across different humor styles and cultural contexts.

First Experiments:
1. Evaluate CFunModel on humor recognition benchmarks to establish baseline performance
2. Test crosstalk response selection accuracy on held-out Chinese dialogue pairs
3. Generate jokes and evaluate coherence and humor quality against baseline models

## Open Questions the Paper Calls Out
None

## Limitations
- No human evaluation of generated humor quality or cultural appropriateness
- Limited ablation studies to isolate contributions of specific dataset components
- Evaluation focused on Chinese language without cross-linguistic validation

## Confidence
- **High confidence**: CFunModel outperforms baseline models on benchmark accuracy metrics
- **Medium confidence**: CFunModel produces more coherent content compared to baselines
- **Low confidence**: CFunModel generates genuinely humorous or culturally appropriate Chinese humor

## Next Checks
1. Conduct human evaluation studies with native Chinese speakers to assess humor quality and cultural appropriateness of generated content
2. Perform comprehensive bias and safety analysis to identify potentially harmful or offensive outputs
3. Test model generalization by evaluating on humor datasets from other languages and cultural contexts