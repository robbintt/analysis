---
ver: rpa2
title: 'Logit Disagreement: OoD Detection with Bayesian Neural Networks'
arxiv_id: '2502.15648'
source_url: https://arxiv.org/abs/2502.15648
tags:
- uncertainty
- bayesian
- detection
- arxiv
- epistemic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses out-of-distribution (OoD) detection in Bayesian
  neural networks (BNNs) by proposing a new method that measures epistemic uncertainty
  using logit disagreement across posterior samples. While traditional methods like
  mutual information and predictive entropy have limitations in distinguishing in-distribution
  (iD) and OoD data, this work introduces three novel epistemic uncertainty scores
  based on measuring disagreement in maximum logit values.
---

# Logit Disagreement: OoD Detection with Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2502.15648
- Source URL: https://arxiv.org/abs/2502.15648
- Authors: Kevin Raina
- Reference count: 40
- Key outcome: Proposes three logit-based epistemic uncertainty scores (Disagreement Score, Weight Entropy, Std of Log-Logits) that significantly improve OoD detection in BNNs, reducing false negatives compared to mutual information while maintaining comparable performance to predictive entropy.

## Executive Summary
This paper addresses the challenge of out-of-distribution (OoD) detection in Bayesian neural networks (BNNs) by introducing a novel approach that measures epistemic uncertainty through logit disagreement across posterior samples. Traditional methods like mutual information and predictive entropy often struggle to distinguish between in-distribution (iD) data with aleatoric uncertainty (noise) and true OoD samples. The proposed method operates on pre-softmax logits, capturing disagreement in the model's internal evidence state, which proves to be a more robust indicator of OoD status. Through extensive experiments on MNIST and CIFAR10 benchmarks, the three proposed scores demonstrate significant improvements in reducing false-negative rates while maintaining competitive overall performance.

## Method Summary
The approach measures epistemic uncertainty in BNNs by computing disagreement scores across posterior weight samples using pre-softmax logits. For each input, M=500 weight samples are drawn from the variational posterior, and the maximum logit value across classes is extracted. These logits are truncated to avoid negative values (z* = z if z>0 else ε), then normalized to form a probability-like distribution. Three uncertainty scores are computed: Disagreement Score (reciprocal of sum of squared normalized logits), Weight Entropy (entropy of normalized logits), and standard deviation of log-logits. The method is evaluated on MNIST and CIFAR10 with various OoD datasets, comparing against mutual information and predictive entropy baselines using AUROC and FNR95 metrics.

## Key Results
- Logit disagreement scores significantly outperform mutual information in reducing false-negative rates (FNR95) on benchmark datasets
- All three proposed scores achieve comparable or better performance than predictive entropy on MNIST and CIFAR10
- The method shows particular robustness on CIFAR100, where it matches the best-performing method (predictive entropy)
- Weight entropy consistently performs well across all metrics and datasets

## Why This Works (Mechanism)

### Mechanism 1: Logit Disagreement as Epistemic Uncertainty
Measuring variation in maximum logit values across posterior samples provides a robust signal for OoD data, distinct from predictive entropy. If an input is iD, posterior samples produce consistent logit values (low disagreement). If OoD, the model "guesses," resulting in high variance across samples about the magnitude of evidence for the predicted class. This disagreement is quantified using the Disagreement Score derived from Effective Sample Size. The core assumption is that high disagreement in pre-softmax outputs correlates strongly with epistemic uncertainty rather than aleatoric uncertainty.

### Mechanism 2: Bypassing Softmax Miscalibration
Using raw logits instead of softmax probabilities avoids the overconfidence typically induced by softmax normalization. Softmax tends to push probabilities toward 0 or 1 even for ambiguous inputs, masking true uncertainty. By operating on pre-softmax logits with truncation of negative values, the method preserves the "distance" from decision boundary without artificial confidence boost. The structure of maximum logit is hypothesized to be more informative for OoD detection than normalized probability distribution.

### Mechanism 3: Epistemic-Aleatoric Disentanglement
The proposed scores isolate epistemic uncertainty more effectively than Mutual Information, reducing false negatives on noisy in-distribution data. Predictive Entropy conflates aleatoric and epistemic uncertainty, while standard MI often underperforms. By measuring disagreement on logits rather than information gain on distributions, the method focuses specifically on inconsistency of the model's internal evidence state, which is hypothesized to be the primary indicator of OoD status.

## Foundational Learning

- **Concept:** Variational Inference & Mean-Field Approximation
  - Why needed here: The paper assumes BNN uses variational inference to approximate posterior p(ω|D) with qθ(ω). You must understand that "sampling models" means sampling weights from this variational distribution, not training multiple independent models.
  - Quick check question: How does the "Bayesian" nature of the network differ from a standard ensemble of deterministic networks in terms of weight derivation?

- **Concept:** Epistemic vs. Aleatoric Uncertainty
  - Why needed here: Core motivation is failure of Predictive Entropy (Total Uncertainty) to distinguish noisy data from unknown data. You need to distinguish reducible uncertainty (model blindness) from irreducible uncertainty (data noise).
  - Quick check question: Would adding more training data reduce the uncertainty score for a "noisy" iD sample or an "OoD" sample?

- **Concept:** Softmax Calibration & Logits
  - Why needed here: The paper pivots from probabilities to logits. Understanding that softmax squashes magnitude into probability is crucial to seeing why logits might preserve distance/evidence information lost in probability output.
  - Quick check question: Why might a high softmax probability (0.99) be less trustworthy than a high logit value (100.0) in the context of deep learning calibration?

## Architecture Onboarding

- **Component map:** BNN Backbone -> Sampling Module (M=500) -> Logit Extractor -> Truncation Unit (z* = max(z, ε)) -> Disagreement Calculator (DS, WE, Std of LLs)

- **Critical path:** The computational bottleneck is the Sampling Module. Unlike deterministic networks (1 forward pass), this requires M forward passes (500 used in paper). Deployment requires hardware capable of batch processing these Monte Carlo samples efficiently or reducing M with acceptable variance.

- **Design tradeoffs:**
  - Scores: Disagreement Score (DS) vs. Weight Entropy (WE) vs. Std of Log-Logits. Paper suggests DS often performs best on FNR95, but all three are close. Start with DS.
  - Samples (M): High M (500) increases accuracy but kills latency. Low M increases noise in disagreement estimate.
  - Thresholding: Paper uses FNR95 for evaluation, but in production, you must select a threshold on disagreement score based on your specific safety tolerance.

- **Failure signatures:**
  - Confident OoD: If model has "memorized" or generalized unexpectedly well to OoD data, logit disagreement will be low (false negative).
  - Negative Logit Saturation: If network produces mostly negative logits for OoD, they are all truncated to ε. This makes distribution uniform (low disagreement), potentially hiding OoD signal.

- **First 3 experiments:**
  1. Sanity Check (iD vs Far-OoD): Train BNN on MNIST, test Logit Disagreement vs FashionMNIST. Verify histogram separation shown in Figure 3.
  2. Ablation on Samples (M): Run detection with M=10, 50, 100, 500. Plot stability of Disagreement Score to determine minimum viable sampling budget for latency constraints.
  3. Robustness to Noise: Train on "Clean" data, test on "Noisy" iD data (e.g., add Gaussian noise to MNIST inputs). Compare Logit Disagreement vs Predictive Entropy to verify proposed method doesn't spike (falsely flag OoD) on mere aleatoric noise.

## Open Questions the Paper Calls Out

### Open Question 1
Does combining logit disagreement scores with Laplace approximation-based posterior inference improve OoD detection compared to mean field variational inference? The current work only validates proposed scores under mean field variational inference with diagonal Gaussian posteriors.

### Open Question 2
Can other BNN feature spaces beyond logits yield better epistemic uncertainty measures for OoD detection? The paper focuses exclusively on logits as proxy for disagreement measurement; earlier layer features remain unexplored in BNN context.

### Open Question 3
Would using the full logit vector rather than only the maximum logit capture additional discriminative information for OoD detection? The methodology considers only maximum logit value across posterior samples, discarding information from non-maximum logits that may contain useful uncertainty signals.

### Open Question 4
How do logit disagreement scores perform on larger-scale datasets and more challenging near-OoD scenarios beyond MNIST and CIFAR10? Experiments are limited to MNIST and CIFAR10 benchmarks, with CIFAR100 (a near-OoD dataset) showing weakest detection performance across all methods.

## Limitations
- Method may fail when OoD data is semantically close to iD (e.g., CIFAR100 vs CIFAR10), leading to confident incorrect predictions and low disagreement
- Computational overhead of M=500 posterior samples per inference may be prohibitive for real-time applications
- Performance relies on assumption that outlier structure exists in positive logits, which may not hold for all OoD distributions

## Confidence
- High: Logit disagreement effectively captures epistemic uncertainty distinct from aleatoric uncertainty
- Medium: The method outperforms MI in reducing false negatives on noisy iD data
- Low: The method's performance is consistent across all OoD scenarios and network architectures

## Next Checks
1. Conduct ablation study with varying M (10, 50, 100, 500) to determine minimum viable sampling budget maintaining detection accuracy
2. Evaluate method on OoD datasets semantically similar to iD (e.g., CIFAR100 vs CIFAR10) to assess robustness to close-domain OoD detection
3. Perform detailed calibration analysis of proposed scores compared to MI and predictive entropy to ensure no new calibration issues are introduced