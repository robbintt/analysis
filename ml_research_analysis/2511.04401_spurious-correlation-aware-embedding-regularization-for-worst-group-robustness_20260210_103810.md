---
ver: rpa2
title: Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness
arxiv_id: '2511.04401'
source_url: https://arxiv.org/abs/2511.04401
tags:
- spurious
- core
- scer
- worst-group
- spur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCER directly regularizes the embedding space to mitigate spurious
  correlations by decomposing worst-group error into spurious and core components,
  then enforcing constraints that reduce reliance on domain-specific artifacts while
  strengthening label-consistent features. This approach improves worst-group accuracy
  on multiple vision and language benchmarks, achieving 91.2% on Waterbirds, 91.4%
  on CelebA, and 73.6% on ColorMNIST.
---

# Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness

## Quick Facts
- arXiv ID: 2511.04401
- Source URL: https://arxiv.org/abs/2511.04401
- Authors: Subeen Park; Joowang Kim; Hakyung Lee; Sunjae Yoo; Kyungwoo Song
- Reference count: 37
- One-line primary result: Achieves 91.2% worst-group accuracy on Waterbirds, 91.4% on CelebA, and 73.6% on ColorMNIST, outperforming existing methods even under extreme imbalance.

## Executive Summary
SCER directly regularizes the embedding space to mitigate spurious correlations by decomposing worst-group error into spurious and core components, then enforcing constraints that reduce reliance on domain-specific artifacts while strengthening label-consistent features. This approach improves worst-group accuracy on multiple vision and language benchmarks, achieving 91.2% on Waterbirds, 91.4% on CelebA, and 73.6% on ColorMNIST. SCER outperforms existing methods even under extreme imbalance, such as when a subpopulation is completely absent from training, and integrates seamlessly with environment-inference frameworks.

## Method Summary
SCER combines GroupDRO's worst-group classification loss with embedding regularization that directly addresses spurious correlations. The method computes group-wise mean embeddings, derives spurious directions (domain differences within the same class) and core directions (class differences within the same domain), then explicitly penalizes classifier weight alignment with spurious directions while rewarding core alignment. This embedding-level regularization reduces reliance on spurious features more effectively than indirect methods like sample reweighting.

## Key Results
- Achieves 91.2% worst-group accuracy on Waterbirds, 91.4% on CelebA, and 73.6% on ColorMNIST
- Outperforms existing methods even when subpopulations are completely absent from training
- Integrates seamlessly with environment-inference frameworks for cases without explicit group labels
- Provides 72.8% worst-group accuracy on ColorMNIST with ρ=95% spurious correlation using Σ-norm alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct embedding-level regularization reduces reliance on spurious features more effectively than indirect methods like sample reweighting.
- Mechanism: SCER computes group-wise mean embeddings, derives spurious directions (domain differences within the same class) and core directions (class differences within the same domain), then explicitly penalizes classifier weight alignment with spurious directions while rewarding core alignment.
- Core assumption: Spurious and core features occupy separable geometric directions in embedding space that can be identified through group-wise mean differences.
- Evidence anchors: [abstract] "decomposes worst-group error into spurious and core components, then imposes explicit constraints at the embedding level"; [section 3.2] Formally defines ∆spur = E[µ(y,di) - µ(y,dj)] and ∆core = E[µ(yi,d) - µ(yj,d)]

### Mechanism 2
- Claim: Theoretical decomposition of worst-group error into spurious and core terms enables principled loss function design.
- Mechanism: Under Gaussian assumptions, Theorem 1 shows E_wge = Φ(±½cor(β*,∆_spur)∥∆_spur∥_Σ - cor(β*,∆_core)∥∆_core∥_Σ). This motivates L_spur to penalize spurious alignment and L_core to encourage core alignment.
- Core assumption: Data follows group-conditional Gaussian distribution with shared covariance Σ; core and spurious directions are constant across groups.
- Evidence anchors: [abstract] "We show theoretically that worst-group error is influenced by how strongly the classifier relies on spurious versus core directions"; [section 3.3, Theorem 1] Formal decomposition with proof

### Mechanism 3
- Claim: Covariance-normalized (Σ-norm) alignment measurement captures embedding geometry better than Euclidean norm.
- Mechanism: The Σ-norm ∥v∥_Σ = √(v^T Σ v) accounts for feature correlations and scale, making directional alignment measurements geometrically meaningful.
- Core assumption: Empirical covariance Σ of embeddings accurately captures the geometric structure of the representation space.
- Evidence anchors: [section 3.2, Table 7] Σ-norm achieves 72.8% vs Euclidean 70.0% worst-group accuracy on ColorMNIST (ρ=95%)

## Foundational Learning

- Concept: **Group Distributionally Robust Optimization (GroupDRO)**
  - Why needed here: SCER builds on GroupDRO's worst-group classification loss L_wge as one component of the total loss.
  - Quick check question: Can you explain how GroupDRO dynamically upweights groups with higher loss during training?

- Concept: **Spurious Correlations and Subpopulation Shift**
  - Why needed here: Core problem formulation; understanding how models exploit dataset biases (e.g., Waterbirds: background correlates with bird type) is essential.
  - Quick check question: For a Waterbirds classifier, what is the spurious feature and which subpopulation is typically worst-group?

- Concept: **Mahalanobis Distance (Σ-norm)**
  - Why needed here: SCER uses Σ-norm throughout for alignment computation; understanding covariance-normalized distances is critical.
  - Quick check question: Why does Mahalanobis distance account for feature correlations while Euclidean distance does not?

## Architecture Onboarding

- Component map: Feature extractor f_w: X → R^p -> Classifier f_β: R^p → Y -> Group-wise mean computation -> Direction derivation -> Covariance estimation -> Alignment computation -> Loss combination

- Critical path: 1. Forward pass → embeddings x_emb 2. Compute group means and batch covariance Σ 3. Derive ∆_spur and ∆_core from means 4. Compute correlations with classifier weights 5. Combine worst-group classification loss + embedding regularization 6. Backpropagate and update (w, β)

- Design tradeoffs:
  - **λ_spur vs λ_core**: Joint optimization outperforms individual terms; balance suppression vs preservation
  - **Batch size**: Larger batches improve Σ estimation but increase memory
  - **Group label dependency**: Requires (y, d) labels; EIIL integration helps when unavailable
  - **Compute overhead**: ~50% slower than ERM (0.0988s vs 0.0654s/iteration)

- Failure signatures:
  1. Missing subpopulations: Accuracy drops to 59.6% when one group absent, though still better than baselines
  2. Poor covariance estimation: Small batches cause unstable Σ-norm
  3. Direction inseparability: When ∆_spur and ∆_core are aligned, regularization conflicts

- First 3 experiments:
  1. **Reproduce ColorMNIST (ρ=95%)**: Compare ERM, GroupDRO, SCER; target SCER ≥72.8% worst-group accuracy
  2. **Ablation study**: Test L_spur only, L_core only, both combined; verify complementary contribution
  3. **Sensitivity analysis**: Vary λ_spur, λ_core on CelebA; replicate Table 11 to confirm hyperparameter stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCER maintain theoretical guarantees and empirical effectiveness when the group-conditional Gaussian distribution assumption is violated in real-world data?
- Basis in paper: [explicit] The theoretical analysis in Appendix A.2 states: "We assume that the data follows a group-conditional Gaussian distribution x|(y, d)∼ N(µ(y,d),Σ) where Σ is positive definite." The paper acknowledges this is a simplifying assumption.
- Why unresolved: Real-world data distributions may not follow Gaussian assumptions, and the impact of this mismatch on the worst-group error decomposition bounds remains uncharacterized.
- What evidence would resolve it: Empirical evaluation on datasets with known non-Gaussian distributions, or theoretical extensions relaxing distributional assumptions.

### Open Question 2
- Question: How can the performance gap between SCER with explicit group labels versus inferred environment labels be reduced or eliminated?
- Basis in paper: [explicit] Table 4 shows EIIL + SCER achieves 72.6% accuracy without explicit labels, but this represents a gap compared to the fully supervised setting. The paper notes EIIL achieves "over 95% agreement with actual spurious groups," yet performance still degrades.
- Why unresolved: Environment inference errors propagate through SCER's regularization terms, but the sensitivity of SCER to environment label noise is not characterized.
- What evidence would resolve it: Systematic analysis of SCER robustness to varying levels of environment label noise, or development of noise-aware regularization variants.

### Open Question 3
- Question: Is there a principled, automatic method for selecting the regularization hyperparameters λ_spur and λ_core without requiring validation set tuning?
- Basis in paper: [inferred] The sensitivity analysis in Tables 6 and 11 demonstrates performance varies with hyperparameter choices, but selection requires manual tuning. The paper provides no theoretical guidance on optimal hyperparameter values relative to dataset properties like spurious correlation strength.
- Why unresolved: Current practice relies on validation-based model selection, which may be unavailable when minority groups are scarce or absent.
- What evidence would resolve it: Development of adaptive schemes that set λ values based on observable properties of the embedding space.

## Limitations
- The approach's effectiveness depends on the assumption that spurious and core directions are separable in embedding space, which may not hold in all scenarios.
- SCER's performance degrades when subpopulations are missing from training, though still outperforms baselines in such cases.
- The method requires explicit group labels for computing group-wise means, limiting applicability in unsupervised settings without environment inference integration.

## Confidence
- **High Confidence**: Empirical results demonstrating superior worst-group accuracy across multiple benchmarks (Waterbirds, CelebA, ColorMNIST). The implementation of the loss function and its integration with existing frameworks is clearly specified.
- **Medium Confidence**: Theoretical decomposition of worst-group error into spurious and core components. The geometric intuition behind Σ-norm alignment is reasonable but lacks extensive empirical validation.
- **Low Confidence**: Assumptions about data distribution (Gaussian) and the universal applicability of the direction separability assumption across diverse datasets.

## Next Checks
1. **Robustness to Non-Gaussian Data**: Evaluate SCER on datasets known to deviate from Gaussian assumptions (e.g., heavy-tailed distributions) to test the validity of the theoretical framework.
2. **Direction Separability Analysis**: Conduct experiments to measure the correlation between spurious and core directions across different datasets and embedding spaces to assess the assumption's validity.
3. **Group Label Sensitivity**: Test SCER's performance with varying levels of group label noise and missing group information to understand its practical limitations in real-world scenarios.