---
ver: rpa2
title: An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop
  Question Answering
arxiv_id: '2503.23415'
source_url: https://arxiv.org/abs/2503.23415
tags:
- answer
- decoding
- react
- question
- faithful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how decoding methods can enhance the faithfulness
  of large language model (LLM) generations in multi-hop question answering. Retrieval-augmented
  generation and agentic frameworks like ReAct provide access to external knowledge,
  but LLMs often produce hallucinations by not remaining faithful to retrieved context.
---

# An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2503.23415
- Source URL: https://arxiv.org/abs/2503.23415
- Authors: Alexander Murphy; Mohd Sanad Zaki Rizvi; Aden Haussmann; Ping Nie; Guifu Liu; Aryo Pradipta Gema; Pasquale Minervini
- Reference count: 18
- Primary result: Faithful decoding methods combined with ReAct framework improve multi-hop QA accuracy by 50%+ on HotpotQA

## Executive Summary
This paper investigates how decoding methods can enhance the faithfulness of LLM generations in multi-hop question answering tasks. Retrieval-augmented generation and agentic frameworks like ReAct provide access to external knowledge, but LLMs often produce hallucinations by not remaining faithful to retrieved context. The study systematically evaluates three training-free decoding strategies—Context-Aware Decoding (CAD), Decoding by Contrasting Layers (DoLa), and Decoding by Contrasting Retrieval Heads (DeCoRe)—when combined with the ReAct framework.

Experiments on three multi-hop QA datasets (HotpotQA, 2WikiMultihopQA, and MuSiQue) with Qwen2-7b-Instruct and Llama3-8b-Instruct show consistent improvements. Using ReAct with DoLa increases HotpotQA answer F1 from 19.5 to 32.6 (over 50% relative improvement). Format adherence also improves significantly, with CAD increasing correct ReAct trace formatting on HotpotQA from 47.9% to 74.5% for Qwen. Retrieval metrics show similar gains, with DeCoRe improving HotpotQA support recall from 44.3% to 46.6% for Qwen.

## Method Summary
The study combines ReAct, an agentic framework for multi-hop QA, with three training-free faithful decoding methods: CAD, DoLa, and DeCoRe. ReAct iteratively searches and reasons over retrieved documents using a thought-action-observation loop. The decoding methods modify token sampling to increase faithfulness to retrieved context—CAD by contrasting context-conditioned and unconditioned probabilities, DoLa by contrasting logits from different transformer layers, and DeCoRe by contrasting outputs from a base LLM versus one with masked retrieval heads. The system is evaluated on three multi-hop QA benchmarks using Qwen2-7b-Instruct and Llama3-8b-Instruct models.

## Key Results
- ReAct with DoLa increases HotpotQA answer F1 from 19.5 to 32.6 (over 50% relative improvement)
- CAD increases correct ReAct trace formatting on HotpotQA from 47.9% to 74.5% for Qwen
- DeCoRe improves HotpotQA support recall from 44.3% to 46.6% for Qwen
- Faithful decoding consistently improves both answer accuracy and reasoning format adherence across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Faithful decoding methods can improve multi-hop QA accuracy by increasing the model's faithfulness to retrieved context, reducing "faithfulness hallucinations" where parametric knowledge overrides provided evidence.
- **Mechanism:** Methods like CAD compute token probabilities both with and without context, amplifying tokens favored by the context-conditioned model and suppressing those favored by the unconditioned model. This biases generation toward information present in retrieved passages.
- **Core assumption:** Retrieved context contains relevant, accurate information that the model would otherwise ignore or contradict using parametric memory.
- **Evidence anchors:**
  - [abstract] "LLMs often fail to remain faithful to retrieved information... combining an agentic framework... with decoding methods that enhance faithfulness can increase accuracy."
  - [section 2.2] "CAD... increases the likelihood of the tokens selected by the model with c while decreasing the likelihood of the tokens selected by the model without c, under the assumption that the former will tend to be more faithful to the context c."
  - [corpus] Related work "Improving Contextual Faithfulness of LLMs via Retrieval Heads-Induced Optimization" identifies correlation between faithfulness and specific attention heads, supporting the premise that faithfulness is addressable via architectural interventions.
- **Break condition:** When retrieved context is irrelevant, incorrect, or incomplete, enforcing faithfulness can degrade performance (as observed with OneR across models).

### Mechanism 2
- **Claim:** Contrastive decoding over internal model components (layers or attention heads) can improve reasoning quality by leveraging where factual or contextual processing is localized.
- **Mechanism:** DoLa contrasts logits from later vs. earlier transformer layers, under the hypothesis that later layers encode more factual, reasoned outputs. DeCoRe contrasts outputs from a base LLM vs. one with masked "retrieval heads"—attention heads implicated in extracting contextual information.
- **Core assumption:** Factual knowledge and contextual integration are functionally localized within specific layers or heads, and contrasting outputs can isolate these signals.
- **Evidence anchors:**
  - [section 2.2] "DoLa... derives its next-token distributions by contrasting the logits obtained from early and later exiting layers, assuming that later layers will tend to generate more factually accurate predictions."
  - [section 2.2] "DeCoRe... dynamically contrasts outputs from a base LLM and an LLM with masked retrieval heads."
  - [corpus] Corpus includes "Retrieval head mechanistically explains long-context factuality" (Wu et al., 2025), which is cited as the basis for DeCoRe, providing external grounding for the retrieval-head concept.
- **Break condition:** If layer or head localization is inconsistent across model architectures or tasks, the contrastive signal may not align with the intended improvement.

### Mechanism 3
- **Claim:** Faithful decoding can improve prompt format adherence in agentic frameworks, which in turn supports more complete and correct reasoning traces.
- **Mechanism:** By increasing the salience of the provided prompt (which includes format instructions), faithful decoders make the model more likely to follow the specified Thought-Action-Observation structure, reducing early termination or malformed outputs.
- **Core assumption:** The prompt's format instructions are present in the context window, and the model has sufficient capacity to follow them when appropriately biased.
- **Evidence anchors:**
  - [section 4.1] "format adherence is either improved or matched when using faithful decoding. For Qwen... CAD increases the number of questions adhering to the format the most... from 47.9% to 74.5%."
  - [section 4.1] "faithful decoders help generate outputs in the correct ReAct format more consistently."
  - [corpus] "CoCoA: Confidence and Context-Aware Adaptive Decoding" (related corpus paper) addresses knowledge conflicts adaptively, suggesting format adherence may also benefit from adaptive context weighting, though this paper does not directly test that.
- **Break condition:** If the prompt is poorly designed or the task is too complex for the model's instruction-following capacity, format improvements alone may not yield better final answers.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The entire study builds on augmenting LLMs with external knowledge. Without understanding RAG basics, the role of ReAct as an iterative, agentic extension of "one-step" RAG is unclear.
  - **Quick check question:** Can you explain the difference between one-step RAG (retrieve-then-read) and the ReAct framework for multi-hop QA?

- **Concept: Faithfulness vs. Factuality in LLMs**
  - **Why needed here:** The paper distinguishes "faithfulness hallucinations" (ignoring context) from general hallucinations (generating incorrect info). Grasping this distinction is essential to understand why faithful decoding is evaluated separately from raw factual accuracy.
  - **Quick check question:** If a model is given a passage saying "Paris is the capital of France" but answers "Lyon," is this a factuality hallucination, a faithfulness hallucination, or both? Why?

- **Concept: Multi-Hop Question Answering**
  - **Why needed here:** The benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue) require reasoning across multiple documents. Understanding why single-hop retrieval fails here clarifies the need for iterative agents like ReAct.
  - **Quick check question:** For the question "Who is the father of the author of *1984*?", what are the two "hops" required to answer it?

## Architecture Onboarding

- **Component map:** ReAct Loop (Prompt -> Thought/Action -> Tool Execution -> Observation -> Repeat) -> Retriever (BM25 search) -> LLM with Faithful Decoding (CAD/DoLa/DeCoRe) -> Evaluator (Answer F1, Support Recall, Format Adherence)

- **Critical path:**
  1. Implement the ReAct prompt template and tool interface (Search/Lookup) for your target KB.
  2. Integrate a standard LLM decoder and validate the agent produces well-formed traces.
  3. Implement at least one faithful decoder (e.g., CAD is conceptually simplest).
  4. Run on a small subset of a multi-hop QA benchmark to compare standard vs. faithful decoding.

- **Design tradeoffs:**
  - **CAD:** Requires context to be explicitly provided; simple to implement; may degrade performance if context is noisy.
  - **DoLa:** Layer-contrast requires access to internal logits; no explicit context dependency; tuning which layers to contrast may be needed.
  - **DeCoRe:** Requires identification and masking of "retrieval heads"; most complex but potentially most targeted.
  - **General:** All faithful decoders add computational overhead; improvements are model- and dataset-dependent (no single method dominates).

- **Failure signatures:**
  - **Low Answer Support Recall:** Faithful decoding cannot help if the retriever fails to fetch relevant documents.
  - **Format collapse:** Agent exits early or produces malformed traces; check if faithful decoding improves format adherence (see Section 4.1).
  - **Context-Parametric Conflict:** Model ignores retrieved evidence despite faithful decoding; may indicate a need for stronger contrastive weighting or better context quality.

- **First 3 experiments:**
  1. **Baseline check:** Run ReAct with standard decoding on 50–100 examples from HotpotQA. Measure Answer F1 and format adherence rate.
  2. **Decoder comparison:** Re-run with CAD, DoLa, and DeCoRe on the same subset. Compare Answer F1 and identify which decoder yields the largest gain.
  3. **Ablation on retrieval quality:** Substitute a weaker retriever (e.g., random or low-quality passages) and observe whether faithful decoding still helps or harms performance, testing the "break condition" from Mechanism 1.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of faithful decoding strategies is model-dependent, with DoLa improving Qwen but worsening Llama performance on the same dataset
- The study only evaluates two model families (Qwen and Llama), limiting generalizability to other LLM architectures
- Results are based on Wikipedia-derived datasets, raising questions about performance in other domains with different retrieval characteristics

## Confidence
**High Confidence:**
- The core finding that combining ReAct with faithful decoding improves both answer accuracy and format adherence is well-supported by consistent improvements across multiple datasets and models
- The mechanism that CAD increases token likelihood for context-conditioned outputs while suppressing unconditioned outputs is clearly demonstrated through experimental results

**Medium Confidence:**
- The claim that contrastive decoding over layers (DoLa) or retrieval heads (DeCoRe) improves reasoning quality is supported by results but shows model-dependent variability
- The observation that faithful decoding improves format adherence in ReAct traces is well-demonstrated but may be partially attributable to increased context salience

**Low Confidence:**
- The generalization of results to other multi-hop QA domains beyond Wikipedia-based datasets is not established
- The claim about computational efficiency or scalability of faithful decoding methods is not addressed in the study

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate the same decoding methods on multi-hop QA datasets from different domains (e.g., scientific papers, medical literature, or legal documents) to assess whether the improvements observed in Wikipedia-based datasets transfer to other knowledge domains with different retrieval characteristics and complexity patterns.

2. **Retrieval Quality Robustness Analysis:** Systematically degrade retrieval quality (using random passages, irrelevant documents, or low-quality sources) and measure how each faithful decoding method performs. This would establish the break conditions where faithful decoding transitions from helpful to harmful, providing clearer guidelines for when to apply these techniques.

3. **Computational Overhead and Scalability Assessment:** Measure and compare the computational cost (inference time, memory usage) of each decoding method relative to standard decoding. Conduct scaling experiments to determine at what model size or batch processing level the overhead becomes prohibitive, establishing practical deployment thresholds for each approach.