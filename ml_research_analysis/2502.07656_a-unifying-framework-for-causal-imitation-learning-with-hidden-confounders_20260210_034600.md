---
ver: rpa2
title: A Unifying Framework for Causal Imitation Learning with Hidden Confounders
arxiv_id: '2502.07656'
source_url: https://arxiv.org/abs/2502.07656
tags:
- expert
- learning
- which
- causal
- confounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a general framework for causal imitation learning
  with hidden confounders that unifies and generalizes several existing settings.
  The key idea is to distinguish between expert-observable confounders (known to the
  expert but not the imitator) and expert-unobservable confounders (hidden from both).
---

# A Unifying Framework for Causal Imitation Learning with Hidden Confounders

## Quick Facts
- **arXiv ID:** 2502.07656
- **Source URL:** https://arxiv.org/abs/2502.07656
- **Reference count:** 36
- **Primary result:** A causal imitation learning framework using instrumental variables and double machine learning that unifies and generalizes existing methods for settings with hidden confounders

## Executive Summary
This paper proposes a general framework for causal imitation learning that handles both expert-observable and expert-unobservable confounders. The key innovation is reformulating imitation learning as a conditional moment restriction problem using trajectory histories as instrumental variables. The authors develop DML-IL, an algorithm based on double machine learning that provably bounds the imitation gap while recovering prior results as special cases. Empirical results on both synthetic and Mujoco tasks demonstrate superior performance compared to state-of-the-art causal imitation learning baselines, particularly when both types of confounders are present.

## Method Summary
The method solves imitation learning with hidden confounders by treating trajectory history as an instrumental variable. It uses a two-stage approach: first fitting a roll-out model to predict future trajectories, then training a policy network using these predictions. Double machine learning with K-fold cross-fitting prevents regularization bias. The algorithm targets the conditional moment restriction E[a_t - π^h(h_t) | h_{t-k}] = 0, where h_{t-k} serves as an instrument to break spurious correlations caused by confounding noise.

## Key Results
- DML-IL outperforms state-of-the-art causal imitation learning baselines, especially when both expert-observable and expert-unobservable confounders are present
- The algorithm's imitation gap is theoretically bounded and shown to recover prior results as special cases
- Empirical evaluation demonstrates significant performance improvements on both toy environments and Mujoco tasks
- The framework successfully handles scenarios where traditional behavioral cloning fails due to confounding bias

## Why This Works (Mechanism)

### Mechanism 1: History as an Instrumental Variable (IV)
Using past trajectory history as an instrumental variable isolates the causal effect of states on actions from spurious correlations induced by confounding noise. The algorithm utilizes the "confounding noise horizon" k to select a history segment h_{t-k}. By definition, the noise u^ε_t at time t is independent of the history at t-k (u^ε_t ⊥ h_{t-k}). This independence allows h_{t-k} to act as a valid IV, satisfying the moment condition E[a_t - π_h(h_t) | h_{t-k}] = 0, thereby filtering out the bias from u^ε_t that plagues standard Behavioral Cloning.

### Mechanism 2: Implicit Confounder Inference via History Dependence
A history-dependent policy π_h can recover performance loss caused by "expert-observable" confounders (u^o_t) that the imitator cannot see directly. While the imitator lacks access to u^o_t (e.g., hidden seasonal demand), the trajectory history h_t encodes latent traces of the expert's past decisions, which were conditioned on u^o_t. By learning π(h_t), the policy implicitly infers the conditional expectation E[u^o_t|h_t], effectively "guessing" the missing expert context from temporal patterns.

### Mechanism 3: Debiasing via Double Machine Learning (DML)
The DML-IL architecture prevents the "regularization bias" often found in standard two-stage IV regression, ensuring the imitation gap shrinks efficiently with sample size. The algorithm splits the problem into two stages: (1) learning a roll-out model M̂ (the nuisance function) and (2) learning the policy π_h. By employing K-fold cross-fitting (training the policy on data folds not used to train M̂), DML-IL ensures the stochastic errors of the first stage do not contaminate the second stage, satisfying the "Neyman orthogonality" condition required for fast convergence rates (O(N^{-1/2})).

## Foundational Learning

**Concept:** Instrumental Variables (IV) & Conditional Moment Restrictions (CMR)
- **Why needed here:** This is the mathematical bedrock the paper uses to transform a biased "causal confusion" problem into a solvable optimization target. Without understanding IVs, the logic of using "past history" to fix "current noise" is opaque.
- **Quick check question:** Can you explain why standard regression fails if the input variable (state) is correlated with the error term (noise), and how an IV breaks this loop?

**Concept:** Double Machine Learning (DML)
- **Why needed here:** The paper selects DML over simpler IV solvers (like DeepIV or DeepGMM) specifically to handle the bias introduced by overfitting in high-dimensional trajectory spaces.
- **Quick check question:** Why does training the policy and the nuisance model on the same data batch introduce bias in causal inference?

**Concept:** Partially Observable Markov Decision Processes (POMDPs)
- **Why needed here:** The framework models the "hidden confounders" effectively as unobserved states. Understanding POMDPs helps contextualize why the history h_t is necessary to infer the belief state.
- **Quick check question:** In a POMDP, why is the current observation s_t insufficient for optimal decision-making?

## Architecture Onboarding

**Component map:**
Data Buffer -> Stage 1 (Nuisance/Roll-out Model M̂) -> Stage 2 (Policy Network π_h) -> Cross-Fitting Orchestrator

**Critical path:** Correctly setting the confounding horizon k is the most sensitive hyperparameter. If k is too small, the instrument is invalid (correlated with noise). If k is too large, the instrument is weak (provides no signal).

**Design tradeoffs:**
- **Horizon k:** Theoretical validity vs. Signal strength. Larger k ensures validity but degrades performance (Proposition 4.3).
- **Rollout Model Complexity:** Must be complex enough to capture dynamics (o(N^{-1/4}) rate) but not so complex that it overfits small datasets.

**Failure signatures:**
- exploding MSE: If Stage 1 (rollout) drifts, Stage 2 trains on garbage data
- Random policy behavior: Often indicates the instrument h_{t-k} is too weak (k is too large) or the expert confounders are effectively random
- Biased convergence: The algorithm converges to a suboptimal policy if cross-fitting is skipped (standard ML bias)

**First 3 experiments:**
1. **Sanity Check (k=1):** Run DML-IL on a synthetic environment with known horizon k=1. Verify that MSE drops significantly compared to Behavioral Cloning.
2. **Ablation on Horizon:** Run the algorithm with k ∈ {1, 5, 10, 20}. Plot the "Ill-posedness" vs. reward to confirm the theoretical trade-off (Performance should drop as k increases).
3. **Cross-Fitting Ablation:** Compare DML-IL (with cross-fitting) vs. a "Naive" two-stage version (without cross-fitting). Measure the imitation gap to observe the regularization bias in the naive version.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the confounding noise horizon k be reliably identified using data-driven conditional independence tests rather than requiring prior domain knowledge?
- **Basis in paper:** [explicit] The authors state in the Future Works section that "there exist tests that can indirectly check whether a candidate IV is valid... It would be interesting future work to incorporate these methods to help identify k."
- **Why unresolved:** The algorithm currently requires k as an input; while Appendix C.2 evaluates performance under misspecification, it does not propose a method to automatically determine k from the dataset.
- **What evidence would resolve it:** A theoretical proof or empirical demonstration showing that a specific conditional independence test can consistently estimate k or detect valid instruments within the DML-IL framework.

**Open Question 2:** How can the causal identification framework be extended to handle non-additive (e.g., multiplicative or non-linear) confounding noise?
- **Basis in paper:** [explicit] The Limitations section notes that the framework relies on an additive noise assumption (Assumption 3.3), and the Future Works section lists "causal identification with non-additive noise" as an orthogonal research direction.
- **Why unresolved:** The derivation of the Conditional Moment Restriction (CMR) in Equation (4) explicitly breaks down if the noise is not additive, rendering the current DML-IL algorithm inapplicable.
- **What evidence would resolve it:** A modified structural equation model and corresponding algorithm that provides identification guarantees for history-dependent policies under non-additive noise structures.

**Open Question 3:** Can the framework be adapted to remain robust when the instrumental variable condition is violated, such as when the confounding horizon is misspecified?
- **Basis in paper:** [inferred] The paper notes in the Future Works that considering "invalid instruments" is a research direction. Furthermore, Appendix C.2 shows performance degrades when k is misspecified (violating IV validity), but no theoretical framework is provided for this bias.
- **Why unresolved:** The theoretical guarantees (Theorem 4.5) rely on the validity of the instrument h_{t-k}; if the horizon is chosen too small, the unconfoundedness condition h_{t-k} ⊥ u^ε_t fails, and the resulting bias is uncharacterized.
- **What evidence would resolve it:** Theoretical bounds on the imitation gap that explicitly account for the degree of instrument invalidity, or an algorithm that synthesizes valid instruments from a set of potentially invalid history steps.

## Limitations
- The confounding noise horizon k must be correctly specified; incorrect choice leads to either invalid instruments (too small) or weak instruments (too large)
- Expert-unobservable confounders u^ε_t are assumed to be independent across time, which may not hold in real-world environments with persistent confounding factors
- The assumption that expert-observable confounders u^o_t leave recoverable traces in trajectory history depends on specific environment structure and may not generalize

## Confidence

**High confidence:** The theoretical framework for unifying causal imitation learning with hidden confounders is well-founded. The instrumental variable approach using history as an IV is mathematically sound when assumptions hold.

**Medium confidence:** The empirical evaluation demonstrates advantages over baselines, but the sample size (40 trajectories × 500 steps) may be insufficient to draw strong conclusions about real-world applicability.

**Low confidence:** The mechanism for inferring expert-observable confounders through history dependence (Mechanism 2) lacks direct empirical validation and theoretical guarantees beyond the toy example.

## Next Checks

1. **Horizon sensitivity analysis:** Systematically vary the confounding horizon k on both toy and Mujoco environments to empirically validate the trade-off between instrument validity and signal strength described in Proposition 4.3.

2. **Cross-fitting ablation study:** Compare DML-IL with and without cross-fitting on high-dimensional tasks to quantify the regularization bias reduction claimed in Mechanism 3.

3. **Confounder recovery test:** In controlled environments where the true expert-observable confounder u^o_t is known, measure the algorithm's ability to recover and utilize this information through history-based inference.