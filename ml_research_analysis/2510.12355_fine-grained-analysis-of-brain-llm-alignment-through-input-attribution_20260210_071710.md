---
ver: rpa2
title: Fine-grained Analysis of Brain-LLM Alignment through Input Attribution
arxiv_id: '2510.12355'
source_url: https://arxiv.org/abs/2510.12355
tags:
- words
- attribution
- brain
- alignment
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fine-grained input attribution framework
  to analyze brain-language model alignment by identifying which words in the input
  most influence predictions of brain activity. The authors compare attributions from
  brain alignment (BA) and next-word prediction (NWP) tasks across five diverse LLMs.
---

# Fine-grained Analysis of Brain-LLM Alignment through Input Attribution

## Quick Facts
- arXiv ID: 2510.12355
- Source URL: https://arxiv.org/abs/2510.12355
- Reference count: 40
- This paper introduces a fine-grained input attribution framework to analyze brain-language model alignment by identifying which words in the input most influence predictions of brain activity.

## Executive Summary
This study introduces a fine-grained input attribution framework to analyze brain-language model alignment by identifying which input words most influence predictions of brain activity. The authors compare attributions from brain alignment (BA) and next-word prediction (NWP) tasks across five diverse LLMs. Results show that BA and NWP rely on largely distinct word subsets: NWP exhibits recency and primacy biases with syntactic focus, while BA prioritizes semantic and discourse-level information with broader recency effects. BA also shows higher attribution spread at middle and late layers compared to NWP. These findings reveal that brain alignment depends on deeper, more distributed linguistic representations than next-word prediction alone. The attribution method is broadly applicable for studying cognitive relevance in language processing.

## Method Summary
The framework computes input attributions for brain alignment and next-word prediction by training a ridge regression brain encoding model and using gradient × input (GXI) to attribute importance to input words. For BA, the model predicts brain activity from LLM embeddings; for NWP, it predicts the next word. The method handles the hemodynamic delay by concatenating four consecutive TR embeddings, decomposes the learned weights into per-TR linear projections, and computes gradients of the respective loss functions with respect to input tokens. Attribution scores are aggregated to the word level and analyzed for intersection, positional bias, and spread patterns.

## Key Results
- BA and NWP exhibit low word-level attribution overlap (IoU), indicating reliance on distinct linguistic features
- BA shows higher attribution spread at middle and late layers compared to NWP
- NWP displays bimodal primacy and recency biases, while BA exhibits monotonic broad recency effects
- Attribution spread patterns suggest BA integrates higher-order semantic and discourse structures

## Why This Works (Mechanism)

### Mechanism 1: Divergent Feature Reliance
Brain Alignment (BA) and Next-Word Prediction (NWP) depend on largely distinct subsets of linguistic features, with BA relying more heavily on distributed semantic information than surface-level syntax. Attribution spread analysis reveals that BA requires a broader set of unique words to reach cumulative importance thresholds at middle and late layers, whereas NWP achieves thresholds with fewer, localized words at early layers. This suggests BA integrates higher-order structures (semantics/discourse) that emerge deeper in the network, while NWP utilizes local collocational cues available early.

### Mechanism 2: Task-Specific Positional Bias
NWP exhibits a bimodal "primacy and recency" bias (focusing on sentence edges), while BA exhibits a monotonic, broad recency bias. Center of Mass (CoM) analysis and positional distribution plots show NWP attributions spiking at the start and end of contexts. In contrast, BA attributions decay smoothly from the most recent words, implying the brain prioritizes a continuous "working memory" of context over structural anchors.

### Mechanism 3: Gradient-Based Attribution as a Causal Proxy
Gradient × Input (GXI) serves as a valid proxy for identifying context words causally necessary for predicting brain activity. By decomposing the learned ridge regression weights into per-TR linear projections, the framework computes gradients of the MSE loss with respect to input tokens. Masking the top 1% of attributed words causes a near 100% drop in predictive power, validating that high attribution scores identify functionally critical inputs.

## Foundational Learning

- **Brain Encoding Models (Linear Probes)**: This paper defines alignment via the performance of a linear mapping (ridge regression) from LLM embeddings to fMRI voxels. You must understand that this measures the geometric similarity of the representation spaces, not the LLM's ability to "think" like a brain. Quick check: If the ridge regression achieves high correlation but uses features spread across all layers, does the LLM have a specific "brain region" equivalent?

- **Gradient × Input (GXI)**: The paper uses GXI as the primary attribution method. You need to distinguish this from standard backpropagation; it weights the gradient by the input magnitude (∇x · x), capturing both sensitivity and feature presence. Quick check: Why might GXI be preferred over Integrated Gradients (IG) for a large-scale study involving 5 models and thousands of contexts?

- **Hemodynamic Delay & Temporal Resolution**: The architecture bridges fast text processing (word-level) and slow brain recordings (TR-level). Understanding that the model concatenates 4 previous TR embeddings to account for the delay in blood-oxygen-level-dependent (BOLD) signals is essential for interpreting the "recency" results. Quick check: In the pipeline, why are token embeddings averaged to the word level before being averaged again to the TR level?

## Architecture Onboarding

- **Component map**: Frozen LLM Backbone -> Temporal Aggregator -> Projection Head -> Attribution Engine
- **Critical path**: The decomposition of the learned weight matrix (Shape: 4H, V) into four distinct linear layers (Shape: H, V) is the non-obvious step that enables the gradient to flow correctly from the concatenated delayed state back to individual TRs and ultimately the input words.
- **Design tradeoffs**:
  - GXI vs. IG: The paper uses GXI for speed but validates with IG. GXI is 10-20x faster but theoretically less grounded. Expect noise in single-word attributions; rely on aggregate trends (CoM, IoU).
  - Context Window (640 vs. 80): Longer contexts (640 words) highlight oscillatory artifacts in Llama3 but provide necessary history for semantic integration. Shorter contexts (80 words) are more robust but lose discourse-level signals.
- **Failure signatures**:
  - High IoU at Low Thresholds: If IoU is high (e.g., > 0.5) for top 10% words, it indicates the brain encoding model has failed to learn distinct semantic features and is merely tracking next-word surrogates.
  - Random Baseline Equivalence: If masking "important" words drops performance no more than masking random words, the attribution method is not capturing functional relevance (likely due to gradient saturation).
- **First 3 experiments**:
  1. Sanity Check (Masking): Select one model (e.g., Gemma-2B). Train the encoding model. Compute attributions. Mask the top 5% of words and verify that Pearson correlation drops significantly (>50%) compared to masking random words.
  2. Layer-wise IoU Analysis: Compute attribution for NWP and BA at early, middle, and late layers. Plot IoU. Verify the "U-shape" or divergence pattern to confirm BA relies more on late layers.
  3. Positional Bias Visualization: Pick a context window (e.g., 640 words). Plot the proportion of top-attributed words by distance from the most recent word. Confirm NWP shows edge peaks while BA shows a broad recency curve.

## Open Questions the Paper Calls Out

### Open Question 1
How do BA attribution patterns evolve during LLM training, and at what point do BA and NWP word importance patterns diverge? The current study uses only frozen, pretrained models. AlKhamissi et al. showed BA-NWP correlation decouples during training, but the attribution framework has not been applied longitudinally to track how word importance for each task shifts across training checkpoints.

### Open Question 2
Do brain-tuned LLMs (models fine-tuned specifically for brain alignment) show attribution patterns that differ systematically from standard pretrained models? All experiments use frozen pretrained models without BA optimization. It is unknown whether fine-tuning for brain alignment would strengthen the semantic/discourse emphasis already observed, shift attribution spread patterns, or alter positional biases.

### Open Question 3
Can attribution patterns be linked to behavioral measures of comprehension or memory, providing cognitive validation of brain-relevant representations? The current framework validates attributions through masking experiments showing predictive importance, but does not establish whether high-attribution words correspond to information humans actually remember or comprehend better.

### Open Question 4
What specific stimulus properties trigger the oscillatory attribution pattern observed in Llama3.2-1B, and does this pattern reflect a meaningful processing strategy? The authors note Llama3.2-1B shows a unique oscillatory pattern on the HP dataset that disappears with shorter contexts and on MRH, concluding it "may be stimulus- and context-dependent, rather than a direct consequence of Llama3.2-1B's architecture." The precise stimulus features driving this remain unidentified.

## Limitations
- The attribution method identifies important words but cannot directly measure whether these represent syntax, semantics, or discourse features
- The framework assumes that averaging token embeddings to word level and then to TR level preserves relevant temporal dynamics
- The study does not establish whether attribution patterns generalize across individuals or reading materials

## Confidence
- **High confidence**: The differential attribution spread between BA and NWP at different layers (BA requires broader semantic context) is robustly supported by the IoU and spread metrics across multiple models and thresholds
- **Medium confidence**: The positional bias patterns (NWP bimodal vs BA monotonic recency) are well-supported but may be influenced by dataset-specific reading patterns and context window lengths
- **Low confidence**: The claim that BA integrates "higher-order linguistic structures" is inferential - the attribution method identifies important words but cannot directly measure whether these represent syntax, semantics, or discourse features

## Next Checks
1. **Ablation of temporal aggregation**: Recompute attributions using token-level predictions (with appropriate hemodynamic modeling) rather than aggregated word/TR levels to test whether the coarse discretization affects the semantic vs syntactic distinction
2. **Cross-dataset replication**: Apply the attribution framework to naturalistic language datasets with different structures (dialog, expository text) to verify that the BA/NWP attribution divergence holds across reading contexts
3. **Controlled semantic manipulation**: Systematically insert or remove specific semantic content words in controlled contexts and measure how this affects brain alignment predictions compared to NWP performance, directly testing the claimed semantic reliance of BA