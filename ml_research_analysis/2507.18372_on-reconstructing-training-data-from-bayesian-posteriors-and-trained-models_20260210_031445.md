---
ver: rpa2
title: On Reconstructing Training Data From Bayesian Posteriors and Trained Models
arxiv_id: '2507.18372'
source_url: https://arxiv.org/abs/2507.18372
tags:
- data
- training
- reconstruction
- bayesian
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a general statistical framework for analyzing
  training data reconstruction (TDR) attacks. It formulates the TDR problem as approximating
  the empirical distribution of training data using weighted pseudo-data, and shows
  this is equivalent to minimizing maximum mean discrepancy (MMD) with a kernel determined
  by model gradients.
---

# On Reconstructing Training Data From Bayesian Posteriors and Trained Models

## Quick Facts
- arXiv ID: 2507.18372
- Source URL: https://arxiv.org/abs/2507.18372
- Reference count: 20
- One-line primary result: A general statistical framework unifies Bayesian and non-Bayesian training data reconstruction attacks using MMD-based analysis

## Executive Summary
This paper develops a unified statistical framework for analyzing training data reconstruction (TDR) attacks on both Bayesian and non-Bayesian models. The key insight is that TDR can be formulated as minimizing Fisher divergence between the true posterior and a weighted pseudo-posterior, which is equivalent to minimizing Maximum Mean Discrepancy (MMD) with a kernel determined by model gradients. The framework reveals that model complexity creates a fundamental privacy-performance tradeoff: more complex models extract more features from training data, improving performance but increasing vulnerability to reconstruction attacks. Numerical experiments demonstrate that while exact reconstruction is impossible, sufficient statistics like data count, means, and variances can be accurately recovered.

## Method Summary
The paper formulates TDR as approximating the empirical distribution of training data using weighted pseudo-data, which is equivalent to minimizing MMD with a kernel determined by model gradients. For Bayesian models, the attack minimizes Fisher divergence between the true posterior and a weighted pseudo-posterior using an integration-by-parts trick that makes the objective tractable without knowing the training data. The framework generalizes to non-Bayesian models and introduces a sliced Fisher divergence variant for computational efficiency. The approach reveals which features of training data can be recovered based on model complexity, showing that more complex models enable more complete reconstruction by extracting richer features from the data.

## Key Results
- Training data reconstruction attacks can be analyzed using a unified MMD-based framework for both Bayesian and non-Bayesian models
- Model complexity determines which training data features can be recovered, creating a fundamental privacy-performance tradeoff
- The first score-matching method for Bayesian models generalizes existing TDR methods for non-Bayesian models
- Numerical experiments on Bayesian linear regression show accurate recovery of sufficient statistics (data count, means, variances) but not exact reconstruction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data reconstruction attacks can be formulated as minimizing Fisher divergence between the true posterior and a weighted pseudo-posterior, which is tractable without knowing the training data.
- Mechanism: An integration-by-parts trick removes the dependence on ∇log π_X (which requires unknown data X), yielding FD(π_X, π_w,Z) = E_π_X[Tr(∇²_θ log π_w,Z)] + (1/2)E_π_X[||∇_θ log π_w,Z||²] + C. The adversary optimizes weights w and pseudo-data Z against this objective using only posterior samples and gradients of the pseudo-likelihood.
- Core assumption: The adversary has access to T samples from the posterior π_X (Assumption 3.2) and can query the likelihood function and its gradients.
- Evidence anchors:
  - [Section 3.1]: "The advantage of the Fisher divergence is that the dependence on ∇ log π_X... can be removed by an integration-by-parts trick"
  - [Definition 3.3]: Formalizes the reconstruction problem as arg min_{w,Z} FD(π_X, π_w,Z)
  - [corpus]: No direct corpus validation; related work on Bayesian inference methods exists but doesn't address this specific TDR mechanism.

### Mechanism 2
- Claim: Minimizing Fisher divergence is equivalent to minimizing Maximum Mean Discrepancy (MMD) with a kernel determined by model gradients, revealing which data features are recoverable.
- Mechanism: Theorem 3.4 proves FD(π_X, π_w,Z) = (1/2)MMD_k(P_X, P_w,Z)² where k(x,x') = ∫⟨∇_θ log l(θ,x), ∇_θ log l(θ,x')⟩dπ_X(θ). The feature map φ(x) = ∇_θ log l(·, x) determines extractable features—more expressive models have richer feature maps, enabling more complete reconstruction.
- Core assumption: The kernel's expressiveness directly maps to recoverable features; characteristic kernels would enable full reconstruction.
- Evidence anchors:
  - [Theorem 3.4]: Establishes the MMD equivalence and kernel definition
  - [Example 3.2]: Demonstrates that for Gaussian mean location, only N and Σx_n are recoverable (matching the feature map)
  - [corpus]: Weak corpus support; MMD is established in ML literature but this specific application to TDR characterization is novel.

### Mechanism 3
- Claim: Model complexity creates a privacy-performance tradeoff: more complex models enable better performance but increase vulnerability by extracting more features from training data.
- Mechanism: The feature map φ(x) = ∇_θ log l(·, x) depends on model structure. Deeper/wider neural networks produce more expressive gradients, yielding more discerning kernels in Theorem 4.3. This explains why Buzaglo et al. (2023) observed better reconstruction with more neurons per layer.
- Core assumption: Model architecture directly determines the richness of ∇_θ l(θ*, ·), which determines kernel expressiveness.
- Evidence anchors:
  - [Section 3.2]: "Theorem 3.4 shows that if a training data reconstruction attack aims to reduce the Fisher divergence... then this is equivalent to minimising the MMD... The gradient of the log-likelihood function completely determines the features that can be reconstructed"
  - [Section 4.2]: Cites Buzaglo et al. (2023, Figure 7) showing increased neurons improve reconstruction quality
  - [corpus]: Indirect support from literature on model expressiveness, but no corpus papers directly validate this TDR-specific tradeoff.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is the core metric relating reconstruction success to model properties. Understanding that MMD compares expectations of kernel feature maps (Eq. 2) is essential for interpreting what features can be recovered.
  - Quick check question: Given φ(x) = (1, x, x²), what moments of two distributions would MMD compare?

- Concept: Score Matching / Fisher Divergence
  - Why needed here: The attack objective is formulated via Fisher divergence. Understanding the integration-by-parts trick that makes score matching tractable (Eq. 4) is crucial for implementation.
  - Quick check question: Why does score matching avoid computing the normalizing constant that makes direct likelihood optimization difficult?

- Concept: Sufficient Statistics
  - Why needed here: The paper frames recoverable information in terms of sufficient statistics—if a model's sufficient statistic doesn't capture certain data features, those features cannot be reconstructed (Example 3.1, Lemma 5.1).
  - Quick check question: In linear regression with design matrix X and targets y, what are the sufficient statistics, and what does this imply about recoverable information?

## Architecture Onboarding

- Component map:
  - Posterior sampler -> Pseudo-data generator -> Score estimator -> Loss computer -> Optimizer

- Critical path: Initialize pseudo-data → Sample posterior → Compute pseudo-score gradients → Estimate Fisher divergence → Backprop to update w, Z → Repeat until convergence → Extract statistics from converged w, Z

- Design tradeoffs:
  - M (pseudo-data points): Higher M increases approximation capacity but increases optimization difficulty and memory. Paper tested M ∈ {50, 100, 200, 400, 800, 1600}.
  - Standard vs. Sliced Fisher Divergence: SFD has O(1) vs. O(d) computational cost per gradient estimate but requires sampling slicing vectors v.
  - Initialization strategy: Paper uses prior-informed initialization for pseudo-data; random initialization may require more iterations.

- Failure signatures:
  - Weights not converging to N: P_m w_m ≈ 1 suggests initialization or learning rate issues
  - Feature statistics not converging: May indicate model is too simple to extract those features (check sufficient statistics)
  - Gradient instability: High variance in score estimates; reduce learning rate or increase L (slicing samples)

- First 3 experiments:
  1. Replicate the kidscore momiq experiment (Section 5): Use posteriordb model, verify convergence of sum(w) → N and empirical means/variances → true values across different M values.
  2. Ablation on model complexity: Test with different feature vectors ψ(x) in Bayesian linear regression (e.g., polynomial degrees 1-5) and verify higher-degree polynomials recover higher-order moments as predicted by Theorem 3.4.
  3. Non-Bayesian validation: Apply the non-Bayesian variant (Definition 4.2) to a simple trained network and compare reconstruction quality against model width, verifying the relationship described in Section 4.2 and matching Buzaglo et al. (2023) observations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise quantitative trade-off between model complexity (which facilitates reconstruction) and training dataset size (which impedes it), and can this trade-off be characterized theoretically?
- Basis in paper: [explicit] The authors state "Using a model with more features was seen to combat the issue of larger training data sets causing worse training data reconstruction but an exact characterisation of the trade off is an open problem" (Section 3.2).
- Why unresolved: The paper establishes that both factors influence reconstruction difficulty but does not derive a formal relationship between them.
- What evidence would resolve it: Theoretical bounds relating model capacity, dataset size N, and reconstruction error; empirical validation across varied architectures and dataset sizes.

### Open Question 2
- Question: Can more advanced score-matching methods (e.g., denoising score matching) improve reconstruction accuracy for Bayesian models compared to the standard and sliced Fisher divergence approaches presented?
- Basis in paper: [explicit] The authors note "Analogous results for even more sophisticated score matching methods such as de-noising are left as future work" (Section 3.1).
- Why unresolved: Only standard and sliced Fisher divergence were implemented and analyzed.
- What evidence would resolve it: Comparative experiments using denoising score matching on the same Bayesian reconstruction tasks, with metrics on convergence speed and reconstruction fidelity.

### Open Question 3
- Question: How can the MMD kernel characterization be used to quantitatively assess a model's intrinsic vulnerability to reconstruction attacks before deployment?
- Basis in paper: [explicit] The authors write "This could be used to quantitatively evaluate how susceptible a model is to reconstruction attacks by evaluating how characteristic the corresponding kernel based on the model features is" (Section 6).
- Why unresolved: The theoretical connection between kernel characteristics and vulnerability is established, but no practical metric or auditing procedure was developed.
- What evidence would resolve it: A formal vulnerability score derived from kernel properties, validated against actual reconstruction success rates across diverse model architectures.

## Limitations
- The computational complexity of estimating Fisher divergence scales poorly with dimension, and the sliced variant may introduce approximation errors not characterized in the paper
- The connection between kernel expressiveness and reconstructable features is established theoretically but lacks empirical validation across diverse model architectures
- The framework focuses on sufficient statistics recovery but doesn't address whether the recovered statistics enable meaningful privacy breaches beyond synthetic examples

## Confidence
- **High**: The mathematical framework connecting Fisher divergence, MMD, and feature maps is rigorously proven. The integration-by-parts trick for tractable score matching is well-established.
- **Medium**: The interpretation of MMD kernel expressiveness as determining reconstructable features follows logically from theory but requires more empirical validation. The sufficient statistic characterization is mathematically sound but its practical implications need testing.
- **Low**: The claimed privacy-performance tradeoff based on model complexity is primarily supported by one cited study (Buzaglo et al., 2023) and lacks systematic validation across different model families and architectures.

## Next Checks
1. **Empirical feature map validation**: Systematically vary model complexity (depth, width, activation functions) in Bayesian neural networks and measure reconstruction quality for different feature types (means, variances, correlations). Verify the predicted relationship between model expressiveness and recoverable features matches Theorem 3.4 predictions.
2. **Scalability benchmark**: Implement the framework on increasingly high-dimensional datasets (e.g., MNIST, CIFAR-10) and measure computational cost, reconstruction accuracy, and convergence behavior. Compare standard vs. sliced Fisher divergence performance and identify practical limits.
3. **Privacy utility analysis**: Given reconstructed sufficient statistics (means, variances, etc.), evaluate whether this information enables meaningful privacy attacks (e.g., membership inference, attribute inference) beyond what's demonstrated in synthetic examples. Test if partial reconstruction poses real privacy risks.