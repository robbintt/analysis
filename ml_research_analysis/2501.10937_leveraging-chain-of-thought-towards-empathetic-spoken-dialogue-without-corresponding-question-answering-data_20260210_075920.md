---
ver: rpa2
title: Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding
  Question-Answering Data
arxiv_id: '2501.10937'
source_url: https://arxiv.org/abs/2501.10937
tags:
- speech
- arxiv
- content
- emotion
- empathetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LPE, a novel method that enables empathetic
  spoken dialogue generation without requiring spoken question-answering datasets.
  The approach leverages a two-stage training process that first aligns speech content
  and emotion features with LLM embedding space using ASR and SER data, then employs
  Chain-of-Thought prompting to guide the model in generating empathetic responses.
---

# Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data

## Quick Facts
- arXiv ID: 2501.10937
- Source URL: https://arxiv.org/abs/2501.10937
- Reference count: 40
- Empathetic spoken dialogue generation without spoken QA data, achieving competitive performance to cascaded systems using zero-shot CoT prompting

## Executive Summary
This paper introduces LPE, a novel method that enables empathetic spoken dialogue generation without requiring spoken question-answering datasets. The approach leverages a two-stage training process that first aligns speech content and emotion features with LLM embedding space using ASR and SER data, then employs Chain-of-Thought prompting to guide the model in generating empathetic responses. The method uses WavLM for speech encoding, feature adapters for modality alignment, and LLaMA2-7B as the LLM decoder. Evaluation shows that LPE outperforms speechLLM baselines in both objective metrics and subjective evaluation scores for content, empathy, and clarity.

## Method Summary
LPE employs a two-stage training process for empathetic spoken dialogue generation. Stage 1 aligns speech content features with LLM embedding space using ASR training on LibriSpeech with WavLM-large and adapter modules. Stage 2 jointly trains content and emotion feature alignment using multitask ASR+SER training on IEMOCAP+MEAD with emotion classification auxiliary loss. The system uses WavLM-large encoder with subsampler and multiscale adapters, and a frozen LLaMA2-7B-chat decoder. Chain-of-Thought prompting with explicit steps (Listen→Perceive→Express) guides empathetic response generation without requiring manual QA data annotation.

## Key Results
- LPE achieves BLEU-1: 16.13, BLEU-4: 2.85, BERTScore: 82.9 on synthetic test data
- Outperforms speechLLM baselines (SALMONN, Qwen-Audio-Chat) in both objective and subjective metrics
- GPT-4o-rated Content score: 3.99/5, Empathy score: 3.72/5, Clarity score: 4.97/5
- Zero-shot CoT prompting with explicit steps performs best among CoT variants

## Why This Works (Mechanism)
The approach works by decomposing the complex task of empathetic spoken dialogue generation into manageable sub-tasks through Chain-of-Thought prompting. The two-stage training process enables the model to first learn speech-to-text alignment (Stage 1) and then learn emotion-aware response generation (Stage 2) without requiring paired spoken QA data. The feature adapters bridge the modality gap between speech features and LLM embedding space, while the multitask training with emotion classification auxiliary loss ensures the model captures both content and emotional aspects of dialogue.

## Foundational Learning
- **Chain-of-Thought prompting**: Breaking complex reasoning into intermediate steps (needed for guiding empathetic response generation; check: outputs follow logical reasoning sequence)
- **Multitask learning with auxiliary loss**: Joint training of multiple related tasks (needed for learning both ASR and SER simultaneously; check: both ASR WER and SER accuracy improve)
- **Feature adapters for modality alignment**: Lightweight modules that adapt pretrained model features to new modalities (needed for bridging speech and text embeddings; check: adapter weights converge during training)

## Architecture Onboarding

**Component Map**: WavLM encoder -> Subsampler adapter -> Multiscale adapter -> LLaMA2 decoder

**Critical Path**: Speech input → WavLM encoding → Subsample projection → Emotion feature extraction → LLM decoding → Empathetic response

**Design Tradeoffs**: Uses frozen LLM to avoid catastrophic forgetting while training adapters; employs multitask learning to capture both content and emotion; uses synthetic data for consistent evaluation but may limit generalization

**Failure Signatures**: Outputs transcriptions instead of responses (Stage 2 skipped), repetitive/meaningless output (long CoT prompts), poor emotion recognition (multiscale adapter not trained with SER data)

**3 First Experiments**:
1. Verify Stage 1 training: Check ASR WER on LibriSpeech validation set after training
2. Verify Stage 2 training: Check SER accuracy on IEMOCAP session 5 after multitask training
3. Test CoT prompting: Generate responses with different CoT variants and check for logical flow

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation based on synthetic test data (AlpacaTTS) may not reflect real human speech patterns
- Limited test set (IEMOCAP session 5) provides insufficient statistical power for generalization claims
- Human evaluation covers only three dimensions with limited granularity
- Critical implementation details (prompt templates, adapter specifications) are not published

## Confidence
**Performance claims (Medium confidence)**: Measurable improvements but based on synthetic data and limited test sets
**Methodology claims (Medium confidence)**: Well-described framework but insufficient implementation details for reproduction
**Ablation study claims (High confidence)**: Internally consistent comparison of CoT variants with clear patterns

## Next Checks
1. Replicate performance on human speech: Generate new test set using diverse human speakers and evaluate generalization beyond synthetic data
2. Publish prompt templates and adapter specifications: Release exact implementations to enable independent verification
3. Expand human evaluation scope: Conduct broader evaluations across additional dimensions and diverse annotators