---
ver: rpa2
title: 'JudgeRLVR: Judge First, Generate Second for Efficient Reasoning'
arxiv_id: '2601.08468'
source_url: https://arxiv.org/abs/2601.08468
tags:
- reasoning
- training
- judgerlvr
- rlvr
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of excessive verbosity in LLM reasoning
  induced by RLVR training, where models produce long, backtracking-filled solutions
  that waste computation without improving accuracy. The authors propose JudgeRLVR,
  a two-stage paradigm where the model first learns to judge the correctness of solution
  responses, then fine-tunes on generation with the learned discriminative prior.
---

# JudgeRLVR: Judge First, Generate Second for Efficient Reasoning

## Quick Facts
- arXiv ID: 2601.08468
- Source URL: https://arxiv.org/abs/2601.08468
- Reference count: 19
- Key outcome: JudgeRLVR improves math reasoning efficiency by first training a model to judge solution correctness, then using that knowledge for generation, achieving +3.7 points accuracy gain with 42% shorter outputs on in-domain tasks.

## Executive Summary
JudgeRLVR addresses the problem of excessive verbosity in LLM reasoning induced by RLVR training, where models produce long, backtracking-filled solutions that waste computation without improving accuracy. The authors propose a two-stage paradigm where the model first learns to judge the correctness of solution responses, then fine-tunes on generation with the learned discriminative prior. Experiments on Qwen3-30B-A3B show JudgeRLVR improves in-domain math accuracy by +3.7 points while reducing generation length by 42%, and delivers +4.5 points average accuracy gain on out-of-domain tasks, demonstrating enhanced efficiency and generalization.

## Method Summary
JudgeRLVR employs a sequential two-stage training paradigm. In Stage 1 (Judging), the model is trained to judge solution correctness by analyzing problem-solution pairs and outputting verdicts. This stage uses balanced datasets with hard negative mining to ensure robust discriminative learning. In Stage 2 (Generating), the model is initialized from the judge and trained with vanilla RLVR to generate solutions. The approach uses DAPO optimization with dynamic sampling that filters out uninformative samples (pass rates 0 or 1), and employs specific prompt formats for both judging and generating tasks.

## Key Results
- In-domain math tasks: +3.7 points accuracy gain with 42% reduction in generation length
- Out-of-domain tasks: +4.5 points average accuracy gain across GPQA Diamond, IFEval, LiveCodeBenchv6, MMLU-Redux, and ZebraLogic
- Sequential staging superiority: outperforms mixed strategy ablation and judge-only variants

## Why This Works (Mechanism)

### Mechanism 1
Training a model to judge solution correctness before generation improves reasoning efficiency by internalizing a discriminative prior that identifies error patterns early, pruning unproductive reasoning branches before they are expanded. This assumes discriminative capability transfers to generative behavior, reducing trial-and-error exploration.

### Mechanism 2
Sequential judge-then-generate staging yields cleaner policy learning than interleaved training because separating stages allows the model to consolidate an error-aware prior before exploiting it during generation, avoiding gradient interference between two objectives.

### Mechanism 3
Reduced backtracking markers in outputs indicate internalized verification rather than explicit trial-and-error, as the model shifts from externalizing search to internal decision-making, producing information-dense reasoning. This assumes transition-word frequency is a valid proxy for backtracking behavior and reasoning quality.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: JudgeRLVR builds on RLVR as the base training paradigm; understanding sparse reward signals from final-answer correctness is prerequisite
  - Quick check question: Can you explain why RLVR tends to produce verbose reasoning traces despite only optimizing for final correctness?

- Concept: **Policy Gradient Methods (GRPO/DAPO family)**
  - Why needed here: The paper uses DAPO for training; understanding on-policy sampling and dynamic reward filtering is essential
  - Quick check question: How does dynamic sampling filter out uninformative samples with pass rates 0 or 1?

- Concept: **Perplexity as a Style Divergence Metric**
  - Why needed here: Perplexity changes between models quantify style transfer from judging training
  - Quick check question: Why does increasing perplexity relative to Base SFT indicate style shift rather than degradation?

## Architecture Onboarding

- Component map: SFT base model → Construct judgment dataset (balanced correct/incorrect per problem) → Train judge (145 steps) → Initialize generator from judge weights → Train generator with Vanilla RLVR (105 steps)
- Critical path: SFT base model → 2. Construct judgment dataset (balanced correct/incorrect per problem) → 3. Train judge (145 steps) → 4. Initialize generator from judge weights → 5. Train generator with Vanilla RLVR (105 steps)
- Design tradeoffs: Judge Only skips Stage 2 → discriminative capability does not translate to concise generation (ablation shows +92% length increase); Mixed Strategy interleaves both → gradient interference, unstable performance on OOD tasks; Sequential requires more engineering but yields cleaner efficiency-accuracy trade-off
- Failure signatures: If generation length increases after Stage 2 → check whether Stage 1 converged properly (verify verdict accuracy); If OOD performance degrades → verify class balancing did not overfit to in-domain error patterns; If perplexity remains flat during Stage 1 → judgment training may not be inducing style shift; check data diversity
- First 3 experiments: 1. Replicate Judge Only ablation: train Stage 1, skip Stage 2, measure if accuracy drops and length increases as reported; 2. Run perplexity tracking: compute Base SFT PPL on sampled outputs during Stage 1 to verify style transfer signal; 3. Ablate hard negative mining: train with random sampling instead of moderate-difficulty problems; compare efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
Does the judge-then-generate paradigm generalize across diverse model architectures (e.g., dense transformers, smaller models) beyond the MoE-based Qwen3-30B-A3B tested? The paper uses a single model architecture without architecture ablation, leaving scalability to other architectures untested.

### Open Question 2
What is the optimal allocation of training steps between the judging and generating stages, and how does this vary with model size or data scale? The paper uses 145 judging + 105 generating steps without systematic sweep of this ratio, making it unclear whether this split is near-optimal.

### Open Question 3
Why does the "Judge Only" condition fail to improve generation quality despite acquiring discriminative capability? The authors observe this unresolved pattern where better judging does not reliably translate into better generation without subsequent RLVR.

### Open Question 4
What causes the task-dependent length behavior where some out-of-domain tasks (IFEval, ZebraLogic) show increased generation length despite improved accuracy? The paper notes this pattern but doesn't fully explain whether it reflects task-intrinsic verification requirements or limitations of math-trained judges on non-math tasks.

## Limitations
- Sequential staging mechanism lacks theoretical grounding for why interleaved training degrades performance
- Use of linguistic proxies (transition-word frequency) to measure reasoning quality is not externally validated
- Scalability to smaller models or non-MoE architectures remains untested
- Robustness to noisy or ambiguous problem data is unknown

## Confidence
- High confidence: In-domain math accuracy gains (+3.7 points) and length reduction (42%) from JudgeRLVR
- Medium confidence: Out-of-domain generalization gains (+4.5 points average) and efficiency improvements
- Medium confidence: Sequential staging superiority over interleaved training (based on ablation)
- Low confidence: Linguistic proxy validity for reasoning quality (transition-word frequency)

## Next Checks
1. Replicate Judge Only ablation: Train Stage 1 only, skip Stage 2, and verify if accuracy drops while length increases as reported
2. Run perplexity tracking: Compute Base SFT PPL on sampled outputs during Stage 1 to confirm style transfer signal
3. Ablate hard negative mining: Train with random sampling instead of moderate-difficulty problems and compare efficiency gains