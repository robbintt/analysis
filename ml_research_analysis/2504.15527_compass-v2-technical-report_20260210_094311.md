---
ver: rpa2
title: Compass-V2 Technical Report
arxiv_id: '2504.15527'
source_url: https://arxiv.org/abs/2504.15527
tags:
- data
- compass-v2
- dataset
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Compass-v2, a Mixture-of-Experts (MoE) model
  designed for Southeast Asian (SEA) languages and e-commerce applications. The model
  addresses the underrepresentation of low-resource SEA languages in mainstream LLMs
  by incorporating 30B total parameters with 5B active parameters, leveraging shared
  and specialized expert modules.
---

# Compass-V2 Technical Report

## Quick Facts
- arXiv ID: 2504.15527
- Source URL: https://arxiv.org/abs/2504.15527
- Reference count: 24
- Primary result: State-of-the-art performance among sub-30B models in SEA multilingual and e-commerce tasks

## Executive Summary
Compass-v2 is a Mixture-of-Experts (MoE) language model specifically designed for Southeast Asian languages and e-commerce applications. The model features 30B total parameters with 5B active parameters, utilizing shared and specialized expert modules to optimize both performance and efficiency. By incorporating a high-quality SEA dataset and a large-scale e-commerce corpus, Compass-v2 addresses the underrepresentation of low-resource SEA languages in mainstream LLMs while achieving competitive results against larger models like GPT-4o and Qwen2.5-72B at lower inference costs.

## Method Summary
Compass-v2 employs a Mixture-of-Experts architecture with 30 billion total parameters and 5 billion active parameters, featuring both shared and specialized expert modules. The model was trained on a curated high-quality SEA dataset combined with a large-scale e-commerce corpus to enhance multilingual and domain-specific performance. Unlike conventional approaches that use separate models for different reasoning capabilities, Compass-v2 integrates both fast and deep reasoning within a unified framework. The training methodology leverages the MoE architecture's ability to selectively activate relevant expert pathways, optimizing computational efficiency while maintaining strong performance across SEA languages and e-commerce tasks.

## Key Results
- State-of-the-art performance among sub-30B models in SEA multilingual and e-commerce tasks
- Competitive results against larger models like GPT-4o and Qwen2.5-72B while maintaining lower inference costs
- Unified framework supporting both fast and deep reasoning within a single model architecture

## Why This Works (Mechanism)
The Mixture-of-Experts architecture enables selective activation of specialized pathways, allowing the model to efficiently handle diverse SEA language patterns and e-commerce-specific contexts. By curating high-quality SEA and e-commerce datasets, the model develops robust representations for low-resource languages while maintaining strong performance on domain-specific tasks. The unified fast and deep reasoning framework eliminates the need for separate models, reducing deployment complexity while preserving capability diversity.

## Foundational Learning
- Mixture-of-Experts (MoE) Architecture: Why needed - Enables efficient scaling by activating only relevant expert pathways; Quick check - Verify expert specialization and routing effectiveness
- Southeast Asian Language Characteristics: Why needed - Addresses low-resource language challenges and linguistic diversity; Quick check - Test model performance across all supported SEA languages
- E-commerce Domain Knowledge: Why needed - Provides context-specific understanding for product descriptions and customer interactions; Quick check - Evaluate model on diverse e-commerce task benchmarks

## Architecture Onboarding

Component Map: Input -> Tokenization -> MoE Layer (30B total, 5B active) -> Output Generation

Critical Path: Tokenization → Expert Selection → Parameter Activation → Response Generation

Design Tradeoffs: The MoE architecture balances model capacity (30B parameters) with computational efficiency (5B active parameters), enabling competitive performance at lower inference costs compared to dense models of similar size.

Failure Signatures: Potential routing errors could lead to suboptimal expert selection, degraded performance on specific SEA languages, or inconsistent e-commerce task handling.

First Experiments:
1. Benchmark model performance across all supported SEA languages using standardized multilingual datasets
2. Test e-commerce task performance on diverse product categories and customer query types
3. Evaluate inference cost and latency compared to baseline models and larger competitors

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely on specific benchmarks without comprehensive third-party validation
- Insufficient detail about dataset composition and potential biases in SEA and e-commerce corpora
- Long-term performance stability and robustness across diverse real-world e-commerce scenarios remain unverified

## Confidence

| Claim | Confidence |
|-------|------------|
| Architectural design choices are technically sound | High |
| Performance comparisons against benchmark models are reasonable | Medium |
| Unified fast and deep reasoning capabilities are empirically validated | Medium |

## Next Checks
1. Conduct independent benchmarking of Compass-v2 using diverse, publicly available SEA language datasets to verify the reported performance advantages
2. Perform comprehensive bias and fairness audits on the model's outputs across all supported SEA languages and e-commerce domains
3. Execute long-term deployment monitoring to assess model drift, performance degradation, and robustness under varying e-commerce query distributions