---
ver: rpa2
title: Harnessing Consistency for Robust Test-Time LLM Ensemble
arxiv_id: '2510.13855'
source_url: https://arxiv.org/abs/2510.13855
tags:
- token
- arxiv
- ensemble
- consistency
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robustness of LLM ensemble methods against
  erroneous signals arising from heterogeneous tokenization schemes and varying model
  expertise. It proposes CORE, a plug-and-play technique that harnesses model consistency
  at both token and model levels.
---

# Harnessing Consistency for Robust Test-Time LLM Ensemble

## Quick Facts
- arXiv ID: 2510.13855
- Source URL: https://arxiv.org/abs/2510.13855
- Reference count: 37
- Primary result: CORE achieves 1.3% and 2.8% average performance gains on Top-2 and Top-3 model ensembles respectively

## Executive Summary
This paper introduces CORE (Consistency-based Robust Ensemble), a plug-and-play method that improves test-time LLM ensemble robustness by leveraging model consistency at both token and model levels. The approach addresses challenges from heterogeneous tokenization schemes and varying model expertise by computing token-level consistency scores through a low-pass filter mechanism and model-level consistency through confidence-weighted agreement. CORE requires access to token-level logits and demonstrates consistent performance improvements across six diverse benchmarks using multiple model combinations and ensemble strategies.

## Method Summary
CORE operates as an inference-time technique that doesn't require training. It first aligns token probability distributions from assistant models to the main model's vocabulary using methods like MINED or UNITE. For each decoding step, it computes a reference probability distribution by averaging aligned probabilities, then calculates token consistency scores using an RBF kernel that penalizes tokens with high disparity. Model-level consistency aggregates these token scores weighted by inverse entropy, and the final ensemble output is a weighted combination where assistant model weights are normalized by their consistency scores, with the main model weight clipped at a minimum of 0.5.

## Key Results
- CORE consistently improves ensemble performance across all tested benchmarks
- Average performance gain of 1.3% on Top-2 model ensembles
- Average performance gain of 2.8% on Top-3 model ensembles
- Robustness demonstrated across diverse tasks including GSM8K, PIQA, SAMSum, TriviaQA, NaturalQuestions, and MMLU

## Why This Works (Mechanism)
The method works by treating token probability disparity as a signal of uncertainty or tokenization misalignment. When assistant models produce significantly different probability distributions at specific token positions compared to the consensus, this indicates potential issues that should be downweighted. The low-pass filter (RBF kernel) converts statistical disparity into a consistency score that effectively reduces the influence of unreliable tokens. Model-level consistency then aggregates these token-level signals to identify which assistant models are providing high-quality, consistent contributions versus those that should be downweighted due to disagreement or low confidence.

## Foundational Learning
- **Token alignment methods (MINED, UNITE, GAC, EVA)**: Required to map assistant model logits to main model vocabulary for meaningful probability comparison. Quick check: Verify alignment matrices produce reasonable overlap statistics.
- **RBF kernel low-pass filtering**: Converts token probability disparity into consistency scores that penalize outliers. Quick check: Test with synthetic distributions to verify expected smoothing behavior.
- **Entropy normalization**: Used to weight model contributions based on confidence, preventing overconfident but wrong models from dominating. Quick check: Validate that high-entropy tokens receive appropriate consistency penalties.

## Architecture Onboarding
- **Component map**: Token logits -> Alignment mapping -> Token consistency scoring -> Model consistency aggregation -> Weighted ensemble fusion
- **Critical path**: The token alignment step is most critical as errors here propagate through the entire consistency calculation pipeline
- **Design tradeoffs**: Fixed RBF kernel parameters (σ=0.5) provide simplicity but may not adapt well to all domains; main model weight clipping (≥0.5) ensures stability but may limit assistant model contributions
- **Failure signatures**: Negative ensemble performance (degradation vs. best single model), high token disparity values indicating alignment issues, or model consistency scores that don't correlate with actual accuracy
- **First experiments**: 1) Test token alignment accuracy with controlled vocabulary mismatches, 2) Verify RBF kernel produces expected consistency scores on synthetic probability distributions, 3) Validate model consistency aggregation correctly identifies high-quality assistant models

## Open Questions the Paper Calls Out
- How can consistency-based ensemble methods be adapted for closed-source or black-box LLMs where access to token-level logits is restricted? The current method requires full internal access to token-level probabilities.
- Can a dynamic decision mechanism be developed to determine "when to ensemble" based on the real-time confidence of the main model? The current framework applies uniformly regardless of main model confidence.
- What are the most effective principled criteria for selecting which models to include in the ensemble prior to inference? Current approach uses Top-k based on general performance without pre-selection filtering.

## Limitations
- Requires access to token-level logits, preventing direct application to commercial API-based models
- Fixed RBF kernel parameters may not adapt well across different domains or tokenization schemes
- Limited testing scope with 3-8B parameter models, unclear how performance scales to larger models

## Confidence
- **High Confidence**: General framework of using consistency measures to improve ensemble robustness is well-supported by experimental results
- **Medium Confidence**: Specific RBF kernel parameterization and entropy-based weighting are justified through ablation studies
- **Low Confidence**: Claims about robustness against heterogeneous tokenization schemes are supported primarily through controlled experiments

## Next Checks
1. Cross-architectural validation: Test CORE with models of different architectural families to verify robustness claims extend beyond parameter-matched models
2. Ablation on token alignment methods: Systematically compare CORE's performance using different alignment baselines to isolate consistency scoring contribution
3. Confidence calibration analysis: Evaluate whether model consistency scores correlate with actual model accuracy on held-out validation sets