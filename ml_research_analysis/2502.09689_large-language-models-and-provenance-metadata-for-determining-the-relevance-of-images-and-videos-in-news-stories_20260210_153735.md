---
ver: rpa2
title: Large Language Models and Provenance Metadata for Determining the Relevance
  of Images and Videos in News Stories
arxiv_id: '2502.09689'
source_url: https://arxiv.org/abs/2502.09689
tags:
- media
- provenance
- article
- metadata
- misinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for determining the relevance of
  images and videos in news articles by combining large language models (LLMs) and
  provenance metadata. The approach analyzes the article text, media captions, and
  provenance metadata (e.g., origin time, location, edits) to assess whether the media
  is contextually relevant and unaltered.
---

# Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories

## Quick Facts
- arXiv ID: 2502.09689
- Source URL: https://arxiv.org/abs/2502.09689
- Reference count: 21
- Authors: Tomas Peterka; Matyas Bohacek
- Primary result: Introduces a method combining LLMs and C2PA provenance metadata to assess image/video relevance in news articles, outputting structured assessments with reasoning

## Executive Summary
This paper addresses the challenge of detecting out-of-context misinformation in news media by developing a system that analyzes both article text and cryptographic provenance metadata. The method uses an LLM to evaluate whether attached images or videos are contextually relevant by examining their origin time, location, and edit history against the article content. A prototype web interface demonstrates the approach using Newspaper4k for article scraping, C2PA for metadata extraction, and Phi-3 for analysis, providing structured relevance assessments with explicit reasoning.

## Method Summary
The system analyzes news articles by extracting text and media, then retrieving C2PA provenance metadata from images/videos. These inputs are formatted into a structured prompt for the Phi-3 LLM, which evaluates three criteria: location-time relevance, tampering evidence, and overall contextual relevance. The LLM returns a JSON-structured assessment with boolean flags and reasoning strings. The implementation uses Newspaper4k for article parsing, a C2PA parser for metadata extraction, and Gradio for the web interface, operating without external fact retrieval by relying on the model's parametric knowledge.

## Key Results
- Proposes a novel method combining LLM reasoning with cryptographic provenance metadata for media relevance assessment
- Demonstrates a working prototype that analyzes articles and provides structured relevance judgments with reasoning
- Identifies critical limitations including metadata availability, LLM accuracy, and the need for evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Cross-Modal Relevance Reasoning
The system feeds the LLM article text, media captions, and provenance metadata to assess temporal-spatial consistency. The LLM's world knowledge and logical reasoning capabilities judge whether media origin matches article context. Break condition: LLM hallucinates facts about geography, events, or timelines.

### Mechanism 2: Cryptographic Provenance Metadata as Ground Truth
C2PA-signed metadata provides trustworthy signals about media origin, enabling fact-based verification beyond semantic coherence. The system extracts origin time, location, and edit history fields. Break condition: Media lacks C2PA metadata or cryptographic chain is broken.

### Mechanism 3: Structured Output with Chain-of-Thought Elicitation
Prompting for explicit reasoning before assessment improves interpretability and helps identify model failure modes. The system requires structured JSON output with boolean flags and string reasons. Break condition: LLM generates malformed JSON or plausible but factually wrong reasoning.

## Foundational Learning

- **C2PA (Coalition for Content Provenance and Authenticity)**: Cryptographic metadata standard the system depends on; understanding its fields and adoption limitations is essential. Quick check: Can you name three fields C2PA metadata typically includes and explain why cryptographic signing matters for tamper detection?

- **Out-of-context misinformation**: The core problem being solved; distinct from deepfakes because media is authentic but misattributed. Quick check: Why is detecting out-of-context media harder than detecting AI-generated deepfakes according to the paper?

- **LLM structured output / function calling**: The system relies on parsing JSON output from the LLM; understanding prompting strategies for reliable structured output is practical knowledge. Quick check: What failure modes might occur when forcing an LLM to output structured JSON, and how does the paper's prompt design attempt to mitigate them?

## Architecture Onboarding

- **Component map**: Newspaper4k -> C2PA parser -> Phi-3 LLM -> Gradio UI
- **Critical path**: URL or structured article input → Newspaper4k extraction → C2PA metadata parsing → Metadata + article text formatted into prompt → Phi-3 inference → JSON parsing → Display in Gradio UI (assessment boxes + chat)
- **Design tradeoffs**: Small model (Phi-3) chosen for efficiency vs. larger models with potentially better reasoning; no external fact retrieval; optional provenance metadata means system may produce poorly-sourced conclusions when metadata absent; chain-of-thought included for debugging/transparency
- **Failure signatures**: Empty or missing C2PA metadata → LLM still attempts assessment with insufficient grounding; hallucinated facts in reasoning → confident but incorrect relevance judgment; malformed JSON output → parsing failure; article describes recent events outside training data → LLM cannot accurately judge temporal relevance
- **First 3 experiments**: 1) Run system on 20 news articles with known C2PA metadata vs. 20 without; quantify assessment frequency and confidence language differences. 2) For 10 articles with ground-truth provenance, manually verify each boolean flag and reason string; categorize error types. 3) Replace Phi-3 with larger model (e.g., Llama-3-70B or GPT-4) on identical inputs; compare assessment accuracy and reasoning quality.

## Open Questions the Paper Calls Out

- How do existing LLMs quantitatively perform on media relevance tasks when benchmarked against a curated dataset of news articles with verified provenance metadata? The authors note lack of evaluation datasets prevents benchmark evaluations.

- What strategies are most effective for determining overall article relevance when multiple attached images yield conflicting assessments (e.g., one relevant and one tampered)? The current prototype does not define aggregation logic for contradictory signals.

- To what extent do pre-existing biases in LLMs regarding demographic groups or political topics skew the accuracy of media relevance assessments? The authors acknowledge LLM biases but have not audited performance across diverse topics.

- How can the system be modified to reliably indicate uncertainty or withhold judgment when provenance metadata is absent or incomplete? The current method may struggle to admit insufficient data rather than hallucinating a justification.

## Limitations
- C2PA metadata adoption is limited, meaning most media lacks the cryptographic signals needed for grounded assessment
- LLM reliance on parametric knowledge without external retrieval creates vulnerability to factual errors about geography, events, and timelines
- Chain-of-thought reasoning may be post-hoc justification rather than evidence-based inference, and does not guarantee accuracy

## Confidence
- **Medium Confidence**: Core mechanism combining LLM reasoning with provenance metadata is technically sound and addresses documented misinformation detection gaps
- **Low Confidence**: Real-world accuracy cannot be determined without evaluation datasets; the paper acknowledges this gap explicitly
- **Medium Confidence**: Choice of Phi-3 as small, efficient model is reasonable for prototyping, but performance tradeoffs versus larger models remain untested

## Next Checks
1. **Metadata Availability Assessment**: Systematically test the pipeline on 50 diverse news articles to quantify the percentage with extractable C2PA metadata and measure how often the system produces assessments versus falling back on LLM-only reasoning.

2. **Ground Truth Validation Study**: Using articles with known provenance (either C2PA metadata or verified manual annotation), evaluate the accuracy of all three assessment components: location relevance, time relevance, and tampering detection. Categorize error types and measure false positive/negative rates.

3. **Model Capability Comparison**: Replace Phi-3 with a larger model (e.g., GPT-4 or Llama-3-70B) and run identical inputs through both systems. Compare assessment accuracy, reasoning quality, and hallucination rates to determine whether model size meaningfully impacts performance for this task.