---
ver: rpa2
title: 'BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better
  Edge Selection'
arxiv_id: '2510.25786'
source_url: https://arxiv.org/abs/2510.25786
tags:
- edges
- circuit
- edge
- scores
- greedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of circuit discovery in mechanistic
  interpretability, specifically focusing on the selection stage of building subgraphs
  that capture a model''s behavior for a given task. The authors propose three key
  improvements to the standard pipeline: (1) using bootstrapping to identify edges
  with consistently-signed attribution scores, filtering out unstable ones; (2) introducing
  a ratio-based selection strategy (PNR) that balances the proportion of positive
  and negative edges; and (3) replacing greedy selection with an integer linear programming
  (ILP) formulation for globally optimal subgraph construction.'
---

# BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection
## Quick Facts
- arXiv ID: 2510.25786
- Source URL: https://arxiv.org/abs/2510.25786
- Reference count: 7
- This work proposes three key improvements to the standard circuit discovery pipeline: bootstrapping for edge stability, PNR ratio-based selection for balanced edge selection, and ILP for globally optimal subgraph construction, yielding more faithful circuits across multiple models and tasks.

## Executive Summary
This paper addresses a fundamental challenge in mechanistic interpretability: constructing faithful circuits that accurately capture a model's behavior for specific tasks. The authors focus on the selection stage of circuit discovery, proposing three methodological improvements to the standard pipeline. Their approach combines bootstrapping to identify stable edges, a ratio-based selection strategy (PNR) that balances positive and negative edges, and an integer linear programming formulation for globally optimal subgraph construction. Applied to models like GPT-2, Gemma-2, and Qwen-2.5 across tasks such as IOI, MCQA, and ARC-E, their methods demonstrate improved circuit faithfulness compared to prior approaches.

## Method Summary
The authors propose three key improvements to the circuit discovery pipeline. First, they use bootstrapping to identify edges with consistently-signed attribution scores, filtering out unstable ones. Second, they introduce a ratio-based selection strategy (PNR) that balances the proportion of positive and negative edges. Third, they replace greedy selection with an integer linear programming (ILP) formulation for globally optimal subgraph construction. These methods are applied to models including GPT-2, Gemma-2, and Qwen-2.5 across various tasks, demonstrating improved circuit faithfulness through better edge selection and more principled subgraph construction.

## Key Results
- ILP combined with PNR improves CMD (lower is better) across most model-task combinations
- Bootstrapping improves CPR (higher is better) across most model-task combinations
- The proposed methods yield more faithful circuits than prior approaches on the MIB benchmark

## Why This Works (Mechanism)
The improvements work by addressing key limitations in the standard circuit discovery pipeline. Bootstrapping identifies stable edges by measuring consistency across multiple sampling runs, filtering out edges with unstable attribution scores. The PNR ratio-based selection strategy ensures balanced representation of both positive and negative edges, preventing bias toward one type. The ILP formulation replaces greedy selection with a global optimization approach that considers all possible edge combinations simultaneously, leading to more optimal subgraph construction. Together, these methods create more faithful circuits by ensuring edge stability, balanced representation, and globally optimal selection.

## Foundational Learning
- **Bootstrapping**: A statistical technique for estimating the distribution of a statistic by resampling with replacement. Why needed: To measure edge stability across multiple sampling runs and filter out unstable edges. Quick check: Verify that bootstrapped confidence intervals capture true edge stability.
- **Attribution Scores**: Measures of how much each input feature contributes to a model's output. Why needed: To quantify the importance of individual edges in the circuit. Quick check: Ensure attribution scores are properly normalized and comparable across edges.
- **Integer Linear Programming (ILP)**: A mathematical optimization method where variables are restricted to integer values and the objective function is linear. Why needed: To find globally optimal subgraph constructions rather than relying on greedy selection. Quick check: Verify that ILP formulation correctly captures all constraints and objectives.
- **Circuit Faithfulness**: The degree to which a discovered circuit accurately represents the model's actual decision-making process. Why needed: To evaluate the quality of the discovered circuits. Quick check: Compare discovered circuits against ground truth circuits when available.
- **Edge Selection Strategies**: Methods for choosing which edges to include in the final circuit. Why needed: To construct subgraphs that capture the model's behavior for a given task. Quick check: Evaluate different selection strategies on benchmark tasks.

## Architecture Onboarding
- **Component Map**: Data -> Attribution Scores -> Bootstrapping (Edge Stability) -> Edge Selection (PNR/ILP) -> Circuit Construction
- **Critical Path**: The most important sequence is attribution score computation → bootstrapping for stability → ILP-based selection, as this ensures both stable and globally optimal edge selection.
- **Design Tradeoffs**: The ILP approach provides globally optimal solutions but at significant computational cost, while greedy methods are faster but may miss optimal configurations. The tradeoff between computational efficiency and solution quality must be carefully considered.
- **Failure Signatures**: Circuits may fail to capture true model behavior if bootstrapping fails to identify stable edges, if PNR ratio is poorly calibrated, or if ILP becomes intractable for larger models. Symptoms include poor faithfulness metrics and unstable results across runs.
- **First Experiments**: 1) Apply bootstrapping to a small circuit and verify edge stability improvements. 2) Compare PNR selection against greedy selection on a simple task. 3) Run ILP on a minimal circuit to establish baseline performance and computational requirements.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies heavily on the assumption that ground truth circuits in the MIB benchmark are accurate representations of the model's actual decision-making process.
- The ILP approach becomes computationally expensive and intractable for larger models beyond certain sizes.
- The generalization of these methods to tasks outside the evaluated set (IOI, MCQA, ARC-E) and to architectures beyond Transformers remains untested.

## Confidence
- **High confidence**: The improvements to edge selection methodology (bootstrapping for stability, PNR for balance, ILP for global optimization) are technically sound and the empirical results on the benchmark tasks are clearly demonstrated.
- **Medium confidence**: The claim that these methods produce "more faithful circuits" depends on the validity of the ground truth circuits in the MIB benchmark, which introduces uncertainty about the true faithfulness of the results.
- **Medium confidence**: The practical utility of these methods for real-world mechanistic interpretability work is supported by the results but tempered by computational constraints.

## Next Checks
1. Apply the ILP and bootstrapping methods to an independently-verified circuit (where ground truth is known through exhaustive analysis) to validate that the methods genuinely improve faithfulness beyond the benchmark setting.
2. Conduct runtime and memory profiling of the ILP implementation across a range of model sizes to establish concrete scalability limits and identify potential optimization opportunities.
3. Test the proposed edge selection strategies on a diverse set of tasks beyond the MIB benchmark (e.g., factual recall, reasoning tasks) to assess generalization across different circuit architectures and behaviors.