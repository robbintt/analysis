---
ver: rpa2
title: 'Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse'
arxiv_id: '2512.17108'
source_url: https://arxiv.org/abs/2512.17108
tags:
- video
- memory
- retrieval
- decoder
- execution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently running multi-stage
  video-language pipelines on mobile devices, where redundant model loads and fragmented
  execution lead to high latency and memory overhead. The authors propose ATOM, a
  system that modularizes video-language models into reusable encoder and decoder
  components, enabling persistent reuse and parallel execution across subtasks.
---

# Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse

## Quick Facts
- arXiv ID: 2512.17108
- Source URL: https://arxiv.org/abs/2512.17108
- Reference count: 5
- Primary result: 27-33% faster end-to-end latency on smartphones vs non-reuse baselines

## Executive Summary
This paper introduces ATOM, a system for efficiently running multi-stage video-language pipelines on mobile devices. The core innovation is modularizing video-language models into reusable encoder and decoder components, enabling persistent reuse across subtasks and parallel execution. By eliminating redundant model loads and leveraging concurrent processing, ATOM achieves significant latency improvements (27-33%) on commercial smartphones while maintaining quality within acceptable bounds (≤2.3 R@1 and ≤1.5 CIDEr degradation).

## Method Summary
ATOM addresses the inefficiency of multi-stage video-language pipelines on edge devices by introducing a reuse-centric architecture. The system modularizes complex pipelines into persistent encoder and decoder components that can be shared across subtasks. This eliminates repeated model loading overhead and enables parallel execution of different pipeline stages. The modular design allows for concurrent processing while maintaining quality through careful resource management and caching strategies.

## Key Results
- 27-33% faster end-to-end latency on commercial smartphones compared to non-reuse baselines
- Quality retention within 2.3 R@1 for retrieval and 1.5 CIDEr for captioning
- Minimal memory overhead of approximately 40MB
- Effective deployment of complex video-language tasks on edge devices without significant quality loss

## Why This Works (Mechanism)
ATOM works by addressing the fundamental inefficiency in multi-stage video-language pipelines where models are repeatedly loaded and unloaded for different subtasks. The mechanism relies on persistent module reuse - once an encoder or decoder is loaded, it remains available for subsequent tasks. Parallel execution is enabled through careful orchestration of these reusable components, allowing different stages of multiple pipelines to run concurrently without resource conflicts.

## Foundational Learning
- **Video-Language Model Modularity**: Breaking down complex pipelines into reusable encoder/decoder components; needed to identify shared resources across subtasks; quick check: verify component boundaries don't break task-specific optimizations
- **Persistent Caching**: Maintaining loaded models in memory across multiple inference calls; needed to eliminate repeated loading overhead; quick check: monitor memory usage patterns during concurrent execution
- **Concurrent Pipeline Execution**: Running multiple pipeline stages in parallel while managing shared resources; needed to maximize hardware utilization; quick check: verify no resource contention causes pipeline stalls
- **Quality-Efficiency Tradeoff**: Balancing performance gains against marginal quality degradation; needed to ensure practical utility; quick check: validate quality metrics remain within acceptable bounds
- **Edge Device Resource Constraints**: Understanding memory, compute, and thermal limitations of mobile hardware; needed to optimize for real deployment scenarios; quick check: test across different device specifications

## Architecture Onboarding

Component Map: Video Input -> Encoder Module -> Decoder Module -> Output Generation
Critical Path: Input preprocessing → Encoder inference → Decoder inference → Post-processing → Output

Design Tradeoffs: ATOM prioritizes latency reduction through reuse and parallelization at the cost of modest memory overhead (~40MB). The modular approach trades some potential optimization opportunities for flexibility across different video-language tasks. Quality degradation is accepted within defined bounds to achieve efficiency gains.

Failure Signatures: Excessive memory usage when scaling to many concurrent tasks, quality degradation beyond acceptable thresholds under high load, or pipeline stalls due to resource contention. Performance bottlenecks may occur when the number of parallel tasks exceeds hardware capabilities.

First Experiments:
1. Measure baseline latency and quality for single-task execution without reuse
2. Evaluate memory overhead with persistent module loading across multiple tasks
3. Test parallel execution scalability with increasing numbers of concurrent subtasks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may not generalize beyond tested video-language tasks (retrieval and captioning)
- ~40MB memory overhead could be significant on resource-constrained devices
- Long-term user experience impact of marginal quality degradation remains unclear
- Specific implementation optimizations are not fully disclosed, limiting reproducibility

## Confidence

High: Latency improvements (27-33%) on tested smartphones
Medium: Quality retention (R@1, CIDEr) and memory overhead estimates
Medium: Generalization to other video-language tasks beyond retrieval and captioning

## Next Checks
1. Evaluate ATOM's performance across a broader range of video-language tasks (e.g., video question answering, summarization) and compare against task-specific optimized pipelines.
2. Test scalability under multi-task scenarios and varying device resource constraints to quantify the ~40MB memory overhead impact.
3. Conduct ablation studies to isolate the contribution of modular reuse versus parallel execution to the reported latency gains.