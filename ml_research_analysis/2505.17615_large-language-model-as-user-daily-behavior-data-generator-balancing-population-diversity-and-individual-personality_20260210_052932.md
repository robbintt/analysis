---
ver: rpa2
title: 'Large language model as user daily behavior data generator: balancing population
  diversity and individual personality'
arxiv_id: '2505.17615'
source_url: https://arxiv.org/abs/2505.17615
tags:
- data
- user
- behavior
- synthetic
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BehaviorGen, a framework that leverages large
  language models (LLMs) to generate high-quality synthetic user behavior data. The
  approach addresses privacy concerns in behavior prediction by simulating user behavior
  based on profiles and real events, enabling data augmentation and replacement in
  behavior prediction models.
---

# Large language model as user daily behavior data generator: balancing population diversity and individual personality

## Quick Facts
- arXiv ID: 2505.17615
- Source URL: https://arxiv.org/abs/2505.17615
- Reference count: 29
- Large language model generates synthetic user behavior data that improves prediction accuracy by up to 18.9% while preserving privacy

## Executive Summary
This paper introduces BehaviorGen, a framework that leverages large language models (LLMs) to generate high-quality synthetic user behavior data. The approach addresses privacy concerns in behavior prediction by simulating user behavior based on profiles and real events, enabling data augmentation and replacement in behavior prediction models. The framework is evaluated across three scenarios: pretraining augmentation, fine-tuning replacement, and fine-tuning augmentation. Results show significant improvements in human mobility and smartphone usage predictions, with gains of up to 18.9%. The generated data effectively captures both population-level diversity and individual-level specificity, demonstrating the potential of BehaviorGen to enhance user behavior modeling through flexible and privacy-preserving synthetic data generation.

## Method Summary
BehaviorGen uses GPT-4o to generate synthetic user behavior sequences conditioned on user profiles and sample behaviors. The framework employs a role-setting prompt that assigns the LLM the task of behavior generation, incorporates user attributes (age, education, gender, consumption, occupation), and enforces strict output format constraints. Weekly segmented generation is used to balance diversity and coherence while maintaining robustness across time periods. The generated sequences follow a structured format (weekday, timestamp, location, intent) and are validated through distributional similarity metrics. The framework supports three deployment modes: pretraining augmentation (mixing real and synthetic data), fine-tuning replacement (using only synthetic data), and fine-tuning augmentation (supplementing limited real data with synthetic sequences).

## Key Results
- Pretraining augmentation improves prediction performance by 2.6% and 6.9% on two datasets
- Fine-tuning replacement achieves 62.0% and 87.8% of real-data fine-tuning gains while preserving privacy
- Fine-tuning augmentation achieves up to 18.9% improvement when limited individual data is available
- Generated data captures both population-level diversity and individual-level specificity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate synthetic behavior data that captures both population diversity and individual specificity when conditioned on user profiles and sample behaviors.
- Mechanism: The framework prompts an LLM with (1) explicit role assignment as "behavior generator," (2) user profile attributes (age, education, gender, consumption, occupation), (3) a small sample of real behavior events, and (4) strict output format constraints. This conditions generation on both demographic priors and observed patterns.
- Core assumption: LLMs have internalized behavioral regularities from pretraining corpora that transfer to structured sequence generation when properly constrained.
- Evidence anchors:
  - [abstract] "By simulating user behavior based on profiles and real events, BehaviorGen supports data augmentation and replacement"
  - [Section 4.1.1] "by explicitly defining the role, the LLM is better equipped to understand the task structure and objectives"
  - [Table 4] Ablation shows removing profile drops KS_P from 0.327 to 0.231 and increases JSD from 0.029 to 0.053
- Break condition: If generated sequences show high n-gram overlap with training samples or fail distributional similarity tests (JSD > 0.1), profile conditioning is insufficient.

### Mechanism 2
- Claim: Segmented generation (weekly chunks) improves long-sequence coherence by reducing context drift while preserving cross-segment diversity.
- Mechanism: Rather than generating month-long sequences in one pass, behaviors are generated week-by-week. Each segment receives the same profile context but generates independent weekly trajectories, preventing the LLM from drifting into repetitive or incoherent patterns.
- Core assumption: Weekly segmentation aligns with natural behavioral periodicity (work-week cycles) and fits within LLM context windows without degradation.
- Evidence anchors:
  - [Section 4.1.1] "segmentation balances diversity and faithfulness while ensuring robustness across weeks"
  - [Table 4] "no_segment" achieves 22.5% Pass@1 vs. 100% for segmented approach, indicating usability collapse without segmentation
  - [Appendix A.4] Figure 4 shows performance convergence at 7 days of input data
- Break condition: If inter-segment consistency metrics (temporal transition probabilities) diverge significantly from real data distributions, segmentation strategy needs adjustment.

### Mechanism 3
- Claim: Synthetic data utility varies by deployment scenario—augmentation benefits pretraining (diversity), while replacement preserves fine-tuning privacy with acceptable performance gap.
- Mechanism: Three distinct use cases: (1) Pretraining augmentation mixes real + synthetic data to improve population-level pattern coverage; (2) Fine-tuning replacement uses only synthetic data to eliminate privacy exposure; (3) Fine-tuning augmentation supplements sparse individual data (~100 logs) with personalized synthetic sequences.
- Core assumption: The performance-privacy tradeoff is acceptable when synthetic data captures intent distributions closely enough for downstream tasks.
- Evidence anchors:
  - [Table 1] Pretraining augmentation: 2.6% and 6.9% improvement on two datasets
  - [Table 2] Fine-tuning replacement: achieves 62.0% and 87.8% of real-data fine-tuning gains
  - [Table 3] Fine-tuning augmentation: up to 18.9% improvement with limited real data
  - [Figure 2] Intent distribution analysis shows synthetic data aligns with individual patterns, not population averages
- Break condition: If replacement scenario achieves <40% of real-data performance or augmentation degrades baseline performance, synthetic data fidelity is insufficient for that domain.

## Foundational Learning

- Concept: Two-stage behavior prediction paradigm (population pretraining → individual fine-tuning)
  - Why needed here: BehaviorGen's value proposition depends on understanding where synthetic data fits in this pipeline. Pretraining needs diversity; fine-tuning needs personalization.
  - Quick check question: Can you explain why adding synthetic data at pretraining vs. fine-tuning stages has different effects on model behavior?

- Concept: Prompt engineering with format constraints and role setting
  - Why needed here: The framework relies entirely on prompt design to coerce structured outputs from general-purpose LLMs. Without format restrictions, Pass@1 drops to 0%.
  - Quick check question: What happens to the output if you remove format restrictions from the prompt?

- Concept: Privacy-utility tradeoff metrics (uniqueness testing, membership inference, differential privacy)
  - Why needed here: The paper claims privacy preservation, but validation requires understanding overlap ratios, attack success rates, and privacy budgets (ε values).
  - Quick check question: If 80% of generated trajectories have <30% overlap with any real trajectory, what does this imply about copy-vs-generate behavior?

## Architecture Onboarding

- Component map: User Profiles + Sample Behaviors -> Prompt Constructor (role + format + segment logic) -> LLM Generator (gpt-4o-2024-0806) -> Synthetic Behavior Sequences [weekday, timestamp, location, intent] -> Downstream Models (PITuning / Bert4Rec) -> Three deployment modes: Pretrain-Aug / Finetune-Replace / Finetune-Aug

- Critical path:
  1. Profile extraction from user metadata (5 attributes)
  2. Sample behavior selection (~7 days for convergence per Appendix A.4)
  3. Prompt assembly with format constraints
  4. Weekly-segmented generation (minimum 90 lines/month)
  5. Validation: format parsing + distributional metrics (KS_P, JSD)

- Design tradeoffs:
  - More input data → better personalization but higher privacy exposure
  - Stricter format constraints → higher Pass@1 but may suppress natural variation
  - Weekly vs. daily segments: weekly balances context retention and diversity

- Failure signatures:
  - Pass@1 < 50%: Format constraints not enforced or LLM model changed
  - JSD > 0.1 between synthetic and real distributions: Profile conditioning failing
  - >50% trajectory overlap with training data: Model memorizing instead of generating
  - Replacement scenario <40% of real-data performance: Individual specificity not captured

- First 3 experiments:
  1. Ablation on profile attributes: Generate synthetic data with profile subsets (age-only, demographics-only, full profile) and measure KS_P/JSD to identify critical attributes for your domain.
  2. Segment length sweep: Test 1-day, 3-day, 7-day, 14-day segments and plot Pass@1 vs. sequence coherence metrics to validate weekly assumption for your data.
  3. Privacy-utility frontier: For replacement scenario, vary the amount of real data used for prompting (10, 50, 100, 200 logs) and plot performance gap vs. membership inference attack success rate.

## Open Questions the Paper Calls Out

- **Question:** How can fairness-aware techniques be effectively integrated into the data generation pipeline to mitigate biases stemming from both empirical seed data and the LLM's pretraining corpus?
- **Basis in paper:** [explicit] The authors identify two sources of bias (empirical data representation and LLM corpus) and list applying fairness-aware techniques as a planned mitigation strategy.
- **Why unresolved:** The current study focuses on generation quality and privacy, leaving the implementation and validation of bias mitigation methods as future work.
- **What evidence would resolve it:** Experiments demonstrating reduced demographic bias in the synthetic data without compromising the fidelity of behavioral patterns.

- **Question:** Can data-efficient generation methods be developed to reduce the dependency on massive behavioral datasets without compromising the performance of behavior prediction models?
- **Basis in paper:** [explicit] The authors state that "developing more data-efficient generation methods is crucial" to improve scalability and practicality.
- **Why unresolved:** Current behavior prediction scenarios typically involve large volumes of training data, and the proposed framework's efficiency in low-data regimes (beyond the noted "limited records" for fine-tuning) needs further exploration.
- **What evidence would resolve it:** Successful generation of high-quality synthetic data using significantly fewer real-world logs or tokens compared to the current methodology.

- **Question:** How can the underlying LLMs be specialized or improved to better understand and model the complex temporal dynamics of human daily activities?
- **Basis in paper:** [explicit] The authors suggest that "improving the underlying LLMs to better understand and model human daily activities will be key" to generating higher-quality synthetic data.
- **Why unresolved:** The study utilizes general-purpose models (GPT-4o), which may lack specialized mechanisms for capturing intricate short-term behavioral fluctuations.
- **What evidence would resolve it:** Comparative analysis showing that behavior-specialized LLMs outperform general-purpose LLMs in capturing non-routine or complex activity sequences.

## Limitations

- Privacy claims lack formal differential privacy guarantees, with membership inference attacks achieving 65% success rates
- Performance retention in replacement scenarios (62-88%) varies significantly across datasets, suggesting domain-specific limitations
- Framework effectiveness depends on behavioral pattern regularity and intent categorization granularity, limiting generalizability to more complex behavior spaces

## Confidence

**High Confidence**: Pretraining augmentation consistently improves population-level pattern coverage across both datasets. The ablation studies clearly demonstrate that profile conditioning is necessary for capturing individual specificity. The weekly segmentation strategy demonstrably prevents coherence collapse.

**Medium Confidence**: Fine-tuning augmentation achieves the largest performance gains (up to 18.9%), but this is measured on synthetic-to-synthetic evaluation rather than real-world deployment. The replacement scenario's 62-88% performance retention is encouraging but may not hold for more complex behavior prediction tasks or different LLM models.

**Low Confidence**: The privacy preservation claims lack formal guarantees. The 65% membership inference attack success rate, while below baseline, still indicates non-trivial information leakage. The framework's robustness to adversarial prompting or prompt injection attacks is untested.

## Next Checks

1. **Distributional Consistency Audit**: Generate synthetic data for 50 randomly selected users and perform KS tests against real data for each behavioral attribute (timestamp distributions, location transitions, intent frequencies). Report per-user p-values and identify failure modes.

2. **Cross-Domain Transferability**: Apply BehaviorGen to a different behavior prediction domain (e.g., web browsing logs or financial transaction sequences) with at least 1,000 users and 10 intent categories. Compare performance retention ratios across domains.

3. **Privacy Budget Quantification**: Implement formal differential privacy accounting for the generation process. Measure ε values under various data sharing scenarios and determine the minimum privacy budget required to maintain acceptable utility levels.