---
ver: rpa2
title: 'Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions'
arxiv_id: '2512.02682'
source_url: https://arxiv.org/abs/2512.02682
tags:
- agents
- safety
- risks
- interaction
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that single-agent safety mechanisms fail in
  multi-LLM ecosystems because local alignment does not prevent emergent collective
  risks. It proposes the Emergent Systemic Risk Horizon (ESRH) framework, which formalizes
  how systemic instability arises from interaction structure, not isolated misbehavior.
---

# Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions

## Quick Facts
- arXiv ID: 2512.02682
- Source URL: https://arxiv.org/abs/2512.02682
- Reference count: 12
- Single-agent safety mechanisms fail in multi-LLM ecosystems

## Executive Summary
This paper identifies a critical gap in current AI safety research: while single-agent alignment focuses on individual model behavior, multi-LLM systems face emergent risks that arise from interactions between agents. The authors propose the Emergent Systemic Risk Horizon (ESRH) framework, which demonstrates that systemic instability emerges from interaction structures rather than isolated misbehavior. They introduce Institutional AI, a governance architecture that embeds adaptive oversight, peer evaluation, and functional differentiation directly within multi-agent systems. This work shifts safety analysis from isolated models to system-level resilience, enabling new evaluation methods that capture collective behavior over time.

## Method Summary
The paper presents a conceptual framework combining theoretical analysis with a proposed governance architecture. The ESRH framework formalizes how systemic instability arises from interaction patterns, while Institutional AI provides a practical implementation approach. The methodology involves defining a three-tier taxonomy of risks (micro, meso, macro) and proposing governance mechanisms to address each level. However, the paper lacks empirical validation, relying instead on theoretical arguments and architectural proposals for future testing.

## Key Results
- Single-agent safety mechanisms are insufficient for multi-LLM ecosystems
- Systemic risks emerge from interaction structure, not isolated misbehavior
- Proposed Institutional AI governance architecture embeds oversight directly in multi-agent systems

## Why This Works (Mechanism)
The ESRH framework captures how multi-agent interactions create feedback loops and emergent behaviors that individual safety mechanisms cannot address. When multiple LLMs interact, local alignment (ensuring each agent behaves safely) does not prevent collective risks like semantic drift, coordination failures, or false consensus. The Institutional AI architecture works by distributing governance functions across the system, using peer evaluation and functional differentiation to create self-regulating mechanisms that adapt to emerging risks.

## Foundational Learning
- ESRH framework - Why needed: Predicts systemic instability from interaction patterns
  Quick check: Test framework against known multi-agent failure cases
- Three-tier risk taxonomy - Why needed: Classifies risks by scale and interaction type
  Quick check: Validate risk categories with multi-agent system practitioners
- Institutional AI governance - Why needed: Embeds oversight in system architecture
  Quick check: Compare governance effectiveness against baseline approaches

## Architecture Onboarding
Component map: Institutional AI -> Adaptive Oversight -> Peer Evaluation -> Functional Differentiation
Critical path: Risk detection -> Governance response -> System adaptation
Design tradeoffs: Centralized vs. distributed governance, real-time vs. batch evaluation
Failure signatures: Coordination breakdowns, governance bottlenecks, evaluation inconsistencies
First experiments:
1. Test ESRH predictions across varying interaction topologies
2. Measure semantic drift rates with different governance architectures
3. Evaluate Institutional AI's effectiveness against coordination failures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the need for empirical validation of the proposed frameworks.

## Limitations
- Lack of empirical validation for ESRH framework's predictive power
- Absence of quantitative measures for Institutional AI effectiveness
- Subjective boundaries between micro, meso, and macro risk categories
- Missing implementation details for practical deployment

## Confidence
High: Single-agent safety mechanisms are insufficient for multi-agent ecosystems
Medium: Three-tier taxonomy classification and general Institutional AI architecture
Low: ESRH framework's formal mathematical structure for predicting systemic instability

## Next Checks
1. Implement a simulation environment testing ESRH predictions across varying interaction topologies and agent configurations
2. Conduct controlled experiments measuring semantic drift and prompt infection rates in multi-agent systems with different governance architectures
3. Develop quantitative metrics to evaluate Institutional AI's effectiveness in preventing false consensus and coordination failures compared to baseline approaches