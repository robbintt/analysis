---
ver: rpa2
title: 'Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size
  with ResNets'
arxiv_id: '2511.20888'
source_url: https://arxiv.org/abs/2511.20888
tags:
- norm
- htmc
- which
- circuit
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a deep connection between deep neural networks
  and circuit complexity theory. It demonstrates that in the Harder than Monte Carlo
  (HTMC) regime, the set of real-valued functions computable within a certain error
  becomes convex, allowing for the definition of a HTMC norm on functions.
---

# Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets

## Quick Facts
- arXiv ID: 2511.20888
- Source URL: https://arxiv.org/abs/2511.20888
- Authors: Arthur Jacot
- Reference count: 40
- Primary result: Establishes convexity in HTMC regime linking ResNets to circuit complexity minimization

## Executive Summary
This paper bridges deep learning and circuit complexity theory by demonstrating that deep neural networks, particularly Residual Networks (ResNets), implicitly solve a convex version of the Minimum Circuit Size Problem (MCSP). The work establishes that in the Harder than Monte Carlo (HTMC) regime, the set of real-valued functions computable within a certain error becomes convex, allowing for the definition of a HTMC norm on functions. This norm is tightly related to the parameter complexity of ResNets through a sandwich bound, showing that minimizing the ResNet parameter complexity is equivalent to finding the minimal circuit size that fits the data. This theoretical framework provides insight into why DNNs are so successful at learning complex functions - they implement a computational Occam's razor by finding the simplest algorithm that fits the data.

## Method Summary
The paper establishes a deep connection between deep neural networks and circuit complexity theory through a novel mathematical framework. It introduces the concept of HTMC convexity, where the set of functions computable within a certain error bound becomes convex in the HTMC regime. This allows for the definition of a HTMC norm that characterizes function complexity. The authors then introduce Tetrakis functions as approximations of the vertices of the HTMC unit ball, which could enable convex optimization methods for circuit size minimization. The central result is a sandwich bound that tightly relates the HTMC norm to the ResNet parameter complexity, measured by a weighted ℓ1 norm of the network parameters.

## Key Results
- Establishes convexity of function spaces in the HTMC regime, enabling norm-based characterization of circuit complexity
- Proves a sandwich bound tightly relating HTMC norm to ResNet parameter complexity, connecting deep learning to MCSP
- Introduces Tetrakis functions as convex approximations for vertices of HTMC unit ball, suggesting new optimization approaches

## Why This Works (Mechanism)
The paper demonstrates that deep neural networks implement a computational Occam's razor by implicitly solving a convex version of the Minimum Circuit Size Problem. In the HTMC regime, the convexity property allows the complex discrete problem of finding minimal circuits to be approximated by a continuous optimization problem. ResNets, through their architectural design, naturally align with this convex structure, making them particularly effective at finding the simplest algorithm that fits the data. The sandwich bound shows that minimizing the ResNet parameter complexity directly corresponds to minimizing circuit size, providing theoretical justification for the empirical success of deep learning.

## Foundational Learning

**HTMC Regime**: The Harder than Monte Carlo regime where certain error bounds are achieved
- Why needed: Provides the specific conditions under which convexity emerges
- Quick check: Verify that the regime conditions match the network architecture and problem setup

**HTMC Norm**: A norm defined on functions computable within certain error bounds in the HTMC regime
- Why needed: Provides a measure of function complexity that connects to circuit size
- Quick check: Confirm the norm satisfies the properties of a mathematical norm (positivity, homogeneity, triangle inequality)

**Tetrakis Functions**: Approximations of the vertices of the HTMC unit ball
- Why needed: Enable convex optimization methods for circuit size minimization
- Quick check: Verify that Tetrakis functions properly approximate the vertices and maintain convexity properties

## Architecture Onboarding

**Component Map**: Input data → HTMC transformation → Convex function space → HTMC norm calculation → ResNet parameter optimization → Minimal circuit size approximation

**Critical Path**: The HTMC-convexity transformation is the critical path, as it enables the entire theoretical framework by making the discrete circuit complexity problem convex

**Design Tradeoffs**: The paper focuses on ResNets specifically, which may limit generalizability to other architectures like CNNs or transformers that have different parameter complexity measures

**Failure Signatures**: If the HTMC convexity assumptions are violated (e.g., wrong error bounds or architecture), the sandwich bound relationship breaks down and the connection to circuit complexity is lost

**First Experiments**:
1. Verify HTMC convexity empirically across different network depths and error tolerances
2. Test the sandwich bound relationship between HTMC norm and ResNet parameter complexity on benchmark datasets
3. Implement and benchmark Tetrakis-based optimization against standard training methods

## Open Questions the Paper Calls Out
None

## Limitations
- The convexity results are derived under specific assumptions about network architecture and error tolerance that may not generalize to all practical scenarios
- The sandwich bound is based on asymptotic analysis that may not fully capture finite-sample behavior
- The practical utility of Tetrakis functions for actual circuit size minimization remains theoretical without empirical validation

## Confidence
- Theoretical framework and HTMC convexity: Medium - mathematically rigorous but with restrictive assumptions
- Sandwich bound relationship: High - well-established under stated conditions
- Practical applicability of Tetrakis functions: Low - theoretical proposal without empirical validation

## Next Checks
1. Empirical testing of the convexity property across different HTMC regimes and network depths to verify the theoretical bounds hold in practice
2. Implementation and benchmarking of Tetrakis-based optimization methods against standard training approaches on real-world datasets
3. Extension of the analysis to alternative architectures (CNNs, transformers) to assess the generality of the HTMC-convexity connection