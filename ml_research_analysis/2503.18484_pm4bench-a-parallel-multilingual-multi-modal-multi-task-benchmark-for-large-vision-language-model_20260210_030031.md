---
ver: rpa2
title: 'PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large
  Vision Language Model'
arxiv_id: '2503.18484'
source_url: https://arxiv.org/abs/2503.18484
tags:
- arxiv
- vision
- performance
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PM4Bench addresses the challenge of fairly evaluating multilingual
  vision-language models by introducing a parallel corpus across 10 languages, eliminating
  content bias, and incorporating a vision setting where text and queries are embedded
  in images. The benchmark covers three tasks: MDUR (multi-discipline understanding
  and reasoning), MIQA (multi-image question answering), and MSOCR (multi-scale OCR
  challenge).'
---

# PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model

## Quick Facts
- **arXiv ID**: 2503.18484
- **Source URL**: https://arxiv.org/abs/2503.18484
- **Reference count**: 22
- **Primary result**: Introduces a parallel corpus across 10 languages to fairly evaluate multilingual vision-language models, revealing OCR bottlenecks and cross-lingual disparities.

## Executive Summary
PM4Bench is a novel benchmark designed to evaluate multilingual vision-language models (LVLMs) across three tasks—multi-discipline understanding (MDUR), multi-image question answering (MIQA), and multi-scale OCR challenge (MSOCR)—in both traditional and vision settings. The benchmark's key innovation is its parallel corpus design, which uses semantically identical content across 10 languages (English, Chinese, Korean, Thai, Vietnamese, Russian, Hungarian, Serbian, Czech, Arabic) to isolate language-specific capabilities. By embedding text in images for the vision setting, the benchmark exposes OCR as a critical bottleneck, revealing significant performance drops and cross-lingual disparities compared to traditional input methods.

## Method Summary
PM4Bench evaluates LVLMs on three tasks: MDUR (1730 MCQ/language from MMMU-pro), MIQA (218 open-ended QA/language from MMDU), and MSOCR (100 images/language with decreasing font sizes 40→2). The evaluation uses two input settings: traditional (separate text/image inputs) and vision (text embedded in images). Models are assessed via exact match for MDUR/MSOCR and LLM-as-a-judge (DeepSeek-v3.2) for MIQA across six dimensions. The benchmark includes 10 languages and evaluates 10 LVLMs including GPT-5 series, Gemini-3-Pro-Preview, and Qwen3-VL series. Performance is measured per-language with cross-lingual disparity quantified via coefficient of variation (S_cv = σ/μ × 100%).

## Key Results
- Vision setting causes significant performance drops compared to traditional setting, with OCR capability identified as the key bottleneck.
- Cross-lingual disparities are substantial, with Arabic showing particularly poor performance in vision setting.
- Scaling models improves cross-lingual consistency, but OCR remains a critical challenge across all model sizes.
- MSOCR scores correlate strongly with performance on other tasks, suggesting OCR capability is fundamental to LVLM performance.

## Why This Works (Mechanism)

### Mechanism 1: Parallel Corpus Design Isolates Cross-Lingual Capability
- Claim: A strictly parallel corpus enables accurate comparison of language processing abilities by removing content variance as a confounding variable.
- Mechanism: Using identical questions and images across all 10 languages ensures performance differences reflect the model's inherent language capacity rather than task difficulty variations.
- Core assumption: Translations are semantically equivalent and visual embedding is consistent across languages.
- Evidence anchors: Abstract states parallel corpus enables "fair and accurate cross-lingual comparisons"; section 3.1 emphasizes semantic identity across languages.

### Mechanism 2: Vision Setting Reveals OCR Bottlenecks
- Claim: Processing text queries visually exposes OCR capability as a critical bottleneck for downstream reasoning.
- Mechanism: Converting text to pixels forces models to first perform OCR before reasoning, creating a propagation failure point that degrades overall performance.
- Core assumption: Visual text representation introduces noise and additional processing steps that primarily cause performance degradation.
- Evidence anchors: Abstract identifies OCR as "key bottleneck"; section 5.4 distinguishes text processing mechanisms between settings.

### Mechanism 3: Scaling Mitigates Cross-Lingual Disparity via Improved Multimodal Perception
- Claim: Larger models disproportionately improve performance and consistency on OCR-dependent tasks, reducing cross-lingual gaps.
- Mechanism: Greater representational capacity allows models to learn more robust visual features for diverse scripts, lifting challenging vision-language task performance.
- Core assumption: Scale benefits apply more strongly to low-level visual perception than high-level reasoning.
- Evidence anchors: Abstract notes scaling "improves cross-lingual consistency"; section 5.3 shows S_cv consistently decreases with model size.

## Foundational Learning

- **Parallel Corpus in Evaluation**: Essential for understanding fair cross-lingual comparison; quick check: Can you definitively conclude a performance difference between English and Thai is due to language capability if different images/questions are used? Why or why not?

- **Input Modalities (Traditional vs. Vision Setting)**: Critical for interpreting performance differences; quick check: In the "vision setting," does the model receive the question as a text string? Describe what it actually receives.

- **OCR as a Sub-task in Multimodal Reasoning**: Key to understanding the identified bottleneck; quick check: A model in the "vision setting" receives an image of a math problem with text and gives the wrong answer. Describe how OCR failure could be the root cause even if the model is a good reasoner.

## Architecture Onboarding

- **Component map**: Data Source (MMMU-pro, MMDU) → Translation Pipeline (Kimi K2 → Human Experts → Claude Selection) → Data Rendering Engine (HTML-to-screenshot) → LVLM Under Test → Evaluator (Exact Match/LLM-as-Judge)
- **Critical path**: 1) Generate parallel data for MDUR in both settings. 2) Run LVLM inference. 3) Calculate performance drop and S_cv across languages. 4) Run OCR evaluation and correlate scores.
- **Design tradeoffs**: Parallel vs. authentic content (fairness vs. cultural authenticity); automated vs. human translation (cost vs. quality).
- **Failure signatures**: Large vision-traditional score drop indicates visual text perception weakness; high cross-lingual variance (S_cv) indicates inconsistent multilingual capabilities.
- **First 3 experiments**: 1) Reproduce OCR-Vision correlation between OCR-only accuracy and vision task scores (PCC > 0.5 confirms OCR bottleneck). 2) Ablate input modality to measure vision setting drop. 3) Test scaling on cross-lingual consistency by comparing S_cv across model sizes.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dedicated multilingual OCR training datasets enhance LVLM performance and reduce cross-lingual disparities? The paper identifies this as a key future direction but lacks empirical validation of whether OCR improvement via training directly translates to equitable performance gains across all evaluated languages.

- **Open Question 2**: Can MSOCR serve as an efficient proxy for approximating LVLM capabilities on complex vision-reasoning tasks? While MSOCR performance correlates with other tasks, the paper hasn't validated it as a predictive proxy on larger scale or wider language sets.

- **Open Question 3**: How does external OCR tool integration compare to internal model fine-tuning for mitigating vision setting performance drops? The paper suggests external tools as a cost-effective strategy but only tests theoretical upper bounds with ground-truth text injection.

## Limitations
- Translation quality and cultural adequacy across 10 languages were not quantitatively validated, potentially affecting semantic equivalence assumptions.
- Performance drop in vision setting could stem from either OCR failures or the model's inability to jointly process embedded text with other images, though the paper attributes it primarily to OCR.
- Cross-lingual consistency analysis compares different model families (GPT-5 vs Qwen3), introducing architectural confounding beyond pure scale effects.

## Confidence

- **High Confidence**: Performance drops in vision vs traditional settings and OCR identification as limiting factor for multilingual tasks.
- **Medium Confidence**: Parallel corpus design enables fair cross-lingual comparison by eliminating content bias.
- **Low Confidence**: Scaling disproportionately improves perceptual capabilities (OCR) over reasoning to reduce cross-lingual disparity.

## Next Checks

1. **Validate Translation Quality**: Implement blind human evaluation by bilingual experts assessing semantic equivalence and cultural appropriateness across all 10 languages with inter-rater reliability metrics.

2. **Isolate OCR vs Joint Processing**: Create controlled experiments where models receive (a) embedded text only, (b) images only, and (c) both together in vision setting to disentangle OCR capability from multimodal reasoning.

3. **Test Within-Model Scaling**: Evaluate multiple sizes of same LVLM family (e.g., Qwen3-VL-1.5B, 7B, 14B) on MSOCR to confirm improved cross-lingual consistency correlates with scale within single architecture, not just across families.