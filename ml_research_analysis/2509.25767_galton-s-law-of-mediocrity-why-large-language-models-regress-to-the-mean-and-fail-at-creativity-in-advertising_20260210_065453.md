---
ver: rpa2
title: 'Galton''s Law of Mediocrity: Why Large Language Models Regress to the Mean
  and Fail at Creativity in Advertising'
arxiv_id: '2509.25767'
source_url: https://arxiv.org/abs/2509.25767
tags:
- original
- extreme
- expansion
- creative
- mild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models generate fluent but generic text, especially
  in creative domains like advertising. We formalize this as Galton-style regression
  to the mean in language and test it using a staged forgetting and expansion protocol
  on 1,045 ad concepts.
---

# Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising

## Quick Facts
- **arXiv ID**: 2509.25767
- **Source URL**: https://arxiv.org/abs/2509.25767
- **Reference count**: 16
- **Primary result**: LLMs regress toward generic outputs in creative domains unless explicitly guided by creative markers

## Executive Summary
This paper demonstrates that large language models exhibit "Galton's Law of Mediocrity" when generating creative content, systematically reverting to generic, statistically probable outputs rather than preserving distinctive creative elements. Through a staged forgetting and expansion protocol applied to 1,045 advertising concepts, the study shows that creative features (metaphors, emotions, visual cues) disappear first during compression while factual content persists. During expansion, outputs become longer and more varied but fail to recover original meaning or creativity, with semantic similarity plateauing at 0.22-0.29. The study finds that providing explicit creative markers improves semantic alignment and stylistic balance, raising cosine similarity to 0.24-0.37 and METEOR to 0.13-0.26, though outputs still rely on familiar tropes.

## Method Summary
The study employs a two-phase creativity stress test on 1,045 advertising concepts from the 55mV Idea Database. Phase 1 involves staged forgetting: compressing ads at three levels (mild 35%, moderate 70%, extreme 95%) using temperature=0.7 across six models. Phase 2 expands from compressed inputs using plain expansion (no guidance) or marker-driven expansion (3-4 creative markers per ad). The chained design feeds outputs from mild→moderate→extreme stages. Evaluation uses four metrics: cosine similarity (TF-IDF), METEOR, entropy (Shannon over unigrams), and 4-gram uniqueness, plus qualitative analysis of marker survival and emergent idea types.

## Key Results
- During forgetting, creative elements (metaphors, emotions, visual cues) disappeared first while factual content remained, with emotional and metaphorical markers declining by 60-70% between mild and extreme stages
- Plain expansion produced lexically diverse outputs (entropy 5.60-6.20, n-gram uniqueness 0.99-1.00) but failed semantic recovery (cosine 0.22-0.29, METEOR 0.13-0.19)
- Marker-driven expansion improved semantic alignment (cosine 0.30-0.43, METEOR 0.21-0.33) but outputs still relied on familiar tropes

## Why This Works (Mechanism)

### Mechanism 1: Probability-Weighted Forgetting Hierarchy
- **Claim**: Under compression, LLMs retain high-probability factual content while discarding low-probability creative elements.
- **Mechanism**: Next-token prediction training privileges statistically common patterns; when asked to compress, models preserve "safe" information with high prior probability and drop rare expressions (metaphors, emotional cues, visual details).
- **Core assumption**: Compression prompts trigger the model's default bias toward high-likelihood outputs without explicit instruction to preserve creativity.
- **Evidence anchors**:
  - [abstract]: "When ad ideas were simplified step by step, creative features such as metaphors, emotions, and visual cues disappeared early, while factual content remained, showing that models favor high-probability information."
  - [Section 3.1.1]: Emotional and metaphorical markers declined by 60-70% between mild and extreme forgetting; survival scores fell to 0.28 and 0.33 respectively at extreme stage.
  - [corpus]: Related work (Wenger & Kenett, 2025) documents "creative homogeneity" across diverse LLMs—models converge on similar responses in divergent thinking tasks, consistent with regression toward high-probability outputs.

### Mechanism 2: Surface Novelty Without Semantic Recovery
- **Claim**: Plain expansion from compressed inputs produces lexically diverse outputs that lack meaningful alignment with original creative intent.
- **Mechanism**: Without external cues, expansion relies on the model's learned priors—generating fluent, varied text from the statistical space of advertising language rather than reconstructing the specific original idea.
- **Core assumption**: High entropy and n-gram uniqueness indicate surface-level variation but do not guarantee recovery of original semantic content or creative distinctiveness.
- **Evidence anchors**:
  - [Section 3.2.1]: Plain expansion achieved entropy values exceeding original baselines (5.60-6.20 vs. 5.31) and near-perfect n-gram uniqueness (0.99-1.00), yet cosine similarity remained low (0.22-0.29) and METEOR plateaued at 0.13-0.19.
  - [Section 3.4]: Qualitative analysis showed 71% of emergent ideas in plain expansion were metaphorical, often recycling familiar clichés ("whisper secrets," "crafted sanctuary") rather than genuine originality.
  - [corpus]: Related paper "Hallucination or Creativity" (arXiv:2602.02290) notes that standard metrics fail to capture whether generated content represents valuable creativity vs. superficial novelty.

### Mechanism 3: Marker-Guided Partial Restoration
- **Claim**: Providing explicit creative markers (extracted from originals) improves semantic alignment and stylistic balance during expansion, though familiar tropes persist.
- **Mechanism**: Markers act as external anchors that bias generation toward specific creative dimensions (visual, emotional, metaphorical), partially counteracting the model's default regression toward generic outputs.
- **Core assumption**: Markers reduce the search space for generation, guiding the model toward target content rather than relying solely on learned priors.
- **Evidence anchors**:
  - [abstract]: "Providing ad-specific cues such as metaphors, emotional hooks and visual markers improved alignment and stylistic balance, though outputs still relied on familiar tropes."
  - [Section 3.2.2]: Marker-driven expansion achieved higher cosine similarity (0.30-0.43 at extreme) and METEOR (0.21-0.33) compared to plain expansion, with GPT-5 Thinking showing strongest recovery (cosine 0.43, METEOR 0.33).
  - [Section 3.4 Figure 8]: Marker-driven prompting shifted creative type distribution—metaphorical ideas dropped from 71% to a more balanced mix including emotional, visual, and slogan-like content.

## Foundational Learning

- **Concept: Regression to the Mean (Galton's Law)**
  - **Why needed here**: The paper's central theoretical framing—explaining why LLM outputs converge on "safe, generic phrasing" rather than retaining extreme creative traits.
  - **Quick check question**: If you asked an LLM to summarize a highly unusual, creative advertisement and then expand it back, would you expect the result to match the original's distinctiveness? Why or why not?

- **Concept: Token-Level Probability and Entropy**
  - **Why needed here**: Explains the mechanism behind forgetting (high-probability tokens survive) and the evaluation metrics used (entropy measures lexical diversity).
  - **Quick check question**: Why might a high-entropy output still fail to recover an original idea's semantic content?

- **Concept: Semantic vs. Surface-Level Metrics**
  - **Why needed here**: The paper uses four distinct metrics (cosine similarity, METEOR, entropy, n-gram uniqueness) to differentiate between "sounds new" and "recovers meaning."
  - **Quick check question**: If an expansion has 99% n-gram uniqueness but only 25% cosine similarity to the original, what does this tell you about the output?

## Architecture Onboarding

- **Component map**: 55mV Idea Database -> Forgetting Compression (mild→moderate→extreme) -> Expansion (plain/marker-driven) -> Four Metrics + Qualitative Analysis
- **Critical path**:
  1. Extract original ad concepts from 55mV database.
  2. Run forgetting compression (3 levels × 3 runs × 6 models).
  3. Extract creative markers from originals for marker-driven condition.
  4. Run expansion from extreme forgetting outputs (plain + marker-driven).
  5. Compare all outputs against originals using four metrics.
  6. Conduct qualitative coding on sampled outputs for emergent idea types.
- **Design tradeoffs**:
  - Chained expansion vs. independent regeneration: Chained design reflects realistic iterative refinement but may propagate errors; independent regeneration would isolate each stage but lose the cumulative effect.
  - Marker extraction via LLM: Efficient but introduces potential extraction errors; manual extraction would be more accurate but not scalable.
  - TF-IDF embeddings for cosine similarity: Computationally efficient but may miss nuanced semantic relationships; sentence-level transformers would capture more meaning but increase cost.
- **Failure signatures**:
  - Plain expansion shows high entropy + low cosine similarity → surface novelty without semantic recovery.
  - Marker-driven expansion shows moderate gains but high n-gram uniqueness → markers guide content but model still defaults to familiar phrasing.
  - Model-specific: Gemini collapses fastest during forgetting; GPT-5 Thinking retains most during expansion.
- **First 3 experiments**:
  1. Reproduce Phase 1 forgetting on a sample of 20 ads across two model families; verify that creative markers (metaphor, emotion) decline faster than factual content using manual annotation.
  2. Test marker-driven vs. plain expansion on 10 held-out ads; compute all four metrics and compare against paper's reported ranges to validate the improvement signal.
  3. Vary marker specificity: Compare ad-specific markers (as used in paper) vs. generic creative cues (e.g., "use a metaphor") on 10 ads; measure whether specificity drives the observed gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the "regression to mediocrity" phenomenon persist in creative domains beyond advertising, such as fiction, poetry, or scientific ideation?
- **Basis in paper**: [explicit] The Conclusion lists "domain specificity (ads)" as a primary limitation of the current study.
- **Why unresolved**: The experiments were conducted exclusively on the 55mV Idea Database, limiting the generalizability of the findings to other creative fields.
- **What evidence would resolve it**: Replicating the forgetting-expansion stress test on diverse creative corpora (e.g., literary texts, design patents) to see if factual content still outlasts metaphorical content.

### Open Question 2
- **Question**: Can training objectives that explicitly penalize clichés and reward diversity successfully counteract the statistical pull toward the mean?
- **Basis in paper**: [explicit] The Discussion suggests future work should focus on "designing training objectives that reward diversity and penalize clichés."
- **Why unresolved**: The study evaluated existing state-of-the-art models via inference-time prompting but did not investigate how fundamental changes to training loss functions affect creative retention.
- **What evidence would resolve it**: Training a model with a diversity-sensitive loss function and comparing its performance on the forgetting stress test against standard likelihood-trained models.

### Open Question 3
- **Question**: How do prototype-distance scores or human judgments compare to lexical metrics (e.g., METEOR, entropy) in evaluating the recovery of true originality?
- **Basis in paper**: [explicit] The Discussion recommends "incorporating richer evaluation measures such as human judgments or prototype-distance scores."
- **Why unresolved**: The study relied heavily on automated metrics which struggled to distinguish between surface novelty and semantic depth.
- **What evidence would resolve it**: A comparative evaluation where human experts rate the "creative fidelity" of expansions, correlated against automated scores to validate or refute the metric trends.

## Limitations

- **Dataset Generalizability**: Findings based on 1,045 advertising concepts from proprietary database may not generalize to other creative domains or cultural contexts.
- **Metric Validity**: Four quantitative metrics may not fully capture nuanced dimensions of creativity; limited qualitative sampling (25 ads for marker survival, 100 for emergent ideas).
- **Model-Specific Effects**: Results show model variation but don't systematically explore whether differences reflect fundamental architectural variations or prompt sensitivity.

## Confidence

- **High Confidence**: The regression-to-mediocrity phenomenon is well-supported across multiple metrics and models. The forgetting hierarchy (creative elements disappearing before factual content) is consistently observed and aligns with theoretical expectations about probability-weighted information retention.
- **Medium Confidence**: The marker-driven improvement in semantic alignment is demonstrated but shows limited absolute gains. While cosine similarity improves from 0.22-0.29 to 0.30-0.43 and METEOR from 0.13-0.19 to 0.21-0.33, these remain modest improvements that don't fully restore original creative intent.
- **Low Confidence**: The claim that LLMs "fail at creativity" is interpretive rather than empirically proven. The study shows models produce generic outputs but doesn't definitively establish whether this represents true creative failure or simply different creative approaches than human-generated content.

## Next Checks

1. **Cross-Domain Replication**: Test the forgetting-expansion protocol on non-advertising creative content (technical documentation, poetry, scientific abstracts) to assess whether the regression-to-mean pattern holds across domains.

2. **Human Preference Study**: Conduct blinded evaluations where humans rate both original and regenerated creative concepts for novelty, coherence, and appeal. This would validate whether metric-based findings align with actual creative quality judgments.

3. **Prompt Engineering Ablation**: Systematically vary the compression and expansion prompts (temperature, specificity, explicit creativity instructions) to identify which prompt features most effectively preserve creative elements during the forgetting-expansion cycle.