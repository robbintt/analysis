---
ver: rpa2
title: Reusing Trajectories in Policy Gradients Enables Fast Convergence
arxiv_id: '2506.06178'
source_url: https://arxiv.org/abs/2506.06178
tags:
- trajectories
- policy
- estimator
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sample inefficiency of policy gradient
  (PG) methods in reinforcement learning by reusing past trajectories. The authors
  propose RT-PG (Reusing Trajectories - Policy Gradient), a novel algorithm that estimates
  the policy gradient using a power mean-corrected multiple importance weighting estimator
  to combine on-policy and off-policy data from recent iterations.
---

# Reusing Trajectories in Policy Gradients Enables Fast Convergence

## Quick Facts
- arXiv ID: 2506.06178
- Source URL: https://arxiv.org/abs/2506.06178
- Authors: Alessandro Montenegro; Federico Mansutti; Marco Mussi; Matteo Papini; Alberto Maria Metelli
- Reference count: 40
- One-line primary result: RT-PG achieves rO(ε⁻¹) sample complexity with full trajectory reuse, the best known for PG methods

## Executive Summary
This paper addresses the sample inefficiency of policy gradient methods in reinforcement learning by reusing past trajectories through a novel algorithm called RT-PG. The method combines on-policy and off-policy data using a power mean-corrected multiple importance weighting estimator to achieve faster convergence while using the same amount of fresh data. RT-PG demonstrates theoretical improvements in sample complexity from O(ε⁻²) to rO(ε⁻¹) and validates these gains through experiments showing superior performance on continuous control tasks compared to standard PG baselines and state-of-the-art variance-reduced methods.

## Method Summary
RT-PG (Reusing Trajectories - Policy Gradient) estimates policy gradients by combining fresh trajectories from the current policy with stored trajectories from recent iterations using a Multiple Power Mean (MPM) estimator. The algorithm maintains a sliding window buffer of size ω containing trajectories collected from past policies, along with their associated parameters. For each stored trajectory, it computes adaptive coefficients based on estimated χ²-divergence between policies, then aggregates weighted gradient estimates using power mean correction. The policy is updated via gradient ascent using these combined estimates, with trajectories older than ω iterations being pruned. The method requires careful tuning of window size, batch size, and learning rate to balance sample efficiency gains against computational overhead and potential instability from importance weight estimation.

## Key Results
- Achieves rO(ε⁻²ω⁻¹) sample complexity for reaching ε-approximate stationary point, improving over standard PG's O(ε⁻²)
- With full trajectory reuse (ω = K), achieves rO(ε⁻¹) rate, the best known in literature for PG methods
- Significant empirical improvement on continuous control tasks (CartPole, HalfCheetah-v4, Swimmer-v4) over both standard PG and state-of-the-art variance-reduced methods
- Demonstrates faster convergence using same fresh data budget, confirming theoretical sample efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reusing off-policy trajectories via importance weighting provides unbiased gradient estimates under specific independence conditions
- Mechanism: MIW estimator computes weighted gradients using past policy trajectories with importance weights correcting for distribution mismatch
- Core assumption: Target parameter θ is statistically independent of history H_k; bounded χ² divergence between trajectory distributions
- Evidence anchors:
  - [Section 2] Defines MIW estimator with importance weights
  - [Section 3] Fact 3.1 shows target bias when θ_k depends on history; Fact C.1 proves unbiasedness when θ is history-independent
  - [corpus] Related work on off-policy actor-critic confirms importance of handling off-policy data properly

### Mechanism 2
- Claim: Power mean correction bounds importance weights, trading small bias for controlled variance reduction
- Mechanism: PM correction replaces vanilla IWs with harmonic mean bounded by 1/λᵢ,ₖ, preventing unbounded weights from causing gradient estimate explosions
- Core assumption: Coefficients λᵢ,ₖ must scale appropriately with sample size N and window size ω
- Evidence anchors:
  - [Section 4] "The PM-corrected weight is bounded by 1/λᵢ,ₖ"
  - [Section 5, Theorem 5.2] Derives concentration bound for MPM estimator
  - [corpus] Divergence-augmented methods similarly control policy ratio variance

### Mechanism 3
- Claim: Theoretical analysis via covering arguments controls target bias when parameters depend on learning history
- Mechanism: Since θ_k lies within a ball centered at θₖ₀ with radius determined by step size and window size, uniform concentration over covering bounds error at history-dependent target
- Core assumption: Estimation error is Lipschitz in θ; policies satisfy smoothness conditions
- Evidence anchors:
  - [Section 5, Theorem 5.3] "To control the target bias, we analyze sup_{θ∈Θₖ} ||p̂^MPM J(θ) - ∇J(θ)||₂"
  - [Appendix D, Lemma D.3] Proves Lipschitz constant for MPM estimation error
  - [corpus] Evidence weak—covering argument techniques are standard but not explicitly discussed

## Foundational Learning

- Concept: **Importance Sampling for Off-Policy Evaluation**
  - Why needed here: RT-PG's core operation is reweighting off-policy trajectories to estimate on-policy gradients; understanding IW properties is prerequisite
  - Quick check question: If behavioral policy πθᵢ has zero probability of taking action a in state s, but target policy πθ assigns positive probability, what happens to the importance weight for trajectories containing that (s,a) pair?

- Concept: **Martingale Concentration (Freedman's Inequality)**
  - Why needed here: Paper's theoretical analysis relies on Freedman's inequality to bound estimation error for MPM estimator under sequential learning
  - Quick check question: In a martingale difference sequence, why does bounding both the maximum value M and cumulative variance V matter for concentration?

- Concept: **Sample Complexity Analysis for Non-Convex Optimization**
  - Why needed here: Paper's main claim is improved sample complexity; understanding stationary points and convergence rates is essential
  - Quick check question: What does it mean to find an ε-approximate stationary point, and why is this the standard convergence criterion for policy gradient methods?

## Architecture Onboarding

- Component map: Policy Updater -> MPM Estimator -> Divergence Estimator -> Trajectory Buffer
- Critical path:
  1. Collect N fresh trajectories with current policy πθₖ
  2. For each stored trajectory τᵢ,ⱼ (i ∈ [k-ωₖ+1, k]), compute: (a) log-probability under current policy, (b) divergence estimate D̂ᵢ, (c) coefficients αᵢ,ₖ, λᵢ,ₖ
  3. Aggregate MPM gradient estimate using all trajectories in window
  4. Update policy parameters
  5. Prune trajectories older than ω iterations

- Design tradeoffs:
  - **Window size ω**: Larger ω → better sample efficiency (Õ(ε⁻²ω⁻¹)) but higher memory (O(ε⁻¹) trajectories) and per-iteration compute
  - **Batch size N**: Must scale with dΘ/D; smaller N reduces compute but risks λᵢ,ₖ > 1 (invalid correction)
  - **Coefficient design**: Theoretical coefficients require unknown divergence bound D; practical adaptive coefficients introduce estimation variance

- Failure signatures:
  - **Unbounded gradients**: If λᵢ,ₖ too small or divergence underestimated, PM correction fails → IW explosion → gradient norm spikes
  - **Stagnation**: If step size too small relative to window, old trajectories provide redundant information, slowing progress
  - **Memory blowup**: Full reuse (ω = K) requires storing O(ε⁻¹) trajectories—impractical for long training runs

- First 3 experiments:
  1. **Sanity check**: Implement MPM estimator with fixed λᵢ,ₖ = 1.0 (no correction) on simple environment (CartPole); verify gradient variance reduction vs. GPOMDP with same total trajectory budget
  2. **Ablation on window size**: Test ω ∈ {1, 2, 4, 8} with fixed N, measuring convergence speed vs. wall-clock time to find practical sweet spot
  3. **Divergence estimation validation**: Compare theoretical coefficients (using hand-tuned D) vs. adaptive coefficients; measure variance of gradient estimates across random seeds to quantify added estimation noise

## Open Questions the Paper Calls Out

- Can the bounded χ²-divergence assumption be relaxed for RT-PG without degrading sample complexity?
- Can theoretical guarantees be extended to support dynamic, adaptive coefficient design used in practical implementation?
- Is it possible to achieve dimension-free convergence rate for RT-PG while reusing trajectories?

## Limitations

- Theoretical guarantees rely heavily on bounded χ²-divergence assumption that may not hold for policies with complex or rapidly changing action distributions
- Analysis assumes independent trajectory samples and policy smoothness that practical deep RL policies may violate
- Convergence rates assume optimal window sizes and batch sizes that depend on unknown constants, making practical tuning challenging
- Theoretical window size of O(ε⁻¹) for full reuse may be impractical due to memory constraints in long training runs

## Confidence

**High Confidence** (Mechanism 1 - Importance Weighting): Well-established RL literature, mathematically rigorous unbiasedness proof under independence conditions

**Medium Confidence** (Mechanism 2 - Power Mean Correction): Sound theoretical bounds, but practical effectiveness depends on accurate divergence estimation and appropriate coefficient scaling

**Medium Confidence** (Mechanism 3 - Covering Arguments): Standard non-convex optimization technique, but specific application to policy gradient methods with sequential learning introduces complexities

## Next Checks

1. **Empirical robustness test**: Run RT-PG with varying divergence estimation quality (adding noise to D̂ᵢ) to quantify sensitivity to estimation errors and validate stability of adaptive coefficients

2. **Memory-accuracy tradeoff**: Systematically vary window size ω from 1 to 50 on HalfCheetah-v4, measuring both convergence speed and wall-clock time to identify practical sweet spot where benefits plateau

3. **Gradient variance analysis**: Compare gradient variance (across multiple seeds) between RT-PG with adaptive coefficients versus fixed theoretical coefficients to quantify cost of adaptive estimation in terms of variance inflation