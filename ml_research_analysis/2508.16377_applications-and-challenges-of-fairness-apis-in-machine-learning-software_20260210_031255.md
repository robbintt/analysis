---
ver: rpa2
title: Applications and Challenges of Fairness APIs in Machine Learning Software
arxiv_id: '2508.16377'
source_url: https://arxiv.org/abs/2508.16377
tags:
- fairness
- bias
- apis
- repositories
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application and challenges of fairness
  APIs in machine learning software. We analyze 204 GitHub repositories that utilize
  13 fairness APIs to understand their real-world use cases and challenges.
---

# Applications and Challenges of Fairness APIs in Machine Learning Software

## Quick Facts
- **arXiv ID:** 2508.16377
- **Source URL:** https://arxiv.org/abs/2508.16377
- **Reference count:** 40
- **Primary result:** Analysis of 204 GitHub repositories reveals fairness APIs are used for learning/exploration (70.59%) and real-world problems (29.4%), with developers facing significant challenges in understanding fairness concepts and selecting appropriate metrics.

## Executive Summary
This empirical study investigates how fairness APIs are applied in real-world machine learning software and the challenges developers face. By analyzing 204 GitHub repositories using 13 fairness libraries and 4,212 GitHub issue discussions, the research identifies that fairness APIs serve primarily for learning and exploration or solving real-world problems across 17 distinct applications. The study reveals that developers predominantly rely on utility metrics (accuracy, false positives) rather than specialized fairness metrics for bias detection, favor in-processing mitigation strategies, and struggle significantly with requirement analysis and understanding fairness concepts. These findings highlight critical gaps in fairness API design, documentation, and education that must be addressed to improve real-world adoption.

## Method Summary
The study collected GitHub repositories using GitHub Code Search API with queries for 13 specific fairness libraries, filtering to 204 Python repositories that actively used these APIs. Researchers conducted open card sorting to classify use cases, performed AST analysis to verify API usage, and applied LDA topic modeling (MALLET) with T=33 topics on 4,212 GitHub issue discussions from 11 API libraries. The methodology included coherence score optimization (0.53) and manual topic merging to identify developer challenges across the software development lifecycle.

## Key Results
- **Use Case Distribution:** 70.59% of repositories use fairness APIs for learning/exploration, while 29.4% address real-world problems across 17 unique applications
- **Bias Detection Approach:** 83.33% of non-generic repositories detect bias using utility metrics (accuracy, false positives) rather than specialized fairness metrics
- **Mitigation Strategy:** In-processing approaches dominate (44.8% usage), with Fairness Constraints being the most common specific method (20.7%)
- **Developer Challenges:** Requirement Analysis-related topics show highest coverage (55%), particularly "Opinion" seeking and "Understanding Fairness Definition"

## Why This Works (Mechanism)

### Mechanism 1: The Utility-Metric Proxy Effect
- Claim: Developers substitute complex fairness metrics with familiar utility metrics to bridge the gap between standard ML evaluation and fairness auditing
- Core assumption: Developers prioritize metrics they already understand and can easily map to existing dashboards over theoretically purer but complex fairness definitions
- Evidence: RQ2 results show 83.33% of projects detect bias using utility metrics while individual/subgroup metrics were non-existent

### Mechanism 2: The In-Processing Integration Loop
- Claim: In-processing mitigation strategies are favored because they integrate fairness constraints directly into model optimization, keeping solutions within developer's immediate control
- Core assumption: Practitioners prefer modifying training code—a familiar workflow—over pre-processing data pipelines or post-processing outputs
- Evidence: RQ3 identifies In-processing as most common approach (44.8%), with Fairness Constraints accounting for 20.7% of usage

### Mechanism 3: The Requirement Ambiguity Bottleneck
- Claim: Primary friction point is translating abstract ethical requirements into concrete API configurations, causing bottleneck in Requirement Analysis phase
- Core assumption: Lack of standardized fairness definitions for specific domains forces developers to rely on community consensus rather than definitive documentation
- Evidence: RQ4 shows "Requirement Analysis Related Topics" have highest coverage (55%), specifically "Opinion" and "Understanding Fairness Definition" sub-topics

## Foundational Learning

- **Concept: Group vs. Individual Fairness**
  - Why needed: Study reveals massive gap in usage; developers exclusively use Group fairness while Individual/Subgroup fairness usage is near zero
  - Quick check: Can you identify a scenario where a model is "fair" to Gender A vs. B (Group), but discriminates against a specific individual within Group A (Individual)?

- **Concept: ML Pipeline Stages (Pre/In/Post-processing)**
  - Why needed: Paper maps API usage to specific pipeline stages; knowing where bias enters vs. where it is mitigated is essential for diagnosing why repos use different approaches
  - Quick check: If you only have access to trained model and outputs, but not training data, which mitigation category must you use?

- **Concept: SDLC in ML (Software Development Life Cycle)**
  - Why needed: RQ4 demonstrates fairness challenges span entire SDLC, with "Requirement Analysis" and "Maintenance" being high-friction areas
  - Quick check: Why might "fixing warnings" be considered Maintenance challenge rather than Development challenge in context of evolving fairness libraries?

## Architecture Onboarding

- **Component map:** Raw Data + Sensitive Attributes -> Pre-processing Module (Reweighing/Disparate Impact Remover) -> Training Core (Model + In-processing Constraints) -> Post-processor (Calibrated Equalized Odds/Reject Option Classification) -> Evaluation (Utility Metrics + Group Fairness Metrics)

- **Critical path:** Data Ingest -> Requirement Analysis (Defining Protected Groups) -> Pre-processing (Optional) -> In-processing (Dominant path) -> Evaluation

- **Design tradeoffs:**
  - In-processing vs. Pre-processing: In-processing offers tighter integration but requires model retraining; Pre-processing allows data reuse but may lose predictive signal
  - Utility vs. Fairness Metrics: Relying on Utility metrics (easier) risks missing structural bias; using specialized Group metrics requires deep domain knowledge

- **Failure signatures:**
  - "Opinion" Loop: High volume of "Opinion" (20%) and "Dataset Usage" (33%) issues without resolution indicates stalled onboarding
  - Utility-Only Audits: Systems that only track Utility metrics (83.33% of cases) may pass internal QA while failing fairness audits
  - Installation Drift: 12% of issues related to installation/deployment suggest environment fragility

- **First 3 experiments:**
  1. Baseline Audit: Run standard prediction task using only Utility metrics to establish "fairness-blind" baseline
  2. Metric Correlation Check: Compare Utility metrics against Group Fairness metrics on same dataset to verify if "Utility-Metric Proxy" holds or fails
  3. In-processing Intervention: Implement basic Fairness Constraint (e.g., Exponentiated Gradient) and measure drop in Utility vs. gain in Fairness

## Open Questions the Paper Calls Out

- **Open Question 1:** Why are individual and subgroup fairness metrics largely ignored in real-world open-source projects compared to group fairness metrics?
  - Basis: RQ2 summary states future studies should explore challenges of developing and adopting finer-level bias detection metrics since they were non-existent in dataset
  - Why unresolved: Paper observes lack of usage but doesn't empirically determine root cause (computational cost vs. lack of awareness)
  - Resolution evidence: Interviews with developers or quantitative analysis of performance trade-offs between group and individual metrics

- **Open Question 2:** How can developers systematically select most effective mitigation strategy for specific application contexts?
  - Basis: RQ3 summary highlights need to study which mitigation approach is better in given scenario to guide developers, as multiple approaches are currently used for identical use cases
  - Why unresolved: Current usage is mixed and no clear guidelines exist in studied repositories for selecting one algorithm over another
  - Resolution evidence: Comparative benchmarking study of mitigation effectiveness across different domains (e.g., healthcare vs. legal)

- **Open Question 3:** What specific barriers prevent adoption of advanced mitigation techniques, such as adversarial learning, in open-source software?
  - Basis: Authors note several advanced algorithms were never used and suggest investigating whether methods are underused due to complexity, lack of documentation, or perceived ineffectiveness
  - Why unresolved: Study identifies absence of usage but cannot distinguish cause purely from static code analysis
  - Resolution evidence: Survey of ML practitioners focusing on usability and documentation gaps for these specific algorithms

## Limitations
- Qualitative nature of topic merging in RQ4 introduces subjectivity; mapping from 33 LDA topics to 10 final categories depends on human judgment not fully specified
- GitHub Code Search API's dynamic results mean exact reproducibility requires original search snapshot or replication package
- Focus on Python repositories may not generalize to fairness API usage in other programming languages or enterprise settings

## Confidence

- **High Confidence:** Taxonomy of use cases (Learning vs. Real-world) and dominance of utility metrics for bias detection (83.33%) are well-supported by quantitative data
- **Medium Confidence:** Interpretation of developer challenges as primarily requirement analysis bottlenecks is supported but relies on qualitative topic modeling that could be interpreted differently
- **Medium Confidence:** Conclusion that in-processing is dominant mitigation strategy is data-driven but may reflect API availability rather than practitioner preference

## Next Checks

1. Replicate the LDA topic modeling with same hyperparameters (T=33, α=50/T, β=0.01) on GitHub issue corpus to verify coherence score and topic stability
2. Conduct focused survey of developers from the 204 repositories to validate whether identified challenges (particularly "Opinion seeking" and "Understanding Fairness Definition") align with their actual experience
3. Test the utility-fairness correlation on subset of real-world applications to empirically verify whether "Utility-Metric Proxy" mechanism actually corresponds to fair outcomes in practice