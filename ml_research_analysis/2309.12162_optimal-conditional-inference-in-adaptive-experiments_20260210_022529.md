---
ver: rpa2
title: Optimal Conditional Inference in Adaptive Experiments
arxiv_id: '2309.12162'
source_url: https://arxiv.org/abs/2309.12162
tags:
- inference
- conditional
- assumption
- note
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inference in batched adaptive experiments, focusing
  on conditional inference given the experimental design (stopping time, assignment
  probabilities, and target parameter). The authors show that without restrictions,
  only last-batch inference is optimal.
---

# Optimal Conditional Inference in Adaptive Experiments

## Quick Facts
- **arXiv ID**: 2309.12162
- **Source URL**: https://arxiv.org/abs/2309.12162
- **Reference count**: 40
- **Primary result**: Without restrictions, only last-batch inference is optimal in adaptive experiments; location-invariant designs allow recovery of earlier batch information via a sufficient statistic, improving inference by 10-50% in median confidence interval length.

## Executive Summary
This paper studies conditional inference in batched adaptive experiments where data-dependent stopping times and assignment probabilities complicate standard statistical inference. The authors show that under general adaptivity, only using the final batch of data is optimal for conditional inference. However, for location-invariant experimental designs, they identify a sufficient statistic combining the last batch with a linear combination of earlier batches, enabling more powerful inference. They also characterize optimal inference for polyhedral algorithms and prove uniform asymptotic validity over non-Gaussian data.

## Method Summary
The method involves conditional inference given the realized experimental design (stopping time, assignment probabilities, and target parameter). For location-invariant designs, a precision-weighted sum of batch-arm means (the "Leftover" statistic L) forms a sufficient statistic with the last batch mean. The optimal estimator combines these using generalized least squares. For polyhedral algorithms, conditional distributions are derived via truncated Gaussian sampling based on linear constraints defining the selection events.

## Key Results
- Without restrictions on the adaptive design, only last-batch inference is optimal and cannot be improved
- For location-invariant designs, a sufficient statistic (L, X_T) exists, enabling 10-50% improvement in median confidence interval length
- Polyhedral conditioning provides optimal inference for algorithms with linear selection constraints
- All proposed methods achieve uniform asymptotic validity over non-Gaussian data

## Why This Works (Mechanism)

### Mechanism 1: The Optimality of Last-Batch-Only Under General Adaptivity
- **Claim**: In the absence of restrictions on the experimental design, inference using only the last batch of data is optimal and cannot be improved upon.
- **Mechanism**: Conditional validity given the target parameter η requires validity given the entire history of pre-final batch data, effectively neutralizing historical data.
- **Core assumption**: The experimental design chooses the target parameter η as an arbitrary function of the history.
- **Evidence anchors**: Lemma 2.1 establishes that conditional validity given η requires validity given X_{1:T-1}.
- **Break condition**: If the design is restricted such that η is not an arbitrary function of history, this mechanism ceases to be binding.

### Mechanism 2: Recovery of "Leftover" Information via Location Invariance
- **Claim**: If the experimental design is location-invariant, a scalar statistic L exists that captures information from earlier batches orthogonal to the data used for adaptive decisions.
- **Mechanism**: Location-invariant algorithms base decisions on contrasts (differences between arms). A precision-weighted sum of batch-arm means, L, is independent of these contrasts and forms a sufficient statistic with the last batch.
- **Core assumption**: Assignment probabilities, stopping time, and target are measurable with respect to batch-arm differences.
- **Evidence anchors**: Theorem 3.7 states that (L, X_T) is sufficient for μ with respect to the conditional distribution X_{1:T} | (T, ΔX_{1:T-1}).
- **Break condition**: If the design depends on the absolute level of means, the distribution of L given the design is no longer free of μ.

### Mechanism 3: Polyhedral Conditioning for Known Algorithms
- **Claim**: When the adaptive design depends on the data through linear inequalities, optimal conditional inference is computationally tractable via truncated normal distributions.
- **Mechanism**: Discrete selection events define a polyhedron in which the data must fall. Conditioning on this polyhedral event derives the distribution of the test statistic without discarding the direction of selection.
- **Core assumption**: The stopping time and target parameter depend on the data only through polyhedral events.
- **Evidence anchors**: Theorem 5.3 derives the conditional distribution as a truncated Gaussian.
- **Break condition**: If the selection rule involves non-linear constraints or unknown decision logic, the polyhedral formulation is invalid.

## Foundational Learning

- **Sufficient Statistics**
  - **Why needed here**: The core theoretical advance relies on identifying a pair (L, X_T) that is sufficient for the parameter μ given the constraints.
  - **Quick check question**: If you condition on a statistic S, can the remaining data distribution still depend on the parameter μ?

- **Adaptive Experiments (Bandits)**
  - **Why needed here**: The problem setting is defined by data-dependent sampling probabilities. Standard CLT-based inference fails due to circular dependency: outcomes → policy → outcomes.
  - **Quick check question**: Why does the independence assumption required for the standard Central Limit Theorem break in a Thompson Sampling experiment?

- **Selective Inference (Conditioning)**
  - **Why needed here**: The paper prioritizes conditional validity (coverage given the specific design realized) over unconditional validity.
  - **Quick check question**: Why is it risky to report an unconditionally valid confidence interval if the data generating process suggests you are in a subset of the sample space where the method is known to undercover?

## Architecture Onboarding

- **Component map**: Data → Invariance Check → Compute L (if location-invariant) → Calculate GLS estimator S* → Construct CIs → Evaluate coverage
- **Critical path**: Accurate estimation of the variance Σ is vital. The statistic L and the weights in the Polyhedral method both scale with precision (1/σ²). Errors here propagate non-linearly.
- **Design tradeoffs**:
  - *Leftover vs. Polyhedral*: "Leftover" is robust to incomplete knowledge of the algorithm (only requires location invariance). "Polyhedral" is more powerful but brittle.
  - *Conditional vs. Unconditional*: Gaining conditional validity costs interval width compared to methods like ZJM, which are only unconditionally valid.
- **Failure signatures**:
  - Location Invariance Violation: If assignment probabilities depend on raw magnitude, the "Leftover" L statistic will be biased.
  - Polyhedral Mis-specification: Using the Polyhedral method with non-linear constraints or unknown decision logic will result in incorrect conditioning sets.
  - Pruning Failure: The asymptotics require "pruning" small assignment probabilities in the last batch. If an arm gets 1 sample in the last batch, variance estimation breaks down.
- **First 3 experiments**:
  1. **Sanity Check (Simulation)**: Implement the "Leftover" estimator S* on a 2-batch Thompson sampling simulation. Verify that the average confidence interval width is ≈10% smaller than "Last-batch-only".
  2. **A/B Test (Algorithms)**: Run "Leftover" vs. "Polyhedral" on a synthetic ε-greedy dataset. Confirm "Polyhedral" is narrower, but verify both maintain nominal coverage.
  3. **Stress Test (Robustness)**: Test the "Leftover" method on a non-location-invariant design. Measure the coverage drop to quantify the cost of assumption violation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the optimal inference procedures for polyhedral algorithms be adapted for high-dimensional settings with a large number of arms or batches?
- **Basis in paper**: The paper notes the procedure is computationally efficient "at least for moderately-sized (T, K)" (p. 19).
- **Why unresolved**: The method relies on Gibbs sampling for truncated Gaussians, which faces computational hurdles in high dimensions.
- **What evidence would resolve it**: Derivation of approximate algorithms or complexity bounds for high-dimensional polyhedral constraints.

### Open Question 2
- **Question**: In what practical settings might batch-wise statistics follow non-Gaussian exponential family distributions?
- **Basis in paper**: Remark 5.1 notes that while the general recipe extends to exponential families, "a non-Gaussian exponential family structure for batchwise statistics would be hard to motivate in practice."
- **Why unresolved**: The standard motivation relies on the Central Limit Theorem for batch means, making non-Gaussian cases theoretically possible but practically obscure.
- **What evidence would resolve it**: Identification of specific experimental designs or data types where non-Gaussian sufficient statistics naturally arise.

### Open Question 3
- **Question**: How significant are the efficiency gains when applying Rao-Blackwellization to standard unconditional estimators using the sufficient statistics derived in this paper?
- **Basis in paper**: Remark 5.2 states that the estimator from Zhang et al. (2020) "is thus dominated by its conditional expectation given the sufficient statistics (i.e. Rao–Blackwellization)."
- **Why unresolved**: The paper focuses on establishing the dominance and validity of conditional procedures rather than explicitly deriving the improved unconditional estimator.
- **What evidence would resolve it**: Derivation and finite-sample comparison of the Rao-Blackwellized estimator's risk against standard unconditional procedures.

## Limitations
- The conditional inference framework hinges critically on accurate knowledge of the adaptive design, making it brittle to violations of location invariance or mis-specification of selection rules.
- The polyhedral approach requires exact specification of linear constraints and is computationally intensive for large numbers of arms or batches.
- The asymptotics require pruning small assignment probabilities, which may not be feasible in low-budget experimental settings.

## Confidence

- **High Confidence**: The optimality of last-batch-only inference under general adaptivity (Lemma 2.1 is straightforward).
- **Medium Confidence**: The sufficiency of (L, X_T) under location invariance (Theorem 3.7 is rigorous but depends on correct variance estimation).
- **Medium Confidence**: The computational tractability of polyhedral conditioning (Theorem 5.3 is correct but requires careful implementation).

## Next Checks

1. **Coverage Robustness Test**: Implement the "Leftover" method on a non-location-invariant design and measure the drop in coverage to quantify the cost of violating the core assumption.

2. **Polyhedral Sensitivity Analysis**: Run the polyhedral inference on a simulated ε-greedy experiment with known selection rules. Systematically mis-specify the linear constraints and measure the impact on coverage and interval length.

3. **Low-Budget Stress Test**: Test both the "Leftover" and polyhedral methods in a setting with very small last-batch sample sizes (no pruning possible). Assess whether the variance estimation breaks down and coverage fails.