---
ver: rpa2
title: 'GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems
  Across Different Scales'
arxiv_id: '2506.06634'
source_url: https://arxiv.org/abs/2506.06634
tags:
- geld
- neural
- time
- instances
- tsps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GELD, a unified neural TSP solver that effectively
  addresses both small- and large-scale instances. The core innovation is a broad
  global assessment and refined local selection framework, combining a lightweight
  Global-view Encoder with a heavyweight Local-view Decoder.
---

# GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales

## Quick Facts
- arXiv ID: 2506.06634
- Source URL: https://arxiv.org/abs/2506.06634
- Reference count: 14
- GELD solves TSP with up to 744,710 nodes without divide-and-conquer, outperforming state-of-the-art models in both solution quality and inference speed.

## Executive Summary
This paper introduces GELD, a unified neural TSP solver that effectively addresses both small- and large-scale instances. The core innovation is a broad global assessment and refined local selection framework, combining a lightweight Global-view Encoder with a heavyweight Local-view Decoder. The encoder employs a novel low-complexity attention mechanism (RALA) to efficiently capture global node features, while the decoder uses local search within a restricted candidate set to accelerate decision-making. A two-stage training strategy enhances generalization across scales. Extensive experiments on synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models in both solution quality and inference speed. Notably, GELD can solve TSPs with up to 744,710 nodes without relying on divide-and-conquer strategies. Additionally, GELD functions as a post-processing method to significantly improve existing solutions with minimal extra computation time.

## Method Summary
GELD employs an encoder-decoder architecture with a lightweight Global-view Encoder using Region-Average Linear Attention (RALA) and a heavyweight Local-view Decoder with restricted candidate sets. The model uses a two-stage training strategy: Stage 1 applies supervised learning on TSP-100 instances, while Stage 2 employs self-improvement learning with curriculum scaling from 100 to 1,000 nodes using beam search and parallel reconstruction. The decoder operates autoregressively, selecting the next node from its k-nearest neighbors based on attention scores that integrate distance information. A key innovation is the dynamic normalization during post-processing, which re-scales node coordinates to diversify model inputs and improve solutions iteratively.

## Key Results
- Solves TSP instances with up to 744,710 nodes without divide-and-conquer strategies
- Outperforms seven state-of-the-art models on both solution quality and inference speed
- Achieves average gap of 1.26% relative to optimal/LKH3 solutions
- Functions as effective post-processing method to improve existing solutions

## Why This Works (Mechanism)

### Mechanism 1: Region-Average Linear Attention (RALA)
RALA enables scaling to extreme graph sizes (e.g., 744k nodes) by reducing the quadratic bottleneck of standard attention to linear complexity. Instead of computing an n×n attention matrix, RALA partitions nodes into m regions (e.g., a 3×3 grid). It creates regional "proxies" by averaging node embeddings per region. Nodes attend to these proxies rather than every other node, effectively compressing global context into m channels. The core assumption is that the global topological information required for TSP can be effectively approximated by coarse-grained regional averages rather than fine-grained pairwise node interactions.

### Mechanism 2: Refined Local Selection (Constrained Decoder)
Restricting the decoder's decision space to a local neighborhood accelerates inference and improves solution quality by reducing search noise. The Local-view Decoder does not consider all n nodes. Instead, it masks the probability distribution to only the k-nearest neighbors of the current node. This aligns with the "refined local selection" principle, focusing high-compute attention layers only on relevant candidates. The core assumption is that optimal TSP moves are predominantly local in Euclidean space, and the global context provided by the Encoder is sufficient to guide the high-level direction without the Decoder needing a global view.

### Mechanism 3: Input Diversification for Post-Processing
Treating the model as a dynamic function rather than a fixed mapping allows it to iteratively improve solutions via Re-Construction (RC). GELD normalizes node coordinates based on the current sub-problem (randomly selected sub-solution). During RC, this re-normalization shifts the coordinate space and alters the RALA regional assignments. This "diversifies" the model's input, forcing the attention mechanism to view the same tour differently and potentially correct previous errors. The core assumption is that neural models are sensitive to coordinate normalization and regional partitioning; shifting these parameters creates a valid exploration path for improvement.

## Foundational Learning

- **Concept: Encoder-Decoder Attention Architectures**
  - Why needed here: GELD relies on a split architecture where the Encoder builds a static representation (Global) and the Decoder constructs the solution step-by-step (Local).
  - Quick check question: How does separating the "global assessment" (Encoder) from the "local selection" (Decoder) differ from a standard Transformer that does both simultaneously?

- **Concept: Complexity in Attention Mechanisms**
  - Why needed here: The primary innovation is RALA, which claims O(n) complexity. Understanding standard O(n²) self-attention is required to appreciate why the paper partition nodes into regions.
  - Quick check question: Why does standard attention scale quadratically with the number of nodes, and how does summarizing nodes into regions avoid this?

- **Concept: Autoregressive vs. Non-Autoregressive Decoding**
  - Why needed here: The Local-view Decoder is autoregressive (selecting nodes one by one).
  - Quick check question: What is the trade-off between the inference speed of autoregressive models versus the potential for error accumulation, which GELD attempts to fix with RC?

## Architecture Onboarding

- **Component map:** Input -> Normalizer (Dynamic based on sub-solution) -> Global-view Encoder (GE) (Linear Projection -> RALA (Partition m, Proxy Avg, Linear Attention) -> Global Embeddings) -> Local-view Decoder (LD) (k-Nearest Neighbor Selector -> Heavyweight Attention (RMSNorm + Distance Matrix injection) -> Softmax -> Next Node) -> Post-Processor (Re-Construction (RC) loop using the Normalizer to jitter inputs)

- **Critical path:** The RALA partition logic. The code must deterministically map normalized coordinates to one of the m×m regions and compute the average proxy. If this mapping is buggy or gradients don't flow through the averaging operation correctly, the "Global Assessment" fails, and the Decoder operates blindly.

- **Design tradeoffs:**
  - m (Regions): Small m (e.g., 1) maximizes speed but loses local nuance. Large m approaches standard attention complexity.
  - k (Local Range): Small k maximizes speed but restricts the search space, potentially trapping the tour in local optima.
  - Training: Two-stage training is computationally expensive (SL + SIL) compared to single-stage methods, but required for cross-scale generalization.

- **Failure signatures:**
  - OOM (Out of Memory) on large graphs: If RALA is implemented incorrectly or m is too large, complexity reverts to quadratic.
  - Disjointed Tours: If k is too small or Encoder weights are untrained, the Decoder may select nodes that appear locally optimal but create dead-ends.
  - Stagnation in RC: If normalization doesn't sufficiently alter the regional assignments, the post-processing loop will yield zero improvement.

- **First 3 experiments:**
  1. Complexity Verification: Benchmark inference time and memory for RALA vs. Standard Attention on random graphs (N=100 to N=100,000) to confirm the O(n) vs O(n²) curve.
  2. Ablation on k: Run inference on TSP-5000 with varying k (e.g., 20, 50, 100, 200) to plot the Pareto frontier of Speed vs. Tour Quality.
  3. RC Validation: Take a fixed sub-optimal tour, apply GELD-RC with and without the dynamic normalization step to quantify the "diversified input" gain.

## Open Questions the Paper Calls Out

### Open Question 1
Can the GELD framework be effectively adapted to solve complex vehicle routing problems (VRPs) with side constraints, such as the Capacitated Vehicle Routing Problem (CVRP)? The conclusion states: "Going forward, we plan to extend the capability of GELD to solve more complex COPs, such as the capacitated vehicle routing problem." This remains unresolved as the current study focuses exclusively on the fundamental Euclidean TSP, which lacks the capacity constraints, multiple vehicles, and depot logic inherent in CVRPs.

### Open Question 2
Does incorporating cross-distribution instances during training significantly improve GELD's generalization to out-of-distribution (OOD) patterns? In Section 5.2, the authors hypothesize that "Incorporating cross-distribution instances during training may further enhance our model's performance in this regard, which we plan to explore in future research." The current model is trained on a single (uniform) distribution, and while it outperforms baselines, the authors note a slight performance decrease on non-training distributions (clustered, explosion).

### Open Question 3
Can the number of regions (m) in the RALA mechanism be dynamically adjusted or made instance-adaptive to optimize the trade-off between local granularity and computational efficiency? Section 5.4 identifies a trade-off: smaller m favors efficiency while larger m prioritizes local details. The authors currently fix m to 9 via hyperparameter tuning. A static m assumes a "one-size-fits-all" granularity, potentially missing nuances in highly clustered instances or wasting computation on uniform ones.

## Limitations
- RALA mechanism's effectiveness depends on appropriate regional partitioning - if optimal tours require fine-grained long-range dependencies, regional averaging could degrade performance
- Locality assumption for the decoder may fail on non-Euclidean or specially constructed TSP instances
- Two-stage training strategy is computationally expensive compared to single-stage approaches

## Confidence
- **High confidence** in overall performance claims based on extensive benchmarking against seven state-of-the-art models
- **Medium confidence** in RALA mechanism's theoretical advantages, as implementation details are somewhat abstracted
- **Medium confidence** in post-processing effectiveness, as mechanism relies on specific coordinate normalization behaviors

## Next Checks
1. **Complexity Verification**: Benchmark inference time and memory for RALA vs. standard attention on graphs ranging from 100 to 100,000 nodes to empirically confirm the claimed O(n) vs O(n²) scaling.

2. **RALA Ablation**: Systematically vary the number of regions (m) from 1×1 to 10×10 on TSP-5000 instances to quantify the trade-off between computational efficiency and solution quality.

3. **Locality Assumption Test**: Generate TSP instances with known long-range dependencies (e.g., "hidden path" problems) and test GELD's performance with varying k values to identify the breaking point of the local selection assumption.