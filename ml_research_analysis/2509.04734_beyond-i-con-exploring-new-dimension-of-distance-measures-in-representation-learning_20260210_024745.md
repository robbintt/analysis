---
ver: rpa2
title: 'Beyond I-Con: Exploring New Dimension of Distance Measures in Representation
  Learning'
arxiv_id: '2509.04734'
source_url: https://arxiv.org/abs/2509.04734
tags:
- learning
- divergence
- i-con
- loss
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the I-Con framework by replacing KL divergence
  with alternative f-divergences in representation learning. The authors propose Beyond
  I-Con as a systematic approach to explore different statistical divergences for
  optimization objectives.
---

# Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning

## Quick Facts
- **arXiv ID:** 2509.04734
- **Source URL:** https://arxiv.org/abs/2509.04734
- **Reference count:** 5
- **Primary result:** Beyond I-Con achieves state-of-the-art clustering (68.40±0.29 Hungarian accuracy) and improves supervised learning accuracy by replacing KL divergence with alternative f-divergences

## Executive Summary
This paper challenges the assumption that KL divergence is the optimal choice for representation learning objectives. By systematically exploring alternative f-divergences within the I-Con framework, the authors demonstrate that different statistical divergences can yield superior performance across multiple tasks. The work shows that KL divergence's unbounded nature leads to gradient instability and the crowding problem in dimensionality reduction, while bounded divergences like Total Variation and Jensen-Shannon provide more stable optimization and better-separated clusters. The empirical results span unsupervised clustering of DINO-ViT embeddings, supervised contrastive learning on CIFAR-10, and SNE visualization, establishing a new paradigm for divergence selection in representation learning.

## Method Summary
The paper extends the I-Con framework by replacing KL divergence with alternative f-divergences in three distinct representation learning tasks. The approach modifies the original I-Con loss functions to incorporate Total Variation distance, Hellinger distance, and Jensen-Shannon divergence. For unsupervised clustering, the authors adapt the PMI algorithm to work with f-divergences and train a linear classifier on DINO-ViT embeddings. In supervised contrastive learning, they replace the standard KL-based contrastive loss with JSD while maintaining the same architecture and training protocol. For SNE dimensionality reduction, they implement bounded divergences to mitigate the crowding problem, comparing cluster separation against KL-based SNE. The systematic evaluation demonstrates that divergence choice significantly impacts performance across different representation learning paradigms.

## Key Results
- PMI clustering with Total Variation distance achieves 68.40±0.29 Hungarian accuracy on ViT-L/14 DINO embeddings (state-of-the-art)
- Supervised contrastive learning with JSD achieves 90.84±0.11 test accuracy versus 90.03±0.14 for KL-based methods
- SNE with bounded f-divergences produces better-separated clusters than KL-based SNE, solving the crowding problem

## Why This Works (Mechanism)
The paper identifies two primary mechanisms through which alternative f-divergences improve representation learning. First, KL divergence's unbounded nature causes large gradient spikes during optimization, leading to unstable training dynamics. Second, KL's tendency to over-penalize dissimilar points creates the crowding problem in dimensionality reduction, where clusters overlap and become indistinguishable. Bounded divergences like TV and JSD provide more stable gradients and prevent excessive penalties for dissimilar points, resulting in better-separated clusters and more reliable optimization.

## Foundational Learning

**f-divergence mathematics:** Understanding the family of f-divergences (TV, Hellinger, JSD) and their mathematical properties, particularly boundedness and gradient behavior. Needed because different divergences have distinct optimization characteristics that impact representation quality.

**Quick check:** Verify that TV and JSD are bounded in [0,1] while KL is unbounded, and understand how this affects gradient magnitudes during training.

**I-Con framework basics:** Grasping how the Information Contrastive (I-Con) framework unifies representation learning objectives through divergence minimization. Needed because Beyond I-Con builds directly on this foundation and modifies its core components.

**Quick check:** Confirm understanding of how I-Con optimizes a lower bound on mutual information using contrastive objectives.

**Crowding problem in dimensionality reduction:** Understanding why KL divergence causes points to collapse into dense regions in low-dimensional spaces. Needed because this is the primary motivation for using bounded divergences in SNE.

**Quick check:** Visualize KL-SNE embeddings showing overlapping clusters versus well-separated clusters with bounded divergences.

## Architecture Onboarding

**Component map:** Data → Embedding Extractor (DINO-ViT) → Divergence-based Objective → Linear Classifier/Probe → Evaluation Metrics. For SupCon: Data → ResNet-50 → JSD Contrastive Loss → Linear Probe → Accuracy. For SNE: Data → CNN → SNE Loss (f-divergence) → 2D/3D Visualization → Cluster Separation Analysis.

**Critical path:** Embedding extraction → Divergence-based loss computation → Gradient backprop → Parameter update → Evaluation. The divergence choice directly impacts gradient stability and final representation quality.

**Design tradeoffs:** KL divergence provides theoretical guarantees for certain objectives but suffers from unbounded gradients and crowding. Bounded divergences offer more stable optimization at the potential cost of some theoretical properties. Task-specific divergence selection (TV for clustering, JSD for supervised learning) optimizes for empirical performance rather than theoretical consistency.

**Failure signatures:** Gradient explosion during early training indicates KL divergence issues. Overlapping clusters in SNE visualizations confirm the crowding problem. Poor clustering accuracy suggests suboptimal divergence-task pairing.

**First experiments:** 1) Implement SupCon with KL vs JSD on CIFAR-10 and compare linear probe accuracy. 2) Run PMI clustering with TV divergence on DINO-ViT embeddings and measure Hungarian accuracy. 3) Visualize SNE embeddings using KL vs bounded divergences on CIFAR-10 and quantitatively assess cluster separation.

## Open Questions the Paper Calls Out

**Open Question 1:** Is the superior performance of bounded divergences primarily attributable to mitigating the "crowding problem" or to improved gradient stability during optimization?

**Basis in paper:** The authors hypothesize two distinct causes in Section 5: (1) KL unboundedness causes crowding by over-penalizing dissimilar points, and (2) KL causes unstable gradients (Figure 2).

**Why unresolved:** The paper presents evidence for both (visual separation in Figure 1 and gradient spikes in Figure 2) but does not isolate the causal contribution of either factor to the final accuracy gains.

**What evidence would resolve it:** An ablation study using divergence variants that decouple boundedness from gradient variance, or explicit metrics quantifying cluster overlap vs. gradient stability correlations with performance.

**Open Question 2:** Can theoretical principles be established to map specific divergence measures to optimal task performance (e.g., Total Variation for clustering vs. JSD for supervised learning)?

**Basis in paper:** The results show different divergences excel at different tasks (TV wins in Table 1/Clustering; JSD wins in Table 2/Supervised), but the paper provides no theoretical explanation for why specific divergences suit specific I-Con applications.

**Why unresolved:** The work serves as an empirical "roadmap" demonstrating that divergence choice matters, but lacks a theoretical framework to predict the optimal divergence a priori.

**What evidence would resolve it:** A theoretical analysis linking the statistical properties of the target task's label distribution (e.g., uniform vs. long-tail) to the geometric properties of the chosen f-divergence.

**Open Question 3:** Do the benefits of non-KL divergences extend to the broader set of I-Con methods, such as masked image modeling or language modeling?

**Basis in paper:** The paper tests only three applications (clustering, SupCon, SNE) and focuses on vision, whereas the original I-Con framework unified over 23 methods across modalities.

**Why unresolved:** The authors demonstrate the approach on specific vision tasks, leaving the efficacy of Beyond I-Con on other major representation learning paradigms (like BERT-style masked modeling) untested.

**What evidence would resolve it:** Applying the Beyond I-Con framework to masked autoencoder (MAE) or language modeling objectives to observe if JSD/TV provide similar convergence or representational benefits.

## Limitations

- Critical implementation details for f-divergence loss formulations and normalization procedures are underspecified
- Exact PMI clustering algorithm modifications and probability distribution computations remain unclear
- Results depend on specific data augmentations and CNN architectures not fully described in the paper

## Confidence

**High confidence** in theoretical framework and mathematical derivations
**Medium confidence** in reported clustering results (PMI with TV divergence achieving 68.40±0.29 accuracy)
**Medium confidence** in SupCon results (JSD achieving 90.84±0.11 accuracy) due to potential implementation variations
**Low confidence** in SNE visualization claims without access to exact CNN architecture and methodology

## Next Checks

1. **Implement and compare all three divergences (KL, TV, JSD) for PMI clustering** on ViT-L/14 embeddings, verifying that TV divergence consistently outperforms KL across multiple runs

2. **Reproduce the gradient stability analysis** by plotting gradient norms during early training for KL vs bounded divergences in the SupCon framework, confirming that bounded divergences show reduced gradient spikes

3. **Visualize SNE embeddings for all divergences** on CIFAR-10 and quantitatively measure cluster separation using silhouette scores or similar metrics to validate the crowding problem solution claim