---
ver: rpa2
title: Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans
arxiv_id: '2512.23693'
source_url: https://arxiv.org/abs/2512.23693
tags:
- preference
- response
- feedback
- responses
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fine-grained human feedback approach for
  aligning large language models (LLMs) using localized text span annotations. Instead
  of global A/B preference judgments, human annotators highlight "liked" and "disliked"
  spans within model responses and provide categorical rationales for their preferences.
---

# Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans

## Quick Facts
- arXiv ID: 2512.23693
- Source URL: https://arxiv.org/abs/2512.23693
- Reference count: 17
- Introduces fine-grained human feedback using localized text span annotations for LLM alignment

## Executive Summary
This paper presents a novel approach to aligning large language models through fine-grained human feedback on text spans. Instead of traditional global preference judgments, annotators highlight specific "liked" and "disliked" spans within model responses and provide categorical rationales. The model then incrementally rewrites disliked spans, creating an improvement chain where each adjacent step forms a preference pair for alignment training. Experiments demonstrate that models trained on stepwise improvement pairs outperform those trained on traditional preference pairs or full contrastive rewrites, achieving higher Elo scores in both human and automated evaluations.

## Method Summary
The approach involves annotators marking preferred and non-preferred text spans within model responses, along with categorical reasons for their choices. The base model then generates incremental rewrites of the disliked spans, creating a chain of improvements. Each adjacent pair in this chain serves as a preference pair for direct alignment training. This method captures more nuanced preferences than global judgments and produces more effective preference pairs for model training, resulting in better alignment with less annotation overhead.

## Key Results
- Models trained on stepwise improvement pairs achieved higher Elo scores than those trained on traditional preference pairs
- The approach demonstrates greater sample efficiency in preference tuning
- Fine-grained span-level feedback yields stronger alignment performance in both human and automated evaluations

## Why This Works (Mechanism)
The method works by breaking down preference judgments into localized, actionable feedback that directly targets specific problematic text spans. By creating incremental improvement chains rather than just contrasting good and bad examples, the model receives clearer supervision signals about how to improve. This granular approach allows the model to learn specific transformation patterns and understand the reasoning behind preferences through categorical rationales, leading to more effective alignment.

## Foundational Learning

### Text Span Annotation
- **Why needed**: To provide precise, localized feedback rather than ambiguous global judgments
- **Quick check**: Annotators can consistently identify and categorize problematic spans across multiple responses

### Incremental Rewrite Chains
- **Why needed**: To create meaningful intermediate training examples that show gradual improvement
- **Quick check**: Each rewrite step produces a measurable quality improvement over the previous version

### Preference Pair Generation
- **Why needed**: To convert qualitative feedback into quantitative training signals for model alignment
- **Quick check**: Adjacent pairs in improvement chains consistently show the preferred option winning in pairwise comparisons

## Architecture Onboarding

### Component Map
Human Annotation -> Span Classification -> Incremental Rewrite -> Preference Pair Generation -> Model Training

### Critical Path
Human annotator identifies disliked spans → Model generates rewrite → Preference pair created → Model training on pairs

### Design Tradeoffs
- **Granularity vs. overhead**: More detailed span-level feedback provides better signals but requires more annotator effort
- **Incremental vs. complete rewrites**: Stepwise improvements create better training pairs but may miss alternative solutions
- **Categorical rationales**: Add interpretability but introduce potential subjectivity

### Failure Signatures
- Inconsistent span annotation across different annotators
- Incremental rewrites fail to improve quality monotonically
- Preference pairs become ambiguous or contradictory

### First Experiments
1. Test annotator consistency on identifying and categorizing spans
2. Validate that incremental rewrites consistently improve output quality
3. Compare Elo scores between models trained on stepwise vs. traditional preference pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Human annotator subjectivity and variability in feedback quality
- Linear improvement assumption may miss alternative high-quality solutions
- Limited qualitative assessment of real-world application effectiveness

## Confidence

**High**: The incremental rewrite method produces more effective preference pairs than traditional pairwise or contrastive methods (supported by Elo score improvements)

**Medium**: The sample efficiency gains are significant across all tested domains (results show improvement but may depend on specific task characteristics)

**Medium**: The span-level feedback captures more nuanced preferences than global judgments (methodologically sound but needs broader validation)

## Next Checks

1. Conduct ablation studies removing the categorical rationale component to isolate its contribution to model performance

2. Test the approach across diverse domains beyond the current experimental setup to assess generalizability

3. Implement a long-term stability test to verify that models trained with stepwise improvements maintain quality over extended use periods