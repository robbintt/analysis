---
ver: rpa2
title: 'Reasoning with Sampling: Your Base Model is Smarter Than You Think'
arxiv_id: '2510.14901'
source_url: https://arxiv.org/abs/2510.14901
tags:
- sampling
- arxiv
- base
- grpo
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a training-free sampling algorithm that significantly\
  \ boosts single-shot reasoning performance of base language models. The core idea\
  \ is to sample from a \"power distribution\" p^\u03B1 (\u03B11), which sharpens\
  \ the base model distribution to favor higher-likelihood reasoning traces."
---

# Reasoning with Sampling: Your Base Model is Smarter Than You Think

## Quick Facts
- arXiv ID: 2510.14901
- Source URL: https://arxiv.org/abs/2510.14901
- Reference count: 40
- Key outcome: Training-free sampling algorithm boosts base model reasoning performance to match or exceed RL-posttraining while maintaining diversity

## Executive Summary
This paper presents a training-free inference-time method that significantly improves single-shot reasoning performance of base language models. The core innovation is sampling from a "power distribution" p^α (α>1) using Metropolis-Hastings Markov Chain Monte Carlo, which sharpens the base model distribution to favor higher-likelihood reasoning traces. Across multiple base models and reasoning tasks including MATH500, HumanEval, and GPQA Diamond, the method achieves near-equal or superior performance compared to state-of-the-art RL-posttraining while maintaining generation diversity.

## Method Summary
The method samples from p^α using MCMC with random resampling proposals. It decomposes generation into blocks, running N_MCMC Metropolis-Hastings iterations per block. For each iteration, it randomly selects a position, resamples the suffix using the base model as proposal, and accepts/rejects based on likelihood ratios. The block-sequential approach avoids poor initialization and reduces mixing time. Key hyperparameters: α=4.0, B=192, N_MCMC=10, τ=0.25 for reasoning tasks.

## Key Results
- Matches or exceeds GRPO RL-posttraining on MATH500, HumanEval, and GPQA Diamond
- Maintains generation diversity while improving accuracy
- Achieves superior out-of-domain generalization compared to RL-trained models
- Requires ~9× more tokens but no additional training compute

## Why This Works (Mechanism)

### Mechanism 1: Power Distribution Sharpening
Sampling from p^α upweights tokens with fewer but higher-likelihood future completions, favoring reasoning paths that avoid "critical windows" leading to low-quality outputs. This differs from low-temperature sampling which uses exponent of sums rather than sum of exponents. The power distribution computes next-token weights as Σ p(complete_sequence)^α, while low-temperature uses (Σ p(complete_sequence))^α.

### Mechanism 2: Metropolis-Hastings for Autoregressive Models
MCMC approximates sampling from p^α without computing the intractable normalization constant. Randomly select position t, resample from that position using proposal LLM, accept/reject based on likelihood ratio A(x',x) = min(1, p^α(x')·q(x|x') / p^α(x)·q(x'|x)). Satisfies irreducibility and aperiodicity requirements for convergence.

### Mechanism 3: Block-Sequential Progressive Sampling
Decomposing generation into blocks with intermediate distributions ∅→p(x_0:B)^α→...→p(x_0:T)^α avoids poor initialization and reduces mixing time. Sample from π_k (distribution over kB tokens), use as initialization for MCMC targeting π_{k+1}. Each stage runs N_MCMC resampling steps before extending.

## Foundational Learning

- **Metropolis-Hastings MCMC**: Core algorithm enabling sampling from unnormalized p^α without computing partition functions. Quick check: Given proposal q(x'|x) and target π(x), write the MH acceptance ratio. What happens if π is only known up to a constant?
- **Autoregressive Language Models**: Understanding p(x_t|x_{<t}) decomposition and how joint p(x_0:T) factors; needed to understand why low-temp ≠ power sampling. Quick check: Why does sampling token-by-token from p(x_t|x_{<t})^α not sample from the joint p(x)^α?
- **Distribution Annealing/Tempering**: Provides intuition for why p^α sharpens distributions; connects to diffusion literature. Quick check: As α→∞, what happens to the mode structure of p^α? What value of α balances quality vs. diversity?

## Architecture Onboarding

- **Component map**: Proposal LLM -> Power evaluator -> MCMC loop -> Block scheduler
- **Critical path**: Initialize empty sequence → For each block k: autoregressively extend B tokens → Run N_MCMC iterations: sample index m∈[1,kB], resample suffix, compute acceptance, accept/reject → Fix prefix, proceed to next block → Return final sequence
- **Design tradeoffs**: Larger B reduces blocks but requires more N_MCMC; higher α increases sharpening but risks mode collapse; higher N_MCMC improves approximation but increases compute ~N_MCMC×T²/4B
- **Failure signatures**: Stuck sequences (acceptance rate near 0 suggests α too high), no improvement over base (N_MCMC too low or α too close to 1), excessive compute (B too small), coherent but wrong (base likelihood doesn't track correctness)
- **First 3 experiments**: Validate MH implementation on toy binary sequences; hyperparameter sweep on α∈{2,3,4,5} and N_MCMC∈{2,5,10} on 100 MATH problems; diversity check generating 16 samples per problem computing pass@k curves

## Open Questions the Paper Calls Out

1. **Exponential mixing times**: The authors warn that exponential mixing time is the main downside of MCMC algorithms, exacerbated by high dimensionality. The experiments use T≈680 and B=192; it's untested if efficiency holds for much longer sequences (4k+ tokens).

2. **RL baseline comparison**: The paper benchmarks against GRPO models trained only on MATH dataset. It's unclear if the "outperformance" is due to the sampling method or the narrowness of the RL baseline's training data. A comparison against a general-purpose RL-posttrained model (trained on math, code, and instruction data) remains untested.

3. **Compute efficiency**: The paper calculates 8.84× generation overhead but only compares against RL (training-time compute). The trade-off between MCMC's "free" generation and Best-of-N's verifier-heavy approach remains unquantified. A Pareto frontier analysis comparing accuracy per FLOP is needed.

4. **Optimal α selection**: The paper notes choosing α=4.0 empirically, suggesting optimal value requires tuning per model rather than deriving from theory. It's unknown if efficacy correlates with base model architecture characteristics like pre-training dataset entropy or model size.

## Limitations
- Assumes base model likelihood correlates with reasoning quality without direct validation
- Requires ~9× more token generation, potentially prohibitive for longer sequences
- Block size B=192 appears somewhat arbitrary without sensitivity analysis across different model sizes
- Computational overhead scales as N_MCMC·T²/4B, making longer sequences expensive

## Confidence

- **High confidence**: The core MCMC algorithm correctly implements sampling from p^α (verified through Proposition 1 and empirical results). The mathematical framework is sound.
- **Medium confidence**: The empirical performance gains are real and reproducible across benchmarks, though the mechanism explanation could be more rigorously validated.
- **Low confidence**: The claim that this "reveals" underutilized capabilities in base models—this could alternatively be interpreted as a different decoding strategy that happens to work well.

## Next Checks

1. **Correlation validation**: On a held-out set of 100 problems, measure the actual correlation between base model log-likelihoods and ground-truth correctness. Compute Pearson/Spearman coefficients to quantify whether higher likelihood truly tracks better reasoning quality.

2. **Mechanism ablation**: Compare three variants on the same benchmark: (a) standard low-temperature sampling, (b) true power distribution sampling (current method), and (c) importance-weighted sampling that upweights correct reasoning traces. This would isolate whether the benefit comes from power distribution sharpening specifically versus general likelihood weighting.

3. **Scaling analysis**: Measure accuracy and computational cost across varying sequence lengths (e.g., T∈{512, 1024, 2048, 3072}) to understand how the N_MCMC·T²/4B scaling affects real-world utility. Plot accuracy vs. FLOPs to identify the sweet spot for different deployment scenarios.