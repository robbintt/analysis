---
ver: rpa2
title: 'D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented
  Reasoning'
arxiv_id: '2601.08282'
source_url: https://arxiv.org/abs/2601.08282
tags:
- question
- retrieval
- answer
- plan
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces D\xB2Plan, a dual-agent dynamic global planning\
  \ paradigm designed to address two critical failure modes in search-augmented LLMs:\
  \ ineffective search chain construction and reasoning hijacking by peripheral evidence.\
  \ The method employs a Reasoner agent that constructs and dynamically adapts explicit\
  \ global plans during multi-hop reasoning, and a Purifier agent that assesses retrieval\
  \ relevance and distills key information."
---

# D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning

## Quick Facts
- arXiv ID: 2601.08282
- Source URL: https://arxiv.org/abs/2601.08282
- Reference count: 40
- Key outcome: Achieves 3.8% average absolute improvement over state-of-the-art baselines across six QA benchmarks

## Executive Summary
This paper introduces D$^2$Plan, a dual-agent dynamic global planning paradigm designed to address two critical failure modes in search-augmented LLMs: ineffective search chain construction and reasoning hijacking by peripheral evidence. The method employs a Reasoner agent that constructs and dynamically adapts explicit global plans during multi-hop reasoning, and a Purifier agent that assesses retrieval relevance and distills key information. A two-stage training framework combines supervised fine-tuning on synthesized trajectories with reinforcement learning using plan-oriented rewards. Experimental results demonstrate that D$^2$Plan achieves an average absolute performance improvement of 3.8% over state-of-the-art baselines across six challenging QA benchmarks, while enabling more coherent query formulation and stronger resilience to irrelevant information.

## Method Summary
D$^2$Plan addresses retrieval-augmented reasoning failures through a dual-agent architecture. The Reasoner agent constructs explicit global plans and dynamically adapts them during multi-hop reasoning, while the Purifier agent evaluates retrieval relevance and distills key information. The system uses a two-stage training approach: first supervised fine-tuning on synthesized trajectories, then reinforcement learning with plan-oriented rewards. The dynamic planning component allows the system to adjust its reasoning strategy based on retrieved evidence, while the Purifier ensures that only relevant information influences the reasoning process. This architecture specifically targets the problems of ineffective search chain construction and reasoning hijacking that commonly plague search-augmented LLMs.

## Key Results
- Achieves 3.8% average absolute performance improvement over state-of-the-art baselines across six QA benchmarks
- Demonstrates more coherent query formulation compared to single-agent approaches
- Shows stronger resilience to irrelevant information during reasoning processes

## Why This Works (Mechanism)
D$^2$Plan works by explicitly separating the reasoning and purification functions into two specialized agents. The Reasoner constructs and dynamically adapts global plans, allowing for flexible reasoning paths that can adjust to new evidence. The Purifier acts as a quality control mechanism, filtering out irrelevant or misleading information that could hijack the reasoning process. This separation of concerns allows each agent to specialize in its core function - the Reasoner focuses on logical progression while the Purifier ensures information quality. The dynamic adaptation capability means the system doesn't follow rigid reasoning paths but can pivot when evidence suggests a better approach, while the purification step prevents the system from being led astray by noisy or irrelevant retrievals.

## Foundational Learning

**Retrieval-Augmented Reasoning**: The process of combining search capabilities with language model reasoning to answer complex questions. Why needed: Single LLMs often lack sufficient knowledge for complex queries, making external information retrieval essential. Quick check: Can the system identify when to retrieve information versus when to reason from existing knowledge?

**Multi-Hop Reasoning**: Sequential reasoning processes that require multiple inference steps to reach a conclusion. Why needed: Complex questions often require chaining together multiple pieces of information or reasoning steps. Quick check: Can the system maintain logical consistency across multiple reasoning steps?

**Query Formulation**: The process of constructing effective search queries to retrieve relevant information. Why needed: Poor query formulation leads to irrelevant retrievals that can derail the reasoning process. Quick check: Does the system generate queries that balance specificity with breadth?

## Architecture Onboarding

**Component Map**: User Query -> Reasoner Agent (Plan Construction) -> Search Engine -> Retrieved Evidence -> Purifier Agent (Relevance Assessment) -> Distilled Information -> Reasoner (Plan Adaptation) -> Final Answer

**Critical Path**: The core execution path follows User Query through the Reasoner for initial plan construction, then to the Search Engine, followed by the Purifier's relevance assessment, and back to the Reasoner for plan adaptation and final answer generation. This loop continues until the reasoning process completes.

**Design Tradeoffs**: The dual-agent approach adds complexity and computational overhead compared to single-agent systems, but provides better separation of concerns and specialized optimization. The dynamic planning capability trades off between exploration of different reasoning paths and potential inefficiency from over-adaptation. The Purifier adds a computational step but prevents more costly reasoning errors.

**Failure Signatures**: Common failure modes include: (1) Reasoner getting stuck in suboptimal plans despite Purifier feedback, (2) Purifier being too conservative and filtering out relevant information, (3) Disagreements between agents leading to reasoning deadlocks, (4) Over-reliance on dynamic adaptation causing instability in reasoning paths.

**First Experiments**: (1) Baseline comparison without Purifier to quantify its contribution to performance, (2) Static planning version to measure the value of dynamic adaptation, (3) Ablation study removing either agent to understand individual contributions to the 3.8% improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains of 3.8% may not justify the added complexity and computational overhead for all applications
- Reliance on synthesized trajectories for supervised fine-tuning may not capture real-world reasoning diversity
- Reinforcement learning rewards focused on planning quality might over-optimize planning at expense of final answer accuracy

## Confidence

- **High Confidence**: Dual-agent architecture design, identification of two specific failure modes, basic experimental methodology showing baseline improvements
- **Medium Confidence**: Generalizability of 3.8% improvement across domains, effectiveness of two-stage training approach
- **Low Confidence**: Claims about resilience to irrelevant information without quantitative measures, superiority of dynamic over static planning without comparative analysis

## Next Checks
1. Conduct comprehensive ablation studies to isolate individual contributions of Reasoner and Purifier agents, testing whether simpler architectures could achieve similar performance gains

2. Evaluate computational overhead and inference latency compared to single-agent baselines, particularly for deployment in time-sensitive applications, and assess whether performance gains justify added complexity

3. Test method's robustness on out-of-distribution questions and adversarial examples designed to trigger reasoning hijacking, measuring both success rates and failure modes to understand limits of resilience claims