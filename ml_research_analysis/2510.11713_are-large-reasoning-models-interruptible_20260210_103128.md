---
ver: rpa2
title: Are Large Reasoning Models Interruptible?
arxiv_id: '2510.11713'
source_url: https://arxiv.org/abs/2510.11713
tags:
- reasoning
- interrupt
- road
- problem
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how large reasoning models (LRMs) behave when
  their inference process is interrupted mid-thought, either to obtain a quick answer
  or to incorporate new information. The authors introduce two types of interruptions:
  time-constrained (hard: immediate answer; soft: speedup) and update-driven (new
  context mid-inference).'
---

# Are Large Reasoning Models Interruptible?

## Quick Facts
- arXiv ID: 2510.11713
- Source URL: https://arxiv.org/abs/2510.11713
- Reference count: 40
- Large reasoning models (LRMs) fail unpredictably when interrupted mid-inference, with accuracy drops up to 60% on update-driven interrupts

## Executive Summary
This paper examines how large reasoning models behave when their inference process is interrupted mid-thought, either to obtain a quick answer or to incorporate new information. The authors introduce two types of interruptions: time-constrained (hard: immediate answer; soft: speedup) and update-driven (new context mid-inference). Across math and coding benchmarks, static evaluations overestimate LRM robustness—accuracy can drop by up to 60% with late updates. Novel failure modes are identified: reasoning leakage (continuing to think in the answer), panic (premature termination under speedup), and self-doubt (ignoring updates). Prompt guidance mitigates self-doubt on simpler tasks but harder domains remain fragile. The work demonstrates that current LRMs are not inherently interruptible and require targeted evaluation and design for dynamic environments.

## Method Summary
The authors evaluate interruptibility using three mechanisms: hard interruption (forcing immediate answer with `<end-thinking>`), soft interruption (speedup pressure), and update-driven interruption (injecting new context mid-inference). They test four LRMs (GPT-OSS, Qwen3, Magistral, Gemini) on GSM8K, MATH500, LiveCodeBench, and AIME across multiple interruption points (10%-90% of reasoning trace). Failure modes are classified via LLM classifiers and output length analysis. Prompt guidance is tested as a mitigation strategy. The dynamic framework simulates interruptions by injecting tokens at relative positions in the reasoning trace and resuming generation.

## Key Results
- Static evaluations overestimate LRM robustness: accuracy drops up to 60% with late updates
- Three failure modes identified: reasoning leakage, panic, and self-doubt
- Prompt guidance mitigates self-doubt on simple tasks (GSM8K, MATH500) but not harder domains (AIME, LiveCodeBench)
- Model size affects reasoning leakage: smaller models (1.7B) leak more than larger ones

## Why This Works (Mechanism)

### Mechanism 1: Anytime Approximation Under Hard Interruption
LRMs exhibit approximately monotonic accuracy improvement as reasoning budget increases, but this property degrades under early interruption. The model's partial reasoning trace encodes intermediate representations that can be closed into an answer when forced. Earlier interruption points yield lower-quality partial traces, producing worse answers. However, models often violate the termination signal by continuing reasoning in the answer segment. Core assumption: the reasoning trace accumulates useful signal linearly or near-linearly with token position. Evidence: general anytime behavior observed across models, but reasoning leakage occurs when models continue thinking outside designated tokens.

### Mechanism 2: Context Integration Failure Under Update-Driven Interruption
Models fail to incorporate mid-inference updates due to "self-doubt"—second-guessing whether the update is authoritative rather than integrating it directly. When an update is injected at position X, models often engage in verification loops ("But the initial problem statement said...") rather than committing to the new information. This is exacerbated by late-stage interruptions where sunk-cost in the original reasoning is higher. Core assumption: models interpret updates as potentially adversarial or erroneous unless explicitly validated. Evidence: self-doubt observed when performance degrades while incorporating updated information, with models continuing original thinking process.

### Mechanism 3: Panic Response Under Soft Speedup Pressure
When instructed to speed up without forced termination, models may prematurely abandon reasoning entirely, producing incorrect answers far below baseline. The "hurry up" signal is interpreted as a directive to conclude immediately rather than compress reasoning. Models close their thinking block within <1% of remaining context limit, bypassing verification steps they would normally perform. Core assumption: models lack meta-cognitive control over reasoning compression and default to early termination as the simplest compliance strategy. Evidence: up to 80% of performance loss attributable to this panic behavior, model-dependent response to speedup signals.

## Foundational Learning

- **Autoregressive Context Accumulation**: Why needed: Interruptions inject tokens mid-generation, and understanding how the model conditions on prior context vs. new tokens is essential for predicting failure modes. Quick check: If a model has generated tokens [r_1, r_2, ..., r_X] and receives interrupt tokens i, what is the new conditioning context for subsequent generation?

- **Test-Time Compute Budgets**: Why needed: The paper operationalizes interruption as a compute budget constraint; knowing how reasoning effort correlates with accuracy helps interpret the "anytime behavior" claim. Quick check: What is the trade-off between reasoning token count and answer quality in standard LRM evaluation?

- **Chain-of-Thought Structure in LRMs**: Why needed: Reasoning leakage depends on how models distinguish "thinking" tokens from "answer" tokens; understanding this boundary clarifies why leakage occurs. Quick check: In a typical LRM, what structural markers delimit the reasoning trace from the final answer, and what happens if they are forcibly closed early?

## Architecture Onboarding

- **Component map**: Input -> Inference engine (generates reasoning trace r and answer a) -> Interrupt controller (injects interrupt tokens i at position X) -> Update validator (ensures a*(q) ≠ a*(q, u)) -> Answer evaluator (computes A_i(X) and L_i(X))

- **Critical path**: 1) Generate full reasoning trace to establish baseline length R_T, 2) Simulate interrupt at position X × R_T with appropriate interrupt tokens, 3) Resume generation and capture post-interrupt output, 4) Classify failure mode (leakage, panic, self-doubt) via LLM classifier or length threshold

- **Design tradeoffs**: Assistant-turn vs. user-turn interrupts (assistant-turn more reliable across models; user-turn causes formatting failures in Qwen3/Magistral), hard vs. soft interrupts (hard guarantees termination but provokes leakage; soft preserves structure but risks panic), prompt guidance vs. no guidance (guidance reduces self-doubt on simple tasks but limited effect on AIME)

- **Failure signatures**: Reasoning leakage (answer length >2× baseline; reasoning-style text in answer segment), panic (post-interrupt reasoning <1% of remaining context limit; immediate incorrect answer), self-doubt (post-interrupt trace contains explicit verification loops; final answer ignores update)

- **First 3 experiments**: 1) Baseline hard interrupt sweep: Interrupt at X ∈ {0.1, 0.3, 0.5, 0.7, 0.9} with `<end-thinking>` on GSM8K; plot A(X) and L(X) vs. baseline. Look for monotonicity and leakage threshold. 2) Self-doubt quantification: On update-driven AIME subset, compare accuracy with/without prompt guidance at X=0.5; manually inspect traces for verification loops. 3) Model scaling probe: Run Qwen3-1.7B, 8B, 32B on hard-interrupt MATH500 to test if reasoning leakage scales inversely with model size.

## Open Questions the Paper Calls Out

### Open Question 1
How do LRMs perform under noisy, adversarial, or multi-turn interruptions compared to the single, well-defined events tested in this study? Basis: Section 7 states interruptions may be noisy, adversarial, or multi-turn in practice. Unresolved because current evaluation is restricted to clean, single-step context updates. Resolution: Evaluations incorporating adversarial interruptions and multi-turn dialogue scenarios.

### Open Question 2
Do the failure modes persist in non-STEM domains such as collaborative writing, planning, or open-ended dialogue? Basis: Section 7 notes evaluation relies primarily on math and programming tasks. Unresolved because conclusions drawn from benchmarks requiring definitive correct answers. Resolution: Application to open-ended generation and planning benchmarks.

### Open Question 3
Why does interruptible robustness degrade significantly in smaller parameter models (e.g., 1.7B) compared to larger counterparts? Basis: Section 6.1 observes scaling limit where small models struggle to generalize on updates. Unresolved because authors did not isolate whether this is due to model capacity or training data composition. Resolution: Ablation studies analyzing internal state updates or specific fine-tuning on smaller models.

### Open Question 4
How can model training or architectures be explicitly designed to support interruptibility, rather than relying on inference-time prompt engineering? Basis: Section 7 states robust interruptibility requires dedicated evaluation and design. Unresolved because paper identifies pathologies but does not propose training methodology. Resolution: Training runs utilizing data augmentation strategies based on the paper's defined interruption scenarios.

## Limitations

- Simulated interruptions rather than real-time user interactions may not capture full complexity of dynamic environments
- Focus on reasoning-oriented LRMs may not extend to non-reasoning models or multimodal systems
- Self-doubt mitigation strategies (prompt guidance) show limited effectiveness on harder domains (AIME, LiveCodeBench)
- No exploration of computational overhead or safety implications of interruptible inference in production systems

## Confidence

**High Confidence**: LRMs exhibit anytime behavior where earlier interruption degrades accuracy (quantitative results across multiple models and benchmarks); Reasoning leakage is a real failure mode (measured via output length and qualitative analysis); Static evaluations systematically overestimate LRM robustness under interruption (demonstrated through controlled experiments)

**Medium Confidence**: Self-doubt mechanism explains update integration failures (supported by qualitative analysis but lacks comprehensive causal validation); Prompt guidance effectively mitigates self-doubt on simpler tasks (shown for GSM8K/MATH500 but not harder domains); Model size affects reasoning leakage prevalence (observed trend but limited model coverage)

**Low Confidence**: Panic response is primarily driven by speedup signals rather than model-specific factors (inconsistent across models, mechanism unclear); The proposed failure modes are exhaustive and mutually exclusive (classification methodology not fully specified)

## Next Checks

1. **Real-time Interruptibility Benchmark**: Implement a live evaluation framework where human annotators can inject updates during model inference, measuring both accuracy retention and response latency. This would validate whether simulated interruptions capture real-world interruptibility challenges and test the scalability of mitigation strategies to truly dynamic scenarios.

2. **Cross-Domain Transfer Study**: Evaluate the same interruption protocols on non-reasoning LRMs (e.g., general-purpose LLMs) and multimodal models to determine whether reasoning leakage and self-doubt are intrinsic to autoregressive generation or specific to reasoning-oriented architectures. This would clarify the generalizability of the proposed failure modes.

3. **Safety and Security Analysis**: Design experiments testing whether reasoning leakage exposes sensitive intermediate reasoning in domains like code generation or mathematical problem-solving, and whether interruptible inference introduces new attack vectors through mid-generation token injection. This would address the unexplored safety implications of the core findings.