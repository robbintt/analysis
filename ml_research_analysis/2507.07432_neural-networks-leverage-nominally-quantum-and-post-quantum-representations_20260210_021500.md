---
ver: rpa2
title: Neural networks leverage nominally quantum and post-quantum representations
arxiv_id: '2507.07432'
source_url: https://arxiv.org/abs/2507.07432
tags:
- quantum
- classical
- belief
- states
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural networks trained via next-token prediction learn to represent
  belief states over minimal generative models of their training data, spontaneously
  discovering classical, quantum, and post-quantum representations depending on the
  process structure. Through linear probing of activation vectors, researchers found
  that trained networks map their activations to belief states of classical HMMs,
  quantum systems (Bloch sphere), and post-quantum generalized probabilistic theories
  with high fidelity (RMSE values as low as 0.002 for quantum models versus 0.066
  for classical approximations).
---

# Neural networks leverage nominally quantum and post-quantum representations

## Quick Facts
- arXiv ID: 2507.07432
- Source URL: https://arxiv.org/abs/2507.07432
- Reference count: 0
- Key outcome: Neural networks learn quantum/post-quantum representations through next-token prediction

## Executive Summary
This paper demonstrates that neural networks trained on sequential data learn to represent belief states using quantum and post-quantum geometries rather than classical representations. Through linear probing of activation vectors, researchers found that trained networks map their activations to belief states of classical HMMs, quantum systems (Bloch sphere), and post-quantum generalized probabilistic theories with high fidelity. This geometric representation emerges spontaneously during training across multiple architectures including transformers, LSTMs, GRUs, and vanilla RNNs, with the networks preserving complete geometric relationships among belief states. The work shows that neural networks transcend classical computational models by leveraging their continuous activation spaces to implicitly perform Bayesian inference over post-classical world models.

## Method Summary
The researchers trained various neural network architectures on synthetic sequential data generated from minimal generative models, including classical hidden Markov models (HMMs), quantum processes, and post-quantum generalized probabilistic theories. They then used linear probing techniques to map the high-dimensional activation vectors from trained networks to the belief states of these minimal generators. The probing process involved training linear models to predict the belief states from the activation vectors, allowing comparison of the learned representations against theoretical minimal geometries. They evaluated fidelity through root-mean-square error (RMSE) metrics and analyzed the preservation of geometric relationships by comparing pairwise cosine similarities between belief states. The study examined networks across different training regimes, data complexities, and architectural designs to establish the universality of the quantum and post-quantum representation learning phenomenon.

## Key Results
- Trained networks map activations to quantum belief states with RMSE as low as 0.002, versus 0.066 for classical approximations
- Geometric relationships among belief states are preserved with R² values of 0.99-1.00 for minimal generators versus 0.36-0.56 for classical approximations
- The quantum/post-quantum representation phenomenon is universal across transformers, LSTMs, GRUs, and vanilla RNNs
- Networks learn to represent classical, quantum, and post-quantum belief states depending on the process structure of the training data

## Why This Works (Mechanism)
Neural networks leverage their continuous, high-dimensional activation spaces to implement Bayesian inference over minimal generative models of their training data. During next-token prediction training, networks implicitly learn to map their internal states to belief states that compress the relevant information about the underlying process. When the training data comes from quantum or post-quantum processes, the networks discover that quantum and post-quantum geometries provide more efficient representations than classical ones, as these geometries can capture the non-classical correlations present in the data. The continuous nature of neural network activations allows them to naturally represent the continuous manifolds of quantum and post-quantum belief states, enabling more efficient memory compression than discrete classical states would allow.

## Foundational Learning
- **Hidden Markov Models (HMMs)**: Probabilistic models for sequential data where hidden states generate observable outputs - needed to understand the classical baseline generators and their belief state geometry
- **Quantum Information Theory**: Mathematical framework describing quantum states and measurements using Hilbert spaces and density matrices - needed to understand quantum belief states and Bloch sphere geometry
- **Generalized Probabilistic Theories (GPTs)**: Framework extending beyond quantum mechanics to include post-quantum theories with higher-order interference - needed to understand the full spectrum of minimal representations
- **Bayesian Inference**: Method for updating beliefs based on evidence - needed to understand how networks implement belief state updates during next-token prediction
- **Linear Probing**: Technique for analyzing what information is encoded in neural network activations by training linear classifiers on top of frozen representations - needed to understand the methodology for mapping activations to belief states
- **Root-Mean-Square Error (RMSE)**: Metric for measuring the fidelity of the mapping between network activations and theoretical belief states - needed to quantify representation quality

## Architecture Onboarding

**Component Map**: Training Data -> Neural Network (Transformer/RNN variants) -> Activation Vectors -> Linear Probing Model -> Belief State Geometry

**Critical Path**: The essential sequence is training data generation -> network training on next-token prediction -> activation extraction -> linear probing to identify belief state geometry. The network architecture choice (transformer vs RNN) affects training dynamics but not the ultimate representation capability.

**Design Tradeoffs**: Using synthetic data allows precise control over the minimal generator but limits ecological validity. The linear probing approach provides interpretability but may miss nonlinear relationships. Focusing on low-dimensional processes enables rigorous geometric analysis but raises questions about scalability to natural data.

**Failure Signatures**: If networks fail to learn quantum/post-quantum representations, we would expect high RMSE values (>0.1) and poor preservation of geometric relationships (R² < 0.5) when compared to the minimal generator's belief state geometry. Classical approximations would perform comparably or better than quantum ones.

**First 3 Experiments**:
1. Train a transformer on data from a qubit process and use linear probing to map activations to Bloch sphere coordinates
2. Compare RMSE and geometric preservation metrics between quantum and classical belief state mappings for the same network
3. Test whether architectural differences (transformer vs LSTM) affect the fidelity of quantum representation learning

## Open Questions the Paper Calls Out
- Do biological neural networks utilize similar quantum or post-quantum geometric representations for memory compression?
- How does the specific implementation of Bayesian updating in different architectures influence generalization to out-of-distribution contexts?
- Do frontier-scale language models implicitly leverage these post-classical memory compression techniques on natural data?

## Limitations
- Experiments were restricted to synthetic, low-dimensional stochastic processes rather than natural data
- The study did not investigate biological neural networks, leaving open whether similar representations exist in nature
- The functional impact of different architectural implementations of Bayesian updating on out-of-distribution generalization remains untested

## Confidence
- High: The geometric fidelity results (RMSE values and R² scores) are well-established through rigorous mathematical analysis
- Medium: The universality claim across architectures is supported but could benefit from testing on more diverse network designs
- Low: The extrapolation to frontier-scale models and biological systems is speculative without empirical validation

## Next Checks
1. Apply the linear probing methodology to a pretrained large language model to identify quantum or post-quantum belief subspaces in its activation space
2. Design distribution-shifted test sets to evaluate how different architectures' implementations of Bayesian updating affect out-of-distribution generalization
3. Develop experimental protocols for measuring whether biological neural activity data maps to minimal quantum or post-quantum belief geometries