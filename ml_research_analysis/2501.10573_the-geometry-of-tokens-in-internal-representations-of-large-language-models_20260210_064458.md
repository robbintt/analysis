---
ver: rpa2
title: The Geometry of Tokens in Internal Representations of Large Language Models
arxiv_id: '2501.10573'
source_url: https://arxiv.org/abs/2501.10573
tags:
- token
- layers
- tokens
- prompts
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the geometric properties of token embeddings
  in transformer models, focusing on their connection to next-token prediction. It
  employs intrinsic dimension, neighborhood overlap, and cosine similarity to probe
  the distribution of token representations across layers, using shuffled prompts
  as a control to disrupt syntactic and semantic structure.
---

# The Geometry of Tokens in Internal Representations of Large Language Models

## Quick Facts
- arXiv ID: 2501.10573
- Source URL: https://arxiv.org/abs/2501.10573
- Reference count: 40
- Key outcome: Intrinsic dimension of token embeddings correlates with next-token prediction loss and increases with syntactic disruption via shuffling

## Executive Summary
This paper investigates the geometric properties of token embeddings in transformer models, specifically their intrinsic dimension, neighborhood overlap, and cosine similarity across layers. The authors use shuffled prompts as a control to disrupt syntactic and semantic structure, revealing that intrinsic dimension peaks in early-to-middle layers and that this peak is higher for shuffled data. Critically, they find a positive correlation between intrinsic dimension and cross-entropy loss, suggesting geometric properties encode prediction uncertainty. The work provides a novel geometric perspective on how transformers process information and offers potential interpretable metrics for model behavior.

## Method Summary
The authors extract hidden states from three pretrained decoder-only transformer models (LLAMA 3 8B, Mistral 7B, Pythia 6.9B) across 32 layers. They use The Pile-10K dataset filtered for prompts with N ≥ 1024 tokens, truncated to exactly N = 1024 tokens. Block shuffling is applied with nBlocks = 4^S where S ∈ {0,1,...,5}. Geometric properties are computed using the TWO-NN estimator for intrinsic dimension (range scaling=2), k-NN algorithms for neighborhood overlap (k=2), and cosine similarity. The analysis correlates these geometric measures with cross-entropy loss per prompt.

## Key Results
- Intrinsic dimension peaks in early-to-middle layers and is higher for shuffled prompts
- Neighborhood overlap decreases with shuffling, indicating disrupted token relationships
- Cosine similarity increases with shuffling, showing tokens become more aligned
- Intrinsic dimension correlates positively with cross-entropy loss, suggesting geometric properties encode prediction uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intrinsic dimension acts as a geometric proxy for prediction uncertainty (cross-entropy loss).
- **Mechanism:** High intrinsic dimension in residual stream correlates with high dimension in logits, which exhibit higher softmax entropy (S ~ log D_M), which correlates with higher cross-entropy loss.
- **Core assumption:** The distribution of logit vectors on their manifold approximates simple cases where entropy scales logarithmically with dimension.
- **Evidence anchors:** [abstract] correlation between geometric properties and cross-entropy loss; [section 5.1] expected softmax entropy ~ log D_M
- **Break condition:** If unembedding matrix creates non-linear projection decoupling residual stream ID from logit ID.

### Mechanism 2
- **Claim:** Shuffling forces higher-dimensional, aligned geometric configuration.
- **Mechanism:** Shuffled tokens lack sequential predictive structure, causing the model to spread them out (increasing ID) and align them toward generic direction (increasing cosine similarity) rather than creating specific semantic relationships.
- **Core assumption:** Shuffling destroys semantic dependencies without altering unigram frequency distribution.
- **Evidence anchors:** [abstract] ID increases with shuffling, cosine similarity rises; [section 4.1] shuffled tokens distributed along same direction, structured prompts more orthogonal
- **Break condition:** If model overfits to noise and collapses shuffled inputs to lower dimensions.

### Mechanism 3
- **Claim:** Neighborhood Overlap detects preservation of token relationships across layers.
- **Mechanism:** NO measures shared k-nearest neighbors between adjacent layers. Structured data maintains neighborhoods (high NO) while shuffled data causes neighborhoods to shift chaotically (lower NO) around ID peak layers.
- **Core assumption:** Meaningful semantic processing requires consistent local geometry across layers.
- **Evidence anchors:** [abstract] NO decreases with shuffling; [section 4.3] shuffled NO lower than structured case around ID peak layers
- **Break condition:** If attention heads act purely globally, attending to all tokens equally.

## Foundational Learning

### Concept: Intrinsic Dimension (ID)
- **Why needed here:** ID quantifies the "complexity" or "volume" of the manifold tokens occupy, making "geometry" meaningful beyond abstract distance.
- **Quick check question:** How does the TWO-NN estimator relate the ratio of nearest-neighbor distances to the dimension of the data?

### Concept: Empirical Measure (Mean-Field View)
- **Why needed here:** The paper frames token evolution as particles interacting based on their collective distribution (empirical measure), not just isolated paths.
- **Quick check question:** In the mean-field picture, what determines the evolution of a single token x_i(ℓ)?

### Concept: Softmax Entropy vs. Cross-Entropy Loss
- **Why needed here:** The paper bridges geometry (ID) and performance (Loss) through entropy, requiring distinction between expected entropy (Contextual Entropy) and specific loss against ground-truth token.
- **Quick check question:** Why does high softmax entropy generally imply a higher expected cross-entropy loss?

## Architecture Onboarding

### Component map:
Tokenized prompts (N ≥ 1024) -> Residual Stream across 32 layers -> GRIDE/TWO-NN for ID, k-NN for NO/Cosine -> Logits -> Softmax Probabilities

### Critical path:
Extract residual stream (hidden state) after MLP update at each layer, representing token's position in d-dimensional space.

### Design tradeoffs:
- GRIDE Scaling: Lower scaling (n2=2) captures local manifold structure; higher scaling (n2=8) captures global shape but requires more tokens
- Prompt Length: Authors enforce N=1024 for reliable ID estimation; shorter prompts yield noisy estimates

### Failure signatures:
- Constant ID: Suggests model not training or extraction hook placed incorrectly
- NO = 0: Indicates layer-to-layer representations orthogonal/random, suggesting broken residual connections or untrained weights

### First 3 experiments:
1. **Reproduce Shuffle Dynamics:** Run single prompt through small model with shuffle indices S=0 to S=5. Plot ID vs. Layer to confirm peak shift.
2. **ID-Loss Correlation:** Scatter plot ID of final layer against cross-entropy loss for 100 random prompts to verify positive correlation.
3. **Scale Ablation:** Calculate ID at range scalings 2, 4, and 8 to observe how dimension estimate changes with neighborhood size.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does intrinsic dimension evolve during training, specifically regarding the trade-off between increasing model expressivity and minimizing loss?
- **Basis in paper:** [explicit] Authors defer investigation of ID rising during training then slightly decreasing as loss minimization improves.
- **Why unresolved:** Paper focuses on inference in pre-trained models with only brief qualitative observation of training dynamics.
- **What evidence would resolve it:** Comprehensive longitudinal study tracking ID and loss across numerous training checkpoints for various architectures.

### Open Question 2
- **Question:** Does multiscale analysis with larger kNN ranges reveal additional geometric relations or robustness in observables?
- **Basis in paper:** [explicit] Authors note multiscale analysis can reveal further relations among observables while experiments were limited to low range scaling.
- **Why unresolved:** Current study restricted to specific neighborhood scales (scaling = 2, 4, 8), potentially missing global structural properties.
- **What evidence would resolve it:** Experiments with significantly larger kNN ranges to verify if correlations with loss and shuffling persist.

### Open Question 3
- **Question:** Does the relationship S ~ log D_M hold for the complex manifolds found in actual LLMs?
- **Basis in paper:** [explicit] Authors show relation holds for toy examples but state it might not hold for generic manifolds and reserve investigation for future work.
- **Why unresolved:** Mathematical derivation limited to simplified cases (uniform distributions on simple shapes) not validated on complex LLM manifolds.
- **What evidence would resolve it:** Theoretical proof or empirical validation demonstrating log-linear relationship holds for actual logit distributions.

### Open Question 4
- **Question:** Can other geometric observables beyond ID, cosine similarity, and NO be identified to understand how token geometry encodes next-token probability distributions?
- **Basis in paper:** [explicit] Conclusion suggests considering other geometric observables and understanding their relation to next token probabilities.
- **Why unresolved:** Current work limited to three specific metrics, with authors implying these may not capture full picture of empirical measure dynamics.
- **What evidence would resolve it:** Identification of new geometric metrics (e.g., curvature, topological persistence) showing strong correlation with next-token prediction accuracy.

## Limitations
- Correlation vs. causation gap: The causal chain from geometry to loss remains inferential and relies on untested assumptions about unembedding matrix linearization
- Sampling limitations: Experiments use prompts from The Pile filtered to N ≥ 1024, potentially not capturing full diversity of real-world usage
- Model scope restrictions: Study examines three decoder-only transformers at similar scales, limiting generalizability to other architectures

## Confidence

**High Confidence**: Core observational claims about geometric properties - intrinsic dimension peaks in middle layers, shuffled prompts exhibit higher peaks, neighborhood overlap decreases with shuffling around these peaks, and cosine similarity increases with shuffling.

**Medium Confidence**: Correlation between intrinsic dimension and cross-entropy loss, though interpretation as causal relationship through entropy requires additional validation.

**Low Confidence**: Theoretical mechanism linking geometric properties to prediction uncertainty through manifold entropy scaling, and broader claim that these properties serve as interpretable metrics for model behavior.

## Next Checks

**Check 1: Manifold-to-Loss Causal Chain Validation**: Design experiment testing whether manipulating intrinsic dimension of residual stream representations causes predictable changes in next-token prediction loss by training models with explicit geometric constraints and measuring resulting loss-ID relationship.

**Check 2: Cross-Architecture Generalization**: Apply geometric analysis framework to models with different architectural innovations - encoder-decoder transformers, smaller models (500M parameters), and non-transformer architectures like state-space models to validate whether geometric patterns are fundamental to autoregressive prediction.

**Check 3: Semantic vs. Syntactic Contribution Separation**: Develop nuanced control conditions isolating syntactic from semantic disruption through word-order shuffling within sentences, synonym replacement, and syntactic structure preservation with semantic scrambling to clarify which aspects of prompt structure drive observed geometric changes.