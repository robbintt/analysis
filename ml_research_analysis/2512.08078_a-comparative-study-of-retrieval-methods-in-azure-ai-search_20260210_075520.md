---
ver: rpa2
title: A Comparative Study of Retrieval Methods in Azure AI Search
arxiv_id: '2512.08078'
source_url: https://arxiv.org/abs/2512.08078
tags:
- retrieval
- semantic
- keyword
- search
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates retrieval strategies within Microsoft Azure's
  RAG framework to identify effective approaches for Early Case Assessment (ECA) in
  eDiscovery. We compared the performance of Azure AI Search's keyword, semantic,
  vector, hybrid, and hybrid-semantic retrieval methods, presenting the accuracy,
  relevance, and consistency of each method's AI-generated responses.
---

# A Comparative Study of Retrieval Methods in Azure AI Search

## Quick Facts
- **arXiv ID**: 2512.08078
- **Source URL**: https://arxiv.org/abs/2512.08078
- **Reference count**: 1
- **Primary result**: Hybrid semantic retrieval produced the most consistently relevant results in legal RAG applications, but at higher computational cost

## Executive Summary
This study evaluates five Azure AI Search retrieval methods (keyword, semantic, vector, hybrid, hybrid-semantic) in a RAG framework for legal eDiscovery tasks using the Jeb Bush email dataset. The analysis reveals that no single retrieval method performs consistently across all scenarios—keyword-based retrieval excels for structured data with exact matches while vector retrieval captures semantic similarity. Hybrid semantic retrieval showed the most consistently relevant results but required higher computational resources. The study found that many errors attributed to "LLM hallucination" were actually rooted in retrieval method design deficiencies, demonstrating that retrieval quality is the primary determinant of downstream model performance.

## Method Summary
The researchers compared five Azure AI Search retrieval methods using the Jeb Bush email dataset (290,000+ documents chunked into 491,482 segments). Each method retrieved top 50 chunks per query, with top 5 sent to gpt-4.1-mini (temperature=0) for response generation. Manual qualitative evaluation assessed accuracy, relevance, and consistency across 9 test prompts spanning structured queries (case numbers, quoted phrases) and conceptual queries (business forecasts, policy positions).

## Key Results
- Hybrid semantic retrieval achieved the highest consistency but incurred 3-4x computational overhead compared to keyword-only methods
- Keyword retrieval was fastest and most accurate for exact matches (case numbers, quoted phrases) but missed semantically relevant content
- Vector retrieval captured conceptual matches but failed on rare terms and alphanumeric identifiers not well-represented in embedding space
- Semantic reranking improved recall but sometimes elevated misleading content when initial results lacked relevant material

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword retrieval succeeds on exact lexical matches (IDs, case numbers, quoted phrases) because it indexes and matches text tokens directly.
- Mechanism: Keyword search uses term frequency-inverse document frequency (TF-IDF) to match query terms against indexed text. When a query contains a unique alphanumeric string like "TO 98-103033, 32" or a quoted nickname like 'T-Squred', the exact token match locates the relevant documents regardless of semantic meaning.
- Core assumption: The target information contains distinctive lexical tokens that appear explicitly in the query and are indexed as discrete terms.
- Evidence anchors:
  - [abstract]: "Keyword retrieval was fastest but missed semantically relevant content"
  - [section]: "Unsurprisingly, methods utilizing keyword search enable RAG to respond with correct references to the emails. Conversely, the vector embedding of the prompt does not embed the alphanumeric case number as a meaningful semantic feature"
  - [corpus]: Related work on hybrid retrieval (arXiv:2509.13603, arXiv:2508.04683) confirms keyword methods remain essential for structured queries but require augmentation for semantic tasks.
- Break condition: Queries using synonyms, paraphrases, or conceptual descriptions without shared vocabulary will fail to retrieve relevant documents.

### Mechanism 2
- Claim: Vector retrieval captures conceptual similarity by embedding queries and documents into a shared numerical space, but fails on rare terms or domain-specific identifiers that lack semantic density in the embedding model's training data.
- Mechanism: Both query and indexed text are converted to vectors (1536 dimensions using text-embedding-ada-002). Retrieval uses cosine similarity to find documents with similar vector representations, enabling matches even when exact terms differ.
- Core assumption: The embedding model has learned meaningful semantic representations for the domain vocabulary, and relevant content shares conceptual features with the query.
- Evidence anchors:
  - [abstract]: "Vector retrieval captured conceptual matches but failed on rare terms"
  - [section]: "The vector retrieval method successfully identified emails containing specific orange product forecast numbers, whereas the keyword method failed to locate such forecasts"
  - [section]: "The vector embedding of the prompt does not embed the alphanumeric case number as a meaningful semantic feature, causing the vector retrieval method to fail"
  - [corpus]: arXiv:2508.04683 confirms hybrid retrieval combining keyword and vector methods reduces hallucinations by addressing complementary failure modes.
- Break condition: Queries for rare identifiers, misspellings, or domain-specific jargon not well-represented in the embedding model's training distribution will produce poor similarity scores.

### Mechanism 3
- Claim: Semantic reranking can recover relevant documents missed by initial keyword ranking, but can also elevate misleading or tangential content when the initial candidate pool lacks truly relevant material.
- Mechanism: Reranking applies a semantic scoring model to reorder initial retrieval results based on deeper contextual similarity to the query. This can surface documents that were ranked lower by lexical methods but are semantically relevant. However, reranking only reorders—it cannot recover documents absent from the initial candidate set.
- Core assumption: The top-k initial retrieval results contain the relevant documents, just poorly ordered. If relevant documents fall outside the initial retrieval window, reranking amplifies the "best available" option, which may be misleading.
- Evidence anchors:
  - [abstract]: "Hybrid-semantic improved recall but sometimes elevated misleading content"
  - [section]: "Upon examining the citations in the LLM response, it appears that re-ranking elevated a text chunk to the top retrieval results, which was part of speech given by President George Bush. Because the LLM lacks source awareness, it inferred that the retrieved text represented Jeb Bush's views"
  - [section]: "Semantic reranking improved recall in many cases but also elevated misleading or tangential text when the initial results lacked relevant material"
  - [corpus]: arXiv:2505.04732 discusses LLM-based reranking tradeoffs; arXiv:2508.04683 notes reranking quality depends heavily on initial retrieval coverage.
- Break condition: When initial retrieval fails to surface relevant documents within the candidate pool, reranking will confidently promote the "least wrong" option, potentially causing confident but incorrect LLM outputs.

## Foundational Learning

- Concept: **RAG (Retrieval-Augmented Generation) Architecture**
  - Why needed here: The entire study evaluates retrieval methods within a RAG pipeline; understanding how retrieval quality directly constrains generation quality is essential.
  - Quick check question: If your RAG system returns incorrect answers, what diagnostic step should you perform first—examine the retrieved chunks or adjust the LLM temperature?

- Concept: **Chunking Strategy and Its Retrieval Implications**
  - Why needed here: The study used 2,000-token chunks with 500-token overlap. Chunk boundaries affect whether relevant information is captured in a single retrievable unit.
  - Quick check question: If a relevant sentence spans two chunks, what happens to retrieval probability for that information?

- Concept: **Grounding and Attribution in LLM Outputs**
  - Why needed here: The study shows LLMs lack "source awareness" and will attribute retrieved content to the query subject even when inappropriate (e.g., attributing George Bush's views to Jeb Bush).
  - Quick check question: An LLM correctly answers a question using retrieved documents. How do you verify the answer reflects the intended source's perspective rather than a tangentially related document?

## Architecture Onboarding

- Component map: Query → Azure AI Search (retriever) → Top 50 chunks → Reranking (if applicable) → Top 5 chunks → gpt-4.1-mini (generator) → Response with citations
- Critical path: Query → Retrieval method (top 50 chunks) → Reranking (if applicable) → Top 5 chunks to LLM → Response with citations
- Design tradeoffs:
  - **Keyword vs. Vector**: Precision on exact matches vs. recall on conceptual queries
  - **Hybrid vs. Single-method**: Computational cost vs. coverage
  - **Reranking**: Improved relevance vs. risk of elevating tangential content when initial pool lacks relevant documents
  - **Temperature=0**: Deterministic outputs for retrieval comparison vs. reduced response diversity
- Failure signatures:
  - **Keyword failure**: Query uses synonyms or conceptual language; returns "no information found"
  - **Vector failure**: Query contains IDs, rare terms, or quoted exact phrases; returns off-topic conceptual matches
  - **Reranking amplification error**: LLM confidently cites a document that is tangentially related but factually incorrect for the specific query (e.g., misattribution of views between people with similar names)
  - **Source confusion**: Retrieved chunks from secondary sources are presented as primary source statements
- First 3 experiments:
  1. **Baseline calibration with known-answer queries**: Create prompts with verifiable factual answers (like P1's case number). Test each retrieval method to establish which methods handle structured vs. conceptual queries correctly.
  2. **Attribution stress test**: Design queries where documents from multiple related entities exist (like P7-P9's Jeb vs. George Bush scenario). Verify whether your retrieval + generation pipeline correctly attributes information to the intended source.
  3. **Quote vs. natural language comparison**: Run the same query in quoted-exact-match form and natural-language paraphrase form (like P4 vs. P5). Document which retrieval methods succeed for each variant.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can an agentic system accurately evaluate incoming queries to dynamically select the optimal retrieval strategy (e.g., keyword vs. vector) for legal RAG applications?
  - Basis in paper: [explicit] The authors propose exploring an "agentic system to evaluate each query and select the retrieval strategy... that would likely produce the most accurate results."
  - Why unresolved: The current study manually compared fixed strategies; it did not implement or test an automated, dynamic selector.
  - What evidence would resolve it: A benchmark comparison showing an agentic selector's accuracy and latency against fixed-strategy baselines across diverse query types.

- **Open Question 2**: Do the observed trade-offs between retrieval methods persist across larger, more diverse legal datasets beyond the Jeb Bush email corpus?
  - Basis in paper: [explicit] The authors call for "benchmarking retrieval strategies across larger and more diverse datasets to validate performance in real-world legal scenarios."
  - Why unresolved: The current findings rely on a single dataset (Jeb Bush emails), limiting generalizability to other document types or legal domains.
  - What evidence would resolve it: Replication of the study methodology on varied legal corpora (e.g., contracts, litigation filings) showing similar performance hierarchies.

- **Open Question 3**: Can visualizing retrieval and reranking behavior reduce user reliance on incorrect AI responses by revealing why specific documents surfaced?
  - Basis in paper: [explicit] The authors list "visualizing retrieval and reranking behavior" as a necessary step to help users understand document surfacing.
  - Why unresolved: The study identified that semantic reranking can elevate misleading content (e.g., P7/P8), but did not test interfaces to explain this behavior to users.
  - What evidence would resolve it: User studies measuring the detection rate of retrieval errors when participants are provided with retrieval score visualizations versus raw text.

- **Open Question 4**: Can feedback loops be implemented to adaptively optimize retrieval configurations over time based on user interactions or query outcomes?
  - Basis in paper: [explicit] The authors suggest "incorporating and testing feedback loops to inform a system to adaptively select more efficient retrieval methods over time."
  - Why unresolved: The current workflow is static; the system does not learn from the success or failure of previous retrieval attempts to adjust parameters.
  - What evidence would resolve it: Longitudinal performance metrics showing improved accuracy or reduced computational cost in a system utilizing adaptive feedback loops.

## Limitations
- **Domain specificity**: Findings may not generalize beyond formal business/political correspondence with consistent formatting
- **Configuration sensitivity**: Results depend heavily on specific chunk size (2,000 tokens) and overlap (500 tokens) parameters
- **Manual evaluation**: Qualitative assessments lack standardized scoring criteria, introducing reviewer subjectivity

## Confidence
- **High confidence**: Keyword retrieval effectiveness for exact matches (P1, P2, P8 results), vector retrieval's semantic capture (P3, P4, P6 results), and the fundamental finding that retrieval quality determines downstream generation quality
- **Medium confidence**: Hybrid retrieval performance claims, as these depend on specific parameter tuning and may vary with different weighting schemes
- **Low confidence**: Generalizability to other domains and document types, particularly highly unstructured or multimodal content

## Next Checks
1. **Cross-domain validation**: Replicate the evaluation on a different document corpus (e.g., scientific literature or customer support tickets) to test generalizability of retrieval method rankings
2. **Chunk size sensitivity analysis**: Systematically vary chunk sizes (1,000 tokens, 3,000 tokens) and overlap percentages to determine optimal configurations for each retrieval method
3. **Dynamic reranking threshold testing**: Experiment with different top-k retrieval windows before reranking (top 20, top 100) to quantify the trade-off between initial recall and reranking effectiveness