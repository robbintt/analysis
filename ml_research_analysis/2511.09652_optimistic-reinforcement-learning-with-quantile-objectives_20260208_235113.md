---
ver: rpa2
title: Optimistic Reinforcement Learning with Quantile Objectives
arxiv_id: '2511.09652'
source_url: https://arxiv.org/abs/2511.09652
tags:
- quantile
- learning
- lemma
- page
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops UCB-QRL, an optimistic reinforcement learning\
  \ algorithm for the \u03C4-quantile objective in episodic Markov decision processes.\
  \ The method constructs \u21131 confidence sets around empirical transition models\
  \ and plans optimistically over models that respect a quantile margin assumption,\
  \ which ensures local regularity of the quantile backup."
---

# Optimistic Reinforcement Learning with Quantile Objectives

## Quick Facts
- arXiv ID: 2511.09652
- Source URL: https://arxiv.org/abs/2511.09652
- Reference count: 40
- Primary result: UCB-QRL achieves O((2/κ)^(H+1) H √(SAT H log(2SAT H/δ))) regret for τ-quantile objectives under margin assumptions.

## Executive Summary
This paper develops UCB-QRL, an optimistic reinforcement learning algorithm for the τ-quantile objective in episodic Markov decision processes. The method constructs ℓ1 confidence sets around empirical transition models and plans optimistically over models that respect a quantile margin assumption, which ensures local regularity of the quantile backup. Under this assumption, the authors prove a high-probability regret bound that explicitly quantifies how quantile sensitivity impacts learning efficiency, with larger margins (κ) yielding better regret guarantees. The algorithm and analysis extend classical optimism techniques to risk-sensitive objectives, providing a principled framework for quantile-optimal learning in safety-critical applications.

## Method Summary
The algorithm maintains per-(h,s,a) visit counts and empirical transitions, constructs ℓ1 confidence balls with radius f_δ(n) = c√(log(2SATH/δ)/max{1,n}), and builds C^t_δ as the set of transitions within these confidence bounds. It intersects this with C_κ, the set of models respecting a quantile margin assumption (ensuring local Lipschitz regularity). The optimistic planner solves max_π max_{P∈C̄^t_δ,κ} V^{π,P}_{τ,0}(s̄), rolling out the resulting policy for one episode before updating counts. The paper suggests combining QMDP backward DP with EVI-style optimistic selection for the planner, though implementation details are deferred to future work.

## Key Results
- UCB-QRL achieves O((2/κ)^(H+1) H √(SAT H log(2SAT H/δ))) high-probability regret bound
- Regret bound explicitly depends on κ, the problem's quantile sensitivity parameter
- Algorithm extends classical UCB to quantile objectives, handling non-smooth Bellman backups
- Confidence sets constructed using Weissman's ℓ1 concentration with explicit radius formula
- Quantile margin assumption enables propagation of confidence bounds through horizon

## Why This Works (Mechanism)

### Mechanism 1
Optimistic planning over statistically plausible models achieves sublinear regret for quantile objectives. The algorithm constructs ℓ1 confidence balls around empirical transition estimates and selects the policy–model pair that maximizes the τ-quantile value within this set. By construction, the true model lies in the confidence set with high probability (≥1−δ), so optimism guarantees the planned value upper-bounds the optimal quantile value.

### Mechanism 2
The quantile margin κ controls local Lipschitz regularity of the quantile backup, enabling propagation of confidence-set membership through the horizon. Under the margin condition, the CDF of the continuation-mixture has a jump ≥ κ at the τ-quantile, yielding Q_τ(ν) - Q_τ(μ) ≤ (2/κ)W_1(μ, ν). The factor (2/κ)^H arises from iterating this sensitivity bound across H stages.

### Mechanism 3
The continuation-mixture representation converts multi-step quantile backup analysis into single-step Wasserstein distance control. Definition 1 constructs Z_{s,a,h}(p; V^{π,P}_{·,h+1}) whose CDF is Σ_i p_i ϕ_i(x). Lemma 10 shows this equals (in law) V^{π,P}_{U,h+1}(S_{h+1}) using a single uniform auxiliary variable U, enabling coupling-based W_1 bounds.

## Foundational Learning

**Quantile value functions in MDPs (QMDP)**
Why needed: The objective maximizes V^{π,P⋆}_{τ,0}(s̄) (τ-quantile of cumulative return), not expected return. Standard Bellman equations don't apply.
Quick check: Can you explain why Q_τ(X) := inf{x : P(X ≤ x) ≥ τ} differs from E[X] and why the backup operator for quantiles is non-smooth?

**Optimism-in-the-face-of-uncertainty (UCB/UCRL2)**
Why needed: UCB-QRL adapts classical UCB to quantile objectives. Understanding EVI and confidence-set planning is prerequisite.
Quick check: How does UCRL2 construct confidence sets, and why does optimism guarantee low regret when the true model is in the confidence set?

**Wasserstein distance and total variation**
Why needed: Lemmas 6-9 bound quantile sensitivity via W_1, controlled by TV, controlled by ℓ1 norm on transition vectors.
Quick check: What is the relationship W_1(μ,ν) ≤ H·TV(μ,ν) for measures supported on [0,H], and why does this enable the regret analysis?

## Architecture Onboarding

**Component map:**
Estimator -> Confidence builder -> Optimistic planner -> Executor

**Critical path:**
1. Initialize counts → 2. Build confidence set C̄^t+1_δ,κ → 3. Run quantile-aware optimistic planner → 4. Execute π^{t+1} → 5. Update counts → loop

**Design tradeoffs:**
- κ knowledge: The algorithm's bound requires κ, but κ is problem-dependent. Practical heuristic: start with κ_0=1 and decrease if empirical regret appears linear.
- Planner complexity: Exact optimistic planning (Line 9) requires solving a max-max problem over π and P. The paper notes this is open; suggested direction is QMDP DP + EVI.
- Computational vs statistical efficiency: Tighter confidence sets (e.g., Bernstein bonuses) could remove √H factor but require variance estimation.

**Failure signatures:**
- Linear regret with no convergence: κ may be misspecified (too large); the true model may violate margin condition. Reduce κ adaptively.
- Excessively conservative policies: Overly large confidence radius c causes excessive exploration; tune c or use per-episode adaptive δ.
- Numerical instability in planner: Quantile backups involve suprema over set-valued maps; ensure discretization of [0,1] for q-levels.

**First 3 experiments:**
1. Synthetic MDP with known κ: Construct a finite-horizon MDP with deterministic rewards and explicitly compute κ from the continuation-mixture CDF. Validate that regret scales as O((2/κ)^H √(SATH)) by varying H and κ independently.
2. Ablation on κ-awareness: Run UCB-QRL with C̄^t_δ,κ vs. C^t_δ only (ignoring margin). The paper conjectures the latter yields linear regret on some MDP instances.
3. Comparison to risk-neutral UCBVI: On an MDP where optimal τ-quantile policy differs from optimal expectation policy, compare UCB-QRL vs UCBVI in terms of τ-quantile regret (not expected regret).

## Open Questions the Paper Calls Out

**Open Question 1**
Are the $(2/\kappa)^H$ and $\sqrt{SAT H}$ dependencies in the regret bound information-theoretically optimal, or can they be improved? Whether this dependence can be improved is open and requires establishing minimax and instance-dependent lower bounds for quantile-regret under margin assumptions.

**Open Question 2**
Can UCB-QRL be extended to infinite-horizon discounted settings with stationary policies and convergence guarantees? This requires developing a discounted continuation-mixture operator, an appropriate contraction/sensitivity inequality for the quantile backup, and stationary $\ell_1$-confidence sets for model uncertainty.

**Open Question 3**
Is the quantile margin assumption (Assumption 1) necessary for achieving sublinear regret, or can it be relaxed? The paper conjectures that Assumption 1 is necessary to achieve sublinear regret for Quantile MDPs, but no formal necessity proof or counterexample construction exists.

**Open Question 4**
Can Bernstein-type confidence bonuses improve the $\sqrt{H}$ factor in the regret bound for quantile objectives? A formal analysis is left to future work, though the paper suggests this may remove the extra $\sqrt{H}$ factor.

## Limitations

- The algorithm's optimistic planning step (Line 9) lacks explicit pseudocode or implementation details, relying on future work combining Li et al. DP with EVI
- The quantile margin κ is assumed known, though it's problem-dependent in practice, with only heuristic adaptive reduction suggested
- The theoretical bound assumes deterministic rewards, with stochastic rewards noted as a minor extension without formal analysis

## Confidence

- High confidence: Core regret bound derivation, confidence set construction, and quantile margin assumption (Section 6.1-6.2)
- Medium confidence: Practical feasibility of optimistic planning (Algorithm 1, Line 9) due to missing implementation details
- Medium confidence: Adaptive κ estimation heuristics (Section 5.3) lacking principled procedure

## Next Checks

1. Implement a simple Chain MDP with deterministic rewards, compute ground-truth κ from the continuation-mixture CDF, and verify that regret scales as O((2/κ)^H √(SATH)) by varying H and κ independently.

2. Test UCB-QRL with and without the margin-respecting constraint C_κ. The paper conjectures ignoring the margin yields linear regret on some MDPs—validate this empirically.

3. Compare UCB-QRL vs UCBVI on an MDP where optimal τ-quantile policy differs from optimal expectation policy, measuring τ-quantile regret (not expected regret) to test if quantile-aware optimism materially changes exploration.