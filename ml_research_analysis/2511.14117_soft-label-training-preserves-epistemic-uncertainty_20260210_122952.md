---
ver: rpa2
title: Soft-Label Training Preserves Epistemic Uncertainty
arxiv_id: '2511.14117'
source_url: https://arxiv.org/abs/2511.14117
tags:
- training
- soft-label
- annotation
- entropy
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper challenges the standard practice of collapsing multi-annotator\
  \ label distributions into single \"hard\" labels, arguing that annotation distributions\
  \ themselves represent the true ground truth for ambiguous data. By training models\
  \ to predict full annotation distributions rather than majority-vote labels, soft-label\
  \ training produces models that better preserve epistemic uncertainty\u2014matching\
  \ human-level disagreement patterns rather than expressing false confidence."
---

# Soft-Label Training Preserves Epistemic Uncertainty

## Quick Facts
- **arXiv ID**: 2511.14117
- **Source URL**: https://arxiv.org/abs/2511.14117
- **Reference count**: 28
- **Primary result**: Soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy while maintaining equivalent accuracy to hard-label training.

## Executive Summary
This paper challenges the conventional practice of collapsing multi-annotator label distributions into single "hard" labels, arguing that annotation distributions themselves represent the true ground truth for ambiguous data. By training models to predict full annotation distributions rather than majority-vote labels, soft-label training produces models that better preserve epistemic uncertainty—matching human-level disagreement patterns rather than expressing false confidence. The approach repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.

## Method Summary
The core innovation involves training models to predict full annotation distributions rather than single majority-vote labels. Instead of using standard cross-entropy loss with hard labels, the method employs a loss function that minimizes the divergence between the model's predicted distribution and the empirical distribution of human annotations. This allows models to capture and express epistemic uncertainty by reproducing the same patterns of disagreement observed in human annotators, rather than forcing confident predictions on ambiguous data points.

## Key Results
- Soft-label training achieves 32% lower KL divergence from human annotations compared to hard-label training
- Models show 61% stronger correlation between model and annotation entropy
- Performance matches hard-label training accuracy while reducing overfitting and enabling longer effective training

## Why This Works (Mechanism)
Soft-label training works by aligning model behavior with human epistemic uncertainty. When humans disagree on labels for ambiguous data, this disagreement reflects genuine uncertainty about the correct classification. By training models to reproduce these disagreement patterns rather than forcing confident predictions, the approach ensures models express appropriate uncertainty levels. The method captures the full information content of annotation distributions, which standard hard-label approaches discard during the aggregation step.

## Foundational Learning
- **Epistemic uncertainty**: Uncertainty due to lack of knowledge or ambiguity in the data; needed to understand what the method aims to preserve
- **Aleatoric uncertainty**: Inherent randomness in the data; quick check: distinct from epistemic uncertainty, which the method targets
- **KL divergence**: Measure of difference between probability distributions; needed to quantify how well models match annotation distributions
- **Entropy correlation**: Statistical measure of relationship between distributions; needed to evaluate preservation of uncertainty patterns
- **Cross-entropy loss**: Standard classification loss; needed as baseline for comparison
- **Majority voting**: Common aggregation method for multi-annotator data; needed to understand what soft-label training replaces

## Architecture Onboarding

**Component map**: Raw annotations → Distributional loss → Model outputs → KL divergence minimization

**Critical path**: The essential training loop consists of: 1) obtaining multi-annotator labels for each training example, 2) computing empirical label distribution, 3) forward pass through model to get predicted distribution, 4) computing KL divergence between empirical and predicted distributions, 5) backpropagation using this divergence as loss.

**Design tradeoffs**: The approach trades increased output dimensionality (predicting full distributions vs single labels) for better uncertainty preservation. This requires more parameters and potentially more complex loss functions, but claims comparable training times. The method assumes annotation distributions represent true epistemic uncertainty rather than annotator noise or systematic bias.

**Failure signatures**: Models may underperform when annotation distributions reflect annotator unreliability rather than genuine ambiguity, or when the task expects consensus despite inherent uncertainty. The approach could also fail to scale efficiently to very large output spaces.

**First experiments**:
1. Train a simple CNN on CIFAR-10 with soft labels derived from 5 annotators per image
2. Compare KL divergence between model predictions and annotation distributions versus hard-label baseline
3. Measure entropy correlation between model and annotation distributions

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that annotation distributions represent epistemic uncertainty may not hold for all annotation scenarios, particularly with non-expert annotators or tasks expecting consensus
- Computational overhead from predicting full distributions could impact scalability for very large datasets or models
- Evaluation metrics capture distributional similarity but may not fully reflect practical downstream task performance

## Confidence
- **High confidence**: Empirical demonstration of lower KL divergence and stronger entropy correlation compared to hard-label training
- **Medium confidence**: Claim of maintaining equivalent accuracy to hard-label training across different architectures and tasks
- **Medium confidence**: Philosophical framing that annotation distributions represent "true ground truth" for ambiguous data

## Next Checks
1. **Ablation study on annotator expertise**: Compare soft-label training performance when using expert versus crowd-sourced annotations to determine robustness to annotation quality variations
2. **Downstream task generalization**: Test whether models trained with soft labels maintain epistemic uncertainty preservation when fine-tuned on downstream tasks
3. **Scale-up validation**: Evaluate computational efficiency and uncertainty preservation at scale using larger datasets and model architectures