---
ver: rpa2
title: 'OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian
  Optimization in TinyML Keyword Spotting'
arxiv_id: '2512.19739'
source_url: https://arxiv.org/abs/2512.19739
tags:
- oasi
- initialization
- size
- tinyml
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles multi-objective hyperparameter optimization for
  tinyML keyword spotting, where models must balance high accuracy with strict size
  constraints. It introduces Objective-Aware Surrogate Initialization (OASI), which
  uses Multi-Objective Simulated Annealing to seed Bayesian optimization with diverse,
  Pareto-relevant candidates, unlike generic sampling methods.
---

# OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting

## Quick Facts
- arXiv ID: 2512.19739
- Source URL: https://arxiv.org/abs/2512.19739
- Authors: Soumen Garai; Suman Samui
- Reference count: 12
- Primary result: OASI achieves highest hypervolume (0.0627) and lowest generational distance (0.0) for TinyML keyword spotting under tight evaluation budgets.

## Executive Summary
This work tackles multi-objective hyperparameter optimization for tinyML keyword spotting, where models must balance high accuracy with strict size constraints. It introduces Objective-Aware Surrogate Initialization (OASI), which uses Multi-Objective Simulated Annealing to seed Bayesian optimization with diverse, Pareto-relevant candidates, unlike generic sampling methods. Evaluated on Google Speech Commands v2 with a depthwise separable CNN, OASI consistently outperforms LHS, Sobol, and random initialization, achieving the highest hypervolume (0.0627) and lowest generational distance (0.0) under tight evaluation budgets. Statistical analysis confirms superior consistency, delivering top Pareto-front models with >90% accuracy and <0.11 MB size, demonstrating the critical role of objective-aware initialization in constrained tinyML deployment.

## Method Summary
OASI addresses multi-objective Bayesian optimization (MOBO) for tinyML keyword spotting by generating objective-aware initial samples through Multi-Objective Simulated Annealing (MOSA). The method uses dual acceptance criteria (accuracy and size) to guide MOSA chains toward Pareto-relevant regions, then selects a diverse subset via maximin rules for MOBO initialization. This approach seeds Gaussian process surrogates with informative points near the accuracy-size trade-off frontier, improving Expected Hypervolume Improvement acquisition. The method was evaluated on Google Speech Commands v2 using a depthwise separable CNN with hyperparameters including layers (1-3), filters (16-64), and dropout, optimizing for accuracy while minimizing model size under 2 MB.

## Key Results
- OASI achieved the highest hypervolume (0.0627) compared to LHS (0.0587), Sobol (0.0588), and Random (0.0516) initialization methods.
- OASI achieved the lowest generational distance (0.0) indicating superior Pareto front convergence.
- Models initialized with OASI consistently delivered >90% accuracy with <0.11 MB size under tight evaluation budgets.
- Statistical analysis confirmed OASI's superior consistency, though differences were not significant at α=0.05 threshold (Kruskal-Wallis p=0.144).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Objective-aware initialization improves MOBO convergence under tight evaluation budgets.
- Mechanism: OASI uses Multi-Objective Simulated Annealing (MOSA) with dual acceptance criteria—probabilistic acceptance for accuracy improvements (p_acc = 1 if A(h_next) > A(h_curr), else exp(-ΔA/T_acc)) AND size reductions (p_size = 1 if S(h_next) < S(h_curr), else exp(-ΔS/T_size)). A candidate must satisfy both criteria (u_acc < p_acc ∧ u_size < p_size) to be accepted, explicitly encoding the accuracy–size trade-off into seed generation.
- Core assumption: Assumes short MOSA chains (40–50 iterations) can identify Pareto-relevant regions more efficiently than uniform sampling, which holds when the search space contains exploitable structure near the Pareto front.
- Evidence anchors:
  - [abstract] "leverages Multi-Objective Simulated Annealing (MOSA) to generate a seed Pareto set of high-performing and diverse configurations that explicitly balance accuracy and model size"
  - [section] Algorithm 2 lines 7–12: dual acceptance criteria requiring both accuracy and size conditions
  - [corpus] Limited direct corpus support; related work "Informed Initialization for Bayesian Optimization" addresses initialization quality but not MOSA specifically
- Break condition: If the objective landscape is highly multimodal with disconnected Pareto regions, short MOSA chains may converge to local fronts, reducing seed diversity.

### Mechanism 2
- Claim: Diverse subset selection from MOSA archive prevents surrogate bias.
- Mechanism: After MOSA chains complete, OASI applies a maximin diversity rule (line 16: SelectDiverseSubset) to select n initial points from archive I_A. This balances objective-awareness (points from MOSA) with space-filling coverage, preventing surrogate posterior collapse to narrow regions.
- Core assumption: Assumes the maximin criterion effectively balances diversity against objective quality; trade-off parameterization is not detailed in the paper.
- Evidence anchors:
  - [section] "D_0 is selected from I_A using a maximin rule to ensure broad coverage of H"
  - [section] Table I: OASI provides "Pareto-focused coverage; robust, repeatable initialization under tight budgets"
  - [corpus] "SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization" addresses multi-objective initialization via meta-learning, suggesting diversity-quality trade-offs are recognized but not directly validating maximin
- Break condition: If n (initial sample size) is too small relative to search space dimensionality, even diverse selection may miss critical Pareto regions.

### Mechanism 3
- Claim: Better-initialized surrogates yield more effective Expected Hypervolume Improvement (EHVI) acquisition.
- Mechanism: OASI's objective-aware seeds provide GP surrogates with informative posterior means/variances early. This improves EHVI acquisition (h_next = argmax α_EHVI(h)) because hypervolume improvement estimates depend on surrogate uncertainty—better initial coverage of the Pareto-relevant region reduces wasted exploration.
- Core assumption: Assumes the GP kernel and hyperparameters are appropriate for the structured, mixed search space; the paper does not detail GP configuration.
- Evidence anchors:
  - [section] "MOBO alleviates this cost by fitting Gaussian process (GP) surrogates for each objective... The quality of the initialization set D_0 is critical, as it determines the early posterior means and variances"
  - [section] Fig. 4 shows OASI seeds clustered near the Pareto front versus scattered LHS/Sobol/Random points
  - [corpus] "Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors" discusses surrogate quality effects on acquisition, supporting the general principle
- Break condition: If GP surrogates are poorly specified (e.g., inappropriate kernel for discrete hyperparameters), even high-quality initialization cannot compensate.

## Foundational Learning

- Concept: **Multi-Objective Optimization and Pareto Dominance**
  - Why needed here: OASI operates in a bi-objective space (maximize accuracy, minimize size). Understanding that Pareto-optimal solutions are non-dominated (no solution is strictly better on all objectives) is essential to interpret hypervolume and generational distance metrics.
  - Quick check question: Given two models A (89% accuracy, 0.08 MB) and B (92% accuracy, 0.12 MB), which dominates which? (Answer: Neither dominates—trade-off exists.)

- Concept: **Gaussian Process Surrogates in Bayesian Optimization**
  - Why needed here: MOBO relies on GP surrogate models to approximate expensive objective functions. Understanding how GP posterior uncertainty drives acquisition (exploration vs. exploitation) clarifies why initialization quality matters.
  - Quick check question: Why does a GP surrogate with high posterior variance in unexplored regions encourage exploration? (Answer: Acquisition functions like EI/EHVI weigh improvement potential against uncertainty.)

- Concept: **Simulated Annealing Acceptance Criteria**
  - Why needed here: OASI adapts MOSA with temperature-controlled probabilistic acceptance. Understanding the temperature schedule (cooling rate α) and Metropolis criterion explains how exploration transitions to exploitation.
  - Quick check question: If T → 0 in simulated annealing, what happens to the acceptance probability for worse solutions? (Answer: p → 0, so only improving moves are accepted.)

## Architecture Onboarding

- Component map: Search Space H → OASI/MOSA Initializer → Initial Archive I_A → Maximin Selection → D_0 (n seeds) → GP Surrogates (GP_acc, GP_size) → EHVI Acquisition → h_next → Model Training (DS-CNN on D_tr) → θ*(h) → Evaluation on D_val → (Acc, Size) → Archive Update → Pareto Front Extraction
- Critical path: OASI initialization → GP surrogate quality → EHVI acquisition efficiency → Pareto front quality under budget T
- Design tradeoffs:
  - Computation time: OASI adds ~29% overhead (1934s vs. ~1500s) for initialization; acceptable when evaluation budget is tight but may not scale to very large search spaces.
  - Statistical significance: Kruskal-Wallis p=0.144 indicates differences are not significant at α=0.05; OASI's advantage is consistency, not magnitude.
  - Generalization: Tested only on DS-CNN with GSC v2; Assumption: mechanism transfers to other architectures/tasks is unvalidated.
- Failure signatures:
  - Hypervolume stagnates early → initialization may be too narrow; increase MOSA chain count or cooling schedule.
  - High generational distance → surrogate is misfitted; check GP kernel appropriateness for discrete hyperparameters.
  - Inconsistent Pareto fronts across runs → MOSA temperature parameters (T_0, α) may need tuning for problem landscape.
- First 3 experiments:
  1. Reproduce OASI vs. LHS/Sobol/Random on GSC v2 with DS-CNN; verify hypervolume (target: OASI ~0.0627) and generational distance (target: OASI ~0.0).
  2. Ablate MOSA chain parameters (N_chains, N_iter, T_0, α) to measure sensitivity; track initialization overhead vs. final hypervolume.
  3. Transfer OASI to a different TinyML task (e.g., image classification on CIFAR-10 with MobileNet) to test generalization; compare against same baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can OASI demonstrate a statistically significant advantage over standard initialization methods given the reported Kruskal-Wallis test result ($p=0.144$) failed to meet the $\alpha=0.05$ threshold?
- Basis: [explicit] The abstract and conclusion note that while OASI performs best, statistical analysis confirmed a "non-significant overall difference with respect to the $\alpha=0.05$ threshold."
- Why unresolved: The current number of experimental runs may lack the statistical power to definitively reject the null hypothesis despite the observed improvements in hypervolume.
- What evidence would resolve it: A larger scale study with increased iterations or runs to achieve $p < 0.05$ in non-parametric testing.

### Open Question 2
- Question: At what evaluation budget size does the benefit of OASI initialization diminish compared to generic sampling methods?
- Basis: [explicit] Table I lists a limitation: "gains diminish with large budgets."
- Why unresolved: The study focuses on "tight evaluation budgets" typical of TinyML, but the exact inflection point where simpler methods (like Sobol) catch up due to the surrogate model maturing is not defined.
- What evidence would resolve it: A sensitivity analysis plotting performance gap vs. total budget size to find the crossover point.

### Open Question 3
- Question: Does OASI transfer effectively to other TinyML tasks with different objective landscape structures, such as visual wake-word detection?
- Basis: [explicit] The conclusion states the technique applies to the "majority of TinyML applications" despite being implemented solely for Keyword Spotting (KWS).
- Why unresolved: The efficiency of MOSA-based seeding relies on the specific trade-off structure between accuracy and size in KWS; different data modalities may yield different Pareto geometries.
- What evidence would resolve it: Benchmarks on non-audio TinyML datasets (e.g., Visual Wake Words) comparing OASI against LHS/Sobol.

## Limitations
- Statistical significance: Kruskal-Wallis test shows p=0.144, failing to meet α=0.05 threshold for definitive significance despite observed improvements.
- Overhead concerns: OASI adds ~29% initialization overhead, which may become prohibitive for larger search spaces.
- Limited generalization: Tested only on DS-CNN architecture with Google Speech Commands v2, making cross-task transfer assumptions unverified.

## Confidence
- High confidence: OASI improves initialization diversity and Pareto-front proximity compared to LHS/Sobol/Random (supported by Figure 4 and Table I).
- Medium confidence: The MOSA dual-acceptance mechanism effectively encodes accuracy-size trade-offs into seed generation (mechanism is described but hyperparameter sensitivity is unvalidated).
- Medium confidence: Better initialization yields more effective EHVI acquisition (supported by surrogate quality arguments and related work, but GP kernel specifics are unspecified).
- Low confidence: OASI's mechanism generalizes to architectures beyond DS-CNN (no cross-architecture validation provided).

## Next Checks
1. Perform ablation study on MOSA hyperparameters (N_chains, N_iter, T_0, α) to quantify their impact on initialization quality and final hypervolume.
2. Conduct cross-architecture transfer test: apply OASI to a different TinyML task (e.g., CIFAR-10 with MobileNet) and compare against baselines under identical conditions.
3. Test OASI under varying evaluation budgets (T) to determine the break-even point where initialization overhead is justified by convergence gains.