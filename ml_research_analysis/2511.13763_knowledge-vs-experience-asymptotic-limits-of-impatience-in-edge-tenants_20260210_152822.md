---
ver: rpa2
title: 'Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants'
arxiv_id: '2511.13763'
source_url: https://arxiv.org/abs/2511.13763
tags:
- queue
- reneging
- jockeying
- time
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analytically and empirically compares two information\
  \ feeds for guiding reneging and jockeying decisions in a dual M/M/1 queue system:\
  \ (1) a closed-form Markov estimator of residual sojourn time and (2) an online-trained\
  \ actor\u2013critic policy. Under a total-time patience model with unequal service\
  \ rates, the authors prove that as queue backlog grows, unconditional abandonment\
  \ probability approaches 1 and the probability of a successful jockey approaches\
  \ 0, regardless of the information model."
---

# Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants

## Quick Facts
- **arXiv ID:** 2511.13763
- **Source URL:** https://arxiv.org/abs/2511.13763
- **Reference count:** 17
- **Primary result:** Under total-time patience with unequal service rates, abandonment probability approaches 1 and successful jockey probability approaches 0 as queue backlog grows, regardless of information model quality.

## Executive Summary
This paper analytically and empirically compares two information feeds for guiding reneging and jockeying decisions in a dual M/M/1 queue system: (1) a closed-form Markov estimator of residual sojourn time and (2) an online-trained actor–critic policy. Under a total-time patience model with unequal service rates, the authors prove that as queue backlog grows, unconditional abandonment probability approaches 1 and the probability of a successful jockey approaches 0, regardless of the information model. They also show that under a mild sublinear-error condition, both estimators yield the same asymptotic behavior (robustness), though they differ at finite queue lengths. Empirical results confirm these asymptotic limits and quantify finite-backlog differences in delays, reneging rates, and jockeying behavior. The findings indicate that value-of-information is critical in finite regimes but irrelevant asymptotically, guiding lightweight telemetry and decision-logic design for edge systems.

## Method Summary
The paper compares two decision policies for a dual M/M/1 queue under total-time patience: a Markov estimator using Erlang-k CDF formulas and an actor-critic trained via 2-layer ReLU networks (128 units) with softmax actor and scalar critic. The simulator implements Poisson arrivals (λ ∈ {3,5,7,9,11,13,15}), exponential service with heterogeneous rates μ₁ ≠ μ₂, and FCFS discipline. The Markov module computes renege/jockey probabilities via closed-form Erlang(k, μ) and birth-death uniformization; the actor-critic is trained for 100 episodes × 100 epochs using cross-entropy with rewards for successful service vs. abandonment. Both modules observe queue lengths, service rates, and elapsed time, and output action probabilities. The key metric is whether both modules yield identical asymptotic limits under a sublinear estimator-error condition |Ŵᵢ(n) - Wᵢ(n)|/n → 0.

## Key Results
- Under total-time patience with heterogeneous service rates, abandonment probability → 1 as backlog → ∞, regardless of information model quality.
- Probability of successful jockey → 0 as backlog → ∞, regardless of information model quality.
- Under sublinear estimator error, both Markov and actor-critic models yield identical asymptotic behavior, though they differ at finite queue lengths.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Under total-time patience with heterogeneous service rates, abandonment probability approaches 1 as queue backlog grows, regardless of information model quality.
- **Mechanism:** Total wait W₁(n) grows linearly with backlog n (slope 1/μ₁), so for any fixed patience T, Pr{W₁(n) ≤ T} → 0 exponentially fast by large deviations bounds. Since total sojourn W_total ≥ W₁(n), Pr{served before T} ≤ Pr{W₁(n) ≤ T} → 0.
- **Core assumption:** Service times are i.i.d. exponential; patience T is fixed and finite; μ₁ ≠ μ₂ (heterogeneity).
- **Evidence anchors:**
  - [abstract] "total wait grows linearly so abandonment is inevitable...as the backlog approaches towards infinity"
  - [Section II.A] "Each request has a random patience T measured from the instant of entry...remaining patience after switching is T - t₀"
  - [Appendix Proof II.1] Cramér/Chernoff bound applied to Sn/n ≥ x gives exponential tail decay with rate function I(x) = x - 1 - ln(x)
- **Break condition:** If patience scales with n (e.g., T = O(n)) rather than remaining fixed, the linear growth of wait time no longer guarantees abandonment.

### Mechanism 2
- **Claim:** The probability of a successful jockey (switch and complete before patience) vanishes as backlog → ∞.
- **Mechanism:** Any successful jockey event A ⊆ {W_total ≤ T}. Since Pr{W_total ≤ T} → 0 (Mechanism 1), Pr(A) → 0 by sub-probability containment. Time already spent in original queue cannot be "erased" by switching.
- **Core assumption:** Total-time patience (patience clock not reset on jockey); finite alternative queue length m at switch time.
- **Evidence anchors:**
  - [abstract] "probability of a successful jockey approaches 0, regardless of the information model"
  - [Section II.B.2] "For the selfish and impatience tenant, the probability that after jockeying that tenant reneges...Pr_switch_fail"
  - [Appendix Eq. 32-33] "A ⊆ {W_total ≤ T}...Pr(A) ≤ Pr(W_total ≤ T)"
- **Break condition:** If patience resets on jockey (queue-time patience model), or if target queue is empty with high probability, successful jockey probability may remain non-trivial.

### Mechanism 3
- **Claim:** Under sublinear estimator error condition |Ŵᵢ(n) - Wᵢ(n)|/n → 0, both Markov and actor-critic information models yield identical asymptotic reneging/jockeying behavior.
- **Mechanism:** By Slutsky's theorem, Ŵ₁(n)/n → 1/μ₁ regardless of estimator, so pairwise comparisons Ŵ₁(n) > Ŵ₂(m) have the same asymptotic sign as true comparisons W₁(n) > W₂(m). Estimator error is o(n), dominated by linear signal O(n).
- **Core assumption:** Sublinear error growth—estimator bias/variance must not grow linearly with backlog; typical for consistent estimators with O_p(√n) or O_p(1) error.
- **Evidence anchors:**
  - [abstract] "under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness)"
  - [Theorem II.2 Eq. 26] "|Ŵᵢ(n) - Wᵢ(n)|/n → 0 (n → ∞)"
  - [Appendix Proof II.2 Eq. 36] "Ŵᵢ(n)/n = Wᵢ(n)/n + o_p(1) → 1/μᵢ"
- **Break condition:** If estimator has linear bias (e.g., systematic underestimation growing as cn), the sublinear condition fails and asymptotic decisions may diverge.

## Foundational Learning

- **Concept: M/M/1 Queue and Sojourn Time Distribution**
  - Why needed here: The entire analysis builds on Erlang-k distributed waiting times for k customers ahead; understanding E[W] = k/μ is essential for interpreting reneging thresholds.
  - Quick check question: If μ = 2 requests/second and k = 10 customers ahead, what is the expected remaining wait?

- **Concept: Total-Time vs. Queue-Time Patience Models**
  - Why needed here: The asymptotic results critically depend on patience being consumed continuously from entry (total-time), not resetting on jockey.
  - Quick check question: Under total-time patience with T = 5s, if a customer waits 3s before jockeying, what is their remaining patience in the new queue?

- **Concept: Actor-Critic Q-Learning with Softmax Policies**
  - Why needed here: The "experience" model uses a two-layer actor-critic with softmax action selection; understanding Q*(s,a) and policy gradient helps interpret why learned policies differ at finite backlogs.
  - Quick check question: In Eq. 23-24, what does the discount factor γ control in the reneging vs. jockeying Q-functions?

## Architecture Onboarding

- **Component map:**
  - Arrival process → Dual M/M/1 queues → State observation (kᵢ, kⱼ, μᵢ, μⱼ, T, t₀) → Information module (Markov estimator or actor-critic) → Decision logic (renege/jockey probabilities) → Action execution (local processing or queue switch)

- **Critical path:**
  1. Customer arrives, samples patience T, observes queue state (kᵢ, kⱼ)
  2. Information module estimates remaining sojourn (Markov: Erlang-k CDF; Actor-critic: forward pass)
  3. Decision logic evaluates Pr{renege} and Pr{jockey} using Eqs. 3-4 (Markov) or policy πθ(a|s) (actor-critic)
  4. If jockey: transit time, arrival at target queue position, resume with remaining patience T - t₀
  5. If renege: local processing takes T_local

- **Design tradeoffs:**
  - Markov estimator: zero training cost, noisier decisions (Figure 2), accurate asymptotics
  - Actor-critic: training overhead (~20-40 episodes), smoother decisions, slightly lower reneging at finite backlogs, same asymptotics
  - Telemetry frequency: more updates help at finite backlogs, irrelevant asymptotically (value-of-information vanishes)
  - Assumption: The paper does not quantify control-plane overhead bytes/sec for each approach.

- **Failure signatures:**
  - Non-arrival-dominated regime: if λ ≈ μ (near-saturation), queue growth is slow and finite-regime behavior persists longer
  - Patience scaling failure: if T scales with workload, asymptotic limits may not apply
  - Actor-critic divergence: if training episodes < 20, losses haven't stabilized (Figure 3), policy may be inconsistent

- **First 3 experiments:**
  1. **Baseline validation:** Implement dual M/M/1 with λ ∈ {3,5,7,9,11,13,15}, μ₁ ≠ μ₂, T fixed; measure reneging/jockeying rates vs. backlog. Confirm Pr{renege} → 1 as n → ∞.
  2. **Finite-regime comparison:** At queue lengths 5-50, compare Markov vs. actor-critic reneging rates. Quantify the multiplicative gap in jockeying rates (Figure 2, log-scale differences).
  3. **Sensitivity to sublinear condition:** Inject controlled estimator bias ε·n (linear) vs. ε·√n (sublinear); verify that linear bias breaks asymptotic robustness while sublinear preserves it.

## Open Questions the Paper Calls Out

- **Question:** How does the cost of telemetry (signaling overhead, energy, compute) alter the optimal choice between analytic estimators and learned policies in finite regimes?
  - **Basis in paper:** [explicit] The authors conclude that "bridging theory to deployment requires compact testbeds that measure signaling cost (bytes/sec)... so one can quantify value-of-information per byte."
  - **Why unresolved:** The current work compares the decision logic of the two feeds in isolation, treating information acquisition as a given, whereas real systems face strict resource budgets for control plane communication.
  - **What evidence would resolve it:** An extension of the current simulation framework that incorporates a cost function for state observation, demonstrating the net utility of each method under constrained bandwidth or energy budgets.

- **Question:** To what extent do action latency and information staleness degrade the relative performance of the actor-critic compared to the Markov estimator?
  - **Basis in paper:** [explicit] The paper notes the need to measure "freshness (age/error), action latency, and end-to-end key performance indicators" to bridge the gap between theory and deployment.
  - **Why unresolved:** The simulation assumes instantaneous state observation and action execution, ignoring the delays inherent in edge environments which may disproportionately affect the multi-step actor-critic updates or render the "current" queue state invalid.
  - **What evidence would resolve it:** Empirical results from a physical testbed or simulations that inject propagation delays and processing latency between the observation of queue state $s_t$ and the execution of action $a_t$.

- **Question:** Do the asymptotic limits of inevitable abandonment (Theorem II.2) hold for non-Markovian service distributions, such as heavy-tailed or deterministic service times?
  - **Basis in paper:** [inferred] The theoretical proofs rely on the memoryless property of the exponential service times (Erlang distribution) to derive the linear growth of wait times and the sublinear error condition.
  - **Why unresolved:** Real-world edge computing workloads often exhibit variability that violates the M/M/1 assumption, and it is unclear if the "robustness" of the information models persists when service variance changes.
  - **What evidence would resolve it:** A generalization of the proofs in the Appendix to M/G/1 or G/G/1 queuing systems, or simulations showing that the convergence of reneging rates remains stable under Weibull or Pareto service distributions.

## Limitations
- The paper does not specify the distribution of patience T beyond it being finite and fixed.
- The analysis is limited to dual queues and does not explore extensions to multiple queues or more complex topologies.
- All results assume i.i.d. exponential service times, which may not hold for real-world edge workloads.

## Confidence
- **High confidence:** Asymptotic abandonment (Pr{renege} → 1) and vanishing jockeying (Pr{successful jockey} → 0) under total-time patience.
- **Medium confidence:** Asymptotic equivalence of information models under sublinear error.
- **Low confidence:** Practical impact of finite-backlog differences.

## Next Checks
1. **Robustness to patience distribution:** Re-run simulations with Pareto-distributed patience (instead of fixed T) and verify whether asymptotic limits still hold.
2. **Estimator bias sensitivity:** Inject controlled linear bias into the Markov estimator and measure its effect on asymptotic behavior to test the sublinear-error condition.
3. **Multi-queue extension:** Extend the simulator to three queues and empirically test whether the "robustness" conclusion extends beyond the dual-queue case.