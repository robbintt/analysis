---
ver: rpa2
title: 'FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval'
arxiv_id: '2507.07135'
source_url: https://arxiv.org/abs/2507.07135
tags:
- image
- fashion
- images
- dataset
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FACap, a large-scale fashion dataset for composed
  image retrieval (CIR), addressing the challenge of insufficient high-quality data
  in the fashion domain. The dataset is constructed using a two-stage annotation pipeline
  powered by a vision-language model (VLM) and a large language model (LLM), generating
  detailed modification texts for image pairs.
---

# FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval

## Quick Facts
- arXiv ID: 2507.07135
- Source URL: https://arxiv.org/abs/2507.07135
- Reference count: 40
- The paper introduces FACap, a large-scale fashion dataset for composed image retrieval (CIR), and proposes FashionBLIP-2, achieving state-of-the-art results on the Fashion IQ benchmark.

## Executive Summary
This paper addresses the challenge of insufficient high-quality data for fashion-domain composed image retrieval (CIR). The authors introduce FACap, a large-scale dataset of 227,680 synthetic triplets constructed using a two-stage annotation pipeline powered by vision-language models (VLMs) and large language models (LLMs). They also propose FashionBLIP-2, which fine-tunes BLIP-2 with lightweight adapters and a multi-head query-candidate matching mechanism to capture fine-grained fashion-specific details. Experimental results demonstrate significant performance improvements over existing methods, particularly for retrieval with detailed modification texts, establishing new state-of-the-art results on the Fashion IQ benchmark.

## Method Summary
The method involves constructing FACap through a two-stage pipeline: first, a VLM generates detailed captions for individual images using noisy metadata to reduce hallucination; second, an LLM synthesizes modification texts by comparing paired captions. FashionBLIP-2 is built by fine-tuning BLIP-2 with lightweight adapter modules inserted into the frozen ViT backbone, and a multi-head matching mechanism that compares multiple feature views instead of a single global embedding. The model is trained in two stages: first on FACap with CIR and CTR contrastive losses, then fine-tuned on Fashion IQ with only CIR loss.

## Key Results
- FACap dataset contains 227,680 synthetic triplets constructed from Fashion200k and DeepFashion-MultiModal images.
- FashionBLIP-2 achieves state-of-the-art performance on Fashion IQ benchmark, with significant improvements in Recall@10 and Recall@50.
- The multi-head matching mechanism improves fine-grained retrieval accuracy by preserving localized fashion details compared to global average pooling.

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Synthetic Triplet Generation
The pipeline decouples image understanding from relative reasoning. First, a VLM generates detailed captions for individual images, conditioned on noisy metadata to reduce hallucination. Second, an LLM processes only the text of paired captions to synthesize a concise "modification text" describing the delta between reference and target. Core assumption: VLMs perform better at grounded single-image description than at direct multi-image comparison reasoning. Evidence: two-stage approach described in Section 3.1. Break condition: VLM hallucination errors propagate to modification texts.

### Mechanism 2: Adapter-Based Domain Specialization
Lightweight adapter modules allow a general-purpose ViT to capture fine-grained fashion features without catastrophic forgetting. The mechanism freezes pre-trained ViT weights and injects trainable bottleneck layers into each transformer layer. Core assumption: low-rank updates are sufficient to shift feature space for domain-specific retrieval. Evidence: adapter implementation in Section 4.2. Break condition: bottleneck dimension too small to encode fashion complexity.

### Mechanism 3: Multi-Head Query-Candidate Matching
Retrieval accuracy improves by comparing multiple distinct feature "views" rather than a single global average embedding. The mechanism applies token mixing and channel mixing to project embeddings into lower-dimensional space, computing final similarity as sum of cosine similarities across heads. Core assumption: fine-grained fashion details are localized in different token embeddings. Evidence: multi-head matching described in Section 4.3. Break condition: token mixing weights not trained effectively, causing heads to become redundant.

## Foundational Learning

- **Concept: Composed Image Retrieval (CIR)**
  - Why needed: This is the core task definition. Unlike text-to-image (T2I) or image-to-image, CIR requires fusing a visual reference with a textual modification.
  - Quick check: How does CIR differ from standard text-to-image retrieval regarding the role of the text encoder?

- **Concept: BLIP-2 and the Q-Former**
  - Why needed: The paper modifies BLIP-2. The Q-Former acts as a bridge between the Image Encoder and the LLM, using "queries" to extract visual features.
  - Quick check: What is the function of the learnable queries in the Q-Former architecture, and what dimensionality do they produce?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed: The model is trained using L_CIR and L_CTR, both contrastive losses. You need to understand how the model pulls positive pairs closer and pushes negative pairs apart.
  - Quick check: In the context of the CIR loss L_CIR, what constitutes a "negative pair" during training?

## Architecture Onboarding

- **Component map:** Reference Image + Modification Text → ViT-G (Frozen) + Adapters → Q-Former → Multi-Head Matching → Candidate Image → ViT-G (Frozen) → Q-Former → Multi-Head Matching → Similarity Score

- **Critical path:** Loss → Multi-Head Matching → Q-Former → Adapters (stopping at frozen ViT). Efficiency comes from updating only Adapters and Q-Former (~1-5% of total params) while keeping ViT frozen.

- **Design tradeoffs:**
  - Synthetic vs. Real Data: Trades authenticity for massive scale (227k triplets) and detailed granularity.
  - Global vs. Multi-Head: Trades simplicity for computational overhead to gain fine-grained accuracy.

- **Failure signatures:**
  - Hallucination Propagation: If modification texts describe non-existent differences, model learns to match noise.
  - Head Collapse: If multi-head weights poorly initialized, all heads may learn identical features.
  - Loss of Generalization: If fine-tuned too aggressively on Fashion IQ, model may lose robustness learned from FACap.

- **First 3 experiments:**
  1. Verify Adapter Efficiency: Train with and without adapter modules to confirm domain adaptation hypothesis.
  2. Ablate Matching Strategy: Compare Global Average Pooling vs. Multi-Head Matching to quantify fine-grained token preservation gain.
  3. Stress Test Data Quality: Evaluate on enhFashionIQ benchmark to verify performance with more detailed modification texts.

## Open Questions the Paper Calls Out
- **Can FACap and FashionBLIP-2 be effectively combined with UniFashion's generation-based pretraining tasks?** The authors identify complementary nature but did not integrate them.
- **How can the conflict between CTR task and downstream fine-tuning be mitigated?** CTR improves zero-shot but decreases fine-tuning performance due to "bias towards detailed textual descriptions."
- **What specific data quality metrics or curation strategies are required to overcome diminishing returns from scaling up FACap?** Performance gain from 50% to 100% dataset is small, suggesting need to focus on data quality over volume.

## Limitations
- Synthetic data generation pipeline introduces uncertainty about quality and representativeness, with moderate manual inspection scores (2.90-3.24).
- Multi-head matching mechanism benefits are demonstrated empirically but specific architectural choices appear arbitrary without sensitivity analysis.
- Claim that adapters preserve general knowledge lacks ablation showing catastrophic forgetting on non-fashion domains.

## Confidence
- **Data Quality:** Medium - Synthetic pipeline reduces multi-image reasoning burden but relies heavily on VLMs' accuracy; moderate manual inspection scores indicate potential noise.
- **Adapter Effectiveness:** Medium - Parameter efficiency supports claims but lacks ablation for catastrophic forgetting on non-fashion domains.
- **Multi-Head Matching:** Medium - Empirical improvements (3.35% R@10 gain) demonstrated but architectural choices not justified with sensitivity analysis.

## Next Checks
1. Perform ablation study removing adapters in Stage 2 to test whether frozen domain specialization prevents catastrophic forgetting on general visual concepts.
2. Evaluate model performance when multi-head matching is replaced with simple global average pooling to isolate contribution of fine-grained token preservation.
3. Conduct human evaluation of 100 randomly sampled FACap modification texts to quantify hallucination rates and semantic accuracy compared to ground truth image differences.