---
ver: rpa2
title: 'MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation
  in LLMs'
arxiv_id: '2510.22967'
source_url: https://arxiv.org/abs/2510.22967
tags:
- evaluation
- long-form
- factuality
- debate
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating long-form factuality
  in Large Language Models (LLMs), which is critical in high-stakes domains like biomedicine,
  law, and education. Existing evaluation methods often fail for long-form content
  due to complex reasoning chains and intertwined perspectives.
---

# MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs

## Quick Facts
- arXiv ID: 2510.22967
- Source URL: https://arxiv.org/abs/2510.22967
- Authors: Yucheng Ning; Xixun Lin; Fang Fang; Yanan Cao
- Reference count: 0
- Primary result: Proposes MAD-Fact, a multi-agent debate framework for long-form factuality evaluation that outperforms baselines on LongFact and LongHalluQA benchmarks

## Executive Summary
This paper addresses the challenge of evaluating long-form factuality in Large Language Models (LLMs), which is critical in high-stakes domains like biomedicine, law, and education. Existing evaluation methods often fail for long-form content due to complex reasoning chains and intertwined perspectives. To address this, the authors propose MAD-Fact, a Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs. MAD-Fact incorporates a fact importance hierarchy to capture varying significance of claims in long-form texts and employs a multi-agent debate system to mitigate single-model biases. The framework is evaluated on two benchmarks, LongFact and LongHalluQA, demonstrating that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content.

## Method Summary
MAD-Fact uses a three-stage architecture: Clerk decomposes long-form responses into atomic claims, Jury (three Evaluator agents with distinct professional roles) debates each claim across two rounds using confidence-thresholded retrieval, and Judge aggregates judgments via majority voting with weighted scoring based on fact importance. The framework employs a pyramid model where claims mentioned by multiple expert models receive higher weights, and uses a Serper API for retrieval-augmented verification when agents' confidence falls below a threshold. The system was evaluated on LongFact (English) and LongHalluQA (Chinese) benchmarks using weighted precision, recall, and F1 metrics.

## Key Results
- MAD-Fact outperforms baseline models on both LongFact and LongHalluQA benchmarks
- Larger LLMs generally maintain higher factual consistency across benchmarks
- Domestic Chinese models excel on Chinese content in LongHalluQA
- Fact importance weighting achieves Pearson r=0.701 correlation with human ratings
- Ablation shows debate component contributes 0.06 F1 improvement (0.88 to 0.82)

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent debate reduces single-model verification bias through structured cross-validation. Three Evaluator agents with distinct professional roles sequentially debate each atomic claim, accessing shared message pool and knowledge base. Agents can maintain stance, correct peers, or self-reflect across rounds before majority voting. Core assumption: role diversity creates sufficient perspective variation to expose reasoning errors that single verifier would miss. Evidence: ablation shows removing debate drops F1 from 0.88 to 0.82 on BingCheck TRUE labels. Break condition: if all agents share similar training data biases, debate may reinforce errors rather than correct them.

### Mechanism 2
Weighting facts by importance better aligns evaluation with human judgment than uniform weighting. A pyramid model assigns weights based on how many expert models mention each claim. Claims mentioned by all three receive weight 4; by two receive weight 3, etc. Weighted precision/recall/F1 are computed using these scores. Core assumption: frequency of mention across strong models correlates with factual importance to user. Evidence: weighted F1@γ=0.8 achieves Pearson r=0.701 (p=0.036) correlation with human ratings. Break condition: if expert models share systematic blindspots for domain-specific but important facts, pyramid will underweight them.

### Mechanism 3
Conditional retrieval balances verification accuracy against computational cost. Each Evaluator agent estimates confidence before responding. If confidence ≥ threshold θ, respond directly; if < θ, invoke external search first. Three debate rules govern retrieval: autonomous/optional, mandatory first-round, dynamic based on consensus. Core assumption: models produce reasonably calibrated confidence estimates; retrieval helps most when internal knowledge is uncertain. Evidence: w/o Search drops TRUE-label F1 from 0.88 to 0.84 on BingCheck. Break condition: poorly calibrated confidence causes agents to skip necessary retrieval.

## Foundational Learning

- **Atomic Claim Decomposition**
  - Why needed: Long-form responses must be broken into individually verifiable units before fact-checking.
  - Quick check question: Given "The Zhuang population exceeds 18 million and is mainly distributed in Guangxi," what are two atomic claims the Clerk should extract?

- **Multi-Agent Debate with Shared Memory**
  - Why needed: Understanding how agents access message history and retrieved knowledge across rounds is essential for debugging.
  - Quick check question: In Round 2, can Agent C see what Agents A and B said in Round 1? Can Agent A see what Agent B retrieved?

- **Confidence-Thresholded Decision Making**
  - Why needed: Agents switch between direct response and retrieval based on confidence thresholds.
  - Quick check question: If θ=0.7 and an agent's confidence is 0.65, which response strategy fires?

## Architecture Onboarding

- **Component map:** Clerk Agent -> Jury (3 Evaluator Agents) -> Judge Agent
- **Critical path:** Input (question q_i, response a_i) → Clerk → atomic claims → Jury debate (2 rounds, 3 agents) → individual judgments → Judge aggregates → weighted precision/recall/F1
- **Design tradeoffs:** Agent count N=3 vs. cost (more agents = more API calls); Rule 1 (optional retrieval) vs. Rule 2 (mandatory retrieval): Rule 2 more accurate but higher cost; Single-model initialization (GPT-4o-mini) vs. multi-model: single-model performs better due to easier consensus
- **Failure signatures:** Early wrong consensus in Rule 3 (dynamic termination) causing unrecoverable errors; Multi-model initialization causing higher disagreement and mutual misleading; Role descriptions too similar → low perspective diversity; Search tool returns irrelevant/noisy results
- **First 3 experiments:** 1) Reproduce ablation: Run w/o Debate, w/o Role-Playing, w/o Search on FacToolQA subset to verify component contributions match Table 6; 2) Compare debate rules: Test Rule 1 (free), Rule 2 (mandatory-search), Rule 3 (adaptive) on 50 samples; measure F1 vs. retrieval cost; 3) Sanity check weighting: On 20 LongFact samples, compare unweighted F1 vs. weighted F1@γ=0.8 correlation with your own manual ratings

## Open Questions the Paper Calls Out

### Open Question 1
Can lightweight collaboration strategies—such as dynamically adjusting the number of agents, optimizing debate rounds, or employing model distillation—reduce the computational cost of MAD-Fact without sacrificing evaluation accuracy? Basis: Discussion section states system's reliance on multiple models "increases token consumption and operational costs" and explicitly calls for future work to explore "lightweight collaboration strategies" to support deployment in real-world scenarios. Why unresolved: Current implementation prioritized performance over efficiency to outperform baselines like SAFE, and did not evaluate trade-offs involved in reducing scale of multi-agent system. What evidence would resolve it: Comparative study measuring F1-score versus token consumption/latency for MAD-Fact variants utilizing dynamic agent counts or distilled models.

### Open Question 2
Do mechanisms such as confidence-based weighting or cross-agent consistency verification effectively mitigate "communication hallucinations" (erroneous consensus) in multi-agent debates? Basis: Discussion section identifies "communication hallucinations" as a limitation where agents reach misleading consensus, and suggests specific future research directions including "confidence-based weighting" and "cross-agent consistency verification" to address this. Why unresolved: Paper observes cases (e.g., in Rule 3 evaluations) where early incorrect consensus led to system failure, but did not implement or test proposed mitigation strategies. What evidence would resolve it: Ablation experiments showing reduction in error rates for controversial claims when confidence weighting is applied to agent votes compared to current standard majority voting.

### Open Question 3
How does the MAD-Fact framework perform when applied to uncurated, real-world user-generated content in high-risk domains like biomedicine and finance? Basis: Discussion section notes that current data (LongHalluQA) is derived from curated sources rather than real-world content, and suggests future work "integrate practical real-world sources" to improve generalization. Why unresolved: Current evaluation relies on constructed benchmarks (LongFact, LongHalluQA) which may not capture noise, linguistic variability, or complexity of actual user queries in sensitive domains. What evidence would resolve it: Evaluation results from new benchmark dataset constructed from actual user logs in medical or financial contexts, comparing MAD-Fact against human expert verification.

## Limitations
- Reliance on multiple models increases token consumption and operational costs
- Debate termination rules can cause unrecoverable errors if agents reach wrong consensus early
- Fact importance weighting lacks direct human validation studies
- Performance depends on quality of retrieval results from external search tools

## Confidence
- High confidence: Multi-agent debate reduces single-model bias (supported by ablation showing F1 drop from 0.88 to 0.82 when debate removed)
- Medium confidence: Larger LLMs maintain higher factual consistency (observed across benchmarks but causation not isolated from other factors)
- Low confidence: Fact importance weighting meaningfully improves evaluation (only indirect correlation evidence available)

## Next Checks
1. **Human validation study:** Conduct expert blind evaluation on 50 long-form samples comparing uniform vs. importance-weighted scoring to directly test correlation with human judgment.
2. **Robustness to model biases:** Test MAD-Fact with three Evaluator agents initialized from the same biased model family to assess whether debate amplifies shared blindspots.
3. **Debate rule comparison:** Systematically compare all three debate rules (autonomous, mandatory-retrieval, adaptive) on 100 samples measuring both accuracy and retrieval cost to optimize the tradeoff.