---
ver: rpa2
title: 'FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in
  FL'
arxiv_id: '2502.16396'
source_url: https://arxiv.org/abs/2502.16396
tags:
- poisoning
- attacks
- learning
- fednia
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data poisoning attacks in federated
  learning, where malicious clients can compromise global models by contributing tampered
  updates. The proposed method, FedNIA, introduces a novel defense framework that
  analyzes layerwise activation patterns in client models by injecting random noise
  inputs and using an autoencoder to detect abnormal behaviors indicative of data
  poisoning.
---

# FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in FL

## Quick Facts
- arXiv ID: 2502.16396
- Source URL: https://arxiv.org/abs/2502.16396
- Authors: Ehsan Hallaji; Roozbeh Razavi-Far; Mehrdad Saif
- Reference count: 34
- Primary result: Proposed method achieves overall accuracy of 1.0 under data poisoning attacks while defending against sample poisoning, label flipping, and backdoors without requiring a central test dataset.

## Executive Summary
This paper addresses data poisoning attacks in federated learning where malicious clients compromise global models by contributing tampered updates. The proposed FedNIA framework introduces a novel defense mechanism that analyzes layerwise activation patterns in client models by injecting random noise inputs and using an autoencoder to detect abnormal behaviors indicative of data poisoning. Unlike existing defenses, FedNIA does not require a central test dataset and can defend against diverse attack types, including sample poisoning, label flipping, and backdoors, even with multiple attacking nodes. Experimental results on non-iid federated datasets demonstrate that FedNIA maintains robust performance, achieving an overall accuracy of 1.0 in critical scenarios, significantly outperforming other methods like FedAvg, AdaDP, and DP, which exhibit much lower accuracy under similar attack conditions.

## Method Summary
FedNIA is a preprocessing defense framework that filters malicious client updates before aggregation in federated learning. The server generates random noise inputs and passes them through each client model to extract layerwise activation patterns. These activations are analyzed using a layerwise autoencoder trained on global model activations to detect anomalies. Clients whose activation patterns produce high reconstruction errors are filtered out before FedAvg aggregation. The method claims to defend against sample poisoning, label flipping, and backdoor attacks without requiring a central test dataset, achieving robust performance across various attack scenarios.

## Key Results
- FedNIA achieves overall accuracy of 1.0 under data poisoning attacks while defending against sample poisoning, label flipping, and backdoors
- Maintains robust performance across attack ratios from 2.5% to 20% malicious clients
- Significantly outperforms baseline methods like FedAvg, AdaDP, and DP, which show much lower accuracy under similar attack conditions
- Successfully defends against multiple attack types without requiring a central test dataset

## Why This Works (Mechanism)

### Mechanism 1: Noise-Induced Activation Profiling
Models trained on poisoned data produce distinguishable activation patterns when exposed to random noise inputs, enabling server-side detection without accessing client data. The server generates random noise inputs and passes them through each client model to extract layerwise activations, which are averaged across multiple noise samples to create stable activation profiles that reflect internal model behavior rather than data-dependent outputs. Poisoning attacks alter learned representations in ways that persist even when models process out-of-distribution noise inputs.

### Mechanism 2: Autoencoder-Based Reference Reconstruction
An autoencoder trained on global model activations learns to reconstruct benign activation patterns, failing to reconstruct anomalous patterns from poisoned models. At each FL round, the autoencoder is trained on averaged global model activations as the reference, using layer-wise sub-encoders/decoders to handle varying activation dimensions. The reconstruction loss averages per-layer RMSE, normalizing for layer size differences. The global model (aggregated from mostly benign clients) provides a stable reference; poisoned client activations will deviate sufficiently from this reference.

### Mechanism 3: Statistical Threshold Filtering
Reconstruction errors follow a distribution where malicious clients exhibit significantly higher errors, enabling statistical separation. For each client, compute reconstruction error, and the filtering threshold uses the mean plus scaled standard deviation. Updates with reconstruction errors above the threshold are excluded from aggregation. Malicious activations produce systematically higher reconstruction errors than benign ones; the threshold captures this separation.

## Foundational Learning

- **Federated Averaging (FedAvg)**: Why needed here - FedNIA is designed as a preprocessing filter before FedAvg aggregation; understanding the baseline helps evaluate overhead and compatibility. Quick check: Can you explain how FedAvg aggregates local updates and why it's vulnerable to poisoning?

- **Autoencoder Reconstruction Loss**: Why needed here - The core detection mechanism relies on reconstruction error as an anomaly signal; intuition about what autoencoders learn to reconstruct is essential. Quick check: Why would an autoencoder trained on normal activations fail to reconstruct anomalous ones?

- **Data Poisoning Attack Taxonomy**: Why needed here - FedNIA claims defense against sample poisoning, label flipping, and backdoors; understanding their differences clarifies why a unified detector is challenging. Quick check: What distinguishes a targeted backdoor attack from untargeted label flipping in terms of model behavior?

## Architecture Onboarding

- **Component map**: Noise Generator -> Activation Extractor -> Global Reference Computer -> Layerwise Autoencoder -> Error Computer & Threshold Calculator -> Update Filter -> Aggregator

- **Critical path**: 1. Receive client updates → 2. Generate noise → 3. Extract activations → 4. Train AE on global activations → 5. Compute reconstruction errors → 6. Apply threshold → 7. Aggregate filtered updates

- **Design tradeoffs**: Detection accuracy vs. compute overhead (AE training adds O(βη|θ|) complexity); Noise sample count (ν=100) stabilizes estimates but increases cost; Threshold scaling factor λ balances attack detection vs false positives; Layer-wise AE handles dimension differences but increases parameters

- **Failure signatures**: Cold start (early rounds have unreliable AE); Majority attack (r ≥ k/2 makes global model poisoned); Adaptive attackers (optimize for low reconstruction error); Subtle backdoors (minimal activation perturbations fall below threshold)

- **First 3 experiments**: 1. Baseline replication: Run FedNIA on EMNIST/Fashion-MNIST with δ=0, verify convergence matches FedAvg within ~50-100 rounds; 2. Single attack vector test: Inject label-flipping attack at δ=10%, measure detection rate and final accuracy; 3. AE sensitivity analysis: Vary β (AE training epochs) from 10-100 and ν (noise samples) from 50-200, plot detection accuracy vs runtime

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational efficiency of FedNIA be refined to reduce the linear runtime overhead added by the noise injection and autoencoder training process? The authors state that future research could explore ways to improve the algorithm's efficiency to mitigate the higher computational cost compared to FedAvg. A modified algorithm or optimization strategy that lowers the training time per FL round (currently ~1.75s vs ~0.25s for FedAvg with 100 clients) without reducing detection accuracy would resolve this.

### Open Question 2
Is FedNIA robust against adaptive adversaries who are aware of the noise analysis defense and optimize their malicious updates to mimic normal activation patterns? The Threat Model section states that attackers have "no knowledge of the aggregation structure on the server," implying the current evaluation assumes non-adaptive attackers. Experimental results showing FedNIA's performance against white-box attackers who optimize malicious updates to bypass the specific autoencoder loss function used by the defense would resolve this.

### Open Question 3
How does the performance of FedNIA scale to federated networks with significantly larger client populations or extreme data heterogeneity? The Experimental Setup limits the total number of clients to 50-100 and creates non-IID data via random sampling. It is unclear if the autoencoder threshold remains effective when the number of clients scales to thousands, or if extreme heterogeneity causes benign client activations to drift too far from the global state, causing false positives. Evaluation metrics demonstrating the method's detection precision and false positive rates in simulations with thousands of clients or highly distinct data distributions would resolve this.

## Limitations
- Critical implementation details remain unspecified, including threshold scaling factor λ, noise input distribution, and autoencoder layer dimensions
- Computational overhead from 50-epoch autoencoder training per round may be prohibitive for practical deployment
- Claims of 1.0 accuracy under all attack conditions lack statistical error bars or significance testing for individual scenarios
- Limited evaluation on client populations beyond 50-100 clients, raising questions about scalability

## Confidence
- **High confidence**: The core claim that noise-induced activation analysis can distinguish poisoned from benign models is well-supported by experimental results
- **Medium confidence**: The claim that FedNIA defends against all three attack types with equal effectiveness, as the paper doesn't provide per-attack type ablation studies
- **Low confidence**: The claim that FedNIA maintains 1.0 accuracy under all attack conditions, as this appears in Figure 3 without statistical error bars

## Next Checks
1. **Noise distribution sensitivity test**: Reproduce baseline experiments varying Z^t from uniform [0,1] to Gaussian N(0,1) to input-matched noise, measuring detection accuracy and false positive rates
2. **Autoencoder training overhead measurement**: Instrument FedNIA implementation to measure wall-clock time per FL round with β=50 vs β=10 vs β=100, comparing against FedAvg baseline
3. **Adaptive attacker simulation**: Implement an attacker that optimizes poisoned models specifically to minimize reconstruction error on the noise inputs Z^t, measuring whether FedNIA's detection rate degrades