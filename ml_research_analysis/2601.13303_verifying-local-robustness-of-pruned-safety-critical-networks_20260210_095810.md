---
ver: rpa2
title: Verifying Local Robustness of Pruned Safety-Critical Networks
arxiv_id: '2601.13303'
source_url: https://arxiv.org/abs/2601.13303
tags:
- pruned
- pruning
- neural
- networks
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how neural network pruning affects formal\
  \ local robustness verification in safety-critical domains. Using ResNet4 models\
  \ and the \u03B1,\u03B2-CROWN verifier, we evaluate pruning ratios from 10% to 80%\
  \ on MNIST and NASA JPL Mars Frost datasets."
---

# Verifying Local Robustness of Pruned Safety-Critical Networks

## Quick Facts
- arXiv ID: 2601.13303
- Source URL: https://arxiv.org/abs/2601.13303
- Reference count: 17
- Key outcome: Pruned neural networks can achieve higher formal verification success rates than unpruned baselines, with optimal pruning ratios varying significantly between datasets (40% for MNIST, 70-90% for JPL Mars Frost).

## Executive Summary
This paper investigates how neural network pruning affects formal local robustness verification in safety-critical domains. Using ResNet4 models and the α,β-CROWN verifier, the authors evaluate pruning ratios from 10% to 80% on MNIST and NASA JPL Mars Frost datasets. Results show a non-linear relationship: light pruning (40%) improves verification on MNIST, while heavy pruning (70%-90%) is optimal for JPL. This suggests that reduced connectivity simplifies the search space for formal solvers, with optimal ratios varying significantly between datasets. The findings demonstrate that pruned networks can outperform unpruned baselines in proven L∞ robustness properties, offering critical insights for deploying formally verified, efficient DNNs in high-stakes environments where reliability is essential.

## Method Summary
The study trains ResNet4 models on MNIST and JPL Mars Frost datasets to target accuracy thresholds (99.4% and 84% respectively). Magnitude-based L1 pruning is applied to convolutional layers at ratios from 10% to 80%, followed by finetuning to recover accuracy. The α,β-CROWN verifier then evaluates local robustness on L∞ perturbations across epsilon values (MNIST: 0.006-0.008; JPL: 0.0003-0.0009), with 5-minute timeouts per input. Ten models per configuration are evaluated using different random seeds to ensure statistical significance.

## Key Results
- MNIST verification improves with 40% pruning: 84.4±13.5 verified instances vs. 81.6±22.3 unpruned at ε=0.006
- JPL verification peaks at 70-80% pruning: 82.1±2.1 verified instances vs. 77.3±6.0 unpruned at ε=0.0003
- Excessive pruning (80%+) degrades verification success below baseline levels
- Verification variance decreases with optimal pruning, indicating more stable decision boundaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning reduces connectivity, which simplifies the formal verification search space for neural network verifiers.
- Mechanism: Removing weights reduces the number of active neurons and connections that bound-propagation verifiers (like α,β-CROWN) must track. Fewer constraints → smaller linear programming relaxations → faster/more successful verification completions within time limits.
- Core assumption: Verification complexity scales with network connectivity; reduced parameter count translates to reduced constraint complexity for the solver.
- Evidence anchors:
  - [abstract]: "This suggests that reduced connectivity simplifies the search space for formal solvers"
  - [section 3.2]: MNIST 40% pruned achieves 84.4±13.5 verified instances vs. 81.6±22.3 unpruned at ε=0.006; JPL 70% pruned achieves 82.1±2.1 vs. 77.3±6.0 unpruned at ε=0.0003
  - [corpus]: Limited direct evidence—neighbor papers focus on pruning for efficiency/robustness training, not formal verification tractability
- Break condition: Excessive pruning (80%+ on MNIST) degrades verifiability (51.2 vs 81.6 baseline), suggesting connectivity reduction helps only until decision boundaries become unstable.

### Mechanism 2
- Claim: Optimal pruning ratio for verifiability is dataset-dependent, with simpler datasets benefiting from light pruning and complex datasets from heavy pruning.
- Mechanism: Dataset complexity (feature dimensionality, class separation margin, inherent noise) affects how much redundancy exists in learned representations. MNIST (low-complexity, 10 classes, clear structure) retains decision boundary integrity under light pruning; JPL Mars Frost (higher complexity, fine-grained features) may over-parameterize initially, enabling aggressive pruning without boundary distortion.
- Core assumption: Different datasets induce different levels of neural network over-parameterization; pruning effectiveness correlates with redundancy level.
- Evidence anchors:
  - [abstract]: "light pruning (40%) in MNIST and heavy pruning (70%–90%) in JPL improve verifiability"
  - [section 3.2]: Table 2 shows MNIST optimal at 40-50%; Table 3 shows JPL optimal at 70-80%
  - [corpus]: No direct corpus evidence on dataset-dependent pruning ratios for verification
- Break condition: Inverting the pattern (heavy pruning on MNIST, light on JPL) degrades verifiability below baseline—suggests mismatch between pruning level and dataset characteristics.

### Mechanism 3
- Claim: Magnitude-based L1 pruning on convolutional layers preserves accuracy while enabling verification gains, but only when finetuning recovers task performance.
- Mechanism: Smallest-magnitude weights contribute minimally to activations; removing them creates sparse networks with equivalent function but fewer verification constraints. Finetuning adjusts remaining weights to restore decision boundaries within the reduced parameter space.
- Core assumption: Weight magnitude correlates with functional importance; convolutional layers contain more redundancy than residual/fully-connected layers.
- Evidence anchors:
  - [section 1.1]: "weights with the smallest absolute values...are likely less important and pruning them would have minimal impact"
  - [section 2.2]: Table 1 shows all pruned models maintain comparable accuracy (MNIST ~99.4%, JPL ~84%) after finetuning
  - [corpus: S2AP paper]: Adversarial pruning methods can compress networks while preserving robustness—consistent with magnitude-based approaches
- Break condition: Pruning residual/fully-connected layers (excluded per methodology) may break this mechanism; skipping finetuning prevents accuracy recovery.

## Foundational Learning

- **Concept: Local Robustness Verification (L∞ norm)**
  - Why needed here: Core evaluation metric; understanding what ε bounds mean is essential for interpreting Tables 2-3.
  - Quick check question: Given ε=0.006, does a perturbed pixel within ±0.006 of original count as within the robustness ball?

- **Concept: Magnitude-Based (L1) Pruning**
  - Why needed here: The pruning method used; affects which weights are removed and how network structure changes.
  - Quick check question: If weight A = -0.001 and weight B = 0.05, which is pruned first under L1 magnitude pruning?

- **Concept: Bound Propagation Verifiers (α,β-CROWN)**
  - Why needed here: The verification tool; understanding its constraint-solving approach clarifies why reduced connectivity helps.
  - Quick check question: Why would fewer active neurons reduce the complexity of linear relaxation bounds?

## Architecture Onboarding

- **Component map:** Training (ResNet4) → Pruning (L1, conv-only) → Finetuning → Verification (α,β-CROWN)
- **Critical path:** Pruning ratio selection → finetuning convergence → verification success rate. Incorrect ratio selection (too aggressive for dataset) causes verification degradation despite maintained accuracy.
- **Design tradeoffs:**
  - Higher pruning ratio → smaller model, faster inference, potentially better verification, but risk of decision boundary instability
  - Dataset complexity → determines optimal pruning window (narrower for simple datasets)
  - Verification timeout (5 min) → limits tractable model size; pruning directly expands tractability envelope
- **Failure signatures:**
  - Verification success drops sharply with excessive pruning (MNIST 80%: 51.2 vs baseline 81.6)
  - High variance in verification results indicates unstable decision boundaries (unpruned MNIST std=22.3 vs 40% pruned std=13.5)
  - Accuracy maintained but verifiability degraded suggests boundary distortion without functional loss
- **First 3 experiments:**
  1. Replicate MNIST experiment at 40% pruning with ε ∈ {0.006, 0.007, 0.008}; compare verified instance counts against unpruned baseline to validate improvement signal.
  2. Test intermediate pruning ratios (35%, 45%, 55%) on your target dataset to locate optimal region before full sweep.
  3. Run verification timeout analysis: test whether pruned models complete verification faster (measure solver iterations, not just success rate) to confirm search space reduction mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What underlying mechanisms cause the optimal pruning ratio for verifiability to differ significantly across datasets (e.g., 40% for MNIST vs. 70%–80% for JPL)?
- Basis in paper: [explicit] The authors state that the "surprising" difference in optimal ratios "warrants an investigation into why different datasets yield different optimal pruning ratios."
- Why unresolved: The study identifies the divergent outcomes but does not isolate specific data characteristics (e.g., dimensionality, feature sparsity) or decision boundary geometries that drive these requirements.
- What evidence would resolve it: A theoretical or empirical analysis linking intrinsic dataset properties to the changes in the verification search space induced by pruning.

### Open Question 2
- Question: How do alternative pruning criteria (beyond magnitude-based) and granularities impact the local robustness verification search space?
- Basis in paper: [explicit] The authors note that settings like "pruning criterion and granularity" likely have different impacts across tasks and explicitly call for "explanations of these findings."
- Why unresolved: The methodology is restricted to unstructured L1-pruning on convolutional layers; it does not test if structured pruning or gradient-based criteria yield similar verification benefits.
- What evidence would resolve it: Comparative experiments evaluating diverse pruning strategies (e.g., filter pruning, random pruning) against the $\alpha, \beta$-CROWN verifier.

### Open Question 3
- Question: Does the relationship between heavy pruning and improved verifiability generalize to deeper, industrial-scale architectures?
- Basis in paper: [inferred] The paper utilizes a small ResNet4 architecture to manage computational complexity, acknowledging that verifying "large-scale models remains a significant barrier."
- Why unresolved: It is unclear if the reduced connectivity simplifies the solver's search space effectively in much deeper networks (e.g., ResNet50) where the combinatorial complexity is exponentially higher.
- What evidence would resolve it: Scaling the verification experiments to standard industrial backbones (e.g., ResNet18/50) to observe if similar optimal pruning ratios emerge.

## Limitations

- Lack of architectural specifications for ResNet4 makes exact replication challenging
- JPL dataset preprocessing details and train/test split are not provided
- Only evaluates one pruning method (magnitude-based L1) and one verifier (α,β-CROWN)
- Analysis limited to two specific datasets without exploring broader dataset-complexity relationships

## Confidence

- **High confidence**: The observation that light pruning (40%) improves verification on MNIST while heavy pruning (70-90%) benefits JPL. This pattern is directly supported by the presented results.
- **Medium confidence**: The mechanism that pruning simplifies the formal verification search space. While results are consistent, the corpus lacks direct evidence linking connectivity reduction to verification tractability.
- **Medium confidence**: The dataset-dependent nature of optimal pruning ratios. The evidence is strong within the tested datasets, but broader validation across diverse domains is needed.

## Next Checks

1. **Replication test**: Replicate the MNIST 40% pruning experiment across multiple epsilon values to verify the claimed improvement in verified instances against the unpruned baseline.
2. **Intermediate ratio exploration**: Test intermediate pruning ratios (35%, 45%, 55%) on your target dataset to identify the optimal pruning window before committing to a full parameter sweep.
3. **Solver performance analysis**: Measure not just verification success rates but also solver iterations and completion times for pruned vs. unpruned models to directly validate the search space simplification hypothesis.