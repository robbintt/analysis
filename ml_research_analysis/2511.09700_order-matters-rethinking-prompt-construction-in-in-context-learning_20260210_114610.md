---
ver: rpa2
title: 'Order Matters: Rethinking Prompt Construction in In-Context Learning'
arxiv_id: '2511.09700'
source_url: https://arxiv.org/abs/2511.09700
tags:
- ordering
- selection
- examples
- sensitivity
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relative importance of example selection
  versus ordering in in-context learning (ICL). The authors conduct controlled experiments
  across classification and generation tasks using models ranging from 0.5B to 27B
  parameters.
---

# Order Matters: Rethinking Prompt Construction in In-Context Learning

## Quick Facts
- arXiv ID: 2511.09700
- Source URL: https://arxiv.org/abs/2511.09700
- Reference count: 22
- One-line primary result: Variance from example ordering in ICL is comparable to variance from changing example sets entirely.

## Executive Summary
This paper challenges the conventional focus on example selection in in-context learning (ICL) by demonstrating that the order of examples is equally important. Through controlled experiments across classification and generation tasks using models from 0.5B to 27B parameters, the authors find that performance variance due to different orderings is comparable to variance from using entirely different example sets. They show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. However, these orderings rarely transfer well across different datasets, indicating that order sensitivity is highly dataset-dependent.

## Method Summary
The authors conduct sensitivity analysis by sampling M demonstration sets and generating P random permutations for each set. They measure selection sensitivity (variance across demonstration sets) and ordering sensitivity (variance across permutations per set) using grouped standard deviation. For ordering optimization, they evaluate permutations on a development set and select the highest-performing one, then test on held-out data. Experiments span classification datasets (AG News, NYT-Topics, NYT-Locations, DBPedia, MMLU) with k=2×|classes| shots and generation datasets (GSM8K, MMLU-Pro, MATH) with k=8 shots. Default orderings are alphabetical for generation and grouped by label then alphabetically for classification.

## Key Results
- Average selection sensitivity (0.02251) is only ~14% more than average ordering sensitivity (0.01970)
- Evaluating 16-32 permutations recovers over 98% of oracle accuracy for classification
- Best orderings rarely transfer across datasets, performing at random-average levels when transferred
- Performance plateaus after evaluating 64-128 permutations for ordering optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Variance from reordering examples is comparable to variance from changing example sets entirely.
- **Mechanism**: Attention patterns in Transformers distribute differently across positions; early positions typically receive higher cumulative attention, creating implicit weighting by position regardless of content quality. A strong example in a low-attention position underperforms a mediocre example in a high-attention position.
- **Core assumption**: Position-specific attention weights are at least partially independent of token content.
- **Evidence anchors**:
  - [abstract] "variance in performance due to different example orderings is comparable to that from using entirely different example sets"
  - [section 3.3] "average selection sensitivity (0.02251) is only ~14% more than average ordering sensitivity (0.01970)"
  - [corpus] Related work (Zhao et al., 2021) attributed order sensitivity to "model's over-reliance on early-context examples" — confirms positional attention hypothesis.
- **Break condition**: If models used position-invariant attention or explicit position encoding was removed, ordering effects would diminish substantially.

### Mechanism 2
- **Claim**: Strong orderings can be identified using a development set without test-label oracle access.
- **Mechanism**: Permutation-performance correlations generalize within the same distribution. A ordering that yields high accuracy on dev samples tends to preserve relative attention advantages on test samples from the same distribution, enabling greedy selection over candidate permutations.
- **Core assumption**: Development and test sets are drawn from sufficiently similar distributions that attention patterns transfer.
- **Evidence anchors**:
  - [abstract] "strong orderings can be identified using only a development set, achieving performance close to an oracle"
  - [section 4.2] "evaluating just 16–32 permutations recovers over 98% of the oracle accuracy [classification]"
  - [corpus] Weak/no direct corpus evidence on *why* dev-to-test transfer works; mechanism remains inferred.
- **Break condition**: Distribution shift between dev and test would degrade ordering transfer accuracy toward random baseline.

### Mechanism 3
- **Claim**: Optimal orderings are dataset-specific and do not transfer across datasets.
- **Mechanism**: Effective orderings depend on label-to-position mappings and class boundary structures unique to each dataset. A permutation that spaces classes evenly for one label set may create uneven coverage for another, breaking transfer.
- **Core assumption**: Order effects are mediated by task-specific class structure, not universal positional priors.
- **Evidence anchors**:
  - [section 4.2] "transferring the best ordering from one dataset to another results in test accuracy similar to the average random ordering"
  - [section 5] "orderings are hard to transfer across different datasets"
  - [corpus] No corpus papers directly address cross-dataset ordering transfer; this finding appears novel.
- **Break condition**: If order effects were driven purely by universal recency bias (not class structure), some transfer would persist across datasets.

## Foundational Learning

- **Concept: Grouped standard deviation for sensitivity measurement**
  - **Why needed here**: The paper uses this metric to quantify how much performance varies when changing one factor (ordering or selection) while holding the other fixed.
  - **Quick check question**: If you ran 10 different example sets with 10 orderings each, would you compute standard deviation row-wise (for ordering) or column-wise (for selection)?

- **Concept: Development-set oracle approximation**
  - **Why needed here**: The method evaluates candidate orderings on a held-out dev set to approximate what an oracle (with test-label access) would select.
  - **Quick check question**: Why is this a valid approximation only when dev and test come from the same distribution?

- **Concept: Permutation search budget**
  - **Why needed here**: The paper shows diminishing returns after 64–128 permutations; knowing this prevents wasted compute.
  - **Quick check question**: If you have budget for only 20 permutation evaluations, would you expect 95%+ of oracle accuracy on a generation task?

## Architecture Onboarding

- **Component map**:
  [Example Pool] → [Demonstration Set Sampler (M sets)] → [Permutation Generator (P per set)] → [Default Ordering Module] → [Dev Set Evaluator] → [Highest-Dev Selector] → [Test Evaluator]

- **Critical path**:
  1. Define default ordering (classification: group by label → alphabetically; generation: alphabetically).
  2. Sample M demonstration sets (M = 10 is standard).
  3. For each set, generate P random permutations (P = 64–128 for near-optimal; P = 10 for sensitivity analysis).
  4. Evaluate each permutation on D_dev (N_dev ≥ 250 for plateau).
  5. Select highest-dev permutation and evaluate on D_test.

- **Design tradeoffs**:
  - **P (permutations) vs. compute**: 16 permutations gives ~98% oracle for classification; generation needs 64–100.
  - **N_dev size vs. data availability**: Performance plateaus after ~250 examples; larger dev sets yield marginal gains.
  - **Model size vs. sensitivity**: Smaller models (0.5B–2B) show higher sensitivity; larger models (27B) are more stable but not immune.

- **Failure signatures**:
  - Transferred ordering from Dataset A performs at random-average level on Dataset B → expected, not a bug.
  - Dev-selected ordering underperforms average → check for distribution mismatch between D_dev and D_test.
  - High variance across demonstration sets (σ(P) ≫ σ(M)) → selection matters more; increase shot count k or improve example retrieval.

- **First 3 experiments**:
  1. **Replicate sensitivity ratio on a new dataset**: Compute σ(M) and σ(P) for your target task with M=10, P=10. Confirm r = σ(P)/σ(M) is in 0.9–1.5 range.
  2. **Dev-set ordering search with budget sweep**: Vary P ∈ {16, 32, 64, 128} and plot highest-dev test accuracy vs. max oracle. Identify saturation point for your task type.
  3. **Cross-dataset transfer sanity check**: Take best ordering from Dataset A, apply to Dataset B. Confirm performance drops to near-average, validating dataset-specificity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the relative sensitivities of ordering versus selection persist in state-of-the-art commercial models (e.g., full-scale GPT-5 or Claude) that exceed the 27B parameter limit studied here?
- **Basis in paper:** [Explicit] The authors explicitly state in the Future Work section: "Future work should test whether these findings hold for larger, state-of-the-art models like GPT-5."
- **Why unresolved:** The study was computationally and financially constrained to open-source models (0.5B–27B) and GPT-5 Nano; the scaling laws for order sensitivity at the frontier remain unverified.
- **What evidence would resolve it:** Replicating the sensitivity metrics ($\sigma_{selection}$ vs. $\sigma_{ordering}$) on proprietary models with hundreds of billions or trillions of parameters.

### Open Question 2
- **Question:** How does the balance between selection and ordering sensitivity shift in structured generation tasks like code synthesis or retrieval-augmented generation (RAG)?
- **Basis in paper:** [Explicit] The authors note the need to "extend to new task formats such as code generation and retrieval-augmented generation."
- **Why unresolved:** The current study is restricted to standard classification and natural language generation; tasks requiring strict logical syntax or external context may exhibit different ordering dynamics.
- **What evidence would resolve it:** Applying the grouped standard deviation methodology to code benchmarks (e.g., HumanEval) and RAG pipelines to compare variance ratios.

### Open Question 3
- **Question:** Is the "intertwined importance" of selection and ordering a universal phenomenon across non-English languages?
- **Basis in paper:** [Explicit] The paper lists "explore other languages" as a specific direction for future work.
- **Why unresolved:** All experiments were conducted on English benchmarks; morphological or syntactic properties of other languages might decouple the relationship between example choice and sequence.
- **What evidence would resolve it:** Evaluating the sensitivity metrics on multilingual datasets to determine if the ~14% gap between selection and ordering sensitivity holds globally.

### Open Question 4
- **Question:** Can principled "default" orderings be developed that transfer across datasets, or is dataset-specific optimization always necessary?
- **Basis in paper:** [Inferred] The authors found that orderings discovered on one dataset "rarely transfer well" to another (Table 7), leading to the conclusion that sensitivity is "highly dataset-dependent."
- **Why unresolved:** While the paper demonstrates that dev-set search finds good local orderings, it does not propose a method for finding universal ordering heuristics.
- **What evidence would resolve it:** A study identifying features (e.g., semantic entropy, class distribution) that predict cross-dataset transferability for specific permutations.

## Limitations
- Model scaling uncertainty: Findings based on 0.5B-27B parameter models; unknown if ordering sensitivity persists in frontier models
- Default ordering bias: Alphabetical and label-grouping defaults may not reflect optimal real-world prompting strategies
- Cross-dataset mechanism: Dataset-specificity demonstrated but underlying mechanism (class structure dependence) is inferred rather than directly measured

## Confidence
- **High confidence**: Empirical finding that ordering variance is comparable to selection variance (σ^(P)/σ^(M) ≈ 0.9-1.5 across tasks)
- **Medium confidence**: Practical utility of dev-set ordering selection with 16-32 permutation budget
- **Medium confidence**: Dataset-specificity claim based on negative transfer results

## Next Checks
1. **Model scaling experiment**: Replicate the sensitivity ratio σ^(P)/σ^(M) on a 70B+ parameter model to determine if ordering effects diminish with scale
2. **Real-world prompt comparison**: Compare dev-set ordering optimization against a state-of-the-art prompt engineering baseline to establish practical value
3. **Task-family transfer analysis**: Systematically test ordering transfer across datasets within the same task family to determine if dataset-specificity holds at task-family level