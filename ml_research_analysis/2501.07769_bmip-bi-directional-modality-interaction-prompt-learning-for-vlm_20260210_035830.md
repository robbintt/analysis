---
ver: rpa2
title: 'BMIP: Bi-directional Modality Interaction Prompt Learning for VLM'
arxiv_id: '2501.07769'
source_url: https://arxiv.org/abs/2501.07769
tags:
- prompt
- learning
- vision
- bmip
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting vision-language
  models (VLMs) to downstream tasks using prompt learning, specifically focusing on
  the limitations of single-modal prompts or uni-directional modality interaction
  that overlook the powerful alignment effects resulting from the interaction between
  vision and language modalities. The authors propose a novel prompt learning method
  called Bi-directional Modality Interaction Prompt (BMIP) that dynamically weights
  bi-modal information through learning the information of the attention layer, enhancing
  trainability and inter-modal consistency compared to simple information aggregation
  methods.
---

# BMIP: Bi-directional Modality Interaction Prompt Learning for VLM

## Quick Facts
- **arXiv ID:** 2501.07769
- **Source URL:** https://arxiv.org/abs/2501.07769
- **Authors:** Song-Lin Lv; Yu-Yang Chen; Zhi Zhou; Ming Yang; Lan-Zhe Guo
- **Reference count:** 21
- **Primary result:** Novel prompt learning method BMIP achieves 0.82% average performance gain over state-of-the-art MaPLe across 15 benchmarks

## Executive Summary
This paper introduces BMIP (Bi-directional Modality Interaction Prompt), a novel prompt learning method for vision-language models that addresses limitations of single-modal prompts and uni-directional modality interaction. The method dynamically weights bi-modal information through attention-guided learning, enhancing trainability and inter-modal consistency compared to simple aggregation methods. Comprehensive experiments on 15 benchmarks demonstrate superior performance across open-world generalization, cross-dataset transfer, and domain generalization tasks.

## Method Summary
BMIP extends deep prompt tuning by introducing bi-directional interaction between vision and language modalities in CLIP. The method learns independent prompts for both modalities at matching depths, then uses attention-weighted cross-modal projections to dynamically substitute original prompts. Projection heads transform prompts between modality spaces, while learned linear layers extract attention weights to control the substitution process. This approach preserves modality-specific information while enabling effective cross-modal alignment, with prompt replacement (rather than addition) maintaining trainability near convergence.

## Key Results
- Achieves 0.82% absolute average performance gain over previous state-of-the-art MaPLe across 15 benchmarks
- Demonstrates consistent improvements on datasets with imbalanced text and image information like EuroSAT and Flowers102
- Outperforms current methods across all three evaluation paradigms: open-world generalization, cross-dataset transfer, and domain generalization
- Flexible architecture that can be combined with other prompt-based methods for additional performance enhancement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention-guided dynamic weighting enables more effective cross-modal information exchange than static aggregation methods.
- **Mechanism:** The model extracts attention weights from each layer's attention output, then learns 1×1 linear layers to map these attention patterns to substitution weights. These weights control how much cross-modal projected information replaces original prompts.
- **Core assumption:** Attention weights encode meaningful signal about prompt importance that correlates with optimal cross-modal information flow.
- **Evidence anchors:** [abstract] "dynamically weights bi-modal information through learning the information of the attention layer"; [Section 4.3] "we propose a learnable aggregation function using the output weight of the attention layer"; [Table 5] Ablation comparing Addition, Attention, Joint aggregation shows BMIP's learned weighting outperforms static alternatives.
- **Break condition:** If attention patterns become uniform or noisy, the learned weights may fail to provide meaningful differentiation.

### Mechanism 2
- **Claim:** Independent deep prompts per modality combined with bi-directional projection preserve modality-specific information while enabling cross-modal alignment.
- **Mechanism:** Vision prompts and language prompts are learned independently at matching depths. Projection heads transform each modality's prompts to the other's space, enabling bi-directional information flow without forcing one modality to dominate.
- **Core assumption:** Modality-specific prompts capture complementary information that uni-directional transfer would lose.
- **Evidence anchors:** [Section 4] "Independent vision and language prompts facilitate the collection of information from their respective modalities"; [Section 5.6] "MaPLe† tends to overfit to base classes when parameters are increased, whereas BMIP maintains robust performance".
- **Break condition:** If one modality's information is consistently noisier or less informative for a task, the bi-directional exchange could propagate noise rather than signal.

### Mechanism 3
- **Claim:** Prompt replacement (rather than addition) maintains trainability near convergence by preventing gradient decay.
- **Mechanism:** When attention weights approach zero, prompts become under-utilized (gradients ≈ 0). By substituting prompts with cross-modal projections weighted by learned attention, the model maintains gradient flow and avoids local minima traps.
- **Core assumption:** Corollary 1 holds: min_P' L ≤ min_P L given sufficient expressivity—the replacement strategy expands the optimizable space.
- **Evidence anchors:** [Section 4.4] "using w to combine the original prompt with aggregated information will enhance the trainability of the model"; [Table 5] Comparison with "Addition" aggregation (78.40 HM) vs BMIP (79.04 HM) supports replacement over addition.
- **Break condition:** If the projection heads are underparameterized, cross-modal projections may introduce noise that outweighs trainability benefits.

## Foundational Learning

- **Concept: CLIP Vision-Language Alignment**
  - **Why needed here:** BMIP builds directly on CLIP's dual-encoder architecture and assumes pre-aligned joint embedding space. Understanding cosine similarity matching is essential.
  - **Quick check question:** Can you explain why CLIP's zero-shot classification computes similarity between image features and all class text embeddings?

- **Concept: Prompt Learning in Transformers**
  - **Why needed here:** BMIP extends soft prompt learning to both vision and language branches across multiple depths.
  - **Quick check question:** How do learnable soft prompts differ from discrete text prompts, and what advantage do they provide for gradient-based optimization?

- **Concept: Deep Prompt Tuning (Layer-wise Prompts)**
  - **Why needed here:** Unlike shallow prompting at input only, BMIP injects prompts at multiple encoder layers, influencing intermediate representations.
  - **Quick check question:** Why might deep prompts improve over single-layer prompts for preserving pre-trained knowledge while adapting to downstream tasks?

## Architecture Onboarding

- **Component map:** Image Input → Patch Embedding → [CLS, E, P̃'] → Image Encoder (J layers with vision prompts) → ImageProj → x; Text Input → Word Embedding → [P', W] → Text Encoder (J layers with language prompts) → TextProj → z → Cosine Similarity → Classification

- **Critical path:** The attention-weighted aggregation in Equations 10-11 is the novel contribution. The forward pass through the first J layers uses P'_i (augmented prompts) instead of original P_i. The projection heads (Fv, Fl) and learned linear layers (Lv, Ll) must be initialized and trained together.

- **Design tradeoffs:**
  - Prompt depth J: Deeper prompts capture more information but risk overfitting
  - Prompt length b: Longer prompts increase capacity but add parameters
  - Projection head architecture: Simple linear layers reduce overhead but may limit expressivity

- **Failure signatures:**
  - Performance collapses on base classes but improves on novel classes → overfitting to cross-modal noise
  - EuroSAT-style datasets still underperform → check if vision projection is learning meaningful transformations
  - Training loss plateaus early → attention weights may be saturating

- **First 3 experiments:**
  1. **Reproduce open-world generalization on EuroSAT:** Train with 16-shot setting, evaluate HM and Accuracy. Target: ~86%. Verify attention weights vary across layers and samples.
  2. **Ablate aggregation function:** Compare BMIP vs Addition vs Attention-only on Flowers102. Confirm learned weighting provides >0.5% improvement.
  3. **Cross-dataset transfer sanity check:** Train on ImageNet subset (1K classes, 16-shot), test zero-shot on OxfordPets. Target: >90%. If underperforming, check if prompt depth matches pre-trained model architecture.

## Open Questions the Paper Calls Out
None

## Limitations
- The core mechanism relies on attention weights from intermediate layers, but without access to exact implementation details (prompt depth, length, optimizer settings), it's unclear whether improvements stem from bi-directional interaction or hyperparameter tuning.
- The claim that prompt replacement enhances trainability near convergence lacks direct empirical validation and theoretical proof.
- The assertion that independent deep prompts per modality are essential for preserving complementary information is weakly supported by limited ablation studies.

## Confidence

- **High Confidence:** The general concept of bi-directional prompt interaction and its superiority over uni-directional transfer is well-supported by quantitative results across 15 benchmarks.
- **Medium Confidence:** The mechanism of attention-weighted aggregation improving over static methods is convincing, but the trainability enhancement claim lacks direct evidence.
- **Low Confidence:** The necessity of independent deep prompts per modality for preserving complementary information is weakly supported by the paper's evidence.

## Next Checks

1. **Reproduce EuroSAT 16-shot results:** Train BMIP on EuroSAT with 16-shot per class, report HM and Accuracy. Verify attention weights (wv, wl) vary meaningfully across layers and samples, not saturating at 0 or 1.

2. **Ablate prompt depth J:** Train BMIP with J=3, J=6, J=9 on Flowers102. If performance plateaus early, the benefit may be from increased parameters rather than bi-directional interaction.

3. **Compare with CLIP-tuned baseline:** Train CLIP with soft prompts (no bi-directional interaction) on ImageNet subset (1K classes, 16-shot), evaluate zero-shot on OxfordPets. If CLIP-tuned matches or exceeds BMIP, the gains may not be from modality interaction but from better prompt optimization.