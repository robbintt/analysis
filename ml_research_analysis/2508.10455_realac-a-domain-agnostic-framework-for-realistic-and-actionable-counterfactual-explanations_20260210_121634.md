---
ver: rpa2
title: 'RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual
  Explanations'
arxiv_id: '2508.10455'
source_url: https://arxiv.org/abs/2508.10455
tags:
- features
- data
- realac
- counterfactual
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealAC is a domain-agnostic framework for generating realistic
  and actionable counterfactual explanations by automatically preserving inter-feature
  dependencies through mutual information alignment and incorporating user-defined
  feasibility constraints via binary masking. Evaluated on three synthetic and two
  real datasets, RealAC outperforms state-of-the-art baselines in causal edge score,
  dependency preservation score, and IM1 realism metric, achieving high validity and
  balanced realism without relying on explicit domain knowledge.
---

# RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations

## Quick Facts
- **arXiv ID**: 2508.10455
- **Source URL**: https://arxiv.org/abs/2508.10455
- **Reference count**: 23
- **Primary result**: RealAC outperforms state-of-the-art baselines in causal edge score, dependency preservation score, and IM1 realism metric while achieving high validity and balanced realism without relying on explicit domain knowledge.

## Executive Summary
RealAC introduces a domain-agnostic framework for generating realistic and actionable counterfactual explanations by automatically preserving inter-feature dependencies through mutual information alignment and incorporating user-defined feasibility constraints via binary masking. The framework addresses the challenge of generating counterfactuals that are both causally aligned with the original instance and practically feasible for the user to implement. RealAC operates without requiring explicit domain knowledge or pre-defined causal graphs, making it broadly applicable across different domains and ML models.

## Method Summary
RealAC generates counterfactual explanations through a two-stage optimization process that balances validity (changing the prediction) with realism (maintaining feature dependencies) and feasibility (adhering to user constraints). The framework uses mutual information alignment to preserve statistical dependencies between features in the counterfactual, ensuring that generated instances remain realistic within the data distribution. Binary masking allows users to specify which features can be modified and what ranges are acceptable for each changeable feature. The method works with any differentiable model and can generate multiple counterfactuals per instance, allowing users to choose the most actionable explanation based on their preferences and constraints.

## Key Results
- RealAC achieves significantly higher causal edge scores compared to state-of-the-art baselines, indicating better preservation of feature dependencies in generated counterfactuals
- The framework demonstrates superior dependency preservation scores across three synthetic and two real datasets, maintaining realistic feature relationships
- RealAC attains high IM1 realism metric scores while maintaining validity, outperforming competitors in balancing realism with actionable explanations

## Why This Works (Mechanism)
RealAC works by simultaneously optimizing three key objectives: prediction validity, feature dependency preservation, and feasibility constraints. The mutual information alignment component ensures that the statistical relationships between features in the counterfactual match those in the original data distribution, preventing the generation of unrealistic instances that might occur when features are modified independently. The binary masking mechanism allows users to encode domain knowledge or practical constraints directly into the generation process, ensuring that suggested changes are actually implementable. By framing counterfactual generation as a constrained optimization problem that respects both statistical dependencies and user constraints, RealAC produces explanations that are not only causally meaningful but also actionable in real-world scenarios.

## Foundational Learning
- **Mutual Information**: Measures statistical dependence between variables; needed to preserve realistic feature relationships in counterfactuals; quick check: verify MI values between original and counterfactual features match
- **Counterfactual Explanations**: Generate minimally different instances with changed predictions; needed to provide actionable insights for model decisions; quick check: ensure counterfactuals have opposite class predictions
- **Binary Masking**: Enables feature-specific modification constraints; needed to incorporate user-defined feasibility; quick check: verify masked features remain unchanged
- **Causal Edge Score**: Quantifies structural similarity in feature dependencies; needed to evaluate realism of generated counterfactuals; quick check: compare edge scores between original and counterfactual instances
- **Surrogate Models**: Approximate complex ML models for optimization; needed to make counterfactual generation computationally tractable; quick check: measure surrogate model accuracy on validation set

## Architecture Onboarding

**Component Map**: User Input -> Binary Masking -> Mutual Information Alignment -> Surrogate Model -> Counterfactual Generation -> Evaluation Metrics

**Critical Path**: The critical path flows from user-defined constraints through the optimization process to generate counterfactuals that satisfy validity, realism, and feasibility simultaneously. The mutual information alignment and binary masking components are essential for achieving balanced performance across all three objectives.

**Design Tradeoffs**: RealAC trades computational complexity for domain-agnostic applicability, avoiding explicit causal graph construction while achieving similar or better results. The framework prioritizes flexibility over specialized optimization for specific domains, making it broadly applicable but potentially suboptimal for cases where domain-specific knowledge could be leveraged.

**Failure Signatures**: Common failure modes include generating counterfactuals that violate user constraints when feasibility requirements are too strict, producing unrealistic instances when feature dependencies are weak or non-linear, and failing to change predictions when the surrogate model poorly approximates the original ML model. The framework may also struggle with high-dimensional feature spaces where mutual information estimation becomes unreliable.

**First Experiments**: 
1. Generate counterfactuals for a simple logistic regression model on a binary classification dataset with known feature dependencies
2. Test binary masking functionality by specifying different constraint sets and verifying constraint adherence
3. Compare causal edge scores between RealAC-generated counterfactuals and randomly generated feasible instances

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on surrogate models (e.g., decision trees) to approximate complex ML models may introduce approximation errors that affect explanation quality
- Claims of domain-agnostic applicability are primarily validated on structured tabular data, with uncertainty about performance in unstructured domains like images or text
- The assumption that surrogate model accuracy correlates with explanation quality is not empirically validated in the paper

## Confidence
- **High**: Claims about improved causal edge scores and dependency preservation compared to baselines
- **Medium**: Claims about domain-agnostic applicability and overall superiority across all metrics
- **Low**: Claims regarding the sufficiency of surrogate model accuracy for explanation quality

## Next Checks
1. Evaluate RealAC on unstructured data types (images, text) to verify domain-agnostic claims beyond tabular datasets
2. Conduct ablation studies to quantify the contribution of mutual information alignment versus binary masking components
3. Compare explanation quality when using different surrogate model types (linear models, random forests) to assess robustness to model choice