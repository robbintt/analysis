---
ver: rpa2
title: 'SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native
  Multimodal CT Analysis'
arxiv_id: '2510.02322'
source_url: https://arxiv.org/abs/2510.02322
tags:
- speech
- text
- spoken
- reports
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between spoken and text-based radiology
  reporting in medical AI by developing SpeechCT-CLIP, a model that directly aligns
  spoken radiology reports with 3D CT volumes without requiring transcription. The
  authors create Speech-RATE, a synthetic dataset of 50,188 spoken radiology reports
  paired with CT images using diverse synthetic voices.
---

# SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native Multimodal CT Analysis

## Quick Facts
- **arXiv ID**: 2510.02322
- **Source URL**: https://arxiv.org/abs/2510.02322
- **Reference count**: 0
- **Primary result**: SpeechCT-CLIP achieves F1 score of 0.705 on zero-shot classification, recovering 88% of the performance gap compared to text-based models

## Executive Summary
This paper introduces SpeechCT-CLIP, a novel approach for voice-native multimodal CT analysis that directly aligns spoken radiology reports with 3D CT volumes without requiring transcription. The authors address a critical gap in medical AI where most systems rely on transcribed text, which may not reflect the actual workflow of radiologists who predominantly dictate reports. By developing a synthetic dataset of 50,188 spoken radiology reports (Speech-RATE) and employing knowledge distillation from a pretrained text-image CLIP model, the approach demonstrates that spoken descriptions can effectively support CT classification and retrieval tasks, achieving 88% of text-based performance while eliminating the need for transcription.

## Method Summary
The authors create Speech-RATE, a synthetic dataset of 50,188 spoken radiology reports paired with CT images using diverse synthetic voices generated from the openly available Text-RATE dataset. They employ a two-stage training approach: first, they fine-tune a speech foundation model to encode spoken reports into embeddings aligned with text embeddings using a contrastive loss. Second, they use knowledge distillation from a pretrained text-image CLIP model to transfer multimodal understanding from the text-based system to the speech-based model. The distillation process involves training the speech encoder to mimic the text encoder's behavior when both are given paired text and speech descriptions of the same CT scans, allowing the speech model to leverage the rich semantic knowledge captured by the text-image CLIP model without requiring text at inference time.

## Key Results
- Knowledge distillation from text-image CLIP recovers 88% of the performance gap, improving F1 score from 0.623 to 0.705 on zero-shot classification
- SpeechCT-CLIP demonstrates strong retrieval performance, effectively matching spoken descriptions to relevant CT volumes without requiring text at inference
- The approach eliminates the need for transcription in radiology workflows, potentially improving efficiency and reflecting actual clinical practice

## Why This Works (Mechanism)
The effectiveness of SpeechCT-CLIP stems from leveraging pretrained multimodal representations through knowledge distillation. The text-image CLIP model contains rich semantic understanding of the relationship between radiological descriptions and CT imagery, which is transferred to the speech encoder through the distillation process. This allows the speech model to benefit from the extensive training of the text-based system without requiring large amounts of paired speech-CT data. The contrastive learning objective during speech encoder training ensures that spoken descriptions are mapped to the same embedding space as their corresponding text descriptions, enabling the subsequent knowledge transfer.

## Foundational Learning
- **Knowledge Distillation**: Why needed - To transfer learned representations from a text-based model to a speech-based model without requiring extensive paired speech-CT training data. Quick check - Verify that the student model's embeddings align with the teacher model's embeddings for matched inputs.
- **Contrastive Learning**: Why needed - To align speech and text embeddings in a shared representation space, ensuring that different modalities describing the same content are mapped closely together. Quick check - Confirm that paired speech-text embeddings have higher similarity than random pairings.
- **Multimodal CLIP Models**: Why needed - Provide pretrained cross-modal understanding between radiological descriptions and CT imagery that can be transferred to speech-based systems. Quick check - Validate that the text-image CLIP model achieves strong performance on the target classification and retrieval tasks.
- **Speech Foundation Models**: Why needed - Offer pretrained representations of spoken language that can be fine-tuned for medical domain adaptation. Quick check - Ensure the speech model captures medical terminology and context when fine-tuned on radiology reports.
- **Synthetic Speech Generation**: Why needed - Enable creation of large-scale training datasets with controlled acoustic properties and diverse voices. Quick check - Verify that synthetic speech maintains intelligibility and medical terminology accuracy.
- **Zero-shot Classification**: Why needed - Allow evaluation of model performance without requiring labeled speech data, focusing assessment on the distillation effectiveness. Quick check - Confirm that classification accuracy correlates with embedding similarity to class prototypes.

## Architecture Onboarding

### Component Map
Speech Encoder -> Contrastive Alignment Module -> Knowledge Distillation Layer -> CT Image Encoder (frozen CLIP)

### Critical Path
1. Speech encoder processes spoken report to generate embeddings
2. Contrastive loss aligns speech embeddings with corresponding text embeddings
3. Knowledge distillation loss transfers representations from text encoder to speech encoder
4. Final model uses speech embeddings for classification/retrieval with frozen CT encoder

### Design Tradeoffs
The approach trades the simplicity of direct text processing for the clinical authenticity of speech-based reporting. While synthetic speech enables large-scale training, it may not capture the variability of real clinical dictation. The use of knowledge distillation reduces data requirements but introduces dependency on the quality of the teacher model.

### Failure Signatures
- Poor performance on medical terminology not well-represented in training data
- Degradation when speech contains accents or background noise not present in synthetic data
- Mismatch between the domain of the pretrained text-image CLIP model and the target medical dataset

### First Experiments
1. Test contrastive alignment by measuring embedding similarity between paired and unpaired speech-text samples
2. Validate knowledge transfer by comparing speech and text encoder outputs on identical content
3. Evaluate retrieval performance by measuring ranking accuracy of speech descriptions against CT volumes

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Entire evaluation relies on synthetic speech data, raising questions about real clinical applicability
- Performance may degrade with real speech variability including accents, pronunciation differences, and background noise
- Dependency on the quality and representativeness of the underlying text-image CLIP model for knowledge distillation

## Confidence
- **High**: Methodology of speech-to-text embedding alignment and knowledge distillation framework
- **Medium**: Performance improvements demonstrated on synthetic data
- **Low**: Claims about clinical applicability without real speech validation

## Next Checks
1. Test the model on real clinical speech recordings from actual radiology reports to assess robustness to pronunciation variations and background noise
2. Evaluate performance across multiple languages and dialects to determine generalizability beyond English synthetic speech
3. Conduct radiologist studies to validate whether the model's classifications align with clinical interpretation and whether the speech interface improves workflow efficiency compared to text-based systems