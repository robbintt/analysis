---
ver: rpa2
title: 'Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation
  Tasks'
arxiv_id: '2601.13392'
source_url: https://arxiv.org/abs/2601.13392
tags:
- json
- states
- output
- construction
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) demonstrate\
  \ genuine symbolic reasoning or memorization when constructing deterministic finite\
  \ automata (DFAs) from regular languages. A benchmark dataset is introduced with\
  \ seen construction tasks from public sources and unseen problems generated via\
  \ Arden\u2019s theorem and hand-crafted constraints."
---

# Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks

## Quick Facts
- arXiv ID: 2601.13392
- Source URL: https://arxiv.org/abs/2601.13392
- Reference count: 29
- Large language models struggle with novel DFA construction tasks, indicating memorization over genuine reasoning

## Executive Summary
This study investigates whether large language models (LLMs) demonstrate genuine symbolic reasoning or rely on memorization when constructing deterministic finite automata (DFAs) from regular languages. The researchers introduce a benchmark dataset with both seen construction tasks from public sources and unseen problems generated via Arden's theorem and hand-crafted constraints. Testing reveals that while models achieve high accuracy on seen tasks (84-90%), performance drops sharply on unseen problems (20.67-67%), regardless of prompting strategy. The findings indicate that current LLMs rely on pattern matching rather than robust symbolic reasoning for formal language tasks.

## Method Summary
The researchers constructed a benchmark dataset containing seen DFA construction tasks from public sources and unseen problems generated using Arden's theorem and hand-crafted constraints. They tested multiple LLMs across three prompting strategies: direct prompting, Chain-of-Thought, and Tree-of-Thought. A three-stage hint protocol was implemented to identify and correct shallow errors in automata construction. The study measured performance differences between seen and unseen tasks to assess whether models were demonstrating genuine reasoning capabilities or relying on memorized patterns.

## Key Results
- Models achieve high accuracy on seen tasks (84-90%) but performance drops sharply on unseen problems (20.67-59.12%)
- Systematic failures include incorrect Kleene-star handling, constraint composition errors, and introduction of redundant states
- Three-stage hint protocol corrects shallow errors but cannot resolve globally inconsistent automata

## Why This Works (Mechanism)
The study demonstrates that current LLMs rely on pattern matching and memorization rather than genuine symbolic reasoning when solving formal language problems. The sharp performance drop on unseen tasks, regardless of prompting strategy, indicates that models cannot generalize reasoning principles to novel situations. The inability to correctly handle Kleene-star operations and compose constraints suggests fundamental limitations in understanding formal language theory concepts.

## Foundational Learning
- Arden's theorem: Needed for solving systems of equations representing regular expressions; quick check involves verifying that R = Q + RP has solution R = QP* when P does not contain λ
- DFA construction principles: Essential for understanding state transitions and acceptance conditions; quick check involves tracing input strings through constructed automata
- Regular language constraints: Required for proper automaton design; quick check involves verifying that all constraints are satisfied by test strings

## Architecture Onboarding
- Component map: Problem specification -> DFA construction -> State validation -> Hint protocol
- Critical path: Problem parsing → DFA construction → Error identification → Hint application
- Design tradeoffs: Generalizability vs. memorization, prompt complexity vs. performance, automated vs. interactive correction
- Failure signatures: Incorrect Kleene-star handling, constraint composition errors, redundant state introduction, globally inconsistent automata
- First experiments:
  1. Test models on additional formal language problems (NFA to DFA conversion, CFG parsing)
  2. Implement sophisticated interactive tutoring with targeted feedback on specific error types
  3. Conduct ablation studies varying formal language content in training data

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on narrow class of formal language problems (DFA construction)
- Benchmark size, while carefully constructed, remains relatively small
- Hint protocol may not fully explore potential of interactive problem-solving approaches

## Confidence
- High confidence in observed performance gap between seen and unseen tasks
- High confidence in systematic error patterns (Kleene-star handling, constraint composition, redundant states)
- Medium confidence in interpretation that failures indicate lack of symbolic reasoning

## Next Checks
1. Test same models on additional formal language problems (NFA to DFA conversion, context-free grammar parsing) to assess generalization
2. Implement more sophisticated interactive tutoring system with targeted feedback on specific error types
3. Conduct ablation studies varying proportion of formal language content in model training data