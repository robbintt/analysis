---
ver: rpa2
title: Supply Chain Optimization via Generative Simulation and Iterative Decision
  Policies
arxiv_id: '2507.07355'
source_url: https://arxiv.org/abs/2507.07355
tags:
- order
- decision
- simulation
- shipping
- simulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Sim-to-Dec, a unified framework that integrates
  generative simulation with iterative decision-making for optimizing shipping strategies
  in supply chain transportation. The core method consists of two tightly coupled
  components: a generative simulator that learns transportation dynamics from historical
  data using autoregressive modeling, and a decision-maker that combines historical
  experience with future reward estimation to select optimal shipping modes.'
---

# Supply Chain Optimization via Generative Simulation and Iterative Decision Policies

## Quick Facts
- arXiv ID: 2507.07355
- Source URL: https://arxiv.org/abs/2507.07355
- Reference count: 6
- One-line primary result: 50.3% accuracy improvement on DataCo dataset through generative simulation and iterative decision policies

## Executive Summary
This paper introduces Sim-to-Dec, a unified framework that integrates generative simulation with iterative decision-making for optimizing shipping strategies in supply chain transportation. The core method consists of two tightly coupled components: a generative simulator that learns transportation dynamics from historical data using autoregressive modeling, and a decision-maker that combines historical experience with future reward estimation to select optimal shipping modes. The framework enables risk-free evaluation and refinement of transportation strategies through continuous simulation feedback.

Experiments on three real-world datasets demonstrate that Sim-to-Dec significantly outperforms baseline methods. The simulator achieves 50.3%, 7.3%, and 1.1% improvement in overall accuracy over the strongest baselines on DataCo, GlobalStore, and OAS datasets respectively. The decision-maker shows superior performance in balancing timely delivery rates and profit, with 4.1% and 6.8% improvement in overall metrics and 11.8% and 18.1% reduction in the gap between objectives on DataCo and OAS datasets. The framework maintains robust performance under distribution shifts and provides computational efficiency with only 1.2 seconds processing time on large datasets.

## Method Summary
Sim-to-Dec is a two-stage framework for optimizing shipping mode selection in supply chains. First, a generative simulator is pre-trained on historical order data to learn transportation dynamics through autoregressive modeling of order state transitions (delay risk, delivery time, on-time status). The simulator uses LSTM encoders to process order attributes and shipping mode embeddings, then autoregressively generates order states. Second, a decision network interacts with the frozen simulator to select optimal shipping modes, balancing historical experience with simulator-predicted future rewards. The decision-maker uses Gumbel-Softmax sampling for differentiable discrete mode selection and is trained with a combined loss that incorporates both historical rewards and simulated outcomes.

## Key Results
- Simulator achieves 50.3%, 7.3%, and 1.1% improvement in overall accuracy over strongest baselines on DataCo, GlobalStore, and OAS datasets respectively
- Decision-maker improves overall metrics by 4.1% and 6.8% while reducing objective gaps by 11.8% and 18.1% on DataCo and OAS datasets
- Framework maintains robust performance under distribution shifts, with significant accuracy improvements over baseline methods
- Computational efficiency of 1.2 seconds processing time on large datasets

## Why This Works (Mechanism)

### Mechanism 1
Autoregressive simulation of order state transitions captures fine-grained logistics dynamics better than rule-based or single-step prediction approaches. The simulator decomposes evolutionary attributes into a sequential generation process where each attribute is predicted conditioned on previously generated attributes, creating dependency modeling between state variables that single-step predictors miss. This works because order state attributes have sequential dependencies that can be captured autoregressively. Evidence shows the simulator learns to accurately reflect system dynamics through autoregressive modeling. Break condition: if order state attributes are statistically independent, autoregressive modeling offers no advantage over parallel prediction.

### Mechanism 2
Combining historical experience with simulator-based future estimation creates a dual-perspective decision policy that balances exploitation of known patterns with exploration of counterfactual outcomes. The decision network is trained with two loss components: Lh encourages selecting modes with highest historical expected reward, while Lf trains the network to predict batch-level rewards from simulator outputs. The combined loss L_M = L_f + λ·L_h creates gradient signals from both past data distributions and simulated futures. This works because historical patterns remain partially valid under distribution shift while simulator-predicted outcomes capture dynamics that pure historical extrapolation misses. Evidence shows the framework achieves superior performance in balancing objectives. Break condition: if λ is set too high, the model over-relies on stale historical patterns; if too low, it overfits to simulator artifacts.

### Mechanism 3
Freezing the pre-trained simulator during decision-maker training provides a stable environment for policy exploration while preventing feedback loops that could destabilize learning. The simulator is trained first on historical data to minimize prediction loss, then its parameters are frozen. The decision-maker then interacts with this fixed simulator, receiving consistent gradients from simulated outcomes without the simulator adapting to the decision-maker's behavior. This works because the simulator trained on historical data generalizes sufficiently to accurately predict outcomes for novel decision policies. Evidence shows the frozen simulator provides a stable and reliable risk-free environment. Break condition: if the decision-maker explores policies far outside the historical distribution, simulator predictions may become unreliable extrapolations.

## Foundational Learning

- **Concept: Autoregressive sequence modeling**
  - Why needed here: The simulator generates order states sequentially, where each prediction conditions on previous outputs. Understanding teacher forcing vs. autoregressive inference helps debug why simulator performance might differ between training and deployment.
  - Quick check question: If the simulator makes an error predicting delay risk, will that error propagate to delivery time prediction? (Answer: Yes, by design.)

- **Concept: Gumbel-Softmax reparameterization**
  - Why needed here: The decision-maker samples discrete shipping modes while maintaining differentiability for gradient flow. Without understanding this trick, the loss computation appears non-differentiable.
  - Quick check question: Why not just use argmax to select the shipping mode? (Answer: argmax has zero gradients almost everywhere; Gumbel-Softmax provides continuous relaxation during training.)

- **Concept: Contextual bandits vs. full RL**
  - Why needed here: The paper explicitly frames decision-making as a contextual bandit problem, meaning each decision is treated independently without modeling long-horizon state transitions. This is a deliberate simplification.
  - Quick check question: Would this framework handle scenarios where today's shipping mode choice affects tomorrow's inventory levels? (Answer: Not directly—the current formulation treats each order independently; extending to sequential decisions would require full RL.)

## Architecture Onboarding

- **Component map:**
  Historical Data → [Simulator Training Pipeline] Order Attributes + Shipping Mode → LSTM Encoder → LSTM Decoder → Predictors (risk, time, status) → (freeze parameters Θ) → [Decision Pipeline] Order Attributes → Decision Network M → Softmax → Gumbel-Softmax Sample → Simulated States (from frozen S) → Reward Computation (T_Time, T_Profit) → Combined Loss (L_f + λ·L_h) → Update M parameters Φ

- **Critical path:**
  1. Preprocess datasets into (FI, FD, FE) triplets for each order
  2. Train simulator to convergence on historical data (monitor attribute-level accuracy)
  3. Freeze simulator, train decision network with λ tuning on validation set
  4. Evaluate on held-out test set using normalized T_Time + T_Profit

- **Design tradeoffs:**
  - Simulator capacity vs. overfitting: Table 3 shows Sim-to-Dec requires 350 epochs on DataCo vs. 60-110 for baselines—higher capacity comes with training cost
  - Frozen vs. joint training: Freezing simulator stabilizes learning but prevents adaptation if decision-maker discovers novel policies
  - Batch-level reward vs. per-order: Future estimation loss L_f uses batch-level rewards, which may obscure per-order variation but provides more stable gradients

- **Failure signatures:**
  - Simulator divergence under distribution shift: Figure 4 shows Markov/prediction methods produce distributions very different from test data
  - Decision collapse to single mode: If λ is too low, the model may ignore historical constraints and over-explore
  - Historical overfitting: If λ is too high, the model simply reproduces historical mode selections without improvement

- **First 3 experiments:**
  1. Sanity check: Train simulator only, verify prediction accuracy on held-out test set matches Table 1 (~93% on DataCo). If significantly lower, check data preprocessing and attribute encoding.
  2. Ablation on λ: Run decision-maker with λ ∈ {0.1, 0.5, 1.0, 5.0, 10.0} on validation set. Plot Overall metric vs. λ (should show inverted-U as in Figure 5c-d).
  3. Robustness test: Split data by delivery time (as in Figure 4a) to create train/test distribution shift. Compare simulator output distribution to test distribution.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the discussion section highlights potential areas for future research including extension to other supply chain optimization problems beyond shipping mode selection.

## Limitations
- Critical implementation details missing: hyperparameter values, specific data preprocessing steps, and exact normalization formulas are not specified
- Frozen simulator assumption may limit generalization to out-of-distribution policies despite theoretical justification for stability
- Higher computational overhead compared to baselines, requiring 350 epochs vs. 60-110 for baselines
- Limited evaluation of black swan disruptions and long-horizon state transition error accumulation

## Confidence
- **High confidence** in the autoregressive simulation mechanism and dual-perspective decision architecture (supported by clear equations and ablation studies)
- **Medium confidence** in the frozen simulator design choice (theoretically justified but potentially limiting for novel policies)
- **Low confidence** in exact computational requirements and real-world deployment feasibility (lacking runtime benchmarks beyond single-batch processing)

## Next Checks
1. **Distribution shift robustness test:** Train on orders with delivery time < 7 days, test on orders > 14 days. Compare simulator output distribution divergence and decision performance degradation.
2. **Hyperparameter sensitivity analysis:** Systematically vary λ (0.1, 0.5, 1.0, 5.0, 10.0) and learning rate (1e-3, 1e-4, 1e-5) on validation set. Plot Overall metric curves to identify optimal operating regions.
3. **Simulator generalization evaluation:** Generate synthetic policies (e.g., 80% air shipping for high-value orders) not present in historical data. Measure simulator prediction accuracy degradation and decision-maker reward estimation error compared to baseline methods.