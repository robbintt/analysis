---
ver: rpa2
title: 'Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for
  Procedural Content Generation'
arxiv_id: '2512.10501'
source_url: https://arxiv.org/abs/2512.10501
tags:
- actor
- generation
- critic
- language
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a training-free dual-agent architecture that
  enables off-the-shelf large language models to control complex procedural content
  generation (PCG) tools without fine-tuning. The system pairs an Actor agent, which
  interprets natural language prompts and generates initial parameter configurations,
  with a Critic agent, which evaluates these proposals against API documentation and
  usage examples.
---

# Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation

## Quick Facts
- arXiv ID: 2512.10501
- Source URL: https://arxiv.org/abs/2512.10501
- Reference count: 40
- This work presents a training-free dual-agent architecture that enables off-the-shelf large language models to control complex procedural content generation (PCG) tools without fine-tuning.

## Executive Summary
This paper introduces a dual-agent architecture enabling off-the-shelf large language models to control complex procedural content generation tools without fine-tuning. The system pairs an Actor agent, which interprets natural language prompts and generates initial parameter configurations, with a Critic agent, which evaluates these proposals against API documentation and usage examples. Through iterative dialogic refinement, the agents progressively align generated content with user intent while ensuring functional correctness. Experiments on 3D map generation tasks show the approach achieves a 20% higher success rate compared to single-agent baselines and reduces the need for human follow-up prompts.

## Method Summary
The method employs a dual-agent iterative loop where an Actor agent generates a Parameter Trajectory Sequence (JSON-formatted configuration) and a Critic agent validates it against API documentation and examples. The Actor, configured as a tool planner with system instructions to decouple planning from execution, produces a trajectory summary, tool plan with objectives/tools/arguments/expected results, and risk assessment. The Critic, operating under a conservative policy to only flag blocking issues when certain, reviews five dimensions: Tool Selection, Parameter Correctness, Logic & Sequence, Goal Alignment, and Certainty. The system uses state-replacement context management rather than full history append, iterating until approval or maximum iterations.

## Key Results
- Dual-agent architecture achieves 80% task success rate versus 60% for single-agent baselines
- 20% higher success rate in 3D map generation tasks
- Reduces need for human follow-up prompts through autonomous refinement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling semantic planning from functional verification improves reliability in high-dimensional parameter spaces compared to single-agent generation.
- **Mechanism:** The architecture separates the "what" (Actor interpreting user intent) from the "how" (Critic validating against API constraints). By forcing the Actor to propose and the Critic to disposition, the system reduces the cognitive load on a single model, preventing it from simultaneously hallucinating creative solutions and strict parameter values.
- **Core assumption:** The underlying LLM possesses sufficient reasoning capability to correct its own outputs when presented with specific error feedback.
- **Evidence anchors:**
  - [abstract] "Our system pairs an Actor agent... with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters."
  - [Section 3.1] "The Actor bridges the semantic gap... [while] the Critic prioritizes functional correctness."
  - [corpus] MapStory (arXiv:2505.21966) similarly employs a dual-agent architecture for animation, suggesting generalizability of this separation-of-concerns approach.
- **Break condition:** If the Critic's feedback is ambiguous or if the Actor ignores the critique, the loop fails to converge.

### Mechanism 2
- **Claim:** Static documentation injection grounds the model in domain-specific constraints, mitigating hallucination of invalid parameters.
- **Mechanism:** Instead of relying on pre-trained knowledge (which may be outdated or incomplete regarding specific tools like "TileWorldCreator"), the system injects API documentation and reference examples directly into the context window. This forces the LLM to attend to rigid syntactic rules (e.g., valid value ranges) during generation.
- **Core assumption:** The relevant documentation fits within the model's effective context window and is semantically parseable by the LLM.
- **Evidence anchors:**
  - [abstract] "...bridging the semantic gap... via static documentation injection and in-context reasoning."
  - [Section 4.3] "When supplied with documentation and usage examples, the LLM successfully grounded its generation... sufficient for competent content generation even without... RAG."
  - [corpus] UnrealLLM (cited in Section 2) relies on constructed knowledge bases, whereas this work suggests mere documentation may suffice.
- **Break condition:** Failure occurs if the documentation is incomplete, contradictory, or exceeds the context limit, causing the model to revert to speculative guessing.

### Mechanism 3
- **Claim:** Iterative dialogic refinement converts a zero-shot problem into a chain of verifiable sub-tasks.
- **Mechanism:** Rather than generating a perfect configuration in one pass, the Actor generates an initial "Parameter Trajectory Sequence." The Critic identifies specific "blocking issues." This transforms a complex generation task into a debugging task, which LLMs are often better equipped to handle.
- **Core assumption:** The verification step is accurate; if the Critic falsely flags a valid parameter or misses a fatal error, the refinement loop degrades output quality.
- **Evidence anchors:**
  - [abstract] "Through iterative dialogic refinement, the agents progressively align generated content... while ensuring functional correctness."
  - [Section 3.3] "This interaction replaces the need for gradient updates with in-context reasoning."
  - [corpus] *Procedural Game Level Design with Deep Reinforcement Learning* (arXiv:2510.15120) contrasts this by using RL for stability, highlighting that this paper trades training time for inference-time computation.
- **Break condition:** The mechanism relies on the Critic being strictly conservative (only flagging "blocking issues" when certain); if it becomes "chatty," the system may loop indefinitely.

## Foundational Learning

- **Concept:** **Procedural Content Generation (PCG) Parameters**
  - **Why needed here:** PCG tools are not "prompt-to-image" generators; they are algorithmic engines requiring precise numerical inputs (e.g., "erosion iterations," "noise scale"). Misunderstanding this leads to expecting semantic control where only mathematical control exists.
  - **Quick check question:** Can you explain why setting a "seed" value matters for determinism in a PCG pipeline?

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** The paper's "training-free" claim rests entirely on ICLâ€”the model's ability to infer rules from the prompt (documentation/examples) without weight updates.
  - **Quick check question:** If you provide an LLM with a Python function signature and a usage example in the prompt, and it correctly calls that function, what learning paradigm is it demonstrating?

- **Concept:** **Hallucination in Tool Use**
  - **Why needed here:** The primary role of the Critic agent is to catch the Actor inventing parameters that don't exist in the API.
  - **Quick check question:** Why would an LLM invent a parameter called "smoothness" for a tool that only exposes "roughness," and how does documentation injection prevent this?

## Architecture Onboarding

- **Component map:** User Interface -> Context Manager -> Actor Agent -> Critic Agent -> Executor
- **Critical path:** The **Prompt Formulation (Section 3.2)** is the highest leverage point. If the Actor's prompt doesn't explicitly forbid execution or if the Critic's prompt doesn't enforce "conservative certainty" (only flag certain errors), the system becomes unstable.
- **Design tradeoffs:**
  - **Reliability vs. Latency:** The Critic doubles the inference cost per iteration. The paper admits this incurs "higher latency" compared to single-shot baselines.
  - **Context Window vs. Detail:** You must balance how much API documentation to include versus leaving space for the dialog history.
- **Failure signatures:**
  - **Infinite Loop:** Critic repeatedly finds issues Actor cannot fix (often due to missing context/docs).
  - **Regressive Editing:** Actor over-corrects and breaks a previously working parameter.
  - **Speculative Parameters:** Actor invents parameters not in the docs (indicates Docs were ignored or insufficient).
- **First 3 experiments:**
  1. **Sanity Check (Single Agent):** Run the Actor *without* the Critic on a simple 2D map task to establish a baseline error rate.
  2. **Ablation on Docs:** Run the dual-agent system *without* the Reference Demonstration examples to measure the drop in "first-try" success rate.
  3. **Constraint Stress Test:** Prompt for a specific 3D structure with conflicting constraints (e.g., "steep mountain" but "flat terrain") to observe if the Critic can identify logical impossibilities.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a specific dual-agent architecture with hard-coded prompt structures that may not generalize to other PCG domains or LLM backends.
- Performance is measured on a narrow set of Unity-based map-generation tasks with one plugin, limiting external validity.
- The claim of "training-free" depends entirely on effective in-context learning, which may degrade with longer, more complex documentation or noisy real-world APIs.

## Confidence
- **High confidence**: The dual-agent separation of planning and verification reduces parameter-space errors and supports the reported 20% success rate improvement over single-agent baselines.
- **Medium confidence**: Static documentation injection reliably grounds generation in tool constraints for the tested cases, but scalability to larger APIs or less structured domains is unproven.
- **Low confidence**: The claim that iterative dialogic refinement can fully replace fine-tuning for complex PCG tools is supported only by limited experimental evidence and may not hold for tasks requiring deeper tool-specific reasoning.

## Next Checks
1. **Generalization Test**: Apply the dual-agent system to a new PCG tool (e.g., Blender geometry nodes) with a different parameter space and evaluate success rate and iteration count.
2. **Documentation Robustness Test**: Systematically reduce or corrupt the API documentation in the prompt and measure the impact on Actor and Critic accuracy.
3. **Human Evaluation**: Conduct a user study comparing human satisfaction and task completion time for the dual-agent system versus a fine-tuned baseline or manual parameter tuning.