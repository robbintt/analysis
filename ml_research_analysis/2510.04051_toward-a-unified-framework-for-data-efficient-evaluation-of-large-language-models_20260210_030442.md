---
ver: rpa2
title: Toward a unified framework for data-efficient evaluation of large language
  models
arxiv_id: '2510.04051'
source_url: https://arxiv.org/abs/2510.04051
tags:
- ability
- lego-irt
- evaluation
- benchmarks
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGO-IRT is a unified, data-efficient framework for large language
  model evaluation that addresses limitations of prior IRT-based methods. It natively
  supports both binary and continuous evaluation metrics and introduces a factorized
  architecture to explicitly model and leverage structural knowledge such as correlations
  across metrics and benchmarks.
---

# Toward a unified framework for data-efficient evaluation of large language models

## Quick Facts
- arXiv ID: 2510.04051
- Source URL: https://arxiv.org/abs/2510.04051
- Reference count: 40
- Primary result: LEGO-IRT framework achieves stable LLM capability estimates using 3% of evaluation items with 10% error reduction through structural knowledge incorporation

## Executive Summary
LEGO-IRT introduces a novel framework for evaluating large language models that addresses critical limitations in existing Item Response Theory (IRT) approaches. The framework unifies evaluation across both binary and continuous metrics while explicitly modeling structural knowledge including correlations between metrics and benchmarks. Through extensive experiments with 70 state-of-the-art LLMs across 5 benchmarks, LEGO-IRT demonstrates superior data efficiency and estimation accuracy compared to traditional evaluation methods.

The key innovation lies in the framework's factorized architecture that separates the modeling of item characteristics, ability parameters, and structural relationships. This design enables stable capability estimates with only 3% of evaluation items while reducing estimation error by up to 10% through the incorporation of structural knowledge. The latent ability estimates produced by LEGO-IRT show potential alignment with human preferences, suggesting improved interpretability for LLM assessment.

## Method Summary
LEGO-IRT builds upon Item Response Theory to create a unified evaluation framework for large language models. The framework extends traditional IRT by incorporating factorized architecture that explicitly models both the correlation structure across evaluation metrics and the relationships between different benchmarks. This architectural innovation allows LEGO-IRT to handle both binary and continuous evaluation metrics natively, addressing a key limitation of prior IRT-based methods. The framework estimates model capabilities through latent parameters while leveraging structural knowledge about metric relationships to improve estimation accuracy and stability.

## Key Results
- Stable capability estimates achieved using only 3% of total evaluation items
- Up to 10% reduction in estimation error when incorporating structural knowledge
- Framework tested across 70 state-of-the-art LLMs on 5 diverse benchmarks

## Why This Works (Mechanism)
LEGO-IRT's effectiveness stems from its factorized architecture that explicitly models the complex relationships between evaluation metrics and benchmarks. By treating structural knowledge as first-class citizens in the modeling process, the framework can leverage correlations and dependencies that traditional evaluation methods ignore. This explicit modeling allows for more efficient use of evaluation data, as the framework can infer capabilities from limited samples by understanding how different metrics relate to each other. The native support for both binary and continuous metrics eliminates the need for separate evaluation pipelines, ensuring consistent capability estimation across diverse evaluation scenarios.

## Foundational Learning

**Item Response Theory (IRT)**
Why needed: Provides mathematical foundation for modeling the relationship between examinee ability and item difficulty
Quick check: Can estimate latent abilities from binary response data

**Factor Analysis**
Why needed: Enables decomposition of complex relationships into interpretable components
Quick check: Can represent multivariate relationships through lower-dimensional factors

**Structural Equation Modeling**
Why needed: Allows explicit modeling of relationships between observed and latent variables
Quick check: Can incorporate both measurement and structural components in unified framework

**Bayesian Inference**
Why needed: Provides principled approach for parameter estimation with uncertainty quantification
Quick check: Can handle hierarchical models with partial pooling

## Architecture Onboarding

Component map: Item Characteristics -> Factorized Structure -> Ability Parameters -> Capability Estimates

Critical path: The framework processes evaluation items through three main stages: (1) item characteristic modeling, (2) structural relationship learning, and (3) ability parameter estimation. The factorized architecture enables parallel processing of metric-specific and benchmark-specific relationships.

Design tradeoffs: The framework balances computational efficiency with modeling flexibility by factorizing the joint distribution. While this enables handling of complex structural relationships, it requires careful hyperparameter tuning to prevent overfitting on limited evaluation data.

Failure signatures: Poor convergence may occur when structural relationships are weak or when evaluation metrics have minimal correlation. Over-reliance on structural knowledge can lead to biased estimates when the assumed relationships don't match reality.

First experiments:
1. Verify framework performance on synthetic data with known ground truth
2. Compare estimation accuracy with traditional IRT on binary-only evaluation scenarios
3. Test framework scalability with increasing numbers of metrics and benchmarks

## Open Questions the Paper Calls Out

The paper highlights several open questions regarding the generalizability of LEGO-IRT across different LLM domains and evaluation contexts. Key uncertainties include the framework's performance on specialized models beyond the tested state-of-the-art LLMs, and its effectiveness when applied to evaluation metrics with different distributional properties than those in the original experiments.

## Limitations

- Framework validation limited to specific set of 5 benchmarks, may not generalize to all evaluation scenarios
- Claims about human preference alignment require additional empirical validation across diverse user groups
- Computational complexity of factorized architecture may limit scalability for extremely large evaluation suites

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Stable estimates with 3% items | Medium |
| 10% error reduction from structural knowledge | Medium |
| Human preference alignment | Low |
| Generalizability across domains | Low |

## Next Checks

1. Test the framework's performance on additional LLM domains beyond the current scope, including specialized models and emerging architectures
2. Conduct cross-validation studies comparing LEGO-IRT's human preference alignment against multiple established evaluation frameworks
3. Evaluate the framework's robustness when applied to evaluation metrics with different distributions and correlation structures than those in the original experiments