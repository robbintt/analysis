---
ver: rpa2
title: 'TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities
  of AI'
arxiv_id: '2512.23217'
source_url: https://arxiv.org/abs/2512.23217
tags:
- comfort
- thermal
- human
- agents
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TCEval framework addresses the need for AI benchmarks that
  assess real-world cognitive abilities by using thermal comfort scenarios. It evaluates
  AI systems on cross-modal reasoning, causal association, and adaptive decision-making
  through LLM agents that simulate human-like perception and behavior.
---

# TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI

## Quick Facts
- arXiv ID: 2512.23217
- Source URL: https://arxiv.org/abs/2512.23217
- Authors: Jingming Li
- Reference count: 17
- Primary result: LLM agents achieve up to 57% PMV alignment with human data under ±1 tolerance but fail at discrete classification (AUC ≈ 0.5)

## Executive Summary
TCEval introduces a framework that uses thermal comfort scenarios to evaluate AI systems' real-world cognitive abilities, specifically cross-modal reasoning, causal association, and adaptive decision-making. The framework initializes LLM agents with demographic profiles and environmental data, then assesses their ability to generate thermal comfort judgments comparable to human responses. Experiments with four LLMs reveal that while agents can perform foundational cross-modal reasoning (achieving 57% alignment under ±1 PMV tolerance), they fundamentally lack precise causal understanding, as evidenced by near-random performance in discrete comfort classification and statistically significant distribution divergence from human data. TCEval provides an ecologically valid Cognitive Turing Test for human-centric AI applications.

## Method Summary
The TCEval framework evaluates LLM agents by initializing them with virtual personality attributes (gender, age, height, weight) and providing environmental parameters (temperature, humidity, activity context). Agents respond with clothing insulation selections and thermal comfort feedback, which are compared against human data from the ASHRAE Global Database and Chinese Thermal Comfort Database using PMV alignment metrics. The evaluation employs exact PMV match rates, ±1 tolerance alignment, Wilcoxon Signed-Rank tests for distribution comparison, and ROC-AUC for discrete comfort classification across categories like "cold," "neutral," and "hot."

## Key Results
- LLM agents achieve up to 57% PMV alignment with human data under ±1 tolerance threshold
- Agents perform near-randomly on discrete comfort classification (AUC ≈ 0.5)
- Statistical tests confirm LLM-generated PMV distributions significantly diverge from human data (Wilcoxon p ≈ 0)
- Larger models (DeepSeek-R1:32B at 57%) show better performance than smaller ones (Mistral-Small3.2:24B at 38%)

## Why This Works (Mechanism)

### Mechanism 1
LLM agents can perform cross-modal reasoning by integrating environmental parameters with personal attributes to generate directionally correct thermal comfort judgments. Agents initialized with demographic profiles (age, gender, height, weight) receive numerical environmental data (temperature, humidity) and categorical context (activity type), then synthesize these modalities into PMV-like outputs through natural language inference. Core assumption: The semantic representations learned during pre-training contain sufficient latent knowledge about human thermal physiology to enable approximate comfort reasoning. Evidence anchors: Current LLMs possess foundational cross-modal reasoning ability; the improvement in alignment when allowing a ±1 PMV tolerance suggests that agents can perform cross-modal reasoning by integrating provided environmental and personal data. Break condition: When precise numerical relationships are required—agents fail at discrete classification (AUC ≈ 0.5) and show statistically significant distribution divergence (Wilcoxon p ≈ 0).

### Mechanism 2
Agent alignment with human data scales with model size and architectural optimization for structured reasoning. Larger models (DeepSeek-R1:32B at 57% vs Mistral-Small3.2:24B at 38% under ±1 PMV tolerance) capture more nuanced semantic patterns from training data, while architecture-specific optimizations can partially compensate for smaller scale. Core assumption: Thermal comfort knowledge is emergent from broad training corpora rather than requiring explicit physiological modeling. Evidence anchors: Experiments with four LLMs show that agents achieve up to 57% alignment; larger models like DeepSeek-R1 suggests that scale and broader training data correlate with a better ability to capture the nuanced, non-linear relationships. Break condition: Scale alone does not resolve categorical classification failures; even best models perform near-random on discrete comfort labels.

### Mechanism 3
The framework acts as a "Cognitive Turing Test" by exposing the gap between surface-level reasoning and grounded causal understanding. TCEval requires completing a perception-reasoning-adaptation loop: interpreting sensor data → selecting clothing insulation → providing PMV feedback → suggesting adaptive actions. Failure at any stage reveals incomplete cognitive modeling. Core assumption: Human thermal comfort represents an ecologically valid cognitive task requiring embodied, multi-variable causal reasoning that cannot be solved by pattern matching alone. Evidence anchors: TCEval provides an ecologically valid Cognitive Turing Test; agents often default to a 'neutral' bias, failing to accurately simulate extreme sensations like 'cold' or 'hot'. Break condition: If agents could memorize ASHRAE PMV equations, they would achieve high theoretical alignment but fail human-data alignment—the framework distinguishes these failure modes.

## Foundational Learning

- Concept: **Predicted Mean Vote (PMV) and thermal comfort modeling**
  - Why needed here: The entire framework evaluates LLM outputs against ASHRAE Standard 55-2023 PMV values; understanding that PMV integrates six variables (air temp, radiant temp, humidity, air velocity, clothing insulation, metabolic rate) is essential.
  - Quick check question: Given identical temperature, why would a person at rest feel differently than one exercising?

- Concept: **LLM agent initialization and persona assignment**
  - Why needed here: The methodology requires "initializing LLM agents with virtual personality attributes" before querying comfort feedback; understanding prompt engineering for persona instantiation is critical.
  - Quick check question: What information must be included in an agent's context to simulate a 70-year-old occupant in a warm climate?

- Concept: **Distributional alignment vs. point estimation**
  - Why needed here: The paper shows agents fail exact match (31% best) but improve under ±1 tolerance (57%); understanding tolerance-based evaluation is key to interpreting results.
  - Quick check question: Why might an agent's PMV distribution be statistically distinct from humans even if mean values are similar?

## Architecture Onboarding

- Component map: Persona loading -> Environmental parameter injection -> LLM inference -> PMV extraction -> Statistical comparison
- Critical path: Persona loading → environmental parameter injection → LLM inference → PMV extraction → statistical comparison
- Design tradeoffs:
  - Tolerance threshold (±1 PMV) increases apparent alignment but masks precision failures
  - Using real human datasets vs. theoretical PMV calculations tests different capabilities (human mimicry vs. physics knowledge)
  - Assumption: Agent prompt format (not specified in detail) significantly impacts performance—this is an uncontrolled variable
- Failure signatures:
  - **Neutral bias**: Agents cluster predictions around PMV ≈ 0, failing on extreme sensations
  - **Distribution divergence**: Wilcoxon p-values effectively zero indicates systematic shift, not random noise
  - **Categorical collapse**: AUC ≈ 0.5 on discrete classification indicates agents cannot reliably distinguish "cold" from "cool" from "neutral"
- First 3 experiments:
  1. Replicate the ±1 PMV tolerance test on a held-out subset of ASHRAE data to establish baseline alignment for your target model
  2. Ablate persona attributes one at a time (remove age, then gender, then weight) to measure which contextual cues drive alignment improvements
  3. Compare agent performance on theoretical PMV calculation vs. human-data alignment to diagnose whether failures are physiological (wrong model) or distributional (wrong population)

## Open Questions the Paper Calls Out

- What architectural or training modifications would enable LLMs to overcome their "neutral bias" and accurately simulate extreme thermal sensations ("cold" or "hot")?
  - Basis in paper: The discussion section states that agents "often default to a 'neutral' bias, failing to accurately simulate extreme sensations like 'cold' or 'hot'" and identifies this as a key failure mode.
  - Why unresolved: Current LLMs show near-random performance (AUC ~0.5) in discrete categorical comfort classification, indicating fundamental gaps in grounded psychophysical mappings.
  - What evidence would resolve it: Demonstrated improvement in ROC-AUC scores for extreme comfort categories after targeted architectural interventions or fine-tuning approaches.

- How would the TCEval framework perform when applied to diverse cultural and climatic contexts beyond the Chinese Thermal Comfort Database?
  - Basis in paper: The conclusion notes "existing experiments have only conducted PMV tests on the CTC, and further in-depth experiments will reveal more findings."
  - Why unresolved: Cultural factors significantly influence thermal comfort perception, and the current validation is limited to Chinese and ASHRAE global data without cross-cultural analysis.
  - What evidence would resolve it: Comparative alignment metrics across multiple regional thermal comfort databases (e.g., European, North American, tropical regions) showing consistent or varying performance patterns.

- What is the impact of incorporating longer-term adaptation dynamics and temporal reasoning on LLM agents' thermal comfort decision-making accuracy?
  - Basis in paper: Section 5.3 states that future iterations "could introduce more complex social dynamics, longer-term adaptation, and a wider range of climatic extremes."
  - Why unresolved: Current experiments capture single-point assessments, whereas human thermal comfort involves continuous adaptation cycles over time.
  - What evidence would resolve it: Longitudinal experiment results showing whether sequential decision-making improves alignment with human adaptive behaviors compared to isolated judgments.

## Limitations
- The evaluation framework's ±1 PMV tolerance artificially inflates alignment metrics while masking fundamental failures in precise causal reasoning
- Exact prompt engineering methodology remains unspecified, creating critical reproducibility gaps
- Statistical analysis doesn't distinguish between systematic bias and model architecture limitations
- Human datasets used may have sampling biases affecting generalizability across different populations and climates

## Confidence

- **High confidence**: The fundamental finding that current LLMs fail at discrete thermal comfort classification (AUC ≈ 0.5) and show statistically significant distribution divergence from human data. These results are robust across multiple models and evaluation metrics.
- **Medium confidence**: The interpretation that ±1 PMV tolerance improvements indicate "foundational cross-modal reasoning ability." While the correlation exists, the tolerance may obscure precision failures that are critical for real-world applications.
- **Low confidence**: The scalability claim that larger models inherently perform better on thermal comfort tasks. The evidence shows correlation (DeepSeek-R1:32B at 57% vs Mistral-Small3.2:24B at 38%), but this could reflect architectural differences rather than pure scale effects.

## Next Checks

1. **Tolerance ablation study**: Systematically vary the PMV tolerance threshold (0, ±0.5, ±1, ±1.5) and measure alignment curves to determine if the ±1 improvement represents a meaningful cognitive capability or merely a lenient metric choice.

2. **Architectural intervention test**: Apply specific architectural optimizations (chain-of-thought prompting, structured output formatting, or fine-tuning on thermal datasets) to smaller models and measure whether performance improvements follow scale or intervention patterns.

3. **Population bias analysis**: Stratify human dataset results by demographic factors (age, gender, climate region) and compare agent performance across these strata to identify whether distributional divergence stems from model limitations or dataset representativeness issues.