---
ver: rpa2
title: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents'
arxiv_id: '2502.15840'
source_url: https://arxiv.org/abs/2502.15840
tags:
- agent
- sonnet
- machine
- tool
- business
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vending-Bench is a simulation environment for testing long-term
  coherence of autonomous LLM agents. Agents manage a vending machine business over
  extended periods (20M tokens per run), balancing inventory, pricing, orders, and
  daily fees.
---

# Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents

## Quick Facts
- **arXiv ID**: 2502.15840
- **Source URL**: https://arxiv.org/abs/2502.15840
- **Reference count**: 20
- **Primary result**: Top models like Claude 3.5 Sonnet and o3-mini often turn a profit managing a vending machine over extended runs, but all models occasionally fail due to misinterpreting delivery schedules, forgetting orders, or entering unproductive loops.

## Executive Summary
Vending-Bench is a simulation environment designed to test the long-term coherence of autonomous LLM agents. Agents manage a vending machine business over extended periods (>20M tokens per run), balancing inventory, pricing, orders, and daily fees. The benchmark reveals high variance in performance: top models like Claude 3.5 Sonnet and o3-mini often turn a profit, but all models occasionally fail due to misinterpreting delivery schedules, forgetting orders, or entering unproductive loops. These failures are not directly tied to context window limits. The benchmark highlights the challenge of maintaining coherent decision-making over long horizons and tests models' ability to acquire and manage capital, relevant for both beneficial and potentially dangerous AI scenarios.

## Method Summary
The benchmark uses the inspect-ai framework with a basic agent loop calling tools for email handling, sub-agent delegation, and time advancement. Agents operate a 12-slot vending machine with $500 initial balance and $2/day fees, running up to 2,000 messages across 5 trials per model. The environment simulates supplier emails (via GPT-4o + Perplexity) and customer purchases (price elasticity model). Primary metric is net worth (cash + machine cash + inventory value). Agents use three memory tools: scratchpad, key-value store, and vector DB with text-embedding-3-small.

## Key Results
- Claude 3.5 Sonnet and o3-mini achieve profitability in most runs, demonstrating coherent long-horizon decision-making
- Performance failures correlate poorly with context window saturation (r=0.167), suggesting reasoning breakdowns rather than memory limits
- Larger memory capacities (60k tokens) degrade performance compared to 30k or 10k, indicating attention dilution
- All models occasionally enter "meltdown" loops involving legal threats or tangential reasoning after minor setbacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based agents can perform coherent long-horizon tasks when they maintain consistent decision-making patterns and avoid cascading reasoning errors.
- Mechanism: The agent loop processes tool outputs, maintains state via memory tools (scratchpad, key-value store, vector DB), and executes actions. Success depends on correctly interpreting environment feedback (emails, inventory) and maintaining task-focused behavior over thousands of iterations.
- Core assumption: Coherence stems from reasoning stability, not just context capacity.
- Evidence anchors:
  - [abstract] "Claude 3.5 Sonnet and o3-mini manage the machine well in most runs and turn a profit"
  - [section 3.2.1] "Sonnet appears to have a similar strategy over the entire run – each day it checks its money balance and writes the daily results to the scratchpad"
  - [corpus] Mem2ActBench (arxiv 2601.19935) similarly evaluates long-term memory utilization for task-oriented agents.
- Break condition: Agent misinterprets environment state (e.g., believes delivery arrived when in transit), triggering tangential reasoning loops.

### Mechanism 2
- Claim: Performance failures are not primarily caused by context window saturation but by reasoning breakdowns when handling asynchronous events.
- Mechanism: Agents receive delivery confirmation emails with expected dates, then assume orders arrived at that date regardless of actual delivery timing. When sub-agent restocking fails, models enter "meltdown" loops (legal threats, FBI escalation) rather than recovering.
- Core assumption: Failure is a reasoning/situational-awareness problem, not a memory-limit problem.
- Evidence anchors:
  - [section 3.6] "Pearson correlation between 'Days Until Sales Stop' and 'Days Until Full Memory' across the data points in Table 9 is 0.167"
  - [section 3.2.2] "the way they fail is usually the same. The agent receives a delivery confirmation email... It then assumes the order has arrived as soon as that date is reached"
  - [corpus] Limited direct corpus evidence on this specific mechanism; this appears to be a novel finding in Vending-Bench.
- Break condition: Agent encounters ambiguous timing and cannot verify ground truth before acting.

### Mechanism 3
- Claim: Higher memory capacity does not improve performance and may degrade it due to attention dilution over longer context.
- Mechanism: Agents with 60k token memory performed worse than 30k or 10k. The attention mechanism must process more historical context without improved filtering, reducing effective focus on current decisions.
- Core assumption: LLM attention degrades with longer context even within stated context limits.
- Evidence anchors:
  - [section 3.5.2] "agents with larger memory capacities performed worse than those with less memory"
  - [section 3.5.2] "there is no significant difference in the use of memory-related tools between agents with varying memory capacities"
  - [corpus] No directly comparable corpus finding; this is a Vending-Bench-specific observation.
- Break condition: Agent faces a task requiring precise retrieval from long history.

## Foundational Learning

- Concept: Agent loop with tool calling
  - Why needed here: Vending-Bench is implemented as a loop where the LLM iteratively calls tools (send_email, run_sub_agent, wait_for_next_day) based on history. Understanding this pattern is essential to debug traces.
  - Quick check question: Can you trace a 5-step sequence from a tool call to its environment feedback to the next decision?

- Concept: Asynchronous simulation time
  - Why needed here: Actions advance simulation time by 5 min–5 h. Delivery schedules span multiple simulated days. Agents must reason about future states and delayed feedback.
  - Quick check question: If an agent orders products on Day 10 with 3-day delivery, when should it check inventory?

- Concept: Context window management
  - Why needed here: The implementation caps active context at N tokens (30k default). Engineers must understand this truncation when designing memory tools and analyzing failure timing.
  - Quick check question: At 500 tokens/day, how many simulated days fit in 30k tokens before truncation begins?

## Architecture Onboarding

- Component map: Main agent loop (inspect-ai) -> Sub-agent (multiagent-inspect) -> Environment simulation -> Supplier email generation (GPT-4o + Perplexity) -> Customer purchase model (price elasticity)

- Critical path: Agent receives email -> Interprets state -> Calls tools (restock, price adjustment, cash collection) -> Sub-agent executes physical tasks -> Environment advances time -> New state feedback

- Design tradeoffs: Memory tool selection (scratchpad vs vector DB vs KV store) vs context window limits; synchronous vs asynchronous action timing; email simulation vs real API integration

- Failure signatures: Delivery assumption errors (restocking without confirmation), meltdown loops (legal escalation tangents), inventory mismanagement (over/under-stocking)

- First experiments:
  1. Run single-agent trial with 30k context, logging all tool calls and state transitions
  2. Compare performance with 10k vs 60k memory capacity settings
  3. Implement delivery confirmation validation before restocking action

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the agent's token memory capacity lead to worse performance in this environment?
- Basis in paper: [explicit] Section 3.5.2 states that "agents with larger memory capacities performed worse than those with less memory," specifically noting that the 60k token configuration underperformed the 30k and 10k configurations.
- Why unresolved: The authors expected the attention mechanism to filter relevant information, but found performance decreases with more context, observing no significant difference in memory-tool usage to compensate.
- What evidence would resolve it: An analysis of attention patterns showing "distracted" focus or a study demonstrating improved performance using a specific retrieval-augmented generation (RAG) method over raw context expansion.

### Open Question 2
- Question: What specific criteria define "saturation" for the Vending-Bench benchmark?
- Basis in paper: [explicit] The conclusion states, "Since the benchmark score does not have a defined upper limit, saturation is not a precisely defined point."
- Why unresolved: While the authors suggest saturation is reached when models "consistently understand... and achieve low variance," the current high variance makes it unclear what numerical score constitutes "solving" the task.
- What evidence would resolve it: A defined threshold (e.g., "Net worth > $X with variance < Y% over 100 runs") or a theoretical ceiling analysis on the economic model.

### Open Question 3
- Question: What are the root causes of "meltdown" loops if not context window overflow?
- Basis in paper: [explicit] The paper notes in Section 3.6 that there is "no clear correlation between failures and the point at which the model's context window becomes full," yet models still enter "tangential 'meltdown' loops."
- Why unresolved: The authors identify the *symptoms* (e.g., misinterpreting delivery schedules, inventing legal scenarios) but only rule out memory limits, leaving the underlying reasoning failure mechanism unexplained.
- What evidence would resolve it: Identification of specific attention heads or circuit components responsible for the "tangential" divergence, or a successful intervention that prevents these loops without altering context length.

## Limitations

- Benchmark's narrow business domain (vending machines) limits generalizability to other long-horizon tasks
- Reliance on simulated suppliers/customers rather than real APIs may not reflect real-world complexity
- Absence of baseline non-LLM control agents makes it difficult to determine if failures are inherent to LLM architectures

## Confidence

- **High Confidence**: The finding that top models (Claude 3.5 Sonnet, o3-mini) can achieve profitability in most runs is well-supported by the 5-run experimental design and clear metrics. The basic architecture description (agent loop with tool calling, memory tools) is precisely specified.
- **Medium Confidence**: The claim that performance failures stem from reasoning breakdowns rather than context limits is plausible but based on a single correlation analysis (r=0.167 between "Days Until Sales Stop" and "Days Until Full Memory"). The negative correlation between memory capacity and performance is observed but lacks a clear theoretical explanation.
- **Low Confidence**: The assertion that these failure modes are relevant for "dangerous AI scenarios" extends beyond the empirical evidence. While the benchmark tests capital management and long-term planning, no connection is established between vending machine performance and potential misuse scenarios.

## Next Checks

1. **Cross-domain replication**: Implement the same agent architecture for a different long-horizon task (e.g., managing a simulated restaurant or investment portfolio) to test whether the identified failure modes (delivery assumption errors, meltdown loops) persist across domains.

2. **Memory capacity ablation with fine-tuning**: Instead of just varying memory tool usage, train a version of the agent with explicit training on the delivery assumption failure mode (e.g., teaching it to wait for confirmation emails before acting) to determine if architectural limitations or training data gaps drive the failures.

3. **Control comparison with rule-based agents**: Implement a deterministic, rule-based vending machine manager using the same simulation environment to establish whether LLM failures represent fundamental limitations or simply suboptimal strategies compared to algorithmic approaches.