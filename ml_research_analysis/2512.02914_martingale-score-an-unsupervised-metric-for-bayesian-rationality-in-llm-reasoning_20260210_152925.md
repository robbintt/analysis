---
ver: rpa2
title: 'Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning'
arxiv_id: '2512.02914'
source_url: https://arxiv.org/abs/2512.02914
tags:
- belief
- reasoning
- martingale
- entrenchment
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Martingale Score as an unsupervised metric
  for belief entrenchment in LLM reasoning. The score measures predictability of belief
  updates from prior beliefs, violating the Martingale property of rational Bayesian
  updating.
---

# Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning

## Quick Facts
- arXiv ID: 2512.02914
- Source URL: https://arxiv.org/abs/2512.02914
- Reference count: 18
- One-line primary result: Unsupervised Martingale Score detects belief entrenchment in LLM reasoning, correlating with accuracy drops in domains with ground truth.

## Executive Summary
This paper introduces the Martingale Score as a statistical measure of belief entrenchment in LLM reasoning. The metric quantifies how predictably a model's prior beliefs influence subsequent belief updates, violating the Martingale property of rational Bayesian updating. By regressing belief updates against prior beliefs using OLS regression, the Martingale Score captures the extent to which reasoning is driven by prior entrenchment rather than evidence. Experiments across forecasting, value-laden questions, and academic review domains demonstrate widespread belief entrenchment across models and reasoning techniques, with the score correlating with accuracy degradation in domains with ground truth.

## Method Summary
The Martingale Score is computed by having an LLM judge assign belief probabilities to each step of a reasoning trace, then calculating the OLS regression coefficient β₁ from the equation ∆b = β₁·b_prior + β₀ + ε, where ∆b is the belief update and b_prior is the prior belief. The method tests two reasoning techniques (Chain-of-Thought and Debate) across three prompt conditions (none, prior-conforming, critical-thinking) using multiple model families (GPT-4o, DeepSeek R1/V3, Gemini 2.0 Flash, Llama 4). The score is validated by correlating it with Brier Scores in domains where ground truth exists.

## Key Results
- Belief entrenchment is widespread across all tested models and reasoning techniques, with highest entrenchment in subjective domains and under prior-conforming prompts
- Debate reasoning consistently reduces belief entrenchment compared to Chain-of-Thought, with DeepSeek R1 showing particularly low entrenchment
- The Martingale Score correlates with accuracy degradation (Brier Score) in forecasting and OpenReview domains, validating it as a proxy for truth-seeking ability
- LLM judges achieve 0.72-0.88 agreement when measuring expressed beliefs, enabling unsupervised evaluation

## Why This Works (Mechanism)

### Mechanism 1
The predictability of belief updates from prior beliefs serves as a statistical signal of irrationality. By regressing belief updates (∆b) against prior beliefs (b_prior) using OLS, a positive coefficient β₁ indicates that prior beliefs systematically predict update direction, violating the Martingale property. This breaks when belief updates are non-linear or when evidence selection endogenously correlates with priors.

### Mechanism 2
Process-level predictability correlates with outcome-level accuracy degradation. Higher Martingale Scores (more predictable updates) correspond to higher Brier Scores (lower accuracy) in domains with ground truth. This validates the score as a proxy for truth-seeking ability, though the relationship may vary across domains.

### Mechanism 3
Adversarial reasoning paradigms mitigate belief entrenchment by introducing counter-arguments that disrupt prior-reinforcing tendencies. Debate forces models to weigh evidence against opposing views, reducing the regression coefficient between prior and update. This breaks when debate degrades into sycophancy or judge failure.

## Foundational Learning

- **Concept**: **Martingale Property (in Bayesian Inference)**
  - **Why needed here**: Theoretical foundation; rational belief updates should be statistically exogenous to priors
  - **Quick check question**: If I believe there is a 70% chance of rain tomorrow, what is the rational expectation for my belief before looking at the weather forecast? (Answer: 70%)

- **Concept**: **LLM-as-Judge / Expressed Belief**
  - **Why needed here**: LLMs require external probability assignment; token probabilities are poorly calibrated
  - **Quick check question**: Why can't we just use the token probability of "Yes" vs "No" as the belief? (Answer: LLMs are poorly calibrated; external judge needed)

- **Concept**: **OLS Regression Coefficients**
  - **Why needed here**: Metric defined as slope β₁; engineers must distinguish slope from R²
  - **Quick check question**: In ∆b = β₁·b_prior + ε, what does positive β₁ signify? (Answer: Higher priors predict positive updates)

## Architecture Onboarding

- **Component map**: Data Source -> Reasoner -> Trajectory Parser -> Judge -> Calculator
- **Critical path**: The Judge Evaluation - validity depends entirely on judge's ability to consistently translate text into probabilities
- **Design tradeoffs**:
  - OLS vs. Logistic Regression: OLS preserves magnitude information and avoids logistic instability
  - Expressed vs. Internal Belief: Measures expressed belief via external judge rather than internal states
- **Failure signatures**:
  - High Score + High Accuracy: May indicate rational confirmation rather than bias
  - Judge Inconsistency: Random probabilities cause fluctuating scores or non-significance
  - Domain Mismatch: Correlation with accuracy may not hold uniformly across domains
- **First 3 experiments**:
  1. Judge Consistency Check: Verify GPT-4o judge agreement > 0.7 with human/other models
  2. Baseline Entrenchment: Run Forecasting with GPT-4o CoT, verify positive, significant Martingale Score
  3. Intervention Validation: Run same with DeepSeek R1/Debate, verify score reduction

## Open Questions the Paper Calls Out

- **Open Question 1**: Does external evidence retrieval alter the Martingale Score by decoupling updates from priors?
  - Basis: Future work suggests adding evidence searching component
  - Why unresolved: Current setup focuses on internal reasoning without external tools
  - Resolution: Compare Martingale Scores between standard LLMs and RAG setups on same tasks

- **Open Question 2**: Can Martingale Score function as training objective to reduce entrenchment and improve accuracy?
  - Basis: Future work suggests converting metric to training objective
  - Why unresolved: Metric used only for post-hoc evaluation, not optimization
  - Resolution: Fine-tune with loss penalizing Martingale violations and observe Brier Score changes

- **Open Question 3**: Is Martingale Score valid proxy for other biases like sycophancy and group conformity?
  - Basis: Future work suggests extending to sycophancy, conformity, inverse scaling
  - Why unresolved: Current focus on belief entrenchment doesn't address other social-epistemic biases
  - Resolution: Correlate Martingale Scores with established sycophancy/conformity metrics

## Limitations

- Reliance on LLM judge introduces potential systematic bias despite high inter-judge correlation (0.72-0.88)
- Assumes linear relationships between prior beliefs and updates, may not capture complex reasoning patterns
- Validity as truth-seeking proxy depends on assumption that entrenchment correlates with accuracy loss across all domains

## Confidence

- **High Confidence**: Mathematical foundation of OLS regression is well-specified and theoretically grounded
- **Medium Confidence**: Validation through Brier Score correlation is suggestive but domain-dependent
- **Low Confidence**: Generalizability to different architectures and reasoning structures remains uncertain

## Next Checks

1. **Judge Consistency Stress Test**: Systematically vary judge model to measure sensitivity; target inter-judge correlation > 0.7
2. **Ground Truth Validation Expansion**: Apply to additional domains with ground truth (coding, math); look for correlation coefficients > 0.2
3. **Non-Linear Relationship Test**: Implement non-parametric version (Spearman/random forest) to compare with OLS results and identify non-linear patterns