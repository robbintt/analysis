---
ver: rpa2
title: Code Driven Planning with Domain-Adaptive Critic
arxiv_id: '2509.19077'
source_url: https://arxiv.org/abs/2509.19077
tags:
- building
- self
- planning
- unit
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Code Driven Planning with Domain-Adaptive Critic (CoPiC) addresses
  the high query costs and short-term focus of existing LLM-based planners by generating
  multiple planning programs via an MoE mechanism and using a domain-adaptive critic
  to select plans aligned with long-term rewards. Instead of step-by-step plan refinement,
  CoPiC produces diverse candidate plans through programs and evaluates them with
  a critic fine-tuned via reinforcement learning.
---

# Code Driven Planning with Domain-Adaptive Critic

## Quick Facts
- arXiv ID: 2509.19077
- Source URL: https://arxiv.org/abs/2509.19077
- Reference count: 40
- CoPiC achieves 23.33% higher success rate, 91.27% lower token costs, and 34.76% fewer steps vs. baselines

## Executive Summary
Code Driven Planning with Domain-Adaptive Critic (CoPiC) introduces a novel approach to large language model-based planning that addresses the high computational costs and short-term focus of existing methods. Instead of iterative plan refinement, CoPiC generates multiple diverse planning programs through a Mixture-of-Experts mechanism and evaluates them using a domain-adaptive critic trained via reinforcement learning. The system demonstrates significant improvements across three benchmark domains, achieving substantially better success rates while reducing both computational costs and planning steps.

## Method Summary
CoPiC operates by first generating diverse candidate plans through an MoE-based program generation system, then evaluating these plans using a domain-adaptive critic that has been fine-tuned through reinforcement learning. This approach differs from traditional step-by-step plan refinement by producing multiple complete plans upfront and selecting the one most aligned with long-term rewards. The critic component learns to evaluate plan quality based on the specific reward structure of each domain, enabling better alignment with long-term objectives. The system is evaluated across ALFWorld, NetHack, and StarCraft II Unit Building environments, showing consistent improvements over state-of-the-art baselines.

## Key Results
- 23.33% higher success rate compared to advanced baselines
- 91.27% lower token costs across evaluated domains
- 34.76% reduction in average steps required to complete tasks
- Consistent performance improvements across three diverse benchmark environments

## Why This Works (Mechanism)
The domain-adaptive critic enables better alignment with long-term rewards by learning domain-specific evaluation criteria through reinforcement learning fine-tuning. The MoE mechanism generates diverse candidate plans that explore different strategy spaces, avoiding local optima that single-plan approaches might get stuck in. By evaluating complete plans rather than individual steps, CoPiC can better capture the cumulative effects of actions and make more globally optimal decisions.

## Foundational Learning

**Reinforcement Learning Fine-tuning**: Domain-adaptive critic training through RL enables learning of domain-specific reward evaluation. Why needed: Standard LLM planners lack domain-specific reward understanding. Quick check: Verify critic performance on held-out reward functions.

**Mixture-of-Experts Generation**: MoE mechanism produces diverse planning programs from different expert models. Why needed: Single-plan generation suffers from myopia and local optima. Quick check: Measure diversity entropy across generated plans.

**Program-Based Planning**: Complete plans generated as programs rather than sequential instructions. Why needed: Enables holistic evaluation and better long-term optimization. Quick check: Compare plan coherence metrics vs. step-by-step approaches.

## Architecture Onboarding

**Component Map**: LLM -> MoE Generator -> Candidate Plans -> Domain-Adaptive Critic -> Selected Plan -> Environment

**Critical Path**: The bottleneck is the critic evaluation stage, which must assess multiple candidate plans. This requires balancing evaluation depth with computational efficiency, particularly as the number of candidates increases.

**Design Tradeoffs**: CoPiC trades increased upfront computation (generating multiple plans) for reduced total planning steps and better long-term outcomes. The MoE mechanism adds diversity but also computational overhead, while the critic adds evaluation accuracy but requires domain-specific training.

**Failure Signatures**: Poor critic performance manifests as selecting suboptimal plans despite having good candidates available. MoE failure appears as low plan diversity or repetitive strategy generation. LLM generation failures result in incoherent or invalid plans.

**First Experiments**: 1) Ablation study removing MoE to measure diversity impact, 2) Domain transfer testing critic generalization across environments, 3) Computational overhead analysis including all MoE and critic costs.

## Open Questions the Paper Calls Out

None

## Limitations

The evaluation is limited to three specific domains (ALFWorld, NetHack, and StarCraft II Unit Building), which may not generalize to other complex environments or real-world applications. The claimed "91.27% lower token costs" requires verification of whether this accounts for all computational costs including MoE generation overhead. The domain-adaptive critic's effectiveness depends on the quality of reward function design, which is not fully detailed in the methodology.

## Confidence

High confidence: The reported improvements in success rate (23.33%) and step reduction (34.76%) are based on standard RL benchmarks with established evaluation protocols.

Medium confidence: The token cost reduction claim requires more detailed accounting of computational overhead, particularly the MoE mechanism's contribution to overall costs.

Low confidence: The generalizability claims to broader domains are not sufficiently supported by the current experimental scope.

## Next Checks

1. Evaluate CoPiC on additional diverse domains beyond the three tested, including environments with different reward structures and longer planning horizons to assess generalizability.

2. Conduct ablation studies isolating the contributions of the MoE mechanism versus the domain-adaptive critic to quantify their individual impacts on performance improvements.

3. Perform cost analysis including all computational overhead (MoE generation, critic evaluation, and LLM inference) to verify the claimed 91.27% token cost reduction in real-world deployment scenarios.