---
ver: rpa2
title: 'Explanation Multiplicity in SHAP: Characterization and Assessment'
arxiv_id: '2601.12654'
source_url: https://arxiv.org/abs/2601.12654
tags:
- explanation
- multiplicity
- explanations
- shap
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses explanation multiplicity, where repeated SHAP
  explanations for the same model and input yield substantively different feature
  attributions. The authors develop a methodology to disentangle variability arising
  from model training versus explainer stochasticity.
---

# Explanation Multiplicity in SHAP: Characterization and Assessment

## Quick Facts
- arXiv ID: 2601.12654
- Source URL: https://arxiv.org/abs/2601.12654
- Reference count: 40
- Key outcome: Explanation multiplicity is widespread in SHAP attributions, with variability stemming from both model training and explainer stochasticity, requiring aligned evaluation practices.

## Executive Summary
This paper investigates explanation multiplicity in SHAP, where repeated explanations for the same model and input yield substantively different feature attributions. The authors develop a methodology to disentangle variability arising from model training versus explainer stochasticity. They show that explanation stability depends critically on evaluation metrics: ℓ2 distances can suggest stability while top-k Jaccard and RBO metrics reveal substantial rank disagreement. Across multiple datasets and model classes, they find explanation multiplicity is widespread and persists even for high-confidence predictions.

## Method Summary
The authors develop a framework to analyze SHAP explanation variability by conducting repeated explanations on the same model-input pairs, comparing variability sources through model retraining and explainer randomness control. They employ multiple evaluation metrics including ℓ2 distances, top-k Jaccard similarity, and Rank-Biased Overlap (RBO) to capture different aspects of explanation stability. To contextualize observed disagreement, they derive randomized baselines using Dirichlet and Mallows models, providing principled reference points for interpreting multiplicity. The analysis spans multiple datasets, model classes (neural networks, tree-based models, TabPFN), and scales to assess when model-induced versus explainer-induced variability dominates.

## Key Results
- Explanation multiplicity is widespread across datasets and model classes, with neural models showing higher variability than tree-based models or TabPFN
- ℓ2 distances can suggest stability while top-k Jaccard and RBO metrics reveal substantial rank disagreement in feature importance
- Multiplicity persists even for high-confidence predictions and varies by dataset size: model-induced in small-data regimes, explainer-induced in large-scale settings

## Why This Works (Mechanism)
The analysis works by systematically isolating and measuring two sources of variability: stochasticity in the explainer algorithm itself and variability from training different model instances. By controlling for these sources separately and applying multiple complementary metrics, the framework captures both magnitude and rank-based aspects of explanation disagreement. The randomized baselines provide critical context for interpreting observed variability as either expected noise or problematic instability.

## Foundational Learning

**SHAP (SHapley Additive exPlanations)**: Game-theoretic approach to feature attribution that computes marginal contributions of features to model predictions. Needed to understand the baseline explanation method being analyzed. Quick check: Can you explain why SHAP values sum to the difference between model output and baseline?

**Rank-Biased Overlap (RBO)**: Metric for comparing ranked lists that accounts for top-weighted importance and allows for incomplete list comparisons. Needed because standard overlap metrics don't capture rank position importance. Quick check: Can you describe how RBO differs from standard Jaccard similarity for ranked lists?

**Dirichlet distribution**: Continuous multivariate distribution over the simplex, used here to generate randomized baselines for explanation attribution. Needed to establish principled null distributions for comparison. Quick check: Can you explain why Dirichlet is appropriate for generating randomized feature importance distributions?

**Mallows model**: Probability distribution over permutations that captures the tendency of items to appear in similar ranks. Needed to generate realistic randomized baselines that preserve some rank structure. Quick check: Can you describe how Mallows models differ from uniform random permutations?

## Architecture Onboarding

**Component map**: Model training pipeline -> SHAP explainer -> Multiple explanation runs -> Metric computation (ℓ2, Jaccard, RBO) -> Baseline generation (Dirichlet, Mallows) -> Comparison analysis

**Critical path**: Model training → Multiple SHAP explanations → Metric computation → Baseline comparison → Source attribution (model vs. explainer)

**Design tradeoffs**: SHAP provides theoretically grounded attributions but introduces stochasticity through sampling; multiple metrics capture different stability aspects but may yield conflicting conclusions; randomized baselines provide context but require careful interpretation.

**Failure signatures**: Inconsistent metric results across evaluation methods; high variability that exceeds randomized baselines; instability that correlates with model confidence; dataset-size dependent source attribution.

**First experiments**:
1. Run SHAP explanations multiple times on fixed model-input pairs and compute ℓ2, Jaccard, and RBO distances
2. Train multiple models on same dataset and compare explanation variability patterns
3. Generate Dirichlet and Mallows baselines for each dataset and compare observed variability

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on SHAP excludes comparison with alternative explanation methods like LIME or integrated gradients
- Dataset selection favors tabular and standard ML benchmarks; results may differ for highly structured or multimodal data
- Analysis assumes fixed model architectures per dataset, but hyperparameter sensitivity could amplify or dampen observed variability

## Confidence
High: The characterization of multiplicity sources (model vs. explainer-induced) and the derivation of randomized baselines are methodologically sound and well-supported.

Medium: The ranking instability findings depend on metric choice, suggesting evaluation practice sensitivity rather than universal instability.

Low: The claim that multiplicity is a "normative concern" extends beyond empirical findings into normative territory without sufficient philosophical or stakeholder engagement.

## Next Checks
1. Replicate the analysis using LIME and integrated gradients to determine if multiplicity is SHAP-specific or method-agnostic.
2. Conduct ablation studies varying model hyperparameters to quantify their contribution to explanation variability.
3. Extend the evaluation to multimodal datasets (text, image, graph) to test domain-specific patterns in multiplicity.