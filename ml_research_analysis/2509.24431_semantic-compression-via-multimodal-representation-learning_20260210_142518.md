---
ver: rpa2
title: Semantic Compression via Multimodal Representation Learning
arxiv_id: '2509.24431'
source_url: https://arxiv.org/abs/2509.24431
tags:
- compression
- semantic
- multimodal
- learning
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of semantic compression in multimodal\
  \ representation learning, where high-dimensional embeddings from different modalities\
  \ need to be compressed while preserving their semantic content. The key insight\
  \ is that reducing the modality gap\u2014the residual separation of embeddings from\
  \ different modalities\u2014enables effective semantic compression."
---

# Semantic Compression via Multimodal Representation Learning

## Quick Facts
- arXiv ID: 2509.24431
- Source URL: https://arxiv.org/abs/2509.24431
- Reference count: 0
- Primary result: Achieves comparable or better performance than concatenating multimodal embeddings while using only 50% of the information.

## Executive Summary
This paper introduces a method for semantic compression of multimodal embeddings by reducing the modality gap through high-temperature contrastive learning. The approach extracts centroid embeddings that represent semantic concepts across modalities, enabling aggressive post-training compression while maintaining performance. Evaluated on MSCOCO and Flickr30k datasets, the method shows that centroid-based compression achieves comparable or better classification results than full multimodal concatenation with only 50% information retention.

## Method Summary
The method operates in three stages: (1) pretrain multimodal encoders using InfoNCE loss with high temperature (τ ∈ {0.01, 0.07, 0.1, 0.2, 0.4}) to reduce modality gap and align embeddings across modalities; (2) extract centroid embeddings by averaging instances across modalities for each semantic concept; (3) apply post-training compression via random feature selection (RFS) to the centroids. This approach eliminates redundancy by storing only one embedding per semantic concept instead of M embeddings across modalities, achieving 50% storage reduction while maintaining comparable classification performance.

## Key Results
- Centroid-based compression achieves comparable or better performance than concatenated multimodal embeddings while using only 50% of the information.
- Higher temperatures (e.g., τ=0.4) make the approach more robust to compression, preserving strong performance even with 95% random feature drop.
- The centroid embeddings encapsulate more information and can be further compressed effectively, leading to lower storage requirements without sacrificing performance.

## Why This Works (Mechanism)

### Mechanism 1: Temperature-Controlled Modality Gap Reduction
- Claim: Increasing temperature parameter τ in InfoNCE loss reduces the modality gap, enabling embeddings from different modalities to share a common latent region.
- Mechanism: Higher τ softens the softmax distribution in contrastive loss, distributing gradients more evenly across negatives. This encourages embeddings to form compact semantic clusters rather than maintaining modality-specific regions.
- Core assumption: The relationship between temperature and cluster structure generalizes from unimodal to multimodal spaces.
- Evidence anchors: [abstract] "When the gap is sufficiently reduced through high-temperature training in contrastive learning, embeddings from different modalities but expressing the same semantics share a common portion of the latent space."

### Mechanism 2: Centroid-Based Semantic Representation
- Claim: When modality gap is minimized, averaging embeddings across modalities for a semantic concept produces a centroid that faithfully represents that concept.
- Mechanism: With reduced gap, embeddings from different modalities converge to the same latent region for shared semantics. The centroid captures shared semantic content while eliminating modality-specific redundancy.
- Core assumption: The centroid remains representative even when individual modality embeddings have noise or minor misalignment.
- Evidence anchors: [abstract] "their centroid is a faithful representation of the semantic concept."

### Mechanism 3: Post-Training Compression via Random Feature Selection
- Claim: Centroids from high-temperature training exhibit low intrinsic dimensionality, enabling aggressive compression through simple random feature selection without significant performance loss.
- Mechanism: High-temperature training produces embeddings with concentrated variance in fewer dimensions. Random feature selection preserves performance when intrinsic dimensionality is much lower than total dimensions.
- Core assumption: The intrinsic dimensionality reduction from temperature control applies similarly to multimodal centroids.
- Evidence anchors: [Section 3.2] "higher temperatures are more robust to compression, preserving good performance even in the case of 95% random feature drop."

## Foundational Learning

- **InfoNCE Loss and Contrastive Learning**
  - Why needed here: Understanding how the temperature parameter modulates gradient distribution and cluster formation is essential for controlling the modality gap.
  - Quick check question: Can you explain why lowering temperature sharpens the softmax and emphasizes hard negatives?

- **Modality Gap**
  - Why needed here: This is the central phenomenon the method exploits; recognizing when embeddings cluster by modality vs. semantics is critical for debugging.
  - Quick check question: Given two sets of embeddings from image and text encoders, how would you measure their modality gap?

- **Intrinsic Dimensionality**
  - Why needed here: Determines whether post-training compression (RFS) will succeed; low intrinsic dimensionality enables aggressive compression.
  - Quick check question: If an embedding has 1024 dimensions but only ~50 principal components explain 95% of variance, what does this imply for compression?

## Architecture Onboarding

- Component map: Pretrained multimodal encoders (e.g., CLIP-style) -> High-temperature InfoNCE training loop -> Centroid extraction module -> Post-training compressor (RFS)

- Critical path: 1. Train encoders with sufficiently high temperature → 2. Verify gap is reduced → 3. Extract centroids per semantic concept → 4. Apply RFS at target compression ratio → 5. Evaluate on downstream classification

- Design tradeoffs:
  - Higher τ → better compression robustness but may reduce absolute performance at full dimensionality
  - Storing centroids only → 50% storage reduction for 2 modalities, but loses modality-specific information
  - RFS simplicity vs. learned compression → RFS requires no training but may underperform PCA/hashing on some distributions

- Failure signatures:
  - Centroids underperform concatenated embeddings by >5% → gap likely insufficient, increase τ or verify alignment
  - Performance collapses at moderate compression (e.g., 50% RFS) → intrinsic dimensionality too high; retrain at higher τ or use stronger encoder
  - Large variance across runs → RFS randomness too aggressive; consider fixed seed or deterministic selection

- First 3 experiments:
  1. **Gap validation**: Train encoders at τ ∈ {0.07, 0.2, 0.4} on a small dataset; plot Gap_{m,n} vs. τ to confirm gap reduction trend.
  2. **Centroid vs. concatenation**: On MSCOCO validation split, compare centroid-based classification vs. concatenated embeddings at