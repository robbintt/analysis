---
ver: rpa2
title: 'SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged
  transcription'
arxiv_id: '2508.05554'
source_url: https://arxiv.org/abs/2508.05554
tags:
- speaker
- spgispeech
- snippets
- dataset
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPGISpeech 2.0 is a 3,780-hour speaker-tagged transcription dataset
  derived from earnings calls, designed to improve speaker-related ASR tasks. It provides
  41,593 unique speakers with fully-formatted transcriptions and speaker alignment
  information for multi-talker ASR.
---

# SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription

## Quick Facts
- arXiv ID: 2508.05554
- Source URL: https://arxiv.org/abs/2508.05554
- Authors: Raymond Grossman; Taejin Park; Kunal Dhawan; Andrew Titus; Sophia Zhi; Yulia Shchadilova; Weiqing Wang; Jagadeesh Balam; Boris Ginsburg
- Reference count: 0
- Primary result: 3,780-hour speaker-tagged transcription dataset with 41,593 unique speakers, reducing cpWER from 20.53% to 15.88% with speaker supervision

## Executive Summary
SPGISpeech 2.0 is a 3,780-hour speaker-tagged transcription dataset derived from earnings calls, designed to improve speaker-related ASR tasks. It provides 41,593 unique speakers with fully-formatted transcriptions and speaker alignment information for multi-talker ASR. The dataset includes 50–90 second audio snippets with speaker change markers and word-level timestamps, enabling training of models for speaker diarization and recognition. Experiments with a Canary-170M ASR model and a Sortformer-based model fine-tuned on SPGISpeech 2.0 show reduced concatenated minimum permutation WER (cpWER) from 20.53% to 15.88% and WER from 7.96% to 7.25% when using speaker supervision, outperforming baseline models. SPGISpeech 2.0 is released free for non-commercial use, aiming to advance speech recognition and speaker diarization research.

## Method Summary
SPGISpeech 2.0 is created by extracting 50–90 second snippets from earnings call audio, ensuring each snippet contains at least two speakers and one speaker change. Speaker alignment is generated using Gentle alignment with py-webrtc VAD, then cross-referenced with NeMo forced alignment pipeline to ensure accuracy. The dataset provides word-level timestamps and speaker IDs in SegLST format, with transcripts formatted according to the S&P Global style guide. A Canary-170M ASR model (FastConformer encoder + Transformer decoder) is fine-tuned with speaker supervision using a Sortformer-123M diarization module that adds a parallel encoder and sort loss for speaker permutation resolution.

## Key Results
- cpWER reduced from 20.53% to 15.88% when using speaker supervision during fine-tuning
- WER reduced from 7.96% to 7.25% with speaker supervision enabled
- Dataset contains 41,593 unique speakers across 3,780 hours of audio
- 154,971 training snippets, 6,447 development snippets, and 7,177 test snippets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speaker supervision during fine-tuning improves both transcription accuracy and speaker attribution.
- Mechanism: The Sortformer architecture adds a parallel encoder for speaker diarization to the Canary ASR decoder, enabling joint optimization. The "sort loss" component resolves speaker permutations from permutation-invariant loss during training, allowing the model to learn speaker boundaries and transcription simultaneously.
- Core assumption: Speaker identity information provides a supervisory signal that helps disambiguate overlapping or adjacent speech segments.
- Evidence anchors:
  - [abstract] "Experiments with a Canary-170M ASR model and a Sortformer-based model fine-tuned on SPGISpeech 2.0 show reduced concatenated minimum permutation WER (cpWER) from 20.53% to 15.88% and WER from 7.96% to 7.25% when using speaker supervision"
  - [section 5.1] "Sortformer adds a parallel encoder to the Canary decoder for speaker diarization, and introduces sort loss to deal with the resolution of speaker permutations from permutation-invariant loss during training"
  - [corpus] Weak direct evidence; neighbor papers discuss multi-speaker ASR challenges but do not validate this specific joint training mechanism.
- Break condition: If speaker embeddings are inconsistent across calls or if unknown speakers (-1 labels) dominate training, the supervision signal degrades.

### Mechanism 2
- Claim: Longer audio snippets with multiple speaker turns enable training of more robust diarization systems.
- Mechanism: 50-90 second snippets contain up to 25 unique speaker segments and 2-7 speakers per snippet. This provides sufficient temporal context for models to learn speaker change detection, as opposed to short snippets where speaker boundaries may be absent or too sparse.
- Core assumption: Diarization models require exposure to realistic speaker turnover patterns found in actual multi-party conversations.
- Evidence anchors:
  - [section 1.2] "snippets of 50 to 90 seconds... containing up to seven speakers and 25 unique speaker segments per snippet. Every snippet in the dataset contains at least two speakers and one speaker change"
  - [section 3] "SPGISpeech 2.0 contains significantly longer audio snippets of 50-90 seconds in length to support the development of diarization systems for longer calls"
  - [corpus] Neighbor paper "Survey of End-to-End Multi-Speaker Automatic Speech Recognition" confirms that monaural multi-speaker ASR remains challenging due to overlapping speech, supporting the need for appropriate training data.
- Break condition: If snippet length exceeds model's effective attention window or if speaker turns are too densely overlapping, the model may fail to learn clean boundaries.

### Mechanism 3
- Claim: Cross-referenced forced alignment improves word-level timestamp accuracy for speaker segment boundaries.
- Mechanism: Word-level timestamps are generated using Gentle alignment with py-webrtc VAD, then cross-referenced with NeMo forced alignment pipeline. This redundancy reduces alignment errors at segment boundaries, which is critical for accurate speaker-tagged transcription training.
- Core assumption: Neither alignment pipeline alone is sufficient; cross-referencing catches systematic errors.
- Evidence anchors:
  - [section 3] "these alignments were cross-referenced with the NeMo forced alignment pipeline to ensure accurate word-level alignments, particularly at the start and end of calls"
  - [section 3.1] "Snippets were required to be well-aligned for a minimum of five words at the start and end of each snippet under both the Gentle alignment framework and the NeMo forced alignment framework"
  - [corpus] No direct validation in neighbor papers; this is a dataset construction choice without external corroboration.
- Break condition: If audio quality is poor or overlapping speech is prevalent, forced alignment accuracy degrades regardless of cross-referencing.

## Foundational Learning

- Concept: Speaker diarization vs. speaker recognition
  - Why needed here: The paper distinguishes diarization (partitioning audio into generic speakers like "speaker-1") from recognition (identifying specific known speakers). SPGISpeech 2.0 supports both via anonymous speaker IDs that persist across calls.
  - Quick check question: Given an audio clip with three speakers you've never heard before, which task is applicable?

- Concept: cpWER (Concatenated Minimum Permutation Word Error Rate)
  - Why needed here: This metric captures both transcription accuracy AND speaker attribution accuracy. Standard WER ignores which speaker said what; cpWER penalizes speaker misattribution by requiring optimal permutation matching before computing WER.
  - Quick check question: If a model transcribes perfectly but attributes all words to the wrong speaker, what happens to WER vs. cpWER?

- Concept: Permutation-invariant loss (PIT)
  - Why needed here: Multi-speaker models must handle the ambiguity of which output corresponds to which speaker. PIT computes loss over all speaker permutations and takes the minimum, but this creates a "sort" problem at inference time that Sortformer addresses explicitly.
  - Quick check question: Why can't you simply use cross-entropy loss directly on multi-speaker outputs without handling permutations?

## Architecture Onboarding

- Component map:
  - Input: 16kHz mono audio, 50-90s snippets
  - Canary-170M: FastConformer encoder + Transformer decoder (ASR backbone)
  - Sortformer-123M: Canary base + parallel speaker encoder + sort loss module (joint ASR + diarization)
  - Output: Transcript with speaker change markers ("|") and per-word speaker IDs
  - Data format: SegLST format for speaker alignments, JSON for word-level timestamps

- Critical path:
  1. Pre-train Canary ASR model on general speech data
  2. Fine-tune on SPGISpeech 2.0 with speaker supervision enabled
  3. Sortformer adds speaker encoder branch during fine-tuning
  4. Evaluate using cpWER (requires speaker-attributed references)

- Design tradeoffs:
  - **Snippet length vs. data pool**: Longer snippets (50-90s) reduce usable data pool since multi-speaker segments are rarer in earnings calls (which favor long monologues)
  - **Verbatim vs. styled transcription**: S&P Global style guide removes disfluencies; paper provides algorithmically "corrected" version but this may introduce errors
  - **Known vs. unknown speakers**: Dataset includes "-1" labels for unknown speakers; these provide less supervision signal

- Failure signatures:
  - High cpWER but low WER → speaker attribution failing while transcription works
  - Poor performance on snippets with >4 speakers → model may have implicit speaker count limits
  - Degraded performance on With-PnC vs. Without-PnC → punctuation/capitalization module not generalizing

- First 3 experiments:
  1. Reproduce baseline: Run pre-trained Canary-170M on SPGISpeech 2.0 test set without fine-tuning; expect ~10.81% WER (Without-PnC) per Table 3.
  2. Fine-tune without speaker supervision: Fine-tune Canary-170M on SPGISpeech 2.0 training set using only transcript loss; expect WER improvement but cpWER to remain high (~20%).
  3. Add speaker supervision: Enable Sortformer speaker encoder and sort loss during fine-tuning; expect cpWER to drop to ~15.88% and WER to ~7.25% (With-PnC).

## Open Questions the Paper Calls Out
- What is the impact of training speaker diarization models on algorithmically generated word-level timestamps versus human-annotated ground truth?
- Does the algorithmic restoration of disfluencies and non-verbatim text enable models to learn robust recognition of spontaneous speech features?
- How well do speaker-tagged ASR models trained on this corpus generalize to domains with different acoustic conditions or overlapping speech patterns?

## Limitations
- Dataset construction relies on cross-referenced forced alignment pipelines without external validation of alignment accuracy
- Domain-specific nature of earnings calls (dominated by monologues with limited overlap) may limit generalization to other multi-speaker domains
- Paper lacks ablation studies to isolate contributions of Sortformer architecture versus speaker supervision

## Confidence

- **High Confidence**: The dataset construction methodology (SPGISpeech 2.0 creation with speaker tags) is well-specified with clear file formats and preprocessing steps. The performance improvements (cpWER from 20.53% to 15.88%, WER from 7.96% to 7.25%) are directly reported from controlled experiments comparing models with and without speaker supervision.

- **Medium Confidence**: The mechanism by which speaker supervision improves both transcription and diarization is plausible based on the joint optimization framework, but lacks detailed ablation studies. The Sortformer architecture's specific contributions versus baseline fine-tuning are not fully disentangled.

- **Low Confidence**: Claims about the superiority of cross-referenced alignment pipelines are not externally validated, and the impact of verbatim vs. algorithmically-corrected transcriptions on downstream model performance remains unclear.

## Next Checks

1. **Alignment Quality Validation**: Measure Gentle vs. NeMo forced alignment accuracy on a subset of SPGISpeech 2.0 data by comparing timestamps against human-verified word boundaries, particularly at speaker change points.

2. **Ablation Study**: Train and evaluate models with (a) baseline ASR only, (b) ASR + speaker encoder without sort loss, and (c) full Sortformer, to isolate architectural contributions to cpWER improvements.

3. **Domain Generalization Test**: Evaluate the fine-tuned model on a non-financial multi-speaker dataset (e.g., AMI meeting corpus) to assess whether improvements transfer beyond earnings calls to domains with more overlapping speech and rapid turn-taking.