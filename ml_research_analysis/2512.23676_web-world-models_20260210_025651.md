---
ver: rpa2
title: Web World Models
arxiv_id: '2512.23676'
source_url: https://arxiv.org/abs/2512.23676
tags:
- world
- figure
- page
- state
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Web World Model (WWW), a novel framework
  that bridges fixed-context web frameworks and fully generative world models by separating
  deterministic code (physics) from LLM-driven content (imagination). A suite of WWMs
  is implemented, including an infinite travel atlas, fictional galaxy explorers,
  web-scale encyclopedic worlds, and simulation-like environments, demonstrating that
  web stacks can host controllable yet open-ended environments.
---

# Web World Models

## Quick Facts
- arXiv ID: 2512.23676
- Source URL: https://arxiv.org/abs/2512.23676
- Reference count: 19
- Primary result: Novel framework separating deterministic physics (code) from LLM-driven imagination to create infinite, consistent web-based worlds

## Executive Summary
This paper introduces Web World Models (WWMs), a novel framework that bridges fixed-context web frameworks and fully generative world models by separating deterministic code (physics) from LLM-driven content (imagination). A suite of WWMs is implemented, including infinite travel atlases, fictional galaxy explorers, web-scale encyclopedic worlds, and simulation-like environments, demonstrating that web stacks can host controllable yet open-ended environments. Key design principles include separating concerns between physics and imagination, using typed interfaces as a common language, achieving infinite worlds via deterministic hashing, and enabling graceful degradation.

## Method Summary
The WWM framework uses TypeScript/React 19 with Google GenAI SDK (Gemini Flash) to implement a two-layer architecture: physics layer (Sφ) handles deterministic state transitions and rule enforcement via code, while imagination layer (Sψ) generates rich content through LLM calls constrained by typed JSON schemas. Procedural consistency is achieved through deterministic hashing of coordinates to fixed seeds, enabling infinite worlds without database storage. The system implements graceful degradation with file-backed caches and template fallbacks when LLM APIs are unavailable. The framework is demonstrated across five distinct applications including infinite travel atlases, galaxy exploration, and encyclopedic content generation.

## Key Results
- WWMs maintain thematic consistency and structural integrity across diverse, procedurally generated content
- Object permanence achieved via deterministic hashing enables infinite worlds with zero storage cost
- Typed interfaces effectively constrain LLM outputs to structurally valid states while preserving creative freedom
- Graceful degradation system enables continued operation when LLM APIs are unavailable

## Why This Works (Mechanism)

### Mechanism 1: Physics-Imagination Decomposition
Decomposing world state into deterministic code (physics) and LLM-driven content (imagination) maintains logical consistency while enabling open-ended generation. State transitions occur in strict order—code computes Sφ_{t+1} = f_code(Sφ_t, a_t), then LLM synthesizes Sψ_{t+1} ~ π_θ(·|Sφ_{t+1}). Physics handles invariants (inventories, coordinates, rules); imagination handles descriptions, dialogue, aesthetics. Core assumption: State can be cleanly factored into logical invariants and creative content.

### Mechanism 2: Typed Interfaces as Syntactic Filters
Explicit JSON/TypeScript schemas constrain LLM outputs to structurally valid states, eliminating structural hallucinations while preserving creative freedom within bounds. LLM predicts valid JSON objects conforming to type definitions (e.g., interface Planet{biome: string; hazard: string;}). Validation layer enforces contract before physics execution. Core assumption: Schemas can anticipate all fields necessary for physics execution at design time.

### Mechanism 3: Deterministic Hashing for Object Permanence
Hashing coordinates to fixed seeds forces consistent LLM output across visits, enabling infinite worlds with zero storage cost. Coordinate x → hash function → seed h(x) → frozen LLM sampling → Sψ_t ≡ Sψ_{t+k} if location(t) = location(t+k). Object permanence without database lookups. Core assumption: LLM sampling is sufficiently controlled by seed to produce consistent outputs across invocations.

## Foundational Learning

- **World Models (predictive environment dynamics)**
  - Why needed here: WWMs extend the idea that agents need persistent, predictable environments; understanding Ha & Schmidhuber (2018) helps grasp why code-based physics differs from learned RNN dynamics
  - Quick check question: Why does WWM use deterministic code for physics instead of a learned predictive model?

- **Procedural Generation via Hash Functions**
  - Why needed here: Infinite worlds via deterministic hashing is core to WWM scalability; seeds control randomness to ensure consistency
  - Quick check question: Given coordinate (42, 17), explain how a hash function ensures the same planet description across sessions

- **Schema-Constrained LLM Generation**
  - Why needed here: Typed interfaces are the contract layer between LLM and physics; debugging requires understanding JSON schema validation
  - Quick check question: If LLM outputs {"biome": "desert", "hazards": []} but schema expects hazard: string, where does failure occur and why?

## Architecture Onboarding

- Component map:
  Physics Layer (Sφ) -> Typed Interface Layer -> Validation Layer -> Imagination Layer (Sψ) -> LLM API -> Hash/Seed Generator -> Graceful Degradation Logic

- Critical path:
  1. User action → Physics Layer computes deterministic state transition (Sφ_{t+1})
  2. Updated Sφ passed to Imagination Layer with schema constraints
  3. LLM generates Sψ conforming to typed interface
  4. Validation layer checks output; if invalid, trigger fallback
  5. If LLM unavailable, degrade to cached or template content

- Design tradeoffs:
  - Expressiveness vs. Controllability: Tighter schemas reduce hallucinations but limit creative range
  - Latency vs. Fidelity: Real-time LLM calls enable richness; require fallbacks for slow/unavailable APIs
  - Storage vs. Computation: Hash-based generation avoids DB storage but requires recomputation on visits

- Failure signatures:
  - Schema validation errors (missing fields, wrong types)
  - Consistency breaks (same coordinate yields different content across visits)
  - Degradation loops (stuck in base fidelity due to repeated API failures)
  - State desync (Sφ diverges from what Sψ assumes)

- First 3 experiments:
  1. Build a minimal WWM loop with single typed interface (interface Room{description: string; exits: string[]}); verify physics computes valid state before LLM call
  2. Test hash consistency: generate content for coordinate (x, y), clear cache, regenerate—confirm identical output
  3. Stress test graceful degradation: disable LLM API mid-session, verify fallback to templates; re-enable and confirm recovery

## Open Questions the Paper Calls Out

### Open Question 1
How can WWMs maintain global semantic consistency (e.g., narrative lore, historical timelines) across disparate, procedurally generated locations that do not share a central database? Basis in paper: [inferred] Section 2.3 guarantees object permanence for specific coordinates (local consistency), but lacks mechanism to ensure content generated at coordinate x does not logically contradict content generated at coordinate y (global consistency). Why unresolved: While typed interfaces (Section 2.2) prevent structural hallucinations, they do not inherently constrain LLM from generating logically incompatible facts about shared world state in different regions. What evidence would resolve it: Evaluation framework that queries non-adjacent nodes to measure frequency of cross-location narrative contradictions.

### Open Question 2
To what extent does the infinite, procedural nature of WWMs hinder or facilitate the convergence of agent learning algorithms compared to fixed-context environments? Basis in paper: [inferred] Abstract states agents need to "act, remember, and learn," yet paper focuses entirely on environment architecture without demonstrating or evaluating long-term agent learning within generated worlds. Why unresolved: Standard agent benchmarks rely on repeatable, fixed environments; "Just-In-Time" generation creates non-stationary or unbounded state space that may disrupt standard policy optimization techniques. What evidence would resolve it: Comparative study of agent learning curves against static baseline to see if agents can successfully generalize or memorize strategies in procedurally expanding world.

### Open Question 3
How can the stateless, serverless architecture of WWMs be extended to support real-time, multi-user synchronization without compromising core design principle of avoiding heavy storage? Basis in paper: [explicit] Section 3.5 ("Cosmic Voyager") explicitly lists "shared multiuser tours" as potential extension, acknowledging current architecture designed primarily for single-user exploration. Why unresolved: Framework relies on deterministic hashing to avoid databases (Section 2.3), but multi-user environment requires shared, mutable state (e.g., position of other players) necessitating synchronization layer seemingly at odds with stateless generation model. What evidence would resolve it: Technical implementation of multi-user WWM demo detailing latency and storage overhead required to maintain shared state consistency.

## Limitations
- Deterministic hashing mechanism lacks empirical validation for consistency across different model versions and temperature settings
- Typed interface approach effectiveness depends heavily on schema quality and domain complexity with limited empirical validation
- Graceful degradation system's effectiveness in maintaining user experience during prolonged LLM unavailability remains theoretical
- Multi-user synchronization extension would compromise stateless architecture principles

## Confidence

**High Confidence**: Physics-imagination decomposition architecture is well-specified and theoretically sound. Separation of concerns between deterministic state transitions and creative generation follows established software engineering principles and is directly implementable.

**Medium Confidence**: Typed interface mechanism for constraining LLM outputs appears practical based on schema definitions shown, but real-world effectiveness depends heavily on schema quality and domain complexity. Paper provides implementation examples but limited empirical validation of hallucination reduction.

**Low Confidence**: Deterministic hashing for object permanence, while elegant in concept, lacks empirical evidence for consistency across model updates and different API configurations. Graceful degradation system's real-world performance metrics are not provided.

## Next Checks

1. **Hash Consistency Test**: Generate content for 100 random coordinates, clear all caches, regenerate content with identical model configurations (same temperature, same model version), and measure content overlap. Document any inconsistencies and identify root causes (hash collisions, model variance, etc.).

2. **Schema Robustness Evaluation**: Create a domain with increasingly complex state requirements (starting from simple Room interface to multi-agent social dynamics). Measure hallucination rates and schema violation frequencies as complexity increases. Document breaking points where schemas become insufficient.

3. **Graceful Degradation User Study**: Simulate LLM API failures of varying durations (5 minutes, 1 hour, 24 hours) while users interact with WWM environments. Measure task completion rates, user satisfaction scores, and recovery smoothness when API returns. Compare against baseline systems with no fallback mechanisms.