---
ver: rpa2
title: 'OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory,
  and Textual Inputs'
arxiv_id: '2506.20960'
source_url: https://arxiv.org/abs/2506.20960
tags:
- arxiv
- video
- understanding
- omnieval
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniEval is a benchmark for evaluating omni-modal models that process
  visual, auditory, and textual inputs. It addresses the gap in existing benchmarks
  that fail to capture deep multimodal coupling and synergy.
---

# OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs

## Quick Facts
- **arXiv ID**: 2506.20960
- **Source URL**: https://arxiv.org/abs/2506.20960
- **Reference count**: 40
- **Primary result**: State-of-the-art omni-modal models show significant challenges in real-world understanding, with the best model (Gemini 2.5) achieving only 64.49% overall accuracy on 2617 QA pairs across 12 task types.

## Executive Summary
OmniEval is a comprehensive benchmark designed to evaluate omni-modal models that process visual, auditory, and textual inputs. It addresses limitations in existing benchmarks by focusing on deep multimodal coupling and synergy through tasks requiring tight integration of audio and video inputs. The benchmark includes 810 synchronized videos in Chinese and English, with 2617 question-answer pairs across 3 major task types and 12 sub-task types, including a novel fine-grained video localization task called Grounding. Experiments demonstrate that current state-of-the-art models struggle with real-world multimodal understanding, particularly when information is distributed across modalities with high temporal correlation.

## Method Summary
OmniEval employs a multi-stage methodology: videos are collected from YouTube, Youku, and Bilibili, then processed through visual and audio encoders (1 fps or 64 frames) with automatic speech recognition and caption generation via Qwen2.5-VL-70B. Open-ended questions are generated from captions and subtitles using LLMs, with multiple-choice questions derived from open-ended ones. The dataset undergoes manual curation for quality and relevance. Evaluation uses exact match for multiple-choice, adaptive thresholds and IoU for grounding tasks, and LLM-as-judge for open-ended questions. Models are evaluated with specific frame sampling rates and maximum token limits (1024 default).

## Key Results
- State-of-the-art omni-modal models struggle with real-world multimodal understanding, achieving only 64.49% (Gemini 2.5) to 56.54% (Qwen2.5-Omni-7B) overall accuracy
- Models consistently perform better with textual captions than with raw video frames, indicating underdeveloped direct video understanding capabilities
- The grounding task reveals significant temporal reasoning gaps, with performance varying substantially across models and task types
- Bilingual coverage (Chinese/English) exposes language-specific biases, with performance differences between languages suggesting potential overfitting to English-centric patterns

## Why This Works (Mechanism)

### Mechanism 1
Evaluating models on tasks requiring tight integration of audio and video inputs may expose modality fusion capabilities not captured by independent modality evaluations. The benchmark designs tasks where correct answers require simultaneously processing dynamic visual events, sound events, and associated text (e.g., dialogue/subtitles), forcing models to perform joint reasoning rather than relying on a single dominant modality.

### Mechanism 2
Precise temporal localization tasks may reveal whether models build genuine temporal representations versus relying on global video features or aggregated statistics. The "Grounding" task requires models to identify specific timestamps or time spans when events occur, which cannot be solved from aggregated features alone and necessitates frame-level or second-level understanding.

### Mechanism 3
Including both Chinese and English with open-ended questions may surface language-specific biases and brittleness that multiple-choice-only benchmarks miss. Open-ended questions require generative capabilities and cannot be solved by option elimination, while bilingual coverage prevents models from overfitting to English-centric patterns.

## Foundational Learning

- **Multimodal Fusion Architectures** (e.g., Q-Former, MLP projectors): Understanding how visual/audio encoders connect to LLMs explains why models might struggle with direct video input versus processed captions (Table 6 shows captions outperform raw frames for most models).
- **Temporal Localization Metrics** (IoU, timestamp accuracy): The grounding task uses IoU for time spans (Eq. 2, threshold 0.5) and adaptive thresholds for moments (Eq. 1)—understanding these is essential for interpreting results correctly.
- **LLM-as-Judge Evaluation**: Open-ended questions are evaluated using proprietary LLMs to compute similarity scores (0-1 scale), introducing potential evaluation variance.

## Architecture Onboarding

- **Component map**: Video → Visual encoder (frames at 1fps or 64 frames) + Audio encoder + ASR (Volcano Engine) → Caption generation (Qwen2.5-VL-70B) → Q&A generation → Manual review → Model inference → Format-specific evaluation
- **Critical path**: Video collection and filtering → Caption + ASR extraction → Automated Q&A generation and classification → Manual review and grounding annotation → Model inference → MC/OE/Grounding evaluation
- **Design tradeoffs**: Frame sampling (1 fps vs. 64 frames) trades temporal resolution for computational cost; OE vs. MC balance (1412 OE, 1205 MC) trades generative assessment for standardized comparison; Bilingual coverage increases diversity but may introduce content distribution shifts
- **Failure signatures**: VITA-1.5: Tensor size out of range errors for videos >200 seconds; MiniCPM-O 2.6: Severe performance degradation when adding raw video to audio+caption; General pattern: Models consistently performing better with captions than raw frames
- **First 3 experiments**: 1) Baseline reproduction: Run Qwen2.5-Omni-7B with 1fps, 1024 max tokens; 2) Ablation study: Compare audio-only, audio+caption, and audio+caption+video inputs; 3) Grounding analysis: Evaluate moment-based vs. time span-based accuracy

## Open Questions the Paper Calls Out

1. How can omni-modal models be improved to effectively leverage raw visual frames rather than relying primarily on textual captions for video understanding?
2. What architectural modifications are required for omni-modal models to handle long-duration video and audio inputs without tensor size overflow?
3. How robust is the open-ended question evaluation to the choice of LLM used for automated scoring?

## Limitations
- Evaluation pipeline reproducibility is limited by reliance on proprietary components (Qwen2.5-VL-70B, Volcano Engine ASR, unspecified LLM-as-judge)
- Temporal grounding metric sensitivity may vary with video duration due to adaptive threshold formula
- Model-specific technical failures (tensor size errors, performance degradation) may emphasize hardware limitations over genuine understanding capabilities

## Confidence
- **High Confidence**: The benchmark successfully identifies real-world multimodal understanding gaps across models
- **Medium Confidence**: Bilingual coverage provides genuine robustness assessment despite potential content distribution confounds
- **Medium Confidence**: Grounding task effectively discriminates temporal understanding with careful threshold interpretation

## Next Checks
1. Attempt to reproduce benchmark results using open-source alternatives for caption generation and ASR to assess pipeline sensitivity
2. Systematically analyze grounding task performance across different video durations to validate adaptive threshold scaling
3. Conduct controlled ablation study isolating contribution of each modality to task performance