---
ver: rpa2
title: Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity
  for Modeling Videoconference Conversation Experience
arxiv_id: '2506.13971'
source_url: https://arxiv.org/abs/2506.13971
tags:
- data
- clips
- were
- labeled
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semi-supervised learning (SSL) models using multimodal fusion achieved
  ROC-AUC of 0.9 and F1 score of 0.6 in predicting negative videoconference moments,
  outperforming supervised learning (SL) by up to 4% with the same amount of labeled
  data. The modality-fused co-training SSL model matched 96% of SL model performance
  using only 8% labeled data.
---

# Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience

## Quick Facts
- arXiv ID: 2506.13971
- Source URL: https://arxiv.org/abs/2506.13971
- Reference count: 0
- Primary result: SSL with modality-fused co-training achieved ROC-AUC of 0.9 and F1 of 0.6, outperforming supervised learning by up to 4% with the same labeled data

## Executive Summary
This paper presents a semi-supervised learning (SSL) framework that significantly reduces annotation requirements for detecting negative moments in videoconference conversations. By fusing multimodal features (audio, facial expressions, and text) and applying co-training SSL, the model achieved strong performance using only 8% labeled data compared to supervised learning. Audio and facial features proved most informative, while text contributed minimally. The modality-fused approach captured emergent properties that modality-split methods missed, enabling effective pseudo-label expansion from unlabeled data.

## Method Summary
The study used the RoomReader corpus of 30 Zoom sessions to extract 7-second clips containing gaps or overlaps, then applied VGGish for audio, OpenFace 2.2 for facial action units, and all-MiniLM-L6-v2 for text embeddings. Three SSL approaches were compared: self-training, modality-split co-training (audio vs face+text), and modality-fused co-training (random PCA-split on fused features). A logistic regression base classifier with balanced class weights was wrapped in these SSL frameworks, with hyperparameter tuning via Optuna across 10-fold session-stratified cross-validation.

## Key Results
- Modality-fused co-training achieved ROC-AUC of 0.9 and F1 score of 0.6
- SSL outperformed supervised learning by up to 4% with the same labeled data
- Best SSL model with 8% labeled data matched 96% of SL model's full-data performance
- Audio and facial features were most informative; text contributed little
- SSL benefits most pronounced for ROC-AUC with minimal labeled data; F1 improvements required more labeled samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality-fused co-training outperforms modality-split by leveraging emergent cross-modal patterns
- Mechanism: Two base classifiers trained on randomly-split PCA-transformed fused features jointly assign pseudo-labels to unlabeled clips
- Core assumption: Emergent properties in fused multimodal data are necessary for the task
- Evidence anchors: Abstract performance claims; Discussion on emergent property importance; corpus work on fusion approaches
- Break condition: If modalities are highly correlated or one dominates, random PCA splits may not provide independent views

### Mechanism 2
- Claim: Pseudo-label expansion enables annotation-efficient learning by selectively incorporating high-confidence unlabeled clips
- Mechanism: Self-training iteratively assigns pseudo-labels to unlabeled datapoints exceeding confidence threshold
- Core assumption: High-confidence predictions on unlabeled data are correct
- Evidence anchors: Abstract performance claims; section 4.2 description of pseudo-labeling; general SSL surveys
- Break condition: If negative moments are ambiguous or annotators disagree substantially, pseudo-labels may propagate errors

### Mechanism 3
- Claim: Audio and facial modalities dominate predictive power; text contributes minimally for this task
- Mechanism: Paralinguistic speech features and facial action units encode turn-taking disruptions and affective cues more directly than semantic text content
- Core assumption: Negative videoconference moments manifest primarily through prosodic and visual channels
- Evidence anchors: Abstract modality importance claims; section 5 ablation study results
- Break condition: For conversation types with richer semantic content, text contribution may increase

## Foundational Learning

- Concept: Semi-supervised learning assumptions (cluster, smoothness, low-density separation)
  - Why needed here: SSL success depends on whether videoconference experience data satisfies these; the paper explicitly notes potential violations from ambiguous overlapping speech
  - Quick check question: Can you explain why overlapping speech might violate the low-density assumption in SSL?

- Concept: Co-training independence requirement
  - Why needed here: Co-training assumes two sufficient and redundant views; modality-fused random splitting worked better than modality-split, challenging the independence assumption
  - Quick check question: Why might random PCA-split features outperform semantically meaningful modality splits in co-training?

- Concept: Pseudo-labeling thresholds and class imbalance
  - Why needed here: F1 improvements lagged ROC-AUC at low labeled ratios due to class imbalance sensitivity; threshold selection matters
  - Quick check question: How would you adjust pseudo-label thresholds when positive class is ~4% of data?

## Architecture Onboarding

- Component map: VGGish (audio, 128-dim/0.96s) -> OpenFace 2.2 (facial AUs, 17 per participant) -> all-MiniLM-L6-v2 (text, 384-dim) -> PCA transformation -> SSL wrapper (co-training preferred) -> pseudo-label assignment -> iterative retraining -> evaluation

- Critical path: Clip extraction (targeted/non-targeted) -> multimodal feature extraction -> PCA transformation -> SSL wrapper (co-training preferred) -> pseudo-label assignment -> iterative retraining -> evaluation on holdout sessions

- Design tradeoffs:
  - Logistic regression over neural networks: Prioritizes robustness with limited data over representational capacity
  - Modality-fused vs. modality-split: Fused captures emergence but loses interpretability; split failed significantly
  - Targeted vs. non-targeted clips: Targeted annotation focuses effort; non-targeted provides SSL unlabeled pool

- Failure signatures:
  - Modality-split co-training performed worse than SL (suggests independence assumption violated)
  - F1 unstable at <16% labeled data due to class imbalance
  - Text modality adds minimal value in current task

- First 3 experiments:
  1. Replicate self-training vs. modality-fused co-training comparison on a held-out session to validate 4% improvement claim with identical hyperparameters
  2. Ablate audio and face modalities separately to confirm their relative importance matches paper's ablation study
  3. Test SSL performance degradation by progressively reducing labeled data from 32% to 4% to identify the annotation floor where benefits disappear

## Open Questions the Paper Calls Out
The paper notes that the underlying reason why modality-split co-training underperformed remains to be explored, highlighting that SSL algorithms do not always improve performance. The authors suggest overlapping speech might violate SSL assumptions but didn't isolate the specific mechanism. Additionally, the poor performance of text features raises questions about whether semantic content is truly irrelevant to videoconference fluidity, or if this reflects limitations in the feature extraction approach.

## Limitations
- Limited validation across different videoconference contexts or participant demographics
- Reliance on PCA for feature fusion may obscure modality-specific patterns valuable for interpretation
- Binary classification threshold (2.5 on 5-point scale) is somewhat arbitrary and affects sensitivity to different severity levels

## Confidence

**High confidence**: Modality-fused co-training outperforms modality-split approaches and self-training for ROC-AUC with minimal labeled data

**Medium confidence**: Audio and facial modalities dominate predictive power over text for this specific task

**Low confidence**: F1 score improvements at 8% labeled data ratio - the paper notes this requires more labeled samples

## Next Checks
1. Replicate the 96% performance claim at 8% labeled data using a completely held-out session not seen during hyperparameter tuning
2. Test model robustness by varying the binary threshold from 2.0 to 3.0 to assess sensitivity to classification criteria
3. Evaluate SSL performance on a different videoconference dataset with different conversation topics to validate cross-dataset generalization