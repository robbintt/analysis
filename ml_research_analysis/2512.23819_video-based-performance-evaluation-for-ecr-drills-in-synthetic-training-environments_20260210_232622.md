---
ver: rpa2
title: Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments
arxiv_id: '2512.23819'
source_url: https://arxiv.org/abs/2512.23819
tags:
- metrics
- performance
- training
- team
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a video-based pipeline that automatically extracts
  2D skeletons, gaze vectors, and movement trajectories from training videos of the
  Enter and Clear the Room (ECR) drill. The system computes domain-specific performance
  metrics measuring psychomotor fluency, situational awareness, and team coordination,
  and integrates them into a hierarchical Cognitive Task Analysis (CTA) framework.
---

# Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments

## Quick Facts
- arXiv ID: 2512.23819
- Source URL: https://arxiv.org/abs/2512.23819
- Reference count: 0
- Automatic video-based evaluation of military ECR drill performance using 2D pose estimation and hierarchical metrics

## Executive Summary
This paper presents a video-based pipeline for automatically evaluating military Enter and Clear the Room (ECR) drill performance in synthetic training environments. The system extracts 2D skeletons, gaze vectors, and movement trajectories from training videos to compute 10 domain-specific performance metrics that assess psychomotor fluency, situational awareness, and team coordination. These metrics are integrated into a hierarchical Cognitive Task Analysis framework that provides high-level performance scores. Evaluated on four teams across three trials, the approach demonstrates quantifiable performance trends and supports scalable, objective assessment for After Action Review dashboards.

## Method Summary
The pipeline uses a two-stage pose estimation approach with RTMDet-m detector and RTMPose-x pose estimator (Halpe26 keypoints), fine-tuned on custom annotations. OC-SORT tracking maintains consistent IDs across frames with Kalman filtering, while planar homography maps video coordinates to top-down room maps using 4+ reference points. Gaze triangles are constructed using 20° visual angles from head keypoints. Ten domain-specific metrics measure entrance behavior, point-of-dominance capture, threat coverage, and team coordination. Results are aggregated through weighted hierarchical roll-up with exponential smoothing across trials.

## Key Results
- Pipeline successfully extracted 2D skeletons, gaze vectors, and movement trajectories from ECR training videos
- 10 domain-specific metrics computed automatically from video data
- High-level CTA scores generally improved across trials, reflecting lower-level metric changes
- Framework supports scalable, objective assessment for After Action Review dashboards

## Why This Works (Mechanism)
The approach leverages existing computer vision techniques (pose estimation, tracking, homography) and adapts them to a specific military training context. By extracting geometric and movement features from video, it enables objective quantification of psychomotor skills, situational awareness, and team coordination without requiring wearable sensors or manual annotation.

## Foundational Learning
- 2D Pose Estimation (RTMPose): Extracts human body keypoints from video frames; needed for tracking soldier positions and movements; quick check: visualize keypoints on sample frames.
- Planar Homography: Maps pixel coordinates to real-world room coordinates using reference points; needed to compute spatial metrics on room map; quick check: project known points and verify alignment.
- Gaze Vector Approximation: Estimates visual attention from head orientation using 20° visual angle; needed for situational awareness metrics; quick check: compare against manual gaze annotations if available.

## Architecture Onboarding

**Component Map:** Video -> RTMDet-m -> RTMPose-x -> OC-SORT Tracking -> Homography -> 10 Metrics -> CTA Aggregation

**Critical Path:** Pose estimation and tracking must work reliably for downstream metric computation; gaze vector construction depends on accurate head keypoints; homography accuracy affects all spatial metrics.

**Design Tradeoffs:** 2D pose estimation enables markerless operation but limits gaze accuracy compared to 3D estimation; OC-SORT provides robust tracking but requires re-initialization during extended occlusions; exponential smoothing balances metric stability with responsiveness to performance changes.

**Failure Signatures:** Tracking ID switches when soldiers occlude each other or uniforms blend with background; keypoint detection degrades in low light or with dark uniforms; gaze triangles show systematic angular errors; homography mapping produces misaligned positions on room map.

**3 First Experiments:**
1. Visualize keypoint confidence scores across frames to identify lighting/uniform-related detection issues
2. Plot tracking ID assignments over time to detect and characterize ID switches
3. Overlay projected foot positions on room map to verify homography calibration accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Military training dataset is not publicly available, requiring data access agreement or substitute dataset
- 2D pose estimation inherently limits gaze vector accuracy compared to 3D estimation or eye-tracking sensors
- Tracking robustness challenges in crowded, occluded scenarios with similar-looking soldiers

## Confidence
- High confidence: Pipeline architecture, keypoint extraction methodology, and 10 domain-specific metric definitions are clearly specified
- Medium confidence: Metric aggregation framework and CTA roll-up procedure are well-defined but may vary in parameter values
- Medium confidence: Performance trends are reported but small sample size (4 teams, 3 trials) and lack of external validation limit generalizability

## Next Checks
1. Validate gaze triangle construction by comparing 2D-estimated gaze vectors against manual annotations or eye-tracking data on a subset of frames
2. Test homography mapping accuracy by projecting known reference points from video to map coordinates and measuring reprojection error
3. Implement synthetic video generation with ground-truth annotations to systematically evaluate metric computation accuracy under controlled conditions