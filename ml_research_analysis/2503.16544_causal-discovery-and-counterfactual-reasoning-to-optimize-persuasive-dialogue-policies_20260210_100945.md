---
ver: rpa2
title: Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue
  Policies
arxiv_id: '2503.16544'
source_url: https://arxiv.org/abs/2503.16544
tags:
- causal
- counterfactual
- strategies
- persuasive
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for optimizing persuasive dialogue
  policies using causal discovery and counterfactual reasoning. The approach employs
  the Greedy Relaxation of the Sparsest Permutation (GRaSP) algorithm to identify
  causal relationships between user and system utterance strategies, treating user
  strategies as states and system strategies as actions.
---

# Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies

## Quick Facts
- **arXiv ID:** 2503.16544
- **Source URL:** https://arxiv.org/abs/2503.16544
- **Reference count:** 40
- **Primary result:** Integrating causal discovery with counterfactual reasoning improves reinforcement learning policies for online dialogue systems, achieving higher cumulative rewards and Q-values than baseline methods.

## Executive Summary
This paper introduces a novel approach for optimizing persuasive dialogue policies by combining causal discovery and counterfactual reasoning. The method uses the GRaSP algorithm to identify causal relationships between user and system utterance strategies, treating user strategies as states and system strategies as actions. These causal links inform a Bidirectional Conditional GAN to generate counterfactual utterances, which are then used to train a Dueling Double Deep Q-Network for policy optimization. Experiments with the PersuasionForGood dataset demonstrate measurable improvements in persuasion outcomes compared to baseline approaches.

## Method Summary
The approach operates in three stages: (1) GRaSP discovers causal relationships between 23 user strategy variables and 27 system strategy variables from dialogue data; (2) BiCoGAN generates counterfactual next-state utterances conditioned on (state, action) pairs using the causal graph; (3) D3QN learns an optimal policy from the counterfactual dataset using predicted rewards from an LSTM donation predictor. Strategy prediction is performed by fine-tuned GPT-2 models with 92.3% accuracy on 5-fold CV.

## Key Results
- GRaSP+BiCoGAN achieves the best results overall, with both BiCoGAN and GRaSP+BiCoGAN showing higher cumulative donation amounts than ground truth
- Total predicted donation amount is $508.78 (increase of $31.16) for Causal+BiCoGAN compared to ground truth actual donation of $477.62
- The approach demonstrates measurable increases in cumulative rewards and Q-values compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Causal Discovery Constrains Counterfactual Search Space
- Claim: Identifying strategy-level causal relationships produces higher-quality counterfactual training data than random selection
- Mechanism: GRaSP discovers directed cause-effect pairs between 23 persuadee and 27 persuader strategy variables, informing which alternative system actions are plausible
- Core assumption: Dialogue strategies follow an underlying structural causal model recoverable from observational data
- Evidence anchors: GRaSP identifies user strategies as causal factors influencing system responses; counterfactual data remains identifiable from observed conditions
- Break condition: If causal graph recovery is unreliable, counterfactual actions may be mis-specified, degrading policy quality

### Mechanism 2: BiCoGAN Approximates Dialogue SCM for Realistic Counterfactuals
- Claim: Bidirectional conditional GAN can learn to generate plausible next-state utterances conditioned on (state, action) pairs
- Mechanism: BiCoGAN learns in two directions - generator maps (s_t, a_t, ε_t+1) to ŝ_t+1; encoder inverts s_t+1 back to latent factors
- Core assumption: Dialogue transitions can be modeled as a differentiable SCM with recoverable noise terms
- Evidence anchors: BiCoGAN learns from two directions as described; alternative reality data points remain identifiable from observed conditions
- Break condition: If utterance embedding space fails to capture strategy-relevant semantics, generated counterfactual states may be strategically incoherent

### Mechanism 3: Counterfactual-Augmented D3QN Improves Policy Learning
- Claim: Training D3QN on causally-grounded counterfactual dialogue data yields higher Q-values and cumulative rewards than training on ground-truth alone
- Mechanism: Counterfactual dataset provides diverse (state, action, next-state) tuples that expand the agent's experience beyond observed trajectories
- Core assumption: Counterfactual transitions are sufficiently realistic that their reward signals transfer to actual dialogue policy optimization
- Evidence anchors: Increases in cumulative rewards and Q-values compared to baseline methods; GRaSP+BiCoGAN achieves best results overall
- Break condition: If the reward model is miscalibrated, D3QN may optimize for incorrectly predicted rewards

## Foundational Learning

- **Structural Causal Models (SCMs)**
  - Why needed here: The paper formalizes dialogue transitions as s_t+1 = f(s_t, a_t, ε_t+1); understanding noise terms, interventions, and counterfactual queries is essential
  - Quick check question: Given an SCM, can you explain why observing s_t+1 after a_t differs from asking "what would s_t+1 be if we intervened on a_t?"

- **Causal Discovery (constraint-based and permutation-based)**
  - Why needed here: GRaSP discovers causal structure from observational strategy co-occurrences without experimental interventions
  - Quick check question: Why can't correlation alone tell you that x_ee causes x_er rather than vice versa or via a common cause?

- **Dueling Double DQN (D3QN)**
  - Why needed here: D3QN addresses Q-value overestimation by decoupling action selection from evaluation and splitting Q into V(s) + A(s,a)
  - Quick check question: What problem does the "dueling" architecture solve that standard DQN cannot?

## Architecture Onboarding

- **Component map:**
  GPT-2 strategy prediction -> GRaSP causal discovery -> Retrieval-based counterfactual action selection -> BiCoGAN counterfactual state generation -> LSTM reward prediction -> D3QN policy learning

- **Critical path:**
  1. Annotate/predict strategies for all utterances (GPT-2 fine-tuning on 300 annotated dialogues)
  2. Run GRaSP to discover x_ee → x_er edges
  3. For each dialogue step, identify cause strategies → retrieve top-k effect-linked ER utterances → sample as a'_t
  4. Feed (s_t, a'_t) to trained BiCoGAN generator → obtain s'_t+1
  5. Build ~D with N counterfactual variants per dialogue
  6. Train D3QN on ~D using LSTM reward model for terminal rewards

- **Design tradeoffs:**
  - Retrieval-based vs. generative utterances: Retrieval grounds counterfactuals in real language but limits diversity; generative could hallucinate
  - Strategy granularity: 50 strategies may be too fine (sparse co-occurrence) or too coarse (conflates distinct tactics); choice affects causal graph density
  - BERT embedding dimensionality: 768-dim vectors may over-compress strategy-relevant pragmatics; domain-adaptive pretraining could help but adds cost

- **Failure signatures:**
  - Causal graph too dense/sparse: GRaSP returns near-complete or near-empty graph → counterfactual actions become random or trivially constrained
  - BiCoGAN mode collapse: Generated s'_t+1 lacks diversity → D3QN overfits to narrow counterfactual distribution
  - Reward model misprediction: LSTM systematically under/over-predicts donations for certain strategies → policy learns to exploit reward bugs

- **First 3 experiments:**
  1. Ablate causal discovery: Train D3QN on counterfactuals generated with random action selection vs. GRaSP-guided; compare Q-values and cumulative predicted donations
  2. Validate counterfactual plausibility: Human-annotator study on whether BiCoGAN-generated s'_t+1 utterances are contextually coherent given (s_t, a'_t)
  3. Sensitivity to strategy annotation quality: Train GPT-2 strategy predictor with varying annotation fractions (10%, 30%, 50% of 300 dialogues) and measure downstream policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can user profiling (e.g., personality traits, beliefs, socioeconomic factors) be integrated into the causal discovery process to account for individual persuasion receptiveness?
- **Basis in paper:** The authors state in Section 7 that the current method "does not yet account for user individuality," noting that these factors influence receptiveness and necessitate user profiling.
- **Why unresolved:** The current framework treats user strategies as generic states without distinguishing between different user types or demographic backgrounds, potentially limiting personalization.
- **What evidence would resolve it:** A modified causal model that conditions strategy relationships on user profiles, demonstrating higher cumulative rewards or donation amounts specifically tailored to distinct user clusters.

### Open Question 2
- **Question:** Can self-supervised or weakly supervised causal discovery methods effectively identify dialogue strategy relationships without relying on heavily annotated datasets?
- **Basis in paper:** Section 7 notes that the current approach "relies on annotated datasets, which may introduce biases" and explicitly suggests exploring self-supervised learning to reduce this dependence.
- **Why unresolved:** The current GRaSP implementation and the fine-tuned GPT-2 strategy predictor depend on the 300 annotated dialogues from PersuasionForGood to establish ground truth strategies.
- **What evidence would resolve it:** Successful extraction of valid causal strategy graphs from unannotated or weakly annotated dialogue corpora that yield comparable or superior policy performance to the current supervised baseline.

### Open Question 3
- **Question:** How can adaptive learning mechanisms be developed to track shifting causal relationships as users adjust their responses based on experience during interactions?
- **Basis in paper:** The authors state in Section 7 that "persuasion dynamics evolve as users adjust their responses... which requires adaptive learning mechanisms to track shifting causal relationships."
- **Why unresolved:** The current GRaSP algorithm identifies a static causal graph from the dataset, assuming fixed causal dependencies between strategies rather than temporal or dynamic ones.
- **What evidence would resolve it:** A dynamic causal discovery model that updates edge probabilities in real-time, showing improved Q-value convergence or reward accumulation in multi-turn interactions compared to a static graph.

### Open Question 4
- **Question:** How do actual users perceive and respond to the proposed causality-based counterfactual dialogue policies in a real-world interface?
- **Basis in paper:** Section 7 acknowledges that "this work does not include a real-world user study" and calls for future work to develop an interface to validate the mechanism with actual users.
- **Why unresolved:** The current evaluation is limited to offline metrics (cumulative rewards, Q-values) and predicted donation amounts on the PersuasionForGood dataset, rather than live human feedback.
- **What evidence would resolve it:** Results from a live A/B test comparing user satisfaction scores and actual donation behaviors when interacting with the causal-counterfactual agent versus a baseline persuasive agent.

## Limitations

- The approach relies on noisy strategy annotations for causal discovery and counterfactual generation, which may introduce biases
- Policy improvements are evaluated using predicted rewards rather than actual persuasion outcomes, raising questions about transferability
- The framework assumes dialogue strategies follow a recoverable structural causal model, which may not hold for more naturalistic or multi-turn interactions

## Confidence

- **High:** Causal discovery via GRaSP is a valid method for identifying strategy-level causal relationships from observational data
- **Medium:** BiCoGAN can generate plausible counterfactual dialogue states given (state, action) pairs, though validation is limited
- **Low:** Counterfactual-augmented D3QN policy improvements will transfer to actual persuasion success (donation amounts) beyond predicted rewards

## Next Checks

1. Conduct a human evaluation study where annotators rate the plausibility and coherence of BiCoGAN-generated counterfactual utterances given their context
2. Perform an ablation study comparing D3QN performance trained on ground-truth dialogues vs. randomly generated counterfactuals vs. GRaSP-guided counterfactuals
3. Measure the sensitivity of downstream policy performance to the quality and quantity of strategy annotations used to train the GPT-2 predictor and feed GRaSP