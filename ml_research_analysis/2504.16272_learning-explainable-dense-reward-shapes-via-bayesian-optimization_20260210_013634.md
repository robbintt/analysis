---
ver: rpa2
title: Learning Explainable Dense Reward Shapes via Bayesian Optimization
arxiv_id: '2504.16272'
source_url: https://arxiv.org/abs/2504.16272
tags:
- reward
- optimization
- token-level
- https
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sparse rewards in reinforcement
  learning from human feedback (RLHF) for language model alignment, where only sequence-level
  rewards are provided, leading to poor token-level credit assignment. The authors
  propose a method that uses explainability techniques like SHAP and LIME to estimate
  per-token rewards, and then employs Bayesian Optimization to learn optimal weights
  for combining these token-level signals into a dense reward function.
---

# Learning Explainable Dense Reward Shapes via Bayesian Optimization

## Quick Facts
- arXiv ID: 2504.16272
- Source URL: https://arxiv.org/abs/2504.16272
- Reference count: 40
- Primary result: Bayesian optimization of token-level reward weights improves RLHF training stability and downstream task performance over sparse rewards

## Executive Summary
This paper addresses sparse reward credit assignment in RLHF by converting sequence-level rewards into dense token-level signals using explainability methods (SHAP, LIME). The authors frame this as a bilevel optimization problem where Bayesian Optimization (BO) searches for optimal weights combining multiple attribution sources. Theoretically, they prove that additive feature attribution methods preserve policy invariance under reward shaping. Empirically, dense rewards from explainability reduce PPO value loss and reward overfitting, with BO-optimized weights achieving better downstream performance on benchmarks like AlpacaEval2 and MTBench.

## Method Summary
The approach uses explainability techniques to decompose sequence-level rewards into per-token contributions, then employs Bayesian Optimization to learn optimal weights for combining these signals into a dense reward function. The outer BO loop samples weight vectors over attribution sources (SHAP, LIME, attention, sparse), trains PPO briefly on the shaped rewards, and evaluates on a validation set. The inner PPO loop optimizes the policy using these dense rewards. The framework maintains theoretical guarantees of policy invariance while empirically improving training stability and reducing overfitting compared to sparse rewards.

## Key Results
- Dense rewards from explainability drastically reduce PPO value head loss compared to sparse rewards
- BO-optimized weights achieve higher win rates on AlpacaEval2 and better MTBench scores than fixed-weight baselines
- The method shows improved training stability and reduced reward overfitting across HH-RLHF and Ultrafeedback datasets
- d=4 input combinations (SHAP+LIME+attention+sparse) degraded performance, suggesting BO requires sufficient trials for higher-dimensional search

## Why This Works (Mechanism)

### Mechanism 1: Policy Invariance via Additive Feature Attribution
Additive feature attribution methods (SHAP, LIME) decompose rewards into per-token contributions while maintaining policy invariance through potential-based shaping functions. The local linear approximation g(z′) = φ₀ + Σφᵢz′ᵢ preserves optimal policies when used for reward redistribution.

### Mechanism 2: Improved Credit Assignment Reduces Value Loss
Dense token-level rewards provide intermediate training targets for the value function, reducing variance in value estimates compared to sparse terminal rewards. This leads to more stable policy updates and better approximation of state-action values.

### Mechanism 3: BO Optimizes Complementary Attribution Signals
Bayesian Optimization searches for optimal weights combining multiple attribution sources (SHAP, LIME, attention) through a GP surrogate with log-Noisy Expected Improvement acquisition. The method balances exploration-exploitation to navigate noisy reward surfaces.

## Foundational Learning

- **Potential-Based Reward Shaping (Ng et al., 1999)**: Why needed - The theoretical guarantee that reshaped rewards preserve optimal policies rests on this concept. Quick check - If you add F(s,a,s′) = γΦ(s′) − Φ(s) to rewards, does the optimal policy change? (Answer: No, by construction.)

- **Additive Feature Attribution (SHAP/LIME)**: Why needed - The method assumes these techniques can meaningfully decompose reward predictions into token contributions. Quick check - What constraint must g(z′) satisfy for SHAP values to sum to f(x)? (Answer: Local accuracy: g(z′) ≈ f(hₓ(z′)) when z′ = 1.)

- **Bilevel Optimization**: Why needed - The framework nests policy optimization within weight optimization, requiring validation reward as the outer objective. Quick check - In this bilevel setup, what does the outer loop optimize? (Answer: Weights w for validation reward; inner loop optimizes policy πθ for shaped reward.)

## Architecture Onboarding

- **Component map**: Reward Model -> Explainer Module (SHAP/LIME/attention) -> Reward Shaper (softmax normalization + weight combination) -> PPO Trainer -> BO Controller

- **Critical path**: 1) Initialize GP surrogate with Sobol sampling for first 5 trials 2) For each trial: sample weights → compute shaped rewards → train PPO 10 epochs → evaluate validation → update GP 3) Checkpoint best policy 4) After m trials, retrieve w_best for final training

- **Design tradeoffs**: 25 trials may be insufficient for d≥4 dimensions; full SHAP requires O(k²) operations vs. faster Owen values; 90/10 train/validation split may introduce noise

- **Failure signatures**: BO non-convergence (oscillating validation reward), reward overfitting (high training reward but poor downstream performance), degraded d=4 performance, explainability misalignment with human judgment

- **First 3 experiments**: 1) Sparse reward baseline on HH-RLHF (expect high value loss variance, win-rate <50%) 2) Fixed-weight SHAP dense reward (expect lower value loss, higher win-rates) 3) BO-SHAP with m=25 trials (expect increasing validation reward, improved MTBench/AlpacaEval2)

## Open Questions the Paper Calls Out

- **Dynamic contextual weighting**: Can incorporating contextual information into BO weights enable token-specific reward shaping that outperforms static global weights? The authors suggest this as future work.

- **Human-aligned interpretability signals**: Do human-derived token-level annotations provide more reliable dense rewards than mechanistic methods like SHAP or LIME? The paper notes mechanistic interpretability remains fundamentally suboptimal.

- **BO sample efficiency scaling**: How does BO performance scale with input dimensionality? The authors observed d=4 degradation, suggesting insufficient trials for higher-dimensional search.

## Limitations

- BO-based weight optimization introduces compounding noise from different PPO hyperparameters and limited training epochs per trial
- Reward model fidelity issues may undermine theoretical policy invariance guarantees when attributions don't reflect true contribution
- d=4 combination degradation suggests BO requires proportionally more trials for higher-dimensional search spaces
- Validation reward as BO objective assumes perfect correlation with downstream task performance, which may not hold

## Confidence

- **High confidence**: Policy invariance under additive feature attribution is mathematically proven
- **Medium confidence**: Dense rewards improve training stability and reduce value loss compared to sparse rewards
- **Low confidence**: BO-optimized weights consistently outperform manual weight selection across all configurations

## Next Checks