---
ver: rpa2
title: 'LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional
  Videos?'
arxiv_id: '2601.20705'
source_url: https://arxiv.org/abs/2601.20705
tags:
- video
- arxiv
- mllms
- reve
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEMON, a new benchmark for evaluating multimodal
  large language models (MLLMs) on instructional videos. LEMON includes 2,277 video
  segments with synchronized video, audio, and subtitles, and 4,181 QA pairs across
  29 STEM courses.
---

# LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?

## Quick Facts
- arXiv ID: 2601.20705
- Source URL: https://arxiv.org/abs/2601.20705
- Reference count: 40
- New benchmark (LEMON) reveals MLLMs struggle with temporal reasoning and audio understanding in instructional videos despite subtitles

## Executive Summary
This paper introduces LEMON, a comprehensive benchmark for evaluating multimodal large language models on instructional video understanding. The benchmark features 2,277 video segments with synchronized video, audio, and subtitles from 29 STEM courses, along with 4,181 QA pairs testing six distinct tasks. Experiments on 21 models reveal that while proprietary MLLMs like GPT-5 and Gemini 2.5 Pro excel at basic multimodal comprehension, all models struggle significantly with temporal reasoning, audio understanding, and fine-grained cross-lingual generation in long-form, knowledge-intensive contexts.

## Method Summary
The LEMON benchmark is constructed from 29 STEM courses, with videos segmented into 2,277 clips and synchronized with audio and subtitles. Six tasks are defined: Streaming Perception, OCR-Based Reasoning, Audio Comprehension, Temporal Awareness, Instructional Prediction, and Advanced Expression. The benchmark includes 4,181 QA pairs designed to test various aspects of multimodal understanding. 21 models are evaluated across these tasks, with careful attention to the role of subtitles and audio inputs in model performance.

## Key Results
- Proprietary models (GPT-4o, Gemini 2.5 Pro) outperform open-source alternatives in basic multimodal comprehension
- All models struggle with temporal reasoning and audio understanding despite subtitle availability
- Subtitles provide consistent benefits while audio input offers limited gains in performance
- Models excel at low-level perception but lag in complex reasoning and generation for long-form instructional content

## Why This Works (Mechanism)
Assumption: The benchmark's effectiveness stems from its comprehensive design that integrates multiple modalities (video, audio, subtitles) with domain-specific content (STEM instructional videos) to create realistic evaluation scenarios that expose model limitations in temporal reasoning and cross-modal understanding.

## Foundational Learning
- Multimodal video processing: Required for understanding how models integrate visual, audio, and text streams; quick check: verify models can process individual modalities before multimodal fusion
- Temporal reasoning in videos: Essential for understanding sequential events and causality; quick check: test models on simple before/after relationships
- Cross-modal alignment: Needed for synchronizing information across video, audio, and subtitles; quick check: verify alignment accuracy in segmented clips
- Instructional content comprehension: Critical for domain-specific knowledge extraction; quick check: test models on subject-specific terminology
- Fine-grained generation evaluation: Important for assessing quality of instructional outputs; quick check: compare model outputs against expert-annotated references

## Architecture Onboarding

**Component Map:**
Video encoder -> Audio encoder -> Text encoder -> Multimodal fusion -> Language model -> Output generator

**Critical Path:**
Video/Audio preprocessing -> Feature extraction -> Temporal alignment -> Multimodal fusion -> Reasoning generation

**Design Tradeoffs:**
The benchmark prioritizes comprehensive evaluation over speed, using manual annotation and multiple task types rather than simplified automated metrics.

**Failure Signatures:**
Models fail primarily on temporal reasoning tasks, audio comprehension when subtitles are absent, and cross-lingual generation requiring domain-specific terminology.

**First 3 Experiments:**
1. Evaluate baseline unimodal performance (video-only, audio-only, text-only) before multimodal testing
2. Test model performance on synthetic instructional videos with controlled temporal relationships
3. Conduct ablation studies removing individual modalities to quantify their isolated contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on STEM instructional videos, limiting generalizability to other video genres
- Manual annotation process may introduce subjectivity in task difficulty calibration
- Cross-lingual generation evaluation lacks standardized metrics for fine-grained language quality assessment

## Confidence

**High confidence:**
- Proprietary models outperform open-source alternatives in basic multimodal comprehension (consistent performance gaps across metrics)

**Medium confidence:**
- All models struggle with temporal reasoning and audio understanding (results robust but domain-specific)
- Subtitles provide consistent benefits while audio offers limited gains (methodology may not capture full audio-visual integration value)

## Next Checks

1. Test model performance on LEMON across different video genres (entertainment, news, tutorials) to assess domain generalization

2. Conduct ablation studies removing subtitles and audio individually to quantify their isolated contributions versus interactive effects

3. Evaluate the same models on LEMON using human-annotated ground truth answers to validate automated evaluation metrics