---
ver: rpa2
title: 'To See or To Read: User Behavior Reasoning in Multimodal LLMs'
arxiv_id: '2511.03845'
source_url: https://arxiv.org/abs/2511.03845
tags:
- product
- user
- reasoning
- prediction
- purchase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores whether multimodal large language models (MLLMs)\
  \ benefit more from textual or visual representations of user behavior data for\
  \ next-purchase prediction. The authors introduce BehaviorLens, a benchmarking framework\
  \ that compares three representations\u2014text, scatter plot, and flowchart\u2014\
  of sequential transaction data using six MLLM models."
---

# To See or To Read: User Behavior Reasoning in Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2511.03845
- **Source URL**: https://arxiv.org/abs/2511.03845
- **Reference count**: 21
- **Primary result**: Image-based user behavior representations improve next-purchase prediction accuracy by up to 87.5% compared to text alone

## Executive Summary
This paper investigates whether multimodal large language models (MLLMs) benefit more from textual or visual representations of user behavior data for next-purchase prediction. The authors introduce BehaviorLens, a benchmarking framework that systematically compares three representations—text, scatter plot, and flowchart—of sequential transaction data across six MLLM models. Using a real-world purchase sequence dataset, they find that image-based representations consistently outperform text-based representations, with scatter plots and flowcharts achieving up to 87.5% higher accuracy. The improvement is consistent across multiple models and is not driven by differences in explanation quality. Flowcharts especially benefit Gemini models, while scatter plots tend to perform better for GPT models.

## Method Summary
The authors developed BehaviorLens, a benchmarking framework to compare different representations of user behavior data for MLLM reasoning. They tested three formats—text (purchase sequences as text), scatter plot (visual timeline of transactions), and flowchart (visual sequence of transactions)—using six MLLM models including GPT-4o, GPT-4o-mini, Gemini-1.5-Pro, Gemini-1.5-Flash, Claude-3-5-Sonnet, and Claude-3-5-Haiku. The study used a real-world purchase sequence dataset and evaluated models on next-purchase prediction tasks. They measured both prediction accuracy and explanation quality, finding that visual representations improved accuracy without additional computational cost during inference.

## Key Results
- Image-based representations (scatter plot and flowchart) improved next-purchase prediction accuracy by up to 87.5% compared to text-only input
- Flowcharts were particularly effective for Gemini models, while scatter plots tended to perform better for GPT models
- The accuracy improvement was consistent across multiple MLLM models and was not explained by differences in explanation quality

## Why This Works (Mechanism)
Visual representations provide a holistic view of user journeys that allows MLLMs to better capture patterns in sequential behavior. Unlike text-based descriptions that require sequential parsing, visual formats like scatter plots and flowcharts enable models to simultaneously process temporal patterns, transaction frequencies, and behavioral trends. This holistic encoding appears to align better with how MLLMs process visual information, leading to improved reasoning about user behavior patterns.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: AI systems that can process and reason across multiple data modalities (text, images, etc.) - needed to understand the target models being evaluated; quick check: verify the models can process both text and image inputs
- **Sequential Purchase Data**: Time-ordered records of user transactions - needed to understand the behavioral data being modeled; quick check: confirm the dataset contains timestamped purchase sequences
- **BehaviorLens Framework**: Benchmarking approach for comparing different user behavior representations - needed to understand the evaluation methodology; quick check: verify the framework systematically compares equivalent information in different formats
- **Next-Purchase Prediction**: Task of predicting the subsequent item a user will purchase based on historical behavior - needed to understand the core task being evaluated; quick check: confirm the prediction task uses the most recent transaction as the target
- **Visual Encoding of Behavioral Data**: Converting sequential behavior into visual formats like scatter plots and flowcharts - needed to understand the representation techniques; quick check: verify visual formats preserve all information from text representations

## Architecture Onboarding
**Component Map**: Dataset -> BehaviorLens Framework -> MLLM Models (6 variants) -> Evaluation Metrics
**Critical Path**: Data preparation → Visual/text representation generation → MLLM inference → Accuracy and explanation quality evaluation
**Design Tradeoffs**: Visual representations require additional preprocessing but provide no computational cost during inference; text is simpler but less effective for pattern recognition
**Failure Signatures**: Poor performance on text representations, inconsistent gains across model types, or explanation quality not correlating with accuracy
**First Experiments**: 1) Test visual vs text representations on a small subset of the dataset, 2) Compare accuracy across all six MLLM models, 3) Analyze explanation quality differences between representation formats

## Open Questions the Paper Calls Out
None

## Limitations
- Results may be specific to sequential purchase data and may not generalize to other behavioral domains
- The benchmarking framework assumes equivalence between visual and textual information content without formal validation
- Computational "no additional cost" claim applies only to inference, not model training or adaptation costs
- Study focuses on a single real-world dataset, raising questions about domain transfer

## Confidence
- **High confidence**: Visual representations outperform text for next-purchase prediction accuracy
- **Medium confidence**: Model-specific preferences (flowcharts for Gemini, scatter plots for GPT) are consistent patterns
- **Medium confidence**: Improvement is not explained by explanation quality differences

## Next Checks
1. Test the three representation formats across diverse behavioral domains (e.g., app usage, web browsing, content consumption) to assess generalizability
2. Conduct ablation studies comparing information content between visual and textual representations to verify encoding equivalence
3. Measure end-to-end system costs including any required model adaptation or fine-tuning for visual input processing