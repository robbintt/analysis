---
ver: rpa2
title: Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising
arxiv_id: '2511.14719'
source_url: https://arxiv.org/abs/2511.14719
tags:
- video
- synthetic
- arxiv
- photorealism
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot structure-aware denoising framework
  for enhancing synthetic video realism. The method builds upon a pre-trained diffusion
  video model, using DDIM inversion to encode structural information from the source
  synthetic video into a noise latent.
---

# Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising

## Quick Facts
- **arXiv ID:** 2511.14719
- **Source URL:** https://arxiv.org/abs/2511.14719
- **Authors:** Yifan Wang; Liya Ji; Zhanghan Ke; Harry Yang; Ser-Nam Lim; Qifeng Chen
- **Reference count:** 10
- **One-line primary result:** Zero-shot framework enhances synthetic video realism while preserving semantic and structural consistency, achieving state-of-the-art performance on CARLA driving videos.

## Executive Summary
This paper proposes a zero-shot framework for enhancing synthetic video realism using structure-aware denoising. The method employs DDIM inversion to encode structural information from synthetic videos into noise latents, then regenerates them with spatial conditioning maps (depth, segmentation, edges) and Classifier-Free Guidance. The approach preserves semantic consistency while improving photorealism, particularly for safety-critical objects like traffic lights and signs. Evaluated on CARLA synthetic driving videos, it outperforms baselines in object alignment, perceptual similarity, and photorealism without requiring fine-tuning.

## Method Summary
The framework consists of three main stages: DDIM inversion to encode the synthetic video into a noise latent, structure-aware denoising using spatial conditioning maps, and photorealistic regeneration via Classifier-Free Guidance. The system uses Cosmos-Transfer1 with a DiT-ControlNet architecture, processing depth, segmentation, and edge maps to preserve geometric structure during the denoising process. The method employs dual text prompts - one for content inversion and another for photorealistic guidance - and operates in a zero-shot manner without fine-tuning the base model.

## Key Results
- Outperforms Cosmos-Transfer1 and Wan2.1-VACE baselines on CARLA driving videos
- Preserves identity and color of safety-critical objects like traffic lights and signs
- Achieves LPIPS score of 0.3683, DINO score of 0.550, and CLIP score of 0.751
- Shows improved object alignment (DINO/CLIP scores) and photorealism compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Latent Anchoring via DDIM Inversion
The method deterministically maps synthetic videos to noise latents via DDIM inversion, preserving global structure and motion better than random initialization. This specific latent encoding constrains the diffusion model to reconstruct variations of the original scene rather than hallucinating new content, effectively disentangling structural information from texture/style.

### Mechanism 2: Structure-Aware Conditioning (ControlNet)
Spatial conditioning maps (depth, edges, segmentation) are injected into the denoising backbone via a parallel ControlNet branch, forcing the generator to adhere to the simulator's geometry. This prevents semantic drift in safety-critical objects by steering the attention mechanism to preserve specific shapes even while texture generation shifts toward photorealism.

### Mechanism 3: Semantic-Guided Style Transfer (CFG)
The framework uses different prompts for inversion and denoising, with Classifier-Free Guidance amplifying the direction of the photorealistic prompt while suppressing synthetic textures. This domain shift allows the model to alter surface realism while the inverted latent and spatial maps lock the content structure.

## Foundational Learning

- **Concept: DDIM Inversion**
  - **Why needed here:** Core "zero-shot" trick to find latent code representing input video without retraining encoder
  - **Quick check question:** If you run DDIM inversion on an image and then immediately denoise it with the exact same prompt and seed, what should the output look like?

- **Concept: ControlNet / Adapter Architectures**
  - **Why needed here:** Essential for understanding how structural fidelity is enforced through zero-convolution or parallel branches
  - **Quick check question:** How does a ControlNet modify the forward pass of a standard diffusion model without altering pre-trained weights?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** Critical for balancing original content preservation with new photorealistic style adoption
  - **Quick check question:** How does the CFG scale mathematically combine conditional and unconditional noise predictions, and what happens to output diversity as CFG increases?

## Architecture Onboarding

- **Component map:** Input Processor (Depth/Segmentation/Edges + VideoLLaMA3 prompts) -> Inversion Engine (DDIM loop to xT) -> Generator (Cosmos-Transfer1 DiT + DiT-ControlNet with photorealistic prompt)
- **Critical path:** The Inversion Process is most fragile - misaligned inversion steps or prompts can cause latent xT to fail capturing video dynamics, leading to hallucination or structure loss
- **Design tradeoffs:**
  - CFG Scale: High CFG (11) maximizes photorealism but loses semantic alignment; Low CFG (3) preserves content but looks synthetic; paper settles on CFG=7
  - Inversion vs Random Noise: Inverted latent preserves layout but constrained by source imperfections; random noise offers better texture variety but loses temporal coherence
- **Failure signatures:**
  - Semantic Drift: Traffic light color changes or garbled signs (CFG too high or ControlNet weight too low)
  - Temporal Flickering: Object jittering between frames (temporal module failing or unstable inversion)
  - Artifacts: Visual noise or deformations (prompt conflict between inversion and positive prompts)
- **First 3 experiments:**
  1. Inversion Consistency Check: Single frame inversion and denoising with inversion prompt to verify content preservation
  2. Prompt Ablation: Full clip using same prompt for both stages, then swap to photorealistic prompt to isolate style effect
  3. Safety-Critical Object Test: Traffic light sequence (Redâ†’Green) to verify color state preservation via DINO/CLIP metrics

## Open Questions the Paper Calls Out

### Open Question 1
Does training autonomous driving systems on videos enhanced by this zero-shot method improve downstream task performance compared to using raw synthetic data? The paper explicitly states future work will validate downstream utility and determine if enhanced synthetic data can bridge the synthetic-to-real gap for training autonomous systems.

### Open Question 2
How can temporal consistency be maintained for driving sequences longer than the base model's 121-frame inference window? The limitations section notes the method is constrained by the fixed inference window, requiring chunk-based approaches that can introduce temporal discontinuities at boundaries.

### Open Question 3
How can the framework be made robust to text prompts that semantically conflict with the source video content? The authors note sensitivity to conflicting text prompts that may produce visual artifacts, but provide no current mechanism to mitigate contradiction between realistic positive prompts and synthetic content inversion prompts.

## Limitations
- Zero-shot approach relies heavily on quality of pre-trained models and auxiliary components, inheriting potential biases
- 121-frame maximum processing limit may create temporal artifacts when processing longer driving sequences
- Requires multiple auxiliary models (Depth Anything V2, SAM2, GroundingDINO) increasing system complexity

## Confidence
- **High Confidence:** Structural preservation via DDIM inversion and ControlNet conditioning is well-supported by ablation studies and quantitative metrics
- **Medium Confidence:** Photorealism enhancement claims supported by win-rate comparisons but lack perceptual studies with human raters
- **Low Confidence:** Paper doesn't adequately address failure modes when auxiliary models produce inaccurate spatial maps or explore robustness across diverse synthetic video generators

## Next Checks
1. **Temporal Coherence Test:** Process a 5-minute CARLA driving sequence exceeding 121 frames to evaluate chunk-boundary artifacts and temporal stability
2. **Cross-Simulator Generalization:** Apply framework to synthetic videos from different simulators (GTA V, AirSim) to assess zero-shot capability across rendering engines
3. **ControlNet Robustness Evaluation:** Systematically degrade spatial map quality and measure degradation in both structural preservation and photorealism to establish failure thresholds