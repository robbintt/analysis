---
ver: rpa2
title: 'HealthBench: Evaluating Large Language Models Towards Improved Human Health'
arxiv_id: '2505.08775'
source_url: https://arxiv.org/abs/2505.08775
tags:
- healthbench
- response
- health
- user
- physicians
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HealthBench is a comprehensive, open-source benchmark for evaluating
  large language models in healthcare, addressing the need for meaningful, trustworthy,
  and unsaturated evaluations. It consists of 5,000 realistic multi-turn conversations
  spanning seven themes and five behavioral axes, with responses graded against physician-created
  rubrics.
---

# HealthBench: Evaluating Large Language Models Towards Improved Human Health

## Quick Facts
- arXiv ID: 2505.08775
- Source URL: https://arxiv.org/abs/2505.08775
- Reference count: 40
- Key outcome: Comprehensive, open-source benchmark evaluating large language models in healthcare across 5,000 realistic multi-turn conversations

## Executive Summary
HealthBench addresses the critical need for meaningful, trustworthy, and unsaturated evaluations of large language models in healthcare applications. The benchmark consists of 5,000 realistic multi-turn conversations spanning seven health themes and five behavioral axes, with responses graded against physician-created rubrics. Over 262 physicians across 60 countries contributed to the benchmark's development, ensuring diverse and comprehensive coverage of healthcare scenarios.

Recent model evaluations show significant improvements in healthcare performance, with GPT-4.1 nano outperforming GPT-4o while being 25x more cost-effective. The benchmark also demonstrates that model-based grading can achieve physician-level agreement, providing a scalable and detailed approach to evaluating AI performance in health contexts. HealthBench Consensus, a validated subset of 34 critical criteria, shows error rates have fallen 4x from GPT-3.5 to GPT-4.1, though challenges remain in more complex scenarios measured by HealthBench Hard.

## Method Summary
HealthBench employs a comprehensive evaluation framework using 5,000 multi-turn healthcare conversations graded against physician-created rubrics. The benchmark covers seven health themes and five behavioral axes, with responses evaluated by over 262 physicians across 60 countries. Model performance is measured through both human grading and model-based grading systems, with the latter achieving physician-level agreement. The framework includes specialized subsets like HealthBench Consensus (34 critical criteria) and HealthBench Hard to measure performance in challenging scenarios.

## Key Results
- GPT-4.1 nano outperforms GPT-4o while being 25x more cost-effective
- o3 achieves 60% accuracy on the benchmark
- Model error rates have fallen 4x from GPT-3.5 to GPT-4.1 based on HealthBench Consensus validation

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of healthcare scenarios and its use of physician-created evaluation criteria. By incorporating multi-turn conversations and diverse health themes, HealthBench captures the complexity of real-world healthcare interactions. The physician involvement ensures that evaluations align with clinical standards and practices, while the large scale (5,000 conversations) provides robust statistical power for measuring model performance across different contexts.

## Foundational Learning
- **Multi-turn conversation modeling**: Essential for capturing the iterative nature of healthcare interactions; quick check: verify models can maintain context across multiple exchanges
- **Behavioral axis evaluation**: Critical for assessing not just factual accuracy but also communication quality and safety; quick check: ensure rubric covers empathy, clarity, and risk awareness
- **Physician-created rubrics**: Necessary for clinical validity and relevance; quick check: validate rubrics through expert consensus and real-world applicability testing
- **Cross-cultural healthcare scenarios**: Important for ensuring global applicability; quick check: verify representation of diverse healthcare systems and cultural contexts
- **Cost-performance tradeoffs**: Key for practical deployment considerations; quick check: compare performance per dollar across model variants
- **Model-based grading validation**: Enables scalable evaluation while maintaining quality; quick check: compare model grading against physician consensus on sample conversations

## Architecture Onboarding
**Component Map**: Conversation Generator -> Physician Rubric Creation -> Model Response Generation -> Dual Grading (Physician + Model) -> Performance Analysis -> HealthBench Subsets
**Critical Path**: Physician-created conversations → model responses → rubric-based evaluation → performance scoring → subset analysis
**Design Tradeoffs**: Comprehensive coverage vs. evaluation complexity; physician grading vs. model grading scalability; detailed criteria vs. practical implementation
**Failure Signatures**: Over-reliance on specific health themes; bias toward certain healthcare systems; performance degradation in complex multi-turn scenarios; inconsistency between human and model grading
**3 First Experiments**: 1) Test model performance on HealthBench Hard subset to identify current limitations; 2) Compare human vs. model grading agreement rates across different health themes; 3) Evaluate cost-performance tradeoffs by testing multiple model variants on identical conversations

## Open Questions the Paper Calls Out
None

## Limitations
- Physician-created rubrics may not fully capture real-world clinical decision-making complexity
- International physician panel may contain cultural and systemic biases
- Benchmark may not adequately represent the full spectrum of healthcare scenarios despite its scale

## Confidence
- **High Confidence**: The benchmark's scale (5,000 conversations) and physician involvement are well-documented and reproducible
- **Medium Confidence**: Model performance comparisons across versions are reliable, but may not fully account for differences in prompting strategies or system-level optimizations
- **Medium Confidence**: The 4x reduction in error rates from GPT-3.5 to GPT-4.1 is supported, but may be influenced by benchmark saturation effects

## Next Checks
1. Conduct independent replication studies using HealthBench with diverse healthcare systems to verify cross-cultural generalizability of model performance
2. Implement blinded clinical expert review of model outputs in high-stakes scenarios to validate rubric-based grading against real-world clinical standards
3. Perform longitudinal tracking of model performance across different healthcare domains to identify potential domain-specific weaknesses not captured by aggregate scores