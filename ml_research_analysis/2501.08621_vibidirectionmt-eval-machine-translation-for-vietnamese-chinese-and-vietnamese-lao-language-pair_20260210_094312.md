---
ver: rpa2
title: 'ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao
  language pair'
arxiv_id: '2501.08621'
source_url: https://arxiv.org/abs/2501.08621
tags:
- translation
- machine
- evaluation
- vietnamese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The VLSP 2022-2023 shared tasks focused on Vietnamese-Chinese and
  Vietnamese-Lao machine translation. Five teams participated in 2022 (Chinese-Vietnamese)
  and seven in 2023 (Lao-Vietnamese), with each team submitting systems evaluated
  on 1,000 test sentence pairs.
---

# ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair

## Quick Facts
- **arXiv ID:** 2501.08621
- **Source URL:** https://arxiv.org/abs/2501.08621
- **Reference count:** 28
- **Key outcome:** VLSP 2022-2023 shared tasks on Vietnamese multilingual translation with 5 teams in 2022 (Chinese-Vietnamese) and 7 teams in 2023 (Lao-Vietnamese), evaluated using BLEU, SacreBLEU, and human evaluation.

## Executive Summary
The VLSP 2022-2023 shared tasks focused on Vietnamese-Chinese and Vietnamese-Lao machine translation, with teams submitting systems evaluated on 1,000 test sentence pairs using automatic metrics and human evaluation by language experts. Five teams participated in 2022 (Chinese-Vietnamese) and seven in 2023 (Lao-Vietnamese), with the winning systems leveraging multilingual pre-trained models, back-translation, and ensemble methods. The evaluation demonstrated significant progress in low-resource Vietnamese multilingual translation and highlighted the importance of human evaluation for reliable benchmarking.

## Method Summary
The tasks involved bidirectional machine translation for Vietnamese-Chinese (VLSP 2022) and Vietnamese-Lao (VLSP 2023) language pairs, with teams submitting systems trained on 300K-100K parallel sentence pairs. Key approaches included fine-tuning multilingual models (mBART-50, M2M-100, mT5), back-translation with monolingual data, and ensemble methods. Systems were evaluated using BLEU, SacreBLEU, and human evaluation by 5 language experts. The winning Vi-Zh system (SDS) used mBART-50 fine-tuned on bilingual data, back-translated TF-IDF-filtered monolingual data, and merged synthetic with original data. The winning Vi-Lo system (BlueSky) adapted mBART for Lao through monolingual pre-training.

## Key Results
- 2022 Chinese-Vietnamese: SDS team won with 71.27% average human evaluation score using mBART-50
- 2023 Lao-Vietnamese: Bluesky won with 52.83% average human evaluation score
- MTA AI achieved highest human score (61.31) for Vietnamese-to-Lao but only 41.88 SacreBLEU
- Human evaluation played decisive role in ranking despite strong automatic metric performance

## Why This Works (Mechanism)

### Mechanism 1: Multilingual Pre-training Transfer (mBART)
Fine-tuning multilingual pre-trained models (mBART-50) provides stronger baselines than training from scratch for low-resource pairs. mBART's denoising pre-training on 50 languages creates cross-lingual representations; fine-tuning on limited bilingual data adapts these representations to specific language pairs without requiring large parallel corpora. Core assumption: The pre-training languages share transferable linguistic representations with Vietnamese, Chinese, and Lao. Evidence anchors: SDS achieved 71.27% human evaluation score using mBART-50; mBART-50 is pre-trained on Vietnamese and Chinese.

### Mechanism 2: Back-Translation with Domain-Filtered Monolingual Data
Generating synthetic parallel data via back-translation, especially from domain-selected monolingual corpora, improves translation quality for low-resource pairs. (1) TF-IDF selects domain-relevant sentences from large monolingual corpora; (2) Initial model translates these to create synthetic pairs; (3) Synthetic + authentic data combined for final training. Core assumption: Synthetic data quality depends on domain relevance and initial model translation quality. Evidence anchors: SDS used "data synthesis and back-translation"; TF-IDF selected 200k sentences per language for synthetic data generation.

### Mechanism 3: Linguistic Similarity Exploitation
Language pairs with high lexical overlap (one-to-one word mappings) achieve better translation quality under low-resource conditions. Sino-Vietnamese words map 1:1 to Chinese; Lao-Vietnamese share numerous cognates. These regularities allow models to learn alignments more efficiently from limited data. Core assumption: Linguistic similarity compensates for data scarcity through more regular alignment patterns. Evidence anchors: Teams leveraged one-to-one mapping between Sino-Vietnamese and Chinese words; substantial similarities between Lao and Vietnamese.

## Foundational Learning

- **Concept: Back-translation**
  - **Why needed here:** Core technique for both winning systems (SDS, MTA AI) to augment 100K-300K parallel sentences with synthetic data.
  - **Quick check question:** Can you explain why translating target-language monolingual text back to source creates useful training signal?

- **Concept: Subword tokenization (BPE/SentencePiece)**
  - **Why needed here:** All teams used BPE or SentencePiece to handle vocabulary; critical for Chinese (no word boundaries) and Lao (limited pre-trained tokenizers).
  - **Quick check question:** Why does BPE with 16,000 operations work better for Chinese than for Vietnamese (4,000 operations)?

- **Concept: Ensemble/checkpoint averaging**
  - **Why needed here:** VBD-MT and JNLP averaged last 5 checkpoints; this reduced variance without additional inference cost.
  - **Quick check question:** How does weight averaging differ from beam search ensembling, and what computational tradeoff does it offer?

## Architecture Onboarding

- **Component map:** Monolingual Corpus (25M Vi, 19M Zh, 1.8GB Vi, 1GB Lo) → [TF-IDF Selection or Full] → Domain-Filtered Sentences → [Back-translation via Initial Model] → Synthetic Parallel Data → [Merge with Authentic Parallel Data] → Augmented Training Corpus → [Fine-tune mBART-50 / Transformer WMT] → Final Translation Model → [Optional: Checkpoint Averaging + Post-processing] → Production System

- **Critical path:** Data selection quality → Synthetic data quality → Fine-tuning stability. Errors propagate; poor selection yields noisy synthetic pairs.

- **Design tradeoffs:**
  - **mBART-50 vs. Training from scratch:** mBART faster to converge, better low-resource performance, but requires GPU memory for large model.
  - **TF-IDF vs. Random selection:** TF-IDF improves domain relevance but risks vocabulary narrowing.
  - **Checkpoint averaging vs. Single checkpoint:** Averaging adds robustness but requires storing multiple checkpoints.

- **Failure signatures:**
  - Synthetic data contains systematic mistranslations → Model amplifies errors (check by sampling synthetic pairs).
  - Domain mismatch between monolingual and test data → BLEU high, human scores low.
  - Vocabulary explosion from improper BPE → OOV errors on test set.

- **First 3 experiments:**
  1. **Baseline:** Fine-tune mBART-50 on VLSP training data only; measure BLEU/SacreBLEU on dev set.
  2. **Back-translation:** Generate 50K synthetic pairs from monolingual data using baseline; retrain; compare scores.
  3. **Domain selection:** Apply TF-IDF to select top 200K sentences before back-translation; measure whether synthetic quality improves (spot-check 100 samples).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did no submitted system improve over the baseline for Vietnamese-to-Chinese translation, despite successful improvements in the Chinese-to-Vietnamese direction?
- Basis in paper: [explicit] "on Vietnamese-to-Chinese direction, none was able to improve the baseline translation: despite a deep analysis, we were unable to find a plausible explanation for this surprising outcome."
- Why unresolved: The authors conducted a deep analysis but could not identify the cause of this asymmetry in translation quality improvements between the two directions.
- What evidence would resolve it: A systematic ablation study comparing source and target language properties, training dynamics, and error analysis across both directions could reveal whether the issue stems from tokenization, data quality, model architecture bias, or linguistic asymmetry.

### Open Question 2
- Question: How can the observed discrepancies between automatic metrics (BLEU/SacreBLEU) and human evaluation scores be explained for Vietnamese-Lao translation?
- Basis in paper: [inferred] MTA AI achieved the highest human score (61.31) for Vi→Lo but only 41.88 SacreBLEU, while BLUESKY had higher SacreBLEU (43.08) but lower human score (51.37).
- Why unresolved: The paper reports both metrics but does not analyze why systems with lower automatic scores sometimes achieve superior human evaluations, suggesting automatic metrics may not correlate well with perceived quality for these low-resource languages.
- What evidence would resolve it: Correlation analysis between automatic metrics and human judgments across all submissions, combined with linguistic error analysis to identify which translation qualities automatic metrics fail to capture.

### Open Question 3
- Question: What is the minimum effective training corpus size for acceptable Vietnamese-Lao machine translation quality?
- Basis in paper: [inferred] Vietnamese-Lao had only 100K training pairs versus 300K for Vietnamese-Chinese, and teams relied heavily on back-translation and synthetic data generation (e.g., MTA AI generated 1.5M synthetic pairs).
- Why unresolved: The paper demonstrates that data augmentation helps but does not establish whether the original 100K pairs are sufficient, or what the optimal ratio of authentic to synthetic data should be.
- What evidence would resolve it: Controlled experiments varying training set sizes from the authentic corpus, with and without synthetic data augmentation, measuring both automatic metrics and human evaluation.

## Limitations
- Human evaluation methodology lacks transparency in inter-rater agreement metrics and evaluation criteria details.
- Winning systems rely on domain-specific monolingual data sources (25M Vietnamese, 19M Chinese sentences) that are not publicly specified, making exact reproduction challenging.
- The 1,000-sentence test sets may not capture long-tail translation phenomena or domain robustness.

## Confidence

- **Multilingual pre-training transfer (mBART-50):** High confidence - multiple teams independently achieved top results using mBART fine-tuning, and the pre-training language coverage (50 languages including Vietnamese and Chinese) is well-documented.
- **Back-translation effectiveness:** Medium confidence - while back-translation was used by winning teams and improved scores, the specific domain filtering approach lacks comparative ablation studies to isolate its contribution.
- **Linguistic similarity advantage:** Low confidence - the claim that Sino-Vietnamese and Lao-Vietnamese cognates drive performance gains is stated but lacks quantitative evidence or controlled experiments isolating similarity effects.

## Next Checks

1. **Replicate baseline mBART fine-tuning:** Train mBART-50 on VLSP training data using identical hyperparameters (max_len=100, batch=16, epochs=4, lr=4e-5) and compare achieved BLEU/SacreBLEU scores against published baselines.
2. **Validate synthetic data quality:** Generate 100 sample synthetic sentence pairs using the baseline model's back-translation output and manually assess translation accuracy to confirm data quality before full integration.
3. **Test domain filtering impact:** Conduct controlled experiment comparing TF-IDF-selected monolingual sentences versus randomly sampled sentences for back-translation, measuring whether domain relevance significantly improves final translation quality.