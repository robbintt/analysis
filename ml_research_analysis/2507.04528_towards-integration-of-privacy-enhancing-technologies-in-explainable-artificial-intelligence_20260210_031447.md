---
ver: rpa2
title: Towards integration of Privacy Enhancing Technologies in Explainable Artificial
  Intelligence
arxiv_id: '2507.04528'
source_url: https://arxiv.org/abs/2507.04528
tags:
- attack
- privacy
- explanations
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates Privacy Enhancing Technologies (PETs) integration\
  \ in feature-based Explainable AI (XAI) to defend against attribute inference attacks.\
  \ The study empirically evaluates three PETs categories\u2014synthetic data generation,\
  \ differentially private training, and noise addition\u2014across three AI pipeline\
  \ stages (pre-model, in-model, post-model) using two XAI subcategories (backpropagation\
  \ and perturbation-based)."
---

# Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence

## Quick Facts
- arXiv ID: 2507.04528
- Source URL: https://arxiv.org/abs/2507.04528
- Reference count: 0
- Primary result: Post-model noise addition to XAI explanations most effectively mitigates attribute inference attacks while preserving model utility and explanation quality

## Executive Summary
This paper evaluates Privacy Enhancing Technologies (PETs) integration in feature-based Explainable AI (XAI) to defend against attribute inference attacks. The study empirically evaluates three PETs categories—synthetic data generation, differentially private training, and noise addition—across three AI pipeline stages (pre-model, in-model, post-model) using two XAI subcategories (backpropagation and perturbation-based). The attribute inference attack on explanations is launched to measure privacy effectiveness, while evaluating explanation quality, model accuracy, and performance overhead. Results show that post-model noise addition is most effective, reducing attack success by up to 49.47% while maintaining model utility and explanation quality. Synthetic data and differentially private training show limited mitigation. The study provides actionable recommendations for PETs integration in XAI systems to balance privacy, explainability, and performance.

## Method Summary
The paper conducts a comprehensive evaluation of PETs at three pipeline stages (pre-model synthetic data generation, in-model differentially private training, post-model noise addition) against attribute inference attacks on XAI explanations. The experimental setup uses four tabular datasets (Adult, Credit, Compas, Hospital) with binary sensitive attributes, four XAI methods (IG, SG, SHAP, LIME), and three PET categories. Attack models are trained to infer sensitive attributes from explanations, measuring attack success (F1-score), explanation faithfulness, model accuracy, and performance overhead. The ablation study tests each PET independently, with post-model noise addition showing the most consistent privacy benefits without utility degradation.

## Key Results
- Post-model noise addition reduces attribute inference attack success by up to 49.47% while maintaining explanation quality
- Synthetic data generation shows inconsistent results, increasing attack success in 52% of cases
- Differentially private training provides limited mitigation, effective in only 44% of attacks
- Post-model noise addition is the most effective defense strategy across all tested configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-model noise addition to explanations is the most effective defense against attribute inference attacks from XAI outputs.
- **Mechanism:** Noise perturbs explanation vectors to create adversarial examples that mislead the attack classifier while preserving explanation faithfulness. Gaussian noise (both random and DP-calibrated) disrupts the mapping `φ(x∪s)→s` that attack models learn, pushing attack success toward random guess levels.
- **Core assumption:** The attack model relies on consistent feature attribution patterns to infer sensitive attributes; noise breaks this pattern-to-attribute mapping without altering the underlying model predictions.
- **Evidence anchors:**
  - [abstract]: "The results show that post-model noise addition is most effective at mitigating attribute inference while maintaining explanation quality and model utility."
  - [section 5.1, Table 11]: "Among the 3 stages of the AI pipeline, post-model was found to be effective with consistent improvement in attack mitigation in 93% of attacks... The maximum decrease in attack success for sensitive attribute 1 was 49% on IG and SHAP."
  - [corpus]: Related work "Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization" mentions SHAP-based privacy preservation, but does not directly validate post-model noise mechanisms.
- **Break condition:** If noise magnitude is too high, explanation faithfulness degrades; if too low, attack success remains elevated. The paper tested ε=1 for calibrated noise but did not systematically explore the noise-utility Pareto frontier.

### Mechanism 2
- **Claim:** Differentially private training (DP-SGD) provides limited protection against attribute inference attacks on explanations.
- **Mechanism:** DP-SGD bounds the influence of any single training record on model parameters, which is effective for membership inference. However, attribute inference exploits correlations between sensitive attributes and model behavior/explanations—not individual record memorization. Lower ε values did not improve resilience.
- **Core assumption:** Attribute inference depends on learned feature-attribute correlations rather than record-specific memorization.
- **Evidence anchors:**
  - [section 5.1, Table 10]: "Lower values of ε, in the strong privacy guarantee tier, which are expected to boost privacy, failed to strengthen the resilience of the system against the attack. Mixed responses were seen, with DP training helping with limited mitigation of the attack in 44% of cases."
  - [section 6.1]: "This can be due to the mechanics of DP, which effectively hides the presence of a single record in a dataset, making it more suitable for mitigating membership inference than attribute inference."
  - [corpus]: No direct corpus validation; related works focus on membership inference rather than attribute inference.
- **Break condition:** DP-SGD may be effective if sensitive attributes are independent of non-sensitive features, but the paper shows low correlation (Pearson <0.3) did not prevent attacks.

### Mechanism 3
- **Claim:** Synthetic training data provides inconsistent and often counterproductive protection against attribute inference.
- **Mechanism:** Synthetic data generators (CTGAN, Gaussian Copula, TVAE) learn statistical distributions from original data. If the generator captures correlations between sensitive and non-sensitive attributes, the trained model inherits these patterns, preserving attack surface. In 52% of cases, synthetic data increased attack success.
- **Core assumption:** Attribute inference attacks exploit statistical patterns rather than exact record matches; synthetic data preserves these patterns.
- **Evidence anchors:**
  - [section 5.1, Table 9]: "Limited mitigation was seen in 48% of the attacks while 52% showed an increase in attack success... The maximum increase in attack success was 27% for LIME on Hospital dataset."
  - [section 6.1]: "Thus models trained on synthetic data are susceptible to attribute inference in XAI as determined in other works on non-XAI systems."
  - [corpus]: "Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review" provides broader context but does not offer counter-evidence on synthetic data effectiveness.
- **Break condition:** If synthetic data generators explicitly decouple sensitive attributes from other features during generation, protection may improve—this was not tested in the paper.

## Foundational Learning

- **Concept:** Differential Privacy (ε, δ)-guarantees
  - **Why needed here:** Understanding why DP-SGD failed requires distinguishing between protecting individual membership (DP's strength) vs. protecting sensitive attribute inference (different threat model).
  - **Quick check question:** If ε=0.01 provides strong membership privacy, why doesn't it prevent inferring that a user is under 40 years old from explanation patterns?

- **Concept:** Feature Attribution Methods (Gradient-based vs. Perturbation-based)
  - **Why needed here:** The paper evaluates IG, SmoothGrad (gradient-based) and SHAP, LIME (perturbation-based). These produce different explanation surfaces with varying attack vulnerabilities—SG and LIME showed higher inherent resilience.
  - **Quick check question:** Why would SmoothGrad be more resilient to attribute inference than Integrated Gradients when both use gradient information?

- **Concept:** Attribute Inference vs. Membership Inference
  - **Why needed here:** These are fundamentally different attack types with different defense requirements. The paper's central finding is that defenses effective for one may fail for the other.
  - **Quick check question:** An adversary knows a user's purchase history (non-sensitive) and wants to infer their age (sensitive). Is this membership or attribute inference?

## Architecture Onboarding

- **Component map:** [Training Data] → [Model Training] → [Trained Model] → [XAI Generator] → [Explanations]
         ↓                   ↓                                     ↓
    Pre-model PET      In-model PET                         Post-model PET
    (Synthetic Data)   (DP-SGD)                             (Noise Addition)

- **Critical path:** For privacy-preserving XAI deployment:
  1. Train model normally (no DP-SGD unless membership inference is also a concern)
  2. Generate explanations using chosen XAI method
  3. Apply Gaussian noise to explanation vectors before releasing
  4. Monitor faithfulness metrics to ensure explanation quality

- **Design tradeoffs:**
  | PET Stage | Privacy Benefit | Utility Impact | Performance Cost |
  |-----------|-----------------|----------------|------------------|
  | Pre-model (synthetic) | Inconsistent | -1% to -12% accuracy | High (generation time) |
  | In-model (DP-SGD) | Low | -6% to -37% accuracy | Medium (gradient clipping) |
  | Post-model (noise) | High (up to 49%) | None | Low (random) / Medium (calibrated) |

- **Failure signatures:**
  - Attack F1-score remains above baseline → noise magnitude insufficient or wrong distribution
  - Faithfulness correlation drops significantly → noise too aggressive
  - Model accuracy degrades → accidentally applied noise to predictions instead of explanations only

- **First 3 experiments:**
  1. **Baseline attack measurement:** Train target model, generate SHAP explanations on test set, run attribute inference attack. Record attack F1-score and faithfulness correlation.
  2. **Post-model Gaussian noise sweep:** Apply random Gaussian noise at 5 different standard deviations (0.01, 0.05, 0.1, 0.2, 0.5). Plot attack F1-score vs. faithfulness to find optimal noise level.
  3. **XAI method comparison:** Repeat experiment 2 with LIME. Verify paper's finding that Laplace noise works better with LIME while Gaussian works better with SHAP/IG/SG.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the combination of multiple Privacy Enhancing Technologies (PETs) affect the trade-off between privacy protection, model utility, and explanation quality?
- **Basis in paper:** [explicit] Section 6.5 recommends evaluating the impact of multiple PETs to maximize benefits, noting that hybrid approaches may adversely affect system properties.
- **Why unresolved:** The authors conducted an ablation study introducing only one PET at a time and did not evaluate the interaction effects of combining methods (e.g., synthetic data plus noise addition).
- **What evidence would resolve it:** Empirical results from experiments utilizing combinations of the proposed PETs (pre-model, in-model, and post-model) on the same privacy, utility, and explainability metrics.

### Open Question 2
- **Question:** Are post-model noise addition strategies effective at mitigating privacy leakage in non-feature-based XAI methods (e.g., example-based or concept-based) and against other attack vectors like model extraction?
- **Basis in paper:** [explicit] The Conclusion states that future work could extend the evaluation to other XAI types and attack categories beyond the feature-based methods and attribute inference attacks studied.
- **Why unresolved:** The study restricted its scope to four specific feature-based XAI methods (IG, SG, SHAP, LIME) and a single attack type (attribute inference).
- **What evidence would resolve it:** Experimental data showing the success rate of different privacy attacks on non-feature-based explanations with and without the proposed PET integrations.

### Open Question 3
- **Question:** How does the integration of noise and differential privacy impact the robustness and complexity of explanations, which are critical components of explanation quality?
- **Basis in paper:** [inferred] Section 5.5 (Limitations) explicitly notes that other XAI metrics such as complexity and robustness were not evaluated, despite the authors' emphasis on maintaining explanation quality.
- **Why unresolved:** The paper only measured faithfulness metrics (correlation, estimate, sufficiency) to assess explanation quality, leaving the impact on other dimensions unknown.
- **What evidence would resolve it:** Quantitative analysis of robustness (e.g., stability under input perturbation) and complexity scores for explanations generated under the various PET configurations.

### Open Question 4
- **Question:** Do the defense mechanisms, particularly post-model noise addition, generalize effectively to non-tabular data domains (e.g., images or text) and non-binary sensitive attributes?
- **Basis in paper:** [inferred] Section 5.5 (Limitations) identifies the scope as being restricted to tabular datasets and binary sensitive attributes.
- **Why unresolved:** The characteristics of noise and attribute correlations may differ significantly in high-dimensional unstructured data or continuous attributes compared to the tabular datasets tested.
- **What evidence would resolve it:** Replication of the experimental pipeline using image or text datasets containing continuous or multi-class sensitive attributes.

## Limitations
- Focus on tabular datasets and binary sensitive attributes limits generalizability to non-tabular and continuous data
- No evaluation of adaptive attacks that could optimize against noise-based defenses
- Limited exploration of XAI metrics beyond faithfulness (complexity, robustness not measured)

## Confidence
- **High:** Post-model noise addition effectiveness (empirical results directly support this claim)
- **Medium:** Synthetic data limitations (results show inconsistency but lack exploration of alternative generation strategies)
- **Low:** DP-SGD ineffectiveness for attribute inference (the mechanism is theoretically sound but limited experimental exploration of privacy-utility tradeoffs)

## Next Checks
1. **Noise calibration sweep:** Systematically test multiple noise magnitudes across different distributions to identify optimal privacy-utility tradeoff points
2. **Alternative synthetic data methods:** Evaluate specialized generators designed to decouple sensitive attributes from other features during generation
3. **Adaptive attack evaluation:** Implement attack models that are robust to Gaussian/Laplace noise to assess whether current defenses are vulnerable to more sophisticated adversaries