---
ver: rpa2
title: 'LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body
  Imaging'
arxiv_id: '2502.20985'
source_url: https://arxiv.org/abs/2502.20985
tags:
- segmentation
- lesion
- tracking
- medical
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents LesionLocator, the first 4D promptable framework
  for zero-shot lesion segmentation and longitudinal tracking in 3D medical imaging.
  It overcomes the challenge of limited longitudinal datasets by generating synthetic
  multi-timepoint data from single scans via anatomy-informed transformations.
---

# LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging

## Quick Facts
- arXiv ID: 2502.20985
- Source URL: https://arxiv.org/abs/2502.20985
- Reference count: 40
- Key outcome: First 4D promptable framework achieving near-human accuracy for zero-shot lesion segmentation and longitudinal tracking.

## Executive Summary
This work presents LesionLocator, the first 4D promptable framework for zero-shot lesion segmentation and longitudinal tracking in 3D whole-body imaging. It overcomes the challenge of limited longitudinal datasets by generating synthetic multi-timepoint data from single scans via anatomy-informed transformations. LesionLocator leverages large-scale pretraining on diverse anatomical data, followed by fine-tuning on lesion-specific datasets. Evaluated on six held-out tumor segmentation tasks, it outperforms all existing promptable models by nearly 10 dice points, reaching human-level accuracy. For longitudinal tracking, it achieves state-of-the-art performance with a retrieval rate of 86% and mean lesion center error of 3mm. The model's robustness, accuracy, and generalization are validated across multiple lesion types and modalities. Code and synthetic datasets are made publicly available to advance future research in medical imaging.

## Method Summary
LesionLocator is a 4D promptable framework that enables zero-shot lesion segmentation and longitudinal tracking in 3D medical imaging. It consists of a segmentation network (ResEncL UNet) and a prompt propagation network (GradICON-based). The method uses synthetic longitudinal data generation to overcome the scarcity of real multi-timepoint datasets. During tracking, it employs an autoregressive loop where the previous segmentation mask is warped via a deformation field and used as a dense spatial prompt for the next timepoint. The framework is trained through a large-scale pretraining strategy on diverse anatomical data, followed by fine-tuning on lesion-specific datasets. It supports multiple prompt types including points, boxes, and previous masks for both segmentation and tracking tasks.

## Key Results
- Zero-shot lesion segmentation: Achieves ~79 Dice for point prompts and ~84 Dice for box prompts, outperforming existing promptable models by nearly 10 points.
- Longitudinal tracking: State-of-the-art performance with 86% retrieval rate (CPM@25) and 3mm mean lesion center error (MED).
- Generalization: Validated across six held-out tumor segmentation tasks and multiple imaging modalities (CT, MRI, PET).
- Synthetic data contribution: Improves tracking metrics (CPM@25 from 83.62 to 85.40, MED from 5.09 to 3.89).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Propagating prior segmentation masks as prompts for subsequent timepoints significantly improves longitudinal tracking consistency over point propagation.
- **Mechanism:** The framework utilizes an autoregressive loop where the generated mask $\hat{Y}_t$ from time $t$ is warped via a deformation field $\Phi_t$ and fed as a dense spatial prompt for the segmentation network at time $t+1$. This provides shape and size priors that single points lack.
- **Core assumption:** The deformation field $\Phi$ generated by the propagation module aligns consecutive scans accurately enough to make the warped mask a useful prompt.
- **Evidence anchors:** Table 2 shows "Prev. Gen. Seg" achieving the highest Dice (68.31) and CPM (85.96), outperforming point and box propagation.

### Mechanism 2
- **Claim:** Synthetic longitudinal data generation enables robust tracking training that real-world data scarcity would otherwise prevent.
- **Mechanism:** The model creates synthetic timepoints by applying anatomy-informed transformations to single scans. It simulates lesion growth/shrinkment using random amplitude modulation of deformation fields combined with image-level augmentations to mimic scan variability.
- **Core assumption:** The synthetic augmentations capture the statistical distribution of real disease progression and imaging condition changes.
- **Evidence anchors:** Table 2 shows that adding synthetic data improves lesion retrieval (CPM@25) from 83.62 to 85.40 and drops center error (MED) from 5.09 to 3.89.

### Mechanism 3
- **Claim:** A 3D UNet-based architecture with spatially aligned prompts outperforms transformer-based foundation models for volumetric lesion segmentation.
- **Mechanism:** Unlike SAM-based models that map prompts to latent space, this architecture concatenates prompts (points/boxes) as additional input channels at the highest resolution level of a ResEncL UNet. This leverages the convolutional inductive bias to focus capacity on spatial segmentation.
- **Core assumption:** The features of a lesion can be captured effectively via convolution without requiring global attention mechanisms or text-semantic alignment.
- **Evidence anchors:** Table 1 shows the proposed 3D UNet approach achieving ~79 Dice (point) vs. SAM-Med3D at ~48 Dice.

## Foundational Learning

- **Concept: Deformation Fields & Registration**
  - **Why needed here:** The tracking module relies on registering scans ($I_t$ to $I_{t+1}$) to propagate prompts. You must understand how $\Phi$ (displacement fields) moves coordinates from one image space to another.
  - **Quick check question:** If the regularization weight $\lambda$ is too low in $L_{reg}$, what artifact might appear in the warped image? (Hint: folding/non-physical deformations).

- **Concept: Autoregressive Inference**
  - **Why needed here:** The model feeds its own output back as input for the next step.
  - **Quick check question:** How does the model handle the very first scan in a sequence where no previous mask exists? (Hint: Requires user prompt $p_0$).

- **Concept: Prompt Simulation Strategies**
  - **Why needed here:** To train a promptable model, you must simulate user behavior (clicks, boxes) programmatically from ground truth masks.
  - **Quick check question:** Why does the paper apply a "ball-shaped footprint" to point prompts rather than a single pixel?

## Architecture Onboarding

- **Component map:**
  - Segmentation Network (ResEncL UNet) -> Prompt Propagation Network (GradICON) -> Deformation Field $\Phi$ -> Warped Mask/Prior -> Segmentation Output

- **Critical path:**
  1. User provides Prompt $p_0$ for $I_0$.
  2. Seg Net predicts $\hat{Y}_0$.
  3. Reg Net calculates $\Phi_{0 \to 1}$ between $I_0$ and $I_1$.
  4. $\hat{Y}_0$ is warped via $\Phi_{0 \to 1}$ to create prompt $p_1$.
  5. Seg Net predicts $\hat{Y}_1$ using $I_1$ and $p_1$.

- **Design tradeoffs:**
  - **UNet vs. Transformer:** Chose UNet for spatial alignment and efficiency; trades off the global context handling of Transformers.
  - **Synthetic vs. Real Data:** Chose synthetic scaling to overcome data scarcity; trades off potential domain shift if augmentations are imperfect.

- **Failure signatures:**
  - **Lesion Merging:** Closely located lesions may be tracked as a single entity (mentioned in "Limitations").
  - **False Positives in Tracking:** If the initial segmentation is a false positive, the tracker will propagate it through time.

- **First 3 experiments:**
  1. **Ablate Prompt Types:** Run inference on held-out data using only point prompts vs. box prompts vs. mask prompts to replicate Table 1/2 performance gaps.
  2. **Validation of Synthetic Data:** Train the tracking module using *only* real data vs. *only* synthetic data vs. mixed to isolate the contribution of the augmentation pipeline.
  3. **Stress Test Registration:** Evaluate tracking on scans with extreme patient rotation or breathing artifacts to identify break conditions in the deformation field $\Phi$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework accurately distinguish and track lesions that merge or become closely adjacent over time without manual re-identification?
- Basis in paper: The "Limitations & Future Work" section states that "accurate distinction of closely located or merging lesions remains a challenge."
- Why unresolved: The current architecture struggles with instance separation in complex progression scenarios, necessitating manual intervention (re-clicking) to maintain identity.
- What evidence would resolve it: A benchmark evaluation on longitudinal datasets specifically annotated for merging lesions, showing high instance segmentation accuracy without re-prompting.

### Open Question 2
- Question: Does utilizing a manual segmentation mask as the initial prompt (instead of a point or box) significantly improve longitudinal tracking performance?
- Basis in paper: The authors propose "replacing the point prompt on the initial scan by a manual segmentation" as a future possibility to integrate existing segmentations.
- Why unresolved: It is currently unknown if the added shape information provided by a full mask at time zero translates to better propagation accuracy than sparse prompts.
- What evidence would resolve it: An ablation study comparing tracking Dice and Center Point Matching (CPM@25) scores when using ground-truth masks versus sparse prompts at the initial timepoint.

### Open Question 3
- Question: How robust is the longitudinal tracking performance on non-CT modalities such as MRI and PET?
- Basis in paper: The paper notes the model is "modality-independent" but evaluation is "demonstrated primarily on CT" due to dataset availability.
- Why unresolved: While pretraining included diverse modalities, the specific validation of the tracking module (melanoma dataset) relied on CT, leaving zero-shot MRI/PET tracking efficacy unproven.
- What evidence would resolve it: Evaluation of the tracking metrics (MED, CPM) on a held-out, multi-timepoint MRI or PET dataset using the current model weights.

## Limitations
- Tracking performance relies heavily on synthetic data generation due to limited availability of real longitudinal datasets.
- The framework may struggle with closely located lesions, potentially merging them into a single entity during tracking.
- Significant anatomical changes between timepoints (e.g., surgical resections) could break the deformation field alignment.

## Confidence
- **High Confidence:** Zero-shot segmentation performance claims are well-supported by extensive evaluations across six held-out tumor segmentation tasks.
- **Medium Confidence:** Longitudinal tracking performance relies significantly on synthetic data augmentation, requiring further validation on diverse real-world clinical scenarios.
- **Low Confidence:** "Human-level accuracy" claim for segmentation is difficult to verify without access to human expert performance baselines.

## Next Checks
1. **Ablate Synthetic Data Contribution:** Train the tracking module using only real longitudinal data versus only synthetic data versus the combined approach to quantify the actual contribution of synthetic augmentation.
2. **Cross-Dataset Generalization:** Evaluate the framework on a held-out longitudinal dataset from a different institution or modality than those used in training to assess robustness to domain shifts.
3. **Error Analysis on Challenging Cases:** Systematically analyze failure cases where the model produces large tracking errors, specifically focusing on scenarios with significant anatomical changes or highly variable lesion appearances.