---
ver: rpa2
title: 'TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution'
arxiv_id: '2508.05616'
source_url: https://arxiv.org/abs/2508.05616
tags:
- trajectory
- prediction
- velocity
- heuristics
- heuristic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrajEvo is a framework that uses Large Language Models within an
  evolutionary algorithm to automatically design trajectory prediction heuristics.
  It generates interpretable, computationally efficient heuristics that outperform
  traditional handcrafted methods and generalize better than deep learning models
  to out-of-distribution data.
---

# TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution

## Quick Facts
- arXiv ID: 2508.05616
- Source URL: https://arxiv.org/abs/2508.05616
- Reference count: 40
- Primary result: LLM-driven evolutionary algorithm generates trajectory prediction heuristics that outperform both handcrafted methods and deep learning models on out-of-distribution data while requiring minimal inference time.

## Executive Summary
TrajEvo is a framework that uses Large Language Models within an evolutionary algorithm to automatically design trajectory prediction heuristics. It generates interpretable, computationally efficient heuristics that outperform traditional handcrafted methods and generalize better than deep learning models to out-of-distribution data. The framework employs Cross-Generation Elite Sampling to maintain population diversity and a Statistics Feedback Loop to guide heuristic refinement based on empirical performance. Evaluated on real-world datasets, TrajEvo achieves significant performance improvements, with over 20% better results on unseen out-of-distribution data compared to both heuristic and deep learning baselines, while requiring minimal inference time.

## Method Summary
TrajEvo employs an evolutionary algorithm where Large Language Models act as genetic operators to generate and refine trajectory prediction heuristics. The framework evolves code that takes observed trajectories as input and outputs K=20 future predictions. It uses Cross-Generation Elite Sampling to maintain diversity by selecting mutation candidates from a historical archive, and a Statistics Feedback Loop that analyzes which prediction indices perform best to guide heuristic refinement. The system optimizes for MSE during evolution and evaluates with minADE/minFDE metrics on ETH-UCY and SDD datasets.

## Key Results
- Achieved over 20% better performance on unseen out-of-distribution data compared to both heuristic and deep learning baselines
- Generated heuristics require minimal inference time while maintaining high accuracy
- Demonstrated superior generalization compared to specialized deep learning models when evaluated on datasets not seen during evolution

## Why This Works (Mechanism)

### Mechanism 1: LLM as Genetic Operators for Code Evolution
TrajEvo substitutes traditional random mutation operators with Large Language Models to perform semantically meaningful code modifications. The system prompts an LLM to act as a "Generator" and "Reflector," ingesting parent heuristic code and task descriptions to produce offspring code. This process uses short-term reflections (comparing pairs of code) and long-term reflections (identifying effective design patterns) to guide crossover and mutation. The core assumption is that the LLM possesses sufficient coding ability and domain reasoning to propose functional changes that strictly improve the objective function rather than introducing syntax errors or semantic regressions.

### Mechanism 2: Cross-Generation Elite Sampling (CGES)
Maintaining a history archive of high-performing elites across all generations mitigates premature convergence to local optima. Instead of selecting mutation candidates solely from the current population, CGES samples from a historical archive using a Softmax distribution based on fitness. This reintroduces "forgotten" successful strategies into the gene pool, allowing the LLM to remix them with current best practices. The core assumption is that search landscapes for trajectory heuristics are rugged and diverse strategies from earlier generations remain valid building blocks even if they weren't the global best at the time.

### Mechanism 3: Statistics Feedback Loop (SFL)
Providing the LLM with fine-grained statistical feedback on why a heuristic performed well enables targeted refinement of multi-modal strategies. The framework analyzes the K=20 trajectory samples to see which "index" (strategy) most frequently achieved the minimum ADE. This distribution is fed back to the LLM as context, explicitly guiding it to reinforce successful strategies and prune ineffective ones. The core assumption is that the "diversity" of the K=20 samples is achieved by distinct internal logic blocks within the code, and the LLM can map the statistical success of an output index back to the specific code block responsible for it.

## Foundational Learning

- **Evolutionary Algorithms (EA) & Genetic Operators**: TrajEvo wraps an LLM inside an EA loop. You must understand population management, elitism, crossover (combining parents), and mutation (modifying elites) to troubleshoot why the search might be stagnating. Quick check: Can you explain the difference between "elitism" (keeping the best) and "crossover" (combining traits)?

- **Trajectory Prediction Metrics (minADE/minFDE)**: These are the fitness functions driving the evolution. The system predicts K=20 futures and takes the *minimum* error. Understanding this is vital for diagnosing why the SFL suggests adding "diversity" strategies. Quick check: If a model predicts 20 identical trajectories with perfect accuracy for one agent but fails on another, what is the minADE?

- **Prompt Engineering for Code Generation**: The mechanism relies on specific prompt templates (System Prompts, Reflection Prompts) to coax usable Python code from the LLM. A failure in the architecture is often a failure of the prompt context window. Quick check: How would you format a prompt to force an LLM to output *only* executable Python code without markdown backticks?

## Architecture Onboarding

- **Component map**: Manager/Controller -> LLM Interface (Generator, Reflector, Mutator) -> Evaluator (sandboxed Python) -> Archive (CGES) -> Feedback Module (SFL) -> LLM Interface

- **Critical path**: 1. Seed: Initialize population with CVM-S code variants. 2. Evaluate: Run code on dataset → get MSE & Trajectory Stats. 3. Select: Pick parents (Random + Elite). 4. Reflect: LLM compares parents + SFL stats → Text summary. 5. Generate: LLM creates offspring code based on summary. 6. Iterate: Repeat until max evaluations (e.g., 60).

- **Design tradeoffs**: In-dist vs. OOD: The paper optimizes for MSE on a split of ETH-UCY, claiming this yields better OOD results than deep learning, but optimizing strictly for training set MSE risks overfitting. Speed vs. Stability: The paper uses Gemini 2.0 Flash (fast/cheap). A "smarter" model (e.g., GPT-4o) might write better code but increases evolution cost significantly (though still cheaper than training NNs).

- **Failure signatures**: Syntax Loop: LLM generates code with import errors or shape mismatches repeatedly. Collapse: All heuristics in the population become identical (CGES failure). Reward Hacking: The heuristic learns to predict zero movement (minimizing MSE for standing agents) but fails on moving ones.

- **First 3 experiments**: 1. Sanity Check: Run the framework with a trivial seed (Constant Velocity) and verify it generates a "Constant Velocity with Noise" variant successfully. 2. Ablation (CGES): Disable the history archive (sample only current elites) and plot convergence speed vs. final MSE to validate the diversity claim. 3. OOD Validation: Take the final evolved heuristic from the ETH dataset and run it directly on the SDD dataset (as per Table 3) to confirm the generalization gap over deep learning baselines.

## Open Questions the Paper Calls Out

- **Can the framework integrate multi-modal inputs like semantic maps to improve heuristic context-awareness?**: Current reliance on positional history limits real-world applicability. Evidence to resolve: Heuristics evolved with rich sensor data outperforming positional-only baselines in complex environments.

- **Does optimizing heuristics for downstream tasks like navigation improve real-world utility over standard prediction metrics?**: Standard metrics (ADE/FDE) may not perfectly correlate with success in robotic tasks like planning. Evidence to resolve: Improved navigation outcomes when using heuristics optimized via closed-loop task-specific objectives.

- **Can advanced evolutionary search techniques close the in-distribution accuracy gap with specialized deep learning models?**: There is an inherent complexity trade-off; heuristics evolved for speed may lack the expressivity of large neural networks. Evidence to resolve: Evolved heuristics matching SOTA neural network error rates on in-distribution benchmarks.

## Limitations

- Unknown total number of evolutionary generations and early-stopping criteria critically impacts reproducibility and comparison to other methods
- Dataset preprocessing pipeline and exact evaluation procedure remain underspecified, creating potential implementation discrepancies
- Heuristics do not consistently achieve the lowest error metrics of specialized deep learning models on in-distribution benchmarks

## Confidence

- **High Confidence**: LLM-as-genetic-operator mechanism - well-supported by abstract and methodology section with clear evidence of prompt engineering for code generation
- **Medium Confidence**: Cross-Generation Elite Sampling - moderately supported though direct corpus evidence for this specific technique is weak
- **Medium Confidence**: Statistics Feedback Loop - described in detail but assumption about K=20 samples representing distinct internal logic blocks may not always hold

## Next Checks

1. Reproduce basic evolution: Run the framework with the Constant Velocity seed to verify it generates valid offspring heuristics with improved MSE

2. Validate CGES impact: Perform an ablation study disabling the history archive to quantify its effect on population diversity and final performance

3. Confirm OOD generalization: Evaluate the evolved heuristic on SDD dataset to verify the claimed 20% improvement over deep learning baselines for out-of-distribution data