---
ver: rpa2
title: 'OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating
  System Verification'
arxiv_id: '2504.20964'
source_url: https://arxiv.org/abs/2504.20964
tags:
- kernel
- specification
- system
- llms
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSVBench introduces a benchmark for evaluating LLMs on the task
  of generating formal specifications for verifying the functional correctness of
  operating system kernels. The benchmark consists of 245 tasks derived from the Hyperkernel
  project, each requiring LLMs to synthesize state-machine specifications based on
  system call descriptions and potentially buggy code implementations.
---

# OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification

## Quick Facts
- arXiv ID: 2504.20964
- Source URL: https://arxiv.org/abs/2504.20964
- Reference count: 40
- 12 state-of-the-art LLMs achieve at most 55.1% pass rate on generating formal OS kernel specifications

## Executive Summary
OSVBench introduces a benchmark for evaluating large language models on generating formal specifications for verifying the functional correctness of operating system kernels. The benchmark formulates specification generation as a program synthesis problem with explicit verification assumptions, fixed declarative specifications, and a deterministic synthesis domain. Evaluation of 12 models reveals limited performance, with best models achieving 55.1% pass rate, and significant degradation when buggy kernel implementations are introduced.

## Method Summary
The benchmark consists of 245 tasks derived from the Hyperkernel project, each requiring LLMs to synthesize state-machine specifications based on system call descriptions and potentially buggy code implementations. Tasks are reformulated as program synthesis problems with explicit verification assumptions documenting hardware behavior, fixed declarative specifications focusing attention on state-machine generation, and a deterministic synthesis domain with known constants and Z3 primitives. Each task prompt includes 1-5 few-shot examples demonstrating the mapping from functional description to specification syntax. Generated specifications are evaluated using the Hyperkernel verifier which performs symbolic execution on LLVM IR and invokes Z3 theorem prover to compare against oracle specifications.

## Key Results
- Best model (GPT-4o) achieves 55.1% pass rate on 245 tasks
- Performance degrades significantly with injected bugs: 88.3% of buggy tasks yield lower performance
- No reasoning models consistently outperform traditional instruction-following models despite their capabilities
- Long-context limitations cause performance degradation, especially for smaller models and reasoning models with lengthy traces

## Why This Works (Mechanism)

### Mechanism 1
Reformulating specification generation as a constrained program synthesis problem improves LLM tractability. The benchmark provides three structural constraints: explicit verification assumptions documenting hardware behavior and memory layout, fixed declarative specifications so LLMs focus only on state-machine generation, and a deterministic synthesis domain with known constants, utility functions, and Z3 primitives. These constraints reduce the search space from unconstrained formal logic to a guided code synthesis task within a defined abstract syntax.

### Mechanism 2
Providing few-shot syscall examples with complete specification-implementation pairings enables in-context learning for this specialized domain. Each task prompt includes 1-5 examples containing functional descriptions, code implementations, and oracle specifications. The examples demonstrate the mapping between natural language semantics, concrete code, and state-machine specification syntax. Performance improves monotonically with more examples (0-shot yields 0% success; 5-shot yields best results across all tested models).

### Mechanism 3
Verification-based evaluation using an automated theorem prover provides objective correctness ground truth without human annotation. Generated specifications are fed to the Hyperkernel verifier alongside a declarative specification and kernel implementation. The verifier performs symbolic execution on LLVM IR and invokes Z3. A generated spec is correct iff it produces identical verification results to the oracle spec across all (potentially buggy) kernel implementations.

## Foundational Learning

- **Concept: State-machine specification of OS kernels**
  - Why needed here: OSVBench models kernel execution as state transitions. The LLM must generate Python functions that take an `old_state` and input parameters, apply conditional logic, and return a `new_state`.
  - Quick check question: Given a syscall that closes a file descriptor, what state fields must change between `old` and `new`, and what conditions guard the transition?

- **Concept: Refinement and invariant verification**
  - Why needed here: The benchmark evaluates specs via two theorems: (1) implementation refines the spec, (2) all reachable states satisfy declarative invariants.
  - Quick check question: If a generated specification correctly models state transitions but violates an invariant (e.g., allows a free page to have an owner), which theorem fails, and what error type would the verifier report?

- **Concept: Long-context code generation**
  - Why needed here: Each task prompt is 20k-30k tokens. Performance degrades significantly when context exceeds model capacity.
  - Quick check question: Why might a reasoning model (o1, DeepSeek-R1) underperform a standard instruction-following model (GPT-4o, DeepSeek-Chat) on 30k-token tasks despite superior reasoning capabilities?

## Architecture Onboarding

- **Component map:**
  - Hyperkernel codebase (18k lines, 49 syscalls) -> Programming model (Python classes/constants) -> Prompt constructor (system prompt + verification assumptions + few-shot examples + task question) -> LLM (generates state-machine specification) -> Verifier (compiles spec to SMT, symbolic executes kernel LLVM IR, invokes Z3) -> Evaluation (compares verification results against oracle)

- **Critical path:**
  1. Read and internalize programming model (class hierarchy, available constants, Z3 functions)
  2. Parse functional description and code implementation from task question
  3. Map semantic intent to specification syntax (conditions → `z3.And`/`Or`, state updates → field assignments)
  4. Handle divergent states (nested conditionals producing multiple possible new states)
  5. Output valid Python function matching abstract syntax

- **Design tradeoffs:**
  - Fixed declarative specs vs. joint generation: Trading flexibility for automatic evaluability; prevents false positives from weak invariants
  - Bug injection: Realistic but may confuse models if bugs contradict functional descriptions
  - Python/Z3 DSL vs. native verification languages: Lowers barrier for code-trained LLMs but requires runtime translation to SMT

- **Failure signatures:**
  - **Syntax errors**: Accessing non-existent class fields (e.g., `old.pcipages[pcipn].devid` when model has no `devid`), incorrect Z3 function usage
  - **Semantic errors**: Correct syntax but wrong conditions (e.g., `perm & PTE_W == 0` instead of `!= 0`), often caused by following buggy code implementation rather than functional description
  - **Context overflow**: Near-zero performance on smaller models (Llama-3.1-8B: 10.61%, Qwen2.5-Coder-7B: 4.90%) and reasoning models with long traces (o1: 23.67%)

- **First 3 experiments:**
  1. **Baseline with Doubao-1.5-pro or DeepSeek-Chat**: Run 5-shot evaluation on full 245 tasks; measure Pass@1, syntax/semantic error breakdown; this establishes your reference point.
  2. **Bug sensitivity analysis**: Group results by bug type (incorrect pointer, privilege, memory leak, buffer overflow, bounds checking) and bug count (1-5); identify which vulnerability classes cause the largest performance drops.
  3. **Self-repair loop**: Implement the two-round repair protocol from Section 4.3 (feed back error messages from verifier); measure repair success rate per round and characterize which error types (syntax vs. semantic) are repairable.

## Open Questions the Paper Calls Out

- **Generalization to other OS kernel architectures**: Can the specification generation capabilities validated on OSVBench generalize to other operating system kernel architectures beyond Hyperkernel? The current benchmark is confined to a single kernel architecture and verification toolchain, leaving the transferability of learned skills to other verification frameworks unknown.

- **Reasoning model underperformance**: Why do advanced reasoning models consistently underperform compared to traditional instruction-following models in this long-context verification domain? The paper identifies the performance gap but does not isolate whether the failure is due to context window saturation, attention distraction from long reasoning traces, or the specific nature of logical deduction required for formal verification.

- **Human usability of generated specifications**: To what extent do the generated state-machine specifications possess the readability and maintainability required for human-in-the-loop verification? Current evaluation metrics focus solely on functional correctness, potentially missing qualitative aspects such as readability and adaptability.

## Limitations

- **Verification environment portability**: The benchmark's reliance on Hyperkernel's specific verification toolchain creates significant barriers to replication with modern tooling.
- **Model access and cost**: Key evaluations use closed-source models without specifying API quotas, rate limits, or cost structures, affecting reproducibility.
- **Ground truth validity**: While verification-based evaluation avoids human annotation, it inherits any errors in the oracle specifications or declarative invariants.

## Confidence

- **High confidence**: Task formulation as program synthesis with explicit constraints, monotonic performance gains from few-shot examples, and the general trend of long-context degradation across models.
- **Medium confidence**: The quantitative Pass@1 scores and error-type distributions, as these depend on the unverified verification environment and potential ground truth issues.
- **Low confidence**: Direct comparisons between reasoning and non-reasoning models, given their divergent usage patterns and the lack of ablation on trace length or reasoning steps.

## Next Checks

1. **Verification pipeline validation**: Replicate the verification setup using a single, simple syscall (e.g., `pci_read_config`) to confirm the toolchain compiles, verifies, and produces consistent results. This isolates environment issues before scaling to full evaluation.

2. **Oracle spec audit**: Manually inspect 5-10 oracle specifications and their corresponding declarative invariants for logical consistency and completeness. Identify any missing conditions or invariant violations that could cause false negatives.

3. **Context length ablation**: Evaluate a subset of tasks (e.g., 10 short, 10 medium, 10 long prompts) with and without reasoning traces for o1/DeepSeek-R1. Measure the trade-off between reasoning depth and context window capacity to quantify the observed performance degradation.