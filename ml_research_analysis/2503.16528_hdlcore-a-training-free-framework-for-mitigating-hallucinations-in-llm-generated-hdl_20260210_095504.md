---
ver: rpa2
title: 'HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated
  HDL'
arxiv_id: '2503.16528'
source_url: https://arxiv.org/abs/2503.16528
tags:
- code
- generation
- llms
- design
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HDLCoRe, a training-free framework that significantly
  improves LLM-generated HDL code quality through prompt engineering and retrieval-augmented
  generation. The framework addresses the hallucination problem in LLM-generated HDL
  code caused by data scarcity, implementing a two-component approach: (1) an HDL-aware
  Chain-of-Thought prompting technique with self-verification that classifies tasks
  by complexity and type, incorporates domain-specific knowledge, and guides LLMs
  through step-by-step self-simulation for error correction; and (2) a two-stage heterogeneous
  RAG system that extracts key components from task descriptions to address formatting
  inconsistencies and efficiently retrieves relevant HDL examples through sequential
  filtering and re-ranking.'
---

# HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL

## Quick Facts
- arXiv ID: 2503.16528
- Source URL: https://arxiv.org/abs/2503.16528
- Authors: Heng Ping, Shixuan Li, Peiyu Zhang, Anzhe Cheng, Shukai Duan, Nikos Kanakaris, Xiongye Xiao, Wei Yang, Shahin Nazarian, Andrei Irimia, Paul Bogdan
- Reference count: 32
- One-line primary result: Training-free framework achieves up to 28% higher functional pass@1 rates on HDL generation through prompt engineering and retrieval-augmented generation

## Executive Summary
HDLCoRe addresses the critical problem of hallucinations in LLM-generated HDL code by implementing a training-free framework that combines prompt engineering with retrieval-augmented generation. The framework targets the data scarcity problem that causes LLMs to generate functionally incorrect or syntactically invalid HDL code. By classifying tasks based on complexity and type, and using a two-stage heterogeneous RAG system, HDLCoRe significantly improves code quality without requiring model fine-tuning or external tools.

The framework demonstrates substantial improvements particularly for smaller LLMs, effectively narrowing the performance gap between general and code-specialized models. Through a combination of HDL-aware Chain-of-Thought prompting with self-verification and intelligent example retrieval, HDLCoRe achieves up to 28% higher functional pass@1 rates compared to state-of-the-art training-free methods on the RTLLM2.0 benchmark. The approach is particularly effective because it leverages the LLM's internal reasoning capabilities while grounding generation with relevant examples.

## Method Summary
HDLCoRe implements a two-component approach: (1) an HDL-aware Chain-of-Thought prompting technique that classifies tasks by complexity and type, incorporates domain-specific knowledge, and guides LLMs through step-by-step self-simulation for error correction; and (2) a two-stage heterogeneous RAG system that extracts key components from task descriptions and efficiently retrieves relevant HDL examples through sequential filtering and re-ranking. The framework first uses a Python script to classify tasks as combinational or sequential based on signal keywords, then prompts the LLM to self-assess complexity, routing the task to one of four specific CoT strategies. After initial code generation, the LLM performs self-verification by simulating a testbench textually, with a script summarizing results for refinement. The RAG system uses key component extraction and a two-stage retrieval process with embedding models and cross-encoders to provide relevant examples.

## Key Results
- Achieves up to 28% higher functional pass@1 rates compared to state-of-the-art training-free methods on RTLLM2.0 benchmark
- Significantly improves performance of smaller LLMs, narrowing the gap between general and code-specialized models
- Eliminates need for model fine-tuning or external tools while maintaining or improving code quality
- Qwen2.5-14B outperforms DeepSeek-Coder-V2-6.7B on certain tasks without code-specific training

## Why This Works (Mechanism)

### Mechanism 1: Classification-Based Prompting to Reduce Information Overload
The framework tailors Chain-of-Thought prompts based on task complexity and type (combinational vs. sequential) to prevent information overload in smaller models while providing necessary constraints for complex tasks. A Python script identifies signal keywords to classify tasks, which the LLM then self-assesses for complexity. This routing to one of four specific CoT strategies (SC-HDL, SS-HDL, CC-HDL, CS-HDL) with tailored domain knowledge improves HDL generation quality by matching guidance level to task requirements.

### Mechanism 2: Self-Verification via Internal Simulation
The framework leverages the LLM's internal reasoning capabilities to identify and correct functional errors through textual simulation. After generating initial code, the LLM creates a corresponding testbench and simulates it step-by-step, documenting its thought process for each test case. A script summarizes this output, which is fed back to the LLM for code refinement. This approach uses the model's own reasoning to catch errors without requiring external execution tools.

### Mechanism 3: Two-Stage RAG for Example-Guided Generation
The heterogeneous RAG system improves example relevance through a two-stage retrieval process. Three key components (high-level overview, low-level details, module header) are extracted from task descriptions. A fast embedding model retrieves broader candidates, which a powerful cross-encoder then re-ranks for final selection. This sequential approach combines the efficiency of lightweight models with the precision of complex cross-encoders to provide better context for generation than simple similarity search.

## Foundational Learning

- **Hardware Description Languages (HDL) & Digital Logic**: Understanding the difference between combinational logic (output depends only on current input) and sequential logic (output depends on current input and past state/clocks) is critical for the classification-based prompting mechanism. Quick check: If a module's output only changes on the rising edge of a `clk` signal, is it combinational or sequential logic?

- **Chain-of-Thought (CoT) Prompting**: The core technique used to guide the LLM is HDL-aware CoT. Foundational knowledge of standard CoT—prompting a model to reason step-by-step before generating a final answer—is required to understand how this framework extends it with domain-specific knowledge. Quick check: In a standard CoT prompt, what do you ask the model to do before it gives you the final code solution?

- **Retrieval-Augmented Generation (RAG)**: A major component of the system is a two-stage RAG process. Understanding the basic RAG principle—retrieving relevant documents from a database and providing them as context to the LLM—is essential before learning the specific two-stage, multi-component retrieval method proposed. Quick check: What are the two main steps in a basic RAG system when given a user query?

## Architecture Onboarding

- **Component map**: Task Classifier -> Two-Stage RAG Engine -> HDL-Aware Prompting Module -> Self-Verification Unit -> Target LLM
- **Critical path**: The correctness of generated HDL is most sensitive to the Self-Verification Unit, as the entire pipeline depends on the LLM's ability to accurately simulate digital logic states and the summary script's ability to correctly extract errors from verbose simulation text.
- **Design tradeoffs**:
  - Efficiency vs. Quality in RAG: Uses fast embedding model for initial filtering and slower cross-encoder for final re-ranking, trading slight latency increase for significantly better example relevance
  - Simplicity vs. Guidance in CoT: Classification system gives simple tasks minimal guidance to avoid error introduction while complex tasks get extensive guidance, balancing risk of under-guiding complex tasks against over-constraining simple ones
- **Failure signatures**:
  - Incorrect Task Classification: Script misidentifies sequential task as combinational (or vice versa), leading to inappropriate CoT strategy and functional errors
  - Simulation Hallucination: LLM's self-simulation reports "PASS" for a test case that should fail, meaning code is not refined and error persists
  - Irrelevant RAG Examples: Retrieval system returns syntactically similar but semantically different examples, causing LLM to generate code for wrong function
- **First 3 experiments**:
  1. Ablate the RAG Stages: Run framework with (a) no RAG, (b) first-stage embedding retrieval only, and (c) full two-stage retrieval to isolate re-ranking contribution
  2. Test Classification Sensitivity: Manually force incorrect classifications and measure degradation in code quality to quantify importance of classification mechanism
  3. Probe Simulation Accuracy: Compare LLM's self-simulation results against ground-truth tool-based simulation to reveal rate of mental simulation hallucinations

## Open Questions the Paper Calls Out
None identified in the provided content

## Limitations
- Reliance on LLM self-assessment for task complexity introduces dependency on model's metacognitive accuracy without systematic measurement of this assessment's accuracy
- Effectiveness of self-verification depends on LLM's ability to accurately simulate digital logic states without executing actual code, which may degrade for complex sequential circuits
- Performance fundamentally bounded by quality and diversity of underlying HDL datasets, with computational overhead scaling for very large example repositories

## Confidence

- **High Confidence**: Framework's ability to improve functional pass@1 rates compared to baseline methods is well-supported by RTLLM2.0 benchmark results
- **Medium Confidence**: Specific mechanisms (classification-based prompting, self-verification simulation, two-stage RAG) are theoretically sound but lack ablation studies isolating individual component contributions
- **Medium Confidence**: Claim that framework narrows performance gap between general and code-specialized LLMs is supported by results but comparison doesn't account for model size or training data differences

## Next Checks

1. **Self-Assessment Accuracy Measurement**: Implement systematic evaluation comparing LLM's self-assessed task complexity against ground-truth complexity metrics (cyclomatic complexity, number of sequential elements) to quantify classification accuracy rate and impact on downstream performance

2. **Simulation Fidelity Validation**: Create controlled test suite of HDL designs with known functional properties and compare LLM's self-simulation results against actual hardware simulation tool outputs (ModelSim, VCS) to measure false positive/negative rate in self-verification step

3. **RAG Example Relevance Analysis**: For subset of tasks, manually evaluate semantic relevance of top-N examples retrieved by two-stage RAG versus single-stage retrieval to quantify actual improvement in example quality and determine if re-ranking stage provides diminishing returns for certain task categories