---
ver: rpa2
title: 'The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational
  Modeling'
arxiv_id: '2509.22345'
source_url: https://arxiv.org/abs/2509.22345
tags:
- historical
- language
- invective
- data
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the InviTE corpus, a dataset of nearly 2000
  Early Modern English sentences annotated for invective language in Tudor-era religious
  texts. The annotation scheme captures invective targets, types (literal vs.
---

# The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling

## Quick Facts
- arXiv ID: 2509.22345
- Source URL: https://arxiv.org/abs/2509.22345
- Reference count: 12
- One-line primary result: BERT-based models fine-tuned on historical data, especially MacBERTh, outperform larger zero-shot LLMs on invective detection in Early Modern English (F1 up to 0.89).

## Executive Summary
This paper introduces the InviTE corpus, a dataset of nearly 2000 Early Modern English sentences annotated for invective language in Tudor-era religious texts. The annotation scheme captures invective targets, types (literal vs. metaphorical), and annotator confidence, and was developed through an iterative expert process. Computational experiments show that BERT-based models fine-tuned on historical data, especially MacBERTh, outperform larger zero-shot LLMs on invective detection (F1 up to 0.89), underscoring the value of both domain-adapted pretraining and supervised fine-tuning for this historically and contextually nuanced task.

## Method Summary
The study involves curating a corpus of Early Modern English sentences from EEBO, annotating them for invective language through an iterative expert process, and evaluating computational models on the binary classification task. Fine-tuning encoder-based transformers (MacBERTh, BERT-base, XLM-RoBERTa) for 5 epochs with specific hyperparameters, and comparing against zero-shot LLM prompting (Llama 3.1-8B, Qwen2-7B), with performance measured via 10-fold stratified cross-validation and macro-averaged precision, recall, and F1.

## Key Results
- MacBERTh achieves F1 up to 0.89 on invective detection, outperforming BERT-base (0.83 F1) and zero-shot LLMs.
- LLM zero-shot prompting shows class biases: Llama 3 underpredicts invective, Qwen2 overpredicts it.
- Model error rates increase substantially on "less confident" human annotations, validating the utility of confidence scores.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Adapted Pretraining for Historical Language
- **Claim:** A BERT-based model pre-trained on historical English (MacBERTh) is expected to outperform both modern-domain BERT variants and much larger instruction-tuned LLMs on invective detection in Early Modern English texts.
- **Mechanism:** MacBERTh’s pretraining on a corpus of English texts from 1450–1900 likely enables it to capture period-specific orthographic variation, archaic vocabulary, and syntactic structures. This domain-specific linguistic knowledge allows the model to form superior contextual representations for EModE sentences. These representations are then refined for the downstream classification task through supervised fine-tuning.
- **Core assumption:** The primary driver of the performance difference is the model’s exposure to historical language data during pretraining, rather than model scale or architectural differences alone.
- **Evidence anchors:**
  - [abstract] "Computational experiments show that BERT-based models fine-tuned on historical data, especially MacBERTh, outperform larger zero-shot LLMs on invective detection (F1 up to 0.89)."
  - [Section 6.1] "The best results come from MacBERTh, which reaches 0.89 F1. This makes sense as MacBERTh was pre-trained on historical language... BERT-base (0.83 F1) performs slightly worse... likely because it was pretrained on modern English."
  - [Corpus] Weak direct evidence; the paper does not include ablation studies isolating the pretraining data effect. Neighbor papers on historical NLP (e.g., `InteChar`, `Ground Truth Generation for Multilingual Historical NLP`) suggest domain adaptation is a common strategy, but this is not direct proof.
- **Break condition:** If modern-domain models (e.g., BERT-base) or zero-shot LLMs match or exceed MacBERTh’s performance, or if the performance gain disappears on a different historical task, the mechanism is weakened.

### Mechanism 2: Supervised Fine-Tuning for Nuanced Classification
- **Claim:** Fine-tuning encoder-based models on expert-annotated data is posited to yield better classification for this task than zero-shot prompting of larger LLMs.
- **Mechanism:** Fine-tuning updates the model's weights based on the labeled examples, allowing it to learn subtle, context-dependent decision boundaries (e.g., distinguishing devotional self-deprecation from true invective) as defined by the annotation guidelines. In contrast, zero-shot LLMs must rely on their general pretraining and instruction-following capabilities without being exposed to the task's specific definitions and nuanced examples.
- **Core assumption:** The expert annotations reliably capture the target concept, and the dataset, though small, is sufficiently representative for fine-tuning to generalize.
- **Evidence anchors:**
  - [abstract] "underscoring the value of both domain-adapted pretraining and supervised fine-tuning for this historically and contextually nuanced task."
  - [Section 6.1] "The superior performance of the BERT-based models likely reflects the full fine-tuning on the task which allows to internalize the lexical and contextual cues that distinguish invective from non-invective language. In contrast, the LLMs were used in a prompted zero-shot setting..."
  - [Corpus] Weak. No neighbor papers directly compare fine-tuning against zero-shot prompting for this type of historical annotation task.
- **Break condition:** If few-shot prompting or more detailed instructions enable LLMs to match or surpass fine-tuned performance, the necessity of supervised fine-tuning is reduced.

### Mechanism 3: Iterative Expert Annotation for Subjective Phenomena
- **Claim:** An iterative annotation process with domain experts and confidence scoring can produce a reliable dataset for a subjective, historically situated phenomenon, even when inter-annotator agreement is moderate.
- **Mechanism:** An initial annotation phase reveals challenges (e.g., sparse data, category intersections). The scheme is then refined, and data is augmented via targeted keyword searches. Recording annotator confidence provides a signal that disagreements often correspond to genuinely ambiguous cases, validating the data's utility for training models that must handle uncertainty.
- **Core assumption:** Expert domain knowledge is crucial for correct interpretation, and "less confident" labels are genuinely more ambiguous rather than simply erroneous.
- **Evidence anchors:**
  - [abstract] "The annotation scheme...was developed through an iterative expert process."
  - [Section 4.1] "...after the first annotation phase, we found that a consistent annotation is too challenging... Taking these intersections into account would result in an overly complex annotation scheme."
  - [Section 4.4] "Agreement was reached in 1456 cases, where the average confidence...was very high (97%), while disagreement occurred in 219 cases, where average confidence dropped to 87%. This pattern indicates that disagreements are associated with lower annotator certainty..."
  - [Corpus] Weak. Neighbor papers do not directly validate this specific iterative workflow.
- **Break condition:** If model error rates on "confident" and "less confident" examples are comparable, or if a non-expert annotation process yields similar results, the value of the expert-driven iterative process is diminished.

## Foundational Learning

- **Concept: Encoder-only Transformer Models (e.g., BERT)**
  - **Why needed here:** The paper relies on fine-tuning BERT-style models. Understanding that these models produce fixed contextual embeddings for classification, unlike generative LLMs, is critical.
  - **Quick check question:** Can a standard BERT model generate text token-by-token in a zero-shot setting like GPT?

- **Concept: Fine-tuning vs. Zero-shot Prompting**
  - **Why needed here:** The core experimental comparison depends on this distinction. One method updates model weights; the other does not.
  - **Quick check question:** Does providing a natural language instruction in a prompt permanently change the underlying model weights?

- **Concept: Annotation Scheme Operationalization**
  - **Why needed here:** The paper's main contribution is translating a humanities theory (invectivity) into computable labels. Grasping this translation is key to understanding the data.
  - **Quick check question:** Is the annotation scheme in this paper a direct copy of a modern hate speech definition, or is it adapted for a specific historical context?

## Architecture Onboarding

- **Component map:**
  EEBO → Preprocessing → Expert Annotation → Fine-tuned MacBERTh Classifier

- **Critical path:** Raw EEBO text → Preprocessing → Expert Annotation → Fine-tuned MacBERTh Classifier. This path is the paper's validated solution for high performance.

- **Design tradeoffs:**
  1. **Keyword oversampling for balance vs. natural prevalence:** The paper chose to oversample invective sentences to create a balanced dataset for training. **Tradeoff:** This makes the corpus unsuitable for drawing quantitative historical conclusions about the natural prevalence of invective over time (Section 8, Limitations).
  2. **Complex vs. simple annotation scheme:** Initial categories were simplified to improve annotator consistency. **Tradeoff:** Some nuanced distinctions are lost, but feasibility and agreement improved.

- **Failure signatures:**
  1. **LLM Class Bias:** Llama 3 underpredicts invective (high error on invective class), while Qwen2 overpredicts it (high error on non-invective class). This indicates zero-shot LLMs may have strong default biases not aligned with the task.
  2. **High Error on "Less Confident" Data:** Figure 3 shows model error rates increase substantially on "less confident" examples, signaling that model errors track human uncertainty.

- **First 3 experiments:**
  1. **Reproduce Baseline:** Fine-tune MacBERTh on the InviTE corpus using the paper's 10-fold cross-validation setup and confirm its F1 score relative to BERT-base.
  2. **Probe LLM Bias:** Use the paper's zero-shot prompt on a test subset and calculate precision/recall for both classes to confirm the over/underprediction biases.
  3. **Confidence Analysis:** Isolate the "less confident" test sentences and compare the fine-tuned model's error rate on this subset versus the "confident" subset to validate the utility of the confidence signal.

## Open Questions the Paper Calls Out
- To what extent can few-shot prompting strategies improve the performance of instruction-tuned LLMs on the invective detection task compared to the zero-shot baseline?
- Can supervised models effectively predict the fine-grained annotation categories (invective type and target) available in the InviTE corpus?
- How does the keyword-based selection bias impact the generalizability of the trained models to a naturally distributed corpus of Early Modern English texts?

## Limitations
- The corpus is balanced via keyword oversampling and cannot be used to draw quantitative conclusions about the natural prevalence of invective in Tudor-era texts.
- No ablation studies are provided to isolate the effects of domain adaptation versus model scale or architecture.
- The iterative annotation process, while validated by confidence analysis, lacks direct comparison to alternative workflows.

## Confidence
- **High confidence:** The claim that fine-tuning MacBERTh on the InviTE corpus yields high F1 scores (up to 0.89) for invective detection is strongly supported by the reported results and cross-validation setup.
- **Medium confidence:** The claim that supervised fine-tuning outperforms zero-shot prompting for this nuanced task is supported by the comparative results, but the comparison is not exhaustive.
- **Low confidence:** The paper does not provide sufficient evidence to rule out alternative explanations for the performance gap, such as differences in model size or the specific prompt used for LLMs.

## Next Checks
1. **Reproduce Baseline Results:** Fine-tune MacBERTh, BERT-base, and XLM-RoBERTa on the InviTE corpus using the paper's 10-fold cross-validation setup and confirm the reported F1 scores and relative performance.
2. **Probe LLM Bias:** Apply the fixed zero-shot prompt to the test set and calculate per-class precision and recall for both Llama 3.1-8B and Qwen2-7B to confirm the observed under/overprediction biases.
3. **Validate Confidence Signal:** Isolate "less confident" examples from the test set and compare model error rates on this subset versus "confident" examples to verify that model uncertainty aligns with human annotator uncertainty.