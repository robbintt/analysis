---
ver: rpa2
title: 'MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply
  Chain Optimization'
arxiv_id: '2509.06490'
source_url: https://arxiv.org/abs/2509.06490
tags:
- multi-objective
- policy
- policies
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-objective optimization in supply chain
  management, where traditional methods struggle with real-time adaptability and uncertainty.
  It introduces MORSE, a novel framework that combines Reinforcement Learning (RL)
  and Multi-Objective Evolutionary Algorithms (MOEAs) to search the parameter space
  of policy neural networks, generating a Pareto front of policies.
---

# MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization

## Quick Facts
- **arXiv ID**: 2509.06490
- **Source URL**: https://arxiv.org/abs/2509.06490
- **Reference count**: 11
- **Primary result**: MORSE outperforms state-of-the-art methods in multi-objective supply chain optimization by generating a Pareto front of policies that enable dynamic, risk-sensitive decision-making under uncertainty.

## Executive Summary
This paper introduces MORSE, a novel framework that combines Reinforcement Learning and Multi-Objective Evolutionary Algorithms to address the challenge of real-time adaptability and uncertainty in supply chain management. Traditional methods struggle with balancing multiple competing objectives like profit, emissions, and lead time while adapting to disruptions. MORSE evolves a population of neural network policies using NSGA-II to generate a Pareto front, allowing decision-makers to dynamically switch policies based on current objectives and external conditions. The framework incorporates Conditional Value-at-Risk (CVaR) to enhance risk-sensitive decision-making, demonstrating superior adaptability and performance compared to existing approaches.

## Method Summary
MORSE addresses multi-objective supply chain optimization by evolving neural network policies using NSGA-II to search the parameter space directly, generating a Pareto front of diverse policies. The framework evaluates each policy via episodic rollouts across three objectives: profit maximization, emission minimization, and lead time reduction. A key innovation is the integration of CVaR-based fitness to produce tail-risk-aware policies, improving worst-case performance. Decision-makers can dynamically switch among policies from the Pareto front in response to disruptions like emission penalties or geopolitical tensions, enabling real-time adaptation without retraining.

## Key Results
- MORSE outperforms state-of-the-art methods in inventory management, demonstrating superior adaptability to supply chain dynamics and uncertainty.
- CVaR-trained policies show improved worst-case performance (higher profit CVaR, lower emissions/lead-time CVaR) compared to mean-trained policies across 1,000 Monte Carlo simulations.
- Dynamic policy switching under disruption scenarios (emission penalties, geopolitical tensions) improves trade-offs between profit and emissions compared to fixed policies.

## Why This Works (Mechanism)

### Mechanism 1: Evolutionary Search Over Policy Parameter Space
Using MOEAs to directly optimize neural network weights produces a diverse Pareto front of control policies without requiring scalarization or predefined objective weights. NSGA-II maintains a population of policy parameter vectors, evaluates each via episodic rollouts, performs non-dominated sorting and crowding-distance selection, then applies crossover/mutation. This searches the parameter space globally rather than following gradients, producing multiple Pareto-optimal policies in a single evolutionary run. The fitness landscape over policy parameters must be sufficiently regular for evolutionary operators to discover diverse high-quality regions.

### Mechanism 2: Policy Portfolio Enables Real-Time, Post-Hoc Preference Switching
Maintaining a Pareto front of policies allows decision-makers to adapt in real time to shifting priorities or external shocks by selecting the appropriate policy from the portfolio. After MORSE generates the Pareto front, each policy represents a distinct trade-off across profit, emissions, and lead time. When a disruption occurs (e.g., emission tax at timestep 200), the operator switches to a policy with better emissions characteristics without retraining. The Pareto front must be sufficiently dense and policies must generalize to unseen states during disruption for this to work effectively.

### Mechanism 3: CVaR-Based Fitness Produces Tail-Risk-Aware Policies
Replacing expected-return fitness with Conditional Value-at-Risk in the evolutionary loop yields policies with improved worst-case performance at possible cost to mean performance. For each policy, MORSE collects episodic returns per objective, computes empirical VaR and CVaR, and uses CVaR as fitness in NSGA-II. This optimizes tail performance rather than expectation. The number of evaluation episodes must be large enough to estimate CVaR reliably, and the confidence level must correctly reflect operator risk aversion.

## Foundational Learning

- **Concept**: Multi-Objective Optimization and Pareto Dominance
  - **Why needed here**: MORSE produces a Pareto front; interpreting and selecting among non-dominated policies requires understanding trade-offs, dominance, and crowding distance.
  - **Quick check question**: Given two policies A (profit=90, emissions=80) and B (profit=85, emissions=70), which is Pareto-dominated? (Answer: Neither; they are non-dominated trade-offs.)

- **Concept**: NSGA-II (Non-dominated Sorting Genetic Algorithm II)
  - **Why needed here**: Core evolutionary engine; requires understanding selection by rank + crowding distance, crossover/mutation on real-valued parameters.
  - **Quick check question**: Why does NSGA-II use crowding distance in addition to non-domination rank? (Answer: To maintain diversity across the Pareto front and avoid clustering.)

- **Concept**: Reinforcement Learning MDP and Policy Parameterization
  - **Why needed here**: Policies are neural networks mapping states to actions; episodic returns provide fitness. Basic grasp of state/action/reward/discounting needed.
  - **Quick check question**: In MORSE, how is a policy's fitness computed for objective j? (Answer: By averaging discounted episodic returns or CVaR over n_e episodes.)

## Architecture Onboarding

- **Component map**: Environment -> Policy network -> Evolutionary loop (NSGA-II) -> CVaR module -> Policy selector
- **Critical path**: 1) Implement environment (state, transition, reward vector). 2) Build policy network and inference. 3) Implement NSGA-II loop with vector fitness. 4) Add CVaR fitness variant. 5) Run evolution, extract Pareto front, validate via Monte Carlo.
- **Design tradeoffs**: Population size vs. compute budget (larger populations improve front coverage but increase evaluation cost); episode count vs. CVaR stability (more episodes reduce CVaR variance but multiply compute); network size vs. generalization (larger networks may overfit); discrete vs. continuous transport mode (softmax is tractable but may reduce mode commitment).
- **Failure signatures**: Pareto front collapse (check crowding-distance implementation, mutation rate too low); CVaR estimates unstable (increase n_e, verify return sorting); policy performance degrades under distribution shift (retrain or augment with scenario diversity).
- **First 3 experiments**: 1) Sanity check: Run MORSE on single-objective profit-only variant vs. standard policy-gradient RL baseline. 2) Ablation: Compare mean-fitness vs. CVaR-fitness MORSE on CVaR and mean metrics across 1,000 Monte Carlo episodes. 3) Disruption test: Introduce emission tax at timestep 200; compare fixed-policy vs. policy-switching performance on cumulative profit and emissions.

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (population size, generations, network architecture, evaluation episodes) are unspecified, preventing exact reproduction.
- Precise cost and emission coefficient matrices, demand distributions, and lead-time parameters are not provided, limiting faithful environment recreation.
- Domain-specific validation is incomplete: while tail-risk improvement via CVaR is shown, switching behavior under disruption is asserted but not deeply validated.

## Confidence

- **High**: Evolutionary policy search generates Pareto fronts; CVaR improves tail-risk outcomes; switching provides adaptive control under tested disruptions.
- **Medium**: Generalization of stored policies under distributional shift; robustness of Pareto front to environment parameter changes; CVaR hyperparameter sensitivity.
- **Low**: Broader disruption types not tested; impact of population size/generation limits on front diversity; sensitivity to demand distribution changes.

## Next Checks

1. **Architecture Sensitivity**: Vary neural network size and population parameters; measure impact on Pareto front coverage and computational cost.
2. **Disruption Robustness**: Test switching under multiple disruption types (e.g., demand spikes, lead-time increases) and quantify performance drop if disruptions fall outside training distribution.
3. **CVaR Hyperparameter Sweep**: Evaluate MORSE across multiple Î± values and episode counts; assess trade-off between tail performance and mean performance stability.