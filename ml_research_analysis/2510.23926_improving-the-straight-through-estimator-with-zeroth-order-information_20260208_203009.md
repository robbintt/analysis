---
ver: rpa2
title: Improving the Straight-Through Estimator with Zeroth-Order Information
arxiv_id: '2510.23926'
source_url: https://arxiv.org/abs/2510.23926
tags:
- fogzo
- gradient
- training
- should
- lsmooth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training neural networks with
  quantized parameters, where non-differentiability of quantization operations like
  rounding and sign functions hinders gradient-based optimization. To overcome this,
  the authors propose First-Order-Guided Zeroth-Order Gradient Descent (FOGZO), which
  combines the efficiency of the Straight-Through Estimator (STE) with the theoretical
  soundness of zeroth-order (ZO) methods.
---

# Improving the Straight-Through Estimator with Zeroth-Order Information

## Quick Facts
- arXiv ID: 2510.23926
- Source URL: https://arxiv.org/abs/2510.23926
- Authors: Ningfeng Yang; Tor M. Aamodt
- Reference count: 40
- Key outcome: FOGZO improves quantized network training by combining STE gradients with zeroth-order corrections, achieving 1-8% accuracy gains for vision models and 1-22 perplexity point improvements for language models.

## Executive Summary
This paper addresses the challenge of training neural networks with quantized parameters, where non-differentiability of quantization operations like rounding and sign functions hinders gradient-based optimization. To overcome this, the authors propose First-Order-Guided Zeroth-Order Gradient Descent (FOGZO), which combines the efficiency of the Straight-Through Estimator (STE) with the theoretical soundness of zeroth-order (ZO) methods. FOGZO uses STE gradients as a biased component mixed with unbiased ZO gradients, reducing computation while improving gradient quality. Experiments show FOGZO consistently outperforms STE across diverse models (DeiT, ResNet, LLaMA) and datasets (ImageNet, C4), achieving 1-8% accuracy gains for vision models and 1-22 perplexity point improvements for language models.

## Method Summary
FOGZO improves quantized neural network training by interpolating between biased STE gradients and unbiased zeroth-order gradients. The method constructs a mixed gradient direction that combines the STE gradient (scaled by √β) with a random perturbation vector (scaled by √(1-β)). This mixed gradient is then evaluated using finite differences to estimate the true gradient direction. The mixing ratio β decays from 1 to a minimum value β_min during training, allowing the model to start with pure STE and gradually incorporate ZO corrections. FOGZO requires only 2 additional forward passes per iteration compared to STE's 1, making it computationally feasible for deep networks.

## Key Results
- FOGZO consistently outperforms STE across diverse models (DeiT, ResNet, LLaMA) and datasets (ImageNet, C4)
- Achieves 1-8% accuracy gains for vision models and 1-22 perplexity point improvements for language models
- For a 2-layer MLP on MNIST, FOGZO reduces computation by 796x versus n-SPSA for the same loss
- FOGZO scales effectively with model size and integrates with state-of-the-art quantization methods like LSQ and QuEST

## Why This Works (Mechanism)

### Mechanism 1: Biased-Unbiased Gradient Interpolation
FOGZO improves gradient quality by interpolating between biased STE gradients and unbiased zeroth-order gradients via mixing ratio β. The expected FOGZO gradient E[G] ≈ (βĝĝ^T + (1-β)I)∇L_smooth forms a linear interpolation. The biased term (STE) dominates when trustworthy; the unbiased term (ZO) provides correction when STE misaligns. The core assumption is that STE gradients are "mostly accurate" but occasionally point in wrong directions; zeroth-order gradients are unbiased but high-variance. FOGZO can suppress the flaws of the STE. When the STE is accurate, the biased term is allowed to contribute to G. However, when the STE is inaccurate, the biased term no longer contributes. If STE gradients are consistently wrong (not just edge cases), the interpolation fails; if n is too small, ZO variance dominates and convergence slows.

### Mechanism 2: Adaptive Trust via Dot-Product Gating
The contribution of the STE bias term self-regulates based on alignment with the true gradient direction. The biased term's magnitude depends on ĝ^T ∇L_smooth—a dot product that evaluates to near-zero when STE misaligns, automatically suppressing bad gradient directions without explicit detection. The core assumption is that the true gradient ∇L_smooth exists and the finite-difference approximation captures it sufficiently. When the STE misaligns with ∇L_smooth, the model is in an edge case where the STE gradient is inaccurate, and the biased term has a relatively small magnitude. If the finite-difference approximation is too noisy (small n, large ϵ), the gating signal becomes unreliable.

### Mechanism 3: Hyperparameter Derivation from STE Implicit Smoothing
FOGZO's smoothness ϵ and perturbation p(u) should match the implicit randomized smoothing of the underlying STE. Each STE (identity, hardtanh, tanh, ApproxSign) corresponds to an expected-value smoothing of round/sign. By deriving this (ϵ̄, p̄(u)) pair analytically, FOGZO aligns its perturbation distribution with the STE's implicit behavior. The core assumption is that STE surrogates can be expressed as E_u[round(x + εu)] for some distribution. FOGZO provides explicit (ϵ̄, p̄(u)) for identity-STE, tanh-STE, ApproxSign-STE derived via integration. If the STE cannot be expressed as randomized smoothing, or if quantization scale α varies significantly across layers, the derived ϵ may be mismatched.

## Foundational Learning

- **Concept: Quantization Non-Differentiability**
  - Why needed here: The entire problem stems from round() and sign() having zero gradients almost everywhere.
  - Quick check question: Why can't standard backpropagation train quantized networks directly?

- **Concept: Straight-Through Estimator (STE)**
  - Why needed here: FOGZO uses STE as its biased component; understanding STE's strengths/weaknesses is prerequisite.
  - Quick check question: What surrogate function does identity-STE use for round(), and why does it work empirically despite lacking theoretical justification?

- **Concept: Zeroth-Order Optimization (n-SPSA)**
  - Why needed here: FOGZO's unbiased component uses finite-difference gradient estimation.
  - Quick check question: Why does n-SPSA require 2n forward passes, and what happens to gradient variance as n decreases?

## Architecture Onboarding

- **Component map:**
  STE backward pass -> produces biased gradient g -> Normalization -> converts g to unit vector ĝ -> Random sign flip s_i -> ensures symmetric bias component -> Mixing -> v_i = √β·s_i·ĝ + √(1-β)·u_i -> Finite difference -> G = (1/n) Σ [(L(θ+εv_i) - L(θ-εv_i)) / 2ε] · v_i -> Beta decay -> β linearly decays from 1 to β_min over training

- **Critical path:**
  1. Standard forward+backward pass with STE (Line 5)
  2. Extract and normalize gradient (Lines 6-7)
  3. For each sample i: perturb weights ±εv_i, measure losses (Lines 10-14)
  4. Accumulate ZO gradient estimate (Line 15)
  5. Apply optimizer step with combined gradient

- **Design tradeoffs:**
  - n=1 vs n>1: n=1 adds only 2 forward passes but higher variance; paper uses n=1 for all deep models
  - β_min selection: Too close to 1 → minimal ZO correction; too far from 1 → excessive variance
  - Compute budget: FOGZO adds ~2 forward passes vs STE; can use r% STE prefix to match wall-clock time

- **Failure signatures:**
  - β=1 (no ZO component): FOGZO cannot outperform STE (Figure 2, "const β=1" baseline)
  - 16-bit weight storage: Numerical precision issues when applying small perturbations
  - Very low β early in training: High variance can destabilize learning

- **First 3 experiments:**
  1. Replicate MNIST 2-layer MLP with β∈{0.9, 0.999, 1.0}, n=1, comparing final loss to Figure 1
  2. Ablate ϵ scaling: use ϵ = c·α/(2√3) with c∈{0.5, 1.0, 2.0} to validate Section 3.3 derivation
  3. Test β_min sweep on a small DeiT or ResNet variant, plotting training loss vs β_min to reproduce Figure 2 shape

## Open Questions the Paper Calls Out

### Open Question 1
Can FOGZO be adapted to work effectively with BF16 (16-bit) weights without suffering from precision loss during the in-place weight perturbations? The paper only validates FOGZO with 32-bit weights, while modern LLM pre-training pipelines increasingly adopt BF16 for efficiency. Experiments showing FOGZO maintaining performance gains over STE when weights are stored in BF16, potentially with numerical stability techniques, would resolve this.

### Open Question 2
Does FOGZO scale effectively to billion-parameter language models during full pre-training? The paper only tests LLaMA models up to 300M parameters and explicitly notes resource constraints prevented testing n-SPSA on deeper networks. The computational overhead of 2 additional forward passes per iteration may become prohibitive at billion-parameter scale, and the β scheduling dynamics may change with model scale. Experiments comparing FOGZO vs. STE on models with 1B+ parameters trained on trillions of tokens, measuring both convergence quality and wall-clock time, would resolve this.

### Open Question 3
What formal conditions characterize when the STE produces gradient directions that oppose the true gradient direction? The paper provides a single illustrative counterexample but does not derive formal criteria for predicting when STE gradients will be incorrect. Theoretical analysis establishing conditions on the loss landscape and quantization structure under which STE gradient sign errors occur, validated empirically across architectures, would resolve this.

## Limitations
- FOGZO's improvement relies on STE gradients being "mostly accurate" but occasionally wrong; no systematic analysis of when this assumption fails
- Computational overhead remains 2x STE despite optimizations; wall-clock time savings depend on implementation efficiency
- Experimental results show FOGZO can underperform STE in edge cases (Figure 2: const β=1 vs FOGZO for some configurations)

## Confidence

- **High Confidence:** FOGZO consistently outperforms STE in reported experiments (Tables 1-2, Figures 1-2)
- **Medium Confidence:** The theoretical justification for FOGZO's gradient quality improvement (Section 3.2) is sound but lacks empirical validation beyond synthetic cases
- **Low Confidence:** Claims about hyperparameter derivation matching STE's implicit smoothing (Section 3.3) are theoretically motivated but not extensively validated across different STE types

## Next Checks

1. Systematically characterize failure modes: Identify specific architectures/datasets where FOGZO degrades STE performance and analyze gradient alignment statistics
2. Validate variance reduction: Measure gradient variance across different n values and β schedules to quantify the tradeoff between bias correction and noise amplification
3. Scale sensitivity testing: Evaluate FOGZO's performance degradation when quantization step α varies significantly across layers, testing the robustness of the derived ϵ scaling heuristic