---
ver: rpa2
title: 'The Lie of the Average: How Class Incremental Learning Evaluation Deceives
  You?'
arxiv_id: '2509.22580'
source_url: https://arxiv.org/abs/2509.22580
tags:
- uni00000013
- uni00000011
- uni00000003
- sequences
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a critical flaw in the standard evaluation
  protocol for Class Incremental Learning (CIL), where models are assessed using only
  a few randomly sampled class sequences. This approach leads to unreliable estimates
  of model performance, failing to capture the true performance distribution and underestimating
  variance, which may result in misleading conclusions about model robustness.
---

# The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?

## Quick Facts
- **arXiv ID**: 2509.22580
- **Source URL**: https://arxiv.org/abs/2509.22580
- **Reference count**: 40
- **Primary result**: Standard random sampling evaluation protocols in Class Incremental Learning (CIL) underestimate performance variance, leading to misleading conclusions about model robustness.

## Executive Summary
This paper identifies a critical flaw in the standard evaluation protocol for Class Incremental Learning (CIL), where models are assessed using only a few randomly sampled class sequences. This approach leads to unreliable estimates of model performance, failing to capture the true performance distribution and underestimating variance, which may result in misleading conclusions about model robustness. To address this, the authors propose EDGE, a novel evaluation framework that leverages inter-task similarity to construct extreme class sequences—easy and hard cases—that better approximate the true performance distribution. Through theoretical analysis and exhaustive experiments on CIFAR-100 and ImageNet-R, EDGE demonstrates superior performance in estimating distributional boundaries, achieving lower Jensen-Shannon and Wasserstein distances compared to the traditional Random Sampling protocol. The method also reveals that different models can converge to similar worst-case performance under hard sequences, highlighting the importance of robust evaluation in CIL.

## Method Summary
The paper argues that standard CIL evaluation using random sampling (RS) fails to capture the true performance distribution due to the combinatorial explosion of possible class sequences. EDGE addresses this by using a pre-trained CLIP text encoder to embed class labels and compute inter-task similarity scores. It then generates three sequences: one maximizing similarity (easy), one minimizing similarity (hard), and one random (medium). The greedy algorithm used for sequence generation has O(N³) complexity. The method assumes that semantic similarity correlates with task difficulty and that capturing distributional extremes suffices to approximate the full distribution. Experiments show EDGE achieves lower Jensen-Shannon and Wasserstein divergences compared to RS.

## Key Results
- EDGE consistently achieves lower Jensen-Shannon Divergence and Wasserstein Distance to the ground truth compared to Random Sampling
- Different CIL models converge to similar worst-case performance under hard sequences generated by EDGE
- The method demonstrates robustness across different CLIP encoder sizes (ViT-B/16 vs ViT-L/14)
- Empirical results on CIFAR-100 and ImageNet-R validate the theoretical analysis linking inter-task similarity to generalization error bounds

## Why This Works (Mechanism)

### Mechanism 1
Inter-task semantic similarity functions as a proxy for task difficulty and generalization error in CIL. The theoretical analysis suggests that the upper bound of generalization error is inversely related to inter-task similarity. When adjacent tasks share low semantic similarity, the model must shift parameters more drastically, increasing the risk of "forgetting" and raising the error floor. This assumes semantic similarity derived from label embeddings correlates with feature-space interference during training.

### Mechanism 2
Stratified sampling using "extreme sequences" approximates the true performance distribution more efficiently than uniform random sampling. Random sampling under-samples the tails of the performance distribution due to the combinatorial explosion of class orders. By explicitly constructing sequences that maximize and minimize similarity, EDGE forces sampling of distributional boundaries, reducing the sample complexity required for reliable estimation.

### Mechanism 3
A pre-trained text encoder (CLIP) can structure the combinatorial search space for class ordering without access to image data. The method encodes class labels into vectors and calculates a similarity matrix, then uses a greedy algorithm to permute classes such that sequential tasks maximize or minimize the global inter-task similarity score. This assumes the semantic relationships in the text encoder's latent space align with the visual difficulty/interference of the classes.

## Foundational Learning

- **Concept**: **Class Incremental Learning (CIL) & Catastrophic Forgetting**
  - **Why needed here**: The entire paper is predicated on the fact that CIL models suffer from varying degrees of forgetting depending on the order of class introduction.
  - **Quick check question**: Can you explain why learning task B after task A might degrade performance on A, and how this relates to "inter-task interference"?

- **Concept**: **Combinatorial Explosion & Sampling Bias**
  - **Why needed here**: The paper argues that evaluating CIL is hard because you cannot test every permutation. Understanding why N! grows faster than polynomial sampling capacity is crucial to accepting the paper's premise.
  - **Quick check question**: Why does random sampling fail to estimate the variance of a distribution when the sample size is infinitesimal compared to the population size?

- **Concept**: **Distribution Divergence (JSD & Wasserstein)**
  - **Why needed here**: The paper quantifies "better evaluation" using mathematical distances between distributions. You need to distinguish between matching a point estimate vs. matching a distribution shape.
  - **Quick check question**: If Protocol A yields a lower Jensen-Shannon Divergence to the Ground Truth than Protocol B, what does that imply about Protocol A's estimation of the tails?

## Architecture Onboarding

- **Component map**: Embedding Module (CLIP text encoder) -> Similarity Engine (Cosine Similarity Matrix) -> Sequence Optimizer (Greedy algorithm + Hierarchical Clustering) -> Evaluation Harness
- **Critical path**: The Sequence Optimizer. If the "Hard" sequence is not actually hard, the estimated lower bound will be inflated, negating the protocol's advantage.
- **Design tradeoffs**: Greedy vs. Optimal (the paper uses greedy approach with O(N³) complexity); Encoder Dependency (relying on CLIP ties evaluation to quality of that specific embedding space)
- **Failure signatures**: Flat Distribution Estimate (small variance estimate while Ground Truth has large variance); Mismatched Bounds (Hard sequence accuracy higher than Medium sequence accuracy)
- **First 3 experiments**:
  1. Implement the 6-class, 3-task setup on CIFAR-100. Verify exhaustive computation of 90 permutations and plot histogram. Check if EDGE's 3 points visually span the histogram better than 3 random points.
  2. Calculate the Jensen-Shannon Divergence and Wasserstein Distance for RS vs. EDGE against the exhaustive ground truth. Confirm EDGE < RS.
  3. Run the Sequence Optimizer on a standard benchmark (e.g., full CIFAR-100). Measure wall-clock time to generate sequences. Confirm it is trivial compared to model training time.

## Open Questions the Paper Calls Out
- **Open Question 1**: How can CIL architectures be specifically designed to differentiate performance in worst-case scenarios, rather than converging to similar lower-bound accuracy under hard sequences?
- **Open Question 2**: Is semantic similarity (via CLIP text encoders) a sufficient proxy for inter-task interference, or do visual/feature-space similarity metrics provide a more accurate construction of extreme sequences?
- **Open Question 3**: Does the finding that the "Random Sampling" protocol is inadequate generalize to other continual learning scenarios, such as Task-Incremental Learning or Domain-Incremental Learning?

## Limitations
- The method's effectiveness for datasets with domain-specific concepts or highly abstract categories remains untested, as CLIP may fail to capture true semantic structure relevant to visual learning.
- The theoretical advantage depends on the performance distribution being approximately unimodal and symmetric, which may not hold for all CIL scenarios.
- Computing the full N × N similarity matrix for large class sets could become a bottleneck, though the paper doesn't address potential need for dimensionality reduction.

## Confidence
- **High Confidence**: The identification of random sampling bias in CIL evaluation (Section 3.1) - the combinatorial explosion argument is mathematically sound
- **Medium Confidence**: The theoretical analysis linking inter-task similarity to generalization error bounds (Theorem 3) - while proof logic is presented, practical tightness and dependence on similarity metric warrant scrutiny
- **Medium Confidence**: The claim that EDGE achieves superior distributional estimation - metric comparisons are clear but method's sensitivity to text encoder choice is not fully explored

## Next Checks
1. Apply EDGE to a dataset with known semantic challenges (e.g., medical imaging with fine-grained subtypes) and verify whether CLIP-based similarity still correlates with model performance. Test alternative embedding methods if needed.
2. Generate the full performance distribution for a mid-sized dataset (e.g., CIFAR-10 with 5 tasks of 2 classes) and explicitly test whether it is Gaussian-like. If multi-modal, evaluate whether adding a "medium-hard" sequence improves EDGE's estimation.
3. Replace CLIP with other text encoders (e.g., BERT, T5) or visual-feature clustering and measure the impact on EDGE's ability to predict hard/easy sequences and estimate distribution bounds.