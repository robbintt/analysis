---
ver: rpa2
title: 'QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models'
arxiv_id: '2502.12346'
source_url: https://arxiv.org/abs/2502.12346
tags:
- quzo
- quantized
- quantization
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QuZO, a quantized zeroth-order optimization
  framework for fine-tuning large language models (LLMs) using low-precision forward
  passes without backpropagation. The key innovation addresses the challenge of bias
  introduced by naive quantized random gradient estimators by introducing a stochastic
  quantized perturbation method that ensures unbiased gradient estimation.
---

# QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models

## Quick Facts
- arXiv ID: 2502.12346
- Source URL: https://arxiv.org/abs/2502.12346
- Authors: Jiajun Zhou; Yifan Yang; Kai Zhen; Ziyue Liu; Yequan Zhao; Ershad Banijamali; Athanasios Mouchtaris; Ngai Wong; Zheng Zhang
- Reference count: 34
- Key outcome: QuZO achieves performance comparable to first-order methods in FP8 and superior accuracy in INT8/INT4 settings while reducing memory costs by 2.94× in LLaMA2-7B fine-tuning

## Executive Summary
QuZO introduces a quantized zeroth-order optimization framework that enables fine-tuning of large language models using only forward passes and low-precision arithmetic, eliminating the need for backpropagation and its associated memory overhead. The key innovation addresses the bias introduced by naive quantized random gradient estimators through a stochastic quantized perturbation method that ensures unbiased gradient estimation. By avoiding error-prone low-precision straight-through estimators and leveraging dual-seed stochastic quantization, QuZO achieves strong performance across multiple tasks while reducing memory costs by 2.94× compared to quantized first-order methods.

## Method Summary
QuZO implements zeroth-order optimization for quantized LLMs by replacing backpropagation with forward-only gradient estimation using finite differences. The method generates perturbations through dual-seed stochastic quantization, creating two conditionally independent quantized versions of each perturbation vector. These are used to compute forward pass differences that estimate gradients without requiring backward computation. The estimated gradients are then applied directly to quantized weights using stochastic rounding, eliminating the need for optimizer states and activation storage. This approach seamlessly integrates with quantized LLM inference engines while maintaining unbiased gradient estimation even in ultra-low precision settings.

## Key Results
- Achieves comparable performance to first-order methods in FP8 precision and superior accuracy in INT8/INT4 settings
- Reduces memory costs by 2.94× in LLaMA2-7B fine-tuning compared to quantized first-order methods
- Demonstrates strong results across GLUE, Multi-Choice, and Generation benchmarks
- Shows particular robustness in ultra-low precision settings where first-order methods struggle

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Dual-Seed Quantized Gradient Estimation
Standard quantized perturbations lose unit variance, biasing the random gradient estimator. QuZO generates two quantized versions using different stochastic rounding seeds, ensuring $\mathbb{E}[u_{i,1}u_{i,2}^T] = u_i u_i^T$ and thus unbiased gradient estimation. This works because stochastic rounding satisfies $\mathbb{E}_Q[Q(u)] = u$, and the two seeds produce conditionally independent outputs.

### Mechanism 2: Avoiding Straight-Through Estimator Backpropagation
First-order training relies on STE to approximate gradients, which accumulates error as precision drops. QuZO never computes gradients via backprop, instead estimating them purely from forward pass differences $(L^+_B - L^-_B)/(2\epsilon)$, sidestepping STE entirely and avoiding its associated instability in ultra-low precision settings.

### Mechanism 3: Memory Reduction via Forward-Only Updates
By removing backward pass storage and optimizer states, QuZO achieves 1.4-2.94× memory savings. The method only needs quantized weights and low-precision perturbation vectors, since gradients are estimated from forward losses and applied directly via quantized updates, eliminating the need for activation storage and momentum buffers.

## Foundational Learning

- **Concept**: Zeroth-order optimization via finite differences
  - Why needed: QuZO builds on standard ZO-SGD but adapts it for quantized settings; understanding forward-only gradient estimation is prerequisite.
  - Quick check: Given a scalar function $f$, how would you estimate $f'(x)$ using only function evaluations?

- **Concept**: Stochastic rounding properties
  - Why needed: The unbiasedness proof hinges on $\mathbb{E}[Q(u)] = u$; engineers must understand why stochastic (not deterministic) rounding is essential.
  - Quick check: If you quantize 0.6 to {0, 1} using stochastic rounding with probability proportional to distance, what is $\mathbb{E}[Q(0.6)]$?

- **Concept**: Quantization noise vs. optimization noise
  - Why needed: QuZO trades gradient variance (from ZO estimation) for reduced bias from quantization; distinguishing these noise sources informs hyperparameter choices.
  - Quick check: As you lower precision from INT8 to INT4, which increases more—gradient estimation variance or quantization-induced bias in STE methods?

## Architecture Onboarding

- **Component map**: Quantized forward engine -> Stochastic perturbation generator -> Loss differencer -> Quantized updater

- **Critical path**:
  1. Sample $u_i \sim \mathcal{N}(0, I)$
  2. Quantize to $u_{i,1}, u_{i,2}$ with independent seeds
  3. Run forward passes at $\bar{w} \pm \epsilon u_{i,1}$
  4. Compute $\mu_i$ and update $\bar{w}$ using $u_{i,2}$
  5. Repeat for $n$ queries per step

- **Design tradeoffs**:
  - Query count $n$: Higher $n$ reduces variance but costs more forward passes; paper uses $n=1$ typically
  - Perturbation scale $\epsilon$: Must exceed quantization granularity but stay small enough for valid derivative approximation
  - Hybrid datatypes: INT weights + FP activations can boost accuracy ~1-2% (Appendix B.2)

- **Failure signatures**:
  - Loss oscillating or diverging → $\epsilon$ too large or stochastic rounding broken
  - Accuracy worse than FO at FP16 → check if dual-seed independence is implemented
  - OOM on large models → verify LoRA variant is used and perturbations are quantized

- **First 3 experiments**:
  1. Replicate RoBERTa-Large SST-2 in INT8: Compare QuZO vs. quantized FO; expect QuZO ≥ FO
  2. Ablate dual-seed: Use $u_{i,1} = u_{i,2}$ and confirm accuracy drops (bias returns)
  3. Profile memory on LLaMA2-7B with batch size 8: Confirm QuZO memory ≈ 1/3 of INT8 FO baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can a dedicated hardware implementation of QuZO using real low-precision kernels validate the theoretical reductions in latency and energy consumption? The authors note they haven't implemented real quantized training frameworks using low-precision kernels, relying instead on simulations or standard INT8 CUDA kernels that may not capture specific bottlenecks of true low-bit inference accelerators.

### Open Question 2
Does the QuZO gradient estimation maintain stability and convergence speed when scaling to models with significantly larger parameter counts (e.g., 70B+)? Experiments are limited to LLaMA2-13B, and the variance of zeroth-order estimators typically increases with dimension, potentially requiring more forward passes for massive models.

### Open Question 3
Can QuZO be effectively combined with sparse perturbation techniques to further reduce memory and computation without reintroducing bias? The paper mentions "Sparse MeZO" in Related Work but doesn't explore integration with sparse perturbation masks, leaving uncertainty about mathematical compatibility.

## Limitations

- Several key implementation details remain underspecified, particularly exact quantization scheme parameters (scaling factors, zero points, clamping ranges) for INT4/INT8 formats
- The dual-seed stochastic rounding mechanism requires precise implementation to maintain conditional independence—small deviations could reintroduce bias
- Memory savings comparison assumes idealized quantized forward engines; real-world hardware implementations may yield different results

## Confidence

- **High Confidence**: The core mechanism of unbiased quantized gradient estimation via dual-seed stochastic rounding is well-grounded mathematically and the theoretical derivation is sound
- **Medium Confidence**: The empirical performance claims across multiple tasks and precision levels are supported by experiments, but limited ablation studies and lack of error bars reduce confidence in robustness claims
- **Low Confidence**: The exact implementation details required for perfect reproduction are missing, particularly around quantization parameters and stochastic rounding implementation

## Next Checks

1. **Implement and verify dual-seed stochastic rounding**: Create a test to confirm that E[Q(u)] = u and that u_i1 and u_i2 are conditionally independent. Compare accuracy with a naive single-seed implementation to verify the bias reduction claim.

2. **Profile memory usage across precision levels**: Measure actual memory consumption during training on LLaMA2-7B with varying batch sizes and compare QuZO against quantized first-order methods. Verify the claimed 2.94× reduction holds across different hardware configurations.

3. **Test perturbation scale sensitivity**: Systematically vary ε from 1e-6 to 1e-3 and measure impact on accuracy and convergence stability. Identify the optimal range and test whether the claimed robustness to low precision holds when ε is pushed to its limits.