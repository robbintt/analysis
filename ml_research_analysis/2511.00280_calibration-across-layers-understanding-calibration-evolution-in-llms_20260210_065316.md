---
ver: rpa2
title: 'Calibration Across Layers: Understanding Calibration Evolution in LLMs'
arxiv_id: '2511.00280'
source_url: https://arxiv.org/abs/2511.00280
tags:
- calibration
- accuracy
- final
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates how calibration evolves across transformer\
  \ layers in large language models, focusing on how confidence and accuracy change\
  \ at each stage of the network. By analyzing multiple open-weight models (Phi-2,\
  \ Llama-3-8B, Llama-2-7B, Mistral-7B) on the MMLU benchmark, the authors identify\
  \ a \u201Cconfidence correction phase\u201D in the upper layers: after prediction\
  \ accuracy plateaus, calibration metrics (ECE, MCE) first rise (indicating overconfidence)\
  \ then decline (indicating improved calibration), suggesting that confidence is\
  \ actively adjusted even after decision certainty is reached."
---

# Calibration Across Layers: Understanding Calibration Evolution in LLMs

## Quick Facts
- **arXiv ID**: 2511.00280
- **Source URL**: https://arxiv.org/abs/2511.00280
- **Reference count**: 40
- **Primary result**: Large language models exhibit a "confidence correction phase" in upper layers where calibration improves after accuracy plateaus, with a low-dimensional calibration direction that can improve ECE/MCE without harming accuracy.

## Executive Summary
This paper investigates how calibration evolves across transformer layers in large language models, focusing on how confidence and accuracy change at each stage of the network. By analyzing multiple open-weight models (Phi-2, Llama-3-8B, Llama-2-7B, Mistral-7B) on the MMLU benchmark, the authors identify a "confidence correction phase" in the upper layers: after prediction accuracy plateaus, calibration metrics (ECE, MCE) first rise (indicating overconfidence) then decline (indicating improved calibration), suggesting that confidence is actively adjusted even after decision certainty is reached. The authors further identify a low-dimensional calibration direction in the residual stream whose perturbation improves calibration without harming accuracy, indicating a distributed calibration mechanism across the network.

## Method Summary
The authors analyze layer-wise calibration dynamics using Logit Lens-style probing - projecting residual streams at each layer via the unembedding matrix to compute accuracy, ECE, and MCE. They identify a "confidence correction phase" where accuracy plateaus but calibration improves. A calibration direction vector is computed as the mean normalized difference between residual streams in the correction phase (layers 29-31 for Phi-2), which when added to residuals improves calibration metrics without affecting accuracy. Experiments are conducted on multiple 7B models using MMLU and other benchmarks.

## Key Results
- Calibration metrics (ECE, MCE) show distinct temporal evolution across layers, with overconfidence peaks followed by correction phases after accuracy plateaus
- A low-dimensional calibration direction vector in the residual stream can improve ECE/MCE without harming accuracy when applied as intervention
- The calibration mechanism operates independently from the unembedding null space, suggesting a distributed confidence regulation mechanism
- The calibration direction generalizes across datasets within a model but fails to transfer across architectures

## Why This Works (Mechanism)

### Mechanism 1: Temporal Decoupling of Competence and Confidence
- **Claim:** In transformer models, prediction accuracy stabilizes in intermediate layers, while calibration undergoes a distinct "confidence correction phase" in later layers where overconfidence is actively reduced.
- **Mechanism:** The forward pass exhibits a three-stage dynamic: initial decision formation (accuracy rises), a transient overconfidence peak (ECE rises), and a final correction phase (ECE drops) where the residual stream is modified to align confidence with the plateaued accuracy.
- **Core assumption:** The observed reduction in calibration error (ECE/MCE) in later layers reflects an active recalibration process rather than a passive artifact of normalization or aggregation.

### Mechanism 2: The Calibration Direction Vector
- **Claim:** There exists a low-dimensional direction in the residual stream ($\hat{c}$) that causally influences model confidence without altering the argmax prediction.
- **Mechanism:** The vector $\hat{c}$ is derived from the normalized differences between the residual streams of the final layers (e.g., layers 29–31). Adding this direction ($A'_i = A_i + \eta\hat{c}$) during inference shifts the probability distribution to reduce overconfidence.
- **Core assumption:** The arithmetic mean of layer differences captures a consistent "correction signal" that can be isolated and applied universally across datasets for a specific model.

### Mechanism 3: Independence from Unembedding Null Space
- **Claim:** The discovered calibration mechanism operates distinctly from the "entropy neurons" or null space of the unembedding matrix identified in prior work.
- **Mechanism:** While the unembedding null space marginally affects calibration (seen in MCE fluctuations when truncated), the primary "calibration direction" vector $\hat{c}$ is not aligned with the null space, suggesting confidence is regulated via a distributed subspace rather than solely through the final projection bottlenecks.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** This is the primary metric used to track the "correction phase." Without understanding ECE (weighted difference between confidence and accuracy), you cannot quantify the overconfidence peak or the subsequent correction.
  - **Quick check question:** If a model has 90% accuracy and predicts with 100% confidence, what is the calibration error contribution for that bin?

- **Concept: The Residual Stream**
  - **Why needed here:** The paper relies on "Logit Lens" style probing—projecting the residual stream at layer $i$ to vocabulary space. You must understand that the residual stream accumulates information and that reading from intermediate layers reveals intermediate predictions.
  - **Quick check question:** How does the "Logit Lens" allow us to see the prediction at layer 10 before the network has finished its forward pass?

- **Concept: Unembedding Null Space**
  - **Why needed here:** Prior work attributed calibration to "entropy neurons" writing to the null space of $W_U$. This paper argues for a mechanism *distinct* from this, requiring you to understand what the null space is to appreciate the difference.
  - **Quick check question:** Does a vector in the null space of the unembedding matrix change the logits or the probability distribution after softmax? (Trick question: it changes logits pre-softmax but depends on how it interacts with the principal components).

## Architecture Onboarding

- **Component map:** Input: MCQA Prompt -> Extraction Point: Residual Stream $A_i$ at every layer $i$ -> Probe: Project $A_i$ via Unembedding Matrix $W_U$ -> Logits -> Softmax -> Intervention: Calibration Direction Vector $\hat{c}$

- **Critical path:**
  1. Run inference on a benchmark (e.g., MMLU) and cache residuals $A_i$ for all layers.
  2. Project $A_i$ to logits to calculate Accuracy/ECE curves to identify the "Correction Phase" (layers where Acc is flat but ECE drops).
  3. Compute $\hat{c}$ as the mean normalized difference of residuals in the correction phase (e.g., layers 29, 30, 31).
  4. Intervene: $A'_i = A_i + \eta\hat{c}$.

- **Design tradeoffs:**
  - **Dataset Generality vs. Model Specificity:** The vector $\hat{c}$ generalizes across datasets (MMLU $\to$ TruthfulQA) but *not* across architectures (Phi-2 $\to$ Mistral).
  - **Metric Sensitivity:** MCE (Max Calibration Error) is more sensitive to null-space interventions than ECE.

- **Failure signatures:**
  - **Continuous Accuracy Rise:** In tasks where accuracy keeps rising until the last layer (e.g., reasoning tasks like COPA), the "Correction Phase" is obscured; the method yields noisy or ineffective directions.
  - **Cross-Model Transfer:** Applying a calibration vector derived from one model family to another fails to improve calibration and may degrade accuracy.

- **First 3 experiments:**
  1. **Layer-wise Baseline:** Plot Accuracy vs. ECE for all layers on MMLU to verify the "hump" (overconfidence) and subsequent drop in the specific model you are onboarding.
  2. **Vector Isolation:** Calculate $\hat{c}$ using the last 3 layers of the model. Verify it improves ECE on a held-out split of the same dataset used for calculation.
  3. **Cross-Dataset Transfer:** Take the $\hat{c}$ vector derived from MMLU and test it on a different domain (e.g., Rotten Tomatoes or TruthfulQA) to confirm the subspace is task-agnostic within the model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the calibration direction identified in Phi-2 fail to generalize to other architectures (Mistral, LLaMA-2, LLaMA-3), and what architectural or training factors enable steerable calibration subspaces?
- **Basis in paper:** [explicit] Limitations section states: "the discovered direction in Phi-2 leads to meaningful calibration improvements... it does not generalize to other models such as Mistral or LLaMA-2" and "a more detailed investigation into whether this lack of generalization stems from architecture, training procedure, or representational differences is needed."
- **Why unresolved:** The authors observe the confidence correction phase across all tested models but find the interventional calibration direction only works reliably in Phi-2.
- **What evidence would resolve it:** Systematic comparison of residual stream geometry across architectures; analysis of training objectives and data; identification of architectural features (e.g., layer normalization placement, MLP structure) correlated with calibration subspace steerability.

### Open Question 2
- **Question:** Can gradient-based methods (e.g., derivatives of ECE/MCE with respect to residual activations) identify more robust and generalizable calibration directions than the simple layer-difference approach used in this study?
- **Basis in paper:** [explicit] Appendix H states: "Future work can explore gradient-based approaches, such as computing the derivative of calibration metrics (e.g., ECE, MCE) with respect to residual activations. This could identify directions more causally linked to calibration."
- **Why unresolved:** The current method uses a heuristic (mean of normalized layer differences) that lacks principled causal justification and fails to transfer across models.
- **What evidence would resolve it:** Comparison of calibration improvement using gradient-derived directions vs. layer-difference directions; tests of cross-model transferability; causal intervention studies showing stronger effects.

### Open Question 3
- **Question:** How do layerwise calibration mechanisms emerge during pretraining, and are the observed confidence correction phases learned or architectural inevitabilities?
- **Basis in paper:** [explicit] Conclusion states: "future work may explore how these layerwise calibration mechanisms arise during pretraining."
- **Why unresolved:** The study analyzes only final trained models, not training dynamics; it remains unclear whether calibration phases are emergent properties of scale/data or result from specific training pressures.
- **What evidence would resolve it:** Checkpoint analysis across pretraining; comparison of models trained with different objectives; controlled experiments varying model depth and observing calibration phase emergence.

### Open Question 4
- **Question:** How do in-context learning (few-shot, chain-of-thought) settings alter the layerwise calibration dynamics observed in zero-shot MCQA?
- **Basis in paper:** [explicit] Appendix H states: "Extending this analysis to in-context learning (ICL) settings, such as few-shot or chain-of-thought prompting, may reveal how calibration dynamics change when more contextual supervision is available."
- **Why unresolved:** All experiments use zero-shot prompts; ICL may provide additional supervisory signal that changes where and how confidence forms and is corrected.
- **What evidence would resolve it:** Layerwise calibration curves under few-shot prompting; analysis of whether calibration phases shift earlier with more context; comparison across reasoning vs. knowledge tasks with varying shot counts.

## Limitations
- The calibration direction vector shows poor cross-architecture generalization, limiting practical applicability
- The study is limited to 7B parameter models and may not scale to larger models
- The causal link between observed ECE reduction and active confidence recalibration remains inferential rather than experimentally verified

## Confidence
- **High Confidence**: The empirical observation of accuracy plateauing before calibration improvement (Mechanism 1) - this pattern is consistently visible across all tested models in the layer-wise plots.
- **Medium Confidence**: The existence and effectiveness of the calibration direction vector (Mechanism 2) - while demonstrated, the vector's poor cross-model generalization suggests it may be capturing model-specific artifacts rather than a universal mechanism.
- **Low Confidence**: The claim of active confidence recalibration being distinct from null-space effects (Mechanism 3) - the evidence is primarily based on vector alignment tests rather than causal intervention studies.

## Next Checks
1. **Cross-Model Validation**: Apply the calibration direction vectors derived from Phi-2 to Llama-3-8B and Mistral-7B to quantify the exact performance degradation and identify whether the failure mode is systematic or random.

2. **Null-Space Intervention Study**: Conduct controlled experiments adding vectors from the unembedding matrix null space to residuals, measuring both ECE/MCE changes and alignment with the discovered calibration direction to establish orthogonality claims more rigorously.

3. **Reasoning Task Extension**: Test the layer-wise calibration dynamics on complex reasoning benchmarks (e.g., GSM8K, HumanEval) where accuracy continues rising through later layers, to determine if the "confidence correction phase" emerges as a distinct phenomenon or merges with accuracy gains.