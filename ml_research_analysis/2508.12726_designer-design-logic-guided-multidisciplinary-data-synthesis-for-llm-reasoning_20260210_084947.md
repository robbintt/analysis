---
ver: rpa2
title: 'DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning'
arxiv_id: '2508.12726'
source_url: https://arxiv.org/abs/2508.12726
tags:
- question
- questions
- design
- data
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DESIGNER, a design-logic-guided pipeline
  for synthesizing multidisciplinary reasoning questions. It extracts over 120K "Design
  Logics" that encode how experts transform knowledge into complex exam questions,
  enabling LLMs to generate new questions with the same reasoning patterns from raw
  documents.
---

# DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning

## Quick Facts
- **arXiv ID**: 2508.12726
- **Source URL**: https://arxiv.org/abs/2508.12726
- **Reference count**: 40
- **Key outcome**: DESIGNER extracts design logics from expert reasoning to synthesize 4.7M multidisciplinary questions, improving LLM reasoning when fine-tuned

## Executive Summary
DESIGNER is a pipeline that synthesizes multidisciplinary reasoning questions by extracting "Design Logics" - expert-crafted patterns that encode how domain knowledge transforms into complex exam questions. The system uses a two-stage retrieval-augmented mechanism to generate questions from raw documents, creating large-scale datasets (DLR-Book: 3.04M questions, DLR-Web: 1.66M questions) across 75 disciplines. When Qwen3 and Llama3 models are fine-tuned on these synthesized datasets, they demonstrate substantial improvements in multidisciplinary reasoning performance, surpassing their official final models on standard benchmarks.

## Method Summary
DESIGNER extracts reasoning patterns called "Design Logics" from a corpus of 1,195 complex exam questions spanning 75 disciplines. These patterns encode how experts transform domain knowledge into challenging reasoning questions. The pipeline employs a two-stage retrieval-augmented mechanism that first retrieves relevant Design Logics based on input documents, then generates new questions that preserve the same reasoning pathways. This process enables synthesis of millions of new questions that maintain the complexity and multidisciplinary nature of the original expert-crafted questions.

## Key Results
- Synthesized two large datasets: DLR-Book (3.04M) and DLR-Web (1.66M) questions across 75 disciplines
- Synthesized questions demonstrate higher difficulty and diversity compared to baseline datasets
- Fine-tuning Qwen3 and Llama3 on DESIGNER data substantially improves multidisciplinary reasoning performance, surpassing official model versions

## Why This Works (Mechanism)
DESIGNER works by capturing and reusing the cognitive patterns experts employ when constructing complex reasoning questions. By extracting Design Logics - the explicit reasoning pathways that transform domain knowledge into exam questions - the system can generate new questions that preserve the same level of multidisciplinary complexity. The two-stage retrieval-augmented approach ensures that generated questions are both relevant to the source material and maintain the intended reasoning difficulty, effectively scaling up the expert knowledge embedded in the original question corpus.

## Foundational Learning
**Design Logics**: Expert-crafted reasoning patterns that encode how knowledge transforms into complex questions
- Why needed: Captures the cognitive process experts use to create multidisciplinary reasoning challenges
- Quick check: Can you identify the reasoning pathway from knowledge to question in a sample problem?

**Retrieval-Augmented Generation**: Two-stage process using retrieval followed by synthesis
- Why needed: Ensures generated questions are both relevant and maintain desired complexity
- Quick check: Does the system first find matching patterns before generating questions?

**Multidisciplinary Question Synthesis**: Creating questions that require knowledge from multiple domains
- Why needed: Builds reasoning capabilities that mirror real-world problem-solving
- Quick check: Can the generated questions require integration of knowledge from at least two different fields?

## Architecture Onboarding

**Component Map**: Document Corpus -> Design Logic Extraction -> Retrieval Module -> Question Generation Module -> Synthesized Dataset

**Critical Path**: Raw documents → Design Logic retrieval → Question generation → Quality filtering → Final dataset

**Design Tradeoffs**: 
- Manual extraction of Design Logics provides high-quality patterns but limits scalability
- Synthetic data generation enables massive scale but may not capture all real-world complexity
- Two-stage retrieval ensures relevance but adds computational overhead

**Failure Signatures**:
- Generated questions lack multidisciplinary integration (Design Logic retrieval failure)
- Questions are too simple or repetitive (pattern overfitting)
- Questions don't align with source document content (retrieval-augmentation misalignment)

**3 First Experiments**:
1. Validate Design Logic extraction quality by comparing synthesized questions to original expert questions
2. Test retrieval accuracy by measuring Design Logic relevance to source documents
3. Evaluate question diversity by analyzing semantic and structural variation in synthesized outputs

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness depends on quality of manually curated Design Logic patterns extracted from only 1,195 questions
- Evaluation relies on synthetic benchmarks that may not fully represent real-world multidisciplinary reasoning complexity
- Manual extraction process may embed human bias in how reasoning pathways are represented

## Confidence
- **High Confidence**: Technical feasibility of design-logic-guided question synthesis and basic methodology
- **Medium Confidence**: Scalability claims for synthesizing millions of questions and quantitative improvements from fine-tuning
- **Medium Confidence**: Claims about synthesized questions being "more difficult and diverse" than baselines

## Next Checks
1. Test synthesized questions on completely independent multidisciplinary reasoning benchmarks not used during Design Logic extraction
2. Conduct blind assessments by domain experts to verify whether synthesized questions require intended reasoning patterns
3. Evaluate whether fine-tuning improvements transfer to real-world multidisciplinary problem-solving tasks over time