---
ver: rpa2
title: 'ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using
  Demonstration Retrieval'
arxiv_id: '2509.07512'
source_url: https://arxiv.org/abs/2509.07512
tags:
- sampling
- allabel
- similarity
- entity
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALLabel introduces a three-stage active learning framework for
  entity recognition with LLMs, addressing the high annotation cost in specialized
  domains. It sequentially applies diversity sampling to select representative examples,
  similarity sampling to retrieve examples relevant to all queries, and uncertainty
  sampling to target low-confidence predictions.
---

# ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval

## Quick Facts
- arXiv ID: 2509.07512
- Source URL: https://arxiv.org/abs/2509.07512
- Authors: Zihan Chen; Lei Shi; Weize Wu; Qiji Zhou; Yue Zhang
- Reference count: 29
- Primary result: 5%-10% annotation budget achieves F1 within 2% of full annotation

## Executive Summary
ALLabel introduces a three-stage active learning framework for entity recognition with LLMs, addressing the high annotation cost in specialized domains. It sequentially applies diversity sampling to select representative examples, similarity sampling to retrieve examples relevant to all queries, and uncertainty sampling to target low-confidence predictions. This approach constructs an optimized demonstration corpus for in-context learning. Experiments on three chemistry and materials science datasets show that ALLabel outperforms baselines, achieving F1 scores within 2% of full-annotation performance while using only 5%-10% of the data. The framework improves efficiency by focusing annotation on the most informative samples, reducing costs while maintaining high extraction accuracy.

## Method Summary
ALLabel is a three-stage active learning framework that optimizes demonstration selection for LLM-based entity recognition. The method allocates annotation budget $M$ in a 1:3:1 ratio across three stages: (1) Diversity sampling selects $M/5$ samples using a warm-start core-set algorithm to ensure representative coverage, (2) Similarity sampling selects $3M/5$ samples using sumrank scoring to identify demonstrations frequently relevant to test queries, and (3) Uncertainty sampling selects $M/5$ samples by identifying "weak test points" with low similarity to current demonstrations and re-scoring remaining candidates. The framework uses BM25 or Sentence-BERT similarity metrics and k-shot in-context learning (k=3) with GPT-4o or DeepSeek-V3 for final evaluation.

## Key Results
- ALLabel achieves F1 scores within 2% of full-annotation performance using only 5%-10% of the data
- The 1:3:1 stage proportion allocation outperforms alternatives (1:1:1 and 1:5:1) across all tested datasets
- Diversity sampling with warm-start initialization improves performance by 0.4-0.9 F1 points over cold-start approaches
- Similarity sampling provides the largest performance gain among the three stages when ablated

## Why This Works (Mechanism)

### Mechanism 1: Warm-Start Core-Set for Representative Coverage
Selecting diverse samples first creates a representative surrogate of the full data distribution. A greedy algorithm iteratively adds the sample with maximum distance from the currently selected set, using text similarity as a distance proxy. It initializes with the sample having lowest average similarity to all others, avoiding random cold-start.

### Mechanism 2: Sumrank-Based Similarity Sampling for ICL Alignment
Prioritizing samples that frequently appear as high-quality demonstrations across all test queries improves retrieval corpus utility. For each unlabeled sample, a composite "sumrank" score is computed by traversing all test queries and incrementing the score when the sample ranks among top-k similar examples.

### Mechanism 3: Uncertainty-Similarity Sampling for Hard Examples
Samples with low similarity to the current retrieval corpus represent high-uncertainty test queries and benefit disproportionately from annotation. "Weak test points" (top M/5 queries with least similar demonstrations) are identified, then remaining unlabeled samples are re-scored via sumrank on these weak points only.

## Foundational Learning

- **In-Context Learning (ICL) with Demonstration Retrieval:** Why needed here - ALLabel's entire value proposition assumes ICL performance depends on demonstration quality. Quick check question: Can you explain why retrieving semantically similar demonstrations improves few-shot NER compared to random selection?

- **Active Learning Paradigm:** Why needed here - The three-stage strategy is a selective annotation problem; understanding the explore-exploit tradeoff is essential. Quick check question: What is the cold-start problem in active learning, and why does warm-start core-set address it?

- **Named Entity Recognition Evaluation (F1 with Exact Match):** Why needed here - The paper uses strict JSON-level entity matching with TP/FP/FN definitions specific to structured extraction. Quick check question: For a (precursor_name, amount) entity pair, what counts as a true positive vs. false positive?

## Architecture Onboarding

- **Component map:** Diversity (M/5) -> Similarity (3M/5) -> Uncertainty (M/5) -> Retrieval corpus (M samples) -> LLM inference with k-shot ICL

- **Critical path:** 1) Compute NÃ—N similarity matrix (BM25 or Sentence-BERT), 2) Run diversity sampling (Algorithm 1) with warm-start seed, 3) Compute sumrank for all remaining samples across all test queries, 4) Select top 3M/5 by sumrank, 5) Identify weak test points (low similarity to D1), 6) Re-score remaining samples via sumrank on weak points only, 7) Select final M/5 and construct complete retrieval corpus, 8) Evaluate with k-shot ICL (k=3)

- **Design tradeoffs:** 1:3:1 proportion allocation was validated but may vary by dataset difficulty; BM25 yields slightly higher F1 than SBERT but provides less dense embeddings; framework is deterministic with no variance unlike random/core-set baselines

- **Failure signatures:** Diminishing returns after 5-10% (F1 plateaus); entity type conflicts (expanding pool helps some entities but harms others); pool size too small (<20) causes unstable performance

- **First 3 experiments:** 1) Reproduce warm-start vs. cold-start on held-out domain dataset (expect ~0.4-0.9 F1 improvement), 2) Validate uncertainty-similarity correlation by measuring perplexity at varying similarity levels (confirm negative correlation pattern), 3) Ablate one stage at a time on your dataset (expect largest drop when removing similarity sampling, smallest when removing diversity sampling)

## Open Questions the Paper Calls Out

- Can ALLabel be adapted to optimize for precise, token-level annotation costs rather than just the number of samples?
- How can the framework be modified to prevent performance degradation in specific entity types as the demonstration pool size increases?
- Does the effectiveness of the three-stage (D-S-U) sampling sequence hold across a wider variety of advanced LLM architectures and domains?

## Limitations
- External validity concerns: Framework validated only on chemistry/materials science domains with small datasets (498-696 samples)
- Parameter sensitivity: Optimal configuration likely depends on dataset characteristics; systematic optimization of k and BM25 parameters was not performed
- Evaluation constraints: Strict JSON-based exact match may underestimate partial success; in-domain validation assumes similar query distributions

## Confidence
- **High Confidence (Mechanistic):** Framework architecture and implementation are well-specified and reproducible; deterministic nature enables consistent validation
- **Medium Confidence (Performance Claims):** 5-10% annotation budget claim is well-supported on three tested datasets but may diminish on larger, more heterogeneous datasets
- **Low Confidence (Generalization):** Effectiveness on non-scientific domains, multilingual datasets, or domains with different entity structures remains unproven

## Next Checks
1. **Cross-Domain Transfer Test:** Apply ALLabel to a general-domain NER dataset and compare the 1:3:1 stage allocation against dataset-specific optimization
2. **Correlation Validation Experiment:** Systematically measure the relationship between demonstration-query similarity and entity extraction error across 100+ samples using multiple similarity metrics
3. **Entity Type-Specific Analysis:** Analyze performance gains per entity type when ALLabel is applied versus when only diversity or similarity sampling is used to identify whether certain entity types benefit more from uncertainty sampling