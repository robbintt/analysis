---
ver: rpa2
title: 'Digi-Q: Learning Q-Value Functions for Training Device-Control Agents'
arxiv_id: '2502.15760'
source_url: https://arxiv.org/abs/2502.15760
tags:
- digi-q
- policy
- learning
- training
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of training effective device-control
  agents for Android environments, where collecting fresh on-policy interaction data
  is costly and often impractical. The key insight is to replace expensive on-policy
  reinforcement learning with a two-stage approach: first fine-tuning a VLM to emphasize
  actionable features of the interface, then training a Q-function over frozen VLM
  representations using offline TD-learning.'
---

# Digi-Q: Learning Q-Value Functions for Training Device-Control Agents

## Quick Facts
- **arXiv ID**: 2502.15760
- **Source URL**: https://arxiv.org/abs/2502.15760
- **Reference count**: 40
- **Primary result**: Achieves 21.2% relative improvement over prior offline RL methods on Android device-control tasks, matching on-policy RL performance in some cases

## Executive Summary
This paper addresses the challenge of training effective device-control agents for Android environments where on-policy data collection is costly. The key innovation is a three-stage approach that replaces expensive on-policy reinforcement learning with offline Q-function learning on frozen VLM representations. By first fine-tuning a VLM to emphasize actionable UI features, then training Q-functions via TD-learning on these frozen representations, and finally extracting a policy using Best-of-N ranking, Digi-Q achieves state-of-the-art performance without any environment interaction. The method outperforms prior offline methods by 21.2% relative improvement and matches on-policy RL in some cases while being more data-efficient and scalable.

## Method Summary
Digi-Q employs a three-stage approach: (1) Representation fine-tuning: Fine-tune a VLM (LLaVa-1.5-7B) using binary classification to predict if actions cause significant visual state changes (measured by L2 image distance threshold); (2) TD-learning on frozen VLM representations: Train Q and V function heads (small MLPs) using offline temporal-difference learning with target networks; (3) Best-of-N policy extraction: Sample N actions per state, select the highest-scoring action if it has positive advantage, and train the policy via supervised imitation. The method uses frozen intermediate-layer VLM features to avoid the computational and stability issues of end-to-end fine-tuning while leveraging the representation power of VLMs.

## Key Results
- Achieves 21.2% relative improvement over prior offline RL methods on Android-in-the-Wild tasks
- Matches performance of on-policy RL methods (DigiRL, OPRO) in some cases
- Outperforms AWR (19.4% vs 58.0%) and REINFORCE (37.5% vs 58.0%) on Web Shopping test tasks
- Demonstrates better data efficiency and scalability compared to end-to-end fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1: Representation Fine-Tuning Induces Action-Sensitive Features
Fine-tune the VLM with binary classification to predict whether actions cause significant visual changes (L2 distance threshold ε). This forces the VLM to attend to interactive UI elements and how actions affect them, solving the problem where off-the-shelf VLMs ignore action-relevant information. If actions causing visual changes don't correlate with task progress (e.g., distracting animations), the binary labels will mislead fine-tuning.

### Mechanism 2: Frozen VLM Backbone Prevents TD Instability
After representation fine-tuning, freeze all VLM parameters and train only small MLP heads on frozen embeddings. This avoids the computational and stability issues of end-to-end TD-learning on billion-parameter VLMs. The fine-tuned VLM representations are assumed to already encode all needed features for value prediction. If downstream tasks require features not captured during fine-tuning, the frozen backbone becomes a bottleneck.

### Mechanism 3: Best-of-N Extraction Avoids Negative Gradients and Conservatism
Sample N actions per state, rank by Q-values, and select only positively-scored best actions for policy training. This avoids REINFORCE's destabilizing negative gradient terms and AWR's excessive conservatism. The method only trains on positively-scored best actions, with an implicit KL constraint against the less conservative Best-of-N policy. If Q-function rankings are unreliable (similar scores for good/bad actions), Best-of-N degenerates to random selection.

## Foundational Learning

- **Concept: Temporal-Difference (TD) Learning with Target Networks**
  - **Why needed here:** Digi-Q uses one-step bootstrapped targets; target networks prevent divergence in offline Q-learning.
  - **Quick check question:** If target networks were updated every step instead of via slow EMA, what symptom would you expect to see in Q-value training curves?

- **Concept: Offline RL Distribution Shift**
  - **Why needed here:** Policy must be learned from historical data; actions not in dataset have unreliable Q-estimates.
  - **Quick check question:** Why does Digi-Q sample candidate actions from π_β (behavior cloned policy) rather than from a uniform distribution over all possible actions?

- **Concept: State-Action vs. State-Only Value Functions**
  - **Why needed here:** Q(s,a) allows evaluating multiple actions per state without rollouts, unlike MC state-value methods.
  - **Quick check question:** Given a Q-function and N=16 sampled actions, how would you detect if the Q-function has degenerated into a state-only value function (ignoring action input)?

## Architecture Onboarding

- **Component map:**
  Offline Dataset D → [Stage 1: VLM Fine-Tuning] → Fine-tuned VLM (frozen) → Extract embeddings f(s,a), f(s) → [Stage 2: TD Learning] → Q-function head, V-function head → [Stage 3: Best-of-N Extraction] → Trained policy π_φ

- **Critical path:**
  1. Compute L2 distance between consecutive screenshots; threshold at ε to assign binary labels for representation fine-tuning.
  2. Monitor TD error variance during Q/V training; check target network update rate τ and learning rates if exploding.
  3. Pre-compute K=64 actions per state from π_β; verify diversity (not all identical) for Best-of-N training.

- **Design tradeoffs:**
  - **N (actions sampled):** Higher N improves performance (monotonic gain from N=1 to 16) but increases compute per policy update. Paper uses N=16.
  - **Representation fine-tuning data:** Can use same offline dataset; no need for separate annotations if using visual change heuristic.
  - **VLM backbone size:** Paper uses 7B; smaller models may not capture sufficient UI semantics; larger models increase frozen embedding extraction cost.

- **Failure signatures:**
  - **Q ignores action input:** Check if Q(s, a_i) values are nearly identical for different actions at same state.
  - **Policy doesn't improve:** Check if Best-of-N selected actions have positive advantage; if all negative, threshold prevents learning.
  - **TD divergence:** Q-values explode or oscillate; likely learning rate too high or target network update too fast.

- **First 3 experiments:**
  1. **Sanity check representation fine-tuning:** Visualize VLM attention on UI elements; fine-tuned model should attend to interactive elements more than off-the-shelf model.
  2. **Ablate N for policy extraction:** Run with N=1, N=4, N=8, N=16; expect monotonically improving success rate. If not, Q-function rankings are unreliable.
  3. **Compare TD vs. MC for critic training:** Train Q-function with Monte-Carlo return regression vs. TD; expect TD to outperform by ~20%. If not, check reward sparsity or horizon issues.

## Open Questions the Paper Calls Out
None

## Limitations
- **Visual-change assumption:** The method assumes visual pixel changes reliably indicate actionable state transitions, which may not hold for loading animations or cosmetic changes.
- **Offline-only evaluation:** Cannot verify generalization to out-of-distribution states or unseen UI patterns without online testing.
- **Weak actionability analysis:** Limited analysis of what the fine-tuned VLM actually learns to attend to in terms of actionable features.

## Confidence
- **High confidence:** Computational efficiency gains from frozen VLM representations and small MLP heads are well-supported by architecture design and ablation.
- **Medium confidence:** 21.2% relative improvement over offline baselines is convincing, but comparison to on-policy methods is less direct due to different data collection methods.
- **Low confidence:** Claim that representation fine-tuning "amplifies coverage over actionable information" is weakly supported with limited analysis of learned attention patterns.

## Next Checks
1. **Label quality analysis:** Examine L2 distance-based label distribution across offline dataset; correlate with actual task progress annotations to verify visual-change heuristic captures meaningful actions.
2. **Q-function action sensitivity test:** For subset of states, visualize Q-value distribution across sampled actions; compute entropy of rankings to detect state-only value function degeneration.
3. **OOD generalization probe:** Create out-of-distribution UI states (different layouts, languages, visual styles) not in offline dataset; evaluate whether fine-tuned VLM representations still capture actionable features.