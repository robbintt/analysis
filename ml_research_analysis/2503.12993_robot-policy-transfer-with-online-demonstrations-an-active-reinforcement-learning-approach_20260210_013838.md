---
ver: rpa2
title: 'Robot Policy Transfer with Online Demonstrations: An Active Reinforcement
  Learning Approach'
arxiv_id: '2503.12993'
source_url: https://arxiv.org/abs/2503.12993
tags:
- policy
- transfer
- demonstrations
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Policy Transfer with Online Demonstrations,
  an active learning from demonstrations (LfD) algorithm for robot policy transfer
  that leverages online episodic expert demonstrations during training to address
  covariance shift issues inherent in offline demonstrations. The method extends the
  EARLY framework to policy transfer, using uncertainty-based queries to optimize
  both the timing and content of demonstration requests under a limited budget.
---

# Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach

## Quick Facts
- **arXiv ID:** 2503.12993
- **Source URL:** https://arxiv.org/abs/2503.12993
- **Reference count:** 39
- **One-line primary result:** Active policy transfer method using online episodic demonstrations achieves up to 100% success rates across eight robotic transfer scenarios.

## Executive Summary
This paper introduces Policy Transfer with Online Demonstrations (PTD), an active learning from demonstrations algorithm that addresses covariance shift in robot policy transfer. The method extends the EARLY framework to transfer learning by using uncertainty-based queries to optimize demonstration timing and content during training under a limited budget. Evaluated across eight robotic scenarios with diverse environment characteristics and robot embodiments, PTD achieves significantly higher success rates and improved sample efficiency compared to baselines. Preliminary sim-to-real experiments on a Franka Emika robot manipulator validate the effectiveness of transferred policies in real-world settings.

## Method Summary
PTD extends AWAC (Advantage Weighted Actor-Critic) to policy transfer by initializing the target policy from a source policy and using online episodic demonstrations to address covariance shift. The algorithm maintains a history of TD-error uncertainties from recent rollouts, uses adaptive thresholding to determine when to query demonstrations, and merges both online and demonstration data for policy updates. Key hyperparameters include a history window of 20, query threshold at the 10th percentile of recent uncertainties, and a demonstration budget of 10 episodes per target scenario.

## Key Results
- Achieves 100% success rates in six out of eight transfer scenarios
- Demonstrates significant improvement in sample efficiency compared to AWAC baseline
- Successfully transfers policies to new robot embodiments (UR5) and environment characteristics (friction, object shape)
- Preliminary sim-to-real validation shows effective transfer to Franka Emika robot manipulator

## Why This Works (Mechanism)

### Mechanism 1: Targeted Covariance Shift Mitigation
The algorithm addresses distribution mismatch between offline demonstrations and the active policy by querying demonstrations during training from specific initial states where uncertainty is highest. This updates the data distribution to match the state visitation of the target task policy rather than relying on static offline samples. The core assumption is that the demonstrator can generate valid trajectories from queried initial states that fall within the robot's current failure modes.

### Mechanism 2: Uncertainty-Guided Querying via TD-Error
TD-error magnitude serves as a proxy for policy uncertainty, identifying states where the value function is misaligned with task dynamics. High TD-error implies the agent is "surprised" by outcomes, signaling a need for demonstration. The core assumption is that high TD-error correlates with suboptimal actions rather than just environmental stochasticity.

### Mechanism 3: Adaptive Thresholding for Budget Efficiency
A history-based adaptive threshold optimizes value extracted from limited demonstration budget. The system maintains recent uncertainty values and sets the query threshold based on a percentile of recent errors, ensuring queries only trigger for the worst relative performance as the overall policy improves. The core assumption is that the learning curve is smooth enough for recent history to serve as a valid baseline.

## Foundational Learning

**Concept: Covariance Shift (in LfD)**
- **Why needed:** Core problem the paper claims to solve - policy gradients shift the agent's state distribution, rendering fixed offline demonstrations irrelevant or harmful
- **Quick check:** If I train a robot with data collected by a random policy, why will it fail when it tries to act competently?

**Concept: Advantage Weighted Actor-Critic (AWAC)**
- **Why needed:** Method uses AWAC to blend demonstrations with online RL, forcing policy to maximize likelihood of actions with high advantage
- **Quick check:** How does AWAC differ from standard behavioral cloning when demonstration buffer is empty?

**Concept: Episodic Control / Resets**
- **Why needed:** Method relies on querying demonstration starting from a specific initial state
- **Quick check:** Why is the ability to reset environment to a specific state a hard requirement for this algorithm?

## Architecture Onboarding

**Component map:** Source Policy → Transfer Policy → Uncertainty Estimator → Query Manager → Expert Demonstrator → Demo Buffer + Rollout Buffer → AWAC Update

**Critical path:** Rollout → TD-Error Calc → Update History → Threshold Check → (Optional) Expert Query → Buffer Merge → AWAC Update

**Design tradeoffs:**
- History Window (N_h): Large windows smooth threshold but react slowly; small windows are noisy
- Ratio Threshold (r_query): Controls budget burn rate; 0.1 means querying only worst 10% of rollouts

**Failure signatures:**
- Silent Failure: Q-values collapse to zero, causing agent to stop querying despite failures
- Query Loop: Policy update fails to increase expert action likelihood, causing repeated failures from same state

**First 3 experiments:**
1. Metric Validation: Scatter plot of Uncertainty vs. Episode Return to verify assumed correlation
2. Ablation on Thresholding: Compare Fixed vs. Adaptive vs. Random Querying on single transfer scenario
3. Budget Sensitivity: Run with N_d = {1, 5, 10, 20} to find minimal viable budget for target domains

## Open Questions the Paper Calls Out
1. How to adapt active learning framework to maintain performance while accounting for human factors like fatigue or varying expertise during online data collection?
2. Can the method be generalized to scenarios where source and target tasks have different state-action spaces or policy network structures?
3. Is the method feasible in real-world environments without precise reset functions to specific initial states?
4. How does the method perform under real-world perception covariate shift when relying on raw sensory input rather than privileged state information?

## Limitations
- Evaluation lacks statistical significance tests and multiple random seeds across experiments
- No ablation studies isolating contribution of individual components (uncertainty estimation, adaptive thresholding, online vs. offline)
- Sim-to-real validation limited to single task without quantitative baseline comparison in real environment
- Assumes demonstrator can successfully execute trajectories from queried initial states

## Confidence
- **Medium**: The mechanism linking TD-error to policy uncertainty is well-founded, but lack of statistical validation and component ablation limits confidence in claimed performance gains

## Next Checks
1. **Statistical validation**: Re-run all eight scenarios with 5 random seeds, report mean±std for success rates, and perform paired t-tests against AWAC baseline
2. **Component ablation**: Implement variants with fixed threshold, random querying, and offline-only demonstrations to isolate contribution of adaptive online querying
3. **Query budget sensitivity**: Systematically vary demonstration budget N_d from 1 to 30 episodes across scenarios to identify minimal viable budget and test scalability