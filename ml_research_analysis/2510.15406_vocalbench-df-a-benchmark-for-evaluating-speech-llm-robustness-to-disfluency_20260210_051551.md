---
ver: rpa2
title: 'VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency'
arxiv_id: '2510.15406'
source_url: https://arxiv.org/abs/2510.15406
tags:
- speech
- disfluency
- original
- repetition
- largest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VOCALBENCH-DF, the first benchmark designed\
  \ to evaluate Speech Large Language Models (Speech-LLMs) on their ability to handle\
  \ speech disfluency. It categorizes disfluency along two dimensions\u2014linguistic\
  \ realization and interactional interference\u2014and decomposes them into six subcategories\
  \ and nine subtasks."
---

# VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency

## Quick Facts
- **arXiv ID**: 2510.15406
- **Source URL**: https://arxiv.org/abs/2510.15406
- **Reference count**: 9
- **Primary result**: Introduces VOCALBENCH-DF, the first benchmark evaluating Speech-LLMs' ability to handle speech disfluency across 22 mainstream models, revealing substantial performance degradation particularly under phoneme-level disfluency and long-context modeling.

## Executive Summary
VOCALBENCH-DF is the first benchmark specifically designed to evaluate Speech Large Language Models' (Speech-LLMs) robustness to speech disfluency. The benchmark categorizes disfluency along two dimensions—linguistic realization and interactional interference—and decomposes them into six subcategories and nine subtasks. Through systematic evaluation of 22 mainstream Speech-LLMs, the study reveals that most models experience substantial performance degradation, particularly under phoneme-level disfluency and long-context modeling. The findings identify stronger speech recognition and reasoning capabilities as critical for improving robustness, highlighting the urgent need for methods that enhance disfluency handling to build more inclusive Speech-LLMs.

## Method Summary
The benchmark introduces a comprehensive framework for evaluating Speech-LLMs' ability to handle disfluency by categorizing disfluency types along two dimensions: linguistic realization and interactional interference. This framework decomposes disfluency into six subcategories and nine subtasks, creating a structured evaluation protocol. The study evaluates 22 mainstream Speech-LLMs across these diverse disfluency types using both naturally occurring and synthetically injected disfluency data. Performance is measured through task completion accuracy and robustness metrics, with particular attention to phoneme-level disfluency and long-context modeling challenges. The analysis systematically identifies correlations between model capabilities and disfluency handling performance.

## Key Results
- Most Speech-LLMs experience substantial performance degradation when handling disfluent speech, particularly under phoneme-level disfluency conditions
- Long-context modeling presents significant challenges for Speech-LLMs when processing disfluent speech sequences
- Stronger speech recognition and reasoning capabilities are identified as critical factors for improving robustness to disfluency

## Why This Works (Mechanism)
The benchmark works by providing a systematic framework that captures the complexity of speech disfluency through structured categorization. By decomposing disfluency into six subcategories and nine subtasks across linguistic and interactional dimensions, the evaluation captures both surface-level disruptions and deeper processing challenges. The mechanism reveals that disfluency impacts Speech-LLMs through two primary pathways: degraded speech recognition accuracy due to altered acoustic patterns, and compromised reasoning capabilities when processing interrupted or self-corrected speech segments. The systematic evaluation across 22 models provides robust evidence for performance patterns and capability correlations.

## Foundational Learning

**Speech Disfluency Types**: Understanding different disfluency categories (repetitions, corrections, hesitations) is essential for evaluating model robustness, as each type presents unique processing challenges.
- *Why needed*: Different disfluency types require different handling strategies and impact models differently
- *Quick check*: Can you identify and categorize examples of speech disfluency?

**Speech-LLM Architecture**: Knowledge of how speech input is processed through ASR and LLM components helps understand where disfluency impacts occur
- *Why needed*: Disfluency can affect both speech recognition and language understanding stages
- *Quick check*: Can you trace the data flow from raw audio through to language understanding?

**Performance Evaluation Metrics**: Understanding accuracy metrics and robustness measures is crucial for interpreting benchmark results
- *Why needed*: Different metrics capture different aspects of model performance under disfluency
- *Quick check*: Can you explain what constitutes robust performance versus raw accuracy?

## Architecture Onboarding

**Component Map**: Raw Audio -> Speech Recognition (ASR) -> Language Understanding (LLM) -> Output Generation
- ASR handles acoustic processing and basic transcription
- LLM component processes language understanding and reasoning
- Output generation produces final responses

**Critical Path**: Speech Recognition -> Language Understanding
- Disfluency first impacts ASR accuracy through altered acoustic patterns
- Subsequent LLM processing depends on ASR output quality
- Reasoning capabilities are tested when processing interrupted or corrected speech

**Design Tradeoffs**: The benchmark must balance synthetic disfluency injection (controlled but potentially unrealistic) against natural disfluency data (realistic but harder to control)
- Synthetic injection allows systematic testing but may miss complex interactions
- Natural data provides realism but introduces confounding variables
- The evaluation must consider both acoustic and linguistic dimensions of disfluency

**Failure Signatures**: 
- Performance degradation under phoneme-level disfluency indicates ASR limitations
- Struggles with long-context disfluency suggest LLM reasoning constraints
- Inconsistent performance across disfluency types reveals architectural weaknesses

**First 3 Experiments**:
1. Test a baseline Speech-LLM on clean speech versus disfluent speech to establish performance degradation baseline
2. Evaluate model performance on different disfluency subcategories to identify specific weaknesses
3. Compare synthetic versus naturally occurring disfluency performance to validate benchmark methodology

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond the need for improved disfluency handling methods in Speech-LLMs.

## Limitations
- The benchmark focuses exclusively on English speech data, limiting generalizability to other languages and dialects
- Synthetic disfluency injection is used for some categories, which may not fully capture the complexity of naturally occurring speech disfluencies
- The analysis primarily attributes performance degradation to speech recognition and reasoning capabilities, potentially overlooking other contributing factors such as model architecture or training data composition

## Confidence
- **High**: Performance degradation across Speech-LLMs based on systematic evaluation of 22 mainstream models across multiple disfluency types
- **High**: Phoneme-level disfluency being particularly challenging due to consistent empirical evidence across models
- **Medium**: Stronger speech recognition and reasoning capabilities as critical factors for robustness improvement, as the analysis cannot definitively isolate these factors from other potential contributors

## Next Checks
1. Replicate the benchmark evaluation using naturally occurring speech disfluency data rather than synthetic injection to validate findings
2. Test the benchmark across multiple languages and dialects to assess cross-linguistic robustness patterns
3. Conduct ablation studies isolating speech recognition, reasoning, and other model components to quantify their individual contributions to disfluency handling performance