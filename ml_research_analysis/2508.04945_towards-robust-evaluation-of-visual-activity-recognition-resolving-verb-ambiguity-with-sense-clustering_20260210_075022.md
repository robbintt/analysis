---
ver: rpa2
title: 'Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity
  with Sense Clustering'
arxiv_id: '2508.04945'
source_url: https://arxiv.org/abs/2508.04945
tags:
- verb
- image
- evaluation
- clusters
- verbs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a clustering-based evaluation framework for
  visual activity recognition to address verb ambiguity in activity datasets. The
  authors construct verb sense clusters through a two-step vision-language clustering
  approach, showing that each image maps to around four distinct clusters representing
  different perspectives.
---

# Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering

## Quick Facts
- arXiv ID: 2508.04945
- Source URL: https://arxiv.org/abs/2508.04945
- Reference count: 40
- Primary result: Cluster-based evaluation improves accuracy and human alignment in activity recognition by addressing verb ambiguity

## Executive Summary
This paper addresses the fundamental challenge of verb ambiguity in visual activity recognition by introducing a clustering-based evaluation framework. The authors recognize that activity datasets often contain ambiguous verbs that can represent multiple senses or perspectives, leading to unreliable model evaluation when using exact-match metrics. Through a two-step vision-language clustering approach, they construct verb sense clusters that capture both synonymy and multi-perspective ambiguities, enabling more nuanced assessment of model performance.

The framework demonstrates that cluster-based evaluation not only achieves higher accuracy metrics but also better aligns with human judgments compared to traditional exact-match evaluation. By mapping each image to approximately four distinct clusters containing close to two synonymous verbs on average, the approach provides a more robust foundation for evaluating activity recognition models. The code for this methodology is made publicly available for community adoption.

## Method Summary
The authors propose a two-step vision-language clustering approach to address verb ambiguity in activity recognition datasets. First, they employ vision-language models to generate semantic embeddings for activity verbs and corresponding images. These embeddings capture the contextual meaning of verbs within visual scenes. Second, they apply clustering algorithms to group semantically similar verb senses together, creating clusters that represent distinct interpretations of activities. Manual annotation refines these clusters by resolving ambiguities and ensuring semantic coherence. Each image is then mapped to multiple clusters representing different perspectives or interpretations of the depicted activity. This clustered representation enables evaluation metrics that account for synonymy and contextual variations in activity descriptions, providing a more robust assessment framework than traditional exact-match approaches.

## Key Results
- Cluster-based evaluation achieves higher accuracy metrics compared to exact-match evaluation across multiple activity recognition models
- The framework demonstrates better alignment with human judgments, capturing semantic nuances missed by exact-match approaches
- Each image maps to approximately four distinct clusters on average, with each cluster containing close to two synonymous verbs

## Why This Works (Mechanism)
The approach works by addressing the fundamental limitation of exact-match evaluation: it cannot account for verb ambiguity, synonymy, or multi-perspective interpretations of activities. By clustering verb senses based on their semantic relationships and visual contexts, the framework creates evaluation clusters that reflect how humans naturally interpret and describe activities. This semantic clustering captures both direct synonyms and related activity concepts that might be used interchangeably by different annotators or models. The two-step vision-language approach ensures that clusters are grounded in both linguistic semantics and visual context, making the evaluation more robust to variations in how activities can be described while maintaining semantic fidelity.

## Foundational Learning

Semantic Clustering (why needed: groups related concepts; quick check: ensure clusters capture intuitive groupings)
- Vision-Language Embeddings (why needed: bridge visual and linguistic semantics; quick check: verify embeddings capture contextual meaning)
- Activity Recognition Evaluation (why needed: standard metrics insufficient for ambiguous verbs; quick check: compare exact-match vs cluster-based metrics)
- Manual Annotation Refinement (why needed: ensures cluster quality; quick check: measure inter-annotator agreement)
- Synonymy Detection (why needed: captures equivalent activity descriptions; quick check: validate synonym pairs within clusters)
- Multi-Perspective Interpretation (why needed: accounts for different valid descriptions; quick check: verify multiple clusters per image)

## Architecture Onboarding

Component Map: Vision-Language Model -> Embedding Generation -> Clustering Algorithm -> Manual Annotation -> Evaluation Framework

Critical Path: Vision-Language Model → Embedding Generation → Clustering → Manual Annotation → Cluster-Based Evaluation
The vision-language model generates semantic embeddings that serve as the foundation for clustering. These embeddings are clustered using algorithms that group semantically similar verb senses. Manual annotation refines the clusters to ensure quality. The resulting clusters form the basis for evaluation metrics that better reflect human judgment.

Design Tradeoffs:
The two-step approach balances automation (vision-language clustering) with human oversight (manual annotation), trading computational efficiency for semantic accuracy. Using vision-language models enables cross-modal semantic understanding but may propagate model biases. Manual annotation ensures quality but limits scalability and introduces potential subjectivity.

Failure Signatures:
Poor clustering quality may manifest as semantically incoherent clusters or over-segmentation of related concepts. Vision-language model limitations could lead to embeddings that don't capture subtle semantic distinctions. Manual annotation disagreements may indicate inherently ambiguous concepts that are difficult to cluster definitively.

First Experiments:
1. Test clustering quality by measuring intra-cluster semantic coherence using word similarity metrics
2. Evaluate the impact of different vision-language models on embedding quality and downstream clustering
3. Compare human evaluation alignment between cluster-based and exact-match approaches on held-out test sets

## Open Questions the Paper Calls Out

None

## Limitations

The manual annotation process for verb clusters may contain subjective biases despite multiple annotator involvement, potentially affecting evaluation reliability. The two-step clustering approach might not capture all semantic nuances of complex or context-dependent activity verbs. The methodology's generalizability across diverse activity recognition datasets requires further validation, as the four-cluster average may not hold universally.

## Confidence

High Confidence: Cluster-based evaluation consistently yields higher accuracy metrics than exact-match evaluation across tested models
Medium Confidence: The claimed better alignment with human judgment needs additional validation studies with larger annotator pools
Medium Confidence: The average of two synonymous verbs per cluster is based on specific datasets and may not generalize

## Next Checks

1. Conduct larger-scale human studies comparing exact-match and cluster-based evaluation across multiple independent annotator groups to validate alignment with human judgment
2. Test the clustering methodology on diverse activity recognition datasets beyond the current study to assess generalizability of the four-cluster average
3. Implement ablation studies to quantify the impact of different vision-language model choices on clustering results and final evaluation outcomes