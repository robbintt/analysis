---
ver: rpa2
title: Learning Conformal Explainers for Image Classifiers
arxiv_id: '2509.21209'
source_url: https://arxiv.org/abs/2509.21209
tags:
- prediction
- explanations
- conformal
- conformity
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a conformal prediction-based method for generating
  image explanations that users can directly control in terms of fidelity. The approach
  identifies minimal subsets of salient features that preserve a model's prediction,
  using four proposed conformity functions to quantify how well explanations align
  with model predictions.
---

# Learning Conformal Explainers for Image Classifiers

## Quick Facts
- **arXiv ID:** 2509.21209
- **Source URL:** https://arxiv.org/abs/2509.21209
- **Reference count:** 40
- **One-line primary result:** A conformal prediction-based method that generates image explanations with user-controlled fidelity, where super-pixel-based conformity functions outperform pixel-wise ones in both efficiency and prediction preservation.

## Executive Summary
This paper introduces a method for generating image explanations with formal confidence guarantees using conformal prediction. The approach identifies minimal subsets of salient features that preserve a model's prediction, offering users direct control over the trade-off between explanation compactness and fidelity. By applying four different conformity functions to evaluate feature subsets, the method provides statistically valid confidence guarantees without requiring ground-truth explanations for calibration.

## Method Summary
The method operates as a post-processing layer on top of existing feature attribution techniques. It first trains a black-box classifier (ResNet50), then computes feature attributions using various explainers (FastSHAP, GradientSHAP, etc.). A held-out calibration set is used to determine a threshold score through one of four conformity functions that measure how well feature subsets explain predictions. During inference, this threshold filters attributions to produce a binary mask representing the "sufficient explanation" - the minimal feature subset needed to preserve the original prediction with specified confidence.

## Key Results
- FastSHAP consistently produces the most efficient explanations across all tested datasets
- Super-pixel-based conformity functions (Scaled Values, Super-Pixels) outperform pixel-wise approaches in both informational efficiency and prediction fidelity
- The method successfully provides user-controllable fidelity without requiring ground-truth explanations for calibration
- The conformal framework ensures statistically valid confidence guarantees for the generated explanations

## Why This Works (Mechanism)
The method leverages inductive conformal prediction to convert heuristic feature importance scores into explanations with formal statistical guarantees. By treating feature attribution scores as non-conformity measures and learning a threshold from calibration data, it ensures that the resulting explanations preserve model predictions with user-specified confidence. The four conformity functions provide different strategies for aggregating and thresholding these scores, with super-pixel approaches offering better coherence and efficiency by grouping related pixels.

## Foundational Learning
- **Concept: Inductive Conformal Prediction**
  - **Why needed here:** This is the core statistical engine. It's the framework used to convert heuristic importance scores into explanations with formal, user-specified confidence guarantees.
  - **Quick check question:** Given a calibration set of non-conformity scores, can you compute the threshold for a 95% confidence interval?

- **Concept: Feature Attribution Methods (e.g., SHAP, Integrated Gradients)**
  - **Why needed here:** These are the inputs to the system. The proposed method is a post-processing layer that operates *on top of* these explainers to filter and refine their outputs.
  - **Quick check question:** What is the fundamental difference in how Shapley value-based methods (like FastSHAP) and gradient-based methods (like Integrated Gradients) assign importance?

- **Concept: Image Segmentation (Super-pixels)**
  - **Why needed here:** A critical preprocessing step for two of the four proposed conformity functions. It's the technique that groups pixels into meaningful regions to overcome the noise and sparsity of pixel-level attributions.
  - **Quick check question:** How does the SLIC algorithm differ from simple grid-based image partitioning?

## Architecture Onboarding
- **Component map:** Trained black-box model -> Feature attribution explainer -> Conformity function computation -> Calibration threshold learning -> Inference mask generation
- **Critical path:** The choice and implementation of the **Conformity Function** is the most critical design decision. It dictates what the statistical guarantee means and how efficient (compact) the final explanation will be. Empirical results strongly point to **Scaled Values (Super-Pixel)** as the strongest candidate.
- **Design tradeoffs:**
  - **Pixelwise vs. Super-Pixel:** Pixelwise is simpler but yields noisy, fragmented explanations. Super-Pixel requires an additional segmentation step (e.g., SLIC) but produces more coherent and human-interpretable regions.
  - **Explainer Choice:** FastSHAP is empirically superior but requires training an explainer model. Simpler explainers like Saliency are faster but produce less efficient explanations. The method's quality is fundamentally bounded by the fidelity of the input explainer.
  - **Confidence Level (ε):** A lower ε (e.g., 0.01 for 99% confidence) produces larger, more conservative explanations. A higher ε (e.g., 0.15 for 85% confidence) produces more compact explanations but with a higher risk of excluding important features.
- **Failure signatures:**
  - **Empty Explanation (S_E = ∅):** σ_ε is too high, or the explainer has uniformly low attribution scores. Check calibration data distribution and explainer performance.
  - **Trivial Explanation (S_E ≈ Entire Image):** The explainer is producing uninformative attributions (e.g., all features seem equally important), leading to a very low σ_ε.
  - **Prediction Change:** The explanation *fails* to preserve the model's prediction. This indicates a failure of the underlying assumptions (e.g., non-i.i.d. data, poor explainer fidelity) and is the primary event the confidence level is designed to bound.
- **First 3 experiments:**
  1. Reproduce Baseline Comparison: On a single dataset (e.g., Imagenette), compare the four conformity functions using FastSHAP. Plot SE size vs. Fidelity to confirm the paper's finding that super-pixel methods are more efficient.
  2. Ablate the Explainer: Swap FastSHAP for a simpler explainer (e.g., Saliency or InputXGradient) and measure the degradation in informational efficiency (i.e., how much larger the S_E regions become).
  3. Stress Test the Guarantee: For a fixed explainer and conformity function, systematically vary the confidence level (e.g., 80%, 90%, 95%, 99%) and empirically measure the fidelity on a held-out test set. The observed fidelity should meet or exceed the target confidence level.

## Open Questions the Paper Calls Out
- Can explanation algorithms be trained to explicitly incorporate uncertainty information to enhance robustness, rather than applying conformal prediction as a post-hoc step? The authors state, "Future research directions include the design of explanation algorithms that explicitly incorporate uncertainty information... drawing inspiration from the conformal training of Stutz et al. (2022)." This remains unexplored as the current method applies conformal prediction post-hoc to existing explainers.
- Does training real-time explainers, such as FastSHAP, using super-pixels as input features rather than raw pixels improve both interpretability and computational efficiency? The conclusion suggests "...the training of real-time explainers, e.g., FastSHAP, using super-pixels derived from image segmentation algorithms as input features..." as a promising direction. The current work uses super-pixels only for the conformity function (post-processing), not as the fundamental input unit for the explanation model itself.
- How robust are the validity guarantees and fidelity of the generated explanations in scenarios with strictly limited calibration data? Appendix A notes the method "may be less suitable in scenarios with limited data availability" due to the computational burden and the requirement for a held-out calibration set. While conformal prediction theory assumes exchangeability, the practical degradation of fidelity guarantees and efficiency with small calibration sets was not empirically evaluated in the study.

## Limitations
- The method's performance is fundamentally bounded by the quality of the underlying feature attribution explainer, with simple gradient methods producing noisy attributions that lead to inefficient explanations.
- The approach requires a held-out calibration set, making it less suitable for scenarios with limited data availability.
- Pixel-wise conformity functions produce fragmented explanations that fail to preserve predictions, requiring the additional computational overhead of super-pixel segmentation.

## Confidence
- **High Confidence:** The empirical finding that super-pixel-based conformity functions outperform pixel-wise approaches in both efficiency and fidelity.
- **Medium Confidence:** The claim that FastSHAP consistently produces the most efficient explanations, given limited ablation studies with alternative explainers.
- **Medium Confidence:** The assertion that the method provides "user-controllable" fidelity, as the relationship between confidence levels and actual prediction preservation requires more systematic validation across diverse datasets.

## Next Checks
1. Conduct a systematic ablation study varying the explainer type (Saliency → InputXGradient → GradientSHAP → FastSHAP) on a single dataset to quantify the performance ceiling imposed by attribution quality.
2. Design an experiment that deliberately corrupts the calibration set distribution to test the robustness of the conformal guarantee under distribution shift.
3. Implement the "Scaled Values" conformity function and compare its performance directly against the "Super-Pixels" variant to determine whether normalization alone can match the efficiency gains of spatial grouping.