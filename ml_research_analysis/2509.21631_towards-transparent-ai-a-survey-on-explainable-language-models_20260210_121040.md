---
ver: rpa2
title: 'Towards Transparent AI: A Survey on Explainable Language Models'
arxiv_id: '2509.21631'
source_url: https://arxiv.org/abs/2509.21631
tags:
- language
- methods
- zhang
- explanations
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive review of XAI methods for
  language models, systematically categorizing approaches by transformer architectures
  (encoder-only, decoder-only, encoder-decoder) and evaluating them through plausibility
  and faithfulness. It addresses the challenge of interpreting complex LMs by organizing
  techniques based on architectural characteristics and identifying open research
  challenges including scalability, performance-interpretability trade-offs, and multimodal
  explainability.
---

# Towards Transparent AI: A Survey on Explainable Language Models

## Quick Facts
- **arXiv ID:** 2509.21631
- **Source URL:** https://arxiv.org/abs/2509.21631
- **Reference count:** 40
- **Key outcome:** Comprehensive survey categorizing XAI methods for language models by transformer architectures with evaluation through plausibility and faithfulness metrics

## Executive Summary
This survey systematically reviews Explainable AI (XAI) methods for language models, addressing the critical need for transparency in increasingly complex AI systems. The authors organize approaches based on transformer architectures (encoder-only, decoder-only, encoder-decoder) and evaluate them using plausibility and faithfulness metrics. The work identifies key challenges including scalability, performance-interpretability trade-offs, and multimodal explainability, while providing a detailed taxonomy and comparison of existing methods.

## Method Summary
The survey employs a systematic literature review methodology, categorizing XAI approaches for language models through a multi-dimensional framework. Methods are organized by transformer architecture type and evaluated using established metrics from the ERASER benchmark, including IoU and AUPRC for plausibility and comprehensiveness/sufficiency metrics for faithfulness. The authors analyze the strengths and limitations of each approach, creating a comprehensive taxonomy that maps the current state of XAI for LMs.

## Key Results
- Provides systematic categorization of XAI methods based on transformer architectures
- Evaluates approaches through plausibility (ERASER IoU, AUPRC) and faithfulness metrics
- Identifies open research challenges including scalability and performance-interpretability trade-offs
- Includes detailed taxonomy and comparison table of XAI methods
- Discusses future directions for developing transparent and interpretable AI methods

## Why This Works (Mechanism)
The survey's systematic approach works by providing a structured framework for understanding XAI methods across different transformer architectures. By organizing techniques based on architectural characteristics and evaluating them through standardized metrics, the authors create a foundation for comparing and developing new XAI approaches. The dual focus on plausibility (human interpretability) and faithfulness (faithfulness to the model's actual reasoning) addresses both user needs and technical rigor.

## Foundational Learning
- **Transformer architectures (encoder-only, decoder-only, encoder-decoder)**: Essential for understanding how different XAI methods apply to various LM types and their specific explainability challenges
- **Plausibility vs. faithfulness metrics**: Critical distinction between human interpretability and alignment with model reasoning, necessary for evaluating XAI method effectiveness
- **ERASER benchmark metrics (IoU, AUPRC, comprehensiveness, sufficiency)**: Standardized evaluation framework needed for comparing XAI methods across studies
- **Multimodal explainability**: Understanding how explanation methods extend beyond text to handle vision-language models and other multimodal systems
- **Scalability challenges in XAI**: Recognizing computational constraints when applying explanation methods to large-scale LMs

## Architecture Onboarding

**Component Map:** Input text -> Transformer layers -> Attention mechanisms -> Output predictions -> XAI method (post-hoc, intrinsic, or architecture-specific) -> Explanation visualization

**Critical Path:** Text input → Feature extraction → Attention computation → Prediction → Attribution method → Explanation generation

**Design Tradeoffs:** Performance vs. interpretability (more complex models harder to explain), computational cost vs. explanation quality, global vs. local explanations, model-specific vs. model-agnostic approaches

**Failure Signatures:** Explanations that don't align with model behavior (low faithfulness), overly complex visualizations that lack human interpretability (low plausibility), methods that don't scale to larger models, explanations that fail for specific input types or domains

**First Experiments:** 1) Apply LIME to a simple encoder-only model for text classification, 2) Use attention visualization on a decoder-only model for text generation, 3) Implement integrated gradients for an encoder-decoder model for translation tasks

## Open Questions the Paper Calls Out
- How to develop XAI methods that scale effectively to increasingly large language models without prohibitive computational costs
- What approaches can bridge the gap between plausibility and faithfulness in XAI explanations
- How to extend explainability methods to multimodal language models that integrate vision, text, and other modalities
- What standardized evaluation frameworks can be developed to rigorously compare XAI methods across different studies
- How to balance the trade-off between model performance and interpretability in practical applications

## Limitations
- Rapidly evolving field means some cutting-edge techniques may not be included
- Focus primarily on transformer-based architectures, potentially overlooking emerging non-transformer LM architectures
- Limited standardized benchmarks affect robustness of cross-method comparisons

## Confidence

**High confidence:**
- Categorization framework based on transformer architectures
- Identification of core open research challenges

**Medium confidence:**
- Evaluation of current XAI methods through plausibility and faithfulness metrics
- Assessment of relative importance of identified challenges

## Next Checks
1. Conduct systematic update review every 6-12 months to incorporate newly developed XAI methods and track progress on identified open challenges

2. Develop standardized benchmark suite specifically for LM explainability that combines multiple evaluation metrics for more rigorous cross-method comparisons

3. Extend categorization framework to include non-transformer LM architectures as they emerge and become more prevalent in the field