---
ver: rpa2
title: 'CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in
  Short-Video Search'
arxiv_id: '2511.15443'
source_url: https://arxiv.org/abs/2511.15443
tags:
- retrieval
- search
- crops
- user
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CroPS introduces cross-perspective positive samples from query
  reformulation, recommendation system, and world knowledge via LLMs to break the
  filter bubble effect in dense retrieval. It employs hierarchical label assignment
  with H-InfoNCE loss for fine-grained supervision.
---

# CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search

## Quick Facts
- arXiv ID: 2511.15443
- Source URL: https://arxiv.org/abs/2511.15443
- Reference count: 40
- Key outcome: CroPS achieves 69.1% Recall@100 on click-through data and 40.1% on reformulated-query data, outperforming strong baselines by 9.3% and 7.1% respectively, with +0.869% CTR improvement in online A/B tests.

## Executive Summary
CroPS addresses filter bubble effects in short-video search by introducing cross-perspective positive samples from query reformulation behavior, recommendation system engagement, and LLM-synthesized world knowledge. It employs a dual-encoder architecture with hierarchical label assignment (0-5) and H-InfoNCE loss for fine-grained contrastive learning. The method significantly improves retrieval recall and online engagement metrics while being fully deployed at Kuaishou Search serving hundreds of millions of users daily.

## Method Summary
CroPS enhances dense retrieval training through three positive sample sources: videos consumed after query reformulation (90-second window), recommendation feed interactions from users who issued queries, and LLM-synthesized video metadata. Samples are filtered by a lightweight semantic discriminator with threshold α=0.6, then assigned hierarchical labels based on source confidence. The H-InfoNCE loss enables efficient multi-level contrastive learning where each positive sample is only contrasted against strictly lower-labeled samples, computed in a single forward pass.

## Key Results
- 69.1% Recall@100 on click-through data and 40.1% on reformulated-query data
- 9.3% and 7.1% improvement over strong baselines respectively
- Online A/B tests show +0.869% CTR, +0.483% long-play rate, and -0.646% query reformulation rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-perspective positive sampling mitigates filter bubble effects by introducing relevant but previously unexposed content into training supervision.
- Mechanism: Query reformulation samples capture intent continuity when users refine searches; recommendation samples bridge isolated subsystems; LLM-synthesized samples inject external semantic associations. These three sources expand the positive sample space beyond historical click logs, reducing exposure bias in the retriever's learned relevance boundaries.
- Core assumption: Videos consumed after query reformulation, or engaged with in recommendation feeds, are genuinely relevant to the original query intent.
- Evidence anchors: Table 3 shows progressive Recall@100 improvements: Baseline 59.6% → +Query-level 63.3% → +System-level 65.7% → +World Knowledge 69.1% on click-through data.

### Mechanism 2
- Claim: Hierarchical Label Assignment enables fine-grained relevance learning that binary positive/negative labels cannot capture.
- Mechanism: Assigning discrete labels (0-5) based on sample origin creates graded supervision. Query-level augmented positives receive label 5 (highest) because they encode refined user intent after initial dissatisfaction; system-level and world-knowledge positives receive label 4; clicked videos label 4; exposed-but-unclicked label 3; progressively down to in-batch negatives at label 0.
- Core assumption: Query reformulation behavior represents the most authentic signal of user preference, meriting the highest label.
- Evidence anchors: CroPS† (binary label) shows Recall@100 decreasing by 9.2% on CT and 8.0% on QR compared to CroPS with hierarchical labels.

### Mechanism 3
- Claim: H-InfoNCE loss enables efficient hierarchical contrastive learning without computational overhead compared to standard InfoNCE.
- Mechanism: For each positive sample with label l, only samples with strictly lower labels are treated as negatives. This is implemented via a hierarchy-aware mask matrix that sets scores of higher-or-equal labeled samples to -∞ before contrastive calculation.
- Core assumption: Multi-level contrastive learning in one batch is more effective than sequential binary contrasts.
- Evidence anchors: Table 4 shows H-InfoNCE achieves 69.1% Recall@100 (CT) and 40.1% (QR) in 88h, vs InfoNCE 67.8%/38.9% in 178h.

## Foundational Learning

- Concept: Dense Retrieval with Dual-Encoder Architecture
  - Why needed here: CroPS operates on a dual-encoder (two-tower) retrieval model where query and document encoders independently produce embeddings.
  - Quick check question: Can you explain why dual-encoder architectures enable efficient ANN retrieval but suffer from limited query-document interaction compared to cross-encoders?

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: The paper's H-InfoNCE builds directly on standard InfoNCE.
  - Quick check question: What happens to contrastive learning if a relevant document is incorrectly sampled as a hard negative?

- Concept: Filter Bubble and Exposure Bias in Recommendation Systems
  - Why needed here: The core problem CroPS addresses is not a model architecture issue but a data pipeline issue.
  - Quick check question: If a retrieval model is trained only on historically clicked query-document pairs, what types of relevant content will it systematically fail to surface?

## Architecture Onboarding

- Component map:
  CroPS Data Engine -> Online Search System (P0) -> Query-Level Augmentation (P1) -> System-Level Expansion (P2) -> World Knowledge Enrichment (P3) -> Training Pipeline -> Hierarchical Label Assignment -> H-InfoNCE Loss -> Dual-Encoder Model

- Critical path:
  1. Data collection: Identify query reformulation sequences (90s window) -> extract consumed videos -> filter with discriminator
  2. Cross-system extraction: Query -> user set U -> retrieve recommendation interactions -> filter with discriminator
  3. LLM synthesis: Query + exemplar -> prompt Qwen2.5-14B -> parse JSON metadata
  4. Label assignment: Apply HLA rules (Table 1) to all samples
  5. Training: Forward pass -> compute similarity matrix -> apply hierarchy mask -> H-InfoNCE loss

- Design tradeoffs:
  - Data freshness vs. reliability: Query-level and system-level positives reflect real user behavior but are limited by platform exposure; LLM positives expand coverage but risk hallucination (4.2% irrelevant)
  - Hierarchy granularity vs. simplicity: 6-level labels provide fine-grained supervision but require careful calibration; binary labels are simpler but lose 9.2% Recall@100
  - Discriminator threshold (α): Higher threshold (e.g., 0.8) reduces false positives but limits sample quantity; α=0.6 balances coverage and quality empirically

- Failure signatures:
  - RQR increases online: Query reformulation rate rises -> positive samples are not genuinely relevant -> check discriminator calibration or LLM prompt quality
  - Recall@100 high but NDCG@4 drops: Model retrieves diverse but poorly ranked content -> HLA hierarchy may not align with actual relevance gradations -> validate label assignments against human judgment
  - Training loss plateaus early: H-InfoNCE mask may be too restrictive -> inspect label distribution and mask statistics per batch

- First 3 experiments:
  1. Baseline reproduction: Train dual-encoder (Qwen2.5-0.5B) with standard InfoNCE on P0 -> measure Recall@100 on CT and QR test sets -> establish baseline (expected ~59.6% CT, ~33.0% QR)
  2. Ablation by positive source: Incrementally add P1, then P2, then P3 with standard InfoNCE -> isolate contribution of each augmentation source before introducing hierarchy
  3. HLA validation: Compare CroPS with full H-InfoNCE vs. CroPS† (binary labels) vs. CroPS‡ (query-level label=4) -> confirm hierarchical label impact matches Table 3 results (9.2% and 2.0% degradation respectively)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CroPS be effectively integrated with generative retrieval methods?
- Basis in paper: [explicit] The conclusion states: "Future work will integrate CroPS with generative retrieval methods."
- Why unresolved: CroPS was designed for dual-encoder dense retrieval; generative retrieval uses different paradigms that may require fundamentally different positive sample integration strategies.
- What evidence would resolve it: A study applying CroPS-style multi-perspective positive sampling to generative retrieval architectures, comparing performance against standard generative baselines.

### Open Question 2
- Question: What is the optimal temporal window size for capturing query reformulation behavior?
- Basis in paper: [inferred] The paper sets a 90-second window heuristically ("within a short time window (e.g., 90 seconds)") without ablation or justification for this specific duration.
- Why unresolved: Different query types and user behaviors may require different window sizes; a fixed threshold may miss valid reformulations or include spurious ones.
- What evidence would resolve it: Systematic ablation testing across varying window sizes (e.g., 30s, 60s, 90s, 180s) measuring impact on Recall@100 for QR test data.

### Open Question 3
- Question: How can the rate of irrelevant LLM-generated positive samples be further reduced?
- Basis in paper: [inferred] Manual evaluation shows 4.2% of LLM-generated samples are rated "Irrelevant," which may introduce noise into training despite overall high quality (95.8% relevant).
- Why unresolved: The paper uses one-shot prompting but does not explore filtering mechanisms, multi-stage validation, or improved prompting strategies to reduce irrelevant generations.
- What evidence would resolve it: Comparing rejection sampling, discriminator-based filtering, or improved prompting strategies against the baseline, measuring both relevance rates and downstream retrieval performance.

### Open Question 4
- Question: Would adaptive or query-specific relevance thresholds improve the precision of augmented positive samples?
- Basis in paper: [inferred] The semantic relevance threshold α is empirically set to 0.6 universally, but different query types (ambiguous, specific, long-tail) may benefit from different thresholds.
- Why unresolved: A fixed threshold may be too permissive for broad queries (adding noisy positives) or too restrictive for specific queries (missing valid augmentations).
- What evidence would resolve it: Experiments with query-type-adaptive thresholds or learned threshold predictors, comparing against the fixed α=0.6 baseline on CT and QR metrics.

## Limitations
- Reliance on indirect relevance signals rather than explicit user feedback for positive sample validation
- Non-trivial 4.2% irrelevant rate in LLM-synthesized samples that could introduce noise
- CPSQA dataset used for training is not publicly available, limiting reproducibility
- Hierarchy and discriminator architecture appear heuristic rather than theoretically derived

## Confidence
- **High Confidence**: The 9.3% and 7.1% Recall@100 improvements over baselines, and the 0.869% CTR increase in online A/B tests
- **Medium Confidence**: The mechanism of filter bubble mitigation through multi-perspective sampling
- **Medium Confidence**: The 4.2% irrelevant rate in LLM-synthesized samples

## Next Checks
1. Manually validate relevance of 100 randomly sampled cross-perspective positives from each source (query reformulation, recommendation, LLM) with human annotators
2. Analyze retrieval results for 100 diverse queries comparing baseline vs. CroPS models for diversity and relevance quality
3. Perform ablation testing with discriminator thresholds α=0.5, 0.7, and 0.8 to determine optimal tradeoff between sample quantity and quality