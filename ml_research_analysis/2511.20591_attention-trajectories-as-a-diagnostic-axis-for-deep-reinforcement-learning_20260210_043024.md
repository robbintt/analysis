---
ver: rpa2
title: Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning
arxiv_id: '2511.20591'
source_url: https://arxiv.org/abs/2511.20591
tags:
- attention
- agents
- saliency
- agent
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scientific methodology for analyzing deep
  reinforcement learning (DRL) agent learning through quantitative analysis of saliency
  maps. The key innovation is the hierarchical-attention profile (h-profile), which
  aggregates saliency information at the object and modality level to quantify how
  agents allocate attention over time.
---

# Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.20591
- Source URL: https://arxiv.org/abs/2511.20591
- Authors: Charlotte Beylier; Hannah Selder; Arthur Fleig; Simon M. Hofmann; Nico Scherf
- Reference count: 40
- Primary result: Introduces hierarchical-attention profiles (h-profiles) to analyze and compare DRL agent learning through quantitative analysis of saliency maps, revealing algorithm-specific attention biases and vulnerabilities invisible to performance metrics alone.

## Executive Summary
This paper introduces a scientific methodology for analyzing deep reinforcement learning (DRL) agent learning through quantitative analysis of saliency maps. The key innovation is the hierarchical-attention profile (h-profile), which aggregates saliency information at the object and modality level to quantify how agents allocate attention over time. The methodology was applied across three case studies: comparing four DQN algorithms on Atari benchmarks, custom Pong environments with different reward designs, and biomechanical simulations with multimodal inputs. Key findings include systematic attention profile differences between algorithms even at matched performance levels, reward-driven attention shaping, and dynamic attention reallocation across modalities during sequential tasks.

## Method Summary
The methodology computes hierarchical-attention profiles (h-profiles) by aggregating pixel-level saliency maps into semantically meaningful objects. For each agent checkpoint, a fixed dataset of 50 non-overlapping frames is analyzed. Layer-wise Relevance Propagation (LRP) generates saliency maps from the network's output to the penultimate layer, then propagates relevance to input pixels. These pixel relevances are mapped to predefined object groups (e.g., "ball," "paddle") and averaged to create low-dimensional time-series representing agent focus. The approach was validated through perturbation experiments and dual-ball preference tests across Atari, custom Pong, and biomechanical environments.

## Key Results
- Attention profiles diverge systematically between algorithms even at matched performance levels, with DQN/QR-DQN showing vulnerability to perturbations of highly-attended features
- Reward design shapes agent attention, with corresponding behavioral preferences, revealing unintended reward-driven strategies
- Agents dynamically reallocate attention across modalities during sequential tasks, with potential overfitting to redundant cues
- These attention patterns correspond to measurable behavioral differences, demonstrating empirical links between attention profiles, learning dynamics, and agent behavior

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention Aggregation
Aggregating pixel-level saliency into semantically meaningful objects (h-profiles) transforms noisy interpretability data into a measurable trajectory. By mapping individual pixel relevances to predefined object groups and averaging over a dataset, the system creates a low-dimensional time-series representing agent focus, allowing direct comparison between algorithms where raw saliency maps would be incomparable.

### Mechanism 2: Attention-Robustness Correspondence
Systematic differences in attention profiles between algorithms predict specific vulnerabilities to environmental perturbations that performance scores miss. Algorithms that converge to "brittle" attention profiles (focusing heavily on background features) show high performance drops when those specific features are altered, while algorithms with robust attention maintain performance.

### Mechanism 3: Reward-Driven Attention Shaping
Reward structure is a primary driver of attention trajectory, causing agents to attend to task-irrelevant features if they correlate with reward. The optimization process tunes network weights to maximize reward, shifting the attention profile toward features that provide learning signals or correlate with them.

## Foundational Learning

- **Concept: Layer-wise Relevance Propagation (LRP)**
  - Why needed here: The paper relies on LRP to generate the base saliency maps before aggregation. Unlike gradients, LRP conserves relevance and offers sharper object boundaries, which is critical for the h-profile calculation.
  - Quick check question: Can you explain how LRP differs from standard gradient-based saliency in terms of conservation and localization?

- **Concept: Deep Q-Learning vs. Actor-Critic (PPO/A2C)**
  - Why needed here: The diagnostic results show these algorithm families have fundamentally different attention profiles. Understanding their update rules (TD-error vs. policy gradient) helps hypothesize why one might be more brittle than the other.
  - Quick check question: Why might a value-based method (DQN) attend to static features differently than a policy-based method (PPO)?

- **Concept: Multimodal Fusion**
  - Why needed here: The biomechanical case study analyzes how attention shifts between vision and proprioception.
  - Quick check question: How does the h-profile formula handle inputs that are vectors (joint angles) rather than images (pixels)?

## Architecture Onboarding

- **Component map:** Saliency Backbone (LRP) -> Object Mapper (labeling function) -> Aggregator (h-profile computation) -> Trajectory Database (stores profiles)

- **Critical path:** Defining the object set $O$. The paper emphasizes that objects must be present and non-overlapping for the method to work. Start by extracting RAM values (for Atari) or MuJoCo coordinates to define these masks programmatically.

- **Design tradeoffs:**
  - LRP vs. Gradients: LRP is computationally heavier but produces the "sharpest" maps. Gradient methods are faster but noisier, requiring larger datasets to average out noise.
  - Dataset Size ($N$): Paper uses $N=50$ fixed frames. Larger $N$ increases stability but slows down training evaluation.

- **Failure signatures:**
  - Occluded Objects: If objects overlap in the frame, the h-profile will incorrectly split relevance or fail to attribute it.
  - Negative Relevance: For multimodal tasks, inputs can be negative. You must use the $\alpha\beta$-rule with $\alpha=2, \beta=1$, not the standard rule used for images.

- **First 3 experiments:**
  1. Sanity Check: Train a simple DQN on Breakout. Verify that the h-profile starts with high attention on the "Score" and shifts to the "Ball" as training progresses.
  2. Perturbation Test: Take a trained agent. Occlude the object with the highest h-profile score. Verify performance drops significantly compared to occluding a low-score object.
  3. Algorithm Comparison: Train PPO and DQN on the same Pong environment. Calculate the Euclidean distance between their final h-profiles to confirm they diverge despite similar scores.

## Open Questions the Paper Calls Out

### Open Question 1
Can attention trajectories be used to predict agent robustness and generalization to novel environments before deployment? The authors state "Looking ahead, attention trajectories could be used to predict robustness and generalization, inform algorithm selection, and guide reward shaping or curriculum design." This remains unresolved as the current study demonstrates retrospective diagnostic value but does not establish predictive validity for unseen environments or tasks.

### Open Question 2
How can the h-profile methodology be extended to handle unstructured, high-dimensional inputs where predefined object labels are unavailable? The authors acknowledge the limitation that "it relies on predefined object or modality labels, which makes analysis straightforward in structured tasks but less so in raw, high-dimensional inputs."

### Open Question 3
What is the causal relationship between observed attention patterns and agent behavior, beyond the demonstrated correlational links? While perturbation experiments show that disrupting attended features affects performance, the paper establishes empirical links but does not prove causation between specific attention allocations and behavioral outcomes.

## Limitations
- Methodology requires predefined, non-overlapping object annotations, limiting applicability to environments where semantic segmentation is feasible
- LRP-based saliency generation is computationally expensive compared to gradient methods
- Results may vary with different saliency techniques
- The fixed dataset of 50 frames may not fully capture the agent's attention distribution across all possible states

## Confidence

- **High Confidence:** The core methodology (h-profile computation and its application to algorithm comparison) is well-specified and reproducible. The empirical link between attention profiles and perturbation robustness is demonstrated convincingly.
- **Medium Confidence:** The causal relationship between reward design and attention shaping, while observed, requires further validation across more diverse reward structures. The biomechanical case study's multimodal analysis is promising but limited to a single environment.
- **Low Confidence:** The claim that attention trajectories are a "promising diagnostic axis" is supported by the case studies but lacks broader validation across different task domains and agent architectures.

## Next Checks

1. **Cross-Environment Generalization:** Apply the h-profile methodology to a non-Atari environment (e.g., MuJoCo) to test whether attention-trajectory differences between algorithms persist in continuous control tasks.

2. **Saliency Method Sensitivity:** Repeat the Breakout analysis using gradient-based saliency methods to quantify how much the results depend on LRP specifically.

3. **Temporal Dynamics:** Extend the analysis to measure how quickly attention profiles change during training (e.g., after a single gradient update) to better understand the relationship between learning dynamics and attention shifts.