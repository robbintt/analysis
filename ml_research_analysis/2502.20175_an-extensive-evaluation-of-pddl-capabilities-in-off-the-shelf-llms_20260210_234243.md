---
ver: rpa2
title: An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs
arxiv_id: '2502.20175'
source_url: https://arxiv.org/abs/2502.20175
tags:
- clear
- pddl
- on-table
- planning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated 20 large language models from 7 major families
  on their ability to parse, generate, and reason with Planning Domain Definition
  Language (PDDL). Models were tested on action generation, problem generation, and
  plan generation using zero-shot prompting.
---

# An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs

## Quick Facts
- arXiv ID: 2502.20175
- Source URL: https://arxiv.org/abs/2502.20175
- Reference count: 10
- Primary result: GPT-4o, Qwen2.5-72B-instruct, and Mistral-Large2 performed best at PDDL generation tasks, but even top models struggled with long-horizon planning and semantic equivalence.

## Executive Summary
This study systematically evaluated 20 large language models across 7 major families on their ability to parse, generate, and reason with Planning Domain Definition Language (PDDL). The evaluation covered action generation, problem generation, and plan generation using zero-shot prompting across nine planning domains. While most models could generate syntactically correct PDDL actions with high success rates, performance declined significantly for complex tasks like generating equivalent problems and plans. The results indicate that current LLMs show promise as co-pilots for drafting PDDL but are not yet reliable for independent planning tasks, particularly in long-horizon scenarios.

## Method Summary
The evaluation used three benchmarks: the Oswald et al. (2024) NL-to-action benchmark (160 samples across 9 domains), the Planetarium benchmark (13,203 NL-problem pairs with 10% random sample), and 160 domain-problem pairs for plan generation. Twenty LLMs from OpenAI, Anthropic, Google, Meta, Mistral, DeepSeek, and Alibaba families were tested using zero-shot prompting only. Performance was measured across three validation levels: Parsable (syntax), Solvable (planner integration), and Equivalent (semantic match). The BFWS-FF planner served as a baseline for plan generation, achieving 86.25% success versus ~17% for best LLMs.

## Key Results
- GPT-4o achieved ~99% parsable actions, ~50% equivalent actions, and led all models in problem generation (32% equivalent)
- Most models struggled with problem generation equivalence (1-32% range) and plan generation (mostly 0-17% valid plans)
- Performance followed an inverted-U curve with respect to problem horizon—medium horizons (11-50 actions) outperformed both short and long
- LLaMA family showed systematic syntax errors (":preconditions" instead of ":precondition") across all tasks

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Transfer from Code Training
- Claim: LLMs with code-generation training can produce syntactically valid PDDL because formal language patterns transfer across structured representations.
- Mechanism: PDDL shares structural conventions with programming languages (parenthetical syntax, typed parameters, precondition-effect structure). Models exposed to code corpora recognize and replicate these patterns without understanding planning semantics.
- Core assumption: The training distribution includes sufficient formal language examples that PDDL syntax is reconstructable from pattern matching alone.
- Evidence anchors:
  - [abstract]: "LLMs have exhibited proficiency in code generation and chain-of-thought reasoning, laying the groundwork for tackling automatic formal planning tasks"
  - [section 2.5]: "most models perform well in generating correct actions... difference between [parsable and solvable] for a model is of few points"
  - [corpus]: "Teaching LLMs to Plan" paper explicitly targets instruction tuning for symbolic planning, suggesting baseline code training is insufficient
- Break condition: Novel PDDL constructs without analogs in common programming languages (e.g., derived predicates, durative actions) would show degraded performance.

### Mechanism 2: Semantic-Formal Gap in Equivalence Generation
- Claim: LLMs can parse and generate valid PDDL but cannot reliably produce semantically equivalent formulations because this requires explicit state-space reasoning.
- Mechanism: Generating equivalent problems requires the model to understand the planning state space, identify alternative initial/goal configurations that preserve solvability, and maintain logical consistency. This demands reasoning beyond pattern completion.
- Core assumption: Semantic equivalence in PDDL requires explicit symbolic reasoning capabilities that current LLM architectures do not implement natively.
- Evidence anchors:
  - [abstract]: "performance declined significantly for complex tasks like generating equivalent problems and plans"
  - [section 2.5]: "most models can produce syntactically correct and parsable PDDL problems, their performance declines significantly when required to solve the problem... or producing problems equivalent to the gold"
  - [corpus]: Weak corpus evidence—related papers focus on generation quality, not equivalence specifically
- Break condition: Tasks requiring combinatorial understanding of alternative valid configurations will systematically fail.

### Mechanism 3: Context-Horizon Tradeoff
- Claim: Planning performance follows an inverted-U curve with respect to problem horizon—medium horizons outperform both short and long due to optimal context-information balance.
- Mechanism: Short horizons provide insufficient context for the model to infer correct structure; long horizons exceed the model's coherence window, causing state-tracking failures; medium horizons provide enough context without overwhelming sequential reasoning.
- Core assumption: LLMs have a bounded effective reasoning window where context richness and coherence maintenance are both satisfied.
- Evidence anchors:
  - [section 2.6]: "short horizons perform decently... medium horizons offer a richer and more comprehensive context... scores decrease with long-horizon planning"
  - [section 2.6]: References Li et al. (2024) on long-context LLM struggles with in-context learning
  - [corpus]: "Generating Symbolic World Models" paper addresses test-time scaling, implicitly acknowledging horizon limitations
- Break condition: Tasks requiring both extended context AND precise multi-step state tracking will fail regardless of horizon length.

## Foundational Learning

- Concept: **PDDL Architecture (Domain/Problem/Plan)**
  - Why needed here: The paper evaluates three distinct capabilities; conflating them leads to incorrect system design. Domains define action schemas, problems specify initial/goal states, plans are action sequences.
  - Quick check question: Given a blocks-world scenario, can you identify which information belongs in the domain file versus the problem file?

- Concept: **Classical Planning Search vs. LLM Generation**
  - Why needed here: The paper explicitly compares LLM plan generation against BFWS-FF planner (86.25% success vs. ~17% best LLM). Understanding why symbolic search outperforms neural generation clarifies architectural boundaries.
  - Quick check question: Why does exhaustive state-space search guarantee optimality in ways that next-token prediction cannot?

- Concept: **Parsable vs. Solvable vs. Equivalent Hierarchy**
  - Why needed here: The paper uses three validation levels. Syntax correctness (parsable) ≠ domain integration (solvable) ≠ semantic match (equivalent). System design must specify which level is required.
  - Quick check question: If a generated PDDL action compiles but uses undefined predicates, at which validation level does it fail?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Natural language instructions + PDDL domain context
  - Generation Layer: LLM produces PDDL actions/problems OR plans
  - Validation Layer: Syntax parser → PDDL planner (solvable check) → equivalence metric (ChrF for problems, custom similarity for actions)
  - Plan Verification: VAL tool validates plan-domain-problem compatibility

- **Critical path:**
  1. **Start with action generation** (highest success rates: GPT-4o ~99% parsable, ~50% equivalent)
  2. **Use LLMs for problem drafting only**—expect 20-30% equivalence even with best models
  3. **Never use LLMs for plan generation**—delegate to classical planners (BFWS-FF recommended)
  4. **Implement human-in-the-loop refinement** based on Figure 2 co-pilot analysis

- **Design tradeoffs:**
  - Model selection: GPT-4o/Qwen2.5-72B/Mistral-Large2 for quality; smaller models inadequate for PDDL
  - Cost-performance: Claude-3-Sonnet ($118) and Qwen2.5-72B ($8.8) show similar performance—cost doesn't predict quality across families
  - Horizon targeting: Optimize for medium horizons (11-50 actions); avoid long-horizon direct generation

- **Failure signatures:**
  - LLaMA family: Syntactic bias (`:preconditions` instead of `:precondition`)
  - Gemma/Mistral-7B/Qwen variants: Format errors (`(action...)` instead of `(:action...)`)
  - All models: Goal contamination with undefined domain predicates
  - Long-horizon: State coherence degradation across extended sequences

- **First 3 experiments:**
  1. Replicate action generation on BLOCKS domain with 2-3 candidate models (GPT-4o, Qwen2.5-72B), measuring all three metrics to establish baseline
  2. Test problem generation with controlled horizon lengths (short/medium/long) to identify optimal context window for your domain
  3. Implement co-pilot workflow: measure human correction time for LLM-generated PDDL vs. from-scratch authoring to quantify productivity gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can problem decomposition strategies that hierarchically construct PDDL problems in a bottom-up fashion improve LLM performance on long-horizon planning tasks?
- Basis in paper: [explicit] "We believe that problem decomposition strategies, that would seek to split and hierarchically construct an expected problem in a bottom-up fashion could offer promising directions for future work."
- Why unresolved: The study only evaluated monolithic generation; decomposition was proposed but not tested.
- What evidence would resolve it: Comparative experiments showing decomposed vs. direct generation performance on problems with 50+ actions.

### Open Question 2
- Question: Would few-shot prompting or fine-tuning significantly improve PDDL problem equivalence rates beyond the 32% ceiling observed in zero-shot settings?
- Basis in paper: [explicit] "suggesting that further techniques such as few-shot prompting, fine-tuning, or other enhancements are necessary to assist PDDL problem generation"
- Why unresolved: Authors deliberately excluded few-shot to assess off-the-shelf capabilities.
- What evidence would resolve it: Experiments comparing 1-shot, 3-shot, 5-shot prompting on the same 13,203 problem pairs.

### Open Question 3
- Question: Why do models like Qwen2-1.5B-Instruct perform poorly on action/problem generation but rank top-5 on plan generation?
- Basis in paper: [inferred] The paper notes "behaviours given a specific model are not uniform across tasks" but provides no explanation for this inversion.
- Why unresolved: No analysis of what task-specific capabilities enable small models to outperform larger ones on planning.
- What evidence would resolve it: Probing studies isolating which model components or training data contribute to plan generation vs. PDDL syntax generation.

### Open Question 4
- Question: Do medium-horizon contexts (11-50 actions) genuinely enhance PDDL generation quality, or does the observed improvement stem from dataset artifacts?
- Basis in paper: [inferred] The paper claims medium horizons "strike a balance" but provides no statistical significance testing or domain-level breakdown.
- Why unresolved: The horizon analysis aggregates across domains without controlling for problem complexity or domain structure.
- What evidence would resolve it: Stratified analysis controlling for domain type, with significance tests comparing short, medium, and long horizon performance.

## Limitations

- The evaluation framework assumes zero-shot prompting is the appropriate baseline, though instruction-tuned variants might yield different results.
- Model performance on novel PDDL constructs (durative actions, derived predicates) remains untested.
- The equivalence metrics (set-based similarity for actions, ChrF for problems) may not fully capture semantic validity across diverse planning domains.

## Confidence

- **High confidence:** Most models can generate syntactically valid PDDL actions when given domain context and natural language instructions.
- **Medium confidence:** GPT-4o and similar top-tier models show consistent performance across all three PDDL tasks, though equivalence generation remains challenging.
- **Low confidence:** Long-horizon planning results may be domain-specific; the 11-50 action sweet spot identified needs validation across diverse planning problems.

## Next Checks

1. Test zero-shot performance on PDDL constructs not present in the training benchmarks (derived predicates, durative actions) to identify capability boundaries.
2. Implement instruction-tuning on the Oswald benchmark and re-evaluate to determine if performance gains justify the training overhead.
3. Validate the horizon tradeoff hypothesis by testing models on problems with variable action counts across multiple domains, controlling for domain complexity.