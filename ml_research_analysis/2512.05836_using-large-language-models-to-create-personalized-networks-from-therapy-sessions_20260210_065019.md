---
ver: rpa2
title: Using Large Language Models to Create Personalized Networks From Therapy Sessions
arxiv_id: '2512.05836'
source_url: https://arxiv.org/abs/2512.05836
tags:
- networks
- processes
- personalized
- process
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an end-to-end LLM pipeline to generate session-level
  personalized networks from therapy transcripts for clinical case conceptualization
  and treatment planning. The approach combines prompt-based process detection (binary
  classification and multi-label tagging), iterative clustering into clinically meaningful
  themes, and explainable link generation via ensemble learning.
---

# Using Large Language Models to Create Personalized Networks From Therapy Sessions

## Quick Facts
- arXiv ID: 2512.05836
- Source URL: https://arxiv.org/abs/2512.05836
- Authors: Clarissa W. Ong; Hiba Arnaout; Kate Sheehan; Estella Fox; Eugen Owtscharow; Iryna Gurevych
- Reference count: 40
- Primary result: Multi-step LLM pipeline achieves 72-75% clinical utility scores and 90% expert preference over direct prompting for therapy session network generation

## Executive Summary
This paper presents an end-to-end LLM pipeline that generates session-level personalized psychological networks from therapy transcripts, enabling scalable clinical case conceptualization and treatment planning. The approach combines prompt-based process detection, iterative clustering into clinically meaningful themes, and explainable link generation via ensemble learning. Expert evaluation found the multi-step pipeline significantly outperformed direct prompting, with 72-75% scores for clinical relevance, novelty, and usefulness. The method reduces manual effort while maintaining clinical utility, producing networks for 77 therapy sessions that are released for further research.

## Method Summary
The pipeline processes therapy transcripts through three stages: (1) Process detection using LLaMA-3.1-70B-Instruct with few-shot prompting to identify psychological processes and assign them to 9 EEMM dimensions, (2) Two-step clustering where the model first generates candidate themes then assigns processes to these themes, and (3) Connection generation using a model-based ensemble of LLaMA, Qwen, and GPT-4o-mini with majority voting to create explainable directed links between themes. The approach achieves high process detection accuracy (>90% F1) and produces clinically useful networks evaluated by experts across multiple dimensions including relevance, novelty, and therapeutic usefulness.

## Key Results
- Process detection achieved over 90% precision using few-shot prompting with 50-100 examples
- Multi-step pipeline outperformed direct prompting with 72-75% clinical utility scores and up to 90% expert preference
- Generated networks for 77 therapy sessions across 6 participants (1 MDD, 5 GAD)
- Expert evaluation showed high variance on subjective metrics like "novelty" (kappa=0)

## Why This Works (Mechanism)

### Mechanism 1
Multi-step pipeline decomposition improves clinical utility over end-to-end prompting by breaking network generation into discrete stages (process detection → theme clustering → connection generation) that allow intermediate expert feedback and task-specific prompt optimization, reducing compounding errors from single-shot generation. This structured decomposition mirrors how clinicians build case formulations incrementally.

### Mechanism 2
Few-shot in-context learning enables effective process detection with minimal labeled data by providing K examples (5-100) in prompts that allow the model to infer classification boundaries without gradient updates, leveraging pre-trained knowledge of psychological language patterns. The model generalizes from few examples due to sufficient priors about psychological constructs from pre-training.

### Mechanism 3
Ensemble prompting improves reliability for subjective relationship inference by aggregating predictions from multiple models/prompts via majority voting, reducing variance and biases of any single model particularly valuable for open-ended clinical reasoning. Model disagreements are largely uncorrelated errors rather than systematic biases.

## Foundational Learning

- **In-context learning / Few-shot prompting**: Why needed here - The pipeline avoids fine-tuning; understanding how examples shape model behavior is essential for prompt design. Quick check question: Given the performance curve in Figure 3, what is the minimum K you would recommend for deployment?

- **Extended Evolutionary Meta-Model (EEMM) dimensions**: Why needed here - Process classification maps to 9 EEMM dimensions (Affect, Cognition, Sense of Self, etc.); knowing these enables debugging of misclassifications. Quick check question: An utterance reads "I keep scrolling social media instead of sleeping." Which EEMM dimensions apply?

- **Ensemble aggregation strategies**: Why needed here - Connection generation uses three ensemble types (prompt-based, model-based, temperature-based); understanding trade-offs informs architecture choices. Quick check question: Why might model-based ensembles outperform temperature-based ensembles for clinical reasoning?

## Architecture Onboarding

- **Component map**: Transcript Preprocessing -> Process Detection -> Theme Clustering -> Connection Generation -> Network Visualization
- **Critical path**: Process detection quality gates all downstream stages; if precision is low, clustering receives noisy inputs. Assumption: clustering prompt robustness can partially compensate for detection noise.
- **Design tradeoffs**: Open-source (LLaMA) vs. proprietary models: Privacy vs. potential performance gains; paper uses LLaMA for all transcript-touching stages; Two-step vs. single-step clustering: More LLM calls but better theme coherence (Table 7: 2.15 vs. 1.94 clinical relevance); Ensemble size: 3-model ensemble balances compute cost vs. reliability gains.
- **Failure signatures**: Low inter-annotator agreement on "novelty" (kappa=0) suggests subjective metrics are unstable evaluation targets; Hallucinated connections: Table 12 shows 32-48% disagreement on connection strength within ensembles; Missing non-verbal cues: Paper acknowledges networks are "constrained by verbally mediated information".
- **First 3 experiments**: 1) Process detection ablation: Test K ∈ {0, 1, 5, 10, 25, 50} on held-out transcripts; plot F1 to find diminishing returns point (expect plateau around K=50 based on Figure 3); 2) Clustering strategy comparison: Run single-step vs. two-step on 10 sessions; have 2 clinicians rate theme coherence blind (replicate Table 7 methodology); 3) Ensemble calibration: On 5 sessions with known expert-generated networks, measure precision/recall of detected connections for each ensemble type; assess if model-based superiority holds across clinical populations.

## Open Questions the Paper Calls Out
None

## Limitations
- Networks are constrained by verbally mediated information and cannot capture non-verbal cues (gestures, facial expressions, silences)
- Expert evaluations showed high variance on subjective metrics like "novelty" (kappa=0), indicating potential instability in qualitative assessments
- Generalizability across different clinical populations and therapy modalities remains untested beyond the 6 participants studied

## Confidence

- **High confidence**: Process detection accuracy (>90% precision) and expert preference for multi-step pipeline over direct prompting (up to 90% preference)
- **Medium confidence**: Clinical utility metrics (72-75% scores) which rely on subjective expert ratings with observed variance
- **Low confidence**: Claims about network interpretability and therapeutic usefulness beyond the specific study population

## Next Checks
1. **Cross-population validation**: Apply the pipeline to therapy sessions from different clinical conditions (e.g., PTSD, eating disorders) and compare performance metrics to establish generalizability
2. **Expert reliability assessment**: Conduct inter-rater reliability analysis on a new set of 10 sessions using the same evaluation protocol to quantify measurement stability
3. **Real-world integration test**: Deploy the pipeline in a clinical setting where therapists use generated networks for actual case formulation and measure impact on treatment planning efficiency and quality