---
ver: rpa2
title: Efficient Conformal Prediction for Regression Models under Label Noise
arxiv_id: '2509.15120'
source_url: https://arxiv.org/abs/2509.15120
tags:
- noisy
- prediction
- noise
- regression
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for applying Conformal Prediction
  (CP) to regression models when the calibration set contains noisy labels, a common
  challenge in medical imaging applications. The authors propose a novel algorithm
  that estimates the noise-free CP threshold by leveraging a mathematically grounded
  deconvolution procedure, addressing the continuous nature of regression problems.
---

# Efficient Conformal Prediction for Regression Models under Label Noise

## Quick Facts
- **arXiv ID**: 2509.15120
- **Source URL**: https://arxiv.org/abs/2509.15120
- **Reference count**: 0
- **Primary result**: Novel deconvolution-based method for CP regression under label noise, achieving near-oracle coverage with shorter intervals than standard noisy CP on medical imaging datasets.

## Executive Summary
This paper addresses a critical gap in Conformal Prediction (CP) for regression models: handling noisy labels in the calibration set. Standard CP methods assume clean calibration data, but real-world applications, particularly in medical imaging, often have noisy labels. The authors propose a mathematically grounded deconvolution procedure that recovers the noise-free CP threshold from noisy calibration data. By solving an inverse convolution problem, the method estimates the true coverage distribution and iteratively searches for the optimal threshold. Evaluated on two medical imaging datasets with Gaussian label noise, the approach achieves near-oracle performance—maintaining the target coverage probability while producing significantly shorter confidence intervals than the standard noisy CP method.

## Method Summary
The method modifies the standard split CP approach by replacing the calibration phase with a deconvolution-based procedure. First, a regression model (EfficientNet-B4 with dual heads for mean and variance) is trained using Gaussian negative log-likelihood loss. During calibration, the algorithm computes the noisy CP threshold as a baseline, then iteratively decreases this threshold while using regularized deconvolution to estimate the corresponding coverage from the noisy data. The deconvolution solves a least-squares problem to recover the clean coverage distribution from the observed noisy one, assuming additive Gaussian noise. The iteration stops when the deconvolved coverage drops below the target level (1-α), returning the previous threshold. Noise level σ is estimated from the lowest 1% of predicted variances in the training set.

## Key Results
- Achieves near-oracle performance (90% target coverage) on BoneAge and Chest X-Ray datasets
- Produces significantly shorter confidence intervals than standard noisy CP (e.g., interval length 0.74 vs 1.09 on BoneAge)
- Robust to noise estimation errors (±0.05 from true σ)
- Maintains performance across different noise levels (σ ∈ {0.2, 0.3})

## Why This Works (Mechanism)

### Mechanism 1: Deconvolution-Based Recovery of Noise-Free Coverage Distribution
Under additive Gaussian noise, the observed noisy coverage matrix is the convolution of the clean coverage matrix with the noise kernel. By inverting this convolution relationship via regularized least squares, the method recovers the true coverage distribution needed to compute accurate coverage probability. This works when the noise kernel is known/estimable and conditional independence holds between X and Ỹ given Y.

### Mechanism 2: Iterative Threshold Search with Dispersive Noise Initialization
Dispersive noise (where Ỹ|X is more spread than Y|X) causes standard CP to overestimate the threshold q̂. The algorithm exploits this by starting from the inflated noisy CP threshold and iteratively decreasing it while monitoring deconvolved coverage. The loop terminates when coverage drops below 1-α, returning the previous threshold. This approach assumes noise is dispersive; if noise were contractive, the initialization would underestimate rather than overestimate.

### Mechanism 3: Discretization with Masked Regularized Deconvolution
The continuous label space is discretized into bins (δ_y = 0.01), and empirical coverage matrices are computed via counting. The deconvolution problem is solved via regularized least squares with masking to handle empty bins. This handles finite-sample sparsity and makes the inverse problem tractable. The approach assumes discretization is fine enough to approximate continuous distributions and that regularization appropriately balances stability vs. bias.

## Foundational Learning

- **Concept: Split Conformal Prediction (CP)**
  - Why needed: The entire method modifies CP's calibration phase. Without understanding how CP computes q̂ as the ⌈(n+1)(1-α)⌉/n quantile of calibration scores and constructs intervals C(x) = {y : s(x,y) ≤ q̂}, the deconvolution objective is unintelligible.
  - Quick check: Given calibration scores {0.1, 0.3, 0.5, 0.7, 0.9} and α = 0.1, what is q̂ and what does Theorem 1 guarantee?

- **Concept: Convolution and Deconvolution of Distributions**
  - Why needed: The core insight is that additive noise convolves distributions. Recovering the clean distribution requires solving an inverse problem (deconvolution), which is ill-posed without regularization.
  - Quick check: If Ỹ = Y + ε where ε ~ N(0, σ²) and Y has density p_Y, express p_Ỹ in terms of p_Y. Why is inverting this challenging?

- **Concept: Gaussian Negative Log-Likelihood (NLL) Loss**
  - Why needed: The method requires models that output uncertainty estimates û(x), trained via Gaussian NLL. This enables instance-dependent intervals s(x,y) = |y - ŷ(x)|/û(x) and the noise estimation procedure in Section 3.3.
  - Quick check: How does the loss L = log(û²) + (y - ŷ)²/û² differ from MSE? What does û(x) represent asymptotically?

## Architecture Onboarding

- **Component map**: Train model (clean or noisy labels) -> Run inference on calibration set -> Initialize q from noisy CP -> [Loop: Build M̂_q^n -> Deconvolve to M̂_q^c -> Compute coverage -> If > 1-α: q ← q - δ_q] -> Return final q̂ -> Deploy C(x) = [ŷ(x) - q̂û(x), ŷ(x) + q̂û(x)]

- **Critical path**: EfficientNet-B4 backbone with dual heads (ŷ, log(û²)) -> Gaussian NLL training -> Score function s(x,y) = |y - ŷ(x)|/û(x) -> Discretized calibration matrix builder -> Regularized deconvolution solver -> Iterative threshold search -> Final CP intervals

- **Design tradeoffs**:
  1. Bin width (δ_y = 0.01): Smaller bins increase precision but require more calibration data per bin
  2. Search step (δ_q = 0.05): Smaller steps yield tighter thresholds but more iterations
  3. Noise estimation via lowest 1% û²: Robust if "easy" samples exist; biased if all samples are inherently uncertain

- **Failure signatures**:
  1. Under-coverage (coverage < 89%): σ underestimated or δ_q too large (overshot)
  2. Over-coverage with wide intervals (coverage > 92%): σ overestimated or deconvolution regularized too strongly
  3. Oscillating coverage: δ_q too large relative to coverage sensitivity
  4. Empty bin errors: Calibration set too small for chosen δ_y

- **First 3 experiments**:
  1. Establish baselines: Implement Oracle CP (clean calibration) and Noisy CP on BoneAge dataset. Verify Oracle achieves ~90% coverage, Noisy CP shows ~94% coverage with inflated intervals.
  2. Validate deconvolution component: With known σ = 0.2, run full algorithm. Compare resulting q̂, interval length, and coverage against Table 1 (BoneAge: q̂ ≈ 2.42, length ≈ 0.74, coverage ≈ 90%).
  3. Robustness sweep: Run with σ ∈ {0.15, 0.20, 0.25} when σ_true = 0.2. Confirm coverage degrades gracefully (per Table 1: 90.84% → 90.11% → 89.72%) rather than catastrophically.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the noise model parameters (e.g., $\sigma^2$) be estimated accurately in the absence of "easy" samples (where inherent uncertainty $u^2(x) \approx 0$)?
- **Basis in paper**: [explicit] Section 3.3 states, "Future work may further investigate the problem of estimating the label noise model," noting the current estimator relies on the assumption of "easy" samples.
- **Why unresolved**: The proposed estimator assumes the lowest 1% of predicted variances approximate the label noise, a heuristic that may fail if the dataset lacks samples with near-zero inherent uncertainty.
- **What evidence would resolve it**: Derivation of an estimator independent of low-uncertainty samples or empirical validation on datasets with uniformly high aleatoric uncertainty.

### Open Question 2
- **Question**: Is the method robust to non-Gaussian or heteroscedastic label noise distributions?
- **Basis in paper**: [inferred] While Assumption A.2 theoretically allows for a general kernel $k$, the experimental evaluation (Section 4) is restricted solely to additive Gaussian noise.
- **Why unresolved**: The deconvolution optimization and noise estimation technique are tailored for Gaussian properties; their performance under asymmetric or input-dependent noise remains unverified.
- **What evidence would resolve it**: Experiments applying the method to datasets with structured or skewed noise patterns (e.g., exponential or uniform noise).

### Open Question 3
- **Question**: Can formal coverage guarantees be established for the practical algorithm despite the use of discretization and regularization heuristics?
- **Basis in paper**: [inferred] The paper provides a theoretical grounding for the "conceptual" procedure but implements it using approximations (discretization, masking, and $\ell_2$ regularization) that lack formal error bounds.
- **Why unresolved**: It is unclear if the "strong empirical performance" holds theoretically when the continuous integrals are replaced by finite matrix approximations.
- **What evidence would resolve it**: A theoretical proof bounding the coverage error introduced by the discretization step and the finite calibration set size.

## Limitations
- **Assumption dependency**: Method critically relies on additive Gaussian noise and conditional independence assumptions
- **Parameter sensitivity**: Performance depends on heuristic choices (bin width, regularization, search step)
- **Sample size requirements**: Requires sufficient calibration samples per bin; scalability to high-dimensional label spaces unclear

## Confidence
- **High Confidence**: Deconvolution relationship (Equation 2) is mathematically sound; Dispersive noise causes standard CP to overestimate coverage; Empirical improvements on medical imaging datasets are reproducible
- **Medium Confidence**: Near-oracle performance across tested conditions; Robustness to noise estimation errors (±0.05); Generalization to other regression tasks beyond medical imaging
- **Low Confidence**: Performance with non-dispersive noise; Behavior with non-Gaussian noise distributions; Scalability to problems with many calibration samples or high-dimensional label spaces

## Next Checks
1. **Noise Distribution Sensitivity**: Test the method with Laplacian or uniform noise instead of Gaussian. Does deconvolution still recover accurate thresholds, or is performance severely degraded?
2. **Calibration Set Size Sweep**: Evaluate performance across calibration set sizes (1%, 5%, 10%, 20% of training data). Identify the minimum viable size and characterize degradation patterns.
3. **Non-Dispersive Noise Scenario**: Create synthetic data with contractive noise (Ỹ variance < Y variance). Verify whether the initialization fails as predicted, and test alternative initialization strategies.