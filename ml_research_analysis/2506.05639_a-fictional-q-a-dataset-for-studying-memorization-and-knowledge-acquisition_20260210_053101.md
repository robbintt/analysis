---
ver: rpa2
title: A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition
arxiv_id: '2506.05639'
source_url: https://arxiv.org/abs/2506.05639
tags:
- training
- data
- fictional
- memorization
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic dataset of fictional events and
  associated Q&A pairs to study fact memorization in language models, distinct from
  sequence memorization. The dataset includes realistic, webtext-like documents in
  various styles (news, social media, encyclopedia, corporate, blog) and multiple-choice
  questions to test factual knowledge transfer.
---

# A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition

## Quick Facts
- arXiv ID: 2506.05639
- Source URL: https://arxiv.org/abs/2506.05639
- Reference count: 40
- Primary result: Synthetic fictional datasets enable studying fact memorization distinct from sequence memorization in LLMs

## Executive Summary
This paper introduces a synthetic dataset of fictional events and associated Q&A pairs to study fact memorization in language models, distinct from sequence memorization. The dataset includes realistic, webtext-like documents in various styles (news, social media, encyclopedia, corporate, blog) and multiple-choice questions to test factual knowledge transfer. Experiments on Llama and Gemma models show that training on fictional documents improves both document loss and Q&A accuracy, with larger models achieving higher transfer. Fact memorization is separable from verbatim memorization, as validation loss can decrease while training loss approaches zero. However, the source of knowledge transfer—whether from factual or stylistic memorization—remains non-identifiable, as models improve on held-out questions. Training on highly structured fact sheets (fictsheets) yields the least transfer, suggesting that surface form diversity and realistic text styles are crucial for effective fact learning in LLMs.

## Method Summary
The authors create a synthetic dataset of fictional events, characters, and facts designed to mimic realistic webtext across five document styles: news, social media, encyclopedia, corporate, and blog. Each document contains carefully crafted facts about fictional entities, paired with multiple-choice questions testing knowledge of these facts. The dataset enables controlled experiments where models train on fictional content then are tested on both the original documents and novel questions about the same facts. Experiments use Llama and Gemma architectures ranging from 125M to 7B parameters, trained on varying amounts of fictional data to study memorization patterns and knowledge transfer.

## Key Results
- Training on fictional documents improves both document loss and Q&A accuracy compared to baselines
- Larger models (7B) achieve higher knowledge transfer than smaller models (125M)
- Fact memorization is separable from verbatim memorization: validation loss can decrease while training loss approaches zero
- Fact sheets show the least transfer, indicating surface form diversity is crucial for effective learning
- Models improve on held-out questions, but the source (factual vs stylistic memorization) remains non-identifiable

## Why This Works (Mechanism)
The dataset design enables isolation of fact memorization from sequence memorization by using entirely fictional content with controlled factual structures. Multiple document styles create diverse surface forms for the same underlying facts, allowing researchers to distinguish whether models learn factual knowledge versus memorizing specific phrasings. The synthetic nature ensures no overlap with pretraining data, eliminating confounding from prior exposure. Multiple-choice questions provide a clear evaluation signal for factual knowledge transfer while maintaining experimental control.

## Foundational Learning
- **Factual knowledge representation**: Models must encode relationships between entities and events in memory; needed to distinguish from surface memorization, checked via novel question performance
- **Surface form diversity**: Different phrasings of same facts across document styles; needed to test if models learn abstractions, checked via cross-style generalization
- **Memorization vs generalization boundary**: Training on fictional content creates clean separation; needed to study knowledge transfer mechanisms, checked via held-out question performance
- **Knowledge transfer metrics**: Multiple-choice accuracy on novel questions; needed to quantify factual learning beyond document reconstruction, checked via Q&A evaluation
- **Scale-dependent learning**: Different model sizes show varying transfer; needed to understand capacity effects, checked via size-controlled experiments

## Architecture Onboarding
- **Component map**: Synthetic data generation -> Training on fictional documents -> Multiple-choice Q&A evaluation -> Knowledge transfer analysis
- **Critical path**: Document style diversity → Fact memorization → Knowledge transfer → Novel question performance
- **Design tradeoffs**: Synthetic vs natural data (control vs realism), multiple-choice vs open-ended questions (evaluation ease vs depth), fixed vs variable document lengths (training efficiency vs realism)
- **Failure signatures**: If validation loss doesn't decrease despite training loss approaching zero, suggests sequence memorization without factual learning; if fact sheets show high transfer, suggests surface form isn't important
- **First experiments**: 1) Train 125M model on single document style vs mixed styles, 2) Compare knowledge transfer across all five document types, 3) Test whether pretraining on real data affects fictional fact learning

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic dataset may not capture full complexity of natural language encountered during large-scale pretraining
- Multiple-choice questions with fixed correct answers may overestimate capabilities if models exploit surface-level cues
- Experimental design cannot definitively attribute knowledge transfer to factual versus stylistic memorization

## Confidence
- Core finding (fact memorization differs from sequence memorization): **High**
- Larger models exhibit greater transfer: **Medium**
- Surface form diversity drives effective learning: **Low-Medium**

## Next Checks
1) Conduct controlled experiments varying only document style while holding factual content constant to isolate the contribution of stylistic memorization to knowledge transfer
2) Test the synthetic dataset on additional model architectures (e.g., Mistral, Claude) and scaling regimes to assess generalizability beyond Llama and Gemma families
3) Replace multiple-choice questions with open-ended generation tasks and human evaluation to better measure genuine knowledge acquisition versus pattern matching