---
ver: rpa2
title: 'Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in
  Large Language Model Translations'
arxiv_id: '2506.00748'
source_url: https://arxiv.org/abs/2506.00748
tags:
- translation
- average
- score
- languages
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Translate-with-Care (TWC) dataset, a
  novel benchmark designed to evaluate machine translation systems' ability to handle
  content from genderless languages (such as Persian, Indonesian, and Finnish) into
  natural gender languages (like English) while avoiding gender bias and preserving
  logical coherence. The authors analyze diverse translation technologies, including
  GPT-4, mBART-50, NLLB-200, and Google Translate, revealing significant challenges
  in translating genderless content, particularly in pronoun disambiguation, resulting
  in gender stereotyping and reasoning errors.
---

# Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations

## Quick Facts
- arXiv ID: 2506.00748
- Source URL: https://arxiv.org/abs/2506.00748
- Reference count: 23
- Primary result: Introduces TWC dataset to evaluate gender bias in translating genderless to natural gender languages

## Executive Summary
This paper introduces the Translate-with-Care (TWC) dataset, a novel benchmark designed to evaluate machine translation systems' ability to handle content from genderless languages (such as Persian, Indonesian, and Finnish) into natural gender languages (like English) while avoiding gender bias and preserving logical coherence. The authors analyze diverse translation technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate, revealing significant challenges in translating genderless content, particularly in pronoun disambiguation, resulting in gender stereotyping and reasoning errors. All evaluated models showed a strong preference for masculine pronouns when gender stereotypes could influence choices, with Google Translate and GPT-4 exhibiting particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts.

## Method Summary
The researchers developed the Translate-with-Care (TWC) dataset containing parallel sentences in Persian, Indonesian, and Finnish paired with English translations. They evaluated multiple translation systems including GPT-4, mBART-50, NLLB-200, and Google Translate on the dataset. The evaluation focused on gender pronoun disambiguation accuracy and the presence of gender bias in contexts where gender stereotypes could influence translation choices. The authors then fine-tuned mBART-50 on the TWC dataset and re-evaluated its performance to assess whether training on this targeted dataset could mitigate identified biases and reasoning errors.

## Key Results
- All evaluated models showed a strong preference for masculine pronouns when gender stereotypes could influence choices
- Google Translate and GPT-4 exhibited particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts
- Fine-tuning mBART-50 on the TWC dataset substantially resolved gender biases and reasoning errors, leading to strong generalization and outperforming proprietary LLMs while remaining open-source

## Why This Works (Mechanism)
The fine-tuning approach works by exposing the translation model to carefully curated examples that explicitly disambiguate gender in contexts where genderless languages leave ambiguity. This targeted training helps the model learn to avoid defaulting to masculine pronouns when gender information is genuinely ambiguous. The TWC dataset provides the model with diverse scenarios that challenge its gender reasoning capabilities, forcing it to develop more nuanced understanding of when gender information should or shouldn't be inferred from context.

## Foundational Learning
**Genderless vs. gendered languages**: Understanding how languages encode gender differently is crucial for identifying where translation systems may introduce bias. Quick check: Compare pronoun systems in English vs. Persian/Finnish.

**Pronoun disambiguation**: The process by which translation systems infer gender from context when source language provides no explicit markers. Quick check: Examine how context influences pronoun choice in ambiguous sentences.

**Translation model bias**: Systematic preferences in language models that reflect societal stereotypes rather than linguistic necessity. Quick check: Analyze pronoun distribution across different professional contexts.

## Architecture Onboarding

**Component Map**: TWC Dataset -> Translation Models (GPT-4, mBART-50, NLLB-200, Google Translate) -> Evaluation Metrics -> Fine-tuned mBART-50 -> Re-evaluation

**Critical Path**: TWC dataset creation → Translation model evaluation → Bias identification → mBART-50 fine-tuning → Performance comparison

**Design Tradeoffs**: The study balances between using proprietary models (higher performance baseline) versus open-source alternatives (better transparency and fine-tuning potential). The choice to focus on English as target language simplifies evaluation but limits cross-linguistic insights.

**Failure Signatures**: Models default to masculine pronouns in professional contexts (leadership, success scenarios), showing 4-6x preference for male pronouns when gender stereotypes are present. This manifests as systematic gender stereotyping rather than random variation.

**3 First Experiments**:
1. Evaluate baseline gender pronoun disambiguation accuracy across all translation models
2. Measure gender bias ratios in professional context translations
3. Fine-tune mBART-50 on TWC and compare post-fine-tuning performance against baseline

## Open Questions the Paper Calls Out
None

## Limitations
- The TWC dataset covers only three source languages (Persian, Indonesian, Finnish) and a limited set of semantic categories, which may not represent the full complexity of genderless-to-gendered language translation challenges
- The evaluation focuses primarily on English target translations, limiting insights into cross-linguistic generalizability
- The study does not investigate the temporal stability of the findings or potential changes with newer model versions

## Confidence
- **High confidence**: The core finding that all evaluated models exhibit gender bias when translating genderless languages into English is well-supported by the experimental results
- **Medium confidence**: The claim that fine-tuning mBART-50 substantially resolves these biases is supported but limited to the specific dataset and model configuration tested
- **Medium confidence**: The assertion that mBART-50 outperforms proprietary LLMs after fine-tuning is based on this specific benchmark and may not generalize to all translation tasks

## Next Checks
1. Replicate the experiments with additional genderless source languages (e.g., Turkish, Hungarian) and different language families to assess cross-linguistic generalizability
2. Conduct a longitudinal study evaluating how model performance changes across different versions and update cycles
3. Test the fine-tuned mBART-50 model on external, real-world translation tasks beyond the TWC dataset to verify practical effectiveness