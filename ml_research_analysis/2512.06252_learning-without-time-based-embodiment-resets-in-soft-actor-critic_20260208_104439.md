---
ver: rpa2
title: Learning Without Time-Based Embodiment Resets in Soft-Actor Critic
arxiv_id: '2512.06252'
source_url: https://arxiv.org/abs/2512.06252
tags:
- resets
- continuing
- learning
- time
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates learning in reinforcement learning tasks
  without time-based embodiment resets, which are commonly used to accelerate training
  but can be impractical in real-world applications. The authors propose a continuing
  version of the Soft Actor-Critic (SAC) algorithm and modify reward functions to
  enable learning without episode terminations.
---

# Learning Without Time-Based Embodiment Resets in Soft-Actor Critic

## Quick Facts
- arXiv ID: 2512.06252
- Source URL: https://arxiv.org/abs/2512.06252
- Authors: Homayoon Farrahi; A. Rupam Mahmood
- Reference count: 40
- Primary result: Continuing SAC can match or exceed episodic SAC performance without embodiment resets

## Executive Summary
This paper addresses a critical limitation in reinforcement learning: the reliance on time-based embodiment resets that accelerate training but are impractical in real-world applications. The authors develop a continuing version of Soft Actor-Critic (SAC) that operates without episode terminations, modifying both the algorithm and reward functions to enable learning in reset-free environments. Their key finding is that continuing SAC can achieve performance comparable to or better than episodic SAC while being less sensitive to the discount rate hyperparameter.

The study identifies that removing embodiment resets leads to poor exploration due to a high-variance Q-function that prevents effective state space coverage. To address this, the authors introduce two interventions: layer normalization in the neural network architecture and performance-based adjustment of the temperature parameter. These modifications successfully recover lost performance, providing practical solutions for deploying reinforcement learning in real-world scenarios where resets are not feasible.

## Method Summary
The authors modify SAC to operate in continuing (non-episodic) settings by adapting the algorithm to handle infinite-horizon tasks without termination conditions. They redesign reward functions to incorporate intrinsic motivation signals that encourage exploration without relying on episode boundaries. The continuing SAC implementation maintains the core principles of maximum entropy reinforcement learning while adjusting the optimization objectives to work in reset-free environments. The temperature parameter, which balances exploration and exploitation, is dynamically adjusted based on performance metrics rather than being fixed.

## Key Results
- Continuing SAC matches or exceeds episodic SAC performance across multiple locomotion tasks
- Continuing SAC shows reduced sensitivity to discount rate hyperparameter compared to episodic SAC
- Layer normalization and performance-based temperature adjustment successfully recover exploration performance lost when removing embodiment resets
- High-variance Q-functions are identified as the primary cause of poor exploration in reset-free learning

## Why This Works (Mechanism)
The mechanism relies on maintaining stable Q-function estimates in continuing environments where traditional episodic assumptions break down. By normalizing neural network layers, the variance in Q-value predictions is reduced, enabling more consistent exploration behavior. The dynamic temperature adjustment ensures that the agent maintains appropriate exploration-exploitation balance as learning progresses, preventing premature convergence to suboptimal policies. The modified reward structure provides intrinsic motivation signals that compensate for the absence of episode termination rewards.

## Foundational Learning
- **Continuing vs Episodic RL**: Continuing tasks have no natural termination, requiring infinite-horizon formulations - needed to handle real-world scenarios where resets aren't possible
- **Maximum Entropy RL**: Balances task reward maximization with entropy maximization for exploration - needed to maintain exploration in continuing settings
- **Q-function variance**: High variance leads to unstable policy updates and poor exploration - needed to understand why reset-free learning fails
- **Temperature parameter**: Controls exploration-exploitation trade-off in SAC - needed to maintain learning stability
- **Layer normalization**: Normalizes activations across layers to reduce internal covariate shift - needed to stabilize Q-function estimates
- **Intrinsic motivation**: Reward signals that encourage exploration of novel states - needed to replace episode termination rewards

## Architecture Onboarding

**Component Map:**
Continuing SAC -> Layer Normalization -> Temperature Adjustment -> Performance Recovery

**Critical Path:**
1. Agent observes state and receives reward
2. Q-function estimates state-action values with layer normalization
3. Policy updates based on normalized Q-values
4. Temperature parameter adjusts based on performance metrics
5. Agent selects actions balancing exploration and exploitation

**Design Tradeoffs:**
- Fixed vs dynamic temperature: Dynamic provides better adaptation but adds complexity
- Standard vs normalized layers: Normalization improves stability but may reduce representational capacity
- Intrinsic vs extrinsic rewards: Intrinsic maintains exploration but may dilute task-specific learning signals

**Failure Signatures:**
- High Q-function variance indicates exploration problems
- Premature convergence to local optima
- Sensitivity to discount rate hyperparameters
- Poor state space coverage

**First Experiments:**
1. Compare continuing SAC with and without layer normalization on a simple locomotion task
2. Test different temperature adjustment schedules on continuing vs episodic performance
3. Measure Q-function variance during training to validate the exploration hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to simulated locomotion tasks, may not generalize to complex real-world scenarios
- Effectiveness of interventions lacks systematic ablation studies to isolate individual contributions
- Proposed explanations for exploration issues are theoretically plausible but not definitively proven through direct measurement

## Confidence
- **High confidence**: Continuing SAC matching episodic performance is well-supported experimentally
- **Medium confidence**: High-variance Q-function explanation for poor exploration is plausible but not proven
- **Medium confidence**: Intervention effectiveness demonstrated but mechanisms need further investigation

## Next Checks
1. Test proposed interventions on non-locomotion tasks (manipulation or navigation) to assess generalizability
2. Conduct ablation studies to isolate contributions of layer normalization versus temperature adjustment
3. Measure and report Q-function variance during training to directly validate the exploration hypothesis