---
ver: rpa2
title: 'GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models'
arxiv_id: '2506.10047'
source_url: https://arxiv.org/abs/2506.10047
tags:
- prompts
- prompt
- image
- diversity
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenBreak is a red-teaming framework that fine-tunes a large language
  model to automatically discover adversarial prompts capable of bypassing safety
  filters and generating harmful content in text-to-image models. It uses supervised
  fine-tuning on curated datasets followed by reinforcement learning with multi-objective
  rewards (toxicity, stealth, diversity) to optimize for both evasion and image harmfulness.
---

# GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models

## Quick Facts
- arXiv ID: 2506.10047
- Source URL: https://arxiv.org/abs/2506.10047
- Reference count: 40
- Key outcome: GenBreak achieves high toxic bypass rates (e.g., 70% on nudity content for commercial APIs) while maintaining semantic fluency and prompt diversity, outperforming existing red-teaming methods.

## Executive Summary
GenBreak is a red-teaming framework that fine-tunes a large language model to automatically discover adversarial prompts capable of bypassing safety filters and generating harmful content in text-to-image models. It uses supervised fine-tuning on curated datasets followed by reinforcement learning with multi-objective rewards (toxicity, stealth, diversity) to optimize for both evasion and image harmfulness. Evaluated on both open-source and commercial T2I models, GenBreak demonstrates strong transferability from surrogate training environments to black-box commercial APIs.

## Method Summary
GenBreak employs a two-stage approach: first, supervised fine-tuning on curated attack datasets (Category Rewrite and Pre-Attack) to adapt an LLM to red-teaming tasks; second, reinforcement learning with Group Relative Policy Optimization using multi-objective rewards to maximize toxicity while maintaining prompt stealth and diversity. The framework trains against a surrogate T2I model with integrated safety filters, then transfers learned strategies to attack commercial APIs. Training uses Llama-3.2-1B-Instruct with LoRA adapters and GRPO updates based on an ensemble of toxicity evaluators.

## Key Results
- Achieved 70% Toxic Clean Bypass Rate on nudity content for commercial APIs
- Outperformed existing methods like CRT and ART in both bypass rates and diversity metrics
- Demonstrated strong transferability from open-source surrogate models to black-box commercial systems
- Maintained lexical and semantic diversity while achieving high attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised fine-tuning on curated attack datasets provides the base capability for red-teaming before RL optimization.
- **Mechanism**: The SFT stage uses two datasets—Category Rewrite (15,000 adversarial transformations generated via Gemini 2.0 Flash) and Pre-Attack (~2,000 high-TBS pairs collected by iteratively attacking SD 2.1 with an uncensored LLM). This adapts general-purpose LLMs to the specific task of generating harmful-yet-stealthy prompts.
- **Core assumption**: The quality of SFT data directly affects the search space explored during RL; poor initial data may cause convergence to suboptimal strategies.
- **Evidence anchors**: [abstract] "Our approach combines supervised fine-tuning on curated datasets with reinforcement learning via interaction with a surrogate T2I model."

### Mechanism 2
- **Claim**: Multi-objective reward design jointly optimizes for toxicity, filter evasion, and prompt diversity—preventing the collapse seen in single-objective approaches.
- **Mechanism**: The RL objective combines: (1) Toxicity reward R_tox (ensemble of MHSC, LLaVAGuard, NudeNet), (2) Bypass reward R_bypass = R_tox × I[bypass], (3) Clean reward R_clean = R_tox × I[no blacklist], and (4) three diversity rewards (lexical via SelfBLEU, semantic via sentence embeddings, image via DreamSim). GRPO updates the policy using group-relative advantages.
- **Core assumption**: The reward weights (λ_1 through λ_8) are correctly balanced; over-weighting diversity may reduce attack success, while under-weighting it leads to mode collapse.
- **Evidence anchors**: [Section 3.3.2] Full reward definitions and formulas provided.

### Mechanism 3
- **Claim**: Training against a surrogate open-source T2I model with simulated filters produces prompts that transfer to black-box commercial systems.
- **Mechanism**: The red-team LLM is trained on SD 2.1/SD 3 Medium equipped with an integrated filter (toxicity classifier + NSFW detector + image safety checker). The converged policy generates prompts that bypass similar-but-not-identical filters in commercial APIs.
- **Core assumption**: Commercial T2I services use safety mechanisms qualitatively similar to the integrated filter; large distribution shifts would reduce transfer.
- **Evidence anchors**: [Section 4.3] Transfer attack results: 70% TBR on leonardo.ai (nudity), 30% on fal.ai, 47% on stability.ai.

## Foundational Learning

- **Concept: Reinforcement Learning with Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: GRPO is the core RL algorithm used to optimize the red-team LLM. It differs from standard PPO by computing advantages relative to a group of samples rather than using a value function.
  - **Quick check question**: Can you explain why GRPO's group-relative advantage estimation might be more stable than PPO's value-function-based advantages for this task?

- **Concept: Multi-objective optimization with scalarized rewards**
  - **Why needed here**: The framework balances competing objectives (toxicity vs. stealth vs. diversity) via weighted sum. Understanding trade-offs is critical for tuning reward weights.
  - **Quick check question**: If increasing λ_3 (clean reward weight) from 1.0 to 5.0 improved nudity attack TCBR on SD3M, what negative side effect might occur on diversity metrics?

- **Concept: Transfer attacks in adversarial ML**
  - **Why needed here**: The practical value of GenBreak depends on prompts trained on open-source models working against commercial APIs with unknown defenses.
  - **Quick check question**: What two factors do the authors hypothesize explain GenBreak's transferability, and how would you test each hypothesis?

## Architecture Onboarding

- **Component map**: Red-team LLM -> SFT Datasets -> Surrogate T2I Model -> Integrated Filter -> Toxicity Evaluator -> Reward Aggregator -> GRPO Trainer

- **Critical path**:
  1. Curate SFT datasets (Category Rewrite via Gemini, Pre-Attack via iterative attacks)
  2. Fine-tune base