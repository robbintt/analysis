---
ver: rpa2
title: 'DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language
  Models in Code Generation'
arxiv_id: '2503.10452'
source_url: https://arxiv.org/abs/2503.10452
tags:
- code
- complexity
- problems
- dynacode
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DynaCode, a dynamic, complexity-aware benchmark
  for evaluating large language models (LLMs) on code generation. It addresses the
  limitations of static benchmarks by generating up to 189 million unique nested code
  problems across 4 complexity units and 16 call-graph structures, mitigating data
  contamination.
---

# DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation

## Quick Facts
- **arXiv ID:** 2503.10452
- **Source URL:** https://arxiv.org/abs/2503.10452
- **Reference count:** 23
- **Primary result:** Introduces a dynamic benchmark generating up to 189M unique nested code problems to evaluate LLMs on code generation across 4 complexity units and 16 call-graph structures

## Executive Summary
DynaCode addresses the limitations of static benchmarks by creating a dynamic, complexity-aware evaluation framework for large language models on code generation tasks. The benchmark generates unique nested code problems by combining unit functions through directed acyclic graphs, mitigating data contamination concerns. By separating code complexity (cyclomatic complexity) from call-graph complexity (nesting depth and branching), DynaCode provides a two-dimensional evaluation framework that reveals specific LLM weaknesses in handling multi-branch dependencies and complex execution flows.

## Method Summary
DynaCode generates nested code problems by classifying MBPP+ and LeetCode problems into 4 complexity units based on cyclomatic complexity, then combining them via 16 call graph structures (up to 5 nodes) while validating I/O type compatibility with MonkeyType. The system creates unique problems by dynamically assembling unit functions into DAGs, generating prompts and ground-truth code. Evaluation uses Pass@1 execution against test cases at temperature=0, with performance measured across both complexity dimensions to identify model strengths and weaknesses in handling sequential versus multi-branch execution flows.

## Key Results
- Performance drops 16.8% to 45.7% compared to MBPP+ across 12 LLMs
- Consistent degradation as complexity increases from Unit 1 to Unit 4
- LLMs perform significantly better on sequential call graphs (G1-G8) than multi-branch structures (G9-G16)
- "Problem Understanding" errors increase from 64.1% (Unit 1) to 88.8% (Unit 4)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A two-dimensional complexity framework (code complexity vs. call-graph complexity) exposes LLM limitations that static benchmarks miss.
- **Mechanism:** DynaCode separates internal logic difficulty (cyclomatic complexity of unit functions) from structural execution difficulty (nesting depth and branching in call graphs). By evaluating these independently, the benchmark isolates whether a model fails due to hard logic or complex dependency management.
- **Core assumption:** Models degrade predictably and differently across these two dimensions; specifically, that execution dependency is a distinct skill from logic generation.
- **Evidence anchors:**
  - [section 3.2] Defines the complexity matrix $C = \{c_{\xi,\eta}\}$ combining code units $\xi$ and graph levels $\eta$.
  - [section 4.2] Shows performance drops are consistent across units but vary by model architecture (e.g., GPT-4o resilience vs. others).
  - [corpus] Neighbors like *NetPress* support the shift toward dynamic, controllable benchmark generation.

### Mechanism 2
- **Claim:** Dynamic composition of "unit" functions into nested call graphs mitigates data contamination by forcing generalization over memorization.
- **Mechanism:** Instead of fixed problems, the system generates unique prompts by chaining function inputs/outputs via Directed Acyclic Graphs (DAGs). This prevents the model from relying on cached solutions for specific prompts, as the "path" through the functions is constantly changing.
- **Core assumption:** Contamination manifests as memorization of prompt-response pairs; if the prompt structure is novel but the sub-problems are known, the model must still reason about the composition.
- **Evidence anchors:**
  - [abstract] Claims the benchmark generates "up to 189 million unique nested code problems."
  - [section 4.3] Fine-tuning on unit functions resulted in poor performance on the full DynaCode (15.2% for GPT-3.5), suggesting memorizing units does not solve the dynamic composition.

### Mechanism 3
- **Claim:** LLMs exhibit a specific structural weakness in "Problem Understanding" that correlates with the multi-branch complexity of call graphs.
- **Mechanism:** As call graphs move from linear (sequential) to multi-branch (parallel) structures, the error rate for "Problem Understanding" (e.g., AssertionErrors) increases significantly. This suggests the model fails to maintain coherent state or logic flow when execution paths diverge or merge.
- **Core assumption:** The increase in AssertionErrors/ValueErrors in complex graphs is a proxy for the model's inability to model the execution flow mentally.
- **Evidence anchors:**
  - [section 4.2] "LLMs perform significantly better on sequentially structured graphs... performance drops considerably [in] multi-branch structures."
  - [section 4.3] Table 3 shows "Problem Understanding" errors rising from 64.1% (Unit 1) to 88.8% (Unit 4).
  - [corpus] *Evaluating Code Reasoning Abilities...* confirms LLMs struggle with nested constructs and dependencies.

## Foundational Learning

- **Concept: Cyclomatic Complexity**
  - **Why needed here:** This is the metric defining the x-axis of DynaCode (Units 1-4). It quantifies the number of linearly independent paths through the code.
  - **Quick check question:** If a function has 5 `if/else` blocks in sequence, does it have higher cyclomatic complexity than a function with 5 nested `if` statements?

- **Concept: Directed Acyclic Graphs (DAGs) in Function Calls**
  - **Why needed here:** DynaCode constructs benchmarks using DAGs to ensure valid function composition (no circular calls) and distinct topological levels.
  - **Quick check question:** Why is the "acyclic" constraint critical for automatically generating runnable test cases for an LLM?

- **Concept: Data Contamination (in Benchmarks)**
  - **Why needed here:** Understanding the problem DynaCode solves. Contamination occurs when test data leaks into training data, inflating performance metrics on static sets like MBPP.
  - **Quick check question:** Why does generating 189 million variations effectively "solve" the contamination issue for a static-parameter model?

## Architecture Onboarding

- **Component map:** Source Pool -> Classifier (Radon) -> Type Inferrer (Monkeytype) -> Graph Generator -> Assembler -> Evaluator
- **Critical path:** The I/O type matching is the bottleneck. The system cannot simply randomly connect functions; it must verify that `Function A` returns a type acceptable as input to `Function B`. Without valid type flow, the nested code is syntactically invalid.
- **Design tradeoffs:**
  - **Node limit:** Capped at 5 nodes per graph. This ensures the problem is solvable by current LLMs but may underestimate LLM capacity for longer contexts.
  - **Metric Choice:** Uses Cyclomatic Complexity rather than lines of code. This focuses on logical difficulty (branching) rather than verbosity.
- **Failure signatures:**
  - **Bad Generation:** If the generated nested code fails to execute (even before the LLM sees it), it is discarded. This happens if I/O types theoretically match but fail structurally (e.g., expecting a list, getting a specific object).
  - **Context Management Errors:** In evaluation, `NameError` or `UnboundLocalError` indicates the LLM failed to pass variables correctly between the sub-functions in the nested prompt.
- **First 3 experiments:**
  1.  **Run the Sequential vs. Parallel Baseline:** Evaluate your model on Graphs 1-4 (Sequential) vs Graphs 9-16 (Multi-branch) to establish a "dependency handling" profile.
  2.  **Contamination Check:** Fine-tune a smaller model (e.g., Llama-8B) on the Unit Functions only, then test on the full DynaCode. Verify if the model generalizes or collapses (expecting ~10-20% performance).
  3.  **Error Stratification:** Run inference on Unit 4 problems. Manually inspect the "Problem Understanding" errors (AssertionErrors) to see if the failure is in the logic of the sub-functions or the integration of their outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM performance degrade when call-graph node count exceeds 5, and what is the threshold where current architectures fundamentally fail?
- **Basis in paper:** [explicit] The limitations section states: "DynaCode primarily focuses on relatively call-graph structures, with a maximum node count of 5... We will extend DynaCode to include more complex call-graph structures in future work."
- **Why unresolved:** The authors capped complexity at 5 nodes to ensure manageable complexity for current LLMs, but did not systematically explore failure modes beyond this limit.
- **What evidence would resolve it:** Empirical evaluation of LLMs on call graphs with 6-15 nodes, measuring Pass@1 degradation curves and identifying critical thresholds.

### Open Question 2
- **Question:** Can the complexity-aware framework generalize to multi-language benchmarks while preserving the correlation between cyclomatic complexity and model performance?
- **Basis in paper:** [inferred] The methodology relies on Python-specific tools (Radon, MonkeyType) and Python ground truth code, with no discussion of language-agnostic applicability.
- **Why unresolved:** Cyclomatic complexity and call-graph semantics may behave differently across languages (e.g., C pointers, Java exceptions, Rust ownership), affecting benchmark validity.
- **What evidence would resolve it:** Cross-language experiments showing consistent performance degradation patterns when applying equivalent complexity units to Java, C++, or JavaScript problems.

### Open Question 3
- **Question:** What architectural modifications would improve LLMs' ability to handle multi-branch dependencies identified as a key weakness?
- **Basis in paper:** [explicit] The analysis reveals "LLMs perform well on sequential call graphs but struggle with complex, multi-branch dependencies, highlighting their difficulty in handling deeply nested execution flows and long-range function interactions."
- **Why unresolved:** The paper identifies the weakness but does not propose or test interventions (e.g., explicit call-graph attention mechanisms, intermediate verification steps).
- **What evidence would resolve it:** Comparative evaluation of models augmented with call-graph-aware components showing improved performance specifically on G9-G16 structures.

## Limitations

- The 5-node limit per graph may underrepresent real-world code complexity and LLM capacity for longer contexts
- The cyclomatic complexity metric may not fully capture logical difficulty for algorithms where complexity emerges from data structure manipulation
- The type matching mechanism may exclude valid function combinations where type conversion is trivial for humans but not programmatically detectable

## Confidence

- **High Confidence (8/10):** The core mechanism of using cyclomatic complexity and call-graph metrics to create a two-dimensional evaluation framework is well-supported by both theoretical justification and empirical results.
- **Medium Confidence (6/10):** The claim that DynaCode effectively mitigates data contamination is supported by the fine-tuning experiment, but this could be further validated with larger-scale memorization tests.
- **Medium Confidence (6/10):** The characterization of LLMs' specific weakness in multi-branch call graph structures is supported by error pattern analysis, but the mechanism connecting this to "Problem Understanding" errors could be more rigorously established.

## Next Checks

1. **Scale Validation:** Extend the node limit from 5 to 10 and evaluate whether performance degradation continues linearly or plateaus, testing the upper bounds of LLM code generation capabilities.

2. **Type System Robustness:** Implement a more flexible type matching system that includes common type conversions (int→float, list→iterable) and measure the impact on the number and quality of generated problems.

3. **Unit Function Difficulty Calibration:** Systematically vary the difficulty of individual unit functions within each complexity unit to determine whether the cyclomatic complexity thresholds accurately capture relative difficulty for different model sizes.