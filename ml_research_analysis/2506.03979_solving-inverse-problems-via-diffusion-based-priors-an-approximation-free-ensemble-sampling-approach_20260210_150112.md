---
ver: rpa2
title: 'Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free
  Ensemble Sampling Approach'
arxiv_id: '2506.03979'
source_url: https://arxiv.org/abs/2506.03979
tags:
- arxiv
- diffusion
- preprint
- inverse
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an ensemble-based posterior sampling method
  that integrates diffusion models with sequential Monte Carlo to solve Bayesian inverse
  problems without heuristic approximations. The key innovation is a rigorous derivation
  of the PDE dynamics governing posterior evolution under a diffusion prior, revealing
  a modified diffusion term and reweighting term distinct from existing approaches.
---

# Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach

## Quick Facts
- arXiv ID: 2506.03979
- Source URL: https://arxiv.org/abs/2506.03979
- Authors: Haoxuan Chen; Yinuo Ren; Martin Renqiang Min; Lexing Ying; Zachary Izzo
- Reference count: 40
- Key outcome: AFDPS methods outperform state-of-the-art diffusion-based solvers on multiple imaging inverse problems (Gaussian deblurring, motion deblurring, super-resolution, box inpainting) on FFHQ-256 and ImageNet-256 datasets, achieving superior PSNR and LPIPS metrics.

## Executive Summary
This paper proposes an ensemble-based posterior sampling method that integrates diffusion models with sequential Monte Carlo to solve Bayesian inverse problems without heuristic approximations. The key innovation is a rigorous derivation of the PDE dynamics governing posterior evolution under a diffusion prior, revealing a modified diffusion term and reweighting term distinct from existing approaches. The method implements these dynamics using stochastic weighted particle methods with resampling, offering two algorithmic variants: SDE-based and ODE+corrector. Theoretically, the authors prove convergence of the ensemble method to the PDE dynamics in the many-particle limit and provide error bounds relating posterior sampling accuracy to score function quality. Empirically, the proposed AFDPS methods outperform state-of-the-art diffusion-based solvers on multiple imaging inverse problems.

## Method Summary
The method starts with a pre-trained diffusion model that defines a prior distribution. For an inverse problem with observed data y, the goal is to sample from the posterior p(x|y) ∝ p(x) p(y|x). The authors rigorously derive a PDE that describes how this posterior evolves backwards in diffusion time. This PDE is then solved numerically using an ensemble of weighted particles, which is an instance of Sequential Monte Carlo. The particles evolve according to stochastic differential equations (SDE) or ordinary differential equations (ODE), with their weights updated to reflect the changing posterior density. A resampling step prevents weight degeneracy and ensures efficient use of the ensemble. The method avoids heuristic approximations used in other diffusion-based inverse problem solvers by deriving the dynamics directly from the underlying probability distributions.

## Key Results
- AFDPS-SDE and AFDPS-ODE outperform state-of-the-art diffusion-based solvers on Gaussian deblurring, motion deblurring, super-resolution, and box inpainting tasks
- Superior PSNR and LPIPS metrics compared to competing methods on FFHQ-256 and ImageNet-256 datasets
- The ODE+Corrector variant achieves comparable performance to SDE with fewer function evaluations through the use of corrector steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A rigorously derived PDE governing posterior evolution enables approximation-free sampling.
- Mechanism: The method starts from the Fokker-Planck equation for the prior diffusion process. By substituting the posterior distribution (defined as the prior times a likelihood factor), the authors derive an exact PDE for the posterior's time evolution. This PDE includes a modified drift term and a unique reweighting term that accounts for the likelihood. The key innovation is the exactness of this derivation, which avoids the heuristic approximations used in other methods (like DPS or ILVR) to estimate terms like ∇x log pt(y|xt). The method simulates this PDE directly using weighted particles.
- Core assumption: The log-likelihood function is twice differentiable. The derivation holds for the continuous-time limit; numerical discretization introduces error not covered by this theoretical guarantee.
- Evidence anchors:
  - [abstract] "By examining how the prior evolves through the diffusion process encoded by the pre-trained score function, we derive a modified partial differential equation (PDE) governing the evolution of the corresponding posterior distribution."
  - [section 1, Introduction] "The key innovation is a rigorous derivation of the PDE dynamics governing posterior evolution under a diffusion prior, revealing a modified diffusion term and reweighting term distinct from existing approaches."
  - [corpus] Related work like "A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models" also formulates the problem using gradient flows (related to PDEs), but AFDPS focuses on a specific particle-based SMC method derived from a new PDE. "On Stability and Robustness of Diffusion Posterior Sampling" highlights issues in existing solvers that this method aims to avoid.

### Mechanism 2
- Claim: An ensemble of weighted particles and strategic resampling approximates the solution to the posterior PDE.
- Mechanism: The derived posterior PDE is solved numerically using a stochastic weighted particle method, which is an instantiation of Sequential Monte Carlo (SMC). An ensemble of N particles evolves according to the PDE's dynamics (stochastic differential equation for position, ordinary differential equation for weight). The position dynamics include a new guidance term: -V(t)^2 * ∇xµy, which pulls particles towards high-likelihood regions. The weight dynamics increase the mass of particles in high-probability-density regions of the evolving posterior. A resampling step (Algorithm 1) is crucial: it prevents weight degeneracy (where a single particle dominates) by eliminating low-weight particles and duplicating high-weight ones, ensuring efficient use of the ensemble.
- Core assumption: Assumption 4.4, which states that a function of the likelihood and score is bounded and Lipschitz continuous, is required for the theoretical convergence proof. This assumption is enforced in practice by the resampling step.
- Evidence anchors:
  - [abstract] "The method implements these dynamics using stochastic weighted particle methods with resampling..."
  - [section 3.2, Posterior Sampling via Weighted Particles] "Such resampling sub-routine essentially performs global moves by eliminating low-weight particles and duplicating high-weight ones... However, the resampling approach is computationally more efficient as the weight dynamics (3.6) can be parallelized."
  - [corpus] The paper "Solving Bayesian inverse problems with diffusion priors and off-policy RL" also uses an ensemble/particle approach but frames it as a reinforcement learning problem, contrasting with AFDPS's PDE-driven SMC approach.

### Mechanism 3
- Claim: Explicit guidance by the log-likelihood gradient is essential for accurate posterior reconstruction.
- Mechanism: The particle dynamics include a term proportional to -∇xµy, the negative gradient of the log-posterior's potential. This term explicitly guides each particle in the direction that increases the likelihood of the observed data. This is a key difference from prior work like the FK-Corrector (from the concurrent work by Skreta et al.), which lacks this explicit guidance term in its drift. This guidance ensures particles remain consistent with the measurement throughout the reverse diffusion process, not just at the end.
- Core assumption: The gradient ∇xµy must be computable and its magnitude balanced against the score function's influence. The paper notes this guidance is similar to techniques in SGS+DM and ADMM but is derived systematically here.
- Evidence anchors:
  - [abstract] "This PDE includes a modified diffusion term and a reweighting term..." (The guidance is part of the modified dynamics).
  - [section 3.2, Remark 3.1] "...the −V(t)^2∇xµy term in our AFDPS drift marks a critical difference... Our empirical results... demonstrate that this specific term plays a vital role..."
  - [corpus] "EquiReg: Equivariance Regularized Diffusion for Inverse Problems" discusses incorporating likelihood terms, but AFDPS does so via a PDE-derived gradient term in the particle dynamics.

## Foundational Learning

- Concept: Fokker-Planck Equation
  - Why needed here: The entire method is derived by transforming the Fokker-Planck equation for the prior into a new PDE for the posterior. Understanding how diffusion processes relate to PDEs for probability densities is non-negotiable.
  - Quick check question: Given a simple SDE dx = -x dt + dW, can you write down the corresponding PDE for the evolution of the probability density p(x, t)?

- Concept: Sequential Monte Carlo (SMC) / Particle Filtering
  - Why needed here: The core algorithm is an SMC sampler. One must understand why weighted particles are used, why weights degenerate over time, and how resampling solves this problem.
  - Quick check question: In a particle filter, if the Effective Sample Size (ESS) drops close to 1, what has happened and how does a resampling step fix it?

- Concept: Diffusion Models as Priors (Reverse-Time SDE/ODE)
  - Why needed here: The method builds upon a pre-trained diffusion model. You need to understand how a network (score function) parameterizes the reverse-time drift term of a diffusion SDE/ODE to generate samples from a prior.
  - Quick check question: In the reverse-time SDE formulation, what two terms does the score function ∇x log p(x, t) appear in, and how do they differ between the SDE and the probability flow ODE?

## Architecture Onboarding

- Component map:
  - Pre-trained score network ϕθ(x, t) -> Particle System (positions and weights) -> Guidance Module (computes -V(t)^2 ∇xµy) -> Resampling/Correction Module

- Critical path: The success of the entire system hinges on the **Guidance Module**. The derivation in Lemma B.1 and B.2 must be correctly implemented to form the drift for the particle dynamics. An error here defeats the purpose of the "approximation-free" method. The next most critical step is the **Resampling Module**; without it, all but one particle will quickly have near-zero weight, wasting computation.

- Design tradeoffs:
  - **SDE (AFDPS-SDE) vs. ODE+Corrector (AFDPS-ODE)**: The SDE variant is simpler, with stochasticity aiding exploration. The ODE+Corrector variant uses a deterministic drift but requires a computationally expensive corrector step at each timestep to maintain diversity. The paper shows they have complementary strengths.
  - **Ensemble Size (N)**: Larger N improves multi-modality capture and robustness but increases memory and compute linearly. The paper used N=10 for SDE and N=5 for ODE to equalize NFE.
  - **Resampling Threshold (c)**: A smaller threshold avoids resampling more often, potentially preserving diversity but risking weight collapse. A larger threshold resamples frequently, which can cause sample impoverishment (all particles becoming clones).
  - **Guidance Strength (η)**: The paper suggests η (Remark B.8) can be tuned. η=1 (default AFDPS) uses strong guidance. η=0 recovers FK-Corrector dynamics. This is a trade-off between following the prior vs. the likelihood.

- Failure signatures:
  - **Blurred/low-detail reconstructions**: The guidance term may be too weak or the prior too strong. Check the magnitude of ∇xµy relative to the score.
  - **All particles converge to the same mode**: Weight collapse due to insufficient resampling or too few particles. Check ESS plots; if it drops to <2 consistently, the ensemble is failing.
  - **Unstable/overflowing weights**: The weight dynamics (Eq 3.6) may be numerically unstable for large t or if the log-likelihood is poorly scaled. Implementation must use log-weights as shown in Algorithm 2.
  - **Inconsistent results across seeds**: The stochasticity in the SDE is too high, or the corrector steps in ODE are insufficient. Reduce stochasticity or increase corrector iterations.

- First 3 experiments:
  1. **Verify Gradient Guidance**: Implement the forward model for a simple Gaussian deblurring task. Run a small ensemble (N=5) of *unweighted* particles with and without the guidance term (-∇xµy). Visualize the trajectories. The guided particles should move towards the high-likelihood region, while unguided ones should sample from the prior, ignoring the measurement.
  2. **Validate Resampling**: Use a simple 1D or 2D multi-modal posterior. Run the particle system with a low ESS threshold (e.g., c=0.2, resample rarely) vs. a high one (c=0.9, resample often). Plot the final particle distribution. The high-threshold run should show particle collapse to a single mode; the low-threshold run should (imperfectly) cover more modes.
  3. **Score vs. Likelihood Balance**: On a single image from FFHQ, run the full pipeline. Then, artificially scale the likelihood term µy by a factor (e.g., 0.5, 1.0, 2.0) in the guidance term. Observe the reconstruction: the small factor should yield a noisy/prior-dominated image; the large factor should yield an image that is very data-consistent but potentially noisy or unnatural (ignoring the prior). This builds intuition for the η parameter.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on Assumption 4.4 (boundedness and Lipschitz continuity of a likelihood-related function) and on the quality of the pre-trained score function. Poor score approximations can degrade the ensemble method's performance, as indicated by the error bounds in Theorem 4.1.
- The numerical discretization of the derived PDE introduces approximation errors not covered by the theoretical guarantees. The convergence proof (Theorem 4.2) applies to the continuous-time limit.
- The method's performance depends critically on the resampling strategy. Setting the ESS threshold too low risks weight collapse to a single mode; setting it too high causes sample impoverishment. Optimal tuning is problem-dependent.

## Confidence
- High: The derivation of the PDE dynamics from the Fokker-Planck equation is mathematically rigorous and the particle method implementation is standard.
- Medium: The empirical performance claims are well-supported by the presented experiments, but comparisons with more recent concurrent methods could strengthen the claims.
- Medium: The theoretical convergence results are sound but apply to the idealized continuous-time limit, not the discrete implementation.

## Next Checks
1. Conduct ablation studies on the resampling frequency (varying the ESS threshold) on a challenging multi-modal inverse problem to quantify the impact of resampling strategy on posterior exploration.
2. Test the AFDPS methods on a larger-scale dataset (e.g., CelebA-HQ 1024x1024) to evaluate scalability and identify any performance degradation compared to smaller datasets.
3. Implement a variant that uses a different particle resampling scheme (e.g., systematic resampling) to verify that the core performance gains are not specific to the multinomial resampling used in the paper.