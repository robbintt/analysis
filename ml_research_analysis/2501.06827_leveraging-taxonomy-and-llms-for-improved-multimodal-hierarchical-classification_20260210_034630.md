---
ver: rpa2
title: Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification
arxiv_id: '2501.06827'
source_url: https://arxiv.org/abs/2501.06827
tags:
- hierarchical
- classification
- levels
- consistency
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a taxonomy-embedded transitional LLM-agnostic
  framework for multi-modal hierarchical classification. The approach addresses limitations
  of traditional hierarchical classifiers by enforcing consistency across hierarchical
  levels through a taxonomy-aware transitional layer.
---

# Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification

## Quick Facts
- **arXiv ID:** 2501.06827
- **Source URL:** https://arxiv.org/abs/2501.06827
- **Reference count:** 17
- **Primary result:** Taxonomy-embedded transitional LLM-agnostic framework for multi-modal hierarchical classification

## Executive Summary
This paper introduces a taxonomy-embedded transitional LLM-agnostic framework for multi-modal hierarchical classification. The approach addresses limitations of traditional hierarchical classifiers by enforcing consistency across hierarchical levels through a taxonomy-aware transitional layer. The framework was evaluated on the MEP-3M e-commerce dataset using various backbone LLMs, demonstrating significant improvements in consistency, exact match, and accuracy metrics compared to flat classification approaches. The proposed method effectively leverages hierarchical relationships to enhance classification performance while maintaining flexibility across different LLM architectures.

## Method Summary
The TTC framework uses a two-stage training process: first, LoRA fine-tune a multimodal backbone LLM on a balanced subset of the MEP-3M dataset; second, train TTC classifier heads while freezing the backbone. The TTC layer implements hierarchical consistency by using transitional matrices that encode parent-child relationships in the taxonomy. At inference, predictions flow top-down, with each level's prediction gating the next via transitional matrix multiplication. The model employs temperature-scaled softmax with masked logits and hierarchical loss weighting to balance coarse and fine-grained learning across levels.

## Key Results
- Consistency improved by 0.078-0.11 across backbone models
- Exact Match increased by 0.06-0.082 for most models
- Accuracy@ℓ3 improved by up to 18% (OpenFlamingo)
- Some models showed HF1-Score decreases (-0.035 to -0.057) while gaining consistency

## Why This Works (Mechanism)

### Mechanism 1
Transitional matrices enforce parent-child constraints across hierarchical levels by masking invalid predictions. A binary matrix M[ℓi,ℓi+1] encodes "subclass-of" relationships (1 if child belongs to parent, 0 otherwise). Predicted probabilities from level ℓi are multiplied by this matrix to produce attention scores m[ℓi+1] = ŷ[ℓi] × M[ℓi,ℓi+1], which are applied via element-wise product to the next level's logits before softmax. This masks out classes inconsistent with the parent prediction.

### Mechanism 2
Temperature-scaled softmax with masked logits amplifies the effective probability gap between valid and invalid children. By zeroing out logits for invalid children before softmax, the model redistributes probability mass only among valid descendants. Combined with temperature scaling, this sharpens decisions at deeper levels where classes are more granular and harder to distinguish.

### Mechanism 3
Hierarchical loss weighting with level-specific importance factors balances coarse vs. fine-grained learning. The loss function includes π[ℓi] weights for each level's cross-entropy loss. This allows tuning emphasis: higher weights on upper levels encourage correct high-level decisions before fine-grained ones, aligning with the top-down inference flow.

## Foundational Learning

- **Hierarchical Classification Paradigms (Flat vs. Local vs. Global)**
  - Why needed here: TTC is a global approach with local-style per-level classifiers but added inter-level communication. Understanding why flat classifiers produce inconsistent predictions (Figure 1a: "Pearl" under "Fruit") is prerequisite to appreciating TTC's design.
  - Quick check question: Given a 3-level taxonomy with 4 ℓ1 classes and 10 ℓ2 classes per ℓ1, what's the dimension of M[ℓ1,ℓ2]?

- **Attention Mechanisms (Additive vs. Multiplicative)**
  - Why needed here: TTC uses transitional matrices as multiplicative attention, not the common additive query-key-value attention. Recognizing this distinction clarifies why the mechanism is simpler but requires explicit taxonomy encoding.
  - Quick check question: How does TTC's attention (ŷ × M) differ from standard transformer attention (softmax(QK^T/√d)V)?

- **Multimodal Feature Fusion**
  - Why needed here: The backbone LLMs (LLaVA, mPLUG-Owl, etc.) produce joint features from image+text. TTC operates on these fused representations; understanding how multimodal fusion works clarifies what information TTC can leverage.
  - Quick check question: If visual features fail to distinguish "Apple" from "Pear" but textual features succeed, where does the burden of discrimination lie in TTC?

## Architecture Onboarding

- **Component map:** (image, text) → Multimodal Backbone LLM → Joint latent feature vector a → TTC Layer (Dense[ℓ1] → softmax → ŷ[ℓ1]; ŷ[ℓ1] × M[ℓ1,ℓ2] → m[ℓ2]; Dense[ℓ2] ◦ m[ℓ2] → softmax → ŷ[ℓ2]; repeat for ℓ3...ℓn)

- **Critical path:**
  1. Backbone produces joint features (frozen after LoRA fine-tuning).
  2. TTC classifiers (W[ℓi], b[ℓi]) are trained on labeled hierarchy.
  3. At inference, predictions flow top-down; each level's prediction gates the next via transitional matrix multiplication.

- **Design tradeoffs:**
  - **Consistency vs. HF1-Score:** Some models (LLaVA-1.5, InstructBLIP) show HF1-Score drops (-0.035, -0.057) for consistency gains (+0.096, +0.11). Evaluate whether your task prioritizes coherent hierarchies or raw F1.
  - **Sequential vs. Parallel:** TTC processes levels sequentially; cannot parallelize across hierarchy depth. Latency scales with number of levels.
  - **Taxonomy Quality:** Method assumes correct, complete taxonomy; noisy or missing parent-child edges cause incorrect masking.

- **Failure signatures:**
  - **Flat parent predictions:** If ŷ[ℓi] is near-uniform, m[ℓi+1] provides weak guidance; deeper predictions revert to backbone behavior.
  - **Error propagation:** Wrong parent prediction forces all descendants into incorrect subtree; recovery impossible without bottom-up correction (not implemented).
  - **HF1-Score degradation:** Observed in 4/8 models; indicates over-constraining when backbone already has strong hierarchical understanding.

- **First 3 experiments:**
  1. **Ablation on matrix design:** Replace binary M with learned continuous weights; test if soft attention improves over hard masking (compare consistency and HF1-Score).
  2. **Error propagation analysis:** Measure how often wrong ℓ1 predictions cause wrong ℓ2/ℓ3; quantify recovery rate if parent is incorrect.
  3. **Taxonomy depth scaling:** Test on hierarchies with 2 vs. 5 levels; measure latency and whether consistency gains hold or diminish at depth.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating bottom-up information (child-to-parent validation) improve prediction accuracy compared to the current strictly top-down approach? The current approach "only considers top-down transitions, ignoring bottom-up information that could enhance prediction consistency across levels."

### Open Question 2
How can the TTC framework be modified for parallel processing to reduce latency in real-time applications? The "sequential nature of the TTC design limits its parallelizability, as predictions for different levels must be processed one after another."

### Open Question 3
Can the trade-off between strict consistency and Hierarchical F1-Score (HF1) be mitigated for high-performing backbone models? While enforcing the taxonomy improves structural coherence, it may occasionally penalize the model on the HF1 metric, which balances precision and recall across the hierarchy.

## Limitations
- The sequential nature of TTC limits parallelizability, increasing computational costs for real-time applications.
- Some backbone models show HF1-Score decreases while gaining consistency, indicating a potential trade-off.
- The framework only considers top-down transitions, ignoring bottom-up information that could enhance prediction consistency.

## Confidence

| Claim | Confidence |
|-------|------------|
| Method improves consistency across hierarchical levels | High |
| Temperature-scaled softmax with masked logits sharpens decisions | Medium |
| Sequential processing creates latency limitations | High |
| HF1-Score trade-off exists for some models | High |

## Next Checks

1. Verify that the transitional matrix M correctly encodes the taxonomy relationships by checking that M[i,j]=1 only when class j is a valid child of class i.
2. Test the sensitivity of HF1-Score to the importance factors π[ℓi] by running ablation studies with different weight configurations.
3. Measure inference latency scaling with hierarchy depth to quantify the parallelizability limitations mentioned in the paper.