---
ver: rpa2
title: Metadata Extraction Leveraging Large Language Models
arxiv_id: '2510.19334'
source_url: https://arxiv.org/abs/2510.19334
tags:
- extraction
- metadata
- chunk
- document
- fields
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a metadata extraction system leveraging Large
  Language Models (LLMs) for contract review. The approach focuses on three key components:
  robust text conversion using Azure Document Intelligence, strategic chunk selection
  via NER-enhanced Borda re-ranking and model-based chunk re-ranker, and advanced
  LLM techniques including Chain of Thought prompting and structured tool calling.'
---

# Metadata Extraction Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2510.19334
- Source URL: https://arxiv.org/abs/2510.19334
- Reference count: 19
- System achieves F1 scores improving from 0.66 (baseline) to 0.76-0.80 with proposed methods

## Executive Summary
This paper presents a metadata extraction system for contract review that leverages Large Language Models (LLMs) through a three-component architecture. The system combines robust text conversion using Azure Document Intelligence, strategic chunk selection via NER-enhanced Borda re-ranking and model-based chunk re-ranker, and advanced LLM techniques including Chain of Thought prompting and structured tool calling. Experiments on the CUAD dataset show F1 scores improving from 0.66 (baseline) to 0.76-0.80 with the proposed methods. On an internal lease contract dataset with 40+ fields, F1 scores improved from 0.36 to 0.43. The system also includes an LLM-based grader for error correction and online monitoring, achieving 80.5% match rate with ground truth compared to the agent's 73.3%.

## Method Summary
The system extracts metadata from legal contracts through a pipeline that first converts documents to text using Azure Document Intelligence, then selects relevant chunks via NER-enhanced Borda re-ranking and neural re-ranker, and finally processes chunks with LLM extraction agents using Chain of Thought prompting and JSON tool calling. A separate LLM grader validates and corrects extractions, enabling error correction through retry mechanisms. The approach focuses on three key innovations: strategic chunk selection to improve accuracy even with smaller context windows, Chain of Thought prompting for complex reasoning fields, and validation-as-correction via the LLM grader.

## Key Results
- F1 scores improve from 0.66 (baseline) to 0.76-0.80 with chunk re-ranking and CoT prompting on CUAD dataset
- Internal lease contract dataset: F1 improves from 0.36 to 0.43 across 40+ fields
- LLM grader achieves 80.5% match rate with ground truth vs agent's 73.3%
- Claude 3.5 Sonnet with tool calling achieves 0.48 F1 vs 0.36 baseline on internal dataset

## Why This Works (Mechanism)

### Mechanism 1: Strategic Chunk Selection via NER-Enhanced Re-ranking
The system scores chunks using four signals (per-field similarity, total-field similarity, per-field NER match count, total-field NER match count), then combines them via weighted Borda re-ranking. A neural re-ranker further learns to predict chunk-field relevance from embeddings, BM25 scores, and linguistic features. Evidence shows smaller, well-selected context windows can outperform larger context windows using the same LLM.

### Mechanism 2: Validation-as-Correction via LLM Grader
The grader receives document text, metadata template, and agent-extracted values, then outputs corrected values and confidence scores. This approach achieves higher accuracy than the original extraction agent because validation/correction is cognitively easier than extraction from scratch. The grader enables retry mechanisms and online monitoring without logging user documents.

### Mechanism 3: Chain-of-Thought Prompting for Complex Reasoning Fields
CoT prompts instruct the LLM to output reasoning in tags before final answers, making inference steps explicit and traceable. This approach shows substantial improvements for fields requiring multi-step reasoning, such as date calculations and cross-references.

## Foundational Learning

**F1 Score (Harmonic Mean of Precision and Recall)**
- Why needed: Primary evaluation metric throughout the paper; understanding why F1 matters for extraction tasks where false positives and false negatives have different costs
- Quick check: If a system extracts 10 values, 8 correct, but misses 5 true values, what are precision, recall, and F1?

**Borda Count Ranking**
- Why needed: Core to chunk selection mechanism—understanding how multiple scoring signals combine into a final ranking
- Quick check: If 4 chunks receive ranks (1st, 2nd, 3rd, 4th) across two scoring methods, how does Borda count determine the winner?

**Named Entity Recognition (NER) Labels**
- Why needed: Drives chunk boosting—understanding why mapping fields to entity types (DATE, ORG, PERSON) improves retrieval
- Quick check: Which SpaCy NER labels would you map to a "Governing Law" field? What are the limitations?

## Architecture Onboarding

**Component map:**
PDF Document -> Azure Document Intelligence (OCR + layout preservation) -> Chunking Pipeline -> Chunk Re-ranker (NER boosting + neural model) -> LLM Extraction Agent (CoT + JSON tool calling) -> LLM Grader (validation + correction) -> Structured JSON Output

**Critical path:** OCR quality -> chunk relevance -> LLM extraction quality. Errors compound downstream; poor OCR cannot be recovered by better chunk selection.

**Design tradeoffs:**
- Context window vs. cost: Smaller windows (4096 tokens) with re-ranking achieve 0.76 F1; larger windows (8192) achieve 0.80 but at higher cost
- Model selection: Gemini Flash with CoT offers cost efficiency; Claude 3.5 Sonnet with tool calling offers highest accuracy (0.48 vs 0.36 baseline)
- Grader overhead: Adds inference cost but enables retry mechanisms and online monitoring

**Failure signatures:**
- Low F1 on multi-Select fields (identified challenge)
- ORDINAL field extraction at 49.5% (monitor dashboard shows specific field-type failures)
- Temperature=0 still shows run-to-run variation (LLM non-determinism affects reliability)

**First 3 experiments:**
1. OCR quality baseline: Run same 10 contracts through Azure Document Intelligence vs pdftotext, measure character error rate and downstream F1 impact
2. Chunk re-ranking ablation: Compare baseline (cosine similarity only) vs NER boosting vs neural re-ranker on CUAD subset, plot F1 vs context window size
3. Grader agreement analysis: Run extraction agent and grader on same documents, measure agreement rate and identify systematic disagreement patterns by field type

## Open Questions the Paper Calls Out

**Open Question 1**
How can uncertainty quantification strategies mitigate run-to-run variations in LLM extraction outputs when temperature parameters are set to zero?
- Basis: The conclusion notes that "inherent uncertainty in LLM outputs presents a stability challenge," observing variations even at zero temperature.
- Why unresolved: Production environments require high reliability, yet current deterministic settings still yield inconsistent results.
- What evidence would resolve it: A demonstration of reduced variance across identical extraction runs using new mitigation protocols.

**Open Question 2**
To what extent can field-type-specific ranking models improve extraction performance compared to the current generalized chunk re-ranker?
- Basis: The authors identify "potential for developing field-type-specific ranking models" for metadata like dates or legal clauses.
- Why unresolved: The current unified re-ranker (F1 0.80) still lags significantly behind the theoretical oracle performance (F1 0.94).
- What evidence would resolve it: Empirical results showing higher F1 scores on the CUAD dataset using specialized rankers for specific field types.

**Open Question 3**
Do specialized techniques like hierarchical reasoning improve accuracy for complex multi-Select fields with numerous options?
- Basis: The conclusion highlights "particular challenges in handling multi-Select fields" and suggests exploring "hierarchical reasoning."
- Why unresolved: Multi-Select fields require complex reasoning across multiple document sections, which current baseline methods struggle to execute accurately.
- What evidence would resolve it: Comparative accuracy metrics on multi-Select fields in the internal dataset against standard Chain of Thought baselines.

## Limitations
- Performance on multi-Select fields remains challenging due to complex reasoning requirements across document sections
- Field-type-specific models not explored, limiting potential improvements for specialized metadata types
- System heavily dependent on specific LLM models and proprietary OCR technology not fully disclosed

## Confidence
- **High confidence**: Strategic chunk selection effectiveness (supported by ablation studies showing 0.66→0.80 F1 improvement), validation-as-correction mechanism (grader achieving 80.5% vs agent 73.3% match rate)
- **Medium confidence**: Chain of Thought prompting benefits (limited comparison data, weak corpus signals)
- **Low confidence**: Generalizability to documents outside the CUAD/internal lease contract scope, particularly for complex multi-select fields and abstract legal concepts

## Next Checks
1. **OCR quality impact assessment**: Compare Azure Document Intelligence vs alternative OCR on 20 contracts, measuring character error rates and downstream F1 degradation
2. **Chunk selection ablation study**: Implement baseline cosine similarity chunking, NER boosting, and neural re-ranker on CUAD subset, measuring incremental F1 gains across 4096/8192 token windows
3. **Grader agreement analysis**: Run extraction agent and grader on 50 contracts, categorize systematic disagreements by field type and document structure to identify blind spots