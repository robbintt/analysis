---
ver: rpa2
title: 'Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach'
arxiv_id: '2510.13094'
source_url: https://arxiv.org/abs/2510.13094
tags:
- unlearning
- gaussian
- certifiability
- high
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of machine unlearning in high-dimensional\
  \ regimes (p ~ n) where standard optimization assumptions break down. The authors\
  \ introduce \u03B5-Gaussian certifiability, a robust notion of privacy that optimally\
  \ captures noise-adding mechanisms in high dimensions."
---

# Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach

## Quick Facts
- **arXiv ID:** 2510.13094
- **Source URL:** https://arxiv.org/abs/2510.13094
- **Reference count:** 40
- **Primary result:** Single-step Newton unlearning with Gaussian noise achieves both privacy (ε-Gaussian certifiability) and accuracy (vanishing GED) in high-dimensional proportional regimes where p ~ n

## Executive Summary
This paper addresses machine unlearning in high-dimensional settings where standard optimization assumptions break down. The authors introduce ε-Gaussian certifiability as a robust privacy notion that optimally captures noise-adding mechanisms in high dimensions. They theoretically analyze a Newton-based unlearning algorithm with Gaussian noise perturbation, showing that a single Newton step suffices to achieve both privacy and accuracy in this setting. This contrasts with prior work requiring at least two Newton steps under ε-certifiability. Experimental results validate the theoretical findings, demonstrating superior performance of Gaussian noise over Laplace noise across various settings.

## Method Summary
The method employs a single-step Newton unlearning algorithm: starting from a ridge-regularized model β̂, it computes β̂^(1)_M = β̂ - H⁻¹_M ∇L_M(β̂) where H_M is the Hessian over the removal set, then adds Gaussian noise b ~ N(0, σ²I_p) with σ calibrated to achieve (ϕ, ε)-Gaussian certifiability. The noise scale depends on removal set size m, regularization λ, and dimension p through r = C₁(n)√(C₂(n)m³)/(2λνn) with σ = r/ε. The approach relaxes standard strong convexity assumptions to O(1/n) scaling appropriate for high dimensions, enabling single-step guarantees.

## Key Results
- A single Newton step with calibrated Gaussian noise achieves both ε-Gaussian certifiability (privacy) and vanishing generalization error divergence (accuracy) in p ~ n regimes
- ε-Gaussian certifiability is proven to be the tightest possible characterization of privacy for Gaussian mechanisms via the Gaussian trade-off function
- Experimental validation shows Gaussian noise outperforms Laplace noise, with GED scaling as m^1.5 rather than the theoretical m²√(polylog(n)/n) bound
- The method breaks down when m grows faster than n^{1/4}/polylog(n), requiring increasingly large noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single Newton step with Gaussian noise can achieve both ε-Gaussian certifiability (privacy) and vanishing generalization error divergence (accuracy) in high-dimensional proportional regimes (p ∼ n).
- **Mechanism:** The Newton step approximates the retrained model's optimum in one iteration because, under relaxed assumptions on the loss (convexity, not Ω(1) strong convexity), the Hessian curvature scales appropriately with dimension. Adding Gaussian noise calibrated to the residual error between the Newton update and the true retrained model obscures information about the removed data. The Gaussian noise is "dimension-free" – its privacy guarantee depends on the ℓ₂ norm of the residual, not directly on p, making it suitable for high dimensions.
- **Core assumption:** Assumptions (A1)-(A4) on the loss and regularizer (e.g., convex ℓ, ν-strongly convex r) and (B1)-(B2) on sub-Gaussian data. Crucially, this relaxes the standard Ω(1) strong convexity and O(1) smoothness assumed in low-dimensional unlearning work.
- **Evidence anchors:** [abstract] "Our analysis shows that a single Newton step, followed by a well-calibrated Gaussian noise, is sufficient to achieve both privacy and accuracy in this setting." [Section 4.3, Theorem 2 & 3] Prove the (ϕ, ε)-GPAR and vanishing GED for the one-step noisy Newton algorithm. [corpus] Related work "Certified Data Removal Under High-dimensional Settings" (arXiv:2505.07640) also analyzes high-dimensional unlearning but under a different (ε-certifiability) framework, finding two Newton steps are needed, underscoring the importance of the certification notion.
- **Break condition:** The guarantees hold only if the number of removal requests m satisfies m = o(n^{1/4}/polylog(n)). If m grows too large relative to n, the required noise variance may degrade accuracy unacceptably.

### Mechanism 2
- **Claim:** ε-Gaussian certifiability is a more natural and achievable privacy notion in high dimensions than prior notions like ε-certifiability or (ε,δ)-certifiability.
- **Mechanism:** ε-Gaussian certifiability is defined via the Gaussian trade-off function f_{G,ε}(α) = Φ(Φ^{-1}(1-α) - ε). This function is the tightest possible characterization of privacy for a Gaussian mechanism (by Blackwell ordering). In high dimensions, a broad class of isotropic log-concave noise-adding mechanisms converge in behavior to this Gaussian trade-off, making it a canonical choice. Prior notions require disproportionately large noise (e.g., Laplace) to meet the same privacy standard when the underlying mechanism is Gaussian, leading to poor accuracy.
- **Core assumption:** The privacy mechanism relies on adding isotropic noise (Gaussian or similar). The high-dimensional regime where p is large.
- **Evidence anchors:** [abstract] "Our result leads us to conclude that the discrepancy in the number of steps arises because of the sub optimality of the notion of ε-certifiability and its incompatibility with noise adding mechanisms, which ε-Gaussian certifiability is able to overcome optimally." [Section 3.2, Lemma 1 (Dimension freeness)] "For any μ₁, μ₂ ∈ ℝᵖ and σ > 0 let ε := (1/σ)∥μ₁ - μ₂∥₂. Then T(μ₁ + σN(0,Iₚ), μ₂ + σN(0,Iₚ)) ≡ T(N(0,1), N(ε,1))." [corpus] Weak direct evidence in provided corpus; relies heavily on the paper's internal references to Dong et al. (2021, 2022) for universality and tightness of Gaussian trade-offs.
- **Break condition:** If the underlying unlearning mechanism does not add isotropic log-concave noise, the universality property may not apply, and another certification notion might be tighter.

### Mechanism 3
- **Claim:** Generalization Error Divergence (GED) is a more suitable accuracy metric for high-dimensional unlearning than excess risk.
- **Mechanism:** GED measures the difference in expected loss on a fresh test point between the unlearned model and the ideally retrained model. This conditions on the dataset D and averages over algorithm randomness and the test point. In high dimensions, the excess risk compared to a population minimizer can diverge (as shown in appendix against Sekhari et al. (2021)), whereas GED can still vanish, providing a meaningful comparison between the unlearned and retrained models.
- **Core assumption:** The loss function ℓ(y|xᵀβ) is the primary metric of interest for model quality.
- **Evidence anchors:** [Section 3.3, Definition 3] Defines GED. [Section D.3, Eq. (25)-(27)] Shows how excess risk bounds from Sekhari et al. (2021) diverge in the p ∼ n regime, motivating GED. [corpus] No direct corpus support; this is a methodological contribution specific to this paper's theoretical critique.
- **Break condition:** If the practitioner cares about a different notion of model quality not captured by the expected loss ℓ, GED may not be the appropriate metric.

## Foundational Learning

- **Concept: Differential Privacy (DP) and Hypothesis Testing**
  - **Why needed here:** The paper's privacy framework (ε-Gaussian certifiability) is directly inspired by and extends the hypothesis testing interpretation of differential privacy (Dong et al., 2022). Understanding trade-off functions and how they quantify indistinguishability between outputs on adjacent datasets is crucial.
  - **Quick check question:** Can you explain why a trade-off function T(P, Q)(α) being closer to the diagonal implies stronger privacy between distributions P and Q?

- **Concept: High-Dimensional Asymptotics (p ∼ n)**
  - **Why needed here:** The entire theoretical contribution is framed in the proportional regime where standard optimization assumptions break down. Concepts like strong convexity constants scaling as O(1/n) (not Ω(1)) are central to why prior results fail and new analysis is needed.
  - **Quick check question:** Why does the condition number of the Hessian for a ridge-regularized loss typically worsen as p grows with n, breaking Ω(1) strong convexity assumptions?

- **Concept: Newton's Method in Optimization**
  - **Why needed here:** The unlearning algorithm analyzed is a single step of Newton's method. Understanding the Newton update (inverse Hessian times gradient) and its quadratic convergence properties (locally) is necessary to grasp why it can approximate retraining efficiently.
  - **Quick check question:** How does the Newton step differ from a gradient descent step, and why might it be more suitable for approximating the new optimum after data removal?

## Architecture Onboarding

- **Component map:** Initial Trainer (A) -> Store β̂ & T(Dₙ) -> Newton Approximator (B) -> Gaussian Noise Injector (C) -> Unlearned Model (D)
- **Critical path:** Training (A) → Store β̂ & T(Dₙ) → [On removal request M] → Compute Newton step → Inject calibrated Gaussian noise → Output unlearned model β̃_M. The most critical step is calibrating σ correctly based on the current m, n, p, λ, and ν to balance privacy and accuracy.
- **Design tradeoffs:**
  - **Noise scale (σ):** Larger σ improves privacy (smaller ε for fixed r) but increases GED. Must be set minimally per Theorem 2.
  - **Assumption relaxation vs. generality:** Using (A1)-(A4) allows high-dimensional theory but excludes some non-convex models. Standard strong convexity assumptions would simplify proofs but limit applicability.
  - **Caching Hessian:** Storing the full Hessian is O(p²) memory. For very large p, this may be infeasible, requiring approximate Hessian or first-order unlearning methods (not analyzed here).
- **Failure signatures:**
  - **Non-vanishing GED:** If m grows too fast relative to n (break condition), or assumptions (A1)-(A4)/(B1)-(B2) are violated, GED may not converge to zero with increasing data.
  - **Privacy breach:** If σ is underestimated (e.g., due to incorrect estimation of C₁(n), C₂(n) constants in practice), the (ϕ, ε)-GPAR guarantee may fail.
  - **Numerical instability:** Hessian inversion for Newton step can be unstable for ill-conditioned problems common in high dimensions.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Replicate the paper's Figure 1: Logistic regression with n=p, varying p, m=1,5,10. Compare GED for Gaussian vs. Laplace noise (as a proxy for ε-certifiability). Verify Gaussian noise GED decreases ~p^{-0.5}.
  2. **Scaling Test:** Fix ε and p, vary m up to ~n^{0.25}. Plot GED vs. m to empirically validate the m^{1.5} scaling trend suggested by Figure 4 and check the break condition.
  3. **Robustness Check:** Test on a model outside the GLM family but with similar structure (e.g., a small multi-layer perceptron with convex loss). Measure GED to see if theoretical insights transfer, noting where assumptions may break.

## Open Questions the Paper Calls Out

- **Question:** Can the ε-Gaussian certifiability framework and single-step Newton unlearning guarantees be extended to non-convex loss functions under stronger constraints on the removal set size m?
  - **Basis in paper:** [explicit] Section D.1 states: "we expect the theorems presented in this paper to extend to certain non-convex loss functions, potentially under stronger constraints on the size of the removal set... The main remaining challenge, however, is to verify whether these assumptions hold for widely used AI models such as transformers or multilayer perceptrons (MLPs)."
  - **Why unresolved:** The current proofs rely on convexity to ensure unique minima and well-behaved Newton steps. Non-convex landscapes introduce multiple local minima and may break the argument that the post-removal minimizer stays within the same basin of attraction.
  - **What evidence would resolve it:** A theoretical analysis showing that for specific non-convex loss classes (e.g., those satisfying the PL condition), the single-step noisy Newton method achieves ε-GPAR with vanishing GED, potentially with stricter requirements on m = o(n^α) for some α < 1/4.

## Limitations
- The theoretical guarantees are limited to convex loss functions and linear models with ridge regularization, excluding more complex architectures like neural networks
- The noise calibration requires precise knowledge of constants C₁(n) and C₂(n) that are stated only as "O(polylog(n))" without explicit values
- The method breaks down when the number of removal requests grows faster than n^{1/4}/polylog(n), limiting scalability for large removal sets

## Confidence
- **High:** The characterization of ε-Gaussian certifiability as the optimal privacy notion for Gaussian mechanisms in high dimensions is well-founded, with strong theoretical grounding in the Gaussian trade-off function universality
- **Medium:** The claim that a single Newton step suffices for both privacy and accuracy in high dimensions, while theoretically proven, requires careful calibration of noise parameters that may be challenging in practice
- **Medium:** The experimental validation using synthetic logistic regression data supports the theoretical claims, though the results are limited to this specific setting

## Next Checks
1. **Numerical Verification of Noise Scaling:** Implement the one-step Newton algorithm with calibrated Gaussian noise on synthetic data (n=p ranging from 500 to 5000) and verify that the GED scales as O(m^{1.5}/n^{1/2}) as predicted, testing across multiple values of m up to the theoretical break point.

2. **Cross-Validation of Privacy Guarantees:** Conduct hypothesis testing experiments to empirically validate the (ϕ, ε)-Gaussian certifiability bounds by attempting to distinguish models trained with and without specific data points, measuring the trade-off function T(α) across various ε values.

3. **Assumption Sensitivity Analysis:** Systematically relax assumptions (A1)-(A4) and (B1)-(B2) in simulation to identify which assumptions are most critical for the theoretical guarantees, particularly testing scenarios where the Hessian is poorly conditioned or data deviates from sub-Gaussian distributions.