---
ver: rpa2
title: 'HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations'
arxiv_id: '2503.07833'
source_url: https://arxiv.org/abs/2503.07833
tags:
- sentence
- edited
- hallucination
- error
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HalluVerse25 is a fine-grained multilingual dataset for LLM hallucination
  detection in English, Arabic, and Turkish, containing 1310, 828, and 978 sentence
  pairs respectively. The dataset categorizes hallucinations into entity-level, relation-level,
  and sentence-level errors, with hallucinations injected by an LLM into factual biographical
  sentences and validated through human annotation (Cohen''s Kappa: 0.748-0.805).'
---

# HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations

## Quick Facts
- arXiv ID: 2503.07833
- Source URL: https://arxiv.org/abs/2503.07833
- Authors: Samir Abdaljalil; Hasan Kurban; Erchin Serpedin
- Reference count: 13
- Contains 1310 English, 828 Arabic, and 978 Turkish sentence pairs with annotated hallucinations

## Executive Summary
HalluVerse25 is a fine-grained multilingual dataset designed for evaluating LLM hallucination detection across English, Arabic, and Turkish. The dataset categorizes hallucinations into three types: entity-level, relation-level, and sentence-level errors. Hallucinations were injected into factual biographical sentences using an LLM and validated through human annotation with good inter-annotator agreement (Cohen's Kappa: 0.748-0.805). The dataset provides a comprehensive resource for testing LLM hallucination detection capabilities in multilingual settings.

## Method Summary
The dataset was constructed using Wikipedia biographical sentences as source material. Hallucinations were injected into these sentences using GPT-4o, creating 3148 total sentence pairs across three languages. Each pair consisted of an original factual sentence and a modified hallucinated version. Human annotators validated the hallucinations and categorized them into entity-level (changes to specific entities), relation-level (changes to relationships between entities), or sentence-level (addition of unsupported claims). The dataset was then used to evaluate multiple LLMs on their ability to detect and classify these hallucinations using accuracy, precision, and recall metrics.

## Key Results
- GPT-4o achieved the highest accuracy across all languages and hallucination types
- phi-4 performed competitively, particularly excelling in entity and relation hallucination detection
- All models consistently misclassified sentence-level hallucinations as entity errors
- Performance varied significantly across languages, with English showing highest accuracy

## Why This Works (Mechanism)
The dataset's effectiveness stems from its fine-grained categorization of hallucination types, which allows for detailed analysis of model capabilities. By distinguishing between entity, relation, and sentence-level hallucinations, the dataset reveals systematic weaknesses in current LLMs' understanding of factual relationships and context. The multilingual approach ensures that models cannot rely on language-specific cues and must demonstrate genuine comprehension across linguistic boundaries.

## Foundational Learning
- Multilingual dataset construction: Creating balanced datasets across languages requires understanding linguistic diversity and annotation challenges. Quick check: Verify language balance and annotation quality across all three languages.
- Hallucination categorization: Distinguishing between hallucination types requires clear operational definitions and consistent annotation protocols. Quick check: Review inter-annotator agreement scores and annotation guidelines.
- LLM-based data generation: Using LLMs to generate training data introduces systematic biases that must be understood and controlled. Quick check: Analyze hallucination patterns for consistency with known LLM behaviors.

## Architecture Onboarding

**Component Map:** Wikipedia sentences -> LLM hallucination injection -> Human validation -> Dataset creation -> LLM evaluation

**Critical Path:** Source sentence selection → Hallucination injection → Human validation → Model evaluation → Performance analysis

**Design Tradeoffs:** Using LLM-generated hallucinations provides scalability but may not reflect natural error patterns. Domain-specific (biographical) source material ensures factual consistency but limits generalizability.

**Failure Signatures:** Systematic misclassification of sentence-level hallucinations as entity errors suggests models rely on surface features rather than semantic understanding.

**First Experiments:**
1. Test model performance on source sentences without injected hallucinations to establish baseline
2. Evaluate cross-lingual transfer by training on one language and testing on others
3. Analyze error patterns to identify whether models use position-based or semantic cues

## Open Questions the Paper Calls Out
None

## Limitations
- Domain-specific corpus limited to Wikipedia biographical sentences
- LLM-generated hallucinations may not represent naturally occurring errors
- Small test sets (328-978 examples per language) may provide unstable performance estimates
- Limited evaluation metrics (accuracy, precision, recall) without F1-score or confusion matrices

## Confidence
- Dataset quality and annotation reliability: High
- Multilingual evaluation results: Medium (limited by sample size and domain specificity)
- Model performance comparisons: Medium (constrained by evaluation methodology)
- Systematic misclassification patterns: High

## Next Checks
1. Replicate the evaluation using a broader range of source text domains (news articles, scientific papers, dialogue) to assess domain generalizability
2. Conduct ablation studies to determine whether model performance varies based on hallucination injection method (LLM-generated vs. human-generated errors)
3. Implement cross-validation with larger test sets and additional evaluation metrics (F1-score, confusion matrices) to better understand performance distributions and error patterns