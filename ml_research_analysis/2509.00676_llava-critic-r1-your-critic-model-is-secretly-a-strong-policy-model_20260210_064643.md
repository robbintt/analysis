---
ver: rpa2
title: 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model'
arxiv_id: '2509.00676'
source_url: https://arxiv.org/abs/2509.00676
tags:
- critic
- reasoning
- training
- llava-critic-r1
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the standard separation between critic models
  (which evaluate responses) and policy models (which generate them) in vision-language
  modeling. The authors propose performing reinforcement learning directly on a base
  generative model using preference-labeled critic datasets, producing LLaVA-Critic-R1
  - a model that optimizes for preference judgments while retaining generation ability.
---

# LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model

## Quick Facts
- arXiv ID: 2509.00676
- Source URL: https://arxiv.org/abs/2509.00676
- Authors: Xiyao Wang; Chunyuan Li; Jianwei Yang; Kai Zhang; Bo Liu; Tianyi Xiong; Furong Huang
- Reference count: 27
- Primary result: Direct RL on critic data produces a model competitive as both critic and policy, achieving SoTA 71.9 on MMMU at 7B scale

## Executive Summary
This work challenges the standard separation between critic models (which evaluate responses) and policy models (which generate them) in vision-language modeling. The authors propose performing reinforcement learning directly on a base generative model using preference-labeled critic datasets, producing LLaVA-Critic-R1 - a model that optimizes for preference judgments while retaining generation ability. Surprisingly, LLaVA-Critic-R1 not only becomes a top-performing critic but also emerges as a competitive policy model, matching or surpassing specialized reasoning VLMs across 26 visual reasoning and understanding benchmarks with an average gain of +5.7% over its base model. Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, achieving state-of-the-art performance of 71.9 on MMMU at the 7B scale. Additionally, the enhanced critic capability enables effective test-time scaling through self-critique, yielding an average +13.8% improvement on five representative reasoning tasks without additional training.

## Method Summary
The method reorganizes preference-labeled critic datasets into verifiable training signals and performs reinforcement learning directly on a base generative model. Specifically, the authors strip away GPT-generated rationales and evaluation metrics from pairwise critic data, retaining only image-question-response triplets with ground-truth preference labels. This forces the model to develop autonomous reasoning rather than reproducing external judgments. Rewards combine preference accuracy (α=0.9) with format adherence (1-α=0.1). The approach is implemented using Group Relative Policy Optimization (GRPO) on Qwen-2.5-VL-7B base, with thinking templates requiring explicit reasoning tags and boxed answers.

## Key Results
- LLaVA-Critic-R1 achieves state-of-the-art 71.9 on MMMU at 7B scale, outperforming specialized reasoning VLMs
- Self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training
- Direct RL on critic data produces a model competitive as both critic and policy, with +5.7% average gain over base model across 26 benchmarks
- Training on critic data improves both critic and policy capabilities through enhanced visual perception and structured reasoning

## Why This Works (Mechanism)

### Mechanism 1: Verifiable RL from Preference Data
Converting preference-labeled critic data into a verifiable RL task enables direct policy optimization without knowledge distillation. The authors strip away GPT-generated rationales and evaluation metrics from pairwise critic data, retaining only image-question-response triplets with ground-truth preference labels. This forces the model to develop autonomous reasoning rather than reproducing external judgments. Rewards combine preference accuracy (α=0.9) with format adherence (1-α=0.1). The core assumption is that models develop stronger reasoning when forced to derive evaluation criteria internally rather than imitating external annotators.

### Mechanism 2: Dual-Path Improvement via Critic Data Exposure
Training on critic data improves both critic and policy capabilities through enhanced visual perception and structured reasoning. Two synergistic effects: (1) comparing response pairs requires identifying hallucinations and image-grounded details, improving perception; (2) the format reward enforces "think-then-answer" generation patterns, strengthening step-by-step reasoning. Ablation shows format-only training improves reasoning but degrades perception tasks.

### Mechanism 3: Test-Time Self-Critic Scaling
A unified critic-policy model enables effective test-time scaling through self-evaluation without external reward models. Generate N candidate responses with temperature 0.9, then perform recursive pairwise comparisons using the same model as critic. This "Best-of-N" approach yields +13.8% average improvement across 5 benchmarks, substantially outperforming majority vote baselines.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Core training objective; differs from PPO by using group-level advantage estimation without learned value function.
  - **Quick check question:** Can you explain how GRPO reduces variance compared to vanilla policy gradient while avoiding a separate critic network?

- **Concept: Preference Learning / Bradley-Terry Modeling**
  - **Why needed here:** Underlying structure of pairwise comparison data; understanding why preference labels enable scalar reward computation.
  - **Quick check question:** Given pairwise preference data A>B, B>C, how would Bradley-Terry assign scores?

- **Concept: Chain-of-Thought with Structured Output**
  - **Why needed here:** Format reward depends on proper use of `##` and `\boxed{}` tokens; model must learn to separate reasoning from final answers.
  - **Quick check question:** Why might enforcing explicit reasoning structure improve out-of-distribution generalization?

## Architecture Onboarding

- **Component map:**
  Critic Data (40K pairwise) -> RL Prompt Template -> GRPO Training -> LLaVA-Critic-R1 -> Policy Inference (Think-then-answer template) -> Task Responses; Critic Inference (Pairwise comparison) -> Best-of-N Selection

- **Critical path:**
  1. Data preprocessing: Strip GPT rationales, retain (image, question, response_1, response_2, preference_label)
  2. Prompt formatting: Apply thinking template from Table 2
  3. Reward computation: r = 0.9 * r_pref + 0.1 * r_format
  4. GRPO training on Qwen-2.5-VL-7B base (or stronger reasoning model for R1+)
  5. Evaluation: Apply same template for both policy and critic tasks

- **Design tradeoffs:**
  - Cold-start RFT vs. SFT+RFT: Cold-start preserves policy capability better; SFT+RFT slightly improves critic benchmarks but degrades generalization
  - Critic-only vs. joint training: Policy-then-critic training (R1+) achieves best balance; mixed training underperforms both
  - Base model selection: Starting from strong reasoning model (ThinkLite, MiMo-VL) yields better final policy but slightly weaker critic

- **Failure signatures:**
  - Policy performance degrades after ~350 GRPO steps (overfitting to critic objective)
  - Self-critic effectiveness drops if critic capability lags policy capability
  - VLM agent capabilities can be lost if base model already underwent heavy policy training

- **First 3 experiments:**
  1. Sanity check: Train with format-only reward (α=0); verify reasoning improves but perception degrades per Table 5
  2. Scaling curve: Plot self-critic performance vs. N (2, 4, 8, 16, 32, 64, 128); expect plateau around 64-128 per Figure 2
  3. Training dynamics: Checkpoint every 50 steps; measure both critic (VLRewardBench) and policy (MMMU, MathVista) to identify optimal stopping point before policy degradation

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms cause the divergence between critic and policy performance in later stages of critic training (steps 200-400)? The paper observes this phenomenon but does not isolate the causal factors or propose interventions to maintain alignment throughout extended training.

### Open Question 2
Why does critic training fail to recover GUI agent capabilities lost during policy-focused training? The paper demonstrates the failure but provides no explanation for why agent grounding degrades irreversibly or differs from other capabilities that critic training enhances.

### Open Question 3
Why do current critic benchmarks (VLRewardBench, MM-RLHF) fail to capture limitations in self-critic selection ability? The disconnect between benchmark performance and actual selection quality suggests existing metrics do not measure the right capabilities for test-time scaling.

## Limitations
- The approach's effectiveness depends heavily on the quality and diversity of the critic data; limited coverage of certain domains may restrict generalization
- Self-critic performance, while improved, still shows substantial gaps compared to oracle evaluation, indicating limitations in preference judgment calibration
- The irreversible loss of GUI agent capabilities during policy training suggests the method cannot recover all types of specialized abilities

## Confidence
**High Confidence:** The experimental methodology is sound, with clear training procedures and comprehensive benchmarking across 26 tasks. The observation that critic training improves policy performance is empirically validated through controlled ablations.

**Medium Confidence:** The mechanism explaining why critic training transfers to policy improvement (enhanced visual perception + structured reasoning) is plausible but under-supported by direct evidence. The format enforcement's role in reasoning improvement needs more rigorous isolation.

**Low Confidence:** Claims about the universality of the approach (that "any" critic data can train strong policies) extrapolate beyond the tested datasets. The optimal stopping point of ~350 GRPO steps appears empirically determined but lacks theoretical justification.

## Next Checks
1. **Dataset Contribution Analysis:** Train separate models on individual preference datasets (VLFeedback only, RLHF only, RLHF-V only) and measure both critic and policy performance to identify which datasets drive which improvements.

2. **Self-Critic Calibration Study:** Systematically compare self-critic performance against oracle critic (human or GPT-4) on the same candidate sets to quantify the calibration gap and identify failure modes in preference judgment.

3. **Format Template Ablation:** Test alternative reasoning formats (different delimiters, no explicit thinking section, free-form vs structured) to isolate whether the observed reasoning improvements are due to format enforcement specifically or any structured output constraint.