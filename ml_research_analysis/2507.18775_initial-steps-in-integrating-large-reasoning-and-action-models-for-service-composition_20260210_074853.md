---
ver: rpa2
title: Initial Steps in Integrating Large Reasoning and Action Models for Service
  Composition
arxiv_id: '2507.18775'
source_url: https://arxiv.org/abs/2507.18775
tags:
- service
- composition
- reasoning
- large
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an architectural framework for integrating
  Large Reasoning Models (LRMs) and Large Action Models (LAMs) to advance automated
  service composition. The framework addresses the limitations of existing approaches
  by combining LRM's deep reasoning capabilities with LAM's direct action execution
  abilities.
---

# Initial Steps in Integrating Large Reasoning and Action Models for Service Composition

## Quick Facts
- arXiv ID: 2507.18775
- Source URL: https://arxiv.org/abs/2507.18775
- Reference count: 33
- Primary result: Conceptual framework for LRM-LAM integration in service composition

## Executive Summary
This paper proposes an architectural framework that integrates Large Reasoning Models (LRMs) and Large Action Models (LAMs) to advance automated service composition. The framework addresses limitations of existing approaches by leveraging LRM's deep reasoning capabilities for semantic understanding and compositional planning, combined with LAM's direct action execution abilities for API interfacing and real-time failure handling. The three-layer architecture consists of request analysis and service discovery, service composition, and service execution with adaptation, coordinated through a middleware layer. This approach aims to transform service composition from a semi-automated process to a fully automated system capable of interpreting natural language requests, discovering services, composing optimal workflows, and executing them while handling failures.

## Method Summary
The proposed method introduces a three-layer architectural framework for automated service composition. Layer 1 uses LRM for request analysis, semantic understanding, and service selection while Layer 2 uses LRM for composition planning and optimization. Layer 3 employs LAM for composition specification, service interfacing, and execution with monitoring. A coordination layer middleware orchestrates interactions between layers and manages data generation for continuous training. The framework leverages model specialization where LRMs handle reasoning-intensive tasks and LAMs handle execution-intensive tasks, with feedback loops enabling iterative refinement. Implementation requires selecting appropriate models, creating a service registry, and developing coordination protocols to bridge semantic and execution gaps.

## Key Results
- Conceptual framework addresses limitations of general-purpose LLMs in service composition
- Three-layer architecture separates reasoning (LRM) from execution (LAM) tasks
- Coordination layer enables feedback-driven adaptation and continuous training
- Framework aims to achieve Level 5 automation in service composition systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning reasoning and action across specialized model types may address the complementary limitations of general-purpose LLMs in service composition.
- Mechanism: LRMs handle semantic understanding, constraint inference, and compositional planning (Layer 1–2 tasks), while LAMs handle API interfacing, execution, and real-time failure recovery (Layer 3 tasks). The division leverages LRM strength in "deep, explicit reasoning" and LAM strength in "directly executing actions in environments."
- Core assumption: Reasoning and action capabilities are sufficiently separable that model specialization yields better outcomes than a single general-purpose model.
- Evidence anchors: [abstract] "LRMs address the challenges of semantic reasoning and ecosystem complexity while LAMs excel in dynamic action execution and system interoperability. However, each paradigm has complementary limitations."
- Break condition: If LRMs cannot reliably produce structured composition plans that LAMs can parse and execute, or if handoff overhead exceeds gains from specialization.

### Mechanism 2
- Claim: A coordination layer enables feedback-driven adaptation by routing execution results back to reasoning layers.
- Mechanism: The middleware maintains composition state across layers, routes monitoring signals from Layer 3 to Layer 2, and triggers iterative refinement or recomposition. This creates cyclic flow rather than pure sequential pipeline.
- Core assumption: Real-time execution feedback is sufficiently structured and informative for LRMs to adjust compositions effectively.
- Evidence anchors: [section 4] "The coordination layer serves two essential functions. First, it coordinates the operation of the inference layers, enabling information flow and continuous adaptation... It can also route execution results from Layer 3 back to Layer 2, enabling iterative and reflective refinement."
- Break condition: If feedback signals are noisy, delayed, or ambiguous such that LRM refinement degrades rather than improves compositions.

### Mechanism 3
- Claim: Data generation pipelines from inference outputs may enable continuous model improvement over time.
- Mechanism: Composition Data Generation and Execution Data Generation components document rationale, failure patterns, and recovery strategies, feeding the Training Phase for fine-tuning or new model training.
- Core assumption: Generated data from service composition and execution is of sufficient quality and diversity to improve model performance in this domain.
- Evidence anchors: [section 4] "The coordination layer can be responsible for systematically capturing, processing, and generating data from each inference layer for the training phase."
- Break condition: If generated training data introduces bias, reinforces errors, or lacks sufficient signal for meaningful improvement.

## Foundational Learning

- Concept: Service Composition Pipeline Phases
  - Why needed here: The architecture maps directly to six phases (request specification → discovery → composition → execution → monitoring → adaptation). Understanding this pipeline is prerequisite to placing LRM vs. LAM responsibilities.
  - Quick check question: Can you name the phase where a system switches to a backup weather service after an API error?

- Concept: LRM vs. LAM Capability Profiles
  - Why needed here: Layer assignments depend on matching model strengths to task requirements (reasoning vs. action). Misalignment causes architecture failures.
  - Quick check question: Which model type should handle "inferring implicit constraints not formally specified in the user request"?

- Concept: Feedback Loop Design in AI Systems
  - Why needed here: The coordination layer's value depends on well-designed feedback from execution to reasoning. Poorly structured feedback breaks the adaptation mechanism.
  - Quick check question: What information must Layer 3 send to Layer 2 to enable "iterative and reflective refinement"?

## Architecture Onboarding

- Component map:
  Layer 1 (LRM/LAM): UC Request Analysis (LRM) -> Service Metadata Retrieval (LAM) -> Semantic Understanding (LRM) -> Service Selection (LRM)
  Layer 2 (LRM only): Composition Planning -> Optimization & Selection -> Composition Validation
  Layer 3 (LAM only): Composition Specification -> Service Interfacing -> Service Execution -> Execution Monitoring -> Failure Handling
  Coordination Layer: Middleware orchestrating layer interactions + data generation for training
  Training Phase: Model updates from generated composition/execution data

- Critical path: Request Analysis → Semantic Understanding → Service Selection → Composition Planning → Composition Specification → Service Execution → Monitoring → (potential) Failure Handling → (potential) feedback to Composition Planning

- Design tradeoffs:
  - Sequential vs. interleaved layer interaction: Pure sequential is simpler but less adaptive; interleaved enables real-time adjustment but increases coordination complexity
  - LRM-only vs. LRM+LAM in Layer 1: LAM may improve metadata retrieval precision but adds integration overhead
  - Training frequency: Continuous training improves adaptation but risks instability; batch training is stable but slower to improve

- Failure signatures:
  - LRM produces composition plan with incompatible service interfaces → Layer 3 execution fails at Service Interfacing
  - LAM executes correctly but monitoring fails to detect subtle data quality issues → downstream failures with no triggered adaptation
  - Feedback loop overwhelms LRM with noise → composition quality degrades over time
  - Training data captures systematic errors → model reinforces bad patterns

- First 3 experiments:
  1. Layer 2 validation test: Provide LRM with sample service catalogs and natural language requests; measure composition plan correctness (valid service combinations, constraint satisfaction) before any LAM integration
  2. Layer 3 execution test: Given valid composition plans, execute via LAM against mock APIs with injected failures; measure detection rate and recovery strategy appropriateness
  3. End-to-end feedback test: Run full pipeline with intentional service failures; verify whether Layer 3 feedback triggers appropriate Layer 2 recomposition and whether composition improves over 5–10 iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current state-of-the-art Large Reasoning Models (LRMs) and Large Action Models (LAMs) perform when evaluated specifically on the distinct tasks defined by the architectural layers (Request Analysis, Composition, and Execution)?
- Basis in paper: [explicit] The authors explicitly call for the "evaluation and adaptation of existing LRMs and LAMs specifically for automated service composition" to identify limitations regarding the tasks in the proposed layers.
- Why unresolved: The paper is a conceptual proposal; no empirical evaluation of existing models against this specific framework has been conducted yet.
- What evidence would resolve it: Benchmark results showing the accuracy and robustness of models like OpenAI o1 or DeepSeek R1 when performing service discovery, composition validation, and failure handling tasks.

### Open Question 2
- Question: How can the cycles of discovery, reasoning, execution, and adaptation be orchestrated in a flexible and context-aware manner rather than a strictly sequential pipeline?
- Basis in paper: [explicit] The conclusion identifies the need to explore "how cycles of discovery, reasoning, execution, and adaptation can be orchestrated in flexible and context-aware ways."
- Why unresolved: While the paper proposes a coordination layer, the specific logic for managing dynamic interactions and feedback loops between layers remains undefined.
- What evidence would resolve it: A working prototype demonstrating a non-linear workflow where execution failures dynamically trigger re-planning without human intervention.

### Open Question 3
- Question: What specific coordination protocols or middleware mechanisms are required to successfully bridge the semantic gap between LRM reasoning outputs and LAM execution inputs?
- Basis in paper: [inferred] The paper assumes a middleware layer can "maintain a consistent representation" and route information, but does not define the specific protocols to translate high-level reasoning into low-level API calls (Challenge 2).
- Why unresolved: The framework describes the "what" (coordination) but not the "how" (implementation details) of translating semantic intent into grounded action.
- What evidence would resolve it: Design of a data model and API schema that successfully maps abstract composition plans to concrete service instances without data loss or ambiguity.

### Open Question 4
- Question: To what extent does the integrated LRM-LAM approach improve reliability and real-time adaptation in dynamic environments compared to standard LLM-based agents?
- Basis in paper: [explicit] The authors call for an "investigation of the effectiveness and reliability of prototypes... dynamically adapting service compositions in real-time."
- Why unresolved: Challenge 3 (Service execution and adaptation) is noted as largely unexplored in current literature, and the proposed framework has not yet been tested against dynamic service failures.
- What evidence would resolve it: Comparative studies showing failure recovery rates and execution latency between the proposed LRM-LAM architecture and single-model approaches in volatile environments.

## Limitations
- Architectural framework remains conceptual without empirical validation on real service composition tasks
- No quantitative evaluation metrics or baselines established for LRM-LAM integration performance
- Critical implementation details for coordination layer state management and feedback routing are underspecified
- Training data generation pipeline lacks validation that generated data improves model performance

## Confidence
- High Confidence: The problem framing and need for specialized reasoning/action model integration is well-established in literature
- Medium Confidence: The three-layer architectural approach logically addresses identified limitations of general-purpose models
- Low Confidence: Claims about iterative refinement and training data generation effectiveness remain purely theoretical

## Next Checks
1. Implement and benchmark LRM-only composition planning on realistic service catalogs to establish baseline performance
2. Develop and test LAM execution layer with mock APIs including failure injection scenarios to validate recovery mechanisms
3. Conduct end-to-end experiments measuring whether execution feedback successfully triggers improved compositions over multiple iterations