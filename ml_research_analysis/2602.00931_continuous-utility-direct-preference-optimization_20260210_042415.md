---
ver: rpa2
title: Continuous-Utility Direct Preference Optimization
arxiv_id: '2602.00931'
source_url: https://arxiv.org/abs/2602.00931
tags:
- strategy
- problem
- phase
- preference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CU-DPO replaces binary preference supervision with continuous
  utility scores to capture fine-grained reasoning quality, enabling large language
  models to adaptively select and execute optimal mathematical reasoning strategies.
  The method uses a two-phase training pipeline: Phase 1 optimizes strategy selection
  through best-vs-all comparisons across diverse cognitive approaches, improving selection
  accuracy from 35-46% to 68-78% across seven base models.'
---

# Continuous-Utility Direct Preference Optimization

## Quick Facts
- arXiv ID: 2602.00931
- Source URL: https://arxiv.org/abs/2602.00931
- Reference count: 40
- Primary result: Continuous utility scores improve mathematical reasoning strategy selection accuracy from 35-46% to 68-78%

## Executive Summary
CU-DPO introduces a novel approach to mathematical reasoning in large language models by replacing binary preference supervision with continuous utility scores that capture fine-grained reasoning quality. The method employs a two-phase training pipeline that first optimizes strategy selection across diverse cognitive approaches, then refines execution quality using margin-stratified pairs. This approach yields substantial improvements in both in-distribution and out-of-distribution reasoning performance, with theoretical guarantees of improved sample complexity over binary preference methods.

## Method Summary
CU-DPO uses continuous utility scores to evaluate reasoning strategies rather than binary preferences. The method implements a two-phase training pipeline: Phase 1 optimizes strategy selection through best-vs-all comparisons across diverse cognitive approaches, improving selection accuracy significantly. Phase 2 refines execution quality using margin-stratified pairs. The approach demonstrates that reasoning can be decomposed into strategy choice and execution refinement, with theoretical analysis proving a Θ(K log K) sample complexity improvement over binary preferences and showing DPO converges to the entropy-regularized utility-maximizing policy.

## Key Results
- Strategy selection accuracy improves from 35-46% to 68-78% across seven base models
- Downstream reasoning gains reach up to +6.6 points on in-distribution benchmarks
- Effective transfer to out-of-distribution tasks demonstrated
- Θ(K log K) sample complexity improvement over binary preferences proven theoretically

## Why This Works (Mechanism)
The core mechanism leverages continuous utility scores to capture nuanced quality differences between reasoning strategies rather than forcing binary choices. This allows the model to learn fine-grained distinctions in strategy effectiveness and adaptively select optimal approaches. The two-phase training pipeline enables focused optimization: first learning to distinguish between strategies (selection), then refining the quality of reasoning execution. This decomposition mirrors human problem-solving approaches and allows for more targeted optimization of both strategy choice and implementation quality.

## Foundational Learning

**Utility-based optimization**: Why needed - enables capturing nuanced quality differences between strategies. Quick check - verify utility scores correlate with actual reasoning performance across diverse problems.

**Two-phase training pipeline**: Why needed - separates strategy selection from execution refinement for more effective learning. Quick check - confirm that phase 1 improvements transfer to phase 2 and final performance.

**Entropy regularization**: Why needed - prevents premature convergence to suboptimal strategies and maintains exploration. Quick check - verify that entropy regularization improves final performance compared to unregularized optimization.

## Architecture Onboarding

Component map: Input problems -> Strategy selection module -> Multiple reasoning strategy generators -> Utility scoring system -> Best strategy selection -> Execution refinement module -> Final answer

Critical path: Problem input → Strategy selection → Strategy execution → Utility scoring → Model update

Design tradeoffs: Continuous utilities vs binary preferences (complexity vs expressiveness), two-phase vs single-phase training (simplicity vs effectiveness), entropy regularization vs pure optimization (stability vs speed).

Failure signatures: Poor strategy selection accuracy, failure to transfer across domains, overfitting to training utility distributions, degradation in out-of-distribution performance.

First experiments:
1. Compare strategy selection accuracy between continuous utility and binary preference baselines on held-out validation sets
2. Evaluate downstream reasoning performance on in-distribution vs out-of-distribution benchmarks
3. Test sensitivity to utility scoring noise by introducing controlled perturbations in utility values

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on mathematical reasoning may limit generalizability to other domains
- Two-phase training pipeline adds significant complexity to the alignment process
- Limited analysis of how utility scoring biases might propagate through optimization

## Confidence

**Theoretical analysis of sample complexity**: High confidence
**Phase 1 strategy selection improvements**: Medium confidence
**Phase 2 execution refinement results**: Medium confidence
**Downstream reasoning gains on out-of-distribution tasks**: Medium confidence
**Generalizability to non-mathematical reasoning domains**: Low confidence

## Next Checks

1. Test CU-DPO on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to evaluate domain generalizability and identify potential limitations in strategy selection for different reasoning types.

2. Conduct ablation studies comparing the two-phase training pipeline against single-phase approaches with different utility aggregation strategies to determine whether the complexity is justified by performance gains.

3. Implement robustness tests using noisy or biased utility scores to evaluate the method's sensitivity to scoring errors and identify potential failure modes in real-world deployment scenarios.