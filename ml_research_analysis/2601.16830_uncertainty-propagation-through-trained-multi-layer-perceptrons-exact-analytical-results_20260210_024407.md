---
ver: rpa2
title: 'Uncertainty propagation through trained multi-layer perceptrons: Exact analytical
  results'
arxiv_id: '2601.16830'
source_url: https://arxiv.org/abs/2601.16830
tags:
- analytical
- uncertainty
- expressions
- gaussian
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides exact analytical expressions for uncertainty
  propagation through single-hidden-layer MLPs with ReLU activation functions when
  inputs follow a multivariate Gaussian distribution. The key contributions are: Closed-form
  analytical expressions for the mean and variance of the output without resorting
  to infinite series expansions, unlike previous work.'
---

# Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results

## Quick Facts
- arXiv ID: 2601.16830
- Source URL: https://arxiv.org/abs/2601.16830
- Reference count: 20
- This paper provides exact analytical expressions for uncertainty propagation through single-hidden-layer MLPs with ReLU activation functions when inputs follow a multivariate Gaussian distribution.

## Executive Summary
This paper addresses the problem of propagating multivariate Gaussian input uncertainty through a trained single-hidden-layer MLP with ReLU activations. Unlike previous approaches that rely on infinite series expansions, the authors derive closed-form analytical expressions for the mean and variance of the output. These expressions are functions of standard 1D and 2D Gaussian integrals, providing exact characterization of the output distribution. The approach was validated on a realistic test problem involving state-of-health prediction for lithium-ion cells from Electrical Impedance Spectroscopy data, demonstrating consistency with Monte Carlo sampling approaches.

## Method Summary
The method involves propagating input uncertainty through the neural network using exact analytical expressions. Given input mean λ and covariance Λ, the approach computes the output mean and variance by first propagating through the first layer to obtain μ and Σ, then computing hidden layer mean γ and covariance Γ using rectified moment theorems. The key innovation is Theorem 2, which provides analytical expressions for bivariate rectified moments using bivariate normal CDFs, and Appendix B (Theorem 3) which handles the corner case of near-singular correlations. The validation compares analytical results against Monte Carlo sampling with 10^6 samples, targeting RMSE < 10^-5.

## Key Results
- Closed-form analytical expressions for output mean and variance without infinite series expansions
- Validation shows RMSE < 10^-5 for both mean and variance estimates when compared to Monte Carlo sampling with 10^6 samples
- The error decreases proportionally to 1/√n as expected for Monte Carlo methods
- Exact analytical characterization is feasible for single-hidden-layer MLPs with ReLU activations

## Why This Works (Mechanism)
The approach works because ReLU activation creates a piecewise linear transformation that can be characterized using rectified moments of Gaussian distributions. By expressing the output statistics in terms of standard Gaussian integrals (1D and 2D), the authors obtain exact closed-form solutions rather than approximations. The key insight is that the non-linearity introduced by ReLU can be handled analytically through the properties of Gaussian distributions and their rectified moments.

## Foundational Learning
- **Rectified Gaussian Moments**: Understanding how to compute expectations of max(X,0) where X follows a Gaussian distribution. Why needed: Core to computing hidden layer statistics after ReLU activation. Quick check: Can you derive E[max(X,0)] for X~N(μ,σ²)?
- **Bivariate Normal CDF**: The function Φ₂(x,y;ρ) that gives the probability that two correlated standard normals fall below thresholds x and y. Why needed: Central to computing covariance between hidden units. Quick check: Does your implementation handle the case where ρ→±1?
- **Uncertainty Propagation in Linear Systems**: How input uncertainty propagates through linear transformations. Why needed: Forms the basis for the first layer propagation. Quick check: Can you verify that the linear transformation of a Gaussian remains Gaussian?
- **ReLU Non-linearity**: The max(x,0) activation function and its properties. Why needed: The source of non-linearity that makes analytical propagation challenging. Quick check: Sketch the output distribution when a Gaussian passes through ReLU.
- **Gaussian Integration**: Standard results for integrating functions against Gaussian densities. Why needed: The analytical expressions rely on computing various Gaussian integrals. Quick check: Can you evaluate ∫max(x,0)φ(x)dx where φ is the standard normal PDF?
- **Moment Matching**: Computing statistical moments from distribution parameters. Why needed: The goal is to characterize the output distribution through its moments. Quick check: Given a distribution's mean and variance, can you reconstruct it approximately?

## Architecture Onboarding

**Component Map**: Input Gaussian (λ,Λ) -> Linear Layer (μ,Σ) -> Hidden Layer (γ,Γ) via ReLU -> Output Layer (mean,variance)

**Critical Path**: The analytical computation flows through the three layers, with the critical computation being the evaluation of Theorem 2 for the hidden layer covariance, which requires accurate computation of bivariate normal CDFs.

**Design Tradeoffs**: The exact analytical approach trades computational complexity of evaluating special functions (bivariate normal CDF) against the statistical accuracy and reproducibility benefits over Monte Carlo sampling. This is particularly valuable when high precision is required or when computational resources for large Monte Carlo simulations are limited.

**Failure Signatures**: 
- Division-by-zero errors when input correlations approach ±1 (handled by Theorem 3)
- Numerical precision issues in bivariate normal CDF evaluation when correlations are extreme
- Incorrect moment calculations leading to negative variances (should be prevented by the mathematics but can occur due to numerical errors)

**First 3 Experiments**:
1. Implement Theorem 1 for univariate rectified moments and verify against numerical integration for various input means and variances
2. Implement Theorem 2 for bivariate rectified moments and test the numerical stability as correlation approaches ±1
3. Create a simple random MLP and random Gaussian input, then verify that the analytical output mean/variance matches Monte Carlo simulation with 10^6 samples

## Open Questions the Paper Calls Out

**Open Question 1**: How does the computational complexity of these closed-form analytical expressions compare to sampling-based approaches (e.g., Monte Carlo) as the input dimension and hidden layer width scale? The paper notes this comparison is left as future work, focusing instead on derivation and validation of accuracy.

**Open Question 2**: Can exact analytical expressions be derived for propagating uncertainty through MLPs with more than one hidden layer without resorting to Gaussian approximations at intermediate layers? The authors note this is a more challenging task since hidden layer outputs become non-Gaussian after the first ReLU activation.

**Open Question 3**: Is it possible to derive exact closed-form expressions for other common activation functions (e.g., GELU, Sigmoid, Tanh) or non-Gaussian input distributions? The authors suggest this would likely require different mathematical techniques since the current proofs rely heavily on ReLU properties and standard Gaussian integrals.

**Open Question 4**: How robust is the assumption of multivariate Gaussian inputs when applied to real-world datasets that may exhibit skewness or heavy tails? The paper acknowledges that real-world testing scenarios often treat the ML model as a black box where the input distribution might not strictly be Gaussian.

## Limitations
- The methodology is restricted to single-hidden-layer ReLU networks and Gaussian input uncertainty
- Exact numerical reproduction is limited by missing dataset details and trained model weights
- Numerical precision in evaluating bivariate normal CDF becomes challenging when input correlations are very high

## Confidence

**High Confidence**: The analytical derivation of mean and variance expressions for the output distribution is mathematically rigorous and internally consistent.

**Medium Confidence**: Numerical validation via Monte Carlo sampling is sound, but exact reproduction of the reported results is blocked by missing data/model details.

**Medium Confidence**: The stated RMSE < 10⁻⁵ accuracy is plausible given the Monte Carlo sample size, but depends critically on the precision of special function evaluations.

## Next Checks
1. Implement and test the corner-case handling (Theorem 3 in Appendix B) for near-singular correlation matrices to ensure numerical stability
2. Cross-validate the analytical output mean and variance against a Monte Carlo simulation with 10⁶ samples for multiple random MLPs and input distributions
3. Benchmark the precision of the bivariate normal CDF implementation (e.g., scipy.special.bvn) to confirm it meets the ~10⁻⁵ accuracy requirement