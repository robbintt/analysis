---
ver: rpa2
title: 'FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization
  for Financial Applications'
arxiv_id: '2511.14865'
source_url: https://arxiv.org/abs/2511.14865
tags:
- data
- user
- recommendation
- sequential
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FinTRec, a transformer-based framework for
  unified contextual ads targeting and personalization in financial services. The
  method addresses challenges of long-range user interactions across digital and physical
  channels, and the need for coordinated models across interrelated products.
---

# FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications

## Quick Facts
- arXiv ID: 2511.14865
- Source URL: https://arxiv.org/abs/2511.14865
- Reference count: 40
- Primary result: 55.38% log loss reduction for conversion rate prediction over tree-based baseline

## Executive Summary
FinTRec introduces a unified transformer-based framework for contextual ads targeting and personalization in financial services, addressing the challenge of modeling long-range user interactions across multiple digital and physical channels. The system leverages a proprietary foundational model to aggregate heterogeneous user context, then employs LoRA fine-tuning for efficient product adaptation while enabling cross-product signal sharing. Experimental results demonstrate significant improvements in both click-through and conversion rate prediction, with offline simulations and online A/B tests validating the approach's effectiveness in reducing technical debt and improving user engagement.

## Method Summary
FinTRec employs a dual-transformer architecture with an encoder-only model for conversion rate prediction and a decoder-only model for click-through rate prediction, both incorporating sequential user interactions, static features, dynamic context, and 768-dim foundational model embeddings. The system uses causal masking by timestamp for the CTR model and bidirectional processing for the CVR model, with LoRA fine-tuning enabling efficient adaptation across interrelated financial products while sharing signals. Training employs AdamW optimization with specific hyperparameters, and the final ranking combines urgency, CTR, and CVR predictions weighted by product value.

## Key Results
- 55.38% reduction in log loss for conversion rate prediction compared to production tree-based baseline
- Product adaptation achieves up to 26.85% improvement in recall@1 over product-specific baselines
- LoRA fine-tuning achieves comparable gains to full fine-tuning using less than 5% of model parameters

## Why This Works (Mechanism)
The transformer architecture captures long-range dependencies in user behavior across both digital and physical touchpoints, while the unified framework enables coordinated learning across interrelated financial products. LoRA fine-tuning provides an efficient mechanism for product adaptation that reduces training costs and technical debt while maintaining cross-product signal sharing. The integration of foundational model embeddings captures deep historical context that enhances prediction accuracy beyond surface-level interactions.

## Foundational Learning
- **Transformer Attention Mechanisms**: Self-attention allows the model to weigh the importance of different historical interactions, essential for capturing user intent patterns across time. Quick check: Verify attention weights highlight relevant temporal sequences.
- **Low-Rank Adaptation (LoRA)**: Enables efficient fine-tuning by decomposing weight updates into low-rank matrices, critical for adapting to multiple products without full retraining. Quick check: Confirm parameter reduction while maintaining performance.
- **Causal vs. Bidirectional Processing**: Decoder-only causal masking preserves temporal order for CTR prediction, while encoder-only bidirectional processing captures complete context for CVR prediction. Quick check: Validate temporal consistency in predictions.
- **Foundational Model Embeddings**: 768-dim FM provides deep transaction history context that surface features cannot capture. Quick check: Measure performance degradation when removing FM embeddings.
- **Multi-task Learning**: Joint training on CTR and CVR objectives enables complementary signal sharing. Quick check: Compare single-task vs. multi-task performance.

## Architecture Onboarding
- **Component Map**: User Context (S_u, F_s, F_d, F_fm) → Dual Transformer Models (CTR Decoder, CVR Encoder) → LoRA Adapters → Product-Specific Predictions
- **Critical Path**: Tokenization → Embedding Fusion → Transformer Processing → Product Adaptation → Final Ranking
- **Design Tradeoffs**: Encoder-only for CVR provides bidirectional context at the cost of temporal ordering; decoder-only for CTR preserves causality but limits context access; LoRA balances adaptation efficiency with parameter economy.
- **Failure Signatures**: High log loss indicates inadequate contextual modeling or poor FM integration; poor recall@1 suggests ineffective product adaptation or insufficient training data.
- **First Experiments**: 1) Train base models without LoRA to establish baseline performance; 2) Implement LoRA with varying ranks to identify optimal configuration; 3) Conduct ablation study removing FM embeddings to quantify their contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- The proprietary foundational model embedding component creates an insurmountable barrier to exact reproduction of claimed performance gains
- LoRA hyperparameters (rank parameters and layer selection) are underspecified, limiting reproducibility of parameter efficiency claims
- The absolute performance metrics depend heavily on undisclosed implementation details of tokenization and foundational model training

## Confidence
- **High Confidence**: Architectural framework and methodological soundness are clearly specified
- **Medium Confidence**: Product adaptation improvements are convincing but constrained by proprietary dependencies
- **Low Confidence**: Absolute performance numbers cannot be independently verified without foundational model access

## Next Checks
1. Implement proxy embedding model using available transaction history data and measure performance degradation
2. Systematically vary LoRA rank parameters and layer selection to identify optimal configurations
3. Conduct ablation studies removing cross-product signals to quantify benefits of unified training