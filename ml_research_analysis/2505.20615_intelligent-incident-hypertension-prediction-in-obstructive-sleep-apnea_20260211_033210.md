---
ver: rpa2
title: Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea
arxiv_id: '2505.20615'
source_url: https://arxiv.org/abs/2505.20615
tags:
- sleep
- hypertension
- feature
- obstructive
- apnea
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a deep learning approach for predicting incident
  hypertension in obstructive sleep apnea (OSA) patients using polysomnography signals.
  The method extracts features from multiple sleep signals, transforms them into 2D
  representations, and applies Discrete Cosine Transform (DCT)-based transfer learning
  using EfficientNet architectures.
---

# Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea

## Quick Facts
- **arXiv ID**: 2505.20615
- **Source URL**: https://arxiv.org/abs/2505.20615
- **Reference count**: 36
- **Primary result**: 72.88% AUC achieved with 60-minute windows and DCT layer at EfficientNet block 6

## Executive Summary
This study introduces a deep learning approach for predicting incident hypertension in obstructive sleep apnea (OSA) patients using polysomnography signals. The method extracts features from multiple sleep signals, transforms them into 2D representations, and applies Discrete Cosine Transform (DCT)-based transfer learning using EfficientNet architectures. A DCT layer with soft thresholding is used to preserve spectral information and improve robustness. Models were evaluated using 10-fold cross-validation across various time window lengths (9–60 minutes).

## Method Summary
The approach processes polysomnography signals (EEG, ECG, respiratory, SpO2, etc.) by first preprocessing each channel (artifact removal, bandpass filtering), then segmenting into windows. For each window, features are extracted including respiratory event counts/durations, statistical descriptors (mean, std, skewness, kurtosis), and HRV metrics (SDNN, RMSSD, pNN50). These window-level features are concatenated into 2D arrays (features × windows) forming pseudo-images. A truncated EfficientNet-B0 (ImageNet pretrained) is used with a DCT-2D layer and soft thresholding inserted at block 6. The model is trained with 10-fold cross-validation using binary cross-entropy loss.

## Key Results
- Best performance: 69.86% accuracy and 72.88% AUC achieved with 60-minute windows
- DCT integration at deeper EfficientNet truncation depths (blocks 5–6) outperformed shallower placements
- Model outperformed existing approaches: cSPPSG (71% AUC) and AHI-based methods (67% AUC)
- Performance degraded with shorter window lengths (9–30 minutes)

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Feature Preservation via DCT Layer
Replacing standard convolution with a Discrete Cosine Transform (DCT)-based block preserves spectral information that would otherwise be lost. Traditional CNNs apply ReLU activations, which discard negative values. In the frequency domain, this eliminates potentially informative negative-frequency coefficients. The DCT layer uses soft thresholding instead: `SoftThreshold(x, τ) = sign(x) × max(|x| - τ, 0)`, pruning noisy low-amplitude coefficients while retaining both positive and negative high-amplitude components. The DCT's orthogonality also decorrelates features.

### Mechanism 2: Transfer Learning from 2D Image Architectures
Pre-trained 2D CNNs (EfficientNet, MobileNet, ResNet) can be effectively adapted to polysomnography-derived "pseudo-images" despite originating from natural image domains. Window-level features (HRV metrics, respiratory event counts, statistical descriptors) are arranged into 2D arrays (features × windows), creating pseudo-images. Pre-trained convolutional filters detect cross-feature and cross-window patterns. The DCT block insertion at deeper truncation depths (blocks 5–6) allows frequency-domain refinement of increasingly abstract representations.

### Mechanism 3: Multi-Signal Temporal Integration
Combining all polysomnography signals (EEG, ECG, respiratory, SpO2, etc.) preserves temporal correlations across physiological systems that single-signal approaches miss. OSA causes intermittent hypoxia, sleep fragmentation, and sympathetic activation—processes spanning respiratory, cardiac, and neurological systems. By extracting features from all signals within synchronized time windows (9–60 minutes), the model captures cross-system perturbations. Longer windows (60 min) outperform shorter ones, suggesting aggregated temporal patterns are predictive.

## Foundational Learning

- **Concept: Discrete Cosine Transform (DCT)**
  - Why needed here: Understanding how DCT differs from DFT (real-valued vs. complex-valued, energy compaction properties) explains why it enables soft thresholding without complex number complications.
  - Quick check question: Why would ReLU applied to DCT coefficients destroy more information than ReLU applied to time-domain signals?

- **Concept: Soft Thresholding in Signal Processing**
  - Why needed here: Soft thresholding is the core nonlinearity replacing ReLU in the DCT domain. Understanding its denoising origins clarifies why it preserves both positive and negative coefficients.
  - Quick check question: What happens to a coefficient with magnitude 0.5 when threshold τ = 1.0 is applied? What if τ = 0.3?

- **Concept: Transfer Learning with Truncation Depths**
  - Why needed here: The paper strategically inserts DCT blocks at different EfficientNet depths (blocks 3–6). Understanding feature hierarchy (early: edges/textures; late: semantic concepts) explains why deeper insertions worked better.
  - Quick check question: Why might frequency-domain transformation be more beneficial for late-stage abstract features than early-stage low-level features?

## Architecture Onboarding

- **Component map**: Raw PSG Signals -> Preprocessing -> Windowed Feature Extraction -> 2D Pseudo-Image Construction -> Truncated EfficientNet-B0 with DCT-2D block at depth 6 -> Classification Head

- **Critical path**: The DCT block placement depth (DCT@5 vs DCT@6) directly determines AUC performance (70.73% → 72.88%). Window length is secondary but impactful (60 min optimal).

- **Design tradeoffs**:
  - Window length vs. granularity: Longer windows improve performance but reduce temporal resolution for pinpointing risk periods
  - DCT depth vs. computational cost: Deeper DCT insertion requires more feature map transformations but yields better AUC
  - Pre-trained vs. scratch training: Transfer learning helps with limited medical data but introduces domain mismatch assumptions

- **Failure signatures**:
  - AUC drops below 68%: Check DCT block placement—may be too shallow (blocks 1–3)
  - High accuracy but low AUC (~60%): Model may be overfitting to majority class; examine class balance
  - Performance collapses with shorter windows: Feature extraction may be window-size dependent; verify HRV metric computation handles small N gracefully

- **First 3 experiments**:
  1. Ablate DCT block entirely: Train EfficientNet-B0 without DCT layer on same data to quantify soft thresholding contribution
  2. Vary soft threshold τ: Sweep τ ∈ {0.01, 0.1, 0.5, 1.0} to find optimal noise-filtering level
  3. Single-signal vs. multi-signal comparison: Train identical architecture using only SpO2 features vs. full PSG to validate multi-signal hypothesis

## Open Questions the Paper Calls Out
- Will the DCT-enhanced model maintain its predictive performance when applied to external, independent sleep cohorts?
- How does the specific tuning of the soft thresholding parameter (τ) influence the retention of spectral features and overall accuracy?
- What is the marginal predictive gain of the novel DCT-PSG features compared to the static clinical variables (age, BMI, BP)?

## Limitations
- Limited external validation with no independent cohort testing
- Heavy reliance on manual feature engineering rather than end-to-end learning
- Critical hyperparameters (learning rate, soft threshold τ, window overlap) not specified

## Confidence
- DCT-based frequency preservation improves AUC: Medium confidence
- Transfer learning from ImageNet works for PSG pseudo-images: Medium confidence  
- Multi-signal integration outperforms single-signal approaches: Low confidence

## Next Checks
1. Test the trained model on an external OSA cohort (e.g., MrOS Sleep Study) to assess real-world performance
2. Train identical architecture without the DCT block to quantify the exact contribution of frequency-domain feature preservation
3. Train separate models using only the most predictive single signal (likely SpO2) versus the full PSG to empirically validate the multi-signal hypothesis with matched experimental conditions