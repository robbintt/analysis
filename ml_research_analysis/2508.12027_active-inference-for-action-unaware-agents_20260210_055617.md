---
ver: rpa2
title: Active inference for action-unaware agents
arxiv_id: '2508.12027'
source_url: https://arxiv.org/abs/2508.12027
tags:
- free
- energy
- inference
- agents
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two implementations of active inference agents
  in discrete grid-world environments: "action-aware" agents that know their past
  actions and "action-unaware" agents that must infer them. The study investigates
  how this architectural difference affects learning and decision-making.'
---

# Active inference for action-unaware agents

## Quick Facts
- **arXiv ID**: 2508.12027
- **Source URL**: https://arxiv.org/abs/2508.12027
- **Reference count**: 40
- **Primary result**: Action-unaware agents can match performance of action-aware agents in grid-world navigation despite higher computational cost

## Executive Summary
This paper investigates two implementations of active inference agents in discrete grid-world environments: "action-aware" agents that know their past actions and "action-unaware" agents that must infer them. Both use variational free energy minimization but differ in how they compute policy-conditioned free energies. Action-aware agents update one collection of variational distributions for past states, while action-unaware agents must update distributions for all possible policies at each time step. The experiments demonstrate that action-unaware agents can achieve similar performance to action-aware agents in relatively simple environments, suggesting explicit knowledge of past actions may not be essential for effective active inference-based decision-making.

## Method Summary
The study compares action-aware and action-unaware active inference agents in T-maze and 3×3 grid-world environments. Both agent types use variational free energy minimization for perception, planning, and learning, but differ in their generative models. Action-aware agents have access to an efference copy of past actions, while action-unaware agents must infer past actions from observation sequences alone. The agents learn transition dynamics (B-matrix) from scratch with Dirichlet priors, while the emission matrix (A) is known and diagonal. Policy selection uses expected free energy balancing exploration and exploitation. Action selection uses Bayesian model averaging across all policies. Learning updates occur at episode end based on accumulated observations.

## Key Results
- Action-unaware agents successfully learned transition dynamics and reached goal states with comparable success rates to action-aware agents
- Action-aware agents showed slightly better learning speed and lower free energy values in T-maze experiments
- In 3×3 grid world, action-unaware agents discovered multiple optimal policies but still achieved goal-reaching performance comparable to action-aware agents
- Action-unaware agents experienced higher computational costs due to O(nm) scaling versus O(n) for action-aware agents

## Why This Works (Mechanism)

### Mechanism 1: Policy-Conditioned State Estimation (Smoothing)
Agents can infer past actions and states solely from observation sequences by evaluating which action sequences best explain sensory evidence. In action-unaware schemes, each policy is a complete trajectory of past, present, and future actions. The agent computes policy-conditioned free energies for every possible policy, identifying which policy trajectory is most probable given observed data.

### Mechanism 2: Balancing Exploration via Expected Free Energy
Agents achieve comparable performance by using Expected Free Energy (EFE) to drive both goal-seeking and uncertainty reduction. EFE contains intrinsic "novelty" terms that assign value to visiting states with uncertain transition dynamics, compelling the agent to test unknown transitions and facilitate learning even without knowing which specific actions cause changes.

### Mechanism 3: Bayesian Model Averaging for Action Selection
Selecting the most probable action across all policies stabilizes behavior in action-unaware agents. Instead of executing a single most probable policy, the system computes a weighted average of actions recommended by all policies, with weights equal to the probability of each policy itself. This acts as a buffer against uncertainty in individual policy posterior estimates.

## Foundational Learning

- **Concept: Variational Free Energy (VFE)**
  - **Why needed here**: This is the objective function the agent minimizes to perform perception, quantifying the divergence between the agent's internal model and sensory reality
  - **Quick check question**: Can you explain how minimizing VFE mathematically relates to maximizing the likelihood of the agent's observations?

- **Concept: The Generative Model (POMDP)**
  - **Why needed here**: The paper defines the agent not by the environment itself but by its internal approximation. You must distinguish between the A matrix (emission/observation) and B tensor (transitions) to understand what is being learned
  - **Quick check question**: If the A matrix is fixed but the B tensor is randomized, what specific type of uncertainty must the agent resolve to succeed?

- **Concept: Efference Copy vs. Corollary Discharge**
  - **Why needed here**: The paper's core distinction is biological. Action-aware agents simulate a "corollary discharge" (knowing the motor command), while action-unaware agents rely on "reafference" (sensory feedback) to infer movement
  - **Quick check question**: In the action-unaware architecture, does the generative model contain the "ground truth" action variable as a direct input, or is it inferred?

## Architecture Onboarding

- **Component map**: Generative Model -> Variational Posterior -> Inference Engine -> Actuator Interface
- **Critical path**: The cycle follows Algorithm S1 (Action-unaware): Perception (update state beliefs for every policy), Planning (compute EFE and update policy probabilities), Action (select action via Bayesian model average), Learning (update Dirichlet parameters at episode end)
- **Design tradeoffs**: Action-unaware agents scale as O(nm) (states × policies) vs O(n) for action-aware, creating a significant bottleneck for long horizons. Action-unaware is argued to be more biologically plausible (no internal copy of motor commands), suitable for modeling simple organisms or low-level reflex arcs
- **Failure signatures**: Runaway Free Energy if transition model fails to converge, leading to random policy selection. Policy Collapse where sub-optimal policies gain higher probability than optimal ones due to misleading EFE
- **First 3 experiments**:
  1. T-Maze Navigation (Action-Unaware): Reproduce basic 4-step navigation to verify agent can learn B matrix from scratch without action labels
  2. T-Maze Navigation (Action-Aware): Run same task with action-aware flag and compare convergence speed against action-unaware agent
  3. Grid World Scaling: Implement 5-step grid world to observe computational cost increase and monitor policy-conditioned free energy plots

## Open Questions the Paper Calls Out

- **Open Question 1**: Can approximation techniques like weight-based sampling enable action-unaware agents to scale to high-dimensional action spaces?
  - **Basis**: The authors speculate that mechanisms such as weight-based sampling of action sequences may provide an affordable implementation in high-dimensional action-sequence spaces
  - **Why unresolved**: The combinatorial explosion of evaluating all possible past and future action sequences creates a computational bottleneck currently limiting action-unaware agents to simple grid-world environments
  - **What evidence would resolve it**: Successful deployment of action-unaware agents using sampling approximations in complex environments with high-dimensional action spaces

- **Open Question 2**: Why does the increased probability of sub-optimal policies in later learning episodes not cause a substantial deterioration in task performance?
  - **Basis**: The authors leave investigations into the reasons why agents' performance does not deteriorate more substantially despite the increased probability of sub-optimal policies for future work
  - **Why unresolved**: Experiments show a counter-intuitive divergence where policy probabilities favor sub-optimal paths, yet agents still reach goals, suggesting a disconnect between probability mass and behavioral outcome
  - **What evidence would resolve it**: Analysis of relative contributions of policy-conditioned free energy versus expected free energy during action selection when sub-optimal policies have higher probability mass

- **Open Question 3**: Are action-unaware agents structurally better suited for fixed-horizon tasks while action-aware agents are better for indefinite-horizon tasks?
  - **Basis**: The discussion posits that action-unaware agents may be more tailored for finite-horizon tasks whereas action-aware agents appear to be more congenial to indefinite-duration tasks
  - **Why unresolved**: This distinction is inferred from architectural handling of policies but was not empirically tested against different temporal horizons in experiments
  - **What evidence would resolve it**: Comparative simulations of both agent types in matching environments manipulated to have fixed versus variable/infinite episode durations

## Limitations

- **Computational scaling**: Action-unaware agents require O(nm) updates versus O(n) for action-aware, making the approach impractical for environments with large state or action spaces
- **Biological plausibility claim**: The assertion that action-unaware agents are more biologically plausible due to lacking efference copies is theoretical without empirical validation against neural or behavioral data
- **Limited environmental complexity**: The paper only demonstrates results in small grid-worlds with limited horizons, leaving open questions about performance in more complex, stochastic environments

## Confidence

- **High confidence**: Action-unaware agents can achieve comparable goal-reaching performance to action-aware agents in small, deterministic environments
- **Medium confidence**: The computational disadvantage of action-unaware agents (requiring O(nm) updates) is correctly characterized and significant for scaling
- **Low confidence**: The claim that action-unaware agents are more "biologically plausible" due to lacking efference copies is theoretical without empirical validation

## Next Checks

1. Test scaling limits by implementing the 5-step grid world with 9 states (256 policies) and measure computation time per timestep versus action-aware baseline
2. Evaluate performance under increased stochasticity by adding transition noise to the grid world and measuring success rate degradation for both agent types
3. Implement a corridor-following task requiring sequential commitment where averaging across diverse policies leads to invalid action sequences, testing the break condition for Bayesian model averaging