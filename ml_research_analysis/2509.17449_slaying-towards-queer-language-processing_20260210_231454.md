---
ver: rpa2
title: 'SLAyiNG: Towards Queer Language Processing'
arxiv_id: '2509.17449'
source_url: https://arxiv.org/abs/2509.17449
tags:
- slang
- queer
- terms
- term
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first annotated dataset for queer slang,
  SLAyiNG, to address the lack of representation of queer language in NLP models.
  The dataset contains 695 slang terms and their definitions, collected from various
  sources, and includes real-world usage examples from subtitles, social media posts,
  and podcasts.
---

# SLAyiNG: Towards Queer Language Processing

## Quick Facts
- arXiv ID: 2509.17449
- Source URL: https://arxiv.org/abs/2509.17449
- Authors: Leonor Veloso; Lea Hirlimann; Philipp Wicke; Hinrich Schütze
- Reference count: 28
- Primary result: First annotated dataset for queer slang (695 terms, 197,958 examples); o3-mini achieves 0.746 Krippendorff's alpha for pre-filtering.

## Executive Summary
This paper introduces SLAyiNG, the first annotated dataset for queer slang, addressing the lack of representation of queer language in NLP models. The dataset contains 695 slang terms and their definitions, collected from various sources, with real-world usage examples from subtitles, social media posts, and podcasts. The authors propose a pre-filtering and annotation pipeline involving both human annotators and AI models to create a comprehensive and fair dataset. Preliminary results show that state-of-the-art reasoning models like o3-mini can serve as annotation tools for sense disambiguation, achieving an average Krippendorff's alpha of 0.746 with human annotators.

## Method Summary
The authors present a 5-step annotation pipeline for queer slang terms, combining expert and community-driven annotation. They collect 695 slang terms and their definitions from various sources, then scrape real-world usage examples from subtitles, social media posts, and podcasts. The pipeline uses o3-mini for pre-filtering examples based on sense alignment, followed by manual annotation of pre-filtered examples by human experts. The authors plan to recruit community annotators across diverse queer identities to validate a sample of annotations. Preliminary experiments show that o3-mini can serve as a pre-filtering tool, achieving an average Krippendorff's alpha of 0.746 with human annotators.

## Key Results
- SLAyiNG contains 695 queer slang terms with 197,958 real-world usage examples from Reddit, podcasts, and subtitles.
- o3-mini achieves 0.746 Krippendorff's alpha for sense disambiguation, enabling pre-filtering of examples before human annotation.
- 57.48% of terms have high model agreement (α > 0.6 or F1 > 0.8), allowing efficient filtering of false positives.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based pre-filtering reduces human annotation burden for sense disambiguation in specialized slang corpora.
- Mechanism: A high-performing reasoning model (o3-mini) annotates examples for sense alignment; high inter-annotator agreement with humans (Krippendorff's α = 0.746 average) allows the model to filter false positives before manual review.
- Core assumption: The 0.746 α on 750 examples generalizes to the full 197,958-example raw dataset, and model errors are evenly distributed across term types.
- Evidence anchors:
  - [abstract]: "Reaching an average Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models can serve as tools for pre-filtering."
  - [section 5]: "57.48% of terms have a Krippendorff alpha score > 0.6 or an F1 score > 0.8."
  - [corpus]: Related work (SlangDIT, Shona slang dataset) validates LLM-based slang tasks but does not directly test pre-filtering pipelines; corpus evidence for this specific mechanism is weak.
- Break condition: If model agreement drops below α = 0.6 on new term cohorts (e.g., emerging slang or highly polysemous terms), the pre-filtering stage fails and requires full human annotation.

### Mechanism 2
- Claim: Community-driven annotation is necessary for handling reclaimed slurs and detecting harmful content.
- Mechanism: Approximately 15% of terms are slurs or offensive; ingroup usage is often non-harmful, while outgroup usage may be harmful. Automated systems cannot reliably distinguish these contexts without community input.
- Core assumption: Ingroup members can consistently identify harmful vs. reclaimed usage, and crowdsourced annotations will align with community norms.
- Evidence anchors:
  - [abstract]: "The complex and often sensitive nature of queer language data requires expert and community-driven annotation efforts."
  - [section 4.2.2]: "Ingroup usages of slurs are much less likely to be considered harmful."
  - [corpus]: Dorn et al. (2024), cited in the paper, shows gender-queer dialect bias in harmful speech detection; corpus supports the broader claim but not the specific community annotation mechanism.
- Break condition: If community annotators show low agreement on harm labels (α < 0.6), the harm detection task becomes unreliable, and the dataset may require alternative labeling schemes.

### Mechanism 3
- Claim: Embedding-based definition deduplication preserves distinct senses while removing redundancy.
- Mechanism: Term definitions are embedded using all-mpnet-base-v2; if cosine similarity > 0.7, the shorter definition is discarded, retaining more descriptive versions.
- Core assumption: A 0.7 cosine threshold correctly separates redundant definitions from distinct senses, and embedding similarity correlates with semantic redundancy.
- Evidence anchors:
  - [section 4.1.2]: "If two definitions have a similarity score higher than 0.7, we discard the shorter one."
  - [section A.4, Table 2]: Shows examples like "egg" (0.72 similarity, senses kept) vs. "man pussy" (0.66, senses kept but noted as problematic).
  - [corpus]: No direct corpus evidence for this threshold in slang contexts; mechanism relies on internal validation.
- Break condition: If distinct senses are incorrectly merged (e.g., regional or subcultural variations with high similarity), the dataset loses nuance, and downstream tasks may underperform.

## Foundational Learning

- Concept: Sense disambiguation (WSsim task)
  - Why needed here: Queer slang terms often have non-queer senses (e.g., "mother," "read"); annotators must rate definition applicability on a Likert scale to isolate queer usage.
  - Quick check question: Given the sentence "She really read him for filth at the ball," does the definition "To criticize or insult" apply fully (5), partially (3), or not at all (1)?

- Concept: Krippendorff's alpha for ordinal data
  - Why needed here: Measures inter-annotator agreement across three annotators and between humans vs. LLM; handles ordinal scales and missing data better than Cohen's kappa.
  - Quick check question: If two annotators consistently differ by one point on the 5-point scale (e.g., 4 vs. 5), will α be high or low? (High, if consistent.)

- Concept: Ingroup vs. outgroup authorship
  - Why needed here: Determines whether a term's usage is reclaimed (ingroup) or potentially harmful (outgroup); critical for harm detection labels.
  - Quick check question: In "We're serving cunt tonight," is the author likely ingroup or outgroup? What if the sentence is "They're all cunts"? (First: ingroup; second: context-dependent, possibly outgroup.)

## Architecture Onboarding

- Component map:
  Term sources (GSSO, lgbtDB, Chew Glossary, Wiktionary) -> Definition cleaner (all-mpnet-base-v2 embeddings, 0.7 cosine threshold) -> Raw dataset (Reddit, Podscripts, OpenSubtitles) -> Pre-filtering pipeline (o3-mini sense alignment) -> Manual annotation (sense disambiguation, harm detection, ingroup membership) -> Community validation

- Critical path:
  1. Term collection -> definition cleaning -> raw dataset scraping
  2. Pre-filtering with o3-mini -> identify high-agreement terms (57.48%)
  3. Manual annotation of pre-filtered examples -> community validation
  4. Release cleaned dataset (target: 2,000+ examples)

- Design tradeoffs:
  - 0.7 cosine threshold: Balances redundancy removal vs. sense preservation; may over-merge rare senses
  - o3-mini vs. human annotators: Faster but fails on 42.52% of terms (e.g., "anticistamines," "cunt" with syntactic shifts)
  - Reddit-heavy dataset (58%): Rich but platform-specific; may miss spoken/older slang contexts

- Failure signatures:
  - Low α on specific terms (e.g., neologisms, syntactically flexible slurs)
  - Harm detection disagreement > 30% on reclaimed slurs
  - Embedding deduplication collapses distinct subcultural senses

- First 3 experiments:
  1. Validate o3-mini agreement on a held-out set of 100 examples, focusing on low-agreement terms (e.g., "cunt," "anticistamines"); measure α per term
  2. Test alternative embedding models (e.g., domain-specific or multilingual) for definition deduplication; compare sense preservation via manual review
  3. Pilot community annotation on 50 examples with reclaimed slurs; measure inter-annotator α for harm labels and ingroup identification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed 5-step annotation pipeline successfully scale to produce a clean dataset of 2000+ examples with acceptable inter-annotator agreement across all three annotation tasks?
- Basis in paper: [explicit] The authors describe their pipeline as "ongoing" and state they "plan to implement in future work" the full pipeline including community annotator recruitment and iterative annotation checks.
- Why unresolved: Only preliminary experiments on sense disambiguation with 750 examples have been completed; the full pipeline involving harmful content detection, author ingroup membership, and community validation remains untested.
- What evidence would resolve it: Completion of the pipeline with reported Krippendorff's alpha scores across all three tasks, plus final dataset size and quality metrics.

### Open Question 2
- Question: What linguistic or distributional characteristics predict whether state-of-the-art LLMs can reliably perform sense disambiguation on specific queer slang terms?
- Basis in paper: [explicit] The authors observe that o3-mini achieves negative agreement (−0.833) on some terms like "anticistamines" and zero agreement on others like "cunt," hypothesizing this relates to "relatively lower presence... in the models' training corpora."
- What evidence would resolve it: Systematic analysis correlating term frequency in pretraining corpora, term recency, morphological complexity, and polysemy with LLM annotation agreement scores.

### Open Question 3
- Question: Will community annotators from diverse queer identities validate or significantly revise the annotations produced by the expert author annotators and LLM pre-filtering?
- Basis in paper: [explicit] The authors state that "ethical concerns regarding using LLMs as a sole annotator for potentially offensive terms... make expert and community-driven annotation necessary" and plan to recruit annotators "across a wide range of identities."
- What evidence would resolve it: Comparison of agreement rates between community annotators and author annotators, particularly on sentences flagged as potentially offensive or containing reclaimed slurs.

### Open Question 4
- Question: How effectively does the SLAyiNG methodology transfer to queer slang in other languages and cultural contexts?
- Basis in paper: [explicit] The authors list as their "main limitation" that "this work (at its current stage and at its completion) stems from its sole focus on the English language," noting that "queer language and slang are varied across languages and cultures."
- What evidence would resolve it: Application of the same data curation and annotation pipeline to non-English queer slang sources, with comparison of term overlap, annotation difficulty, and LLM performance across languages.

## Limitations
- The pre-filtering pipeline's generalizability remains uncertain, as the 0.746 Krippendorff's alpha was measured on 750 examples and may not hold for the full 197,958-example dataset.
- Community annotation's reliability is untested at scale; low inter-annotator agreement on harm labels (α < 0.6) could compromise the dataset's utility for harm detection.
- The 0.7 cosine threshold for definition deduplication is heuristic and lacks empirical validation in slang contexts, risking the loss of distinct subcultural senses.

## Confidence
- **High**: The dataset's construction (695 terms, 197,958 examples) is methodologically sound, and the need for community-driven annotation is well-supported by prior work on dialect bias in NLP.
- **Medium**: The claim that o3-mini can pre-filter examples is plausible but requires validation on held-out data, particularly for low-agreement terms.
- **Low**: The assertion that embedding-based deduplication reliably preserves distinct senses is speculative without empirical testing.

## Next Checks
1. Measure o3-mini's agreement on a held-out set of 100 examples, focusing on low-agreement terms (e.g., "cunt," "anticistamines"); report Krippendorff's alpha per term.
2. Compare definition deduplication using all-mpnet-base-v2 vs. a domain-specific embedding model; manually review 50 term pairs to assess sense preservation.
3. Pilot community annotation on 50 reclaimed slur examples; measure inter-annotator agreement for harm labels and ingroup identification.