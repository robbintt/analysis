---
ver: rpa2
title: Shoot First, Ask Questions Later? Building Rational Agents that Explore and
  Act Like People
arxiv_id: '2510.20886'
source_url: https://arxiv.org/abs/2510.20886
tags:
- questions
- board
- answer
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for evaluating and improving language
  models' (LMs) ability to act as rational information-seeking agents. The authors
  introduce Collaborative Battleship, a dialogue-based game where a partially-informed
  Captain must balance exploration (asking questions) and action (taking shots) to
  find hidden ships, while a fully-informed Spotter provides yes/no answers.
---

# Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People

## Quick Facts
- arXiv ID: 2510.20886
- Source URL: https://arxiv.org/abs/2510.20886
- Reference count: 40
- Language models can be made into rational information-seeking agents using Bayesian inference strategies that approach human-level performance

## Executive Summary
This paper introduces a framework for building rational information-seeking agents that balance exploration (asking questions) and exploitation (taking actions) like humans do. The authors develop Collaborative Battleship, a dialogue-based game where a partially-informed Captain must find hidden ships by asking yes/no questions to a fully-informed Spotter. Through human experiments and a new BATTLESHIPQA dataset (931 gold-labeled questions), they establish baselines for human-like information seeking. They then create Bayesian-inspired inference strategies that significantly improve language model performance: code generation improves Spotter accuracy by up to 14.7%, EIG-based question selection achieves 94.2% of optimal information gain, and particle-based belief updating enables weaker models to outperform both humans and stronger models at dramatically lower cost.

## Method Summary
The framework uses three Bayesian-inspired strategies: Q_Bayes samples candidate questions and selects the one with highest Expected Information Gain (EIG) computed via code execution on a particle-based belief distribution; M_Bayes selects moves by computing hit probabilities under the belief state using weighted particles; D_Bayes decides between asking or shooting using one-step lookahead. For Spotter agents, questions are translated to executable Python functions that compare the visible partial state against ground truth. The particle system maintains a weighted sample approximation over possible board states, updated via Bayes rule with BSC noise modeling. The approach generalizes to other domains like Guess Who? and demonstrates that Bayesian inference can compensate for weaker language model capabilities while remaining cost-effective.

## Key Results
- Code generation improves Spotter QA accuracy by 14.7% absolute over baseline approaches
- EIG-based question selection achieves up to 94.2% of the information-theoretic ceiling (0.227 bits per question)
- Llama-4-Scout with Bayes-QMD outperforms both humans (8%→82% win rate) and GPT-5 (0%→67% win rate) at ~1% of GPT-5's cost
- The framework generalizes to Guess Who? with similar performance gains (+42.4 p.p. for Llama-4-Scout, +28.3 p.p. for GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1: Code Generation for Grounded Question-Answering
Translating natural language questions into executable Python functions improves Spotter answering accuracy by up to 14.7% absolute over direct LM baselines. The LM generates a function `answer(true_board, partial_board) -> bool` that explicitly compares the visible partial state against the ground truth, forcing concrete grounding rather than relying on implicit reasoning. Code execution provides an external verification signal that bypasses the LM's tendency to confabulate. Core assumption: questions are translatable to deterministic functions over board state; the LM has sufficient Python proficiency to express the logic correctly. Evidence: "code improves Spotter QA by 13.2% absolute accuracy points over Base; combining code generation with chain-of-thought (CoT + Code) yields even greater 14.7% gains" (section 4.2). Break condition: Questions requiring pragmatic interpretation (vague, discourse-dependent) may not be expressible as deterministic functions; code syntax errors or logic bugs introduce noise.

### Mechanism 2: EIG-based Question Selection via Inference Scaling (Q_Bayes)
Sampling multiple candidate questions from an LM and selecting the one with highest Expected Information Gain yields questions approaching the information-theoretic ceiling (up to 94.2% of optimal). The system samples K questions (e.g., 10), translates each to a program, executes it against the current particle-based belief distribution to estimate P(answer="yes"), then computes EIG via the closed-form binary entropy expression. Selecting max-EIG questions eliminates redundant queries. Core assumption: the LM's question distribution contains high-EIG candidates; EIG computed over approximate belief correlates with true utility. Evidence: "EIG scales with the number of candidate questions sampled...up to 0.227 bits per question (up to 94.2% of the info-theoretic ceiling)" (section 4.3.1). Break condition: If the LM's prior question distribution lacks diverse high-EIG candidates, resampling won't help; computational cost scales linearly with K.

### Mechanism 3: Particle-based Belief Updating for Move Selection (M_Bayes)
Maintaining a weighted particle approximation over possible board states enables accurate hit probability estimation and superior move selection, lifting weaker LMs to superhuman performance. Initialize N particles (valid board configurations) from prior; after each question-answer pair, reweight particles via Bayes update with BSC noise model, then resample. Hit probability for each tile is the weighted fraction of particles with a ship there. Core assumption: the particle approximation remains faithful to the true posterior; the generative model (board prior) is correct. Evidence: "M_Bayes, which explicitly marginalizes over the implications of each question to compute π_t, provides a more reliable mechanism" (section 4.3.1). Break condition: Particle degeneracy (all weight on few particles) after many updates; intractable for larger state spaces without efficient samplers.

## Foundational Learning

- Concept: Bayesian Experimental Design and Expected Information Gain
  - Why needed here: EIG is the core objective for selecting which questions to ask; it quantifies how much a question reduces uncertainty about latent state.
  - Quick check question: Given a current belief distribution over hypotheses, which of two candidate questions has higher EIG if their P(yes) values are 0.5 and 0.9?

- Concept: Sequential Monte Carlo / Particle Filtering
  - Why needed here: Exact Bayesian inference over board configurations is intractable (~2^30 valid states); SMC provides a tractable weighted-sample approximation.
  - Quick check question: If you have 100 particles and observe an answer that contradicts 90 of them, how do you update and what problem might arise?

- Concept: Binary Symmetric Channel (BSC) Noise Model
  - Why needed here: Models Spotter errors (misreading, miscommunication); the ε parameter controls how aggressively to update beliefs on contradictory evidence.
  - Quick check question: If ε=0.1 and you observe "No" to a question, what weight does a particle that predicts "Yes" retain relative to one that predicts "No"?

## Architecture Onboarding

- Component map:
  Captain Agent: Question Generator (LM) → K candidate questions → Question-to-Code Translator (LM) → executable functions → EIG Scorer (executes code on particles) → ranked questions → Move Selector (M_Bayes) → max hit-probability tile → Decision Module (D_Bayes) → ask vs. shoot via one-step lookahead
  Spotter Agent: Question-to-Code Translator (LM) → Code Executor → Yes/No answer
  Shared: Particle System (SMC) → {(s_j, w_j)} belief state

- Critical path: Question generation → code translation → EIG computation → decision → (if shoot) move selection via particle hit probabilities. The particle system is read at EIG and move stages and written to after each answer observation.

- Design tradeoffs:
  - Larger K (more candidate questions) improves EIG but increases latency and cost linearly.
  - More particles N improve belief fidelity but O(N) per EIG evaluation; paper uses practical defaults (see Appendix B).
  - ε parameter: higher values make the agent more robust to Spotter errors but slower to converge.

- Failure signatures:
  - Redundant questions (EIG≈0): indicates LM question distribution is narrow or belief is already collapsed.
  - Valid but wrong answers: Spotter code has logic bugs; validate against held-out gold labels.
  - Particle collapse: all weight on one hypothesis early; check ε setting and resampling frequency.
  - High variance in EIG estimates: insufficient particles or unstable code execution.

- First 3 experiments:
  1. Reproduce SpotterQA accuracy with/without code generation on the gold dataset (931 questions) to validate grounding mechanism.
  2. Sweep K ∈ {1, 3, 5, 10} for Q_Bayes on CaptainQA; plot EIG vs. K to confirm scaling relationship (Fig. 4b).
  3. Run Llama-4-Scout with +Bayes-QMD vs. GPT-5 LM-only head-to-head on held-out boards to validate win-rate claims (Fig. 18).

## Open Questions the Paper Calls Out

### Open Question 1
Can extending the framework to infer the noise parameter ε (rather than fixing it) improve agent robustness when collaborating with heterogeneous human partners of varying reliability? The authors state: "In place of a fixed ε, a more robust approach would be to infer ε to account for the differences in reliability across individual Spotters." This limits the model's ability to capture epistemic vigilance behaviors observed in human-human interactions.

### Open Question 2
Can RSA-style pragmatic reasoning frameworks be integrated with the current Bayesian Experimental Design approach to improve handling of discourse-dependent, vague, and ambiguous questions? The authors note: "Collaborative Battleship gives rise to many rich pragmatic behaviors...RSA-style frameworks could yield agents capable of more sophisticated pragmatic reasoning." LMs degrade significantly on "complex" questions requiring context (GPT-4o: 72.8%→60.4% simple to complex), while humans maintain consistent accuracy (92.8%→91.9%).

### Open Question 3
To what extent do the Bayesian strategies generalize to information-seeking settings that require learning a generative world model rather than using a hand-specified one? The authors state: "While implementable by hand in a domain like Battleship, in more general settings, we may wish to learn a generative model of world states represented as code (e.g., via MSA) or images (e.g., via VAEs or diffusion)." Real-world scientific discovery or diagnostic settings may have unknown or intractable hypothesis spaces that must be learned.

### Open Question 4
Can the one-step lookahead decision strategy (D_Bayes) be extended to multi-step planning while remaining computationally tractable for real-time human-agent collaboration? The paper notes that "optimizing over longer horizons is possible, the problem of belief-space planning is PSPACE-hard" and uses one-step lookahead as "simple and effective." However, humans exhibit sophisticated temporal strategies (some front-load questions, others interleave throughout gameplay), suggesting non-myopic planning may be important.

## Limitations

- Scalability to larger state spaces beyond 8×8 Battleship and 100-character Guess Who? remains untested
- Assumption that questions are translatable to deterministic code functions may not hold for nuanced or context-dependent queries
- Claims of superhuman performance and cost-effectiveness are based on limited comparisons and estimated API pricing

## Confidence

- High: Spotter accuracy improvements via code generation (validated on gold dataset); EIG-based question selection scaling with candidate sampling (shown in Fig. 4b)
- Medium: Human-like information-seeking baselines (based on single game per participant); generalization to Guess Who? (single game setting, no comparison to human baselines)
- Low: Claims of superhuman performance (Llama-4-Scout beating humans and GPT-5); cost-effectiveness ratios (based on estimated API pricing)

## Next Checks

1. **Particle System Validation**: Run SMC with varying particle counts (N=50, 100, 200) on the 18 boards; measure belief collapse rates and EIG variance to establish minimum viable N.
2. **D_Bayes Decision Logic**: Implement and test the one-step lookahead decision (Eq. 7) with γ=0.5 and γ=0.9 on representative game states; verify it prevents myopic behavior and balances exploration vs. exploitation.
3. **Cross-Model Generalization**: Apply the Bayes-QMD pipeline to Claude-3-Haiku and Gemini-1.5-Flash; compare win rates against the same human and GPT-5 baselines to test whether improvements transfer across model families.