---
ver: rpa2
title: 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions'
arxiv_id: '2510.26768'
source_url: https://arxiv.org/abs/2510.26768
tags:
- amo-bench
- reasoning
- high
- llms
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMO-Bench, a new benchmark for evaluating
  large language models' mathematical reasoning abilities using 50 original, Olympiad-level
  problems. Unlike existing benchmarks that use publicly available competition problems
  and are approaching saturation, AMO-Bench features entirely novel problems cross-validated
  by experts to ensure difficulty comparable to or exceeding IMO standards.
---

# AMO-Bench: Large Language Models Still Struggle in High School Math Competitions
## Quick Facts
- arXiv ID: 2510.26768
- Source URL: https://arxiv.org/abs/2510.26768
- Reference count: 18
- Top model achieves only 52.4% accuracy on novel Olympiad-level problems

## Executive Summary
AMO-Bench introduces a new benchmark for evaluating large language models' mathematical reasoning abilities using 50 original, Olympiad-level problems that are cross-validated by experts to ensure difficulty comparable to or exceeding IMO standards. Unlike existing benchmarks that use publicly available competition problems and are approaching saturation, AMO-Bench features entirely novel problems requiring only final answers for automatic grading. The benchmark reveals that even top-tier models struggle significantly, with the best model achieving only 52.4% accuracy and most models scoring below 40%. Models also generate substantially more output tokens on AMO-Bench compared to easier benchmarks, suggesting the problems' complexity, while showing promising scaling benefits from increased test-time computation.

## Method Summary
The authors created AMO-Bench by generating 50 original high school math competition problems with expert cross-validation to ensure difficulty comparable to IMO standards. Problems are designed to require only final answers rather than proofs, enabling automatic grading through parser-based or LLM-based methods depending on answer type. The benchmark includes human-annotated reasoning paths to support research on solution transparency and error analysis. Models are evaluated based on their accuracy in solving these problems, with additional analysis of output token generation to assess problem complexity and scaling behavior.

## Key Results
- Top-performing model achieves only 52.4% accuracy on AMO-Bench
- Most models score below 40% accuracy on the benchmark
- Models generate substantially more output tokens on AMO-Bench (average ~37K) compared to AIME (average ~6-7K)
- Performance improves nearly linearly with the logarithm of output length, suggesting benefits from test-time scaling

## Why This Works (Mechanism)
The benchmark works by presenting novel, Olympiad-level problems that require genuine mathematical reasoning rather than pattern matching or memorization of known solutions. By requiring only final answers, the benchmark enables automatic evaluation while maintaining high standards for mathematical reasoning. The expert cross-validation ensures problems are genuinely challenging and comparable to top-tier competitions, preventing models from exploiting known problem patterns or solution templates.

## Foundational Learning
- **Mathematical Olympiad problem structures**: Understanding competition formats like algebra, geometry, number theory, and combinatorics is essential for creating appropriate benchmark problems. Quick check: Verify problems cover diverse mathematical domains with balanced representation.
- **Automatic grading systems**: Parser-based and LLM-based evaluation methods enable scalable assessment without manual grading. Quick check: Test grading accuracy on known solutions to ensure reliability.
- **Test-time scaling principles**: The relationship between computation and performance in language models for complex reasoning tasks. Quick check: Measure performance across different model inference budgets.

## Architecture Onboarding
Component map: Problem Generation -> Expert Validation -> Model Evaluation -> Automatic Grading -> Performance Analysis
Critical path: Novel problem creation and validation directly impacts model evaluation quality and benchmark reliability.
Design tradeoffs: Final-answer format enables automatic grading but may miss nuanced understanding; novel problems prevent saturation but require extensive validation.
Failure signatures: Models showing high token generation but low accuracy indicate confusion rather than productive reasoning; consistent failure patterns suggest systematic weaknesses in mathematical reasoning.
First experiments:
1. Run baseline models on AMO-Bench to establish performance distribution
2. Compare token generation patterns between AMO-Bench and existing benchmarks
3. Test grading system accuracy using known correct and incorrect solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 50 problems may limit statistical power and generalizability
- Performance gap could reflect problem domain coverage differences rather than pure difficulty
- Increased token generation may reflect presentation differences rather than reasoning complexity
- Correlation-based scaling analysis may not establish causation

## Confidence
- Novel benchmark difficulty and performance gap: High
- Model scaling benefits with test-time computation: Medium
- Problem generation and validation methodology: High
- Comparison to existing benchmarks (AIME): Medium

## Next Checks
1. Expand the benchmark to include a larger set of problems (e.g., 100+ problems) to improve statistical robustness and reduce potential sampling bias
2. Conduct ablation studies comparing AMO-Bench performance across different model prompting strategies and formatting to isolate the impact of presentation from problem difficulty
3. Perform controlled experiments using the same models on both AMO-Bench and existing benchmarks with standardized problem presentation to better quantify the difficulty gap