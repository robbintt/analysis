---
ver: rpa2
title: 'Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for
  Cross-Modal Memory Enhancement'
arxiv_id: '2506.00030'
source_url: https://arxiv.org/abs/2506.00030
tags:
- modality
- training
- multimodal
- fusion
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of modality imbalance in multimodal
  learning, where dominant modalities overshadow weaker ones, leading to biased learning
  and suboptimal fusion. The authors propose a Shapley Value-guided alternating training
  framework that dynamically prioritizes under-optimized modalities to achieve better
  balance and enhance fusion.
---

# Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement

## Quick Facts
- arXiv ID: 2506.00030
- Source URL: https://arxiv.org/abs/2506.00030
- Reference count: 27
- Primary result: Introduces Shapley Value-guided alternating training and cross-modal memory module to address modality imbalance, achieving state-of-the-art performance on four multimodal benchmarks.

## Executive Summary
This paper addresses the critical issue of modality imbalance in multimodal learning, where dominant modalities overshadow weaker ones, leading to biased learning and suboptimal fusion. The authors propose a novel framework that uses Shapley Value-based scheduling to dynamically prioritize under-optimized modalities during training, combined with a cross-modal memory module that enhances feature alignment and fusion. Evaluated on four benchmark datasets using both conventional and LLM-based backbones, the method achieves state-of-the-art performance while introducing a novel multimodal equilibrium metric (EDM) to quantify modality balance. The approach demonstrates strong robustness under missing modality conditions and provides insights into the effectiveness of alternating training for multimodal learning.

## Method Summary
The framework addresses modality imbalance through three key innovations: (1) Shapley Value-guided dynamic scheduling that prioritizes weaker modalities based on their marginal contribution to performance, (2) a cross-modal alignment module with correlation-based feature mapping and an LSTM-inspired memory unit for fusion, and (3) a hybrid-order inference strategy that aggregates predictions across multiple modality processing orders. The training alternates between modalities based on their computed Shapley Values, while the memory module aligns features and preserves modality-specific information. During inference, the model aggregates representations from all possible modality orderings to enhance robustness. The approach is evaluated on four multimodal benchmark datasets using both conventional backbones and LLM-based encoders.

## Key Results
- Achieves state-of-the-art performance on MVSA-Single, CREMA-D, Kinetics-400, and IEMOCAP datasets
- Introduces Equilibrium Deviation Metric (EDM) showing superior modality balance compared to baselines
- Demonstrates robustness under missing modality conditions with minimal performance degradation
- Outperforms conventional joint training and naive alternating approaches across all benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1: Shapley Value-Guided Dynamic Scheduling
The framework mitigates modality imbalance by adaptively prioritizing under-optimized modalities during training. At the end of each epoch, the model computes the Shapley Value $\phi(M^{(i)})$ for each modality to quantify its contribution to performance. The training order for the subsequent epoch is sorted ascending by these values, ensuring modalities with lower contribution (the "weaker" or "lazy" ones) are optimized first. This mechanism assumes the marginal performance gain calculated via Shapley values is a valid inverse proxy for a modality's "learning need" (i.e., low contribution implies the modality needs more training).

### Mechanism 2: LSTM-Based Cross-Modal Memory Alignment
A memory module enhances fusion by aligning features at both feature and sample levels, preventing information loss during alternating steps. A correlation matrix $C^{(ij)}$ aligns stronger modality features to the weaker modality's space. An LSTM-inspired unit then governs the fusion: a "forget gate" scales the weak modality's original features, and an "input gate" integrates the aligned stronger features. This mechanism assumes gating mechanisms designed for temporal sequences can effectively manage "short-term cross-modal updates" in non-temporal multimodal fusion tasks.

### Mechanism 3: Hybrid-Order Inference Strategy
The framework aggregates predictions across different modality processing orders at inference time to improve robustness when ground truth is unavailable to determine the dominant modality. During inference, rather than picking a single fixed order, the model computes representations for all possible modality permutations, averages the resulting features, and then classifies. This assumes the average representation across multiple orderings is more robust than any single optimized ordering derived during training.

## Foundational Learning

- **Concept: Shapley Values (Game Theory)**
  - Why needed here: This is the mathematical foundation for the "fairness" metric used to schedule training. Without this, one cannot interpret why a modality is prioritized.
  - Quick check question: Can you explain how the marginal contribution of a player in a coalition game maps to a modality's contribution to classification accuracy?

- **Concept: Alternating vs. Joint Optimization**
  - Why needed here: The paper positions itself as a fix to the failures of joint training (gradient dominance) and naive alternating training (lack of fusion awareness).
  - Quick check question: Why does simultaneous gradient descent in joint training often fail to learn robust unimodal representations?

- **Concept: Equilibrium Deviation Metric (EDM)**
  - Why needed here: This is the novel evaluation metric proposed to quantify "balance." Standard accuracy is insufficient to measure modality laziness.
  - Quick check question: If a model has 90% accuracy but an EDM of 50%, what does that imply about its reliance on specific modalities?

## Architecture Onboarding

- **Component map**: Encoders -> Shapley Value Calculator -> Priority Sorter -> Correlation Matrix Scaler -> LSTM Gating Unit (Forget/Input/Output) -> Linear Classifier

- **Critical path**:
  1. **Validation Step**: Run current model on validation set to calculate Shapley Values $\phi(M^{(i)})$.
  2. **Scheduling**: Sort modalities by $\phi$ (low to high).
  3. **Alternating Step**: Train encoder $f_i$ for the lowest-ranked modality while freezing others; update Memory Module.

- **Design tradeoffs**:
  - **Scheduling Cost vs. Balance**: Calculating Shapley values requires multiple forward passes on the validation set. The paper uses an approximation but this adds overhead per epoch.
  - **EDM vs. Accuracy**: Optimizing for a low EDM (perfect balance) may technically lower peak accuracy if the dominant modality is significantly more informative.

- **Failure signatures**:
  - **Oscillating Loss**: If the Shapley values flip-flop every epoch without convergence, the learning rate may be too high or the validation set too small.
  - **Stagnant EDM**: If the EDM metric does not decrease, the Memory Module may be misaligned or the scheduler is failing to identify the weaker modality.

- **First 3 experiments**:
  1. **Baseline Verification**: Implement a "Concat" fusion baseline and measure the accuracy gap between the dominant modality (running alone) and the full multimodal model to confirm the imbalance problem exists.
  2. **Ablation on Order**: Compare random ordering vs. Shapley-guided ordering to isolate the impact of the dynamic scheduler.
  3. **Missing Modality Test**: Drop the dominant modality during inference and measure the performance drop relative to the baseline to verify robustness claims.

## Open Questions the Paper Calls Out

### Open Question 1
Can the hybrid-order inference strategy scale effectively to tasks involving more than three modalities given the factorial growth of sequence permutations? The experimental validation is limited to bi-modal and tri-modal datasets, but computational overhead becomes prohibitive for high-modality settings with $n > 3$.

### Open Question 2
Does the proposed alternating training framework maintain its efficacy when applied to full parameter fine-tuning of Multimodal Large Language Models (MLLMs)? The current validation is limited to feature extraction levels, and it's uncertain if the framework conflicts with stability requirements of updating massive parameter spaces.

### Open Question 3
Can the performance gap between the proposed hybrid-order fusion and the theoretical "unlockable potential" upper bound be closed? The current method averages predictions over fixed orders during inference, but identifying the single optimal order for a specific sample without ground-truth labels remains unsolved.

## Limitations
- Computational overhead of Shapley Value computation may become prohibitive for large-scale datasets or many modalities
- Approximation quality of marginal contribution sampling is not empirically validated
- Memory module design lacks strong external validation in cross-modal context
- Experiments limited to datasets with existing imbalance; effectiveness on balanced datasets untested

## Confidence
- **High Confidence**: Problem formulation (modality imbalance) is well-supported; Shapley Value scheduling is mathematically sound; empirical results show consistent improvements
- **Medium Confidence**: Hybrid-order inference is logically consistent but computational cost not extensively quantified; ablation studies sufficient for component validation
- **Low Confidence**: Cross-modal memory module design is novel but lacks strong external validation; theoretical justification for LSTM gating in non-temporal fusion is weak

## Next Checks
1. **Ablation on Shapley Approximation**: Compare exact vs. sampled Shapley Value computation to quantify trade-off between accuracy and speed; validate whether approximation introduces bias in modality prioritization

2. **Generalization to Balanced Datasets**: Test the method on a balanced multimodal dataset to verify whether the framework degrades performance when imbalance is absent

3. **Scaling to Four+ Modalities**: Extend experiments to datasets with more than three modalities to evaluate whether factorial scaling of hybrid-order inference remains tractable and whether Shapley-based scheduler scales effectively