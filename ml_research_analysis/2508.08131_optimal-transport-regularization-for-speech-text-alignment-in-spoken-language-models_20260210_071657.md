---
ver: rpa2
title: Optimal Transport Regularization for Speech Text Alignment in Spoken Language
  Models
arxiv_id: '2508.08131'
source_url: https://arxiv.org/abs/2508.08131
tags:
- speech
- embeddings
- transport
- slms
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Optimal Transport Regularization (OTReg)
  to address the modality gap between speech and text representations in Spoken Language
  Models (SLMs). The key idea is to formulate speech-text alignment as an optimal
  transport problem, using the optimal transport plan to derive a regularization loss
  that aligns speech embeddings with transcript embeddings.
---

# Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models

## Quick Facts
- arXiv ID: 2508.08131
- Source URL: https://arxiv.org/abs/2508.08131
- Authors: Wenze Xu; Chun Wang; Jiazhen Yu; Sheng Chen; Liang Gao; Weihong Deng
- Reference count: 26
- Primary result: OTReg improves cross-domain generalization in multilingual ASR, reducing WER on CoVoST-2 (13.12) and FLEURS (6.90) test sets

## Executive Summary
This paper introduces Optimal Transport Regularization (OTReg) to address the modality gap between speech and text representations in Spoken Language Models (SLMs). The key idea is to formulate speech-text alignment as an optimal transport problem, using the optimal transport plan to derive a regularization loss that aligns speech embeddings with transcript embeddings. The method is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Experimental results on multilingual ASR tasks demonstrate that OTReg enhances speech-text alignment and reduces the modality gap, improving cross-domain generalization and outperforming CTC-based approaches.

## Method Summary
OTReg formulates speech-text alignment as an optimal transport problem, deriving a regularization loss from the optimal transport plan to align speech and transcript embeddings. The method integrates seamlessly into existing SLM training procedures without requiring additional labels or learnable parameters. By leveraging optimal transport theory, OTReg effectively reduces the modality gap between speech and text representations, enhancing cross-domain generalization in multilingual ASR tasks.

## Key Results
- OTReg improves cross-domain generalization in multilingual ASR tasks
- Achieves WERs of 13.12 on CoVoST-2 and 6.90 on FLEURS test sets
- Outperforms base model and CTC-based approaches in speech-text alignment

## Why This Works (Mechanism)
The paper posits that optimal transport provides a principled way to measure and minimize the distance between speech and text distributions in a shared embedding space. By treating alignment as a transport problem, the method captures the underlying geometric structure of the modality gap and regularizes the model to produce more consistent representations across modalities.

## Foundational Learning
- **Optimal Transport Theory**: Why needed - Provides a mathematical framework for measuring distributional distances; Quick check - Verify Wasserstein distance computations are correctly implemented
- **Speech-Text Modality Gap**: Why needed - Understanding the fundamental challenge in SLM training; Quick check - Analyze embedding spaces for cross-modal consistency
- **Regularization Techniques**: Why needed - Ensures stable training and prevents overfitting; Quick check - Monitor training dynamics with and without OTReg

## Architecture Onboarding

**Component Map**: Speech Encoder -> Speech Embeddings -> Optimal Transport Loss -> SLM Training

**Critical Path**: The optimal transport plan computation is the critical path, as it directly influences the regularization loss and subsequent model updates.

**Design Tradeoffs**: The method trades computational overhead for improved alignment quality, with no additional learnable parameters required.

**Failure Signatures**: Poor alignment quality may manifest as high optimal transport costs or degraded performance on cross-domain test sets.

**First Experiments**:
1. Verify optimal transport plan computation on synthetic alignment tasks
2. Measure embedding space consistency before and after OTReg integration
3. Compare training stability with and without OTReg across multiple seeds

## Open Questions the Paper Calls Out
None

## Limitations
- Optimal transport formulation may not capture all aspects of the complex speech-text modality gap
- Computational overhead of calculating optimal transport plans during training
- Reliance on specific assumptions about shared embedding space viability across languages and acoustic conditions

## Confidence
- **High Confidence**: Core methodology of using optimal transport regularization is sound and well-grounded
- **Medium Confidence**: Experimental results are promising but limited to specific datasets and model architectures
- **Low Confidence**: Claim of no additional learnable parameters may oversimplify implicit model dependencies

## Next Checks
1. Evaluate OTReg's performance on additional multilingual ASR benchmarks and low-resource language scenarios
2. Conduct ablation studies to isolate the contribution of optimal transport regularization
3. Investigate computational overhead and scalability for larger models and real-time applications