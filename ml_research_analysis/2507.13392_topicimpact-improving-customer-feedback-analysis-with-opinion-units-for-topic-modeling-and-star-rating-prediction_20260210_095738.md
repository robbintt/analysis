---
ver: rpa2
title: 'TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic
  Modeling and Star-Rating Prediction'
arxiv_id: '2507.13392'
source_url: https://arxiv.org/abs/2507.13392
tags:
- topic
- sentiment
- opinion
- units
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopicImpact improves customer feedback analysis by preprocessing
  reviews into opinion units using LLMs, then clustering these units via topic modeling.
  This approach generates coherent, interpretable topics linked to sentiment scores
  and business metrics like star ratings.
---

# TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction

## Quick Facts
- **arXiv ID:** 2507.13392
- **Source URL:** https://arxiv.org/abs/2507.13392
- **Reference count:** 22
- **Primary result:** R² = 0.726 for star-rating prediction using sentiment-split topic clustering

## Executive Summary
TopicImpact improves customer feedback analysis by restructuring the topic modeling pipeline to operate on opinion units extracted via LLM rather than full reviews. The system generates coherent, interpretable topics that are linked to sentiment scores and business metrics like star ratings. Experiments on restaurant reviews show that combining topic assignment with sentiment scores (via data splitting and sentiment-aware embeddings) achieves high predictive accuracy for star ratings (R² = 0.726) and high topic coherence (precision 86-92%). This method enables businesses to identify key topics, their sentiment, and impact on ratings, offering a flexible, interpretable alternative to classification for exploratory analysis.

## Method Summary
TopicImpact processes customer reviews by first extracting opinion units using GPT-4, where each unit contains a label, excerpt, and sentiment score. These units are then embedded and clustered into topics using BERTopic. Three methods are compared: (1) clustering all opinion units with general embeddings, (2) clustering with sentiment-aware embeddings, and (3) splitting opinion units by sentiment before clustering each subset separately with general embeddings. Topic features are constructed by averaging sentiment scores per review, and multiple linear regression predicts star ratings from these features. The best method (3) achieves R² = 0.726 on restaurant review data.

## Key Results
- Sentiment-split clustering (Method 3) achieves the highest star-rating prediction accuracy with R² = 0.726
- Topic coherence remains high across all methods, with precision scores of 86-92%
- Sentiment-aware embeddings improve prediction but reduce topic coherence compared to general embeddings
- The method enables identification of topics most strongly associated with customer satisfaction through regression coefficients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing full reviews into opinion units improves topic coherence by isolating individual aspects before clustering.
- **Mechanism:** An LLM extracts opinion units—consisting of a label, excerpt, and sentiment score—from reviews, separating multiple opinions within a single text. Topic clustering then operates on these isolated units rather than mixed-aspect documents.
- **Core assumption:** LLMs can accurately extract and isolate distinct opinion units from unstructured reviews.
- **Evidence anchors:**
  - [abstract] "We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units..."
  - [section] "By clustering these opinion units instead of entire reviews, TopicImpact generates more coherent and interpretable topic clusters."
  - [corpus] Weak direct support; related work on sentiment-topic models exists (e.g., "A Multifacet Hierarchical Sentiment-Topic Model"), but does not validate this specific LLM-based extraction and splitting mechanism.
- **Break condition:** LLM extraction quality degrades for reviews with complex, interwoven opinions or domain-specific jargon, resulting in noisy or fragmented opinion units.

### Mechanism 2
- **Claim:** Separating positive and negative opinion units before clustering improves star-rating prediction accuracy.
- **Mechanism:** Opinion units are split into two datasets based on LLM-assigned sentiment scores (≤5 vs. >5). Each dataset is clustered independently using general-purpose embeddings, allowing regression to learn separate coefficients for positive and negative instances of a topic.
- **Core assumption:** The LLM-assigned sentiment score is reliable enough for a binary split.
- **Evidence anchors:**
  - [abstract] "Experiments show that combining topic assignment with sentiment scores (via data splitting and sentiment-aware embeddings) achieves high predictive accuracy for star ratings (R² = 0.726)..."
  - [section] "Method 3... splits the dataset based on LLM-sentiment scores into positive and negative opinion units before clustering each split separately, achieves the highest accuracy with an R2 value of 0.726..."
  - [corpus] Several related papers (e.g., "Analyzing User Perceptions of LLMs on Reddit") perform joint sentiment-topic analysis, but do not specifically validate this pre-clustering split method.
- **Break condition:** LLM sentiment scores are systematically biased or inconsistent (e.g., misinterpreting sarcasm), causing opinion units to be misclassified, corrupting topic clusters and regression coefficients.

### Mechanism 3
- **Claim:** Multiple linear regression on topic-sentiment features quantifies each topic's impact on a business metric.
- **Mechanism:** A regression model is built where independent variables represent topics. For each review, a topic's value is the average sentiment score of its associated opinion units (or 0 if absent). Beta coefficients indicate direction and strength of association with the star rating.
- **Core assumption:** The relationship between topic sentiment and star rating is approximately linear and additive; topics are sufficiently independent.
- **Evidence anchors:**
  - [abstract] "...correlating topics with business metrics like star ratings through regression analysis, [the system] identifies which topics most strongly influence customer satisfaction."
  - [section] "In Step C, the relationship between the chosen topics and a business metric... is analyzed using multiple linear regression (MLR)... provides coefficients... reflecting their strength of association..."
  - [corpus] Related work ("A Multifacet Hierarchical Sentiment-Topic Model" and cited "Explaining the stars") uses regression-based approaches for aspect-level rating explanation.
- **Break condition:** Key topics interact non-linearly or exhibit multicollinearity, making individual coefficient interpretation unreliable.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - **Why needed here:** This is the core task the paper addresses; understanding it clarifies why "opinion units" are a useful construct.
  - **Quick check question:** How does ABSA differ from document-level sentiment classification?

- **Concept: BERTopic and Embedding-based Clustering**
  - **Why needed here:** This is the core unsupervised technique used to group opinion units into topics after LLM extraction.
  - **Quick check question:** What is the role of UMAP and HDBSCAN in the standard BERTopic pipeline?

- **Concept: Multiple Linear Regression (MLR) Interpretation**
  - **Why needed here:** The system's final output—ranking topics by impact—is derived from MLR beta coefficients.
  - **Quick check question:** In this context, what does a negative β-coefficient for a "Service" topic signify?

## Architecture Onboarding

- **Component map:** Raw reviews -> LLM Preprocessing -> Embedding -> Clustering -> Feature Assembly -> Regression -> Insight Dashboard
- **Critical path:** LLM Preprocessing → Embedding → Clustering → Feature Assembly → Regression. Errors in extraction propagate through the entire pipeline.
- **Design tradeoffs:**
  - **General vs. sentiment-aware embeddings:** General yields higher topic coherence; sentiment-aware improves prediction but splits topics by sentiment. Best method (M3) uses general embeddings on pre-split data.
  - **K (number of topics):** Lower K yields broader topics; higher K increases granularity but may reduce coherence.
  - **Topic modeling vs. classification:** Flexibility and exploration vs. control over definitions.
- **Failure signatures:**
  - **High outlier rate (>30%):** Indicates embedding or UMAP issues.
  - **Low topic precision (human eval):** Clusters incoherent; adjust embedding model or K.
  - **Low R² (<0.4):** Check sentiment incorporation (Method 1 vs. 3) or topic relevance.
- **First 3 experiments:**
  1. **Baseline replication:** Implement Method 1 on a sample dataset; measure topic precision and R².
  2. **Ablation study:** Compare Method 3 vs. Method 1 on same data; validate R² improvement.
  3. **Domain transfer:** Apply full pipeline (Method 3) to a different corpus (e.g., hotel reviews); evaluate coherence and R².

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can keyword or example-guided topic seeding be integrated into the TopicImpact framework to effectively combine the predefined segmentation of classification with the exploratory benefits of topic modeling?
- **Basis in paper:** [explicit] The Conclusion states, "A promising avenue for future research is integrating keyword or example-guided topic seeding within our framework... [to] combine the predefined segmentation of classification with the exploratory... benefits of topic modeling."
- **Why unresolved:** The current system relies on unsupervised clustering (BERTopic) which, while flexible, suffers from a lack of control over topic granularity (e.g., "tiramisu" vs. "dessert") compared to classification methods.
- **What evidence would resolve it:** Experiments demonstrating that seeded clustering produces user-aligned categories without incurring the high compute costs associated with re-classifying datasets for every new hypothesis.

### Open Question 2
- **Question:** How does the performance of TopicImpact vary across domains with differing opinion structures, such as retail product reviews or employee surveys, particularly regarding opinion length and nuance?
- **Basis in paper:** [explicit] The Conclusion suggests, "Verifying the performance of TopicImpact in other domains, such as retail product reviews, course evaluations, or employee satisfaction surveys, could improve opinion analysis in these critical areas."
- **Why unresolved:** The evaluation was restricted to the Yelp dataset of US restaurant reviews, and the authors note that other domains may present distinct challenges like increased opinion nuance or varied opinion lengths.
- **What evidence would resolve it:** Benchmarking the pipeline (opinion unit extraction and regression) on datasets from non-restaurant domains (e.g., e-commerce, HR surveys) and analyzing the fidelity of extracted opinion units in these contexts.

### Open Question 3
- **Question:** Do the observed trade-offs between general-purpose and sentiment-aware embeddings regarding topic and sentiment coherence persist across a wider variety of embedding models?
- **Basis in paper:** [explicit] The Limitations section notes, "While the comparison reveals clear trends between the general and sentiment-aware embeddings, further validation using a larger number of embedding models would enhance the reliability and generalizability of these conclusions."
- **Why unresolved:** The study only compared a single general-purpose model (all-mpnet-base-v2) against a single sentiment-aware model (SentiCSE), leaving the generalizability of the specific performance trends uncertain.
- **What evidence would resolve it:** A replication of the clustering and regression experiments using multiple distinct architectures for both general and sentiment-aware embedding categories to confirm if the trend (general = high topic precision, sentiment-aware = lower topic precision) is robust.

## Limitations
- The method relies heavily on LLM preprocessing quality, with limited empirical validation of extraction accuracy across diverse review domains
- Performance evaluation is restricted to restaurant reviews, limiting generalizability to other domains with different review structures
- Absence of standardized benchmarks makes it difficult to contextualize the R² = 0.726 result against established ABSA methods

## Confidence
- **Topic coherence and prediction claims (M3 results):** High confidence, supported by systematic ablation experiments comparing three methods on the same dataset
- **LLM preprocessing reliability:** Medium confidence, mechanism described but limited empirical validation of extraction accuracy
- **Cross-domain generalizability:** Low confidence, validation limited to restaurant reviews across three cuisines

## Next Checks
1. **Ablation study on LLM quality:** Systematically evaluate how LLM extraction errors (measured via human annotation) correlate with downstream topic coherence and prediction accuracy
2. **Cross-domain transfer test:** Apply Method 3 to a non-restaurant domain (e.g., hotel or product reviews) and compare performance metrics
3. **Benchmark comparison:** Implement and compare against established ABSA methods (e.g., TopicBERT) on a standardized dataset to contextualize the R²=0.726 result