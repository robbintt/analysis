---
ver: rpa2
title: 'Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark'
arxiv_id: '2512.20174'
source_url: https://arxiv.org/abs/2512.20174
tags:
- document
- retrieval
- queries
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NL-DIR, a new benchmark for natural language-based
  document image retrieval (DIR) that uses fine-grained textual queries to retrieve
  relevant document images from a large gallery. The NL-DIR dataset consists of 41K
  authentic document images from the Industry Documents Library, each paired with
  five high-quality, semantically rich queries generated via large language models
  and manual verification.
---

# Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark

## Quick Facts
- arXiv ID: 2512.20174
- Source URL: https://arxiv.org/abs/2512.20174
- Reference count: 40
- New benchmark NL-DIR with 41K document images paired with 5 fine-grained queries each

## Executive Summary
This paper introduces NL-DIR, a new benchmark for natural language-based document image retrieval (DIR) that uses fine-grained textual queries to retrieve relevant document images from a large gallery. The NL-DIR dataset consists of 41K authentic document images from the Industry Documents Library, each paired with five high-quality, semantically rich queries generated via large language models and manual verification. The authors evaluate both zero-shot and fine-tuned performance of mainstream contrastive vision-language models and OCR-free visual document understanding models. A two-stage retrieval method is proposed, combining a recall stage with a cross-attention-based re-ranking stage, achieving strong performance while maintaining time and space efficiency.

## Method Summary
The authors create the NL-DIR dataset by curating 41K document images from the UCSF Industry Documents Library and generating five fine-grained queries per image using LLMs followed by human verification. They evaluate both zero-shot and fine-tuned performance of mainstream contrastive vision-language models and OCR-free visual document understanding models. A two-stage retrieval method is proposed, combining a recall stage with a cross-attention-based re-ranking stage, achieving strong performance while maintaining time and space efficiency. The study highlights that models pre-trained on image-text contrastive tasks, fine-grained interaction for semantic understanding, and OCR-free approaches excel, especially in low-quality images or queries containing visual elements.

## Key Results
- NL-DIR dataset contains 41K document images with 5 high-quality, semantically rich queries per image
- Two-stage retrieval method (recall + cross-attention re-ranking) achieves strong performance with maintained efficiency
- OCR-free visual document understanding models show superior performance, especially on low-quality images and visual element queries

## Why This Works (Mechanism)
The two-stage retrieval pipeline works by first using a recall model to quickly identify candidate documents, then applying a cross-attention-based re-ranker to refine the results. This approach balances efficiency and accuracy by leveraging the speed of initial coarse matching with the precision of fine-grained semantic understanding in the re-ranking stage. The cross-attention mechanism allows the model to focus on relevant document regions when evaluating query-document similarity, improving retrieval quality.

## Foundational Learning
- **Cross-modal retrieval**: Needed because document images and natural language queries exist in different semantic spaces. Quick check: Can the model align visual document features with textual query embeddings?
- **Contrastive learning**: Essential for pre-training models to understand document-image and text relationships. Quick check: Does the model learn meaningful document-image-text associations?
- **OCR-free document understanding**: Critical for handling documents where text extraction is noisy or unreliable. Quick check: Can the model understand documents without explicit text recognition?
- **Cross-attention mechanisms**: Required for fine-grained semantic understanding between queries and document regions. Quick check: Does the model attend to relevant document areas when matching queries?

## Architecture Onboarding

**Component Map**: Document Gallery -> Recall Model -> Candidate Set -> Cross-Attention Re-ranker -> Final Results

**Critical Path**: Image Query → Vision-Language Encoder → Embedding Space → Similarity Search → Cross-Attention Re-ranker → Retrieved Documents

**Design Tradeoffs**: The two-stage approach trades some initial retrieval accuracy for significant gains in efficiency and overall retrieval quality. The recall stage prioritizes speed and broad coverage, while the re-ranking stage provides precision at the cost of additional computation.

**Failure Signatures**: Models may fail when queries contain ambiguous language, when document images have severe quality issues, or when visual elements in queries cannot be adequately captured by text-based representations. The cross-attention mechanism may struggle with extremely long documents or complex layouts.

**3 First Experiments**:
1. Evaluate baseline retrieval performance using only the recall stage to establish efficiency metrics
2. Test cross-attention re-ranker performance on a subset of challenging queries to validate precision gains
3. Compare OCR-based vs OCR-free models on low-quality document subsets to quantify the advantage of visual understanding

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset sourced from specific industry documents, limiting generalizability to other document types
- Evaluation focuses on English queries, not addressing cross-lingual retrieval challenges
- Scalability analysis limited to current dataset size (41K documents)

## Confidence
- Dataset construction methodology: High
- Two-stage retrieval approach effectiveness: Medium
- Generalizability to other document domains: Medium
- Cross-lingual retrieval capabilities: Low

## Next Checks
1. Test the benchmark and proposed method on diverse document types (invoices, forms, receipts) to assess domain generalization
2. Conduct thorough analysis of query complexity and retrieval performance across different semantic categories
3. Evaluate the two-stage retrieval method on larger-scale datasets (1M+ documents) to verify claimed efficiency gains and identify potential bottlenecks