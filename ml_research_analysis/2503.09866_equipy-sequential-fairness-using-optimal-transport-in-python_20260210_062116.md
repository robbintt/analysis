---
ver: rpa2
title: 'EquiPy: Sequential Fairness using Optimal Transport in Python'
arxiv_id: '2503.09866'
source_url: https://arxiv.org/abs/2503.09866
tags:
- fairness
- predictions
- sensitive
- fair
- unfairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EquiPy, a Python package for achieving algorithmic
  fairness across multiple sensitive variables using optimal transport theory. The
  package addresses the challenge of maintaining predictive accuracy while mitigating
  biases in machine learning models, particularly when dealing with complex scenarios
  involving multiple sensitive attributes like gender and race.
---

# EquiPy: Sequential Fairness using Optimal Transport in Python

## Quick Facts
- arXiv ID: 2503.09866
- Source URL: https://arxiv.org/abs/2503.09866
- Reference count: 5
- Primary result: Python package using optimal transport to achieve algorithmic fairness across multiple sensitive variables while maintaining predictive accuracy

## Executive Summary
EquiPy is a Python package that addresses algorithmic fairness across multiple sensitive attributes using optimal transport theory. The package employs Wasserstein barycenters to sequentially transform model predictions, ensuring Demographic Parity across sensitive groups while minimizing impact on predictive accuracy. By breaking down the complexity of multiple variables into manageable sub-problems, EquiPy provides both efficient correction and interpretable results for fairness-aware machine learning practitioners.

## Method Summary
The core methodology utilizes sequential Wasserstein barycenter computations to transform model predictions and achieve fairness across multiple sensitive variables. The approach works by first computing Wasserstein barycenters for pairs of sensitive groups, then iteratively applying corrections to achieve Demographic Parity. This sequential mechanism optimizes only for specified variables while maintaining predictive performance. The implementation is model-agnostic and compatible with various ML frameworks, offering a user-friendly interface with fit and transform methods similar to scikit-learn.

## Key Results
- Successfully reduced unfairness metrics from 0.437 to 0.067 for ethnicity-based predictions
- Effectively handled multiple sensitive attributes with minimal performance degradation
- Provided comprehensive visualization tools for analyzing fairness-performance trade-offs

## Why This Works (Mechanism)
The method works by leveraging optimal transport theory to redistribute model predictions across sensitive groups. By computing Wasserstein barycenters, it finds an optimal transport plan that minimizes the distance between distributions while satisfying fairness constraints. The sequential approach allows for handling multiple sensitive attributes by iteratively correcting predictions, ensuring that each group's distribution is aligned with the fair barycenter. This mathematical framework provides a principled way to balance fairness and accuracy trade-offs.

## Foundational Learning
**Optimal Transport Theory**: Mathematical framework for comparing probability distributions
- Why needed: Provides the mathematical foundation for measuring and minimizing distribution differences
- Quick check: Verify understanding of Wasserstein distance vs. other distribution metrics

**Wasserstein Barycenters**: Weighted averages of probability distributions
- Why needed: Enables finding fair intermediate distributions across multiple groups
- Quick check: Understand how barycenters differ from simple arithmetic means

**Demographic Parity**: Fairness metric requiring independence between predictions and sensitive attributes
- Why needed: Defines the fairness objective the algorithm optimizes for
- Quick check: Recognize the difference between Demographic Parity and other fairness definitions

**Quantile Functions**: Functions mapping probabilities to values
- Why needed: Enables transformation between different group distributions
- Quick check: Verify understanding of inverse transform sampling

**Cumulative Distribution Functions**: Functions mapping values to probabilities
- Why needed: Essential for computing Wasserstein distances and barycenters
- Quick check: Confirm understanding of CDF properties and applications

## Architecture Onboarding
**Component Map**: Data -> Group Statistics -> Wasserstein Barycenter -> Transformation Map -> Fair Predictions

**Critical Path**: The sequential correction pipeline where predictions flow through group-wise transformations to achieve fairness

**Design Tradeoffs**: 
- Accuracy vs. Fairness: Minimizing performance degradation while maximizing fairness improvement
- Computational Efficiency vs. Precision: Balancing exact optimal transport computations with approximate methods for scalability

**Failure Signatures**: 
- No improvement in fairness metrics despite correction
- Significant performance degradation (>5%) with minimal fairness gain
- Runtime errors during barycenter computation for large datasets

**First Experiments**:
1. Apply EquiPy to a binary classification dataset with known sensitive attributes and verify fairness improvement
2. Test the package with a regression task and compare performance before and after correction
3. Evaluate computational runtime on increasing dataset sizes to assess scalability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does mitigating bias for a specific set of observed sensitive attributes impact fairness with respect to unobserved or latent sensitive attributes?
- Basis in paper: The authors note that "ethnicity is often unavailable to practitioners," raising the concern that "ensuring fairness for one observed sensitive attribute (for example, sex) does not inadvertently compromise fairness for another unobserved attribute (for example, ethnicity)."
- Why unresolved: The sequential mechanism optimizes only for the specified variables; the correlation structure between observed and unobserved attributes in the wild is unpredictable and not controlled by the algorithm.
- What evidence would resolve it: Empirical studies on datasets containing "hidden" sensitive attributes, measuring the unfairness of the corrected model against these hidden variables.

### Open Question 2
- Question: Can the sequential Wasserstein barycenter framework be extended to enforce fairness definitions other than Demographic Parity, such as Equalized Odds?
- Basis in paper: The paper restricts scope to Demographic Parity (independence), stating in Footnote 2 that "other common metrics such as Equalized Odds are not directly applicable to the regression case."
- Why unresolved: The current mathematical formulation relies on transporting distributions to a barycenter to satisfy independence (Demographic Parity), which does not guarantee the conditional independence required for Equalized Odds.
- What evidence would resolve it: A theoretical derivation or algorithm modification that successfully maps unfair scores to a fair distribution while satisfying separation (Equalized Odds) constraints.

### Open Question 3
- Question: Is it feasible to adapt the sequential correction mechanism to handle continuous sensitive attributes rather than discrete ones?
- Basis in paper: The methodology relies on grouping data by discrete sensitive features ($A \subset \mathbb{N}^r$) to compute group-wise quantile functions and cumulative distribution functions.
- Why unresolved: The discrete group formulation is fundamental to the current implementation, and moving to continuous attributes would require defining transport maps over continuous spaces rather than discrete categories.
- What evidence would resolve it: A functional extension of the `MultiWasserstein` class that utilizes conditional density estimation to perform corrections on continuous variables without binning.

## Limitations
- Current implementation limited to Demographic Parity fairness definition
- Computational complexity may scale poorly with large datasets and many sensitive attributes
- Assumes discrete sensitive attributes rather than continuous ones

## Confidence
**High confidence**: Model-agnostic implementation claim, user-friendly interface design
**Medium confidence**: Fairness improvement metrics, scalability claims
**Low confidence**: Performance on real-world datasets with hidden sensitive attributes

## Next Checks
1. Test the package on benchmark fairness datasets (e.g., Adult, COMPAS) with established fairness baselines
2. Evaluate performance degradation across different ML model types (tree-based, neural networks, linear models)
3. Conduct statistical significance testing on fairness improvements across multiple runs and data splits