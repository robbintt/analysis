---
ver: rpa2
title: 'Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks'
arxiv_id: '2505.23192'
source_url: https://arxiv.org/abs/2505.23192
tags:
- images
- aigc
- prompt
- image
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bypassing AIGC (AI-generated
  content) detectors using semantic prompt attacks. The authors propose an automated
  adversarial prompt generation framework that leverages a grammar tree structure
  and a variant of the Monte Carlo tree search algorithm (UCT-Rand) to systematically
  explore the semantic prompt space.
---

# Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks

## Quick Facts
- **arXiv ID:** 2505.23192
- **Source URL:** https://arxiv.org/abs/2505.23192
- **Reference count:** 15
- **Key outcome:** First place in AIGC adversarial detection competition using automated semantic prompt attacks

## Executive Summary
This paper addresses the challenge of bypassing AI-generated content (AIGC) detectors using semantic prompt attacks. The authors propose an automated adversarial prompt generation framework that leverages a grammar tree structure and a variant of the Monte Carlo tree search algorithm (UCT-Rand) to systematically explore the semantic prompt space. The method generates diverse, controllable prompts that consistently evade both open-source and commercial AIGC detectors, achieving first place in a real-world AIGC adversarial detection competition. The framework can also be used to construct high-quality adversarial datasets for training and evaluating more robust AIGC detection systems.

## Method Summary
The paper introduces an automated adversarial prompt generation framework that systematically explores the semantic prompt space to evade AIGC detectors. The core innovation is the use of a grammar tree structure combined with a UCT-Rand algorithm variant to automatically generate diverse and controllable prompts. Unlike manual prompt engineering approaches, this method can efficiently discover semantic transformations that fool detectors across multiple text-to-image models including Flux, Midjourney, and Stable Diffusion. The framework not only achieves high evasion rates but also produces high-quality adversarial datasets that can be used to train more robust detection systems.

## Key Results
- Achieved first place in a real-world AIGC adversarial detection competition
- Consistently evaded both open-source and commercial AIGC detectors across multiple T2I models
- Demonstrated superior performance compared to manual prompt engineering approaches
- Generated high-quality adversarial datasets for training more robust AIGC detectors

## Why This Works (Mechanism)
The approach works by systematically exploring the semantic space of prompts rather than relying on human intuition. The grammar tree structure provides a comprehensive representation of possible semantic transformations, while the UCT-Rand algorithm efficiently searches this space to find combinations that evade detectors. This automated exploration discovers semantic patterns that human engineers might miss, creating more diverse and effective adversarial prompts.

## Foundational Learning
1. **Monte Carlo Tree Search (MCTS)** - Used to systematically explore the prompt space; needed for efficient search of high-dimensional semantic transformations
2. **UCT-Rand Algorithm** - A variant of MCTS with random exploration; needed to balance exploitation of known good prompts with discovery of new adversarial patterns
3. **Grammar Tree Structures** - Hierarchical representation of semantic prompt components; needed to organize and traverse the semantic space systematically
4. **Semantic Prompt Space** - The multidimensional space of possible prompt variations; needed to understand the scope of potential adversarial attacks
5. **Adversarial Dataset Generation** - Creating training data that exposes detector weaknesses; needed for improving future detection systems
6. **AIGC Detection Systems** - The target systems being evaded; needed to understand the attack surface and evaluation metrics

## Architecture Onboarding

**Component Map:** Grammar Tree -> UCT-Rand Search -> Prompt Generation -> AIGC Model -> Detector Evaluation

**Critical Path:** The core workflow involves constructing the grammar tree from seed prompts, using UCT-Rand to explore and select promising semantic transformations, generating new adversarial prompts, evaluating them through AIGC models, and measuring evasion success against detectors.

**Design Tradeoffs:** The framework trades computational complexity for systematic coverage of the prompt space, versus manual approaches that are faster but potentially miss optimal solutions. The grammar tree structure enables controlled exploration but may have blind spots in semantic transformations not captured by the initial grammar rules.

**Failure Signatures:** The system may fail when semantic transformations fall outside the defined grammar rules, when detectors evolve to recognize the generated adversarial patterns, or when the UCT-Rand search converges prematurely on local optima rather than global solutions.

**First Experiments:**
1. Evaluate the framework against a diverse set of detection systems beyond the current benchmark set
2. Compare automated prompt generation results with expert human prompt engineering
3. Test whether adversarial datasets improve detector robustness when used for training

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on image generation models, not extensively tested against text, audio, or video AIGC detectors
- Grammar tree structure may have inherent blind spots in the prompt space it can explore
- Performance may degrade when applied to different model architectures or detection systems not represented in current benchmarks

## Confidence
- **High confidence** in technical implementation and competition results - concrete achievements with measurable outcomes
- **Medium confidence** in framework's ability to generate high-quality adversarial datasets for training more robust detectors - requires further empirical validation
- **Medium confidence** in claims about automated prompt exploration addressing controllability and efficiency limitations - needs more systematic comparison with manual approaches

## Next Checks
1. Test UCT-Rand algorithm performance against diverse detection systems including specialized detectors for different AIGC modalities (text, audio, video)
2. Conduct systematic comparison between automated prompt generation and expert human prompt engineering to quantify improvements in controllability and efficiency
3. Evaluate whether adversarial datasets generated by this framework actually improve detector robustness when used for training through controlled experiments