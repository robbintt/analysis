---
ver: rpa2
title: Learning from Natural Language Feedback for Personalized Question Answering
arxiv_id: '2508.10695'
source_url: https://arxiv.org/abs/2508.10695
tags:
- feedback
- user
- personalized
- response
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VAC introduces a novel framework for personalized question answering
  that replaces scalar reward signals with natural language feedback (NLF) generated
  from user profiles and question narratives. The framework alternates between optimizing
  a feedback model to produce actionable guidance and fine-tuning a policy model to
  generate improved personalized responses without requiring feedback at inference
  time.
---

# Learning from Natural Language Feedback for Personalized Question Answering

## Quick Facts
- arXiv ID: 2508.10695
- Source URL: https://arxiv.org/abs/2508.10695
- Reference count: 40
- Personalized QA framework using natural language feedback achieves 13.6% relative improvement over non-personalized baselines

## Executive Summary
This paper introduces VAC (Value-driven Adaptive Control), a novel framework for personalized question answering that replaces traditional scalar reward signals with natural language feedback (NLF). The approach alternates between optimizing a feedback model to produce actionable guidance and fine-tuning a policy model to generate improved personalized responses. Experiments on the LaMP-QA benchmark demonstrate that VAC achieves significant performance gains over both non-personalized and existing personalized baselines while being more computationally efficient.

## Method Summary
VAC implements an iterative training framework where a feedback model generates natural language guidance to improve policy responses. The process alternates between two phases: first, the feedback model is trained to generate NLF that maximizes evaluation scores when applied to policy outputs; second, the policy model is fine-tuned using these feedback-enhanced responses. Both models use Qwen 2.5 7B architecture, with Contriever for retrieval and LoRA for parameter-efficient fine-tuning. The framework trains for three iterations, generating multiple feedback candidates per sample and selecting the highest-scoring ones for policy training.

## Key Results
- 13.6% relative improvement over non-personalized baselines on LaMP-QA benchmark
- 3.6% improvement over best personalized baseline while being 1.9× more efficient
- 6.0% improvement over reinforcement learning with scalar rewards
- Human evaluation preferred VAC in 44% of comparisons against state-of-the-art method

## Why This Works (Mechanism)
VAC works by replacing sparse scalar rewards with rich, interpretable natural language feedback that provides more actionable guidance for model improvement. The iterative training loop allows both the feedback generation and policy refinement to mutually improve, creating a self-reinforcing cycle. Natural language feedback can capture nuanced aspects of personalization that scalar rewards cannot express, leading to more effective optimization of personalized response quality.

## Foundational Learning
- **Natural Language Feedback Generation**: Learning to produce actionable guidance instead of scalar scores - needed to provide interpretable improvement signals; quick check: feedback should be specific and actionable
- **Iterative Model Training**: Alternating between feedback model and policy optimization - needed to create self-reinforcing improvement cycle; quick check: both models should show consistent performance gains
- **Personalized Evaluation Metrics**: Aspect-based scoring using LLM judges - needed to measure nuanced personalization quality; quick check: metric should correlate with human preferences
- **Retrieval-Augmented Generation**: Using Contriever for relevant context retrieval - needed to incorporate user-specific information; quick check: retrieved documents should match user profile and query
- **Parameter-Efficient Fine-Tuning**: LoRA implementation for adaptation - needed to reduce computational cost; quick check: LoRA should achieve similar performance to full fine-tuning with fewer parameters
- **Feedback Application**: Revising responses based on NLF - needed to translate guidance into improved outputs; quick check: revised responses should score higher than originals

## Architecture Onboarding

**Component Map:** User Query -> Contriever Retriever -> Policy Model -> Feedback Model -> Revised Response -> Evaluation Metric

**Critical Path:** Query → Retrieval → Policy Generation → Feedback Application → Policy Fine-tuning → Improved Response

**Design Tradeoffs:** Uses natural language feedback instead of scalar rewards for richer signals, but requires more complex training pipeline and evaluation infrastructure. The iterative approach improves performance but increases training time compared to single-pass methods.

**Failure Signatures:** Feedback model generates generic or unhelpful guidance; policy fails to incorporate feedback effectively; evaluation metric doesn't correlate with actual personalization quality.

**First Experiments:**
1. Test feedback generation quality by applying feedback to random responses and measuring score changes
2. Verify retrieval quality by checking if top-10 results contain relevant user-specific information
3. Validate iterative training by comparing performance across the three training iterations

## Open Questions the Paper Calls Out
1. **Extending to Reasoning Traces**: The framework could be extended beyond response-level generation to provide feedback over reasoning traces, enabling more personalized and transparent multi-step reasoning. Current VAC operates solely on final response outputs, and reasoning traces involve different feedback requirements.

2. **Cross-Architecture Generalization**: Investigate effectiveness on different LLM architectures, including reasoning-focused models. Only Qwen 2.5-7B was tested due to computational constraints (750+ GPU hours).

3. **Narrative-Free Adaptation**: How dependent is VAC on access to question narratives during training, and can it be adapted to settings where such narratives are unavailable? The framework requires narratives for feedback generation, creating a potential supervision bottleneck.

## Limitations
- Performance degradation on Art & Entertainment domain (0.3454 vs 0.3518 for PlanPers) with no investigation into root causes
- Computational intensity requiring 750+ GPU hours, limiting experimentation with different architectures
- Missing implementation details for evaluation prompt format, hindering exact reproduction of results

## Confidence
- **High Confidence**: 13.6% relative improvement over non-personalized baselines is well-supported by methodology
- **Medium Confidence**: 1.9× efficiency improvement requires careful validation across computational setups
- **Low Confidence**: Exact scoring mechanism for personalized aspects using Qwen 2.5 32B is not fully reproducible without referenced evaluation prompt details

## Next Checks
1. Verify evaluation prompt implementation by testing multiple prompt variations to ensure consistency with reported metrics
2. Confirm narrative availability by testing training pipeline with and without question narratives to assess impact on performance
3. Benchmark efficiency measurement by independently measuring training steps and compute time across all baselines to verify 1.9× efficiency claim