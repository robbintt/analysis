---
ver: rpa2
title: Block removal for large language models through constrained binary optimization
arxiv_id: '2602.00161'
source_url: https://arxiv.org/abs/2602.00161
tags:
- blocks
- block
- arxiv
- optimization
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing large language
  models (LLMs) by removing entire transformer blocks, a problem complicated by the
  exponential complexity of selecting the optimal set of blocks to remove. The authors
  formulate this as a constrained binary optimization (CBO) problem, mapping it to
  an Ising model where low-energy solutions correspond to high-performing pruned models.
---

# Block removal for large language models through constrained binary optimization

## Quick Facts
- arXiv ID: 2602.00161
- Source URL: https://arxiv.org/abs/2602.00161
- Reference count: 40
- The paper introduces a constrained binary optimization (CBO) method that maps block removal to an Ising model, outperforming state-of-the-art techniques with up to 6 percentage points MMLU improvement at 40%+ compression.

## Executive Summary
This paper addresses the challenge of compressing large language models by removing entire transformer blocks through a constrained binary optimization framework. The authors formulate block selection as a QUBO problem, leveraging a second-order Taylor expansion of the loss function to capture interactions between blocks and enable efficient ranking of candidate configurations. Their method outperforms existing approaches across multiple benchmarks while maintaining the ability to generalize to heterogeneous architectures like mixture-of-experts models. A key insight is that low-energy excited states of the CBO problem often yield better pruning configurations than the mathematical ground state.

## Method Summary
The method adds a coupling variable α_i to each transformer block's attention and FFN outputs, initialized to 1. Using 2048 calibration samples, the authors compute a Hessian approximation H_0 ≈ (1/m) A^T A from per-sample gradients. They then solve the CBO problem min x^T H_0 x subject to sum(x_i) = M blocks to remove. The solution can be found via brute-force enumeration for N ≤ 40 or QUBO solvers for larger models. Block selection prioritizes low-energy states, with the option to inspect excited states that may outperform the ground state. Optional knowledge distillation fine-tuning (1 epoch Llama, 0.5 epoch Qwen) helps recover performance.

## Key Results
- MMLU scores improve by up to 6 percentage points compared to state-of-the-art methods at 40%+ parameter reduction
- The 17th excited state (removing Block 2) outperforms the ground state across multiple benchmarks
- Successfully generalizes to heterogeneous architectures, removing blocks from NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 without retraining
- Demonstrates robust performance across MMLU, Winogrande, GSM8K, BBH, Hellaswag, and ARC Challenge benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Second-Order Taylor Expansion Captures Block Interactions
The method uses a second-order Taylor expansion to capture interaction effects between non-adjacent transformer blocks that greedy methods miss. By retaining the Hessian term (H_0), the quadratic objective x^T H_0 x encodes how removing one block alters the optimal state of others. This assumes the model is sufficiently well-trained so that the first-order gradient term ∇L(α_0) ≈ 0. If the model is under-trained or the loss landscape is highly non-convex, the second-order approximation may diverge from the true loss.

### Mechanism 2: Ising Energy Landscape as a Performance Proxy
Mapping the block selection problem to an Ising model creates an energy landscape that serves as a proxy for pruned model performance. Binary selection variables x_i act as spins, and the optimization solver searches for low-energy states. The correlation between low-energy configurations and higher benchmark scores (e.g., MMLU) validates this approach. The assumption is that the Hessian approximation accurately reflects the Fisher information matrix, ensuring the energy term meaningfully represents model distortion.

### Mechanism 3: Robustness via Low-Energy "Excited States"
The exact ground state is not always the optimal pruning configuration; slightly higher-energy "excited states" often yield better empirical results. The Ising formulation allows enumeration of low-energy spectra, revealing that the 17th excited state (removing Block 2) outperforms the ground state. This challenges the heuristic that only consecutive/later blocks should be removed. The assumption is that the "loss proxy" doesn't perfectly correlate with specific downstream capabilities, creating situations where slightly sub-optimal energy solutions are functionally superior.

## Foundational Learning

- **Concept: Taylor Expansion & Hessians**
  - Why needed here: The entire method rests on approximating the change in loss (δL) using the curvature (Hessian) rather than just the slope (Gradient).
  - Quick check question: Why does the method assume the gradient term ∇L(α_0) is zero, and what happens if that assumption is wrong?

- **Concept: Fisher Information Matrix**
  - Why needed here: Calculating the exact Hessian for LLMs is computationally prohibitive. The paper uses the Fisher matrix approximation (E[gg^T]) which requires only gradients.
  - Quick check question: How does Eq. (6) (H_0 ≈ (1/m) A^T A) simplify the computation required for the Ising model coupling strengths?

- **Concept: QUBO & Ising Models**
  - Why needed here: The pruning problem must be converted into a format solvable by specialized optimization hardware or algorithms.
  - Quick check question: How do the constraints (remove exactly M blocks) translate into the "fixed magnetization" constraint mentioned in the text?

## Architecture Onboarding

- **Component map:** Input calibration data -> Hessian Builder -> CBO Solver -> Selector -> Retraining (optional)
- **Critical path:**
  1. Sample calibration data (2048 samples used in paper)
  2. Compute Hessian approximation (dominant compute cost)
  3. Solve optimization for desired removal rate M
  4. Inspect low-energy states to find structurally sound configuration
  5. Prune and fine-tune

- **Design tradeoffs:**
  - Solver Accuracy vs. Speed: For N ≈ 40 blocks, brute force is feasible. For N > 100, approximate solvers (Simulated Annealing/Gurobi) risk convergence to local minima.
  - Excited vs. Ground State: The Ground state is the mathematical answer; Excited states (e.g., 17th) offer structural diversity that may generalize better.

- **Failure signatures:**
  - Calibration Mismatch: If calibration data is distinct from evaluation data, the Hessian directs pruning incorrectly.
  - Numerical Instability: The paper notes "energies are close to degenerate," implying numerical precision errors might shuffle excited state order.
  - Hessian Approximation Error: If the model is not well-trained, the first-order term is non-zero, invalidating the quadratic objective.

- **First 3 experiments:**
  1. Validation Check: On a small model (e.g., 12 layers), compare "Predicted Loss Change" (x^T H x) vs. "Actual Loss Change" after pruning to validate the Taylor approximation.
  2. State Space Analysis: For a target compression (e.g., 25%), run the solver to output top 20 states. Plot "Energy" vs. "MMLU Score" to verify if correlation is strictly monotonic or if excited states diverge.
  3. Heterogeneous Test: Apply the method to a model with varying block types (e.g., MoE vs Dense) to see if the Hessian distinguishes their importance differently than simple magnitude-based pruning.

## Open Questions the Paper Calls Out

**Can CBO be combined with layer-merging techniques?** The authors state in Section 4.2, "Exploring how these techniques can be effectively combined is left for future work." This represents a promising direction for further compression gains beyond what either technique achieves alone.

**Can automated selection improve excited state identification?** Section 5 suggests, "similarity or diversity scores like the Pearson coefficient could be introduced to automatically select promising candidate configurations." Currently, the method relies on manual inspection to identify specific excited states that outperform the ground state.

**How does including first-order terms affect accuracy?** The paper neglects the first-order term (∇L(α_0) ≈ 0), in contrast to prior work where it improved results. This assumption may not hold for all architectures or calibration datasets.

## Limitations

- The method's effectiveness critically depends on the representativeness of the calibration dataset, which isn't fully explored in experiments.
- Numerical precision issues arise when energy levels are "close to degenerate," potentially affecting reproducibility.
- GSM8K performance is notably easier to recover through fine-tuning, suggesting pruning may be more damaging to specific capabilities than aggregate metrics indicate.

## Confidence

**High Confidence:** The core mechanism of using second-order Taylor expansion to approximate block interactions and formulate block removal as a constrained binary optimization problem.

**Medium Confidence:** The claim that low-energy excited states often outperform the ground state, as theoretical justification remains heuristic and stability across random seeds is unclear.

**Medium Confidence:** The generalization to heterogeneous architectures like MoE models, as results are promising but limited to a single heterogeneous model type.

**Low Confidence:** The assertion that the Hessian approximation method is computationally efficient enough for practical deployment on very large models, as timing benchmarks and scalability constraints are not discussed.

## Next Checks

**Validation Check 1:** Run ablation studies varying the calibration dataset composition. Test whether pruning decisions made using calibration data from a different domain (e.g., code data for a model evaluated on MMLU) lead to suboptimal or damaging block removals, quantifying the correlation between calibration data representativeness and pruning quality.

**Validation Check 2:** Implement numerical stability analysis by perturbing the Hessian computation with controlled noise levels. Determine at what noise threshold the excited state rankings become unstable, and assess whether the 17th excited state consistently outperforms the ground state across multiple random seeds.

**Validation Check 3:** Test the method on a dense model scaled to match the parameter count of the Nemotron MoE model. Compare whether the energy landscape and pruning decisions differ systematically between dense and sparse architectures, and whether the excited state phenomenon persists in purely dense models.