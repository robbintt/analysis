---
ver: rpa2
title: Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models
arxiv_id: '2509.26626'
source_url: https://arxiv.org/abs/2509.26626
tags:
- reasoning
- aggregation
- population
- solutions
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Recursive Self-Aggregation (RSA), a test-time
  scaling method for improving large language model reasoning by iteratively refining
  a population of candidate solutions. RSA combines the strengths of parallel and
  sequential scaling by maintaining a population of reasoning chains and recursively
  aggregating subsets of these chains to generate improved solutions.
---

# Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models

## Quick Facts
- arXiv ID: 2509.26626
- Source URL: https://arxiv.org/abs/2509.26626
- Reference count: 40
- Primary result: RSA enables a 4B parameter model to match performance of much larger reasoning models

## Executive Summary
This paper introduces Recursive Self-Aggregation (RSA), a test-time scaling method that iteratively refines reasoning solutions by aggregating diverse solution populations. RSA maintains N candidate solutions and recursively aggregates subsets of K candidates to generate improved solutions, achieving substantial performance gains on challenging reasoning tasks. The method combines the breadth of parallel sampling with the depth of sequential refinement, enabling smaller models to match larger reasoning models' performance. RSA is simple to implement, requires no external verifiers, and scales effectively with compute budgets.

## Method Summary
RSA maintains a population of N reasoning solutions and iteratively refines them through K-way aggregation. For each iteration, N random subsets of K candidates are selected, and the model is prompted to synthesize an improved solution by extracting correct reasoning fragments from each subset. This process repeats for T iterations, with the final answer sampled from the last population. The method leverages implicit verification through aggregation prompts rather than external verifiers. An aggregation-aware reinforcement learning variant trains models to better perform the aggregation task during inference.

## Key Results
- RSA with Qwen3-4B-Instruct achieves Pass@1 of 38.8% on AIME-25, competitive with much larger models
- RSA consistently outperforms both purely parallel (majority voting) and purely sequential (self-refinement) scaling methods
- Aggregation-aware RL fine-tuning further improves performance, especially on code reasoning tasks
- RSA enables effective test-time scaling for models up to 70B parameters

## Why This Works (Mechanism)

### Mechanism 1
Aggregating multiple reasoning chains enables bootstrapping from partially correct intermediate steps across different solution trajectories. RSA samples K solutions from N candidates and prompts the LLM to synthesize an improved solution by extracting correct reasoning fragments from each. The model implicitly identifies which intermediate steps are valid and combines them, discarding errors. This relies on the model's implicit verification capability to distinguish correct from incorrect reasoning when presented comparatively.

### Mechanism 2
Maintaining population size N larger than aggregation size K preserves reasoning diversity across iterations, enabling effective exploration-exploitation balance. Random subsampling creates different aggregation sets at each step, allowing high-quality reasoning patterns to propagate gradually through the population rather than all solutions converging immediately. This mirrors genetic algorithm dynamics where selective pressure must be balanced against diversity loss.

### Mechanism 3
Aggregation-aware RL training aligns the model's learned behavior with the test-time aggregation task, preventing distribution shift that degrades RSA performance. Standard RL trains models to generate correct solutions from scratch. Aggregation-aware RL additionally trains on prompts containing K candidate solutions, teaching the model to synthesize from provided reasoning chains. This is implemented via a 50-50 mix of standard and aggregation prompts during RLOO training.

## Foundational Learning

- Concept: **Test-time scaling taxonomy (parallel vs. sequential vs. hybrid)**
  - Why needed here: RSA is explicitly a hybrid method; understanding this positioning clarifies what tradeoffs it's designed to address.
  - Quick check question: Can you explain why purely parallel scaling wastes depth and purely sequential scaling wastes breadth?

- Concept: **Implicit verification in LLMs**
  - Why needed here: RSA relies on the model's ability to recognize correct reasoning when presented with alternatives, rather than using explicit reward models or verifiers.
  - Quick check question: What's the difference between a model verifying its own output (self-verification) and implicit verification through aggregation?

- Concept: **Evolutionary algorithm dynamics (crossover, selection pressure, diversity)**
  - Why needed here: RSA is explicitly motivated by evolutionary methods; the N/K/T parameters map directly to population size, crossover pool, and generations.
  - Quick check question: In genetic algorithms, what happens if selection pressure is too high relative to population size?

## Architecture Onboarding

- Component map: Population initializer -> Subsampler -> Aggregation prompt builder -> Iterative refinement loop -> Termination handler
- Critical path: `Initial sampling (N calls) → Subsample K → Aggregate (N calls) → Repeat T-1 times → Final selection`
- Design tradeoffs:
  - N vs K: N controls asymptotic quality; K controls mixing speed. With fixed compute (N×T), larger N requires proportionally larger K or T to converge.
  - K vs context length: K is bounded by model context window minus query and output tokens. Paper finds diminishing returns beyond K=3-4.
  - T vs latency: More iterations improve quality but increase latency linearly. Paper shows monotonic gains through T=10 on most tasks.
- Failure signatures:
  1. Diversity collapse: All N solutions converge to near-identical outputs before T steps → suggests K too large or N too small
  2. Stagnant Pass@1: Gap between Pass@N and Pass@1 doesn't close over iterations → suggests insufficient T or mixing issues
  3. RL degradation: Aggregation-aware model underperforms base → check training data contamination or insufficient aggregation prompt ratio
- First 3 experiments:
  1. Implement RSA with N=16, K=4, T=10 on AIME-25. Compare against self-refinement (K=1, N=1, T=10) and majority voting (N=160, T=1) to verify the paper's reported ~20-30 point Pass@1 improvements.
  2. Fix compute budget (N×T=160), vary N∈{4,8,16,32} with corresponding T. Plot Pass@1 vs. N to find the optimal balance for your target latency constraints.
  3. With N=16, T=10, test K∈{1,2,3,4}. Verify that K=1 (equivalent to parallel self-refinement) significantly underperforms K≥2, confirming the core mechanism that combining diverse chains matters.

## Open Questions the Paper Calls Out

### Open Question 1
Can combining RSA with explicit verification mechanisms (e.g., self-verification to filter low-quality candidates) further improve performance by introducing a fitness function to the evolutionary aggregation loop? The current RSA method relies solely on implicit verification through aggregation prompts, without any explicit scoring or filtering of candidates between iterations.

### Open Question 2
Can multi-step reinforcement learning be used to train models for the full end-to-end RSA procedure, rather than training only for single-step aggregation? The aggregation-aware RL approach in this paper only optimizes single-step aggregation, not the recursive multi-step procedure that characterizes RSA.

### Open Question 3
Does the aggregation-aware RL training generalize to domains not seen during training, and what factors determine transferability? The paper notes "massive gains on LiveCodeBench, despite the fact that our training dataset completely lacks any coding problems," suggesting strong out-of-domain transfer, but this observation is not systematically investigated.

### Open Question 4
What is the optimal stopping criterion for RSA iterations, given that diversity monotonically decreases and performance can degrade on some tasks after many steps? The paper shows monotonic improvement on most tasks but notes a "significant downward trend on Reasoning Gym Cognition + ARC after five steps," yet uses a fixed T=10 steps throughout.

## Limitations
- RSA fundamentally depends on LLM's implicit verification capability, which may fail when candidates share systematic misconceptions
- The method requires maintaining large populations (N=16) and multiple iterations (T=10), increasing inference costs
- Aggregation-aware RL shows mixed results and limited generalization, evaluated on only two models with limited task coverage

## Confidence
**High Confidence**: The empirical improvements on diverse benchmarks (AIME-25, HMMT-25, LiveCodeBench-v6, Reasoning Gym, SuperGPQA) are well-documented with statistically significant results. The core claim that RSA outperforms both purely parallel and sequential scaling methods is strongly supported by the data.

**Medium Confidence**: The mechanism claims about bootstrapping from partial solutions and maintaining diversity through population dynamics are logically sound but rely on unverified assumptions about LLM capabilities. The evolutionary algorithm analogy provides intuition but lacks formal analysis of convergence properties.

**Low Confidence**: The aggregation-aware RL approach shows mixed results and limited generalization. The claim that it enables massive gains on LiveCodeBench despite no coding in training data is impressive but based on only two model evaluations. The scalability of RSA to much larger models (>70B parameters) or its performance on non-reasoning tasks remains unknown.

## Next Checks
1. Apply RSA to a non-reasoning task like summarization or question answering where reasoning chains are less structured. Measure whether RSA still outperforms parallel scaling, validating that the aggregation mechanism works beyond explicit reasoning tasks.

2. Systematically vary N from 4 to 64 while maintaining constant compute (N×T) and measure the point of diminishing returns. This would establish theoretical bounds on RSA's effectiveness and clarify when larger populations genuinely improve performance versus simply adding noise.

3. Design adversarial benchmark sets where all K candidates share the same fundamental misconception. Test whether RSA degrades to the level of the worst candidate or if the aggregation mechanism can sometimes rescue correct reasoning. This would quantify the limits of implicit verification and identify scenarios where external verification remains necessary.