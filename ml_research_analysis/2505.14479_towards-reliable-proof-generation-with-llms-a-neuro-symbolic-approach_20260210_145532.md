---
ver: rpa2
title: 'Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach'
arxiv_id: '2505.14479'
source_url: https://arxiv.org/abs/2505.14479
tags:
- proof
- problems
- proofs
- problem
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a neuro-symbolic approach for generating formal
  mathematical proofs using LLMs, combining analogy retrieval and symbolic verification.
  Given a geometry problem, they abstract it, retrieve structurally similar problems
  with known proofs, and use these as few-shot examples to guide an LLM.
---

# Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach

## Quick Facts
- arXiv ID: 2505.14479
- Source URL: https://arxiv.org/abs/2505.14479
- Authors: Oren Sultan; Eitan Stern; Dafna Shahaf
- Reference count: 16
- One-line primary result: Neuro-symbolic approach combining analogy retrieval and iterative verification improves formal proof generation accuracy by 58%-70% over baseline on geometry problems.

## Executive Summary
This paper introduces a neuro-symbolic framework for generating formal mathematical proofs using large language models (LLMs). The approach combines analogy-based retrieval of structurally similar problems with iterative verification using a symbolic prover. Given a geometry problem, the system abstracts it, retrieves analogous problems with known proofs, and uses these as few-shot examples to guide an LLM. A symbolic verifier iteratively checks generated proofs and provides feedback until correctness is achieved or a retry limit is reached. Evaluated on the FormalGeo-7k dataset using OpenAI's o1 model, the method achieves significant accuracy improvements over baselines, demonstrating the effectiveness of combining LLM generation with structured verification.

## Method Summary
The approach involves abstracting geometry problems by replacing entities and numbers with placeholders, then retrieving structurally similar problems using a neural network trained to predict proof similarity based on Jaccard similarity of constructions, conditions, and goals. The system constructs a narrowed theorem dictionary containing only theorems used in the retrieved analogies, reducing the search space from 18K to 2.5K tokens on average. An LLM generates proofs using these few-shot examples and the narrowed dictionary, while a symbolic verifier (Z3) iteratively checks the proofs and provides tiered feedback (syntax, premise violations, goal entailment) to guide corrections through up to 5 retries.

## Key Results
- 58%-70% improvement in proof accuracy over baseline on FormalGeo-7k dataset
- Top-100 analogies achieve 96% theorem coverage with only 26.66 theorems on average (13.6% of full dictionary)
- Allowing retries results in average gains of 20% (analogy) to 28% (base) over single attempts
- Both analogy retrieval and verification contribute independently to accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1: Structure-Guided Retrieval for Search Space Reduction
Providing structurally analogous problems with known proofs restricts the LLM's search space to relevant theorems, improving accuracy and reducing token cost. The system abstracts problems and retrieves analogies using Jaccard similarity, constructing a narrowed theorem dictionary from analogies rather than the full 18K token dictionary. The core assumption is that structurally similar problems yield similar proof sequences. If the target problem requires a theorem not present in the top-k analogies (the remaining 4% uncovered), the narrowed dictionary prevents a correct solution unless the LLM hallucinates or generalizes unexpectedly.

### Mechanism 2: Iterative Verification with Tiered Feedback
An external symbolic verifier significantly increases proof success rates by identifying specific logical or syntax errors, allowing the LLM to iteratively self-correct. After the LLM generates a proof, a symbolic verifier checks for three error tiers: syntax violations, premise violations, and goal entailment. The system feeds the first detected error back as natural language context for a retry. The core assumption is that the LLM can map natural language error descriptions back into correct formal syntax and logic. If the retry limit (set to 5) is reached without resolving logical inconsistencies, the loop terminates unsuccessfully.

### Mechanism 3: Hybrid Neuro-Symbolic State Tracking
Decoupling the generation of proof steps from the validation of state allows the system to handle algebraic complexity that LLMs typically fail to track consistently. The LLM proposes a sequence of theorem applications, while the symbolic engine maintains the formal state via SMT constraints, ensuring every step strictly follows from premises. The core assumption is that the symbolic verifier can fully capture the domain's logic without gaps. The system explicitly notes a limitation where the Z3 prover lacks native support for inverse trigonometric functions, potentially failing on problems requiring derivation of angles from ratios.

## Foundational Learning

**Satisfiability Modulo Theories (SMT)**
- Why needed here: The core of the verifier relies on Z3, an SMT solver, to handle algebraic and logical constraints of geometry. Understanding SMT is required to debug why the verifier might reject a seemingly valid proof or fail to solve for a variable.
- Quick check question: Can you explain why an SMT solver might struggle with non-linear trigonometric identities compared to linear angle equalities?

**In-Context Learning (Few-Shot Prompting)**
- Why needed here: The system relies entirely on the LLM's ability to learn the proof pattern from the analogous problems provided in the prompt without weight updates.
- Quick check question: How does the ordering of few-shot examples potentially impact the LLM's attention mechanism and subsequent proof generation?

**Abstract Syntax Trees (AST) / Formal Grammars**
- Why needed here: The LLM must output proofs in a specific formal language (GDL) defined by a dictionary. Understanding the structure of this language is necessary to interpret Tier 1 (syntax) errors.
- Quick check question: Given the `THEOREM_SEQUENCE` format, what constitutes a syntax error versus a logical (premise) error?

## Architecture Onboarding

**Component map:**
Abstraction Module -> Analogy Retriever (3-layer MLP) -> Context Builder -> LLM (o1) -> Symbolic Verifier (Z3)

**Critical path:** The Retrieval Quality determines the Theorem Dictionary Coverage. If the retrieved analogies do not use the theorems required for the target problem, the LLM is effectively "blind" to the necessary logic, causing a Tier 2 (Missing Premise) or Tier 3 (Goal Not Reached) failure that cannot be fixed by retries.

**Design tradeoffs:**
- Simple vs. Semantic Abstraction: The authors chose a simple placeholder abstraction for robustness, sacrificing potential deep semantic matching for ease of implementation.
- Cost vs. Coverage: Increasing `k` (number of analogies) raises coverage and accuracy but increases token costs and context noise.
- Verifiability vs. Generality: The verifier is custom-built for geometry, making the architecture less portable to other domains like calculus without significant engineering.

**Failure signatures:**
- Tier 1 Loop: The LLM repeatedly hallucinates invalid theorem names or signatures.
- Tier 3 Stall: The proof is syntactically valid but under-constrained, and the LLM cannot deduce the missing constraint.
- Trigonometric Failure: The system fails silently or incorrectly on problems involving arcs or inverse functions.

**First 3 experiments:**
1. Ablation on `k`: Run the pipeline with k=[5, 10, 20, 50, 100] to verify the trade-off curve between token count and "Theorem Coverage" on a held-out set.
2. Error Tier Analysis: Isolate the verifier and feed it correct proofs with synthetic errors to validate that the feedback mechanism correctly identifies the specific tier and missing premise.
3. Cross-Domain Retrieval: Test the analogy retriever on a dataset like MathLib or a different formal logic set to see if structural similarity correlates with proof similarity outside of geometry.

## Open Questions the Paper Calls Out

**Generalizability to Other Domains:** "Our experiments focus on Euclidean geometry, and results may differ in other formal domains... further evaluation is needed to confirm its effectiveness beyond geometry."

**More Nuanced Abstraction Schemas:** "Exploring more nuanced abstraction schemas (e.g., ones that keep information about shared symbols between words) is left for future work."

**Effect of k Values:** "Exploring other values of k is left for future work" and "testing the effect of k in more depth is left for future work."

**Visual Diagram Integration:** The paper notes errors related to "No access to diagrams" and cites prior work showing multimodal LLMs struggle with visual math, creating unresolved tension about whether diagrams would help or hurt performance.

## Limitations

- Geometry-specific scope limits immediate generalizability to other formal domains
- Partial theoretical guarantees with empirical rather than proven coverage bounds
- Computational overhead from iterative verification loop not fully characterized
- Trigonometric limitations where Z3 prover lacks native support for inverse functions

## Confidence

**High Confidence:** The core claim that combining analogy retrieval with symbolic verification improves proof generation accuracy is strongly supported by the 58%-70% improvement over baseline and consistent ablation results.

**Medium Confidence:** The specific mechanisms are well-described, but exact implementation details (Z3 encoding schema, full prompt format) are not fully specified, limiting independent verification.

**Low Confidence:** The generalizability to non-geometric domains and long-term reliability of the iterative correction loop under diverse problem distributions remain speculative without further testing.

## Next Checks

1. **Coverage-Abstraction Trade-off:** Systematically vary abstraction depth and measure its impact on retrieval quality and final proof accuracy to validate the "simple abstraction" design choice.

2. **Error Tier Breakdown Analysis:** For a held-out set of problems, log the distribution of error tiers across all retries to identify whether the system gets "stuck" in a particular tier and if certain error types are more resistant to correction.

3. **Cross-Domain Retrieval Test:** Apply the analogy retriever and proof pipeline to a non-geometric formal dataset to assess whether structural similarity in conditions/goals correlates with proof similarity outside geometry.