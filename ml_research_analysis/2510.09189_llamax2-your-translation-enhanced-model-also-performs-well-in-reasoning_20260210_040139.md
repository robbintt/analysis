---
ver: rpa2
title: 'LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning'
arxiv_id: '2510.09189'
source_url: https://arxiv.org/abs/2510.09189
tags:
- translation
- multilingual
- languages
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen3-XPlus addresses the challenge of maintaining reasoning capabilities
  in translation-enhanced LLMs by introducing a layer-selective tuning approach. Unlike
  existing methods that rely on large-scale training data, this work uses a small
  amount of parallel data and applies selective fine-tuning on an instruct model,
  avoiding catastrophic forgetting.
---

# LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning

## Quick Facts
- arXiv ID: 2510.09189
- Source URL: https://arxiv.org/abs/2510.09189
- Authors: Changjiang Gao; Zixian Huang; Jingyang Gong; Shujian Huang; Lei Li; Fei Yuan
- Reference count: 40
- Primary result: Layer-selective tuning achieves 15+ spBLEU and 40+ xComet gains on low-resource languages while preserving reasoning capabilities

## Executive Summary
Qwen3-XPlus addresses the challenge of maintaining reasoning capabilities in translation-enhanced LLMs by introducing a layer-selective tuning approach. Unlike existing methods that rely on large-scale training data, this work uses a small amount of parallel data and applies selective fine-tuning on an instruct model, avoiding catastrophic forgetting. The proposed two-stage training process tunes bottom and top layers while freezing middle layers, preserving general reasoning abilities.

The approach significantly reduces complexity and enhances accessibility for multilingual enhancement. Qwen3-XPlus achieves over 15+ spBLEU and 40+ xComet points in low-resource languages such as Swahili, while maintaining reasoning performance comparable to the base Qwen3 model on 15 reasoning benchmarks.

## Method Summary
Qwen3-XPlus uses layer-selective tuning on Qwen3 instruct models through a two-stage training process. Stage 1 trains only the bottom 4 layers, while Stage 2 trains only the top 15 layers, with middle layers frozen throughout. The method uses 0.8B tokens from NLLB and OPUS-100 covering 17 languages, processed through six-step preprocessing including quality filtering and instruction formatting. Training uses 1 epoch per stage with LR 1e-5, cosine scheduler, and bf16 precision across 8Ã—H800 GPUs.

## Key Results
- Achieves over 15+ spBLEU and 40+ xComet gains on low-resource languages
- Maintains reasoning performance comparable to base Qwen3 model on 15 reasoning benchmarks
- Successfully avoids catastrophic forgetting while using minimal parallel data
- Effective across 17 languages including low-resource languages like Swahili

## Why This Works (Mechanism)
The layer-selective tuning approach works by isolating the translation enhancement to specific model layers while preserving the core reasoning capabilities in other layers. By freezing middle layers and only tuning bottom and top layers separately, the method prevents catastrophic forgetting of general reasoning abilities that plague existing translation enhancement approaches.

## Foundational Learning
- **Layer-wise training**: Selective tuning of specific model layers prevents interference with preserved capabilities. Quick check: Monitor gradient norms to verify only targeted layers are being updated.
- **Catastrophic forgetting**: Fine-tuning all layers simultaneously can erase previously learned capabilities. Quick check: Compare performance on reasoning tasks before and after fine-tuning.
- **Instruction formatting**: Structured prompts guide the model's behavior during fine-tuning. Quick check: Sample model outputs to verify instruction comprehension.
- **Quality filtering**: Removing low-quality translation pairs improves model performance. Quick check: Calculate loss percentiles against FLORES-101 dev set.
- **Multi-stage training**: Separate bottom and top layer tuning provides better control. Quick check: Validate layer freezing by examining parameter updates.

## Architecture Onboarding
- **Component map**: Qwen3-Instruct Base Model -> Bottom Layer Fine-tuning (Stage 1) -> Top Layer Fine-tuning (Stage 2) -> Qwen3-XPlus
- **Critical path**: Preprocessing -> Stage 1 training -> Stage 2 training -> Evaluation
- **Design tradeoffs**: Minimal data usage vs. potential underfitting; layer selectivity vs. comprehensive tuning
- **Failure signatures**: Performance drop on reasoning benchmarks indicates middle layer tuning or catastrophic forgetting
- **First experiments**: 1) Verify layer freezing implementation with gradient norm logging, 2) Test quality filtering thresholds on sample data, 3) Run single batch of Stage 1 training to validate setup

## Open Questions the Paper Calls Out
None

## Limitations
- Method effectiveness on truly low-resource scenarios beyond the reported 17 languages remains untested
- Claim of "matching" base model reasoning performance is relative to baseline Qwen3-8B-8192 limitations
- Absence of explicit instruction templates creates significant implementation risk
- Generalizability to other base models and language sets is unproven

## Confidence
- **High Confidence**: Translation enhancement methodology and catastrophic forgetting avoidance
- **Medium Confidence**: Preservation of reasoning capabilities claim
- **Low Confidence**: Generalizability to other base models and language sets

## Next Checks
1. **Layer Configuration Validation**: Verify layer freezing implementation by logging gradient norms per layer during a single batch. Only specified bottom 4 layers (Stage 1) or top 15 layers (Stage 2) should show non-zero gradients.
2. **Data Quality Verification**: Sample and manually evaluate 100 translated pairs from each target language to confirm quality filtering preserves adequate low-resource language data.
3. **Reasoning Benchmark Reproduction**: Independently run the 15 reasoning benchmarks on both base Qwen3-8B-Instruct and final Qwen3-XPlus model using identical evaluation protocols.