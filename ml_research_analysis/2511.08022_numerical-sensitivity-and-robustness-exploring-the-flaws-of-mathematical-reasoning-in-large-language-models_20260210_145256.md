---
ver: rpa2
title: 'Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical
  Reasoning in Large Language Models'
arxiv_id: '2511.08022'
source_url: https://arxiv.org/abs/2511.08022
tags:
- perturbation
- llms
- original
- reasoning
- house
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reveals the shortcomings and limitations of current LLMs
  in their reasoning capabilities through numerical sensitivity and robustness experiments.
  We propose a novel evaluation paradigm by injecting additional semantically irrelevant
  perturbation sentences and gradually increasing the perturbation intensity.
---

# Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.08022
- **Source URL:** https://arxiv.org/abs/2511.08022
- **Reference count:** 40
- **Key result:** Numerical perturbations cause 10-51% performance drops across models; instruction removal yields 20-40% accuracy, suggesting pattern matching over reasoning

## Executive Summary
This work reveals critical limitations in current LLMs' mathematical reasoning capabilities through controlled perturbation experiments. By injecting semantically irrelevant sentences with and without numerical content, the study demonstrates that models exhibit stable performance against non-numerical noise but suffer significant degradation from numerical distractors. The findings suggest models lack robust mechanisms for filtering irrelevant numerical information and may rely on pattern matching rather than genuine logical reasoning, as evidenced by maintained accuracy even when core problem instructions are removed.

## Method Summary
The study employs a novel evaluation paradigm that systematically injects perturbation sentences into mathematical problems from GSM8K and AIME25 benchmarks. Perturbations are generated with varying intensity levels (baseline to excessive) and types (numerical vs. non-numerical). A secondary method removes core questioning instructions while preserving problem context. Five LLMs are evaluated across 15 configurations using temperature 0.7 sampling, with accuracy measured against ground truth solutions.

## Key Results
- Models maintain stable performance against non-numerical perturbations but degrade significantly with numerical content (9-51% drop)
- Performance declines non-linearly as perturbation intensity increases, with numerical content causing steeper degradation
- When core questioning instructions are removed, models maintain 20-40% accuracy, suggesting pattern matching over reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs exhibit heightened sensitivity to numerical information in context, causing disproportionate performance degradation compared to non-numerical semantic perturbations.
- **Mechanism:** Numerical tokens trigger retrieval of associated arithmetic patterns from training data. When irrelevant numbers appear, models cannot reliably distinguish which numbers belong to the target problem, leading to incorrect integration of distractor values into calculations.
- **Core assumption:** Models lack a semantic filtering mechanism to separate task-relevant from task-irrelevant numerical information.
- **Evidence anchors:** [abstract] models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information; [section 4.2.1] Performance drops 9.63% (Qwen2.5-Math-7B) to 16.07% (max) with single numerical perturbation vs. near-zero with non-numerical; [corpus] Shrestha et al. (2025) confirms LLMs perform worse on larger numerical ranges.

### Mechanism 2
- **Claim:** Mathematical problem-solving behavior exhibits a pattern-matching signature that persists even when core task instructions are absent.
- **Mechanism:** Models match input patterns against memorized solution templates. When presented with problem context without the final query sentence, models still generate the canonical answer, suggesting the solution is retrieved rather than computed from the question structure.
- **Core assumption:** High accuracy on benchmark datasets partially reflects pattern overlap between training and test distributions.
- **Evidence anchors:** [abstract] in the absence of core questioning instruction, models can still maintain an accuracy of 20%-40%; [section 4.2.3] GPT-4o achieves 36.61% accuracy on problems with question removed; [corpus] Mirzadeh et al. (2024) shows models perform differently on same template with different numbers.

### Mechanism 3
- **Claim:** Robustness to perturbations degrades non-linearly as distractor density increases, with numerical content creating steeper decline curves.
- **Mechanism:** Attention mechanisms distribute computation across all context tokens. As distractor count increases, attention to task-relevant information dilutes proportionally. Numerical content may have higher baseline attention weights due to training emphasis on math problems.
- **Core assumption:** The model lacks explicit mechanisms to weight information relevance.
- **Evidence anchors:** [section 3.2] Five-level perturbation: baseline (1 sentence) → excessive (2× original sentence count); [section 4.2.2] Qwen2.5-Math-7B drops from 94.09% to 67.32% (26.77% decline) under maximum numerical perturbation; [corpus] Related work on adversarial perturbations confirms reasoning failures under adversarial prompts.

## Foundational Learning

- **Concept: Perturbation-based robustness evaluation**
  - **Why needed here:** The paper's core methodology. Understanding how to construct controlled input variations that stress-test specific capabilities without changing the ground-truth solution.
  - **Quick check question:** Can you distinguish between perturbations that preserve vs. alter the valid solution path?

- **Concept: Pattern matching vs. compositional reasoning**
  - **Why needed here:** Central to interpreting the "missing instruction" results. You must understand why template retrieval produces correct answers without explicit queries.
  - **Quick check question:** If a model outputs the right answer for a problem it wasn't asked to solve, what does that imply about its solution process?

- **Concept: Numerical token attention dynamics**
  - **Why needed here:** Explains why numerical perturbations cause disproportionate degradation. Numbers may have learned attention priors that make them harder to ignore.
  - **Quick check question:** How would you test whether numerical distractors are weighted differently than textual distractors in attention layers?

## Architecture Onboarding

- **Component map:** Original Problem → Perturbation Injector → Perturbed Problem → LLM → Response
- **Critical path:**
  1. GSM8K/AIME25 benchmark sampling
  2. Perturbation sentence generation (GPT-4 assisted for non-numerical; GSM8K extraction for numerical)
  3. Progressive injection (baseline → excessive)
  4. Model inference (temperature 0.7)
  5. Accuracy comparison across perturbation levels

- **Design tradeoffs:**
  - **Semantically irrelevant vs. relevant perturbations:** Paper chooses irrelevance to isolate attention/capacity limits from genuine problem complexity increases
  - **Numerical vs. non-numerical:** Disentangles token-type sensitivity from pure semantic noise effects
  - **Instruction removal vs. perturbation:** Tests memory-retrieval hypothesis through a different methodological lens

- **Failure signatures:**
  - Models incorporate distractor numbers into calculations (Figure 5: OpenAI-o3 computes all three embedded problems)
  - Correct answers on instruction-removed problems with identical reasoning traces to original
  - Linear or super-linear accuracy decay under increasing perturbation intensity

- **First 3 experiments:**
  1. **Replicate numerical vs. non-numerical baseline perturbation** on a 50-problem GSM8K subset with Qwen2.5-Math-7B. Verify the 9-10% gap.
  2. **Ablate perturbation position:** Insert distractors at beginning vs. middle vs. end of problem text to test attention distribution hypotheses.
  3. **Test instruction removal on synthetic problems** not in public benchmarks. If accuracy drops to near-zero, pattern-matching hypothesis weakened; if maintained, potential data leakage concern validated.

## Open Questions the Paper Calls Out

None

## Limitations

- Study relies exclusively on GSM8K and AIME25 benchmarks, which may not represent full diversity of mathematical reasoning tasks
- Perturbation generation uses GPT-4 for non-numerical distractors and GSM8K extraction for numerical ones, introducing potential generation bias
- Analysis focuses on accuracy metrics without examining quality distribution of incorrect answers, which could reveal different failure modes

## Confidence

- **High confidence** in the numerical sensitivity mechanism: Multiple models show consistent, large performance drops (9-51%) specifically with numerical perturbations
- **Medium confidence** in the pattern-matching hypothesis: While instruction-removal results are compelling (20-40% accuracy), alternative explanations exist including semantic similarity to training data
- **Medium confidence** in the non-linear degradation model: Experimental design shows clear trends, but exact functional form requires more granular measurement

## Next Checks

1. **Cross-dataset validation**: Test the same perturbation methodology on completely different mathematical reasoning benchmarks (e.g., MATH, SVAMP) to determine if numerical sensitivity is a general phenomenon or GSM8K-specific.

2. **Attention visualization**: Use attention weight analysis to directly observe whether numerical tokens receive disproportionately higher attention compared to non-numerical tokens during perturbed problem processing, providing mechanistic evidence for Mechanism 1.

3. **Template overlap quantification**: Analyze the overlap between perturbed problem structures and publicly available training corpora to establish whether high accuracy on instruction-removed problems reflects memorization rather than reasoning capability.