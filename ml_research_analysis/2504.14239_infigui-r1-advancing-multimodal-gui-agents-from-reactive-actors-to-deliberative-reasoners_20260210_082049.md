---
ver: rpa2
title: 'InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative
  Reasoners'
arxiv_id: '2504.14239'
source_url: https://arxiv.org/abs/2504.14239
tags:
- reasoning
- arxiv
- agent
- agents
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving reasoning capabilities
  in Multimodal Large Language Model (MLLM) based GUI agents. Current approaches often
  rely on manually designed templates or lack sufficient depth for complex tasks requiring
  planning and error recovery.
---

# InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners

## Quick Facts
- **arXiv ID:** 2504.14239
- **Source URL:** https://arxiv.org/abs/2504.14239
- **Reference count:** 40
- **Primary result:** Achieved state-of-the-art cross-platform grounding (87.5% on ScreenSpot) and 71.1% success rate on complex AndroidControl-High tasks using a 3B parameter model

## Executive Summary
The paper addresses the challenge of improving reasoning capabilities in Multimodal Large Language Model (MLLM) based GUI agents. Current approaches often rely on manually designed templates or lack sufficient depth for complex tasks requiring planning and error recovery. To overcome this, the authors propose the Actor2Reasoner framework, which aims to transform GUI agents from reactive actors to deliberative reasoners. This is achieved through a two-stage training approach: Stage 1, Reasoning Injection, employs Spatial Reasoning Distillation to inject foundational cross-modal reasoning capabilities, and Stage 2, Deliberation Enhancement, uses Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction to refine planning and reflection abilities. The resulting InfiGUI-R1-3B agent demonstrates strong performance, achieving state-of-the-art cross-platform grounding (87.5% average on ScreenSpot) and high success rates on complex trajectory tasks (71.1% on AndroidControl-High), even with a smaller 3B parameter model.

## Method Summary
The Actor2Reasoner framework transforms reactive GUI agents into deliberative reasoners through a two-stage process. Stage 1 employs Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models (QwQ-32B and Qwen2.5-VL-32B) to the student model (Qwen2.5-VL-3B) by fine-tuning on reasoning trajectories with explicit steps. Stage 2 uses Reinforcement Learning with RLOO algorithm, incorporating Sub-goal Guidance that rewards accurate intermediate sub-goals and Error Recovery Scenario Construction that creates failure-and-recovery training scenarios from identified error-prone steps. The training pipeline includes identifying reasoning bottlenecks, generating teacher reasoning trajectories, fine-tuning with SFT, constructing error scenarios, and RL optimization on mixed datasets including AndroidControl trajectories, ScreenSpot grounding, and VQA/detection data.

## Key Results
- Achieved 87.5% average cross-platform grounding accuracy on ScreenSpot benchmark
- Reached 71.1% success rate on complex AndroidControl-High trajectory tasks
- Demonstrated superior performance with a 3B parameter model compared to larger models on reasoning-intensive tasks

## Why This Works (Mechanism)

### Mechanism 1: Spatial Reasoning Distillation Breaks Direct Perception-Action Links
The framework inserts explicit reasoning steps between perception and action through teacher-student distillation, enabling the model to integrate GUI visual-spatial information before generating actions rather than relying on implicit pattern matching. This is implemented by identifying "reasoning bottleneck samples" where the base model fails on full tasks but succeeds with ground-truth sub-goals, then fine-tuning the student model to predict both reasoning and action patterns.

### Mechanism 2: Sub-goal Guidance Provides Intermediate Reward Signal for Planning
During RL training, the model is rewarded for generating accurate intermediate sub-goals extracted from reasoning text, providing forward-looking task decomposition signals even when final action execution fails. This conditional reward structure offers targeted feedback on planning quality when the agent struggles with accurate action execution.

### Mechanism 3: Error Recovery Scenario Construction Trains Reflective Correction
The framework constructs synthetically failure-and-recovery scenarios at identified error-prone steps, training the model to recognize error states and execute corrective strategies. Two scenario types are created: Error Escape scenarios where the erroneous next observation is presented with escape action rewards, and Back on Track scenarios where the original observation with error history is presented with correct action rewards.

## Foundational Learning

- **Concept: Policy Gradient RL (REINFORCE family)**
  - **Why needed here:** Stage 2 uses RLOO algorithm, a variant of REINFORCE with leave-one-out baseline, to optimize the agent's policy without training a separate critic model
  - **Quick check question:** Can you explain why RLOO's baseline (average reward of other samples in batch) reduces variance compared to vanilla REINFORCE without requiring a value function?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** Stage 1 relies on distilling reasoning capabilities from larger teacher models (Qwen2.5-VL-32B, QwQ-32B) to the smaller 3B student through supervised fine-tuning on generated reasoning trajectories
  - **Quick check question:** What is the difference between distilling reasoning steps versus distilling only final outputs, and why might the former be important for GUI tasks?

- **Concept: Reward Shaping with Intermediate Signals**
  - **Why needed here:** The framework's sub-goal reward and multi-component accuracy reward (R_type, R_param, R_subgoal) are forms of reward shaping that provide denser feedback than sparse success/failure signals
  - **Quick check question:** Why might rewarding sub-goal quality even when the action fails help learning, and what risks does this introduce?

## Architecture Onboarding

- **Component map:** Base MLLM (Qwen2.5-VL-3B-Instruct) -> Teacher models (Qwen2.5-VL-32B-Instruct for spatial compression, QwQ-32B for reasoning) -> Scoring LLM (lightweight sub-goal evaluation) -> RL infrastructure (RLOO implementation) -> Data pipeline (AndroidControl, ScreenSpot, VQA/detection)

- **Critical path:** 1) Identify reasoning bottleneck samples using base model, 2) Generate teacher reasoning trajectories, 3) Stage 1 SFT training on distilled dataset, 4) Identify error-prone steps via sampling, 5) Construct error recovery scenarios, 6) Stage 2 RL training with mixed data, 7) Evaluate on ScreenSpot and AndroidControl

- **Design tradeoffs:** 3B vs larger models for deployment efficiency requiring careful training; rule-based rewards vs learned reward models to avoid critic training but potentially miss nuanced signals; synthetic vs real failure data for scalability but potential transfer issues; two-stage vs end-to-end training for stable foundation-building but added complexity

- **Failure signatures:** Stage 1 produces verbose but ungrounded reasoning indicating spatial compression/distillation alignment failure; RL reward plateaus early suggesting scoring LLM inconsistency or reward weight imbalance; model escapes correctly but never recovers indicating insufficient "Back on Track" scenarios; performance degrades on held-out platforms suggesting overfitting to training platforms

- **First 3 experiments:** 1) Ablation of Stage 1 distillation to isolate reasoning injection contribution, 2) Sub-goal reward weight sweep to find optimal balance, 3) Error scenario proportion analysis to identify saturation point

## Open Questions the Paper Calls Out
None

## Limitations
- The quality and reliability of teacher model reasoning trajectories remains unverified, risking the student inheriting flawed patterns if teachers generate superficially plausible but incorrect reasoning
- Sub-goal scoring LLM consistency is critical but not evaluated, potentially corrupting the RL reward signal and degrading planning capabilities
- Error recovery scenario construction assumes synthetic failures generalize to real ones, but this transfer is unproven without systematic analysis of actual failure modes

## Confidence
- **High:** Cross-platform grounding performance claims (87.5% on ScreenSpot) - straightforward metric evaluations
- **Medium:** Trajectory success rates (71.1% on AndroidControl-High) - complex end-to-end evaluation with potential measurement artifacts
- **Low:** Specific contribution of each training component (reasoning distillation, sub-goal guidance, error recovery) - cannot be isolated without proper ablation studies

## Next Checks
1. **Teacher reasoning quality audit:** Manually examine 50 randomly selected teacher-generated reasoning trajectories for logical consistency, spatial accuracy, and relevance to actual GUI actions
2. **Sub-goal scoring validation:** Evaluate the scoring LLM's inter-rater reliability by computing agreement scores on 200 reasoning samples with human ground truth sub-goals
3. **Error scenario coverage analysis:** Compare the distribution of synthetic error scenarios to actual failure patterns observed during agent deployment to quantify transfer validity