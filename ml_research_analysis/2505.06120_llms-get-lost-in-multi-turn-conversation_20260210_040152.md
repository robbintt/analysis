---
ver: rpa2
title: LLMs Get Lost In Multi-Turn Conversation
arxiv_id: '2505.06120'
source_url: https://arxiv.org/abs/2505.06120
tags:
- conversation
- instruction
- multi-turn
- sharded
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the performance of large language models\
  \ (LLMs) in multi-turn, underspecified conversations. It finds that all tested models\u2014\
  from small open-weight to state-of-the-art closed-weight LLMs\u2014exhibit a significant\
  \ drop in performance in multi-turn settings compared to single-turn, with an average\
  \ degradation of 39%."
---

# LLMs Get Lost In Multi-Turn Conversation

## Quick Facts
- arXiv ID: 2505.06120
- Source URL: https://arxiv.org/abs/2505.06120
- Reference count: 40
- Primary result: All tested LLMs show 39% average performance drop in multi-turn vs single-turn conversations

## Executive Summary
This study reveals a significant performance degradation in large language models during multi-turn conversations, with an average 39% drop in accuracy compared to single-turn settings. The research demonstrates that this degradation stems primarily from reliability issues rather than aptitude loss, with models making premature assumptions and struggling to recover when given new information. The phenomenon affects models across the spectrum, from small open-weight models to state-of-the-art closed-weight systems.

The paper introduces a novel "sharded simulation" framework to systematically evaluate how LLMs handle underspecified conversations, revealing that models often become "lost in conversation" by making assumptions, relying too heavily on previous incorrect answers, and failing to adapt when new context is provided. This reliability crisis in multi-turn settings presents a critical challenge for deploying LLMs in real-world conversational applications.

## Method Summary
The researchers developed a "sharded simulation" framework that systematically tests LLMs on underspecified conversational tasks. This approach involves breaking down conversations into discrete turns where models must handle ambiguity and incomplete information. The evaluation measures both aptitude (ability to solve the task) and reliability (consistency in handling ambiguous situations) across single-turn and multi-turn scenarios, revealing the specific ways models fail to maintain performance as conversation complexity increases.

## Key Results
- All tested LLMs (small open-weight to state-of-the-art closed-weight) show 39% average performance degradation in multi-turn settings
- Performance drop primarily driven by reliability issues, not aptitude loss
- Models exhibit three main failure modes: premature assumptions, overreliance on previous incorrect answers, and difficulty recovering with new information

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Sharded simulation**: A framework for systematically evaluating conversational performance by breaking conversations into discrete turns with controlled ambiguity
  - Why needed: To isolate and measure specific failure modes in multi-turn conversations
  - Quick check: Verify the framework can reproduce known conversational challenges

- **Underspecified conversation**: Conversational scenarios where information is incomplete or ambiguous, requiring clarification
  - Why needed: Real-world conversations often involve uncertainty that models must navigate
  - Quick check: Test with both highly ambiguous and clearly specified prompts

- **Aptitude vs reliability metrics**: Dual evaluation framework measuring both task-solving ability and consistency in handling ambiguity
  - Why needed: Distinguish between fundamental capability loss and consistency issues
  - Quick check: Apply to both single-turn and multi-turn scenarios to validate separation

## Architecture Onboarding

Component Map:
Sharded Simulation Framework -> Evaluation Pipeline -> Performance Metrics (Aptitude + Reliability) -> Failure Mode Analysis

Critical Path:
1. Generate underspecified prompts
2. Execute sharded conversation simulation
3. Measure performance degradation across turns
4. Analyze failure modes and reliability drops

Design Tradeoffs:
- Systematic control vs. natural conversation realism
- Quantitative metrics vs. qualitative behavioral insights
- Single-turn comparison baseline vs. alternative evaluation methods

Failure Signatures:
- Premature assumption of missing context
- Overreliance on incorrect previous answers
- Inability to incorporate new clarifying information

First Experiments:
1. Test same framework on non-underspecified (fully specified) conversations
2. Vary temperature settings systematically to isolate temperature effects
3. Implement explicit recovery prompts to test mitigation strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "lost in conversation" phenomenon and associated reliability degradation occur in open-ended, creative tasks?
- Basis in paper: [explicit] The authors state in Section 9 that they restricted experiments to analytical tasks and that "Determining whether degradation occurs – and if so, identifying the magnitude – on creative tasks is an important direction for future work."
- Why unresolved: Creative writing evaluation is still an active area of research, making it difficult to measure the "aptitude" and "reliability" metrics used in this study.
- What evidence would resolve it: Applying the sharded simulation framework to creative writing benchmarks (e.g., story generation) using human or model-based evaluation criteria for coherence and style.

### Open Question 2
- Question: Do specific task properties, such as solution non-decomposability and high complexity, predict the severity of multi-turn performance degradation?
- Basis in paper: [explicit] Section 7.3 hypothesizes that "LLMs tested on tasks with the aforementioned three properties [generative, complex, non-decomposable] will likely get lost in conversation."
- Why unresolved: The study tested a limited set of tasks and found translation (episodic/decomposable) did not degrade, but a systematic ablation of these specific properties was not performed.
- What evidence would resolve it: A controlled experiment varying task complexity and decomposability across a wider range of tasks to isolate their effect on the "lost" metric.

### Open Question 3
- Question: Can multi-turn reliability be improved at default temperatures (T=1.0) through specific training or architectural modifications?
- Basis in paper: [explicit] Section 7.2 issues a call to action for LLM builders to "prioritize the reliability of the models they build" and achieve low unreliability "at unmodified temperature (T=1.0)."
- Why unresolved: The paper demonstrates that lowering temperature is ineffective for multi-turn settings, leaving the method for achieving intrinsic reliability unknown.
- What evidence would resolve it: Training models with specific multi-turn consistency rewards or architectural changes, then measuring the unreliability metric (U) in the sharded simulation environment.

### Open Question 4
- Question: Does the performance degradation observed in multi-turn conversations generalize to non-English languages and multimodal inputs?
- Basis in paper: [explicit] Section 9 identifies the focus on "text-only tasks in the English language" as a limitation and suggests establishing the scope of degradation in other settings is necessary.
- Why unresolved: It is unknown if the linguistic structures of other languages or the integration of visual/audio data mitigates or exacerbates the "premature assumption" behaviors.
- What evidence would resolve it: Adapting the sharded simulation and evaluation metrics for multilingual benchmarks or multimodal tasks (e.g., image-based coding).

## Limitations

- The study focuses on a specific type of underspecified conversations that may not generalize to all multi-turn conversational scenarios
- Sample size of conversations tested may not capture the full spectrum of conversational complexities encountered in real-world applications
- Analysis does not deeply explore other potential contributing factors such as model architecture limitations or training data biases

## Confidence

- High confidence: The finding that all tested LLMs show performance degradation in multi-turn settings compared to single-turn (39% average drop) is well-supported by experimental results
- Medium confidence: The characterization of performance degradation as primarily due to unreliability rather than aptitude loss is plausible but could benefit from additional validation
- Medium confidence: The identification of specific failure modes (premature assumptions, overreliance on previous answers, difficulty recovering with new information) is based on qualitative analysis that may miss nuances

## Next Checks

1. **Generalization across prompt types**: Validate findings using broader range of underspecified prompt categories beyond tested ones, including different domains, complexity levels, and types of ambiguity

2. **Alternative attribution analysis**: Conduct controlled experiment comparing models with identical architectures but different training regimes (pre-trained only vs. fine-tuned on conversational data) to isolate whether degradation stems from architectural limitations or lack of appropriate fine-tuning

3. **Recovery mechanism evaluation**: Design systematic study testing various recovery strategies (explicit context resetting, meta-prompting for self-correction, multi-stage reasoning prompts) to quantify which approaches are most effective at mitigating the "lost in conversation" phenomenon