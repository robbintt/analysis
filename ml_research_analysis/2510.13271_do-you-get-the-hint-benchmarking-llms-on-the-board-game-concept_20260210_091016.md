---
ver: rpa2
title: Do You Get the Hint? Benchmarking LLMs on the Board Game Concept
arxiv_id: '2510.13271'
source_url: https://arxiv.org/abs/2510.13271
tags:
- clues
- concept
- llms
- game
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Concept, a word-guessing board game, as a
  benchmark to evaluate large language models' (LLMs) abductive reasoning abilities
  in a natural language setting. The game involves one player providing clues from
  a predefined set to describe a concept, while another player must guess the concept.
---

# Do You Get the Hint? Benchmarking LLMs on the Board Game Concept

## Quick Facts
- arXiv ID: 2510.13271
- Source URL: https://arxiv.org/abs/2510.13271
- Authors: Ine Gevers; Walter Daelemans
- Reference count: 15
- Primary result: LLMs struggle with abductive reasoning in Concept, with no model exceeding 40% success rate.

## Executive Summary
This study introduces Concept, a word-guessing board game, as a benchmark to evaluate large language models' (LLMs) abductive reasoning abilities in a natural language setting. The game involves one player providing clues from a predefined set to describe a concept, while another player must guess the concept. The researchers collected game logs in English, Dutch, French, and Spanish, and tested seven state-of-the-art LLMs using both static and dynamic prompting approaches. Results showed that LLMs struggled significantly compared to humans, with no model exceeding a 40% success rate. Specifically, models had difficulty interpreting strategic intent from clues and updating initial hypotheses with new information. Performance was consistently worse in lower-resource languages, and even reasoning models did not outperform non-reasoning models. The findings highlight LLMs' limitations in abstract reasoning tasks that require combining symbolic clues and adapting to sequential information updates.

## Method Summary
The study evaluates seven state-of-the-art LLMs on the Concept board game using both static and dynamic prompting approaches. Game logs were collected from Board Game Arena in English, French, Dutch, and Spanish, filtered for completeness. Prompts were structured hierarchically by color groups and appended with human incorrect guesses. Models were tested with fixed temperature and output token limits, and performance was measured using exact match precision at k. The dataset and prompts are available for reproducibility.

## Key Results
- LLMs significantly underperform humans on Concept across all languages.
- Static and dynamic prompting yield similar performance; dynamic prompting is harder due to sequential hypothesis revision.
- Models struggle with sequential hypothesis revision and category compliance, often repeating incorrect guesses.
- Performance is consistently worse in lower-resource languages.
- Reasoning models do not outperform non-reasoning models, suggesting extended reasoning chains may amplify early errors.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can perform associative lookup on individual clues but fail to integrate them into a coherent hypothesis that reflects strategic intent.
- **Mechanism:** The model retrieves candidate concepts matching each clue independently, but lacks a mechanism to weight and combine these into a constrained semantic search that reflects what a human clue-giver would plausibly intend.
- **Core assumption:** Abductive reasoning requires maintaining multiple competing hypotheses and pruning them as new evidence arrives—this is not natively supported by next-token prediction.
- **Evidence anchors:**
  - [abstract] "LLMs struggle with interpreting other players' strategic intents, and with correcting initial hypotheses given sequential information updates."
  - [section 4.2] "LLMs often get stuck pursuing wrong directions, whereas humans are better at revising their earlier interpretations."
  - [corpus] Related work on Codenames (arXiv:2502.11707) confirms LLMs struggle with lateral thinking and ToM in word games.
- **Break condition:** If models were simply missing knowledge, retrieval-augmented approaches would close the gap. The fact that adding more information (dynamic prompting) *decreases* performance suggests the failure is in hypothesis coordination, not knowledge access.

### Mechanism 2
- **Claim:** Hierarchical clue structures require explicit relational reasoning that degrades when flattened into linear prompts.
- **Mechanism:** Each color-group in Concept encodes a sub-hierarchy (pawn → cubes). The model must bind low-level clues to their parent high-level clue, not to the target concept directly. Without explicit structure-preserving representations, the model treats all clues as siblings.
- **Core assumption:** Chain-of-thought prompting approximates relational reasoning, but only when the model has learned to emit intermediate binding steps.
- **Evidence anchors:**
  - [section 1] "Low-level clues should be interpreted in relation to the high-level clue of that same color."
  - [section 3.2] "We organize the clues from Player 1 in a hierarchical structure (grouping clues of the same color together), since tests on the separate validation set showed that this technique results in better performance."
  - [corpus] No direct corpus evidence on hierarchical prompt structures for games; this remains underexplored.
- **Break condition:** If hierarchical reorganization did not improve performance at all, the mechanism would be purely about knowledge, not structure. The improvement observed suggests partial but incomplete leverage.

### Mechanism 3
- **Claim:** Reasoning models' extended chain-of-thought does not help because it amplifies early incorrect associations rather than correcting them.
- **Mechanism:** Longer reasoning traces provide more tokens for the model to rationalize its initial (incorrect) hypothesis, not to explore alternatives. This matches findings that reasoning models sometimes underperform on tasks requiring belief revision.
- **Core assumption:** Reasoning models optimize for coherence, not for exploration or hypothesis falsification.
- **Evidence anchors:**
  - [section 4.2.3] "The reasoning model (GPT-OSS-120B) does not outperform non-reasoning models... longer reasoning processes does not necessarily help the model to arrive at useful associations or correcting the erroneous directions."
  - [section 4.2] "Models often repeat their previous (incorrect) answer, even though the prompt is updated with this information."
  - [corpus] Hassid et al. (2025, cited in paper) find shorter thinking chains can improve reasoning—a pattern consistent with over-anchoring.
- **Break condition:** If reasoning models performed better on dynamic prompts where sequential updates are explicit, the mechanism would be about context length, not belief revision. They perform worse.

## Foundational Learning

- **Concept: Abductive Reasoning**
  - **Why needed here:** Unlike deduction (given rules → conclusion) or induction (given examples → rule), abduction requires inferring the *most plausible explanation* from incomplete evidence. This is the core cognitive demand of Concept.
  - **Quick check question:** Given clues "Building" + "Metal" + "France," why is "Eiffel Tower" more plausible than "generic factory"? What if you later learn "Blue, White, Red"—does your hypothesis change?

- **Concept: Theory of Mind in Games**
  - **Why needed here:** To guess correctly, the player must model what the clue-giver *intends* to communicate, not just what the clues literally denote. This requires representing another agent's knowledge and goals.
  - **Quick check question:** If a clue-giver marks "Animal" and "Stripes," why might they mean "zebra" not "tiger"? What does this reveal about their likely knowledge vs. yours?

- **Concept: Static vs. Dynamic Evaluation**
  - **Why needed here:** The paper tests both "all clues at once" (static) and "clues revealed sequentially with feedback" (dynamic). Understanding why dynamic is harder reveals limits in sequential belief updating.
  - **Quick check question:** If you guess wrong and are told "no," what should change in your next guess? How would you formalize this as an update rule?

## Architecture Onboarding

- **Component map:** Game logs -> Filter rounds -> Format clues hierarchically -> Apply prompt templates -> Run inference -> Normalize outputs -> Exact match evaluation

- **Critical path:**
  1. Load game logs -> filter rounds per Table 1 (exclude empty clues; optionally filter incomplete rounds where the initial clue is missing at game end)
  2. Format clues hierarchically (color groups, high/low level distinction)
  3. For static: present full clue set, allow 10 guesses
  4. For dynamic: step through clue reveals aligned with human gameplay timing
  5. Normalize output (ASCII fixes, prefix stripping) -> exact match evaluation

- **Design tradeoffs:**
  - Exact match is strict (synonyms rejected) but enables clear human baseline comparison
  - Single prompt across models sacrifices per-model optimization for reproducibility
  - Translating prompts to target language outperformed English instructions (counterintuitive but validated)

- **Failure signatures:**
  1. **Repetition loops:** Model repeats incorrect guess even when told it's wrong
  2. **Category violations:** Guessing "food" when top-level clue is "building"
  3. **Early anchoring:** First guess sets direction; subsequent clues fail to redirect
  4. **Output format drift:** Model ignores instruction to output only the concept, hits token limit

- **First 3 experiments:**
  1. **Baseline replication:** Run static prompt on English data with GPT-4.1-mini and LLaMA-3.3-70B; confirm ~30-40% success rate
  2. **Ablation by clue count:** Plot success rate vs. number of clues provided; expect negative correlation for models (more clues ≠ better) vs. humans
  3. **Hypothesis tracking:** Log model's first guess and whether final correct answer (if any) shares semantic category; quantify how often models switch categories vs. stay anchored

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset's filtering criteria for "valid" rounds are underspecified beyond removing rounds with empty clues; it is unclear whether incomplete rounds (e.g., early exits) are excluded, which could affect reported success rates.
- The exact mapping from BGA's internal clue hierarchy to the hierarchical prompt structure is not documented, raising concerns about reproducibility.
- The study uses exact match evaluation, which may penalize correct but semantically equivalent guesses (e.g., "France" vs. "French"), though the authors acknowledge this limitation and normalize outputs.
- The paper does not report per-language per-model variance or confidence intervals, making it difficult to assess whether differences between models or languages are statistically significant.

## Confidence

- **High confidence:** LLMs significantly underperform humans on Concept across all languages; static and dynamic prompting yields similar performance; models struggle with sequential hypothesis revision and category compliance.
- **Medium confidence:** Reasoning models do not outperform non-reasoning models, and hierarchical prompt structuring provides marginal but inconsistent gains; the claim that these failures reflect abductive reasoning deficits rather than simple knowledge gaps is plausible but not definitively proven.
- **Low confidence:** The explanation that LLMs' failure is due to inability to maintain and update multiple competing hypotheses is theoretically motivated but lacks direct causal evidence; the assertion that models get "stuck" due to early anchoring is based on observed behavior but not rigorously validated.

## Next Checks

1. **Category compliance audit:** Re-run the evaluation, logging whether each model's first guess violates the highest-level clue's category (e.g., guessing a food when the first clue is "Building"). Quantify the proportion of such violations to test if early anchoring to the wrong semantic category is a dominant failure mode.
2. **Hypothesis switching analysis:** For dynamic prompts, track whether the model's final correct answer (if any) belongs to the same semantic category as its first guess. Measure the proportion of cases where models switch categories vs. remain anchored, and compare with human patterns.
3. **Per-language per-model significance testing:** Re-analyze the results to compute 95% confidence intervals for success rates by model and language. Test whether differences in performance (e.g., between English and Dutch, or between GPT-4.1-mini and LLaMA3.3-70B) are statistically significant.