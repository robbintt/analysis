---
ver: rpa2
title: 'GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents'
arxiv_id: '2601.09770'
source_url: https://arxiv.org/abs/2601.09770
tags:
- arxiv
- visual
- tool
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of visual grounding in GUI agents
  by introducing a reinforcement learning framework that enables active visual perception.
  Unlike static approaches, the model learns to decide when and how to invoke visual
  tools (e.g., cropping or zooming) through a two-stage reasoning process.
---

# GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents

## Quick Facts
- **arXiv ID**: 2601.09770
- **Source URL**: https://arxiv.org/abs/2601.09770
- **Reference count**: 5
- **Primary result**: GUI-Eyes-3B achieves 44.8% grounding accuracy on ScreenSpot-Pro using only 3k labeled samples

## Executive Summary
This paper addresses the challenge of visual grounding in GUI agents by introducing a reinforcement learning framework that enables active visual perception. Unlike static approaches, the model learns to decide when and how to invoke visual tools (e.g., cropping or zooming) through a two-stage reasoning process. A progressive perception strategy coordinates coarse exploration and fine-grained grounding, guided by a two-level policy. To support learning, a spatially continuous reward function is designed, combining location proximity and region overlap to alleviate reward sparsity. Evaluated on the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines, demonstrating strong sample efficiency and generalization.

## Method Summary
The method introduces a two-stage reasoning process where Stage 1 predicts an initial grounding point and decides whether/how to invoke visual tools (crop/zoom) with parameters. The resulting transformed image feeds Stage 2, which performs refined prediction. This creates a perception–reasoning–perception loop that concentrates model capacity on task-relevant regions. Training uses Group Relative Policy Optimization (GRPO) with a multi-factor reward combining accuracy (0.6), format compliance (0.1), and tool usage (0.3), where the tool reward integrates center proximity and region overlap. The base model is Qwen2.5-VL-3B, trained on 3,000 samples from OS-Atlas, OS-Genesis, GUI-R1, and AndroidControl datasets.

## Key Results
- GUI-Eyes-3B achieves 44.8% grounding accuracy on ScreenSpot-Pro with only 3k training samples
- Outperforms both supervised and RL-based baselines on the benchmark
- Demonstrates strong sample efficiency compared to methods requiring larger training sets
- Shows adaptive tool-use behavior, with the model learning when to invoke tools versus direct prediction

## Why This Works (Mechanism)

### Mechanism 1: Progressive Perception Decomposition
Decomposing visual grounding into coarse exploration (Stage 1) and fine-grained localization (Stage 2) improves accuracy on complex, high-resolution interfaces. Stage 1 predicts an initial grounding point and decides whether/how to invoke visual tools (crop/zoom) with parameters. The resulting transformed image feeds Stage 2, which performs refined prediction. This creates a perception–reasoning–perception loop that concentrates model capacity on task-relevant regions. Core assumption: The base VLM has sufficient latent spatial understanding that can be unlocked by providing focused visual input rather than full screenshots.

### Mechanism 2: Spatially Continuous Reward for Dense Supervision
A reward function combining center proximity and region overlap provides gradient-like feedback that accelerates policy learning compared to sparse binary success signals. R_tool = λ_center · exp(-α(d(c,gt_bbox)/σ)²) + λ_overlap · IoU(crop_bbox, gt_bbox). The exponential proximity term gives partial credit for near-miss center predictions; the overlap term rewards crop regions that include the target. This addresses reward sparsity endemic to GUI environments where success is typically all-or-nothing. Core assumption: The reward shaping genuinely guides toward correct behavior rather than gaming.

### Mechanism 3: Learned Tool Policy vs. Fixed Heuristics
Dynamically learning when and how to apply visual tools outperforms static cropping strategies. The policy jointly learns (a) whether tool invocation is necessary for a given task and (b) optimal tool parameters (center, size, zoom scale). This allows adaptive behavior—skipping tools for easy targets, aggressive cropping for cluttered regions. Core assumption: The 3K training samples provide sufficient coverage of tool-use scenarios for generalization.

## Foundational Learning

- **Concept: Policy Gradient Methods (GRPO variant)**
  - Why needed here: Training uses Group Relative Policy Optimization, an agent-centric PPO variant computing advantages via within-batch normalization rather than value function estimation.
  - Quick check question: Can you explain why GRPO's advantage computation (Equation 3) normalizes rewards across sampled responses rather than using a learned critic?

- **Concept: Vision-Language Model Coordinate Prediction**
  - Why needed here: The base model (Qwen2.5-VL-3B) must output discrete tokens that encode continuous (x, y) coordinates. Understanding this tokenization is essential for debugging grounding failures.
  - Quick check question: How does a VLM represent spatial coordinates in its output vocabulary, and what failure modes does this introduce?

- **Concept: Reinforcement Learning Reward Shaping**
  - Why needed here: The paper's core contribution is a carefully-shaped reward combining three terms. Understanding reward design trade-offs (dense vs. sparse, proxy vs. ground-truth) is prerequisite to extending this work.
  - Quick check question: What risks arise when using proxy rewards (like region overlap) that don't perfectly correlate with task success?

## Architecture Onboarding

- **Component map:**
  ```
  [Input: Screenshot + Instruction]
           ↓
  [Stage 1 Prompt] → [VLM Backbone] → [Tool Decision: crop/zoom/direct + parameters]
           ↓                                    ↓
  [Visual Tool Application] ←───────── [If tool selected: generate cropped/zoomed patch]
           ↓
  [Stage 2 Prompt + Transformed Image] → [VLM Backbone] → [Final coordinate prediction]
           ↓
  [Reward Computation: R_acc + R_format + R_tool]
           ↓
  [GRPO Policy Update via Equation 4]
  ```

- **Critical path:** The Stage 1 → Tool Application → Stage 2 chain. If tool parameters are miscalibrated (e.g., crop region misses target entirely), Stage 2 receives uninformative input and cannot recover.

- **Design tradeoffs:**
  - Two-stage vs. single-stage: Adds inference overhead (~2x forward passes) but enables active perception
  - λ_acc vs. λ_tool weights (Table 3): Higher λ_acc (0.6) prioritizes final accuracy; higher λ_tool encourages tool-use learning but risks reward hacking
  - Crop vs. Zoom tools: Share parameters but zoom provides pixel-level detail while crop reduces context

- **Failure signatures:**
  - Model always predicts `crop_size: [0, 0]` (skips tools): May indicate λ_tool too low or reward not propagating
  - Tool centers consistently far from ground-truth: Check σ normalization in Equation 2 matches your data scale
  - Format errors in action output: R_format not sufficiently penalizing malformed outputs

- **First 3 experiments:**
  1. **Baseline ablation:** Train with λ_tool = 0 (no tool reward) vs. full reward on a held-out validation split. Confirm progressive inference requires explicit tool-use supervision.
  2. **Reward coefficient sweep:** Replicate Table 3 on your target domain. Optimal λ values may shift depending on interface complexity (e.g., mobile vs. desktop vs. CAD).
  3. **Tool necessity analysis:** Measure tool invocation rate vs. task difficulty (bounding box size, visual clutter). Verify model learns adaptive tool-use rather than always/never invoking tools.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the performance gap between text-based and icon-based grounding be closed using targeted visual pretraining or symbol-aware augmentation strategies?
- **Basis in paper**: Section 4.2 notes that while improvements on text tasks are substantial, improvements on icon-based queries are slightly smaller, stating: "Future efforts could further enhance this capacity by incorporating targeted visual pretraining or lightweight symbol-aware augmentation strategies."
- **Why unresolved**: The current RL framework optimizes general grounding but does not include specific mechanisms to handle abstract visual symbols (icons) as effectively as textual elements.
- **What evidence would resolve it**: A study evaluating GUI-Eyes when initialized with different visual pretraining objectives or trained with icon-specific data augmentations, specifically measuring the delta in icon-grounding accuracy versus text-grounding accuracy.

### Open Question 2
- **Question**: How robust is the reward function to variations in hyperparameter settings, and can the tuning of weighting coefficients ($\lambda_{acc}, \lambda_{tool}, \lambda_{format}$) be automated?
- **Basis in paper**: The ablation study (Table 3) shows that shifting reward weights causes accuracy to drop from 44.8% to 41.2%. The appendix confirms coefficients are "selected via grid search," implying performance relies heavily on manual tuning.
- **Why unresolved**: The paper does not explore adaptive reward mechanisms or methods to mitigate the sensitivity of the policy gradient updates to these specific manual weightings.
- **What evidence would resolve it**: Experiments demonstrating stable convergence across different random seeds or datasets without manual coefficient retuning, or the implementation of an automated meta-learning approach for reward weighting.

### Open Question 3
- **Question**: Does the active perception framework scale to multi-step GUI automation tasks where the visual state changes dynamically after each action?
- **Basis in paper**: The paper focuses on "visual grounding" (single-step point prediction). While the method uses a two-stage reasoning process, it does not evaluate the agent on long-horizon trajectories where the "original image patch" changes based on previous actions.
- **Why unresolved**: The current "perception–reasoning–perception" loop refines the view of a static screenshot, but it is unclear if the policy generalizes to a loop where the environment state evolves based on the agent's clicks.
- **What evidence would resolve it**: Evaluation of the GUI-Eyes policy on a multi-turn benchmark requiring sequential actions, rather than single-step grounding benchmarks like ScreenSpot.

### Open Question 4
- **Question**: To what extent would the inclusion of additional visual tools (beyond crop and zoom) further improve grounding accuracy in complex interfaces?
- **Basis in paper**: Section 3.2 Remark states, "Our framework supports both Crop and Zoom tools," and treats zoom as a visual transformation of crop. It does not investigate other potential tools like edge detection, OCR highlighting, or color filtering.
- **Why unresolved**: The agent's ability to perceive is constrained by the limited toolset. It is unknown if the "active perception" logic successfully scales to more abstract tools that might help in cluttered or low-contrast UI scenarios.
- **What evidence would resolve it**: An extension of the action space to include auxiliary visual filters and an analysis of the agent's learned decision-making regarding when to deploy these novel tools versus standard cropping.

## Limitations

- **Reward shaping validity**: The specific combination of center proximity and IoU terms may overfit to ScreenSpot-Pro's annotation style and could break on low-resolution interfaces or semantic targets spanning multiple UI elements.
- **Dataset representation gap**: Training on 3K samples from limited sources may not capture edge cases like modal dialogs, animated elements, or non-rectangular targets that appear in real deployment.
- **Tool parameter generalization**: The learned crop/zoom parameters are optimized for the training distribution and may not generalize well to extreme aspect ratios or zoomed-in details outside the learned policy's comfort zone.

## Confidence

- **High confidence**: Progressive perception decomposition improves grounding accuracy on complex interfaces (supported by direct comparison to single-stage baselines and independent validation from GUI-ARP).
- **Medium confidence**: The spatially continuous reward function accelerates learning (ablation shows performance degradation when removed, but no external validation exists).
- **Low confidence**: Generalization to unseen GUI domains and tasks with minimal fine-tuning (only 3K samples used; claims of strong efficiency not independently verified).

## Next Checks

1. **Cross-domain robustness test**: Evaluate GUI-Eyes-3B on a held-out dataset from a different GUI agent benchmark (e.g., MagicGUI's mobile domain) without retraining to measure true generalization.
2. **Reward shaping ablation with synthetic data**: Generate synthetic grounding tasks with controlled difficulty (target size, clutter) and compare GRPO with discrete vs. continuous rewards to isolate the effect of the proposed reward formulation.
3. **Tool necessity analysis on real deployments**: Instrument the policy to log tool invocation frequency and accuracy per task category on actual user interfaces (not benchmark datasets) to verify adaptive tool-use behavior.