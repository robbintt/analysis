---
ver: rpa2
title: 'Adversarial Attacks Against Automated Fact-Checking: A Survey'
arxiv_id: '2509.08463'
source_url: https://arxiv.org/abs/2509.08463
tags:
- attacks
- adversarial
- claims
- evidence
- fever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews adversarial attacks targeting
  automated fact-checking (AFC) systems, which remain vulnerable despite significant
  advances in verification accuracy. We introduce a novel taxonomy categorizing attacks
  based on target components (claim, evidence, or claim-evidence pairs) and edit granularity
  (character to article level), revealing 53 distinct attack strategies.
---

# Adversarial Attacks Against Automated Fact-Checking: A Survey

## Quick Facts
- arXiv ID: 2509.08463
- Source URL: https://arxiv.org/abs/2509.08463
- Reference count: 40
- Introduces novel taxonomy categorizing 53 adversarial attack strategies against AFC systems

## Executive Summary
This survey systematically examines adversarial attacks targeting automated fact-checking (AFC) systems, revealing significant vulnerabilities despite advances in verification accuracy. The authors introduce a comprehensive taxonomy categorizing attacks based on target components (claim, evidence, or claim-evidence pairs) and edit granularity (character to article level), identifying 53 distinct attack strategies. Generation-based attacks dominate claim manipulation while evidence attacks show stronger resilience, with only 13 of 53 attacks currently addressed by defenses. Performance evaluation across 15 datasets demonstrates that adversarial claims can degrade model accuracy by up to 40% and retrieval recall by over 20%, with NEI verdicts being particularly susceptible.

## Method Summary
The survey systematically reviews existing literature on adversarial attacks against AFC systems, organizing findings into a novel taxonomy that categorizes attacks by target component and edit granularity. The authors analyze 53 distinct attack strategies across multiple dimensions, evaluate their effectiveness using performance metrics from 15 datasets, and assess the current state of defensive countermeasures. The methodology involves comprehensive literature analysis, systematic categorization of attack types, and quantitative evaluation of attack effectiveness on established AFC benchmarks.

## Key Results
- 53 distinct adversarial attack strategies identified across claim, evidence, and claim-evidence targets
- Generation-based attacks dominate claim manipulation while evidence attacks show stronger resilience
- Less than a quarter of attack vectors currently defended, with 13 of 53 attacks addressed by existing defenses
- Adversarial claims degrade model accuracy by up to 40% and retrieval recall by over 20%

## Why This Works (Mechanism)
Adversarial attacks against AFC systems exploit vulnerabilities in how models process and verify claims against evidence. The effectiveness stems from the fundamental challenge of distinguishing between genuine variations in claims and malicious manipulations designed to deceive verification systems. By targeting specific components of the AFC pipeline - claims, evidence, or their relationships - attackers can systematically degrade system performance while remaining undetected by conventional validation methods.

## Foundational Learning
- Claim manipulation techniques: Understanding how claims can be subtly altered while maintaining semantic similarity is crucial for identifying attack vectors
- Evidence retrieval vulnerabilities: Quick check: Test retrieval systems with adversarially modified evidence to identify weak points
- Claim-evidence relationship attacks: Why needed: These attacks exploit the semantic gap between claims and supporting evidence
- Model robustness assessment: Quick check: Measure performance degradation under various attack scenarios
- Defensive countermeasure evaluation: Why needed: To determine which attack vectors remain unprotected

## Architecture Onboarding
Component map: Claim/Evidence Extraction -> Fact Verification Model -> Verdict Generation
Critical path: Input processing → Feature extraction → Evidence retrieval → Claim-evidence matching → Classification
Design tradeoffs: Balancing verification accuracy against robustness to adversarial inputs
Failure signatures: Performance degradation, increased false positives/negatives, retrieval recall reduction
First experiments: 1) Test baseline AFC accuracy on clean data, 2) Apply character-level attacks to claims, 3) Evaluate evidence retrieval under adversarial modifications

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Focus primarily on English-language datasets limits generalizability to multilingual contexts
- Rapidly evolving attack methods may not be fully captured by current taxonomy
- Defense evaluation limited to existing literature rather than comprehensive real-world testing

## Confidence
High confidence in attack categorization methodology
Medium confidence in reported performance degradation figures
Medium confidence in defense coverage assessment
Low confidence in generalizability to non-English and multimodal contexts

## Next Checks
1. Conduct cross-lingual validation by replicating attack effectiveness studies across multiple language datasets
2. Perform longitudinal studies tracking evolution of attack strategies and defensive countermeasures over time
3. Design and implement comprehensive benchmark suites including multimodal content and temporal dynamics