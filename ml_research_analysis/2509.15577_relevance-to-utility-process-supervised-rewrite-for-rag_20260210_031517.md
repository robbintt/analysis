---
ver: rpa2
title: 'Relevance to Utility: Process-Supervised Rewrite for RAG'
arxiv_id: '2509.15577'
source_url: https://arxiv.org/abs/2509.15577
tags:
- document
- documents
- answer
- utility
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between retrieval relevance and generative
  utility in Retrieval-Augmented Generation (RAG) systems, where retrieved documents
  may be topically relevant but fail to support effective reasoning during answer
  generation. The authors propose R2U, a training framework that optimizes document
  rewriting by directly maximizing the probability of generating correct answers through
  process supervision.
---

# Relevance to Utility: Process-Supervised Rewrite for RAG

## Quick Facts
- **arXiv ID**: 2509.15577
- **Source URL**: https://arxiv.org/abs/2509.15577
- **Reference count**: 27
- **Key outcome**: R2U achieves 6.8% F1 improvement over naive RAG and 5.6% over best existing bridging methods through process-supervised document rewriting

## Executive Summary
This paper addresses the critical gap between retrieval relevance and generative utility in Retrieval-Augmented Generation (RAG) systems, where topically relevant documents often fail to support effective reasoning during answer generation. The authors propose R2U, a training framework that optimizes document rewriting by directly maximizing the probability of generating correct answers through process supervision. R2U approximates true utility via joint observation of rewriting and answering in the reasoning process, and distills this supervision by scaling from LLMs. The framework demonstrates substantial performance gains across multiple open-domain question-answering benchmarks.

## Method Summary
R2U introduces a novel training framework that optimizes document rewriting by directly maximizing the probability of generating correct answers through process supervision. The approach approximates true utility by jointly observing rewriting and answering in the reasoning process, then distills this supervision by scaling from LLMs. The framework constructs utility-improvement signals by measuring the generator's gain of the answer under rewritten contexts. This direct optimization of generative utility, rather than retrieval relevance, represents a significant departure from traditional RAG approaches.

## Key Results
- R2U achieves an average F1 score increase of 6.8% over the naive RAG baseline
- Performance improves by 5.6% over the best existing bridging methods
- Consistent improvements demonstrated across multiple open-domain question-answering benchmarks (HotpotQA, MuSiQue, CRAG, MS MARCO)

## Why This Works (Mechanism)
R2U works by recognizing that retrieval relevance and generative utility are distinct objectives that traditional RAG systems optimize separately. By directly optimizing document rewriting through process supervision, R2U ensures that retrieved documents are not just topically relevant but actually support effective reasoning. The framework approximates true utility by observing the joint effect of rewriting and answering, creating a more holistic optimization target. The utility-improvement signal construction captures the actual benefit of rewritten contexts on answer quality, rather than relying on proxy metrics.

## Foundational Learning
- **Process supervision**: Observing intermediate reasoning steps during generation to provide more granular supervision signals; needed to capture the reasoning process, not just final outputs
- **Utility-based optimization**: Directly optimizing for the generation of correct answers rather than retrieval relevance; needed because relevance doesn't guarantee utility for reasoning
- **Teacher-student distillation**: Scaling supervision signals from large teacher models to smaller student rewriters; needed for practical deployment of process-supervised training
- **Joint observation**: Simultaneously tracking rewriting and answering to approximate true utility; needed to capture the full reasoning pipeline
- **Utility-improvement signals**: Measuring the gain in answer quality under rewritten contexts; needed to construct meaningful optimization objectives
- **Bridging optimization**: Connecting retrieval and generation stages through intermediate rewriting; needed to address the disconnect between retrieval and generation objectives

## Architecture Onboarding

**Component Map**
Teacher LLM -> Process Supervision -> Student Rewriter -> RAG System -> Answer Generator

**Critical Path**
Document Retrieval -> Rewriting (R2U) -> Context Provision -> Answer Generation -> Evaluation

**Design Tradeoffs**
- **Computation vs Performance**: Process supervision requires significant computational resources but provides substantial performance gains
- **Scalability vs Accuracy**: Teacher model size impacts supervision quality but increases cost
- **Generality vs Specialization**: Framework optimized for QA may need adaptation for other RAG applications

**Failure Signatures**
- Diminished returns with very large teacher models due to diminishing supervision value
- Spurious correlations from teacher model reasoning traces affecting student rewriter behavior
- Computational bottlenecks during process supervision data generation
- Generalization issues when applying to non-QA RAG tasks

**First 3 Experiments to Run**
1. Ablation study removing process supervision to isolate its contribution to performance gains
2. Domain transfer evaluation on specialized datasets (e.g., biomedical, legal) to test generalization
3. Computational cost analysis comparing inference latency and resource requirements against traditional RAG

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does R2U's performance improve when using teacher models larger than 70B parameters?
- **Basis in paper**: "Due to this expense, we were unable to conduct data generation with larger teacher models beyond 70B."
- **Why unresolved**: High computational cost of generating process-supervised data prevented experimentation with larger models.
- **What evidence would resolve it**: Experiments using 100B+ parameter teacher models (e.g., GPT-4, Claude) with comparison of student rewriter performance.

### Open Question 2
- **Question**: Does R2U generalize to RAG applications beyond question answering?
- **Basis in paper**: "our evaluation is focused on QA tasks, leaving broader RAG applications for future work."
- **Why unresolved**: All experiments were conducted on QA benchmarks (HotpotQA, MuSiQue, CRAG, MS MARCO); no evaluation on other RAG use cases.
- **What evidence would resolve it**: Evaluation on non-QA RAG tasks such as summarization, dialogue, fact-checking, or code generation with retrieval.

### Open Question 3
- **Question**: Can the spurious correlation problem from teacher models be effectively mitigated?
- **Basis in paper**: "fully eliminating all non-causal patterns inherited from the teacher remains challenging."
- **Why unresolved**: Utility-improvement filtering and shortcut filtering reduce but do not eliminate non-causal supervision signals from teacher reasoning traces.
- **What evidence would resolve it**: Analysis measuring the prevalence of spurious patterns in distilled rewriters and development of additional filtering mechanisms with quantitative improvement metrics.

## Limitations
- **Scalability concerns**: Framework depends on LLM-based process supervision, raising questions about computational cost and latency for production deployments
- **Evaluation scope**: Confined to open-domain question-answering benchmarks, limiting generalization to specialized domains and other RAG applications
- **Approximation errors**: The approximation of true utility through joint observation may introduce biases or errors that propagate through training

## Confidence

**High confidence**: The core problem formulation and theoretical framework are well-established and technically sound
**Medium confidence**: The experimental results show strong performance improvements, but the evaluation scope is limited
**Low confidence**: The practical scalability and generalization across diverse domains remain unproven

## Next Checks
1. Conduct ablation studies isolating the contributions of process supervision versus the utility-improvement signal construction
2. Evaluate performance across specialized domains (e.g., biomedical, legal) and different RAG application types (e.g., dialogue systems, summarization)
3. Benchmark the computational cost and latency impact compared to traditional RAG systems at scale