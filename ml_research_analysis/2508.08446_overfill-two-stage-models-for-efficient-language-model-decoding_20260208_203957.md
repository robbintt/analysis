---
ver: rpa2
title: 'OverFill: Two-Stage Models for Efficient Language Model Decoding'
arxiv_id: '2508.08446'
source_url: https://arxiv.org/abs/2508.08446
tags:
- arxiv
- overfill
- pruned
- decoding
- prefill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models face high inference costs due to memory-bound
  decoding. OverFill proposes a two-stage approach: a full model for compute-bound
  prefill and a pruned model for memory-bound decoding.'
---

# OverFill: Two-Stage Models for Efficient Language Model Decoding

## Quick Facts
- arXiv ID: 2508.08446
- Source URL: https://arxiv.org/abs/2508.08446
- Authors: Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed Abdelfattah, Alexander M. Rush
- Reference count: 23
- Primary result: 83.2% accuracy improvement over standalone pruned models

## Executive Summary
Large language models face high inference costs due to memory-bound decoding stages. OverFill proposes a two-stage approach using a full model for compute-bound prefill and a pruned model for memory-bound decoding. This method improves accuracy-efficiency tradeoffs by leveraging more compute during prefill while maintaining minimal latency overhead. In a 3B-to-1B configuration, OverFill outperforms standalone pruned models by 83.2% and instruction-tuned models by 52.6% across benchmarks while being Pareto-optimal.

## Method Summary
OverFill exploits the computational asymmetry between prefill (compute-bound) and decode (memory-bound) stages in LLM inference. The method uses width pruning to reduce model size while preserving KV cache compatibility through layer count preservation. Channel importance is determined via activation magnitude aggregation on calibration data. The full model remains frozen during training of the pruned decoder, which is initialized by selecting high-importance channels. This two-stage serving architecture enables efficient inference with minimal quality degradation.

## Key Results
- 83.2% accuracy improvement over standalone pruned models on GSM8K-CoT, ARC, MMLU, MATH, WMT16, IFEval, NQ, MMLU-Redux, and CRUXEval
- 52.6% improvement over instruction-tuned models across same benchmarks
- Pareto-optimal latency-accuracy tradeoff with minimal overhead from full model prefill
- Matches or exceeds same-sized models trained from scratch while using less training data

## Why This Works (Mechanism)

### Mechanism 1
Allocating more parameters to prefill improves generation quality with minimal latency overhead because prefill is compute-bound while decode is memory-bound. During prefill, the full model θ processes all input tokens in parallel, loading parameters once. During decode, the pruned model θ′ generates tokens autoregressively, reloading parameters at each step. Since decode dominates total latency for long sequences, reducing |θ′| yields disproportionate speedups while the one-time prefill overhead from using |θ| remains bounded.

### Mechanism 2
Width pruning preserves KV cache compatibility, enabling seamless handoff from full to pruned model. Width pruning reduces hidden dimension D to D′ while maintaining the cache structure R^{2×L×D}. The pruned model uses subsets of attention projections and FFN weights. Critically, layer count L remains unchanged, so the KV cache shape is preserved across the stage boundary.

### Mechanism 3
Freezing the full model while training only the pruned decoder preserves prefill quality while enabling efficient adaptation. The pruned decoder θ′ is initialized by selecting high-importance channels from θ (via L2 norm aggregation on calibration activations). During training on instruction data, only θ′ is updated via standard teacher forcing on output tokens y. The full model remains frozen, ensuring the prefill representations remain well-calibrated.

## Foundational Learning

**Prefill vs. Decode computational profiles:** Why needed here: The entire method rests on exploiting their different bottlenecks. Quick check question: For a model where decode latency dominates, would shrinking the decode model help more than shrinking the prefill model?

**Width vs. Depth pruning in Transformers:** Why needed here: Width pruning preserves KV cache dimensions; depth pruning would change the layer count L and break cache compatibility. Quick check question: If you pruned 4 layers from a 28-layer model, what would happen to the KV cache at the prefill-to-decode handoff?

**Channel importance via activation magnitudes:** Why needed here: This is how OverFill selects which parameters to keep without requiring gradient computation. Quick check question: Why might L2 norm of activations be a reasonable proxy for channel importance? What are failure modes?

## Architecture Onboarding

**Component map:** Full model (θ) → Calibration pass → Width pruning transform → Pruned decoder (θ′) → Two-stage inference

**Critical path:** Calibration dataset → aggregate activations → compute channel importance → extract θ′ from θ → train θ′ on instruction data → deploy with two-stage serving

**Design tradeoffs:** Higher pruning ratio P → faster decode but larger capacity gap. Larger full model → better prefill quality but higher memory during prefill phase. Training data: OverFill uses less data than training from scratch, but still requires instruction-tuning data.

**Failure signatures:** Cache shape mismatch at stage boundary (indicates pruning broke dimensionality). Accuracy drops sharply for long generations (capacity gap widening). High prefill latency overhead (suggests prompt-to-generation ratio is unfavorable).

**First 3 experiments:** 
1. Pruning ratio sweep: Start with P ∈ {0.25, 0.45, 0.7} on a small model (3B→1B) using OpenHermes-2.5; measure accuracy on ARC and WMT16 to understand task sensitivity.
2. Latency breakdown: Measure prefill vs. decode latency separately at varying generation lengths (128, 512, 1024 tokens) to verify decode dominance in your target workload.
3. Ablation on freezing: Compare freezing full model vs. joint training to validate the asymmetric update strategy on a held-out validation set.

## Open Questions the Paper Calls Out

**Can periodically refreshing the prefill state maintain OverFill's accuracy benefits uniformly throughout extremely long sequences?** The authors plan to explore whether periodically refreshing the prefill can help maintain its benefits uniformly throughout the entire sequence. An ablation study injecting intermediate "refresh" calls to the full model during long-chain generation tasks would resolve this.

**Does pruning attention heads in conjunction with width pruning optimize KV cache size without destroying the representation compatibility?** The paper notes that pruning attention could further optimize KV cache size and decoding efficiency. Experiments applying structured attention pruning to the decoder would resolve this.

**Can OverFill be effectively combined with asymmetric quantization to further accelerate the distinct prefill and decode stages?** Section 2 states that high-precision prefill and low-precision decode could be considered for future work. Implementation of an OverFill variant using FP16 for full prefill model and INT4/INT8 for pruned decoder would resolve this.

## Limitations
- Calibration data sensitivity: Channel importance scores depend heavily on calibration dataset distribution
- Task-specific capacity gaps: Accuracy degradation increases with generation distance from prompt
- Training data efficiency claims: Pruned architectures may need more adaptation data than full architectures

## Confidence
**High Confidence:** The fundamental insight about prefill being compute-bound while decode is memory-bound is well-established. The mechanism of width pruning preserving KV cache compatibility is technically sound.

**Medium Confidence:** Empirical claims about accuracy improvements are supported by benchmark results, but comparisons depend on specific pruning ratios and task mixes. The Pareto-optimality claim requires further validation.

**Low Confidence:** The generalizability of the asymmetric freezing strategy across different model architectures and instruction datasets is not well-established. Limited ablation studies on this design choice.

## Next Checks
1. **Calibration set sensitivity study:** Systematically vary calibration dataset composition and size to quantify how pruning channel selection changes across different prompt distributions.

2. **Long-generation capacity analysis:** Design controlled experiment measuring accuracy degradation as a function of generation length for different pruning ratios using tasks requiring coherent multi-paragraph outputs.

3. **Training data efficiency validation:** Compare total data requirements (including calibration and instruction tuning) for OverFill versus training same-sized model from scratch, measuring final performance as a function of total tokens processed.