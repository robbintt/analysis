---
ver: rpa2
title: 'M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language
  Models'
arxiv_id: '2510.19358'
source_url: https://arxiv.org/abs/2510.19358
tags:
- arxiv
- m3-slu
- speech
- task
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3-SLU, a multimodal benchmark designed to
  evaluate speaker-attributed reasoning in multi-speaker, multi-turn spoken language
  understanding. While current multimodal models excel at general comprehension, they
  struggle to identify "who said what" in complex conversations.
---

# M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2510.19358
- **Source URL:** https://arxiv.org/abs/2510.19358
- **Reference count:** 0
- **Primary result:** Introduces M3-SLU, a multimodal benchmark for speaker-attributed reasoning in multi-speaker, multi-turn spoken language understanding.

## Executive Summary
Current multimodal large language models excel at general comprehension but struggle to identify "who said what" in complex conversations. M3-SLU addresses this gap by introducing a benchmark specifically designed to evaluate speaker-attributed reasoning capabilities. The benchmark leverages four open corpora (CHiME-6, MELD, MultiDialog, and AMI) to create over 12,000 validated instances featuring long, overlapping dialogues. Two key tasks are proposed: Speaker-Attributed QA and Speaker Attribution via Utterance Matching, both requiring accurate speaker identification. Experimental results show that even state-of-the-art cascaded and end-to-end models achieve only moderate performance, especially on speaker attribution tasks, highlighting a critical gap in multi-speaker reasoning capabilities.

## Method Summary
M3-SLU constructs a benchmark using four open dialogue corpora to create over 12,000 validated instances of multi-speaker, multi-turn conversations. The benchmark proposes two tasks: Speaker-Attributed QA (answering questions while identifying the speaker) and Speaker Attribution via Utterance Matching (matching utterances to speakers). The evaluation framework tests both cascaded systems (separate ASR and multimodal models) and end-to-end models on their ability to correctly identify speakers and attribute utterances in complex, overlapping dialogue scenarios.

## Key Results
- Current multimodal models achieve only moderate performance on speaker-attributed reasoning tasks
- Speaker attribution tasks prove significantly more challenging than general comprehension tasks
- State-of-the-art cascaded and end-to-end models show similar limitations in multi-speaker reasoning
- The benchmark reveals a critical gap in current multimodal systems' ability to handle "who said what" scenarios

## Why This Works (Mechanism)
The benchmark works by creating controlled yet realistic scenarios where speaker attribution is essential for correct comprehension. By using validated instances from diverse conversational datasets, M3-SLU ensures that models must rely on both contextual understanding and speaker-specific cues to succeed, rather than just general language modeling.

## Foundational Learning
- **Multimodal model architecture:** Why needed - To process and integrate multiple input modalities (audio, text, visual); Quick check - Can the model handle synchronized multimodal inputs?
- **Speaker diarization:** Why needed - To identify and track individual speakers in conversations; Quick check - Does the system maintain speaker identity across turns?
- **Conversational context modeling:** Why needed - To understand relationships and dependencies between utterances; Quick check - Can the model track dialogue flow and speaker changes?
- **Attribution reasoning:** Why needed - To correctly assign statements to their respective speakers; Quick check - Does the model maintain speaker consistency in its outputs?
- **Overlapping speech handling:** Why needed - To manage scenarios where multiple speakers talk simultaneously; Quick check - Can the system disambiguate speakers in overlap regions?

## Architecture Onboarding

**Component Map:** ASR → Speaker Diarization → Multimodal Encoder → Reasoning Module → Output Layer

**Critical Path:** Audio Input → ASR → Speaker Segmentation → Multimodal Fusion → Attribution Reasoning → Speaker-Attributed Output

**Design Tradeoffs:** 
- Cascaded vs. end-to-end approaches: Cascaded systems offer modularity but suffer from error propagation; end-to-end systems are more compact but harder to train
- Real-time vs. batch processing: Real-time requires efficient speaker tracking but may sacrifice accuracy
- Granularity of speaker attribution: Fine-grained attribution provides more detail but increases complexity

**Failure Signatures:**
- Misattribution of overlapping utterances
- Loss of speaker identity across long conversations
- Confusion between similar-sounding speakers
- Failure to maintain context across speaker turns

**First Experiments:**
1. Test speaker identification accuracy on single-speaker vs. multi-speaker scenarios
2. Evaluate model performance with increasing dialogue length and complexity
3. Assess attribution accuracy when speakers have similar speaking patterns

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the moderate performance of state-of-the-art models on speaker attribution tasks raises questions about the fundamental challenges in multi-speaker reasoning and the potential need for new architectural approaches.

## Limitations
- The benchmark datasets may not fully represent the diversity of real-world conversational contexts
- Moderate model performance could be influenced by dataset-specific biases rather than fundamental limitations
- The evaluation methodology may not capture all aspects of real-world speaker attribution challenges
- Lack of detailed error analysis makes it difficult to identify specific failure modes

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| M3-SLU addresses a critical gap in multi-speaker reasoning | High |
| Benchmark representativeness and difficulty level | Medium |
| Generalizability of results to real-world applications | Low |

## Next Checks
1. Conduct error analysis on model predictions to identify specific failure modes in speaker attribution (e.g., overlapping speech, speaker role ambiguity)
2. Test M3-SLU on additional conversational datasets (e.g., TV shows, podcasts) to assess generalizability
3. Evaluate human performance on M3-SLU tasks to establish a baseline for model comparison and benchmark difficulty