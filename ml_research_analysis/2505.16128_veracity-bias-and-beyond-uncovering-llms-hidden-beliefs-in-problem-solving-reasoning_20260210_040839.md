---
ver: rpa2
title: 'Veracity Bias and Beyond: Uncovering LLMs'' Hidden Beliefs in Problem-Solving
  Reasoning'
arxiv_id: '2505.16128'
source_url: https://arxiv.org/abs/2505.16128
tags:
- bias
- llms
- groups
- evaluation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reveals that large language models (LLMs) exhibit systematic
  biases linking solution correctness to demographic groups, beyond explicit social
  context provocations. Through experiments on five prevalent LLMs across mathematics,
  coding, commonsense reasoning, and writing tasks, the research identifies two forms
  of veracity bias: Attribution Bias, where models disproportionately attribute correct
  solutions to certain demographic groups, and Evaluation Bias, where identical solutions
  receive different assessments based on perceived authorship.'
---

# Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning

## Quick Facts
- arXiv ID: 2505.16128
- Source URL: https://arxiv.org/abs/2505.16128
- Authors: Yue Zhou; Barbara Di Eugenio
- Reference count: 40
- Key outcome: LLMs exhibit systematic biases linking solution correctness to demographic groups, showing Attribution Bias and Evaluation Bias across multiple task types

## Executive Summary
This study reveals that large language models exhibit systematic biases in attributing solution correctness to different demographic groups, extending beyond explicit social context provocations. Through controlled experiments on five prevalent LLMs across mathematics, coding, commonsense reasoning, and writing tasks, the research identifies two forms of veracity bias: Attribution Bias, where models disproportionately attribute correct solutions to certain demographic groups, and Evaluation Bias, where identical solutions receive different assessments based on perceived authorship. The findings demonstrate pervasive biases, with African-American groups consistently receiving fewer correct attributions and more incorrect ones in math and coding, while Asian authors are least preferred in writing evaluation. Additional experiments reveal that LLMs automatically assign stereotypical colors to racial groups in visualization code, suggesting these biases are deeply embedded in models' reasoning processes.

## Method Summary
The research conducted controlled experiments using five prevalent LLMs across four task types: mathematics, coding, commonsense reasoning, and writing. The study employed a systematic approach of prompting models with solutions attributed to different demographic groups and measuring attribution and evaluation biases. For coding tasks, the researchers analyzed color assignment patterns in visualization code generation, examining how models automatically associate colors with different racial groups. All experiments used zero-shot prompting without fine-tuning or prompt engineering variations.

## Key Results
- LLMs show Attribution Bias, disproportionately attributing correct solutions to certain demographic groups across all task types
- Evaluation Bias manifests as different assessments of identical solutions based on perceived authorship demographics
- African-American groups consistently receive fewer correct attributions and more incorrect ones in math and coding tasks
- Asian authors are least preferred in writing evaluation tasks
- LLMs automatically assign stereotypical colors to racial groups in visualization code generation

## Why This Works (Mechanism)
The mechanisms underlying these biases remain unclear in the study, as the authors do not provide a detailed explanation of why LLMs develop these demographic attribution patterns. The biases appear to be learned from training data rather than explicitly programmed, suggesting that statistical correlations in the training corpus have been internalized by the models.

## Foundational Learning
- **Zero-shot prompting**: Understanding how to effectively prompt LLMs without fine-tuning is crucial for evaluating inherent model biases
- **Attribution bias measurement**: Methods for quantifying how models assign correctness to different demographic groups
- **Evaluation bias detection**: Techniques for identifying when models assess identical content differently based on demographic context
- **Visualization code analysis**: Understanding how models generate and associate colors with demographic attributes in code
- **Cross-model bias comparison**: Methods for comparing bias patterns across different LLM architectures and training approaches

## Architecture Onboarding
- **Component map**: User prompts -> LLM inference engine -> Response generation -> Bias manifestation
- **Critical path**: Prompt encoding -> Contextual embedding -> Generation decoding -> Output bias
- **Design tradeoffs**: Zero-shot evaluation vs. fine-tuning control, general capability vs. bias mitigation, task specificity vs. broad applicability
- **Failure signatures**: Systematic over/under-attribution to specific demographics, inconsistent evaluation of identical content, stereotypical color associations
- **First 3 experiments to run**:
  1. Temperature sensitivity test to assess impact on bias manifestation
  2. Prompt engineering variations to test bias mitigation potential
  3. Fine-tuning on bias-corrected datasets to evaluate remediation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- All experiments rely on zero-shot prompting without exploring different prompting strategies or model fine-tuning approaches
- Confidence levels are Medium for core findings but Low for visualization color assignment results
- Limited exploration of bias mitigation techniques or temperature settings
- Experimental scenarios may not fully capture real-world complexity of educational or evaluation contexts

## Confidence
- Core veracity bias findings: Medium confidence
- Visualization color assignment findings: Low confidence
- Generalization to real-world applications: Medium confidence

## Next Checks
1. Test whether temperature settings, prompt engineering, or model fine-tuning can mitigate the identified veracity biases
2. Replicate experiments with additional demographic groups and intersectional identities to assess the breadth of bias patterns
3. Conduct human evaluation studies to validate whether the LLM judgments align with human expectations and to assess the practical impact of these biases in real educational settings