---
ver: rpa2
title: From Image Captioning to Visual Storytelling
arxiv_id: '2508.14045'
source_url: https://arxiv.org/abs/2508.14045
tags:
- visual
- storytelling
- story
- image
- bart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses visual storytelling by proposing a two-stage
  approach that treats the task as a superset of image captioning. The method first
  generates captions for individual images using a vision-to-language model, then
  transforms these captions into coherent narratives using a language-to-language
  model.
---

# From Image Captioning to Visual Storytelling

## Quick Facts
- arXiv ID: 2508.14045
- Source URL: https://arxiv.org/abs/2508.14045
- Reference count: 21
- Proposes a two-stage approach treating visual storytelling as a superset of image captioning

## Executive Summary
This paper addresses visual storytelling by proposing a two-stage approach that treats the task as a superset of image captioning. The method first generates captions for individual images using a vision-to-language model, then transforms these captions into coherent narratives using a language-to-language model. The framework combines ClipCap with either T5 or BART as the storyteller, and is trained separately on different data subsets to enhance robustness. Evaluation on the VIST dataset shows that the proposed method achieves competitive performance across multiple metrics, including RoViST and ideality, which measures linguistic closeness to human-written stories.

## Method Summary
The paper proposes a modular two-stage approach for visual storytelling. First, a vision-to-language model (ClipCap) generates captions for each individual image in a sequence. Then, a language-to-language model (either T5 or BART) transforms these captions into a coherent narrative story. The models are trained separately on different data subsets to enhance robustness. This approach is lightweight and reproducible while maintaining competitive performance on standard visual storytelling benchmarks.

## Key Results
- Achieves competitive performance on VIST dataset across multiple metrics including RoViST and ideality
- Generates stories that are more coherent, visually grounded, and human-like compared to several state-of-the-art baselines
- Demonstrates strong linguistic closeness to human-written stories through automated metrics

## Why This Works (Mechanism)
The approach leverages the idea that visual storytelling can be decomposed into two distinct tasks: image captioning and narrative generation. By treating visual storytelling as a superset of image captioning, the method allows for specialized models to handle each stage effectively. The first stage ensures visual grounding through detailed image descriptions, while the second stage focuses on narrative coherence and linguistic quality. This separation of concerns allows each model to specialize in its respective domain, potentially leading to better overall performance than monolithic approaches.

## Foundational Learning

- ClipCap: Vision-to-language model for image captioning. Why needed: Provides visual grounding and detailed image descriptions as input for narrative generation. Quick check: Verify it can generate accurate, descriptive captions for individual images.

- T5/BART: Language-to-language models for narrative transformation. Why needed: Convert descriptive captions into coherent, flowing narratives. Quick check: Ensure the model can maintain coherence while transforming captions into stories.

- RoViST metric: Measures visual grounding in generated stories. Why needed: Evaluates how well stories relate to visual content. Quick check: Confirm metric aligns with human judgment of visual grounding.

- Ideality metric: Measures linguistic closeness to human-written stories. Why needed: Assesses naturalness and fluency of generated narratives. Quick check: Validate metric correlates with human preferences.

## Architecture Onboarding

Component map: ClipCap (Image captioning) -> T5/BART (Narrative transformation) -> Generated story

Critical path: Image sequence → ClipCap → Captions → T5/BART → Final narrative

Design tradeoffs: The two-stage approach trades end-to-end optimization for modularity and interpretability. While potentially less optimal than unified models, it offers better control, easier debugging, and the ability to swap components.

Failure signatures: Poor visual grounding may indicate issues with the captioning stage; lack of narrative coherence suggests problems in the transformation stage. Disjointed stories could result from insufficient training data for either component.

3 first experiments:
1. Validate individual components: Test ClipCap on image captioning benchmarks, T5/BART on text-to-text tasks
2. Ablation study: Compare performance with and without the narrative transformation stage
3. Human evaluation: Assess story quality, coherence, and visual grounding against baseline approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily relies on automated metrics that may not fully capture nuanced aspects of narrative quality
- Focus on VIST dataset may limit generalization to other storytelling contexts or domains
- Lightweight approach may struggle with complex temporal dependencies and contextual relationships compared to more sophisticated end-to-end models

## Confidence
- High: Competitive performance on established metrics and effectiveness of two-stage architecture
- Medium: Generated stories are more coherent and human-like (relies on subjective human/LLM evaluations)
- Low: Generalization beyond VIST dataset and ability to produce emotionally resonant narratives

## Next Checks
1. Conduct ablation studies to isolate the contribution of each stage (image captioning vs. narrative transformation) to overall performance.
2. Evaluate the approach on additional visual storytelling datasets or real-world image sequences to assess generalization.
3. Perform qualitative analysis with diverse human evaluators to assess emotional resonance, engagement, and narrative depth beyond automated metrics.