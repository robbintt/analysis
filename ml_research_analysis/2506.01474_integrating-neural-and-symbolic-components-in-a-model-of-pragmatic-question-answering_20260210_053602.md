---
ver: rpa2
title: Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering
arxiv_id: '2506.01474'
source_url: https://arxiv.org/abs/2506.01474
tags:
- pragmatic
- question
- human
- questioner
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neuro-symbolic framework that integrates
  large language models (LLMs) with probabilistic cognitive models to simulate pragmatic
  question-answering. The method replaces hand-specified components of cognitive models
  with LLM-based evaluators and proposers, systematically varying the degree of neural
  integration.
---

# Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering

## Quick Facts
- arXiv ID: 2506.01474
- Source URL: https://arxiv.org/abs/2506.01474
- Authors: Polina Tsvilodub; Robert D. Hawkins; Michael Franke
- Reference count: 40
- Primary result: Hybrid models achieve performance on par with or exceeding traditional probabilistic models in predicting human answer patterns, particularly when LLMs are used for utility evaluation and alternative generation rather than truth-conditional semantics.

## Executive Summary
This paper introduces a neuro-symbolic framework that integrates large language models (LLMs) with probabilistic cognitive models to simulate pragmatic question-answering. The method replaces hand-specified components of cognitive models with LLM-based evaluators and proposers, systematically varying the degree of neural integration. The primary result shows that hybrid models achieve performance on par with or exceeding traditional probabilistic models in predicting human answer patterns (matching a baseline of 0.154 Jensen-Shannon divergence), particularly when LLMs are used for utility evaluation and alternative generation rather than truth-conditional semantics. The success critically depends on how LLMs are integrated, with fine-grained task decomposition improving performance. The framework offers a promising path toward more flexible, scalable models of pragmatic language use while highlighting key design considerations for balancing neural and symbolic components.

## Method Summary
The method replaces hand-specified components of probabilistic cognitive models with LLM-based evaluators and proposers, systematically varying the degree of neural integration. LLM modules (goal proposer, question proposer, response proposer, utility evaluator, semantic evaluators) scaffold a WebPPL probabilistic cognitive model. The framework incrementally replaces symbolic components with neural ones: (1) LLM utilities only, (2) LLM semantics only, (3) LLM semantics & utilities, (4) LLM base-level semantics & utilities, (5) LLM semantics, utilities, DPs, (6) full NeSy (adds response/question proposers), (7) prompt-based questioner (with/without goals), (8) one-shot chain-of-thought. Utility evaluator prompts ask for 0â€“100 ratings; semantic evaluators return 'yes'/'no'; response proposer generates n=10 alternatives; question proposer generates n=3 alternatives; goal proposer generates text-based goals. Manual appends of "no-options" and "all-options" responses supplement the proposer output set. Models use GPT-4o-mini (

## Key Results
- Hybrid models achieve performance on par with or exceeding traditional probabilistic models in predicting human answer patterns
- Models using LLMs for utility evaluation and alternative generation perform better than those using LLMs for truth-conditional semantics
- Fine-grained task decomposition improves LLM integration performance
- The baseline performance is 0.154 Jensen-Shannon divergence
- Different integration strategies (varying which components use LLMs) yield different performance levels

## Why This Works (Mechanism)
The success of hybrid models appears to stem from leveraging LLMs' strengths in evaluation and generation while maintaining symbolic components for structured reasoning. When LLMs are used for utility evaluation and alternative generation, they can leverage their pattern-matching capabilities and contextual understanding to produce more nuanced evaluations and diverse alternatives. The framework's incremental approach allows for systematic testing of different integration strategies, revealing that not all components benefit equally from neural integration. The performance gain likely comes from combining the flexibility and scalability of LLMs with the structured, interpretable nature of symbolic reasoning.

## Foundational Learning
- LLMs can effectively replace traditional utility evaluators when given appropriate prompts
- Alternative generation benefits from LLM capabilities in producing diverse responses
- Truth-conditional semantic evaluation may be better handled by symbolic components
- Fine-grained decomposition of tasks into separate LLM modules improves performance
- The choice of which components to neuralize significantly impacts overall model performance

## Architecture Onboarding
The neuro-symbolic framework follows a modular architecture where LLM components can be incrementally added to a probabilistic cognitive model scaffolded in WebPPL. Each LLM module has a specific role: the goal proposer generates text-based goals, the question proposer generates alternative questions, the response proposer generates response alternatives, the utility evaluator provides 0-100 ratings, and semantic evaluators return binary 'yes'/'no' judgments. The framework allows systematic variation in neural integration by controlling which components use LLMs versus symbolic implementations. The architecture supports both fine-grained decomposition (separate modules for each task) and more integrated approaches (chain-of-thought, prompt-based questioning).

## Open Questions the Paper Calls Out
- How to optimally balance neural and symbolic components for different pragmatic tasks
- Whether the observed performance gains generalize to other domains beyond question-answering
- The impact of different LLM model sizes and types on hybrid performance
- How to handle cases where LLM outputs are inconsistent or unreliable
- Whether the framework can scale to more complex pragmatic scenarios

## Limitations
- The study focuses on a specific type of pragmatic question-answering task, limiting generalizability
- Results depend on the quality and consistency of LLM outputs, which can vary
- The manual addition of "no-options" and "all-options" responses suggests limitations in the response proposer
- The framework requires careful prompt engineering for each component
- Performance gains may not scale linearly with increased complexity of pragmatic scenarios

## Confidence
Medium. The methodology appears sound with systematic testing of different integration strategies. However, the results are based on a specific domain and task type, and the performance gains, while significant, show that optimal integration depends on careful component selection. The manual interventions required (appending response options) suggest some limitations in the fully automated approach.

## Next Checks
- Verify the specific prompts used for each LLM component to understand their effectiveness
- Examine the distribution of human answer patterns to better understand the baseline
- Investigate the consistency of LLM outputs across different runs and conditions
- Compare the computational efficiency of hybrid models versus purely symbolic approaches
- Test the framework on additional pragmatic tasks beyond question-answering