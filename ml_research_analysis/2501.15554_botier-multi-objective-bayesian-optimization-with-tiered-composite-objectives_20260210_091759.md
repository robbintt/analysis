---
ver: rpa2
title: 'BoTier: Multi-Objective Bayesian Optimization with Tiered Composite Objectives'
arxiv_id: '2501.15554'
source_url: https://arxiv.org/abs/2501.15554
tags:
- objective
- optimization
- objectives
- function
- botier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BoTier, a novel composite objective function
  for multi-objective Bayesian optimization with hierarchical preferences over both
  experiment outcomes and input parameters. BoTier addresses limitations of existing
  methods by providing a fully differentiable, auto-differentiable scalarization that
  can flexibly represent hierarchies of objectives in scientific optimization problems.
---

# BoTier: Multi-Objective Bayesian Optimization with Tiered Composite Objectives

## Quick Facts
- arXiv ID: 2501.15554
- Source URL: https://arxiv.org/abs/2501.15554
- Reference count: 40
- Key outcome: BoTier consistently outperforms Chimera and EHVI in multi-objective optimization with hierarchical preferences, particularly when used as a composite objective requiring fewer experimental evaluations to satisfy multiple objectives simultaneously.

## Executive Summary
This paper introduces BoTier, a novel composite objective function for multi-objective Bayesian optimization with hierarchical preferences over both experiment outcomes and input parameters. BoTier addresses limitations of existing methods by providing a fully differentiable, auto-differentiable scalarization that can flexibly represent hierarchies of objectives in scientific optimization problems. The method was benchmarked across synthetic analytical surfaces and real-world chemistry problems, demonstrating that BoTier consistently outperforms alternative approaches including Chimera and EHVI, particularly when used as a composite objective.

## Method Summary
BoTier operates within standard Bayesian optimization but modifies the acquisition function computation via a composite scalarization. The method uses threshold-gated hierarchical scalarization where objective ψᵢ only contributes to the score after all superordinate objectives ψⱼ (j < i) exceed their thresholds tⱼ. This is implemented through Monte-Carlo posterior sampling where separate GPs are fit to each objective, Ξ is computed per sample using smooth approximations of Heaviside and min functions, and the Monte-Carlo acquisition is optimized via gradient-based methods. The approach enables auto-differentiable optimization while preserving hierarchical preference structure.

## Key Results
- BoTier outperforms Chimera and EHVI in all benchmark cases, requiring fewer experiments to satisfy hierarchical objectives
- Composite objective application (after surrogate inference) is consistently better than black-box scalarization (before inference)
- Independent single-task GPs are sufficient or superior to multi-output GPs for composite objectives
- Performance is robust across analytical surfaces and real-world chemistry problems including Suzuki-Miyaura coupling and enzymatic alkoxylation

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Threshold-Gated Scalarization
BoTier enables sample-efficient optimization by encoding strict priority ordering through threshold-gated objective contributions. The scalarization uses min(ψᵢ, tᵢ) and Heaviside product ∏H(ψⱼ - tⱼ) so that objective ψᵢ only contributes to the score after all superordinate objectives exceed their thresholds. This creates discontinuous gradient flow that prioritizes satisfying higher-tier objectives before lower-tier ones influence search direction. The core assumption is users can define meaningful satisfaction thresholds that partition the search space into acceptable/unacceptable regions per objective.

### Mechanism 2: Composite Objective Application via Monte-Carlo Posterior Sampling
Applying scalarization after surrogate model inference (composite) rather than before (black-box) improves sample efficiency by avoiding redundant learning of input-output relationships. Instead of computing Ξ on observations then fitting a GP to scalar scores, BoTier fits separate GPs to each objective, draws K posterior samples ξᵏᵢ(x) ~ p(fᵢ(x)|D), computes Ξ per sample, then averages utility via Monte-Carlo integration. This prevents the surrogate from "re-learning" known functional dependencies.

### Mechanism 3: Smooth Approximations Enabling Auto-Differentiation
Replacing discrete Heaviside and min functions with sigmoidal/log-sum-exp approximations enables gradient-based acquisition optimization without sacrificing ranking behavior. H(x) ≈ 1/(1 + e⁻ᵏˣ) and min(x₁,x₂) ≈ (x₁e⁻ᵏˣ¹ + x₂e⁻ᵏˣ²)/(e⁻ᵏˣ¹ + e⁻ᵏˣ²) provide continuously differentiable proxies. The smoothing parameter k controls approximation tightness, allowing backpropagation through the acquisition function for efficient optimization.

## Foundational Learning

- **Bayesian Optimization Loop (surrogate → acquisition → evaluation → update)**: BoTier operates within standard BO but modifies the acquisition function computation via composite scalarization. Understanding the base loop is prerequisite to seeing where BoTier plugs in. *Quick check: Can you explain why acquisition functions balance exploration vs. exploitation?*

- **Pareto Fronts and Hypervolume**: The paper positions BoTier against Pareto-oriented methods like EHVI. Understanding that Pareto methods map full trade-off surfaces while BoTier targets specific preference-defined regions clarifies when each is appropriate. *Quick check: Why would mapping the full Pareto front be inefficient when objective priorities are known in advance?*

- **Monte-Carlo Acquisition Functions (e.g., q-EI via sampling)**: BoTier's composite objective requires evaluating Ξ over posterior samples, not analytical expectations. The MC acquisition framework (popularized by BoTorch) is the computational substrate. *Quick check: How does MC acquisition enable non-analytical utility functions while remaining differentiable?*

## Architecture Onboarding

- **Component map**:
User Inputs → Objective Definitions (ψᵢ) + Thresholds (tᵢ) + Hierarchy Ordering
                        ↓
              Independent GP Surrogates (one per objective)
                        ↓
         Posterior Sampling (K samples per candidate x)
                        ↓
       BoTier Scalarization Ξ computed per sample (smooth H/min approximations)
                        ↓
       Monte-Carlo Acquisition (e.g., Expected Improvement on Ξ)
                        ↓
              Acquisition Optimization (gradient-based via auto-diff)
                        ↓
                    Next Experiment x*

- **Critical path**: Correctly specifying thresholds is the highest-leverage design choice. Poorly calibrated thresholds (infeasible or too loose) will dominate failure modes more than algorithm selection.

- **Design tradeoffs**:
  - **Composite vs. Black-box**: Composite is uniformly better in benchmarks but requires separate GP models per objective. For >5-10 objectives, computational cost may become noticeable.
  - **Single-task vs. Multi-output GPs**: Paper finds single-task GPs adequate or superior (SI Figs. S12, S17), but this may not generalize to all problem classes.
  - **Smoothing parameter k**: Higher k → better threshold approximation but potential numerical issues. Paper uses fixed k; sensitivity analysis in SI Figs. S13, S18 suggests moderate robustness.

- **Failure signatures**:
  - Optimization stalls without satisfying first objective → thresholds likely infeasible or initialization poor.
  - Satisfies early objectives but never later ones → thresholds for later objectives may be unrealistically tight given feasible region.
  - Erratic acquisition optimization → k too high causing gradient instability near thresholds.

- **First 3 experiments**:
  1. **Synthetic validation**: Implement BoTier on a 2-objective analytical surface (e.g., Branin with added input-cost objective) with known optima to verify threshold behavior and composite-vs-black-box delta before real-world deployment.
  2. **Threshold sensitivity sweep**: On target problem domain, run 3-5 campaigns with threshold variations (±10-20% per objective) to characterize feasible region boundaries before full optimization.
  3. **Baseline comparison on real data**: Run BoTier (composite), Chimera (black-box), and Sobol sampling in parallel on an emulated problem (e.g., Suzuki-Miyaura coupling dataset from paper) with budget of 30-50 evaluations to establish expected performance gains in your compute environment.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do composite objectives consistently outperform black-box approaches even when all objectives depend solely on experiment outputs, not inputs? The authors observed this surprising result but deferred systematic analysis, and their experiments showed multi-output GP surrogates did not improve performance over single-task models.

- **Open Question 2**: How should the smoothing parameter k be selected optimally, and how sensitive is BoTier's performance to this choice? The paper introduces k as a "user-defined smoothness parameter" for differentiable approximations but provides no selection guidance beyond showing benchmarks for different values.

- **Open Question 3**: Will incorporating priors over physically meaningful quantities further amplify the advantages of composite over black-box objectives? The authors foresee this effect will be even more pronounced when incorporating priors but did not test this conjecture.

## Limitations

- Performance in higher-dimensional input spaces (>6-8 dimensions) and with more than 3-4 objectives remains untested
- Choice of smoothing parameter k appears robust in tested ranges but sensitivity to this hyperparameter across diverse problem landscapes is incompletely characterized
- Does not establish whether BoTier would outperform other modern MOO acquisition functions like MESMO or ParEGO in all scenarios

## Confidence

- **High confidence**: BoTier's mechanism of threshold-gated hierarchical scalarization and its implementation as a differentiable composite objective are well-specified and reproducible.
- **Medium confidence**: The claim that composite objectives consistently outperform black-box alternatives across all problem types is supported by benchmarks but may not generalize to problems with strong objective correlations or different dimensionalities.
- **Medium confidence**: The assertion that independent single-task GPs are sufficient or superior to multi-output GPs for composite objectives holds in tested chemistry problems but may not extend to domains where cross-objective dependencies are critical.

## Next Checks

1. **Dimensionality Scaling Test**: Apply BoTier to analytical surfaces with 6-8 input dimensions and 3-5 objectives to verify performance retention as problem complexity increases.
2. **Correlation Sensitivity Analysis**: Construct benchmark problems with varying degrees of objective correlation (0-90%) to identify conditions where independent GP modeling breaks down versus when cross-correlation modeling becomes essential.
3. **Cross-Domain Benchmarking**: Compare BoTier against MESMO and ParEGO on non-chemical optimization problems (e.g., engineering design, financial portfolio optimization) to establish domain-specific versus universal advantages.