---
ver: rpa2
title: '"Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large
  Language Models'
arxiv_id: '2507.05424'
source_url: https://arxiv.org/abs/2507.05424
tags:
- context
- prompt
- instruction
- sentences
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoPE introduces a framework for measuring how large language models\
  \ (LLMs) use contextual versus parametric knowledge in multilingual settings. Using\
  \ a MultiWikiAtomic dataset spanning English, Spanish, and Danish, the study reveals\
  \ a persistent \u201Clost-in-the-later\u201D effect where models prioritize earlier\
  \ context over later, even when later information is relevant."
---

# "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models

## Quick Facts
- arXiv ID: 2507.05424
- Source URL: https://arxiv.org/abs/2507.05424
- Reference count: 29
- Primary result: Introduces CoPE framework revealing persistent "lost-in-the-later" positional bias in LLMs, where later context is underutilized even when relevant.

## Executive Summary
This paper introduces the Contextual and Parametric Knowledge Estimation (CoPE) framework to quantify how large language models (LLMs) balance contextual versus parametric knowledge in multilingual settings. Using a MultiWikiAtomic dataset spanning English, Spanish, and Danish, the study reveals a persistent "lost-in-the-later" effect where models prioritize earlier context over later, even when later information is relevant. CK scores plateau at about 70%, and reasoning models use context least, especially when prompted with Chain-of-Thought (CoT), which also shortens responses and lowers recall. A CK-informed prompting strategy significantly improves contextual grounding, reduces hallucination, and produces more balanced context usage. Applied to summarization, CK prompting increases factual consistency by 6-7% and reduces reliance on parametric memory, demonstrating its effectiveness across tasks.

## Method Summary
The CoPE framework measures how LLMs balance contextual knowledge (CK) versus parametric knowledge (PK) by decomposing context and responses into atomic sentences, then classifying each response atom as entailed by context (CK) or not (PK) using bidirectional NLI entailment scoring. The framework processes inputs through atomic segmentation using GPT-4o, followed by entailment classification with mDeBERTa-v3-base-xnli-multilingual-nli at threshold t=0.7. The MultiWikiAtomic dataset contains 15,000 atomic sentences across three languages from Wikipedia contexts of varying lengths (0-50 sentences). CK scores are computed as the percentage of response sentences entailed by context, with additional analysis of context recall distribution across quartiles to reveal positional biases.

## Key Results
- LLMs exhibit a persistent "lost-in-the-later" positional bias, with later context quartiles receiving disproportionately low recall despite being equally informative
- CK scores plateau around 70% across model sizes and languages, with reasoning models showing lowest contextual grounding
- Chain-of-Thought prompting degrades contextual grounding by reducing response length and encouraging synthesis beyond context
- CK-informed prompting significantly improves factual consistency in summarization by 6-7% while reducing parametric knowledge reliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit a structural positional bias toward earlier tokens, independent of content relevance or context length.
- Mechanism: Attention mechanisms and pretraining on sequential corpora may create an inductive bias where early tokens receive disproportionately higher effective attention weights. The paper's randomization test showed only ~5% shift in recall distribution when context order was scrambled, suggesting the effect is not content-driven but structurally embedded.
- Core assumption: The bias originates from pretraining dynamics rather than architectural constraints alone (hypothesized, not confirmed).
- Evidence anchors:
  - [abstract] "lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context"
  - [section 4.3] "this pattern holds across all tested models, including both general-purpose and reasoning-focused ones"
  - [corpus] Neighbor papers discuss context-memory conflicts but do not address positional grounding directly; limited external validation available.
- Break condition: If architectural interventions (e.g., position-agnostic attention) eliminate the bias without harming performance, the mechanism is primarily positional encoding-driven rather than pretraining-induced.

### Mechanism 2
- Claim: Higher contextual grounding (CK) correlates with reduced hallucination, but grounding does not guarantee factual correctness.
- Mechanism: When models anchor outputs to provided context via entailment-verified atomic sentences, they reduce reliance on parametric knowledge that may be outdated or fabricated. FActScore analysis showed CK-high responses converge across model sizes to similar factual consistency.
- Core assumption: Hallucination rates can be meaningfully approximated by measuring deviations from contextual grounding (behavioral proxy).
- Evidence anchors:
  - [section 5.6] "higher CK scores are associated with higher FActScores... supports our hypothesis that grounding responses in context reduces hallucination risk"
  - [section 5.5] "CK reflects grounding to input, not real-world truth, means models are following context even if that context is factually wrong"
  - [corpus] Neighbor "ParamMute" paper explores suppressing parametric knowledge for faithful RAG, offering convergent evidence that reducing PK reliance can improve faithfulness.
- Break condition: If CK-grounded responses on counterfactual contexts show higher hallucination than PK-grounded responses, the mechanism is confounded by context quality.

### Mechanism 3
- Claim: Chain-of-thought prompting degrades contextual grounding because reasoning steps consume token budget and encourage synthesis beyond context.
- Mechanism: CoT outputs are over 50% shorter in final answers; reasoning tokens displace context-grounded content. Additionally, CoT prompts encourage "reflection and synthesis beyond the given context," shifting the generation objective away from strict grounding.
- Core assumption: Token budget constraints directly trade off against context utilization (simplified model of generation).
- Evidence anchors:
  - [section 5] "CoT outputs are often over 50% shorter, as reasoning steps consume much of the token budget"
  - [section 4.2] reasoning models "step-by-step answers likely drift from the original input, which makes it harder to stay grounded"
  - [corpus] No direct external validation found in neighbor papers for CoT's effect on grounding specifically.
- Break condition: If increasing max_tokens for CoT prompts fully restores CK scores to non-CoT levels, the mechanism is primarily budget-driven rather than reasoning-induced.

## Foundational Learning

- Concept: **Natural Language Inference (NLI) / Entailment**
  - Why needed here: CoPE's CK/PK classification depends on bidirectional entailment scoring between context and response atomic sentences using mDeBERTa-v3.
  - Quick check question: Given context "Paris is the capital of France" and response "France's capital city is Paris," does entailment hold in both directions?

- Concept: **Atomic Sentence Segmentation**
  - Why needed here: The framework decomposes context and responses into minimal standalone propositions to enable fine-grained attribution.
  - Quick check question: Is "Marie Curie won two Nobel Prizes and discovered radium" atomic, or should it be split?

- Concept: **Context Recall Distribution**
  - Why needed here: Understanding how models retrieve from segmented context (quartiles) reveals positional biases that aggregate CK scores mask.
  - Quick check question: If a model has CK=70% but CR_Q4=10% and CR_Q1=40%, what problem does this indicate?

## Architecture Onboarding

- Component map: Input context -> Atomic segmentation (GPT-4o) -> Quartile segmentation -> NLI classification (mDeBERTa-v3) -> CK/PK classification -> Aggregation

- Critical path:
  1. Input context → atomization → segment into quartiles
  2. Model response → atomization
  3. Each response atom scored against all context atoms via NLI
  4. Aggregate to CK score and per-quartile recall distribution

- Design tradeoffs:
  - Threshold t=0.7 chosen via calibration; lower increases CK false positives, higher increases PK false positives
  - Atomic segmentation adds inference cost but enables fine-grained analysis
  - Multilingual NLI model traded some English accuracy for cross-lingual coverage

- Failure signatures:
  - CK scores near 100% with high hallucination → context itself contains errors
  - Flat CR distribution but low overall CK → model underutilizes all context equally
  - High variance in CK across repeated runs → temperature/sampling issues

- First 3 experiments:
  1. Replicate CK scoring on a held-out domain (e.g., legal documents) to test generalization beyond Wikipedia-style text
  2. Ablate the CK Prompt's two components (strict grounding instruction vs. balanced quartile instruction) to isolate which drives improvement
  3. Test whether increasing max_tokens for CoT prompts recovers CK scores, validating the token-budget mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the "lost-in-the-later" effect primarily caused by architectural attention mechanisms or a bias induced by pretraining on sequential data?
- Basis in paper: [explicit] The authors state in Section 4.3 that the effect persists across architectures and input orders, suggesting "a potential explanation is that pretraining introduces a bias toward earlier tokens... Future work should explore this direction."
- Why unresolved: The paper rules out surface-level data order and long-context limitations as causes but does not isolate the specific mechanism (architecture vs. training data distribution) driving the bias.
- What evidence would resolve it: A comparative analysis of models trained on datasets with deliberately shuffled document structures versus standard sequential corpora.

### Open Question 2
- Question: How does the balance between contextual knowledge (CK) and parametric knowledge (PK) change in crosslingual scenarios?
- Basis in paper: [explicit] In the Conclusion, the authors state: "Future work may explore more complex multilingual settings, including crosslingual scenarios, to further refine CK-PK analysis."
- Why unresolved: The current experiments are restricted to monolingual settings (English, Spanish, Danish) where the input and output languages match.
- What evidence would resolve it: Applying the CoPE framework to a dataset where the provided context is in one language and the model is prompted to generate a response in a different target language.

### Open Question 3
- Question: Can the CoPE framework maintain accuracy when applied to noisy, informal text such as social media data or conversational transcripts?
- Basis in paper: [inferred] The Limitations section notes CoPE has not been tested on conversational data or tweets, which are "noisy and informal," making it challenging to create reliable atomic sentences.
- Why unresolved: The MultiWikiAtomic dataset relies on structured Wikipedia sentences; it is unclear if the atomic segmentation and NLI classification hold up against fragmented or slang-heavy text.
- What evidence would resolve it: Validating CoPE's atomic segmentation and entailment scores on datasets comprising informal social media dialogue or meeting transcripts.

## Limitations

- The CK score measures adherence to context rather than factual accuracy, meaning the framework cannot distinguish between well-grounded misinformation and hallucination
- The atomic sentence segmentation process relies heavily on GPT-4o's interpretation, which may introduce inconsistencies across languages and topics
- The framework assumes parametric knowledge can be cleanly separated from contextual knowledge, but in practice these knowledge sources may overlap in ways the entailment-based approach cannot capture

## Confidence

- **High Confidence**: The "lost-in-the-later" positional bias finding across multiple models and languages is robust, with the randomization test showing minimal content influence on recall distribution. The correlation between higher CK scores and reduced hallucination (lower FActScore) is also well-supported across experimental conditions.

- **Medium Confidence**: The claim that reasoning models use context least is supported by the data but may be confounded by their tendency to synthesize beyond context rather than a fundamental inability to ground. The effectiveness of CK-informed prompting shows consistent improvements but relies on a relatively small prompt modification.

- **Low Confidence**: The assertion that CoT's degradation of contextual grounding is primarily due to token budget constraints lacks direct causal evidence. The mechanism by which positional bias emerges from pretraining dynamics remains speculative without architectural intervention studies.

## Next Checks

1. **Architectural Intervention Test**: Apply position-agnostic attention mechanisms (e.g., relative positional encoding variants) to a base model and measure whether the lost-in-the-later effect persists. If eliminated without harming overall performance, this would confirm the positional encoding mechanism.

2. **Token Budget Manipulation**: Run CoT prompts with doubled max_tokens (4096 instead of 2048) on reasoning models and measure whether CK scores recover to non-CoT levels. This would directly test whether the degradation is budget-driven versus reasoning-induced.

3. **Cross-Domain Generalization**: Apply the CoPE framework to legal documents or scientific literature with dense, technical language. If positional biases persist in domains with different writing styles and information density, this would strengthen the claim that the effect is structural rather than domain-specific.