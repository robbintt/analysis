---
ver: rpa2
title: Efficient Swap Regret Minimization in Combinatorial Bandits
arxiv_id: '2602.02087'
source_url: https://arxiv.org/abs/2602.02087
tags:
- regret
- algorithm
- swap
- where
- combinatorial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first efficient no-swap-regret algorithm
  for combinatorial bandits, a setting where the number of actions grows exponentially
  with the problem dimensionality. The algorithm achieves polylogarithmic dependence
  on the action space size both in regret and per-iteration complexity, resolving
  a long-standing open problem.
---

# Efficient Swap Regret Minimization in Combinatorial Bandits

## Quick Facts
- arXiv ID: 2602.02087
- Source URL: https://arxiv.org/abs/2602.02087
- Reference count: 40
- First efficient no-swap-regret algorithm for combinatorial bandits with polylogarithmic dependence on action space size

## Executive Summary
This paper introduces the first efficient algorithm for swap regret minimization in combinatorial bandit problems, where the action space grows exponentially with problem dimensionality. The authors develop a multi-scale framework combining lazy learners with different update frequencies, novel unbiased reward estimation, and careful variance control to achieve polylogarithmic regret bounds. The algorithm, called Swap-ComBCP, decomposes swap regret into external regret terms that can be efficiently bounded using barycentric spanner exploration.

## Method Summary
The paper presents a novel approach that transforms the swap regret minimization problem into a collection of external regret problems. The key innovation is a multi-scale framework where multiple lazy learners operate at different update frequencies, allowing efficient exploration of the exponentially large combinatorial action space. The algorithm uses a novel unbiased reward estimator with careful variance control and employs barycentric spanners for exploration. This decomposition enables the achievement of polylogarithmic regret bounds while maintaining per-iteration complexity that is polynomial in the problem parameters.

## Key Results
- First efficient no-swap-regret algorithm for combinatorial bandits
- Achieves swap regret of O(T log(d log T)/log T)
- Polylogarithmic dependence on action space size both in regret and per-iteration complexity
- Algorithm is implementable in several well-studied combinatorial settings (m-sets, spanning trees, paths)
- Lower bound shows result is tight for polynomial-time regimes

## Why This Works (Mechanism)
The algorithm works by decomposing the swap regret objective into a collection of external regret problems, which can be solved efficiently using lazy learning techniques. The multi-scale framework allows learners operating at different time scales to explore the combinatorial space efficiently without incurring prohibitive computational costs. The barycentric spanner exploration ensures sufficient coverage of the action space while the novel reward estimator maintains unbiasedness despite the complex feedback structure. This decomposition strategy transforms an intractable problem into a tractable one by leveraging the structure of combinatorial spaces.

## Foundational Learning

**Combinatorial Bandits**: Multi-armed bandit problems where actions are subsets of items from a ground set, rather than individual arms. *Why needed*: The exponential growth of the action space makes standard bandit algorithms inefficient. *Quick check*: Can be modeled as choosing m-sets, spanning trees, or paths from a graph.

**Swap Regret**: A stronger notion of regret where the benchmark allows for correlation between actions chosen by the player and those of an adversary. *Why needed*: Provides stronger performance guarantees than external regret in online learning. *Quick check*: Measures performance against adaptive adversaries that can correlate their actions.

**Barycentric Spanners**: A set of distributions over actions that can approximate any action distribution within a constant factor. *Why needed*: Enables efficient exploration in large combinatorial action spaces. *Quick check*: For matroid constraints, exists with poly(d, 1/ε) distributions to ε-approximate any distribution.

**Lazy Learning**: A technique where updates are performed infrequently at geometrically increasing intervals. *Why needed*: Reduces computational complexity in online learning with large action spaces. *Quick check*: Updates occur at times t = 2^i for increasing i, trading off regret for computational efficiency.

**Unbiased Reward Estimation**: Constructing estimators that maintain expected reward accuracy despite partial information feedback. *Why needed*: Essential for unbiased gradient estimates in bandit settings. *Quick check*: Estimator expectation equals true reward despite only observing one action's outcome.

## Architecture Onboarding

**Component Map**: Multi-scale lazy learners -> Barycentric spanner exploration -> Unbiased reward estimation -> Regret decomposition -> Swap regret minimization

**Critical Path**: The algorithm alternates between exploration (sampling from barycentric spanners) and exploitation (following lazy learner recommendations), with the multi-scale framework ensuring both sufficient exploration and computational efficiency.

**Design Tradeoffs**: The algorithm trades off between exploration frequency and computational efficiency through the multi-scale framework, where more frequent updates provide better regret bounds but at higher computational cost. The barycentric spanners provide a compact representation of the action space but require careful construction for each combinatorial structure.

**Failure Signatures**: Poor performance may manifest as high variance in reward estimates, insufficient exploration of the combinatorial space, or computational bottlenecks when sampling from large barycentric spanners. The algorithm may also suffer if the underlying combinatorial structure lacks efficient sampling oracles.

**First Experiments**:
1. Implement and test on the m-sets combinatorial setting with small ground sets to verify theoretical guarantees
2. Compare against standard combinatorial bandit algorithms on synthetic spanning tree instances
3. Evaluate performance under non-stationary reward distributions to test robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes linear reward structure, limiting applicability to non-linear reward scenarios
- Requires efficient sampling oracles for underlying combinatorial structures, which may not exist for all problem variants
- Focuses on full information feedback, with potential challenges in extending to bandit feedback scenarios
- May face practical implementation challenges in extremely large-scale combinatorial settings

## Confidence
High: Theoretical regret bounds and algorithmic efficiency are rigorously proven
Medium: Practical implementation and empirical performance across diverse settings
Low: Extension to non-stationary environments and bandit feedback scenarios

## Next Checks
1. Implement the algorithm for the spanning tree combinatorial setting and empirically verify the claimed polylogarithmic complexity in practice, comparing against baseline approaches
2. Test the algorithm's performance under non-stationary reward distributions to evaluate its robustness beyond the stationary setting assumed in the analysis
3. Extend the theoretical analysis to bandit feedback scenarios (partial information) and derive corresponding regret bounds for this more challenging setting