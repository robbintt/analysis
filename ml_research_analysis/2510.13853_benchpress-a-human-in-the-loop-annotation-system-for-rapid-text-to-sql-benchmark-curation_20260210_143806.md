---
ver: rpa2
title: 'BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark
  Curation'
arxiv_id: '2510.13853'
source_url: https://arxiv.org/abs/2510.13853
tags:
- annotation
- benchpress
- enterprise
- text-to-sql
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BenchPress addresses the challenge of creating high-quality, domain-specific
  text-to-SQL benchmarks for enterprise data warehouses, where existing public benchmarks
  fail to capture schema complexity, domain terminology, and privacy constraints.
  The system combines retrieval-augmented generation with human-in-the-loop annotation,
  using LLMs to propose multiple natural language descriptions for SQL queries that
  domain experts then refine and validate.
---

# BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation

## Quick Facts
- arXiv ID: 2510.13853
- Source URL: https://arxiv.org/abs/2510.13853
- Authors: Fabian Wenz; Omar Bouattour; Devin Yang; Justin Choi; Cecil Gregg; Nesime Tatbul; Çağatay Demiralp
- Reference count: 30
- Primary result: 93% annotation accuracy vs 73.9% manual, 84% time reduction (102.1→16.1 min) on enterprise queries

## Executive Summary
BenchPress addresses the challenge of creating high-quality, domain-specific text-to-SQL benchmarks for enterprise data warehouses, where existing public benchmarks fail to capture schema complexity, domain terminology, and privacy constraints. The system combines retrieval-augmented generation with human-in-the-loop annotation, using LLMs to propose multiple natural language descriptions for SQL queries that domain experts then refine and validate. In a user study with 18 participants, BenchPress achieved 93% annotation accuracy compared to 73.9% for manual annotation, reduced annotation time by 84% on enterprise queries, and produced more semantically faithful natural language descriptions as measured by backtranslation fidelity.

## Method Summary
BenchPress is an annotation system that generates natural language descriptions for SQL queries using retrieval-augmented generation (RAG). Users upload enterprise SQL logs and schemas or select from existing benchmarks (Beaver, Bird, Spider, Fiben). The system parses SQL using sqlglot, builds vector indexes with Sentence-BERT embeddings, and retrieves similar examples plus relevant tables to ground LLM prompts. For each query, the LLM generates 4 candidate NL descriptions, which humans then select, rank, or edit. The system optionally decomposes nested queries into CTEs to improve accuracy. Annotation quality is measured by accuracy (expert validation) and backtranslation fidelity (SQL regeneration from NL).

## Key Results
- BenchPress achieved 93% annotation accuracy versus 73.9% for manual annotation in user study
- Reduced annotation time by 84% on enterprise queries (102.1 to 16.1 minutes)
- Produced higher backtranslation fidelity scores for semantic faithfulness of NL descriptions

## Why This Works (Mechanism)

### Mechanism 1: Contextual Grounding via RAG
The system improves annotation relevance and accuracy by retrieving semantically similar SQL queries and schema context to ground the LLM, rather than relying on the model's parametric memory alone. BenchPress uses dense vector embeddings to index uploaded SQL logs and schemas. During the annotation loop, relevant tables and prior examples are retrieved and injected into the prompt, providing the LLM with domain-specific usage patterns. Effectiveness degrades if the schema is undocumented or if vector embeddings fail to capture semantic nuance of cryptic column names.

### Mechanism 2: Cognitive Offloading via Candidate Selection
Shifting the human role from synthesis to verification reduces cognitive load and speeds up the annotation process. The LLM generates four candidate natural language descriptions. Annotators select, rank, or edit these drafts rather than generating text from zero. This leverages human strengths in error correction while automating text generation. If the LLM consistently hallucinates or fails to map domain terminology, humans revert to manual synthesis, negating the speed advantage.

### Mechanism 3: Complexity Reduction via Decomposition
Decomposing nested SQL queries into linear components (CTEs) improves annotation accuracy for complex enterprise workloads. For nested queries, BenchPress rewrites SQL into Common Table Expressions, generates descriptions for simpler sub-queries, then recomposes them into final explanation. This reduces reasoning burden on both LLM and annotator. If SQL query is obfuscated or uses dynamic SQL strings that parser cannot convert into valid CTEs, decomposition fails, forcing annotation on raw complex query.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** BenchPress relies on RAG to bridge gap between generic LLM knowledge and private, domain-specific schemas. Without RAG, LLM would struggle to interpret enterprise-specific jargon.
  - **Quick check question:** If system retrieves SQL example with different table aliases than current query, does LLM ignore retrieval or get confused by mismatch?

- **Concept:** Common Table Expressions (CTEs)
  - **Why needed here:** This is structural tool BenchPress uses to flatten complexity. Understanding CTEs is necessary to debug why recomposed description might fail to capture nested subquery logic.
  - **Quick check question:** Can you identify distinct logical blocks in nested `WHERE` clause that would benefit from being extracted into separate CTE descriptions?

- **Concept:** Backtranslation (Round-Trip Testing)
  - **Why needed here:** This is primary evaluation metric for semantic fidelity in paper. Tests if annotated NL preserves enough information to regenerate original SQL.
  - **Quick check question:** If backtranslated SQL query runs but retrieves different rows than original, does failure lie in NL description (ambiguity) or LLM's interpretation?

## Architecture Onboarding

- **Component map:** Client (Browser) -> Server (SQL logs/schemas storage, RAG vector store, sqlglot parsing) -> LLM Interface (prompt construction and candidate generation)
- **Critical path:** The "Cold-Start" Annotation Loop. First query in project has no prior examples to retrieve. System must rely solely on schema retrieval and LLM parametric knowledge. User's first edits are crucial as they populate RAG index for future queries.
- **Design tradeoffs:**
  - Privacy vs. RAG Efficiency: SQL logs stored server-side to enable vector search, rather than keeping purely local. Necessary because browser storage is too limited for enterprise logs.
  - Latency vs. Context: Retrieving all relevant schema columns vs. top-k examples. System prioritizes schema completeness over example volume to manage prompt length.
- **Failure signatures:**
  - Schema Ambiguity: Multiple tables with identical column names leading to incorrect context retrieval
  - Hallucination Loop: LLM candidates consistently using terms not found in schema, requiring constant human correction
  - Decomposition Errors: Parser failing to validly rewrite complex nested queries into CTEs, causing system crash or malformed prompts
- **First 3 experiments:**
  1. Cold Start Profiling: Measure annotation time and accuracy for first 10 queries vs last 10 queries in session to quantify impact of growing RAG index
  2. Decomposition Ablation: Disable CTE decomposition step for highly nested queries and measure drop in backtranslation fidelity scores
  3. Context Sensitivity: Intentionally remove schema retrieval component and observe rate of hallucinated table references in generated candidates

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating text-to-SQL generation for iterative validation further increase annotation accuracy and speed beyond current SQL-to-NL approach? [explicit] "While the current system focuses on SQL-to-text annotation, a natural next step is to incorporate text-to-SQL generation for iterative validation."
- **Open Question 2:** Do state-of-the-art models overfit to canonical NL formulations in public benchmarks, failing to generalize to realistic, ambiguous, or underspecified query variants? [explicit] "Although many models achieve near-perfect performance on these datasets, it remains unclear whether they have overfit to canonical NL formulations."
- **Open Question 3:** How does initializing BenchPress with examples from public text-to-SQL datasets affect annotation quality and efficiency compared to cold-start condition evaluated in user study? [explicit] "BenchPress therefore assumes a cold start by default, while in future enterprise use cases we intend to offer both options."
- **Open Question 4:** Can BenchPress maintain annotation quality and efficiency when deployed in fully local, privacy-preserving configuration without centralized server-side storage? [inferred] Current architecture stores SQL logs on centralized server due to browser storage limitations, creating potential tension with enterprise privacy requirements.

## Limitations
- The 4-candidate generation threshold appears arbitrary without ablation studies showing optimal candidate count
- User study sample size (18 participants) limits generalizability, and all participants were presumably familiar with SQL
- RAG mechanism's effectiveness heavily depends on quality of vector embeddings and semantic similarity between SQL syntax and natural language intent

## Confidence
- **High Confidence:** Measured reduction in annotation time (84% improvement) and absolute accuracy figures (93% vs 73.9%) are well-supported by user study methodology
- **Medium Confidence:** Mechanism claims about RAG contextual grounding and cognitive offloading through candidate selection are logically sound but rely on assumptions about LLM behavior
- **Low Confidence:** Complexity reduction via CTE decomposition lacks direct empirical validation in paper

## Next Checks
1. Cold-Start Performance Analysis: Measure annotation accuracy and time for first 10 queries in new project versus last 10 queries to quantify how much growing RAG index contributes to performance improvements
2. Decomposition Necessity Test: Run controlled experiment disabling CTE decomposition step for nested queries (complexity ≥3) and measure specific drop in backtranslation fidelity to validate this mechanism's contribution
3. Context Sensitivity Evaluation: Systematically remove schema retrieval while keeping SQL query and prior examples, then measure increase in hallucinated table references to quantify RAG's impact on domain terminology accuracy