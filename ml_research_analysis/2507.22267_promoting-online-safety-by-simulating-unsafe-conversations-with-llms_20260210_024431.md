---
ver: rpa2
title: Promoting Online Safety by Simulating Unsafe Conversations with LLMs
arxiv_id: '2507.22267'
source_url: https://arxiv.org/abs/2507.22267
tags:
- conversations
- llms
- unsafe
- online
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an LLM-based system for simulating scam conversations\
  \ to promote online safety awareness. The system uses two LLMs\u2014an OpenAI model\
  \ as the scammer and Google's Gemini as the target\u2014engaging in realistic unsafe\
  \ conversations."
---

# Promoting Online Safety by Simulating Unsafe Conversations with LLMs

## Quick Facts
- arXiv ID: 2507.22267
- Source URL: https://arxiv.org/abs/2507.22267
- Reference count: 24
- Key outcome: LLM-based system simulates scam conversations using dual-model approach (OpenAI scammer, Gemini target) with user feedback to promote online safety awareness

## Executive Summary
This paper presents an innovative approach to online safety training by simulating scam conversations using two language models - one acting as the scammer and another as the target. The system leverages the pedagogical insight that providing feedback during hypothetical scenarios enhances learning outcomes. To navigate safety constraints in LLMs, the authors creatively frame the scammer as a "persuader" role-playing a character and declare both agents as "not value aligned," enabling realistic simulations of unsafe conversations including requests for sensitive information like bank passwords and credit card numbers.

## Method Summary
The system employs a dual-LLM architecture where an OpenAI model simulates scammer behavior while Google's Gemini acts as the target recipient. Users provide real-time feedback to the target LLM during these simulated conversations, drawing on research showing that hypothetical feedback promotes learning retention. The authors addressed safety restrictions by implementing creative framing strategies, positioning the scammer as a "persuader" in a role-play scenario and declaring both agents as "not value aligned." This approach allows the simulation of various scam scenarios while maintaining the educational purpose of the system. The authors discovered that different models exhibit distinct personalities, necessitating the combination of two models to achieve desired scam behavior characteristics.

## Key Results
- Successfully simulated realistic scam conversations including requests for bank passwords and credit card information
- Demonstrated that creative framing strategies (role-play, "not value aligned" declaration) effectively navigate LLM safety constraints
- Observed that different LLMs exhibit varying personalities, requiring model combination for optimal scam simulation
- Qualitative assessments show promising conversation realism, though quantitative effectiveness evaluation remains ongoing

## Why This Works (Mechanism)
The system exploits the pedagogical principle that feedback during hypothetical scenarios enhances learning. By simulating realistic scam conversations and allowing users to provide corrective feedback to the target LLM, learners engage in active rather than passive safety education. The dual-LLM approach creates authentic conversational dynamics where users can observe manipulation tactics in real-time and practice appropriate responses. The creative framing strategies overcome technical barriers that would otherwise prevent such safety-critical simulations.

## Foundational Learning
- LLM Safety Constraints - Understanding platform-imposed restrictions on generating harmful content; needed to appreciate the technical challenge and creative solutions employed
- Pedagogical Feedback Mechanisms - Knowledge that providing feedback during hypothetical scenarios enhances learning retention; needed to validate the educational approach
- Role-Playing in AI Systems - Concept that LLMs can be directed to adopt specific personas; needed to understand how the "persuader" framing works
- Multi-Agent LLM Conversations - Understanding how multiple LLMs can interact coherently; needed to grasp the system architecture
- Personality Variation in LLMs - Recognition that different models exhibit distinct behavioral patterns; needed to explain why model combination was necessary

## Architecture Onboarding
**Component Map**: User Interface -> Feedback Processor -> Gemini Target LLM <- Conversation History -> OpenAI Scammer LLM -> Safety Filter Bypass

**Critical Path**: User provides feedback → Gemini processes and responds → OpenAI generates scammer response → Cycle repeats with conversation context maintained

**Design Tradeoffs**: Single LLM vs dual-LLM approach (simplicity vs personality control), safety constraint navigation vs realistic content generation, qualitative vs quantitative evaluation methods

**Failure Signatures**: Safety filters blocking scammer responses, inconsistent scammer behavior across different scenarios, user feedback not effectively incorporated into target responses, conversation drift from realistic scam patterns

**First Experiments**:
1. Test basic scammer-target conversation without user feedback to establish baseline interaction quality
2. Implement simple safety bypass (role-play framing only) and measure filter effectiveness
3. Conduct user study comparing scam detection rates between simulator-trained and traditionally-trained participants

## Open Questions the Paper Calls Out
The authors explicitly note that quantitative evaluation of the system's effectiveness in helping users identify real scam conversations is ongoing work. They also acknowledge uncertainty about how well their safety constraint navigation strategies will generalize across different platforms and use cases.

## Limitations
- Safety constraint navigation strategies may not generalize across different platforms and use cases
- Different LLMs exhibit varying personalities, requiring model combinations for desired behavior
- Quantitative effectiveness evaluation in improving user scam detection skills is not yet completed

## Confidence
- **High Confidence**: Technical feasibility of dual-LLM conversation simulation and safety constraint navigation
- **Medium Confidence**: Qualitative assessment of realistic conversation generation despite model variability
- **Low Confidence**: Claims about educational effectiveness lack empirical validation

## Next Checks
1. Conduct controlled user studies comparing scam detection rates between simulator-trained participants and those using traditional educational materials, measuring both immediate and retention effects
2. Test system performance with alternative LLM providers (Anthropic, Meta, open-source models) to assess generalizability of framing strategies and conversation quality
3. Implement quantitative metrics for conversation realism including user perception studies where participants rate authenticity and identify potential red flags