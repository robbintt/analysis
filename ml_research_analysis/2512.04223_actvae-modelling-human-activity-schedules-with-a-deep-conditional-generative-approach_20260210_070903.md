---
ver: rpa2
title: 'ActVAE: Modelling human activity schedules with a deep conditional generative
  approach'
arxiv_id: '2512.04223'
source_url: https://arxiv.org/abs/2512.04223
tags:
- schedules
- activity
- labels
- generative
- actvae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce ActVAE, a deep conditional generative model for activity
  schedules, combining structured latent generative modeling (VAE) with conditional
  capability. This allows schedules to be generated based on input labels such as
  age or employment status.
---

# ActVAE: Modelling human activity schedules with a deep conditional generative approach

## Quick Facts
- **arXiv ID**: 2512.04223
- **Source URL**: https://arxiv.org/abs/2512.04223
- **Reference count**: 14
- **Primary result**: Introduces ActVAE, a conditional VAE that generates more realistic and diverse activity schedules than purely generative or purely conditional baselines

## Executive Summary
ActVAE is a deep conditional generative model for human activity schedules that combines structured latent generative modeling with conditional capability. The model generates activity schedules based on input labels such as age or employment status, addressing a key limitation of existing generative models. Evaluated against two baselines using joint density estimation and mutual information analysis, ActVAE demonstrates superior performance in generating realistic and diverse schedules, particularly where label effects are strong. The model is practical for transport demand modeling, trains rapidly, and scales to large populations.

## Method Summary
ActVAE is a conditional VAE that encodes activity schedules as sequences of activity types and continuous durations, conditioned on demographic labels. The model uses a 4-layer LSTM encoder/decoder with latent dimension 6, injecting label information through addition to encoder hidden states and decoder inputs. Training optimizes a combined loss of cross-entropy for activities, MSE for durations, and KL divergence for regularization. The model generates schedules by sampling from the latent space and conditioning on labels, producing sequences that respect the 24-hour time budget constraint.

## Key Results
- ActVAE generates more realistic and diverse schedules than both purely generative (GenerativeRNN) and purely conditional (ConditionalRNN) baselines
- Random variation in schedules is found to be relatively more significant than label-driven variation
- The model demonstrates practical utility for transport demand modeling with rapid training and scalability to large populations

## Why This Works (Mechanism)

### Mechanism 1: Disentanglement of Randomness via Latent Bottleneck
The architecture forces a bottleneck using a VAE, encouraging the latent space to ignore information available in labels while storing only residual randomness required to reconstruct specific schedules. This enables controlled generation while preserving diversity.

### Mechanism 2: Continuous Encoding for Temporal Precision
Representing time continuously as (Activity Token, Continuous Duration) pairs rather than discrete buckets provides more precise duration representation and better data efficiency than massive classification approaches.

### Mechanism 3: Inverse Label Weighting for Distributional Fidelity
Reweighting the loss function by inverse label frequency prevents the model from defaulting to average behavior for rare demographics, improving joint density estimation across the full label distribution.

## Foundational Learning

- **Variational Auto-Encoders (VAE) & The Reparameterization Trick**: Required to understand how backpropagation flows through stochastic sampling steps. Quick check: Explain why we sample ε ~ N(0,1) and compute z = μ + σ · ε instead of sampling z directly during training.
- **Mutual Information (MI) Estimation**: Used to quantify "disentanglement" and prove the latent space isn't memorizing labels. Quick check: If MI(z; y) were high (e.g., equal to I(x; y)), what would happen to conditional generation capability?
- **Earth Mover's Distance (EMD) / Wasserstein Metric**: Preferred over KL divergence for evaluating distribution similarity in this context because standard accuracy doesn't apply to distributions.

## Architecture Onboarding

- **Component map**: Schedule (Activity Token + Continuous Duration) + Labels (Categorical Embeddings) -> Encoder (LSTM) -> Latent Block (μ, σ) -> Decoder (LSTM) -> Softmax (Activity) + Sigmoid (Duration)
- **Critical path**: Data prep (NTS trip diaries to continuous format) → Conditionality injection (label addition vs. concatenation) → Normalization (24-hour budget enforcement)
- **Design tradeoffs**: Increasing label embedding size improves conditional accuracy but degrades generative creativity; larger latent dimensions capture more variation but risk label entanglement
- **Failure signatures**: "Average" trap (identical outputs for different inputs), infeasibility (non-home-based schedules), underestimation of change (generated schedules too similar across labels)
- **First 3 experiments**: 1) Latent sweep (varying dimensions 2, 6, 16) to find disentanglement elbow, 2) Ablation on injection method (addition vs. concatenation) measuring EMD, 3) Disentanglement check (train classifier to predict y from z)

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on NTS 2023 data may not capture emerging patterns like post-pandemic remote work effects
- Assumes stationarity in label-activity relationships across different geographic contexts
- Evaluation focuses on structural feasibility but doesn't explicitly validate rare behavioral phenomena

## Confidence

| Claim | Confidence |
|-------|------------|
| Feasible schedule generation (EMD metrics) | High |
| Label-driven variation is "relatively minor" | Medium |
| Superiority over baselines in joint density estimation | Medium |

## Next Checks
1. **Cross-validation on temporal splits**: Train on NTS 2019-2022, test on 2023 to assess temporal generalization
2. **Downstream application testing**: Use ActVAE-generated populations in traffic assignment model and compare congestion estimates
3. **Rare label behavior analysis**: Systematically generate schedules for all label combinations including rare ones and validate against NTS microdata