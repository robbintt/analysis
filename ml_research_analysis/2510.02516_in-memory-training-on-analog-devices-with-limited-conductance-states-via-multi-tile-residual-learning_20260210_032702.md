---
ver: rpa2
title: In-memory Training on Analog Devices with Limited Conductance States via Multi-tile
  Residual Learning
arxiv_id: '2510.02516'
source_url: https://arxiv.org/abs/2510.02516
tags:
- analog
- training
- tile
- equation
- wmin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training deep neural networks
  on analog in-memory computing hardware with limited conductance states (typically
  4-5 bits), which causes convergence failure due to quantization noise. The authors
  propose a multi-tile residual learning framework that represents high-precision
  weights using multiple low-precision analog tiles with geometric scaling factors,
  where each tile learns to compensate the residual error left by preceding tiles.
---

# In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning

## Quick Facts
- arXiv ID: 2510.02516
- Source URL: https://arxiv.org/abs/2510.02516
- Authors: Jindan Li; Zhaoxian Wu; Gaowen Liu; Tayfun Gokmen; Tianyi Chen
- Reference count: 40
- Primary result: Multi-tile residual learning framework achieves comparable accuracy to mixed-precision digital approaches on analog in-memory hardware with only moderate overhead

## Executive Summary
This paper addresses the fundamental challenge of training deep neural networks on analog in-memory computing hardware with limited conductance states (4-5 bits), where quantization noise typically causes convergence failure. The authors propose a novel multi-tile residual learning framework that represents high-precision weights using multiple low-precision analog tiles with geometric scaling factors. Each tile learns to compensate the residual error left by preceding tiles, enabling accurate training despite hardware limitations.

Theoretical analysis demonstrates that the optimality gap shrinks exponentially with the number of tiles, providing strong justification for the approach. Extensive experiments on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 show consistent accuracy improvements over state-of-the-art in-memory analog training methods, with the framework maintaining accuracy where other methods fail, particularly when scaling to larger models.

## Method Summary
The multi-tile residual learning framework represents each high-precision weight using multiple low-precision analog tiles arranged in a geometric progression. The first tile stores the coarse weight representation, while subsequent tiles learn to compensate for the quantization error of previous tiles. During training, gradients are backpropagated through all tiles, and each tile's conductance is updated to minimize its respective residual error. The geometric scaling between tiles ensures efficient weight representation while maintaining convergence properties. The framework operates within standard in-memory computing architectures by treating each tile as an independent weight storage unit that can be accessed and updated during training iterations.

## Key Results
- Achieved comparable accuracy to mixed-precision digital approaches while using only 4-5 bit analog conductance states
- Exponential reduction in optimality gap with increasing number of tiles, as proven theoretically
- Consistent accuracy improvements across all tested datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) compared to state-of-the-art analog training methods
- Maintained accuracy when scaling to larger models where competing approaches showed significant degradation

## Why This Works (Mechanism)
The method works by decomposing high-precision weights into a series of low-precision components that collectively approximate the full precision weight. Each tile captures a different scale of the weight representation, with geometric scaling ensuring that finer details are progressively captured by subsequent tiles. During training, the residual error from each tile's quantization becomes the target for the next tile, creating a hierarchical compensation structure. This approach effectively distributes the quantization noise across multiple tiles, preventing any single tile from accumulating excessive error. The geometric progression ensures that computational resources are allocated efficiently, with more bits dedicated to coarser weight components that have greater impact on model accuracy.

## Foundational Learning
**In-memory computing fundamentals**: Understanding how analog crossbar arrays perform matrix-vector multiplication directly in memory by leveraging Ohm's law and Kirchhoff's current law. Needed to grasp the hardware constraints and opportunities. Quick check: Can you explain how conductance values in a crossbar array multiply with input voltages to produce output currents?

**Weight quantization and its impact**: Knowledge of how reducing weight precision affects model accuracy and training dynamics, particularly the trade-off between hardware efficiency and computational accuracy. Needed to understand why 4-5 bit precision typically causes convergence failure. Quick check: What is the typical accuracy drop when reducing from 32-bit to 4-bit weights in standard neural networks?

**Residual learning principles**: Understanding how residual connections and error compensation work in deep learning architectures. Needed to grasp how each tile compensates for previous tile errors. Quick check: How do residual connections help with training very deep networks?

## Architecture Onboarding

**Component map**: Input data -> Analog crossbar array (Tile 1) -> Residual error computation -> Analog crossbar array (Tile 2) -> ... -> Final weight output. Each tile operates independently but contributes to a unified weight representation through geometric scaling.

**Critical path**: Data flows through all tiles sequentially during weight computation, with gradients flowing backward through all tiles during training. The critical timing path involves the slowest tile's update cycle, as all tiles must complete their updates before proceeding to the next training iteration.

**Design tradeoffs**: More tiles provide better accuracy but increase hardware overhead and power consumption. Geometric scaling factors must be carefully chosen to balance precision distribution across tiles. The framework trades increased hardware complexity for improved accuracy in low-precision analog systems.

**Failure signatures**: Training divergence when geometric scaling factors are poorly chosen, insufficient number of tiles for the required precision, or when conductance drift causes accumulated errors across tiles. Hardware non-idealities like asymmetric conductance updates can also cause systematic errors.

**Three first experiments**: 1) Verify single-tile baseline performance with 4-bit weights on MNIST to establish quantization floor. 2) Test two-tile configuration with varying geometric scaling factors to find optimal parameter settings. 3) Compare three-tile versus four-tile configurations on CIFAR-10 to determine diminishing returns point.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the work. The scalability of the approach to extremely large models (>100M parameters) remains unexplored, as does the framework's applicability to sequence models and other non-convolutional architectures. The sensitivity to hardware non-idealities beyond conductance quantization, such as parasitic resistances and temperature variations, also represents an area for future investigation.

## Limitations
- Hardware feasibility concerns regarding exponential scaling assumptions in the presence of non-uniform conductance variations and temperature-dependent drift
- Limited evaluation scope focused on image classification tasks, with no validation on NLP, graph neural networks, or other domains
- Hardware overhead characterization may not scale favorably for very large models (>100M parameters)

## Confidence
High: Theoretical convergence proof for exponential error reduction
Medium: Empirical results on tested image classification datasets
Low: Generalization to transformer architectures and non-vision domains

## Next Checks
1. Test the multi-tile residual learning framework on transformer-based architectures for language tasks to verify cross-domain applicability
2. Characteristically measure the actual hardware overhead (area, power, latency) when scaling to models with 50M+ parameters to validate the "moderate overhead" claim
3. Evaluate robustness to conductance drift and non-idealities by simulating long-term operation under realistic temperature variations and device aging scenarios