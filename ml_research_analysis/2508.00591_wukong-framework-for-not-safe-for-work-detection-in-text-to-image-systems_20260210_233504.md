---
ver: rpa2
title: Wukong Framework for Not Safe For Work Detection in Text-to-Image systems
arxiv_id: '2508.00591'
source_url: https://arxiv.org/abs/2508.00591
tags:
- nsfw
- image
- wukong
- diffusion
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Wukong, a framework for efficient and accurate
  NSFW detection in text-to-image generation. The key idea is to leverage intermediate
  latent representations from early denoising steps in diffusion models, combined
  with pre-trained cross-attention layers, to detect NSFW content before full image
  generation.
---

# Wukong Framework for Not Safe For Work Detection in Text-to-Image systems

## Quick Facts
- arXiv ID: 2508.00591
- Source URL: https://arxiv.org/abs/2508.00591
- Authors: Mingrui Liu; Sixiao Zhang; Cheng Long
- Reference count: 40
- Primary result: Achieves ROC AUC > 0.95 on Wukong-Demons dataset while running 40x faster than image-based filters

## Executive Summary
Wukong introduces a novel framework for efficient NSFW detection in text-to-image generation by analyzing intermediate latent representations during early denoising steps. Instead of waiting for full image generation or relying on text analysis, Wukong leverages pre-trained cross-attention weights from diffusion models to detect unsafe content patterns in the latent space at step T_C=10. The framework achieves comparable accuracy to image-based filters while offering significant computational efficiency, making it practical for real-time content moderation.

## Method Summary
Wukong analyzes intermediate latent representations φ(x_{T-T_C}) from early denoising steps (T_C=10) in diffusion models to detect NSFW content before full image generation. The framework reuses pre-trained cross-attention weights from the U-Net to project NSFW concept queries against these latents, combined with a lightweight trainable head. It operates as a multi-label binary classifier across 7 NSFW categories, trained with BCE loss on the Wukong-Demons dataset. The approach bypasses text-based adversarial attacks by directly analyzing visual potential in latent space rather than input prompts.

## Key Results
- ROC AUC > 0.95 on Wukong-Demons dataset
- 40x faster than image-based filters while maintaining comparable accuracy
- Strong robustness against adversarial prompts (ROC AUC 0.83-0.91)
- Effective at T_C=10 steps, providing early detection without full generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NSFW semantic content is determined early in the diffusion denoising process, allowing detection before image generation completes.
- Mechanism: Initial denoising steps (e.g., first 10 steps) establish the global semantic structure of the image. By analyzing the intermediate latent representation at an early step T_C, the model infers the final visual content without waiting for the full 50-step cycle.
- Core assumption: The high-level semantic features of "unsafe" content (e.g., nudity, violence layout) are embedded in the latent space earlier than the high-frequency details (texture, lighting).
- Evidence anchors:
  - [abstract]: "early denoising steps define the semantic layout of the image"
  - [section 1, Figure 1]: Demonstrates that changing a prompt after step 1 does not fully alter the image, proving early steps lock in semantic content.
  - [corpus]: Weak/Demonstrative. Corpus papers (e.g., TokenProber, AEIOU) focus on prompt attacks and text-based defenses; they do not validate the specific timing of semantic formation in latents.
- Break condition: If an adversarial attack specifically targets the high-frequency refinement stage (late denoising steps) to introduce NSFW content, early detection at T_C will likely fail.

### Mechanism 2
- Claim: Pre-trained cross-attention weights can be repurposed as fixed feature extractors for NSFW concepts.
- Mechanism: The framework reuses the Key and Value weights (W_K, W_Q) from the pre-trained U-Net's cross-attention layers. Instead of processing the user prompt, these weights project a set of "NSFW concept queries" (e.g., "sexual", "violence") against the intermediate latent representation to detect alignment.
- Core assumption: The cross-attention maps for specific unsafe words (like "blood") are spatially distinct and identifiable even in the noisy early latents.
- Evidence anchors:
  - [section 4.2]: "we reuse the parameters from these layers... specifically, we transform the NSFW embeddings into a query matrix... using the key weight matrix"
  - [section 4.2.1, Figure 2]: Visual evidence that attention maps for "sexual" and "car" highlight relevant regions even in early steps.
  - [corpus]: Not applicable. The reuse of pre-trained attention weights for a separate classification task is a specific architectural claim in this paper, not validated by the provided corpus.
- Break condition: If the U-Net's cross-attention layers are heavily overfitted to the specific syntax of prompts rather than visual concepts, the projection of abstract NSFW concepts may fail to trigger meaningful attention scores.

### Mechanism 3
- Claim: Visual-based detection in latent space mitigates text-based adversarial attacks.
- Mechanism: By inspecting the visual trajectory (intermediate latents) rather than the input text, Wukong bypasses the semantic ambiguity and obfuscation used in adversarial prompts. The detector evaluates the *visual potential* of the generation at step T_C.
- Core assumption: An unsafe prompt, even if obfuscated, will result in an unsafe trajectory in the latent space immediately.
- Evidence anchors:
  - [section 6.6, Table 3]: Reports high ROC AUC (0.83–0.91) on adversarial datasets (MMA, SneakyPrompt) where text-based baselines fail.
  - [section 6.6]: "Wukong's ability to directly analyze intermediate latent representations... allows it to identify visually NSFW content independent of textual phrasing."
  - [corpus]: Strong contextual support. Papers like *TokenProber* and *PLA* discuss sophisticated prompt attacks that bypass text filters, validating the need for non-textual mechanisms.
- Break condition: If the adversarial prompt successfully generates a "safe" latent structure that only "unfolds" into NSFW content in the final decoding or late denoising steps, this mechanism will result in false negatives.

## Foundational Learning

- Concept: **Diffusion Latent Space Structure**
  - Why needed here: Wukong operates on φ(x_{T-T_C}), which is a compressed, noisy representation in the VAE latent space, not pixel space. Understanding that latents encode shape/semantics before pixels is critical.
  - Quick check question: Does a "naked woman" in step 1 latent space look like a woman, or does it look like noise that *mathematically correlates* with the concept "woman"?

- Concept: **Cross-Attention Mechanics (Q, K, V)**
  - Why needed here: The framework relies on swapping the standard text-conditioned Query (Q) for a fixed NSFW Query while keeping the Key (K) and Value (V) projections from the U-Net.
  - Quick check question: In a standard Stable Diffusion block, which matrix (Q, K, or V) represents the image features being conditioned, and which represents the text prompt?

- Concept: **Multi-Label Binary Classification**
  - Why needed here: The model outputs a 7-dimensional vector where each element is an independent probability (e.g., an image can be both "Violent" and "Shocking").
  - Quick check question: Why use Binary Cross Entropy (BCE) instead of Softmax for the loss function in this architecture?

## Architecture Onboarding

- Component map: Text Prompt (s) + Random Seed (z) -> Frozen U-Net (steps 1-T_C) -> Fixed NSFW Query Projection -> Trainable Attention Head -> 7-category NSFW probabilities

- Critical path:
  1. Run diffusion for T_C - 1 steps normally
  2. At step T_C, capture the intermediate latent state *before* the final cross-attention layer
  3. Compute Attention between Fixed NSFW Queries and the Latent State
  4. If output > threshold δ (default 0.5), halt generation; otherwise, continue to step 50

- Design tradeoffs:
  - **Latency vs. Accuracy (T_C)**: The paper suggests T_C=10. Lowering T_C reduces latency but risks missing semantic formation. Raising T_C improves accuracy but negates the efficiency gains.
  - **Fixed vs. Fine-tuned Weights**: The authors freeze the U-Net and attention projections to save compute and preserve pre-trained knowledge. This limits adaptation to novel NSFW types not seen in pre-training.

- Failure signatures:
  - **High False Positive Rate on "Medical" contexts**: The mechanism detects "visual blood" (semantic layout). It may flag legitimate medical prompts (e.g., "surgery scene") as "Violence" because the early latent structure is similar.
  - **Late-stage Attacks**: If an attack manages to keep the early steps "safe" and injects NSFW content only in the final denoising steps (typically noise refinement), Wukong will miss it.

- First 3 experiments:
  1. **Visualize Latents**: Reproduce Figure 2. Generate attention maps for "Violence" and "Sexual" at steps 1, 5, and 10 to verify that spatial localization actually occurs early for your specific test cases.
  2. **Sweep T_C**: Run evaluation on the Wukong-Demons validation set varying T_C from 1 to 20. Plot ROC AUC vs. latency to find the optimal "elbow" for your hardware constraints.
  3. **Ablate Adversarial Defense**: Test the model against simple prompt obfuscation (e.g., replacing "blood" with "red liquid"). Compare the detection score against a text-only blacklist to confirm the visual-inference advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Wukong's intermediate-latent detection approach be extended to DiT-style diffusion architectures (e.g., Stable Diffusion 3, FLUX) that replace U-Net backbones with transformer-only designs?
- Basis in paper: [explicit] The conclusion states: "recent diffusion models such as DiT adopt fully transformer-based backbones instead of the U-Net-style architectures used in Stable Diffusion. While this work focuses on U-Net–based diffusion models with cross-attention mechanisms, extending Wukong to DiT-style architectures is a promising direction for future work."
- Why unresolved: Wukong explicitly reuses pre-trained cross-attention parameters from U-Net's transformer blocks, which may not have direct equivalents in DiT architectures with different attention patterns and conditioning mechanisms.
- What evidence would resolve it: A modified Wukong framework applied to DiT-based models, evaluated on comparable NSFW detection benchmarks, demonstrating whether intermediate representations in DiT denoising contain similarly detectable NSFW signals.

### Open Question 2
- Question: How robust is Wukong against adaptive adversarial attacks specifically designed to manipulate early denoising latent representations rather than textual prompts?
- Basis in paper: [inferred] The paper evaluates robustness against three prompt-based adversarial attacks (MMA, SneakyPrompt, Ring-A-Bell), but these target text filters. Since Wukong operates on latent representations, an attacker with knowledge of the detection mechanism could potentially craft adversarial noise or prompts optimized to evade detection at early denoising steps.
- What evidence would resolve it: Evaluation against white-box adversarial attacks that optimize perturbations to minimize Wukong's detection scores while preserving NSFW content in final images.

### Open Question 3
- Question: How sensitive is Wukong's performance to the choice and granularity of NSFW concept embeddings used as query vectors?
- Basis in paper: [inferred] The approach relies on 7 predefined NSFW category concepts (condensed to single-word embeddings like "Illegal" and "Wound"). The ablation study shows removing category-specific supervision degrades performance, but the optimal conceptual vocabulary—particularly across cultural contexts or evolving definitions of harmful content—remains unexplored.
- What evidence would resolve it: Systematic evaluation varying the number, specificity, and linguistic framing of NSFW concept embeddings, including cross-cultural annotation studies.

### Open Question 4
- Question: What is the impact of VLM-based labeling noise in the Wukong-Demons dataset on classifier generalization?
- Basis in paper: [inferred] Labels are assigned using GPT-4o-mini with only ~10% manual verification. While high agreement is reported, systematic labeling errors—particularly for ambiguous or borderline content—could propagate to the trained classifier and create blind spots not captured by the validation set.
- What evidence would resolve it: Human annotation of a held-out test set with multiple annotators, measuring inter-annotator agreement and comparing classifier performance against human ground truth versus VLM-generated labels.

## Limitations

- The framework may fail against late-stage adversarial attacks that introduce NSFW content only in final denoising steps
- Heavy reliance on automated labeling (GPT-4o-mini) introduces potential bias and consistency issues
- Limited validation on non-U-Net architectures like DiT-style models

## Confidence

**High Confidence**: The efficiency claims (40x faster than image-based filters) and basic detection performance on clean data (ROC AUC > 0.95) are well-supported by the experimental results.

**Medium Confidence**: The adversarial robustness claims (ROC AUC 0.83-0.91) are supported but the evaluation methodology for prompt attacks could be more rigorous.

**Low Confidence**: The dataset quality and labeling consistency are uncertain due to heavy reliance on automated generation and labeling.

## Next Checks

1. **Late-stage Attack Vulnerability**: Design and test adversarial prompts that generate "safe" latents in early steps (T_C=10) but transform into NSFW content only in the final denoising steps (30-50). Measure false negative rates specifically for this attack vector to quantify the framework's vulnerability window.

2. **Cross-Attention Weight Ablation**: Conduct an ablation study comparing Wukong's performance when using: (a) original frozen weights, (b) randomly initialized weights, and (c) fine-tuned weights. This would establish the true contribution of pre-trained knowledge versus the architecture design.

3. **Label Quality Audit**: Sample 100 images from the Wukong-Demons test set and have human annotators independently label them across all 7 NSFW categories. Compare agreement rates with the GPT-4o-mini + OpenAI moderation ground truth to quantify labeling accuracy and potential systematic biases.