---
ver: rpa2
title: 'Assessing LLM Text Detection in Educational Contexts: Does Human Contribution
  Affect Detection?'
arxiv_id: '2508.08096'
source_url: https://arxiv.org/abs/2508.08096
tags:
- text
- texts
- human
- detection
- contribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks the performance of state-of-the-art LLM text
  detectors in educational contexts, introducing a novel dataset (GEDE) with over
  12,500 LLM-generated and 900 human-written essays across varying contribution levels.
  It proposes contribution levels to capture diverse LLM usage practices, from fully
  human-written to actively "humanized" texts.
---

# Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?

## Quick Facts
- arXiv ID: 2508.08096
- Source URL: https://arxiv.org/abs/2508.08096
- Authors: Lukas Gehring; Benjamin Paaßen
- Reference count: 40
- This study benchmarks state-of-the-art LLM text detectors in educational contexts, introducing a novel dataset (GEDE) with over 12,500 LLM-generated and 900 human-written essays across varying contribution levels.

## Executive Summary
This paper systematically evaluates the performance of current LLM text detectors in educational contexts, introducing a novel dataset (GEDE) with over 12,500 LLM-generated and 900 human-written essays across eight contribution levels. The study finds that zero-shot methods (DetectGPT, Fast-DetectGPT) significantly outperform supervised models, particularly for intermediate levels like LLM-improved human-written texts. However, detection performance degrades substantially for shorter texts and when minimizing false positives, making current detectors unsuitable for reliable deployment in educational settings where false accusations can severely impact students' lives.

## Method Summary
The authors created the GEDE dataset containing 916 human essays from three corpora (AAE: 402, PERSUADE: 75, BAWE: 439) and 12,500+ LLM-generated essays using GPT-4o-mini and Llama-3.3-70B-Instruct. Eight contribution levels were defined ranging from fully human-written to actively "humanized" texts. Five detectors were benchmarked: DetectGPT and Fast-DetectGPT (zero-shot probability curvature methods), Ghostbuster (feature-based), RoBERTa (supervised fine-tuning), and GPTZero (proprietary API). The RoBERTa model was fine-tuned on binary Human vs. Task labels with batch size 32, test_size 0.2, and 5-8 epochs, selecting the best epoch by validation loss. Zero-shot methods used GPT-2 as a surrogate model, while texts were truncated to 320 words and DIPPER was used for the Humanize level.

## Key Results
- Zero-shot methods (DetectGPT, Fast-DetectGPT) outperform supervised models, especially for intermediate levels like LLM-improved human-written texts
- Detection performance drops significantly for texts under 50 words, making detectors unsuitable for short-answer assessments
- All detectors show steep F1 decline when threshold optimization enforces false positive rate ≤5%, which is problematic for educational deployment

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot probability curvature methods detect fully LLM-generated text by identifying statistical signatures in token probability distributions. DetectGPT and Fast-DetectGPT compare the log probability of input text against perturbed/re-sampled versions. LLM-generated text tends to lie at local maxima of the probability distribution—small rewrites decrease probability more consistently than for human text, creating detectable curvature patterns. The core assumption is that the source LLM's probability distribution is accessible or can be approximated by a surrogate model. Break condition: When texts are paraphrased or "humanized" using DIPPER, the statistical signature is disrupted—Fast-DetectGPT drops from 0.98 ROC-AUC (Task) to 1.00 on Humanize.

### Mechanism 2
Supervised detectors fail to generalize to intermediate contribution levels because training data doesn't represent the full spectrum of human-LLM collaboration. RoBERTa was fine-tuned on binary Human vs. Task labels. When encountering Improve-Human or Rewrite-Human texts—which contain mixed human and LLM signatures—the model applies learned decision boundaries that were never calibrated for these distributions, leading to systematic misclassification toward the LLM class. Break condition: Training must explicitly include all contribution levels; even then, generalization to new LLMs remains fragile.

### Mechanism 3
Detection reliability in educational settings is fundamentally constrained by the false positive rate tolerance, not raw accuracy. Educational contexts require near-zero false accusations. When threshold optimization enforces FPR ≤ 5%, all detectors show steep F1 decline—Fast-DetectGPT drops from F1≈0.82 to ≈0.69. This occurs because human and LLM-generated score distributions overlap significantly for intermediate contribution levels. Break condition: When policy requires FPR < 5%, current detectors become effectively unusable.

## Foundational Learning

- **Concept**: ROC-AUC vs. F1 at constrained FPR
  - Why needed here: The paper reports both metrics, but educational deployment decisions hinge on F1@low-FPR, not raw AUC. Understanding this distinction prevents overestimating detector readiness.
  - Quick check question: If a detector achieves 0.95 ROC-AUC but F1 drops to 0.40 at 5% FPR, is it deployment-ready for high-stakes educational contexts?

- **Concept**: Contribution levels as a spectrum (not binary)
  - Why needed here: Real student writing exists on a continuum from unassisted to AI-polished to AI-generated. Binary classifiers trained only on extremes cannot reliably identify intermediate cases.
  - Quick check question: A student uses ChatGPT to fix grammar on their original essay. Which contribution level is this, and how would a detector trained only on Human/Task classify it?

- **Concept**: Zero-shot vs. supervised detection tradeoffs
  - Why needed here: Zero-shot methods generalize better across datasets and models but require probability access; supervised methods are faster at inference but brittle. System design must choose based on deployment constraints.
  - Quick check question: You need to detect text from an unknown future LLM. Which approach (zero-shot or supervised) is more likely to work, and what assumptions does it require?

## Architecture Onboarding

- **Component map**: Text -> length check -> score computation -> threshold comparison -> binary classification. For ensemble: combine scores.

- **Critical path**: Input preprocessing: Truncate to ≤512 tokens (RoBERTa constraint) or ≤320 words per paper's setup. Scoring engines: DetectGPT (perturbation-based), Fast-DetectGPT (conditional probability), Ghostbuster (feature + logistic regression), RoBERTa (fine-tuned transformer), GPTZero (proprietary API). Threshold layer: Configurable—F1-optimal, Youden's Index, or FPR-constrained. Policy boundary selector: Maps contribution levels to acceptable/unacceptable based on institutional policy.

- **Design tradeoffs**:
  - Accuracy vs. fairness: Lower FPR reduces false accusations but misses more AI-assisted work
  - Speed vs. coverage: Fast-DetectGPT is 340x faster than DetectGPT but both require model access; supervised models are faster but brittle
  - Label boundary position: Stricter boundaries (only Human acceptable) improve detection but penalize legitimate AI-assisted editing

- **Failure signatures**:
  - High variance in human text scores (Figure 2) → false positives even on clean human writing
  - Score clustering around 0/1 for RoBERTa → threshold sensitivity spikes (Figure 3b cliff at FPR<0.35)
  - Performance collapse on texts <50 words → do not use for short-answer assessment

- **First 3 experiments**:
  1. Baseline replication: Run Fast-DetectGPT on GEDE Human vs. Task subset; verify ROC-AUC ≈0.98 matches paper
  2. Boundary sensitivity: Sweep label boundary from Human-only to Rewrite-Human-acceptable; plot ROC-AUC degradation curve (replicate Table 3)
  3. Length stress test: Truncate texts to 50/100/150/200/250 words; identify minimum viable length for each detector (replicate Figure 5)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do state-of-the-art detection methods generalize to educational tasks involving non-English languages or non-essay formats such as programming code and mathematical proofs? The authors explicitly note their data is limited to English-language essays and encourage future research into other educational domains and languages.

- **Open Question 2**: How does detection performance for summary-based generation levels change when using authentic human-written notes compared to the AI-generated summaries used in this study? The authors acknowledge using the T5 model to generate summaries for the "Summary" contribution level rather than human notes.

- **Open Question 3**: Does training supervised detectors on intermediate contribution levels (e.g., "Improve-Human" or "Rewrite-Human") significantly improve their ability to generalize compared to standard binary training? The authors found that supervised models like RoBERTa failed on intermediate levels and explicitly recommend including all levels during training.

## Limitations

- Current detectors produce unacceptable false positive rates in educational settings, particularly when threshold optimization enforces FPR ≤5%
- Detection performance degrades substantially for texts under 100 words, limiting applicability to short-answer assessments
- Supervised models cannot generalize to intermediate contribution levels without explicit training on the full spectrum of human-LLM collaboration

## Confidence

**High confidence**: Current detectors produce unacceptable false positive rates in educational settings; zero-shot methods outperform supervised models for fully LLM-generated text; detection performance degrades significantly for texts under 100 words.

**Medium confidence**: Supervised models cannot generalize to intermediate contribution levels without explicit training; cross-dataset performance is poor due to distribution shift; adversarial attacks substantially reduce detection accuracy.

**Low confidence**: Specific threshold values for educational deployment are optimal; one attack type represents the full space of adversarial techniques; current detector architecture cannot be improved for this use case.

## Next Checks

1. **Surrogate Model Dependency Test**: Replicate zero-shot detector performance using different surrogate models (e.g., Llama-3 instead of GPT-2) to verify that detector effectiveness depends on surrogate-LLM compatibility rather than universal statistical signatures.

2. **Intermediate Level Training Impact**: Train supervised models explicitly on all contribution levels (not just Human vs Task) and measure whether this eliminates the performance gap on Improve-Human and Rewrite-Human texts, isolating whether the failure is due to training data distribution or model architecture.

3. **Real-World False Positive Impact Assessment**: Conduct a controlled deployment simulation where detectors run on actual student essays with human expert review of flagged cases, measuring the operational false positive rate and institutional response burden rather than just statistical metrics.