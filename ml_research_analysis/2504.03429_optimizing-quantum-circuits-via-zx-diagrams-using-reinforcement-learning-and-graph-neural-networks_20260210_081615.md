---
ver: rpa2
title: Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and
  Graph Neural Networks
arxiv_id: '2504.03429'
source_url: https://arxiv.org/abs/2504.03429
tags:
- circuit
- quantum
- circuits
- optimization
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reducing two-qubit gate count
  in quantum circuits, which is critical for mitigating noise on near-term quantum
  hardware. The authors introduce a novel reinforcement learning framework that uses
  ZX calculus, graph neural networks, and tree search to discover arbitrary circuit
  optimization rules.
---

# Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and Graph Neural Networks

## Quick Facts
- arXiv ID: 2504.03429
- Source URL: https://arxiv.org/abs/2504.03429
- Reference count: 40
- Outperforms PyZX full reduce by up to 30% on CNOT gate reduction for random 5-qubit circuits

## Executive Summary
This paper introduces a reinforcement learning framework for quantum circuit optimization that learns to minimize two-qubit gate counts by operating directly on ZX diagrams. The method uses graph neural networks to predict optimal sequences of ZX-calculus rewrite rules, combined with tree-based search that allows non-monotonic optimization paths and backtracking. Experiments demonstrate superior performance compared to state-of-the-art optimizers on diverse random circuits, with strong generalization capabilities and competitive results on large 50-qubit circuits using peephole optimization.

## Method Summary
The approach converts quantum circuits to ZX diagrams (tensor networks of Z-spiders and X-spiders) and trains a reinforcement learning agent to apply sequences of ZX-calculus rewrite rules to minimize CNOT gates. A GNN policy predicts both the rule type and application position based on node features encoding admissible actions. The agent explores a search tree of transformations, sampling which nodes to expand using learned weights, allowing backtracking when rule applications increase gate count. After K steps, the best circuit in the tree is extracted back to standard form. Training uses PPO with 8 parallel environments on 5-qubit random circuits, achieving strong generalization to unseen circuit distributions.

## Key Results
- Achieves up to 30% reduction in CNOT count compared to PyZX full reduce on random 5-qubit circuits
- Outperforms template-matching methods and shows strong generalization to circuits with different gate ratios
- Scales to 50-qubit circuits using peephole optimization with competitive performance against Qiskit
- Agent learns optimization strategies rather than relying on fixed heuristics

## Why This Works (Mechanism)

### Mechanism 1
ZX-calculus provides a complete, searchable representation space for circuit optimization. Quantum circuits are converted to ZX diagrams, and a small set of rewrite rules (spider fusion, un-fusion, π-commute, color change, bialgebra, Euler) is complete—meaning any equivalent circuit can be reached via some rule sequence. The RL agent searches this space rather than applying fixed heuristic sequences. Core assumption: The agent can learn to navigate the exponentially large space of rule sequences to find low-CNOT outcomes.

### Mechanism 2
GNN-based policies learn action-selection conditioned on local and global graph structure. The ZX diagram is treated as a graph with node features encoding which rules are admissible at each position. A GNN with 4 GCN layers propagates information, enabling the policy to predict both the rule type and application position. A second GNN aggregates graph-level information for tree-node selection. Core assumption: Admissible-action features provide sufficient signal for the policy; GCN layers capture relevant neighborhood structure.

### Mechanism 3
Tree-based search with learned node selection enables non-monotonic optimization paths. Instead of committing to a single action sequence, the method maintains a search tree of explored transformations. At each step, the agent samples which tree node to expand (via learned weights accumulated along root-to-node paths), then applies a rule to generate a child node. After K steps, the best circuit in the tree is selected. This allows revisiting and backtracking. Core assumption: The learned node-selection policy can identify which partial sequences deserve further exploration.

## Foundational Learning

- **ZX-calculus basics (spiders, wires, rewrite rules)**: Why needed here: The entire method operates on ZX diagrams; understanding what rules do and what "completeness" means is prerequisite to interpreting agent behavior. Quick check: Can you explain what happens when two same-colored spiders connected by a wire are fused?
- **Reinforcement learning (MDP, policy, reward, PPO)**: Why needed here: The agent learns via PPO; understanding the reward formulation (CNOT reduction) and policy structure is necessary for debugging and extension. Quick check: Why does the reward use the best circuit in the tree rather than the leaf node?
- **Graph Neural Networks (message passing, permutation invariance)**: Why needed here: The policy and value functions are GNNs; understanding how information propagates through the ZX graph clarifies what patterns the agent can detect. Quick check: Why might admissible-action features be preferred over raw node features for this task?

## Architecture Onboarding

- **Component map**: Circuit → ZX Diagram → GNN Encoder (rule/position predictor + tree-level policy) → Tree Search Loop → ZX Diagram → Circuit
- **Critical path**: ZX conversion must preserve equivalence → GNN must predict valid rule-position pairs → Tree search explores K steps → Extraction must succeed → Reward from best tree node
- **Design tradeoffs**: Rule set size vs search space complexity; Tree budget (K=128) vs latency; Extraction levels vs optimization gains; GNN vs MLP for peephole optimization
- **Failure signatures**: Extraction failure (check gflow preservation); Policy collapse (monitor entropy); No improvement (verify training convergence); Scalability issues (GNN inference slows)
- **First 3 experiments**: 1) Reproduce Table I comparing CNOT count vs PyZX full reduce on 5-qubit random circuits; 2) Ablate tree search to quantify backtracking contribution; 3) Test extraction sensitivity by forcing different extraction levels

## Open Questions the Paper Calls Out

- **Can GNN-based policies trained solely on small circuits generalize to optimize large-scale quantum circuits natively without partitioning?** The authors expect generalization capabilities to allow training on small graphs to optimize much larger sizes, leaving this as future work. The current evaluation of large (50-qubit) circuits relies on peephole optimization and replaces the GNN with an MLP.

- **Does providing the agent access to the complete set of ZX-calculus transformation rules enable superior optimization for alternative metrics like T-count minimization?** The authors note the agent currently lacks access to the complete set of transformations and propose extending this for targets like T-count minimization in future work.

- **How can random circuit generators be designed to preserve local structure to better represent real-world quantum algorithms for training and benchmarking?** The authors observe a distribution shift where large random circuits lose local structure, making optimization difficult, and suggest investigating realistic random-circuit generators.

## Limitations
- Extraction brittleness: RL agent may generate non-trivial diagrams requiring aggressive fallback extraction, potentially undoing optimizations
- Scalability concerns: Large circuit optimization relies on peephole partitioning rather than native end-to-end optimization
- Limited baseline comparison: Main results compare only against PyZX full reduce rather than multiple state-of-the-art optimizers

## Confidence

- **High Confidence**: The core RL + ZX calculus framework is technically sound and builds on established methods (PyZX, GNNs, PPO). The experimental methodology for measuring CNOT reduction is straightforward and reproducible.
- **Medium Confidence**: The reported performance improvements over baselines are credible given the methodology, but the limited baseline comparison and lack of detailed ablation studies reduce confidence in the claimed significance of the contributions.
- **Low Confidence**: Claims about discovering "new optimization patterns" and achieving "competitive performance" on 50-qubit circuits are not well-supported by quantitative evidence in the paper.

## Next Checks

1. **Ablation of Tree Search**: Run the trained agent with and without the tree-based search mechanism (single-path execution only) to quantify the contribution of backtracking to performance improvements.

2. **Extraction Level Analysis**: For each test circuit, record the extraction level required and the corresponding CNOT count to reveal whether the agent learns to generate extractable diagrams or relies on aggressive fallback extraction.

3. **Pattern Discovery Validation**: Analyze the agent's rule application sequences on circuits where it outperforms PyZX to extract common rule subsequence patterns and verify they represent genuinely new optimizations.