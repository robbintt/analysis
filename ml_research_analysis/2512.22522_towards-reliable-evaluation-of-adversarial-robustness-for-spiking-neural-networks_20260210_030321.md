---
ver: rpa2
title: Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks
arxiv_id: '2512.22522'
source_url: https://arxiv.org/abs/2512.22522
tags:
- gradient
- adversarial
- surrogate
- assg
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating adversarial robustness
  in Spiking Neural Networks (SNNs), which is complicated by the binary and discontinuous
  nature of spike activations leading to gradient vanishing. The authors propose a
  framework that combines an Adaptive Sharpness Surrogate Gradient (ASSG) method with
  a Stable Adaptive Projected Gradient Descent (SA-PGD) attack.
---

# Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks

## Quick Facts
- arXiv ID: 2512.22522
- Source URL: https://arxiv.org/abs/2512.22522
- Authors: Jihang Wang; Dongcheng Zhao; Ruolin Chen; Qian Zhang; Yi Zeng
- Reference count: 40
- Primary result: Adaptive Sharpness Surrogate Gradient (ASSG) achieves 88.44% attack success rate on CIFAR-10 vs 84.04% with fixed sharpness

## Executive Summary
This paper addresses the challenge of evaluating adversarial robustness in Spiking Neural Networks (SNNs), which is complicated by the binary and discontinuous nature of spike activations leading to gradient vanishing. The authors propose a framework that combines an Adaptive Sharpness Surrogate Gradient (ASSG) method with a Stable Adaptive Projected Gradient Descent (SA-PGD) attack. ASSG dynamically adjusts the surrogate gradient's sharpness during attack iterations based on input distribution, mitigating gradient vanishing while improving gradient accuracy. SA-PGD incorporates adaptive step size and momentum with L∞-norm clipping for more stable convergence. Extensive experiments show ASSG achieves 88.44% attack success rate on CIFAR-10 compared to 84.04% with fixed sharpness, and 93.19% on CIFAR-100 versus 91.40%, significantly improving evaluation reliability across different SNN architectures, neuron models, and training methods.

## Method Summary
The authors propose ASSG+SA-PGD as a framework for reliable adversarial robustness evaluation of SNNs. ASSG adaptively evolves the surrogate gradient sharpness α based on the membrane potential distribution using exponential moving averages of |u| and its deviation. This is theoretically justified by proving gradient-vanishing bounds using Jensen's inequality. SA-PGD improves upon standard PGD by incorporating adaptive step sizes with L∞ clipping before projection to prevent any single dimension from dominating. The framework is evaluated across CIFAR-10/100 datasets with various SNN architectures (SEWResNet19, VGG9), neuron models (LIF, IF, PSN), and training methods (AT, RAT, AT+SR, TRADES).

## Key Results
- ASSG achieves 88.44% attack success rate on CIFAR-10 vs 84.04% with fixed sharpness (4% improvement)
- ASSG achieves 93.19% attack success rate on CIFAR-100 vs 91.40% with fixed sharpness
- PSN-based SNN shows 78.49% robustness with HART vs 97.25% with ASSG, revealing current overestimation
- SA-PGD maintains advantage over APGD and Adam-PGD at 100+ iterations when combined with ASSG

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Sharpness Surrogate Gradient (ASSG)
- Dynamically adjusts surrogate gradient sharpness per-neuron-per-timestep based on input distribution to improve gradient accuracy while controlling gradient vanishing during adversarial attacks
- Uses exponential moving average of membrane potential difference |u^{l}_{i,t}| to compute adaptive sharpness α^{l}_{i,t} = ω/(M + γD)
- Sharper surrogates where |u| is small (near-threshold), smoother surrogates where |u| is large

### Mechanism 2: Theoretical Upper Bound on Gradient Vanishing
- Proves G(x) = ∫_{-x}^{x} g(t)dt is concave on [0, +∞), enabling bound E[G(αX)] ≤ A if α ≤ G^{-1}(A)/E[X]
- Provides principled way to set α to guarantee gradient information preservation

### Mechanism 3: L∞-Constrained Adaptive Optimization (SA-PGD)
- Combines adaptive step sizes with per-iteration L∞ clipping before projection for more stable convergence under imprecise gradients
- Clips update to [-η_k, η_k] before projection, preventing any single dimension from dominating

## Foundational Learning

- **Concept: Surrogate Gradient Methods**
  - Why needed here: Replaces non-differentiable Heaviside step function H(x) with smooth approximations to enable backpropagation
  - Quick check question: For spike activation s = H(V - V_{th}), what does the surrogate gradient approximate and what does sharpness parameter α control?

- **Concept: Gradient Vanishing in Sharp Surrogates**
  - Why needed here: Sharper surrogates better approximate H'(x) but vanish faster for |x| >> 0, motivating adaptive approach
  - Quick check question: For g(x) = αk(αx), what happens to the gradient magnitude when input is far from decision boundary?

- **Concept: Projected Gradient Descent under L∞ Constraints**
  - Why needed here: Adversarial attacks require perturbations within ε-ball; SA-PGD's per-step clipping before projection differs from standard APGD
  - Quick check question: In PGD with L∞ constraint ε, how does Π^ε_∞(x') differ from clip(x', x-ε, x+ε)?

## Architecture Onboarding

- **Component map**: Input x → SNN with LIF/IF/PSN neurons over T timesteps → logits → Loss computation → ASSG Module (EMA tracking → α computation) → Surrogate gradient → Input gradient → SA-PGD Update (momentum/oscillation tracking → clipped update → projection)

- **Critical path**: Table 1 shows ASSG+SA-PGD achieves 88.44% ASR vs. 84.06% for fixed-α+APGD on CIFAR-10 AT. Gradient approximation and optimization are coupled—SA-PGD shows minimal improvement with poor gradients (BPTR, RGA).

- **Design tradeoffs**:
  - Higher A (0.82→0.90): Allows sharper gradients → potentially better accuracy but more vanishing risk; ternary search recommended
  - More iterations: Fig. 5 shows gains continue to 1000 iterations but computational cost scales linearly
  - Relaxation coefficient γ: Table S3 shows γ=1.5→3.5 improves ASR (88.44%→89.30%) by providing more conservative α estimates

- **Failure signatures**:
  - Loss converges too early with fixed-α (Fig. 4): surrogate either too sharp (vanishing) or too smooth (inaccurate)
  - ASR decreases at high iterations with Adam-PGD (Fig. 5): missing L∞ clipping causes instability
  - False robustness: Table 3 shows PSN at 78.49% with HART vs. 97.25% with ASSG—standard methods underestimate vulnerability

- **First 3 experiments**:
  1. Reproduce Table 2 comparison: Attack CIFAR-10 AT model with Atan surrogate using fixed α∈{2,4,8} vs. ASSG (A=0.87). Verify ASSG achieves ~4% higher ASR.
  2. Ablate SA-PGD components: Compare PGD, APGD, Adam-PGD, and SA-PGD with ASSG on CIFAR-10 (Fig. 5 setup). Confirm SA-PGD maintains advantage at 100+ iterations.
  3. Test neuron generalization: Attack PSN-based SNN (Table 3) with ASSG+SA-PGD. Verify >95% ASR, demonstrating method independence from specific neuron dynamics.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical gradient-vanishing bounds rely on assumptions about input distribution expectations that may not hold for complex image data
- Adaptive mechanism's performance depends heavily on the relaxation parameter γ, requiring careful tuning per dataset
- SA-PGD algorithm introduces multiple hyperparameters (β_m, β_v) that interact non-trivially with surrogate gradient quality

## Confidence

- **High confidence**: The empirical improvement of ASSG over fixed-sharpness surrogates (88.44% vs 84.04% ASR on CIFAR-10) is well-supported by ablation studies
- **Medium confidence**: Theoretical gradient-vanishing bound provides principled justification, but practical applicability depends on distributional assumptions not fully validated on real data
- **Low confidence**: Claims about current SNN robustness being "overestimated" rely on comparing against methods that may have their own evaluation limitations not fully explored

## Next Checks

1. Test ASSG with different relaxation coefficients (γ ∈ [1.0, 4.0]) on CIFAR-10 to verify claimed sensitivity and identify optimal ranges for various architectures
2. Apply framework to naturally trained (non-adversarially trained) SNNs to assess whether overestimation claims hold beyond AT models
3. Evaluate gradient variance and vanishing degree across attack iterations to empirically validate theoretical bounds hold in practice