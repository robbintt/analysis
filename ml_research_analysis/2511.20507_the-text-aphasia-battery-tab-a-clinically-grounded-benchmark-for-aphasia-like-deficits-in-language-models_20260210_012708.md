---
ver: rpa2
title: 'The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like
  Deficits in Language Models'
arxiv_id: '2511.20507'
source_url: https://arxiv.org/abs/2511.20507
tags:
- aphasia
- language
- features
- clinical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Text Aphasia Battery (TAB) addresses the challenge of assessing
  language model impairments in a clinically-grounded manner by adapting established
  aphasia assessment frameworks for text-only evaluation. The benchmark comprises
  four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition,
  which evaluate complementary aspects of linguistic function using binary scoring
  criteria derived from clinical aphasia research.'
---

# The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models

## Quick Facts
- arXiv ID: 2511.20507
- Source URL: https://arxiv.org/abs/2511.20507
- Reference count: 39
- The TAB provides a text-only framework for assessing aphasia-like linguistic deficits in language models, validated with automated evaluation achieving reliability comparable to human raters.

## Executive Summary
The Text Aphasia Battery (TAB) addresses the challenge of assessing language model impairments in a clinically-grounded manner by adapting established aphasia assessment frameworks for text-only evaluation. The benchmark comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition, which evaluate complementary aspects of linguistic function using binary scoring criteria derived from clinical aphasia research. To enable large-scale use, an automated evaluation protocol using Gemini 2.5 Flash was validated against expert human annotators, achieving prevalence-weighted Cohen's Kappa of 0.255 for model-consensus agreement compared to 0.286 for human-human agreement, demonstrating comparable reliability. The TAB provides a standardized, text-based framework for analyzing linguistic breakdown in artificial systems while maintaining clinical relevance through its grounding in the Quick Aphasia Battery and APROCSA frameworks.

## Method Summary
The TAB adapts the Quick Aphasia Battery (QAB) and APROCSA clinical frameworks into a text-only benchmark for assessing aphasia-like deficits in language models. The benchmark includes four subtests: Connected Text (5 open-ended prompts with 19 binary APROCSA features), Word Comprehension (5 forced-choice items), Sentence Comprehension (5 Yes/No questions), and Repetition (5 items with exact string matching). An automated evaluation protocol uses Gemini 2.5 Flash with few-shot prompting to score the Connected Text responses, validated against 561 samples including AphasiaBank transcripts and lesioned LLM outputs. The automated protocol achieves prevalence-weighted Cohen's kappa of 0.255 compared to human consensus, demonstrating reliability comparable to human raters while enabling large-scale evaluation.

## Key Results
- Automated evaluation using Gemini 2.5 Flash achieves prevalence-weighted Cohen's kappa of 0.255 for model-consensus agreement versus 0.286 for human-human agreement
- The text-only adaptation successfully preserves core linguistic dimensions (anomia, agrammatism, discourse coherence, repetition fidelity) from clinical frameworks
- Automated protocol enables consistent feature identification across large datasets without requiring manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only evaluation can isolate behavioral signatures of linguistic breakdown that parallel clinical aphasia profiles.
- Mechanism: The TAB restricts inputs and outputs to text, eliminating multimodal dependencies while preserving assessable linguistic features adapted from APROCSA and QAB.
- Core assumption: Aphasic language deficits leave recognizable patterns in transcribed text that don't require prosodic, gestural, or sensory context to detect.
- Evidence anchors:
  - [abstract] "The TAB reframes aphasia assessment from a neuropsychological tool to a behavioral benchmark, identifying patterns of language degradation in LLMs that are analogous, but not homologous, to human syndromes."
  - [section 4.1] "The core linguistic dimensions affected in aphasia—lexical retrieval (anomia), syntactic structure (agrammatism, paragrammatism), discourse coherence (empty speech), and repetition fidelity—are directly identifiable in transcribed text."
  - [corpus] Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes provides corroborating evidence that lesioned LLMs exhibit aphasic-like patterns.

### Mechanism 2
- Claim: Few-shot in-context learning with a large LLM can automate feature detection at reliability comparable to expert human raters when prevalence-weighted.
- Mechanism: Gemini 2.5 Flash receives definitions, examples, and two annotated transcripts, then outputs binary presence/absence for 19 APROCSA-adapted features.
- Core assumption: The judge model's internal linguistic representations align sufficiently with clinical feature definitions to enable reliable transfer.
- Evidence anchors:
  - [abstract] "We validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's κ = 0.255 for model–consensus agreement vs. 0.286 for human–human agreement)."
  - [section 5.2] "This few-shot prompting approach enables consistent feature identification across large datasets without requiring manual annotation."

### Mechanism 3
- Claim: Prevalence-weighted aggregation prevents artificial inflation of reliability metrics when features have highly variable occurrence rates.
- Mechanism: Rather than macro-averaging across all 19 features, each feature's Cohen's κ contribution is weighted by its positive instance count.
- Core assumption: Zero-prevalence features in the validation set are absent due to sampling, not because they are undetectable.
- Evidence anchors:
  - [section 6.2] "An unweighted macro-average, which treats all features equally, is problematic when features have vastly different clinical prevalence rates... the model appears to have perfect agreement on zero-prevalence features (contributing 1.0 to its average)."
  - [table 1] Shows unweighted average κ of 0.794 (model) vs. 0.061 (human), demonstrating the artifact; weighted averages converge to 0.255 vs. 0.286.

## Foundational Learning

- Concept: **Aphasia types and clinical assessment frameworks**
  - Why needed here: The TAB adapts the QAB and APROCSA; understanding what these measure (e.g., Broca's vs. Wernicke's profiles, fluency vs. comprehension tradeoffs) is necessary to interpret TAB outputs meaningfully.
  - Quick check question: Can you explain why traditional aphasia batteries are multimodal and what information is lost in text-only adaptation?

- Concept: **Inter-rater reliability (Cohen's κ, prevalence weighting)**
  - Why needed here: The paper's validation hinges on demonstrating that automated evaluation matches human reliability; understanding κ interpretation (e.g., "fair" agreement at ~0.25–0.30) is essential to evaluate this claim.
  - Quick check question: Why does unweighted averaging inflate model performance when many features have zero prevalence?

- Concept: **LLM evaluation paradigms (behavioral benchmarks vs. probing)**
  - Why needed here: The TAB is a behavioral benchmark, not a probe of internal representations; distinguishing these approaches clarifies what conclusions TAB can and cannot support.
  - Quick check question: If an LLM fails TAB's Sentence Comprehension subtest, what can and cannot be inferred about its syntactic representations?

## Architecture Onboarding

- Component map:
  Subtest 1 (Connected Text) -> 19 binary APROCSA features -> automated or manual scoring
  Subtest 2 (Word Comprehension) -> binary correct/incorrect
  Subtest 3 (Sentence Comprehension) -> binary correct/incorrect
  Subtest 4 (Repetition) -> exact string match
  Automated Evaluator -> Gemini 2.5 Flash with few-shot prompt -> JSON feature output

- Critical path:
  1. Administer all 4 subtests to target model via standardized prompts (use system prompts to avoid AI refusals)
  2. For Subtests 2–4, apply algorithmic scoring (exact match or binary classification)
  3. For Subtest 1, run responses through automated evaluator with provided prompt template
  4. Aggregate feature scores; optionally compute prevalence-weighted κ against reference annotations

- Design tradeoffs:
  - Binary vs. graded scoring: TAB uses binary (0/1) for scalability; APROCSA originally uses 5-point scales. Tradeoff: granularity vs. annotation speed and agreement.
  - 5 items per subtest: Prioritizes rapid assessment (15–20 min) over comprehensive coverage. Risk: ceiling effects in high-performing models.
  - Text-only modality: Enables LLM evaluation but excludes prosody, gesture, and motor speech features relevant to human aphasia.

- Failure signatures:
  - Ceiling effects: Instruction-tuned models may score near-perfectly on Word/Sentence Comprehension; consider harder foils or more complex syntax.
  - Judge bias: Automated evaluator may systematically differ from human raters on specific features; monitor per-feature κ, not just aggregate.
  - Refusal / meta-commentary: Models may refuse personal prompts ("As an AI..."); use system prompts to establish conversational context without persona requests.

- First 3 experiments:
  1. Baseline intact model: Administer TAB to an unmodified instruction-tuned LLM (e.g., Llama-3-Instruct) to establish ceiling performance and identify which subtests/features show variance.
  2. Targeted lesioning: Apply parameter zeroing or scaling to specific components (e.g., attention heads in early vs. late layers) and observe which TAB features degrade. Compare patterns to clinical aphasia profiles.
  3. Judge robustness check: Evaluate the same responses using multiple judge models (e.g., Gemini 2.5 Flash, GPT-4o, Claude) and compare per-feature agreement to assess judge-dependent bias.

## Open Questions the Paper Calls Out
- How robust is the automated evaluation protocol across different judge models and prompting strategies?
- Can systematic lesion studies using TAB reveal isomorphisms between LLM architectural components and human neuroanatomical substrates of language?
- Do TAB subtest scores correlate with performance on established LLM linguistic benchmarks, or do they capture distinct dimensions of language competence?

## Limitations
- Limited generalizability to diverse clinical populations - validation primarily on Western, English-speaking stroke survivors
- Judge model dependency - reliability depends critically on Gemini 2.5 Flash's linguistic representations
- Clinical validity boundaries - identifies analogous patterns rather than true neurological deficits

## Confidence
- High confidence: The automated evaluation protocol achieves comparable reliability to human raters (κ = 0.255 vs 0.286) under stated validation conditions
- Medium confidence: The TAB reliably distinguishes between intact and lesioned models across different architectures
- Low confidence: Claims about clinical relevance beyond pattern recognition exceed the benchmark's current validation scope

## Next Checks
1. Cross-judge validation: Evaluate the same Connected Text responses using 3-5 different judge models and calculate per-feature Cohen's κ matrices
2. Clinical population expansion: Test TAB on aphasia samples from non-Western populations and alternative etiologies
3. Feature prevalence sensitivity analysis: Systematically vary the prevalence distribution in validation sets to quantify how prevalence weighting affects κ reliability estimates