---
ver: rpa2
title: Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding
  Question-Answering Data
arxiv_id: '2501.10937'
source_url: https://arxiv.org/abs/2501.10937
tags:
- speech
- arxiv
- content
- emotion
- empathetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LPE, a novel method that enables empathetic
  spoken dialogue generation without requiring spoken question-answering datasets.
  The approach leverages a two-stage training process that first aligns speech content
  and emotion features with LLM embedding space using ASR and SER data, then employs
  Chain-of-Thought prompting to guide the model in generating empathetic responses.
---

# Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data

## Quick Facts
- **arXiv ID:** 2501.10937
- **Source URL:** https://arxiv.org/abs/2501.10937
- **Reference count:** 40
- **Key outcome:** Novel LPE method achieves empathetic spoken dialogue without QA data using CoT prompting, outperforming speechLLM baselines

## Executive Summary
This paper introduces LPE (Listen, Perceive, Express), a novel method that enables empathetic spoken dialogue generation without requiring spoken question-answering datasets. The approach leverages a two-stage training process that first aligns speech content and emotion features with LLM embedding space using ASR and SER data, then employs Chain-of-Thought prompting to guide the model in generating empathetic responses. The method uses WavLM for speech encoding, feature adapters for modality alignment, and LLaMA2-7B as the LLM decoder. Evaluation shows that LPE outperforms speechLLM baselines like SALMONN and Qwen-Audio-Chat in both objective metrics and subjective evaluation scores.

## Method Summary
The LPE framework consists of a two-stage training process. Stage 1 trains a subsampler adapter on LibriSpeech to align speech content features with LLM embedding space. Stage 2 performs multitask learning on ASR and SER tasks using IEMOCAP, MEAD, and replayed LibriSpeech data, training a multiscale adapter to align emotion features. The method employs zero-shot Chain-of-Thought prompting with three predefined steps: listen (extract content), perceive (recognize emotion), and express (generate empathetic response). WavLM-large serves as the speech encoder, while LLaMA2-7B-chat provides the frozen decoder backbone. The approach achieves competitive performance to cascaded systems while requiring no manual QA data annotation.

## Key Results
- LPE achieves BLEU-1: 16.13, BLEU-4: 2.85, and BERTScore: 82.9, outperforming speechLLM baselines
- Subjective evaluation scores: Content (3.99/5), Empathy (3.72/5), Clarity (4.97/5)
- Zero-shot CoT prompting significantly outperforms few-shot CoT, which causes the model to focus on textual content over input speech
- The approach requires no manual QA data annotation while achieving competitive performance to cascaded systems

## Why This Works (Mechanism)
The method works by creating a bridge between speech modality and LLM embedding space through specialized adapters. The subsampler adapter extracts content features from raw audio, while the multiscale adapter aligns emotion features with LLM space. The Chain-of-Thought prompting provides explicit reasoning steps that guide the model to consider both content and emotion before generating responses. This architectural design allows the model to leverage large-scale ASR and SER data that is more readily available than spoken QA datasets, while the CoT framework ensures empathetic response generation through structured reasoning.

## Foundational Learning
- **Multitask Learning**: Training on multiple related tasks simultaneously to improve generalization across ASR and SER tasks. Why needed: Enables the model to learn both content and emotion features efficiently. Quick check: Verify training loss decreases for both ASR and SER tasks.
- **Modality Alignment**: Using adapters to map speech features into LLM embedding space. Why needed: Allows frozen LLM to process speech-derived features without retraining. Quick check: Confirm content and emotion embeddings are properly concatenated for LLM input.
- **Chain-of-Thought Prompting**: Providing step-by-step reasoning instructions for complex tasks. Why needed: Guides the model to consider content and emotion before generating empathetic responses. Quick check: Test different CoT prompt templates on validation data.
- **Adapter-based Fine-tuning**: Using lightweight adapters instead of full model fine-tuning. Why needed: Preserves LLM capabilities while adding speech processing abilities. Quick check: Monitor adapter parameter updates during training.
- **Speech Encoding**: Using WavLM for robust speech feature extraction. Why needed: Provides high-quality audio representations for content and emotion recognition. Quick check: Validate WavLM outputs on speech recognition benchmarks.

## Architecture Onboarding

**Component Map:** WavLM -> Subsampler Adapter -> Multiscale Adapter -> LLaMA2-7B

**Critical Path:** Raw speech → WavLM encoding → Subsampler adapter (content) + Multiscale adapter (emotion) → LLM input → Empathetic response

**Design Tradeoffs:** Adapter-based approach preserves LLM capabilities but adds complexity; zero-shot CoT avoids manual data annotation but depends on prompt quality; two-stage training enables efficient learning but requires careful stage coordination.

**Failure Signatures:** Model outputs transcriptions instead of responses (Stage 2 training failure); low empathy scores (inadequate emotion feature alignment); focus on text examples over speech (few-shot CoT usage).

**First Experiments:**
1. Test subsampler adapter on LibriSpeech to verify content feature extraction
2. Evaluate multiscale adapter on IEMOCAP for emotion recognition accuracy
3. Apply zero-shot CoT prompting on synthetic speech to validate response generation

## Open Questions the Paper Calls Out

**Open Question 1:** Can the LPE framework be extended to generate spoken audio responses rather than text? The paper notes this as a current limitation, as the architecture utilizes a text-based LLaMA2-7B decoder.

**Open Question 2:** Why does few-shot Chain-of-Thought prompting degrade content performance in this speech-based architecture? The authors observe that few-shot examples cause the model to focus on textual content over input speech, but the root cause remains unclear.

**Open Question 3:** What is the precise relationship between Chain-of-Thought prompt complexity and generation quality in spoken dialogue systems? The results suggest sensitivity to prompt length and structure, but systematic evaluation across prompt variations is needed.

## Limitations
- Relies on publicly available but domain-mismatched data (LibriSpeech, IEMOCAP) which may limit generalization to real-world conversational scenarios
- Zero-shot CoT prompting depends heavily on manually defined prompt steps that are not fully disclosed
- Subjective evaluation using GPT-4o introduces potential bias and may not fully capture human-perceived empathy
- Computational requirements (2× A100 40G, gradient accumulation) may limit accessibility for replication

## Confidence

- **High confidence** in the core methodology and two-stage training framework
- **Medium confidence** in the effectiveness of zero-shot CoT prompting due to limited prompt disclosure
- **Medium confidence** in the claimed performance improvements given the synthetic test data
- **Low confidence** in generalization to real-world conversational scenarios

## Next Checks

1. Implement and test the exact CoT prompt template on held-out conversational data to verify the claimed improvement over few-shot approaches
2. Evaluate the model on naturalistic spoken dialogue datasets beyond IEMOCAP to assess real-world robustness
3. Conduct human evaluation studies to validate the GPT-4o subjective scoring, particularly for empathy assessment