---
ver: rpa2
title: 'FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data'
arxiv_id: '2512.02350'
source_url: https://arxiv.org/abs/2512.02350
tags:
- policy
- local
- learning
- offline
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of performance degradation in
  offline Federated Reinforcement Learning (FRL) when data quality varies across clients.
  Existing methods struggle when some clients have low-quality offline data, which
  can "contaminate" the global policy during aggregation.
---

# FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data

## Quick Facts
- arXiv ID: 2512.02350
- Source URL: https://arxiv.org/abs/2512.02350
- Reference count: 40
- This paper introduces FOVA, an offline federated RL framework that achieves 20%-50% performance improvements on standard benchmarks by addressing mixed-quality data scenarios.

## Executive Summary
FOVA addresses performance degradation in offline federated reinforcement learning when data quality varies across clients. Existing federated methods struggle when some clients have low-quality offline data, which can "contaminate" the global policy during aggregation. The proposed framework introduces a vote mechanism that selects the best behavior from global, local, and behavioral policies based on Q-values, and an Advantage-Weighted Regression (AWR) method that ensures consistent optimization objectives between local and global policies. These components help identify high-return actions and prevent low-quality behaviors from degrading performance. Theoretical analysis proves FOVA guarantees strict policy improvement over behavioral policies, with convergence properties established under standard assumptions.

## Method Summary
FOVA operates by broadcasting a global policy and Q-function to clients, who then update their local models using a three-step process. First, a vote mechanism selects actions by maximizing Q-values across local, global, and behavioral policies. Second, a Conservative Q-Learning critic is updated with vote-based regularization. Third, a policy update uses Advantage-Weighted Regression to align local optimization with global objectives. The server aggregates updated parameters using FedAvg. The framework specifically handles mixed-quality data scenarios by filtering low-quality behaviors at the client level before aggregation.

## Key Results
- FOVA achieves average score improvements of 20%-50% over existing methods on standard D4RL benchmarks
- In mixed-quality data scenarios, FOVA maintains performance improvements of up to 56% while baselines degrade significantly
- Theoretical guarantees prove strict policy improvement over behavioral policies under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1: Vote-Based Behavior Selection
The vote mechanism mitigates performance degradation by selectively identifying high-return actions from a candidate set of policies before local policy evaluation. During local evaluation, the system compares Q-values from three sources (local policy, local behavioral policy, and global policy) and selects the action that maximizes the Q-value. This effectively filters out low-quality behaviors locally before they can contaminate the Q-function update.

### Mechanism 2: Consistent Optimization via Advantage-Weighted Regression (AWR)
The AWR component resolves objective inconsistency between local updates (optimizing for local data) and global aggregation (optimizing for the server policy). It modifies the local policy improvement objective to maximize an advantage-weighted objective while staying close to the behavioral policy, ensuring that local training directly benefits the global policy.

### Mechanism 3: Conservative Q-Learning with Vote Regularization (VCQL)
The framework maintains safety by integrating the vote policy into a Conservative Q-Learning (CQL) regularization term. Standard offline RL suffers from extrapolation error, and FOVA adapts this by calculating the regularization term using the vote policy rather than just the behavior policy, yielding a lower bound on the true value function.

## Foundational Learning

- **Concept: Offline Reinforcement Learning & Distribution Shift**
  - Why needed here: FOVA is built specifically for *offline* FRL, where agents cannot interact with the environment. You must understand why offline RL fails (distributional shift/extrapolation error) to grasp why mechanisms like CQL and behavioral constraints (KL divergence) are necessary.
  - Quick check question: Why does standard off-policy RL often fail when trained solely on a fixed dataset without environment interaction?

- **Concept: Federated Averaging (FedAvg) and Heterogeneity**
  - Why needed here: The paper positions itself against standard FedAvg approaches. You need to understand that FedAvg averages model parameters and how "mixed-quality" data creates a specific form of non-IID heterogeneity that degrades the global model.
  - Quick check question: In standard FedAvg, what happens to the global model if 50% of clients train on random noise while 50% train on expert data?

- **Concept: Advantage Functions & KL Constraints**
  - Why needed here: The AWR mechanism relies on maximizing the *advantage* (A(s,a) = Q(s,a) - V(s)) subject to a KL-divergence constraint. Understanding this trade-off (reward vs. proximity to behavioral policy) is essential for tuning the β and λ parameters.
  - Quick check question: What does a high KL divergence between a new policy and a behavioral policy typically indicate in the context of offline RL safety?

## Architecture Onboarding

- **Component map:**
  - Client Side: Offline Dataset -> Vote Module -> Critic (Q-Net) -> Actor (Policy Net)
  - Server Side: Aggregator (FedAvg) -> Broadcaster

- **Critical path:**
  1. Server broadcasts global policy π̄ and Q-function Q̄ to all clients
  2. Client Loop: Sample batch from local D_k
  3. Vote: Compute Q-values for actions from π_k, π_β_k, π̄; select max to define π_v
  4. Critic Update: Update local Q_k using VCQL loss (Bellman error + CQL penalty with π_v)
  5. Actor Update: Update local π_k using AWR objective (maximize advantage weighted by exp(Advantage/β))
  6. Sync: Upload π_k, Q_k to server -> Average -> Repeat

- **Design tradeoffs:**
  - Computational Overhead: The vote mechanism requires evaluating Q-values for three policies per state, increasing inference cost during training (Table IV shows ~2% runtime increase)
  - Stability vs. Performance: High values of α (conservatism) and β (AWR temperature) ensure safety but may limit the policy's ability to exceed the behavioral policy's performance
  - Assumption: The paper assumes linear aggregation suffices if local objectives are aligned; complex aggregation rules (like robust aggregation) are not used but might be needed for Byzantine failures

- **Failure signatures:**
  - Objective Misalignment: If server return is consistently lower than average client return (Fig 1b), the AWR objective is not correctly aligning local updates to the global policy
  - Quality Contamination: If introducing low-quality clients drops global performance drastically (>40%), the Vote mechanism is failing to filter bad actions (check Q-value scale/normalization)
  - Collapse: If Q-values diverge to infinity, the CQL regularization weight α is too low or the vote policy is selecting OOD actions with unpenalized high Q-values

- **First 3 experiments:**
  1. Sanity Check (Mixed Quality): Replicate the "Mixed Quality" scenario (Fig 5/6). Train with 2 Expert clients and 2 Random clients. Verify that FOVA maintains performance while baselines (FedAvg, DRPO) collapse
  2. Ablation (Vote vs. AWR): Run FOVA with *only* the Vote mechanism and *only* the AWR mechanism (Fig 7d). Confirm that both are required for the full performance gain (synergy effect)
  3. Hyperparameter Sensitivity: Sweep λ and β (Fig 7a) on a single task (e.g., Hopper) to find the stable region where policy improvement is guaranteed without becoming too conservative

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the vote mechanism be optimized to reduce computational and memory overhead while maintaining performance discrimination?
- Basis in paper: [explicit] Section VI states the vote mechanism "incurs non-negligible GPU memory overhead" and suggests future work on "memory-efficient vote through selective value estimation."
- Why unresolved: The current method requires simultaneous Q-value evaluations for global, local, and behavioral policies, creating a resource bottleneck on edge devices.
- What evidence would resolve it: A modified architecture demonstrating reduced memory usage per client without statistically significant degradation in normalized scores on mixed-quality benchmarks.

### Open Question 2
- Question: How can FOVA automatically adapt hyperparameters (λ, β) to varying levels of data quality heterogeneity?
- Basis in paper: [explicit] Section VI notes that "certain hyperparameters currently lack adaptive tuning capabilities" and identifies this as a concrete direction for future investigation.
- Why unresolved: The performance of the Advantage-Weighted Regression (AWR) relies on fixed coefficients, which may be suboptimal as data distributions change.
- What evidence would resolve it: Theoretical and empirical validation of a self-tuning mechanism that adjusts λ and β dynamically in response to real-time metrics of client data heterogeneity.

### Open Question 3
- Question: Can FOVA be extended to handle dynamic environments where the MDP transitions or reward structures are non-stationary?
- Basis in paper: [explicit] Section VI lists "adapting FOVA to dynamic environments" and developing algorithms to "detect and respond to changes in real time" as a primary future direction.
- Why unresolved: The current framework assumes a static MDP and fixed offline datasets, limiting applicability in scenarios where environmental dynamics evolve after deployment.
- What evidence would resolve it: Successful integration of recurrent structures or memory mechanisms that allow the global policy to maintain performance under concept drift in the underlying MDP.

## Limitations

- Implementation Complexity: The method requires simultaneous implementation of three components (Vote mechanism, AWR, CQL regularization) making debugging and hyperparameter tuning non-trivial
- Behavioral Policy Estimation: The need to sample from local behavioral policies (π_β_k) adds an implicit requirement for behavior cloning models that isn't fully detailed in the methodology
- Scalability to Real-World Settings: While experiments use D4RL benchmarks with synthetic heterogeneity, the approach's robustness to non-stationary client participation and communication constraints in production environments remains untested

## Confidence

- **High Confidence**: The theoretical guarantees (policy improvement over behavioral policy, convergence under standard assumptions) are well-supported by the mathematical proofs in Appendix A
- **Medium Confidence**: The empirical results showing 20%-50% performance improvements are convincing, though the specific hyperparameter configurations (particularly CQL alpha values) could benefit from more transparency
- **Low Confidence**: The claim about "56% performance improvement in mixed-quality data scenarios" lacks context about baseline selection and variance reporting across runs

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct systematic sweeps of the CQL regularization weight (α) and AWR temperature (β) across multiple random seeds to identify stable operating regions and quantify performance variance
2. **Behavior Policy Fidelity Study**: Compare results using different methods for estimating local behavioral policies (direct dataset sampling vs. behavior cloning models) to assess sensitivity to this implementation detail
3. **Scaling Experiment**: Test FOVA with increasing numbers of clients (K=5→20→50) on the same benchmark tasks to evaluate computational scaling and performance stability under realistic federated conditions