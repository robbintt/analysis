---
ver: rpa2
title: Understanding Fairness and Prediction Error through Subspace Decomposition
  and Influence Analysis
arxiv_id: '2510.23935'
source_url: https://arxiv.org/abs/2510.23935
tags:
- fairness
- sensitive
- learning
- information
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in machine learning by proposing
  a principled framework that adjusts data representations to balance predictive utility
  and fairness. The core idea uses sufficient dimension reduction to decompose the
  feature space into target-relevant, sensitive, and shared components, enabling controlled
  removal of sensitive information.
---

# Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis

## Quick Facts
- **arXiv ID**: 2510.23935
- **Source URL**: https://arxiv.org/abs/2510.23935
- **Reference count**: 40
- **Key outcome**: Proposes Sequential Fair Projection (SFP) method using sufficient dimension reduction to decompose feature space into target-relevant, sensitive, and shared subspaces, achieving strong fairness improvements while preserving predictive accuracy on real-world datasets.

## Executive Summary
This paper addresses fairness in machine learning by proposing a principled framework that adjusts data representations to balance predictive utility and fairness. The core idea uses sufficient dimension reduction to decompose the feature space into target-relevant, sensitive, and shared components, enabling controlled removal of sensitive information. The method selectively removes directions informative about the sensitive attribute while preserving task-relevant information, offering flexible control over the fairness-utility trade-off. Theoretical analysis shows how prediction error and fairness gaps evolve as shared subspaces are added, while influence functions characterize the asymptotic behavior of parameter estimates. Experiments on synthetic and real-world datasets demonstrate that SFP achieves strong fairness improvements while preserving predictive accuracy.

## Method Summary
The Sequential Fair Projection (SFP) method estimates central subspaces SY|X and SZ|X via sufficient dimension reduction (SDR), then identifies their intersection through generalized eigenvalue decomposition. This yields three orthogonal components: target-relevant/Z-orthogonal (retained), shared Y-Z (controllable), and Z-only (removed). The method sequentially adds shared dimensions to the projection, monotonically decreasing prediction error while reintroducing bias in a controlled manner. The final model is selected by minimizing demographic parity gap subject to maintaining at least 95% of original validation accuracy. Implementation uses MSAVE for SDR estimation, ladle estimator for intersection dimension, and logistic regression as the downstream model.

## Key Results
- On Adult dataset, SFP achieves 76.88% accuracy with lowest demographic parity gap (3.78%) among compared methods
- Sequential addition of shared directions monotonically decreases prediction error while increasing fairness gaps in a controlled manner
- Influence function analysis provides asymptotic normality guarantees for parameter estimates under the fair projection framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing the feature space into target-relevant, sensitive, and shared subspaces enables principled removal of bias while preserving predictive utility.
- **Mechanism**: The method estimates central subspaces SY|X and SZ|X via sufficient dimension reduction (SDR). Their intersection (shared directions) is identified through a generalized eigenvalue decomposition: M_Y Σ M_Z ν = λΣ ν (Theorem 4.1). This yields three components: (1) Y-relevant/Z-orthogonal (retained), (2) shared Y-Z (controllable), (3) Z-only (removed).
- **Core assumption**: Both Y and Z admit low-dimensional linear sufficient reductions: Y ⊥⊥ X | B_Y^T X and Z ⊥⊥ X | B_Z^T X (Equations 2-4).
- **Evidence anchors**: [abstract] "Using sufficient dimension reduction, we decompose the feature space into target-relevant, sensitive, and shared components"; [Section 3.2] "We decompose SY|X into two orthogonal parts: one shared with SZ|X and one independent of it"
- **Break condition**: If Y and Z share no subspace (s=0), fairness comes at zero utility cost; if SY|X ⊂ SZ|X (complete overlap), no fair representation exists without utility loss.

### Mechanism 2
- **Claim**: Sequentially adding shared dimensions monotonically decreases prediction error while reintroducing bias in a controlled manner.
- **Mechanism**: The projection P^(m) = B̂B̂^T + Φ_mΦ_m^T includes m of s shared directions. Theorem 3.2 proves approximation error Δ(m) = E[Var(f*(X)|Ξ^(m))] is non-increasing in m. Distance covariance (Theorem 3.3) quantifies fairness degradation as shared information re-enters.
- **Core assumption**: The Bayes predictor f̃^(m)(Ξ^(m)) = E[Y|Ξ^(m)] accurately reflects downstream model behavior.
- **Evidence anchors**: [Section 3.3] "Δ(m+1) ≤ Δ(m) for all m ∈ {0,...,s-1}"; [Section 6.1, Figure 1] Simulation shows MSE decreases while MSE gap between groups increases as m grows
- **Break condition**: If dimension estimation (Ŝ) is inaccurate, the wrong number of shared directions may be retained or removed.

### Mechanism 3
- **Claim**: Influence function analysis characterizes how fair projections affect parameter estimation and prediction uncertainty.
- **Mechanism**: The composed mapping F_n → P^(m)(F_n) → θ^(m)(F_n, P^(m)) → f(·; θ^(m)) admits asymptotic normality (Theorem 5.2). The variance involves a trade-off: lower dimension reduces asymptotic variance, but truncating directions introduces bias via ‖θ̃^(m) - θ̃‖² (Corollary 5.3).
- **Core assumption**: All statistical functionals (M_Y, M_Z, Σ, θ) are Hadamard differentiable with zero-mean, finite-variance influence functions.
- **Evidence anchors**: [Section 5.2] "√n(θ̂_n^(m) - θ̃^(m)) → N(0, Var[...])" (Equation 8); [Corollary 5.3] "E(‖θ̂_n^(m) - θ̂_n‖²) ≤ ‖θ̃^(m) - θ̃‖² + (1/n)Tr(Var[...])"
- **Break condition**: Non-differentiable loss functions or non-regular models violate asymptotic normality guarantees.

## Foundational Learning

- **Concept: Sufficient Dimension Reduction (SDR)**
  - Why needed here: Core theoretical tool; assumes Y ⊥⊥ X | B^T X, meaning low-dimensional projections preserve all predictive information.
  - Quick check question: Can you explain why B_z^T X captures all Z-relevant information in X per Equation (1)?

- **Concept: Influence Functions**
  - Why needed here: Enables asymptotic analysis of how perturbations (e.g., adding/removing directions) propagate through estimation and prediction.
  - Quick check question: What does R*(S) = ∂R[(1-ε)F₀ + εδ_S]/∂ε|_{ε=0} represent?

- **Concept: Distance Covariance (dCov)**
  - Why needed here: Model-agnostic fairness measure capturing statistical dependence between predictor and sensitive attribute.
  - Quick check question: Why does dCov²(f̃^(m), Z) = 0 at m=0 imply fairness (Theorem 3.3)?

## Architecture Onboarding

- **Component map**: Data (X, Y, Z) → SDR Candidate Matrices (M̂_Y, M̂_Z via SIR/SAVE/DR) → Intersection Estimation (M̂_{Y,Z} → Φ̂ via GEVD) → Sequential Projections (P^(m) for m=0,...,Ŝ) → Post-SDR Training (fit model on P^(m)X) → Model Selection (min DP subject to accuracy ≥ 95% baseline)

- **Critical path**:
  1. Estimate M_Y and M_Z using Sliced Inverse Regression (SIR) or SAVE with (p+1) slices
  2. Compute Σ̂ = cov(X), form M̂_{Y,Z} = M̂_Y Σ̂ M̂_Z
  3. Apply ladle estimator for rank Ŝ (intersection dimension)
  4. Extract Φ̂ as leading Ŝ eigenvectors of M̂_{Y,Z}
  5. Estimate B̂_{Y,Q_z} from (Y, Q_z X) where Q_z = I - P_z
  6. Form P^(m) and train downstream models sequentially

- **Design tradeoffs**:
  - SDR method choice: SIR is faster but may miss nonlinear structure; SAVE captures more but requires more data
  - Threshold τ (default 95%): Higher preserves more utility but accepts more bias
  - Step size for m: Stepping every 2-3 dimensions reduces computation but may skip optimal trade-off

- **Failure signatures**:
  - Ŝ ≈ p: Linear SDR assumption violated; no low-dimensional fair representation exists
  - Accuracy drops >10% at m=0: High overlap between Y and Z subspaces
  - High variance across replications: Unstable SDR estimates, increase sample size or reduce dimension

- **First 3 experiments**:
  1. Replicate synthetic experiment (Section 6.1) with n=5000, p=10, q=r=8, s=6; verify RMSE decreases and RMSE gap increases with m
  2. Run SFP on Adult dataset with gender as Z; confirm competitive accuracy (≥76%) and low DP gap (≤5%)
  3. Stress-test with violated SDR assumption: add ‖X‖²/30 term to Y and Z generation; check if trade-off curve still emerges (Section 6.3, Figure 2)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the Sequential Fair Projection framework be extended to capture nonlinear dependencies between representations and sensitive attributes using nonlinear sufficient dimension reduction?
- **Basis in paper**: [explicit] "A promising future direction is to extend the framework using nonlinear SDR theory. In this approach, the representation is first mapped into a higher-dimensional feature space using a predefined kernel function."
- **Why unresolved**: Current framework relies on linear SDR assumptions that limit ability to mitigate nonlinear dependencies embedded in representations.
- **What evidence would resolve it**: Development and empirical validation of a kernel-based or neural SDR extension showing improved fairness-utility trade-offs on datasets with known nonlinear biases.

### Open Question 2
- **Question**: What is the theoretical relationship between the number of shared dimensions retained and specific fairness metrics beyond distance covariance?
- **Basis in paper**: [inferred] The paper shows approximation error decreases monotonically with m and uses dCov to measure fairness, but does not provide closed-form relationships for common metrics like demographic parity or equalized odds as m varies.
- **Why unresolved**: Theoretical characterization in Section 3.3 focuses on dCov rather than widely-used fairness metrics, limiting interpretability for practitioners.
- **What evidence would resolve it**: Derivation of theoretical bounds connecting the shared subspace dimension to DP gap, TPR gap, or other standard fairness metrics.

### Open Question 3
- **Question**: How robust is the method to errors in estimating the intersection subspace dimension and the central subspaces under model misspecification?
- **Basis in paper**: [inferred] Section 6.3 shows SFP maintains utility-fairness trade-offs when linear SDR is violated, but offers only a heuristic remedy of retaining leading eigenvalue directions.
- **Why unresolved**: No theoretical guarantees on estimation consistency or fairness-utility trade-off quality when the low-rank assumption fails or rank is misspecified.
- **What evidence would resolve it**: Theoretical analysis of estimation error propagation and empirical evaluation with varying degrees of nonlinearity in data-generating processes.

## Limitations
- The framework's theoretical guarantees rely on SDR assumptions that may fail when Y and Z share substantial high-dimensional structure (Ŝ ≈ p), limiting applicability
- Experimental validation focuses on linear models and controlled synthetic settings; generalization to complex architectures remains untested
- The influence function analysis assumes smooth, differentiable models—violated by non-regular losses or discrete predictions

## Confidence
- **High**: SDR-based subspace decomposition and sequential projection mechanics (Section 3)
- **Medium**: Theoretical error bounds and fairness-utility trade-off (Theorems 3.2-3.3)
- **Medium**: Asymptotic normality and influence-based uncertainty characterization (Section 5)
- **Medium**: Empirical performance on standard fairness benchmarks (Section 6)

## Next Checks
1. **Stress-test SDR assumption**: Generate data where Y and Z share full high-dimensional structure (complete overlap); verify whether the method degrades gracefully or produces unstable estimates
2. **Cross-model generalization**: Apply SFP to tree-based models (e.g., XGBoost) and neural networks; compare trade-off curves to logistic regression baseline
3. **Sample complexity analysis**: Systematically vary n (5k → 50k) and p (10 → 100) in synthetic experiments; quantify how Ŝ stability and prediction error scale