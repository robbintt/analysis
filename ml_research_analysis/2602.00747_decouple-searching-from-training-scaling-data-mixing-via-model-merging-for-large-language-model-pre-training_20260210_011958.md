---
ver: rpa2
title: 'Decouple Searching from Training: Scaling Data Mixing via Model Merging for
  Large Language Model Pre-training'
arxiv_id: '2602.00747'
source_url: https://arxiv.org/abs/2602.00747
tags:
- data
- arxiv
- training
- mixture
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of determining optimal data
  mixtures for large language model (LLM) pre-training, particularly when balancing
  general competence with proficiency on hard tasks like math and code. The proposed
  Decouple Searching from Training Mix (DeMix) framework uses model merging to construct
  proxy models that predict optimal data ratios without requiring additional training.
---

# Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training

## Quick Facts
- **arXiv ID**: 2602.00747
- **Source URL**: https://arxiv.org/abs/2602.00747
- **Reference count**: 40
- **Primary result**: 6.4× cost reduction (212B vs 1344B tokens) while achieving superior mixture quality via model merging-based proxy generation

## Executive Summary
This paper introduces DeMix, a framework that addresses the computational bottleneck in finding optimal data mixture ratios for LLM pre-training. Instead of training a proxy model for each candidate mixture, DeMix uses model merging to construct proxy models without additional training. By training component models on individual datasets and merging them with weighted linear combinations, DeMix enables unlimited evaluation of sampled mixtures at zero training cost. The method achieves higher proxy accuracy and better mixture quality than training-based approaches while requiring significantly less computational budget.

## Method Summary
DeMix decouples mixture searching from training by using model merging to create proxy models that approximate the performance of models trained on specific data mixtures. The framework trains a shared base model and component models on individual datasets, then generates proxies via weighted linear merging (M_mix = Σ αᵢ Θᵢ). A LightGBM predictor is trained to map mixture ratios to benchmark rankings, enabling efficient search for optimal ratios. The approach iteratively samples mixtures, evaluates merged proxies, and refines predictions to identify high-quality data mixtures without the computational overhead of training separate models for each candidate ratio.

## Key Results
- Achieves 6.4× cost reduction (212B vs 1344B tokens) compared to conventional training-based approaches
- Higher proxy accuracy with Spearman's ρ scores significantly above training-based baselines
- Superior mixture quality as measured by benchmark ranks across general, math, and code tasks
- Releases DeMix Corpora, a 22T-token dataset with validated mixtures for LLM pre-training research

## Why This Works (Mechanism)
The method works by leveraging the fact that model merging can approximate the behavior of models trained on different data mixtures without requiring actual training. By creating component models on individual datasets and combining them with appropriate weights, DeMix generates proxies that reflect the expected performance of full training runs. This eliminates the need to train separate proxy models for each candidate mixture ratio, dramatically reducing computational costs while maintaining prediction accuracy. The iterative sampling and prediction refinement ensures convergence to high-quality mixtures.

## Foundational Learning
- **Model merging fundamentals**: Understanding how linear combinations of model parameters can approximate models trained on different data distributions (why needed: core to proxy generation; quick check: verify merged models maintain domain-specific capabilities)
- **Data mixture optimization**: Principles of balancing multiple data sources to achieve desired model capabilities across different task domains (why needed: defines the optimization objective; quick check: validate component models perform well on their target domains)
- **Spearman's rank correlation**: Statistical measure for evaluating proxy accuracy by comparing predicted vs. actual rankings (why needed: primary proxy accuracy metric; quick check: compute ρ between merged proxies and reference models)
- **LightGBM for regression**: Gradient boosting framework used to predict mixture rankings from mixture ratios (why needed: enables efficient mixture search; quick check: verify prediction accuracy on held-out mixtures)

## Architecture Onboarding

**Component map:**
Shared base model → 7 component models → Merged proxies → LightGBM predictor → Optimal mixture

**Critical path:**
Train base model (50B tokens) → Train component models (7×30B tokens) → Sample and merge proxies (3× iterations) → Train LightGBM predictor → Predict and validate optimal mixture (50B tokens)

**Design tradeoffs:**
- **Linear vs. non-linear merging**: Linear merging is computationally efficient but may not capture complex interactions between datasets
- **Proxy evaluation frequency**: More proxies improve accuracy but increase evaluation costs; 224 proxies found to be optimal
- **Predictor model choice**: LightGBM provides good accuracy with limited training data but may overfit with too many proxies

**Failure signatures:**
- Merged proxies show low Spearman's ρ (<0.5) with reference models → Component models undertrained or too dissimilar
- Final mixture quality plateaus with more proxies (>224) → Predictor overfitting to noise in proxy rankings
- High variance in benchmark scores across merged proxies → Inconsistent component model training or evaluation

**First experiments to run:**
1. Train shared base model on 50B general tokens and verify baseline performance
2. Train all 7 component models and validate each achieves meaningful scores on target domain benchmarks
3. Generate 10-20 merged proxies and evaluate Spearman's ρ with reference models to verify proxy quality

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends on quality and diversity of component models, but dataset filtering parameters are not fully disclosed
- 6.4× cost reduction assumes component model pretraining (80B tokens) is acceptable overhead, which may not hold for all scenarios
- Linear merging weights may be insufficient for capturing complex interactions between datasets

## Confidence

**High confidence**: Proxy accuracy results (Spearman's ρ), computational cost reduction estimates, core algorithmic framework

**Medium confidence**: Mixture quality improvements relative to baselines, generalizability of cost reduction claim

## Next Checks
1. Verify each component model achieves meaningful performance on its target domain benchmarks before merging
2. Test whether proxy accuracy plateaus or degrades when generating more than 224 mixture evaluations
3. Evaluate how cost reduction ratio changes when varying the number of candidate datasets or using different model sizes