---
ver: rpa2
title: Optimal Implicit Bias in Linear Regression
arxiv_id: '2506.17187'
source_url: https://arxiv.org/abs/2506.17187
tags:
- convex
- have
- optimal
- which
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding the optimal implicit
  bias that leads to the best generalization performance in over-parameterized linear
  regression. The authors analyze interpolators obtained by minimizing convex potentials
  subject to interpolation constraints and characterize their precise asymptotic generalization
  error in terms of the over-parameterization ratio, noise variance, eigenspectrum
  of data covariance, and distribution of the true parameter.
---

# Optimal Implicit Bias in Linear Regression

## Quick Facts
- **arXiv ID**: 2506.17187
- **Source URL**: https://arxiv.org/abs/2506.17187
- **Reference count**: 40
- **One-line primary result**: Characterizes optimal implicit bias for over-parameterized linear regression through precise asymptotic analysis

## Executive Summary
This paper addresses the fundamental question of finding the optimal implicit bias in over-parameterized linear regression. The authors analyze interpolators obtained by minimizing convex potentials subject to interpolation constraints, providing a precise asymptotic characterization of their generalization error. They establish tight lower bounds on achievable generalization error using Fisher information and construct optimal convex potentials that achieve these bounds under log-concavity conditions. The work provides both theoretical foundations and practical guidance for selecting implicit biases that minimize generalization error in high-dimensional settings.

## Method Summary
The paper analyzes over-parameterized linear regression where infinitely many solutions interpolate the training data. A separable convex potential $\Psi(\beta) = \sum_{i=1}^n \psi(\beta_i, \Sigma_{i,i})$ selects a unique interpolator by minimizing the potential over the interpolation set. Using the Convex Gaussian Minimax Theorem (CGMT) framework, the authors reduce the high-dimensional problem to a scalar minimax problem, deriving a system of two nonlinear equations that characterize the asymptotic generalization error. They establish fundamental lower bounds using Fisher information of the prior distribution convolved with Gaussian noise, and construct optimal potentials that achieve these bounds when the convolved distribution is log-concave.

## Key Results
- Provides precise asymptotic characterization of generalization error for separable convex potentials via a system of two nonlinear equations
- Establishes tight lower bounds on generalization error using Fisher information of convolved priors
- Constructs optimal convex potentials that achieve the lower bounds under log-concavity conditions
- Shows that for Gaussian priors, the optimal potential corresponds to weighted ridge regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The asymptotic generalization error is determined by solving a system of two nonlinear equations derived from CGMT.
- Mechanism: In over-parameterized regime with infinitely many interpolating solutions, a separable convex potential selects a unique solution. The CGMT framework reduces high-dimensional analysis to a scalar minimax problem, where the generalization error converges to $\alpha_\psi^2$.
- Core assumption: Gaussian features and noise with proportional high-dimensional limit.
- Break condition: Non-separable or non-convex potentials, or non-Gaussian feature distributions invalidate the characterization.

### Mechanism 2
- Claim: A tight lower bound on generalization error is governed by Fisher information of the prior convolved with Gaussian.
- Mechanism: The fundamental limit $\alpha_*^2$ cannot be beaten by any convex-potential interpolator, involving the weighted Fisher information $I_\Lambda(V_\alpha)$ of the convolved distribution.
- Core assumption: Well-defined Fisher information and computable weighted Fisher information.
- Break condition: The bound's tightness is not guaranteed for all parameters; non-log-concave convolved distributions may make the bound unachievable.

### Mechanism 3
- Claim: The optimal convex potential can be explicitly constructed from the log-concave density of the Gaussian-convolved prior.
- Mechanism: When $P_{V_{\alpha^*}}$ is log-concave, the optimal potential $\psi^*$ is constructed via a Moreau envelope of the negative log density, achieving the lower bound $\alpha_*^2$.
- Core assumption: The convolved distribution must be log-concave, with log-concave priors being sufficient.
- Break condition: Non-log-concave convolved distributions yield non-convex potentials, invalid for the considered class.

## Foundational Learning

### Moreau Envelope & Proximal Operator
- Why needed here: The system of equations and optimal potential construction are expressed using Moreau envelopes and their derivatives.
- Quick check question: For $\psi(y) = |y|$ (L1 norm), what is the proximal operator $\text{prox}_\psi(x; \alpha)$?

### Fisher Information
- Why needed here: The lower bound uses Fisher information of the convolved prior, connecting estimation theory to generalization.
- Quick check question: What is the Fisher information of $X \sim \mathcal{N}(0, \sigma^2)$, and what does it quantify?

### Log-concavity
- Why needed here: Log-concavity of the convolved prior determines whether the optimal potential is convex and the bound is achievable.
- Quick check question: Is the PDF of a Gaussian distribution log-concave? What about a mixture of two Gaussians with means very far apart?

## Architecture Onboarding

### Component map
Problem Specification ($\delta, \sigma^2, P_B, \Lambda$) -> Analysis Engine (CGMT) -> Characterization Module (System of equations (6)) -> Fundamental Limits Module -> Lower Bound (equation 10) -> Optimal Potential Constructor (Theorem 3.6)

### Critical path
To predict generalization error for a given potential: 1) Specify problem parameters; 2) Formulate potential $\psi$; 3) Numerically solve system (6) to find $\alpha_\psi$; 4) Predicted error is $\alpha_\psi^2$.

### Design tradeoffs
Tradeoff between prior information richness and algorithm simplicity. Optimal potential requires knowing $P_B$, while simpler potentials like L2 may be near-optimal without this knowledge, especially in high-noise or isotropic regimes.

### Failure signatures
Solution $\alpha_\psi$ to system (6) not existing or not being unique (Moreau envelope not strictly concave). If $P_{V_{\alpha^*}}$ is not log-concave, optimal potential constructor yields non-convex function, invalid for claimed interpolator class.

### First 3 experiments
1. **Validate Asymptotics for L2**: Simulate with $n=2000, m=400$ and Gaussian data. Train minimum L2-norm interpolator and compare empirical error to theoretical prediction from solving (6).
2. **Test Optimality Construction**: Use sparse Gaussian prior for $B$. For given $\delta$ and low noise $\sigma^2$, verify log-concavity, construct optimal potential $\psi^*$, and implement SMD. Compare error to standard L1 and L2 interpolators.
3. **Probe Log-Concavity Boundary**: Choose prior $P_B$ as mixture of two Gaussians with means far apart (non-log-concave). Construct $\psi^*$ from (13) and check convexity. Observe if generalization error fails to meet theoretical lower bound $\alpha_*^2$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the characterization of optimal implicit bias and derived lower bounds be extended to non-asymptotic settings with finite dimensions?
- Basis in paper: Section 2.1 states CGMT allows non-asymptotic analysis "which we defer for future work." Section 6 lists extending to non-asymptotic settings as important future direction.
- Why unresolved: Current analysis relies on proportional asymptotic limit to solve nonlinear equations and establish tight bounds.
- What evidence would resolve it: Deriving non-asymptotic generalization bounds for optimal convex potential or providing finite-sample convergence rates.

### Open Question 2
- Question: How does optimal implicit bias change when generalizing to non-linear models like deep neural networks?
- Basis in paper: Section 5 notes results may not directly translate to non-linear settings. Section 6 concludes generalizing to non-linear models is future direction.
- Why unresolved: Theoretical derivation relies on linearity and Gaussianity, which may not hold for deep learning feature representations.
- What evidence would resolve it: Theoretical extension to non-linear settings or empirical validation showing optimal convex potential improves generalization in deep architectures.

### Open Question 3
- Question: What is the optimal implicit bias for minimizing generalization error in classification tasks?
- Basis in paper: Section 6 explicitly states it would be "interesting to... study the role of implicit bias in classification problems."
- Why unresolved: Current work focuses on square loss for regression, while classification uses cross-entropy or logistic loss.
- What evidence would resolve it: Deriving optimal convex potential for interpolating classifiers or formulating equivalent lower bound on classification error.

### Open Question 4
- Question: Do generalization error limits and optimal potential construction for Gaussian data hold for sub-Gaussian distributions?
- Basis in paper: Section 5 notes while authors believe results depend only on second-order statistics due to universality, "this remains to be shown" for specific analysis.
- Why unresolved: Proofs utilize specific properties of CGMT and Gaussian feature assumptions.
- What evidence would resolve it: Proof establishing universality of derived asymptotic error equations for sub-Gaussian covariates.

## Limitations
- Theoretical results rely heavily on Gaussian feature assumptions and high-dimensional proportional limits, limiting direct applicability to real-world non-Gaussian data
- Optimal potential construction requires log-concavity of convolved prior, which may not hold for certain priors or noise levels
- Paper does not address computational complexity of finding and implementing optimal potential in practice

## Confidence

- **High Confidence**: Asymptotic characterization of generalization error for separable convex potentials (Theorem 3.1) and fundamental lower bound (Theorem 3.4)
- **Medium Confidence**: Construction of optimal convex potential (Theorem 3.6) - theoretical derivation sound but achievability requirement and lack of practical implementation details reduce confidence
- **Low Confidence**: Practical significance of optimal potential - theoretical construction provided but limited empirical evidence on real-world performance

## Next Checks

1. **Log-concavity Verification**: Systematically test log-concavity of convolved prior distribution across different parameter regimes (varying δ, σ², and signal priors) to determine precise boundary conditions where optimal potential construction fails.

2. **Non-Gaussian Feature Analysis**: Extend theoretical framework to sub-Gaussian or heavy-tailed feature distributions and quantify how much asymptotic generalization error predictions deviate from Gaussian case in practical over-parameterized settings.

3. **Computational Tractability Study**: Implement optimal potential construction for various priors and compare computational cost of optimization (e.g., SMD convergence time) against standard potentials, measuring practical tradeoff between optimality and computational efficiency.