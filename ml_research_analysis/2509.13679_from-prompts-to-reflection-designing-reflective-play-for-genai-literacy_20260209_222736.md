---
ver: rpa2
title: 'From Prompts to Reflection: Designing Reflective Play for GenAI Literacy'
arxiv_id: '2509.13679'
source_url: https://arxiv.org/abs/2509.13679
tags:
- genai
- players
- game
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ImaginAItion is a multiplayer party game that promotes reflective
  understanding of generative AI through constrained prompting and peer comparison.
  In ten sessions with 30 adults, gameplay led to 73.8% of participants achieving
  calibrated understandings of model behaviors.
---

# From Prompts to Reflection: Designing Reflective Play for GenAI Literacy

## Quick Facts
- arXiv ID: 2509.13679
- Source URL: https://arxiv.org/abs/2509.13679
- Reference count: 40
- Key outcome: 73.8% of participants achieved calibrated understandings of model behaviors through constrained prompting and peer comparison gameplay

## Executive Summary
ImaginAItion is a multiplayer party game designed to build critical generative AI literacy through reflective gameplay. By constraining prompt length and enabling peer comparison of AI-generated images, the game surfaces model biases and interpretation differences that single-player use typically obscures. In ten sessions with 30 adults, gameplay led to significant improvements in calibrated understanding of model behaviors, with players developing strategies to mitigate defaults and recognizing human-AI perceptual mismatches.

## Method Summary
The study used a web-based multiplayer game with 6 rounds of constrained prompting (70 seconds per round) followed by voting and reveal phases. Thirty participants were recruited and divided into 10 groups of 3 players each, playing 60-minute sessions that included pre-survey, gameplay, and post-survey/interview. The game used OpenAI's gpt-image-1 API and employed structured logging of prompts, images, votes, and timestamps. Reflection outcomes were coded using a 3x3 transition matrix to measure shifts from pre to post-game understanding.

## Key Results
- 73.8% of participants achieved calibrated understanding of model behaviors
- Players developed strategies to mitigate model defaults through controlled prompting
- Group composition and repeated exposure influenced reflection quality
- Players recognized systematic biases in GenAI and differences between human and AI interpretation

## Why This Works (Mechanism)

### Mechanism 1: Constrained Prompting for Hypothesis Testing
- **Claim:** Constraining prompt length forces players to externalize mental models of GenAI defaults
- **Mechanism:** Brevity reward creates tradeoffs between specification and omission, embedding assumptions about model defaults
- **Core assumption:** Players will actively reason about what to omit rather than guessing randomly
- **Break condition:** If prompts are too short (<3 tokens), players may default to random guessing

### Mechanism 2: Multi-Level Structured Contrast
- **Claim:** Comparing outputs at multiple levels surfaces misalignments single-player use would miss
- **Mechanism:** Three simultaneous comparison axes create cognitive dissonance when expectations fail
- **Core assumption:** Players will actively engage in comparison rather than passively accepting results
- **Break condition:** If groups are homogeneous, peers may reinforce rather than challenge assumptions

### Mechanism 3: Repeated Exposure with Category Variation
- **Claim:** Exposing players to multiple categories across rounds enables pattern recognition
- **Mechanism:** Six rounds spanning different categories help players detect recurring default behaviors
- **Core assumption:** Players will abstract patterns across rounds rather than treating each independently
- **Break condition:** If players don't recognize category connections, they may fail to generalize

## Foundational Learning

- **Concept:** Under-specification vs. over-specification tradeoffs
  - **Why needed here:** Players must understand omitting details invites model defaults while over-specifying risks unintended interpretations
  - **Quick check question:** Can you predict what visual elements the model will "fill in" if you only prompt "a CEO"?

- **Concept:** Human-AI perception gaps
  - **Why needed here:** The game reveals humans and models prioritize different prompt aspects
  - **Quick check question:** When you see an AI-generated image that differs from your intent, how do you determine whether the fault lies in your prompt or the model's interpretation?

- **Concept:** Probabilistic output behavior
  - **Why needed here:** Understanding identical prompts can yield different outputs is essential for interpreting inconsistency
  - **Quick check question:** If you submit the exact same prompt twice and get different results, what model characteristic does this demonstrate?

## Architecture Onboarding

- **Component map:** Frontend (React/Vite) -> WebSocket -> FastAPI Backend -> OpenAI gpt-image-1 API -> Game State Manager (Python dataclasses) -> Structured Logging

- **Critical path:** Player submits prompt -> token counting (300ms debouncing) -> gpt-image-1 API (~30s) -> Async processing -> Results broadcast (<1s) -> Vote aggregation -> Reveal phase with token overlap highlighting

- **Design tradeoffs:** Chose gpt-image-1 (no temperature/seed control) to match real-world API constraints but limits experimental rigor; Breach penalty creates incentive for brevity but may encourage overly risky minimal prompts

- **Failure signatures:** High "Uncorrected" outcomes in realism/co-occurrence categories suggest insufficient scaffolding; Homogeneous groups showing fewer bias discussions indicate need for diversity-aware matching

- **First 3 experiments:**
  1. A/B test prompt category frequency: Run sessions where bias categories appear 3x each vs. once each
  2. Group composition manipulation: Systematically vary group diversity and measure reflection outcome distributions
  3. Longitudinal transfer test: Survey participants 2 weeks post-gameplay to assess whether prompting strategies persist

## Open Questions the Paper Calls Out

1. **Long-term behavior change:** Does participation lead to sustained changes in prompting strategies and GenAI beliefs outside game context? The current study only captured immediate shifts, not whether transformative reflections persist over time.

2. **Group composition effects:** How does group composition (demographic diversity and expertise mix) causally influence reflection depth? The study relied on ad-hoc friend groups and strangers, resulting in sample size too small to draw statistical conclusions.

3. **Adaptive game mechanics:** Can dynamic, adaptive game mechanics improve coverage of the "unlimited AI hypothesis space" compared to static prompt pools? The current randomized static pool may fail to address specific player misconceptions.

## Limitations

- **Temporal model drift:** The study's core mechanism depends on specific model failure modes that may not persist with API updates
- **Social learning dependency:** Reflection outcomes heavily depend on group composition and interaction quality, but mechanisms for ensuring productive discourse aren't fully specified
- **Limited generalization evidence:** While the game builds calibrated understanding, there's limited evidence about whether this translates to improved real-world GenAI usage

## Confidence

- **High Confidence:** The game successfully engages players in reflective discourse about model behaviors (73.8% calibration rate)
- **Medium Confidence:** The three proposed mechanisms effectively explain observed outcomes, though systematic variation testing is lacking
- **Low Confidence:** The intervention's effectiveness will persist with future model versions or transfer to other GenAI literacy contexts

## Next Checks

1. **Model Version Replication Test:** Run the exact same game protocol with current OpenAI image generation API to quantify how many original biases remain detectable.

2. **Longitudinal Transfer Assessment:** Conduct follow-up surveys 2-4 weeks after gameplay to assess whether participants' prompting strategies and reflective awareness persist.

3. **Mechanistic Isolation Experiment:** Design A/B conditions that isolate each proposed mechanism to quantify the relative contribution of social learning versus prompting constraints.