---
ver: rpa2
title: 'MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic
  Networks'
arxiv_id: '2509.06303'
source_url: https://arxiv.org/abs/2509.06303
tags:
- above
- lemma
- where
- page
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new inference framework, MOSAIC, for change-point
  detection in dynamic networks with low-rank and sparse-change structures. The authors
  establish the minimax-optimal detection boundary, which depends on the sparsity
  of changes, and develop a theoretical test using eigen-decomposition of mean matrices
  that approaches this minimax rate with only a minor logarithmic loss.
---

# MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks

## Quick Facts
- **arXiv ID:** 2509.06303
- **Source URL:** https://arxiv.org/abs/2509.06303
- **Reference count:** 40
- **Primary result:** Minimax-optimal test for change-point detection in dynamic networks with low-rank and sparse-change structures

## Executive Summary
This paper proposes MOSAIC, a new inference framework for detecting change points in dynamic networks. The method exploits low-rank structures in network adjacency matrices and adapts to the sparsity of changes through edge screening. The authors establish a minimax-optimal detection boundary that depends on the sparsity of changes and develop a theoretical test using eigen-decomposition of mean matrices. A novel residual-based technique constructs a pivotal test statistic that converges to a standard normal distribution under the null hypothesis and achieves full power under the alternative hypothesis.

## Method Summary
MOSAIC combines three key innovations: (1) sparsity-adaptive edge screening that identifies strong signal edges before aggregation, (2) low-rank denoising via eigen-decomposition that reduces variance from O(1) to O(1/n), and (3) residual-based pivotal normalization that yields a test statistic converging to standard normal under the null. The method splits data to ensure independence, applies spectral smoothing to estimate the low-rank structure, screens edges based on thresholded CUSUM statistics, and aggregates residuals over the screened set to form the final test statistic.

## Key Results
- Establishes minimax-optimal detection boundary for change-point detection in dynamic networks
- Achieves detection rate within logarithmic factor of theoretical lower bound
- Demonstrates empirical effectiveness through simulations and real data application
- Proves pivotal statistic converges to standard normal under null hypothesis

## Why This Works (Mechanism)

### Mechanism 1: Sparsity-Adaptive Edge Screening
The method applies a hard threshold to a CUSUM-type statistic to identify a "strong signal" edge set S, aggregating the final statistic only over this screened subset. This reduces noise accumulation from unchanged edges and adapts to unknown change sparsity level s*. The screening works when change sparsity is small relative to network size and signal strength exceeds the threshold. Break condition: if change is dense (s* ≍ n), screening may discard relevant signals.

### Mechanism 2: Low-Rank Denoising via Eigen-Decomposition
Instead of using raw adjacency matrix averages, the method uses a smoothed version derived from top K eigenvectors. Projecting onto the low-rank subspace filters out noise orthogonal to this subspace before constructing the CUSUM statistic Z(τ). This reduces variance from O(1) to O(1/n). Break condition: if network is full-rank or rank K is severely misspecified, projection fails to denoise.

### Mechanism 3: Residual-Based Pivotal Normalization
The empirical test constructs residuals by subtracting the eigen-smoothed estimator from one time segment using the estimator from the other segment. Under the null hypothesis, these residuals are mean-zero with simplified dependency structure, satisfying conditions for the Martingale Central Limit Theorem. This yields a test statistic that converges to standard normal distribution. Break condition: if time series exhibits strong temporal dependence, the martingale CLT argument fails.

## Foundational Learning

- **Concept: Minimax-Optimal Detection Boundary**
  - Why needed: Defines fundamental limit of detectability as function of sparsity, establishing theoretical baseline for "optimal" performance
  - Quick check: If change sparsity s* increases, does minimum required signal strength ε_n² increase or decrease? (Answer: Generally increases, making detection harder)

- **Concept: CUSUM (Cumulative Sum) Statistic**
  - Why needed: Fundamental building block for detecting mean shifts, adapted to matrix setting with eigen-smoothing
  - Quick check: How does CUSUM statistic behave if mean is constant versus sudden shift? (Answer: Constant mean leads to bounded variance; shift causes statistic to drift/diverge)

- **Concept: Eigen-Structure of Random Matrices (Spiked Model)**
  - Why needed: Method relies on distinguishing "signal" (spiked eigenvalues) from "noise" (bulk eigenvalues) for effective denoising
  - Quick check: Why does method require "separation" condition between eigenvalues? (Answer: To ensure eigenvectors can be stably estimated and distinguished from noise floor)

## Architecture Onboarding

- **Component map:** Input → Data Splitter → Eigen-Smoother → Residual Generator → Screening Module → Aggregator → Decision Engine
- **Critical path:** Input → Eigen-Smoother → Residual Generator → Screening Module → Aggregator → Decision. Estimation of ρ_n (sparsity) and screening threshold are tightly coupled steps.
- **Design tradeoffs:** Data splitting sacrifices sample size for robustness; rank selection K requires balancing smoothing against signal preservation
- **Failure signatures:** Conservative detection if screening threshold too high; size distortion if network not low-rank or temporal dependencies exist
- **First 3 experiments:**
  1. Null Validation: Run simulations under H₀ to verify empirical size matches nominal level α
  2. Sparsity Power Analysis: Vary change sparsity s* to confirm phase transition matches theoretical boundary
  3. Rank Robustness: Misspecify rank K (e.g., K=2 when K*=3) to verify claimed robustness

## Open Questions the Paper Calls Out
1. **Temporal Dependence Extension:** Can framework extend to dynamic network models with time-series dependency over time horizon? (Paper states: "would be interesting to extend this work to more general dynamic network model settings with time series dependency")
2. **Intermediate Regime Boundary:** What is minimax-optimal detection boundary for intermediate change-sparsity regime where √c_ξ n ≤ s* ≤ √n? (Paper notes: "we do not claim that the result in this regime is optimal")
3. **Causal Inference Integration:** How can framework adapt to incorporate network causal inference perspectives? (Paper states: "beyond the scope of current paper" and "interesting topic for future research")

## Limitations
- Critical dependence on sparsity assumption (s* << n) with limited validation for larger networks
- Requires temporal independence of adjacency matrices, which may be violated in real-world networks
- Empirical validation limited to moderate network sizes (n ≤ 300) and controlled simulation settings

## Confidence
- **High confidence:** Theoretical minimax rate derivation and asymptotic normality under null hypothesis
- **Medium confidence:** Power guarantee under alternative hypothesis and phase transition behavior
- **Medium confidence:** Empirical size control and power comparisons with baselines

## Next Checks
1. Test method on simulated data with temporally correlated adjacency matrices to assess sensitivity to independence assumption
2. Evaluate performance when change sparsity s* is not small relative to n (e.g., s* > n/2) to verify fallback behavior
3. Conduct systematic experiments varying K from severely underspecified to grossly overspecified across different signal-to-noise ratios