---
ver: rpa2
title: 'Large Language Models'' Varying Accuracy in Recognizing Risk-Promoting and
  Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination
  and Heated Tobacco Products'
arxiv_id: '2507.04364'
source_url: https://arxiv.org/abs/2507.04364
tags:
- health
- accuracy
- sentiments
- public
- messages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the accuracy of three large language models
  (GPT, Gemini, and LLAMA) in detecting risk-promoting versus health-supporting sentiments
  in public health discourse on Facebook and Twitter. Using 1,600 human-annotated
  social media messages about HPV vaccination and heated tobacco products, the models
  were tested for their ability to classify sentiment with high accuracy.
---

# Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products

## Quick Facts
- **arXiv ID:** 2507.04364
- **Source URL:** https://arxiv.org/abs/2507.04364
- **Reference count:** 14
- **Primary result:** LLAMA consistently outperformed GPT and Gemini in classifying health-related sentiment on social media, achieving accuracy as high as 98% for certain tasks

## Executive Summary
This study evaluated three large language models (GPT-4 Turbo, Gemini-1.0 Pro, and LLAMA-3) for their ability to detect risk-promoting versus health-supporting sentiments in public health discourse on Facebook and Twitter. Using 1,600 human-annotated social media messages about HPV vaccination and heated tobacco products, the models were tested across multiple sentiment categories. LLAMA demonstrated superior performance overall, while all models showed systematic variations in accuracy by platform, health topic, and sentiment type. The study highlights both the potential and limitations of using LLMs for public health sentiment analysis.

## Method Summary
The study collected 1,600 human-annotated social media messages (400 per topic Ã— platform) from Facebook and Twitter, balanced between risk-promoting and health-supporting sentiments. Three LLMs were evaluated using zero-shot prompting with majority voting from 20 response instances per message. Accuracy was calculated through 1,000 Monte Carlo simulations sampling 3 responses per message. The evaluation focused on five sentiment categories and analyzed performance variations across platforms, health topics, and sentiment types.

## Key Results
- LLAMA consistently achieved higher accuracy than GPT and Gemini, reaching up to 98% for certain classification tasks
- Models were more accurate at detecting risk-promoting sentiment on Facebook (longer messages) versus health-supporting sentiment on Twitter (shorter messages)
- Neutral sentiment detection remained challenging across all models, with most misclassifications involving confusion between polarized sentiments and neutral categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can classify health-related sentiment with high accuracy when provided with sufficient context
- Mechanism: The transformer architecture allows LLMs to weigh the importance of different words in a sequence (self-attention), enabling them to associate specific linguistic patterns and terminology with sentiment categories defined in a prompt. Longer texts (Facebook) provide more context tokens, reducing ambiguity and allowing the model to form a more confident probability distribution over the sentiment labels
- Core assumption: The model's pre-training data contains similar linguistic patterns and discourse structures to the target public health domain
- Evidence anchors:
  - [abstract] "...LLMs... demonstrate substantial accuracy in classifying risk-promoting and health-supporting sentiments..."
  - [section] "Facebook messages, which allow for longer-form writing, yielded higher accuracy for detecting risk-promoting sentiments, compared with short-form Twitter content. Longer messages may contain additional context and semantic cues, facilitating more reliable classification."
  - [corpus] *Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese Micro-bloggers* (uses LLMs for sentiment on social media, related task)
- Break condition: Accuracy will degrade for novel health topics with unique jargon not well-represented in the pre-training data, or for short, ambiguous messages lacking sufficient context

### Mechanism 2
- Claim: Differences in model performance are driven by variations in training data composition and model architecture
- Mechanism: Each model (GPT, Gemini, LLAMA) has a unique architecture and was trained on a distinct dataset. LLAMA's superior performance in this study suggests its training data may have a higher representation or better weighting of the specific public health discourse found on the tested platforms, or its architecture is more efficient at extracting the relevant sentiment signals from the provided prompts
- Core assumption: LLAMA's superior performance is not solely due to chance or hyperparameter settings but reflects a fundamental suitability for this specific task derived from its design and data
- Evidence anchors:
  - [section] "...LLAMA consistently demonstrated higher accuracy for both risk-promoting and health-supporting sentiments compared with GPT and Gemini." and "Each model utilizes a unique architecture and was trained on distinct datasets."
  - [abstract] "...notable discrepancies emerge by platform, health issue, and model type."
  - [corpus] Evidence is weak/missing in provided corpus for a direct comparison of LLAMA vs GPT vs Gemini on this specific task
- Break condition: This specific ranking of models (LLAMA > GPT > Gemini) should not be generalized to all tasks or languages without empirical validation. A different domain (e.g., clinical notes vs. social media) could yield a different performance ranking

### Mechanism 3
- Claim: Accuracy is systematically lower for neutral or ambiguous sentiment due to a lack of discriminative linguistic features
- Mechanism: Sentiment classification relies on the presence of polarity cues (words/phrases with positive or negative sentiment). "Neutral" sentiment is often the absence of strong polarity or a balance of conflicting cues. LLMs, trained to predict the next token, struggle more with this absence or balance because the probabilistic path to a "neutral" classification is less distinct than to a polarized one
- Core assumption: The "neutral" category includes messages that are not merely factual but may be nuanced or ambivalent, making them harder to categorize
- Evidence anchors:
  - [section] "Additional analysis on neutral sentiment revealed challenges for LLM-based classification... Overall, the accuracy of detecting neutral sentiment was consistently lower..." and "Most misclassifications occurred when human-classified risk-promoting... or health-supporting messages were misclassified as neutral by the language model..."
  - [abstract] "...challenges LLMs face in reliably detecting neutral messages."
  - [corpus] Corpus evidence for the neutral sentiment mechanism specifically is weak/missing in the provided neighbors
- Break condition: Providing explicit definitions and examples of neutral sentiment in the prompt or using a model fine-tuned on a dataset rich in neutral examples could mitigate this

## Foundational Learning

- Concept: **Transformer Self-Attention and Context Window**
  - Why needed here: Understanding that LLMs classify sentiment by processing the entire input sequence and weighing the relationship between words. The difference in accuracy between short (Twitter) and long (Facebook) posts is directly tied to how much context fits in the window and how attention mechanisms function
  - Quick check question: How does a larger context window in Facebook posts theoretically improve classification accuracy compared to tweets?

- Concept: **Probabilistic Text Generation**
  -