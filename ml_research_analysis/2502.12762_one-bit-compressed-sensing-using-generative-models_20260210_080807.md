---
ver: rpa2
title: One-bit Compressed Sensing using Generative Models
arxiv_id: '2502.12762'
source_url: https://arxiv.org/abs/2502.12762
tags:
- algorithm
- signal
- generative
- performance
- measurements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reconstructing sparse signals
  from one-bit compressed measurements using deep generative models. The proposed
  approach leverages a pre-trained generative model to map from a low-dimensional
  latent space to the space of sparse vectors, enabling reconstruction by searching
  over the generator's range.
---

# One-bit Compressed Sensing using Generative Models

## Quick Facts
- **arXiv ID:** 2502.12762
- **Source URL:** https://arxiv.org/abs/2502.12762
- **Authors:** Swatantra Kafle; Geethu Joseph; Pramod K. Varshney
- **Reference count:** 40
- **Primary result:** Achieves an order of magnitude improvement in mean squared error over traditional one-bit compressed sensing methods (BIHT, YP) on MNIST and Fashion-MNIST datasets.

## Executive Summary
This paper proposes a novel approach to one-bit compressed sensing that leverages pre-trained deep generative models to reconstruct sparse signals from sign measurements. The method formulates reconstruction as an optimization problem over the generator's latent space, enabling recovery of both signal amplitude and direction - a capability traditionally impossible in one-bit CS. Theoretical guarantees on reconstruction accuracy and sample complexity are provided, showing the algorithm output is close to the projection of the true sparse vector onto the generator's range.

## Method Summary
The algorithm trains a variational autoencoder (VAE) on sparse signal data, then freezes the decoder (generator G) to serve as a structured prior. Given one-bit measurements y = sign(Ax*), it searches over the latent space z to find G(z) that minimizes an objective balancing the ℓ2 norm of the generated signal and its correlation with the measurements. This optimization is performed via gradient descent with multiple random restarts. The approach reduces the search space from high-dimensional signal space to low-dimensional latent space, while the learned generative model provides structural constraints beyond simple sparsity.

## Key Results
- Achieves an order of magnitude improvement in MSE compared to BIHT and YP on MNIST and Fashion-MNIST datasets
- Successfully recovers both amplitude and direction of signals, unlike traditional one-bit CS methods
- Demonstrates robustness to noise in measurements and measurement matrix uncertainties
- Shows sample complexity scaling with latent dimension rather than signal dimension, requiring fewer measurements than traditional algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A trained generative model provides a structured prior that captures signal information beyond simple sparsity, enabling more accurate reconstruction from fewer measurements.
- **Mechanism:** The VAE learns to map a low-dimensional latent space (Rs, s≪n) to the manifold of structured sparse vectors. By constraining reconstruction to the generator's range S, the algorithm implicitly enforces structural constraints that would otherwise require explicit, often intractable, regularization terms.
- **Core assumption:** The training distribution adequately covers the test signal distribution; representation error min_z‖G(z) - x*‖ is small.
- **Evidence anchors:**
  - [abstract] "the generative model can learn additional structural information about the signal beyond sparsity"
  - [Section II] "the generator model can learn any additional structure in the desired set of sparse vectors that arise due to the underlying physics or system properties"
  - [corpus] Related work on adaptive sampling with VAE (arXiv:2512.03525) similarly leverages learned structure for improved sampling, suggesting the mechanism generalizes.
- **Break condition:** Distribution shift between training and testing (e.g., MNIST-trained model tested on Omniglot) causes representation error to dominate; Fig. 6 shows NMSE stagnates above BIHT baseline when m > 300.

### Mechanism 2
- **Claim:** Optimizing in the low-dimensional latent space rather than the high-dimensional signal space reduces both sample complexity and computational burden.
- **Mechanism:** The loss function l_loss(G(z)) = ‖G(z)‖² - √(2π)/m · y^T A G(z) is minimized over z ∈ Rs with s ≪ n. This reduces the effective search space from Rn to Rs, improving resolvability per measurement and simplifying the non-convex optimization landscape.
- **Core assumption:** Gradient descent finds a good approximate solution (small δ) despite non-convexity; the paper empirically validates this with random restarts.
- **Evidence anchors:**
  - [Section II] "the objective function in (4) is minimized with respect to the latent variable z ∈ Rs rather than the unknown vector x ∈ Rn. This results in a smaller search space, as s ≪ n"
  - [Theorem 1] Measurement bound m ≥ Cε⁻²s(r² + d log LNw_max) depends on latent dimension s, not signal dimension n directly
  - [corpus] Compressed BC-LISTA (arXiv:2601.23148) similarly exploits low-rank structure for efficient sparse recovery, consistent with dimensionality reduction as a general mechanism.
- **Break condition:** If the generator architecture is underparameterized (cannot represent the signal manifold), the representation error term in (6) dominates regardless of optimization quality.

### Mechanism 3
- **Claim:** The objective function jointly recovers both amplitude and direction by balancing norm regularization against measurement consistency.
- **Mechanism:** The first term ‖G(z)‖² prevents unbounded norm growth; the second term -√(2π)/m y^T A G(z) maximizes correlation between predicted and observed signs. Together, they enable recovery of signal magnitude—traditionally impossible from one-bit measurements alone—because the generative prior constrains plausible amplitudes.
- **Core assumption:** The generator's range includes signals with appropriate norm characteristics; measurement matrix A satisfies restricted isometry-like properties (Gaussian assumption in Theorem 1).
- **Evidence anchors:**
  - [Section II] "the two terms of the objective function jointly optimize the representation error"
  - [Section IV-A] "generative models are capable of learning the distribution of compressed signals. When well-trained, they recover both the magnitude and direction of the compressed signal"
  - [corpus] No direct corpus evidence for this specific amplitude recovery mechanism in one-bit CS; related work on denoising guarantees (arXiv:2504.01046) addresses measurement design but not amplitude-direction coupling.
- **Break condition:** Under heavy sign-flip noise (α approaching 0.5), Corollary 1 shows error diverges; the bound requires α > 2/3 for meaningful guarantees.

## Foundational Learning

- **Concept: Variational Autoencoder (VAE) architecture**
  - **Why needed here:** The generator G is implemented as a VAE decoder; understanding encoder-decoder coupling, latent space regularization, and reconstruction loss is essential for diagnosing training quality.
  - **Quick check question:** Can you explain why the VAE's KL divergence term helps ensure the latent space is well-covered during training?

- **Concept: Lipschitz continuity and its role in generalization bounds**
  - **Why needed here:** Theorem 1's measurement bound scales as (LN w_max)^d; understanding how weight bounds and activation choice affect the Lipschitz constant directly informs architecture decisions.
  - **Quick check question:** Given a ReLU network (1-Lipschitz activation) with weight matrices bounded by w_max, what is the Lipschitz constant of a 4-layer network?

- **Concept: One-bit compressed sensing fundamentals**
  - **Why needed here:** The baseline comparison requires understanding why traditional methods (BIHT, YP) only recover direction, and how the sign measurement model y = sign(Ax*) differs from linear measurements.
  - **Quick check question:** Why does one-bit CS with zero reference level lose all amplitude information without additional constraints?

## Architecture Onboarding

- **Component map:**
  - VAE Encoder: 784 → 500 → 500 → 40 (maps images to latent z ∈ R^40)
  - VAE Decoder (Generator G): 40 → 500 → 500 → 784 (maps latent to reconstruction)
  - Measurement simulator: A ∈ R^(m×784) with columns uniformly sampled from unit hypersphere
  - Optimization loop: Gradient descent on l_loss(G(z)) with respect to z (not G's weights)

- **Critical path:**
  1. Train VAE on dataset (200 epochs, Adam, lr=0.001) → freeze generator weights
  2. Generate one-bit measurements y = sign(Ax*) for test images
  3. Initialize z randomly, run gradient descent (10 restarts × 100 steps), select lowest-loss solution
  4. Compute x̂ = G(ẑ) and evaluate MSE/NMSE against x*

- **Design tradeoffs:**
  - **Latent dimension s:** Larger s reduces representation error but increases measurement requirements (m ∝ s) and optimization difficulty
  - **Network depth d:** Deeper networks can represent more complex manifolds but increase Lipschitz constant, raising measurement bound
  - **Restart count vs. step budget:** Paper uses 10 restarts × 100 steps; fewer restarts risk local minima, more steps per restart may not escape bad basins

- **Failure signatures:**
  - **High MSE with low NMSE:** Generator captures direction but systematically misestimates amplitude → check norm distribution of training data
  - **NMSE plateaus as m increases:** Representation error dominates → generator architecture insufficient for data complexity (cf. Fashion-MNIST vs. MNIST in Fig. 5)
  - **Performance collapse under noise:** Sign-flip rate > 1/3 violates Corollary 1 assumptions → incorporate noise parameter α into modified loss (11)

- **First 3 experiments:**
  1. **Baseline reproduction on MNIST:** Train VAE with 40-500-500-784 architecture, evaluate MSE/NMSE vs. m (200–2000 measurements), verify order-of-magnitude improvement over BIHT
  2. **Noise sensitivity sweep:** Fix m = 784, vary sign-flip probability (1–α) from 0.05 to 0.35, compare against Corollary 1's predicted error inflation
  3. **Architecture ablation:** Reduce latent dimension to s = 20 and increase to s = 80; measure when representation error begins to dominate (look for NMSE plateau onset)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical guarantees for reconstruction accuracy and sample complexity be extended to a broader class of measurement matrices beyond Gaussian random matrices?
- **Basis in paper:** [explicit] The Conclusion states: "Note that all the theoretical results presented in this work are applicable only to Gaussian measurement matrices. Extending these results to a broader class of measurement matrices is a direction for future work."
- **Why unresolved:** The proof techniques in the paper rely on specific properties of Gaussian matrices, such as the distribution of entries $A_{i,j} \sim \mathcal{N}(0, 1/m)$ used in Lemma 1 and Lemma 2 to derive the concentration inequalities and Gaussian mean width bounds.
- **What evidence would resolve it:** A theoretical proof showing that the reconstruction error bounds and measurement complexity hold for other matrix ensembles (e.g., sub-Gaussian, partial Fourier, or deterministic matrices) while maintaining similar probability bounds.

### Open Question 2
- **Question:** Can rigorous convergence guarantees be established for the gradient descent optimization process to ensure it finds the approximate solution assumed in the theoretical analysis?
- **Basis in paper:** [inferred] The paper assumes in Theorem 1 that $\tilde{z}$ minimizes the cost function to within an additive $\delta$ of the optimum. However, the text acknowledges that the "objective function to be optimized is non-convex," and while gradient descent is used, the authors only "corroborate this assumption empirically."
- **Why unresolved:** Non-convex landscapes can contain spurious local minima or saddle points that might trap gradient descent, meaning there is no theoretical certainty that the algorithm will achieve the small $\delta$ required for the error bounds to apply in practice.
- **What evidence would resolve it:** A formal analysis of the loss landscape proving that gradient descent converges to a global minimum (or a local minimum within the specified error $\delta$) with high probability, or a counter-example showing where it fails.

### Open Question 3
- **Question:** How can the algorithm be adapted to maintain performance when the test signal distribution differs significantly from the training distribution, without requiring full model retraining?
- **Basis in paper:** [inferred] In Section IV.C (Limitations), the authors note that when testing on Omniglot with a model trained on MNIST, the performance degrades due to high representation error. They conclude that "any shift in the signal distribution over time can result in degradation of reconstruction performance" and suggest retraining.
- **Why unresolved:** Retraining generative models is computationally expensive and assumes access to the new data distribution in advance. The paper does not explore domain adaptation or robustness techniques that could bridge this gap in real-time or with limited data.
- **What evidence would resolve it:** Modifications to the training objective or optimization process (e.g., domain-invariant learning) that demonstrate stable NMSE performance when the generative prior is mismatched to the test data.

## Limitations

- **Distribution Shift Vulnerability:** Performance degrades significantly when test data distribution differs from training distribution, requiring full model retraining rather than adaptation techniques.
- **Conservative Theoretical Bounds:** Measurement complexity guarantees use worst-case assumptions about network weights and Lipschitz constants that may overestimate requirements for practical implementations.
- **Limited Noise Analysis:** Theoretical and empirical robustness analysis focuses only on sign-flip noise, with no consideration of additive measurement noise or quantization effects.

## Confidence

- **High Confidence:** The mechanism of reducing optimization dimensionality by searching over the generator's range (Mechanism 2) is well-supported by both theory and empirical results. The experimental methodology and baseline comparisons are clearly described and reproducible.
- **Medium Confidence:** The claim that the algorithm recovers both amplitude and direction (Mechanism 3) is supported by experiments but relies on assumptions about the generator's training that are not fully validated. The theoretical guarantees, while rigorous, use conservative bounds that may overestimate measurement requirements.
- **Low Confidence:** The robustness claims under measurement matrix uncertainties lack quantitative validation, and the paper does not explore scenarios where the generator's learned structure might actually hinder recovery.

## Next Checks

1. **Distribution Shift Analysis:** Evaluate algorithm performance when training and test distributions differ (e.g., train on MNIST, test on Fashion-MNIST or Omniglot) to quantify the impact of representation error from distribution mismatch.

2. **Architecture Sensitivity:** Systematically vary VAE depth, latent dimension, and training hyperparameters to identify when representation error dominates the reconstruction error and how this scales with signal complexity.

3. **Noise Model Expansion:** Extend the noise analysis beyond sign-flip to include additive Gaussian noise and quantization error, measuring performance degradation and comparing against modified versions of BIHT that incorporate generative priors.