---
ver: rpa2
title: 'WhAM: Towards A Translative Model of Sperm Whale Vocalization'
arxiv_id: '2512.02206'
source_url: https://arxiv.org/abs/2512.02206
tags:
- codas
- audio
- whale
- sperm
- wham
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WhAM (Whale Acoustics Model), the first transformer-based
  model for generating synthetic sperm whale codas from audio prompts. The model is
  built by fine-tuning VampNet, a masked acoustic token model pretrained on musical
  audio, using 10,000 coda recordings collected over two decades.
---

# WhAM: Towards A Translative Model of Sperm Whale Vocalization

## Quick Facts
- arXiv ID: 2512.02206
- Source URL: https://arxiv.org/abs/2512.02206
- Reference count: 40
- WhAM is the first transformer-based model for generating synthetic sperm whale codas from audio prompts using iterative masked token prediction

## Executive Summary
WhAM (Whale Acoustics Model) introduces a transformer-based approach for generating synthetic sperm whale codas from audio prompts. The model fine-tunes VampNet, a masked acoustic token model pretrained on musical audio, using 10,000 coda recordings collected over two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features while demonstrating capabilities in acoustic translation, pseudocoda generation, and emergent classification performance.

The model achieves strong performance on rhythm type, social unit, and vowel classification tasks despite being trained for generation rather than classification. Expert marine biologists evaluated the synthetic codas through perceptual studies, finding that while the model successfully captures rhythmic patterns, some click-level features and background noise characteristics differ from natural codas. The approach requires only five days of training on a single GPU and shows promising results with relatively small datasets.

## Method Summary
WhAM builds on VampNet, a masked acoustic token model, by fine-tuning it on sperm whale coda recordings. The model uses the Descript Audio Codec (DAC) to tokenize 16kHz audio into discrete token stacks, then applies a bidirectional transformer with LoRA adapters to predict masked tokens iteratively. Training occurs in two phases: first adapting to general bioacoustic domains, then specializing on sperm whale codas. The iterative parallel decoding gradually unmasks tokens over multiple forward passes, allowing generation of contextually appropriate coda-like audio from any input prompt.

## Key Results
- First transformer-based model for generating synthetic sperm whale codas from audio prompts
- Strong performance on rhythm type (87.4%), social unit (85.2%), and vowel classification (85.2%) despite generative training objective
- FAD score of 0.21 between natural coda distributions, establishing threshold for "indistinguishable" quality
- Requires only 5 days of training on a single GPU with 10,000 coda recordings
- Successfully performs acoustic translation of various audio inputs into sperm whale vocalization style

## Why This Works (Mechanism)

### Mechanism 1: Iterative Masked Token Prediction for Acoustic Translation
Masked acoustic token modeling enables both coda generation and cross-domain acoustic translation. The Descript Audio Codec (DAC) tokenizes 16kHz audio into discrete token stacks; a bidirectional transformer learns to predict masked tokens from context; iterative parallel decoding gradually unmasks tokens over multiple forward passes (50-107 steps depending on input type), allowing the model to generate contextually appropriate coda-like audio from any prompt. Core assumption: DAC, trained primarily on speech and music, adequately represents sperm whale click acoustics despite known reconstruction weaknesses at 1-6kHz and 8-10kHz bands.

### Mechanism 2: Two-Phase Domain Adaptation for Bioacoustic Transfer
Sequential domain adaptation → species-specific finetuning enables effective transfer from music-pretrained models to whale codas. Phase 1 finetunes VampNet on general animal vocalizations (FSD, AudioSet, WMMS, BirdSet; ~122 hours) to establish bioacoustic representations; Phase 2 specializes on sperm whale codas (DSWP+CETI; ~6 hours); LoRA preserves pretrained knowledge while learning species-specific patterns. Core assumption: Animal vocalization datasets capture sufficient bioacoustic diversity to bridge music→whale gap without catastrophic forgetting.

### Mechanism 3: Emergent Supervision from Generative Objective
Training for generation creates embeddings useful for downstream classification without explicit supervision. To generate realistic codas, the model must learn acoustic features (rhythm, spectral properties, individual variation); these latent representations encode task-relevant structure that transfers to classifiers via simple linear probes. Core assumption: Features necessary for realistic generation overlap with features necessary for classification tasks.

## Foundational Learning

- **Masked Language/Acoustic Modeling (BERT-style pretraining)**: WhAM's MATM is a bidirectional transformer trained on the cloze task for acoustic tokens; understanding how masked prediction forces context learning is essential. Quick check: Why does bidirectional attention allow better context representation than unidirectional (GPT-style) for this task?

- **Residual Vector Quantization (RVQ) for Audio Codecs**: DAC tokenizes audio as hierarchical token stacks (14 codebook levels × 120 timesteps); each level refines the reconstruction. Quick check: How does RVQ achieve higher fidelity than single-codebook quantization with the same effective vocabulary size?

- **Fréchet Audio Distance (FAD) for Distribution Similarity**: FAD measures acoustic similarity between generated and natural coda distributions; FAD < 0.21 indicates "indistinguishable" quality. Quick check: Why is FAD between two disjoint sets of natural codas non-zero, and how does this establish a practical threshold?

## Architecture Onboarding

- **Component map**: Audio → downsample (16kHz) → normalize → tokenize (DAC) → apply masking → MATM forward passes × N steps → iterative token unmasking → detokenize → synthetic coda output → mean-pool embeddings → train linear probe

- **Critical path**: 1) Input audio → downsample to 16kHz, normalize → tokenize → apply masking (periodic/onset strategy per Table 4) 2) MATM forward passes × N steps (50-107 iterations) → iterative token unmasking 3) Detokenize → synthetic coda output 4) For downstream tasks: mean-pool MATM embeddings → train linear probe

- **Design tradeoffs**: Fixed vs. finetuned codec (paper keeps DAC frozen for efficiency; Section 5 notes this may limit representation of vowel-like patterns at 3.7-5.7kHz); LoRA vs. full finetuning (LoRA preserves music-pretrained knowledge but may limit depth of adaptation to whale acoustics); Masking strategy (periodic masking preserves rhythm; onset masking preserves click timing; tradeoff between faithful variation and creative generation)

- **Failure signatures**: Unrealistic amplitude variation between sequential clicks within a coda; "Wavy" DC offset oscillation vs. stable natural recording offset; Missing or exaggerated multipulsed structure within individual clicks (IPI ~3-4ms in natural codas); Overly uniform broadband spectral content (natural clicks trail off at high frequencies); Echolocation-like consistent ICIs (~0.4s) instead of coda-like stereotyped rhythmic patterns

- **First 3 experiments**: 1) Tokenizer reconstruction baseline: Pass natural codas through tokenizer→detokenizer (no MATM), measure per-frequency MSE (replicate Section D.4) to identify codec limitations before model work 2) Domain adaptation ablation: Train species-specific only (skip Phase 1), measure FAD reduction across marine mammal inputs to quantify domain adaptation contribution (reference Section D.3 patterns) 3) Masking parameter sweep: Vary periodic mask width (12-18) and onset mask width (1-21) on held-out codas; measure FAD and conduct expert 2AFC to find generation quality sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
Would finetuning the acoustic tokenizer or developing specialized bioacoustic codecs improve the model's ability to capture nuanced features like vowel-like spectral patterns? Basis in paper: Section 5 notes the current fixed codec (trained on music) may limit representation of specific sperm whale features and suggests finetuning the entire codec as future work. Why unresolved: The current implementation freezes the Descript Audio Codec (DAC) to save computation, potentially missing nuances in the 3.7-5.7kHz band. What evidence would resolve it: A comparison of reconstruction loss and downstream vowel classification accuracy between the current model and versions with adapted tokenizers.

### Open Question 2
How can the "semantic gap" between acoustic translation and semantic understanding of communication be bridged using the WhAM framework? Basis in paper: Section 5 states that while WhAM performs acoustic translation, "future work should explore ways to bridge this semantic gap." Why unresolved: WhAM currently operates purely at the signal level for style transfer and does not model the meaning or functional context of the vocalizations. What evidence would resolve it: Demonstrating that generated codas elicit appropriate behavioral responses in sperm whales during playback experiments, indicating preserved semantic content.

### Open Question 3
Can unsupervised exploration of WhAM's latent space reveal novel, biologically relevant coda features not currently defined by experts? Basis in paper: Section 5 suggests "future work could explore unsupervised learning approaches to uncover new coda features." Why unresolved: Current evaluations only test against known categories (rhythm, social unit); the model may encode structures that human experts have not yet defined. What evidence would resolve it: Clustering latent representations to find new distinct groups of codas that correlate with observable behavioral or environmental contexts.

## Limitations

- Codec representational adequacy: The Descript Audio Codec (DAC) was trained on speech and music, not sperm whale acoustics, with documented reconstruction weaknesses at frequencies where vowel-like patterns occur (3.7-5.7kHz)
- Domain adaptation marginal benefit: Skipping the general bioacoustic adaptation phase doesn't significantly harm most downstream tasks except Social Unit classification (-10.9 points)
- Biological response validation: Expert evaluations focus on acoustic properties rather than whether synthetic codas would be recognized or responded to by live sperm whales

## Confidence

**High Confidence**: 
- The model successfully generates synthetic codas from audio prompts using the described iterative masked token prediction approach
- The domain adaptation training pipeline functions as described and achieves the stated computational efficiency (5 days on single GPU)
- Downstream classification performance on the three tasks (coda detection, rhythm, vowel) is measurably better than baselines

**Medium Confidence**:
- The claim that generative training creates useful representations for downstream classification is supported by experimental results but lacks strong theoretical grounding in the bioacoustic literature
- The acoustic translation capability (converting various inputs to whale vocalization style) works as demonstrated but the biological interpretability of these translations remains unclear

**Low Confidence**:
- The claim that WhAM represents "the first transformer-based model for generating synthetic sperm whale codas" is difficult to verify given the nascent state of marine bioacoustic generative modeling literature
- The assertion that synthetic codas preserve "key acoustic features of the source recordings" is qualified by the expert evaluation noting differences in click-level features and background noise characteristics

## Next Checks

1. **Codec Ablation Study**: Train an identical WhAM architecture but with a codec trained on sperm whale vocalizations rather than speech/music. Compare FAD scores, downstream classification accuracy, and expert perceptual ratings to quantify the representational cost of using the current DAC tokenizer.

2. **Biological Response Validation**: Conduct playback experiments where live sperm whales are exposed to natural codas, synthetic WhAM codas, and baseline synthetic codas. Measure response rates, behavioral changes, and acoustic responses to determine if synthetic codas elicit biologically meaningful reactions.

3. **Temporal Structure Analysis**: Perform detailed analysis of inter-click intervals (ICIs) and multi-pulse structure in generated codas versus natural codas. Use statistical tests to quantify whether the model captures the stereotyped rhythmic patterns and individual variation patterns that characterize sperm whale coda communication.