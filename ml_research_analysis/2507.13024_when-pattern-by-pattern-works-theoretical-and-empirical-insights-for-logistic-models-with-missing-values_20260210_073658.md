---
ver: rpa2
title: 'When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic
  Models with Missing Values'
arxiv_id: '2507.13024'
source_url: https://arxiv.org/abs/2507.13024
tags:
- mice
- missing
- logistic
- mean
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of logistic regression with missing
  covariates, which is challenging because parameter estimation alone is insufficient
  for prediction on incomplete data. The authors introduce a Pattern-by-Pattern (PbP)
  strategy that learns one logistic model per missingness pattern, and prove it closely
  approximates Bayes probabilities under a Gaussian Pattern Mixture Model (GPMM) across
  MCAR, MAR, and MNAR settings.
---

# When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values

## Quick Facts
- **arXiv ID:** 2507.13024
- **Source URL:** https://arxiv.org/abs/2507.13024
- **Reference count:** 40
- **Primary result:** Pattern-by-Pattern (PbP) logistic regression approximates Bayes probabilities under Gaussian Pattern Mixture Models and excels for large sample sizes

## Executive Summary
This paper addresses the challenge of logistic regression with missing covariates, where parameter estimation alone is insufficient for prediction on incomplete data. The authors introduce a Pattern-by-Pattern (PbP) strategy that learns one logistic model per missingness pattern and prove it closely approximates Bayes probabilities under a Gaussian Pattern Mixture Model (GPMM) across MCAR, MAR, and MNAR settings. Empirically, they compare PbP against imputation and EM methods across classification, probability estimation, calibration, and inference. Their comprehensive analysis reveals that mean imputation serves as a good baseline for low sample sizes, while PbP excels for large sample sizes. The best overall performance is achieved by non-linear multiple iterative imputation techniques that incorporate the response label (Random Forest MICE with response).

## Method Summary
The Pattern-by-Pattern (PbP) strategy learns one logistic regression model for each distinct missingness pattern observed in the training data. Under the GPMM assumption, each pattern's Bayes probability can be expressed as a Probit model, and since the logistic sigmoid closely approximates the Probit function, fitting a logistic model per pattern closely approximates the true Bayes predictor. The method requires identifying all unique missingness patterns, training separate logistic regressions on the observed features for each pattern, and routing test samples to the appropriate model based on their missingness pattern. The approach is theoretically robust to MNAR settings under GPMM, unlike standard imputation methods.

## Key Results
- PbP approximates Bayes probabilities under GPMM with error â‰ˆ 0.018 due to the sigmoid-probit approximation
- Real-world missingness patterns are highly concentrated (top 10 patterns cover >80% of observations in 18/20 datasets)
- Mean.IMP is a strong baseline for small sample sizes, while PbP excels for large sample sizes
- MICE.RF.Y (Random Forest MICE with response) achieves the best overall performance but is computationally expensive

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Pattern-by-Pattern (PbP) strategy approximates Bayes optimal probabilities for logistic regression under Gaussian Pattern Mixture Models (GPMM).
- **Mechanism:** The Bayes probability for a specific missingness pattern can be expressed exactly as a Probit model under GPMM assumptions. Since the logistic sigmoid function is numerically close to the Probit function (approximation error $\approx 0.018$), fitting a logistic model per pattern closely approximates the true Bayes predictor.
- **Core assumption:** Data follows a Gaussian Pattern Mixture Model (GPMM), where the distribution of $X$ varies by missingness pattern but remains Gaussian.
- **Evidence anchors:**
  - [abstract] "We prove that a Pattern-by-Pattern strategy (PbP)... accurately approximates Bayes probabilities under a Gaussian Pattern Mixture Model (GPMM)."
  - [section 3.2] Theorem 3.5 establishes the approximation bound using the difference between scaled sigmoid and probit functions.
  - [corpus] Related work confirms that standard linear methods often fail to capture conditional probability structures when data is missing.
- **Break condition:** If the covariate distribution is non-Gaussian (e.g., heavy-tailed or multimodal) or the missingness mechanism creates non-linear dependencies that the logistic link cannot capture, the approximation may degrade.

### Mechanism 2
- **Claim:** PbP is computationally feasible in practice despite the theoretical exponential growth of missingness patterns ($2^d$).
- **Mechanism:** Real-world data exhibits highly concentrated missingness structures, often dominated by a small number of distinct patterns (e.g., "complete case" and "missing one feature"). This sparsity limits the number of models PbP must train.
- **Core assumption:** The missingness structure in the deployment data mirrors the concentration observed in standard benchmarks (R-miss-tastic).
- **Evidence anchors:**
  - [section 6] "In 18 of the 20 datasets, the 10 most frequent patterns cover over 80% of observations."
  - [section 6] Analysis of the "soybean" dataset ($d=36$) showing only 9 distinct patterns out of 68 billion theoretical possibilities.
  - [corpus] Corpus signals are weak for specific pattern concentration rates in *all* domains, suggesting this is data-dependent.
- **Break condition:** If data corruption is random and unstructured (e.g., random sensor failure across many dimensions), the number of patterns increases, causing sample sparsity per pattern and high computational overhead.

### Mechanism 3
- **Claim:** Non-linear iterative imputation (MICE.RF.Y) that includes the response label provides the best predictive performance across diverse sample sizes.
- **Mechanism:** Including the response variable $Y$ during the imputation phase allows the imputer to capture relationships between covariates and the outcome, preserving signal for the downstream classifier. Random Forest imputation captures non-linearities that linear methods (like standard MICE or Mean Imputation) miss.
- **Core assumption:** The relationship between covariates and the response is non-linear or complex; sufficient training time is available.
- **Evidence anchors:**
  - [abstract] "The best overall performance is achieved by non-linear multiple iterative imputation techniques that incorporate the response label (Random Forest MICE with response)..."
  - [section 5.4] Results show MICE.RF.Y outperforms others in non-linear feature settings.
  - [corpus] External corpus signals emphasize "Distribution-Preserving" imputation and "Contrastive Frameworks" for missing data, supporting the need for complex imputers.
- **Break condition:** In low-sample regimes, complex imputers may overfit or become unstable; computational cost may be prohibitive for very large datasets.

## Foundational Learning

- **Concept: Gaussian Pattern Mixture Model (GPMM)**
  - **Why needed here:** This is the core theoretical assumption required for PbP's approximation guarantees. It assumes that within any specific missingness pattern, the observed data follows a multivariate Gaussian distribution.
  - **Quick check question:** Does the distribution of observed features shift significantly depending on which other features are missing? (If yes, the "Pattern Mixture" aspect is relevant).

- **Concept: Bayes Probability vs. Bayes Classifier**
  - **Why needed here:** The paper proves PbP approximates the Bayes *probability* (the true $P(Y=1|X)$), not just the classification boundary. This is crucial for applications requiring calibrated risk scores (e.g., medical diagnosis).
  - **Quick check question:** Is the goal to minimize 0-1 loss (classification) or to estimate the true likelihood of an event (probability estimation)?

- **Concept: Missingness Mechanisms (MCAR, MAR, MNAR)**
  - **Why needed here:** PbP is theoretically robust to MNAR settings under GPMM, while methods like SAEM or standard MICE are typically designed for MAR. Understanding the missingness mechanism determines which baseline is theoretically justified.
  - **Quick check question:** Does the probability of a value being missing depend on the value itself (MNAR) or only on observed values (MAR)?

## Architecture Onboarding

- **Component map:**
  1. **Input Layer:** Raw features $X$ and Missingness Mask $M$
  2. **Router:** Identifies the missingness pattern $m$ for a given sample
  3. **Model Zoo:**
      - *PbP Branch:* Dictionary of $K$ trained logistic models, keyed by pattern $m$
      - *Imputation Branch:* Pre-trained imputer (e.g., MICE.RF.Y) + single Logistic Regression
  4. **Prediction:** Selected model outputs probability $\eta$

- **Critical path:**
  1. **Pattern Analysis:** Scan training data to count pattern frequencies. If patterns > $N/10$ (heuristic), PbP is likely infeasible
  2. **Training:** Train one logistic regression per frequent pattern. Handle rare patterns via imputation or a "catch-all" model
  3. **Inference:** Route incoming sample to the specific model matching its missingness mask

- **Design tradeoffs:**
  - **Speed vs. Accuracy:** Mean Imputation + Mask is fastest (baseline). MICE.RF.Y is most accurate but slowest
  - **Consistency vs. Sample Size:** PbP is consistent (converges to Bayes) but requires large $N$ per pattern. Mean Imputation is inconsistent but stable for small $N$
  - **Inference Capability:** PbP cannot recover global parameters $\beta^*$ (it learns $\alpha_m$ per pattern). Use MICE.Y or SAEM if parameter inference is the goal

- **Failure signatures:**
  - **High Cardinality Patterns:** PbP training time explodes; models train on single-digit samples
  - **Non-Gaussian Features:** PbP probability estimation error increases (Theorem 3.5 bound loosens)
  - **Rare Patterns at Test Time:** PbP has no model for a previously unseen pattern. *Mitigation:* Fallback to Mean Imputation or a "global" model for the observed subset

- **First 3 experiments:**
  1. **Pattern Concentration Audit:** Plot the cumulative density of the top 20 missingness patterns. Validate that the top 10 cover >80% of data to justify PbP
  2. **Gaussianity Check:** Test observed covariates for normality within the largest missingness patterns. If severely non-Gaussian, prioritize MICE.RF.Y over PbP
  3. **Sample Size Sweep:** Compare Mean.IMP vs. PbP vs. MICE.RF.Y on a held-out set across $N \in \{500, 5000, 50000\}$ to locate the crossover point where PbP overtakes Mean Imputation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical guarantees for PbP be extended beyond the Gaussian Pattern Mixture Model (GPMM) assumption to broader distribution families?
- Basis in paper: [explicit] Section 3.3 states "the approximation deteriorates, underscoring the importance of the Gaussian assumption in Theorem 3.5." The non-linear feature experiments show degraded performance when Gaussian assumptions are violated.
- Why unresolved: The theoretical proofs rely critically on properties of the Gaussian distribution and the probit function; whether similar approximation bounds exist for other distributions remains unexplored.
- What evidence would resolve it: Derivation of approximation bounds for PbP under alternative distributional assumptions (e.g., mixtures, heavy-tailed distributions), or identification of necessary and sufficient conditions beyond GPMM.

### Open Question 2
- Question: What are the precise sample complexity requirements for PbP to achieve near-Bayes performance across different missingness patterns?
- Basis in paper: [explicit] The paper repeatedly notes PbP "requires large training sets for convergence" and "excels for large sample sizes," but provides no formal sample complexity analysis.
- Why unresolved: The theoretical results address consistency but not finite-sample rates; the curse of dimensionality discussion addresses pattern count but not sample size per pattern.
- What evidence would resolve it: Formal derivation of sample complexity bounds as a function of dimension, number of patterns, and missingness rates, validated through controlled experiments.

### Open Question 3
- Question: Can the theoretical framework be extended to other generalized linear models (e.g., Poisson, ordinal regression) beyond logistic and probit?
- Basis in paper: [inferred] Section 3.1 mentions generalized linear models broadly but only analyzes Probit and logistic cases. The paper states this is the "first to exhibit a classification model (Probit), which remains well-specified on each missing data pattern."
- Why unresolved: The specific interaction between the link function and Gaussian conditional expectations that enables PbP may not generalize to other GLM link functions.
- What evidence would resolve it: Theoretical analysis of whether analogous approximation results hold for other link functions, or identification of which GLM families permit similar PbP guarantees.

## Limitations

- The theoretical guarantees for PbP hinge critically on the GPMM assumption, with approximation quality deteriorating for non-Gaussian feature distributions
- Computational feasibility analysis relies on observed pattern concentration in benchmark datasets, which may not generalize to domains with unstructured data corruption
- The assertion that PbP provides better calibration than imputation methods is primarily theoretical, with limited exploration of practical impact on downstream decision-making

## Confidence

- **High Confidence:** The PbP approximation mechanism under GPMM (Mechanism 1) is rigorously proven with explicit error bounds. The empirical finding that real-world missingness patterns are highly concentrated (Mechanism 2) is well-supported by the 20-dataset analysis.
- **Medium Confidence:** The performance ranking of methods (PbP > Mean.IMP for large N, MICE.RF.Y optimal overall) is robust across simulations and real data, though computational costs and convergence issues (especially for SAEM) introduce practical variability.
- **Low Confidence:** The paper's assertion that PbP provides "better calibration" than imputation methods is primarily theoretical. While MCB is measured, the practical impact on downstream decision-making in real-world applications remains unexplored.

## Next Checks

1. **Pattern Concentration Audit:** Before implementing PbP, analyze the top 20 missingness patterns in your deployment data. If the top 10 cover <60% of observations, PbP may be computationally infeasible.
2. **Gaussianity Check:** Test the normality of observed features within the largest missingness patterns. If features are heavy-tailed or multimodal, prioritize MICE.RF.Y over PbP.
3. **Sample Size Crossover Point:** For your specific dataset size and dimensionality, empirically determine when PbP overtakes Mean.IMP in performance through a small-scale validation sweep.