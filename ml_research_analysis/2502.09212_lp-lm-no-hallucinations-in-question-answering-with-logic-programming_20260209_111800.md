---
ver: rpa2
title: 'LP-LM: No Hallucinations in Question Answering with Logic Programming'
arxiv_id: '2502.09212'
source_url: https://arxiv.org/abs/2502.09212
tags:
- lp-lm
- prolog
- term
- grammars
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LP-LM is a question-answering system that uses logic programming
  to ground answers in a knowledge base, avoiding the hallucinations common in large
  language models. It parses natural language questions into Prolog terms using Definite
  Clause Grammar (DCG) and retrieves answers by matching against a knowledge base
  of fact terms.
---

# LP-LM: No Hallucinations in Question Answering with Logic Programming

## Quick Facts
- arXiv ID: 2502.09212
- Source URL: https://arxiv.org/abs/2502.09212
- Authors: Katherine Wu; Yanhong A. Liu
- Reference count: 18
- One-line primary result: LP-LM uses logic programming to ground answers in a knowledge base, avoiding hallucinations common in large language models by parsing questions into Prolog terms via Definite Clause Grammar.

## Executive Summary
LP-LM is a question-answering system that leverages logic programming to produce reliable answers without hallucinations. It parses natural language questions into Prolog terms using Definite Clause Grammar (DCG) with Probabilistic Context-Free Grammar (PCFG) probabilities, then retrieves answers by unifying these terms against a knowledge base of fact terms. The system uses tabling for efficient parsing, achieving linear time complexity for grammars with sufficient rules. Experiments show LP-LM outperforms LLMs like GPT-4o and Gemini on simple fact-based questions, producing correct answers without the hallucinations typical of neural models.

## Method Summary
LP-LM implements a symbolic QA pipeline using XSB Prolog v5.0. Natural language statements are parsed into Prolog terms via DCG with PCFG probabilities, generating the most probable constituency parse tree. These terms are inserted into a knowledge base using `add_kb/1`. Questions are similarly parsed and unified against the KB using `query_kb/1`, returning answers only when exact matches exist. The system leverages tabling for memoization, enabling linear-time parsing for large grammars. Key components include a tokenizer, DCG parser, dynamic Prolog fact store, and unification engine. The approach guarantees no hallucinations by construction, as answers are strictly grounded in the KB.

## Key Results
- LP-LM achieves zero hallucinations on simple fact-based questions by grounding answers in a Prolog knowledge base
- Outperforms GPT-4o, GPT-4o mini, and Gemini on basic factual queries with 100% accuracy when facts exist in KB
- Demonstrates linear-time parsing complexity for grammars with 100+ rules using tabling, outperforming NLTK Viterbi parser

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic DCG Parsing for Semantic Representation
Parsing natural language into Prolog terms via DCG with PCFG probabilities enables structured, unambiguous representation of queries. Input sentences are tokenized and matched against DCG rules with associated probabilities. The parser enumerates possible parse trees, computes the most probable constituency tree, and emits a corresponding Prolog term (e.g., "Bob runs" → `runs(bob)`). Core assumption: Sentences conform to the PCFG rule set; valid parses exist for the input.

### Mechanism 2: Unification-Based KB Retrieval
Prolog unification grounds answers strictly in known facts, eliminating hallucination by construction. Query terms contain variables (e.g., `run(X)` for "who runs"). Prolog unifies these against KB facts (e.g., `run(Bob)`), instantiating variables only when matches exist. Yes/no queries check for exact term presence. Core assumption: The KB contains relevant facts expressed as Prolog terms compatible with query structure.

### Mechanism 3: Tabling for Linear-Time Parsing
Tabling memoizes subgoal results, enabling linear-time parsing for large grammars by avoiding redundant recomputation. During DCG parsing, tabled predicates store computed answers for each goal. Repeated calls retrieve cached results. For grammars with 100+ rules, the paper reports observed linear scaling in input length. Core assumption: Grammar has "sufficiently many" rules; tabling overhead is amortized by reduction in exponential re-evaluation.

## Foundational Learning

- **Prolog Unification**
  - Why needed here: Core retrieval mechanism—queries unify with KB terms to instantiate answers.
  - Quick check question: Given `foo(a, X)` and `foo(Y, b)`, what is the unified term? (Answer: `foo(a, b)` with `X=b, Y=a`)

- **Definite Clause Grammars (DCG)**
  - Why needed here: Declarative syntax for encoding PCFG rules; preprocessor translates to Prolog clauses for parsing.
  - Quick check question: What Prolog clause does `s --> np, vp.` expand to? (Answer: `s(A, B) :- np(A, C), vp(C, B).`)

- **Tabling (Memoization) in Logic Programming**
  - Why needed here: Ensures parsing terminates and scales; prevents infinite loops on left-recursive grammars.
  - Quick check question: Why does tabling help with left-recursion? (Answer: Cached results for recursive calls prevent re-evaluation of identical subgoals.)

## Architecture Onboarding

- Component map: Tokenizer -> DCG Parser (XSB Prolog) -> Parse Tree + Prolog Term -> Unification Engine -> KB -> Answer
- Critical path: Input sentence → tokenization → DCG parsing → parse tree + Prolog term (with probability) → if statement: add_kb inserts term into KB → if query: query_kb unifies term against KB → returns answer → post-process to natural language (optional)
- Design tradeoffs: Coverage vs. reliability (manual PCFG rules ensure precise parsing but limit generalization); KB scope (closed-world assumption guarantees no hallucination but requires comprehensive fact ingestion); Reasoning depth (current system handles retrieval only; deductive/inductive reasoning is future work)
- Failure signatures: Parse failure (input doesn't match any DCG rule → no term generated); KB miss (term generated but no unifying fact → no answer, not hallucination); Ambiguity explosion (multiple high-probability parses → system returns most probable; may misalign with user intent if grammar underspecified)
- First 3 experiments: Grammar coverage test (input sentences from target domain; measure parse success rate and identify missing rules); KB retrieval accuracy (insert known facts; query with wh- and yes/no questions; verify unification returns correct answers); Efficiency benchmark (time parsing on grammars of varying size and sentence lengths; compare against NLTK Viterbi to reproduce paper's linear scaling claim)

## Open Questions the Paper Calls Out

### Open Question 1
How can LP-LM be extended to support deductive and inductive reasoning capabilities beyond simple fact retrieval? The conclusion states that LP-LM is currently limited to simple retrieval tasks and lists supporting reasoning capabilities (deductive and inductive) as plans for future work. The current implementation relies on direct unification between parsed questions and a static knowledge base, lacking mechanisms for inference or logical derivation. A modified version successfully answering multi-hop questions or deriving new facts from existing rules without explicit pre-insertion would resolve this.

### Open Question 2
Can LLM-based pre-processing be integrated to handle ungrammatical inputs without compromising LP-LM's zero-hallucination guarantee? Section 5 suggests "augmenting" LP-LM with LLMs or NLP techniques to distill core facts from sentences that violate grammatical rules. While proposed as a solution for input generalization, the authors note that implementation is a future plan, and the impact on the system's reliability remains untested. Experiments showing that an LLM pre-processor can normalize noisy or ungrammatical text into valid Prolog terms without introducing factual errors would resolve this.

### Open Question 3
What methods can automate the expansion of Probabilistic Context-Free Grammar (PCFG) rules to reduce the manual effort required for generalization? Section 5 identifies that manually adding grammar rules is tedious and notes that "generalizing the system" is a goal for future work. The system currently requires manual authoring of grammar rules to cover new sentence structures, which limits scalability. A system update or methodology that automatically learns or generates PCFG production rules from a text corpus while maintaining linear time parsing efficiency would resolve this.

## Limitations
- Coverage limited by manually specified PCFG grammar and lexicon, severely restricting generalization to diverse linguistic expressions
- Closed-world KB assumption guarantees no hallucinations but requires comprehensive fact ingestion upfront, making system brittle to missing knowledge
- Paper lacks details on handling multi-hop reasoning, negation, quantification, or complex temporal reasoning common in real-world QA

## Confidence
- **High Confidence**: Core mechanism of parsing natural language to Prolog terms via DCG and using unification for KB retrieval is well-established and correctly described. Claim of avoiding hallucinations through this approach is logically sound.
- **Medium Confidence**: Tabling efficiency claims are supported by presented timing experiments, but full reproducibility depends on access to complete grammar files and lexicon. Comparison against LLMs is based on limited question sets.
- **Low Confidence**: Paper does not address handling of negation, quantification, or complex temporal reasoning. Claims about handling "simple fact-based questions" are vague regarding what complexity threshold is supported.

## Next Checks
1. **Grammar Coverage Test**: Systematically test parser against diverse corpus of simple fact-based sentences (100+ sentences from news headlines or Wikipedia facts) to measure parse success rate and identify specific linguistic patterns that fail.

2. **KB Retrieval Completeness**: Construct KB with 100+ diverse facts covering different sentence structures and query patterns. Measure retrieval accuracy across different question types (wh-questions, yes/no, multiple answers) and identify patterns of KB misses.

3. **Cross-Implementation Benchmark**: Reproduce timing experiments using both XSB Prolog and SWI-Prolog to verify tabling provides consistent linear scaling benefits. Test on grammars of varying sizes (3, 20, 50, 100+ rules) and sentence lengths (1-50 tokens).