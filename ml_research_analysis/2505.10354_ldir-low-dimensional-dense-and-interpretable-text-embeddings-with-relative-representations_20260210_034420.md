---
ver: rpa2
title: 'LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative
  Representations'
arxiv_id: '2505.10354'
source_url: https://arxiv.org/abs/2505.10354
tags:
- text
- embeddings
- dense
- ldir
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LDIR, a method for generating low-dimensional
  dense and interpretable text embeddings. The approach uses farthest point sampling
  to select anchor texts from a corpus and computes relatedness scores between each
  text and these anchors, resulting in embeddings where each dimension reflects semantic
  relatedness to a specific anchor.
---

# LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations

## Quick Facts
- arXiv ID: 2505.10354
- Source URL: https://arxiv.org/abs/2505.10354
- Reference count: 14
- Primary result: Achieves competitive semantic textual similarity performance with only 500 dimensions using interpretable anchor-based embeddings

## Executive Summary
LDIR introduces a method for generating low-dimensional dense text embeddings where each dimension represents semantic relatedness to a specific anchor text. The approach uses farthest point sampling to select diverse anchor texts from a corpus and computes cosine similarity scores between input texts and these anchors. Experiments show LDIR achieves competitive performance across semantic textual similarity, retrieval, and clustering tasks while using 500 dimensions compared to the 10,000+ dimensions required by previous interpretable methods.

## Method Summary
LDIR encodes a large corpus (MEDI2) with a pre-trained encoder, then applies farthest point sampling (FPS) to select n anchor texts that maximize pairwise diversity. For any input text, the embedding is computed as n cosine similarity scores between the text and each anchor. The method requires no fine-tuning and works with any encoder. FPS is shown to outperform uniform sampling and K-Means, particularly at low dimensions. Optimal anchor texts are 20-100 tokens in length, with longer texts degrading performance.

## Key Results
- Achieves 82.82 average STS score with 500 dimensions using AngIE encoder, outperforming interpretable baselines
- 500-dimensional LDIR matches or exceeds 200-dimensional performance of black-box models on retrieval tasks
- Outperforms K-Means and uniform sampling for anchor selection, especially at 200 dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative representations via anchor texts preserve semantic structure while enabling interpretability.
- Mechanism: Each embedding dimension explicitly encodes cosine similarity between the input text and a specific anchor text, transforming abstract dense vectors into human-readable relatedness scores.
- Core assumption: The backbone encoder's similarity space captures meaningful semantic relationships that transfer to the anchor-based representation.
- Evidence anchors:
  - [abstract] "The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling."
  - [section 3.2] Eq. 6 shows e_interp_dense(t) = [REL(a1, t), ..., REL(an, t)] where REL uses cosine similarity.
  - [corpus] Moschella et al. (2023) in related work demonstrates relative representations enable zero-shot latent space communication, supporting the transfer assumption.
- Break condition: If backbone encoder produces poor similarity structure (e.g., collapsed representations), anchor-based projections will inherit these failures.

### Mechanism 2
- Claim: Farthest Point Sampling (FPS) selects semantically diverse anchors that maximize representational coverage.
- Mechanism: FPS iteratively selects points maximizing minimum distance to all previously selected anchors, ensuring anchors span the semantic space rather than clustering in dense regions.
- Core assumption: Semantic diversity in anchor selection translates to information-rich dimensions in the final embedding.
- Evidence anchors:
  - [section 3.3] "FPS generally performed the best... The advantage of FPS in the low-dimensional range stems from its maximum diversity sampling strategy."
  - [section 5.1] Figure 3(a) shows FPS outperforms uniform sampling and K-Means, particularly at lower dimensions (200-500).
  - [corpus] No direct corpus evidence on FPS specifically for text anchors; assumption relies on geometric intuition from point cloud literature.
- Break condition: If corpus has highly skewed semantic distribution, FPS may select outlier texts as anchors that don't generalize well to downstream tasks.

### Mechanism 3
- Claim: Dense floating-point relatedness scores provide more expressive power than binary 0/1 embeddings at lower dimensionality.
- Mechanism: Continuous values in [0,1] range allow fine-grained semantic distinctions, reducing the need for thousands of dimensions required by binary QA-embeddings.
- Core assumption: The continuous relatedness distribution contains meaningful gradations that binary thresholding would destroy.
- Evidence anchors:
  - [abstract] LDIR achieves competitive performance with "only 500 dimensions" vs. QAEmb-MBQA's 10,654 dimensions.
  - [table 6] LDIR (500 dim, dense) achieves 82.82 STS average vs. CQG-MBQA (9,614 dim, binary) at 77.60.
  - [corpus] Related work on sparse autoencoders for interpretability suggests dense representations can be decomposed into interpretable components, supporting the dense-to-interpretable pathway.
- Break condition: If relatedness scores collapse to extreme values (near 0 or 1), the advantage over binary embeddings diminishes.

## Foundational Learning

- Concept: **Cosine Similarity and Vector Space Semantics**
  - Why needed here: LDIR fundamentally relies on cosine similarity to compute anchor-text relatedness; understanding why cosine (not Euclidean) is used for semantic similarity is essential.
  - Quick check question: Given two text embeddings [1,2,3] and [2,4,6], what is their cosine similarity and why does this matter for semantic interpretation?

- Concept: **Relative vs. Absolute Representations**
  - Why needed here: LDIR transforms absolute embeddings into relative representations; grasping this paradigm shift explains why interpretability improves without catastrophic performance loss.
  - Quick check question: If you have embeddings in an arbitrary coordinate system, how would representing them relative to 3 fixed reference points change what each dimension means?

- Concept: **Curse of Dimensionality and Dimensionality Reduction**
  - Why needed here: The paper claims 500 dimensions suffice where 10,000+ were previously needed; understanding dimensionality-precision tradeoffs contextualizes this achievement.
  - Quick check question: Why might a 500-dimensional dense representation capture more information than a 10,000-dimensional sparse binary vector?

## Architecture Onboarding

- Component map: Corpus Processing -> Anchor Selection -> Embedding Generation -> Evaluation
- Critical path:
  1. Backbone encoder quality determines upper bound of LDIR quality
  2. Anchor diversity (via FPS) determines information preservation at low dimensions
  3. Anchor text length (20-100 tokens optimal) affects anchor quality—long texts introduce noise
- Design tradeoffs:
  - **Dimensionality vs. Performance**: 200 dims sufficient for clustering; 500 dims better for retrieval/STS
  - **Interpretability vs. Performance**: Fine-grained relatedness (Section 5.3) improves interpretability but reduces STS performance (61.39 vs. 74.44)
  - **Encoder choice**: AngIE best for STS/retrieval; SBERT competitive for clustering; encoder choice should match downstream task
- Failure signatures:
  - STS scores significantly below backbone encoder → anchor selection failed (check FPS implementation)
  - High cognitive load after binarization (>50) → anchors not semantically diverse
  - Clustering performance drops vs. backbone → dimensionality too low for task complexity
- First 3 experiments:
  1. **Reproduce STS results with AngIE backbone**: Encode STS benchmark texts, compute 500-dim LDIR embeddings using pre-extracted anchors, calculate Spearman correlation. Target: 82.0+ average.
  2. **Ablate anchor sampling methods**: Compare FPS vs. uniform vs. K-Means on same corpus with 200 anchors. Verify FPS advantage at low dimensions.
  3. **Test anchor length sensitivity**: Extract anchors from short (<20 tokens), medium (20-100), and long (>100) text subsets. Confirm medium-length anchors perform best per Figure 3(b).

## Open Questions the Paper Calls Out

- **Open Question 1**: How can interpretability be formally defined and measured for dense text embeddings, moving beyond binary "cognitive load" metrics?
  - Basis in paper: [explicit] The authors state in Section 4.3 that designing suitable metrics for dense embeddings "remains a valuable and open question" and list it as future work in the Conclusion.
  - Why unresolved: Current metrics like "cognitive load" rely on binarization and inner products, which are originally designed for binary vectors and do not directly apply to continuous floating-point representations.
  - What evidence would resolve it: The proposal and validation of a new quantitative metric that correlates with human judgment of interpretability for dense vectors without requiring binarization.

- **Open Question 2**: Can anchor texts be optimized or learned for specific downstream tasks rather than sampled generically from a corpus?
  - Basis in paper: [explicit] The limitations section notes that currently "anchor texts are sampled from the entire corpus," and suggests future work could "further optimize these anchor texts for different downstream tasks."
  - Why unresolved: The current method (Farthest Point Sampling) selects task-agnostic anchors to maximize diversity, potentially missing task-specific semantic nuances required for optimal performance in specialized domains.
  - What evidence would resolve it: Experiments demonstrating that a supervised or task-aware anchor selection strategy yields higher performance on specific benchmarks (e.g., medical retrieval) compared to the generic FPS method.

- **Open Question 3**: Does LDIR actually enhance human understanding or trust in model outputs compared to existing baselines?
  - Basis in paper: [explicit] The limitations section explicitly states that "human evaluation and application scenarios of interpretable text embeddings can be further explored, which is also crucial for explainable AI."
  - Why unresolved: The current evaluation relies on automated performance metrics (STS, nDCG) and theoretical cognitive load calculations, lacking empirical verification of whether humans find the "relatedness to anchor" logic genuinely interpretable.
  - What evidence would resolve it: A user study where participants successfully predict embedding behavior or debug semantic errors using LDIR dimensions more effectively than using black-box embeddings.

## Limitations

- The method's performance heavily depends on the quality of the underlying encoder embeddings
- FPS anchor selection lacks extensive comparative validation against other sampling strategies
- The assumption that cosine similarity space meaningfully captures semantic relationships is foundational but not independently verified
- Anchor length optimization (20-100 tokens) is empirically observed but underlying reasons for performance degradation with longer texts are not fully explained

## Confidence

**High Confidence**: The mechanism of relative representations via anchor texts (Mechanism 1) is theoretically sound and empirically supported by consistent performance improvements across all three task types.

**Medium Confidence**: The FPS algorithm's superiority for anchor selection (Mechanism 2) is demonstrated within the paper's controlled experiments but lacks external validation.

**Low Confidence**: The claim that dense floating-point relatedness scores are universally superior to binary embeddings (Mechanism 3) is task-dependent and may not hold for all downstream applications.

## Next Checks

1. **Encoder Dependency Analysis**: Systematically evaluate LDIR performance across a broader range of encoder qualities (from smaller to larger models) to determine whether reported performance gains are consistent across the encoder quality spectrum or specific to the high-quality AngIE model.

2. **Anchor Diversity Validation**: Implement and compare multiple anchor selection strategies (beyond FPS, uniform, and K-Means) including K-Center, random sampling with diversity constraints, and clustering-based approaches to rigorously establish whether FPS is indeed optimal.

3. **Task-Specific Performance Breakdown**: Analyze LDIR's performance on individual datasets within each task category (e.g., which specific STS datasets drive the 82.82 average, which retrieval tasks benefit most from 500 vs. 200 dimensions) to identify scenarios where the method excels or underperforms relative to baselines.