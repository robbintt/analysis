---
ver: rpa2
title: 'Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression
  Perception and Contextual Awareness'
arxiv_id: '2501.07978'
source_url: https://arxiv.org/abs/2501.07978
tags:
- video
- facial
- event
- events
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately describing facial
  expressions in videos, which is difficult for current video Multimodal Large Language
  Models (MLLMs) due to limited visual token capacity and lack of specialized datasets.
  To tackle this, the authors introduce a new instruction-following dataset, FDA,
  containing 5,033 high-quality video clips annotated manually with over 700,000 tokens,
  designed to improve MLLMs' ability to discern subtle facial nuances.
---

# Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression Perception and Contextual Awareness

## Quick Facts
- arXiv ID: 2501.07978
- Source URL: https://arxiv.org/abs/2501.07978
- Reference count: 40
- Model FaceTrack-MM achieves TEM score of 0.364 on FEC-Bench, outperforming existing methods

## Executive Summary
This paper addresses the challenge of accurately describing facial expressions in videos, which is difficult for current video Multimodal Large Language Models (MLLMs) due to limited visual token capacity and lack of specialized datasets. To tackle this, the authors introduce a new instruction-following dataset, FDA, containing 5,033 high-quality video clips annotated manually with over 700,000 tokens, designed to improve MLLMs' ability to discern subtle facial nuances. They propose FaceTrack-MM, a novel model that leverages a dynamic video face tracking module to focus on the main character's facial expressions, even in complex multi-person scenarios. Additionally, they introduce a novel evaluation metric, Temporal Event Matching (TEM), combining event extraction, relation classification, and the longest common subsequence (LCS) algorithm to assess content and temporal sequence consistency. Experimental results on FEC-Bench, a benchmark for this task, demonstrate that FaceTrack-MM significantly outperforms existing methods, achieving a TEM score of 0.364, highlighting its superior capability in capturing and describing dynamic facial expressions.

## Method Summary
The authors introduce FaceTrack-MM, which enhances video MLLMs for facial expression description through three key innovations: (1) a dynamic video face tracking module that uses StrongSORT and K-means clustering to identify and track the main character's face in multi-person videos, (2) an auxiliary facial visual encoder (FaceXFormer) that captures fine-grained facial details missed by general CLIP encoders due to token budget constraints, and (3) instruction tuning on a manually curated dataset (FDA) containing 5,033 video clips with detailed facial expression annotations. The model uses LoRA fine-tuning (r=64, α=128) on VideoLLaMA2-7B, processing 16 frames per video at 16 fps with dual-stream feature extraction (general visual features via CLIP-ViT-Large and facial-specific features via FaceXFormer).

## Key Results
- FaceTrack-MM achieves TEM score of 0.364 on FEC-Bench, significantly outperforming existing methods
- The dynamic face tracking module improves TEM from 0.321 to 0.357 over instruction tuning alone
- Adding FaceXFormer further improves TEM to 0.364, demonstrating the value of specialized facial encoding
- Instruction tuning alone improves TEM from 0.258 (baseline) to 0.321 (+25% relative)

## Why This Works (Mechanism)

### Mechanism 1
Dynamic video face tracking with trajectory clustering improves facial feature encoding for the main character in multi-person scenes. The module downsamples video to 16 fps, extracts face landmarks and features per frame, applies StrongSORT multi-object tracking, then clusters trajectories using total occupied area and intra-trajectory cosine similarity to distinguish main characters from background characters. Core assumption: The main character's face occupies more cumulative screen space and has higher feature consistency across frames than background characters. Evidence: Section 4.2.2 describes K-means clustering with area and similarity features, Table 2 shows tracking improves TEM from 0.321 to 0.357, and DEFT-LLM paper addresses feature entanglement but uses different approach. Break condition: If main character's face is frequently occluded or appears smaller than background characters.

### Mechanism 2
Specialized facial visual encoding (FaceXFormer) captures fine-grained facial details that general CLIP encoders miss due to token budget constraints. CLIP-ViT-Large provides general visual features; FaceXFormer extracts specialized facial features from tracked face regions; both are projected via STC module and concatenated before the LLM. Core assumption: General-purpose visual encoders distribute representational capacity across entire frames, leaving insufficient capacity for small facial regions. Evidence: Section 4.2.1 introduces FaceXFormer architecture, Introduction cites Qwen2-VL's 138-token limit per frame, Table 2 shows FaceXFormer improves TEM from 0.357 to 0.364, and Face-LLaVA uses specialized face understanding through instruction tuning. Break condition: If FaceXFormer's pre-training domain does not generalize to dynamic expressions or extreme head poses.

### Mechanism 3
Instruction tuning on manually curated facial expression captions with diverse phrasings improves temporal description accuracy. 20 instruction templates generated via ChatGPT and manually filtered; each training sample randomly selects one instruction to encourage robustness. LoRA fine-tuning (r=64, α=128) applied to VideoLLaMA2-7B base. Core assumption: Manual annotations capturing facial dynamics provide signal absent from general video captioning datasets. Evidence: Section 4.3 describes template generation and selection, Section 3.3 cites 10.9 average facial action keywords, Table 2 shows instruction tuning improves TEM from 0.258 to 0.321, and Face-LLaVA and DEFT-LLM both employ instruction tuning for facial tasks. Break condition: If instruction templates are too semantically similar or annotations contain systematic biases.

## Foundational Learning

- **Concept: Multi-Object Tracking (MOT) with Re-ID Features**
  - **Why needed here:** Understanding how StrongSORT maintains identity across frames explains why face tracking succeeds where per-frame detection fails.
  - **Quick check question:** Given a 3-second clip at 16 fps where a person turns their head away for 10 frames, would MOT lose the trajectory? (Answer: Depends on re-ID feature matching threshold and max age parameter.)

- **Concept: Visual Token Budgeting in MLLMs**
  - **Why needed here:** The paper's core motivation is token scarcity for facial regions; understanding this constraint explains why auxiliary encoders help.
  - **Quick check question:** If a model has 2048 visual tokens total and processes 16 frames, how many tokens per frame are available? (Answer: 128, matching the Qwen2-VL constraint cited.)

- **Concept: Longest Common Subsequence (LCS) for Sequence Alignment**
  - **Why needed here:** TEM metric uses LCS to evaluate temporal ordering; understanding LCS helps interpret why this metric penalizes out-of-order events.
  - **Quick check question:** For sequences [A,B,C] and [A,C,B], what is the LCS length? (Answer: 2, either [A,B] or [A,C].)

## Architecture Onboarding

**Component map:**
Video Input -> CLIP-ViT-Large -> STC Projector -> Visual Tokens
Video Input -> Dynamic Face Tracker -> FaceXFormer -> STC Projector -> Facial Tokens
[Visual + Facial + Text Tokens] -> Mistral-7B-Instruct -> Caption

**Critical path:**
1. Video -> Frame downsampling (16 fps)
2. Face detection + feature extraction (DaMOFD + TransFace)
3. StrongSORT trajectory generation
4. K-means clustering (area + similarity features) -> main trajectory selection
5. Main face frames -> FaceXFormer -> facial features
6. Full frames -> CLIP -> general features
7. Both feature streams -> STC projector -> LLM input

**Design tradeoffs:**
- **16 fps downsampling:** Balances temporal fidelity vs. computation; may miss sub-100ms micro-expressions
- **K-means with K=2:** Assumes binary main/background distinction; fails for ensemble scenes with multiple protagonists
- **LoRA (r=64):** Limits fine-tuning capacity; may underfit complex facial dynamics compared to full fine-tuning

**Failure signatures:**
- **Hallucinated emotions:** Model attributes emotions not visible in video -> check if annotation training data includes subjective inferences
- **Static descriptions despite motion:** Model ignores expression changes -> verify face tracking is actually extracting varied frames
- **Wrong character focus:** Model describes background face -> inspect trajectory clustering output

**First 3 experiments:**
1. **Ablation on frame rate:** Train with 8, 16, 24 fps to test whether temporal granularity matters for micro-expressions; expect diminishing returns above 16 fps for standard expressions
2. **Trajectory clustering robustness:** Replace K-means with hierarchical clustering or add minimum-face-size filter; evaluate on multi-person test subset
3. **Cross-dataset generalization:** Evaluate FaceTrack-MM on held-out emotion datasets (e.g., FERV39K) without further fine-tuning to assess overfitting to FDA annotation style

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the K-means clustering approach for "Main Trajectory Selection" when multiple characters have similar screen presence or face sizes? Basis: Section 4.2.2 states the model uses K-means clustering (k=2) on total area and cosine similarity of face trajectories to distinguish the "main character cluster" from the background. Unresolved because the method assumes a clear distinction in spatial-temporal features between main and background characters, which may fail in dialogue scenes where two characters have equal screen time and face proximity. What evidence would resolve it: Evaluation results on a specific subset of multi-person dialogue videos where interlocutors have roughly equal frame coverage, measuring tracking accuracy and caption consistency.

### Open Question 2
To what extent does the exclusion of "subjective speculative descriptions" from the training data impact the model's ability to infer emotional context? Basis: Section 3.2 and Figure 10 show that while the dataset includes "reasonable inferences" (marked with identifiers like `[]`), the authors explicitly remove these subjective inferences for the instruction tuning phase. Unresolved because the paper does not ablate the effect of training with versus without these inferences, making it unclear if the removal creates a model that is more factual but less emotionally intelligent or context-aware. What evidence would resolve it: An ablation study comparing the proposed model against a version trained on the full dataset including subjective inferences, evaluated on metrics requiring emotional reasoning rather than just physical description.

### Open Question 3
Does the Temporal Event Matching (TEM) metric maintain consistency when utilizing different LLM backbones for the event extraction and relation classification steps? Basis: Section 3.4 and Algorithm 1 describe the TEM metric as relying on ChatGPT for event extraction and relation classification. Unresolved because evaluation metrics based on proprietary LLMs can suffer from versioning instability or specific model biases, and the reliance on ChatGPT to determine "Same Meaning" or "Opposite Meaning" introduces a dependency that may not be deterministic across different LLMs. What evidence would resolve it: Correlation analysis of TEM scores generated by different LLM backbones (e.g., GPT-4, Claude, open-source Llama 3) against human evaluation scores on the same set of generated captions.

## Limitations

- **Generalization to Unseen Expression Types**: FDA dataset may contain annotation biases toward common expressions, limiting model's ability to accurately describe rare or subtle expressions
- **Multi-character Scene Robustness**: K-means trajectory clustering assumes binary main/background character distinction, which may fail in complex scenes with multiple protagonists
- **Temporal Granularity vs. Computational Cost**: 16 fps downsampling balances efficiency and expression capture, but may miss micro-expressions (50-200ms)

## Confidence

- **High Confidence**: TEM metric design and implementation using LCS for temporal consistency evaluation; ablation studies clearly show improvements from individual components
- **Medium Confidence**: Effectiveness of trajectory clustering for main character identification; while ablation shows tracking helps, clustering approach's robustness to complex multi-person scenarios needs more rigorous testing
- **Low Confidence**: Claim that FaceXFormer captures fine-grained facial details missed by CLIP; paper assumes this but does not provide direct comparison of facial feature quality

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate FaceTrack-MM on emotion recognition datasets (e.g., FERV39K, AFEW) without fine-tuning to assess whether the model has overfit to FDA's annotation style and expression distribution

2. **Temporal Resolution Analysis**: Systematically vary the frame rate (8, 16, 24, 30 fps) during training and testing to identify the optimal trade-off between micro-expression capture and computational efficiency

3. **Multi-Character Robustness Benchmark**: Create or identify test cases with 3+ characters of similar prominence, where the model must correctly identify and describe expressions for each protagonist in sequence