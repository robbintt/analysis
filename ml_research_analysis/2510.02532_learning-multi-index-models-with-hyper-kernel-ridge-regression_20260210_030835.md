---
ver: rpa2
title: Learning Multi-Index Models with Hyper-Kernel Ridge Regression
arxiv_id: '2510.02532'
source_url: https://arxiv.org/abs/2510.02532
tags:
- theorem
- learning
- kernel
- page
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces hyper-kernel ridge regression (HKRR), a method
  that blends kernel methods with neural network representation learning to overcome
  the curse of dimensionality in learning multi-index models (MIMs). HKRR learns a
  low-dimensional linear transformation combined with a nonlinear function, extending
  kernel ridge regression by optimizing over a family of kernels rather than a fixed
  one.
---

# Learning Multi-Index Models with Hyper-Kernel Ridge Regression

## Quick Facts
- arXiv ID: 2510.02532
- Source URL: https://arxiv.org/abs/2510.02532
- Authors: Shuo Huang; Hippolyte Labarrière; Ernesto De Vito; Tomaso Poggio; Lorenzo Rosasco
- Reference count: 40
- One-line primary result: Hyper-kernel ridge regression (HKRR) learns low-dimensional linear transformations with nonlinear functions to overcome the curse of dimensionality in multi-index models.

## Executive Summary
This paper introduces hyper-kernel ridge regression (HKRR), a method that combines kernel methods with neural network representation learning to address the curse of dimensionality in learning multi-index models (MIMs). HKRR learns a low-dimensional linear transformation combined with a nonlinear function by optimizing over a family of kernels rather than a fixed one. The key theoretical contribution is a sample complexity bound showing HKRR achieves exponential dependence on the true latent dimension rather than the ambient dimension, with convergence rates of order $m^{-\theta\zeta}$. The authors propose two optimization algorithms—Variable Projection (VarPro) and Alternating Gradient Descent (AGD)—with AGD showing greater stability and performance in practice. Experiments demonstrate HKRR's effectiveness, particularly when the latent dimension is overestimated.

## Method Summary
HKRR learns a low-dimensional linear transformation $B$ mapping from ambient dimension $D$ to latent dimension $d$, combined with a nonlinear function learned through kernel methods. The method optimizes over hyper-kernels $k(Bx, Bx')$ rather than fixed kernels, enabling learning of the projection matrix $B$ alongside kernel coefficients $\alpha$. Two optimization algorithms are proposed: VarPro, which analytically solves for $\alpha$ at each step, and AGD, which performs alternating gradient descent on both $B$ and $\alpha$. The method uses Nyström approximations to reduce computational cost while preserving theoretical guarantees. The approach is specifically designed for multi-index models where the target function has the form $f^*(x) = g^*(B^*x)$ for some low-rank $B^*$ and smooth $g^*$.

## Key Results
- HKRR achieves sample complexity bounds with exponential dependence on the latent dimension $d^*$ rather than the ambient dimension $D$
- AGD algorithm outperforms VarPro in practice, showing greater stability and ability to escape local minima
- Nyström approximations preserve theoretical guarantees while reducing computational cost from cubic to roughly quadratic
- Overestimating the latent dimension $d$ is robust to accuracy loss, while underestimating causes severe degradation

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Reduction via Latent Projection
HKRR circumvents the curse of dimensionality by learning a linear projection $B$ that restricts the effective hypothesis space to the latent low-dimensional subspace of a Multi-Index Model. Standard KRR on high-dimensional ambient data incurs sample complexity costs exponential in the ambient dimension $D$. HKRR optimizes over hyper-kernels $k(Bx, Bx')$, effectively performing KRR in a space of dimension $d$. Theoretical results indicate the convergence rate scales with the latent dimension $d^*$ rather than $D$.

### Mechanism 2: Stability via Alternating Gradient Descent (AGD)
Joint optimization of projection matrix $B$ and kernel coefficients $\alpha$ using AGD offers superior stability compared to VarPro. While VarPro analytically solves for $\alpha$ at every step (closed-form) and updates $B$ via gradient descent, this makes $B$ highly sensitive to the specific $\alpha$ solution, often trapping optimization in local minima. AGD performs local gradient steps on both $B$ and $\alpha$, allowing the optimizer to traverse the loss landscape more broadly and escape critical points.

### Mechanism 3: Approximation via Nyström Subsampling
Nyström subsampling preserves the statistical consistency of HKRR while reducing computational complexity. Full KRR requires inverting an $m \times m$ kernel matrix. Nyström methods approximate this using a subset of inducing points. The error analysis decomposes the risk into estimation error and computational error. By ensuring the number of inducing points scales appropriately with the effective dimension, the computational error remains bounded within the same order as the estimation error.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: HKRR is fundamentally a kernel method; understanding that functions in an RKHS can be evaluated via inner products $\langle f, k(x, \cdot) \rangle$ is necessary to grasp the representer theorem used to solve for coefficients $\alpha$
  - Quick check question: Can you explain why the Representer Theorem allows us to optimize over an infinite-dimensional function space by solving a finite-dimensional linear system?

- **Concept: Multi-Index Models (MIMs)**
  - Why needed here: The entire theoretical justification for HKRR relies on the assumption that the target function $f^*$ is a composition of a linear map $B$ and a non-linear map $g$. Understanding this structure is vital for interpreting the results
  - Quick check question: If a regression function is $f(x) = \sin(w_1^T x) + \cos(w_2^T x)$, what is the "latent dimension" $d^*$ and what does the matrix $B$ represent?

- **Concept: Sample Complexity & The Curse of Dimensionality**
  - Why needed here: The paper's primary contribution is a sample complexity bound. One must understand that standard non-parametric rates scale as $m^{-2r/(2r+D)}$ to appreciate why the HKRR rate ($m^{-\theta\zeta}$ dependent on $d^*$) is a significant theoretical improvement
  - Quick check question: How does the convergence rate of a standard Lipschitz regressor change as the input dimension $D$ approaches infinity?

## Architecture Onboarding

- **Component map:** Input Layer ($X \in \mathbb{R}^D$) -> Projection Layer (Learnable $B \in \mathbb{R}^{d \times D}$) -> Latent Space ($Z = BX \in \mathbb{R}^d$) -> Kernel Layer (Kernel computation $K_{i,j} = k(z_i, z_j)$) -> Prediction Layer (Linear combination $\sum \alpha_i k(z_i, \cdot)$)

- **Critical path:**
  1. Initialize $B$ (often via sampling or cross-validation) and select Nyström inducing points
  2. Alternate Optimization:
     - Step A: Update $B$ using gradient descent on the regularized loss
     - Step B: Update $\alpha$ (either analytically for VarPro or via GD for AGD)
  3. Validate to tune latent dimension $d$ and regularization $\lambda$

- **Design tradeoffs:**
  - Latent Dimension ($d$): Underestimating $d$ ($d < d^*$) severely hurts accuracy. Overestimating ($d > d^*$) is robust but increases computational cost
  - Algorithm Choice: VarPro is faster per iteration but gets stuck in local minima; AGD is slower per iteration but robust to local minima
  - Inducing Points ($\tilde{m}$): Larger $\tilde{m}$ improves accuracy but linearly increases memory and computation time

- **Failure signatures:**
  - Vanishing Gradients / Stagnation: VarPro iterations stabilize at a high loss value (stuck in local minimum). Fix: Switch to AGD or re-initialize $B$
  - Numerical Instability: $\alpha$ coefficients explode during updates. Fix: Increase regularization $\lambda$ or check the kernel scale parameter $\gamma$
  - Slow Convergence: AGD takes excessive iterations. Fix: Use a backtracking line search or adapt learning rates

- **First 3 experiments:**
  1. Synthetic MIM Validation: Generate data via $y = \sin(B^* x) + \epsilon$ with known $d^*$. Run HKRR with $d=d^*$ to verify recovery of the latent subspace (check if $B \approx B^*$ up to rotation)
  2. Optimizer Comparison (VarPro vs. AGD): On a 2D toy problem, visualize optimization trajectories to observe VarPro getting stuck in local minima while AGD converges globally
  3. Sensitivity Analysis: Test performance as $d$ varies ($d < d^*$, $d = d^*$, $d > d^*$). Confirm the paper's finding that overparameterization is safer than underestimation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bound for HKRR be tightened to remove the suboptimal factor of 2 in the latent dimension term?
- Basis in paper: [explicit] Remark 4 states that Theorem 1 yields a bound with an extra factor of 2 compared to the conjectured optimal rate, noting that "sharper analysis... is left for future work."
- Why unresolved: The current proof relies on $L^\infty$-based covering number bounds, which introduce this suboptimality
- What evidence would resolve it: A proof using $L^2$-norm estimates or local Rademacher complexity achieving the rate $m^{-2r/(2r+d^*)}$

### Open Question 2
- Question: Can HKRR be extended to provably learn more complex, hierarchical compositional functions beyond single-layer multi-index models?
- Basis in paper: [explicit] The conclusion states, "It would be especially interesting to consider more general forms of compositional functions beyond MIMs."
- Why unresolved: The current theoretical analysis is restricted to the specific compositional structure of Multi-Index Models (linear projection followed by a non-linear function)
- What evidence would resolve it: Theoretical derivations of sample complexity bounds for deep compositional structures or empirical validation on hierarchical datasets

### Open Question 3
- Question: Does the estimated latent dimension $\hat{d}$ systematically exceed the true dimension $d^*$, and can this overparameterization effect be theoretically characterized?
- Basis in paper: [inferred] Section 4.3 notes that experiments suggest the "conjecture that $\hat{d} > d^*$" because overparameterization often yields better results, though Theorem 3 only guarantees adaptivity up to the true dimension
- Why unresolved: The current theory shows the method adapts to the true dimension but does not explain the empirical observation that selecting $d > d^*$ can improve performance
- What evidence would resolve it: A theoretical analysis of the optimization landscape showing preferential convergence to solutions in higher dimensions or a generalization bound that decreases with moderate overparameterization

## Limitations
- The theoretical guarantees rely heavily on the Multi-Index Model assumption, which may not hold for many real-world datasets where the relevant low-dimensional structure is nonlinear rather than linear
- The paper demonstrates success on synthetic data but provides limited empirical validation on real-world datasets, leaving uncertainty about practical applicability
- The convergence analysis assumes analytic kernels and does not address potential issues with non-smooth or piecewise-continuous target functions
- Computational complexity remains high for large datasets even with Nyström approximations, particularly when the latent dimension is overestimated

## Confidence
- High confidence: The theoretical framework connecting HKRR to MIMs and the sample complexity analysis showing exponential dependence on latent dimension rather than ambient dimension
- Medium confidence: The empirical advantage of AGD over VarPro based on synthetic experiments, though real-world performance remains untested
- Medium confidence: The robustness claims regarding overestimation of latent dimension, which need validation on more diverse datasets

## Next Checks
1. Test HKRR on real-world datasets with known low-dimensional structure (e.g., image data with intrinsic low-dimensional manifolds) to verify if the linear projection assumption holds in practice
2. Compare HKRR performance against state-of-the-art deep learning methods on high-dimensional regression tasks where the true dimensionality is unknown
3. Conduct ablation studies systematically varying the kernel type (non-analytic kernels) and target function smoothness to test the robustness of theoretical guarantees beyond the analytic kernel assumption