---
ver: rpa2
title: A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier
  Models
arxiv_id: '2502.15959'
source_url: https://arxiv.org/abs/2502.15959
tags:
- student
- image
- tumor
- teacher
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge distillation (KD) approach to improve
  the transparency of CNN-based medical image classifiers. The method distills knowledge
  from a complex DenseNet121 teacher model into a simpler student CNN with fewer layers.
---

# A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier Models

## Quick Facts
- arXiv ID: 2502.15959
- Source URL: https://arxiv.org/abs/2502.15959
- Reference count: 15
- One-line primary result: Student CNN maintains F1 >0.94 on medical datasets with 5× interpretability speedup

## Executive Summary
This paper proposes a knowledge distillation approach to improve the transparency of CNN-based medical image classifiers. The method distills knowledge from a complex DenseNet121 teacher model into a simpler student CNN with fewer layers. After KD, the student model retains most key features and is used for layer-by-layer interpretability analysis via average feature maps. Experiments on three medical datasets (brain tumor, eye disease, Alzheimer's) show that the student model maintains high accuracy (F1 scores >0.94) and provides faster interpretability (up to 5× speed-up) compared to traditional methods like Grad-CAM and SHAP. Fidelity scores confirm reliable explanations.

## Method Summary
The approach uses DenseNet121 as a teacher model (pretrained on ImageNet) and a custom 5-layer CNN as a student. Knowledge is distilled using a combined loss function that balances hard label cross-entropy with soft label KL divergence. Temperature scaling is applied to the teacher's output to create softer probability distributions. After training, the student model's feature maps are averaged across filters for each layer to create interpretable visualizations. The method aims to maintain classification accuracy while enabling faster and more transparent decision-making.

## Key Results
- Student models achieve F1 scores >0.94 across all three medical datasets
- Average feature maps provide 5× speed-up in interpretability analysis (MET ~15s vs ~68s)
- Computational complexity reduced from 566.89M FLOPs (teacher) to 232.70M FLOPs (student)
- Fidelity scores range from 0.8630 to 0.9764 across XAI methods, validating explanation reliability

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation Compresses Representations While Preserving Class Boundaries
The student model learns from both hard labels (ground truth via cross-entropy) and soft labels (teacher's output probabilities via KL divergence). The combined loss L_distill = α·ℓ_HL + (1-α)·ℓ_SL balances direct supervision with behavioral mimicry. Temperature scaling softens the teacher's output distribution, exposing inter-class relationships that pure one-hot labels hide.

### Mechanism 2: Average Feature Maps Provide Layer-wise Transparency Without Gradient Computation
For a layer with N filters, the average feature map A(i,j) = (1/N) Σ F_k(i,j) aggregates activations spatially. This bypasses gradient-based attribution (Grad-CAM) or Shapley value computation (SHAP), reducing MET from ~67s to ~15s for SHAP-style analysis.

### Mechanism 3: Temperature Parameter Controls Knowledge Transfer Granularity
Softmax_T(z_i) = e^(z_i/T) / Σ e^(z_j/T). Higher T reduces probability differences between classes, revealing the teacher's uncertainty patterns. This guides the student to learn not just "what class" but "how similar to other classes."

## Foundational Learning

- **Knowledge Distillation Basics (Teacher-Student Framework)**: The entire approach hinges on understanding how soft labels transfer learned representations from complex to simple models.
  - Quick check question: Given a 4-class problem where teacher outputs [0.7, 0.2, 0.08, 0.02] and ground truth is class 0, what additional information does the soft label provide compared to one-hot [1,0,0,0]?

- **Temperature Scaling in Softmax**: Temperature T is a critical hyperparameter controlling knowledge transfer quality; improper selection caused 5% accuracy drops in the Eye disease experiments.
  - Quick check question: If T=1 produces softmax output [0.7, 0.2, 0.08, 0.02], what happens to these values when T=10? Will the distribution become sharper or softer?

- **Feature Map Interpretation**: The paper's transparency mechanism relies on reading average feature maps as indicators of model attention; without this, the "explainability" claim cannot be validated.
  - Quick check question: A convolutional layer has 64 filters producing 64 feature maps. After averaging, you see high activation in the top-left quadrant of the image. What does this tell you about the model's decision process, and what information might averaging have discarded?

## Architecture Onboarding

- **Component map**: DenseNet121 (teacher) -> 5-layer CNN (student) -> Average feature maps -> Interpretability visualization

- **Critical path**:
  1. Train teacher (DenseNet121) on medical dataset → obtain P_teacher
  2. Initialize student (5-layer CNN) with random weights
  3. For each training batch: compute teacher output y_T, student output y_S, calculate L_distill, update student via backprop
  4. Grid search over α ∈ {0.4, 0.7} and T ∈ {5, 10, 15} to find optimal student configuration
  5. Extract and average feature maps from all 5 student layers for interpretability analysis

- **Design tradeoffs**:
  - Max pooling (student) vs. average pooling (teacher): Max pooling captures prominent features but may lose spatial context
  - Layer count: 5 layers chosen empirically; fewer layers may fail on complex datasets (Eye disease), more layers reduce interpretability speedup
  - Fidelity vs. Speed: The method trades some fidelity (0.8630 vs 0.9059 on Eye disease) for 5× speed improvement

- **Failure signatures**:
  - Student accuracy >5% below teacher → architecture too shallow, add 1-2 convolutional layers
  - Fidelity score <0.85 on specific classes → average feature maps not capturing class-discriminative regions; consider weighted averaging or attention mechanisms
  - High variance across temperature settings → dataset may have conflicting inter-class relationships; investigate per-class temperature tuning

- **First 3 experiments**:
  1. **Baseline replication**: Train DenseNet121 teacher on brain tumor dataset, then distill to 5-layer student with α=0.7, T=10. Verify F1 >0.97 and MET reduction >4×. Compare feature map visualizations against Grad-CAM on 20 held-out samples.
  2. **Temperature sensitivity analysis**: On Eye disease dataset, run grid search T ∈ {3, 5, 7, 10, 15, 20} with α fixed at 0.4. Plot accuracy vs. T to identify optimal zone and characterize the "noise injection" regime where accuracy degrades.
  3. **Fidelity benchmark**: Compute fidelity scores for all three XAI methods (Grad-CAM, SHAP, average feature maps) across 100 samples per class. Use paired t-tests to determine if differences are statistically significant. If average feature maps underperform on >2 classes, implement weighted averaging based on filter activation magnitude.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the classification performance gap observed in the Eye Disease dataset (approx. 5% drop) be mitigated without increasing the complexity of the student model?
  - Basis in paper: The results section notes the student model demonstrated lower accuracy compared to the teacher on the Eye Disease dataset, stating, "This suggests that the use of shallow networks is limited."
  - Why unresolved: The paper identifies this accuracy trade-off as a limitation of the shallow architecture but does not propose architectural or loss-function modifications to recover the lost accuracy on this specific dataset.

- **Open Question 2**: Does the layer-by-layer average feature map visualization improve diagnostic decision-making compared to standard single-layer heatmaps?
  - Basis in paper: The abstract claims the goal is to "help clinicians better understand and trust" the process. However, the evaluation relies solely on Fidelity scores and computational time (MET), not on human-in-the-loop validation.
  - Why unresolved: Algorithmic transparency metrics (Fidelity) do not necessarily correlate with cognitive utility for medical professionals; it is unclear if the averaged feature maps are semantically meaningful to doctors.

- **Open Question 3**: Is the proposed distillation and averaging method effective for high-dimensional 3D medical imaging volumes?
  - Basis in paper: The study tests exclusively on 2D datasets (MRI slices and Fundus photography). The conclusion claims general applicability to "medical image analysis," but the method relies on simpler 2D spatial features.
  - Why unresolved: 3D volumetric data contains complex spatial relationships and significantly higher feature dimensionality, which may cause the simple "averaging" strategy to obscure critical depth-wise anatomical features.

## Limitations

- The average feature map interpretability method lacks validation against established gradient-based approaches in the corpus
- The optimal temperature parameter appears dataset-dependent with no clear theoretical guidance
- The 5-layer student architecture's capacity limits are demonstrated by the 5% accuracy gap on Eye disease classification

## Confidence

- **High confidence**: Knowledge distillation preserves accuracy (F1 > 0.94) and reduces computational cost (5× speedup)
- **Medium confidence**: Average feature maps provide reliable interpretability (fidelity scores mixed, 0.86-0.98 range)
- **Low confidence**: Temperature scaling recommendations generalize across medical imaging domains

## Next Checks

1. Replicate the Eye disease temperature sensitivity experiment with expanded T ∈ {3, 7, 12, 18} to characterize the degradation threshold more precisely
2. Conduct ablation study comparing average feature maps against Grad-CAM on per-class basis to identify failure modes
3. Test student architecture scalability by incrementally adding convolutional layers until accuracy gap <2% across all three datasets