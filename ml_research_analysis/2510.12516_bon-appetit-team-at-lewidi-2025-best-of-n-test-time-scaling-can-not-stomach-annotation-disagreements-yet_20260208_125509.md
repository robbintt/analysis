---
ver: rpa2
title: 'BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach
  Annotation Disagreements (Yet)'
arxiv_id: '2510.12516'
source_url: https://arxiv.org/abs/2510.12516
tags:
- dataset
- sampling
- response
- task
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Best-of-N sampling improves model performance on math problems
  but not on LeWiDi-2025 tasks with annotation disagreements. While test-time scaling
  methods like Model Averaging and Majority Voting consistently improved performance,
  Best-of-N sampling with step-wise scores did not.
---

# BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)

## Quick Facts
- arXiv ID: 2510.12516
- Source URL: https://arxiv.org/abs/2510.12516
- Authors: Tomas Ruiz; Siyao Peng; Barbara Plank; Carsten Schwemmer
- Reference count: 31
- Best-of-N sampling with step-wise scoring fails on LeWiDi-2025 annotation disagreement tasks

## Executive Summary
This paper evaluates test-time scaling methods on the LeWiDi-2025 shared task, which focuses on predicting annotator disagreement in natural language processing. While Model Averaging and Majority Voting consistently improve performance on soft-label prediction and perspectivist tasks, Best-of-N sampling with step-wise scores fails specifically on annotation disagreement datasets. The authors identify that LLMs generate vaguer reasoning steps for these tasks and use significantly less compute budget compared to mathematical reasoning, making judge discrimination unreliable. The BoN oracle shows high theoretical potential, suggesting implementation rather than fundamental limitations.

## Method Summary
The paper applies four test-time scaling methods to LeWiDi-2025 tasks: Simple Sampling (baseline), Model Averaging (aggregate N soft-label predictions), Majority Voting (most frequent label per annotator), and Best-of-N with step-wise scores (sample N with CoT, score each step using LLM-as-a-judge, select highest-scoring prediction). For soft-label tasks, Model Averaging averages N sampled distributions; for perspectivist tasks, Majority Voting takes the most frequent label per annotator. Best-of-N uses a Qwen3-32B reasoning model to generate N=10 samples with structured CoT steps, then scores each step using DeepSeek-R1-8B as judge with "great/okay/bad" labels, averaging step scores to select the best prediction.

## Key Results
- Model Averaging and Majority Voting consistently improve performance across all LeWiDi datasets and tasks
- Best-of-N sampling with step-wise scores fails on annotation disagreement tasks (no improvement over Simple Sampling)
- BoN oracle analysis shows high theoretical performance potential (Wasserstein distance as low as 0.02-0.10), indicating the method's failure is implementation-related
- Prediction diversity correlates with model performance and oracle gains, serving as a proxy for problem difficulty
- LLMs generate vaguer reasoning steps for LeWiDi tasks and use significantly less compute budget compared to mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
Model Averaging and Majority Voting improve soft-label prediction by aggregating diverse model samples, reducing variance without requiring correctness discrimination. Sampling N predictions and averaging (soft-label) or voting (perspectivist) creates an ensemble effect that smooths outlier predictions while preserving consensus signals. This works particularly well when prediction diversity correlates with problem difficulty.

### Mechanism 2
Best-of-N with step-wise scoring fails on annotation disagreement tasks because LLMs produce vague, under-specified reasoning steps that judges cannot reliably discriminate. Step-wise scoring requires discriminable steps—clear boundaries between "great," "okay," and "bad." For LeWiDi tasks, models generate generic reasoning (e.g., "Assess the likelihood of different annotator interpretations") that applies broadly, making judge scoring unreliable. This contrasts with math, where steps involve concrete calculations with verifiable intermediate results.

### Mechanism 3
Prediction diversity serves as a proxy for problem difficulty and predicts test-time scaling effectiveness—higher diversity enables greater potential gains from aggregation and selection methods. Diversity (pairwise Wasserstein distance between N predictions) captures model uncertainty. Low diversity indicates the model perceives the problem as unambiguous (easy or single-interpretation); high diversity indicates multiple plausible interpretations exist.

## Foundational Learning

- **Soft-label evaluation metrics (Wasserstein/Manhattan distance)**: Understanding why these are used instead of accuracy/cross-entropy is essential for interpreting all results. *Quick check*: Why is Wasserstein distance more appropriate than cross-entropy for Likert-scale soft-labels?

- **Process Reward Models (PRMs) vs Outcome Reward Models (ORMs)**: The paper's step-wise scoring is a PRM approach; understanding this distinction clarifies why the method works in math but fails here. *Quick check*: What makes a PRM more sample-efficient than an ORM for mathematical reasoning?

- **Annotation disagreement as signal (perspectivism)**: The paper's core premise—that disagreement is informative rather than noise—differs from traditional single-ground-truth assumptions. *Quick check*: How does the perspectivist approach change model training compared to majority-vote labeling?

## Architecture Onboarding

- **Component map**: Reasoning LLM (Qwen3-32B) -> Judge LLM (DeepSeek-R1-8B) -> Scoring reducer (mean) -> Selector
- **Critical path**: Prompt engineering → Structured output parsing → Judge scoring → Score aggregation → Final selection. The breakage occurs at step 3: judge scoring fails on vague LeWiDi reasoning steps.
- **Design tradeoffs**: Mean vs product reduction (paper chose mean over Lightman et al.'s product; mean is more forgiving of single bad steps but less discriminative). N=10 samples (standard for BoN, but paper shows increasing N doesn't help when judge quality is the bottleneck). Same-family vs cross-family judge (paper uses different model families to reduce bias, but both struggle on LeWiDi).
- **Failure signatures**: Flat or worsening performance as N increases, judge model variance (DeepSeek consistently worse than Qwen3 judge on MP dataset), vague step generation (steps like "Assess the likelihood" that could apply to any problem).
- **First 3 experiments**: 1) Replicate Model Averaging baseline on one LeWiDi dataset to establish that test-time scaling can work in this domain. 2) Run BoN oracle analysis to measure theoretical ceiling—if oracle shows strong gains but implemented BoN doesn't, the issue is judge/scoring, not sampling diversity. 3) Compare CoT step specificity between math (PRM800K/AIME) and LeWiDi tasks using token count and qualitative inspection to validate the vagueness hypothesis.

## Open Questions the Paper Calls Out

- Can incorporating annotation disagreement tasks into the post-training recipe of reasoning LLMs improve Best-of-N sampling performance on subjective NLP tasks? The authors hypothesize this based on observing that Qwen3 was post-trained on "math, code, logical reasoning, and general STEM problems" but not on perspectivist tasks, but do not test this.

- Can step-wise scoring be adapted with perspectivist-oriented labels (e.g., "plausible," "implausible," "vague") to improve BoN sampling on tasks with interpretative variation? Current labels ("great," "okay," "bad") are borrowed from math reasoning; the authors propose perspectivist alternatives but do not implement or test them.

- Does the correlation between prediction diversity and problem difficulty generalize to other annotation disagreement datasets beyond LeWiDi? The correlation was demonstrated only on four LeWiDi datasets; generalization to other disagreement datasets is untested.

## Limitations

- The paper cannot definitively prove whether Best-of-N failure stems from inherent difficulty in judging vague reasoning steps about interpretation tasks, insufficient judge model capacity, or suboptimal prompt engineering for the judge.

- The diversity-proxy-for-difficulty hypothesis remains correlational; the relationship between diversity and oracle gains could be driven by multiple mechanisms (model uncertainty, prompt sensitivity, task ambiguity) that the paper doesn't disentangle.

- The qualitative analysis shows judges assign vague labels like "Assess the likelihood" to steps, but doesn't establish whether better prompts or judge models could resolve this.

## Confidence

- **High Confidence**: Model Averaging and Majority Voting consistently improve performance across all datasets and tasks. Directly supported by Table 2 and Figure 3.
- **Medium Confidence**: Best-of-N with step-wise scoring fails on LeWiDi tasks due to vague reasoning steps that judges cannot discriminate. Supported by qualitative analysis and oracle results, but causal link is inferred.
- **Low Confidence**: The diversity-proxy-for-difficulty hypothesis represents fundamental understanding of test-time scaling effectiveness. Empirically supported but correlation vs causation not established.

## Next Checks

1. **Judge Model Ablation**: Test Best-of-N with step-wise scoring using different judge models (Qwen3-32B, Claude-3.5-Sonnet, GPT-4o) and with and without step-wise scoring to isolate whether the failure is due to judge quality or the step-wise scoring approach itself.

2. **Step Specificity Intervention**: Modify the reasoning prompt to explicitly require specific, verifiable intermediate steps (e.g., "list three concrete indicators of sarcasm" rather than "assess likelihood of interpretations") and measure whether this improves judge discrimination and Best-of-N performance.

3. **Diversity-Threshold Analysis**: Plot performance gains vs. diversity for each dataset to identify whether there's a minimum diversity threshold required for test-time scaling to be effective, and whether LeWiDi datasets fall below this threshold due to task characteristics rather than implementation issues.