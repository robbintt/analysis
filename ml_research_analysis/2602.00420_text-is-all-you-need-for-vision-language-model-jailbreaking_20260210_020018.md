---
ver: rpa2
title: Text is All You Need for Vision-Language Model Jailbreaking
arxiv_id: '2602.00420'
source_url: https://arxiv.org/abs/2602.00420
tags:
- text-dj
- what
- queries
- harmful
- distraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Text-DJ, a jailbreak attack that bypasses LVLM safety
  filters by exploiting OCR capabilities. Our method decomposes harmful queries into
  benign sub-queries, adds maximally irrelevant distraction queries, and presents
  them as images in a grid.
---

# Text is All You Need for Vision-Language Model Jailbreaking

## Quick Facts
- arXiv ID: 2602.00420
- Source URL: https://arxiv.org/abs/2602.00420
- Reference count: 40
- Key outcome: Text-DJ jailbreak attack bypasses LVLM safety filters by converting harmful text prompts into fragmented visual inputs using OCR and distraction strategies

## Executive Summary
This paper introduces Text-DJ, a novel jailbreak attack that exploits vision-language models' (LVLMs) optical character recognition (OCR) capabilities to bypass safety filters. The attack decomposes harmful queries into benign sub-queries, presents them as images in a grid format, and adds maximally irrelevant distraction queries to prevent semantic linking. By converting text-based prompts into visual inputs, the attack evades text-based safety filters while leveraging the LVLM's OCR to reconstruct the original harmful intent. The method demonstrates high attack success rates across multiple harmful categories on state-of-the-art LVLMs including Qwen3-VL, GPT 4.1 mini, and Gemini.

## Method Summary
The Text-DJ attack operates through a three-stage process. First, harmful queries are decomposed into semantically disconnected sub-queries using LLMs to identify distinct semantic segments. Second, maximally irrelevant distraction queries are generated to prevent semantic linking between sub-queries. Third, both sub-queries and distraction queries are converted into images and arranged in a grid format. The attack employs colorization randomization to further obfuscate the semantic connections. When the LVLM processes the image grid, its OCR capability extracts the text fragments, which can then be semantically linked to reconstruct the original harmful query, bypassing the model's safety filters that would otherwise block the text prompt directly.

## Key Results
- High attack success rates achieved across multiple harmful categories including violence, pornography, and hate speech
- Demonstrated effectiveness on state-of-the-art LVLMs including Qwen3-VL, GPT 4.1 mini, and Gemini
- Ablation studies confirm that cross-modal transformation, semantic distraction strategy, and randomized colorization are critical to attack success

## Why This Works (Mechanism)
The attack exploits a fundamental vulnerability in LVLMs: the disconnect between text-based safety filters and visual input processing. While text-based safety filters effectively block harmful prompts, these same filters are bypassed when the harmful content is presented visually. The LVLM's OCR capability then reconstructs the text, allowing the model to process the harmful intent despite the visual presentation. The semantic distraction strategy prevents the model from linking sub-queries across the grid, while colorization randomization adds another layer of obfuscation. This cross-modal transformation reveals a critical safety alignment gap in LVLMs that treat text and visual inputs through different security paradigms.

## Foundational Learning

**Optical Character Recognition (OCR)**: Technology that extracts text from images. *Why needed*: Essential for LVLMs to process text within visual inputs, enabling the attack's core mechanism. *Quick check*: Verify OCR accuracy across different fonts, languages, and image qualities.

**Semantic Segmentation**: Process of dividing text into distinct semantic units. *Why needed*: Enables decomposition of harmful queries into benign sub-queries that evade detection. *Quick check*: Test whether semantic segmentation maintains the original harmful intent when reconstructed.

**Cross-Modal Safety Alignment**: Consistency of safety measures across text and visual inputs. *Why needed*: Reveals the vulnerability that Text-DJ exploits - different safety protocols for different input modalities. *Quick check*: Compare safety filter effectiveness between text prompts and equivalent visual prompts.

## Architecture Onboarding

**Component Map**: Harmful Query → LLM Decomposition → Semantic Segmentation → Sub-queries + Distraction Queries → Image Generation → Grid Layout → LVLM OCR → Text Reconstruction → Response Generation

**Critical Path**: The attack succeeds when OCR accurately extracts all sub-queries and the LVLM semantically links them to reconstruct the harmful intent despite distraction elements.

**Design Tradeoffs**: Higher fragmentation of queries increases safety evasion but risks OCR errors; more distraction queries improve safety but may overwhelm the LVLM's processing capacity.

**Failure Signatures**: Attack fails when OCR misreads text fragments, when semantic distraction is insufficient to prevent linking, or when the LVLM's safety filters catch the reconstructed harmful intent during response generation.

**First Experiments**: 1) Test OCR accuracy with varying text qualities and layouts; 2) Evaluate semantic distraction effectiveness with different query types; 3) Measure attack success rates across LVLMs with different OCR capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness depends heavily on LVLM OCR accuracy, which varies across models and input quality
- Study focuses primarily on image-grid-based attacks, potentially missing other visual prompt injection vectors
- Paper does not address potential defenses or mitigation strategies for developers

## Confidence
- **High**: Cross-modal transformation successfully evades text-based safety filters
- **Medium**: Semantic distraction strategy prevents semantic linking of sub-queries
- **Medium**: Randomized colorization contributes significantly to attack success

## Next Checks
1. Test attack robustness across varying OCR quality levels and font types to assess real-world applicability
2. Evaluate whether alternative visual prompt injection methods (beyond grid formats) achieve similar success rates
3. Assess attack performance on LVLM models with different OCR capabilities and safety training paradigms