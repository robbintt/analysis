---
ver: rpa2
title: Sentiment Classification of Thai Central Bank Press Releases Using Supervised
  Learning
arxiv_id: '2503.22629'
source_url: https://arxiv.org/abs/2503.22629
tags:
- text
- sentiment
- best
- bank
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applies supervised machine learning to classify the\
  \ sentiment of Thai central bank press releases, using Naive Bayes, SVM, and Random\
  \ Forest models. The dataset consists of 794 pre-labeled sentences from 26 MPC press\
  \ releases spanning 2021\u20132024."
---

# Sentiment Classification of Thai Central Bank Press Releases Using Supervised Learning

## Quick Facts
- arXiv ID: 2503.22629
- Source URL: https://arxiv.org/abs/2503.22629
- Reference count: 0
- This study applies supervised learning to classify sentiment in Thai central bank press releases

## Executive Summary
This paper presents a supervised learning approach to classify the sentiment of Thai central bank press releases using Naive Bayes, SVM, and Random Forest models. The study processes 794 pre-labeled sentences from 26 MPC press releases spanning 2021-2024, achieving a Macro-F1 score of 0.75 with Naive Bayes. The research demonstrates that traditional machine learning can effectively handle sentiment classification in financial communications, even with limited datasets, while highlighting the importance of expert annotation and domain-specific considerations in emerging markets.

## Method Summary
The study employs a supervised learning pipeline for multiclass sentiment classification (negative, neutral, positive) of Bank of Thailand MPC press release sentences. The process involves extracting text from PDFs, preprocessing with custom tokenization and TF-IDF vectorization (unigrams to trigrams), and training three models using 5-fold cross-validation optimized for macro-F1 scoring. Models include Multinomial Naive Bayes (alpha=0.1), SVM (C=10, gamma=0.01, linear kernel), and Random Forest (100 trees, max depth 30). Data is split 80/20 with stratification to maintain class balance.

## Key Results
- Naive Bayes achieved highest performance with Macro-F1 score of 0.75
- SVM achieved Macro-F1 score of 0.71
- Random Forest performed worst with Macro-F1 score of 0.62
- Minority class (negative) comprised 21.3% of the dataset

## Why This Works (Mechanism)

### Mechanism 1
Naive Bayes is optimal for this task because the dataset size (N=794) is insufficient to train complex models like Random Forest without overfitting. Naive Bayes acts as a high-bias classifier that assumes feature independence, preventing it from fitting noise in TF-IDF vectors, whereas Random Forest attempts complex decision boundaries that fail to generalize.

### Mechanism 2
Macro-averaged F1 scoring drives optimization toward the minority class (Negative sentiment), forcing the model to recognize subtle linguistic signals rather than just predicting the majority class. By optimizing hyperparameters specifically for `scoring='f1_macro'`, the training process penalizes models that ignore the minority class (21.3% of data).

### Mechanism 3
TF-IDF with n-grams (unigrams to trigrams) captures the specific "financial syntax" of central bank communications better than simple bag-of-words. Central bank language relies on context where phrases like "inflation is contained" have different sentiment than individual words, and trigrams preserve this short-range sequential context.

## Foundational Learning

- **Stratified K-Fold Cross-Validation**: Why needed? Small dataset (N=794) and imbalanced classes risk validation folds having zero negative samples. Quick check: If you split randomly without stratification, what is the statistical risk regarding the distribution of the "Negative" class in the test set?

- **Bias-Variance Tradeoff in Small Data**: Why needed? Explains why Random Forest failed (Macro-F1 0.62) while Naive Bayes succeeded (0.75). Quick check: Why would adding more trees to a Random Forest not necessarily improve performance on a dataset with fewer than 1,000 samples?

- **Text Preprocessing (Lemmatization & Stop-word Removal)**: Why needed? Financial text contains archaic structures requiring custom tokenization. Quick check: Why might removing standard English stop words (e.g., "up", "down") be risky in financial sentiment context?

## Architecture Onboarding

- **Component map**: PDF extraction -> Text cleaning -> AI-assisted labeling -> Human validation -> TF-IDF vectorization -> GridSearchCV training -> Classification report

- **Critical path**: The Annotation Strategy. Expert revision is the bottleneck; if rushed, the "Ground Truth" becomes noisy, rendering Macro-F1 comparison invalid.

- **Design tradeoffs**: 
  - Complexity vs. Data Volume: Simpler model (NB) chosen because data volume couldn't support Random Forest's variance
  - Accuracy vs. Speed: Binary TF-IDF (presence/absence only) ignores term frequency counts to prioritize speed and reduce noise

- **Failure signatures**:
  - RF Low Recall: Random Forest showed recall of 0.44 for Negative class
  - Ambiguity in Neutral: Neutral class F1-score (0.71 in NB) is lower than Positive/Negative

- **First 3 experiments**:
  1. Baseline Reproduction: Run provided code to verify 0.75 Macro-F1 score on hold-out set
  2. Ablation on N-grams: Retrain with only unigrams to quantify performance drop from losing trigram context
  3. Class Balance Test: Implement Random Forest with `class_weight='balanced'` to rescue Macro-F1 above 0.62

## Open Questions the Paper Calls Out

- Can supervised learning models effectively classify sentiment in Thai-language central bank press releases given language-specific challenges?
- To what extent do Large Language Models (LLMs) outperform traditional supervised classifiers like Naive Bayes in analyzing central bank communications?
- Does the reliance on semi-automated labeling process (ChatGPT plus single-author validation) introduce systematic bias compared to traditional multi-expert annotation?

## Limitations
- Dataset size of 794 labeled sentences may not fully capture central bank communication complexity
- AI-assisted labeling with human validation introduces potential bias without inter-annotator agreement metrics
- Results may not transfer to other financial institutions or markets without validation

## Confidence
- High confidence: Naive Bayes outperforming SVM/Random Forest on this specific dataset size
- Medium confidence: Macro-F1 as optimal evaluation metric for this task
- Medium confidence: TF-IDF with n-grams capturing sufficient context for central bank sentiment

## Next Checks
1. Request access to labeled CSV file or document full manual labeling process to enable reproduction
2. Test whether balanced class weights improve Random Forest performance, specifically targeting minority class recall
3. Gradually increase dataset size through additional MPC releases to test whether SVM/Random Forest eventually surpass Naive Bayes as sample size grows