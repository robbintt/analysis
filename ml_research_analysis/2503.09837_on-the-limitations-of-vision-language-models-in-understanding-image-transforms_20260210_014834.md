---
ver: rpa2
title: On the Limitations of Vision-Language Models in Understanding Image Transforms
arxiv_id: '2503.09837'
source_url: https://arxiv.org/abs/2503.09837
tags:
- image
- clip
- augmented
- augmentation
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fundamental limitations of Vision-Language
  Models (VLMs) like CLIP and SigLIP in understanding basic image transformations.
  Through systematic evaluation using an augmented Flickr8k dataset with 24 transformation
  types, the study reveals that VLMs struggle with comprehending simple image modifications
  such as rotation, brightness/contrast adjustments, and geometric distortions.
---

# On the Limitations of Vision-Language Models in Understanding Image Transforms

## Quick Facts
- arXiv ID: 2503.09837
- Source URL: https://arxiv.org/abs/2503.09837
- Reference count: 40
- Primary result: VLMs achieve high matching accuracy (98-99%) but fail classification (<4%) for basic image transformations

## Executive Summary
This paper investigates fundamental limitations of Vision-Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Through systematic evaluation using an augmented Flickr8k dataset with 24 transformation types, the study reveals that VLMs struggle with comprehending simple image modifications such as rotation, brightness/contrast adjustments, and geometric distortions. While models show reasonable performance in matching augmented images with their descriptions (achieving 98-99% accuracy for CLIP variants), they fail significantly in direct classification tasks, with top-1 accuracy below 4% across all models. The findings suggest that the invariance property of VLMs, while beneficial for semantic understanding, comes at the cost of explicit spatial reasoning capabilities, highlighting the need for new training paradigms that balance invariance with transformation awareness.

## Method Summary
The paper evaluates CLIP and SigLIP models on understanding 24 image transformations using three experimental approaches. The first experiment tests whether augmented images are better matched with augmented captions versus original captions. The second experiment tests whether augmented images are better matched with augmented captions versus original captions. The third experiment directly classifies the transformation type from the image alone. Evaluation uses the Flickr8k dataset with transformations including rotations, flips, color adjustments, and geometric distortions. The study employs three CLIP variants (ViT-B/32, ViT-B/16, ViT-L/14) and two SigLIP models (Base-224, Base-256 Multilingual), measuring cosine similarity between image and text embeddings.

## Key Results
- VLMs achieve 98-99% accuracy in matching augmented images with augmented captions
- Direct classification accuracy for transformation type is below 4% across all models
- Similarity score differences between correct and incorrect matches are minimal (0.02-0.06)
- Top-1 accuracy for most individual augmentations is near 0% in classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Invariance-Understanding Trade-off in Contrastive Learning
VLMs' contrastive training creates semantic invariance that actively suppresses transformation-specific signals. Contrastive learning objectives push semantically similar image-text pairs together regardless of augmentation, pooling representations and discarding explicit spatial/transform information as "noise" to be invariant to. This structural trade-off is inherent to current training paradigms rather than a fixable bug.

### Mechanism 2: Weak Discriminative Signal in Embedding Space
Even when models "correctly" match augmented images to descriptions, the underlying signal is marginally above noise. Experiment 2 shows 98-99% accuracy for CLIP, but Figure 6 reveals mean similarity differences of only 0.02-0.06 between correct and incorrect matches. The embedding space lacks strong discriminative features for transforms, causing classification failure (<4% accuracy) due to insufficient representational separation.

### Mechanism 3: Semantic Dominance Over Spatial/Structural Features
Vision encoders prioritize object-level and scene-level semantics over pixel-level or geometric relationships. ViT patch-based processing averages spatial relationships, and global pooling further dilutes local structure. The model "sees" what's in the image, not how pixels are arranged, reflecting an architectural feature rather than training alone.

## Foundational Learning

- **Contrastive Language-Image Pre-training (CLIP-style training)**: Essential for understanding how contrastive loss creates invariance; without this, the results seem contradictory (good matching, terrible classification). Quick check: Can you explain why maximizing similarity between "a dog" caption and rotated dog images during training might hurt rotation classification later?

- **Embedding Space Geometry**: Required to understand weak similarity score differences (0.02-0.06) and classification failures in relation to cosine similarity and discriminative power. Quick check: If two embeddings have cosine similarity 0.85 vs. 0.82, is this a reliable difference for classification over 27 classes?

- **Vision Transformer (ViT) Patch Processing**: Important for understanding why spatial structure is lost when testing ViT variants (B/32, B/16, L/14). Quick check: How does splitting an image into 16×16 patches and then averaging tokens affect precise spatial information like "rotated 45°"?

## Architecture Onboarding

- **Component map**: Image → patches → linear projection → position embeddings → Transformer layers → [CLS] token or mean pooling → L2 normalize → image embedding. Text → tokens → Transformer → pooled → L2 normalize → text embedding. Shared Embedding Space: L2-normalized outputs from both encoders.

- **Critical path**: Image → patches → linear projection → position embeddings → Transformer layers process patch tokens → [CLS] token or mean pooling → L2 normalize → image embedding. Text → tokens → Transformer → pooled → L2 normalize → text embedding. Cosine similarity computes alignment score.

- **Design tradeoffs**: Invariance vs. Explicit Spatial Awareness (current design favors robustness over transformation understanding), Global Pooling vs. Spatial Preservation (pooling discards patch-level spatial relationships), Scale (B/32 vs. L/14) (larger models slightly improve on some transforms, but fundamental limitation remains).

- **Failure signatures**: High matching accuracy (98%+) with near-zero classification accuracy (<4%) indicates embedding space has weak transformation discriminability. Per-augmentation accuracy near 0% for rotations, flips, and most color transforms shows no single transform is reliably encoded. Small similarity score differences (0.02-0.06) despite high accuracy indicates models are barely above threshold.

- **First 3 experiments**:
  1. Reproduce Experiment 1 on your own VLM: For 100 augmented image-caption pairs, compute whether `sim(I_aug, C_aug) > sim(I_orig, C_aug)`. Expect ~40-50% accuracy.
  2. Measure similarity score gaps: For Experiment 2-style matching, compute not just accuracy but the raw similarity difference distribution. Confirm if it's <0.1 for most samples.
  3. Test a single transform classification: Pick one transform (e.g., "rotate 90 degrees"), generate 50 augmented images, and check if the model ranks "rotated 90 degrees" higher than 26 other transform descriptions. Expect near-random performance.

## Open Questions the Paper Calls Out

- **How can VLM training paradigms be redesigned to balance semantic invariance with explicit transformation awareness?**: The conclusion explicitly calls for "newer training paradigms for Vision Language Models that balance invariance with explicit transformation awareness." Current contrastive learning objectives prioritize semantic robustness over low-level visual fidelity, causing the observed failures in recognizing transforms like rotation or color shifts.

- **Why is there a significant discrepancy between high matching accuracy (98-99%) and near-zero classification accuracy (<4%) for image transforms?**: The paper highlights a stark contrast between Experiment 2 (associating augmented images with augmented captions) and Experiment 3 (classifying the transform from the image alone), but does not explain the mechanistic cause of this gap. It's unclear if the "matching" success is due to genuine understanding or if the text encoder dominates the similarity score while the vision encoder remains "blind" to the specific transform.

- **Can Image2Image models be decoupled from the limitations of their VLM backbones to execute precise geometric instructions?**: The authors note that state-of-the-art editing models (Instruct Pix2Pix, DALL-E 3) fail on simple instructions like "Rotate 90 degrees" and emphasize the need to address these limitations to bridge the gap with traditional editing tools. It's unknown if the failure is purely due to the VLM backbone's inability to encode the instruction, or if the generative diffusion process itself lacks the inductive bias to perform rigid geometric manipulations.

## Limitations

- The evaluation uses a single dataset (Flickr8k), limiting generalizability to other domains or larger-scale datasets.
- The study focuses exclusively on CLIP-style models, leaving open questions about whether these limitations extend to newer architectures or different training regimes.
- The paper doesn't explore whether fine-tuning on transformation-aware tasks could mitigate these limitations, treating the observed behavior as fundamental rather than potentially addressable through training modifications.

## Confidence

- **High Confidence**: The experimental results showing low classification accuracy (<4%) and weak discriminative signals (similarity differences <0.1) are robust and well-documented.
- **Medium Confidence**: The interpretation that contrastive training's invariance property directly causes these limitations is mechanistically plausible but not definitively proven.
- **Medium Confidence**: The claim about downstream application implications (image editing failures) is logically consistent with the findings but not empirically validated in the paper.

## Next Checks

1. Test whether similar transformation understanding limitations appear in non-ViT architectures (e.g., ConvNeXt-based VLMs) or newer models like Flamingo that incorporate spatial attention mechanisms.

2. Systematically vary the transformation description prompts (syntax, specificity, context) to determine if performance improvements suggest the limitation is partially prompt-dependent rather than purely architectural.

3. Train a subset of VLMs on a small auxiliary dataset where augmented images are explicitly paired with transformation labels, then re-evaluate classification performance to test whether this limitation is truly fundamental or training-data-dependent.