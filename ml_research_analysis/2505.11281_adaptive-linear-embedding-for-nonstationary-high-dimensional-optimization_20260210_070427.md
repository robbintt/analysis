---
ver: rpa2
title: Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization
arxiv_id: '2505.11281'
source_url: https://arxiv.org/abs/2505.11281
tags:
- embedding
- rembo
- optimization
- random
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Adaptive Cross Embedding REMBO (SA-cREMBO),
  a novel framework that extends REMBO to support multiple random Gaussian embeddings,
  each capturing a different local subspace structure of the high-dimensional objective.
  The method jointly models an embedding index variable with the latent optimization
  variable via a product kernel in a Gaussian Process surrogate, enabling adaptive
  embedding selection conditioned on location.
---

# Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization

## Quick Facts
- arXiv ID: 2505.11281
- Source URL: https://arxiv.org/abs/2505.11281
- Authors: Yuejiang Wen; Paul D. Franzon
- Reference count: 16
- This paper introduces SA-cREMBO, extending REMBO with multiple random Gaussian embeddings jointly modeled via a product kernel in a GP surrogate, achieving superior sample efficiency on nonstationary high-dimensional benchmarks.

## Executive Summary
This paper presents Self-Adaptive Cross Embedding REMBO (SA-cREMBO), a framework that extends REMBO by supporting multiple random Gaussian embeddings, each capturing different local subspace structures of high-dimensional objectives. The method introduces a discrete embedding index variable jointly modeled with the latent optimization variable via a product kernel in a Gaussian Process surrogate. This enables adaptive embedding selection conditioned on location, effectively capturing locally varying effective dimensionality, nonstationarity, and heteroscedasticity in the objective landscape.

Theoretical analysis of the index-conditioned product kernel demonstrates expressiveness and stability, while experiments on synthetic and real-world high-dimensional benchmarks show significant advantages over traditional REMBO and other low-rank BO methods. The results establish SA-cREMBO as a powerful and flexible extension for scalable BO in complex, structured design spaces.

## Method Summary
SA-cREMBO extends REMBO by using multiple random Gaussian embedding matrices A₁, ..., A_K ∈ R^(D×d) and their orthogonal complements. The method jointly models a discrete embedding index z ∈ {1, ..., K} with the latent optimization variable x via a product kernel K((xi, zi), (xj, zj)) = Kx(xi, xj) · Kz(zi, zj) in a single GP surrogate. This enables information sharing across embeddings and adaptive selection based on location. The method uses orthogonal cross-embedding to complement random projections, expanding subspace coverage without additional random draws. Acquisition optimization is performed over the mixed continuous-discrete input space (x, z) using specialized techniques.

## Key Results
- SA-cREMBO achieves significantly better sample efficiency than standard REMBO on Styblinski-Tang and Harzmann6 benchmarks with D=21 and d=6-8.
- Joint GP modeling over multiple embeddings outperforms independent per-embedding models by sharing statistical strength across correlated subspaces.
- The smooth exponential kernel over discrete embedding indices enables adaptive, location-conditioned embedding selection without introducing discontinuities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint GP modeling over multiple embeddings improves sample efficiency compared to independent per-embedding models.
- Mechanism: A discrete embedding index z ∈ {1, ..., K} is introduced as an additional input dimension. The GP kernel factorizes as K((xi, zi), (xj, zj)) = Kx(xi, xj) · Kz(zi, zj), allowing all evaluations across embeddings to contribute to a unified surrogate. This enables the model to learn correlations between embeddings and share statistical strength.
- Core assumption: Different random embeddings capture partially overlapping or correlated subspaces of the true effective low-dimensional structure; the kernel over z can meaningfully encode this similarity.
- Evidence anchors:
  - [abstract]: "An index variable governs the embedding choice and is jointly modeled with the latent optimization variable via a product kernel in a Gaussian Process surrogate."
  - [Section 4.1]: "This setup enables the model to learn correlations across embeddings. It can emphasize embeddings that better represent the function structure while down-weighting others."
  - [corpus]: Related work on high-dimensional BO embeddings (arXiv:2502.00854) confirms multiple embeddings improve robustness, but notes independent modeling has high cost—supporting the joint modeling motivation.
- Break condition: If embeddings are too dissimilar or the true subspace varies radically across regions, Kz may fail to capture meaningful correlations, leading to conflicting data and degraded predictions.

### Mechanism 2
- Claim: Orthogonal cross-embedding complements random projection by reducing sampling bias and expanding subspace coverage.
- Mechanism: Given a random projection A, cREMBO computes an orthogonal version via Gram-Schmidt/QR decomposition. Points sampled in one embedding are rotated to the orthogonal direction and traced back to low dimensions, generating complementary samples. This expands search into multiple subspaces without additional random draws.
- Core assumption: The objective function has locally varying effective dimensionality; a single random projection may miss important directions or over-emphasize irrelevant ones.
- Evidence anchors:
  - [Section 4, Figure 2 discussion]: "If the sampling bias from random sampling can be compensated and the coverage of sampling can be improved by adding an embedding that is 'orthogonal' to the random embedding, the REMBO can be improved."
  - [Section 4]: "The added orthogonal embedding enables complementary and more efficient exploration by expanding the search into multiple subspaces."
  - [corpus]: Weak direct evidence—no corpus paper explicitly validates orthogonal cross-embedding in BO contexts.
- Break condition: If the true effective subspace is poorly aligned with either the random or orthogonal projection, both may systematically miss the optimum; orthogonality alone does not guarantee alignment.

### Mechanism 3
- Claim: The smooth exponential kernel over discrete embedding indices enables adaptive, location-conditioned embedding selection.
- Mechanism: Kz(zi, zj) = exp(-λ²(zi - zj)²) treats the embedding index as a continuous latent variable, avoiding discontinuities from one-hot or delta kernels. Combined with the product structure, the GP learns which embeddings are relevant in which input regions, effectively performing soft embedding selection during acquisition optimization.
- Core assumption: Embedding indices have latent smoothness—nearby indices correspond to similar subspace structures; this structure is learnable from data.
- Evidence anchors:
  - [Section 4.1]: "This avoids artificially injecting discrete distances or one-hot encodings that introduce magnitude discontinuities and interfere with automatic relevance detection."
  - [Section 4.3]: "The method also provides a mechanism for learning a posterior over the relevance of embeddings."
  - [corpus]: Multi-task GP literature (cited as [13]) supports product kernels for capturing correlations across discrete configurations, but does not directly validate this specific index kernel.
- Break condition: If embedding index ordering is arbitrary (no meaningful adjacency structure), the smooth kernel will impose false correlations and degrade selection quality.

## Foundational Learning

- Concept: **Random Embedding Bayesian Optimization (REMBO)**
  - Why needed here: SA-cREMBO extends REMBO; understanding projection-based dimensionality reduction is prerequisite.
  - Quick check question: Given a D-dimensional objective with effective dimension de << D, explain why optimizing in a randomly projected d-dimensional subspace (d ≥ de) can preserve the global optimum with high probability.

- Concept: **Gaussian Process Kernels with Mixed Variable Types**
  - Why needed here: The core innovation is a product kernel over continuous (x) and discrete (z) inputs.
  - Quick check question: For a product kernel K((x₁, z₁), (x₂, z₂)) = Kx(x₁, x₂) · Kz(z₁, z₂), what happens to the correlation if Kz(z₁, z₂) → 0? How does this affect information sharing across embeddings?

- Concept: **Nonstationarity and Heteroscedasticity in BO**
  - Why needed here: The paper claims SA-cREMBO captures locally varying effective dimensionality and nonstationary landscapes.
  - Quick check question: Why does a single stationary GP kernel struggle with functions that have different length-scales or noise levels in different regions?

## Architecture Onboarding

- Component map:
  Embedding Generator -> Projection Layer -> Joint GP Surrogate -> Acquisition Optimizer -> Evaluation Oracle

- Critical path:
  1. Initialize K random embeddings and compute orthogonal versions
  2. Generate initial samples across embeddings
  3. Fit joint GP with product kernel
  4. Optimize acquisition function over (x, z) jointly
  5. Project selected z to high-dim via embedding A_z
  6. Evaluate, update GP, repeat

- Design tradeoffs:
  - More embeddings (higher K) → Better coverage but larger augmented dataset, slower GP training
  - Delta kernel for Kz → No information sharing across embeddings; equivalent to independent REMBOs
  - Smooth kernel for Kz → Information sharing but requires meaningful index ordering
  - Orthogonal-only vs random+orthogonal — Paper uses both; orthogonal alone may miss directions captured by random

- Failure signatures:
  - GP predictions degrade when embeddings capture conflicting subspace structures (Kz correlations become noisy)
  - Acquisition optimization stalls if mixed-variable solver fails to explore z space adequately
  - Over-exploration at boundaries persists if projection warps too many points outside feasible region
  - No convergence improvement over REMBO if true function lacks locally varying subspace structure

- First 3 experiments:
  1. Baseline comparison: Run SA-cREMBO vs standard REMBO on Styblinski-Tang (D=21, d=8) with K=4 embeddings; log best found value vs iteration count to validate claimed improvement.
  2. Ablation on Kz kernel: Compare delta kernel vs smooth exponential kernel for Kz; measure whether information sharing across embeddings improves sample efficiency.
  3. Embedding count sensitivity: Test K ∈ {2, 4, 8, 16} on a synthetic function with known locally-varying effective dimension; identify saturation point where additional embeddings provide diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the performance of SA-cREMBO to the specific choice of kernel used for the discrete embedding index $z$?
- Basis in paper: [explicit] Section 4.4 lists "Kernel Design for $z$" as a limitation, stating that modeling the discrete index is "nontrivial and sensitive to kernel choice."
- Why unresolved: While the paper proposes a smooth exponential kernel, it acknowledges that other constructions (Delta, Tanimoto) are possible but does not empirically validate their relative performance or stability.
- What evidence would resolve it: Ablation studies comparing the proposed smooth exponential kernel against delta kernels and learned embedding kernels across varying numbers of embeddings.

### Open Question 2
- Question: To what extent does high dissimilarity between embedding matrices degrade the surrogate model due to "conflicting data"?
- Basis in paper: [explicit] Section 4.4 notes "Embedding Interaction Complexity" as a limitation, warning that the model may suffer if embeddings are "too dissimilar."
- Why unresolved: The paper introduces a unified GP to share information but does not quantify the threshold of dissimilarity at which the unified model performs worse than independent models.
- What evidence would resolve it: Analysis of optimization performance on problems where random embeddings are constrained to have varying degrees of subspace overlap or orthogonality.

### Open Question 3
- Question: What is the most efficient strategy for optimizing the acquisition function given the mixed-variable nature of the augmented input space?
- Basis in paper: [explicit] Section 4.4 identifies "Optimization Complexity" as a limitation, noting the need for specialized mixed-variable techniques.
- Why unresolved: The paper suggests methods like MIES or one-hot relaxation but does not compare the computational overhead or convergence quality of these different acquisition strategies.
- What evidence would resolve it: Benchmarks comparing the wall-clock time and regret minimization of exhaustive enumeration versus evolutionary strategies as the number of embeddings $K$ increases.

## Limitations
- Theoretical stability bounds for the product kernel over discrete embedding indices remain unproven; kernel hyperparameters λ may require careful tuning.
- Empirical evaluation is limited to synthetic benchmarks; performance on real-world high-dimensional design spaces is unverified.
- Scalability with large K is untested; GP training cost scales quadratically with data, and mixed-variable acquisition becomes increasingly challenging.

## Confidence
- **High**: Joint GP modeling improves sample efficiency vs. independent embeddings (supported by strong empirical gains and theoretical kernel expressiveness).
- **Medium**: Orthogonal cross-embedding meaningfully expands subspace coverage (supported by mechanism description but weak direct empirical validation).
- **Medium**: Smooth exponential kernel enables adaptive embedding selection (supported by design rationale but kernel hyperparameter sensitivity untested).

## Next Checks
1. Test SA-cREMBO on real-world high-dimensional problems (e.g., neural architecture search or molecular design) to validate synthetic benchmark performance.
2. Conduct ablation studies varying Kz kernel bandwidth λ to quantify impact on embedding selection and GP stability.
3. Benchmark runtime and acquisition optimization convergence as K increases to identify scalability limits.