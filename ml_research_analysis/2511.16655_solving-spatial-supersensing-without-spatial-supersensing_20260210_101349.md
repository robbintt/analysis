---
ver: rpa2
title: Solving Spatial Supersensing Without Spatial Supersensing
arxiv_id: '2511.16655'
source_url: https://arxiv.org/abs/2511.16655
tags:
- spatial
- supersensing
- cambrian-s
- video
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates the VSI-Super benchmarks (VSR and
  VSC) introduced by Cambrian-S for measuring spatial supersensing in video world
  models. The authors introduce NoSense, a simple baseline using only a SigLIP model
  that discards temporal structure, yet achieves near-perfect performance (95%) on
  VSR even for 4-hour videos.
---

# Solving Spatial Supersensing Without Spatial Supersensing

## Quick Facts
- **arXiv ID**: 2511.16655
- **Source URL**: https://arxiv.org/abs/2511.16655
- **Reference count**: 8
- **Primary result**: Simple frame-level retrieval baseline achieves near-perfect VSR performance, revealing benchmark shortcuts.

## Executive Summary
This paper critically evaluates VSI-Super benchmarks (VSR and VSC) for measuring spatial supersensing in video world models. The authors introduce NoSense, a simple baseline using only SigLIP that achieves 95% accuracy on VSR by discarding temporal structure and relying on semantic cues from a few informative frames. Additionally, they propose VSC-Repeat as a sanity check that reveals Cambrian-S' inference method collapses from 42% to 0% accuracy when videos are repeated, indicating it exploits benchmark-specific shortcuts rather than genuine spatial cognition. The findings suggest current VSI-Super benchmarks do not reliably measure spatial supersensing.

## Method Summary
The authors introduce NoSense, a simple baseline using SigLIP that encodes frames independently and retains only the top-4 frames by cosine similarity to the object query. Answer options are scored by aggregating similarities to auxiliary object prompts, achieving near-perfect VSR performance (95%) without temporal reasoning or world modeling. They also propose VSC-Repeat, where VSC videos are repeated multiple times to test Cambrian-S' inference method. This reveals that Cambrian-S' surprise-based segmentation, which assumes environments are never revisited, collapses from 42% to 0% accuracy when videos repeat, indicating shortcut exploitation rather than genuine spatial supersensing.

## Key Results
- NoSense achieves 95% accuracy on VSR even for 4-hour videos using only frame-level semantic retrieval without temporal integration
- Cambrian-S' VSC inference method collapses from 42% to 0% accuracy on VSC-Repeat when videos are repeated multiple times
- Both NoSense and Cambrian-S share functional patterns (streaming encoder, compact memory, query-guided retrieval), suggesting Cambrian-S' gains also derive from retrieval-style shortcuts

## Why This Works (Mechanism)

### Mechanism 1: Frame-Level Semantic Retrieval Without Temporal Integration
- Claim: VSR benchmark can be near-perfectly solved without spatial supersensing, world modeling, or temporal reasoning.
- Mechanism: NoSense encodes frames independently with SigLIP, retains only the top-4 frames by cosine similarity to the object query (no long-term memory), then scores answer options by aggregating similarities to auxiliary object prompts. The pipeline never reasons about motion, continuity, or long-range temporal patterns.
- Core assumption: The VSR task structure—four inserted object appearances—allows retrieval-based solutions where only relative ordering of the most object-relevant frames matters; object appearances are visually distinctive enough for frame-level matching.
- Evidence anchors:
  - [abstract] "NoSense, a simple baseline using only a SigLIP model that discards temporal structure, yet achieves near-perfect performance (95%) on VSR even for 4-hour videos."
  - [section 2, Figure 3] NoSense achieves 98.3% on 10-minute split and ~95% on 2- and 4-hour splits, outperforming Cambrian-S by 55% absolute.
  - [corpus] Limited direct corpus support. Related work (arxiv:2503.01773) discusses spatial reasoning challenges in VLMs but does not address this specific retrieval shortcut.
- Break condition: VSR would require genuine temporal reasoning if objects were not visually distinctive, required state tracking, or demanded understanding spatial relationships beyond visual co-occurrence.

### Mechanism 2: Surprise-Based Segmentation Overfits to Non-Repeating Environments
- Claim: Cambrian-S' VSC inference method relies on a benchmark-specific shortcut—the assumption that environments are never revisited—rather than genuine spatial supersensing.
- Mechanism: Cambrian-S uses surprise-based event segmentation: high prediction error triggers buffer reset, accumulating counts within segments. This implicitly assumes each segment corresponds to a distinct environment visited at most once. When videos repeat (VSC-Repeat), the buffer resets on revisited scenes and re-counts objects, causing overcounting proportional to repeat count.
- Core assumption: Videos contain monotonic sequences where surprise boundaries reliably signal new rooms; rooms are never revisited.
- Evidence anchors:
  - [abstract] "VSC-Repeat...reveals that Cambrian-S' inference method, which relies on surprise-based segmentation, collapses from 42% to 0% accuracy when videos are repeated."
  - [section 3, Figure 5] Mean relative accuracy drops from 42.0% to 3.6% with 2 repeats and to 0% with 5 repeats; predicted counts grow with repeat count.
  - [corpus] No direct corpus evidence for this specific shortcut; spatial reasoning literature does not address this failure mode.
- Break condition: Genuine spatial supersensing would maintain object identity across revisits and output invariant counts regardless of repeat count.

### Mechanism 3: Benchmark-Model Functional Equivalence Enables Shortcut Exploitation
- Claim: Cambrian-S' VSR inference pipeline and NoSense share the same functional pattern, suggesting Cambrian-S' gains also derive from retrieval-style shortcuts rather than spatial cognition.
- Mechanism: Both methods use: (a) streaming vision encoder, (b) compact memory with consolidation (top-k frames), (c) query-guided retrieval. NoSense implements this with a simpler contrastive VLM rather than an MLLM, yet achieves superior performance—indicating the benchmark does not require the additional machinery.
- Core assumption: The functional pattern of "encode, buffer salient frames, retrieve by query" is sufficient for VSR; no 3D state, object tracking, or world modeling is necessary.
- Evidence anchors:
  - [section 2] "Cambrian-S and NoSense share the same functional components...The fact that a simpler instantiation near-perfectly solves VSR suggests that it can be solved without additional inductive biases."
  - [corpus] Limited corpus connections; related benchmark design discussions (arxiv:2509.18905) highlight spatial intelligence evaluation challenges.
- Break condition: Would break if benchmarks required integrating information across non-adjacent frames, maintaining persistent object identity, or reasoning about revisited environments.

## Foundational Learning

- **Shortcut Learning / Spurious Correlations**: The paper's core argument is that performance improvements on VSI-Super stem from exploiting benchmark-specific shortcuts rather than developing intended spatial supersensing capabilities. *Quick check*: Can you identify what surface-level cue (e.g., "rooms never repeat," "four insertions") a model might use instead of the intended capability?

- **Benchmark Meta-Evaluation / Stress Testing**: The paper introduces VSC-Repeat as a "sanity check" methodology—invariance tests that preserve ground truth while breaking benchmark assumptions—to expose shortcut exploitation. *Quick check*: What transformation (repeating scenes, shuffling segments, inserting revisits) would keep the correct answer unchanged but potentially break a model's heuristic?

- **Atemporal vs. Temporal Reasoning in Video**: Key distinction between image-level semantic matching (bag-of-frames) vs. genuine temporal integration, state maintenance, and long-horizon reasoning. *Quick check*: Does the task require understanding "when" and "how" events unfold over time, or only "what" appears in isolated frames?

## Architecture Onboarding

- **Component map**: NoSense: SigLIP encoder → top-4 frame buffer (query similarity ranking) → auxiliary object similarity scoring → MCQ answer selection. Cambrian-S VSC: Frame encoder → latent prediction model (surprise signal) → segment buffer → segment-level count estimates → aggregation.

- **Critical path**: 
  1. Frame encoding (independent for NoSense; sequential with prediction for Cambrian-S)
  2. Memory consolidation (top-4 frames by similarity vs. segment-based buffering with surprise-triggered resets)
  3. Query-guided inference (retrieval for NoSense; count aggregation for Cambrian-S)

- **Design tradeoffs**:
  - Simplicity vs. generality: NoSense achieves strong VSR performance with minimal machinery but cannot generalize to tasks requiring true temporal reasoning.
  - Benchmark specificity vs. robustness: Cambrian-S VSC inference is optimized for non-repeating environments and fails catastrophically on VSC-Repeat.
  - Memory efficiency: NoSense uses constant 4-frame buffer, scaling to arbitrary video length; Cambrian-S requires segment-level storage.

- **Failure signatures**:
  - NoSense failure mode: Tasks requiring motion understanding, state changes, or integration across non-contiguous frames.
  - Cambrian-S VSC failure mode: Overcounts objects proportionally to environment revisits (predicted count ≈ ground truth × repeat count).
  - Shared failure: Neither maintains persistent object identity across experiences; neither performs genuine spatial supersensing.

- **First 3 experiments**:
  1. Reproduce NoSense on VSR: Implement SigLIP-based baseline with top-4 frame retrieval; verify ~95% accuracy across 10-min to 4-hour splits; ablate with CLIP vs. SigLIP variants.
  2. Run VSC-Repeat: Concatenate each 10-min VSC video with itself 1–5 times; confirm Cambrian-S mean relative accuracy collapse from 42% → 0%; analyze overcounting ratio vs. repeat count.
  3. Design stress test for temporal reasoning: Create VSR variant where object appearances are not visually distinctive (e.g., identical objects in different poses) or require tracking state changes; identify where NoSense breaks and whether Cambrian-S generalizes.

## Open Questions the Paper Calls Out

- **How can future spatial supersensing benchmarks incorporate invariance checks to prevent exploitation of structural shortcuts?**: The authors suggest that "invariance checks should be built in" to keep ground truth fixed while altering superficial structure, such as repeating scenes or shuffling segments. *Unresolved because*: Current benchmarks like VSC lack these checks, allowing models to fail when simple perturbations (repeating videos) are applied. *Evidence needed*: A new benchmark where model performance remains robust under transformations like VSC-Repeat.

- **Can inference methods be developed that maintain state across revisited environments rather than relying on surprise-based segmentation?**: The paper asks if methods "respect invariances" and notes that a supersensing system "should recognize views of the same scene and keep object-count predictions unchanged." *Unresolved because*: Current surprise-based segmentation collapses when videos are repeated, proving it relies on the assumption that environments are never revisited. *Evidence needed*: A model that maintains high accuracy on VSC-Repeat by successfully identifying unique objects across looped video segments.

- **What is the simplest streaming baseline capable of exploiting regularities in newly proposed spatial supersensing tasks?**: The authors state, "For any proposed spatial supersensing task, we should ask: what is the simplest streaming baseline that exploits obvious regularities?" *Unresolved because*: The success of the NoSense baseline on VSR indicates that benchmark designers often underestimate how easily semantic cues can replace temporal reasoning. *Evidence needed*: Routine publication of "stress test" baselines alongside new benchmark results to verify the necessity of complex temporal mechanisms.

## Limitations

- The paper's core claim that VSR can be solved without spatial cognition relies entirely on benchmark-specific shortcut exploitation, but it's unclear whether this truly invalidates the VSR task design or merely exposes a vulnerability that more sophisticated models could address while still requiring spatial reasoning.

- The VSC-Repeat sanity check demonstrates Cambrian-S' failure mode but doesn't prove that no model could solve VSC robustly—the benchmark may simply be poorly designed for the intended capability.

- Limited direct corpus evidence connects these specific findings to broader literature on spatial reasoning evaluation, leaving uncertainty about generalizability.

## Confidence

- **High confidence**: NoSense achieves ~95% accuracy on VSR using only frame-level semantic retrieval without temporal integration. The mechanism (top-4 frame selection by similarity) is clearly specified and reproducible.

- **Medium confidence**: Cambrian-S' VSC inference fails on VSC-Repeat due to surprise-based segmentation assumptions. While the failure pattern is documented, the broader implications for spatial supersensing evaluation remain uncertain.

- **Low confidence**: The broader claim that VSI-Super benchmarks "do not reliably measure spatial supersensing" - this extrapolates from specific failure modes to general benchmark invalidation without considering potential mitigations.

## Next Checks

1. **Temporal reasoning stress test**: Design VSR variants where objects appear in visually identical but spatially distinct contexts, or require tracking state changes across non-adjacent frames. This would reveal whether NoSense's success depends on visual distinctiveness rather than true retrieval capability.

2. **Cross-benchmark generalization**: Implement a model that combines NoSense's retrieval efficiency with Cambrian-S' temporal segmentation. Test whether such hybrid approaches can maintain VSR accuracy while improving VSC robustness on VSC-Repeat.

3. **Robustness to prompt variations**: Systematically vary object and auxiliary prompts in NoSense to determine sensitivity to prompt engineering vs. genuine semantic understanding. This would clarify whether high VSR accuracy stems from prompt memorization or actual retrieval capability.