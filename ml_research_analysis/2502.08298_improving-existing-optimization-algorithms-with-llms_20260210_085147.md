---
ver: rpa2
title: Improving Existing Optimization Algorithms with LLMs
arxiv_id: '2502.08298'
source_url: https://arxiv.org/abs/2502.08298
tags:
- cmsa
- code
- optimization
- llms
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  enhance existing optimization algorithms. The authors apply GPT-4o to improve the
  Construct, Merge, Solve, and Adapt (CMSA) hybrid metaheuristic for the Maximum Independent
  Set problem.
---

# Improving Existing Optimization Algorithms with LLMs

## Quick Facts
- arXiv ID: 2502.08298
- Source URL: https://arxiv.org/abs/2502.08298
- Reference count: 40
- Primary result: LLM-proposed heuristics incorporating age parameters improve CMSA's solution quality for Maximum Independent Set problems.

## Executive Summary
This paper demonstrates that large language models can enhance existing optimization algorithms through in-context prompting. The authors apply GPT-4o to improve the Construct, Merge, Solve, and Adapt (CMSA) hybrid metaheuristic for the Maximum Independent Set problem. By providing complete C++ code as context, the LLM proposes novel heuristics that integrate the age parameter of CMSA, leading to improved solution quality particularly on larger and denser graphs. The study shows that LLMs can act as tools for algorithmic improvement rather than just creating new algorithms from scratch.

## Method Summary
The methodology involves providing complete CMSA C++ code (~400 lines) as in-context prompt to GPT-4o, asking it to improve the construction phase by incorporating the age parameter. The LLM generates two heuristic variants: V1 uses weighted selection combining age and degree, while V2 adds entropy-adjusted probabilities. Implementation errors are corrected through iterative human-LLM feedback. Algorithm parameters are tuned using irace, and results are statistically validated across three graph types with 1440 test instances total.

## Key Results
- LLM-generated CMSA variants outperform expert-designed heuristic, particularly on larger and denser graphs
- V1 variant (age+degree weighting) showed statistically significant improvements over baseline CMSA
- The age integration strategy proposed by LLM was novel and had not been explored by CMSA researchers
- V2 variant with entropy adjustment did not improve performance relative to V1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context prompting with complete algorithm implementations enables LLMs to propose domain-meaningful heuristic improvements.
- Mechanism: The LLM receives the full CMSA C++ code as context, allowing it to identify underutilized parameters (specifically the `age` variable) and propose novel integration strategies that domain experts had not explored.
- Core assumption: The LLM's pre-training corpus contains sufficient optimization algorithm patterns to recognize structural improvement opportunities when given specific code context.
- Evidence anchors:
  - [abstract] "By leveraging in-context prompts, the LLM proposes novel heuristics that incorporate the age parameter of CMSA."
  - [section 3.3.1] "This use of the age values never occurred to anyone working on CMSA algorithms."
  - [corpus] Related work (LlaMEA, irace-evo) confirms LLMs can generate novel metaheuristics, though primarily for creating new algorithms rather than improving existing ones—suggesting this in-context improvement approach is a distinct mechanism.
- Break condition: If the target algorithm exceeds the LLM's context window or contains domain-specific conventions absent from pre-training data, the LLM may propose syntactically correct but semantically inappropriate modifications.

### Mechanism 2
- Claim: Composite weighting functions that combine multiple algorithm parameters can improve solution quality by balancing exploration-exploitation dynamically.
- Mechanism: The LLM proposed Pw(v) = (1/2 + age(v) + 1) / (1 + degree(v)), which favors vertices with low age (promoting diversity) and low degree (greedy objective), creating adaptive selection pressure throughout the search process.
- Core assumption: Combining age-based diversity with degree-based greediness yields better solutions than either heuristic alone, particularly as problem instances scale.
- Evidence anchors:
  - [section 3.3.1] Equation 1 defines the weighted probability mechanism.
  - [section 4.2] "Both LLM-generated CMSA variants outperform the standard CMSA variant with growing graph size and density."
  - [corpus] Weak direct evidence—corpus papers focus on generating new algorithms rather than parameter-combination heuristics within existing frameworks.
- Break condition: If age values are not properly normalized or the weight function produces numerical instability, selection probabilities become unreliable; the paper notes a division-by-zero bug requiring human correction (age initialized to -1).

### Mechanism 3
- Claim: Iterative human-LLM feedback loops compensate for LLM code-generation errors while preserving innovative algorithmic insights.
- Mechanism: The researcher identifies implementation errors (e.g., segmentation faults, division by zero) and feeds error messages back to the LLM, which refines the code while maintaining the core heuristic innovation.
- Core assumption: LLMs can correct syntax and runtime errors when given specific feedback without abandoning the underlying algorithmic improvement.
- Evidence anchors:
  - [section 3.4] "She resolves these errors by iteratively copying and pasting the error messages back into the LLM, engaging in a trial-and-error feedback process."
  - [figure 3b] Shows explicit dialogue where segmentation fault is reported and LLM provides corrected code.
  - [corpus] Chen et al. (2023) cited in paper confirms natural language feedback improves code generation quality.
- Break condition: If errors are conceptual rather than syntactic (e.g., the entropy adjustment in V2 did not improve performance), iterative feedback may not converge to a superior solution.

## Foundational Learning

- Concept: **CMSA (Construct, Merge, Solve & Adapt) hybrid metaheuristic**
  - Why needed here: The target algorithm being improved; understanding its four phases and the role of the `age` parameter is essential to comprehend what the LLM modified.
  - Quick check question: Can you explain why the `age` parameter exists in CMSA and what happens when a solution component's age exceeds `agemax`?

- Concept: **Maximum Independent Set (MIS) problem**
  - Why needed here: The combinatorial optimization problem being solved; the heuristic operates on graph structures selecting non-adjacent vertices.
  - Quick check question: Given a graph with vertices {A, B, C} and edges {(A,B), (B,C)}, what is the maximum independent set?

- Concept: **In-context learning with code prompts**
  - Why needed here: The methodology relies on providing complete code as context rather than natural language descriptions alone.
  - Quick check question: What is the difference between asking an LLM to "improve a greedy heuristic" versus providing the actual C++ implementation and asking for specific parameter integration?

## Architecture Onboarding

- Component map: Original C++ CMSA code -> GPT-4o prompt with code context -> LLM-generated heuristic variants -> C++ implementation with data structure optimizations -> irace parameter tuning -> Statistical evaluation on test instances

- Critical path:
  1. Prepare complete algorithm code as context (essential: include parameter definitions and their semantic roles)
  2. Design in-context prompts specifying which function to improve and what parameters to consider
  3. Generate initial heuristic proposal; review for implementation errors
  4. Iterate with error feedback until code compiles and executes correctly
  5. Run parameter tuning (irace) on tuning instances before evaluation
  6. Evaluate on held-out test instances with statistical validation

- Design tradeoffs:
  - **V1 vs. V2**: V1's simpler age+degree weighting outperformed V2's entropy adjustment—simpler mechanisms may generalize better
  - **Heuristic quality vs. code efficiency**: PERF variants showed no significant quality improvement despite code optimizations—suggesting memory/RAM benefits were not realized in solution quality
  - **Automation level**: Manual error correction required; fully autonomous agent-based systems (discussed in Section 5) remain future work

- Failure signatures:
  - Division by zero when age=-1 not handled (requires adding offset in denominator)
  - Segmentation faults from LLM-suggested data structure changes (cache-aligned vectors, bitset sizing)
  - Entropy adjustment (V2) degrades performance relative to V1 on all graph types
  - LLM-generated code that compiles but implements incorrect logic (requires functional testing, not just compilation)

- First 3 experiments:
  1. Reproduce the baseline comparison: Run original CMSA vs. LLM-CMSA-V1 on Barabási-Albert graphs with |V|=500, m=2 to verify the paper's reported improvement magnitude (~5-10% better objective values).
  2. Test generalization: Apply the same LLM-improvement methodology to a different CMSA implementation (e.g., Minimum Dominating Set) to assess whether the age-integration pattern transfers across problems.
  3. Ablation study: Isolate the contribution of age-weighting vs. degree-weighting by testing variants with only age or only degree to determine if the improvement comes primarily from one factor or their combination.

## Open Questions the Paper Calls Out

- Question: Do open-weight or alternative proprietary LLMs perform comparably to GPT-4o in improving complex optimization algorithms?
  - Basis in paper: [explicit] The authors state in Section 5 that "future work could benefit from comparing additional LLMs, especially open-weight ones," noting this as a study limitation.
  - Why unresolved: The experimental scope was restricted to GPT-4o (version 2024-11-20) due to space constraints.
  - What evidence would resolve it: A comparative study benchmarking GPT-4o against models like Llama 3 or DeepSeek on the CMSA heuristic generation task.

- Question: Can specialized benchmarks be developed to effectively assess which LLMs are capable of "discovering" better heuristics for optimization?
  - Basis in paper: [explicit] Section 5 identifies the "Creation of specialized benchmarks" as a necessary new line of research to assess heuristic discovery capabilities.
  - Why unresolved: While general code generation benchmarks exist, there are currently no established benchmarks specifically tailored for optimization algorithm improvements.
  - What evidence would resolve it: The construction of a standardized dataset of optimization problems and algorithm baselines to evaluate LLM-generated heuristics.

- Question: Can autonomous code agents successfully replace manual human interaction in the optimization code refinement process?
  - Basis in paper: [explicit] Section 5 suggests "Integration of LLM-based agents" to delegate tasks like executing code and correcting errors, potentially advancing the field.
  - Why unresolved: The current methodology relies on a manual "trial-and-error feedback" loop between a human researcher and the LLM.
  - What evidence would resolve it: A platform where autonomous agents collaborate to debug and implement optimizations without human intervention, achieving equal or superior results.

## Limitations
- The study tested only one algorithm-class combination (CMSA-MIS), limiting generalizability across different optimization problems
- Iterative human correction was required for LLM code-generation errors, indicating full automation remains distant
- V2 variant with entropy adjustment underperformed V1, showing not all LLM suggestions yield benefits

## Confidence
- **High confidence**: CMSA improvement mechanism (age parameter integration), statistical significance of V1 improvements, and the core methodology of using in-context code prompts for heuristic enhancement.
- **Medium confidence**: Generalization potential across different optimization problems and algorithms, as the study tested only one algorithm-class combination.
- **Low confidence**: Long-term reliability of LLM suggestions, as the paper notes that exact reproduction is not guaranteed due to LLM stochasticity.

## Next Checks
1. Apply the same LLM-improvement methodology to a different CMSA variant (e.g., Minimum Vertex Cover) to test cross-problem generalization of age-parameter integration.
2. Conduct ablation studies isolating age-weighting vs. degree-weighting contributions to quantify which factor drives improvements.
3. Measure runtime overhead of LLM-enhanced heuristics to evaluate the computational cost-benefit tradeoff relative to solution quality gains.