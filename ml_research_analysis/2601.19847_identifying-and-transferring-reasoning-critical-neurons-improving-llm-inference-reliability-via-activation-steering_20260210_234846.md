---
ver: rpa2
title: 'Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference
  Reliability via Activation Steering'
arxiv_id: '2601.19847'
source_url: https://arxiv.org/abs/2601.19847
tags:
- reasoning
- adaras
- activation
- steering
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaRAS, a lightweight inference-time framework
  that improves the reliability of LLM reasoning by selectively steering neuron activations.
  The method identifies Reasoning-Critical Neurons (RCNs) through contrastive analysis
  of correct and incorrect reasoning trajectories, then applies adaptive activation
  steering only when a failure is predicted.
---

# Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering

## Quick Facts
- arXiv ID: 2601.19847
- Source URL: https://arxiv.org/abs/2601.19847
- Reference count: 40
- Primary result: AdaRAS improves reasoning accuracy by up to 13% on AIME-24/25 via activation steering of Reasoning-Critical Neurons

## Executive Summary
AdaRAS is a lightweight inference-time framework that improves the reliability of large language models on complex reasoning tasks by selectively steering neuron activations. The method identifies Reasoning-Critical Neurons (RCNs) through contrastive analysis of correct and incorrect reasoning trajectories, then applies adaptive activation steering only when a failure is predicted. Experiments on 10 math and coding benchmarks show consistent accuracy gains, including over 13% improvement on AIME-24 and AIME-25. The approach is parameter-free, requires no additional training or sampling, and generalizes well across tasks and model scales.

## Method Summary
AdaRAS improves reasoning reliability by identifying neurons whose activations differ in polarity between correct and incorrect reasoning traces, then steering these neurons only when failure is predicted. The method first generates contrastive data via self-sampling to create pairs of correct and incorrect reasoning trajectories. It then identifies RCNs using a polarity-aware mean-difference criterion that selects neurons whose average activation polarity reverses between correct and incorrect traces. An adaptive intervention module predicts failure likelihood and gates steering accordingly. During inference, the method injects steering vectors into MLP blocks to shift activations toward the "correct reasoning" subspace, stabilizing reasoning trajectories without semantic drift.

## Key Results
- Achieves over 13% improvement on AIME-24 and AIME-25 benchmarks
- Maintains or improves accuracy across 10 math and coding benchmarks
- Reduces reasoning trajectory fluctuations while preserving semantic representations
- Works across different model scales and generalizes from math to coding tasks

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Critical Neuron Identification via Polarity-Aware Mean Difference
AdaRAS identifies neurons whose activation polarity differs between correct and incorrect reasoning traces. The method computes mean activation differences across full reasoning trajectories, not just final tokens, and retains only neurons showing sign flips. This polarity-based filtering is more effective than magnitude-only differences. Evidence shows probing classifiers achieve AUROC up to 0.76 on AIME, and removing polarity filtering degrades performance by 4-9%.

### Mechanism 2: Adaptive Intervention via Failure Prediction
A lightweight attention-based classifier predicts reasoning failure using early-layer activations selected via F-statistic. Steering is applied only when failure is predicted, avoiding perturbation of correct trajectories. The predictor achieves AUROC of 0.83 on AIME, and removing adaptive intervention reduces AIME-24 accuracy from 60.87% to 56.52%.

### Mechanism 3: Trajectory-Level Stabilization Without Semantic Drift
Activation steering reduces fluctuations in latent reasoning trajectories (lower magnitude) while preserving semantic representations (stable angle). Steered samples show significantly lower magnitude values with minimal angle changes, indicating more direct reasoning paths. This stabilization correlates with improved accuracy without semantic drift.

## Foundational Learning

- **Concept: Activation Engineering / Steering**
  - Why needed here: AdaRAS modifies neuron activations at inference time without retraining; understanding that activations can be shifted to control behavior is prerequisite.
  - Quick check question: Can you explain how adding a steering vector to an MLP block differs from fine-tuning the model weights?

- **Concept: MLP Layers as Pattern Encoders in Transformers**
  - Why needed here: The paper focuses on MLP post-activation outputs (SwiGLU) as the intervention target, based on prior work that MLPs encode high-level semantic patterns.
  - Quick check question: In a standard transformer decoder, where does the MLP block sit relative to attention, and what is its hypothesized functional role?

- **Concept: Probing Classifiers for Representational Analysis**
  - Why needed here: Preliminary analysis uses probing to show neuron activations predict correctness; the adaptive intervention module also uses a classifier trained on activations.
  - Quick check question: If you train a linear probe on hidden states to predict a binary label, what does the probe weight vector represent?

## Architecture Onboarding

- **Component map:**
  1. Contrastive Data Constructor: Generates paired correct/incorrect traces via high-temperature self-sampling
  2. RCN Identifier: Computes mean-difference scores, filters by polarity, selects top-K neurons
  3. Adaptive Intervention Module: Attention-based classifier trained on early-layer activations to predict failure
  4. Steering Applicator: Injects steering vectors into MLP blocks during forward pass

- **Critical path:**
  1. Sample N inputs from target domain
  2. Generate 8 traces per input at T=1.0, retain questions with 4 correct + 4 incorrect
  3. Compute mean activation per neuron per trace, then mean-difference across all pairs
  4. Filter neurons with sign flip, rank by |S(l,i)|, select top-K (K=50)
  5. Train failure predictor on training split activations
  6. At inference: if predictor flags failure, apply steering with strength α

- **Design tradeoffs:**
  - K (neurons to steer): Peak at ~50 neurons (~0.03% of total), degradation beyond 2000
  - α (steering strength): Peak at 0.3-0.4; drops to baseline only at α≥0.7
  - Source dataset for RCNs: AIME-identified RCNs transfer well to other math and coding tasks

- **Failure signatures:**
  - Random steering degrades AIME-24 from 47.83% to 34.78%
  - Removing polarity filter drops AIME-24 from 60.87% to 56.52%
  - Steering too many neurons (~2000) causes ~6% accuracy drop

- **First 3 experiments:**
  1. Reproduce polarity filtering ablation: Compare top-K selection with vs. without polarity filter on held-out AIME samples
  2. Validate failure predictor separability: Visualize projection of correct vs. incorrect traces onto learned direction; target AUROC >0.80
  3. Test cross-dataset transfer: Identify RCNs on AIME, apply to GSM8K and HumanEval without re-identification

## Open Questions the Paper Calls Out

### Open Question 1
Does AdaRAS generalize to diverse model architectures (beyond Qwen/LLaMA) and complex reasoning domains such as spatial or multi-hop reasoning? The paper notes evaluation is currently restricted to Qwen3 series and STEM benchmarks, requiring future work on broader architectures and settings.

### Open Question 2
Can the reliance on paired contrastive data be relaxed for models at capability extremes where obtaining both correct and incorrect trajectories is difficult? The authors note the requirement for contrastive pairs may limit applicability for models where generating these specific pairs is non-trivial.

### Open Question 3
Can advanced interpretability tools like Sparse Auto Encoders (SAEs) or vocabulary projection provide more precise identification of Reasoning-Critical Neurons than the mean-difference criterion? The paper suggests future work could incorporate SAEs or vocabulary projection analyses to further elucidate the mechanistic underpinnings of RCNs.

## Limitations

- The method relies on paired contrastive data, which may be difficult to obtain for very weak or very strong models
- Evaluation is limited to Qwen3 series and STEM benchmarks, with uncertain generalization to other architectures and reasoning domains
- The RCN identification method depends on specific interpretation of mean activation differences that hasn't been validated across diverse tasks

## Confidence

- **High confidence**: Core accuracy improvements on AIME-24/25; failure of random steering and excessive neuron counts; importance of polarity filter constraint
- **Medium confidence**: Adaptive intervention mechanism's contribution beyond unconditional steering; trajectory stabilization interpretation based on magnitude/angle metrics
- **Low confidence**: Generalizability of polarity-based neuron selection to other reasoning domains; robustness of failure predictor across different datasets and model scales

## Next Checks

1. Test polarity filtering robustness by identifying RCNs on a non-math dataset (e.g., HumanEval) and evaluating whether the same polarity-based selection criterion yields beneficial neurons for that domain.

2. Ablate the failure predictor on multiple benchmarks to quantify the marginal benefit of adaptive intervention versus unconditional steering across diverse reasoning tasks.

3. Validate the trajectory stabilization hypothesis by correlating magnitude/angle metric changes with specific error types (e.g., hallucination, arithmetic errors) rather than just overall accuracy.