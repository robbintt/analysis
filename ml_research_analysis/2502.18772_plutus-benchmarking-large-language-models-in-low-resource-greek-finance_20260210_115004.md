---
ver: rpa2
title: 'Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance'
arxiv_id: '2502.18772'
source_url: https://arxiv.org/abs/2502.18772
tags:
- financial
- greek
- language
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Plutus-ben, the first comprehensive Greek\
  \ financial evaluation benchmark, and Plutus-8B, the first Greek financial LLM,\
  \ addressing the lack of domain-specific resources for Greek financial NLP. The\
  \ benchmark includes five core tasks\u2014numeric and textual NER, QA, abstractive\
  \ summarization, and topic classification\u2014supported by high-quality datasets\
  \ annotated by expert native Greek speakers."
---

# Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance

## Quick Facts
- arXiv ID: 2502.18772
- Source URL: https://arxiv.org/abs/2502.18772
- Reference count: 40
- Primary result: Plutus-8B achieves 0.60 mean score on Greek financial NLP benchmark, outperforming GPT-4 (0.52) and English financial models (0.14)

## Executive Summary
This work introduces Plutus-ben, the first comprehensive Greek financial evaluation benchmark, and Plutus-8B, the first Greek financial LLM, addressing the lack of domain-specific resources for Greek financial NLP. The benchmark includes five core tasks—numeric and textual NER, QA, abstractive summarization, and topic classification—supported by high-quality datasets annotated by expert native Greek speakers. Evaluation of 22 LLMs, including both general and financial models, reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. Plutus-8B, fine-tuned on Greek financial data, achieves SOTA performance with a mean score of 0.60, significantly outperforming models like GPT-4 (0.52) and demonstrating the value of targeted Greek financial adaptation. The work underscores the limitations of cross-lingual transfer and highlights the necessity of financial expertise in Greek-trained models. All resources are publicly released to promote reproducible research and advance multilingual inclusivity in finance.

## Method Summary
The method involves fine-tuning Llama-Krikri-8B-Instruct using LoRA (r=16, alpha=32, no dropout) with int4 quantization on a domain-specific Greek financial instruction dataset (Plutus-instruction). The training uses AdamW optimizer with lr=5e-4, cosine schedule, 3 epochs, block size 4096 tokens, gradient accumulation step 4, batch size 1, and bf16 mixed precision. Evaluation is conducted on 4x A100 80GB GPUs with max generation 8192 tokens for summarization and 1024 tokens for other tasks. The benchmark includes five tasks across three new datasets (GRFinNUM: 500 samples, GRFinNER: 500 samples, GRFinQA: 540 samples) plus two existing ones (GRFNS-2023: 262 samples, GRMultiFin: 268 samples), with metrics including Entity F1 for NER, Accuracy for QA/classification, and Rouge-1 for summarization.

## Key Results
- Plutus-8B achieves a mean score of 0.60, outperforming GPT-4 (0.52) and all other evaluated models
- English financial models (FinLLaMA-8B, Finma-7B) perform poorly at 0.14 mean score, demonstrating cross-lingual transfer limitations
- Larger models like Qwen2.5-72B do not consistently outperform smaller ones, showing scaling alone is insufficient
- Summarization remains the most challenging task, with even the best model achieving only 0.34 Rouge-1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted instruction fine-tuning on domain-specific, linguistically-matched data improves performance on low-resource, domain-specific NLP tasks.
- **Mechanism:** The model's weights are adapted to the statistical patterns, vocabulary, and reasoning structures present in the high-quality, expert-annotated Greek financial instruction dataset (Plutus-instruction). This specialises the model for the target domain (finance) and language (Greek), mitigating gaps in its pre-training data.
- **Core assumption:** The foundational model (`Llama-Krikri-8B-Instruct`) has sufficient base linguistic capability in Greek; the bottleneck is a lack of exposure to financial terminology, reasoning, and format in that language.
- **Evidence anchors:** [abstract] Plutus-8B, fine-tuned on Greek financial data, achieves SOTA performance with a mean score of 0.60, significantly outperforming models like GPT-4 (0.52). [section 4] "Plutus-8B... fine-tuned exclusively on Greek financial data, achieves the highest mean score of 0.60, outperforming all baseline models."

### Mechanism 2
- **Claim:** Cross-lingual transfer from high-resource languages (e.g., English) to a low-resource language (Greek) is insufficient for complex, specialized domains like finance.
- **Mechanism:** Models primarily trained on English financial corpora (e.g., `FinLLaMA-8B`) fail to transfer domain knowledge when the linguistic structure, morphology, and terminology differ significantly, as is the case with Greek.
- **Core assumption:** This assumes that financial concepts and terminology are not perfectly isomorphic across languages and that a model cannot reliably "translate and reason" its way to expert-level performance in a low-resource domain without explicit training.
- **Evidence anchors:** [abstract] "These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text." [section 5.1] "English financial models like Finma-7B and FinLLaMA-8B perform poorly on Greek tasks, each registering a mean score of only 0.14... This reflects the difficulty of transferring financial expertise developed from high-resource English data to the Greek context."

### Mechanism 3
- **Claim:** Scaling model parameter count alone yields diminishing returns for low-resource, specialized domains without corresponding domain-specific training data.
- **Mechanism:** Increasing model size (e.g., from 8B to 72B parameters) increases representational capacity, but without exposure to the relevant data distributions (Greek finance), this capacity is not utilized effectively.
- **Core assumption:** Assumes that the model has not already been trained on a substantial corpus of Greek financial text during its pre-training phase, making its performance on this specific benchmark a test of its ability to generalize.
- **Evidence anchors:** [abstract] "Evaluation of 22 LLMs... reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps." [section 5.1] "Larger models generally perform better, but scaling alone does not consistently translate to superior results for Greek financial tasks... Qwen2.5-72B failing to outperform Qwen2.5-32B, proving that scaling alone is not the answer."

## Foundational Learning

- **Concept: Low-Resource Language Adaptation**
  - **Why needed here:** Greek is considered a low-resource language for NLP, especially in the financial domain. The paper's core contribution is a methodology for adapting LLMs to such a setting.
  - **Quick check question:** Why does the paper use LoRA for fine-tuning Plutus-8B instead of full parameter fine-tuning? What are the trade-offs?

- **Concept: Evaluation Benchmarks for Specialized Domains**
  - **Why needed here:** Evaluating a financial LLM requires more than general-purpose benchmarks. This paper introduces `Plutus-ben`, which includes task definitions, curated datasets, and specific metrics.
  - **Quick check question:** What are the five core NLP tasks in `Plutus-ben`, and why are they considered essential for evaluating financial LLMs?

- **Concept: Linguistic and Domain Complexity in Finance**
  - **Why needed here:** The paper highlights how Greek's morphology and the specialized nature of financial text create compounding challenges.
  - **Quick check question:** How does the performance gap between Greek general models (like Meltemi-7B) on textual NER versus numeric NER illustrate the separate challenges of linguistic and domain adaptation?

## Architecture Onboarding

- **Component map:** `Llama-Krikri-8B-Instruct` -> `Plutus-instruction` dataset -> LoRA fine-tuning -> `Plutus-8B` -> `Plutus-ben` benchmark evaluation

- **Critical path:**
  1. Data Curation & Annotation: Gather raw financial documents and have expert native Greek speakers annotate them according to strict guidelines
  2. Instruction Data Conversion: Transform the annotated datasets into a structured `instruction -> input -> output` format using Greek expert-crafted prompts
  3. Model Fine-Tuning: Apply LoRA to adapt `Llama-Krikri-8B-Instruct` on the `Plutus-instruction` dataset
  4. Evaluation: Run the fine-tuned model (`Plutus-8B`) and baselines through the `Plutus-ben` suite, comparing automated metrics and human evaluation results

- **Design tradeoffs:**
  - Data vs. Task Coverage: Held out `GRFinQA` dataset from training to evaluate generalization, trading peak performance for robust out-of-sample capability measurement
  - Model Size vs. Efficiency: Choosing 8B parameter model with LoRA and quantization for computational efficiency over potentially higher (but not guaranteed) performance of larger models
  - Automated vs. Human Eval: Supplemented automated metrics with human evaluation to capture nuances in fluency, coherence, and factuality that automated scores miss

- **Failure signatures:**
  - Near-Zero NER Scores: Baseline models scoring near zero on NER tasks indicates complete failure to adapt to Greek financial entity structures
  - Stagnant Summarization: Modest gains in summarization (Rouge-1 ~0.34) signal persistent difficulty with long-context understanding and generation
  - Factuality Gaps in Human Eval: Low human factuality scores (e.g., FinLLaMA-8B at 1.54/5) point to generating plausible-sounding but incorrect financial statements

- **First 3 experiments:**
  1. Establish a Baseline: Evaluate foundational model and strong general-purpose model (e.g., GPT-4) on full `Plutus-ben` suite to quantify initial performance gap
  2. Ablate the Training Data: Fine-tune foundational model using different subsets of `Plutus-instruction` data to measure contribution of each task to overall performance
  3. Benchmark Generalization: Evaluate fully fine-tuned `Plutus-8B` on held-out `GRFinQA` dataset and compare performance to baseline to validate generalizable financial reasoning

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the relative effectiveness of data augmentation, synthetic data generation, versus cross-lingual transfer learning for improving Greek financial LLM performance under low-resource constraints?
- **Open Question 2:** How does scaling model size beyond 8B parameters affect Greek financial task performance, particularly for long-context tasks like abstractive summarization?
- **Open Question 3:** What additional task types beyond the current five should be included to comprehensively evaluate Greek financial LLM capabilities?
- **Open Question 4:** To what extent can cross-lingual transfer from high-resource financial languages (English, Chinese) overcome the scarcity of Greek financial training data?

## Limitations
- Cross-lingual transfer remains fundamentally constrained for specialized domains like finance, with English financial models achieving only 0.14 mean scores
- Summarization tasks show persistent underperformance even for the best model, with only 0.34 Rouge-1 achieved
- Evaluation dataset sizes, while substantial for low-resource settings, may not fully capture real-world Greek financial application complexity

## Confidence

**High Confidence:** The mechanism of domain-specific instruction fine-tuning on linguistically-matched data improving performance is strongly supported by the 0.60 mean score achieved by Plutus-8B versus 0.52 for GPT-4 and 0.14 for English financial models.

**Medium Confidence:** The claim that scaling alone yields diminishing returns is supported by the Qwen2.5-72B versus Qwen2.5-32B comparison, but the evaluation includes only a limited set of scaled models.

**Low Confidence:** The assertion that cross-lingual transfer is fundamentally insufficient may be premature, as the evaluation does not test more sophisticated transfer learning approaches or intermediate multilingual training strategies.

## Next Checks

1. **Ablation Study on Training Data:** Evaluate Plutus-8B performance when fine-tuned on progressively smaller subsets of the Plutus-instruction dataset to determine minimum viable dataset size and identify which tasks contribute most to performance gains.

2. **Cross-Lingual Transfer Experiment:** Fine-tune a multilingual model (e.g., XGLM or BLOOM) on English financial data and evaluate zero-shot performance on Greek tasks, comparing against reported English-only model performance.

3. **Long-Context Summarization Test:** Design a controlled experiment using synthetic long financial documents to isolate whether Plutus-8B's summarization limitations stem from context length handling, financial reasoning complexity, or Greek language processing challenges.