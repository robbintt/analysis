---
ver: rpa2
title: Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context
  Learning
arxiv_id: '2601.08105'
source_url: https://arxiv.org/abs/2601.08105
tags:
- query
- queries
- few-shot
- examples
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses query suggestion for retrieval-augmented generation
  (RAG) agents, focusing on improving user interaction when queries are unanswerable
  due to either missing workflows or lack of knowledge. The core challenge is to suggest
  semantically similar queries that are actually answerable by the RAG agent, despite
  the complexity of multi-step workflows and tool dependencies.
---

# Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning

## Quick Facts
- arXiv ID: 2601.08105
- Source URL: https://arxiv.org/abs/2601.08105
- Authors: Fabian Spaeh; Tianyi Chen; Chen-Hao Chiang; Bin Shen
- Reference count: 18
- One-line result: Dynamic few-shot learning improves query suggestion answerability by up to 40% over baselines while maintaining semantic similarity

## Executive Summary
This paper introduces a query suggestion method for retrieval-augmented generation (RAG) agents that addresses the challenge of suggesting answerable queries when users submit unanswerable ones. The approach uses query templating to abstract workflows, dynamic few-shot learning with robust retrieval of relevant examples, and self-learning via post-hoc response evaluation. Experiments on three real-world datasets show that dynamic few-shot learning significantly outperforms static few-shot learning and retrieval-only baselines in both semantic similarity and answerability, with improvements in answerability of up to 40% in some cases, while maintaining high similarity to original queries. The method also demonstrates low sensitivity to training set size, achieving strong performance with as few as 500 examples.

## Method Summary
The method employs query templating to reduce queries to their workflows by masking entity values, enabling retrieval of semantically similar answerable queries. Dynamic few-shot learning with robust retrieval uses majority voting among retrieved examples to filter out hallucinated labels. The approach is self-learning, requiring no manual labels, and instead uses the RAG agent itself to classify and explain answerability after execution. Experiments on three real-world datasets demonstrate that dynamic few-shot learning significantly outperforms static few-shot learning and retrieval-only baselines in both semantic similarity and answerability.

## Key Results
- Dynamic few-shot learning achieves up to 40% improvement in answerability compared to retrieval-only baselines
- Method maintains high semantic similarity to original queries while improving answerability
- Performance remains strong with as few as 500 training examples, showing low sensitivity to training set size
- Outperforms static few-shot learning across all three real-world datasets tested

## Why This Works (Mechanism)

### Mechanism 1: Query Templating for Workflow Abstraction
- Claim: Masking entity values in queries enables retrieval of workflow-similar examples independent of specific data values.
- Mechanism: An LLM replaces all tool-call arguments with their entity type labels (e.g., "September 2021" → "[timespan]"), producing a template that represents the underlying workflow. Templates are embedded for similarity search.
- Core assumption: Queries with similar workflows but different values should map to the same template; LLMs can reliably identify and mask entity values given tool argument descriptions.
- Evidence anchors: [abstract] "employing query templating to reduce queries to their workflows and retrieve semantically similar answerable queries"
- Break condition: If tools have poorly documented argument types, or if entity identification is inconsistent across queries, template quality degrades and retrieval relevance drops.

### Mechanism 2: Robust Dynamic Few-Shot Retrieval with Local Majority Voting
- Claim: Dynamically retrieving relevant few-shot examples with majority-vote filtering improves suggestion answerability over static prompts.
- Mechanism: Given a user query template embedding, retrieve similar examples from a database. Cluster nearby examples and assign answerability labels via local majority vote, suppressing hallucinated or inconsistent labels. Provide up to 5 positive (answerable) and 5 negative (no workflow) examples to the LLM.
- Core assumption: Hallucinated or faulty labels exist in the example pool but are locally inconsistent; majority voting approximates ground truth.
- Evidence anchors: [abstract] "dynamic few-shot learning with robust retrieval of relevant examples"
- Break condition: If the example pool is small or systematically biased, majority voting cannot correct errors; if θ_sim and θ_div thresholds are poorly tuned, retrieval returns irrelevant or overly redundant examples.

### Mechanism 3: Self-Learning via Post-Hoc Response Evaluation
- Claim: The RAG agent can label its own queries after execution, enabling unsupervised construction of a labeled example database.
- Mechanism: After the RAG executes a query, an LLM reviews the full tool-call chain, reasoning steps, and final response to classify answerability (answerable / no workflow / no knowledge) and generate an explanation. Templates and labels are stored for future retrieval.
- Core assumption: Post-hoc evaluation is more reliable than pre-execution prediction; the evaluating LLM can accurately infer why a workflow succeeded or failed.
- Evidence anchors: [abstract] "This approach is self-learning, requiring no manual labels and instead using the RAG agent itself to classify and explain answerability"
- Break condition: If the RAG's self-reflection module is unreliable or the evaluating LLM lacks sufficient tool documentation, labels may be incorrect, propagating errors into the example pool.

## Foundational Learning

- Concept: **Few-shot in-context learning**
  - Why needed here: The core suggestion generation relies on providing the LLM with positive and negative examples (with explanations) at inference time, rather than fine-tuning.
  - Quick check question: Can you explain why dynamic example retrieval outperforms a static prompt with the same examples?

- Concept: **Embedding-based semantic retrieval**
  - Why needed here: Templated queries are converted to embeddings; cosine similarity drives example retrieval. Understanding nearest-neighbor search is essential for tuning thresholds.
  - Quick check question: Given two template embeddings, how would you compute their cosine similarity, and what does a high value indicate about workflow relatedness?

- Concept: **Agentic RAG with tool-calling**
  - Why needed here: The entire problem is scoped to RAG agents that execute multi-step workflows via tool calls. Distinguishing "no workflow" from "no knowledge" failures requires understanding tool execution chains.
  - Quick check question: For a query that returns an empty SQL result, would you classify it as "no workflow" or "no knowledge"? Why?

## Architecture Onboarding

- Component map:
  1. **Templating Module** — LLM-based entity masking; outputs template string.
  2. **Embedding Store** — Database of (template embedding, answerability label, explanation) tuples.
  3. **Robust Retrieval** — Algorithm 1; takes query template embedding, returns filtered positive/negative examples.
  4. **Suggestion Generator** — LLM prompted with retrieved examples, generates answerable templates.
  5. **Value Imputer** — LLM fills masks with values from original query, tool responses, or example values.
  6. **Self-Labeling Module** — Post-execution classifier that updates the embedding store.

- Critical path:
  1. User submits query → RAG agent executes → if unanswerable, trigger suggestion pipeline.
  2. Templating Module masks entities → embed template.
  3. Robust Retrieval fetches top-k positive/negative examples.
  4. Suggestion Generator produces answerable template(s).
  5. Value Imputer fills masks → return suggested query to user.
  6. In parallel, Self-Labeling Module classifies original query and stores (template, label, explanation).

- Design tradeoffs:
  - **Static vs. dynamic few-shot**: Static is simpler but lacks adaptability; dynamic requires embedding infrastructure but handles diverse workflows.
  - **Template granularity**: Overly specific templates reduce retrieval recall; overly generic templates conflate distinct workflows.
  - **Self-labeling vs. human labeling**: Self-labeling scales but risks label noise; human labeling is accurate but costly.
  - **Number of examples**: More examples improve coverage but consume context window; paper uses ≤5 per class.

- Failure signatures:
  - **High similarity, low answerability**: Suggestion generator is ignoring negative examples; check prompt formatting and example balance.
  - **Low similarity suggestions**: Retrieval thresholds too strict or template quality poor; audit templating output.
  - **Label drift over time**: Self-labeling accumulating errors; periodically audit example pool or inject human-validated examples.
  - **Repeated similar suggestions**: θ_div too low; clusters not merging properly.

- First 3 experiments:
  1. **Baseline comparison**: Implement static few-shot and retrieval-only baselines; measure answerability rate and cosine similarity on a held-out query set to reproduce Figure 6 trends.
  2. **Ablation on robustness**: Disable majority voting (set θ_div = 1.0 to skip clustering); compare answerability to detect hallucination impact.
  3. **Training set size sensitivity**: Vary the number of labeled examples from 100 to 2000; plot answerability vs. training size to validate the claim that ~500 examples suffice (Figure 7).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does dynamic few-shot learning achieve higher answerability than the retrieval-only approach, when retrieval-only suggestions are drawn exclusively from pre-validated answerable templates?
- Basis in paper: [explicit] Authors state: "Surprisingly, dynamic few-shot learning surpasses the answerability of the retrieval-only approach... A reason why dynamic few-shot learning can nonetheless outperform the retrieval-only approach is that after seeing relevant positive and negative few-shot examples, the LLM decides on more conservative questions which can be answered with higher certainty."
- Why unresolved: The proposed explanation about "conservative questions" remains speculative; no ablation or analysis confirms the mechanism.
- What evidence would resolve it: Qualitative analysis comparing query characteristics (e.g., complexity, specificity) between dynamic few-shot and retrieval-only suggestions; controlled experiments varying the conservativeness of generated queries.

### Open Question 2
- Question: How robust is the self-learning labeling mechanism to systematic errors in the LLM's own answerability classification?
- Basis in paper: [inferred] The self-learning approach uses the RAG agent itself to label queries, but no analysis addresses whether systematic biases could propagate and degrade suggestion quality over time.
- Why unresolved: The paper demonstrates feasibility but does not analyze failure modes or error accumulation in self-labeled training data.
- What evidence would resolve it: Experiments measuring label accuracy against human annotations; analysis of label drift over successive self-learning iterations.

### Open Question 3
- Question: How does the method generalize to RAG systems with different tool types (e.g., API calls, knowledge graphs) beyond the relational database and Python execution tools studied?
- Basis in paper: [inferred] All experiments use a proprietary copilot operating on a relational database with optional Python execution. Generalization to other tool paradigms is unstated.
- Why unresolved: The templating approach relies on tools describing their arguments; different tool types may have different abstraction challenges.
- What evidence would resolve it: Evaluation on RAG agents with diverse tool types, reporting answerability and similarity metrics across configurations.

## Limitations
- Template quality depends heavily on LLM's ability to reliably identify and mask entity values across diverse queries
- Majority-voting threshold sensitivity not validated; poor tuning could lead to over-merging or under-merging
- Self-labeling error accumulation risk not quantified; systematic label noise could degrade performance over time

## Confidence

### High Confidence
- Overall improvement trend in answerability (up to 40%) over baselines is well-supported by reported experiments and aligns with dynamic few-shot retrieval mechanism

### Medium Confidence
- Claim that ~500 examples suffice for strong performance is supported by Figure 7, but diminishing returns beyond this point and noisier dataset generalization are untested
- Robustness of majority voting is theoretically sound but lacks empirical validation against systematic label bias

### Low Confidence
- Generalizability to RAG agents with very different tool schemas (e.g., non-SQL tools, APIs with complex parameters) is not demonstrated
- The paper focuses on SQL/Python workflows, and templating mechanism may not scale to richer or more varied tool sets without significant adaptation

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary θ_sim and θ_div in Algorithm 1 and measure their impact on answerability and similarity. Identify optimal ranges and failure modes (e.g., over-merging causing loss of diversity).
2. **Label Noise Audit**: Manually sample 100 self-labeled queries from the example database and compute precision/recall of answerability labels. Compare performance with and without majority voting to quantify hallucination suppression.
3. **Cross-Domain Generalization**: Apply the method to a RAG agent with a different tool set (e.g., API calls, document search, code generation) and evaluate whether template masking and retrieval remain effective. Identify required adaptations for non-SQL workflows.