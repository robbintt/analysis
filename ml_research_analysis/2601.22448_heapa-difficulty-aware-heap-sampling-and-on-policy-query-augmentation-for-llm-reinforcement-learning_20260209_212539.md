---
ver: rpa2
title: 'HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for
  LLM Reinforcement Learning'
arxiv_id: '2601.22448'
source_url: https://arxiv.org/abs/2601.22448
tags:
- sampling
- pool
- uni00000013
- heapa
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HEAPA improves RLVR training by focusing on the moving capability
  frontier: it maintains a bounded prompt pool, samples near a dynamic boundary between
  hard and easy prompts using heap-based boundary sampling, and grows the pool on-policy
  via student-generated queries annotated by a lightweight teacher. The method stabilizes
  correlated augments with lineage-aware pool statistics re-estimation and controlled
  reinsertion.'
---

# HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.22448
- **Source URL:** https://arxiv.org/abs/2601.22448
- **Reference count:** 40
- **Primary result:** HeaPA consistently reaches target accuracy with fewer rollout tokens, at comparable wall-clock time, and its gains increase with model scale.

## Executive Summary
HeaPA improves RLVR training by focusing on the moving capability frontier: it maintains a bounded prompt pool, samples near a dynamic boundary between hard and easy prompts using heap-based boundary sampling, and grows the pool on-policy via student-generated queries annotated by a lightweight teacher. The method stabilizes correlated augments with lineage-aware pool statistics re-estimation and controlled reinsertion. Across two datasets, two optimizers, and seven benchmarks, HeaPA consistently reaches target accuracy with fewer rollout tokens, at comparable wall-clock time, and its gains increase with model scale. The primary benefit comes from coupling frontier-focused sampling with stable on-policy pool growth, rather than from any single component.

## Method Summary
HeaPA is a difficulty-aware sampling and on-policy query augmentation framework for RLVR training. It maintains a bounded prompt pool with a cold queue for unscored items and two scored partitions (low=hard, high=easy). A dual-heap structure enables efficient boundary sampling near the partition boundary, concentrating rollouts on medium-difficulty prompts. On-policy augmentation generates new queries from the student policy, annotated asynchronously by a lightweight teacher, and inserts verified pairs into the pool. Lineage-aware pool statistics re-estimation (PathAgg or ChildAgg) propagates descendant signals to prevent correlated bursts from destabilizing the curriculum. The method achieves sample-efficiency gains by focusing rollouts on the moving capability frontier and growing the pool with queries aligned to the student's current ability.

## Key Results
- HeaPA consistently reaches target accuracy with fewer rollout tokens, at comparable wall-clock time.
- Gains increase with model scale.
- Outperforms uniform sampling and prioritized sampling across two datasets, two optimizers, and seven benchmarks.

## Why This Works (Mechanism)

### Mechanism 1: Heap-based Boundary Sampling Concentrates Learning Signal
- **Claim:** Sampling prompts from a dynamic medium-difficulty band improves sample-efficiency over uniform or tail-heavy sampling.
- **Mechanism:** A dual-heap pool tracks a boundary between high (easy) and low (hard) pool-statistic partitions. Sampling alternates between "easiest hard" and "hardest easy" items, producing mixed-success rollout groups that yield higher-magnitude, lower-variance within-prompt advantages.
- **Core assumption:** Medium-difficulty prompts produce non-degenerate advantages that are more informative than all-correct or all-incorrect groups.
- **Evidence anchors:**
  - [abstract]: "maintains a bounded prompt pool, samples near a dynamic boundary between hard and easy prompts using heap-based boundary sampling."
  - [§6.2, Figure 3]: Heap-based sampling shifts probability mass toward medium-reward regions.
  - [corpus]: DAST (arXiv:2503.09029) argues for difficulty-aware focus but does not implement boundary tracking.
- **Break condition:** If pool-statistic estimates become stale, the boundary may misalign, causing over-selection of now-easy or now-hard prompts.

### Mechanism 2: On-Policy Query Augmentation Expands the Frontier Aligned with the Student
- **Claim:** Generating new queries from the student policy and verifying answers with a lightweight teacher yields higher-reward, higher-advantage signals over training.
- **Mechanism:** Conditioned on sampled prompts, π_θ generates candidate augmented queries via controlled edits (numeric perturbations). An external teacher T asynchronously annotates verifiable answers, forming new (prompt, ground-truth) pairs inserted as cold items.
- **Core assumption:** The teacher provides sufficiently accurate answers for policy-generated queries, and augmented queries remain structurally valid.
- **Evidence anchors:**
  - [abstract]: "grows the pool on-policy via student-generated queries annotated by a lightweight teacher."
  - [§6.1, Figure 2]: Augmented queries start with lower mean reward than seed queries but their reward and advantage signals rise more sharply with training and model scale.
  - [corpus]: Rethinking On-policy Optimization for Query Augmentation (arXiv:2510.17139) explores on-policy query augmentation for IR, not RLVR.
- **Break condition:** If teacher verification is noisy or delayed, unverified/incorrectly labeled queries may enter the pool.

### Mechanism 3: Lineage-Aware Pool Statistics Re-estimation Stabilizes Correlated Augments
- **Claim:** Topology-aware aggregation over the augmentation graph prevents correlated inserts from distorting sampling priorities.
- **Mechanism:** An augmentation lineage graph G_t tracks parent-to-child edges. Pool statistics are periodically refreshed by propagating descendant signals upward using PathAgg or ChildAgg.
- **Core assumption:** The lineage graph is approximately a DAG or tree, and descendant signals are predictive of parent region learnability.
- **Evidence anchors:**
  - [abstract]: "stabilizes correlated augments with lineage-aware pool statistics re-estimation and controlled reinsertion."
  - [§5.2, Table 1]: PathAgg consistently matches or exceeds ChildAgg across datasets and optimizers.
  - [corpus]: No corpus papers implement lineage-aware re-estimation for RLVR.
- **Break condition:** If the graph develops dense cycles or missing nodes, aggregation may become unreliable.

## Foundational Learning

- **Concept: Group-based RLVR (GRPO/DAPO)**
  - **Why needed here:** HeaPA modifies sampling, not the policy update rule. Understanding how within-prompt baselines and advantages rely on mixed-success rollout groups is essential.
  - **Quick check question:** Given a group of 16 rollouts with 15 correct and 1 incorrect, compute the per-rollout advantages—do they provide strong learning signal?

- **Concept: Heap Data Structures**
  - **Why needed here:** The dual-heap pool enables O(log N) boundary tracking and sampling.
  - **Quick check question:** Explain how to efficiently retrieve the minimum of a max-heap and the maximum of a min-heap simultaneously.

- **Concept: Asynchronous Producer-Consumer Pipelines**
  - **Why needed here:** Teacher verification runs asynchronously; the main RL loop drains verified results without blocking.
  - **Quick check question:** Sketch a design where a worker thread enqueues tasks and a separate thread consumes and returns results without stalling the main training loop.

## Architecture Onboarding

- **Component map:**
  - Cold queue (unscored) -> Dual heaps (low/high partitions) -> Boundary sampler -> RLVR rollout engine -> Archive -> On-policy augmentation -> Teacher verification -> Lineage graph -> Pool statistics refresh -> Reinsertion controller

- **Critical path:**
  1. BOUNDARYSAMPLE draws a batch from the dual-heap pool.
  2. Rollout engine generates and verifies responses; updates per-record pool statistics.
  3. Trained records move to archive; on-policy augmentation proposes new candidates.
  4. Teacher verification workers return verified pairs; these are inserted into the cold queue.
  5. On recycle trigger, REFRESHPOOLSTATISTIC propagates signals via lineage graph, and REINSERTBATCHED returns archived records.

- **Design tradeoffs:**
  - Heap count (H): H=2 (default) performs best; higher H degrades due to over-partitioning the hard tail.
  - Augmentation budget: Generating too many candidates can overwhelm verification and introduce correlated bursts.
  - Teacher strength vs. cost: The paper uses GPT-5-nano; weaker teachers may increase label noise, stronger teachers increase latency and cost.
  - PathAgg vs. ChildAgg: PathAgg is more stable under unbalanced subtrees; ChildAgg is simpler but noisier.

- **Failure signatures:**
  - Stale boundary: Pool statistics not refreshed frequently enough → sampling concentrates on now-saturated prompts.
  - Correlated insert burst: Many children from few parents without re-estimation → boundary oscillates, curriculum becomes unstable.
  - Verification backlog: Teacher latency exceeds augmentation rate → pool growth stalls, cold queue empties.
  - Over-partitioning with H>10: Sampler increasingly selects extreme-hard prompts → near-zero advantages, wasted rollouts.

- **First 3 experiments:**
  1. Ablate heap count: Run H∈{2,5,10,15,20} on a small corpus; confirm H=2 maximizes average accuracy.
  2. Isolate augmentation contribution: Disable on-policy augmentation; compare against full HeaPA to quantify gains from pool growth vs. sampling alone.
  3. Stress-test verification latency: Artificially delay teacher responses; measure pool growth rate and final performance to identify the latency tolerance threshold.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the heuristic of restricting augmentations to numeric changes limit the exploration of the reasoning frontier compared to semantic or structural augmentations?
- **Open Question 2:** How does HeaPA perform in domains with noisy or non-discrete reward signals (e.g., code generation or open-ended reasoning), given the current reliance on exact match verification?
- **Open Question 3:** To what extent does the verification teacher's capability limit the pool's growth into higher-difficulty regions?

## Limitations
- The paper does not specify the partition fraction α and rebalancing triggers for the dual-heap pool, leaving an ambiguity in the sampling logic.
- The exact controlled reinsertion batch size, timing, and optional easy-item mixing proportion are not detailed, which may impact pool diversity and sampling stability.
- While lineage-aware refresh stabilizes correlated augments in the experiments, the paper does not explore scenarios where the augmentation graph becomes cyclic or where verification latency exceeds augmentation rate.

## Confidence
- **High confidence** in the empirical claim that heap-based boundary sampling improves sample-efficiency over uniform or tail-heavy sampling, given consistent gains across datasets, optimizers, and benchmarks.
- **High confidence** that on-policy augmentation yields higher-reward, higher-advantage signals over training, supported by reward trajectory analysis and model-scale experiments.
- **Medium confidence** in the claim that lineage-aware pool statistics re-estimation is essential for stability, as the paper shows improved performance over naive re-estimation but does not report failure cases where the lineage graph is dense or delayed.
- **Medium confidence** in the generality of the approach, as experiments are limited to math and reasoning benchmarks and do not include code, multimodal, or open-ended generation tasks.

## Next Checks
1. **Isolate augmentation contribution**: Run HeaPA with on-policy augmentation disabled; compare sample-efficiency and final accuracy against full HeaPA to quantify gains from pool growth vs. sampling alone.
2. **Stress-test verification latency**: Artificially delay teacher responses; measure pool growth rate, sampling diversity, and final performance to identify the latency tolerance threshold.
3. **Test curriculum robustness**: Intentionally introduce bursts of correlated augments without lineage refresh; observe whether boundary oscillation or curriculum collapse occurs, and whether PathAgg vs. ChildAgg mitigates the effect.