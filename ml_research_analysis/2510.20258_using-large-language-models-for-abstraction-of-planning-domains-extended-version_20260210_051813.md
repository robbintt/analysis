---
ver: rpa2
title: Using Large Language Models for Abstraction of Planning Domains - Extended
  Version
arxiv_id: '2510.20258'
source_url: https://arxiv.org/abs/2510.20258
tags:
- domain
- abstraction
- room
- actions
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates using GPT-4o to automatically generate
  abstract PDDL planning domains and problem instances from concrete ones based on
  a given abstraction purpose. Three categories of abstractions are explored: abstracting
  over alternative concrete actions, sequences of actions, and action/predicate parameters.'
---

# Using Large Language Models for Abstraction of Planning Domains - Extended Version

## Quick Facts
- arXiv ID: 2510.20258
- Source URL: https://arxiv.org/abs/2510.20258
- Reference count: 40
- Key outcome: GPT-4o can generate mostly correct PDDL abstractions, with near 100% accuracy for parameter abstraction but higher error rates for complex sequence abstractions.

## Executive Summary
This paper investigates using GPT-4o to automatically generate abstract PDDL planning domains and problem instances from concrete ones based on a given abstraction purpose. Three categories of abstractions are explored: abstracting over alternative concrete actions, sequences of actions, and action/predicate parameters. Experiments with 35 benchmark examples show that GPT-4o can generate mostly correct results, especially for parameter abstraction (near 100% accuracy), but makes more mistakes as abstraction complexity increases, with action sequence abstraction being the most challenging. The study demonstrates that LLMs can effectively produce useful planning abstractions in simple settings, though quality degrades with difficulty.

## Method Summary
The study employs GPT-4o via Azure OpenAI to generate abstract PDDL domains from concrete ones, guided by natural language abstraction purposes. The approach uses zero-shot prompting for parameter abstraction and one-shot prompting for other categories, augmented with chain-of-thought reasoning and role-play prompting. Thirty-five benchmark examples across four abstraction types (alternative actions, action sequences, parameter abstraction, and alternative sequences) are used for evaluation. Generated domains are validated using VAL for syntax checking and Fast Downward for planning validity, with final semantic correctness determined by human experts.

## Key Results
- GPT-4o achieves near 100% accuracy for parameter abstraction tasks
- Accuracy decreases significantly for action sequence abstraction, which is identified as the most challenging category
- The LLM generates mostly correct results for simple abstraction tasks but shows quality degradation as abstraction complexity increases
- Human evaluation confirms the semantic correctness of generated abstractions, with automated tools used for syntax validation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs effectively perform *ontological merging* by leveraging semantic priors to group subtypes into generalized super-types.
- **Mechanism:** The model utilizes embedded linguistic knowledge to identify "is-a" relationships (e.g., "hotel" and "airbnb" are both "accommodations") and applies this mapping to PDDL types, predicates, and parameters.
- **Core assumption:** The concrete domain uses terminology that aligns with the semantic space the LLM was trained on.
- **Evidence anchors:** Example 1 explicitly demonstrates merging `hotel` and `airbnb` into `accommodation`.
- **Break condition:** The concrete domain uses jargon or arbitrary naming conventions that lack semantic relation.

### Mechanism 2
- **Claim:** Structured reasoning via Chain-of-Thought (CoT) and Role-Play prompting reduces syntax errors in simple parameter abstraction.
- **Mechanism:** By forcing the LLM to adopt an expert persona and explicitly "think step-by-step" about what to remove versus what to keep, the system reduces hallucination rates for straightforward removal tasks.
- **Core assumption:** The LLM can reliably follow the specific algorithmic steps defined in the system prompt.
- **Evidence anchors:** Table 4 shows near 100% success for "Abstraction of Action/Predicate Parameters" using zero-shot prompting.
- **Break condition:** The reasoning chain exceeds the model's context window or logical tracking capacity.

### Mechanism 3
- **Claim:** Performance degrades significantly when abstraction requires inferring dynamic temporal sequences (merging actions) due to difficulties in maintaining state consistency.
- **Mechanism:** While LLMs handle static type hierarchies well, they struggle to compress multi-step action sequences while correctly synthesizing the intermediate preconditions and effects into valid STRIPS syntax.
- **Core assumption:** The "purpose of abstraction" provided in natural language is sufficient for the model to deduce complex dynamic correspondences without explicit mapping rules.
- **Evidence anchors:** "Action sequence abstraction being the most challenging... quality degrades with difficulty."
- **Break condition:** The user provides a low-level domain with high branching factors or opaque action names.

## Foundational Learning

- **Concept:** **PDDL (Planning Domain Definition Language) - STRIPS Fragment**
  - **Why needed here:** The paper relies entirely on the LLM's ability to read and write valid PDDL syntax. Understanding STRIPS limitations is crucial for debugging the LLM's errors.
  - **Quick check question:** Can you identify why using the keyword "OR" inside an action precondition violates the STRIPS fragment used in this paper?

- **Concept:** **Bisimulation & Refinement Mapping (BDL17 Framework)**
  - **Why needed here:** This is the theoretical gold standard the paper measures success against. An abstraction is "sound" if every high-level action/trace corresponds to a valid low-level trace.
  - **Quick check question:** If a high-level action `book_transport` abstracts `book_flight` and `book_train`, what low-level condition must be true for the abstraction to be considered "sound"?

- **Concept:** **In-Context Learning (Zero-shot vs One-shot)**
  - **Why needed here:** The paper evaluates these strategies differently for different abstraction types.
  - **Quick check question:** Why might providing a single example help the LLM understand the *level* of granularity required for an abstraction better than a natural language instruction alone?

## Architecture Onboarding

- **Component map:** Input (Concrete PDDL + Natural Language Purpose) -> Prompting Module (System/User messages with CoT & Role-Play) -> LLM Core (GPT-4o) -> Validation Module (VAL -> Fast Downward) -> Human Evaluator
- **Critical path:** The prompt engineering in the **Prompting Module**. If the natural language "purpose of abstraction" is ambiguous, or if the system prompt fails to constrain the output to strict STRIPS, the validation modules will reject the generated PDDL.
- **Design tradeoffs:**
  - *One-shot vs. Zero-shot:* One-shot yields higher accuracy for complex "Alternative Actions" but requires curating examples. Zero-shot is cheaper/faster but hallucinates more on complex logic.
  - *Automated vs. Human Validation:* The architecture relies on human experts for final validation because multiple valid abstractions exist for a single domain.
- **Failure signatures:**
  - **STRIPS Violations:** Generated PDDL includes `or`, `not`, or `when` in preconditions
  - **Predicate Drift:** The LLM invents new predicates in the high-level domain
  - **Over-abstraction:** Merging distinct actions into a generic type
- **First 3 experiments:**
  1. **Parameter Abstraction Test:** Run the `Travel02` example (Zero-shot) to verify near-100% success rate in removing a specific type.
  2. **Sequence Stress Test:** Run a `DeliveryRobot` domain (One-shot) to see if the LLM correctly merges sequential actions.
  3. **Syntax Check:** Specifically prompt for "Alternative Sequences" to reproduce the "incorrect use of keyword OR" error.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs generate the refinement mapping required to formally verify that the generated abstraction is sound and complete relative to the concrete domain?
- **Basis in paper:** Section 8 states the intention to "examine extending the current abstraction task to incorporate the generation of a refinement mapping."
- **Why unresolved:** The current study focuses solely on generating PDDL domains and problem instances, leaving the formal connection between high-level and low-level theories undefined.
- **What evidence would resolve it:** A system where the LLM outputs a valid mapping that allows an automated verifier to confirm soundness and completeness properties.

### Open Question 2
- **Question:** To what extent can human expert evaluation be replaced by automated validation tools for verifying the semantic correctness of LLM-generated abstractions?
- **Basis in paper:** Section 8 identifies the "development of (partially) automated validation mechanisms" as a significant area for future work.
- **Why unresolved:** Current evaluation relies heavily on human experts because syntax validators cannot assess semantic correctness or handle the variability in naming conventions and structure.
- **What evidence would resolve it:** An automated tool capable of detecting semantic errors in complex abstractions without human intervention.

### Open Question 3
- **Question:** Can LLMs effectively generate abstractions for planning domains defined in the more expressive ADL fragment of PDDL?
- **Basis in paper:** Section 8 lists considering "the more expressive ADL fragment of PDDL" as a specific future direction.
- **Why unresolved:** The experiments in this paper were restricted to the STRIPS fragment, which lacks complex features like conditional effects and disjunctive preconditions.
- **What evidence would resolve it:** Experimental results on a benchmark suite of ADL domains showing similar accuracy levels to those achieved in the STRIPS experiments.

### Open Question 4
- **Question:** How can the "appropriate level" of abstraction be formalized or constrained to prevent the LLM from over-abstracting or under-abstracting?
- **Basis in paper:** Section 6 notes that the authors "do not impose a precise level of abstraction, and leave investigating appropriate levels of abstraction" for future work.
- **Why unresolved:** The paper observes that LLMs may abstract concepts to varying degrees that may not align with the specific needs of a planning problem.
- **What evidence would resolve it:** Prompting strategies or fine-tuning methods that allow users to specify or control the granularity of the abstraction output.

## Limitations

- The paper demonstrates LLM-based abstraction in controlled benchmark settings, but real-world domains may contain noisy or ambiguous terminology that breaks semantic priors.
- Human evaluation metrics (CN/AUC) lack precise definitions in the paper, making it difficult to replicate or compare scoring standards across studies.
- The one-shot examples are not provided in full, so exact correspondence between prompts and generated outputs cannot be validated without reconstruction.

## Confidence

- **High:** GPT-4o reliably performs parameter abstraction (near 100% accuracy) using zero-shot prompting with CoT and role-play.
- **Medium:** LLMs can abstract over alternative concrete actions and action sequences, but with higher error rates and syntax violations in complex cases.
- **Low:** The study's benchmarks represent a curated subset of planning domains; generalization to arbitrary domains with diverse naming conventions remains unproven.

## Next Checks

1. **Syntax Stress Test:** Feed the LLM domains with domain-specific jargon (e.g., "typeA" and "typeB") to test whether semantic priors break down and cause abstraction failures.
2. **Parameter Drift Analysis:** For parameter abstraction examples, systematically remove the chain-of-thought prompt and measure the increase in syntax or semantic errors.
3. **Sequence Compression Test:** Construct a multi-step domain where sequential actions must be merged while preserving intermediate state constraints; check if the LLM hallucinates new predicates or loses ordering logic.