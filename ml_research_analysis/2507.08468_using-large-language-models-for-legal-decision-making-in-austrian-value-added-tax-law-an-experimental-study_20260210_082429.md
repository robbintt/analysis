---
ver: rpa2
title: 'Using Large Language Models for Legal Decision-Making in Austrian Value-Added
  Tax Law: An Experimental Study'
arxiv_id: '2507.08468'
source_url: https://arxiv.org/abs/2507.08468
tags:
- cases
- legal
- place
- supply
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models for legal decision-making
  in Austrian VAT law using fine-tuning and retrieval-augmented generation. Textbook
  and real-world VAT cases were used to assess accuracy in identifying the place of
  supply and providing legal justifications.
---

# Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study

## Quick Facts
- arXiv ID: 2507.08468
- Source URL: https://arxiv.org/abs/2507.08468
- Reference count: 35
- Large language models (GPT-4o) for Austrian VAT law show RAG outperforms fine-tuning in accuracy and justification quality, but full automation remains impractical.

## Executive Summary
This study evaluates large language models for legal decision-making in Austrian value-added tax (VAT) law, focusing on determining the place of supply and providing legally grounded justifications. Using 74 textbook cases and 20 real-world cases, the authors compare retrieval-augmented generation (RAG) with fine-tuning and their combination. RAG achieved 93.24% accuracy on textbook cases and provided higher-quality justifications (80% correct) for real-world cases compared to fine-tuning (70%). While RAG excels in accuracy and justification, limitations include handling implicit client knowledge and missing contextual documents. The findings suggest RAG is effective for VAT legal reasoning, though full automation remains impractical due to the sensitivity of legal contexts.

## Method Summary
The study fine-tunes GPT-4o in Azure AI Foundry and employs retrieval-augmented generation (RAG) with Austrian/EU VAT legal documents. The RAG system uses text-embedding-ada-002 with chunk size 1024, overlap 0, top k 5, and efSearch 800. Fine-tuning uses batch size 16, learning rate multiplier 2.8, and 3 epochs. The evaluation uses 74 textbook VAT cases from Berger and Wakounig and 20 real-world cases from a tax firm. Accuracy is measured at the country level for place-of-supply identification, and justification correctness is manually evaluated by tax experts. The study compares RAG, fine-tuning, and their combination across these datasets.

## Key Results
- RAG achieved 93.24% accuracy on textbook cases versus 89.19% for fine-tuning and 86.49% for their combination.
- For real-world cases, RAG provided 80% correct justifications versus 70% for fine-tuning.
- RAG excelled in accuracy and justification quality, though full automation remains impractical due to missing contextual documents and implicit client knowledge.

## Why This Works (Mechanism)
RAG outperforms fine-tuning because it provides access to comprehensive legal documents during inference, allowing the model to ground its reasoning in actual regulatory text. The retrieval mechanism ensures that legal justifications are based on authoritative sources rather than relying solely on patterns learned during training. This is particularly important for VAT law where specific regulations and their interpretation determine the correct outcome. The chunk size of 1024 and top-k=5 parameters effectively balance retrieval precision with context richness, while the efSearch=800 optimizes search quality for legal terminology.

## Foundational Learning
- **VAT place-of-supply rules**: Austrian and EU regulations determine tax jurisdiction based on service location, goods delivery, or recipient's business location. Needed for understanding legal context and correct model outputs. Quick check: verify model identifies correct country for cross-border services.
- **RAG retrieval parameters**: Chunk size, overlap, and top-k affect retrieval quality and context relevance. Needed to optimize document retrieval for legal queries. Quick check: compare accuracy with chunk overlap 0 vs 100.
- **Fine-tuning hyperparameters**: Batch size, learning rate multiplier, and epochs control model adaptation to VAT domain. Needed to balance performance and overfitting. Quick check: monitor validation loss across 3 epochs.

## Architecture Onboarding
**Component map:** Legal documents -> Embedding + Chunking -> RAG Retriever -> LLM (GPT-4o) -> Output (Country + Justification)

**Critical path:** User query → RAG retrieval (legal documents) → Prompt template (Listing 1) → LLM → JSON output (country, justification)

**Design tradeoffs:** RAG vs fine-tuning: RAG provides accurate, context-rich justifications but may include irrelevant information; fine-tuning is faster but struggles with legal reasoning depth.

**Failure signatures:**
- Model outputs multiple countries → check JSON output and system prompt emphasis
- Country prediction contradicts justification → reorder JSON keys in prompt template
- RAG retrieval noise → use chunk overlap = 0 and top-k = 5
- Fine-tuning overfitting → monitor validation loss, limit to 3 epochs

**First experiments:**
1. Test RAG accuracy on 74 textbook cases with chunk overlap 0 and top-k 5
2. Evaluate fine-tuning on textbook cases with batch size 16, LR mult 2.8, 3 epochs
3. Compare justification quality for real-world cases between RAG and fine-tuning

## Open Questions the Paper Calls Out
The paper raises questions about the generalizability of these results to other legal domains beyond VAT law, and whether the observed performance differences between RAG and fine-tuning would persist with different model architectures or parameter configurations. The authors also question how to best handle implicit client knowledge that may not be explicitly stated in case descriptions, and whether automated legal decision-making tools can be responsibly deployed given the high stakes of incorrect legal advice.

## Limitations
- Fine-tuning dataset includes non-public lecture notes and exam questions, limiting exact reproduction
- Manual justification evaluation lacks documented rubric, introducing potential rater bias
- Real-world cases have missing attachments, and enrichment process is incompletely specified
- The study focuses specifically on Austrian VAT law, limiting generalizability to other jurisdictions
- No quantitative measure of justification quality beyond binary correct/incorrect classification

## Confidence
- RAG accuracy claim (93.24% vs 89.19%): Medium - transparent methodology but limited public data
- Justification quality claim (80% vs 70%): Medium - manual evaluation lacks documented rubric
- Full automation impracticality: High - authors appropriately acknowledge sensitivity of legal contexts

## Next Checks
1. Replicate RAG accuracy on the publicly available 74 textbook cases to verify the 93.24% baseline
2. Apply the evaluation rubric to real-world cases to test inter-rater reliability of justification correctness
3. Conduct ablation studies with varying chunk overlaps and top-k values to confirm that overlap=0 and top-k=5 are optimal for this legal domain
4. Test whether the performance advantage of RAG extends to other areas of tax law beyond VAT
5. Investigate methods for automatically detecting and incorporating implicit client knowledge from case descriptions