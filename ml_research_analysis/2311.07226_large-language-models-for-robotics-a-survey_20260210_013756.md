---
ver: rpa2
title: 'Large Language Models for Robotics: A Survey'
arxiv_id: '2311.07226'
source_url: https://arxiv.org/abs/2311.07226
tags:
- language
- robot
- robotics
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Large Language
  Models (LLMs) in robotics, covering their applications, challenges, and future directions.
  LLMs have significantly enhanced robot capabilities in perception, decision-making,
  control, and interaction by leveraging their natural language understanding and
  reasoning abilities.
---

# Large Language Models for Robotics: A Survey

## Quick Facts
- arXiv ID: 2311.07226
- Source URL: https://arxiv.org/abs/2311.07226
- Reference count: 40
- One-line primary result: Comprehensive survey of LLMs in robotics covering applications, challenges, and future directions

## Executive Summary
This survey examines how Large Language Models are transforming robotics by enabling advanced perception, decision-making, control, and human-robot interaction. LLMs enhance robot capabilities through their natural language understanding and reasoning abilities, leading to innovations like Vision-Language-Action models, multi-agent systems, and embodied intelligence. The integration allows robots to perform complex tasks, adapt to dynamic environments, and interact more naturally with humans, though significant challenges remain in dataset scarcity, training transfer, security, and ethical considerations.

## Method Summary
The survey synthesizes existing research on LLM applications in robotics, focusing on Vision-Language-Action (VLA) models that integrate perception, language understanding, and action generation. The primary method involves using LLMs as cognitive planners that decompose high-level instructions into executable subtasks, while VLA models convert these plans into continuous action sequences. Training procedures combine large-scale internet data with robot trajectory data, using transformer architectures to output tokenized action sequences. The approach emphasizes closed-loop feedback mechanisms for error recovery and task verification.

## Key Results
- LLMs significantly improve long-horizon task success by decomposing abstract instructions into executable sub-goals
- VLA models enhance generalization by treating robotic control as sequence generation, integrating vision, language, and action in shared latent spaces
- Closed-loop feedback mechanisms mitigate execution errors by grounding LLM reasoning in physical reality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs improve long-horizon task success by decomposing high-level instructions into executable sub-goals
- **Mechanism:** LLMs act as semantic reasoning engines using pre-trained world knowledge and chain-of-thought prompting to parse abstract goals into actionable steps
- **Core assumption:** LLMs possess sufficient world knowledge for physically feasible steps and robots have compatible low-level skills
- **Evidence anchors:** Abstract mentions "improved task success rates through LLM-based planning frameworks"; section 4.2.1 describes LLM reasoning for subtask decomposition
- **Break condition:** Fails when instructions require physical nuance absent from text-based training or context window is exceeded

### Mechanism 2
- **Claim:** VLA models enhance generalization by treating robotic control as sequence generation
- **Mechanism:** VLA models tokenize actions alongside visual and text tokens, learning shared latent spaces where semantic understanding influences motor control
- **Core assumption:** Correlation exists between visual-textual patterns in pre-training data and required physical dynamics
- **Evidence anchors:** Abstract highlights "enhanced generalization via vision-language-action (VLA) models"; section 4.3.2 describes integration of perception, language, and action generation
- **Break condition:** Fails in out-of-distribution environments with significantly different visual inputs

### Mechanism 3
- **Claim:** Closed-loop feedback mechanisms mitigate execution errors by grounding LLM reasoning in physical reality
- **Mechanism:** Frameworks use verify-then-act loops where post-execution sensory data is fed back to LLM for success assessment and replanning
- **Core assumption:** Robot perception can reliably detect task failures and LLM can interpret signals to correct logic
- **Evidence anchors:** Section 3.1.2 describes "Agentic Robot" using temporal verifier; section 5.3 discusses REFLECT querying LLMs for failure reasoning
- **Break condition:** Fails if error detection latency is too high or verifier logic is brittle

## Foundational Learning

- **Concept: Transformer Sequence Modeling**
  - **Why needed here:** Both "Brain" (LLM) and "Body" (VLA) rely on Transformer architectures; understanding attention and sequence-to-sequence generation is crucial
  - **Quick check question:** Can you explain how a token in a sequence attends to previous tokens to predict the next step?

- **Concept: Affordance and Grounding**
  - **Why needed here:** Core challenge is "grounding" - linking text to physical objects (mapping "cup" to visual geometry and grasping action)
  - **Quick check question:** How does a robot determine how to interact with an object based solely on visual input?

- **Concept: Reinforcement Learning vs. Imitation Learning**
  - **Why needed here:** Survey discusses training VLA models on datasets (Imitation) vs. optimizing for rewards (RL)
  - **Quick check question:** What is the primary difference in data requirements between training via behavior cloning versus online reinforcement learning?

## Architecture Onboarding

- **Component map:** Perception Layer (Vision Encoder + Pre-trained VLM) -> Decision Layer (LLM) -> Control Layer (VLA Policy) -> Feedback Loop (Verifier/Reflector)
- **Critical path:** Translation of LLM's textual plan into VLA's continuous action vector; interface misalignment causes hallucinated actions or freezing
- **Design tradeoffs:**
  - Latency vs. Capability: Large cloud-based LLM offers better reasoning but introduces latency; smaller local VLA is faster but lacks deep reasoning
  - Generalization vs. Precision: Models trained on massive diverse datasets generalize better but may lack task-specific precision
- **Failure signatures:**
  - Hallucinated Plans: LLM proposes physically impossible steps (e.g., "Pour water" when cup is empty)
  - Grounding Errors: VLA attends to wrong object in cluttered scene
  - Tokenization Mismatches: VLA outputs action sequence exceeding robot's physical limits
- **First 3 experiments:**
  1. Run OpenVLA on a Simulation: Load pre-trained VLA in LIBERO or simple pick-place sim to observe zero-shot generalization
  2. Implement "Say-Can" Style Planner: Connect LLM API to primitive skills (pick(x), move_to(y)) and test ordering based on natural language prompts
  3. Analyze Failure Recovery: Introduce perturbation during task execution and implement text-based feedback loop for replanning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can autonomous systems guarantee physical actions generated by LLMs do not cause unintended harm?
- **Basis in paper:** Explicitly identifies "Security of Task Executing" as top priority; current systems focus on affordance rather than consequences
- **Why unresolved:** Systems determine feasible actions based on current state without fully modeling dangerous consequences
- **What evidence would resolve it:** Verification mechanisms predicting physical outcomes and constraining action generation to ensure safety boundaries

### Open Question 2
- **Question:** How can models trained in simulation effectively transfer to real-world scenarios despite feedback mechanism disparities?
- **Basis in paper:** Highlights "Training Scenario Transfer" as critical direction; models suffer drastic performance drops (90% to 10%) when moving from simulation to reality
- **Why unresolved:** Disparity lies in feedback mechanisms; simulated environments provide straightforward feedback while real-world feedback is complex and nuanced
- **What evidence would resolve it:** Training methodologies or "world models" bridging simulation dynamics and real-world physics without losing accuracy

### Open Question 3
- **Question:** How can we unify processing of modalities beyond vision and language (tactile, olfactory) within single robotic architecture?
- **Basis in paper:** Section 8.3 points out VLA models handle vision and language but lack unified processing for other inputs like touch or smell
- **Why unresolved:** Existing architectures lack standardized method to tokenize/embed non-visual/non-linguistic sensory data compatible with Transformer architectures
- **What evidence would resolve it:** Multi-modal model architecture successfully integrating haptic or olfactory feedback into action generation pipeline alongside visual and textual inputs

## Limitations
- Survey lacks specific implementation details for coordination mechanisms between LLMs and robotic controllers
- Training hyperparameters and model architectures described at high level without sufficient technical specificity
- Limited empirical data on failure rates and performance bounds across diverse robotic tasks
- Ethical considerations and security measures discussed conceptually but lack concrete frameworks or metrics

## Confidence
- **High Confidence:** Survey accurately describes existing VLA models (RT-2, OpenVLA) and their general capabilities
- **Medium Confidence:** Claims about improved task success rates through LLM-based planning are supported by references but lack specific performance benchmarks
- **Low Confidence:** Predictions about future developments and unresolved challenges lack empirical validation

## Next Checks
1. Reproduce OpenVLA zero-shot generalization capabilities in a simulation environment like LIBERO
2. Implement a simple "Say-Can" style planner connecting LLM API to primitive robot skills
3. Test failure recovery by introducing perturbations during task execution and implementing text-based feedback loops