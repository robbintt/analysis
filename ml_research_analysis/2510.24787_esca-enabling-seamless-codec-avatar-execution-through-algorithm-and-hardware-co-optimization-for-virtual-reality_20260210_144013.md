---
ver: rpa2
title: 'ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware
  Co-Optimization for Virtual Reality'
arxiv_id: '2510.24787'
source_url: https://arxiv.org/abs/2510.24787
tags:
- quantization
- activation
- codec
- avatar
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying high-fidelity codec
  avatars on resource-constrained VR devices by proposing a comprehensive algorithm-hardware
  co-design framework. The core method introduces two key innovations: input channel-wise
  activation smoothing (ICAS) to equalize activation scales across channels, and facial-feature-aware
  smoothing (FFAS) to preserve critical facial details.'
---

# ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality

## Quick Facts
- arXiv ID: 2510.24787
- Source URL: https://arxiv.org/abs/2510.24787
- Reference count: 40
- Key outcome: +0.39 FovVideoVDP improvement over 4-bit baseline, 3.36× latency reduction, 100 FPS on VR hardware

## Executive Summary
This paper addresses the challenge of deploying high-fidelity codec avatars on resource-constrained VR devices by proposing a comprehensive algorithm-hardware co-design framework. The core method introduces two key innovations: input channel-wise activation smoothing (ICAS) to equalize activation scales across channels, and facial-feature-aware smoothing (FFAS) to preserve critical facial details. These techniques are combined with a UV-weighted Hessian-based quantization strategy to achieve high-quality 4-bit inference. Additionally, a custom hardware accelerator is designed to exploit structured sparsity in transposed convolutions, delivering a 3.36× latency reduction. Experimental results demonstrate up to +0.39 improvement in FovVideoVDP quality scores over the best 4-bit baseline and sustained 100 FPS rendering rate, enabling real-time photorealistic avatar execution on edge VR platforms.

## Method Summary
The ESCA framework combines algorithmic and hardware optimizations for efficient codec avatar execution. Algorithmically, it introduces ICAS to smooth activation scales across channels and FFAS to preserve critical facial features during quantization. These are paired with UV-weighted Hessian-based quantization for 4-bit inference. On the hardware side, a custom accelerator exploits structured sparsity in transposed convolutions through input-combining logic, reducing latency by 3.36×. The method targets the decoder stage of photorealistic codec avatars, using MultiFace dataset for evaluation and achieving 100 FPS rendering rates while maintaining visual fidelity.

## Key Results
- +0.39 FovVideoVDP improvement over best 4-bit baseline
- 3.36× latency reduction on custom hardware accelerator
- Sustained 100 FPS rendering rate on VR hardware
- High-quality 4-bit inference without retraining

## Why This Works (Mechanism)

### Mechanism 1: Input Channel-wise Activation Smoothing (ICAS)
- **Claim:** Equalizing activation scales across channels reduces quantization error caused by outlier values, enabling stable 4-bit inference.
- **Mechanism:** The paper proposes scaling input channels by a factor $s_c$ and inversely scaling the corresponding weight kernels ($1/s_c$). This mathematical equivalence ($Y = \tilde{X} * \tilde{W}$) migrates the "difficulty" of quantization from the activations (which have outliers) to the weights (which are more stable), effectively smoothing the activation distribution without retraining.
- **Core assumption:** The outlier activation patterns are consistent enough that static smoothing factors determined during calibration generalize well to unseen inference data.
- **Evidence anchors:**
  - [abstract]: "...introduces... input channel-wise activation smoothing (ICAS) to equalize activation scales across channels..."
  - [Page 4]: "This formulation introduces a pair of transformations... The scale values are carefully determined using the calibration dataset to yield activations that are more suitable for quantization..."
  - [corpus]: Corpus evidence regarding specific quantization mathematics for avatars is missing; related papers focus on avatar social perception rather than compression mechanisms.
- **Break condition:** If the input distribution shifts significantly (e.g., extreme lighting changes causing new outlier patterns), the pre-computed smoothing factors may fail to contain activation ranges, leading to clipping or overflow.

### Mechanism 2: Facial-Feature-Aware Smoothing (FFAS)
- **Claim:** Uniform smoothing degrades high-frequency details; selectively preserving variance in critical regions maintains visual fidelity.
- **Mechanism:** FFAS uses facial region masks to calculate activation variance per channel in areas like the eyes and mouth. It identifies "sensitive" channels (top-k% variance) and exempts them from ICAS smoothing (sets $s_c=1$). This prevents the suppression of fine details (e.g., lip creases) while still smoothing less critical background channels.
- **Core assumption:** High activation variance within a masked region directly correlates with perceptual importance to the human viewer.
- **Evidence anchors:**
  - [abstract]: "...facial-feature-aware smoothing (FFAS) to preserve critical facial details."
  - [Page 6]: "FFAS exempts these top-k% channels from ICAS smoothing... preserving essential expression details, reduced global artifacts..."
  - [corpus]: Weak. While corpus papers discuss gaze and expression (e.g., AnimeGaze), they do not validate signal processing techniques for preserving these features during quantization.
- **Break condition:** If the rigid mask alignment fails during extreme head poses or expressions, FFAS might smooth critical features or preserve noise in irrelevant background areas.

### Mechanism 3: Structured Sparsity Exploitation via Input-Combining
- **Claim:** The intrinsic "checkerboard" sparsity of transposed convolutions can be hardware-accelerated to reduce latency by ~3.36x.
- **Mechanism:** Transposed convolutions insert zeros (stride effects), creating >85% sparsity in the activation matrix. The accelerator divides these matrices into tiles, discards fully-zero tiles, and vertically stacks the non-zero ones. Modified Processing Elements (PEs) use multiplexers (MUXs) to select the compressed non-zero inputs for Multiply-Accumulate (MAC) operations, avoiding redundant zero-computations.
- **Core assumption:** The sparsity pattern remains sufficiently structured (checkerboard-like) to allow efficient tiling and stacking without excessive indexing overhead.
- **Evidence anchors:**
  - [abstract]: "...custom hardware accelerator is designed to exploit structured sparsity in transposed convolutions, delivering a 3.36× latency reduction."
  - [Page 7]: "This high degree of sparsity significantly degrades the efficiency... input-combining mechanism... to compress the activation input..."
  - [corpus]: Missing. No related hardware architectures for codec avatars were found in the provided corpus.
- **Break condition:** If the model architecture changes to standard convolutions or dense layers, the input-combining logic (optimized for stride-induced zeros) would provide no benefit and could introduce overhead.

## Foundational Learning

- **Concept: Transposed Convolution & Checkerboard Artifacts**
  - **Why needed here:** The paper targets the specific inefficiency of transposed convolutions (zero-insertion) which causes both the sparsity pattern the hardware exploits and the activation outliers the algorithm smooths.
  - **Quick check question:** How does inserting zeros between input pixels (stride > 1) create a structured sparsity pattern in the resulting im2col matrix?

- **Concept: Post-Training Quantization (PTQ) vs. QAT**
  - **Why needed here:** The authors use PTQ because Quantization-Aware Training (QAT) is computationally impractical for the large-scale VAE decoder used in Codec Avatars.
  - **Quick check question:** Why does the paper prefer offline calibration (PTQ) over retraining the network with quantization noise (QAT)?

- **Concept: UV Mapping in Facial Rendering**
  - **Why needed here:** The method uses UV coordinates to weight the Hessian matrix during quantization, prioritizing bits for areas like the nose and eyes over the sides of the head.
  - **Quick check question:** In the context of the UV-weighted Hessian, why would a pixel on the nose have a higher "importance weight" than a pixel on the neck?

## Architecture Onboarding

- **Component map:** VAE Decoder (Transposed Convs) -> Calibration (Compute Smoothing Factors $s_c$) -> FFAS (Mask-based Selection) -> UV-Weighted Quantization -> Custom Hardware Accelerator

- **Critical path:** The decoding stage (specifically the transposed convolution layers) dominates the 39.6ms baseline latency. The interaction between the *Input-Combining* logic and the *Systolic Array* throughput is the primary performance driver.

- **Design tradeoffs:**
  - **Smoothing (ICAS) vs. Detail (FFAS):** ICAS generally helps quantization but blurs details; FFAS restores details but requires mask management.
  - **Area vs. Latency:** The Input-Combining logic adds MUX complexity to the PE but claims to reduce compute cycles by 75% (ideal case).

- **Failure signatures:**
  - **Temporal Flickering:** Indicates smoothing factors ($s_c$) are insufficient or calibration data wasn't representative.
  - **Blurred Eyes/Mouth:** Suggests FFAS threshold ($k$) is too low or masks are misaligned.
  - **Checkerboard Artifacts:** May occur if quantization levels (4-bit) are too coarse for the smoothed activations without proper UV-weighting.

- **First 3 experiments:**
  1. **Latency Profiling:** Measure decoder inference time on the proposed accelerator vs. the Snapdragon XR2 Gen 2 baseline to validate the 3.36x speedup claim.
  2. **Ablation on Smoothing:** Run inference with ICAS-only, FFAS-only, and Full-ESCA to isolate the contribution of facial-feature-aware mask selection on VDP scores.
  3. **Sparsity Stress Test:** Vary the batch size or input resolution to determine if the Input-Combining mechanism maintains efficiency as activation map dimensions change.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Facial-Feature-Aware Smoothing (FFAS) and UV-weighted quantization strategies be effectively adapted to function without explicit UV priors, such as using self-supervised saliency maps?
- **Basis in paper:** [explicit] The Conclusion states: "It relies on accurate facial UV priors... Future work will aim to reduce dependence on UV maps."
- **Why unresolved:** The current method relies on predefined facial region masks and UV coordinates to determine smoothing factors (FFAS) and quantization importance (UV-PTQ). It is untested whether data-driven importance maps can substitute these fixed geometric priors without degrading visual quality.
- **What evidence would resolve it:** Comparative results showing FovVideoVDP (VDP) scores for a UV-free variant of ESCA trained on datasets lacking ground-truth UV maps, demonstrating comparable preservation of facial details.

### Open Question 2
- **Question:** How does the ESCA framework perform when extended to co-optimize the encoding and rendering stages of the VR pipeline?
- **Basis in paper:** [explicit] The Conclusion notes: "It... only accelerates the decoding stage, leaving other parts of the pipeline unoptimized. Future work will... extend co-optimization to the full avatar pipeline."
- **Why unresolved:** The current study isolates the decoder due to its computational intensity from transposed convolutions. The interaction between the proposed 4-bit decoder and a potentially un-optimized encoder or renderer on the resource-constrained SoC remains unknown.
- **What evidence would resolve it:** End-to-end system measurements (latency, power, FPS) on the custom hardware where the encoder is also quantized and accelerated using the proposed techniques.

### Open Question 3
- **Question:** Is the Input Channel-wise Activation Smoothing (ICAS) technique mathematically compatible with decoder architectures utilizing non-linearities that lack scaling invariance (e.g., Sigmoid, Tanh)?
- **Basis in paper:** [inferred] The paper derives the ability to fuse smoothing factors into weights based on the property $\sigma(s \cdot X) = s \cdot \sigma(X)$, explicitly stating adoption of LeakyReLU.
- **Why unresolved:** If a generative decoder uses bounded activation functions like Sigmoid (common in the final output layer or specific GAN architectures), the equality $\text{Sigmoid}(sx) \neq s \cdot \text{Sigmoid}(x)$ holds, potentially preventing the offline fusion of scaling factors and increasing inference overhead.
- **What evidence would resolve it:** An ablation study applying ICAS to a decoder swapping LeakyReLU for Tanh or Sigmoid, measuring the error introduced by the scaling approximation or the latency cost of non-fused online scaling.

### Open Question 4
- **Question:** Can the input-combining hardware mechanism generalize efficiently to other generative architectures (e.g., GANs, Diffusion Upsamplers) that exhibit different stride or padding patterns in transposed convolutions?
- **Basis in paper:** [inferred] The input-combining optimization exploits the specific checkerboard-like sparsity (zeros inserted between pixels) inherent in stride-2 transposed convolutions.
- **Why unresolved:** The efficiency of the input-combining mechanism relies on a specific threshold of structured sparsity (>85%). Architectures with stride-1 transposed convolutions or different padding schemes may not generate sufficient sparsity for the input-combining tiling to yield a throughput benefit over standard dense systolic arrays.
- **What evidence would resolve it:** Benchmarks of the accelerator running diverse generative models (e.g., StyleGAN, Diffusion upscalers) to measure hardware utilization and latency reduction compared to the Codec Avatar baseline.

## Limitations
- **Mask dependency:** FFAS relies on predefined facial region masks, which may not generalize well to extreme expressions or head poses.
- **Architecture specificity:** The input-combining hardware optimization is tailored for transposed convolutions and may not generalize to other generative architectures.
- **UV mapping requirement:** The UV-weighted quantization strategy depends on accurate facial UV priors, limiting its applicability to datasets without ground-truth UV maps.

## Confidence
- **High:** The overall quality improvement claims (FovVideoVDP +0.39 over 4-bit baseline) are supported by the specific mechanisms (ICAS, FFAS, UV-weighted PTQ) described in the paper.
- **Medium:** The 3.36× latency reduction claim is based on the described hardware architecture, but lacks detailed simulation or implementation data to verify its practical impact.
- **Low:** The generalization of the method to different datasets or avatar styles (beyond MultiFace) is not explicitly validated, raising questions about its robustness across diverse scenarios.

## Next Checks
1. **Runtime Smoothing Factor Robustness:** Test ICAS under varying lighting and pose conditions to assess whether static scale factors maintain stability or cause artifacts like clipping.
2. **Mask Precision in FFAS:** Validate the accuracy of facial region masks by evaluating detail preservation (e.g., eye and mouth clarity) across extreme expressions and head poses.
3. **Hardware Adaptability:** Evaluate the input-combining mechanism's efficiency when applied to models with different convolution types (e.g., standard vs. transposed) to confirm its scalability.