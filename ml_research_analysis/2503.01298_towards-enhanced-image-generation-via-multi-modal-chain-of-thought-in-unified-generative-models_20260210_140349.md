---
ver: rpa2
title: Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified
  Generative Models
arxiv_id: '2503.01298'
source_url: https://arxiv.org/abs/2503.01298
tags:
- image
- generation
- arxiv
- generative
- mcot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FoX, a unified generative model that enhances
  image generation by integrating Multimodal Chain of Thought (MCoT). Unlike prior
  approaches that improve basic image generation through architecture refinement,
  FoX addresses complex compositional tasks by decomposing them into planning, acting,
  reflection, and correction steps.
---

# Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models

## Quick Facts
- arXiv ID: 2503.01298
- Source URL: https://arxiv.org/abs/2503.01298
- Authors: Yi Wang; Mushui Liu; Wanggui He; Hanyang Yuan; Longxiang Zhang; Ziwei Huang; Guanghao Zhang; Wenkai Fang; Haoze Jiang; Shengxuming Zhang; Dong She; Jinlong Liu; Weilong Dai; Mingli Song; Hao Jiang; Jie Song
- Reference count: 23
- One-line primary result: FoX achieves 77% GenEval score and state-of-the-art performance on T2I-CompBench through Multimodal Chain of Thought.

## Executive Summary
This paper introduces FoX, a unified generative model that enhances complex image generation through Multimodal Chain of Thought (MCoT). Unlike prior approaches that improve basic image generation through architecture refinement, FoX addresses compositional tasks by decomposing them into planning, acting, reflection, and correction steps. It employs a Functionality-oriented eXperts (FoXperts) architecture that assigns separate experts for visual understanding and generation, avoiding conflicts inherent in modality-oriented designs. Extensive experiments show that FoX achieves state-of-the-art performance on benchmarks such as GenEval (77% overall score) and T2I-CompBench, while maintaining strong results on foundational image understanding and generation tasks.

## Method Summary
FoX is a 1.3B parameter unified multimodal model built on Qwen2-0.5B with three functionality-oriented experts: Linguistic, Semantic Visual, and Generative Visual. The model uses a token-based router to assign inputs to appropriate experts (text→Linguistic, clean image→Semantic Visual, noised image→Generative Visual). It employs a two-stage pre-training: first on 300M image-text pairs for text-to-image generation, then on 120M generation and 20M understanding samples. MCoT is implemented through multi-task joint training on three tasks: Planning & Acting (prompt→caption→layout→image), Reflection (image+prompt→artifact map), and Correction (caption+masked-image→repaired-image). The approach avoids requiring consistent multi-step data tuples by training these tasks independently then combining them at inference.

## Key Results
- FoX achieves 77% overall score on GenEval benchmark, outperforming state-of-the-art T2I models
- 7.24 FID on MS-COCO, surpassing both modality-oriented and dense expert variants
- State-of-the-art performance on T2I-CompBench
- Ablation studies show MCoT steps contribute incrementally: Planning & Acting only (73%), Full MCoT (77%), vs T2I Gen Twice (67%)

## Why This Works (Mechanism)

### Mechanism 1
Functionality-oriented expert separation reduces training conflicts compared to modality-oriented designs. Visual understanding (optimized via comprehension loss) and visual generation (optimized via diffusion/rectified flow loss) impose different gradients on shared parameters. Assigning separate experts—the Semantic Visual Expert for understanding and Generative Visual Expert for generation—allows each to specialize without interference. Core assumption: Optimization conflicts are the primary bottleneck, not parameter count or data scarcity. Evidence: FoX (CIDEr 126.5, FID 7.24) outperforms both dense (116.2, 11.3) and modality-oriented (121.1, 9.56) variants.

### Mechanism 2
MCoT's four-step decomposition (planning, acting, reflection, correction) improves compositional image generation over direct T2I. Complex prompts overwhelm single-pass generation. Planning produces detailed captions and layout boxes that constrain the acting step. Reflection identifies defect regions via artifact heatmaps; correction performs targeted inpainting. This emulates human artistic workflows. Core assumption: Errors are localizable and correctable without full regeneration; decomposition does not introduce cascading errors. Evidence: Full MCoT (0.77 overall GenEval) > Planning & Acting only (0.73) > T2I Gen Twice (0.67).

### Mechanism 3
Multi-task joint training enables MCoT without requiring consistent multi-step data tuples. Rather than requiring {prompt, planning, first-wrong-image, artifact-map, correct-image} tuples, the model is trained on three separate tasks with independently available data: Planning & Acting, Reflection, Correction. This disentangles data requirements. Core assumption: Capabilities learned separately transfer to sequential execution at inference without joint optimization. Evidence: Decouples end-to-end MCoT training into multiple tasks, avoiding difficulty of collecting consistent multi-step data tuples.

## Foundational Learning

- **Mixture of Experts (MoE) with functional routing**
  - Why needed here: FoXperts routes tokens to experts by functionality (understand vs. generate), not modality alone. Understanding MoE routing mechanics is prerequisite to debugging expert specialization.
  - Quick check question: Can you explain why token-to-expert routing based on functionality differs from routing based on input modality?

- **Rectified Flow for continuous image generation**
  - Why needed here: FoX uses rectified flow (Eq. 3-4) rather than discrete tokenization for images. Understanding velocity field prediction and linear interpolation between noise and data is necessary to debug generation quality.
  - Quick check question: How does rectified flow differ from DDPM-style denoising, and why might it suit unified multimodal models?

- **Chain of Thought decomposition patterns**
  - Why needed here: MCoT follows the "human-defined key steps" paradigm (not end-to-end learned CoT). Recognizing this distinction clarifies why explicit step design was necessary and what constraints it imposes.
  - Quick check question: What are the tradeoffs between learned CoT (e.g., DeepSeek-R1) and human-defined decomposition for tasks lacking abundant multi-step training data?

## Architecture Onboarding

- **Component map**: Text tokenizer (Qwen2) → VAE encoder (SD3) → Router (LayerNorm → projection → expert assignment) → Three parallel experts (Linguistic, Semantic Visual, Generative Visual) → Multimodal Attention (cross-expert interaction) → FFN layers → Output head selection (LM head vs. diffusion head)

- **Critical path**: 
  1. Input tokenization (text tokens, clean image tokens, noisy image tokens)
  2. Router assigns each token to appropriate expert
  3. Expert-specific QKV projections
  4. Multimodal attention aggregates across experts (causal for text, bidirectional for vision)
  5. Expert-specific FFN processing
  6. Output head selection based on task (LM head vs. diffusion head)

- **Design tradeoffs**:
  - Functionality separation increases parameter count but reduces gradient interference
  - MCoT inference is 2× slower (two image generations) but improves compositional accuracy
  - Disentangled training simplifies data collection but may miss cross-step optimization opportunities

- **Failure signatures**:
  - **Concept confusion**: Multiple objects merge (e.g., "scissors and bowl" produces hybrid object)—indicates planning or layout failure
  - **Attribute misbinding**: Correct objects with wrong colors—indicates caption planning not grounding attributes to objects
  - **Spatial errors**: Objects in wrong positions—indicates layout boxes not followed in acting step
  - **Incomplete objects**: Missing parts after correction—indicates reflection masking too aggressive or inpainting insufficient

- **First 3 experiments**:
  1. **Expert ablation**: Compare FoX vs. dense vs. modality-oriented variants on MS-COCO (FID, CIDEr) to validate functional separation benefits. Expected: FoX > modality-oriented > dense.
  2. **MCoT step ablation**: Run T2I Gen Twice, Planning & Acting only, and Full MCoT on GenEval. Isolate contribution of reflection+correction. Expected: incremental gains from each step.
  3. **Reflection accuracy test**: Manually annotate artifact maps for 50 generated images; measure IoU between model-predicted heatmaps and ground truth. Threshold: >0.5 IoU to be useful for correction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Multimodal Chain of Thought (MCoT) framework be effectively extended to support fine-grained and customized image editing tasks?
- Basis in paper: The authors explicitly state in Appendix E (Limitations) that they have "not addressed more challenging fine-grained and customized image editing tasks" despite these tasks being ideal for demonstrating the unified reasoning and generation capabilities of the model.
- Why unresolved: The current implementation focuses on image generation and enhancement (correction of defects) rather than user-directed editing of existing images based on complex instructions.
- What evidence would resolve it: A demonstration of FoX performing tasks like object replacement or style transfer on user-provided images using the MCoT pipeline.

### Open Question 2
- Question: What is the inference latency overhead of the full MCoT process compared to direct text-to-image generation, and does it scale linearly with reflection steps?
- Basis in paper: The paper introduces a four-step pipeline (Planning, Acting, Reflection, Correction) to improve quality but does not report inference times or computational costs associated with the additional forward passes required for reflection and correction.
- Why unresolved: While the qualitative results show improved image alignment, the trade-off between generation quality and inference speed remains unstated.
- What evidence would resolve it: Comparative latency benchmarks measuring the time taken for direct T2I generation versus the full MCoT loop on standard hardware.

### Open Question 3
- Question: Would training the model end-to-end with consistent multi-step data tuples yield superior performance over the proposed disentangled multi-task training?
- Basis in paper: The introduction notes that they developed the multi-task training paradigm to "avoid the difficulty and impracticality of collecting consistent multi-step data tuples," acknowledging that forcing the model to generate erroneous images for training is challenging.
- Why unresolved: It is unclear if the disentangled training approach is an optimal architectural solution or merely a practical necessity driven by data availability constraints.
- What evidence would resolve it: An ablation study comparing the current model against a variant trained on a synthetic dataset containing aligned "wrong" images and corresponding corrections.

## Limitations
- Expert architecture benefits are not conclusively isolated from parameter scaling effects
- MCoT sequential reliability has not been validated against end-to-end optimization
- Reflection step's artifact detection accuracy lacks quantitative validation

## Confidence

- **High**: The functional expert separation architecture and its implementation details are clearly specified and reproducible. The two-stage pre-training and multi-task joint training framework are well-documented.
- **Medium**: The MCoT decomposition (planning, acting, reflection, correction) and its step-by-step contribution to GenEval scores are demonstrated, but the sequential reliability of the disentangled training approach remains unverified.
- **Low**: The Reflection step's artifact detection accuracy is not quantitatively validated, making it unclear whether the correction improvements are due to accurate defect localization or other factors.

## Next Checks

1. **Expert ablation with matched parameters**: Train 1.3B dense and modality-oriented variants and compare against FoX on MS-COCO FID and CIDEr to isolate the effect of functional separation from parameter scaling.

2. **Reflection accuracy quantification**: Annotate artifact heatmaps for 100 generated images from the GenEval test set; compute IoU between predicted and ground truth masks to establish a quantitative baseline for Reflection quality.

3. **MCoT sequential vs. joint training**: Train a version where Planning, Reflection, and Correction are optimized jointly on consistent multi-step data tuples; compare against the disentangled approach on GenEval to assess if sequential reliability requires joint optimization.