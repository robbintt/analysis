---
ver: rpa2
title: Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance
arxiv_id: '2505.12334'
source_url: https://arxiv.org/abs/2505.12334
tags:
- user
- chatbot
- dialogue
- background
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for enhancing user-oriented proactivity
  in open-domain dialogue systems by constructing a critic to evaluate proactivity
  and using it to guide dialogue corpus generation with user agents from diverse backgrounds.
  An iterative curriculum learning approach is employed to train the chatbot from
  easy-to-communicate to more challenging users.
---

# Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance

## Quick Facts
- arXiv ID: 2505.12334
- Source URL: https://arxiv.org/abs/2505.12334
- Reference count: 10
- Method improves user-oriented proactivity in open-domain dialogues by combining critic-guided corpus generation with iterative curriculum learning

## Executive Summary
This paper introduces a method for enhancing user-oriented proactivity in open-domain dialogue systems by constructing a critic to evaluate proactivity and using it to guide dialogue corpus generation with user agents from diverse backgrounds. An iterative curriculum learning approach is employed to train the chatbot from easy-to-communicate to more challenging users. Experiments show that the proposed method improves user-oriented proactivity and attractiveness in open-domain dialogues, with UPC outperforming other methods in terms of relevance, user interest, and response value.

## Method Summary
The method constructs a critic using LLM-as-a-judge to evaluate dialogue responses on relevance, interest, and value. During corpus generation, a chatbot converses with user agents representing diverse backgrounds; responses scoring below threshold trigger regeneration with specific feedback. An iterative curriculum learning approach trains the chatbot, using a difficulty measurer to filter samples by communication difficulty and progressively expose the model to more challenging users across iterations.

## Key Results
- UPC achieves higher scores on all three evaluation metrics (Rel., Int., Val.) compared to baseline methods
- Iterative fine-tuning with curriculum learning improves performance over single-iteration approaches
- Critic-guided regeneration produces higher quality training data, with regeneration rates decreasing across iterations as model improves

## Why This Works (Mechanism)

### Mechanism 1
Critic-guided regeneration improves training corpus quality by filtering and refining responses before they become training data. An LLM-based critic scores each chatbot response on relevance, interest, and value (1-5 scale). Responses below threshold trigger regeneration with specific failure feedback. This trial-and-error process produces higher-quality training examples than passive collection. Core assumption: The critic's evaluation correlates with human preferences for proactive dialogue (validated in real user study).

### Mechanism 2
Iterative fine-tuning creates a positive feedback loop where improved models generate better training data for subsequent iterations. Model at iteration k generates dialogue corpus → fine-tune → use improved model for iteration k+1 corpus generation. Each iteration's improvements compound. Core assumption: Fine-tuning on higher-quality generated data transfers to better performance on new user interactions. Evidence: CDC+IFT improves over CDC alone by 0.97%, 1.80%, 3.27% on Rel., Int., Val.

### Mechanism 3
Curriculum learning by communication difficulty accelerates convergence and improves final performance. Difficulty Measurer flags "hard" samples (any score <α OR fewer than β metrics improve after regeneration). Easy samples train early; hard samples deferred until model strengthens. Progressive exposure prevents early training on intractable cases. Core assumption: Communication difficulty is measurable via score thresholds and correlates with learning difficulty.

## Foundational Learning

- **LLM-as-a-judge evaluation**: Why needed: Understanding how an LLM can approximate human judgment for scoring dialogue quality enables the entire critic-guided pipeline. Quick check: Can you explain why a prompted LLM might serve as a reasonable proxy for human preference evaluation, and what biases might it introduce?

- **Curriculum learning**: Why needed: The core training strategy orders samples by difficulty; without this concept, you can't reason about why easy-to-hard ordering helps. Quick check: What is the intuition behind training on easy samples before hard ones, and what determines "difficulty" in this context?

- **User simulation / role-playing agents**: Why needed: The method trains against synthetic user agents; understanding LLM role-play capabilities is essential for evaluating this design choice. Quick check: What are the tradeoffs between training with simulated users vs. real human interactions?

## Architecture Onboarding

- Component map: ISCO-800 Dataset → User Agent Construction (Qwen1.5-72B-Chat) → Chatbot (Qwen1.5-32B-Chat) ↔ Dialogue Generation → Critic (GPT-3.5-turbo) → Score & Feedback → Regeneration Loop → Difficulty Measurer → Easy/Hard Classification → Training Scheduler → Curriculum-ordered Fine-tuning → Iteration Loop (k=1 to K)

- Critical path: Critic prompt design (Figure 3) directly determines training signal quality; Regeneration threshold (α=4) controls corpus quality vs. generation cost tradeoff; Difficulty hyperparameters (α, β in Eqn. 3) define curriculum pacing

- Design tradeoffs: Critic choice (GPT-3.5 cheaper but may diverge from human judgment; GPT-4 more accurate but expensive at scale); Regeneration budget (more attempts → higher quality but diminishing returns); User diversity vs. training stability (ISCO-800 ensures breadth but some occupations may be underrepresented)

- Failure signatures: Regeneration rate doesn't decrease over iterations (model not learning critic preferences); Easy sample rate plateaus early (curriculum too aggressive); Large gap between GPT-3.5 and GPT-4 critic scores (critic misalignment with evaluation)

- First 3 experiments: 1) Critic validation: Run critic on held-out dialogues with human annotations to measure correlation; 2) Ablation on regeneration threshold: Test α ∈ {3, 3.5, 4, 4.5} to find quality-cost sweet spot; 3) Single-iteration baseline: Compare curriculum learning vs. uniform sampling for same total samples

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The method relies heavily on LLM-as-a-judge reliability, which remains a fundamental weakness—GPT-3.5 scores correlate with human preferences in their evaluation but may not generalize to other domains or cultural contexts
- The difficulty measurer's effectiveness is assumed rather than proven, with no external validation of whether score thresholds truly capture learning difficulty
- Regeneration loops create substantial computational overhead, but cost-benefit tradeoffs aren't fully characterized—Table 3 shows decreasing regenerations but doesn't establish optimal thresholds

## Confidence
- High confidence: Iterative curriculum learning structure and its empirical improvements
- Medium confidence: Critic-guided regeneration mechanism (validation limited to single critic-human correlation study)
- Low confidence: Difficulty measurer design and threshold choices (no ablation or external validation)

## Next Checks
1. Cross-critic validation: Test the trained model against GPT-4 and human evaluators on held-out dialogues to verify the critic-guided training actually transfers to independent quality measures
2. Regeneration efficiency analysis: Systematically vary α threshold and maximum regeneration attempts to establish the quality-cost Pareto frontier for different deployment scenarios
3. Curriculum robustness testing: Compare curriculum learning against uniform sampling while controlling for total samples to definitively isolate the curriculum effect from iteration effects