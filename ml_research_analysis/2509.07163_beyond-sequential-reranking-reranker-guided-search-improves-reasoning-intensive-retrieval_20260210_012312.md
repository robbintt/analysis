---
ver: rpa2
title: 'Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive
  Retrieval'
arxiv_id: '2509.07163'
source_url: https://arxiv.org/abs/2509.07163
tags:
- reranker
- retrieval
- documents
- embedding
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reranker-Guided-Search (RGS), a method to
  improve retrieval accuracy under a fixed reranker budget by strategically selecting
  documents based on document-document similarity rather than sequential reranking.
  The approach uses a greedy search on proximity graphs built from document embeddings,
  prioritizing reranking of documents whose similar neighbors have been highly ranked.
---

# Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval

## Quick Facts
- **arXiv ID:** 2509.07163
- **Source URL:** https://arxiv.org/abs/2509.07163
- **Reference count:** 34
- **Primary result:** RGS improves BRIGHT by 3.5 points, FollowIR by 2.9, and M-BEIR by 5.1 NDCG@10 when reranking at most 100 documents

## Executive Summary
This paper introduces Reranker-Guided-Search (RGS), a method to improve retrieval accuracy under a fixed reranker budget by strategically selecting documents based on document-document similarity rather than sequential reranking. The approach uses a greedy search on proximity graphs built from document embeddings, prioritizing reranking of documents whose similar neighbors have been highly ranked. Experiments show significant improvements across three benchmarks when reranking at most 100 documents. Analysis reveals that under high reranker budgets, final accuracy depends more on reranker-groundtruth alignment than embedding quality, and RGS is robust to query embedding perturbations, making it effective for reasoning-intensive retrieval tasks.

## Method Summary
RGS improves retrieval by using a greedy search algorithm on proximity graphs generated from document embeddings. Starting with an initial set of documents retrieved by vector similarity, the method maintains an ordered candidate list and iteratively expands it by adding neighbors of the highest-ranked unexpanded document. The reranker evaluates documents in sliding windows, updating the candidate list order. This process continues until the reranker budget is exhausted. Unlike sequential reranking or embedding-guided selection, RGS strategically explores document neighborhoods based on the reranker's preferences, potentially discovering relevant documents that initial retrieval misses.

## Key Results
- RGS improves BRIGHT by 3.5 points, FollowIR by 2.9 points, and M-BEIR by 5.1 points NDCG@10 when reranking at most 100 documents
- Under Reranker@500 budget, all models converge to similar accuracy, indicating reranker-groundtruth alignment becomes the bottleneck
- RGS remains robust when query embeddings are severely perturbed (w=1.0), showing only slight accuracy decline versus severe degradation in sequential reranking

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Guided Document Discovery via Clustering Hypothesis
- **Claim:** RGS discovers relevant documents beyond the initial retrieval top-k by traversing document neighborhoods rather than relying solely on query-document similarity.
- **Mechanism:** The algorithm builds a proximity graph using DiskANN on document embeddings, then performs greedy expansion from seed documents. When the reranker assigns high scores to a document, its graph neighbors are prioritized for subsequent reranking, creating a traversal pattern that follows document-document similarity rather than query-document similarity alone.
- **Core assumption:** The clustering hypothesis holds: documents similar to relevant documents are themselves likely relevant to the query.
- **Evidence anchors:**
  - [abstract]: "Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity."
  - [section 3.2]: "Inspired by the clustering hypothesis [7], we believe that if a document is judged to be relevant by a reranker, then its similar documents are likely to be relevant as well."
  - [corpus]: Limited direct evidence; clustering hypothesis is classical IR theory, not validated in neighbor papers.
- **Break condition:** Fails when document similarity structure in embedding space does not correlate with topical similarity (e.g., adversarial corpora, cross-lingual mismatch).

### Mechanism 2: Reranker Preference Alignment Overcomes Embedding Limitations
- **Claim:** Under sufficient reranker budget, final accuracy becomes constrained by reranker-groundtruth alignment rather than initial embedding quality.
- **Mechanism:** RGS decouples final accuracy from initial retrieval quality by using the reranker's judgments to guide search. Even with perturbed or low-quality query embeddings, the method can reach relevant documents through graph traversal, as long as document-document similarity structure is preserved.
- **Core assumption:** The reranker has higher capacity to assess query-document relevance than the embedding model, and its preferences align with ground truth labels.
- **Evidence anchors:**
  - [section 5.1]: "all models converge to similar accuracy under the Reranker@500 setting... the embedding model primarily determines how many reranker calls are needed to reach a given retrieval accuracy, while the final performance is ultimately bounded by the reranker's capability."
  - [section 5.2]: "For our RGS method, retrieval accuracy declines only slightly even when the query embedding contains no useful information (i.e., when w = 1)."
  - [corpus]: Neighbor papers (e.g., LimRank, E2Rank) suggest reranker fine-tuning improves alignment, supporting the claim that reranker quality is a bottleneck.
- **Break condition:** Fails when reranker preferences systematically diverge from ground truth (e.g., domain shift, adversarial queries).

### Mechanism 3: Strategic Budget Allocation via Graph Frontier Management
- **Claim:** Maintaining an ordered list of candidates and always expanding from the highest-ranked unexplored document allocates reranker budget more efficiently than sequential or embedding-guided selection.
- **Mechanism:** Unlike prior methods (GAR, SlideGAR) that may delay exploring promising documents due to embedding distance or list position, RGS immediately expands neighborhoods of documents the reranker currently prefers. This creates a directed search that skips low-ranked neighborhoods.
- **Core assumption:** High reranker scores indicate proximity to relevant regions in the document graph.
- **Evidence anchors:**
  - [section 2]: "our RGS maintains an up-to-date document order and always explores the neighbors of the most promising document judged by the reranker. In contrast, GAR and SlideGAR rely on either embedding similarity or sequential order."
  - [figure 1]: Example showing RGS finds D3 (ranked 2812th initially) by exploring D2's neighborhood while skipping C1's low-ranked neighbors.
  - [corpus]: Weak evidence; neighbor papers do not directly compare graph frontier strategies.
- **Break condition:** Fails when reranker scores are noisy or inconsistent across iterations, causing search to oscillate.

## Foundational Learning

- **Concept:** Clustering hypothesis in information retrieval
  - **Why needed here:** The entire RGS algorithm is built on the assumption that document similarity correlates with relevance similarity. Understanding this helps diagnose when graph traversal will fail.
  - **Quick check question:** If document A is relevant to query Q, should document B (semantically similar to A) also be relevant to Q? When might this break?

- **Concept:** Approximate nearest neighbor (ANN) search and proximity graphs
  - **Why needed here:** RGS relies on DiskANN-built graphs for efficient traversal. Understanding graph structure (e.g., out-degree R, pruning) is critical for debugging retrieval paths.
  - **Quick check question:** What is the difference between a KNN graph and a DiskANN proximity graph? Why does RGS perform poorly on KNN graphs?

- **Concept:** Listwise vs. pointwise reranking
  - **Why needed here:** RGS uses listwise rerankers with sliding windows. Understanding how ranking scores are produced helps interpret why certain documents are prioritized.
  - **Quick check question:** If a pointwise reranker scores document A higher than B individually, could a listwise reranker reverse this order? Why?

## Architecture Onboarding

- **Component map:** Document embedding encoder -> DiskANN indexer -> Seed retriever -> Ordered candidate list A -> Listwise reranker -> Expansion module

- **Critical path:**
  1. Build/load DiskANN index on corpus (offline, one-time)
  2. For each query: retrieve seeds -> initialize list A -> iterate expand-rerank until budget exhausted -> return top-10 from A
  3. Budget is measured in total documents seen by reranker (documents can be reranked multiple times)

- **Design tradeoffs:**
  - **Search list size L_s:** Larger values retain more candidates but increase memory; tuned as L_s = 20/30/50 for budget k = 100/300/500
  - **Sliding window size w:** Smaller windows (w=10) provide finer-grained reranking but increase API calls; larger windows (w=20 in SlideGAR) reduce calls but may miss local orderings
  - **Number of seed points:** k/5 seeds balance exploration breadth vs. reranker cost; too few seeds may miss relevant regions
  - **Graph type:** DiskANN proximity graphs outperform KNN and random graphs; KNN lacks long-range edges needed for traversal

- **Failure signatures:**
  - **Accuracy plateaus despite increasing budget:** Reranker-groundtruth misalignment (see Figure 6: "seen but not selected" category grows)
  - **No improvement over sequential reranking:** Document embedding space lacks meaningful similarity structure (test with perturbation analysis)
  - **Large variance across seeds:** Graph structure is poorly connected or reranker is inconsistent
  - **High latency:** Reranker API calls dominate; consider batching or reducing window size

- **First 3 experiments:**
  1. **Reproduce BRIGHT results with fixed budget k=100:** Compare RGS vs. RR vs. SlideGAR using BGE-Large embeddings and Gemini-2.0-Flash reranker. Verify ~3.5 point NDCG@10 improvement.
  2. **Ablate graph structure:** Run RGS on DiskANN, KNN, and random graphs (Figure 5). Confirm DiskANN is necessary and sufficient.
  3. **Perturb query embeddings:** Following Section 5.2, corrupt query embeddings at w=0.5 and w=1.0. Verify RGS maintains >80% of original accuracy while RR degrades sharply.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section.

## Limitations
- The method relies heavily on the clustering hypothesis, which may not hold in all domains or corpora where document similarity structure differs from relevance structure
- Performance under low reranker budgets remains less impressive compared to high-budget scenarios, suggesting limited universal superiority
- The sequential nature of the greedy expansion creates a dependency chain that may hinder parallelization and increase latency

## Confidence
- **High Confidence:** The empirical results showing RGS outperforming sequential reranking on multiple benchmarks (BRIGHT, FollowIR, M-BEIR) with statistically significant improvements.
- **Medium Confidence:** The theoretical mechanism that reranker-groundtruth alignment becomes the bottleneck under high budgets, based on the analysis in Section 5.1 and Figure 6.
- **Medium Confidence:** The robustness claim regarding query embedding perturbations, though the experimental evidence is somewhat limited in scope.

## Next Checks
1. **Cross-domain validation:** Test RGS on non-English corpora or specialized domains (e.g., biomedical, legal) where document similarity structures may differ substantially from the tested datasets.
2. **Ablation on reranker types:** Evaluate RGS with pointwise rerankers versus listwise rerankers to determine if the greedy search mechanism is agnostic to reranker architecture.
3. **Real-time efficiency measurement:** Measure end-to-end latency including DiskANN indexing and greedy traversal to quantify practical deployment constraints beyond just accuracy improvements.