---
ver: rpa2
title: Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive
  Samples
arxiv_id: '2511.22141'
source_url: https://arxiv.org/abs/2511.22141
tags:
- text
- modality
- query
- retrieval
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a similarity standardization approach to mitigate
  the modality gap in multi-modal retrieval without manually labeled data or image
  captioning. The method computes modality-specific mean and variance from pseudo-positive
  pairs, which are constructed by retrieving the most similar text and image candidates
  for each query.
---

# Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples

## Quick Facts
- arXiv ID: 2511.22141
- Source URL: https://arxiv.org/abs/2511.22141
- Authors: Shuhei Yamashita; Daiki Shirafuji; Tatsuhiko Saito
- Reference count: 40
- One-line result: Similarity standardization using pseudo-positive pairs significantly improves multi-modal retrieval performance, achieving 64% average Recall@20 gains on MMQA and 28% on WebQA for cross-modality queries.

## Executive Summary
This paper addresses the modality gap problem in multi-modal retrieval, where text queries retrieve text candidates with higher cosine similarity than relevant images, even when images are semantically correct. The proposed method, similarity standardization with pseudo-positive samples, computes modality-specific mean and variance from pseudo-positive pairs (constructed via top-1 retrieval) and uses these statistics to standardize cosine similarity scores. This makes cross-modality scores comparable without requiring manually labeled data or image captioning. Experiments on two multi-modal QA benchmarks with seven vision-language models show significant performance improvements, particularly for image retrieval tasks.

## Method Summary
The method addresses the modality gap by standardizing similarity scores using modality-specific statistics. For each query, it retrieves the most similar text and image candidates to construct pseudo-positive pairs. From these pairs, it computes mean (μ) and variance (σ²) separately for text and image modalities. At retrieval time, cosine similarities are standardized using z-scores: sim(q,d) = (cos(f(q), f(d)) - μ_m) / σ_m for modality m. This transformation makes scores from different modalities directly comparable. The approach requires only one-time pre-computation of statistics from the training set, avoiding the need for manual labels or image captioning.

## Key Results
- Similarity standardization with pseudo-positive pairs achieves 64% average Recall@20 gains on MMQA when queries and targets belong to different modalities
- The method outperforms E5-V, which uses image captioning, while preserving visual information as evidenced by superior image retrieval performance
- Image retrieval performance improves significantly (Recall@20 from ~0 to substantial values) across multiple CLIP-based models on both MMQA and WebQA benchmarks
- The approach successfully bridges the modality gap without degrading overall retrieval quality, though text retrieval performance shows slight degradation in some cases

## Why This Works (Mechanism)

### Mechanism 1
Standardizing similarity scores using modality-specific statistics makes cross-modality scores comparable. Text and image embeddings occupy different regions in the shared space, causing cosine similarities to have different scales by modality. By computing z-scores separately for each modality, scores are placed on a common scale. This works when the modality gap manifests primarily as scale/shift differences that can be corrected via standardization.

### Mechanism 2
Pseudo-positive pairs (highest-similarity retrieved items) can replace manually labeled data for estimating modality-specific statistics. Rather than requiring ground-truth query-positive pairs, the method constructs pseudo pairs via argmax over cosine similarity. These pseudo pairs provide sufficient signal to estimate μ and σ even though they may contain noise. This works when top-1 retrieved items produce similarity distributions whose mean and variance approximate those of true positives.

### Mechanism 3
Positively skewed image similarity distributions cause relevant images (as outliers) to receive higher standardized scores post-normalization. CLIP-based models produce long-tailed similarity distributions for images (high positive skewness). Standardization amplifies outliers—often including the correct image—allowing them to outrank text candidates despite the residual gap. This works when relevant images tend to be high-similarity outliers rather than central in the distribution.

## Foundational Learning

- **Concept: Modality Gap in Contrastive VLMs**
  - Why needed here: The entire method is designed to mitigate this gap; understanding its origin (contrastive training, embedding geometry) is prerequisite.
  - Quick check question: Can you explain why text queries retrieve text candidates with higher cosine similarity than relevant images, even when images are semantically correct?

- **Concept: Z-score Standardization**
  - Why needed here: The core operation transforms raw cosine similarities into standardized scores using modality-specific mean and variance.
  - Quick check question: Given μ_text=0.7, σ_text=0.1 and μ_image=0.3, σ_image=0.05, compute standardized scores for cos_text=0.8 and cos_image=0.4. Which ranks higher after standardization?

- **Concept: Pseudo-labeling / Self-training**
  - Why needed here: The method constructs pseudo-positive pairs without manual labels; understanding the trade-offs of noisy supervision is relevant.
  - Quick check question: What conditions make pseudo-labels reliable enough for statistic estimation even if they're not accurate for supervised learning?

## Architecture Onboarding

- **Component map:** Embedding generation -> Pseudo-pair constructor -> Statistic estimator -> Standardization module -> Ranking

- **Critical path:**
  1. Pre-compute embeddings for entire database (one-time cost)
  2. Build pseudo-pairs from training queries (requires forward passes)
  3. Compute and store μ_text, σ_text, μ_image, σ_image
  4. At inference, apply standardization to all query-candidate similarities before ranking

- **Design tradeoffs:**
  - Pseudo vs. labeled pairs: Pseudo-pairs require no annotation but may introduce noise if retrieval quality is poor
  - Static vs. dynamic statistics: Paper uses fixed statistics from training split; dynamic updating could handle distribution shift (noted as limitation)
  - Modality-specific vs. unified: Separate statistics preserve modality-specific calibration; unified would lose this signal

- **Failure signatures:**
  - TextQ performance drops significantly (observed in Table 3): over-correction favoring images
  - ImageQ Recall@k remains near zero (baseline condition): standardization not applied or statistics corrupted
  - Large divergence between Std and Ours results: pseudo-pair quality issue, possibly due to database bias

- **First 3 experiments:**
  1. Reproduce baseline gap: Run retrieval on MMQA/WebQA with raw cosine similarity; verify ImageQ Recall@20 ≈ 0 for CLIP models
  2. Validate standardization effect: Compare TextQ vs. ImageQ performance using labeled statistics (Std) vs. pseudo-pairs (Ours); confirm pseudo-pairs match or exceed labeled performance
  3. Skewness analysis: Plot similarity distributions for text and image candidates; compute skewness values to verify Mechanism 3 holds for your target VLM

## Open Questions the Paper Calls Out

- **Open Question 1:** How can modality-specific statistics be dynamically updated to maintain retrieval performance in databases where content and distributions shift over time? The paper notes that pre-computed statistics may become obsolete in real-world systems and suggests future work on developing mechanisms to dynamically update these statistics.

- **Open Question 2:** Can the standardization approach be refined to mitigate the performance degradation observed in text retrieval (TextQ) while preserving the significant gains in image retrieval (ImageQ)? While the paper reports substantial gains for ImageQ, it shows slight performance drops for TextQ, suggesting the standardization technique is not yet fully balanced across modalities.

- **Open Question 3:** To what extent does the accuracy of the initial retrieval step influence the reliability of the modality-specific statistics derived from pseudo-positive samples? The method assumes top-retrieved candidates are valid positives, but it's unclear if this relies on skewness of the distribution or requires a high density of actually relevant candidates in the top-1 position.

## Limitations
- The method requires static modality-specific statistics, making it vulnerable to distribution shift in dynamic environments
- No analysis of computational overhead for large-scale deployment, particularly the one-time cost of constructing pseudo-pairs across massive databases
- The assumption that pseudo-positive pairs provide reliable statistics is not fully validated through error rate analysis
- Limited generalizability testing beyond CLIP-based models to other VLM architectures

## Confidence
- **High confidence:** The core mechanism of similarity standardization works as described, with clear mathematical formulation and consistent experimental results
- **Medium confidence:** Pseudo-positive pairs are sufficient for statistic estimation, though the robustness to noisy labels is not exhaustively explored
- **Medium confidence:** The skewness amplification mechanism is plausible and supported by empirical data, but the theoretical foundation could be strengthened

## Next Checks
1. **Pseudo-pair quality analysis:** Measure precision@k of pseudo-positive retrieval (e.g., how often top-1 pseudo-positive is actually relevant) and correlate with downstream performance degradation
2. **Cross-model validation:** Apply the method to non-CLIP VLMs (LLaVA, SigLIP) and verify that skewness patterns and standardization effectiveness transfer across architectures
3. **Dynamic adaptation test:** Implement periodic re-computation of modality statistics on held-out validation sets and measure performance stability under distribution shift scenarios