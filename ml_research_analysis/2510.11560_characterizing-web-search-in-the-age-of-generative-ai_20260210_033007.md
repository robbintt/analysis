---
ver: rpa2
title: Characterizing Web Search in The Age of Generative AI
arxiv_id: '2510.11560'
source_url: https://arxiv.org/abs/2510.11560
tags:
- search
- organic
- generative
- engines
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares traditional Google search with four generative
  AI search engines (Google AI Overview, Gemini, GPT-4o Search, GPT-4o with Search
  Tool) across queries from four domains. The study finds that generative engines
  generally cover a wider range of sources than traditional search, with AI Overviews
  accessing domains not found in top-10 or top-100 organic results.
---

# Characterizing Web Search in The Age of Generative AI

## Quick Facts
- arXiv ID: 2510.11560
- Source URL: https://arxiv.org/abs/2510.11560
- Reference count: 40
- Primary result: Generative search engines access more diverse sources than traditional search, with significant variation in how much they rely on external versus internal knowledge

## Executive Summary
This study systematically compares traditional Google search with four generative AI search engines (Google AI Overview, Gemini, GPT-4o Search, GPT-4o with Search Tool) across 400 queries from four domains. The research reveals that generative engines generally cover a wider range of sources than traditional search, with AI Overviews accessing domains not found in top-10 or top-100 organic results. While overall topical coverage is similar between generative and traditional search, each engine surfaces distinct sets of concepts, and traditional search often outperforms on ambiguous queries. The findings highlight the need for new evaluation methods that account for source diversity, conceptual coverage, and synthesis behavior in generative search systems.

## Method Summary
The study employs a systematic methodology to compare generative and traditional search engines. Researchers compiled 400 queries across four domains (technology, education, health, and personal assistance) and collected responses from five search engines. For each query, they analyzed both source citations and conceptual coverage by extracting entities from search results and comparing overlap using Jaccard similarity. The analysis focused on snippets rather than full-page content to reflect user behavior, and employed temperature 0 for deterministic outputs from generative models. The comparison framework measured both the diversity of sources accessed and the topical coverage of concepts presented in responses.

## Key Results
- Generative engines cover more diverse sources than traditional search, with AI Overviews accessing domains not found in top-10 or top-100 organic results
- Each search engine (generative and traditional) surfaces a distinct set of concepts, with Jaccard similarity of concept coverage remaining below 0.4 across engines
- Traditional Google search outperforms generative engines on ambiguous queries, suggesting different strengths for different query types
- GPT-Tool (GPT-4o with Search Tool) cites significantly fewer sources on average compared to other generative engines, indicating higher reliance on internal knowledge

## Why This Works (Mechanism)
The comparative analysis reveals that generative search engines fundamentally change how information is synthesized and presented to users. By accessing diverse sources and presenting synthesized responses, these systems create a different information retrieval experience compared to traditional ranked lists. The variation in source citation behavior across generative models demonstrates that different architectural approaches (standalone models versus tool-enhanced systems) lead to distinct information gathering and presentation patterns.

## Foundational Learning

### Information Retrieval vs. Generative Search
**Why needed**: Understanding the fundamental shift from ranked lists to synthesized responses
**Quick check**: Compare Jaccard similarity of source coverage between traditional and generative search for identical queries

### Source Diversity Metrics
**Why needed**: Evaluating the breadth of information accessed beyond traditional ranking positions
**Quick check**: Measure unique domains accessed by AI Overviews versus top-10 organic results

### Conceptual Coverage Analysis
**Why needed**: Assessing whether different systems present overlapping or complementary information
**Quick check**: Calculate entity overlap between search engines using Jaccard similarity

### Query Ambiguity Classification
**Why needed**: Understanding when different search paradigms excel
**Quick check**: Compare performance on ambiguous versus specific queries across search types

## Architecture Onboarding

### Component Map
User Query -> Search Engine Processing -> Source Retrieval/Generation -> Response Synthesis -> Output with Citations

### Critical Path
Query parsing → Source selection/retrieval → Content synthesis → Response generation → Citation attachment

### Design Tradeoffs
Standalone models vs. tool-enhanced systems: GPT-Tool shows higher internal knowledge reliance with fewer citations versus other generative engines that access more external sources

### Failure Signatures
- Limited source diversity when systems rely heavily on internal knowledge
- Concept coverage gaps when engines access non-overlapping source sets
- Performance degradation on ambiguous queries for generative systems

### First 3 Experiments
1. Test source diversity by comparing unique domains accessed by each engine for identical queries
2. Measure concept coverage overlap between engines using entity extraction and Jaccard similarity
3. Evaluate performance differences on ambiguous versus specific queries across all search types

## Open Questions the Paper Calls Out

### Open Question 1
How does the topical coverage of generative search change when analyzing full webpage content rather than just titles and snippets?
- Basis: The authors state future work should study deeper rankings and compare snippet analysis to full-page content evaluation
- Why unresolved: The study was restricted to snippets to reflect user behavior, but this ignores information contained in underlying webpages
- Evidence needed: A comparative study scraping full URLs to measure concept overlap against the snippet-only baselines established in this paper

### Open Question 2
How do generative search engines perform in multi-turn conversational contexts compared to the single-turn queries analyzed in this study?
- Basis: The authors note they did not consider multi-turn conversational searches and future work can expand on these datasets
- Why unresolved: Current evaluation relies on independent, static queries, ignoring the iterative nature of chat-based search
- Evidence needed: Extending the dataset to include follow-up queries and measuring context retention and source consistency over a session

### Open Question 3
To what extent does non-zero temperature sampling affect the stability of source citations and conceptual coverage in generative search outputs?
- Basis: The authors mention future work should measure output variability through repeated sampling
- Why unresolved: The study used temperature 0 to ensure determinism, leaving the natural stochasticity of LLMs unexplored
- Evidence needed: Re-running queries with varied temperature settings to calculate variance in Jaccard similarity for sources and concept overlap

## Limitations

- Small query set (400 queries total across four domains) may not capture full diversity of real-world search behavior
- Snapshot-based approach reflects a specific moment in time when generative search capabilities were rapidly evolving
- Focus on English-language queries and Western domains potentially misses important cultural and linguistic variations

## Confidence

- High confidence in findings about source diversity differences between generative and traditional search engines
- Medium confidence in claims about topical coverage similarity, depending on specific query set and methodology
- Medium confidence in comparative performance on ambiguous queries, given limited sample size
- Low confidence in broader claims about generative search changing how we search beyond observed behaviors

## Next Checks

1. Replicate the study with a larger, more diverse query set (at least 2000 queries across 10+ domains) to test whether observed patterns hold across different search contexts and user intents
2. Conduct user studies measuring actual search satisfaction and task completion rates with generative versus traditional search to validate the relevance of identified source diversity differences
3. Perform a longitudinal analysis tracking how the same queries are handled across multiple time points (e.g., quarterly) to understand the evolution of generative search capabilities and whether current patterns persist