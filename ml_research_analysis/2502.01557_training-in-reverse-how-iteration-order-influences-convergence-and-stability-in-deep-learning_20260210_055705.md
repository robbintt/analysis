---
ver: rpa2
title: 'Training in reverse: How iteration order influences convergence and stability
  in deep learning'
arxiv_id: '2502.01557'
source_url: https://arxiv.org/abs/2502.01557
tags:
- backward
- forward
- learning
- order
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how the order of gradient updates affects stability
  and convergence in deep learning optimization. Standard SGD processes updates in
  forward order, while the proposed backward-SGD reverses this order, reusing previous
  batches at each step.
---

# Training in reverse: How iteration order influences convergence and stability in deep learning

## Quick Facts
- arXiv ID: 2502.01557
- Source URL: https://arxiv.org/abs/2502.01557
- Reference count: 40
- Primary result: Backward-SGD achieves point convergence while forward-SGD converges to distributions under constant learning rates

## Executive Summary
This work explores how the order of gradient updates affects stability and convergence in deep learning optimization. Standard SGD processes updates in forward order, while the proposed backward-SGD reverses this order, reusing previous batches at each step. The key theoretical result shows that in contractive regions (e.g., near minima), backward-SGD converges to a point while forward-SGD generally converges to a distribution. This improved convergence is attributed to the diminishing stochastic noise in backward-SGD as training progresses, even with fixed learning rates. Experiments on multiple datasets (synthetic, FashionMNIST, CIFAR-10/100) and architectures (MLP, VGG, ResNet) demonstrate that backward-SGD exhibits decreased variance, increased stability, and faster convergence compared to standard forward-SGD. The effect is observed across different optimizers including AdamW.

## Method Summary
The paper compares standard forward-SGD with a novel backward-SGD method that reverses the order of gradient update composition. While standard SGD applies T₁∘T₂∘...∘Tₙ to reach θₙ, backward-SGD applies T₁∘T₂∘...∘Tₙ in reverse order starting from the initial point. This requires storing all batch gradients and recomputing the trajectory at each step, resulting in O(n²) computational complexity. The authors also propose approximation methods using Lie brackets of vector fields and intermittent backward strategies to reduce computational cost. Experiments use ResNet-18 on CIFAR-10/100, MLPs on FashionMNIST, and synthetic regression tasks with batch sizes of 8 and learning rates between 0.00025-0.025.

## Key Results
- Backward-SGD converges to a point while forward-SGD converges to a distribution under constant learning rates in contractive regions
- Backward-SGD shows decreased variance, increased stability, and faster convergence across multiple architectures and datasets
- The effect extends beyond vanilla SGD to adaptive optimizers like AdamW
- Full backward-SGD has O(n²) computational cost, making approximations necessary for practical implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backward-SGD achieves point convergence while forward-SGD only converges to a distribution under constant learning rates
- Mechanism: The reverse composition order causes stochastic noise from recent batches to be multiplied by exponentially decaying factors (e.g., h(1-h)^(n-1)ε_n), causing noise to vanish as n→∞. Forward-SGD adds fresh noise hε_n at each step with constant magnitude
- Core assumption: The update operators T_i become contractions near minima (satisfied when loss Hessian has bounded eigenvalues and learning rate h < 2/λ_max)
- Evidence anchors:
  - [abstract] "backward-SGD converges to a point while the standard forward-SGD generally only converges to a distribution"
  - [Section 3.1] Explicit quadratic loss example showing forward: hε_n + ... + h(1-h)^(n-1)ε_1 vs backward: hε_1 + ... + h(1-h)^(n-1)ε_n
  - [corpus] Weak/absent - neighboring papers focus on second-order methods, not iteration order effects
- Break condition: Contraction assumption fails if learning rate too large or loss landscape highly non-convex far from minima

### Mechanism 2
- Claim: Backward iterates converge to points sampled from the forward stationary distribution
- Mechanism: By Theorem 2.6, the backward limit point θ* is distributed according to the same stationary distribution μ_θ* that forward iterates converge to in distribution. This preserves generalization properties while improving stability
- Core assumption: Random operators T_i are i.i.d. and backward iterates converge to a random point (satisfies Theorem 2.6 conditions)
- Evidence anchors:
  - [Section 2.2] "the random point θ* is distributed according to the same forward iterate stationary distribution μ_θ*"
  - [Figure 3] Intermittent backward experiment shows backward trajectories converge to different points while forward oscillates between them
  - [corpus] No direct evidence - this appears to be a novel finding
- Break condition: If operators are not i.i.d. (e.g., curriculum learning changes batch distribution over time), the relationship may not hold

### Mechanism 3
- Claim: Backward-SGD reduces variance and accelerates convergence without sacrificing generalization
- Mechanism: The point convergence property eliminates perpetual oscillation around solutions, producing more stable training curves. Since convergence points are sampled from the forward distribution, test performance is preserved on average
- Core assumption: Training occurs in small-batch, constant-learning-rate regime where instability is pronounced
- Evidence anchors:
  - [abstract] "improved convergence is attributed to the diminishing stochastic noise in backward-SGD as training progresses"
  - [Figure 2] ResNet-18 on CIFAR-10 shows decreased variance and faster convergence across all 5 seeds
  - [Section A.5] AdamW experiments confirm the effect extends beyond vanilla SGD
  - [corpus] Weak - neighboring papers on second-order optimization don't address iteration order
- Break condition: Full-batch regime (backward and forward coincide); very small learning rates where forward already converges well

## Foundational Learning

- Concept: **Banach Fixed Point Theorem (Contraction Mapping Principle)**
  - Why needed here: Core mathematical tool proving backward-SGD converges to a point. States that a contraction T with k < 1 has a unique fixed point found by iteration
  - Quick check question: Given T(θ) = θ - h∇L(θ) near a minimum with Hessian H, what learning rate ensures T is a contraction? (Answer: h < 2/λ_max where λ_max is largest Hessian eigenvalue)

- Concept: **Markov Chains and Stationary Distributions**
  - Why needed here: Forward-SGD iterates form a Markov chain θ_n = T_n(θ_{n-1}) converging to stationary distribution μ, explaining why forward doesn't converge to points
  - Quick check question: Why does forward-SGD with constant learning rate fail to converge to a point? (Answer: Fresh stochastic noise added at each step prevents convergence to fixed point)

- Concept: **Lie Brackets of Vector Fields**
  - Why needed here: Approximate backward-SGD corrections involve [∇L_i, ∇L_j] = H_i∇L_j - H_j∇L_i, enabling cheaper approximations
  - Quick check question: What does the Lie bracket [V_i, V_j] measure? (Answer: Non-commutativity of applying the two vector field transformations in different orders)

## Architecture Onboarding

- Component map:
  ```
  Standard Training Loop:          Backward-SGD (Naive):
  θ_0 → T_1(θ_0) → T_2(T_1(θ_0))   θ_0 → T_1(θ_0) → T_1(T_2(θ_0))
         ↓                                ↓
       θ_n = T_n(...T_1(θ_0))          θ_n = T_1(...T_n(θ_0))
  
  Key difference: Composition order reversed
  ```

- Critical path:
  1. Store all batch gradients from training start
  2. At step n, replay batches B_1, B_2, ..., B_n in reverse order from initialization
  3. Computational cost: O(n²) for n steps vs O(n) for forward
  4. Alternative: Use approximation θ̃_n = θ_n + h²∑[∇L_i, ∇L_j](θ_0)

- Design tradeoffs:
  - **Full backward**: Best convergence/stability, prohibitive O(n²) cost
  - **Intermittent backward**: Reset periodically, cap computation via window size, may converge to different points at each reset
  - **Approximate backward**: O(n) cost with correction term, but O(h³) error may be significant for practical learning rates
  - **Late-stage backward**: Run forward for exploration, switch to backward for stable convergence (Figure 11 shows this works)

- Failure signatures:
  - Quadratic memory/time growth if implementing full backward naively
  - Different convergence points across seeds (expected, not a bug - but complicates reproducibility studies)
  - Approximation corrections too weak at practical learning rates

- First 3 experiments:
  1. **Toy validation**: Train MLP on synthetic quadratic data (Section 3.1 setup) with batch_size=1, lr=0.05, 1400 steps. Compare forward vs backward variance in loss curves
  2. **Scale test**: ResNet-18 on CIFAR-10, lr=0.025, batch_size=8, 2000 steps. Plot individual seeds (not averaged) to see per-trajectory stability
  3. **Intermittent backward**: MLP on FashionMNIST, switch from forward to backward at step 1000. Observe if backward "locks in" to a point while forward continues oscillating

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can efficient, higher-order approximations of the backward trajectory (beyond the second-order Lie bracket) be developed to replicate backward-SGD stability without the quadratic computational cost?
- Basis: [explicit] Section C states the derived second-order approximation is "not immediately useful because the higher order terms seem to matter," but suggests this path for future research.
- Why unresolved: The paper provides a theoretical correction term but finds it insufficient in practice, leaving the development of a practical, accurate approximation open.
- Evidence: A computationally efficient algorithm incorporating higher-order corrections that achieves convergence variance comparable to exact backward-SGD.

### Open Question 2
- Question: How should the "intermittent backward" strategy (periodic resetting of the backward iteration) be configured to consistently improve test performance rather than converging to arbitrary suboptimal points?
- Basis: [explicit] The conclusion proposes applying backward order on a fixed window or after resets but notes the trajectory may change unpredictably.
- Why unresolved: The paper observes that intermittent backward updates converge to different points depending on the reset time, but does not establish a policy to ensure these points are optimal.
- Evidence: A defined heuristic for reset timing (e.g., based on loss plateau detection) that consistently yields higher test accuracy than standard SGD.

### Open Question 3
- Question: Can the stability of backward trajectories be leveraged to establish more reliable early-stopping criteria or faster hyperparameter screening compared to standard noisy forward trajectories?
- Basis: [explicit] The conclusion suggests this application "may possibly lead toward more reliable early-stopping criteria" by providing stabler, easier-to-interpret learning curves early in training.
- Why unresolved: This is presented as a conjecture; the paper does not verify if the early stability of backward curves correlates better with final generalization than forward curves.
- Evidence: Experiments demonstrating that early metrics from backward-SGD (e.g., loss variance) predict final model performance more accurately than those from forward-SGD.

## Limitations
- Theoretical analysis assumes contractive operators near minima, which may not hold in highly non-convex deep learning landscapes
- Full backward-SGD has O(n²) computational complexity, making it impractical for large-scale training
- Empirical validation focuses on moderate-scale problems and may not generalize to modern large models or distributed settings
- Approximation methods (Lie bracket corrections) have limited accuracy at practical learning rates with O(h³) error bounds

## Confidence
**High Confidence**: The core mathematical result showing backward-SGD converges to points while forward-SGD converges to distributions in contractive regimes. This follows directly from Banach's fixed point theorem and Markov chain theory.

**Medium Confidence**: The practical significance of the variance reduction and stability improvements. While the toy and moderate-scale experiments support this, the effect size and utility on large-scale problems remain to be validated.

**Low Confidence**: The proposed approximation methods (Lie bracket corrections, intermittent backward) achieving comparable benefits with O(n) complexity. The O(h³) error bound for approximations suggests limited accuracy at practical learning rates.

## Next Checks
1. **Scaling Experiment**: Test backward-SGD on a modern architecture (Vision Transformer or larger ResNet variant) with typical batch sizes (128-512) to assess whether stability benefits persist at scale.

2. **Approximation Error Analysis**: Implement the Lie bracket correction method and measure actual error vs full backward-SGD across learning rates h ∈ [0.001, 0.1] to determine if the O(h³) bound accurately predicts practical performance.

3. **Non-convex Landscape Study**: Train on highly non-convex objectives (e.g., deep networks with poor initialization or in bad local minima) to test the contraction assumption's limits and whether backward-SGD's point convergence breaks down.