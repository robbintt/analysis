---
ver: rpa2
title: Diverse In-Context Example Selection After Decomposing Programs and Aligned
  Utterances Improves Semantic Parsing
arxiv_id: '2504.03541'
source_url: https://arxiv.org/abs/2504.03541
tags:
- given
- event
- scud4icl
- clause
- returns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCUD4ICL improves semantic parsing by decomposing programs into
  fragments and generating corresponding sub-utterances. This creates a larger pool
  of focused in-context examples that reduces interference from irrelevant content.
---

# Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing

## Quick Facts
- arXiv ID: 2504.03541
- Source URL: https://arxiv.org/abs/2504.03541
- Reference count: 17
- Diverse in-context example selection with program decomposition and sub-utterance generation consistently improves semantic parsing accuracy, especially for smaller LLMs and lower-resource languages.

## Executive Summary
SCUD4ICL improves semantic parsing by decomposing programs into fragments and generating corresponding sub-utterances. This creates a larger pool of focused in-context examples that reduces interference from irrelevant content. The method selects diverse, relevant examples for each test utterance using a greedy algorithm that balances relevance and diversity. Experiments on SMCalFlow, GeoQuery, and MTOP datasets show consistent accuracy gains over the baseline CoverLS, especially for smaller LLMs, larger trees, and lower-resource languages. Accuracy improvements are particularly notable when training and test distributions differ in length or template structure.

## Method Summary
SCUD4ICL decomposes training programs into subtrees and generates sub-utterances that are linguistically subsumed by the original utterance. The method augments the training pool with these fragments, then selects diverse in-context examples using a greedy algorithm that balances coverage of test utterance tokens, avoids ancestor/descendant redundancy, and maximizes BM25 similarity. The approach addresses interference from irrelevant clauses in whole examples and improves coverage of test query structure through focused fragments.

## Key Results
- SCUD4ICL consistently outperforms CoverLS baseline across all evaluated models (GPT-3.5, Mistral-7B, Llama3-8B) on SMCalFlow, GeoQuery, and MTOP datasets
- Largest gains occur with smaller training pools (T5 vs T10), larger selection budgets (M=10 vs M=5), and when training/test distributions differ in length or template structure
- Subsumed generation provides 2-5% absolute accuracy gains over independent generation, and decomposed examples outperform whole examples by 3-4% absolute accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing programs into focused fragments reduces interference from irrelevant clauses during in-context learning.
- Mechanism: Complex training programs contain substructures only partially relevant to a test query. When whole examples are used as demonstrations, irrelevant clauses may distract the LLM. By extracting subtrees corresponding to specific semantic operations, SCUD4ICL provides cleaner signal for each program component.
- Core assumption: Irrelevant content in whole ICEs causes measurable performance degradation in semantic parsing tasks.
- Evidence anchors:
  - [abstract]: "decomposing the pool of available ICE trees into fragments... minimizes interference from irrelevant content"
  - [Page 2, Col 1]: Shows Figure 1 example where irrelevant clauses (attendees, times) in whole ICEs cause incorrect predictions, while decomposed ICEs lead to correct output
  - [Page 8, Table 8]: SCUD4ICL outperforms WholeExamples across all models (e.g., GPT3.5: 45.8 vs 42.5 on EN), demonstrating fragment selection itself provides gains
  - [corpus]: Levy et al. (2023) established that diverse demonstrations improve compositional generalization; SCUD4ICL extends this by arguing diversity alone is insufficient when examples contain distracting content

### Mechanism 2
- Claim: Sub-utterances generated to be linguistically "subsumed" by original utterances outperform independently generated descriptions.
- Mechanism: When an LLM generates sub-utterances conditioned on both the subtree and the original utterance (instructed to preserve original phrasing), the resulting text matches the linguistic style and domain-specific vocabulary of actual test inputs. Independent generation tends to produce verbatim function-name translations that don't match natural user language.
- Core assumption: LLMs can reliably identify which parts of an original utterance correspond to a given subtree when explicitly prompted.
- Evidence anchors:
  - [Page 4, Col 1]: "our key idea is to view xi,j as a sub-utterance of xi... The LLM is instructed to preserve as much of the original utterance in generating the sub-utterance"
  - [Page 4, Figure 3]: Shows concrete examples where independent generation produces unnatural utterances ("Schedule an event for the 5th of next month" vs. subsumed "make event on 5th today" from original "Let's make a lunch meeting... at the Starbucks Cafe on 5th")
  - [Page 7, Table 5]: Subsumed generation provides consistent gains over independent generation across models (GPT3.5 EN: 45.8 vs 43.4; Mistral:7b HI: 37.2 vs 35.8)

### Mechanism 3
- Claim: Greedy selection balancing relevance and diversity from an augmented fragment pool improves coverage of test query structure.
- Mechanism: The algorithm iteratively selects candidates that: (1) cover uncovered tokens in the test utterance, (2) avoid ancestor/descendant redundancy, (3) introduce novel anonymized templates, and (4) maximize BM25 similarity. This balances covering different aspects of the query while avoiding redundant or conflicting examples.
- Core assumption: Test utterances can be adequately addressed by combining patterns from multiple focused fragments rather than requiring complete program templates.
- Evidence anchors:
  - [Page 5, Algorithm 1]: Formalizes the selection criteria with coverage tracking, template deduplication, and BM25 ranking
  - [Page 6, Results]: "The main reason for the gains over CoverLS is the augmentation of the training pool with decomposed training instances since the algorithm used for selecting the M instances are largely similar"
  - [Page 8, Table 6]: Breakdown by tree depth shows gains concentrated in depth 3 trees for EN (GPT3.5: 36.6→42.9), depth 2 for HI (GPT3.5: 48.9→57.5), suggesting selection targets structures most vulnerable to interference

## Foundational Learning

- Concept: **Abstract Syntax Trees (ASTs) and Subtree Decomposition**
  - Why needed here: SCUD4ICL operates on programs as tree structures, extracting subtrees as fragments. Without understanding AST traversal, subtree extraction, and the relationship between tree structure and semantic meaning, the decomposition strategy is opaque.
  - Quick check question: Given the Lisp-like program `CreateEvent(AND(has_subject(lunch), starts_at(Tomorrow())))`, what are two valid subtrees and what semantic operations do they represent?

- Concept: **In-Context Learning (ICL) and Interference Effects**
  - Why needed here: The paper's core thesis is that irrelevant content in demonstrations interferes with correct generation. Understanding how LLMs use demonstrations—and fail when they contain distractors—is essential for evaluating whether decomposition actually addresses a real problem.
  - Quick check question: If an ICL demonstration contains a program with 10 clauses but the test query only requires 2 of those operations, what specific failure mode might occur during generation?

- Concept: **Determinantal Point Processes (DPPs) and Coverage-Based Selection**
  - Why needed here: The baseline CoverLS and related methods (DPP-based selection) optimize for diversity. Understanding the mathematical formulation of diversity helps contextualize why SCUD4ICL's greedy approach with structural constraints is a departure.
  - Quick check question: In diverse selection, why might maximizing dissimilarity between examples be suboptimal if all examples are equally irrelevant to the test query?

## Architecture Onboarding

- Component map:
  Training Set T → [Decomposition Engine (LLM + syntax constraints)] ↓ Augmented Pool TD (whole + fragments) ↓ [Selection Algorithm] ← Test Utterance x0 ↓ Selected ICEs S (size M) ↓ [Target LLM] → Predicted Program ŷ0

- Decomposition Engine: GPT-4 (default) prompted to extract subtrees and generate subsumed utterances (one-time preprocessing)
- Selection Algorithm: Greedy coverage-based selector (Algorithm 1) running at inference time
- Target LLM: Any instruction-following model (GPT-3.5, GPT-4o, Mistral, Llama evaluated in paper)

- Critical path: The decomposition step is offline and can be precomputed. The critical runtime path is the selection algorithm, which requires BM25 similarity computation and coverage tracking across TD. For a pool of ~500 decomposed examples, selection is sub-second.

- Design tradeoffs:
  - **Decomposition depth**: The paper extracts subtrees at all internal nodes. Shallower decomposition reduces pool size but may miss focused fragments. No explicit depth limit is reported.
  - **Pool size vs. selection budget**: Gains are higher for smaller training pools (T5 vs. T10 in Table 4) and larger budgets (M=10 vs. M=5), suggesting decomposition matters most when selection has flexibility but pool is limited.
  - **LLM for decomposition**: Table 7 shows Mixtral-8x22b decompositions still provide gains over baseline but less than GPT-4o. Trade-off between decomposition cost and quality.
  - **Language familiarity**: Gains are largest for low-resource languages (Hindi) and unfamiliar code schemas, smaller for English. Deployment on familiar domains may see limited benefit.

- Failure signatures:
  - **Exact match degradation**: Table 9 shows SCUD4ICL sometimes underperforms CoverLS on exact match despite execution accuracy gains. Assembling from fragments can produce semantically equivalent but syntactically different programs.
  - **Depth-1 trees show no gains**: Table 6 shows depth-1 test trees have identical performance (0.0 or 50.0 baseline) as they're too simple to benefit from decomposition.
  - **Large pool + small budget**: If training pool is large and M is small, baseline CoverLS may find sufficiently relevant whole examples, reducing decomposition's marginal value.

- First 3 experiments:
  1. **Replicate the T5 → TD augmentation**: Take a small training set (50-100 examples), run decomposition with subsumed generation, verify that fragment utterances are linguistically coherent by manual inspection of 20 random samples.
  2. **Ablate subsumption vs. independent generation**: Using the same fragments, generate utterances both ways and measure accuracy delta on a held-out test split. Expect 2-5% absolute gain from subsumption based on Table 5.
  3. **Profile selection latency**: Measure runtime of Algorithm 1 for pools of varying sizes (100, 500, 1000 fragments) to establish scaling characteristics before deployment. The paper does not report timing data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the internal mechanisms by which an LLM enforces "subsumption" when generating sub-utterances?
- Basis in paper: [explicit] The authors state, "we do not understand how the LLM implements utterance subsumption" regarding the conditional generation of sub-utterances.
- Why unresolved: The study relies on commercial black-box LLMs (GPT-3.5/4), preventing internal analysis of how the model constraints sub-utterances to be semantically contained within the original utterance.
- What evidence would resolve it: Interpretability studies or probing classifiers on open-weights models to identify how attention heads prioritize parent-utterance tokens during generation.

### Open Question 2
- Question: Does the performance gain of decomposition diminish or vanish as the size of the available training pool approaches the full dataset size?
- Basis in paper: [inferred] The authors observe that relative gains over the baseline are higher for smaller training pools ($T_5$) than larger ones ($T_{10}$), suggesting that as the baseline finds more relevant whole examples, the utility of fragments may drop.
- Why unresolved: Experiments were restricted to small, simulated low-resource splits ($T_5, D3$) rather than the full datasets.
- What evidence would resolve it: An ablation study evaluating SCUD4ICL on the complete SMCalFlow or GeoQuery training sets to plot the performance curve relative to pool density.

### Open Question 3
- Question: Can prefix tuning or task-specific adaptation of smaller, in-house LLMs outperform the current in-context learning approach?
- Basis in paper: [explicit] The limitations section lists "experiments with prompt/prefix tuning, and/or setting up a smaller in-house LLM and adapting it to our task" as an area of future interest.
- Why unresolved: The current work relies solely on frozen, general-purpose LLMs accessed via API, without exploring fine-tuning.
- What evidence would resolve it: A comparative study fine-tuning a smaller model (e.g., Llama-3-8b) on the decomposed fragment pool versus the current inference-time selection method.

## Limitations
- Decomposition quality dependency: SCUD4ICL's gains rely on the LLM's ability to correctly identify relevant subtrees and generate linguistically faithful sub-utterances, with ~10% decomposition errors reported.
- Exact match degradation: The method sometimes underperforms baseline on exact match despite execution accuracy gains, suggesting fragment-based assembly may produce syntactically different but semantically equivalent programs.
- Limited linguistic diversity: The method assumes clean subtree-to-sub-utterance alignment, which may break down for idiomatic expressions or highly elliptical language.

## Confidence
- **High confidence**: The decomposition mechanism itself (Mechanism 1) is well-supported by direct ablation studies (Table 8) and qualitative examples (Figure 1). The consistent performance gains across datasets and model sizes provide strong evidence that fragment-based demonstrations reduce interference.
- **Medium confidence**: The subsumption generation advantage (Mechanism 2) is demonstrated but relies on qualitative judgments of "naturalness" and lacks direct comparison to human-written sub-utterances. The gains are consistent but modest (2-5% absolute).
- **Medium confidence**: The selection algorithm's effectiveness (Mechanism 3) is supported by performance improvements but lacks ablations isolating the contribution of each selection criterion (coverage, template dedup, BM25 ranking).

## Next Checks
1. **Manual quality audit of decomposition outputs**: Randomly sample 50 decomposed fragments from SMCalFlow and rate both syntactic correctness and sub-utterance naturalness. This validates whether the reported 10% error rate is representative and identifies failure modes.
2. **Cross-lingual decomposition robustness test**: Apply the decomposition pipeline to a held-out language pair (e.g., FR→EN) not used in the paper and measure degradation in sub-utterance quality. This tests whether decomposition performance transfers across language pairs or is English-centric.
3. **Selection algorithm ablation study**: Implement variants of Algorithm 1 that: (a) disable template deduplication, (b) disable ancestor/descendant checks, (c) use random selection instead of BM25. Measure the contribution of each component to overall accuracy gains.