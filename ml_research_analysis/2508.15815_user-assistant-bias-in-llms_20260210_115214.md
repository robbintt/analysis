---
ver: rpa2
title: User-Assistant Bias in LLMs
arxiv_id: '2508.15815'
source_url: https://arxiv.org/abs/2508.15815
tags:
- bias
- user
- assistant
- qwen
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of user-assistant bias in large
  language models (LLMs), defined as the tendency of an LLM to preferentially rely
  on information from either the user or assistant role when there is a conflict.
  The authors create a task-agnostic benchmark called UserAssist to evaluate this
  bias in 52 frontier models.
---

# User-Assistant Bias in LLMs

## Quick Facts
- **arXiv ID**: 2508.15815
- **Source URL**: https://arxiv.org/abs/2508.15815
- **Reference count**: 19
- **Primary result**: Most instruction-tuned models exhibit strong user bias, while base and reasoning models are close to neutral; bias can be bidirectionally controlled via DPO.

## Executive Summary
This paper introduces the concept of user-assistant bias in large language models, defined as the tendency to preferentially rely on information from either the user or assistant role when conflicting information exists. The authors create a task-agnostic benchmark called UserAssist to evaluate this bias across 52 frontier models. Through controlled experiments, they demonstrate that human-preference alignment amplifies user bias while reasoning fine-tuning reduces it. Most significantly, they show that user-assistant bias can be precisely controlled via direct preference optimization (DPO) and that this control generalizes to realistic multi-turn conversations.

## Method Summary
The authors develop the UserAssist benchmark with synthetic multi-turn dialogues where user and assistant assign conflicting attributes to entities. They measure bias using a score ranging from -1 (assistant-biased) to +1 (user-biased), calculated from generation counts or log probabilities. To control bias, they employ DPO with synthetically generated preference pairs where the "chosen" assignment is determined by role rather than content. The intervention uses LoRA-based fine-tuning with beta=0.1, rank=8 adapters, and is evaluated both on held-out test sets and a realistic multi-turn debate dataset from PhilPapers.

## Key Results
- Most instruction-tuned models exhibit strong user bias, while base and reasoning models are close to neutral
- Human-preference alignment amplifies user bias, while reasoning fine-tuning reduces it
- User-assistant bias can be bidirectionally controlled via DPO on UserAssist-train and generalizes to realistic multi-turn conversations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Explicit role tags function as learned control signals that induce systematic preferences for information sources
- **Mechanism**: During instruction tuning, models learn to use tag identity as a heuristic for weighting information reliability, creating an inductive bias where source tag dictates information integration strategy
- **Core assumption**: The preference shift is caused by tag-to-content correlation in training distribution, not semantic content alone
- **Evidence anchors**: [abstract] "asymmetries in the training data associated with different role tags can introduce inductive biases"; [introduction] "training with tags inevitably involves placing different types of content and different loss masks in different tags"
- **Break condition**: If models showed no preference difference when exact same conflicting content was swapped between User and Assistant tags

### Mechanism 2
- **Claim**: Human-preference alignment amplifies user bias because preference datasets implicitly frame user as primary validation source
- **Mechanism**: RLHF/DPO datasets condition models to view User turn as source of truth/object to maximize, while reasoning fine-tuning conditions models to trust Assistant reasoning traces
- **Core assumption**: Direction of optimization shifts attention weight/trust allocated to respective role tags
- **Evidence anchors**: [abstract] "human-preference alignment amplifies user bias, while reasoning fine-tuning reduces it"; [section 4.2] "Fine-tuning with human-preference datasets... consistently increases user bias across both model backbones"
- **Break condition**: If reasoning fine-tuning also increased user bias, claim that reasoning promotes self-reliance would be weakened

### Mechanism 3
- **Claim**: User-assistant bias corresponds to a latent preference dimension that can be precisely manipulated via targeted DPO
- **Mechanism**: Synthetic conflicts labeled by role preference reshape model's internal reward model, conditioning it to assign higher likelihood to sequences consistent with "chosen" role
- **Core assumption**: Bias is learning a generalizable "policy" for prioritizing role-specific context, not memorizing specific tokens
- **Evidence anchors**: [abstract] "userâ€“assistant bias can be bidirectionally controlled via direct preference optimization (DPO)"; [section 4.3] "user-assistant bias is potentially governed by a shared latent preference dimension"
- **Break condition**: If bias induced by DPO on UserAssist-train failed to generalize to realistic multi-turn debate dataset

## Foundational Learning

### Concept: Inductive Bias
- **Why needed here**: To understand that training data structure (role tags) forces models to make assumptions about how to weigh information, creating systematic preference unrelated to semantic truth
- **Quick check question**: If a model is trained only on data where user is always wrong, would it develop a bias?

### Concept: Direct Preference Optimization (DPO)
- **Why needed here**: The paper uses DPO as primary lever to control bias; understanding how DPO shifts policy by optimizing classifier on preference pairs is critical
- **Quick check question**: How does DPO differ from Reinforcement Learning (PPO) in how it uses reward signal?

### Concept: Sycophancy vs. Stubbornness
- **Why needed here**: Paper defines user-assistant bias as structural precursor to these behaviors; distinguishing "agreeing with user" from "refusing to update on new info" is key to interpreting results
- **Quick check question**: Does high "user bias" in this paper necessarily mean the model is sycophantic in real-world political debate?

## Architecture Onboarding

### Component map
UserAssist-Test -> Evaluation Protocol -> Bias Score -> Intervention Layer (DPO) -> UserAssist-Train -> Validation on Realistic Conversation Dataset

### Critical path
1. Establish baseline bias score on UserAssist-Test for target model
2. Construct DPO pairs from UserAssist-Train (e.g., chosen = assistant_assignment, rejected = user_assignment)
3. Fine-tune and re-evaluate to confirm directional shift
4. Validate generalization on "Realistic Conversation" dataset (PhilPapers debates)

### Design tradeoffs
- **Synthetic Control vs. Realism**: UserAssist offers clean causal attribution but low ecological validity; paper bridges this by validating on PhilPapers debate set, but domain remains restricted
- **Intervention Simplicity vs. Side Effects**: Lightweight DPO effectively controls bias, but authors note potential safety risks (e.g., reducing corrigibility if assistant bias is too high)

### Failure signatures
- **Refusal loops**: If assistant bias is pushed too high, model may ignore user corrections, manifesting as "stubbornness"
- **Echo chambers**: If user bias is pushed too high, model may agree with contradictory user input, manifesting as extreme sycophancy

### First 3 experiments
1. **Baseline Measurement**: Run UserAssist-Test on standard instruct model (e.g., Llama-3.1-8B-Instruct) to confirm expected positive user bias score
2. **Ablation by Objective**: Fine-tune same base model on preference dataset (HH-RLHF) and reasoning dataset (LIMO) separately to observe divergent drift in bias scores
3. **Bidirectional Control**: Perform DPO on UserAssist-Train to flip bias score from positive (user) to negative (assistant) and verify change holds on Object-Color subset when trained only on Symbol-Value subset

## Open Questions the Paper Calls Out

1. **Domain generalization**: Does user-assistant bias generalize to broader and more diverse conversational domains beyond synthetic and philosophical debate settings used in this study? The authors state further study is needed to assess prevalence in broader conversational settings.

2. **Reasoning trace dependency**: Does reduction of user bias via reasoning fine-tuning depend on logical validity of traces, or simply on increased presence of assistant-generated tokens? The authors hypothesize reasoning distillation works by teaching model to "rely on the reasoning trace generated by itself" but don't ablate reasoning content.

3. **Safety impact**: To what extent does bidirectional control of user-assistant bias impact other safety-relevant behaviors, such as corrigibility and resistance to correction? While paper demonstrates bias can be controlled via DPO, it flags these specific safety trade-offs as potential risks without quantifying them.

## Limitations

- **Domain generalization gap**: Realistic dataset (PhilPapers debates) remains limited to academic philosophy topics; unclear whether bidirectional control generalizes to domains with different conversational dynamics
- **Intervention scope uncertainty**: DPO-based bias control shows promising results but paper doesn't investigate potential side effects on other model capabilities
- **Model architecture dependence**: Study focuses on transformer-based LLMs with specific training histories; extent to which user-assistant bias mechanisms apply to other architectures remains unknown

## Confidence

**High confidence**: Existence of user-assistant bias in instruction-tuned models and its measurability through UserAssist benchmark; correlation between human-preference alignment and increased user bias, and reasoning fine-tuning with decreased bias

**Medium confidence**: Claim that user-assistant bias corresponds to "shared latent preference dimension" that can be precisely manipulated via DPO; while bidirectional control experiments are convincing, underlying mechanism requires further investigation

**Low confidence**: Assertion that bias is primarily caused by "asymmetries in training data associated with different role tags" rather than other potential factors like tokenization artifacts or loss function design

## Next Checks

1. **Cross-domain generalization test**: Apply same DPO bias control procedure to completely different domain (e.g., medical diagnosis dialogues or customer service interactions) to verify whether bidirectional bias control generalizes beyond philosophical debates

2. **Capability interference assessment**: After steering model to extreme assistant bias, systematically evaluate performance on unrelated tasks like mathematical reasoning, code generation, or instruction-following to detect any unintended degradation in core capabilities

3. **Alternative architecture validation**: Replicate user-assistant bias measurement and control experiments on non-transformer architectures (e.g., Mamba, RWKV) or models with different pretraining objectives to determine whether observed phenomena are architecture-specific or more fundamental