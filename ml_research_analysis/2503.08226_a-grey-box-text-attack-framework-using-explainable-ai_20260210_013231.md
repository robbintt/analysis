---
ver: rpa2
title: A Grey-box Text Attack Framework using Explainable AI
arxiv_id: '2503.08226'
source_url: https://arxiv.org/abs/2503.08226
tags:
- adversarial
- attack
- sentences
- surrogate
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Grey-box Text Attack Framework using Explainable
  AI to generate adversarial text that can fool transformer models without requiring
  access to the model's internal parameters. The method leverages LIME to identify
  the most influential words in a sentence, then replaces them with semantically similar
  synonyms to create adversarial examples.
---

# A Grey-box Text Attack Framework using Explainable AI

## Quick Facts
- arXiv ID: 2503.08226
- Source URL: https://arxiv.org/abs/2503.08226
- Authors: Esther Chiramal; Kelvin Soh Boon Kai
- Reference count: 17
- Primary result: Grey-box text attack framework using LIME and synonym replacement achieves 91% success rate against BERT with minimal word changes

## Executive Summary
This paper introduces a grey-box text attack framework that generates adversarial examples against transformer models without requiring access to internal model parameters. The approach leverages LIME (Local Interpretable Model-agnostic Explanations) to identify the most influential words in input text, then systematically replaces these words with semantically similar synonyms to create adversarial examples. The framework demonstrates high success rates in fooling sentiment classification models including BERT, RoBERTa, and DistilBERT, while maintaining grammatical coherence and semantic plausibility of the generated adversarial text.

## Method Summary
The framework operates by first applying LIME to explain individual predictions from a surrogate transformer model, identifying words with the highest attribution scores as targets for modification. These influential words are then replaced with synonyms from a thesaurus, with the attack process continuing iteratively until the model's prediction changes or a maximum number of modifications is reached. The generated adversarial examples are validated using the surrogate model to ensure transferability before being tested against the target black-box model. The approach balances attack effectiveness with maintaining the original text's semantic meaning and grammatical structure.

## Key Results
- Achieves 91% success rate in fooling BERT model on sentiment classification tasks
- Successfully transfers attacks to RoBERTa and DistilBERT with comparable success rates
- Generates adversarial examples with minimal word changes while maintaining semantic coherence
- Demonstrates effectiveness across multiple datasets including IMDB and Yelp reviews

## Why This Works (Mechanism)
The framework exploits the sensitivity of transformer models to specific input features identified through LIME explanations. By targeting words with high attribution scores and replacing them with semantically similar alternatives, the method creates subtle perturbations that significantly alter the model's prediction while remaining imperceptible to human readers. The grey-box approach balances between white-box (full model access) and black-box (no model access) attacks by using surrogate models to guide the attack generation process, enabling effective transferability to black-box target models.

## Foundational Learning
**LIME (Local Interpretable Model-agnostic Explanations)**
- Why needed: Provides local approximations of model behavior to identify influential input features
- Quick check: Verify LIME explanations align with intuitive feature importance for simple test cases

**Synonym replacement strategies**
- Why needed: Enables semantic-preserving modifications to create adversarial examples
- Quick check: Ensure thesaurus coverage includes domain-specific vocabulary and maintains grammatical correctness

**Transferability in adversarial attacks**
- Why needed: Allows attacks generated against surrogate models to generalize to black-box targets
- Quick check: Test attack success rates across different model architectures with varying pre-training objectives

## Architecture Onboarding

**Component map**
LIME explainer -> Synonym replacement engine -> Surrogate model validator -> Black-box target model

**Critical path**
Input text -> LIME attribution scores -> Word selection -> Synonym replacement -> Prediction validation -> Adversarial example generation

**Design tradeoffs**
- Balance between attack success rate and semantic preservation
- Computational cost of LIME explanations vs. attack effectiveness
- Choice of surrogate model architecture affecting transferability

**Failure signatures**
- Low LIME attribution scores leading to ineffective word selection
- Poor synonym quality causing semantic drift or grammatical errors
- Limited transferability between surrogate and target model architectures

**First 3 experiments**
1. Baseline test: Apply framework to simple sentiment examples with known vulnerabilities
2. Transferability test: Generate attacks against one transformer model and validate on architecturally different models
3. Synonym quality assessment: Compare attack success rates using different thesaurus sources and replacement strategies

## Open Questions the Paper Calls Out
None

## Limitations
- LIME explanations may not accurately capture true model decision boundaries
- Synonym replacement quality depends on thesaurus coverage and contextual appropriateness
- Transferability assumptions may not hold across diverse model architectures and datasets
- Framework effectiveness untested on languages beyond English and specialized domains

## Confidence
- **High confidence**: Technical methodology combining LIME with synonym replacement is sound and reproducible
- **Medium confidence**: Transferability claims between surrogate and target models may vary based on architectural differences
- **Low confidence**: Claims about framework robustness across diverse languages, domains, and model architectures extend beyond empirical validation

## Next Checks
1. Conduct ablation studies testing framework performance with alternative explanation methods (SHAP, Integrated Gradients) to verify LIME's specific contribution to attack success rates
2. Test transferability across broader range of model pairs, including cross-lingual models and models with different tokenization strategies, to quantify limits of surrogate-based validation
3. Implement human evaluation study to assess whether adversarial examples maintain semantic coherence and grammatical correctness across diverse sentence structures and domains