---
ver: rpa2
title: Towards Autoformalization of LLM-generated Outputs for Requirement Verification
arxiv_id: '2511.11829'
source_url: https://arxiv.org/abs/2511.11829
tags:
- formal
- language
- lean
- natural
- autoformalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an early framework for verifying the correctness
  of LLM-generated structured outputs using autoformalization. The core method involves
  translating natural language requirements and LLM-generated outputs into formal
  Lean propositions, then using a theorem prover to check logical equivalence.
---

# Towards Autoformalization of LLM-generated Outputs for Requirement Verification

## Quick Facts
- arXiv ID: 2511.11829
- Source URL: https://arxiv.org/abs/2511.11829
- Reference count: 27
- This paper introduces an early framework for verifying the correctness of LLM-generated structured outputs using autoformalization.

## Executive Summary
This paper introduces an early framework for verifying the correctness of LLM-generated structured outputs using autoformalization. The core method involves translating natural language requirements and LLM-generated outputs into formal Lean propositions, then using a theorem prover to check logical equivalence. Two experiments demonstrate the approach: one successfully verifies logical equivalence between two differently worded requirements, while the other identifies a logical inconsistency between a natural language requirement and an LLM-generated Gherkin scenario due to extra variables not present in the original requirement. The findings show that autoformalization can effectively detect logical discrepancies in LLM outputs, though the approach currently requires manual variable grounding. The paper establishes a proof-of-concept for formally verifying LLM-generated outputs against original requirements, laying groundwork for future research on automating this verification process and extending it to broader application domains.

## Method Summary
The method involves using DeepSeek-Prover-v2 (7B) to translate natural language requirements into Lean 4 propositions with typed variables and implication structure. The autoformalizer generates `def` statements representing the logical content of requirements. For verification, a second formalization is created (either another requirement or an LLM-generated output), and manual variable grounding aligns semantically equivalent variables across both formalizations. A biconditional theorem is then constructed and submitted to the Lean 4 theorem prover to check logical equivalence. Success indicates the two statements are logically equivalent; failure indicates potential inconsistency requiring investigation.

## Key Results
- Successfully verified logical equivalence between two differently-worded NL requirements (R1 and R2)
- Detected logical inconsistency between a natural language requirement and LLM-generated Gherkin scenario due to extra variables
- Demonstrated that manual variable grounding is essential for meaningful equivalence proofs
- Established proof-of-concept for formally verifying LLM-generated outputs against original requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs fine-tuned on formal mathematical data can translate informal natural language requirements into valid Lean propositions, enabling subsequent formal verification.
- **Mechanism:** DeepSeek-Prover-v2 (7B) uses chain-of-thought reasoning trained on Lean 4 syntax to parse NL semantics and generate formal propositions following an implication structure (condition → action). The model decomposes requirements into typed variables and logical connectives.
- **Core assumption:** The LLM's formal training transfers sufficiently to domain-specific requirement language (e.g., automotive seatbelt logic), and the NL input is unambiguous enough to permit a single valid formalization.
- **Evidence anchors:**
  - [abstract]: "exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements"
  - [section 3.1]: "DeepSeek-Prover-v2 takes this approach by training the model to perform a chain-of-thought process, recursively searching for proofs via subgoal decomposition"
  - [corpus]: "Autoformalizer with Tool Feedback" (arXiv:2510.06857) demonstrates improved formalization via iterative feedback, suggesting the base capability is viable but benefits from refinement
- **Break condition:** When NL contains ambiguous terms (e.g., "vehicle in motion" mapping to either Boolean or Integer types), the LLM may generate syntactically valid but semantically divergent formalizations that cannot be reconciled (see Section 5.1, Figure 10).

### Mechanism 2
- **Claim:** Biconditional equivalence proofs (PA ↔ PB) can formally detect logical consistency or discrepancy between two propositions.
- **Mechanism:** The theorem prover attempts to prove both forward (PA → PB) and reverse (PB → PA) implications. Success confirms equivalence; failure indicates either logical inconsistency or insufficient proof search—interpreted as a discrepancy signal in this framework.
- **Core assumption:** Proof failure reliably indicates logical inconsistency rather than prover limitations, and both propositions share a compatible variable signature after grounding.
- **Evidence anchors:**
  - [abstract]: "the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent"
  - [section 4.2, Figure 8]: "Lean code fails to prove R3 and G3 are logically equivalent" when G3 introduces extra variables (seat_occupancy, final_seatbelt_status) absent from R3
  - [corpus]: "Grammars of Formal Uncertainty" (arXiv:2505.20047) explicitly addresses the epistemological tension between probabilistic LLM outputs and deterministic verification guarantees
- **Break condition:** When propositions have non-overlapping variable sets or type mismatches, the prover cannot construct a valid proof state, yielding false-negative signals that may conflate formalization errors with genuine logical inconsistencies.

### Mechanism 3
- **Claim:** Manual variable grounding establishes semantic equivalence across differently-named variables in separate formalizations, enabling meaningful equivalence proofs.
- **Mechanism:** A human operator inspects LLM-generated Lean code, identifies variables representing the same underlying concept (e.g., "Seatbelt is inactive" vs. "Seatbelt is not plugged in"), and explicitly declares their equivalence to the Lean compiler before proof attempts.
- **Core assumption:** The operator correctly interprets both the original NL intent and the LLM's variable choices, and a consistent system ontology exists to ground against.
- **Evidence anchors:**
  - [section 5.2]: "a critical and inescapable requirement is the grounding of overlapping variables to prove logical equivalence. This necessity arises because Large Language Models (LLMs) frequently interpret and name variables differently across separate formalizations"
  - [section 4.1]: "By grounding, we need to manually point to the Lean compiler that a set of variables having different naming conventions actually refer to the same variable"
  - [corpus]: Limited direct coverage; corpus papers focus on formalization quality and retrieval-augmented approaches rather than grounding automation specifically
- **Break condition:** When no shared ontology or data dictionary exists, grounding becomes ad-hoc and error-prone; automating grounding with LLMs introduces a recursive verification problem (Section 5.3).

## Foundational Learning

- **Concept: Lean 4 theorem prover and syntax**
  - **Why needed here:** The entire verification pipeline outputs Lean propositions and relies on Lean's type system and proof tactics. Understanding `def`, `variable`, `Prop`, and `theorem` syntax is prerequisite to debugging formalization failures.
  - **Quick check question:** Given a Lean proposition `def seatbelt_chime : Prop := (speed ≥ 10 ∧ ¬seatbelt_fastened) → chime_active`, what would a successful biconditional proof against a similar proposition require?

- **Concept: Propositional logic and biconditional equivalence**
  - **Why needed here:** The core verification logic hinges on proving PA ↔ PB. Without understanding implication directionality and truth-table equivalence, one cannot interpret proof failures correctly.
  - **Quick check question:** If proposition A implies B but B does not imply A, what is the truth value of A ↔ B, and what does this mean for requirement consistency?

- **Concept: LLM non-determinism and temperature**
  - **Why needed here:** The paper identifies variable naming inconsistency as a direct consequence of autoregressive generation. Understanding why `pass@1` and `pass@k` differ explains why grounding is unavoidable.
  - **Quick check question:** Why might the same NL requirement formalized twice by the same LLM yield different variable names or types, and how does this affect downstream proof attempts?

## Architecture Onboarding

- **Component map:** NL requirement -> Autoformalizer (DeepSeek-Prover-v2) -> Lean proposition (def) -> Manual grounding -> Combined theorem -> Lean 4 theorem prover -> Proof result
- **Critical path:**
  1. NL requirement → Autoformalizer prompt (Appendix A.1 template)
  2. Autoformalizer → Lean proposition (def statement)
  3. Repeat for second statement (requirement or Gherkin output)
  4. Manual variable grounding (align semantic equivalents across both propositions)
  5. Combined theorem prompt to autoformalizer (Appendix A.2 template)
  6. Lean compiler execution → proof result
  7. Interpretation: success = logical consistency; failure = discrepancy requiring investigation

- **Design tradeoffs:**
  - **7B vs. 671B model:** Paper uses 7B due to resource constraints; larger model may improve formalization accuracy but increases latency and cost
  - **Manual vs. automated grounding:** Manual ensures correctness but doesn't scale; automated approaches (similarity-based embeddings, single-pass context injection) are proposed but unvalidated
  - **Single-pass vs. k-pass formalization:** Paper suggests generating multiple candidates (pass@k) may improve reliability, following findings in related work [15]

- **Failure signatures:**
  - **Type mismatch:** One formalization uses `Bool` for "vehicle in motion" while another uses `Int` (Section 5.1, Figure 10)
  - **Extra variables:** Gherkin output includes `seat_occupancy` and `final_seatbelt_status` not present in original requirement (Section 4.2)
  - **Undefined variable in theorem:** Grounding step omitted or incomplete; Lean compiler rejects theorem construction
  - **Proof timeout or exhaustive search failure:** May indicate prover limitation rather than true logical inconsistency

- **First 3 experiments:**
  1. **Baseline equivalence check:** Run the pipeline on a known-equivalent requirement pair (e.g., R1 and R2 from Section 4.1) to validate end-to-end workflow and confirm manual grounding sufficiency.
  2. **Controlled inconsistency injection:** Deliberately modify a Gherkin scenario to introduce an extra condition or variable, then verify the pipeline correctly flags the inconsistency (replicating R3 vs. G3 result).
  3. **Ambiguity sensitivity test:** Formalize the same ambiguous requirement (e.g., R4 "vehicle in motion") multiple times at different temperatures and analyze variable type/naming variance to quantify grounding burden.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the variable grounding and normalization step be reliably automated given a set of informal statements?
- **Basis:** [explicit] Section 5.4 (RQ2) and Section 5.2.
- **Why unresolved:** LLMs frequently name variables differently across formalizations, and manual grounding is currently the most time-consuming bottleneck in the pipeline.
- **What evidence would resolve it:** Development of a similarity-based grounding method or contextual single-pass formalization that aligns variables to a system ontology without human intervention.

### Open Question 2
- **Question:** What is the most effective method for autoformalizing an informal statement?
- **Basis:** [explicit] Section 5.4 (RQ1).
- **Why unresolved:** Single-pass conversion may lack accuracy for complex requirements, while selecting the most probable option from multiple generations (k-pass) requires further validation.
- **What evidence would resolve it:** Comparative experiments evaluating the reliability and accuracy of single-pass versus k-pass formalization on complex natural language requirements.

### Open Question 3
- **Question:** Can autoformalization be used to develop verification systems for LLMs in environments requiring strict logical consistency?
- **Basis:** [explicit] Section 5.4 (RQ3) and Section 5.3.
- **Why unresolved:** The current study is a limited proof-of-concept; algorithms to verify autoregressive outputs against system knowledge structures (e.g., knowledge graphs) do not yet exist.
- **What evidence would resolve it:** A framework where LLM outputs are formally verified against a constrained set of system capabilities (e.g., smart-home instructions) using the autoformalization pipeline.

### Open Question 4
- **Question:** How can the pipeline resolve ambiguity in natural language that leads to logically valid but formally incompatible outputs?
- **Basis:** [inferred] Section 5.1 illustrates "vehicle in motion" being formalized as both a Boolean and an Integer, causing false inequalities.
- **Why unresolved:** LLMs lack sufficient system context, leading to varying type assumptions for ambiguous terms.
- **What evidence would resolve it:** Integration of a formal data dictionary or system ontology into the prompt to enforce consistent typing.

## Limitations
- Manual variable grounding requirement prevents industrial-scale deployment
- Verification mechanism assumes proof failure indicates logical inconsistency rather than prover limitations
- Limited validation to automotive domain with narrow requirement diversity
- No experimental validation of proposed automation approaches (similarity-based grounding, single-pass formalization)

## Confidence

- **High confidence:** The core mechanism of translating NL requirements to Lean propositions using DeepSeek-Prover-v2 is demonstrated and reproducible, given access to the model and Lean toolchain
- **Medium confidence:** The framework can detect logical discrepancies when extra variables are introduced, as shown in the R3/G3 experiment
- **Low confidence:** Claims about scalability and automation (similarity-based grounding, single-pass formalization) remain speculative without experimental validation

## Next Checks

1. **Multi-candidate formalization study:** Generate 10-20 formalization candidates for each requirement using different random seeds (pass@k=20), then measure the variance in variable naming, typing, and structure. Quantify the grounding burden and assess whether majority voting or clustering could automate grounding.

2. **Cross-domain generalization test:** Apply the framework to requirements from a different domain (e.g., aerospace safety rules or medical device logic) with distinct logical structures. Evaluate whether the current prompt templates and autoformalizer training generalize or require domain adaptation.

3. **Prover limitation characterization:** Systematically test propositions where logical inconsistency is artificially introduced at varying complexity levels. Measure proof success/failure rates and execution times to determine the threshold where prover limitations begin to confound logical verification.