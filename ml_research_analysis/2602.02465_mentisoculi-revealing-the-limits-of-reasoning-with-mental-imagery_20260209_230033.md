---
ver: rpa2
title: 'MentisOculi: Revealing the Limits of Reasoning with Mental Imagery'
arxiv_id: '2602.02465'
source_url: https://arxiv.org/abs/2602.02465
tags:
- reasoning
- visual
- image
- level
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MENTISOCULI is a procedural, stratified benchmark designed to evaluate
  whether models can use mental imagery to solve multi-step visual reasoning tasks.
  Tasks are designed to be difficult to textualize but intuitive to solve visually,
  requiring spatial reasoning, geometric constraints, and sequential manipulation
  of visual states.
---

# MentisOculi: Revealing the Limits of Reasoning with Mental Imagery

## Quick Facts
- **arXiv ID**: 2602.02465
- **Source URL**: https://arxiv.org/abs/2602.02465
- **Reference count**: 40
- **Primary result**: Models fail to effectively leverage visual aids for reasoning even with oracle ground-truth visualizations

## Executive Summary
MENTISOCULI is a procedural, stratified benchmark designed to evaluate whether models can use mental imagery to solve multi-step visual reasoning tasks. The benchmark covers five tasks (FORMBOARD, HINGEFOLDING, PAPERFOLD, RUSHHOUR, SLIDINGPUZZLE) at five difficulty levels, each procedurally generated with ground-truth solution trajectories. Tasks are designed to be difficult to textualize but intuitive to solve visually, requiring spatial reasoning, geometric constraints, and sequential manipulation of visual states.

Across state-of-the-art models including MLLMs, UMMs, latent reasoning models, and video models, performance degrades significantly with difficulty, falling below chance at the highest levels. Even with oracle ground-truth visualizations, models fail to consistently leverage visual aids for reasoning. Models show no improvement from explicit visual generation compared to text-only reasoning, and established reasoning enhancements (in-context learning, prompt optimization, increased reasoning budget, tool use) fail to provide consistent gains. Humans outperform models and adaptively increase reasoning effort with task complexity, a behavior models do not exhibit.

## Method Summary
The benchmark uses procedural generation to create controlled difficulty levels across five visual reasoning tasks. Each task type has specific spatial and geometric constraints that must be manipulated sequentially. Ground-truth solution trajectories are provided for training and evaluation. Models are tested under two conditions: text-only reasoning and oracle-visual reasoning where ground-truth visualizations are provided. Human performance is evaluated on the same tasks under time constraints without iterative refinement capabilities. The benchmark tests various model architectures including MLLMs, UMMs, latent reasoning models, and video models with different reasoning budgets and enhancement techniques.

## Key Results
- Model performance degrades significantly with difficulty, falling below chance at highest levels
- Models show no improvement from explicit visual generation compared to text-only reasoning
- Humans outperform models and adaptively increase reasoning effort with task complexity

## Why This Works (Mechanism)
The benchmark's procedural generation creates controlled difficulty progression that isolates the visual reasoning component from language understanding. By designing tasks that are difficult to textualize but intuitive visually, the benchmark specifically targets models' ability to bridge visual generation and reasoning. The stratified difficulty levels reveal that models fail at this bridging task even when given perfect visual information through oracle ground-truth visualizations. The sequential nature of tasks requiring geometric constraints and spatial manipulation appears to exceed current models' ability to maintain coherent mental imagery across multiple reasoning steps.

## Foundational Learning
MENTISOCULI reveals that current models lack the foundational ability to effectively integrate visual generation with reasoning processes. Despite having access to perfect visualizations through oracle ground-truth images, models cannot leverage these visual aids to improve reasoning performance. This suggests a fundamental limitation in how models bridge the gap between visual input and reasoning processes, indicating that the core architectural components needed for mental imagery-based reasoning are not yet developed.

## Architecture Onboarding
The benchmark exposes that current MLLMs, UMMs, latent reasoning models, and video models share a common limitation in their inability to effectively use visual information for reasoning. Even when provided with ground-truth visualizations, these architectures fail to bridge generation and reasoning capabilities. This suggests that the architectural components responsible for integrating visual and reasoning processes need fundamental redesign rather than incremental improvements through established enhancement techniques like in-context learning or tool use.

## Open Questions the Paper Calls Out
- Why do models fail to leverage perfect visual information when humans can use imperfect visual information effectively?
- What architectural modifications would enable models to adaptively increase reasoning effort with task complexity like humans do?
- How can we design benchmarks that better capture real-world mental imagery problems beyond procedurally generated tasks?
- What specific aspects of visual-to-reasoning bridging are models missing when they fail with oracle visualizations?

## Limitations
- Procedural generation may create structurally simpler tasks than real-world mental imagery problems
- Performance gap could partly reflect evaluation methodology differences between humans and models
- Focus on tasks difficult to textualize may create selection bias toward visual advantages
- Human evaluation without iterative refinement capabilities may underestimate human performance
- Benchmark may not capture all aspects of human mental imagery and reasoning integration

## Confidence
- **High Confidence**: Models consistently underperform humans across all task types and difficulty levels
- **Medium Confidence**: Models cannot effectively bridge generation and reasoning with mental imagery
- **Low Confidence**: Humans adaptively increase reasoning effort with task complexity

## Next Checks
1. Test whether fine-tuning models on MentisOculi training tasks improves held-out test performance
2. Evaluate whether step-by-step visual feedback improves model performance versus single snapshot oracle visualizations
3. Compare model performance on procedurally generated versus human-designed mental imagery problems of equivalent difficulty