---
ver: rpa2
title: Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction
arxiv_id: '2505.21137'
source_url: https://arxiv.org/abs/2505.21137
tags:
- data
- sgec
- feedback
- whisper
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors explore how to improve end-to-end spoken grammatical
  error correction (SGEC) systems by addressing the challenge of limited labelled
  training data. They propose two main approaches: (1) a pseudo-labelling process
  that expands training data from 77 to approximately 2500 hours by leveraging unlabelled
  audio, and (2) prompting Whisper-based SGEC models with fluent transcriptions to
  provide additional contextual guidance.'
---

# Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction

## Quick Facts
- arXiv ID: 2505.21137
- Source URL: https://arxiv.org/abs/2505.21137
- Reference count: 0
- Authors improve SGEC performance by combining pseudo-labelling, model scaling, and prompting approaches

## Executive Summary
This paper addresses the challenge of limited labelled training data for end-to-end spoken grammatical error correction (SGEC) by proposing two complementary approaches: pseudo-labelling to expand training data from 77 to approximately 2500 hours, and prompting Whisper-based models with fluent transcriptions. The authors demonstrate that fine-tuning Whisper with pseudo-labelled data yields a 4.0% relative WER improvement over cascaded systems on Linguaskill, while prompting further enhances performance. Larger Whisper models (large-v2) achieve the best results, with 11.04% WER on Linguaskill and 13.08% WER on Speak & Improve. The prompting approach also significantly improves feedback generation quality, with F0.5 scores approaching those of cascaded systems.

## Method Summary
The authors fine-tune Whisper models (small.en and large-v2) on grammatical error correction references using the Whispergec method. They expand training data through a pseudo-labelling pipeline: unlabelled audio is processed through a cascaded system (ASR → segmentation → fluent transcription → text-based GEC) to generate pseudo-GEC labels. For prompting, the model is trained with audio input plus fluent transcription prompt, where the prompt is generated by Whisperflt with SpecAugment applied during training to align with inference conditions. The small.en model is trained for 30K steps with batch size 5 and learning rate 1e-6, while large-v2 is trained for 2 epochs with batch size 1 and gradient accumulation of 8.

## Key Results
- Pseudo-labelling expands training data from 77 to ~2500 hours, yielding 4.0% relative WER improvement over cascaded systems
- Large-v2 model achieves 11.04% WER on Linguaskill (vs 13.48% for small.en)
- Prompting provides slight WER improvement (11.10% → 11.04%) with more significant gains in feedback generation (F0.5 improves by ~3 points)
- Feedback generation F0.5 scores approach cascaded system performance when using prompting

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Labelling for Data Scaling
A 4-step pipeline (ASR → segmentation → fluent transcription → text-based GEC) converts 2,500 hours of unlabelled L2 speech into (audio, pseudo-GEC) pairs. The E2E model learns to map disfluent speech directly to grammatically corrected text without modular error propagation. Core assumption: pseudo-labels from a cascaded small.en system contain sufficient signal for the target model; label noise does not overwhelm learning. Evidence: Whisper gec (small.en) fine-tuned on pseudo-labels then LNG lbl achieves 12.72% WER vs. cascaded 13.24% (4.0% relative improvement). Break condition: Pseudo-labels generated by smaller models (small.en) do not transfer well to larger models (large-v2)—WER degrades from 11.10% to 12.93% on LNG lbl when training large-v2 on pseudo-labels alone.

### Mechanism 2: Fluent Transcription Prompting
The model receives (audio, fluent text prompt, GEC reference) triplets. The prompt provides a cleaner linguistic scaffold, reducing the cognitive load of simultaneously detecting disfluencies and correcting grammar. At inference, Whisper flt generates the prompt. Core assumption: Fluent prompts are available or can be reliably generated at inference time; the model learns to use prompts as structural guides rather than crutches. Evidence: Prompting narrows F0.5 gap vs. cascaded from 2.17 to -0.57 on LNG lbl; feedback precision improves from 40.38% to 43.92%. Break condition: Performance gains diminish if prompt quality degrades.

### Mechanism 3: Model Scale Effects
Larger Whisper variants (large-v2) inherently perform better at SGEC and feedback, reducing dependence on pseudo-labelled augmentation. Greater model capacity captures complex speech-text mappings and grammatical patterns directly from limited labelled data (77 hours). Core assumption: The pre-trained Whisper large-v2 has sufficient transferable representations; architectural improvements alone can compensate for data scarcity. Evidence: Whisper gec large-v2 achieves 11.10% WER on LNG lbl vs. 13.48% for small.en—17.7% relative improvement without pseudo-labels.

## Foundational Learning

- **Concept: Whisper encoder-decoder architecture**
  - **Why needed here:** The paper fine-tunes Whisper for a task (GEC) outside its original ASR training distribution. Understanding how encoder processes audio and decoder generates conditioned text is essential for prompt integration.
  - **Quick check question:** Can you explain where and how the fluent transcription prompt is injected into the Whisper architecture during training?

- **Concept: Word Error Rate (WER) vs. TER for GEC evaluation**
  - **Why needed here:** The paper adopts WER as its primary SGEC metric, justifying this choice over TER. Understanding why both metrics correlate but WER is preferred informs interpretation of results.
  - **Quick check question:** Why might WER be suitable for SGEC but problematic for standard ASR tasks with paraphrasing?

- **Concept: Pseudo-labelling and teacher-student quality mismatch**
  - **Why needed here:** The counter-intuitive finding that pseudo-labels help small models but harm large models hinges on understanding label noise and capacity interactions.
  - **Quick check question:** If you generated pseudo-labels with a large-v2 cascaded system instead of small.en, would you expect different results for large-v2 E2E training? What evidence from the paper supports your reasoning?

## Architecture Onboarding

- **Component map:**
  Training Pipeline: Audio → [Whisper Encoder] → [Decoder + Fluent Prompt] → GEC Text
  Pseudo-Label Generation: Unlabelled Audio → [Whisper small.en ASR] → Segmentation → [Whisperflt] → Fluent Text → [Text GEC (BART)] → Pseudo-GEC Labels

- **Critical path:** Pseudo-label quality determines small.en E2E success; prompt availability determines inference feasibility; model scale determines baseline performance floor.

- **Design tradeoffs:**
  - Small.en + pseudo-labels: Better data efficiency, requires pipeline maintenance
  - Large-v2: Simpler (no pseudo-labels needed), but higher compute cost
  - Prompting: Adds inference dependency on Whisperflt, improves feedback F0.5 by ~3 points

- **Failure signatures:**
  - WER increases when large-v2 trained on small.en-generated pseudo-labels (11.10% → 12.93%)
  - F0.5 feedback scores lag cascaded systems at lower proficiency levels (A2, B1) even with prompting
  - S&I cross-corpus performance degrades relative to in-domain LNG lbl (Section 4.1: 12.72% → 16.84% WER)

- **First 3 experiments:**
  1. **Replicate small.en Whisper gec baseline** on LNG lbl train/test (77 hours). Target: ~13.48% WER. If significantly worse, check fine-tuning hyperparameters (30K steps, batch 5, lr 1e-6).
  2. **Add fluent prompting** to small.en using Whisperflt outputs. Apply SpecAugment during prompt generation. Target: ~13.21% WER on LNG lbl.
  3. **Test large-v2 without pseudo-labels** to confirm scale gains. Target: ~11.10% WER. If pseudo-labels are added and WER increases, verify teacher model size matches hypothesis.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the results suggest several important areas for future work, particularly regarding the limitations of pseudo-labelling for larger models and the need for proficiency-specific improvements.

## Limitations
- Pseudo-labelling effectiveness is limited by teacher model quality, with large-v2 models showing no improvement when trained on small.en-generated pseudo-labels
- Cross-corpus generalization remains challenging, with performance degrading from 12.72% to 16.84% WER when moving from Linguaskill to Speak & Improve
- Feedback generation still lags cascaded systems at lower proficiency levels (A2/B1) despite prompting improvements

## Confidence
- **High confidence**: Large-v2 model scale improvements (11.10% vs 13.48% WER on LNG lbl) - directly measured, consistent with pre-training benefits
- **Medium confidence**: Pseudo-labelling effectiveness for small models (4.0% relative improvement) - result depends on teacher quality and pseudo-label noise assumptions
- **Medium confidence**: Prompting mechanism and feedback improvements - demonstrated on available data but dependent on Whisperflt availability at inference
- **Low confidence**: Generalization to other L2 proficiency levels and domains - cross-corpus results show significant degradation

## Next Checks
1. **Teacher model capacity experiment**: Generate pseudo-labels using large-v2 cascaded system instead of small.en, then train both small.en and large-v2 E2E models. This directly tests whether the capacity mismatch hypothesis explains the observed degradation pattern (11.10% → 12.93% WER when large-v2 trained on small.en labels).

2. **Prompt quality ablation study**: Evaluate SGEC performance with degraded prompt quality by applying controlled noise or filtering to Whisperflt outputs. This validates whether the prompting mechanism genuinely helps the model versus simply providing cleaner training signals.

3. **Proficiency-level stratified analysis**: Recompute WER and F0.5 scores separately for A2, B1, and B2 proficiency levels in the test sets. This tests whether the observed feedback gap versus cascaded systems is uniform across proficiency levels or concentrated in lower proficiency groups where the paper notes degraded performance.