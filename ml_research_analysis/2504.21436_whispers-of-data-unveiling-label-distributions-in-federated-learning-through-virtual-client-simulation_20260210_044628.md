---
ver: rpa2
title: 'Whispers of Data: Unveiling Label Distributions in Federated Learning Through
  Virtual Client Simulation'
arxiv_id: '2504.21436'
source_url: https://arxiv.org/abs/2504.21436
tags:
- label
- distribution
- data
- client
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses label distribution inference attacks in federated
  learning, where malicious servers attempt to infer private label distributions of
  client datasets. The proposed method estimates the victim client's dataset size
  and constructs virtual clients to simulate various data distribution scenarios.
---

# Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation

## Quick Facts
- **arXiv ID:** 2504.21436
- **Source URL:** https://arxiv.org/abs/2504.21436
- **Reference count:** 0
- **Primary result:** Proposes a label distribution inference attack in federated learning using virtual client simulation and LSTM prediction

## Executive Summary
This study introduces a novel attack framework targeting label distribution privacy in federated learning environments. The proposed method enables malicious servers to infer private label distributions of client datasets by estimating dataset sizes and simulating virtual clients with varying label proportions. Through temporal analysis of virtual client training performance, the attack employs an LSTM-based inference model to predict the victim client's true label distribution with high accuracy across multiple benchmark datasets.

## Method Summary
The attack framework operates by first estimating the victim client's dataset size through label prediction on uploaded model parameters. Virtual clients are then constructed using this estimated size and various label distribution scenarios. During virtual client training, the method tracks temporal generalization performance across labels to capture distribution-sensitive patterns. An LSTM-based inference model analyzes these temporal patterns to predict the victim's true label distribution. The approach is evaluated across multiple datasets (MNIST, Fashion-MNIST, FER2013, AG-News) under both IID and non-IID scenarios, demonstrating effectiveness even under differential privacy defenses.

## Key Results
- Achieves low Wasserstein distances (0.073-0.502) and KL divergences (0.0015-0.0033) across datasets
- Maintains accuracy under differential privacy with L1 distance increasing from 0.1483 to 0.2455 as privacy budget decreases from 5 to 1
- Outperforms state-of-the-art methods with L1 distances of 0.0438-0.0626 across scenarios

## Why This Works (Mechanism)
The attack exploits the relationship between model performance and underlying data distribution during training. By creating virtual clients with controlled label distributions and monitoring their training dynamics, the method captures temporal patterns that are sensitive to the true data distribution. The LSTM model learns to recognize these distribution-specific patterns, enabling accurate inference of the victim's label distribution. The approach is particularly effective because it leverages the inherent connection between model generalization behavior and the statistical properties of training data.

## Foundational Learning
- **Federated Learning:** Distributed machine learning where clients train models locally and share parameters with a central server
  - Why needed: Understanding the attack surface and communication patterns
  - Quick check: Clients share gradients/parameters, not raw data
- **Label Distribution Inference:** Attack targeting the privacy of class distribution information
  - Why needed: Core attack objective and threat model
  - Quick check: Server attempts to infer class proportions from client updates
- **Virtual Client Simulation:** Creating synthetic training scenarios to probe system behavior
  - Why needed: Enables systematic exploration of distribution space
  - Quick check: Generated clients have controlled label distributions for experimentation
- **Temporal Pattern Analysis:** Using time-series data from training processes for inference
  - Why needed: Captures distribution-sensitive training dynamics
  - Quick check: Performance metrics tracked over training epochs
- **LSTM Networks:** Recurrent neural networks for sequence modeling and prediction
  - Why needed: Models temporal dependencies in training performance
  - Quick check: Can capture long-range dependencies in sequential data
- **Differential Privacy:** Privacy-preserving technique that adds noise to protect individual data
  - Why needed: Understanding defense mechanisms against the attack
  - Quick check: Higher privacy budgets provide stronger protection

## Architecture Onboarding
**Component Map:** Dataset → Virtual Client Generator → Training Monitor → LSTM Inference Model → Label Distribution Prediction
**Critical Path:** Virtual client creation → Performance tracking → LSTM training → Distribution prediction
**Design Tradeoffs:** 
- Accuracy vs. privacy: More accurate attacks are harder to defend against
- Computational cost vs. inference quality: More virtual clients improve accuracy but increase resource requirements
- Temporal resolution vs. storage: Finer-grained monitoring provides better patterns but requires more storage
**Failure Signatures:** 
- Poor virtual client diversity leads to incomplete distribution space coverage
- Insufficient training epochs prevent capture of stable temporal patterns
- Model overfitting to specific virtual client characteristics reduces generalization
**First Experiments:**
1. Baseline test: Run attack on IID MNIST without defenses
2. Defense evaluation: Apply differential privacy with varying budgets (1, 3, 5)
3. Dataset generalization: Test on Fashion-MNIST and FER2013 with non-IID settings

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness decreases under strong differential privacy defenses
- Requires knowledge of dataset size estimation, which may not always be feasible
- Performance depends on the diversity and quality of virtual client simulations

## Confidence
**High:** Core attack methodology, benchmark dataset results, differential privacy evaluation
**Medium:** Transferability to real-world federated learning systems, scalability to larger datasets
**Low:** Impact on non-image data types, effectiveness against adaptive defense mechanisms

## Next Checks
1. Validate the attack on real federated learning implementations (e.g., PySyft, FedML)
2. Test the method's effectiveness on non-image datasets (text, tabular data)
3. Evaluate the attack's performance under adaptive defenses that specifically target temporal pattern analysis