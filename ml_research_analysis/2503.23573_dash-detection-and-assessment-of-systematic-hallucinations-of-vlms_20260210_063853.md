---
ver: rpa2
title: 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs'
arxiv_id: '2503.23573'
source_url: https://arxiv.org/abs/2503.23573
tags:
- object
- cluster
- images
- size
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DASH is a large-scale, automatic pipeline for detecting systematic
  hallucinations in vision-language models. It identifies clusters of semantically
  similar images that cause false-positive object detections, even when the object
  is absent.
---

# DASH: Detection and Assessment of Systematic Hallucinations of VLMs

## Quick Facts
- arXiv ID: 2503.23573
- Source URL: https://arxiv.org/abs/2503.23573
- Authors: Maximilian Augustin; Yannic Neuhaus; Matthias Hein
- Reference count: 40
- Primary result: Automatic pipeline discovers over 19,000 clusters of systematic hallucinations across 380 object classes in VLMs

## Executive Summary
DASH is a large-scale pipeline that automatically detects systematic hallucinations in vision-language models by identifying clusters of semantically similar images that cause false-positive object detections. The method combines diffusion-guided adversarial exploration with large-scale image retrieval to discover hallucination patterns that small benchmarks miss. DASH successfully identifies over 19,000 clusters across 380 object categories and demonstrates that hallucinations transfer between VLMs with varying architectural similarities. The pipeline introduces DASH-B, a new benchmark specifically designed to rigorously evaluate VLM hallucination capabilities.

## Method Summary
DASH uses a two-pronged approach: DASH-LLM generates text prompts containing spurious features, while DASH-OPT optimizes diffusion latents to create images that maximize VLM "yes" responses while minimizing object detector confidence. The pipeline retrieves semantically similar real images from ReLAION-5B using CLIP kNN, filters them through OWLv2 object detection (threshold 0.1) and VLM verification, then clusters successful hallucinations using DreamSim embeddings with agglomerative clustering. The process discovers systematic hallucination patterns across 380 object categories and transfers to seven other VLMs, demonstrating architectural vulnerability correlations.

## Key Results
- Discovered over 19,000 hallucination clusters containing 950,000 images across 380 object categories
- Transferred hallucinations to seven other VLMs including QwenV2-72B, with transfer rates varying by shared architectural components
- Introduced DASH-B benchmark specifically designed for rigorous hallucination evaluation
- Fine-tuning PaliGemma on DASH data improved hallucination mitigation while maintaining overall performance

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-Guided Adversarial Exploration
Optimizing diffusion latents produces synthetic images that expose systematic VLM vulnerabilities while remaining on the natural image manifold. The optimization simultaneously maximizes the VLM's probability of answering "Yes" to "Can you see OBJ?" while minimizing an object detector's confidence that OBJ is present. This dual objective finds images in a failure region: plausible enough to fool the VLM but not containing the actual object.

### Mechanism 2: kNN Retrieval Over Large-Scale Image Corpora
Retrieving semantically similar real images from web-scale datasets reveals systematic hallucination patterns that small benchmarks miss. Synthetic query images and text prompts seed CLIP-based kNN retrieval over ReLAION-5B. Successful hallucinations are then exploited by retrieving neighbors of neighbors, clustering images that consistently trigger the same error.

### Mechanism 3: Architecture-Dependent Vulnerability Transfer
Hallucinations discovered for one VLM partially transfer to others based on shared architectural components. Transfer experiments show that the LLM backbone and vision encoder significantly influence vulnerability rates, while model scale has minor effects. Shared components create correlated failure modes.

## Foundational Learning

- **Concept: Latent Diffusion Models and Optimization**
  - Why needed here: DASH-OPT optimizes conditioning variables in SDXL's latent space to generate hallucination-triggering images
  - Quick check question: Can you explain why optimizing latents (rather than pixels) produces more realistic adversarial images?

- **Concept: Vision-Language Model Hallucinations (Type II)**
  - Why needed here: The paper focuses on false-positive object hallucinations where VLMs affirm object presence when absent
  - Quick check question: What is the difference between Type I (free-form caption) and Type II (factual question) hallucinations, and which does DASH target?

- **Concept: Open-World Object Detection**
  - Why needed here: OWLv2 serves as the automatic verifier that objects are genuinely absent
  - Quick check question: Why does the pipeline use a very low detection threshold (0.1), and what tradeoff does this introduce?

## Architecture Onboarding

- **Component map:** Query Generation (DASH-LLM, DASH-OPT) -> Exploration (CLIP kNN retrieval, OWLv2 + VLM filtering) -> Exploitation (kNN retrieval, DreamSim deduplication) -> Clustering (Agglomerative with DreamSim) -> Verification (Human annotation, Transfer testing)

- **Critical path:** Query generation → Exploration retrieval → OWLv2 + VLM filtering → Exploitation retrieval → Clustering → Transfer evaluation → DASH-B benchmark construction

- **Design tradeoffs:**
  - Conservative detector threshold (0.1) reduces false negatives but increases false positives
  - Single-step LCM reduces compute but may limit image quality/diversity
  - DreamSim (not raw CLIP) used for final clustering
  - Web-scale retrieval discovers patterns absent in small datasets but introduces noise

- **Failure signatures:**
  - Optimization collapse: Generated images become unrealistic or minimally change
  - Cluster fragmentation: Too many small clusters indicate weak semantic coherence
  - Detector bias: OWLv2 systematically misses certain object types
  - Prompt sensitivity: PaliGemma shows high variance on training-similar prompts

- **First 3 experiments:**
  1. Reproduce DASH-OPT optimization on a single object class with 25-step optimization
  2. Validate cluster coherence by manually inspecting one object's clusters
  3. Test transfer on a held-out VLM and measure transfer rate against Table 2 baselines

## Open Questions the Paper Calls Out

- **Open Question 1:** How can DASH data be optimally integrated into a curriculum learning scheme to mitigate hallucinations without degrading general VQA and captioning performance?
- **Open Question 2:** Can the DASH pipeline be adapted to reliably identify systematic "false negative" hallucinations given current object detector limitations?
- **Open Question 3:** Does the conservative detection threshold used in DASH fail to capture subtler, high-confidence systematic hallucinations in state-of-the-art VLMs?

## Limitations
- Reliance on ReLAION-5B limits discovery to web-scale biases and may miss niche hallucination patterns
- OWLv2 object detection may miss certain object types or generate false negatives, particularly for stylized/comic versions
- The 5.2% false positive rate introduces noise into clusters

## Confidence
- **High confidence:** Pipeline architecture and methodology for generating hallucination-triggering images
- **Medium confidence:** Transfer results across VLMs for models sharing vision encoders
- **Low confidence:** Extent to which discovered hallucinations generalize beyond tested classes and VLMs

## Next Checks
1. Manually inspect 10 random clusters from different object categories to verify semantic coherence
2. Re-run pipeline with detector thresholds of 0.05 and 0.2 to quantify tradeoff between false negatives and positives
3. Test whether DASH-discovered hallucinations transfer to VLMs trained on different data distributions (e.g., medical or satellite imagery models)