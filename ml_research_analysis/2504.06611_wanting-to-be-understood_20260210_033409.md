---
ver: rpa2
title: Wanting to be Understood
arxiv_id: '2504.06611'
source_url: https://arxiv.org/abs/2504.06611
tags:
- other
- reward
- agent
- agents
- crossing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the hypothesis that humans have an intrinsic
  motivation to understand and be understood, even without extrinsic rewards. The
  authors implement this in reinforcement learning agents through artificial curiosity
  (drive to understand) and imitation, influence/impressionability, and sub-reaction
  time anticipation (drives to be understood).
---

# Wanting to be Understood

## Quick Facts
- arXiv ID: 2504.06611
- Source URL: https://arxiv.org/abs/2504.06611
- Authors: Chrisantha Fernando; Dylan Banarse; Simon Osindero
- Reference count: 23
- Key outcome: Intrinsic motivation for mutual understanding drives social interaction preference over inanimate objects in perceptual crossing

## Executive Summary
This paper investigates whether humans' intrinsic motivation to understand and be understood can be implemented in reinforcement learning agents. The authors test artificial curiosity, imitation, influence/impressionability, and sub-reaction time anticipation as intrinsic rewards in a perceptual crossing paradigm. The key finding is that mutual information-based influence and impressionability rewards successfully drive agents to prefer social interaction over inanimate objects, while artificial curiosity alone does not. Furthermore, these intrinsic motivations enable cooperation in asymmetric tasks where only one agent receives extrinsic reward.

## Method Summary
The study uses a perceptual crossing paradigm where two agents interact on a 1D circular line [0,1) with shadows (offset by 0.2) and stationary objects. Each agent has position, action (left/right/no-op), and receives a 4D observation including previous crossing, last action, and last reward. Agents are trained via PPO with LSTM policies. Five intrinsic reward variants are tested: artificial curiosity using predictor confidence, imitation using 10-step history comparison, influence/impressionability using mutual information on crossing transitions, sub-reaction time with 2-step delays, and an asymmetric extrinsic task where one agent receives reward based on the other's behavior.

## Key Results
- Influence and impressionability rewards produce preference for social interaction over inanimate objects
- Artificial curiosity alone does not drive agents to prefer social interaction
- Intrinsic motivation for being understood enables asymmetric cooperation where only one agent receives extrinsic reward

## Why This Works (Mechanism)

### Mechanism 1
Mutual information-based influence and impressionability rewards produce preference for social interaction over inanimate objects. Agents receive intrinsic reward for maximizing recent contribution of mutual information between their actions and subsequent observations of the other agent, plus mutual information between observations of the other and their subsequent actions. A rolling buffer of 20 4-timestep chunks tracks crossing transitions with Dirichlet priors preventing sensitivity to low counts. Agents detect regularities in action-observation contingencies that indicate reciprocal agency, with the other agent's policy responding systematically to one's own actions.

### Mechanism 2
Artificial curiosity (active inference-type reward) alone does NOT produce preference for social interaction. Predictor LSTM outputs confidence p(c); reward is (1-p) when no crossing occurs, and p^n when crossing occurs with high certainty. This rewards seeking surprising-but-predictable situations. Agents find stationary objects or shadows sufficient sources of predictable surprise, converging to repetitive interactions with inanimate objects rather than social partners.

### Mechanism 3
Intrinsic motivation for being understood enables asymmetric cooperation where only one agent receives extrinsic reward. Agent0 receives extrinsic reward based on Agent1's position relative to a private signal bit (+1/-1). Agent1 has no extrinsic reward but receives intrinsic influence/impressionability rewards. Agent0 implicitly "teaches" Agent1 through interaction patterns. Impressionability (wanting to be influenced) is critical when the learner gains no extrinsic reward directly.

## Foundational Learning

- Concept: Mutual Information (MI) between action-observation sequences
  - Why needed here: The core reward mechanism uses MI to quantify reciprocal influence; understanding contingency tables, Dirichlet priors, and rolling window estimation is essential.
  - Quick check question: Given a 2x2 contingency table of (passive crossings, active crossings), can you compute MI with Laplace smoothing?

- Concept: Proximal Policy Optimization (PPO) in multi-agent settings
  - Why needed here: Agents are trained jointly with PPO via Ray RLlib; understanding advantage estimation, clipping, and shared experience collection matters.
  - Quick check question: How does PPO's clipping objective prevent large policy updates that could destabilize multi-agent learning?

- Concept: LSTM-based next-step prediction with confidence
  - Why needed here: The curiosity reward requires a predictor that outputs both predictions and confidence estimates p(c).
  - Quick check question: How would you extract epistemic uncertainty from an LSTM trained via supervised learning on trajectories?

## Architecture Onboarding

- Component map:
Environment (1D circular, 0-1 wrapped)
├── Agent0: Position, Action ∈ {L, R, no-op}
├── Agent1: Position, Action ∈ {L, R, no-op}
├── Agent0's fixed object
├── Agent1's fixed object
├── Agent0's shadow (offset 0.2)
└── Agent1's shadow (offset 0.2)

Each Agent:
├── Policy LSTM (4D observation → action)
├── Predictor LSTM (observations → crossing prediction + confidence)
└── Reward calculator (MI buffer, imitation detector, or curiosity)

- Critical path: (1) Initialize agents with random policies → (2) Collect episodes (100 steps each) → (3) Update predictor LSTM offline from trajectories → (4) Calculate intrinsic rewards using predictor + MI buffer → (5) PPO policy update → (6) Repeat for 2000+ episodes until self-other crossing proportion dominates.

- Design tradeoffs:
  - MI buffer length (20 chunks) vs. adaptation speed: longer buffers stabilize estimates but slow response to policy changes.
  - Dirichlet prior strength vs. sensitivity: stronger priors prevent overfitting to sparse events but may mask genuine contingencies.
  - Joint training vs. fixed-opponent: current setup learns other-specific policies; population training would generalize but complicates analysis.

- Failure signatures:
  - Agents converge to stationary object interaction (curiosity-only reward).
  - Reward plateaus without self-other preference (insufficient MI signal strength).
  - Stereotyped repetitive patterns without turn-taking (imitation-only without impressionability).
  - Asymmetric task fails: Agent1 doesn't follow Agent0's signal (missing impressionability component).

- First 3 experiments:
  1. Reproduce the artificial curiosity baseline—verify that self-other crossing proportion does NOT exceed 0.3, confirming the negative result.
  2. Implement influence+impressionability reward with full MI calculation—target self-other crossing proportion >0.6 after 1500 episodes.
  3. Ablate impressionability in the asymmetric extrinsic task—confirm Agent1 cooperation drops when Agent1 lacks intrinsic reward for being influenced.

## Open Questions the Paper Calls Out

### Open Question 1
Can agents learn agent-general policies for mutual interaction rather than overfitting to a specific partner? The current experiments only train pairs of agents together, so learned policies may be partner-specific rather than generalizable. Training agents in populations with random partner assignment would test whether policies transfer to novel agents or humans.

### Open Question 2
What happens when agents have mismatched intrinsic reward functions or mutual information calculations? All experiments assume symmetric reward functions; real human interactions may involve asymmetric or misaligned motivations. Running experiments with different reward function parameters would measure coordination success and emergent communication protocols.

### Open Question 3
How can agents learn to distinguish self-caused observations from other-caused observations without prior assumptions? The MI reward calculation relies on distinguishing self vs. other causation, but this capability is built into the setup rather than learned. Testing agents in environments where self-other attribution must be learned from scratch would validate this assumption.

### Open Question 4
Can mutual information rewards account for delayed or temporally flexible imitation, rather than requiring tight sequential causation? Human social tolerance for temporal delays in reciprocal interaction isn't captured by the current tightly-coupled MI calculation. Developing alternative reward formulations using variable time windows would compare coordination patterns to human behavior.

## Limitations

- The mutual information calculation assumes agents can already distinguish self-caused from other-caused observations
- Results are based on fixed-opponent training rather than population-based approaches
- The artificial curiosity negative result may be implementation-specific rather than fundamental

## Confidence

**High Confidence**: The influence+impressionability mechanism reliably produces social preference over inanimate objects.
**Medium Confidence**: The asymmetric cooperation result where intrinsic motivation enables one-agent extrinsic learning.
**Low Confidence**: The claim that artificial curiosity alone produces no social preference.

## Next Checks

1. Test alternative curiosity formulations (e.g., random network distillation, count-based exploration) to determine if social preference emerges with different surprise measures.

2. Conduct population-based training where agents interact with multiple partners rather than fixed-opponent to test whether the social preference generalizes beyond specific policy fingerprints.

3. Implement an interpretability analysis of the learned policies—examine whether agents develop explicit crossing detection or rely on heuristic patterns—to validate that the reward mechanism captures genuine reciprocal understanding rather than behavioral mimicry.