---
ver: rpa2
title: Generalist Foundation Models Are Not Clinical Enough for Hospital Operations
arxiv_id: '2511.13703'
source_url: https://arxiv.org/abs/2511.13703
tags:
- finetuning
- notes
- test
- readmission
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lang1, a family of 100M-7B parameter language
  models pretrained on a blend of 80B clinical tokens from NYU Langone Health's EHRs
  and 627B internet tokens, and evaluates them on ReMedE, a benchmark of five real-world
  hospital operational tasks derived from 668,331 EHR notes. Zero-shot performance
  of both general-purpose and specialized models was poor (36.6%-71.7% AUROC) across
  four tasks, with mortality as an exception.
---

# Generalist Foundation Models Are Not Clinical Enough for Hospital Operations

## Quick Facts
- arXiv ID: 2511.13703
- Source URL: https://arxiv.org/abs/2511.13703
- Reference count: 40
- Generalist models require explicit fine-tuning for hospital operational tasks

## Executive Summary
This paper introduces Lang1, a family of language models (100M-7B parameters) pretrained on both clinical EHR data and web text, and evaluates them on ReMedE, a benchmark of five real-world hospital operational tasks derived from 668,331 EHR notes. The key finding is that while general-purpose and specialized models perform poorly on these tasks without fine-tuning (36.6%-71.7% AUROC), Lang1-1B models, when fine-tuned, outperform both larger generalist models (up to 70x larger) and zero-shot models (up to 671x larger), demonstrating the value of domain-specific pretraining and explicit supervised fine-tuning for operational tasks.

## Method Summary
The authors pretrained Lang1 models on a mix of 80B clinical tokens from NYU Langone Health's EHRs and 627B internet tokens, then fine-tuned them on ReMedE, a benchmark of five hospital operational tasks derived from 668,331 EHR notes. They compared zero-shot and fine-tuned performance across different model sizes, evaluating both temporal generalization (2021-2024) and cross-institutional transfer to MIMIC-III.

## Key Results
- Zero-shot performance of both general-purpose and specialized models was poor (36.6%-71.7% AUROC) across four operational tasks, with mortality as an exception
- After fine-tuning, Lang1-1B outperformed both larger fine-tuned generalist models (up to 70x larger) and zero-shot models (up to 671x larger), improving AUROC by 3.64%-6.75% and 1.66%-23.66%, respectively
- Cross-task scaling was observed with joint fine-tuning, and Lang1-1B effectively transferred to out-of-distribution tasks and an external health system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining on clinical EHR data improves downstream fine-tuning efficiency compared to generalist pretraining.
- Mechanism: Pretraining on EHR notes familiarizes the model with clinical language patterns, terminology, and documentation structures before task-specific learning. This reduces the number of fine-tuning tokens needed to achieve target performance.
- Core assumption: Clinical notes contain linguistic patterns that are prerequisites for operational prediction tasks.
- Evidence anchors:
  - [abstract]: "fine-tuning process is made more efficient by in-domain pretraining on EHR"
  - [section]: Figure 4b shows Lang1-1B (314.6B tokens, including clinical data) outperforms Llama-3.2-1B (9T tokens, no clinical data) when both are fine-tuned, especially in low-data regimes.
  - [corpus]: Related work (NYUTron, clinical language models) supports that EHR pretraining improves downstream task performance, though direct efficiency comparisons are limited.
- Break condition: If fine-tuning data is abundant and representative, the relative advantage of domain pretraining diminishes.

### Mechanism 2
- Claim: Clinical prediction capabilities for hospital operations do NOT emerge from pretraining alone—explicit supervised fine-tuning is required.
- Mechanism: Next-token prediction on unlabeled clinical notes does not create a mapping between clinical text patterns and operational outcomes (readmission, denial, LOS). These require learning a classification boundary with labeled supervision.
- Core assumption: Operational predictions depend on relationships not captured by language modeling objectives.
- Evidence anchors:
  - [abstract]: "predictive capabilities for hospital operations require explicit supervised fine-tuning"
  - [section]: Figure 3 shows zero-shot AUROC on all four non-mortality tasks remained at or below random chance (36.6%-71.7%) throughout the entire pretraining trajectory, while reading comprehension improved steadily.
  - [corpus]: Limited direct corpus support; this appears to be a novel empirical finding distinguishing operational tasks from knowledge tasks.
- Break condition: If the pretraining objective explicitly incorporated prediction-relevant signals (e.g., masked outcome prediction), emergence might occur.

### Mechanism 3
- Claim: Instruction fine-tuning enables cross-task transfer, where supervision on one operational task improves performance on related tasks.
- Mechanism: Multi-task instruction fine-tuning creates shared representations that capture underlying clinical patterns (e.g., patient acuity, complexity) relevant across tasks.
- Core assumption: Related operational tasks share latent clinical factors.
- Evidence anchors:
  - [abstract]: "cross-task scaling with joint fine-tuning on multiple tasks leading to improvement on other tasks"
  - [section]: Figure 5a shows fine-tuning on readmission improves mortality, LOS, CCI, and insurance denial; joint fine-tuning (last row) improves all tasks.
  - [corpus]: Related work on instruction tuning shows transfer benefits, though asymmetry (mortality→LOS but not LOS→mortality) suggests task-specific dependencies.
- Break condition: Transfer may fail when tasks have conflicting or unrelated decision boundaries (e.g., insurance denial has different patterns from clinical outcomes).

## Foundational Learning

- **Concept: AUROC (Area Under ROC Curve)**
  - Why needed here: Primary evaluation metric for all five operational tasks; distinguishes ranking quality from raw accuracy.
  - Quick check question: If a model has AUROC=0.50 on insurance denial, what does that mean about its predictions?

- **Concept: Temporal distribution shift**
  - Why needed here: Models are evaluated on 2024 data (test) despite training on earlier data; performance degrades over time, simulating deployment.
  - Quick check question: Why is a temporal test split more realistic for clinical deployment than a random split?

- **Concept: Instruction fine-tuning with multiple-choice format**
  - Why needed here: All tasks are converted to multiple-choice Q&A format (e.g., "Will patient be readmitted? A. no B. yes") to enable cross-task transfer.
  - Quick check question: How does multiple-choice formatting enable zero-shot transfer to unseen tasks?

## Architecture Onboarding

- **Component map:**
  Pretrain corpus: 80B EHR tokens (NYU Notes+) + 627B web tokens (SlimPajama) -> Model family: Lang1 decoder-only (100M, 1B, 7B parameters, Llama-style architecture) -> Tokenizer: Llama-2-7B SentencePiece (32k vocab) -> Finetuning: Multiple-choice instruction format, AdamW optimizer, cosine annealing -> Evaluation: ReMedE benchmark (5 tasks, 668,331 notes, 2024 temporal test)

- **Critical path:**
  1. Extract and preprocess clinical notes from EHR (remove non-ASCII, filter short notes)
  2. Pretrain with next-token prediction on equal mix of clinical and web data
  3. Save checkpoints every 1M tokens; monitor loss and zero-shot comprehension
  4. Convert labeled clinical data to multiple-choice Q&A format
  5. Fine-tune with hyperparameter search (learning rate 1e-6 to 1e-3, early stopping patience=300)
  6. Evaluate AUROC on 2024 temporal test set

- **Design tradeoffs:**
  - Model scale vs. domain data: 1B with clinical pretraining beats 70B generalists on operational tasks
  - Pretrain vs. fine-tune compute: Fine-tuning is more token-efficient for performance gains, but pretraining enables transfer and low-data regimes
  - Equal vs. weighted sampling: Equal clinical/web mix justified by ablation (web text doesn't hurt, adds general knowledge)

- **Failure signatures:**
  - Zero-shot AUROC at or below random (50%) on 4/5 tasks → indicates pretraining alone insufficient
  - Large performance drop on 2024 temporal test vs. 2021 → temporal drift
  - Fine-tuned model worse than zero-shot → overfitting or incorrect label format

- **First 3 experiments:**
  1. Replicate zero-shot baseline: Evaluate GPT-4o or Llama-3.2-1B on ReMedE tasks without fine-tuning to confirm low baseline (expected: 36-72% AUROC on 4/5 tasks).
  2. Pretrain ablation: Train 100M model on EHR-only vs. EHR+web mix, fine-tune on readmission, compare AUROC.
  3. Fine-tuning efficiency curve: Fine-tune Lang1-1B checkpoints at different pretrain stages (1B, 100B, 300B tokens) with varying fine-tune data sizes to replicate Figure 4a trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will the efficiency gains of in-domain pretraining persist when scaling to a more diverse set of health systems and patient populations?
- Basis in paper: [explicit] The authors state that future work includes "expanding ReMedE to additional institutions, more diverse patient populations."
- Why unresolved: The current study primarily validates internally at NYU Langone and externally on MIMIC-III, leaving the generalizability to varied healthcare settings uncertain.
- What evidence would resolve it: Evaluating Lang1 on new benchmarks derived from geographically and demographically distinct hospital systems.

### Open Question 2
- Question: Can the effectiveness of specialized models be extended to operational tasks that require generation or regression rather than classification?
- Basis in paper: [explicit] The authors list "task types beyond classification outcomes" as a necessary expansion for the benchmark.
- Why unresolved: The ReMedE benchmark currently focuses exclusively on classification tasks (e.g., readmission vs. no readmission).
- What evidence would resolve it: Testing Lang1 on generative tasks (e.g., automated discharge instructions) or continuous regression tasks (e.g., exact length of stay).

### Open Question 3
- Question: Can alternative pretraining objectives enable clinical predictive capabilities to emerge without explicit supervised fine-tuning?
- Basis in paper: [inferred] The authors hypothesize that the mapping from notes to outcomes "must be learned through either task-specific fine-tuning or alternative pretraining objectives," as next-token prediction alone was insufficient.
- Why unresolved: The study only confirmed that next-token prediction fails to induce operational skills; it did not test other unsupervised objectives.
- What evidence would resolve it: Comparing Lang1 against models trained using non-generative objectives (e.g., contrastive learning) on zero-shot ReMedE tasks.

## Limitations

- Single health system training data may introduce institution-specific biases in clinical language and documentation practices
- Temporal validation only spans 3 years, limiting assessment of real-world deployment sustainability
- Benchmark task representativeness may not capture the full spectrum of hospital operational prediction needs

## Confidence

- **High confidence**: Claims about the necessity of fine-tuning for operational tasks and the efficiency benefits of EHR pretraining
- **Medium confidence**: Claims about cross-task transfer through instruction fine-tuning
- **Low confidence**: Claims about Lang1-1B outperforming much larger models (70x) in all settings

## Next Checks

1. **External system validation**: Fine-tune Lang1-1B on operational tasks from a completely different health system (e.g., community hospital vs. academic medical center) to quantify cross-institutional generalization limits.

2. **Multi-year temporal drift quantification**: Evaluate the same Lang1-1B checkpoints on operational tasks across 5+ years of historical data to establish precise degradation curves and identify when models require retraining.

3. **Interpretability and decision impact analysis**: Deploy Lang1-1B predictions in a simulated operational workflow to assess whether the model's recommendations lead to improved resource allocation decisions compared to baseline heuristics or current practices.