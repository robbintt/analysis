---
ver: rpa2
title: Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese
  Speech Annotation
arxiv_id: '2506.07646'
source_url: https://arxiv.org/abs/2506.07646
tags:
- labels
- speech
- annotation
- accent
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically annotating
  phonemic and prosodic labels for Japanese speech, which is essential for building
  high-quality text-to-speech (TTS) datasets but difficult due to polyphones and context-dependent
  pitch accents. The proposed method fine-tunes a pre-trained ASR model (Whisper)
  to simultaneously generate phrase-level graphemes and annotation labels from audio-transcript
  pairs.
---

# Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation

## Quick Facts
- **arXiv ID:** 2506.07646
- **Source URL:** https://arxiv.org/abs/2506.07646
- **Reference count:** 0
- **Primary result:** 0.57% CER for phonemic labeling with subjective MOS comparable to manual annotation

## Executive Summary
This paper addresses the challenge of automatically annotating phonemic and prosodic labels for Japanese speech, which is essential for building high-quality text-to-speech (TTS) datasets but difficult due to polyphones and context-dependent pitch accents. The proposed method fine-tunes a pre-trained ASR model (Whisper) to simultaneously generate phrase-level graphemes and annotation labels from audio-transcript pairs. To improve phonemic labeling, a decoding strategy using dictionary prior knowledge is employed to correct Kanji pronunciations based on the predicted graphemes. Objective evaluations show that the method achieves 0.57% CER for phonemic labeling and 87.32% accuracy for accent phrase boundary prediction, outperforming text-only and audio-only baselines. Subjective MOS tests indicate that TTS models trained with the generated labels achieve naturalness scores comparable to those trained with manual annotations. The approach enables effective construction of Japanese TTS datasets without expensive manual labeling.

## Method Summary
The method fine-tunes Whisper-large-v3-turbo to output both phrase-level graphemes and Japanese TTS labels (phonemic/pronunciation and prosodic markers) from audio-transcript pairs. The decoder is prompted with the transcript to resolve polyphone ambiguity through cross-attention between acoustic and semantic embeddings. A multi-task objective generates both text content and annotations to prevent catastrophic forgetting. During inference, a dictionary-enhanced decoding strategy uses MeCab and UniDic to correct phonemic errors by selecting the closest pronunciation candidate to the predicted graphemes via edit distance, with accent type adjustment based on mora count changes.

## Key Results
- Achieved 0.57% Character Error Rate (CER) for phonemic labeling, significantly outperforming audio-only (1.13%) and text-only (1.08%) baselines
- Attained 87.32% accuracy for accent phrase boundary prediction and 94.58% F1 score for pitch sequences
- Subjective MOS scores (4.29) for TTS synthesized with generated labels are comparable to models trained with manual annotations (4.27)
- Dictionary-enhanced decoding reduced CER from 0.63% to 0.57% compared to transcript-conditioned model alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning the decoder on ground truth transcripts (as a prompt) reduces phonemic ambiguity in polyphones better than audio-only conditioning.
- **Mechanism:** The decoder utilizes cross-attention to jointly reason over acoustic embeddings (from the encoder) and semantic embeddings (from the transcript prompt). This forces the model to resolve "one-to-many" mapping problems (like Kanji readings) by leveraging the explicit text context rather than inferring content solely from potentially ambiguous audio.
- **Core assumption:** The model retains its ability to process the transcript prompt as semantic context even while fine-tuning on annotation tasks; assumes the transcript is error-free.
- **Evidence anchors:**
  - [Section 2.2] "The decoder can attend to both the acoustic information from input speech and the semantic information of the transcript when generating labels."
  - [Section 4] Annt-v3 (transcript-conditioned) achieves 0.63% CER vs Annt-v1 (audio-only) at 1.13% CER.
  - [Corpus] Related work (arXiv:2506.04527) supports that grapheme conditioning improves annotation coherence, though this specific paper focuses on "transcript-prompted" fine-tuning.
- **Break condition:** If the input transcript contains errors or significantly deviates from the spoken content, the prompt may misguide the model, forcing an alignment between mismatched text and audio.

### Mechanism 2
- **Claim:** Simultaneously generating phrase-level graphemes and TTS labels stabilizes fine-tuning by preserving semantic knowledge.
- **Mechanism:** By optimizing the model to output both the text (graphemes) and the annotations, the training objective acts as a multi-task regularizer. This prevents the "catastrophic forgetting" of semantic understanding observed when training solely on TTS labels.
- **Core assumption:** The base Whisper model has sufficient pre-trained knowledge to map audio to graphemes, which serves as a scaffold for the new annotation task.
- **Evidence anchors:**
  - [Section 2.2] "...expecting that the model would understand the speech content before annotation. We found that the model has shown some improvements..."
  - [Table 2] Annt-v2 (outputting both) outperforms Annt-v1 (labels only), but Annt-v3 (transcript-conditioned) is required for best results.
  - [Corpus] Evidence is weak in direct neighbors for this specific "outputting graphemes" vs "labels only" ablation within the Whisper context, making this a paper-specific finding.
- **Break condition:** If the model hallucinates graphemes during inference (e.g., predicting phonetically similar but incorrect words), the subsequent TTS labels will be structurally coherent but semantically wrong.

### Mechanism 3
- **Claim:** A decoding strategy using dictionary lookups corrects phonemic errors by constraining predictions to valid pronunciations.
- **Mechanism:** The model generates a candidate TTS label. Simultaneously, the predicted graphemes are segmented (via MeCab) and looked up in a dictionary (UniDic) to generate all valid pronunciation candidates. The system selects the candidate with the shortest edit distance to the model's prediction, correcting the model's "hallucinated" or phonetically confused pronunciations.
- **Core assumption:** The model's grapheme prediction is accurate enough to retrieve the correct set of dictionary candidates; the accent modification logic (Eq 1) holds for corrected mora counts.
- **Evidence anchors:**
  - [Section 2.3] "We select the one with the shortest edit distance to the pronunciation predicted by the annotation model..."
  - [Section 4] Annt-v4 (with decoding) reduces CER from 0.63% to 0.57% compared to Annt-v3.
  - [Corpus] Neighbor papers (e.g., arXiv:2508.17796) explore trie-based decoding for context biasing, validating the general principle of external constraint mechanisms.
- **Break condition:** If the predicted grapheme is a homophone or a different word entirely (incorrect semantic parsing), the dictionary will retrieve valid pronunciations for the *wrong* word, cementing the error.

## Foundational Learning

- **Concept: Encoder-Decoder Attention (Transformer/Whisper)**
  - **Why needed here:** The proposed method relies on injecting a text prompt into the decoder's input. You must understand that the decoder attends to *both* the encoder's audio features (cross-attention) and the previous tokens/prompt (causal self-attention) to see why transcript prompting grounds the output.
  - **Quick check question:** If I feed a transcript prompt to the decoder, does it modify the audio features in the encoder? (Answer: No, it only changes what the decoder attends to/conditions on).

- **Concept: Japanese Mora and Pitch Accent**
  - **Why needed here:** The paper corrects "accent type" based on mora count changes (Eq 1). You need to know that accent type $n$ depends on mora position to understand why changing pronunciation length ($M_{mod} - M_{orig}$) requires adjusting the accent nucleus location.
  - **Quick check question:** If a word's pronunciation changes from 3 moras to 4 moras, and the accent was on the last mora (type 3), does the accent stay on the "last mora" (type 4) or the "3rd mora" (type 3)?

- **Concept: Fine-tuning Loss Masking**
  - **Why needed here:** The authors set labels for prompt tokens to -100. You need to understand that this prevents the model from being penalized (or optimizing) for the content of the prompt itself, focusing the loss solely on the generated graphemes and TTS labels.
  - **Quick check question:** What would happen to the training dynamics if the loss were calculated on the transcript prompt tokens? (Answer: The model would try to learn to copy the prompt from the audio, which is redundant and wastes capacity).

## Architecture Onboarding

- **Component map:** Log-Mel Spectrogram (Audio) + Tokenized Transcript (Prompt) -> Whisper-large-v3-turbo Encoder-Decoder -> Unicode-mapped Graphemes + `|` + TTS Labels -> MeCab/UniDic Dictionary Lookup + Edit Distance Resolver

- **Critical path:** The mapping of annotation symbols (like `#` or `[`) to **unused Unicode characters** (U+2191, etc.). The authors note that using standard tokens causes hallucinations because of Whisper's pre-training. Getting this token remapping right is the single most fragile implementation detail.

- **Design tradeoffs:**
  - *Constraint vs. Freedom:* The dictionary decoding strategy (Annt-v4) improves CER significantly but introduces a slight degradation in pitch labeling F1 score (Table 2: 94.59 -> 94.58) because the forced pronunciation change can disrupt the original accent type calculation (Eq 1 is a heuristic).
  - *Complexity:* Annt-v3 is purely neural; Annt-v4 introduces an external dependency (MeCab/UniDic) which improves accuracy but adds failure points (e.g., if the dictionary doesn't contain the specific proper noun).

- **Failure signatures:**
  - **Hallucination:** If standard ASCII symbols (`[`, `#`) are used instead of the specified Unicode tokens (`U+2191`, etc.), the model produces "severe hallucinations" during inference (Section 3.3).
  - **Homophone Drift:** If the audio is unclear and the transcript prompt is absent (Annt-v1/v2), the model confuses phonetically similar sounds (e.g., "se" vs "she").
  - **Accent Mismatch:** If the dictionary correction changes the mora count of a "middle high" accent word (type 2 to n-1), the heuristic in Eq 1 might place the accent nucleus incorrectly if the speaker's intent was complex.

- **First 3 experiments:**
  1. **Token Remapping Ablation:** Train two models, one using standard symbols and one using the proposed Unicode tokens (U+2191 etc.), on a small subset of data. Verify that the standard model hallucinates while the Unicode model converges.
  2. **Prompt Dependence Test:** Run inference on Annt-v3 with (a) correct transcript, (b) empty transcript, and (c) random transcript. Measure CER degradation to quantify how much the model relies on the prompt vs. audio.
  3. **Dictionary Correction Unit Test:** Feed the Annt-v4 decoding module a set of predicted graphemes and TTS labels with synthetic errors (e.g., changing "Tokyo" pronunciation length). Verify that Eq 1 correctly adjusts the accent type (e.g., Type $n \to n$ vs Type $0 \to 0$).

## Open Questions the Paper Calls Out
- **Question:** How effectively can the model's output be utilized to construct polyphone disambiguation datasets?
  - **Basis:** The Conclusion states the model "holds potential for constructing polyphone disambiguation datasets."
  - **Why unresolved:** The authors identify this as a future application; the current study evaluates TTS label quality but does not test the data's utility for training disambiguation models.
  - **What evidence would resolve it:** Benchmarks of a Grapheme-to-Phoneme (G2P) disambiguation model trained on the generated dataset against one trained on manually corrected data.

- **Question:** Can this framework successfully train text-based accent sandhi estimation models?
  - **Basis:** The Conclusion lists "training text-based accent sandhi estimation models" as future work.
  - **Why unresolved:** The paper validates the labels for TTS acoustic model training (VITS), but not for training separate linguistic front-end modules like accent estimators.
  - **What evidence would resolve it:** Performance metrics of a text-based prosody prediction model trained using the proposed annotations as ground truth.

- **Question:** Does the model systematically correct inconsistencies in human annotations?
  - **Basis:** In Table 3, the proposed method achieves a higher MOS (4.29) than the Oracle baseline (4.27), which the authors speculate is due to "inconsistent annotation style" in manual data.
  - **Why unresolved:** While the authors offer a hypothesis, they do not analyze whether the model improves prosody by regularizing these inconsistencies or if the result is within statistical noise.
  - **What evidence would resolve it:** A qualitative analysis comparing pitch contours of the synthesized speech against the manual Oracle labels to identify systematic corrections made by the model.

## Limitations
- **Input quality dependency:** The method fundamentally relies on accurate audio-transcript pairs; errors in alignment propagate through the system
- **Accent modification heuristic:** The linear mora count adjustment (Equation 1) may not handle complex pitch accent patterns, especially in conversational speech
- **Dictionary coverage:** Proper nouns, neologisms, and regional pronunciations not in UniDic cannot be corrected by the dictionary-enhanced decoding strategy

## Confidence
- **High Confidence:** The mechanism of transcript-prompted conditioning reducing polyphone ambiguity is well-supported by the ablation study (Annt-v1 vs Annt-v3, 1.13% â†’ 0.63% CER) and aligns with established principles of encoder-decoder attention in transformers
- **Medium Confidence:** The multi-task regularization hypothesis (simultaneous grapheme and label generation) has weaker support, as the Annt-v2 results (1.08% CER) don't clearly demonstrate superiority over Annt-v3 (0.63% CER)
- **Low Confidence:** The dictionary-enhanced decoding's impact on downstream TTS quality is inferred rather than directly measured; while subjective MOS scores are comparable to manual annotation, the paper doesn't provide detailed analysis of how specific phonemic errors propagate to perceptual quality degradation

## Next Checks
1. **Transcript Dependence Quantification:** Systematically vary transcript quality (from perfect alignment to random text) during inference on Annt-v3 to measure the exact contribution of transcript prompting vs audio features to CER reduction

2. **Accent Modification Validation:** Create a test set of words with known pitch accent patterns where pronunciation length changes are induced. Verify whether Equation 1 correctly predicts the new accent type across all accent pattern classes (e.g., word-initial, word-medial, word-final)

3. **Out-of-Domain Generalization:** Evaluate the trained model on conversational speech or accented Japanese from different corpora to determine if the dictionary-enhanced decoding maintains its effectiveness when encountering pronunciation variants not covered by UniDic