---
ver: rpa2
title: Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task
  Mixture-of-Experts
arxiv_id: '2511.17601'
source_url: https://arxiv.org/abs/2511.17601
tags:
- task
- tasks
- scoring
- while
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining many per-task models
  for automated scoring in educational assessment, which is computationally expensive
  and difficult to deploy at scale. To solve this, the authors propose UniMoE-Guided,
  a knowledge-distilled multi-task Mixture-of-Experts (MoE) model that combines a
  shared encoder, a gated MoE block, and task-specific heads.
---

# Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts

## Quick Facts
- arXiv ID: 2511.17601
- Source URL: https://arxiv.org/abs/2511.17601
- Reference count: 3
- Primary result: A knowledge-distilled multi-task Mixture-of-Experts model achieves performance comparable to individually trained models across nine NGSS-aligned science reasoning tasks while being ~6× smaller than maintaining separate models.

## Executive Summary
This paper addresses the scalability challenge in automated scoring by proposing UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) model. The approach combines a shared encoder, gated MoE block, and task-specific heads to balance shared and task-specific processing. Through knowledge distillation from large per-task teacher models, the compact student model achieves comparable performance (average Cohen's κ of 0.656 vs. 0.651) while being significantly more efficient. The model demonstrates strong generalization to new tasks and maintains competitive accuracy across diverse science reasoning assessments.

## Method Summary
UniMoE-Guided employs a three-stage training pipeline: first, it trains individual teacher models for each task; second, it trains a multi-task student model using a shared encoder and gated Mixture-of-Experts layer; third, it applies knowledge distillation to transfer expertise from teachers to the student. The MoE layer contains multiple experts with a gating mechanism that routes inputs based on task and content, enabling the model to learn both shared representations and task-specific nuances. The knowledge distillation phase aligns softened output probabilities between teacher and student models, allowing the compact student to inherit the teachers' expertise while maintaining efficiency.

## Key Results
- Achieves comparable performance to individually trained models (average Cohen's κ of 0.656 vs. 0.651; Macro-F1 of 0.757 vs. 0.744)
- Reduces model size by approximately 6× compared to maintaining separate models
- Outperforms standard multi-task learning on held-out tasks
- Demonstrates strong generalization capability with minimal added parameters

## Why This Works (Mechanism)
The model's effectiveness stems from its hybrid architecture that balances shared and task-specific processing. The Mixture-of-Experts layer allows selective activation of specialized sub-networks based on task requirements, while the shared encoder captures common linguistic and reasoning patterns across tasks. Knowledge distillation enables the compact student to inherit the nuanced scoring patterns learned by larger teacher models without the computational overhead. The gating mechanism ensures that relevant experts are activated for specific task types, while the distillation process preserves the teachers' scoring expertise in a more efficient form.

## Foundational Learning
- Mixture-of-Experts (MoE) architecture: A neural network design where multiple specialized sub-networks (experts) are combined through a gating mechanism. Why needed: Enables selective activation of task-specific processing pathways while maintaining a shared base. Quick check: Verify that gating probabilities vary meaningfully across different task types.
- Knowledge Distillation (KD): A training technique where a compact student model learns from a larger teacher model by matching output distributions. Why needed: Transfers nuanced scoring patterns from complex teachers to efficient students. Quick check: Confirm that teacher and student output distributions align across diverse inputs.
- Multi-task Learning: Training a single model on multiple related tasks simultaneously to capture shared representations. Why needed: Leverages commonalities across scoring tasks to improve generalization. Quick check: Ensure performance on individual tasks doesn't degrade compared to single-task training.

## Architecture Onboarding

Component map: Input -> Shared Encoder -> Gated MoE Layer -> Task-Specific Heads -> Output

Critical path: The data flows from input through the shared encoder, where general linguistic features are extracted. The MoE layer then routes these features through relevant experts based on gating signals, with each expert specializing in different aspects of the scoring task. Task-specific heads process the expert outputs to generate final predictions for each assessment type.

Design tradeoffs: The architecture balances model efficiency against performance by using a shared encoder for common features while maintaining task-specific heads for nuanced scoring. The MoE layer adds conditional computation overhead but enables specialization without full task-specific models. Knowledge distillation adds training complexity but enables transfer from larger, more accurate teachers.

Failure signatures: Performance degradation may occur when tasks have fundamentally different scoring rubrics that share few common features. The gating mechanism might fail to route inputs appropriately if task boundaries are unclear. Knowledge distillation could fail if teacher models are too dissimilar or if the student capacity is insufficient to capture teacher expertise.

First experiments:
1. Evaluate gating activation patterns across different task types to verify appropriate expert routing
2. Test knowledge distillation effectiveness by comparing student performance with and without KD
3. Analyze expert specialization by examining which experts activate for specific rubric criteria

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can advanced continual learning mechanisms eliminate the performance degradation on prior tasks when sequentially integrating new assessments?
- Basis in paper: The authors state the need to "strengthen lifelong learning to further curb residual forgetting during task additions."
- Why unresolved: Current results show a performance drop on old tasks (Cohen's κ: 0.655 → 0.627) after adapting to held-out tasks.
- What evidence would resolve it: Experiments demonstrating statistically insignificant performance loss on Tasks 1–7 after sequential adaptation to Tasks 8–9.

### Open Question 2
- Question: Does expert specialization within the MoE layer align semantically with specific NGSS dimensions (e.g., DCI vs. SEP), and can rubric-aware routing enforce this?
- Basis in paper: The authors propose to "improve interpretability of expert specialization and explore rubric-aware routing."
- Why unresolved: While expert usage varies by task (Figure 3), it is unclear if individual experts capture specific rubric constructs or just general linguistic features.
- What evidence would resolve it: Qualitative and quantitative analysis linking high activation of specific experts to the presence of specific rubric criteria in student responses.

### Open Question 3
- Question: Does incorporating intermediate-layer feature matching improve knowledge transfer compared to the current soft-label distillation approach?
- Basis in paper: The authors list an aim to "enhance the KD strategy to better distill knowledge from teachers."
- Why unresolved: The current method relies solely on aligning softened output probabilities, potentially failing to transfer structural reasoning patterns from the teacher.
- What evidence would resolve it: Ablation studies comparing the current logit-based distillation against hint-based or attention-based distillation methods on the same benchmark.

## Limitations
- Evaluation is limited to nine NGSS-aligned science reasoning tasks, restricting generalizability to other subjects
- Knowledge distillation's contribution appears secondary to the MoE architecture based on ablation results
- No benchmarking against other large-scale multi-task or MoE approaches in the literature
- Does not address potential biases in automated scoring or robustness to adversarial inputs

## Confidence
- Core claims about comparable performance and computational efficiency: High
- Claims about knowledge distillation's contribution: Medium
- Generalizability claims to other domains: Low

## Next Checks
1. Test UniMoE-Guided on non-science domains (e.g., mathematics, language arts) to assess cross-domain generalization
2. Compare UniMoE-Guided against other state-of-the-art multi-task or MoE models on the same tasks to contextualize efficiency gains
3. Evaluate model robustness by testing performance on adversarial inputs or out-of-distribution student responses