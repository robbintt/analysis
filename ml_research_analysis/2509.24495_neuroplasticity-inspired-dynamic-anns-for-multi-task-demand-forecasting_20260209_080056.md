---
ver: rpa2
title: Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting
arxiv_id: '2509.24495'
source_url: https://arxiv.org/abs/2509.24495
tags:
- task
- learning
- multi-task
- training
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Neuroplastic Multi-Task Network (NMT-Net)
  for multi-task demand forecasting, inspired by neuroplasticity in biological systems.
  NMT-Net dynamically adapts its computational graph during training, identifying
  task similarity, training temporary heads, and selectively integrating them based
  on performance.
---

# Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting

## Quick Facts
- arXiv ID: 2509.24495
- Source URL: https://arxiv.org/abs/2509.24495
- Reference count: 40
- Achieved RMSE of 16.10 with standard deviation of 0.21 on DF dataset, outperforming baselines

## Executive Summary
This paper introduces the Neuroplastic Multi-Task Network (NMT-Net), a dynamic neural architecture that adapts its computational graph during training for multi-task demand forecasting. Inspired by biological neuroplasticity, the model identifies task similarity, trains temporary heads, and selectively integrates them based on performance. Evaluated on three Kaggle demand forecasting datasets, NMT-Net demonstrated superior performance with lower RMSE and standard deviation compared to traditional and state-of-the-art multi-task learning methods.

## Method Summary
NMT-Net employs a dynamic computational graph that adapts during training rather than inference. The architecture uses a shared backbone with Cat2Vec embeddings for categorical features, followed by MLP blocks and a dynamic dictionary of regression heads. The method pre-trains on an initial task set, then processes new tasks by computing similarity with existing tasks via RMSE on average feature vectors. For each new task, it trains two temporary heads (one from scratch, one from the most similar task), evaluates them on a validation set, and retains the better performer. The system updates its head dictionary accordingly and continues training.

## Key Results
- Achieved RMSE of 16.10 with standard deviation of 0.21 on DF dataset
- Outperformed traditional baselines (ARIMA, Decision Trees, Random Forests) and state-of-the-art MTL methods (MTL, MTWR, MTL-cluster, TAG)
- Demonstrated consistent performance with significantly lower standard deviation across datasets
- Successfully managed 93 tasks in PDF dataset with fewer than 93 heads, indicating effective task grouping

## Why This Works (Mechanism)

### Mechanism 1
Routing new tasks to existing model heads based on feature similarity reduces error variance and improves stability. The model computes average feature vectors of new tasks and compares them against previously learned tasks using RMSE to identify the most similar task for potential knowledge transfer.

### Mechanism 2
Training temporary "candidate" heads allows empirical validation of knowledge transfer benefits before permanent architectural changes. Upon receiving a new task, the system trains two temporary heads - one initialized from similar task weights and one from general pre-trained weights - then retains only the superior performer based on validation set evaluation.

### Mechanism 3
Modifying the computational graph during training enables dynamic capacity scaling as task space expands. Unlike static MTL, NMT-Net's architecture grows conditionally, adding new heads only when tasks are sufficiently unique or merging hurts performance, while maintaining shared representation learning.

## Foundational Learning

### Concept: Negative Transfer
Why needed: NMT-Net must determine when to share weights, as merging conflicting tasks degrades performance. The temporary head competition explicitly detects and prevents negative transfer.
Quick check: What happens if the model forces a task with a seasonal pattern onto a head trained for a linear trend?

### Concept: Catastrophic Forgetting
Why needed: When updating shared heads with new data, the model risks losing accuracy on original tasks. The paper implies mitigation via joint training sets, though explicit regularization details are sparse.
Quick check: Why is the training set updated to X_sim ∪ X_new rather than just training on X_new?

### Concept: Dynamic vs. Static Graphs
Why needed: NMT-Net alters topology during training, distinguishing itself from standard Dynamic ANNs that skip layers during inference. This enables adaptive architecture growth.
Quick check: Does the network structure change when processing a single data point (inference), or when a new task class is introduced (training)?

## Architecture Onboarding

### Component map
Backbone: Cat2Vec embeddings (vendor/product IDs) → Linear blocks → BatchNorm/Dropout
Head Pool (Dynamic): Dictionary mapping Task Groups → Linear Regression Heads
Controller: Logic implementing Algorithm 1 (Similarity check → Train Θ_0 & Θ_sim → Evaluate → Select)

### Critical path
1. Pre-training: Train single head on X_pre for 100 epochs to establish backbone weights Θ_0
2. Task Loop: Sample X_new for each new task
3. Selection: Train two lightweight candidate heads (one scratch, one fine-tuned)
4. Commit: Swap in winner, discard loser to preserve memory

### Design tradeoffs
- Compute vs. Accuracy: Training two candidates per task doubles compute cost but minimizes risk of bad architecture choices
- Metric Sensitivity: Using RMSE on raw features for similarity is simple but may fail if scale differs from shape

### Failure signatures
- Head Explosion: Too strict similarity threshold creates new head for every task, devolving into independent models
- Stagnation: Weak general head (Θ_0) never wins competition, forcing all tasks into first similar head and causing overloading

### First 3 experiments
1. Sanity Check: Run NMT-Net against standard MLP with single head on DF dataset (Target: RMSE ~16 vs ~18+)
2. Ablation on Similarity: Implement "Random" (RAND) selection variant to confirm intelligent routing is necessary (Target: RAND should show higher Std Dev)
3. Head Count Analysis: Run on PDF dataset (93 tasks) and plot active heads over time, verifying convergence to fewer than 93 heads

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic hyperparameter management during training improve NMT-Net's convergence and accuracy? The authors suggest this could further improve performance, but current implementation uses fixed schedules. Comparative experiments with dynamic learning rates or batch sizes based on task-specific loss plateaus would resolve this.

### Open Question 2
Is NMT-Net framework compatible with sequential architectures such as Transformers or RNNs? The study exclusively used MLPs, though Section 6 identifies potential for integration with other ANN architectures. Application to LSTM or Transformer models on time-series datasets would provide evidence.

### Open Question 3
Does NMT-Net improve sample efficiency and reduce catastrophic forgetting in reinforcement learning tasks? Section 6 suggests application to continual learning in RL setups, but the method was only validated on supervised regression tasks. Evaluation on multi-task RL benchmarks would resolve this.

## Limitations

- Cat2Vec embedding training procedure is mentioned but not detailed, creating uncertainty in reproduction
- Exact computation of "average feature vector" for similarity matching lacks specification
- Random seed values and exact data sampling for the 10% SIDF subset are unspecified

## Confidence

- **High confidence**: Core mechanism of dynamic head creation and selection (Mechanism 2) is well-specified through Algorithm 1
- **Medium confidence**: Similarity-based task routing (Mechanism 1) is conceptually sound but implementation details leave room for interpretation
- **Medium confidence**: Overall architecture and training procedure are reproducible, though dynamic graph adaptation requires careful implementation

## Next Checks

1. Sanity Check (Static vs. Dynamic): Implement standard MLP with single head on DF dataset to verify baseline performance and confirm claimed improvement from dynamic adaptation

2. Ablation Study: Implement "Random" (RAND) selection variant to empirically validate that intelligent task routing based on feature similarity provides measurable benefit over random assignment

3. Head Utilization Analysis: Run on PDF dataset (93 tasks) and track head creation over time, verifying that model converges to fewer than 93 heads (indicating successful task grouping rather than head explosion)