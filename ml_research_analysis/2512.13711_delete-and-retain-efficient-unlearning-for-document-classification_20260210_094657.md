---
ver: rpa2
title: 'Delete and Retain: Efficient Unlearning for Document Classification'
arxiv_id: '2512.13711'
source_url: https://arxiv.org/abs/2512.13711
tags:
- unlearning
- class
- document
- classification
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses efficient class-level unlearning in document
  classification models. The authors propose a two-step method called Hessian Reassignment
  that combines a single influence-style Hessian downweight update with a deterministic
  next-top-1 decision rule for handling deleted-class samples.
---

# Delete and Retain: Efficient Unlearning for Document Classification

## Quick Facts
- arXiv ID: 2512.13711
- Source URL: https://arxiv.org/abs/2512.13711
- Authors: Aadya Goel; Mayuri Sridhar
- Reference count: 30
- Primary result: Achieves retained-class accuracy close to complete retraining while running orders of magnitude faster (53 seconds vs 602 seconds)

## Executive Summary
This paper introduces Hessian Reassignment, a two-step method for efficient class-level unlearning in document classification models. The approach combines a Hessian downweight update with a deterministic next-top-1 decision rule to remove influence from deleted-class samples while maintaining performance on retained classes. The method leverages conjugate gradients to solve the Hessian-vector system, requiring only gradient and Hessian-vector products rather than full Hessian computation.

The authors demonstrate that Hessian Reassignment achieves retained-class accuracy comparable to complete retraining on standard text benchmarks (20 Newsgroups, AG News, DBPedia-14) while operating significantly faster. The method also shows reduced membership inference advantage for removed classes and preserves the confidence distribution of retained classes, as validated by statistical tests.

## Method Summary
Hessian Reassignment addresses class-level unlearning through a two-step process. First, it performs an influence-style Hessian downweight update that reduces the model's sensitivity to deleted-class data by modifying the inverse Hessian approximation. Second, it applies a deterministic next-top-1 decision rule that reassigns any sample that would have been classified into the deleted class to the next most probable retained class. The method uses conjugate gradient optimization to solve the Hessian-vector system efficiently, requiring only gradient and Hessian-vector products rather than full Hessian computation, making it scalable to larger models and datasets.

## Key Results
- Achieved retained-class accuracy close to complete retraining while running orders of magnitude faster (53 seconds vs 602 seconds)
- Consistently lowered membership-inference advantage on removed class with pooled multi-shadow attacks, with AUCc approaching 0.5 (random guessing)
- Maintained effectively unchanged retained-class confidence distribution after unlearning, with Kolmogorov-Smirnov test p-values of 1.0

## Why This Works (Mechanism)
The Hessian Reassignment method works by fundamentally altering how the model responds to deleted-class data through mathematical manipulation of the Hessian matrix. By downweighting the inverse Hessian approximation, the method reduces the model's reliance on parameters that were heavily influenced by deleted-class samples during training. The next-top-1 decision rule provides a deterministic mechanism to handle any residual influence by redirecting predictions away from the deleted class to the most probable alternative. This dual approach ensures that the model maintains its learned patterns for retained classes while effectively "forgetting" the deleted class information.

## Foundational Learning

**Hessian Matrix Computation**
- Why needed: The Hessian captures second-order derivatives that reveal how parameter changes affect the loss function curvature
- Quick check: Verify that Hessian-vector products can be computed efficiently using automatic differentiation without explicitly forming the full Hessian

**Conjugate Gradient Optimization**
- Why needed: Solves large-scale linear systems efficiently without requiring explicit matrix inversion
- Quick check: Confirm convergence of conjugate gradient iterations within acceptable tolerance levels for the specific problem scale

**Influence Functions**
- Why needed: Provides theoretical framework for understanding how training data influences model predictions
- Quick check: Validate that influence estimates correlate with actual prediction changes when removing data points

**Membership Inference Attacks**
- Why needed: Standard metric for quantifying privacy leakage after unlearning
- Quick check: Ensure shadow models are properly trained and attacks achieve reasonable advantage on baseline models

## Architecture Onboarding

**Component Map**
- Training data -> Model parameters -> Hessian computation -> Conjugate gradient solver -> Downweighted parameters -> Decision rule -> Unlearned model

**Critical Path**
The critical computational path involves: gradient computation → Hessian-vector product → conjugate gradient iterations → parameter update. This sequence must be optimized for efficiency since it executes during each unlearning operation.

**Design Tradeoffs**
The method trades some precision in exact parameter recovery for computational efficiency by using approximate inverse Hessian computations via conjugate gradients. This approximation is sufficient for practical unlearning while enabling scalability to larger models.

**Failure Signatures**
Potential failure modes include: conjugate gradient convergence failure on ill-conditioned Hessians, insufficient downweighting leading to residual deleted-class influence, and decision rule conflicts when multiple classes have similar probabilities.

**3 First Experiments**
1. Verify that downweighting a single influential sample reduces its impact on model predictions as measured by influence functions
2. Test that the next-top-1 decision rule correctly redirects samples from the deleted class without creating new classification errors
3. Confirm that combining both steps preserves accuracy on retained classes better than either step alone

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted to class-level unlearning, may not generalize effectively to more complex fine-grained data deletion scenarios
- Evaluation relies heavily on benchmark datasets that may not capture full complexity of real-world document classification tasks
- Method's effectiveness depends on availability of influence-style Hessian computations, which may be challenging for extremely large-scale models

## Confidence

**High Confidence:**
- Computational efficiency gains (53 vs 602 seconds) across multiple benchmarks
- Retained-class accuracy preservation compared to complete retraining

**Medium Confidence:**
- Membership inference advantage reduction using specific attack methodology
- Confidence distribution preservation results limited to Kolmogorov-Smirnov test

## Next Checks

1. **Generalization Testing:** Evaluate on larger-scale document classification datasets (e.g., full Wikipedia corpus or real-world enterprise document collections) to assess scalability beyond benchmark datasets.

2. **Adversarial Robustness Analysis:** Test unlearned model vulnerability to membership inference attacks using alternative attack methodologies and potentially more sophisticated privacy attacks.

3. **Cross-Task Applicability:** Apply Hessian Reassignment to non-text classification tasks (e.g., image classification or structured data) to determine general applicability across different domains and model architectures.