---
ver: rpa2
title: Language Model-Enhanced Message Passing for Heterophilic Graph Learning
arxiv_id: '2505.19762'
source_url: https://arxiv.org/abs/2505.19762
tags:
- node
- graph
- message
- learning
- passing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LEMP4HG, a novel language model (LM)-enhanced
  message passing approach for heterophilic graph learning that generates and fuses
  connection analysis with paired node textual embeddings to produce enriched message
  representations, mitigating contradictory signals in heterophilic regions. An active
  learning strategy guided by the heuristic MVRD (Modulated Variation of Reliable
  Distance) selectively enhances node pairs suffering most from message passing, significantly
  reducing the cost of analysis generation and side effects on homophilic graphs.
---

# Language Model-Enhanced Message Passing for Heterophilic Graph Learning

## Quick Facts
- arXiv ID: 2505.19762
- Source URL: https://arxiv.org/abs/2505.19762
- Authors: Wenjun Wang; Dawei Cheng
- Reference count: 40
- This paper proposes LEMP4HG, a novel language model (LM)-enhanced message passing approach for heterophilic graph learning that generates and fuses connection analysis with paired node textual embeddings to produce enriched message representations, mitigating contradictory signals in heterophilic regions.

## Executive Summary
This paper addresses the challenge of heterophilic graph learning by introducing LEMP4HG, a novel approach that leverages language models to enhance message passing in graph neural networks. The key insight is that standard message passing propagates raw neighbor features, which can introduce contradictory signals in heterophilic regions where neighbors have different labels. LEMP4HG generates connection analysis via an LM to capture semantic relationships between node pairs, then fuses this analysis with node embeddings through a gating mechanism to create enriched message representations.

The method uses an active learning strategy guided by the MVRD heuristic to selectively enhance edges suffering most from message passing, significantly reducing the cost of analysis generation and minimizing side effects on homophilic graphs. Experiments on 16 text-attributed graph datasets demonstrate that LEMP4HG excels on heterophilic graphs while maintaining robust performance on homophilic ones, using a simple GCN backbone with practical budget constraints.

## Method Summary
LEMP4HG enhances message passing for heterophilic graph learning by generating connection analysis via an LM, encoding this analysis, and fusing it with paired node textual embeddings through a gating mechanism. The method uses MVRD (Modulated Variation of Reliable Distance) to identify edges most affected by contradictory message passing, then selectively queries an LM for these edges to generate semantic relationship analysis. This analysis is encoded and combined with node embeddings to produce enriched messages that better capture the true relationships between nodes. The approach is tested on 16 text-attributed graph datasets, demonstrating superior performance on heterophilic graphs while maintaining robustness on homophilic ones through the MVRD-guided selection mechanism.

## Key Results
- LEMP4HG excels on heterophilic graphs while maintaining robust performance on homophilic ones
- The active learning strategy guided by MVRD significantly reduces the cost of analysis generation
- Uses a simple GCN backbone with practical budget constraints
- Tested on 16 text-attributed graph datasets demonstrating superior performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Signal Injection via LM Analysis
Standard message passing fails on heterophilic graphs because propagating raw neighbor features introduces contradictory signals when neighbors are dissimilar. The architecture substitutes raw neighbor features with a "synthesized message" generated by prompting an LM to analyze the textual relationship between paired nodes, then encoding this analysis. This propagates a summary of the *relationship* rather than just node attributes, aligning with the edge's semantic context.

### Mechanism 2: Discriminative Gating for Alignment
Static LM-generated analysis may misalign with dynamic embeddings learned during GNN training. A gating mechanism fuses the static LM analysis embedding with dynamic source/target node embeddings, allowing the model to balance prior semantic knowledge from the LM against structural/feature information learned by the GNN.

### Mechanism 3: Targeted Edge Selection via MVRD
Enhancing all edges is cost-prohibitive and can harm homophilic regions where standard message passing works well. MVRD measures "representation distortion" by calculating the contraction of embeddings between nodes before and after message passing, modulated by their distance to cluster centers. It prioritizes edges where message passing causes the most distortion (likely heterophilic edges) for LM enhancement.

## Foundational Learning

- **Homophily vs. Heterophily**: The premise relies on the idea that "birds of a feather flock together" (homophily) drives standard GNN success, while its opposite (heterophily) breaks it. *Quick check: Can you explain why aggregating neighbor features hurts performance if neighbors have different labels?*

- **Message Passing Neural Networks (MPNNs)**: You must understand the baseline being modified—specifically how nodes aggregate information from $N(v)$—to understand what is being replaced by the "synthesized message." *Quick check: In a standard GCN, what representation is passed from node $j$ to node $i$?*

- **Active Learning (Budgeted)**: The paper uses a heuristic to select which edges to query because querying the LM for *every* edge is too expensive. *Quick check: Why is a "budget" necessary when integrating Large Language Models with Graph Neural Networks?*

## Architecture Onboarding

- **Component map**: SLM (DeBERTa) -> MVRD Heuristic -> LM (Qwen) -> Gating Network -> GCN Backbone
- **Critical path**: 1) Finetune SLM on node classification, 2) Calculate MVRD for all edges using weight-free GCN, 3) Query LM for top-$k$ malignant edges, 4) Encode analysis via SLM, 5) Train GNN using gated fusion of node embeddings and encoded analysis
- **Design tradeoffs**: Accuracy vs. Cost (increasing budget improves heterophilic performance but costs money/time), Generality vs. Specificity (prompt design is crucial), Backbone Complexity (simple 2-layer GCN used)
- **Failure signatures**: Performance degradation on homophilic graphs, OOM on large graphs, high latency from sequential API calls
- **First 3 experiments**: 1) Sanity Check on Cornell/Texas with full budget, 2) Ablation on MVRD vs. Random vs. FeatDiff on Cora, 3) Scalability test on larger homophilic datasets (Pubmed/Photo) with constrained budget

## Open Questions the Paper Calls Out

1. How can prompt designs be optimized to generate connection analysis that maximally enhances message passing, and what specific types of semantic information contribute most to performance?

2. How does the optimal LM query budget scale with intrinsic graph properties such as homophily level, degree distribution, and textual sparsity?

3. Can the LEMP4HG mechanism generalize effectively to large-scale homophilic graphs where message passing is already benign, without introducing noise that degrades performance?

## Limitations

- Performance degradation on complex homophilic graphs where message passing is already effective
- Dependence on accurate semi-supervised clustering for MVRD heuristic
- Potential high latency and cost from LM API queries
- Uncertainty about optimal prompt design for different graph structures

## Confidence

- Core mechanism (LM-enhanced message passing): Medium
- MVRD heuristic's effectiveness: Low to Medium
- Overall approach on homophilic graphs: High for simple datasets, may degrade on complex structures

## Next Checks

1. **MVRD Ablation on Label Scarcity**: Test MVRD performance on heterophilic datasets with artificially reduced label availability (e.g., 10% vs 50% training labels) to assess sensitivity to semi-supervised clustering quality.

2. **Gating Mechanism Analysis**: Extract and analyze the gating weights (α values) on a held-out validation set to verify the model is actually using the LM-enhanced messages rather than ignoring them.

3. **Homophilic Robustness Test**: Systematically test the model on increasingly complex homophilic datasets (starting with Cora, then Citeseer, then larger datasets) with varying budgets to identify the threshold where performance degradation begins.