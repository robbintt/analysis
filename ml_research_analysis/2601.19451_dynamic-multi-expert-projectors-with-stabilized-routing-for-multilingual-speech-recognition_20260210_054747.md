---
ver: rpa2
title: Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech
  Recognition
arxiv_id: '2601.19451'
source_url: https://arxiv.org/abs/2601.19451
tags:
- expert
- projector
- multilingual
- speech
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multilingual ASR in LLM-based
  systems, where a single projector struggles to capture diverse acoustic-to-semantic
  mappings across typologically distinct languages. To overcome this, the authors
  propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense
  gradient flow to all experts, preventing expert collapse and enabling cross-lingual
  sharing.
---

# Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition

## Quick Facts
- **arXiv ID:** 2601.19451
- **Source URL:** https://arxiv.org/abs/2601.19451
- **Reference count:** 0
- **Primary result:** Up to 7.6% relative WER reduction over single-projector baseline on four mid-resource Indic languages

## Executive Summary
This paper addresses the challenge of multilingual ASR in LLM-based systems, where a single projector struggles to capture diverse acoustic-to-semantic mappings across typologically distinct languages. To overcome this, the authors propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse and enabling cross-lingual sharing. Evaluated on four mid-resource Indic languages (Hindi, Marathi, Tamil, Telugu), SMEAR-MoE achieves up to a 7.6% relative WER reduction over the single-projector baseline while maintaining comparable runtime efficiency. Routing analysis further reveals linguistically meaningful expert specialization, with related languages sharing experts, making it a promising approach for scalable, robust multilingual ASR.

## Method Summary
The paper proposes SMEAR-MoE (Stabilized Multi-Expert Acoustic Routing for Multilingual Speech Recognition), a Mixture-of-Experts projector architecture for LLM-based multilingual ASR. The system uses a frozen Whisper large-v3 encoder and frozen Gemma-2-9B LLM, with a trainable SMEAR-MoE projector that includes a shared Conv1D downsampler and 4 MLP experts. The key innovation is a stabilized routing mechanism that ensures dense gradient flow to all experts through soft gating with virtual expert merging, preventing the expert collapse common in standard MoE approaches. The model is trained with a load-balancing loss to maintain even expert utilization across languages, evaluated on IndicVoices and IndicSUPERB datasets, and tested on the VISTAR benchmark across four mid-resource Indic languages.

## Key Results
- SMEAR-MoE achieves up to 7.6% relative WER reduction compared to single-projector baseline
- Maintains comparable runtime efficiency to monolithic projector approaches
- Routing analysis shows linguistically meaningful expert specialization (related languages share experts)
- Prevents expert collapse through stabilized routing with dense gradient flow

## Why This Works (Mechanism)
The stabilized routing mechanism in SMEAR-MoE addresses the fundamental problem of expert collapse in multilingual MoE projectors. By using soft gating with virtual expert merging (averaging gating weights over utterances), the model ensures that all experts receive non-zero gradients during training, preventing the dominance of a few experts. This allows the model to capture diverse acoustic-to-semantic mappings across typologically distinct languages while maintaining balanced expert utilization. The load-balancing loss further reinforces even distribution of routing decisions, enabling cross-lingual sharing where related languages can utilize the same experts while still maintaining language-specific specialization where needed.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: A neural network architecture where multiple specialized networks (experts) are combined through a gating mechanism. Why needed: Enables specialization while maintaining efficiency. Quick check: Verify that gating probabilities sum to 1 across experts.
- **Expert Collapse**: A failure mode in MoE where only a few experts receive significant gradient updates while others become dormant. Why needed: Understanding the core problem SMEAR-MoE addresses. Quick check: Monitor per-expert activation frequencies during training.
- **Soft vs Hard Gating**: Soft gating uses continuous probabilities while hard gating selects discrete experts. Why needed: SMEAR-MoE uses soft gating to enable gradient flow. Quick check: Confirm gating outputs are differentiable probabilities.
- **Load-Balancing Loss**: A regularization term that encourages even utilization of experts. Why needed: Prevents dominance of specific experts. Quick check: Verify that the loss weight (0.2) effectively balances expert usage.
- **Virtual Expert Merging**: Technique that creates a synthetic expert by averaging gating weights. Why needed: Enables stable gradient computation in sparse MoE. Quick check: Validate that merged gates maintain meaningful routing patterns.

## Architecture Onboarding
- **Component Map:** Frozen Whisper Encoder -> SMEAR-MoE Projector (Conv1D Downsampler -> 4 MLP Experts with Soft Gating) -> Frozen Gemma-2-9B LLM
- **Critical Path:** Speech features → Conv1D downsampling → Expert routing → MLP computation → LLM input → Text output
- **Design Tradeoffs:** 4 experts provide sufficient specialization while maintaining runtime efficiency; soft gating enables dense gradients but may reduce computational savings compared to hard gating.
- **Failure Signatures:** Expert collapse (few experts dominate), routing instability (fluctuating gate distributions), and performance degradation on specific languages.
- **First Experiments:** 1) Train with standard MoE (without stabilization) to observe expert collapse; 2) Remove load-balancing loss to test its impact on expert utilization; 3) Vary number of experts (2, 4, 8) to study specialization vs efficiency tradeoff.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How does the utterance-level averaging of gating weights in SMEAR-MoE impact performance on intra-utterance code-switching scenarios?
- **Basis in paper:** [inferred] The methodology (Eq. 7) computes a static average of gates over the entire sequence $T$ to form a single "virtual expert" per utterance, implicitly assuming a consistent acoustic profile throughout the speech segment.
- **Why unresolved:** The paper evaluates monolingual test sets and analyzes cross-lingual sharing, but does not assess situations where the language or acoustic domain changes mid-utterance.
- **What evidence would resolve it:** Evaluation on code-switching benchmarks (e.g., Hindi-English mixed speech) to see if the static averaging dilutes the effectiveness of specialized experts.

### Open Question 2
- **Question:** Can the stabilized routing mechanism maintain performance and prevent collapse when scaled to 100+ languages with diverse typologies?
- **Basis in paper:** [inferred] The authors explicitly limit the experimental scope to "four mid-resource Indic languages" while claiming the method is a "scalable" solution.
- **Why unresolved:** It is unclear if the observed 7.6% WER reduction and soft merging stability generalize to high-resource languages or significantly larger language counts where the "virtual expert" might suffer from representational interference.
- **What evidence would resolve it:** Benchmarking SMEAR-MoE on large-scale multilingual datasets (e.g., FLEURS or Whisper's full training set) to verify if expert utilization remains dense and performance improves over monolithic baselines.

### Open Question 3
- **Question:** Is there an optimal ratio between the number of experts and the number of languages, or can a fixed expert set model an expanding language repertoire?
- **Basis in paper:** [inferred] The experimental design uses 4 experts for 4 languages, and the analysis notes that Telugu exhibits "distributed probabilities," suggesting the fixed expert count may be a limiting factor for certain languages.
- **Why unresolved:** The paper does not ablate the number of experts ($M$) or test if adding more experts allows for finer-grained specialization without reducing gradient density.
- **What evidence would resolve it:** Ablation studies varying the number of experts (e.g., 2, 4, 8, 16) while keeping the language set fixed to measure the impact on routing entropy and WER.

## Limitations
- Missing architectural details for Conv1D downsampler (kernel sizes, strides, channels) and MLP experts (hidden dimensions, layers, activations)
- Downsampling ratio between encoder output and LLM embedding dimension unspecified
- Runtime efficiency claims lack explicit measurements or comparison methodology
- Limited evaluation to four mid-resource Indic languages, raising questions about scalability

## Confidence
- **High Confidence**: Core conceptual contribution and primary experimental finding (7.6% WER reduction)
- **Medium Confidence**: Routing analysis showing linguistically meaningful specialization
- **Low Confidence**: Runtime efficiency claims and scalability assertions

## Next Checks
1. **Architectural Sensitivity Analysis**: Reproduce with multiple plausible Conv1D and MLP configurations to establish whether the 7.6% WER improvement is robust to architectural variations in the unstated components.

2. **Load-Balancing Loss Ablation**: Train identical models with and without the load-balancing loss (λ=0.2) to empirically verify that standard MoE suffers expert collapse while SMEAR-MoE maintains balanced expert utilization.

3. **Routing Pattern Validation**: Implement routing analysis on the reproduced model to confirm that related languages (e.g., Hindi-Marathi vs Tamil-Telugu) show preferential sharing of specific experts, validating the claim of linguistically meaningful specialization.