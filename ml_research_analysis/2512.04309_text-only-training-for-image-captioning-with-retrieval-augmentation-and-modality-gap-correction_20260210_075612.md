---
ver: rpa2
title: Text-Only Training for Image Captioning with Retrieval Augmentation and Modality
  Gap Correction
arxiv_id: '2512.04309'
source_url: https://arxiv.org/abs/2512.04309
tags:
- captions
- image
- training
- captioning
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TOMCap, a text-only image captioning method
  that combines retrieval augmentation with modality gap correction. The approach
  uses a pre-trained CLIP model to encode captions and images, applying mean and standard
  deviation corrections to reduce the modality gap between visual and textual embeddings.
---

# Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction

## Quick Facts
- **arXiv ID**: 2512.04309
- **Source URL**: https://arxiv.org/abs/2512.04309
- **Reference count**: 40
- **Primary result**: Text-only image captioning method achieving 72.7 BLEU-1 and 103.4 CIDEr on MSCOCO

## Executive Summary
This paper introduces TOMCap, a novel text-only approach to image captioning that leverages retrieval augmentation and modality gap correction. The method trains only the cross-attention and LoRA parameters while freezing the GPT-2 decoder, using retrieved captions to condition generation via cross-attention layers. By correcting the distribution mismatch between CLIP visual and textual embeddings through mean and standard deviation adjustments, TOMCap achieves state-of-the-art performance among text-only methods while requiring minimal fine-tuning of the language model.

## Method Summary
TOMCap addresses image captioning through a text-only training paradigm that combines retrieval augmentation with cross-modal distribution correction. The approach uses CLIP to encode both captions and images, then applies Gaussian mean and standard deviation corrections to reduce the modality gap between visual and textual embeddings. During training, the model retrieves semantically similar captions from a large datastore and conditions a frozen GPT-2 decoder through cross-attention layers and prompt engineering. Only the cross-attention parameters and LoRA adapters are fine-tuned, preserving the linguistic knowledge of the pre-trained language model while improving visual grounding.

## Key Results
- Achieves BLEU-1 of 72.7, BLEU-4 of 28.4, METEOR of 25.8, and CIDEr of 103.4 on MSCOCO
- Outperforms previous training-free and text-only methods across all metrics
- Demonstrates strong generalization on NoCaps and robustness to noise injection

## Why This Works (Mechanism)
TOMCap's effectiveness stems from addressing two fundamental challenges in image captioning: the modality gap between visual and textual representations, and the need for visual grounding without extensive image-caption paired training. The modality gap correction ensures that CLIP embeddings from images and text occupy similar statistical distributions, enabling more effective cross-modal reasoning. The retrieval augmentation provides semantically relevant context that guides the language model toward appropriate caption generation, while the minimal fine-tuning approach preserves the linguistic capabilities of the pre-trained GPT-2 model.

## Foundational Learning
- **CLIP embedding space**: CLIP provides a shared representation space for images and text, but these embeddings follow different distributions - critical for understanding cross-modal alignment challenges
- **Cross-attention mechanisms**: Allows the decoder to attend to retrieved caption representations, providing semantic context for generation - essential for integrating retrieval into the decoding process
- **LoRA fine-tuning**: Low-Rank Adaptation enables efficient parameter updates while preserving pre-trained knowledge - necessary for maintaining linguistic quality with minimal training
- **Gaussian distribution correction**: Assumes embedding dimensions follow independent Gaussian distributions, allowing mean and variance adjustments - simplifies cross-modal alignment but may not capture complex relationships
- **Retrieval-based conditioning**: Uses semantically similar captions to guide generation, reducing the need for extensive paired training data - provides visual context without direct image supervision
- **Modality gap**: The statistical difference between visual and textual representations that hinders cross-modal understanding - must be addressed for effective image captioning

## Architecture Onboarding

**Component map**: CLIP encoder -> Embedding correction -> Retrieval module -> Cross-attention layers -> LoRA adapters -> GPT-2 decoder

**Critical path**: CLIP encodes image and retrieved captions -> Gaussian correction applied -> Cross-attention layers integrate retrieved context -> LoRA parameters modulate GPT-2 generation

**Design tradeoffs**: Fine-tuning only cross-attention and LoRA parameters preserves linguistic knowledge but may limit task-specific adaptation; retrieval quality heavily depends on initial datastore composition; Gaussian assumption simplifies correction but may not capture complex modality gaps

**Failure signatures**: Poor retrieval quality leads to irrelevant caption guidance; inadequate modality correction causes misalignment in cross-attention; freezing too many parameters limits caption diversity; performance degradation on out-of-domain data

**First experiments**:
1. Verify that CLIP visual and textual embeddings have different mean and variance statistics across multiple datasets
2. Test retrieval quality by measuring caption similarity scores between retrieved and ground-truth captions
3. Evaluate the impact of different Gaussian correction methods (mean-only vs mean+std) on captioning performance

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Assumes independent Gaussian distributions across embedding dimensions, which may not fully capture complex modality gaps
- Relies on pre-trained CLIP model, introducing uncertainty about generalization to domains where CLIP representations are less effective
- Fine-tuning only cross-attention and LoRA parameters may limit learning of task-specific caption generation patterns
- Evaluated only in English, leaving multilingual performance unexplored

## Confidence
- **High confidence**: MSCOCO benchmark results, retrieval augmentation contribution, modality gap correction contribution
- **Medium confidence**: NoCaps generalization, noise injection robustness, datastore quality impact
- **Low confidence**: Cross-modal distribution assumptions, ablation completeness, out-of-distribution domain performance

## Next Checks
1. Evaluate TOMCap on non-English image captioning datasets to test the English-only limitation claim
2. Test the Gaussian assumption by measuring embedding distribution statistics across diverse image types and comparing against multi-modal correction approaches
3. Conduct ablation studies that vary the datastore size and quality to quantify the sensitivity of retrieval augmentation performance to corpus composition