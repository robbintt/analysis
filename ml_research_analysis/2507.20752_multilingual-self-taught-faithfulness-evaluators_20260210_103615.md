---
ver: rpa2
title: Multilingual Self-Taught Faithfulness Evaluators
arxiv_id: '2507.20752'
source_url: https://arxiv.org/abs/2507.20752
tags:
- sentence
- training
- languages
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing accurate faithfulness
  evaluators for large language models (LLMs) that can operate across multiple languages
  without requiring extensive labeled data. The proposed STEMF framework learns exclusively
  from synthetic multilingual summarization data while leveraging cross-lingual transfer
  learning.
---

# Multilingual Self-Taught Faithfulness Evaluators

## Quick Facts
- **arXiv ID:** 2507.20752
- **Source URL:** https://arxiv.org/abs/2507.20752
- **Reference count:** 30
- **Primary result:** English-only training yields best cross-lingual faithfulness evaluation performance

## Executive Summary
This paper introduces STEMF, a self-taught framework for training multilingual faithfulness evaluators for large language models using only synthetic data. The approach generates faithful and corrupted summaries, filters judgments through iterative self-training, and fine-tunes using LoRA on accepted judgments. Through experiments across seven languages, the framework demonstrates that English-only training produces the best cross-lingual performance, achieving 6.9% average improvement in balanced accuracy compared to multilingual training approaches.

## Method Summary
STEMF uses an iterative self-training loop where an LLM generates synthetic training data (faithful summaries and corrupted variants) and then judges its own outputs. The framework employs an indirect corruption strategy where contradictory documents are generated first, then summarized to create corrupted summaries. Judgments are filtered by accepting only those where the prediction matches the pseudo-label. The model is fine-tuned using LoRA (rank=128, alpha=256, lr=5e-5) on the filtered judgment-explanation pairs. Experiments show that training only the central 50% of layers achieves comparable performance to full-model training while reducing memory usage by half.

## Key Results
- English-only training yields 6.9% average improvement in balanced accuracy across all test languages
- The gemma-2-9b-it based evaluator achieves 87.7% sentence-wise balanced accuracy on FRANK benchmark
- Training only central layers reduces training memory by 50% with comparable performance to full-model training
- Including languages where base model proficiency is below 65% MMMLU score hurts performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training faithfulness evaluators on English-only synthetic data yields better cross-lingual performance than language-specific or mixed-language training.
- **Mechanism:** English data provides higher-quality training signal because (1) base models have stronger proficiency in English (as measured by MMMLU), (2) faithfulness features learned in English transfer to other languages, and (3) the model learns language-agnostic representations of faithfulness violations in its central layers.
- **Core assumption:** Faithfulness detection features are largely language-agnostic and can be learned effectively in one high-proficiency language then applied to others.
- **Evidence anchors:**
  - [abstract]: "training on English data yields the best results across all test languages, with an average improvement of 6.9% in balanced accuracy"
  - [section 5.1]: "evaluators trained on English present a 7.8% average increase in balanced accuracy, while the second highest average improvement is 6.2%" and "it is important to train the starting model only on languages that it is fluent in (as measured by the MMMLU performance), regardless of the test language"
  - [corpus]: Limited direct corpus evidence; related multilingual evaluation papers don't specifically test English-first training strategies
- **Break condition:** Fails when the starting model has very low proficiency in the target language (MMMLU < 65%); the paper found negative correlation between including test language in training and performance when base proficiency is low.

### Mechanism 2
- **Claim:** An LLM can improve its faithfulness evaluation capabilities by training on self-generated contrastive examples (faithful vs. corrupted summaries) through iterative self-judgment filtering.
- **Mechanism:** (1) Generate faithful summaries and corrupted variants; (2) Sample judgments from the current evaluator; (3) Accept only judgments where prediction matches pseudo-label; (4) Fine-tune on accepted judgment-explanation pairs. This creates a curriculum where the model learns from examples it can initially judge correctly.
- **Core assumption:** The model's initial faithfulness detection capability is sufficient to generate useful training signal, and synthetic corruptions are representative of real hallucination patterns.
- **Evidence anchors:**
  - [abstract]: "The approach uses a self-taught methodology where an LLM generates both faithful and corrupted summaries, then fine-tunes itself using its own judgments"
  - [section 5.2]: "the indirect corruption strategy is simpler to execute and can be performed by a relatively smaller LLM" while direct corruption "requires a large model with good multilingual capabilities"
  - [corpus]: Wang et al. (2024b) and Tang et al. (2024a) demonstrate self-taught evaluators in English; this work extends to multilingual settings
- **Break condition:** Fails when the auxiliary model cannot reliably generate faithfulness errors (smaller models struggle with direct corruption), or when judgment acceptance rate drops too low during filtering.

### Mechanism 3
- **Claim:** Training only the central 50% of layers (freezing first and last 25%) achieves comparable faithfulness evaluation performance to full-model training while reducing training memory by half.
- **Mechanism:** Following prior work, early layers handle input language processing, central layers perform abstract task reasoning, and later layers handle output generation. Freezing outer layers preserves multilingual capabilities while allowing task specialization.
- **Core assumption:** Language processing and generation capabilities are localized to outer layers, while task-specific reasoning resides in central layers.
- **Evidence anchors:**
  - [section 5.3]: "after five iterations, there is no significant difference between the two strategies" and "training only the central layers leads to a faster convergence of STEMF"
  - [section 3.2]: "Following the intuition from Wendler et al. (2024) and Tang et al. (2024c), we posit that training only central layers of an LLM improves the LLM's capability at solving a particular task, without impacting its multilingual understanding"
  - [corpus]: Tang et al. (2024c) and Wendler et al. (2024) provide evidence for language-specific vs. task-specific layer localization
- **Break condition:** May fail with different architectures that don't exhibit clear layer functional separation, or when the task requires significant language adaptation beyond reasoning.

## Foundational Learning

- **Concept: Cross-lingual Transfer Learning**
  - Why needed here: The core innovation relies on training in one language and evaluating in multiple others. The paper shows English training generalizes better than multilingual training.
  - Quick check question: Why would training on English data outperform training on the target language when evaluating in that target language?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The framework uses LoRA for efficient fine-tuning. Understanding rank, alpha, and memory trade-offs is essential for implementation.
  - Quick check question: What happens to training memory requirements when you train only central layers vs. the full model with LoRA?

- **Concept: Balanced Accuracy**
  - Why needed here: The paper uses balanced accuracy (average of TPR and TNR) as the primary metric, which handles class imbalance better than standard accuracy.
  - Quick check question: If a dataset has 75% faithful and 25% unfaithful examples, why might standard accuracy be misleading?

## Architecture Onboarding

- **Component map:**
  1. **Data Pipeline:** WikiLingua → Document sampling (1K per iteration) → Summary generation via M → Corruption (indirect: contradicting document → summary; direct: error injection) → Sentence-label triplets
  2. **Judgment Pipeline:** Document-sentence pairs → Current evaluator Ji → Judgment (explanation + prediction) → Filter by pseudo-label match → Accepted judgment dataset
  3. **Training:** SFT with LoRA on judgment dataset → New evaluator Ji+1

- **Critical path:**
  1. Sample 1K documents from WikiLingua evenly distributed across training languages
  2. Generate faithful summaries and corrupted variants (indirect strategy recommended)
  3. Sample judgments from current evaluator; accept if prediction matches pseudo-label (max 2 resamples)
  4. Fine-tune with LoRA (rank=128, alpha=256, lr=5e-5, 1 epoch)
  5. Repeat for 3-5 iterations

- **Design tradeoffs:**
  - **Indirect vs. Direct corruption:** Indirect works with smaller models (7B); direct requires 72B for quality errors
  - **English vs. multilingual training:** English-only typically best unless base model has high target language proficiency (MMMLU > 65%)
  - **Full vs. central layers:** Central-only reduces training memory 50% with faster convergence; inference cost unchanged

- **Failure signatures:**
  - Low judgment acceptance rate in Step 2 → base model too weak
  - Performance degrades after iteration 3 → overfitting to synthetic errors
  - Translation baseline outperforms direct evaluation → very low target language capability
  - Including test language in training hurts performance → base model insufficient proficiency

- **First 3 experiments:**
  1. **Baseline measurement:** Evaluate untrained starting model (e.g., Qwen2.5-7B-Instruct) on MEMERAG/mFACE/FRANK across all target languages to establish initial capability.
  2. **English training run:** Execute 3 iterations of STEMF with English-only WikiLingua data, indirect corruption, full model training. Measure balanced accuracy delta on all test languages.
  3. **Central layers ablation:** Repeat experiment 2 training only central 50% layers. Compare iteration-1 and iteration-5 performance to validate faster convergence claim.

## Open Questions the Paper Calls Out

- **Question:** Can the STEMF framework be extended to evaluate additional dimensions of LLM output quality beyond faithfulness, such as informativeness, harmfulness, and fluency?
- **Question:** What mechanism explains why training exclusively on English data yields the best cross-lingual transfer—is it language-agnostic internal representations of faithfulness or the dominance of English in pre-training data?
- **Question:** Why does including NLI proxy data hurt STEMF performance when prior work (Tang et al., 2024a) found NLI data helpful for training binary faithfulness classifiers?

## Limitations

- The framework assumes document-summary pairs with sentence-level annotations, limiting generalization to other task types
- Self-taught approach requires base models of at least 7B parameters for the indirect corruption strategy
- English-only training advantage may not hold for languages with very different structures from English
- Focus on summarization tasks limits applicability to other generation tasks

## Confidence

**High Confidence:** The core finding that English-only training yields better cross-lingual performance than multilingual training, supported by consistent results across three evaluation benchmarks and seven languages. The comparison between full-model and central-layer training showing comparable performance is also well-supported.

**Medium Confidence:** The self-taught methodology's effectiveness, as the paper demonstrates working results but relies on synthetic data that may not capture all real-world hallucination patterns. The claim that training on languages with base model proficiency below 65% MMMLU score hurts performance is supported but based on limited language combinations.

**Low Confidence:** The specific threshold of 65% MMMLU proficiency for determining whether to include a language in training, which appears to be an empirical observation from this study rather than a theoretically grounded cutoff.

## Next Checks

1. **Architecture Generalization Test:** Apply the STEMF framework to a different LLM architecture (e.g., LLaMA or Mistral) to verify that central-layer training efficiency and English-first transfer learning benefits hold across model families.

2. **Task Diversity Validation:** Evaluate the trained faithfulness evaluators on non-summarization tasks such as open-domain question answering or dialogue generation to assess whether the learned faithfulness features transfer beyond the training domain.

3. **Real-World Error Pattern Analysis:** Compare the synthetic corruption patterns generated by the self-taught approach against human-annotated hallucination datasets to quantify coverage gaps and identify error types that the current methodology may miss.