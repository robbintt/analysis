---
ver: rpa2
title: 'SUPERChem: A Multimodal Reasoning Benchmark in Chemistry'
arxiv_id: '2512.01274'
source_url: https://arxiv.org/abs/2512.01274
tags:
- reasoning
- checkpoint
- chemical
- benchmark
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SUPERChem is a new benchmark for evaluating the chemical reasoning
  abilities of Large Language Models. It includes 500 expert-curated problems that
  cover a wide range of chemistry subfields and are presented in both text and multimodal
  formats.
---

# SUPERChem: A Multimodal Reasoning Benchmark in Chemistry

## Quick Facts
- **arXiv ID:** 2512.01274
- **Source URL:** https://arxiv.org/abs/2512.01274
- **Reference count:** 40
- **Key outcome:** New benchmark shows even top models achieve only ~38% accuracy on expert-curated chemistry problems, close to human baseline

## Executive Summary
SUPERChem is a new benchmark for evaluating the chemical reasoning abilities of Large Language Models. It includes 500 expert-curated problems that cover a wide range of chemistry subfields and are presented in both text and multimodal formats. Each problem comes with detailed, expert-authored solution paths, allowing for process-level evaluation through a new Reasoning Path Fidelity (RPF) metric. Evaluations show that even the best-performing models, such as GPT-5 (High) and Gemini 2.5 Pro, only reach about 38% accuracy, which is close to the human baseline of 40.3%. The benchmark also reveals that models vary in their ability to handle visual information and that some achieve high accuracy through different reasoning strategies. SUPERChem helps identify genuine chemical reasoning versus heuristic guessing, providing a challenging and fair test for advancing AI in chemistry.

## Method Summary
SUPERChem provides 500 expert-curated chemistry problems with both multimodal (image) and text-only variants. Models generate reasoning chains and final answers, which are evaluated using two metrics: Answer Accuracy (exact match) and Reasoning Path Fidelity (RPF). RPF is computed by comparing model reasoning against expert-authored solution paths using a judge model (Gemini 2.5 Pro) that evaluates semantic alignment at weighted checkpoints. The benchmark includes a human baseline (174 undergraduates) and uses pass@k analysis to reveal latent knowledge not captured by single-attempt accuracy. The dataset is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.

## Key Results
- Top models achieve only ~38% accuracy, close to human baseline of 40.3%
- Models show varying abilities to handle visual information (Gemini 2.5 Pro benefits by +4.7%, GPT-4o hindered by -3.8%)
- Pass@k analysis reveals models possess vast chemical knowledge not consistently accessible in single attempts
- RPF distinguishes genuine chemical reasoning from heuristic approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Process-level evaluation via Reasoning Path Fidelity (RPF) discriminates between genuine chemical reasoning and shortcut-driven heuristics.
- **Mechanism:** Expert-authored solution paths are decomposed into weighted checkpoints; a judge model (Gemini 2.5 Pro) evaluates semantic alignment between model reasoning chains and expert checkpoints, rewarding logically sound derivations over correct answers obtained through non-canonical shortcuts.
- **Core assumption:** The judge model can reliably detect semantic equivalence in chemical reasoning despite phrasing differences.
- **Evidence anchors:**
  - [abstract]: "distinguishes high-fidelity reasoners from heuristic ones"
  - [section 4.5]: "DeepSeek-V3.1-Think embodies the second archetype: the effective heuristic reasoner. While achieving a top-tier accuracy of 37.3%, its RPF is notably lower at 52.5%... its methods diverge more significantly from the canonical expert-defined path"
  - [corpus]: ChemVTS-Bench (FMR=0.55) addresses visual-textual-symbolic integration but does not implement checkpoint-based process evaluation.
- **Break condition:** If models learn to generate reasoning that superficially matches checkpoint semantics without genuine understanding, RPF scores may inflate without corresponding capability gains.

### Mechanism 2
- **Claim:** Multimodal input effects are model-dependent rather than universally beneficial, revealing architectural differences in cross-modal fusion.
- **Mechanism:** When equivalent information is presented in visual vs. textual form, performance deltas expose whether models genuinely integrate visual representations or rely on text-based reconstruction. Beneficiaries (e.g., Gemini 2.5 Pro: +4.7pp) show effective fusion; hindered models (e.g., GPT-4o: -3.8pp) experience cognitive interference.
- **Core assumption:** Hand-authored text descriptions provide information parity with images for controlled comparison.
- **Evidence anchors:**
  - [abstract]: "reveals model-dependent effects of visual information"
  - [section 4.4]: "GPT-5 (High)... performance remains remarkably stable... its text-based reasoning prowess is so advanced that it can reconstruct the necessary structural and spatial information almost perfectly from symbolic representations"
  - [corpus]: RxnBench (FMR=0.52) evaluates reaction understanding from literature but does not isolate modality effects.
- **Break condition:** If text descriptions inadvertently encode reasoning hints not present in images, the controlled comparison is compromised.

### Mechanism 3
- **Claim:** Pass@k analysis reveals latent knowledge not captured by single-attempt accuracy, indicating retrieval/reasoning reliability gaps rather than knowledge absence.
- **Mechanism:** Multiple independent attempts increase the probability that stochastic reasoning paths converge on correct solutions; steep pass@k curves (e.g., Gemini 2.5 Pro: 40%â†’71% from k=1 to k=8) indicate knowledge exists but is inconsistently accessed.
- **Core assumption:** Failures at k=1 primarily reflect reasoning path selection failures, not fundamental knowledge gaps.
- **Evidence anchors:**
  - [abstract]: "models vary in their ability to handle visual information and that some achieve high accuracy through different reasoning strategies"
  - [section 4.3]: "all top models possess a vast repository of chemical knowledge that is not consistently accessible in a single trial... Gemini 2.5 Pro ascends from about 40% to a remarkable 71.0%"
  - [corpus]: No corpus papers directly address pass@k analysis for chemical reasoning.
- **Break condition:** If high pass@k scores derive from answer distribution memorization rather than reasoning, the latent knowledge interpretation fails.

## Foundational Learning

- **Concept: Chain-of-thought (CoT) evaluation**
  - Why needed here: SUPERChem evaluates reasoning quality beyond final answers; understanding CoT assessment is prerequisite to interpreting RPF scores.
  - Quick check question: Can you explain why a model might achieve high accuracy but low RPF?

- **Concept: Multimodal fusion in MLLMs**
  - Why needed here: The benchmark's modality comparison requires understanding how models integrate visual and textual information streams.
  - Quick check question: What would cause a model to perform worse with multimodal input than text-only input?

- **Concept: Benchmark saturation and data contamination**
  - Why needed here: SUPERChem's design addresses these issues; understanding them motivates the filtering pipeline and entity masking strategies.
  - Quick check question: Why might a model score 80%+ on ChemBench but only 38% on SUPERChem?

## Architecture Onboarding

- **Component map:** Problem bank (500 questions with multimodal/text variants) -> Expert solution paths with checkpoint annotations -> RPF evaluator (Gemini 2.5 Pro as judge model) -> Filtering pipeline (pass@1 saturation filtering, entity masking) -> Human baseline (174 undergraduate students)

- **Critical path:**
  1. Access SUPERChem-100 for rapid evaluation or SUPERChem-Release (500 questions) for comprehensive assessment
  2. Format inputs per modality (images for MLLMs, hand-authored descriptions for LLMs)
  3. Collect model outputs including reasoning chains and final answers
  4. Compute accuracy (exact match) and RPF (checkpoint-weighted semantic alignment)

- **Design tradeoffs:**
  - Multiple-choice format enables automatic grading but may enable elimination strategies; mitigated by 26-option sets and intermediate calculation requirements (Appendix B)
  - Judge-model-based RPF introduces dependency on evaluator capabilities; cross-validation with human raters not reported
  - Text descriptions provide controlled comparison but may not fully capture visual information density

- **Failure signatures:**
  - High accuracy + low RPF: heuristic reasoning, potential shortcut exploitation
  - Text-outperforms-multimodal: weak cross-modal fusion, visual interference
  - Flat pass@k curve: genuine knowledge gaps vs. retrieval issues
  - Failures concentrated at checkpoints 2.4/2.2/1.7 (product prediction, pathway identification, structure-property correlation): strategic reasoning deficits

- **First 3 experiments:**
  1. **Baseline establishment:** Evaluate target model on SUPERChem-100 with both accuracy and RPF metrics to establish reasoning profile quadrant placement.
  2. **Modality ablation:** Run controlled comparison on the 238-question Multimodal-Essential Subset to classify model as beneficiary/stable/hindered by visual input.
  3. **Breakpoint diagnosis:** Analyze first-failed checkpoint distribution to identify whether failures occur at problem formulation (synthesis/mechanism) or execution (calculation) stages, informing targeted improvement directions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models be trained to more reliably access their latent chemical knowledge in single attempts rather than requiring multiple trials?
- Basis in paper: [explicit] The pass@k analysis (Section 4.3) shows models improve 25-30 percentage points from k=1 to k=8 (e.g., Gemini 2.5 Pro from ~40% to 71%), indicating "the primary challenge for future development lies not just in expanding knowledge, but in cultivating more reliable mechanisms to access and apply it."
- Why unresolved: Current models possess vast chemical knowledge but fail to consistently retrieve and apply it in a single attempt due to stochasticity or suboptimal reasoning path selection.
- What evidence would resolve it: Development of training methods or architectures that narrow the gap between pass@1 and pass@k performance without sacrificing accuracy.

### Open Question 2
- Question: How can models be improved to acquire true mechanistic understanding of reaction pathways rather than relying on pattern-matching heuristics?
- Basis in paper: [explicit] Section 4.6.4 states: "Predicting reaction mechanisms requires a grasp of underlying physical principles that is qualitatively different from the pattern-matching strengths honed by language model training. This core scientific ability is precisely what current LLMs are lacking, and where future research must focus."
- Why unresolved: Breakpoint analysis shows Product Structure Prediction (2.4), Reaction Pathway Identification (2.2), and Structure-Property Correlation (1.7) are the top three failure points, indicating fundamental limitations in mechanistic reasoning.
- What evidence would resolve it: Improved performance on reaction mechanism interpretation and selectivity control sub-tasks, coupled with higher RPF scores on synthesis-related reasoning checkpoints.

### Open Question 3
- Question: How can RPF metrics be refined to assess reasoning efficiency and sequence-sensitive attributes beyond weighted checkpoint matching?
- Basis in paper: [explicit] The conclusion states: "We will also refine RPF to assess efficiency and sequence-sensitive attributes."
- Why unresolved: Current RPF evaluates whether checkpoints are matched but does not penalize unnecessarily long reasoning paths or reward efficient solution strategies.
- What evidence would resolve it: A modified RPF metric that correlates with both correctness and reasoning path conciseness, validated against expert judgments of solution elegance.

## Limitations

- RPF Metric Reliability: Depends entirely on Gemini 2.5 Pro judge model without independent human validation
- Text Description Parity: Hand-authored descriptions may not provide true information equivalence with images
- Knowledge vs. Memorization: Pass@k interpretation assumes failures reflect reasoning path selection rather than knowledge gaps

## Confidence

**High Confidence:** Benchmark construction methodology and overall finding that top models achieve only ~38% accuracy (close to human baseline)
**Medium Confidence:** Modality comparison findings and reasoning archetypes, but dependent on information parity assumptions
**Low Confidence:** RPF metric as measure of genuine reasoning versus heuristic shortcuts due to judge model dependency

## Next Checks

1. **Judge Model Validation:** Conduct human expert validation study to compare human-assigned RPF scores against Gemini 2.5 Pro's judgments
2. **Information Content Analysis:** Perform entropy analysis comparing image-based and text-based representations for information equivalence verification
3. **Cross-Benchmark Consistency:** Evaluate same models on ChemBench vs. SUPERChem to identify specific problem types where performance gaps are largest