---
ver: rpa2
title: Optimal Subspace Inference for the Laplace Approximation of Bayesian Neural
  Networks
arxiv_id: '2502.02345'
source_url: https://arxiv.org/abs/2502.02345
tags:
- subspace
- approximation
- matrix
- parameters
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to subspace inference for
  Laplace approximation in Bayesian neural networks (BNNs). The key idea is to focus
  on the predictive covariance matrix of the linearized Laplace approximation and
  derive the optimal subspace model that minimizes the approximation error.
---

# Optimal Subspace Inference for the Laplace Approximation of Bayesian Neural Networks

## Quick Facts
- **arXiv ID:** 2502.02345
- **Source URL:** https://arxiv.org/abs/2502.02345
- **Reference count:** 40
- **Primary result:** Proposes optimal subspace inference for Laplace approximation in BNNs using predictive covariance minimization, requiring <1% parameters for reliable uncertainty estimates.

## Executive Summary
This paper introduces a novel approach to subspace inference for Laplace approximation in Bayesian neural networks by focusing on the predictive covariance matrix. The authors derive the optimal subspace model that minimizes the approximation error between full and subspace epistemic predictive covariances, showing that it can be constructed using dominant eigenvectors of the predictive covariance matrix. Their method outperforms existing subset selection approaches while requiring significantly fewer parameters to achieve reliable uncertainty estimates.

## Method Summary
The method constructs a projection matrix P that minimizes the Frobenius norm difference between the full epistemic predictive covariance Σ_X and its subspace approximation Σ_{P,X}. The optimal projector is theoretically derived using the dominant eigenvectors of J_X Ψ J_X^T, where J_X is the Jacobian and Ψ is the posterior covariance. In practice, since Ψ is intractable, the authors propose using scalable approximations like KFAC or diagonal GGN combined with a subset of training data to construct the projector. The approach requires training a MAP network, computing an approximate posterior, selecting a data subset, extracting dominant eigenvectors, and building the projection matrix for efficient inference.

## Key Results
- Derivation of optimal subspace model using dominant eigenvectors of predictive covariance matrix
- Empirical demonstration that <1% of parameters suffice for reliable uncertainty estimates on UCI regression datasets
- Proposal of trace criterion as computable proxy for subspace quality when full predictive covariance is unknown
- Outperformance over subset selection methods on regression and classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Optimality via Low-Rank Predictive Covariance
The paper formulates subspace search as a low-rank approximation problem using the Eckart-Young-Mirsky theorem. By projecting parameter space onto dominant eigenvectors of the epistemic predictive covariance J_X Ψ J_X^T, the method ensures the subspace captures the most significant uncertainty directions in output space. This relies on local linearization around MAP estimate and sufficient rank of Jacobian.

### Mechanism 2: Practical Approximation via KFAC and Data Subsampling
Since full posterior covariance Ψ is intractable, the method substitutes it with KFAC or diagonal approximations and constructs projection matrix using a subset of training data rather than full set. This captures functional uncertainty without storing full Hessian, making it scalable to large models.

### Mechanism 3: Trace Criterion for Subspace Quality
The trace of subspace predictive covariance Tr(Σ_{P,X}) serves as computable proxy for subspace quality when full predictive covariance is unknown. Under Loewner order Σ_{P,X} ⪯ Σ_X, higher trace indicates better alignment with dominant eigenspace of true predictive uncertainty.

## Foundational Learning

**Concept: Laplace Approximation (LA) & Generalized Gauss-Newton (GGN)**
- Why needed: Entire method relies on Gaussian posterior approximation centered at MAP estimate; GGN used as positive semi-definite substitute for Hessian
- Quick check: Can you explain why inverting full Hessian is infeasible for modern neural networks, and why GGN matrix is preferred alternative for Laplace approximations?

**Concept: Epistemic vs. Aleatoric Uncertainty**
- Why needed: Paper focuses specifically on reducing epistemic predictive covariance (model uncertainty), distinct from aleatoric uncertainty (noise)
- Quick check: In Eq. (10), which term represents epistemic uncertainty, and how does subspace projection P affect it?

**Concept: Linearized Neural Networks**
- Why needed: Method relies on first-order Taylor expansion of network around MAP estimate to define predictive covariance analytically
- Quick check: How does linearization assumption simplify calculation of predictive covariance, and what assumption does it make about network's behavior near MAP?

## Architecture Onboarding

**Component map:** Trained Model (MAP) -> Posterior Approximator -> Jacobian Engine -> Eigensolver -> Projector Constructor

**Critical path:** Train MAP → Compute/Approximate Posterior Ψ_{approx} → Select Data Subset X' → Compute Jacobians & Eigenvectors → Construct Projector P → Inference

**Design tradeoffs:**
- KFAC vs. Diagonal: KFAC provides better alignment with full Hessian (lower relative error) but higher memory/compute costs
- Subset Size (n): Larger n better approximates full data distribution but increases SVD cost
- Subspace Dimension (s): Increasing s lowers error but increases storage of P (size p×s)

**Failure signatures:**
- High Relative Error with Low Rank: Indicates poor Ψ_{approx} (try KFAC instead of Diagonal) or unrepresentative X'
- Trace Criterion Saturation: If trace increases but error doesn't decrease, captured eigenspace may not align with test distribution
- Dead Parameters: In regression, if gradients vanish for many params, subset method might outperform low-rank method

**First 3 experiments:**
1. Baseline Comparison: Implement "low-rank" projector (using Diagonal approximation) vs. "subset" projector (magnitude-based) on small UCI regression dataset (e.g., Red Wine). Plot Relative Error vs. s.
2. Approximation Quality: Implement KFAC-based projector on classification task (MNIST). Compare Trace Criterion against Relative Error to verify correlation.
3. Ablation on Subset Size: Fix s and vary data subset size n used to find eigenvectors. Observe point of diminishing returns for Relative Error.

## Open Questions the Paper Calls Out

**Open Question 1:** Can a distinct, computationally feasible optimal projection matrix be derived that avoids storage constraints of current solution?
- Basis: Conclusion asks if solution is unique and states, "If there was another optimal solution that is computationally more feasible the practicability could be improved"
- Why unresolved: Current theoretical optimum requires explicitly storing matrix P, which scales with parameter count p
- What evidence would resolve it: Derivation of alternative optimal projector achieving same minimum Frobenius norm error without O(ps) memory storage

**Open Question 2:** To what extent does quality of posterior covariance approximation (Ψ_{approx}) bound error of resulting subspace model?
- Basis: Practical construction relies on approximations like KFAC or diagonal matrices because true posterior covariance Ψ is inaccessible
- Why unresolved: While paper shows KFAC outperforms diagonal approximations empirically, it doesn't provide theoretical guarantees on how error in Ψ_{approx} propagates to predictive covariance error
- What evidence would resolve it: Theoretical bounds linking spectral norm of (Ψ - Ψ_{approx}) to relative error of subspace predictive covariance

**Open Question 3:** Why does approximated low-rank subspace method fail to maintain optimal performance on specific out-of-distribution corruptions?
- Basis: Figure 3 shows theoretical optimum handles corrupted MNIST well, but KFAC-based approximation fails on certain corruptions (e.g., brightness, fog)
- Why unresolved: Authors note performance drop depends on "nature of out-of-distribution data" but don't determine if this is failure of subset selection X' or covariance approximation
- What evidence would resolve it: Ablation studies varying subset X' used for construction on OOD data, or analysis of spectral alignment between approximate and true Hessian eigenspaces under distribution shift

## Limitations

- Theoretical optimality relies on local linearity assumption around MAP estimate, which may not hold for highly non-convex loss landscapes
- Computational bottleneck exists for computing Jacobians for large data subsets, scaling poorly with input dimensionality
- Empirical superiority over subset selection methods primarily demonstrated on specific regression benchmarks, limiting generalization claims

## Confidence

**High confidence:** Mathematical derivation of optimal projector using dominant eigenvectors (Theorem 1) and trace criterion as proxy for subspace quality are theoretically sound and empirically validated.

**Medium confidence:** Empirical claim that <1% of parameters suffice for reliable uncertainty estimates holds for tested UCI regression datasets but may not generalize to all problem domains or network architectures.

**Low confidence:** Superiority over subset selection methods is demonstrated primarily on specific regression benchmarks; generalization to complex vision tasks and other domains remains to be thoroughly tested.

## Next Checks

1. Test the method on non-UCI datasets with known challenging geometries (e.g., multi-modal posterior distributions) to evaluate robustness to local linearity violations.

2. Systematically vary the data subset size n and measure the trade-off between computational cost and approximation accuracy across different posterior approximations (KFAC vs. diagonal).

3. Implement the method on a transformer-based architecture where parameter space is extremely high-dimensional to assess scalability and whether <1% parameter claim holds.