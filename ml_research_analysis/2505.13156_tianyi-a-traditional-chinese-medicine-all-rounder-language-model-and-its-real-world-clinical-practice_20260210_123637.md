---
ver: rpa2
title: 'Tianyi: A Traditional Chinese Medicine all-rounder language model and its
  Real-World Clinical Practice'
arxiv_id: '2505.13156'
source_url: https://arxiv.org/abs/2505.13156
tags:
- clinical
- knowledge
- llms
- tianyi
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Tianyi, a 7.6-billion-parameter large language
  model specifically designed for Traditional Chinese Medicine (TCM). The model was
  pre-trained on 3.4 billion TCM tokens from diverse sources including classical texts,
  textbooks, clinical records, and research articles, then fine-tuned on TCM-specific
  tasks.
---

# Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice

## Quick Facts
- arXiv ID: 2505.13156
- Source URL: https://arxiv.org/abs/2505.13156
- Reference count: 22
- A 7.6B parameter TCM-specific LLM achieving state-of-the-art performance on TCM clinical tasks and real-world influenza treatment prediction

## Executive Summary
This paper presents Tianyi, a 7.6-billion-parameter large language model specifically designed for Traditional Chinese Medicine (TCM). The model was pre-trained on 3.4 billion TCM tokens from diverse sources including classical texts, textbooks, clinical records, and research articles, then fine-tuned on TCM-specific tasks. Tianyi achieves state-of-the-art performance on TCMEval, a comprehensive benchmark covering TCM examinations (75.38% accuracy), clinical tasks (81.21% F1-score), and domain question answering (46.83% Rouge-L). In real-world clinical trials for influenza treatment prediction, Tianyi demonstrates strong capabilities in predicting fever abatement and adverse reactions. Human evaluation by TCM doctors shows Tianyi excels in syndrome diagnosis, prescription generation, and clinical reasoning compared to other models, validating its potential as an AI assistant for TCM clinical practice.

## Method Summary
Tianyi is a 7.6-billion-parameter language model pre-trained on 3.4 billion TCM-specific tokens from diverse sources including classical texts, textbooks, clinical records, and research articles. The model was then fine-tuned on TCM-specific tasks including examinations, clinical reasoning, and question answering. The TCMEval benchmark was developed to comprehensively evaluate TCM domain knowledge across multiple task types. Real-world clinical validation was conducted through influenza treatment prediction scenarios, with human evaluation by TCM doctors assessing performance on syndrome diagnosis, prescription generation, and clinical reasoning tasks.

## Key Results
- Achieves 75.38% accuracy on TCM examinations benchmark
- Scores 81.21% F1 on TCM clinical tasks
- Demonstrates 46.83% Rouge-L on TCM domain question answering
- Shows strong performance in real-world clinical trials for influenza treatment prediction
- Excels in syndrome diagnosis, prescription generation, and clinical reasoning according to TCM doctor evaluations

## Why This Works (Mechanism)
Tianyi's effectiveness stems from its domain-specific pretraining on 3.4B TCM tokens, which grounds the model in traditional Chinese medical knowledge, terminology, and reasoning patterns. The diverse training corpus covering classical texts, clinical records, and research articles provides comprehensive domain coverage that enables the model to understand both theoretical foundations and practical clinical applications. The large parameter count (7.6B) allows the model to capture complex relationships in TCM diagnosis and treatment patterns. Fine-tuning on TCM-specific tasks further adapts the model to clinical reasoning and prescription generation, while the human evaluation by TCM doctors validates that the model's outputs align with professional medical standards and practices.

## Foundational Learning
- **Traditional Chinese Medicine Theory**: Understanding of Yin-Yang, Five Elements, and Qi concepts; why needed for accurate syndrome diagnosis; quick check: can correctly identify syndrome patterns
- **Classical TCM Texts**: Familiarity with Huangdi Neijing and other foundational texts; why needed for theoretical grounding; quick check: can reference appropriate classical formulations
- **TCM Diagnosis Methods**: Pattern differentiation and syndrome identification; why needed for clinical reasoning; quick check: accurate pulse and tongue diagnosis descriptions
- **Herbal Medicine Knowledge**: Properties, functions, and combinations of TCM herbs; why needed for prescription generation; quick check: correct herb selection and dosage
- **TCM Clinical Records**: Understanding of treatment outcomes and patient responses; why needed for real-world application; quick check: can predict treatment effects and adverse reactions

## Architecture Onboarding

**Component Map**: Pre-training Corpus -> 7.6B Parameter Transformer -> Fine-tuning on TCM Tasks -> TCMEval Benchmark -> Clinical Validation -> Human Evaluation

**Critical Path**: TCM-specific pretraining → task-specific fine-tuning → comprehensive evaluation → clinical validation → human assessment

**Design Tradeoffs**: Large parameter count (7.6B) enables better domain understanding but increases computational requirements; diverse pretraining corpus improves generalization but may introduce noise; human evaluation provides clinical relevance but introduces potential subjectivity

**Failure Signatures**: Poor performance on tasks requiring integration of classical theory with modern clinical practice; inability to handle rare or complex syndrome patterns; generation of prescriptions that violate traditional contraindications

**3 First Experiments**:
1. Test model's ability to correctly identify syndrome patterns from clinical descriptions
2. Evaluate prescription generation for common TCM conditions against established treatment protocols
3. Assess clinical reasoning capabilities in predicting treatment outcomes and potential adverse reactions

## Open Questions the Paper Calls Out
None

## Limitations
- Limited external validation beyond the authors' evaluation framework
- Clinical trial scope restricted to one condition (influenza)
- No comparison with general-purpose LLMs (GPT-4, Claude) on the same TCM-specific tasks
- Human evaluation methodology lacks detailed scoring rubrics and inter-rater reliability metrics

## Confidence
- **High confidence** in the technical implementation (7.6B parameter architecture, diverse pre-training corpus of 3.4B tokens)
- **Medium confidence** in benchmark performance claims due to lack of independent validation
- **Medium confidence** in clinical utility based on human evaluation, though methodology details for doctor assessments are limited
- **Low confidence** in real-world clinical generalizability beyond the influenza case study

## Next Checks
1. Independent replication of Tianyi's performance on TCMEval benchmark using the same evaluation protocols
2. Comparative assessment against leading general-purpose LLMs (GPT-4, Claude) on identical TCM clinical reasoning and prescription generation tasks
3. Prospective clinical deployment study across multiple TCM conditions and hospital settings to assess real-world performance variability and safety