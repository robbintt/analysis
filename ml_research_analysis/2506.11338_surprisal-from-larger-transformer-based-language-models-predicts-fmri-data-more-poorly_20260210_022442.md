---
ver: rpa2
title: Surprisal from Larger Transformer-based Language Models Predicts fMRI Data
  More Poorly
arxiv_id: '2506.11338'
source_url: https://arxiv.org/abs/2506.11338
tags:
- surprisal
- data
- language
- fmri
- estimates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the relationship between the size of Transformer-based
  language models and their ability to predict human brain activity during language
  comprehension. Using 17 pre-trained models from three different families (GPT-2,
  GPT-Neo, OPT), surprisal estimates were calculated for two fMRI datasets: Shain
  et al.'
---

# Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly

## Quick Facts
- arXiv ID: 2506.11338
- Source URL: https://arxiv.org/abs/2506.11338
- Reference count: 19
- Larger Transformer models show poorer predictive power for fMRI brain activity patterns

## Executive Summary
This study investigates the relationship between Transformer model size and its ability to predict human brain activity during language comprehension using fMRI data. The authors tested 17 pre-trained models from GPT-2, GPT-Neo, and OPT families, computing surprisal estimates for two fMRI datasets. Surprisal values were convolved with hemodynamic response functions and used as predictors in linear mixed-effects models alongside baseline predictors like word position and length. The results reveal a significant inverse scaling relationship: larger models with more parameters provide poorer fits to brain activity patterns. This finding contrasts with previous work showing that vector representations from larger models yield better brain predictions, highlighting a fundamental difference in how probability estimates versus hidden states relate to neural processing.

## Method Summary
The authors evaluated 17 pre-trained Transformer language models across three families (GPT-2, GPT-Neo, OPT) with varying parameter counts. For each model, they computed surprisal estimates for two fMRI datasets: Shain et al. (2020) and Pereira et al. (2018). These surprisal values were convolved with hemodynamic response functions to account for the slow blood oxygen level-dependent response. The convolved surprisal estimates were then used as predictors in linear mixed-effects regression models alongside baseline predictors (word position, word length). Model performance was assessed by comparing how well different-sized models predicted brain activity patterns in language processing regions.

## Key Results
- Inverse scaling relationship: Larger models showed significantly reduced predictive power for fMRI data compared to smaller models
- The trend was consistent across both fMRI datasets tested (Shain et al., 2020 and Pereira et al., 2018)
- This finding extends previous inverse scaling results from reading times to neural measures of language processing

## Why This Works (Mechanism)
The inverse scaling relationship suggests that larger LMs diverge from human language processing patterns in ways that make their surprisal estimates less aligned with brain activity. The authors speculate this may be due to how larger models handle rare words differently than human language processing, as suggested by previous work on reading times. The mechanism appears to be specific to probability estimates (surprisal) rather than vector representations, as other work has shown positive scaling for vector-based predictions.

## Foundational Learning
- **Surprisal**: A measure of prediction difficulty or unexpectedness in language processing; needed to quantify how well LM predictions match human comprehension difficulty; quick check: verify surprisal calculation follows Shannon information theory
- **Hemodynamic response function**: The physiological response linking neural activity to fMRI signal changes; needed to properly align computational surprisal with slow fMRI measurements; quick check: confirm convolution with canonical HRF was appropriate for the datasets
- **Linear mixed-effects models**: Statistical models accounting for both fixed effects and random variation across subjects; needed to handle the hierarchical structure of fMRI data with multiple subjects and trials; quick check: verify proper specification of random effects structure

## Architecture Onboarding

Component Map:
fMRI datasets -> Preprocessing (HRF convolution) -> Linear mixed-effects models <- Surprisal estimates <- Language models (17 total)

Critical Path:
Language model → Surprisal calculation → HRF convolution → Mixed-effects regression → Brain activity prediction

Design Tradeoffs:
- Choice of HRF convolution affects temporal alignment between surprisal and neural signals
- Selection of baseline predictors (word position, length) vs. more linguistically informed features
- Decision to use surprisal rather than vector representations, which show different scaling properties

Failure Signatures:
- Poor model fit despite appropriate preprocessing could indicate fundamental mismatch between LM and brain processing
- Overfitting risk with many model comparisons across 17 different LMs
- Potential confounds from training data overlap between models and stimulus materials

First Experiments:
1. Test whether inverse scaling holds for other transformer architectures (BERT, T5, PaLM)
2. Compare surprisal-based predictions against vector representation-based predictions using identical models
3. Evaluate alternative baseline models beyond word position and length

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the inverse scaling relationship between model size and surprisal predictive power generalize to non-English languages with different morphological or orthographic properties?
- Basis in paper: [explicit] The authors state in the Limitations section that models were trained on English and datasets were from English speakers, so "these findings may or may not be replicated in other languages."
- Why unresolved: The study restricted its evaluation to English-only models and datasets, leaving cross-linguistic validity untested.
- What evidence would resolve it: Replicating the experimental protocol on fMRI datasets from morphologically rich languages (e.g., Finnish, Turkish) using corresponding multilingual LMs.

### Open Question 2
- Question: Why does surprisal from larger models yield poorer fits to fMRI data (inverse scaling) while vector representations from larger models yield better fits (positive scaling)?
- Basis in paper: [explicit] The Introduction contrasts this study's findings on surprisal with recent work (Schrimpf et al., 2021; Hosseini et al., 2024) that observed positive scaling when using "vectors directly from large language models."
- Why unresolved: The paper establishes that the divergence exists for surprisal but does not investigate the theoretical disconnect between the scaling behaviors of probability estimates versus hidden state representations.
- What evidence would resolve it: A direct comparison of both metrics (surprisal vs. vector representations) using identical model sets and fMRI data to identify the processing level where scaling inverts.

### Open Question 3
- Question: To what extent does word frequency drive the inverse scaling effect in fMRI data, as it does in reading time data?
- Basis in paper: [inferred] The Conclusion speculates that the deviation of larger LMs is due to the same reasons found in reading times (Oh and Schuler, 2023), specifically that they handle rare words differently, but this is not explicitly tested on the fMRI data in this study.
- Why unresolved: The mechanism is proposed by analogy to prior work on reading times rather than being empirically verified through frequency-based analysis on the fMRI datasets used here.
- What evidence would resolve it: An analysis partitioning the fMRI data by word frequency classes to see if the inverse scaling is predominantly driven by the model's fit to low-frequency words.

## Limitations
- Limited to English language models and datasets, restricting cross-linguistic generalizability
- Only tested three model families (GPT-2, GPT-Neo, OPT), leaving other architectures unexamined
- Focus on fMRI data only, without validation across other neural recording modalities like EEG or MEG

## Confidence

| Claim | Confidence |
|-------|------------|
| Inverse scaling relationship is statistically robust for tested datasets | High |
| Results generalize to other model families/architectures | Medium |
| The mechanism relates to rare word processing | Medium |

## Next Checks
1. Test whether inverse scaling holds for other transformer architectures (BERT, T5, PaLM) and for neural data from other recording modalities (EEG, MEG, intracranial recordings)
2. Evaluate alternative baseline models beyond word position and length, including models that capture linguistic structure more explicitly
3. Conduct ablation studies to determine whether the inverse scaling effect persists when controlling for potential confounds such as training data overlap between models and stimulus materials