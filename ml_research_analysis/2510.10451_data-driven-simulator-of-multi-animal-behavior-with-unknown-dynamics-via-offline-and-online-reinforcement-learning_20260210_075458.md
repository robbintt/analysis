---
ver: rpa2
title: Data-driven simulator of multi-animal behavior with unknown dynamics via offline
  and online reinforcement learning
arxiv_id: '2510.10451'
source_url: https://arxiv.org/abs/2510.10451
tags:
- learning
- agents
- reward
- condition
- silkmoth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of simulating multi-animal behaviors
  in biological systems where transition dynamics are unknown. The proposed AnimaRL
  framework combines deep reinforcement learning with a data-driven locomotion parameter
  estimation step, enabling agents to both imitate real trajectories and acquire rewards.
---

# Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning

## Quick Facts
- arXiv ID: 2510.10451
- Source URL: https://arxiv.org/abs/2510.10451
- Reference count: 40
- Key outcome: AnimaRL framework combines deep RL with locomotion parameter estimation to simulate multi-animal behavior and counterfactual scenarios with higher trajectory fidelity than standard baselines.

## Executive Summary
This paper introduces AnimaRL, a framework for simulating multi-animal behaviors when transition dynamics are unknown. The method integrates offline reinforcement learning (DQCIL) with locomotion parameter estimation to enable both imitation of real trajectories and reward-driven behavior. By combining trajectory matching and counterfactual prediction, the system can generate realistic "what-if" scenarios. Evaluated on artificial agents, flies, newts, and silkmoths, the approach achieves improved trajectory fidelity and reward acquisition compared to standard baselines, demonstrating potential for biological system simulation and experimental design.

## Method Summary
The AnimaRL framework addresses the challenge of simulating multi-animal behavior when dynamics are unknown by combining deep reinforcement learning with data-driven locomotion parameter estimation. The method uses a distance-based pseudo-reward (DQDIL) to align cyber-physical states and optionally includes a counterfactual prediction head (DQCIL) for generating what-if scenarios. The locomotion parameter estimation step models animal motion using a parametric model that captures velocity and angular changes, enabling the agent to learn from observed trajectories while acquiring task-specific rewards. The approach was validated on multiple species including artificial agents, fruit flies, newts, and silkmoths.

## Key Results
- AnimaRL outperforms standard imitation and RL baselines in trajectory fidelity and reward acquisition across multiple animal species
- The framework successfully generates counterfactual trajectories that are qualitatively plausible for what-if scenario analysis
- Velocity RMSE varies by species, with flies showing higher error (up to 0.105) compared to artificial agents, newts, and silkmoths (0.0001-0.02)

## Why This Works (Mechanism)
The framework works by separating locomotion dynamics from reward-driven decision-making. The data-driven locomotion parameter estimation captures the physical movement patterns of animals, while the reinforcement learning component learns to optimize behavior within those physical constraints. The distance-based pseudo-reward ensures that the learned policy produces trajectories that match real observations, while the counterfactual prediction head enables generation of alternative scenarios. This hybrid approach allows the system to learn both from data and from reward signals, making it more flexible than pure imitation or pure RL approaches.

## Foundational Learning
- **Deep reinforcement learning (DQCIL)**: Why needed - To learn optimal policies that can acquire rewards while respecting physical constraints. Quick check - Verify that the agent can learn to navigate toward targets while following realistic movement patterns.
- **Data-driven locomotion parameter estimation**: Why needed - To capture the physical movement dynamics of animals when explicit equations of motion are unavailable. Quick check - Ensure velocity and angular changes match observed distributions in the data.
- **Counterfactual prediction**: Why needed - To generate plausible "what-if" scenarios for experimental design and hypothesis testing. Quick check - Validate that generated counterfactuals maintain physical plausibility and behavioral consistency.
- **Distance-based pseudo-reward**: Why needed - To align cyber-physical states between learned policy and real trajectories. Quick check - Confirm that trajectory matching loss decreases during training.

## Architecture Onboarding
Component map: Observation -> Encoder -> Latent State -> Locomotion Estimator + RL Policy -> Action
Critical path: Encoder processes observations, latent state combines with locomotion parameters, RL policy generates actions conditioned on both locomotion and reward objectives.
Design tradeoffs: Pure data-driven approach offers flexibility but may miss known physical constraints; adding physics priors could improve counterfactual accuracy but reduces generality.
Failure signatures: High velocity RMSE indicates poor locomotion estimation; low reward acquisition suggests RL component struggles with task objectives; unrealistic counterfactuals indicate insufficient physical constraints.
First experiments: 1) Test trajectory reproduction on held-out data from the same experimental conditions. 2) Evaluate counterfactual predictions on known scenarios. 3) Compare performance with and without the counterfactual prediction head.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can incorporating mixture or state-switching dynamics improve locomotion parameter estimation for animals with bimodal velocity distributions, such as flies?
- Basis in paper: [explicit] The Conclusion states future work should "consider embedded mixture or state-switching dynamics for bimodal movers such as flies."
- Why unresolved: The current single-mode parametric model ($v' = (1-d)v + ua$) struggles with the fly dataset's specific velocity profile, resulting in significantly higher velocity RMSE (up to 0.105) compared to other species.
- What evidence would resolve it: Demonstrated reduction in velocity estimation error and improved trajectory reproducibility for flies using a model that accommodates both near-zero and high-velocity states.

### Open Question 2
- Question: How does encoding partially known physics directly into the latent state or loss structure affect the accuracy of the counterfactual predictions?
- Basis in paper: [explicit] The Discussion notes the simulator "could be made even more accurate by encoding fully known or partially known dynamics directly into the network's latent state or loss structure."
- Why unresolved: The current approach is purely data-driven, which may limit accuracy when known physical constraints are ignored or when the model attempts to predict "what-if" scenarios without explicit physical priors.
- What evidence would resolve it: A comparative study showing that a hybrid model (data + known physics) outperforms the purely data-driven DQCIL in counterfactual path length prediction and physical consistency.

### Open Question 3
- Question: Does the AnimaRL framework maintain its fidelity and generalization capabilities when scaled to three-dimensional settings?
- Basis in paper: [explicit] The Conclusion lists "scale the framework to three-dimensional settings" as a direction for future work.
- Why unresolved: The current study only validates the method on two-dimensional planar trajectories (artificial agents, flies, newts, silkmoths), leaving the computational and modeling challenges of 3D dynamics untested.
- What evidence would resolve it: Successful application of the framework to 3D trajectory datasets (e.g., bird flight or fish schools) showing comparable reproducibility and reward acquisition metrics to the 2D experiments.

## Limitations
- The evaluation relies on relatively small datasets (hundreds of samples per species), which may not fully capture the complexity of natural animal behaviors.
- Improvements over baselines are modest in some cases, particularly for the silkmoth dataset where gains are minimal.
- Counterfactual prediction validation is limited to qualitative visual inspection rather than rigorous quantitative metrics.

## Confidence
- Trajectory imitation and reward acquisition: High - Supported by quantitative metrics across multiple datasets
- Counterfactual prediction capability: Medium - Lacks rigorous validation beyond visual inspection
- Generalization to new experimental conditions: Low - Current validation is limited to known scenarios

## Next Checks
1. Test the counterfactual prediction capability using held-out experimental conditions not seen during training to assess true generalization.
2. Conduct ablation studies to quantify the individual contributions of the DQDIL and DQCIL components to overall performance.
3. Evaluate the framework's scalability and performance with larger datasets (thousands of samples) to determine if current results hold with more comprehensive behavioral coverage.