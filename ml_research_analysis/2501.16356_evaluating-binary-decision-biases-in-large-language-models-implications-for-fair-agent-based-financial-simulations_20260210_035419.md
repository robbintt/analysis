---
ver: rpa2
title: 'Evaluating Binary Decision Biases in Large Language Models: Implications for
  Fair Agent-Based Financial Simulations'
arxiv_id: '2501.16356'
source_url: https://arxiv.org/abs/2501.16356
tags:
- responses
- random
- llms
- reject
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Large Language Models (LLMs) can
  reliably produce fair binary choices for use in agent-based financial market simulations,
  where true randomness is essential. Using three state-of-the-art GPT models, the
  researchers tested for uniform output distribution and Markovian independence under
  one-shot and few-shot sampling approaches, as well as varying the temperature parameter.
---

# Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations

## Quick Facts
- arXiv ID: 2501.16356
- Source URL: https://arxiv.org/abs/2501.16356
- Reference count: 5
- Large Language Models exhibit significant biases in binary decision-making, with no model simultaneously achieving uniform distribution and Markovian properties

## Executive Summary
This study investigates whether Large Language Models can reliably produce fair binary choices for agent-based financial market simulations, where true randomness is essential. Testing three state-of-the-art GPT models under various sampling approaches and temperature settings, the researchers found significant biases across all models, with GPT-4-0125-preview and GPT-3.5-turbo-0125 showing extreme response skews (98-99% and 87-98% yes responses respectively). While few-shot approaches improved distributional outcomes, none of the models maintained both uniform distribution and Markovian independence properties, highlighting fundamental limitations for their use in financial simulations requiring unbiased decision-making.

## Method Summary
The researchers tested three GPT models (GPT-4o-Mini-2024-07-18, GPT-4-0125-preview, and GPT-3.5-turbo-0125) for their ability to generate unbiased binary decisions. They employed one-shot and few-shot sampling approaches while varying the temperature parameter to assess distributional uniformity and Markovian independence properties. The study compared LLM outputs against true random sequences and human-generated patterns, analyzing 500-length sequences for recency bias and independence violations.

## Key Results
- GPT-4o-Mini-2024-07-18 showed the best performance with 32-43% yes responses in one-shot tests, not statistically different from uniform distribution
- GPT-4-0125-preview exhibited extreme bias with 98-99% yes responses, while GPT-3.5-turbo-0125 showed 87-98% yes responses
- No model simultaneously achieved uniform distribution and Markovian properties across all tested conditions

## Why This Works (Mechanism)
Assumption: The biases observed in LLM binary decision-making likely stem from training data distributions and model architectures that prioritize pattern completion over true randomness generation. These models may be optimized for coherence and predictability rather than stochastic behavior.

## Foundational Learning
Assumption: The study builds upon foundational work in behavioral economics and agent-based modeling, where unbiased random decision-making is crucial for realistic market simulations. Previous research has established that human biases in financial decision-making can be modeled, but this work reveals that LLMs may introduce new, algorithmic biases.

## Architecture Onboarding
Unknown: The paper does not provide specific details about how different model architectures (GPT-4o-Mini vs GPT-4-0125-preview vs GPT-3.5-turbo-0125) contribute to the observed biases. Understanding architectural differences could help explain why some models performed better than others in generating unbiased binary choices.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to three GPT models, restricting generalizability across different LLM architectures
- Binary decision context may not capture complex decision-making patterns in other task domains
- Temperature adjustments and few-shot sampling improved distributional outcomes but failed to resolve Markovian property violations

## Confidence
- High confidence in presence of significant biases across all tested models
- Medium confidence in comparative performance rankings due to limited model versions and parameter settings tested
- Well-supported conclusion that no model simultaneously achieved uniform distribution and Markovian properties

## Next Checks
1. Test additional LLM architectures beyond GPT models to determine if biases are universal or model-specific
2. Conduct longer sequence analyses (beyond 500-length sequences) to examine whether biases compound or diminish over extended decision chains
3. Implement tests using models fine-tuned specifically for unbiased binary decision generation to assess whether architectural limitations or training objectives drive these biases