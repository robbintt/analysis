---
ver: rpa2
title: 'FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series
  Imputation'
arxiv_id: '2512.15116'
source_url: https://arxiv.org/abs/2512.15116
tags:
- time
- imputation
- missing
- series
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multivariate time series imputation, a fundamental
  challenge in applications such as healthcare, traffic forecasting, and biological
  modeling where missing data arises from sensor failures and irregular sampling.
  The authors propose FADTI, a diffusion-based framework that incorporates frequency-domain
  information through a learnable Fourier Bias Projection (FBP) module.
---

# FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation

## Quick Facts
- **arXiv ID**: 2512.15116
- **Source URL**: https://arxiv.org/abs/2512.15116
- **Reference count**: 40
- **Key outcome**: Fourier and Attention Driven Diffusion framework (FADTI) achieves state-of-the-art multivariate time series imputation performance, particularly under high missing rates (10-50%), by incorporating frequency-domain inductive biases through learnable Fourier Bias Projection (FBP) module.

## Executive Summary
This paper addresses the fundamental challenge of multivariate time series imputation where missing data arises from sensor failures and irregular sampling in applications like healthcare, traffic forecasting, and biological modeling. The authors propose FADTI, a diffusion-based framework that incorporates frequency-domain information through a learnable Fourier Bias Projection (FBP) module. FBP injects frequency-domain inductive biases into the generative imputation process by projecting learnable spectral biases onto low-frequency Fourier bands, enabling adaptive encoding of both stationary and non-stationary patterns. The framework combines this with temporal modeling through self-attention and gated convolution.

Experiments on multiple benchmarks, including a newly introduced biological time series dataset (Yeast), show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. The method achieves lower MAE, RMSE, and CRPS scores compared to baselines like CSDI, MTSCI, and TimesNet, demonstrating superior accuracy and robustness in handling various missing patterns and temporal dynamics.

## Method Summary
FADTI uses conditional diffusion modeling for multivariate time series imputation, incorporating a learnable Fourier Bias Projection (FBP) module that projects spectral biases onto low-frequency Fourier bands. The method combines temporal modeling through self-attention or gated convolution with frequency-domain priors learned within the diffusion process. Training involves predicting added Gaussian noise on masked entries using a noise schedule with 50 diffusion steps. Inference uses ancestral DDPM sampling with multiple trajectories averaged to produce final imputations.

## Key Results
- FADTI consistently outperforms state-of-the-art methods (CSDI, MTSCI, TimesNet) across multiple benchmarks including ETT, Weather, METR-LA, and the newly introduced Yeast biological dataset
- The method demonstrates superior performance particularly under high missing rates (10-50%), with FBP module being the primary driver of performance improvements
- FADTI achieves lower MAE, RMSE, and CRPS scores, providing both accurate point estimates and calibrated uncertainty estimates through the diffusion framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting learnable spectral biases onto low-frequency Fourier bands mitigates spectral distortion caused by missing values.
- Mechanism: FBP decomposes input into trend and residual components via learnable moving average, applies one of three Fourier transforms (DFT/STFT/FrSST), then projects onto fixed cosine/sine bases. The bias is learned within the conditional diffusion process rather than directly transforming masked inputs.
- Core assumption: Missing values corrupt global frequency structure; learning biases in diffusion space then projecting spectrally preserves structure better than naive FFT on masked data.
- Evidence anchors:
  - [abstract] "FBP injects frequency-domain inductive biases into the generative imputation process by projecting learnable spectral biases onto low-frequency Fourier bands"
  - [section IV-C] "Instead of directly applying a Fourier transform to masked inputs, which yields distorted spectra... FBP learns the bias within the conditional diffusion process"
  - [corpus] LSCD (arXiv 2506.17039) notes FFT assumes uniform sampling and requires interpolation that distorts spectra—indirectly supports FBP's alternative approach
- Break condition: Highly irregular, event-driven series without smooth trends or quasi-periodic structure may not benefit from low-frequency inductive bias.

### Mechanism 2
- Claim: Conditional diffusion provides calibrated uncertainty estimates under high missing rates.
- Mechanism: Forward diffusion adds Gaussian noise; reverse process conditions on observed values (X_co) via concatenation. Multiple samples from independent trajectories are averaged, enabling CRPS-evaluated distribution quality.
- Core assumption: Gaussian noise preserves local temporal structure; missing regions follow conditional distributions learnable from observed context.
- Evidence anchors:
  - [abstract] "diffusion-based framework... modeling the conditional distribution of missing segments and provides calibrated uncertainty estimates"
  - [section III-C] "pθ(x^ta_0 | x^co_0)" formalizes conditional reverse sampling
  - [corpus] CSDI and MTSCI established conditional diffusion for imputation; FADTI builds on this with frequency priors
- Break condition: Uniform noise severely degrades performance (Figure 8); non-Gaussian observation noise not evaluated.

### Mechanism 3
- Claim: Temporal decomposition + gated convolution complements attention for local dependency modeling.
- Mechanism: Series decomposition (Algorithm 2) separates trend/residual. Gated TCN uses tanh filter and sigmoid gate (Eq. 26), providing fixed receptive field with O(T) complexity vs. attention's O(T²).
- Core assumption: Local temporal patterns and global frequency structures are complementary; modeling both improves robustness.
- Evidence anchors:
  - [section IV-D2] "gated temporal convolutional network... processes inputs... each layer applies two parallel dilated convolutions"
  - [Table VII] Ablation shows both Attention and Conv backbones work with FBP; frequency module is primary driver
  - [corpus] Weak/no direct corpus comparison for gated TCN in imputation
- Break condition: Very long horizons (>288 steps) or >207 variables not systematically evaluated.

## Foundational Learning

- Concept: **Discrete Fourier Transform (DFT) vs. Short-Time Fourier Transform (STFT)**
  - Why needed here: FBP instantiates with three transforms; DFT assumes stationarity (global), STFT provides time-localized frequency via sliding windows.
  - Quick check question: Given a traffic sensor with diurnal peaks and sudden incident spikes, which transform better captures both patterns?

- Concept: **Denoising Score Matching in Diffusion Models**
  - Why needed here: Training objective (Eq. 8, 31) predicts added noise ε rather than directly predicting missing values.
  - Quick check question: Why does restricting the loss to masked entries (Eq. 31) matter for imputation vs. generation?

- Concept: **Time-Frequency Tradeoff (Heisenberg Uncertainty)**
  - Why needed here: STFT has fixed window K_f, trading time resolution for frequency resolution; FrSST attempts sharper localization via synchrosqueezing.
  - Quick check question: What happens to frequency resolution if you halve the STFT window size?

## Architecture Onboarding

- Component map: Input X + Mask M → Conditional Feature Extractor → X_cf → [Temporal Freq Module (FBP + Attn/Conv)] → [Feature Attention Module] → [Gate-Filter Module] → Noise Prediction Head → ε̂

- Critical path: FBP is the primary performance driver (Table VII: worst results always from "None" variants). Start by understanding FBP's trend/residual decomposition (Algorithm 2) and basis projection (Algorithm 3).

- Design tradeoffs:
  - DFT vs. STFT vs. FrSST: DFT best for stationary/periodic (ETT); STFT/FrSST better for non-stationary with contiguous gaps (Yeast, time-wise missing).
  - Attention vs. Conv: Similar performance; Conv has O(T) vs. O(T²) but fixed receptive field.
  - Sample count K: Diminishing returns after n_sample=20 (Figure 5).

- Failure signatures:
  - MAE similar to baselines at low missing rates but diverges at high rates → FBP not learning useful spectral bias; check decomposition kernel K_d.
  - CRPS poor but MAE acceptable → insufficient samples K or noise schedule issue.
  - Uniform noise in forward process → catastrophic failure (Figure 8).

- First 3 experiments:
  1. **Ablate FBP**: Run None-Attn vs. DFT-Attn on a single dataset (ETT, point-wise, 0.5 missing) to confirm FBP contribution before full benchmarks.
  2. **Transform comparison**: On your domain data, compare DFT vs. STFT variants to determine if series is stationary (DFT wins) or non-stationary (STFT/FrSST wins).
  3. **Sample efficiency sweep**: Plot MAE vs. n_sample (2-28) to find plateau point; FADTI should reach baseline-beating performance at n_sample≈6.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FADTI be extended to handle irregular time grids where observations occur at non-uniform intervals?
- Basis in paper: [explicit] The conclusion states "Future work includes extensions to irregular time grids" and the current approach assumes "discrete time steps" with regular sampling.
- Why unresolved: The FBP module uses standard Fourier transforms (DFT, STFT, FrSST) that assume uniformly sampled inputs; irregular grids would require fundamentally different spectral decomposition strategies.
- What evidence would resolve it: A modified FBP that handles irregular timestamps, evaluated on datasets with known non-uniform sampling, demonstrating maintained or improved performance.

### Open Question 2
- Question: How can the framework be adapted for regimes with thousands of variables or very long horizons?
- Basis in paper: [explicit] "From a scalability perspective, our experiments only cover multivariate series with up to D=207 variables (METR-LA), and we have not systematically evaluated very long horizons or regimes with thousands of variables."
- Why unresolved: The self-attention component has quadratic complexity, and diffusion models require multiple denoising steps per prediction, limiting scalability.
- What evidence would resolve it: Systematic evaluation on datasets with D > 1000 variables and T > 1000 steps, with analysis of memory and runtime scaling.

### Open Question 3
- Question: Can FADTI be made robust to non-Gaussian observation noise in the forward diffusion process?
- Basis in paper: [explicit] The noise sensitivity analysis shows "Uniform noise leads to substantial failure, with errors increasing by orders of magnitude" and extending to "non-Gaussian observation noise is left for future work."
- Why unresolved: The diffusion framework assumes Gaussian perturbations for stable score estimation; alternative noise distributions distort denoising trajectories.
- What evidence would resolve it: A modified noise schedule or score matching objective that handles Laplace, uniform, or heavy-tailed noise without catastrophic degradation.

### Open Question 4
- Question: Can the choice between DFT, STFT, and FrSST within FBP be made adaptively based on input characteristics?
- Basis in paper: [inferred] The ablation study (Table VII) shows different Fourier variants excel on different datasets—DFT dominates on ETT, while STFT/FrSST perform better under time-wise missing patterns—suggesting no single transform is universally optimal.
- Why unresolved: The current design requires manual selection of the spectral transform; the relationship between data characteristics (stationarity, missing pattern) and optimal transform remains unexplored.
- What evidence would resolve it: A meta-learning or gating mechanism that selects transforms dynamically, with analysis of which signal properties predict optimal transform choice.

## Limitations

- The novel Yeast biological dataset lacks complete baseline comparisons for some metrics in Table III, creating uncertainty about consistent improvement across all measures
- The method's effectiveness under extremely high missing rates (>80%) and for very long sequences (>288 steps) remains unexplored
- Limited ablation analysis of individual Fourier transforms (DFT vs. STFT vs. FrSST) and their interaction with temporal decomposition components

## Confidence

- **High confidence**: FADTI's overall superiority over baselines at high missing rates (10-50%), the fundamental mechanism of FBP injecting frequency-domain inductive biases, and the conditional diffusion framework for uncertainty estimation
- **Medium confidence**: The relative performance differences between DFT, STFT, and FrSST transforms across datasets, and the specific architectural choices for decomposition and convolution parameters
- **Low confidence**: The claimed effectiveness on the biological domain (Yeast) due to incomplete baseline comparisons, and generalization to missing rates beyond those tested

## Next Checks

1. Conduct complete baseline comparisons on the Yeast dataset, ensuring all metrics (MAE, RMSE, MAPE, CRPS) are reported for both point-wise and time-wise missing patterns
2. Test FADTI at missing rates of 80% and 90% to evaluate the upper bound of its robustness and determine if performance degradation follows predictable patterns
3. Systematically compare all three Fourier transforms (DFT, STFT, FrSST) on a single dataset to identify clear performance characteristics and determine optimal transform selection criteria for different time series properties