---
ver: rpa2
title: 'Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription
  and Detection'
arxiv_id: '2505.16351'
source_url: https://arxiv.org/abs/2505.16351
tags:
- speech
- dysfluency
- detection
- phoneme
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dysfluent-WFST is a zero-shot decoder that jointly transcribes
  phonemes and detects dysfluency in speech, designed for clinical transcription of
  disordered speech. It uses Weighted Finite-State Transducers (WFSTs) to model pronunciation
  behavior and incorporates a dynamic weighting mechanism to handle dysfluencies like
  repetitions, insertions, and deletions.
---

# Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection

## Quick Facts
- arXiv ID: 2505.16351
- Source URL: https://arxiv.org/abs/2505.16351
- Reference count: 0
- Primary result: Zero-shot decoder that jointly transcribes phonemes and detects dysfluency in speech using WFSTs

## Executive Summary
Dysfluent-WFST is a zero-shot decoder that jointly transcribes phonemes and detects dysfluency in speech, designed for clinical transcription of disordered speech. It uses Weighted Finite-State Transducers (WFSTs) to model pronunciation behavior and incorporates a dynamic weighting mechanism to handle dysfluencies like repetitions, insertions, and deletions. The method requires no additional training and operates seamlessly with upstream encoders like WavLM. Dysfluent-WFST achieves state-of-the-art performance on both simulated and real speech data, significantly improving phonetic error rate and dysfluency detection accuracy compared to traditional approaches.

## Method Summary
Dysfluent-WFST operates as a zero-shot framework that jointly transcribes phonemes and detects dysfluency in speech without additional training. The method takes reference text and raw audio as inputs, uses a pre-trained encoder (Wav2Vec2.0 or WavLM-CTC) to generate emission logits, and constructs a decoding graph by composing CTC topology, a novel Reference FST with dynamic dysfluency weights, and the emission FSA. The WFST decoder finds the shortest path through this graph, and dysfluency types are classified by analyzing state transitions. The approach uses a dynamic weighting function that penalizes long-distance transitions based on phoneme distance, making the decoder sensitive to systematic dysfluency patterns.

## Key Results
- Achieves state-of-the-art performance on simulated and real speech data
- Improves phonetic error rate (PER) and weighted phonetic error rate (WPER) compared to traditional approaches
- Achieves 100% repetition detection accuracy but shows variable performance on insertions (8-50%) and deletions (8-50%)
- Demonstrates extreme sensitivity to emission noise, with WPER increasing from ~10% to 30-74% at just σ=0.1 noise levels

## Why This Works (Mechanism)

### Mechanism 1
Explicit WFST state transitions capture dysfluency patterns that unconstrained decoding misses. The WFST encodes four transition types—horizontal (normal), return (repetition), skip (deletion), and backward (insertion)—via graph topology. When the decoder finds the shortest path, traversal through non-horizontal arcs directly labels the dysfluency type. Core assumption: Dysfluent speech follows systematic non-monotonic alignment patterns tied to the reference phoneme sequence, not random insertions.

### Mechanism 2
Dynamic distance-based weighting improves path selection by penalizing implausible long-distance jumps. The error weight for non-forward transitions is scaled by a Gaussian-like factor inversely proportional to the L1 distance between phoneme positions: err = err0 × (1/√(2π)) × e^(-x²/2). Transitions to nearby positions receive higher probability than distant jumps. Core assumption: Phoneme confusions and dysfluencies exhibit locality—speakers are more likely to repeat or insert adjacent phonemes than distant ones.

### Mechanism 3
Zero-shot operation emerges from offloading phoneme recognition to pretrained encoders while WFST handles structured decoding. Upstream encoder produces emission logits Ξx ∈ R^(T×C). WFST decoder composes CTC topology T, reference FST S, and emission graph to form D = (T ∘ S) ∩ ξx, then extracts the shortest path. No gradient updates occur. Core assumption: Pretrained self-supervised encoders provide sufficiently accurate phoneme posteriors even for disordered speech, despite training on fluent speech.

## Foundational Learning

- Concept: Weighted Finite-State Transducers (WFSTs)
  - Why needed here: WFSTs provide the formal graph structure for encoding phoneme sequences, transition costs, and dysfluency arcs. Understanding states, arcs, input/output labels, and composition is essential to follow the decoder design.
  - Quick check question: Given a simple WFST with states {0,1,2}, arcs labeled (input:output/weight), can you trace the shortest path for input sequence [a,b] and explain how weight accumulation affects path selection?

- Concept: Connectionist Temporal Classification (CTC) Topology
  - Why needed here: The CTC graph defines allowed transitions (including blank tokens and self-loops) that compose with the reference FST. The decoder inherits CTC's handling of variable-length alignment.
  - Quick check question: In CTC, why must blank tokens and repeatable symbols be distinguished in the topology graph? How does this interact with WFST composition?

- Concept: Self-Supervised Speech Encoders (WavLM, wav2vec2)
  - Why needed here: These encoders generate the emission matrix. Understanding their pretraining objectives and phoneme-level output capabilities clarifies what priors the WFST decoder receives.
  - Quick check question: What does an emission matrix Ξx ∈ R^(T×C) represent at each timestep t? How would you interpret a high-entropy vs. low-entropy row in terms of phoneme confidence?

## Architecture Onboarding

- Component map:
  1. Input Layer: Reference text → G2P conversion → phoneme sequence R
  2. Audio Encoder: WavLM or wav2vec2 → emission logits Ξx (T × C)
  3. Graph Construction:
     - Reference FST S from phoneme sequence with dysfluency arcs (return, skip, backward)
     - CTC topology T for allowed frame-level transitions
     - Emission graph ξx with edge weights = log P(pt|X)
  4. Composition: D = (T ∘ S) ∩ ξx (using k2 library)
  5. Decoding: shortest_path(D) → phoneme sequence + state history
  6. Dysfluency Extraction: Post-process state transitions to label repetition/deletion/insertion

- Critical path:
  Reference text → phoneme sequence → reference FST construction (with β-weighted dysfluency arcs) → composition with CTC topology → intersection with emission graph → shortest path computation → state history analysis for dysfluency labels

- Design tradeoffs:
  - β parameter: High β → higher dysfluency tolerance but risk of false positives; low β → stricter normal-path preference but may miss real dysfluencies
  - Graph complexity: Adding all possible dysfluency arcs increases search space; constraining to distance-weighted arcs balances expressiveness and efficiency
  - Encoder choice: WavLM-CTC provides stronger emissions than wav2vec2 (per Table 1), but both remain sensitive to noise (Table 3)
  - Zero-shot vs. fine-tuning: No training simplifies deployment but limits adaptation to specific patient populations

- Failure signatures:
  - High WPER with noisy emissions: WFST decoding degrades sharply even at σ=0.1 noise (WPER jumps from ~10% to 30–74% per Table 3)
  - Poor insertion/deletion detection: 8–50% accuracy vs. 100% for repetition (Table 2); skip arcs often have comparable total weight to expected paths, causing confusion
  - Floating-point underflow at high β: Very large β values cause weights to collapse, flattening the graph to near-linear structure (Figure 3)
  - Out-of-lexicon phonemes: If speaker produces sounds not in phoneme inventory, emission graph cannot represent them accurately

- First 3 experiments:
  1. Reproduce Table 1 baseline on simulated repetition data: Compare WavLM-Greedy vs. WavLM-WFST PER/WPER to validate zero-shot improvement. Use provided GitHub implementation with identical encoder checkpoints.
  2. Sensitivity analysis on β: Sweep β ∈ {0, 2, 4, 6, 8, 10} across simu-rep, simu-del, simu-ins datasets (per Figure 3). Plot PER/WPER curves to identify optimal β per dysfluency type and confirm that high β causes underflow.
  3. Noise robustness test: Add Gaussian noise at σ ∈ {0.1, 1.0, 10.0} to emissions (per Table 3) and measure WPER degradation for WFST vs. greedy decoding. Identify the noise threshold where WFST loses advantage, informing deployment constraints for real clinical audio quality.

## Open Questions the Paper Calls Out

### Open Question 1
Can the decoding logic be modified to better differentiate true dysfluencies from low-probability paths when handling skip edges? The authors state that current performance on insertions and deletions is lower because skip edges may be selected simply to reach the endpoint quickly. A modified weighting algorithm that maintains high detection accuracy for deletions and insertions without increasing false alarms would resolve this.

### Open Question 2
Can the WFST decoder be made differentiable to allow for joint training with the upstream speech encoder? The conclusion suggests exploring joint training of the WFST and encoder as future work, identifying that the current lack of a differentiable mathematical representation complicates training. A proposed differentiable loss function or architecture that allows gradients to flow from the WFST decoding results back to the encoder would resolve this.

### Open Question 3
How can the framework be stabilized to handle noise or uncertainty in the encoder's emission matrix? Table 3 and the discussion highlight that WFST decoding lacks robustness, showing that very small amounts of Gaussian noise in the emission logits cause the Weighted Phonetic Error Rate to skyrocket. Experiments showing stable PER and detection accuracy when subjected to controlled perturbations of the input emissions would resolve this.

## Limitations
- Extreme sensitivity to emission noise, with PER increasing from ~10% to 30-74% at just σ=0.1 noise levels
- Limited effectiveness for non-repetition dysfluencies, with insertion/deletion detection accuracy ranging from 8-50%
- Performance on real clinical data remains uncertain despite strong results on simulated datasets

## Confidence

**High Confidence:** The core WFST architecture and zero-shot decoding mechanism are well-specified and technically sound. The graph composition approach and shortest-path decoding are standard operations with predictable behavior.

**Medium Confidence:** Performance claims on simulated data are reproducible given the detailed implementation description, but translation to real clinical data remains uncertain. The β parameter selection methodology lacks explicit guidance beyond Figure 3's qualitative curves.

**Low Confidence:** Claims about clinical utility and generalizability to diverse speech disorders require validation beyond the simulated datasets. The method's effectiveness for non-repetition dysfluencies (insertions, deletions) appears limited based on the reported accuracy metrics.

## Next Checks
1. **Real Data Validation:** Test the framework on clinical speech datasets (e.g., FluencyBank, UCLASS) to verify whether the ~60% insertion detection accuracy and 8-50% deletion detection accuracy from simulated data holds for real disordered speech patterns.

2. **Noise Robustness Benchmark:** Conduct systematic noise injection experiments (varying SNR, background noise types) to establish the operational envelope where WFST decoding outperforms greedy approaches, given the extreme sensitivity shown in Table 3.

3. **Lexicon Coverage Analysis:** Evaluate performance degradation when speakers produce phonemes outside the reference lexicon or when dysfluencies involve novel sound patterns not captured by the fixed WFST topology.