---
ver: rpa2
title: Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural
  Processes
arxiv_id: '2506.09163'
source_url: https://arxiv.org/abs/2506.09163
tags:
- attention
- test
- points
- bsa-tnp
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Biased Scan Attention Transformer Neural
  Process (BSA-TNP), a novel architecture for scalable spatiotemporal inference that
  addresses the accuracy-scalability tradeoff in Neural Processes. The key innovation
  is combining Kernel Regression Blocks with group-invariant attention biases and
  a memory-efficient Biased Scan Attention mechanism.
---

# Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes

## Quick Facts
- **arXiv ID**: 2506.09163
- **Source URL**: https://arxiv.org/abs/2506.09163
- **Reference count**: 40
- **Primary result**: BSA-TNP achieves state-of-the-art performance on multiple spatiotemporal benchmarks while scaling to over 1M test points with 100K context points in under a minute on a single GPU.

## Executive Summary
This paper introduces the Biased Scan Attention Transformer Neural Process (BSA-TNP), a novel architecture for scalable spatiotemporal inference that addresses the accuracy-scalability tradeoff in Neural Processes. The key innovation is combining Kernel Regression Blocks with group-invariant attention biases and a memory-efficient Biased Scan Attention mechanism. The model achieves state-of-the-art performance on multiple benchmarks including 2D Gaussian Processes, epidemiology (SIR model), climate forecasting (ERA5 temperatures), and air quality prediction, while running inference on over 1M test points with 100K context points in under a minute on a single GPU. BSA-TNP demonstrates superior accuracy, faster training times, and translation invariance properties that enable learning at multiple resolutions simultaneously.

## Method Summary
BSA-TNP is a conditional Neural Process variant that uses Kernel Regression Blocks (KRBlocks) with group-invariant attention biases and Biased Scan Attention (BSA) for memory-efficient computation. The architecture takes a 5-tuple input (observations, spatial coordinates, temporal coordinates, feature IDs, and test indicator) and processes it through separate embedding MLPs per feature group, concatenated and passed through 6 KRBlocks. Each KRBlock applies layer normalization, BSA with RBF-network biases, another layer normalization, and a feed-forward network with residual connections. The bias functions encode translation invariance by depending only on pairwise distances between query and key features. BSA achieves constant memory complexity by chunking keys/values and computing attention scores incrementally using JAX's `lax.scan` with gradient checkpointing.

## Key Results
- Achieves state-of-the-art NLL on 2D Gaussian Process regression, outperforming TNP-D by 0.05-0.15 NLL points
- Demonstrates superior performance on ERA5 climate forecasting with 0.8-1.0°C lower MAE than baseline models
- Enables inference on 1M+ test points with 100K context points in under a minute on a single 24GB GPU
- Shows improved extrapolation capabilities on shifted domains compared to non-invariant baselines

## Why This Works (Mechanism)

### Mechanism 1: Group-Invariant Attention Biases Encode Spatial Priors
Encoding translation invariance into attention biases improves generalization and enables extrapolation beyond training domains. The RBF-network biases $B_{ij}^{(h)} = \sum_{\omega \in \{s,t\}} \sum_{f=1}^{F} a_f \exp(-b_f \|q_\omega^{(i)} - k_\omega^{(j)}\|^2)$ depend only on pairwise distances between query/key features, ensuring G-invariance. This constrains the hypothesis space to functions consistent with process symmetries. The underlying stochastic process exhibits stationarity (translation invariance) or partial stationarity.

### Mechanism 2: Biased Scan Attention Achieves Constant Memory via Incremental Softmax
BSA enables O(n_b) memory complexity while supporting arbitrary bias functions. It chunks keys/values into blocks of size B, computing attention scores and custom bias on-the-fly via `jax.lax.scan`. It tracks running maximum score m(x) and normalization constant ℓ(x) across tiles, rescaling the unnormalized output $\tilde{O}$ incrementally. The bias functions can be computed efficiently in a JIT-compiled kernel without materializing full n×n matrices.

### Mechanism 3: KRBlocks Perform Iterative Kernel Regression on Learned Representations
Stacking KRBlocks refines predictions through successive kernel-smoothed aggregations. Each KRBlock applies attention as kernel regression where queries attend to context keys/values with bias-modified kernels. Learned embeddings (e) update across blocks; raw features (x,s,t) pass via residual connections to bias functions. Weight sharing between Q and K projections reduces parameters.

## Foundational Learning

- **Concept: Neural Processes (NPs)**
  - Why needed: BSA-TNP is a TNP variant; understanding the encode-process-decode paradigm and the distinction between CNPs (deterministic) and LNPs (latent-variable) clarifies design choices.
  - Quick check: Can you explain why CNPs factorize predictions as $\prod_i p(f_t^{(i)} | s_t^{(i)}, r_c)$ and what this implies for uncertainty quantification?

- **Concept: Group Theory / Invariance in Machine Learning**
  - Why needed: The paper formalizes translation invariance via group actions; Theorem 1 connects G-stationarity to G-invariant prediction maps.
  - Quick check: If G is the translation group on $\mathbb{R}^2$, what does it mean for a function f to be G-invariant?

- **Concept: Memory-Efficient Attention (Flash Attention family)**
  - Why needed: BSA builds on Flash Attention's tiling strategy; understanding IO-awareness and incremental softmax normalization is prerequisite to modifying BSA.
  - Quick check: Why does Flash Attention avoid materializing the full n×n attention matrix, and what statistics must be tracked across tiles?

## Architecture Onboarding

- **Component map**: Input 5-tuple → separate embedding MLPs per feature group → concatenate → 6× KRBlocks (each: LayerNorm → BSA → LayerNorm → FFN with residuals) → prediction head MLP → distribution parameters

- **Critical path**: 
  1. Ensure embedding function is G-invariant (exclude s,t from embedding for translation invariance)
  2. Pass raw (s,t) through residual connections to bias functions
  3. Configure bias basis function count per modality
  4. Set chunk size for BSA based on GPU memory

- **Design tradeoffs**:
  - Pure G-invariance vs. embedding locations: Invariant models generalize better but fail when locations encode fixed effects (e.g., city identities); embedding locations alongside bias helps for partially stationary processes
  - Number of pseudo-tokens vs. full context attention: Inducing points add hyperparameters and can over-smooth; BSA avoids this but retains O(n_c²) context complexity
  - Bias complexity: More basis functions increase expressivity but add FLOPs

- **Failure signatures**:
  - Poor extrapolation to shifted domains → check if spatial/temporal features were embedded (violates invariance)
  - OOM on large context sets → reduce BSA chunk size or enable gradient checkpointing
  - Over-smoothed predictions with pseudo-token models → switch to BSA-TNP or increase latent count
  - Slow training vs. inference gap → verify bias functions are JIT-compiled

- **First 3 experiments**:
  1. **2D GP regression with domain shift**: Train on $[-0.5, 0.5]^2$, evaluate on shifted $[9.5, 10.5]^2$. Expect BSA-TNP to maintain NLL; TNP-D should fail. Validates G-invariance.
  2. **Ablation on bias vs. embedding**: Compare three variants—RBF bias only, location embedding only, both—on ERA5 held-out region. Quantify contribution of each.
  3. **Scalability stress test**: Profile memory and runtime with context sizes {1K, 10K, 100K} and test sizes {10K, 100K, 1M}. Confirm sub-minute inference at largest scale on 24GB GPU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BSA-TNP be extended to handle dense observation scenarios (e.g., video prediction) while maintaining computational efficiency?
- Basis: The paper explicitly identifies the quadratic complexity in context points as a limitation for dense-observation tasks and suggests incorporating frame summarizations from models like I-JEPA.
- Why unresolved: The authors identify this limitation but do not implement the proposed solution; the quadratic complexity remains a bottleneck for dense-observation tasks.
- What evidence would resolve it: An extension achieving sub-quadratic memory for dense context sets, benchmarked on video prediction tasks with comparison to current memory requirements.

### Open Question 2
- Question: How does the performance of G-invariant attention bias degrade as processes deviate from strict stationarity?
- Basis: The paper notes that while G-invariance is most effective for stationary processes, these biases can still prove useful when a process is only partially stationary.
- Why unresolved: The paper only briefly tests partially stationary processes (Beijing air quality) without systematic analysis of the stationarity-bias usefulness relationship.
- What evidence would resolve it: Controlled experiments varying degrees of non-stationarity (e.g., GP kernels with trend components, non-stationary lengthscales) with quantitative bounds on when translation-invariant bias becomes detrimental.

### Open Question 3
- Question: Can BSA-TNP's architectural innovations transfer to latent variable Neural Process formulations?
- Basis: The paper notes LNPs theoretically enable them to better encode global stochastic behavior, although in practice they tend to perform worse than their conditional counterparts.
- Why unresolved: BSA-TNP is developed only as a conditional NP; whether KRBlocks, BSA, and G-invariant biases benefit latent variable formulations remains untested.
- What evidence would resolve it: Implementing a latent BSA-TNP variant and comparing against BSA-TNP and existing LNP baselines on tasks requiring multi-modal posterior distributions.

## Limitations

- **Context Complexity Wall**: While BSA achieves constant memory for attention computation, the overall model retains O(n_c²) complexity in the number of context points, limiting its applicability to dense observation scenarios like video prediction.

- **Bias Function Expressivity**: The fixed RBF basis functions may be insufficient for complex spatiotemporal dependencies, and the paper does not explore learned bias functions or systematically characterize the relationship between basis function count and task characteristics.

- **Evaluation Scope**: The empirical validation focuses on specific benchmarks without systematic ablation studies on the bias mechanisms, kernel regression block depth, or group invariance properties that would strengthen the theoretical claims.

## Confidence

- **High Confidence**: Claims about memory efficiency and scalability improvements (sub-minute inference on 1M test points) are well-supported by the BSA algorithm description and architecture details.

- **Medium Confidence**: Performance claims on benchmark tasks are credible given the detailed experimental setup, but could benefit from additional baselines and ablation studies.

- **Low Confidence**: Claims about the general superiority of group-invariant biases over location embeddings lack systematic validation across diverse spatiotemporal processes with varying degrees of stationarity.

## Next Checks

1. **Invariance Property Validation**: Design a controlled experiment where BSA-TNP is trained on multiple spatial translations of the same underlying process. Measure whether the model's predictions remain consistent under spatial shifts, directly validating the G-invariance claims.

2. **Bias Function Sensitivity**: Conduct systematic ablation studies varying the number and type of basis functions in the RBF bias network. Compare performance against learned bias functions to quantify the trade-off between inductive bias and expressivity.

3. **Context Complexity Stress Test**: Evaluate BSA-TNP on a dense spatiotemporal task (e.g., video frame prediction) with varying context sizes to quantify the O(n_c²) complexity wall. Compare against inducing-point methods to understand the practical limits of the approach.