---
ver: rpa2
title: A Systematic Study of Compression Ordering for Large Language Models
arxiv_id: '2511.19495'
source_url: https://arxiv.org/abs/2511.19495
tags:
- quantization
- pruning
- compression
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates the impact of different orderings\
  \ of three compression techniques\u2014knowledge distillation, structured pruning,\
  \ and low-bit quantization\u2014on large language models. Using the Qwen2.5-3B model,\
  \ the authors test multiple compression pipelines and find that the sequence Pruning\u2192\
  Knowledge Distillation\u2192Quantization (P-KD-Q) achieves the best balance, yielding\
  \ a 3.68\xD7 compression ratio while preserving strong instruction-following and\
  \ language understanding capabilities."
---

# A Systematic Study of Compression Ordering for Large Language Models

## Quick Facts
- arXiv ID: 2511.19495
- Source URL: https://arxiv.org/abs/2511.19495
- Reference count: 40
- Primary result: P-KD-Q pipeline achieves 3.68× compression while preserving strong instruction-following and language understanding

## Executive Summary
This study systematically evaluates how the ordering of three compression techniques—knowledge distillation, structured pruning, and low-bit quantization—affects large language model performance. Using Qwen2.5-3B as a testbed, the authors compare multiple compression pipelines and find that the sequence Pruning→Knowledge Distillation→Quantization (P-KD-Q) achieves the best balance of compression ratio and capability preservation. The research reveals that early application of quantization causes severe performance degradation due to irreversible information loss that impairs subsequent training, establishing that ordering-aware design is critical for effective model compression.

## Method Summary
The study employs a systematic evaluation framework using the Qwen2.5-3B model to test different compression pipelines. The authors apply three techniques—knowledge distillation, structured pruning, and low-bit quantization—in various sequences across multiple configurations. Performance is measured across instruction-following and language understanding tasks using standardized benchmarks. The methodology includes controlled experiments where quantization is applied at different pipeline stages to isolate its impact on model capability retention.

## Key Results
- P-KD-Q pipeline achieves 3.68× compression ratio with preserved strong performance
- Early quantization leads to irreversible information loss that severely degrades subsequent training
- Sequential ordering of compression techniques significantly impacts final model capability retention

## Why This Works (Mechanism)
The mechanism underlying successful compression ordering centers on information preservation during the pipeline. When pruning removes less critical weights early, it creates a smaller, more focused model architecture without severe information degradation. Knowledge distillation then transfers knowledge from the larger model to this pruned structure while the model parameters remain in higher precision, enabling effective gradient updates. Only after these processes does quantization compress weights to lower bits, minimizing the irreversible information loss that would otherwise corrupt the fine-tuning process. Early quantization disrupts this by introducing quantization noise that propagates through subsequent training stages, creating a compounding negative effect on model capability.

## Foundational Learning
- **Knowledge Distillation**: Teacher-student training where a smaller model learns to mimic a larger model's behavior; needed to transfer capabilities without full parameter count, quick check: verify loss decreases during KD training.
- **Structured Pruning**: Removal of entire weight matrices or neurons based on importance scores; needed to reduce model size while maintaining architectural coherence, quick check: confirm FLOPs reduction matches target ratio.
- **Low-bit Quantization**: Converting weights from higher precision (FP32) to lower precision (INT8/4); needed for maximum memory compression, quick check: measure activation distribution before/after quantization.
- **Compression Pipeline Design**: Strategic ordering of multiple compression techniques; needed to maximize efficiency while minimizing capability loss, quick check: track performance degradation across pipeline stages.
- **Information Irreversibility**: Concept that certain transformations (like aggressive quantization) permanently degrade information content; needed to explain why technique ordering matters, quick check: compare fine-tuning recovery with/without prior quantization.

## Architecture Onboarding
- **Component Map**: Full Model → Pruning → Knowledge Distillation → Quantization → Compressed Model
- **Critical Path**: Pruning must precede distillation to ensure efficient knowledge transfer; quantization must follow both to avoid irreversible information loss
- **Design Tradeoffs**: Earlier pruning enables better distillation but may sacrifice some representational capacity; earlier quantization maximizes compression but irreparably damages fine-tuning capability
- **Failure Signatures**: Early quantization manifests as plateaued or degrading performance during subsequent fine-tuning, regardless of learning rate tuning
- **First Experiments**: 1) Test P-KD-Q vs Q-P-KD ordering on simple classification task; 2) Measure parameter recovery rate after each stage; 3) Analyze activation pattern changes post-pruning vs post-quantization

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to single model (Qwen2.5-3B), limiting generalizability across different model architectures and sizes
- Does not systematically explore the full combinatorial space of all possible compression orderings
- Mechanistic explanation for quantization-induced degradation could benefit from deeper analysis of noise propagation

## Confidence
- High confidence: Relative performance ranking of compression pipelines, particularly P-KD-Q superiority
- Medium confidence: Claim about "irreversible information loss" as primary mechanism for early quantization degradation
- Low confidence: Generalizability of specific compression ratios and absolute performance metrics to other model families

## Next Checks
1. Replicate pipeline comparisons using diverse model architectures (Llama, Mistral) and sizes (1B, 7B, 13B) to assess ordering effects across model families
2. Systematically isolate quantization noise contribution by testing intermediate precision levels (8-bit vs 4-bit) at different pipeline stages
3. Extend evaluation to specialized domains including mathematical reasoning, code generation, and domain-specific benchmarks to determine if pipeline preferences vary by task type