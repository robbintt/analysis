---
ver: rpa2
title: Partially Observable Gaussian Process Network and Doubly Stochastic Variational
  Inference
arxiv_id: '2502.13905'
source_url: https://arxiv.org/abs/2502.13905
tags:
- process
- gaussian
- pogpn
- node
- observed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Partially Observable Gaussian Process
  Network (POGPN) to model real-world process networks with intermediate observations
  that are indirect, noisy, and incomplete. The core idea is to model a joint distribution
  of latent functions of subprocesses and make inferences using observations from
  all subprocesses, incorporating observation lenses (observation likelihoods) into
  the well-established inference method of deep Gaussian processes.
---

# Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference

## Quick Facts
- arXiv ID: 2502.13905
- Source URL: https://arxiv.org/abs/2502.13905
- Authors: Saksham Kiroriwal; Julius Pfrommer; Jürgen Beyerer
- Reference count: 11
- Key outcome: POGPN achieves MAE of 0.3989 on Jura dataset and SMSE of 0.24, mean log loss of 1.04 on EEG dataset, outperforming existing methods

## Executive Summary
This paper introduces the Partially Observable Gaussian Process Network (POGPN) to model real-world process networks with intermediate observations that are indirect, noisy, and incomplete. The core idea is to model a joint distribution of latent functions of subprocesses and make inferences using observations from all subprocesses, incorporating observation lenses (observation likelihoods) into the well-established inference method of deep Gaussian processes. The authors propose two training methods for POGPN to make inferences on the whole network using node observations.

## Method Summary
POGPN models each node in a directed acyclic graph as a Gaussian process whose latent function serves as input to child nodes. Unlike standard GPNs that feed noisy observations directly to children, POGPN uses observation likelihoods (lenses) to connect latent functions to observed data. The model employs doubly stochastic variational inference with inducing points to scale inference and handle non-Gaussian likelihoods. Two training strategies are proposed: ancestor-wise (updating the entire network simultaneously) and node-wise (updating nodes sequentially). The Predictive Log Likelihood (PLL) objective is used alongside the standard ELBO, with PLL providing a tighter bound through logsumexp over Monte Carlo samples.

## Key Results
- On Jura dataset, POGPN achieves MAE of 0.3989, outperforming existing models
- On EEG dataset, POGPN achieves SMSE of 0.24 and mean log loss of 1.04
- POGPN-AL (PLL) consistently outperforms POGPN-AL (ELBO) across benchmark problems
- Incorporating partial observations during training and inference improves predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling latent process dynamics from noisy observations via explicit observation lenses allows correct uncertainty propagation through the network
- **Mechanism:** POGPN models latent function distributions for each node, with child nodes receiving distributions of parent latent functions as input while observations are treated as conditionally independent emissions
- **Core assumption:** True states are hidden and observations are imperfect, potentially non-Gaussian snapshots of these states
- **Evidence anchors:** [Abstract] "incorporates observation lenses (observation likelihoods) into the well-established inference method of deep Gaussian processes"
- **Break condition:** Fails if observation noise is negligible or observation likelihood is misspecified

### Mechanism 2
- **Claim:** Variational inference over joint latent function distribution handles non-Gaussian likelihoods and scales to larger datasets
- **Mechanism:** Uses inducing points for sparse GP approximation and Monte Carlo sampling for gradient estimation through the network
- **Core assumption:** Inducing points provide sufficient statistic and Monte Carlo samples provide unbiased likelihood estimates
- **Evidence anchors:** [Section 3.2] "SVGP approximates the exact posterior with a variational posterior... can scale and be used for non-Gaussian likelihoods"
- **Break condition:** Fails with too few inducing points or local minima due to stochastic gradients

### Mechanism 3
- **Claim:** PLL objective yields better predictive performance than ELBO by providing tighter bound on log expected marginal likelihood
- **Mechanism:** PLL maximizes log of expected probability using logsumexp over Monte Carlo samples
- **Core assumption:** Logsumexp approximation over finite samples captures expected likelihood sufficiently
- **Evidence anchors:** [Section 5] POGPN-AL (PLL) consistently outperforms POGPN-AL (ELBO) in experiments
- **Break condition:** Fails with insufficient MC samples causing high variance

## Foundational Learning

- **Concept:** Gaussian Processes (GPs) & Kernels
  - **Why needed here:** POGPN is built entirely on GPs as building blocks for nodes
  - **Quick check question:** How does the choice of kernel (e.g., RBF vs. Matern) affect smoothness assumptions of latent functions?

- **Concept:** Variational Inference & Inducing Points
  - **Why needed here:** Standard GPs are O(N³); SVGP with inducing points scales inference
  - **Quick check question:** In SVGP, what is the trade-off between increasing inducing points I versus computational cost?

- **Concept:** Directed Acyclic Graphs (DAGs) & Topological Ordering
  - **Why needed here:** POGPN is a DAG; topological sorting ensures correct node computation order
  - **Quick check question:** Why can't POGPN handle cyclic dependencies in its current formulation?

## Architecture Onboarding

- **Component map:** Nodes (W) -> Parent latent outputs + parameters -> Latent function f^(w) -> Observation likelihood p(y|f) -> Observed data y
- **Critical path:**
  1. Forward Pass: Sample latent functions f^(w) in topological order, using parent samples as inputs
  2. Observation: Pass latent samples through likelihood to get predicted y
  3. Loss Calculation: Compare predicted y with ground truth using PLL or ELBO
  4. Backward Pass: Update inducing points and kernel hyperparameters
- **Design tradeoffs:**
  - Ancestor-wise vs. Node-wise Training: Global optimum but expensive vs. cheaper but may miss global interactions
  - ELBO vs. PLL: Better predictive accuracy vs. more standard but potentially less precise
- **Failure signatures:**
  - Mode Collapse: Predictions default to mean; uncertainty balloons
  - Numerical Instability: NaNs during loss computation
  - Slow Convergence: Stale gradients in node-wise training
- **First 3 experiments:**
  1. Jura Dataset: Replicate multi-modal setup predicting Cd from Zn, Ni, Rock, Land
  2. Synthetic Experiment: Reconstruct toy example with switch (f2 < 1.5) to verify discrete jump capture
  3. EEG Dataset: Compare POGPN-AL vs. POGPN-NL on time-series prediction to determine convergence speed

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity scales poorly with network depth and width
- PLL objective lacks theoretical justification beyond empirical observation
- Limited experiments on networks with only 2-4 nodes
- Numerical stability of logsumexp for PLL questionable for higher-dimensional problems

## Confidence

- **High Confidence:** Decoupling latent processes from noisy observations via explicit likelihoods is well-established in GP literature
- **Medium Confidence:** Predictive performance improvements on Jura and EEG datasets are convincing but limited to small-scale problems
- **Low Confidence:** Claim that PLL consistently outperforms ELBO across diverse scenarios is weakly supported

## Next Checks
1. **Scalability Test:** Evaluate POGPN on synthetic networks with 10+ nodes and varying DAG structures to assess computational feasibility and predictive accuracy degradation
2. **Hyperparameter Sensitivity:** Conduct ablation study varying β, MC samples, and inducing point numbers across all datasets to quantify impact on PLL vs. ELBO performance
3. **Robustness to Misspecification:** Test POGPN with intentionally misspecified observation likelihoods to evaluate resilience to model assumption violations