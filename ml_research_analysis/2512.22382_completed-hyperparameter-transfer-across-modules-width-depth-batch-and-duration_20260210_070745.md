---
ver: rpa2
title: Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration
arxiv_id: '2512.22382'
source_url: https://arxiv.org/abs/2512.22382
tags:
- learning
- transfer
- scaling
- per-module
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that per-module hyperparameters can be optimised
  at a small model scale and then transferred to larger models across multiple scaling
  dimensions. The authors introduce Complete(d)P, an extension of CompleteP that supports
  transfer across width, depth, batch size, and training duration, and apply it to
  optimise hyperparameters such as learning rates, AdamW parameters, weight decay,
  and initialisation scales on a per-module basis.
---

# Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration

## Quick Facts
- arXiv ID: 2512.22382
- Source URL: https://arxiv.org/abs/2512.22382
- Reference count: 40
- Primary result: Demonstrated 1.32× speedup transferring per-module hyperparameters from small to 600× larger models across width, depth, batch size, and duration

## Executive Summary
This work introduces Complete(d)P, an extension of the CompleteP framework that enables transfer of per-module hyperparameters across multiple scaling dimensions including width, depth, batch size, and training duration. The authors show that hyperparameters optimized for individual modules at small model scales can be successfully transferred to much larger models, achieving significant computational speedups while maintaining or improving final loss and downstream performance. The approach provides practical guidelines for navigating the high-dimensional per-module hyperparameter space using trust-region random search.

## Method Summary
Complete(d)P extends CompleteP by parameterizing hyperparameters to enable transfer across width, depth, batch size, and duration. The framework optimizes per-module hyperparameters (learning rates, AdamW parameters, weight decay, and initialization scales) at small scales and transfers them to larger models using specific scaling rules. The authors employ trust-region random search to navigate the high-dimensional per-module hyperparameter landscape and demonstrate successful transfer to models 600× larger in compute.

## Key Results
- Achieved 1.32× speedup when transferring per-module hyperparameters to a model 600× larger in compute
- Maintained improved final loss and downstream performance compared to baseline training
- Demonstrated successful transfer across width, depth, batch size, and training duration dimensions
- Provided practical guidelines for optimizing per-module hyperparameters using trust-region random search

## Why This Works (Mechanism)
The approach works by parameterizing hyperparameters in a way that respects the scaling relationships between different model dimensions. By optimizing hyperparameters at small scales and transferring them using theoretically-grounded scaling rules, the method captures the essential relationships between module configurations and their optimal hyperparameters across scales. The trust-region random search provides an efficient way to explore the high-dimensional per-module hyperparameter space.

## Foundational Learning
- **Per-module hyperparameter optimization**: Understanding how to optimize hyperparameters for individual model modules rather than treating the model as a monolithic entity is crucial for achieving efficient transfers across scales
- **Scaling rules for hyperparameters**: Knowledge of how hyperparameters should scale with width, depth, batch size, and duration is essential for successful transfer
- **Trust-region random search**: Familiarity with this optimization technique helps understand how to efficiently explore high-dimensional hyperparameter spaces
- **CompleteP framework**: Understanding the original CompleteP methodology provides context for how Complete(d)P extends it
- **Transformer architecture specifics**: Knowledge of decoder-only transformer architecture helps understand the module-level parameterization

Quick check: Verify understanding of how scaling rules are derived and applied across different dimensions

## Architecture Onboarding
**Component map:** Input -> Embedding modules -> Attention modules -> MLP modules -> Output modules -> Loss calculation
**Critical path:** Data flows through embedding, attention, and MLP modules in sequence, with each module having its own set of hyperparameters that are optimized and transferred
**Design tradeoffs:** Per-module optimization provides fine-grained control but increases hyperparameter space complexity; trust-region random search balances exploration efficiency with computational cost
**Failure signatures:** Poor transfer performance may indicate violated scaling assumptions or inadequate exploration of the hyperparameter space
**First experiments:** 1) Validate scaling rules on width transfer, 2) Test depth transfer with fixed width, 3) Evaluate batch size scaling effects on learning rates

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the Complete(d)P parameterization and per-module hyperparameter transfer hold for architectures other than decoder-only transformers or datasets other than RedPajama?
- Basis in paper: [explicit] The authors state in the Limitations section that the approach "should ideally be verified in other settings" beyond the single autoregressive transformer setup evaluated
- Why unresolved: The study restricts its empirical validation to one specific model type and dataset, leaving the generality of the transfer rules unproven for other domains
- What evidence would resolve it: Successful replication of the transfer performance using Complete(d)P on distinct architectures (e.g., Vision Transformers) or different data modalities

### Open Question 2
- Question: How does the iso-horizon token scaling rule interact with learning rate schedules that are allowed to vary dynamically with the training horizon?
- Basis in paper: [explicit] The authors note they only considered a fixed cosine decay schedule and suggest the iso-horizon rule "might be sub-optimal once we allow the schedule to change with the token horizon"
- Why unresolved: The current theory assumes a fixed schedule; it is unknown if the peak learning rate scaling remains valid when the schedule shape itself is a variable dependent on duration
- What evidence would resolve it: Empirical results showing whether the current square-root scaling law holds or requires modification when jointly optimizing schedules and horizons

### Open Question 3
- Question: Does the observed diminishing speed-up from per-module hyperparameters at larger scales stem from imperfect transfer in the non-asymptotic regime or an intrinsic asymptotic property of infinite-width models?
- Basis in paper: [explicit] The authors observe that speed-ups diminish slowly with scale and explicitly ask future work to find "computationally feasible ways of answering that question"
- Why unresolved: It is computationally prohibitive to run the necessary ablations at massive scales to distinguish between a failure of the transfer parameterization and a fundamental limit of the optimization landscape
- What evidence would resolve it: Theoretical analysis of the infinite-width/depth limit or empirical measurements of per-module HP sensitivity at significantly larger compute scales

## Limitations
- Limited validation to single autoregressive transformer architecture and RedPajama dataset
- Does not extensively explore transfers between structurally dissimilar modules
- Trust-region random search may not guarantee globally optimal hyperparameters
- Sample efficiency relative to alternative hyperparameter optimization methods not benchmarked

## Confidence
- High confidence: The core methodology of Complete(d)P for transferring hyperparameters across width, depth, batch size, and duration is sound and well-validated within the tested regime
- Medium confidence: The claimed 1.32× speedup and improved final loss are valid for the specific Llama-like architecture tested, but generalizability to other architectures requires further validation
- Medium confidence: The practical guidelines for navigating per-module hyperparameter spaces using trust-region random search are useful but may not be optimal for all model families or optimization landscapes

## Next Checks
1. Test hyperparameter transfer across architecturally distinct modules (e.g., from attention to MLP modules) to assess cross-module generalization limits
2. Validate the approach on non-Llama architectures and different model families (e.g., transformers with alternative designs) to establish broader applicability
3. Benchmark trust-region random search against alternative hyperparameter optimization methods (e.g., Bayesian optimization, Hyperband) to quantify sample efficiency and final performance differences