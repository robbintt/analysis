---
ver: rpa2
title: A Computational Approach to Language Contact -- A Case Study of Persian
arxiv_id: '2601.20592'
source_url: https://arxiv.org/abs/2601.20592
tags:
- language
- persian
- languages
- contact
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates structural traces of language contact in
  the internal representations of a monolingual Persian language model, ParsBERT.
  Using probing methods, the research measures how linguistic features from contact
  languages (Arabic, English, French, German, Hindi, Japanese, Russian, Turkish) are
  encoded in ParsBERT's intermediate layers.
---

# A Computational Approach to Language Contact -- A Case Study of Persian

## Quick Facts
- arXiv ID: 2601.20592
- Source URL: https://arxiv.org/abs/2601.20592
- Authors: Ali Basirat; Danial Namazifard; Navid Baradaran Hemmati
- Reference count: 24
- This study investigates structural traces of language contact in the internal representations of a monolingual Persian language model, ParsBERT.

## Executive Summary
This study investigates structural traces of language contact in the internal representations of a monolingual Persian language model, ParsBERT. Using probing methods, the research measures how linguistic features from contact languages (Arabic, English, French, German, Hindi, Japanese, Russian, Turkish) are encoded in ParsBERT's intermediate layers. Results show that universal syntactic categories like UPOS tags are largely insensitive to contact effects, while morphological features like CASE and GENDER are strongly shaped by language-specific structure. Specifically, ParsBERT poorly captures rich inflectional case systems but performs better on analytic case marking, reflecting Persian's own morphosyntactic patterns. The model also encodes gender information more effectively for gender-neutral languages and two-gender systems than for three-gender languages. Attribution analysis reveals that contact-related information is distributed across representations rather than localized in dedicated neurons, with notable selectivity for Japanese due to orthographic differences. These findings suggest that monolingual language models implicitly encode selective traces of language contact, constrained by the structural properties of the training language.

## Method Summary
The study uses ParsBERT, a monolingual Persian language model, to probe for traces of historical language contact. Researchers extracted intermediate representations from all 12 layers of ParsBERT for token-level data from 8 parallel Universal Dependencies treebanks (Arabic, English, French, German, Hindi, Japanese, Russian, Turkish). They applied two analysis methods: (1) variational usable information probes to measure how well linguistic features (language ID, UPOS, CASE, GENDER) can be decoded from representations, and (2) LAPE attribution analysis to identify neurons selectively responsive to specific languages. The study reports normalized scores in range [0,1] for probe performance and neuron counts for attribution patterns across layers.

## Key Results
- Universal POS tags (UPOS) show minimal sensitivity to language contact effects across all tested languages
- CASE feature encoding is poor for languages with rich inflectional case systems but better for languages with analytic case marking, mirroring Persian's morphosyntactic patterns
- ParsBERT encodes gender information more effectively for gender-neutral languages and two-gender systems than for three-gender languages
- Contact-related information is distributed across representations rather than localized in dedicated neurons, with Japanese showing notable selectivity due to orthographic differences

## Why This Works (Mechanism)
The study demonstrates that monolingual language models implicitly encode structural traces of historical language contact through their internal representations. The mechanism works because language models trained on corpora containing contact-induced linguistic variation develop representations that reflect both the target language's structure and the typological properties of contact languages. This encoding is selective and constrained by the training language's own morphosyntactic patterns, with models better representing features structurally similar to their native system.

## Foundational Learning
- **Language Contact**: Why needed: To understand how languages influence each other structurally when in contact. Quick check: Can identify examples of contact-induced changes in real-world languages.
- **Probing Methods**: Why needed: To measure what linguistic information is encoded in neural representations without modifying the model. Quick check: Can explain difference between diagnostic classifiers and information-theoretic probes.
- **Variational Usable Information**: Why needed: To quantify the amount of information about one variable that can be extracted from another representation. Quick check: Can compute normalized usable information from contingency tables.
- **Morphosyntactic Features**: Why needed: To understand how grammatical properties like case and gender vary across languages. Quick check: Can classify languages by their case marking systems (synthetic vs. analytic).
- **Attribution Analysis**: Why needed: To identify which model components are responsible for encoding specific linguistic information. Quick check: Can explain how activation entropy identifies selective neurons.

## Architecture Onboarding
- **Component Map**: PUD Treebanks (8 languages) -> ParsBERT (12 layers, 768-dim) -> Representation Extraction -> Variational Probe / LAPE Attribution -> Layer-wise Analysis
- **Critical Path**: Token representation extraction from ParsBERT layers → Probe training for usable information estimation → LAPE computation for neuron attribution → Layer-wise aggregation and interpretation
- **Design Tradeoffs**: Monolingual vs. multilingual models (confounding vs. specificity), probe vs. attribution methods (quantitative vs. qualitative insights), layer-wise vs. holistic analysis (granularity vs. simplicity)
- **Failure Signatures**: Low probe scores across all features suggest incorrect representation extraction or undertrained probes; zero neurons in LAPE attribution suggests overly strict thresholds
- **First Experiments**:
  1. Verify representation extraction by checking layer dimensions and token alignment across languages
  2. Test probe sensitivity by varying training hyperparameters and observing score stability
  3. Validate LAPE thresholding by comparing results across multiple percentile cutoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Probing methodology lacks specified training hyperparameters (learning rate, epochs, batch size), making exact replication impossible
- LAPE attribution analysis uses unspecified "lowest percentile" thresholds without defining the exact cutoff or exclusion criteria
- Study reports only single-run results with no variance estimates, preventing assessment of result stability
- Corpus size (1000 sentences per language) may be insufficient for robust probing of fine-grained morphological features

## Confidence
- **High confidence**: The pattern that universal syntactic features show minimal contact sensitivity while morphological features exhibit stronger language-specific effects
- **Medium confidence**: Specific quantitative rankings of languages and the claim about gender system complexity affecting encoding
- **Low confidence**: Attribution analysis results and the claim about distributed vs. localized encoding due to arbitrary threshold choices

## Next Checks
1. Systematically vary probe training hyperparameters (learning rate, epochs, batch size) and LAPE percentile thresholds (1%, 5%, 10%) to establish stability ranges for reported patterns
2. Analyze PUD treebanks for potential confounds including word frequency distributions and representation of contact vocabulary
3. Apply identical probing methodology to multilingual Persian models (e.g., mBERT) to distinguish genuine contact traces from general typological biases