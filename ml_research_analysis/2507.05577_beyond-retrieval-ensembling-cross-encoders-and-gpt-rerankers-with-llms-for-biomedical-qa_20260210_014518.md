---
ver: rpa2
title: 'Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for
  Biomedical QA'
arxiv_id: '2507.05577'
source_url: https://arxiv.org/abs/2507.05577
tags:
- documents
- retrieval
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a dense retrieval and LLM-based answer generation
  system for biomedical QA. It uses a local vector database with bge-large-en embeddings
  for scalable semantic search over PubMed, followed by an ensemble of finetuned cross-encoders
  and GPT-based re-rankers to maximize MAP@10.
---

# Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA

## Quick Facts
- **arXiv ID**: 2507.05577
- **Source URL**: https://arxiv.org/abs/2507.05577
- **Reference count**: 31
- **Primary result**: Dense retrieval + LLM re-ranking achieved MAP@10 of 0.1581 (rank 10), with answer generation maF1 of 0.95 for yes/no, MRR of 0.64 for factoid, and mF1 of 0.63 for list questions.

## Executive Summary
This work presents a dense retrieval and LLM-based answer generation system for biomedical QA. It uses a local vector database with bge-large-en embeddings for scalable semantic search over PubMed, followed by an ensemble of finetuned cross-encoders and GPT-based re-rankers to maximize MAP@10. For answer generation, few-shot prompting with instruction-tuned LLMs (GPT-4o-turbo, Mistral-7B) was used across question types. The system achieved strong performance in both retrieval and answer generation, with prompt style notably influencing list and ideal answer quality.

## Method Summary
The system follows a multi-stage pipeline: first, biomedical abstracts are embedded using bge-large-en and stored in a local Qdrant vector database. For retrieval, queries are embedded and top-1k documents are retrieved via ANN search. These candidates are then re-ranked using a finetuned cross-encoder (ms-marco-MiniLM-L12) and a GPT-4o listwise re-ranker, with scores combined in a 1:7 weighted ensemble. Answer generation uses few-shot prompts with GPT-4o-turbo or Mistral-7B, with three prompt styles tested; Style 3, which instructs models to use direct passage phrases, showed the best performance on list and ideal answers.

## Key Results
- Dense retrieval significantly outperformed PubMed API, with Recall@10 of 0.23 vs. 0.10 and Recall@1k of 0.56 vs. 0.23.
- Re-ranking pipeline improved MAP@10 from 0.2156 (top-10 retrieval) to 0.4551 using ensemble of cross-encoder and GPT re-ranker.
- Answer generation: maF1 of 0.95 for yes/no, MRR of 0.64 for factoid, mF1 of 0.63 for list, and ROUGE-SU4 of 0.51 for ideal answers.

## Why This Works (Mechanism)

### Mechanism 1
Dense vector retrieval with bi-encoders captures semantic relationships that keyword-based search misses, improving document recall substantially. The bge-large-en model encodes queries and documents into 1024-dimensional vectors independently. Cosine similarity in this embedding space surfaces semantically related content even without lexical overlap, which is critical in biomedical text where synonymy and jargon abound. Core assumption: Semantic similarity in embedding space correlates with relevance to expert-annotated gold documents. Evidence anchors: Abstract states dense retrieval significantly outperformed PubMed API; Table 1 shows Recall@10 of 0.23 vs. 0.10 for vector DB vs. PubMed API. Break condition: If embedding model is not domain-adapted, semantic drift may reduce precision; biomedical jargon may map poorly without fine-tuning.

### Mechanism 2
A two-stage re-ranking pipeline—bi-encoder for recall, cross-encoder and LLM for precision—improves ranking quality by exploiting query-document interactions that bi-encoders cannot capture. Bi-encoders retrieve top-1k candidates efficiently. Cross-encoders jointly encode query-document pairs, learning finer relevance distinctions. GPT-4o then performs listwise re-ranking on top-30, leveraging reasoning to reorder. A weighted ensemble (weights 1:7 for cross-encoder vs. GPT re-ranker) combines signals. Core assumption: Cross-encoder fine-tuning on domain-specific hard negatives teaches the model to distinguish "somewhat relevant" from "truly relevant." Evidence anchors: Abstract states ensemble of finetuned cross-encoders and GPT-based re-rankers to maximize MAP@10; section notes retrieving 1k followed by re-ranking via finetuned cross encoder achieves an MAP@10 of 0.4337, applying ensemble re-rankers takes us all the way to 0.4551. Break condition: If computational budget limits LLM re-ranking to fewer candidates, or if cross-encoder is under-trained, ensemble gains diminish.

### Mechanism 3
Prompt style—specifically instructing models to use direct passage phrases without paraphrase—improves list and ideal answer quality as measured by overlap-based metrics. Style 3 templates explicitly instruct the model to "prefer answers of only combining direct phrases or sentences from the Passage" and "avoid paraphrasing." This increases n-gram overlap with gold references, boosting ROUGE scores. Core assumption: ROUGE-SU4 improvements reflect better answer quality, not just surface-level gaming of the metric. Evidence anchors: Abstract states prompt style notably influenced list and ideal answer quality; section explains Style 3 explicitly instructed the model to "only combining direct phrases or sentences from the Passage," which increased overlap with the gold snippets and thus boosted ROUGE scores. Break condition: If evaluation shifts to semantic similarity metrics, the advantage of extractive phrasing may decrease or invert.

## Foundational Learning

- **Concept**: Bi-encoder vs. Cross-encoder architectures
  - Why needed here: The system relies on bi-encoders for scalable retrieval and cross-encoders for precise re-ranking. Understanding their trade-offs is essential for debugging pipeline stages.
  - Quick check question: Which architecture independently embeds queries and documents, and which jointly processes pairs?

- **Concept**: Mean Average Precision (MAP) and Reciprocal Rank metrics
  - Why needed here: MAP@10 drives retrieval optimization; MRR determines factoid answer ranking. Misunderstanding these leads to wrong objective functions.
  - Quick check question: Why does MAP@10 penalize a relevant document ranked 7th more than one ranked 2nd?

- **Concept**: Few-shot prompting with instruction-tuned LLMs
  - Why needed here: Answer generation uses 1-shot and 10-shot prompts with specific formatting templates. Knowing how in-context learning works helps iterate on prompt design.
  - Quick check question: What is the difference between providing 1-shot vs. 10-shot examples, and when might more examples not help?

## Architecture Onboarding

- **Component map**: PubMed 2025 Baseline → filter empty abstracts, deduplicate → ~34M records → bge-large-en embeddings (1024-dim) → local Qdrant with HNSW → Query embedding → ANN search → top-1k candidates → Finetuned ms-marco-MiniLM-L12 (cross-encoder) → top-30 → GPT-4o listwise re-ranker → weighted ensemble → top-10 → Few-shot prompt (Style 1/2/3) → GPT-4o-turbo or Mistral-7B-Instruct-v0.3 → exact/ideal answers

- **Critical path**: Retrieval recall determines upper bound of answer quality. If Recall@1k is low, re-ranking cannot recover missing documents. Prioritize embedding quality and index coverage first.

- **Design tradeoffs**:
  - **Vector DB hosting**: Pinecone (managed, costly) vs. Qdrant (local, more ops overhead)
  - **Re-ranking depth**: 1k→top-10 (faster) vs. 2k→top-10 (higher recall, more compute)
  - **LLM choice**: GPT-4o (better factoid/list, higher cost) vs. Mistral-7B (competitive summarization, lower cost)

- **Failure signatures**:
  - **Low Recall@10 but high Recall@1k**: Re-ranker not trained well or insufficient fine-tuning epochs
  - **High MRR on factoid but low list F1**: Prompt template not enforcing exhaustive enumeration
  - **ROUGE scores high but semantic correctness low**: Overfitting to extractive phrasing; may need human evaluation

- **First 3 experiments**:
  1. **Retrieve-and-evaluate baseline**: Query index with bge-large-en, retrieve top-10, measure Recall@10 and MAP@10 without re-ranking. Compare to PubMed API baseline.
  2. **Cross-encoder fine-tuning ablation**: Train ms-marco-MiniLM-L12 on 1 epoch vs. 3 epochs with hard negatives. Measure MAP@10 on held-out validation set.
  3. **Prompt style comparison**: Run GPT-4o-turbo with Style 1, 2, and 3 on 80 sampled questions (20 per type). Measure maF1, MRR, mF1, and ROUGE-SU4 to identify which style optimizes each answer type.

## Open Questions the Paper Calls Out

- **Can LLM-based query reformulation improve the recall of dense retrieval systems in the biomedical domain compared to direct query embedding?**
  - Basis in paper: [explicit] The authors state, "As a future followup, we would like to explore using language models to reformulate the query for improved retrievals."
  - Why unresolved: The current system embeds user questions directly using bge-large-en, but complex biomedical terminology may require expansion or rewriting to match relevant documents effectively.
  - What evidence would resolve it: A comparative analysis of Recall@K and MAP@10 scores between the current direct embedding approach and an approach utilizing LLM-driven query expansion or rewriting.

- **Does implementing snippet filtering or prioritization methods improve the accuracy of exact and ideal answer generation by reducing noise in the context window?**
  - Basis in paper: [explicit] The authors note that "certain questions consistently underperformed... often due to overly long or repetitive snippets that diluted contextual relevance" and suggest "introducing snippet filtering or prioritization methods" as a future direction.
  - Why unresolved: The current pipeline retrieves snippets based on the initial query but does not filter for redundancy or length, potentially distracting the instruction-tuned LLMs during the generation phase.
  - What evidence would resolve it: Ablation studies showing changes in Factoid MRR and Ideal Answer ROUGE scores when redundancy filters or relevance-based prioritization are applied to the retrieved snippet set.

- **To what extent would applying the ensemble re-ranking strategy (fine-tuned cross-encoders and GPT-based re-rankers) to the snippet retrieval phase improve final answer quality?**
  - Basis in paper: [inferred] The paper notes, "Due to time constraints, we weren’t able to replicate our document retrieval & re-ranking setup... for snippets. Across all systems, we used the same snippet retrieval strategy (which was the predecessor...)."
  - Why unresolved: While the sophisticated re-ranking pipeline significantly boosted document retrieval (MAP@10), the snippet retrieval relied on a simpler, less optimized strategy, potentially limiting the quality of the context provided to the answer generator.
  - What evidence would resolve it: A comparison of Phase B answer generation metrics (e.g., Factoid MRR, List F1) using the current snippet retrieval versus the ensemble re-ranking approach applied specifically to snippets.

## Limitations
- Cross-encoder fine-tuning hyperparameters (learning rate, batch size, optimizer) are unspecified, making exact reproduction difficult.
- Base prompt template "Style 3" references external work without full disclosure, potentially affecting reproducibility of answer quality gains.
- Score normalization method for ensemble re-ranking is not detailed, which may impact the exact MAP@10 results.

## Confidence
- **High Confidence**: Dense retrieval outperforming PubMed API baseline (supported by recall@10 and recall@1k metrics).
- **Medium Confidence**: Prompt style improvements for list and ideal answers (ROUGE-SU4 gains reported but not independently validated).
- **Low Confidence**: Ensemble weighting scheme (1:7 ratio) without ablation study or sensitivity analysis.

## Next Checks
1. Reconstruct the full "Style 3" prompt from referenced sources and test if reported ROUGE-SU4 improvements replicate.
2. Implement ablation study varying ensemble weights (e.g., 1:3, 1:5, 1:10) to verify optimal 1:7 ratio.
3. Measure semantic correctness of answers alongside ROUGE metrics to ensure extractive phrasing doesn't sacrifice meaning.