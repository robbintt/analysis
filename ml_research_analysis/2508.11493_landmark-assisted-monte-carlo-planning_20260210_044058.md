---
ver: rpa2
title: Landmark-Assisted Monte Carlo Planning
arxiv_id: '2508.11493'
source_url: https://arxiv.org/abs/2508.11493
tags:
- e-01
- greedy
- problem
- e-02
- landmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper formalizes probabilistic landmarks for stochastic planning
  and adapts the UCT algorithm to use them as subgoals during rollouts, balancing
  between greedy landmark achievement and final goal achievement. LAMP learns three
  Q-functions to estimate the expected utility of achieving the current landmark,
  the final goal, and the next landmark.
---

# Landmark-Assisted Monte Carlo Planning
## Quick Facts
- arXiv ID: 2508.11493
- Source URL: https://arxiv.org/abs/2508.11493
- Reference count: 40
- Landmark-assisted UCT improves performance in 5 of 6 benchmark domains

## Executive Summary
This paper introduces Landmark-Assisted Monte Carlo Planning (LAMP), a method that enhances UCT-based planning in Stochastic Shortest Path problems by using probabilistic landmarks as subgoals during rollouts. Landmarks are extracted from an all-outcomes determinization of the problem and used to guide search toward the final goal. LAMP learns three Q-functions to estimate expected utilities for achieving the current landmark, the final goal, and the next landmark. The approach balances greedy landmark achievement with long-term goal planning using a weighted combination of UCB policies.

## Method Summary
LAMP formalizes probabilistic landmarks for stochastic planning and adapts UCT to use them as subgoals during rollouts. The method learns three Q-functions: Q_φ estimates the expected utility of achieving the current landmark, Q_g estimates the expected utility of achieving the final goal, and Q_LM selects which landmark to target next. Action selection uses a weighted combination of UCB1 policies based on Q_φ and Q_g, controlled by parameter α. Landmarks are extracted using Fast Downward's LMRHW algorithm on the all-outcomes determinization of the problem. The GUBS criterion updates Q-values based on rollout outcomes, and the system balances between greedy landmark achievement and final goal achievement.

## Key Results
- LAMP significantly improves UCT performance in 5 of 6 benchmark domains
- Optimal balance between greedy and long-term goal focus is problem-dependent
- In deadlock-free domains, greedy approaches often outperform standard UCT with fewer rollouts
- The best α value varies by problem (e.g., α=0.8 for elevators, α=0.2 for blocksworld)

## Why This Works (Mechanism)
LAMP works by breaking down the complex planning problem into manageable subgoals through landmarks. By extracting landmarks from the determinized problem, the algorithm identifies critical states that must be achieved to reach the final goal. The three Q-functions allow the planner to evaluate progress at multiple levels: current landmark achievement, next landmark selection, and final goal completion. The weighted UCB action selection balances immediate progress toward the landmark with exploration of paths that may lead to better overall outcomes.

## Foundational Learning
- **Stochastic Shortest Path (SSP) problems**: Why needed - Framework for modeling sequential decision problems with probabilistic outcomes and costs. Quick check - Verify understanding of cost minimization vs. reward maximization.
- **Monte Carlo Tree Search (MCTS)**: Why needed - Search algorithm that uses random sampling to approximate optimal decisions. Quick check - Understand UCT's role in balancing exploration and exploitation.
- **Probabilistic landmarks**: Why needed - Identify critical states that must be achieved to reach goals in stochastic environments. Quick check - Confirm landmark extraction using LMRHW on determinization.
- **GUBS update rule**: Why needed - Incremental update method for Q-values that incorporates goal achievement signals. Quick check - Verify the formula for combining previous Q-value with new information.

## Architecture Onboarding
- **Component map**: Problem → Landmark Extraction → Landmark Graph → UCT Rollouts → Q-value Updates → Policy
- **Critical path**: Extract landmarks → Build landmark graph → Run UCT rollouts with landmark subgoals → Update Q-functions → Select actions using weighted UCB
- **Design tradeoffs**: Greedy vs. exploratory behavior controlled by α parameter; landmark selection vs. action selection; deterministic landmark extraction vs. stochastic planning
- **Failure signatures**: Poor performance in deadlock states when α is too high; high variance in results when α is not optimized for the problem; failure to extract useful landmarks when determinization is poor
- **First experiments**: 1) Verify landmark extraction on simple domains, 2) Test LAMP with α=0 (standard UCT) vs α=1 (fully greedy) on deadlock-free problems, 3) Implement action pruning and measure branching factor reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Performance highly dependent on problem-specific optimal α values
- Landmark extraction requires external tools (Fast Downward) and specific PDDL formulations
- PROST-style action pruning details not fully specified in pseudocode
- May perform poorly in domains with high deadlock risk when using greedy landmark selection

## Confidence
- High Confidence: Core algorithmic framework and three Q-function approach
- Medium Confidence: Landmark extraction process and integration with UCT
- Low Confidence: Exact performance comparisons due to problem-specific nature and benchmark dependencies

## Next Checks
1. Implement landmark safety validation to prevent greedy selection of landmarks leading to deadlock states
2. Run LAMP with all five α values for at least 75 runs per problem to verify problem-dependent optimal balance
3. Implement PROST-style action pruning and measure its impact on branching factor reduction