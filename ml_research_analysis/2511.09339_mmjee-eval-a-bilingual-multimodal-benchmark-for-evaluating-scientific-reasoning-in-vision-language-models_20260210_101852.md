---
ver: rpa2
title: 'mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning
  in Vision-Language Models'
arxiv_id: '2511.09339'
source_url: https://arxiv.org/abs/2511.09339
tags:
- dataset
- questions
- question
- reasoning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mmJEE-Eval is a new benchmark built from India's JEE Advanced exams
  to test scientific reasoning in vision-language models. It includes 1,460 multimodal
  questions in English and Hindi across Physics, Chemistry, and Mathematics.
---

# mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2511.09339
- Source URL: https://arxiv.org/abs/2511.09339
- Reference count: 40
- 17 state-of-the-art models tested, closed models achieved 77-84% accuracy vs open models at 37-45%

## Executive Summary
mmJEE-Eval is a new benchmark built from India's JEE Advanced exams to test scientific reasoning in vision-language models. It includes 1,460 multimodal questions in English and Hindi across Physics, Chemistry, and Mathematics. Evaluation of 17 state-of-the-art models revealed that while closed models like GPT-5 and Gemini 2.5 Pro achieve 77-84% accuracy, open models plateau at 37-45%, a much larger gap than seen on existing benchmarks. Models struggled with bilingual consistency and rarely corrected their own errors, highlighting metacognitive limitations. Performance differences were traced to training methodology rather than model scale, and the benchmark is designed to avoid memorization through annual updates.

## Method Summary
The benchmark consists of 1,460 bilingual (English/Hindi) questions from JEE Advanced exams (2019-2025) across Physics, Chemistry, and Mathematics. Questions include multiple-choice (single and multiple answer), numerical, and matching types, with 442 requiring visual interpretation. Evaluation uses Pass@1 accuracy averaged over k=10 runs as the primary metric, with additional JEE-style marking schemes, confidence thresholding, cross-lingual consistency checks, and metacognitive metrics (error detection and correction rates). The benchmark is designed to be strictly evaluation-only to prevent memorization, with new questions added annually.

## Key Results
- Closed models (GPT-5, Gemini 2.5 Pro) achieved 77-84% accuracy, while open models plateaued at 37-45%
- Cross-lingual consistency was poor, with only 42-60% of questions answered correctly in both languages
- Metacognitive limitations were evident, with models rarely correcting their own errors even when error detection was successful

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its use of authentic JEE Advanced questions that require genuine scientific reasoning rather than pattern matching. The multimodal nature (1,460 questions with 442 requiring visual interpretation) ensures models must integrate visual information with textual problem-solving. The bilingual format (English/Hindi) tests cross-lingual consistency, while the evaluation-only design prevents memorization. The combination of traditional accuracy metrics with metacognitive evaluation (error detection and correction) provides a comprehensive assessment of model reasoning capabilities beyond simple answer correctness.

## Foundational Learning
- **Multimodal scientific reasoning**: Ability to integrate visual information with textual problem-solving, crucial for physics/chemistry diagrams and mathematical notation
- **Bilingual consistency**: Evaluating whether models can maintain performance across language translations, important for real-world deployment in multilingual contexts
- **Metacognitive evaluation**: Measuring error detection and correction capabilities, essential for autonomous reasoning systems
- **JEE Advanced exam structure**: Understanding the specific format and difficulty level of competitive engineering entrance exams in India
- **LaTeX answer extraction**: Using \boxed{} formatting for precise answer retrieval from model outputs

## Architecture Onboarding
- **Component map**: Benchmark dataset -> Question types (MCQ-Single, MCQ-Multiple, Numerical, Matching) -> Language-specific prompts -> Model inference -> Answer extraction (\boxed{}) -> Metric computation (Pass@1, JEE marks, EP/EC)
- **Critical path**: Question retrieval → Language-specific prompt formatting → Model API call → LaTeX answer extraction → Ground truth comparison → Metric aggregation
- **Design tradeoffs**: Evaluation-only vs training data provision (prevents memorization but limits direct model improvement), bilingual vs monolingual focus (broader applicability but increased complexity)
- **Failure signatures**: LaTeX parsing errors (use robust regex extraction), language control drift (add explicit language constraints), numerical precision mismatches (implement tolerance thresholds)
- **First experiments**: 1) Verify dataset download and structure, 2) Test question-type-specific prompt formatting, 3) Implement and validate answer extraction from \boxed{} tags

## Open Questions the Paper Calls Out
- How can training methodologies be adapted to improve open model performance on multimodal scientific reasoning tasks?
- What architectural modifications could enhance bilingual consistency and metacognitive capabilities?
- How should the benchmark evolve to address emerging multimodal reasoning challenges while maintaining comparability with historical results?
- What is the optimal balance between evaluation-only design and providing sufficient context for model improvement?

## Limitations
- Evaluation-only benchmark prevents direct model improvement through training data
- Inference parameters not fully specified, potentially affecting cross-model comparisons
- Self-consistency voting mechanism for confidence thresholding lacks complete specification
- Benchmark focuses on Indian engineering entrance exam context, potentially limiting generalizability to other scientific reasoning domains
- Performance attribution to training methodology rather than scale remains correlational without controlled experiments

## Confidence
- **High Confidence**: Performance gap between closed and open models is well-established
- **Medium Confidence**: Attribution of performance differences to training methodology
- **Medium Confidence**: Metacognitive limitation findings are valid but evaluation methodology may not capture all reasoning refinement

## Next Checks
1. Reproduce cross-lingual consistency results on 2025 held-out set with explicit language constraint prompts
2. Conduct sensitivity analysis on inference parameters (temperature, max_tokens) across model subset
3. Validate numerical precision tolerance implementation using official JEE Advanced answer ranges