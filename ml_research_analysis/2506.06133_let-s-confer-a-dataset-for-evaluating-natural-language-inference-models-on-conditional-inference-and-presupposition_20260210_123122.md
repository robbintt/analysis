---
ver: rpa2
title: 'Let''s CONFER: A Dataset for Evaluating Natural Language Inference Models
  on CONditional InFERence and Presupposition'
arxiv_id: '2506.06133'
source_url: https://arxiv.org/abs/2506.06133
tags:
- inference
- presupposition
- conditional
- sentence
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONFER, a dataset of 18,000 sentence pairs
  to evaluate NLI models on conditional inference and presupposition projection. The
  dataset is generated semi-automatically using linguist-designed templates to control
  syntactic and lexical variation, covering five conditional types and two presupposition
  triggers.
---

# Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition

## Quick Facts
- arXiv ID: 2506.06133
- Source URL: https://arxiv.org/abs/2506.06133
- Reference count: 34
- Primary result: NLI models struggle with presuppositional reasoning in conditionals; fine-tuning on existing datasets often degrades performance

## Executive Summary
This paper introduces CONFER, a dataset of 18,000 sentence pairs designed to evaluate NLI models on conditional inference and presupposition projection. The dataset was generated semi-automatically using linguist-designed templates to systematically cover five conditional types and two presupposition triggers. Experiments show that both fine-tuned transformer models and LLMs struggle with presuppositional reasoning in conditionals, with fine-tuning on existing NLI datasets failing to improve performance. The findings highlight the need for more diverse and structured datasets to advance NLI model capabilities in pragmatic inference.

## Method Summary
CONFER was created using a semi-automated pipeline combining linguist-designed templates with GPT-3.5 few-shot generation. The dataset covers five conditional types representing different antecedent-presupposition relationships, using two triggers ("again" and possessive constructions). Sentence pairs were manually reviewed for anaphora consistency and embedded in negation, interrogation, and factive contexts. Each pair was annotated with E/C/N labels through dual-annotator validation (99.86% agreement). The dataset was evaluated through fine-tuning experiments with RoBERTa and DeBERTa, and prompting experiments with Llama-3-8B-Instruct, Gemma-2B-it, GPT-4o, and DeepSeek-R1-Distill-Qwen-1.5B.

## Key Results
- NLI models struggle with presuppositional reasoning in conditionals, particularly Type 5 (independent antecedent/consequent)
- Fine-tuning on existing NLI datasets (IMPPRES, NOPE) does not improve conditional reasoning and often leads to degradation
- GPT-4o shows marginal improvement with few-shot prompting (64% to 67%) while Llama's performance declines (52% to 39%)
- Models frequently misclassify neutral cases as entailments due to ambiguity in antecedent-consequent relationships

## Why This Works (Mechanism)

### Mechanism 1: Presupposition Persistence Under Entailment-Canceling Environments
- Claim: IF a sentence contains a presupposition trigger, THEN that presupposition persists when embedded in negation, interrogation, or modal contexts—unlike entailments, which cancel.
- Mechanism: Presuppositions represent background assumptions taken for granted by speakers, operating independently of the main proposition's truth value. The projection behavior follows from the presupposition being a precondition for the sentence having a truth value at all.
- Core assumption: Presupposition triggers (e.g., "again," possessive constructions) reliably signal information that survives entailment-canceling operators.
- Evidence anchors:
  - [abstract] "presupposition is...different from entailment in that it is not canceled when placed in entailment-canceling environments"
  - [section 2] Table 1 demonstrates presupposition survival under negation; discusses Karttunen and Stalnaker theories
  - [corpus] Related work (IMPPRES, NOPE) confirms presupposition evaluation requires specialized datasets beyond standard NLI
- Break condition: When the antecedent of a conditional semantically entails the presupposition, projection behavior becomes unpredictable (the "Proviso Problem").

### Mechanism 2: Controlled Template-Based Generation for Logical Coverage
- Claim: IF sentence generation is constrained by linguist-designed templates with explicit A-p (antecedent-presupposition) logical relationships, THEN the resulting dataset systematically covers the inference space that natural corpora miss.
- Mechanism: Five conditional types encode distinct logical relations between antecedent and presupposition: equivalence (A≡p), asymmetric entailment (A→p), reverse entailment (p→A), probabilistic association, and independence. Two triggers ("again," possessive) provide lexical variation within this logical structure.
- Core assumption: The five types adequately represent the theoretical space of presupposition projection in conditionals; templates don't artificially simplify the phenomenon.
- Evidence anchors:
  - [section 4] "we generated 800 sentences per type, using two presupposition triggers...allowing us to create a sizable, lexically diverse, and carefully controlled dataset"
  - [abstract] "18,000 sentence pairs...generated semi-automatically using linguist-designed templates to control syntactic and lexical variation"
  - [corpus] No corpus papers specifically address conditional presupposition projection; existing datasets (IMPPRES, NOPE) avoid complex conditionals
- Break condition: Template-generated sentences may lack discourse context that affects natural presupposition accommodation; real-world conditionals may exhibit phenomena not captured by the five-type taxonomy.

### Mechanism 3: Surface-Level Correlation Learning Fails on Novel Syntactic Structures
- Claim: IF models are fine-tuned on standard NLI datasets lacking complex conditionals, THEN they learn trigger-specific surface correlations that degrade—rather than transfer—when evaluated on conditional presupposition reasoning.
- Mechanism: NLI models appear to learn statistical associations between lexical triggers and inference labels without acquiring the underlying pragmatic reasoning. When faced with conditionals where the antecedent-presupposition relationship varies systematically, these surface patterns mislead.
- Core assumption: Poor transfer indicates models lack genuine understanding rather than merely needing more training data of the same type.
- Evidence anchors:
  - [abstract] "fine-tuning on existing NLI datasets does not necessarily improve their performance"
  - [section 6.1] "high error rate was observed for Entailment → Contradiction in Type 5...models struggle to recognize the entailment relationship when presupposition projection is unclear"
  - [corpus] Related work on NLI robustness (Jeretič et al., Parrish et al.) finds models struggle with context-dependent interpretations beyond surface cues
- Break condition: If training data included systematic coverage of conditional types with explicit logical relationship annotations, transfer might succeed.

## Foundational Learning

- **Concept: Presupposition vs. Entailment**
  - Why needed here: The entire dataset hinges on distinguishing inferences that survive negation (presuppositions) from those that don't (entailments). Without this distinction, the NLI labels appear arbitrary.
  - Quick check question: In "John regrets quitting his job," does the inference "John quit his job" survive if you negate the sentence to "John doesn't regret quitting his job"?

- **Concept: Presupposition Projection from Embedded Clauses**
  - Why needed here: Conditionals embed presuppositions in their consequent, but the conditional as a whole may inherit a modified presupposition. The five types capture different projection patterns.
  - Quick check question: If "If John is a scuba diver, he'll bring his wetsuit" presupposes "John has a wetsuit," does "If John flies to Toronto, he'll bring his wetsuit" presuppose the same thing, or something weaker?

- **Concept: The Proviso Problem**
  - Why needed here: This linguistic puzzle explains why presupposition projection in conditionals is theoretically contested—the expected projection doesn't always match speaker intuitions, motivating the dataset's multi-type design.
  - Quick check question: Why might "If John is a scuba diver, he'll bring his wetsuit" trigger a weaker presupposition than expected (conditional rather than unconditional ownership)?

## Architecture Onboarding

- **Component map:**
  Lexical Database (2,139 items + grammatical features) -> GPT-3.5 Few-Shot Generator + Linguist Templates -> Manual Review & Anaphora Correction -> Embedding Module: Negation, Interrogation, Factive contexts -> Hypothesis Assignment: p = presupposition of consequent -> NLI Annotation: E/C/N labels, dual-annotator validation -> Evaluation Suite: NLI fine-tuning + LLM prompting

- **Critical path:**
  1. Define the five A-p logical relationships with linguistic validity
  2. Design templates that unambiguously realize each relationship
  3. Generate sentences ensuring trigger balance (again vs. possessive)
  4. Verify that hypothesis p correctly captures the presupposition
  5. Confirm annotator agreement (reported: 99.86%)

- **Design tradeoffs:**
  - **Synthetic control vs. naturalness**: Template-based generation ensures logical coverage but may produce less natural discourse than corpus extraction
  - **Type 4 coverage**: Only possessive trigger feasible due to structural constraints—limits trigger-balanced analysis for this type
  - **Label granularity**: Standard 3-way NLI (E/C/N) rather than fine-grained presupposition strength ratings

- **Failure signatures:**
  - Type 5 Entailment → Contradiction errors: Models fail to recognize unconditional presupposition projection when A and p are independent
  - Type 2 Contradiction → Neutral errors: Models conflate structures where A doesn't entail p with genuinely neutral cases
  - LLM few-shot degradation (Llama): Providing Type 5 entailment examples impairs performance on Types 1-3—suggests confusion from structurally similar

- **First experiments:**
  1. Fine-tune RoBERTa-large-MNLI and DeBERTa-large-MNLI on CONFER training set (20% test split)
  2. Evaluate zero-shot performance of Llama-3-8B-Instruct and GPT-4o on CONFER test set
  3. Implement few-shot prompting (1 example per type) for LLM evaluation

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How do humans interpret presuppositions in complex conditional structures, and what theoretical framework best explains this process?
  - Basis in paper: [explicit] The paper states: "How humans interpret presuppositions, particularly in complex structures such as conditionals, remains an open question. The absence of a comprehensive theory complicates the creation of test cases for machine learning models."
  - Why unresolved: Multiple linguistic theories (e.g., Karttunen, Stalnaker) offer competing explanations for the Proviso Problem, but none fully account for why presuppositions project variably across different conditional types.
  - What evidence would resolve it: Large-scale human behavioral experiments measuring presupposition judgments across systematically varied conditional structures could establish which theoretical predictions align with human reasoning.

- **Open Question 2**
  - Question: What specific architectural or training modifications would enable models to capture the interaction between semantic structure and pragmatic reasoning in conditionals?
  - Basis in paper: [explicit] The conclusion states: "We suggest that future research focus on developing models that can account for the complex interaction between semantics and pragmatics in order to enhance the reasoning capabilities of natural language inference models."
  - Why unresolved: Fine-tuning on existing NLI datasets degraded rather than improved conditional reasoning, suggesting current training objectives fail to capture pragmatic inference mechanisms.
  - What evidence would resolve it: Systematic ablation studies comparing models trained with explicit pragmatic supervision (e.g., presupposition annotation, linguistic theory-informed objectives) versus standard NLI training.

## Limitations

- The synthetic template-based generation may not capture the full complexity of natural conditional discourse where presupposition projection depends on conversational context
- The dataset's reliance on only two presupposition triggers (again, possessive) limits generalizability to other linguistic triggers
- Type 4's exclusive use of the possessive trigger creates an imbalance in trigger analysis across conditional types
- The Proviso Problem is acknowledged but not systematically addressed across all conditional types

## Confidence

- **High confidence**: The dataset construction methodology and the fundamental claim that NLI models struggle with conditional presupposition projection
- **Medium confidence**: The specific failure patterns (Type 5 errors, fine-tuning degradation) are well-documented but may reflect template-specific artifacts rather than general model limitations
- **Low confidence**: Claims about LLMs requiring "formal logical systems" to overcome limitations are speculative given the relatively small performance gaps observed

## Next Checks

1. Evaluate human annotators on the same CONFER test set to establish a performance ceiling and determine whether model failures reflect task difficulty or reasoning limitations
2. Test model performance on a subset of naturally-occurring conditional sentences from existing corpora to assess whether template-specific patterns drive the observed failures
3. Implement ablation studies removing the linguistic templates to determine whether the five-type taxonomy captures the full complexity of conditional presupposition projection in naturalistic contexts