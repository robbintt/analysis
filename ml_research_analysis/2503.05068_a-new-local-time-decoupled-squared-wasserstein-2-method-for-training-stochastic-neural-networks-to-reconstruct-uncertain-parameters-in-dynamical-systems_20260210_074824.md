---
ver: rpa2
title: A new local time-decoupled squared Wasserstein-2 method for training stochastic
  neural networks to reconstruct uncertain parameters in dynamical systems
arxiv_id: '2503.05068'
source_url: https://arxiv.org/abs/2503.05068
tags:
- distribution
- function
- parameters
- squared
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel local time-decoupled squared Wasserstein-2
  method for reconstructing the distribution of uncertain model parameters in dynamical
  systems from time-series data. The method combines the advantages of both time-decoupled
  and local squared Wasserstein-2 approaches to account for uncertainties in initial
  conditions and intrinsic stochasticity in the system.
---

# A new local time-decoupled squared Wasserstein-2 method for training stochastic neural networks to reconstruct uncertain parameters in dynamical systems

## Quick Facts
- **arXiv ID**: 2503.05068
- **Source URL**: https://arxiv.org/abs/2503.05068
- **Reference count**: 40
- **Primary result**: Novel method combines local conditioning and time-decoupled W₂ loss to reconstruct parameter distributions in uncertain dynamical systems, outperforming benchmark methods.

## Executive Summary
This paper introduces a local time-decoupled squared Wasserstein-2 (W₂) method for reconstructing the distribution of uncertain parameters in dynamical systems from time-series data. The approach uses a stochastic neural network (SNN) with weight uncertainty to represent parameter distributions and a custom loss function that accounts for both initial condition uncertainty and parameter uncertainty. The method is theoretically grounded with convergence proofs and demonstrates superior performance compared to existing statistical approaches across various dynamical systems including ODEs, PDEs, and SDEs.

## Method Summary
The method trains an SNN where weights are sampled from learnable Normal distributions to represent parameter distributions. For each forward pass, weights are resampled to generate parameter predictions, which are then used to solve the dynamical system. The loss function computes local squared W₂ distances between empirical trajectory distributions conditioned on neighborhoods of initial conditions, averaged over time. This decouples the uncertainty in initial conditions from the uncertainty in parameters. Training uses AdamW optimization with backpropagation through the dynamical system solver. The approach combines theoretical guarantees (necessity of loss minimization for distribution matching) with practical implementation using optimal transport solvers for W₂ distance computation.

## Key Results
- The SNN architecture can approximate any continuous random variable under moderate smoothness assumptions
- The local time-decoupled squared W₂ loss effectively separates parameter uncertainty from initial condition uncertainty
- Numerical experiments on ODEs, PDEs, and SDEs demonstrate accurate parameter distribution reconstruction with computational efficiency gains over benchmark methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the local time-decoupled squared W₂ loss is a necessary condition for matching the distribution of unknown parameters.
- Mechanism: The loss function decomposes trajectory-level comparisons into per-timepoint local W₂ distances conditioned on neighborhoods of initial conditions. This separates uncertainty in parameters from uncertainty in initial states. Theorem 2.2 bounds this loss by the W₂ distance between parameter distributions plus terms controlled by neighborhood size δ, creating a theoretical link between loss minimization and distribution matching.
- Core assumption: Lipschitz continuity of drift/diffusion/jump functions and bounded sixth-order moments of parameters.
- Evidence anchors: [section] Theorem 2.2 proves the bound E[Ŵ²,δ] ≤ 8C₀Tδ²exp(C₀T) + 6C₁T·exp(C₀T)·W₂²(μ,μ̂) + ...; [section] Eq. (2.6) defines the discretized loss used in practice; [corpus] Weak support: related work [51] uses similar W₂-based approaches but without the local conditioning for initial state uncertainty.
- Break condition: If dynamics are not Lipschitz or parameter distributions have heavy tails, the bound may not hold; the δ trade-off may become unmanageable.

### Mechanism 2
- Claim: The SNN architecture with stochastic weights can approximate any continuous random variable under moderate assumptions.
- Mechanism: Weights are sampled from independent Gaussians with learnable means and variances. Theorems 3.1 and 3.2 prove that such an SNN can approximate parameterized Gaussian mixtures, which in turn approximate any smooth density via convolution and spectral collocation.
- Core assumption: Assumption: Target density is smooth with bounded mixed derivatives and finite second moments.
- Evidence anchors: [section] Theorem 3.2 proves Gaussian mixtures approximate smooth densities in W₂ sense; [section] Corollary 3.1 extends to parameterized Gaussian mixtures over input x; [corpus] Related work [54] also uses stochastic weights but lacks universal approximation proofs for this architecture.
- Break condition: Non-smooth or discontinuous densities may require many mixture components; proof assumes bounded support D.

### Mechanism 3
- Claim: The local neighborhood technique (δ) enlarges effective sample size for conditional distributions.
- Mechanism: By pooling trajectories whose initial conditions fall within δ-balls around each reference X₀,j, the method constructs empirical conditional distributions ν̂ₓ₀,δ(t) with more samples than strict conditioning would allow. This reduces variance in W₂ estimation at the cost of bias from pooling slightly different initial conditions.
- Core assumption: Assumption: Initial condition distribution is smooth enough that nearby initial conditions yield similar trajectory distributions.
- Evidence anchors: [section] Definition 2.2 introduces the local squared W₂ distance with δ-neighborhoods; [section] Eq. (2.18) shows trade-off: larger δ increases first term (bias) but decreases h(N#(X₀;δ),ℓ) (variance); [corpus] No direct corpus comparison for this specific local conditioning approach.
- Break condition: If initial condition distribution has sharp discontinuities or dynamics are highly sensitive to initial conditions, pooling introduces unacceptable bias.

## Foundational Learning

- Concept: **Wasserstein-2 distance**
  - Why needed here: The entire loss function is built on W₂; understanding its coupling interpretation is essential for grasping why trajectory comparisons lead to distribution reconstruction.
  - Quick check question: Can you explain why W₂ is a metric (triangle inequality) and how it differs from KL divergence for overlapping distributions?

- Concept: **Stochastic neural networks with weight uncertainty**
  - Why needed here: The SNN is the core model representing parameter distributions; weight sampling determines output distributions.
  - Quick check question: If all weights are deterministic, what happens to the SNN's ability to represent uncertainty?

- Concept: **Jump-diffusion processes and SDE discretization**
  - Why needed here: Numerical experiments include SDEs; understanding Itô integrals and Poisson jumps is necessary to implement the forward model correctly.
  - Quick check question: What is the difference between strong and weak convergence for SDE solvers, and which does this paper use?

## Architecture Onboarding

- Component map: **SNN weights → SNN output → Dynamical system solver → Trajectories → Local W₂ loss → Backpropagation**

- Critical path:
  1. Sample N parameter sets {θ̂ᵢ} from SNN (forward pass, re-sample weights each time)
  2. Solve dynamical system with each θ̂ᵢ to get trajectories {X̂ᵢ(t)}
  3. For each reference initial condition X₀,j, identify neighbors in both ground truth and predicted sets
  4. Compute pairwise distance matrices Cⱼ(tᵢ) for each (j, tᵢ) pair
  5. Solve optimal transport problems via PoT to get W₂² for each (j, tᵢ)
  6. Aggregate to compute total loss and backpropagate

- Design tradeoffs:
  - **δ selection**: Small δ reduces bias but increases variance; paper uses δ=0.1–0.4 depending on initial condition spread. Grid search recommended.
  - **SNN depth/width**: Paper shows 2-4 hidden layers with 10-50 neurons suffice; deeper networks show diminishing returns. ResNet helps slightly for depth >2.
  - **Time discretization**: Finer mesh (smaller Δt) improves loss approximation (Theorem 2.1 shows O(Δt) error for ODEs, O(√Δt) for jump-diffusions) but increases compute.

- Failure signatures:
  - **Degenerate distribution**: If SNN weights are initialized to zero or too small, output collapses; use N(0, 0.01–0.03) initialization.
  - **Parameter scale mismatch**: Parameters spanning orders of magnitude (e.g., 10⁻³ vs 10⁰) cause poor reconstruction of smaller parameters (Example 4.3).
  - **Insufficient neighbors**: If δ is too small relative to initial condition spread, many neighborhoods have N#(X₀;δ)=1, causing unstable W₂ estimates.
  - **Ill-posed dynamics**: If parameters can cause numerical instability (e.g., negative diffusion coefficient), reconstruction fails.

- First 3 experiments:
  1. **Lotka-Volterra ODE (Example 4.1)**: Single uncertain parameter c ~ U(2,4), 2D trajectories. Test with N=200 trajectories, δ=0.4. Compare to time-decoupled W₂, MMD, MSE losses. Expected: local W₂ outperforms in both mean and variance recovery.
  2. **Ablation on δ**: For the PDE example (4.2), vary δ ∈ {0.05, 0.1, 0.2, 0.4} and track reconstruction error vs. neighborhood sample count. Plot trade-off curve.
  3. **SNN capacity test**: For the 8D ocular pharmacokinetics model (Example 4.3), vary hidden layer count {1,2,3,4} and width {5,10,15,20} to identify minimum viable architecture. Confirm 3 layers × 10 neurons is sufficient.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is minimizing the local time-decoupled squared Wasserstein-2 loss function sufficient for reconstructing the distribution of parameters?
- Basis in paper: [explicit] The conclusion states that while Theorem 2.2 proves minimizing the loss is necessary, proving it is sufficient remains an open theoretical challenge.
- Why unresolved: The current analysis bounds the trajectory distance by the parameter distance, guaranteeing necessity but not the converse required for sufficiency.
- What evidence would resolve it: A formal mathematical proof demonstrating that minimizing the loss function guarantees convergence of the approximated parameter distribution to the true distribution.

### Open Question 2
- Question: How can prior information regarding model parameters be incorporated into the method?
- Basis in paper: [explicit] The authors note that reconstruction is less accurate when parameters span different magnitudes (e.g., 10⁻³ vs O(1)) and suggest incorporating priors as a future direction.
- Why unresolved: The current method assumes no prior knowledge ("worst scenario"), leading to high errors for parameters with very small magnitudes or low identifiability.
- What evidence would resolve it: A modified SNN framework that accepts parameter bounds or priors, along with comparative benchmarks against Bayesian inference methods.

### Open Question 3
- Question: How robust is the method to non-negligible measurement errors in the observed time-series data?
- Basis in paper: [explicit] The conclusion states that taking measurement errors into account is necessary when they are not negligible, though the current study assumes they are.
- Why unresolved: The numerical experiments utilized synthetic data generated from differential equations without explicitly modeling observation noise distinct from intrinsic stochasticity.
- What evidence would resolve it: Numerical experiments applying the method to datasets with added measurement noise and/or theoretical analysis deriving error bounds that include observation noise terms.

## Limitations

- Performance degrades for parameters spanning multiple orders of magnitude, as observed in the ocular pharmacokinetics example where rate constants of order 10⁻³ are poorly reconstructed when other parameters are O(1).
- The method assumes Lipschitz continuity of dynamical system components, which may not hold for stiff or highly nonlinear systems.
- Optimal selection of neighborhood size δ remains heuristic rather than principled, potentially affecting reconstruction accuracy.

## Confidence

- **High Confidence**: The SNN architecture can approximate any continuous random variable under the stated smoothness assumptions (Theorem 3.1, 3.2).
- **Medium Confidence**: The local time-decoupled squared W₂ loss effectively separates parameter uncertainty from initial condition uncertainty, supported by Theorem 2.2 and empirical results.
- **Medium Confidence**: Computational efficiency claims relative to benchmark methods, though comparative runtime data is limited.

## Next Checks

1. **Order-of-magnitude robustness test**: Systematically evaluate reconstruction accuracy across parameter scales spanning 10⁻³ to 10¹ to quantify degradation patterns and develop scaling strategies.

2. **δ selection protocol**: Develop an automated method for selecting neighborhood size δ based on initial condition distribution characteristics rather than grid search, and validate its effectiveness across diverse dynamical systems.

3. **Non-Lipschitz system extension**: Test the method on systems with discontinuous dynamics (e.g., piecewise-defined ODEs) to assess theoretical assumption violations and identify failure modes.