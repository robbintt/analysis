---
ver: rpa2
title: 'MathRobust-LV: Evaluation of Large Language Models'' Robustness to Linguistic
  Variations in Mathematical Reasoning'
arxiv_id: '2510.06430'
source_url: https://arxiv.org/abs/2510.06430
tags:
- math
- reasoning
- arxiv
- variation
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathRobust-LV, a test set and evaluation
  methodology to measure the robustness of large language models (LLMs) to linguistic
  variations in mathematical reasoning. The authors generate variations of high school-level
  math problems by changing surface details like names, contexts, and variables while
  preserving numerical structure and answers, mirroring how instructors rephrase problems
  across assessments.
---

# MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning

## Quick Facts
- **arXiv ID**: 2510.06430
- **Source URL**: https://arxiv.org/abs/2510.06430
- **Reference count**: 12
- **Primary result**: Most LLMs experience 2-11% accuracy drops when encountering linguistically varied versions of mathematical problems, revealing robustness challenges in mathematical reasoning

## Executive Summary
This paper introduces MathRobust-LV, a test set and evaluation methodology to measure the robustness of large language models (LLMs) to linguistic variations in mathematical reasoning. The authors generate variations of high school-level math problems by changing surface details like names, contexts, and variables while preserving numerical structure and answers, mirroring how instructors rephrase problems across assessments. Evaluating 34 open and closed-source models ranging from 0.6B to 70B+ parameters, they find that most models experience accuracy drops of 2-11% when moving from baseline to variant problems, with smaller models showing the largest degradation (9-11%). Even strong models show measurable performance declines, while frontier models like GPT-5 and Gemini-2.5-pro remain comparatively stable. The results reveal that linguistic robustness is a fundamental challenge in mathematical reasoning, exposing reasoning vulnerabilities in LLMs even at difficulty levels where they are currently deployed in educational settings.

## Method Summary
The authors create a benchmark for evaluating LLM robustness to linguistic variations in mathematical reasoning by generating multiple surface-level variations of high school-level math problems. They systematically modify problem elements including names, contexts, and variables while preserving the underlying numerical structure and answers. The evaluation encompasses 34 diverse models spanning parameter sizes from 0.6B to 70B+ parameters, including both open-source and closed-source implementations. Performance is measured by comparing accuracy on baseline problems versus their linguistically varied counterparts, revealing how linguistic changes affect model reasoning capabilities.

## Key Results
- Most LLMs show 2-11% accuracy drops when encountering linguistically varied mathematical problems
- Smaller models (0.6B-8B parameters) experience the largest degradation at 9-11%
- Frontier models like GPT-5 and Gemini-2.5-pro remain comparatively stable
- Performance declines occur even at high school difficulty levels where LLMs are currently deployed in educational settings

## Why This Works (Mechanism)
The mechanism underlying these results suggests that LLMs rely heavily on surface-level pattern matching for mathematical problem comprehension. When linguistic variations alter the surface presentation while preserving numerical structure, models that depend on memorized patterns struggle to map the new formulation to the solution strategy. This indicates that mathematical reasoning in current LLMs may be more superficial than genuine logical understanding, as they show vulnerability to changes that preserve mathematical equivalence but alter linguistic framing.

## Foundational Learning
The findings suggest that current LLMs have not fully internalized the semantic meaning of mathematical operations and relationships. Instead, they appear to learn associations between specific linguistic patterns and solution methods. This pattern-matching behavior is efficient for problems that closely match training examples but brittle when faced with novel linguistic formulations. The performance degradation across models of different sizes indicates this is a fundamental limitation in how mathematical reasoning is represented, rather than simply a parameter scale issue.

## Architecture Onboarding
The consistent performance drops across diverse model architectures (from 0.6B to 70B+ parameters) suggest that the linguistic robustness challenge is not specific to any particular architectural design. Whether using transformer-based models, attention mechanisms, or different pretraining objectives, all evaluated models show similar vulnerabilities to linguistic variation. This points to a shared limitation in how different architectures represent and process mathematical language, indicating that architectural innovations alone may not solve this robustness challenge.

## Open Questions the Paper Calls Out
The paper raises questions about whether current mathematical reasoning capabilities in LLMs represent genuine understanding or surface-level pattern matching. It questions how to develop models that can maintain reasoning performance across diverse linguistic presentations, and whether this linguistic robustness can be improved through targeted training approaches. The authors also suggest investigating whether the observed performance gaps indicate fundamental limitations in current architectures or can be addressed through improved pretraining and fine-tuning strategies.

## Limitations
- Test set focuses on high school-level mathematics, limiting generalizability to more complex reasoning tasks
- Evaluation relies on single-answer responses without systematic error analysis to distinguish comprehension versus execution failures
- Does not investigate whether performance degradation correlates with specific types of linguistic changes
- Limited exploration of semantic variations or alternative problem-solving approaches

## Confidence
**High Confidence**: The finding that most LLMs show 2-11% accuracy drops when moving from baseline to variant problems is well-supported by the evaluation of 34 diverse models.

**Medium Confidence**: The claim that smaller models (0.6B-8B parameters) show the largest degradation (9-11%) is supported by the data, though confidence intervals are not provided.

**Low Confidence**: The broader claim that linguistic robustness is a "fundamental challenge" in mathematical reasoning extrapolates beyond the specific evidence provided.

## Next Checks
1. Conduct systematic error classification to determine whether performance drops stem from comprehension failures versus execution failures
2. Design experiments that isolate specific types of linguistic variations to identify which surface changes pose the greatest challenges
3. Test whether models trained on MathRobust-LV variations show improved robustness to novel linguistic variations not seen during training