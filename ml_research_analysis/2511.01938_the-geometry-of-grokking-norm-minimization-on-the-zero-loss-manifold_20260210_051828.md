---
ver: rpa2
title: 'The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold'
arxiv_id: '2511.01938'
source_url: https://arxiv.org/abs/2511.01938
tags:
- learning
- dynamics
- loss
- zero-loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a formal understanding of grokking by showing
  that post-memorization learning dynamics minimize weight norm constrained to the
  zero-loss manifold. The key insight is that after perfect memorization, gradient
  descent is effectively driven by weight decay alone, pushing parameters toward the
  minimum-norm solution on the zero-loss set.
---

# The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold

## Quick Facts
- arXiv ID: 2511.01938
- Source URL: https://arxiv.org/abs/2511.01938
- Reference count: 17
- Key outcome: Post-memorization gradient descent with weight decay minimizes parameter norm on the zero-loss manifold, explaining grokking.

## Executive Summary
This paper provides a formal understanding of grokking by showing that post-memorization learning dynamics minimize weight norm constrained to the zero-loss manifold. The key insight is that after perfect memorization, gradient descent is effectively driven by weight decay alone, pushing parameters toward the minimum-norm solution on the zero-loss set. The paper proves that loss gradients become orthogonal to the zero-loss manifold as models approach it, leaving weight decay as the sole learning driver. Experiments on modular addition confirm that simulated dynamics reproduce both delayed generalization and circular representation learning characteristic of grokking.

## Method Summary
The paper analyzes grokking through the lens of constrained optimization on the zero-loss manifold. After perfect memorization, the model's trajectory is governed by weight decay minimizing the parameter norm while remaining on the set of solutions with zero training loss. The authors prove that loss gradients become orthogonal to this manifold as the model approaches it. To isolate component dynamics, they introduce an approximation where one parameter subset is optimized while treating others as optimal for the current state. Applied to a two-layer network, this yields a closed-form expression for the first layer's post-memorization dynamics. The simulation uses this isolated update rule to reproduce grokking phenomena without full network training.

## Key Results
- Loss gradients become orthogonal to the zero-loss manifold as models approach it, making weight decay the primary driver of post-memorization dynamics
- Closed-form isolated dynamics approximation successfully reproduces delayed generalization and circular representations
- Simulated dynamics match experimental results on modular addition, validating the norm minimization framework

## Why This Works (Mechanism)

### Mechanism 1: Constrained Norm Minimization
Post-memorization learning dynamics are driven by weight decay minimizing the parameter norm, subject to the constraint of remaining on the zero-training-loss manifold. Once the model reaches zero training loss, the optimization problem shifts from minimizing error to minimizing $\|\theta\|^2$ within the set of solutions that perfectly fit the training data. The model traverses this "zero-loss manifold" until it finds the solution with the smallest norm, which corresponds to the generalizing solution.

### Mechanism 2: Gradient Orthogonality
As the model approaches the zero-loss manifold, the loss gradient becomes orthogonal to the manifold, causing weight decay to become the primary driver of movement along the manifold. Theoretical analysis shows that as distance to the zero-loss set decreases, the cosine similarity between the loss gradient and tangent directions on the manifold approaches zero. This means the loss gradient acts as a "constraint force" keeping the model on the manifold, while the weight decay term provides the "propulsion" along the manifold toward lower norms.

### Mechanism 3: Isolated Dynamics via Effective Theory
The learning dynamics of a specific layer can be approximated in isolation by assuming the subsequent layers are instantaneously optimal. By treating the second layer weights as the ridge regression solution for the current first layer, the authors derive a closed-form gradient update for the first layer alone. This simulates the "slow" representation learning on the first layer while the "fast" second layer handles the perfect fitting of outputs.

## Foundational Learning

- **Manifolds and Tangent Spaces**: The paper models the set of perfect solutions as a geometric surface (manifold). Understanding movement along this surface vs. orthogonal to it is central to the theory. Quick check: If a ball rolls down a curved valley (the manifold), which force determines the speed along the valley floor vs. the force keeping it in the valley?

- **Constrained Optimization (Lagrange Multipliers)**: The core argument is that gradient descent with weight decay approximates a constrained optimization problem (minimize norm subject to loss = 0). Quick check: How does adding a constraint (like Loss = 0) change the direction of the gradient compared to unconstrained optimization?

- **Ridge Regression / Pseudo-Inverse**: To isolate the dynamics of the first layer, the paper solves for the optimal second layer analytically using ridge regression. Quick check: Why does the Moore-Penrose pseudo-inverse appear when solving linear systems with more columns than rows (over-parameterized)?

## Architecture Onboarding

- **Component map**: Input (one-hot vectors $x$) -> Layer 1 (Embedding matrix $W_1$) -> ReLU activation -> Layer 2 (Readout matrix $W_2$ treated as optimal) -> Output

- **Critical path**: Implementing the Isolated Dynamics Simulator (Equation 20). This requires computing the activation matrix $H$, the gram matrix $A = (HH^\top)^{-1}$, and the specific gradient term involving Hadamard products. This allows testing the theory without training a full network.

- **Design tradeoffs**:
  - Weight Decay ($\lambda$): Must be small enough to stay on the manifold but large enough to eventually reach the generalizing solution
  - Hidden Dimension ($d_h$): Must be large enough ($> n$) to ensure the zero-loss approximation holds for the second layer

- **Failure signatures**:
  - Immediate Generalization: If weight decay is very large or initialization is lucky, the model skips the memorization phase
  - No Grokking: If the architecture cannot represent the "circular" solution or the task doesn't align with norm-based simplicity biases
  - Loss Instability: If step size is too high relative to the curvature of the zero-loss manifold

- **First 3 experiments**:
  1. Toy Linear Validation: Replicate the 2-param/3-param linear models to verify that trajectories follow norm minimization lines after zero loss
  2. Cosine Similarity Check: Train a small MLP on modular addition and plot the cosine similarity between the actual weight update and the theoretical norm-minimizing direction
  3. Simulated vs. Real Training: Compare the embedding evolution of a standard SGD training run vs. the Isolated Dynamics simulation to see if they both develop circular Fourier features

## Open Questions the Paper Calls Out

- Can the norm minimization framework be extended to describe grokking dynamics under cross-entropy loss? The theoretical proofs rely on MSE properties, whereas cross-entropy is standard for classification tasks.

- How can the "Isolated Dynamics" approximation be generalized to multi-layer (deep) architectures? The closed-form derivation relies on the pseudo-inverse of the second layer, which is not directly applicable to intermediate hidden layers in deeper networks.

- Do the stability and orthogonality guarantees hold for finite, non-vanishing learning rates and weight decay coefficients? Practical training uses finite hyperparameters, potentially causing the trajectory to deviate from the predicted trajectory.

## Limitations

- Theoretical results strictly require infinitesimally small learning rates and vanishing weight decay, which don't match practical training conditions
- The isolated dynamics approximation is mathematically rigorous only for two-layer networks, with generalization to deeper architectures unproven
- Empirical validation is limited to modular addition, a synthetic task that may not capture the complexity of real-world grokking phenomena

## Confidence

- **High**: The geometric intuition that norm minimization drives generalization post-memorization is well-supported by both theory and experiments
- **Medium**: The closed-form isolated dynamics approximation is mathematically rigorous for the two-layer case, but its generalizability to deeper or more complex architectures is unproven
- **Medium**: Empirical validation on modular addition convincingly reproduces grokking phenomena, but the choice of task limits claims about broader applicability

## Next Checks

1. **Generalization to Other Tasks**: Test the isolated dynamics simulation on a different synthetic task (e.g., modular multiplication or a simple Boolean function) to verify the robustness of the circular representation emergence

2. **Deeper Network Analysis**: Apply the theory to a three-layer MLP. Can the approximation be extended by iteratively solving for each layer? Does the norm-minimization principle still govern the final phase of learning?

3. **Effect of Non-Smoothness**: Re-run the modular addition experiment with a smooth activation (e.g., GeLU) instead of ReLU. Quantify the impact on the orthogonality of the loss gradient to the zero-loss manifold and the speed of grokking.