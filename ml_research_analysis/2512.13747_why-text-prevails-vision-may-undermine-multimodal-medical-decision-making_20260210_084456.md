---
ver: rpa2
title: 'Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making'
arxiv_id: '2512.13747'
source_url: https://arxiv.org/abs/2512.13747
tags:
- mllms
- multimodal
- arxiv
- vision
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates why multimodal large language models (MLLMs)\
  \ underperform on medical decision-making (MDM) tasks compared to text-only approaches.\
  \ Through experiments on two challenging datasets\u2014Alzheimer's disease classification\
  \ (OASIS) and multi-label chest X-ray diagnosis (MIMIC-CXR)\u2014the research reveals\
  \ that text-only reasoning consistently outperforms vision-only or multimodal settings."
---

# Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making

## Quick Facts
- arXiv ID: 2512.13747
- Source URL: https://arxiv.org/abs/2512.13747
- Reference count: 40
- Primary result: Text-only reasoning consistently outperforms multimodal approaches in medical decision-making tasks

## Executive Summary
This study investigates why multimodal large language models (MLLMs) underperform on medical decision-making (MDM) tasks compared to text-only approaches. Through experiments on two challenging datasets—Alzheimer's disease classification (OASIS) and multi-label chest X-ray diagnosis (MIMIC-CXR)—the research reveals that text-only reasoning consistently outperforms vision-only or multimodal settings. In fact, adding visual input often degrades performance rather than enhancing it. Vision-only models show particularly poor results, with accuracy near random baselines. To address these limitations, the authors explore in-context learning with annotated exemplars and demonstrate significant improvements, especially for agent-based methods. The findings suggest current MLLMs lack grounded visual understanding in medical domains, highlighting the need for better multimodal integration strategies in healthcare AI applications.

## Method Summary
The study evaluates medical decision-making performance across three modalities: text-only, vision-only, and multimodal settings. Experiments are conducted on two medical datasets: OASIS for Alzheimer's disease classification and MIMIC-CXR for multi-label chest X-ray diagnosis. The researchers compare standard MLLM performance with in-context learning approaches using annotated exemplars. Agent-based methods are specifically examined to assess whether task-specific prompting can improve multimodal integration. Performance metrics include accuracy for classification tasks and F1-score for multi-label diagnosis scenarios.

## Key Results
- Text-only reasoning consistently outperforms both vision-only and multimodal settings across both medical datasets
- Vision-only models perform near random baselines, showing particularly poor results
- In-context learning with annotated exemplars significantly improves agent-based methods' performance
- Adding visual input often degrades rather than enhances overall model performance

## Why This Works (Mechanism)
The paper suggests that current MLLMs lack proper visual grounding for medical domains, leading to degraded performance when visual information is incorporated. The mechanism appears to involve failures in multimodal fusion where visual features are either incorrectly weighted or improperly integrated with textual reasoning. Text-only approaches succeed because they rely on textual descriptions and clinical knowledge that are more reliably processed by the language models, avoiding the pitfalls of unreliable visual feature extraction and integration.

## Foundational Learning
- Multimodal model architecture: Understanding how MLLMs integrate visual and textual information is crucial for interpreting performance gaps
- In-context learning: Examining how exemplar-based prompting affects model reasoning across modalities
- Medical domain knowledge: Understanding domain-specific challenges in medical imaging interpretation

Quick checks:
- Compare attention patterns between modalities to identify integration failures
- Analyze exemplar selection criteria for in-context learning effectiveness
- Evaluate domain knowledge grounding in medical decision-making tasks

## Architecture Onboarding

Component map: Vision encoder -> Fusion module -> Language model -> Decision output

Critical path: Input processing -> Feature extraction -> Multimodal fusion -> Reasoning/decision

Design tradeoffs: The study reveals fundamental tensions between modality integration strategies and domain-specific performance requirements.

Failure signatures: Vision-only models show random baseline performance; multimodal models often underperform text-only counterparts.

First experiments:
1. Ablation study removing visual input to establish text-only baseline performance
2. Vision-only model evaluation to quantify visual modality contribution
3. In-context learning comparison between agent-based and standard prompting approaches

## Open Questions the Paper Calls Out
- How do different multimodal fusion strategies affect performance in medical domains?
- What specific aspects of visual understanding are lacking in current MLLMs for medical applications?
- Can specialized training procedures or architectural modifications improve multimodal medical reasoning?

## Limitations
- Findings based on two specific medical domains (Alzheimer's and chest X-ray) may not generalize to other medical imaging tasks
- In-context learning improvements lack ablation studies to identify specific contributing factors
- Alternative multimodal architectures and training strategies were not explored

## Confidence

High confidence: Text-only models consistently outperform vision-only models across both datasets
Medium confidence: Multimodal integration generally degrades rather than improves performance
Medium confidence: In-context learning with annotated exemplars improves agent-based methods

## Next Checks

1. Test the same experimental setup across diverse medical imaging tasks (e.g., pathology slides, dermatology, CT scans) to assess generalizability of findings
2. Conduct ablation studies on in-context learning components to isolate factors driving performance improvements
3. Evaluate alternative multimodal architectures (e.g., early fusion, cross-attention variants) to determine if current limitations reflect architectural constraints rather than fundamental multimodal reasoning challenges