---
ver: rpa2
title: 'SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments
  with Generative AI'
arxiv_id: '2511.10684'
source_url: https://arxiv.org/abs/2511.10684
tags:
- product
- processes
- spidergen
- process
- products
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpiderGen is an LLM-based workflow that automates generation of
  Product Category Rules Process Flow Graphs (PCR PFGs) for carbon Life Cycle Assessments.
  It uses world knowledge and text processing to derive upstream, core, and downstream
  processes, then orders them into a Directed Acyclic Graph.
---

# SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI

## Quick Facts
- arXiv ID: 2511.10684
- Source URL: https://arxiv.org/abs/2511.10684
- Reference count: 33
- SpiderGen achieves 65% F1-score on PCR PFG generation, outperforming 53% baseline, costing under $1 and taking under 10 minutes per PFG

## Executive Summary
SpiderGen is an LLM-based workflow that automates generation of Product Category Rules Process Flow Graphs (PCR PFGs) for carbon Life Cycle Assessments. It uses world knowledge and text processing to derive upstream, core, and downstream processes, then orders them into a Directed Acyclic Graph. The system was evaluated on 65 real-world LCA documents and achieved an F1-score of 65%, outperforming a one-shot prompting baseline at 53%. SpiderGen costs under $1 and takes under 10 minutes per PFG, compared to traditional methods costing over $25,000 and taking up to 21 days.

## Method Summary
SpiderGen is a four-step pipeline that takes UN CPC codes/descriptions as input and generates PCR PFGs as directed acyclic graphs. The workflow generates diverse sample products, enumerates detailed processes for each, embeds and clusters these processes using SBERT and K-means, summarizes clusters into generalized node labels, and finally orders nodes into edges using implicit phase constraints plus explicit LLM ordering. The system uses OpenAI's o1-mini model (recommended for cost efficiency) and SBERT embeddings for semantic clustering.

## Key Results
- Achieved 65% F1-score on 65 real-world PCR documents, outperforming one-shot baseline at 53%
- Generated PFGs cost under $1 and take under 10 minutes each, versus traditional methods costing over $25,000 and taking up to 21 days
- Shows sensitivity to product category complexity, with lower PMI scores on products with more nodes (e.g., "Railways" with 44 nodes)

## Why This Works (Mechanism)

### Mechanism 1
Generating multiple diverse sample products prior to defining category-level processes improves generalization and reduces overfitting compared to direct prompting. By forcing the LLM to inventory processes for specific, diverse instances (e.g., "Moka Coffee" vs. "Instant Coffee") before aggregating, the workflow filters out niche processes that don't generalize to the broader "Coffee" category. The system then clusters these instances to find the "common denominator" processes.

### Mechanism 2
Hybridizing implicit domain ordering with explicit LLM reasoning improves graph structure (DAG) validity more than relying on LLM reasoning alone. The workflow enforces a hard constraint where all "Upstream" processes must precede "Core" processes, which precede "Downstream" processes. The LLM is only tasked with ordering within these phases, reducing the search space and lowering the probability of topological errors.

### Mechanism 3
Semantic clustering of process descriptions stabilizes the PFG against verbosity and lexical variance in LLM outputs. Instead of using raw LLM text as nodes, SpiderGen embeds process descriptions using SBERT and clusters them, then has an LLM summarize these clusters. This transforms variable-length, specific descriptions into fixed, generalized node labels.

## Foundational Learning

- **Life Cycle Assessment (LCA) & System Boundaries**: Understanding that "Scope" defines whether auxiliary processes (e.g., employee commuting, machine maintenance) are included is critical for diagnosing errors. Quick check: Does the generated PFG include "employee commuting"? Is this a "Core" process or an "Auxiliary" process, and does the selected scope allow it?

- **Directed Acyclic Graphs (DAGs)**: The output is a DAG where cycles are invalid in this context. Quick check: If Node A points to Node B, can Node B point back to Node A?

- **SBERT (Sentence-BERT) Embeddings**: The "Coarse Process Generation" relies on vector embeddings to group similar processes. You need to distinguish between semantic similarity (meaning) and lexical similarity (word matching). Quick check: Why would "Transport via Truck" and "Shipping by Lorry" need to be clustered together even if they share no keywords?

## Architecture Onboarding

- **Component map**: Input UN CPC Code/Description -> Sampler (LLM generates N diverse products) -> Enumerator (LLM lists processes per product) -> Vectorizer (SBERT embeds processes) -> Clusterer (K-Means groups vectors) -> Summarizer (LLM labels clusters) -> Structurer (LLM orders Nodes into Edges) -> Output PFG

- **Critical path**: Step 3 (Clusterer -> Summarizer). The quality of the final PFG hinges on whether the clustering granularity is correct. Too few clusters = loss of detail; too many = redundant nodes.

- **Design tradeoffs**: Model Size vs. Cost: o1-preview offers best performance but costs ~13x more than o1-mini. For high-volume generation, o1-mini is the practical choice, accepting slightly lower PMI scores. Complexity vs. Accuracy: SpiderGen performs worse on complex categories (e.g., "Railways") compared to simple ones (e.g., "Bottled Water").

- **Failure signatures**: "Scope" Mismatch (includes irrelevant processes or misses auxiliary ones), Over-specificity (nodes too granular, e.g., "Driving the truck" instead of "Transportation"), Missing Nodes (fails to capture upstream processes not obvious from description).

- **First 3 experiments**:
  1. Parameter Sweep (Product Count): Run SpiderGen on "Grain Mill Products" (Niche) and "Baked Goods" (Broad) varying sample product count (3, 6, 9, 12, 15) to replicate PMI variance findings.
  2. Baseline Comparison: Generate a PFG for "Asphalt Mixtures" using LLMDirect (CoT) vs. SpiderGen and manually count "Missing" vs. "Wrong" nodes to verify the 65% vs 53% F1 gap.
  3. Phase Ablation: Disable the "Implicit Ordering" constraint (allow LLM to order Upstream/Downstream freely) and measure increase in topological errors or cyclic dependencies.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can human-AI collaboration methods be designed to dynamically define "system boundaries" (scope) for specific product categories? Current static assumptions based on ISO standards cause most errors through hallucinating irrelevant processes or missing auxiliary ones.

- **Open Question 2**: What methodologies can effectively measure and communicate the uncertainty of specific PFG nodes and edges? Current workflow produces deterministic outputs without confidence scores, leaving experts unable to gauge reliability without manual auditing.

- **Open Question 3**: How can automated evaluation metrics be adapted to account for expert disagreement in ground-truth LCA data? Current metrics (like F1-score) rely on single ground truth, failing to capture valid variations in LCA scope or the "under-specified" nature of downstream processes.

## Limitations

- The evaluation framework relies heavily on manual node matching between generated PFGs and ground truth PCR documents, introducing potential subjectivity in F1-score calculation
- The generalizability of the semantic clustering approach lacks direct validation, as no corpus evidence supports SBERT clustering effectiveness for LCA process descriptions
- SpiderGen shows sensitivity to product category complexity, with lower PMI scores on products with more nodes (e.g., "Railways" with 44 nodes)

## Confidence

- **High Confidence**: Cost and time efficiency claims (under $1 and 10 minutes per PFG vs. $25,000 and 21 days) are well-supported by methodology description
- **Medium Confidence**: F1-score improvement over one-shot baseline (65% vs 53%) is supported by evaluation methodology, but manual node matching introduces potential variability
- **Low Confidence**: Generalizability of semantic clustering approach lacks direct validation

## Next Checks

1. **Ablation Study on Clustering**: Run SpiderGen with and without SBERT clustering step on subset of product categories to quantify impact on node specificity and F1-score
2. **Cross-Annotator Reliability**: Have two independent annotators categorize nodes for 10 randomly selected PFGs to measure inter-rater reliability and assess F1-score stability
3. **Complexity Sensitivity Test**: Generate PFGs for categories with varying node counts (simple: "Bottled Water," complex: "Railways") and analyze correlation between node count and PMI score to validate claimed sensitivity to complexity