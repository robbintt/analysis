---
ver: rpa2
title: Surrogate Fitness Metrics for Interpretable Reinforcement Learning
arxiv_id: '2504.14645'
source_url: https://arxiv.org/abs/2504.14645
tags:
- policy
- fidelity
- react
- diversity
- fitness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting reinforcement
  learning (RL) policies by proposing a framework called REACT that generates diverse
  and informative demonstrations of policy behavior. The core idea is to use an evolutionary
  algorithm to optimize initial states in order to produce trajectories that are evaluated
  using a joint surrogate fitness function, which balances local diversity, global
  diversity, and action certainty.
---

# Surrogate Fitness Metrics for Interpretable Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.14645
- Source URL: https://arxiv.org/abs/2504.14645
- Reference count: 40
- One-line primary result: REACT generates more diverse and informative RL policy demonstrations than random sampling, especially in early and intermediate policy training stages.

## Executive Summary
This paper introduces REACT, a framework for generating diverse and informative demonstrations of reinforcement learning (RL) policies by optimizing initial states via evolutionary algorithms. The core innovation is a joint surrogate fitness function that balances local diversity, global diversity, and action certainty to identify trajectories that better represent policy behavior. Evaluated across discrete gridworld and continuous robotic control environments, REACT significantly improves demonstration fidelity compared to random and baseline methods, particularly in early and intermediate policy training stages. The approach offers a scalable, structured method for enhancing RL policy interpretability, especially in safety-critical or explainability-focused domains.

## Method Summary
REACT uses an evolutionary algorithm to optimize initial states in order to produce trajectories that are evaluated using a joint surrogate fitness function, which balances local diversity, global diversity, and action certainty. Binary-encoded initial states undergo tournament selection, crossover (single-point, pc=0.75), and bit-flip mutation (pm=0.5). Each individual represents a starting position; trajectories are sampled by rolling out the fixed policy π from that state. The evolutionary process refines the population toward states that produce high-fitness trajectories. The approach is evaluated across discrete gridworld and continuous robotic control environments.

## Key Results
- REACT significantly improves demonstration fidelity compared to random and baseline methods, particularly in early and intermediate policy training stages.
- In more mature policies, fidelity-based optimization becomes more effective, though REACT still provides broader spatial coverage.
- The joint surrogate fitness function combining local diversity, global diversity, and certainty outperforms simple sum and fidelity-only optimization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing initial states via evolutionary optimization generates more diverse and informative policy demonstrations than random sampling.
- Mechanism: Binary-encoded initial states undergo tournament selection, crossover (single-point, pc=0.75), and bit-flip mutation (pm=0.5). Each individual represents a starting position; trajectories are sampled by rolling out the fixed policy π from that state. The evolutionary process refines the population toward states that produce high-fitness trajectories.
- Core assumption: The policy's behavioral variability is sensitive to initial conditions, and meaningful edge-case behaviors exist in unexplored regions of the initial state space.
- Evidence anchors:
  - [abstract] "We employ an evolutionary optimization framework that perturbs initial states to generate informative and diverse policy demonstrations."
  - [section 4.3] "Following the evaluation of the initial generation, top-performing individuals are selected via tournament selection to generate new individuals through recombination."
  - [corpus] Weak direct corpus support for evolutionary state perturbation specifically for interpretability; neighbor papers focus on policy optimization (BASIL, Trajectory First) rather than demonstration generation.
- Break condition: If the policy is fully deterministic and convergent with near-identical behavior from all initial states (e.g., SAC-150k in FetchReach), evolutionary optimization yields minimal gains over random sampling.

### Mechanism 2
- Claim: A joint surrogate fitness function combining local diversity, global diversity, and certainty identifies trajectories that better represent policy behavior than fidelity alone.
- Mechanism: Fitness F(τ,T) = Dg(τ,T) + min_t∈T ||(Dl(τ)/C(τ)) - (Dl(t)/C(t))||². Dl measures unique states visited within τ; Dg measures minimum distance to existing trajectories in T; C averages action probabilities π(a|s). The Dl/C ratio prioritizes trajectories with high exploration relative to uncertainty.
- Core assumption: Interpretability correlates with covering diverse states (Dl), avoiding redundant demonstrations (Dg), and revealing low-confidence decisions (C).
- Evidence anchors:
  - [abstract] "A joint surrogate fitness function guides the optimization by combining local diversity, behavioral certainty, and global population diversity."
  - [section 3.4] "This formulation ensures that newly selected trajectories balance coverage of unique behaviors (Dg), exploration of diverse state-space interactions (Dl), and exposure of decision-making inconsistencies (C)."
  - [section 6.3] "Using fidelity directly as a fitness function performs worse than even a straightforward sum of the fitness components."
  - [corpus] No direct corpus validation of this specific fitness formulation; surrogate models for RL (neighbor papers) focus on reward approximation, not interpretability metrics.
- Break condition: In highly converged policies with uniform high certainty (C≈1) and consistent behavior, the Dl/C ratio provides little discrimination; global diversity alone may suffice (observed in SAC-50k ablations).

### Mechanism 3
- Claim: Inverse-normalized binary state encoding enables gradient-free exploration of discrete and continuous initial state spaces.
- Mechanism: Each state dimension is encoded as m bits, converted to integer, normalized to [0,1], then mapped to valid state bounds (discrete: floored; continuous: linear scaling). For a 9×9 grid with m=6 bits/dimension, the encoding covers 64 states per dimension with <1.14× probability bias.
- Core assumption: Binary encoding with sufficient bits (m≥6 for 9-state dimensions) minimizes sampling bias while remaining compatible with standard genetic operators.
- Evidence anchors:
  - [section 4.1] "Every dimension d of the startup state space is encoded using a binary encoding e of length m. Each segment of the binary encoding is first mapped to an integer, then normalized, and subsequently mapped to the valid state space."
  - [section 6.2] "For an encoding length of 4, some states are 2 times as probable as others. For an encoding length of 5, this is reduced to 1.33 times... we recommend to use a state encoding length, where a state is less than 1.25 more likely than any other state."
  - [corpus] No corpus papers directly address this encoding scheme; it is a methodological contribution specific to REACT.
- Break condition: If encoding length is too short (m=4 for 9-state dimension), sampling bias distorts population diversity; if too long, computational overhead increases without meaningful diversity gains.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: REACT assumes policy π operates in an MDP ⟨S,A,P,R,μ,γ⟩ and generates trajectories τ = ⟨s₀,a₀,r₁,...⟩ by rolling out π from perturbed initial states s₀.
  - Quick check question: Can you define the transition probability P(s'|s,a) and explain why REACT does not modify it?

- Concept: Evolutionary algorithm operators (selection, crossover, mutation)
  - Why needed here: REACT uses tournament selection, single-point crossover (pc=0.75), and bit-flip mutation (pm=0.5) to evolve the initial state population without gradient information.
  - Quick check question: What is the difference between exploitation (crossover) and exploration (mutation) in this context?

- Concept: Policy certainty/confidence from stochastic policies
  - Why needed here: Certainty C(τ) = (1/|τ|) Σ π(a|s) requires access to action probabilities; deterministic policies require alternative uncertainty measures (e.g., ensemble variance).
  - Quick check question: How would you compute certainty for a deterministic policy?

## Architecture Onboarding

- Component map:
  - StateEncoder: Binary encoding → inverse normalization → environment-compatible initial state s₀
  - TrajectorySampler: Roll out policy π from s₀ for max episode length, remove consecutive duplicates
  - FitnessEvaluator: Compute Dl (unique states/total possible), Dg (min trajectory distance), C (mean π(a|s)), combine via Eq. 5-6
  - EvolutionaryOptimizer: Tournament selection → crossover → mutation → migration (elitist retention)
  - DemonstrationPool: Maintains trajectory set T, prunes extinct individuals each generation

- Critical path: Initialize population (size p=10) → For each individual: sample trajectory → compute fitness → add to T → Repeat for g=40 generations: generate offspring → evaluate → select top p by fitness → Return final T

- Design tradeoffs:
  - Population size: p=10 balances diversity coverage against selection pressure; larger p reduces pressure, slower convergence
  - Encoding length: m=6 bits/dimension for 9-state spaces minimizes bias; continuous spaces require m based on desired precision
  - Fitness components: Joint fitness outperforms simple sum; fidelity-only optimization fails in early-stage policies
  - Mutation/crossover: pm=0.5, pc=0.75 trades off exploration vs exploitation; low pm with high pc under-explores state space

- Failure signatures:
  - Low fidelity IQM (<0.3): Population converged to similar states; increase mutation rate or population size
  - High optimality gap variance with low fidelity: Trajectories diverse but unrepresentative; check Dl/C ratio weighting
  - Certainty component ≈0 throughout: Policy fully converged; REACT provides limited interpretability gains (switch to fidelity optimization or environment perturbation)
  - Encoding bias visible in heatmaps: Increase encoding length m

- First 3 experiments:
  1. Replicate FlatGrid11 (PPO-35k) with p=10, g=40, pm=0.5, pc=0.75, m=6; verify fidelity IQM ≈0.75 and compare state coverage heatmap to Fig. 5e.
  2. Ablation study: Run REACT with only Dg, only Dl, only C, and simple sum; confirm joint fitness achieves highest fidelity as in Fig. 5d.
  3. Test encoding sensitivity: Vary m ∈ {4,5,6,7,8} and plot state occurrence heatmaps; verify m=6 achieves <1.25× probability ratio per Fig. 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the joint fitness function be reformulated to outperform direct fidelity optimization in mature continuous control policies?
- Basis in paper: [explicit] Conclusion states "direct fidelity optimization may be more effective in certain continuous control tasks, suggesting opportunities for refining the fitness function to better suit such domains."
- Why unresolved: REACT underperforms fidelity-based optimization at SAC-100k (fidelity 1.37 vs 2.49) and SAC-150k stages, indicating the diversity-certainty tradeoff may be suboptimal for stable policies.
- What evidence would resolve it: A modified fitness formulation that achieves higher fidelity IQM than direct fidelity optimization on continuous benchmarks like FetchReach, MuJoCo tasks.

### Open Question 2
- Question: Can perturbing environment configurations (obstacles, dynamics, physics parameters) beyond initial states yield more informative demonstrations for converged policies?
- Basis in paper: [explicit] Conclusion: "Future work could extend this framework to explore a broader range of environment configurations, including physical obstacles, dynamic environments, or task redefinitions."
- Why unresolved: Current implementation only modifies agent position/goal, which produces limited behavioral variation when policies are mature and highly consistent.
- What evidence would resolve it: Experiments in HoleyGrid and FetchReach with mutable obstacles, friction, or goal dynamics showing increased demonstration diversity for converged policies.

### Open Question 3
- Question: Do human evaluators judge REACT-generated demonstrations as more interpretable than fidelity-optimized or random baselines?
- Basis in paper: [inferred] Paper acknowledges "trajectory diversity is inherently subjective" and relies solely on automated metrics (fidelity IQM, optimality gap) without human validation.
- Why unresolved: Interpretability claims rest on proxy metrics; no user studies confirm that broader spatial coverage or higher diversity actually improves human understanding.
- What evidence would resolve it: Controlled human subject experiments measuring comprehension accuracy, time-to-insight, and subjective interpretability ratings across REACT, fidelity, and random demonstrations.

## Limitations
- Binary state encoding may underrepresent valid states in continuous spaces if m is insufficient.
- Policy certainty relies on access to action probabilities; deterministic policies require alternative uncertainty measures not validated.
- REACT's effectiveness is strongly tied to the assumption that policy behavior varies meaningfully with initial states—policies with uniform behavior across states limit interpretability gains.

## Confidence
- **High**: Joint surrogate fitness outperforms individual components; encoding length ≥6 bits/dim reduces sampling bias; evolutionary optimization improves fidelity vs random sampling in early/mid policy training.
- **Medium**: Fidelity gains persist in later training stages but are smaller; transferability of encoding scheme to other continuous domains without validation.
- **Low**: Direct corpus validation of the specific fitness formulation; generalizability to non-episodic or multi-goal tasks.

## Next Checks
1. Test REACT on a deterministic policy variant (e.g., hard-max action selection) and compare interpretability gains to stochastic baselines.
2. Evaluate encoding bias across m ∈ {4,5,6,7,8} in continuous FetchReach and plot state occurrence distributions to confirm <1.25× probability ratio at m=6.
3. Apply REACT to a multi-goal RL task (e.g., FetchPickAndPlace) and assess whether the fitness function still balances diversity and certainty without a fixed goal state.