---
ver: rpa2
title: A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking
  on the Blockchain
arxiv_id: '2508.09993'
source_url: https://arxiv.org/abs/2508.09993
tags:
- fairness
- language
- evaluation
- metrics
- protocol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a blockchain-based protocol for transparent,
  reproducible, and immutable fairness evaluation of open-source large language models
  (LLMs). The authors deploy smart contracts on the Internet Computer Protocol (ICP)
  to store benchmark datasets, prompts, and fairness metrics directly on-chain, ensuring
  verifiable linkage between specific model versions and evaluation results.
---

# A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain

## Quick Facts
- **arXiv ID:** 2508.09993
- **Source URL:** https://arxiv.org/abs/2508.09993
- **Reference count:** 6
- **Primary result:** Blockchain-based protocol for transparent, reproducible, and immutable fairness evaluation of open-source LLMs, benchmarking Meta Llama 3.1-8B, DeepSeek R1 Distill Llama 8B, and Mistral 7B Instruct models.

## Executive Summary
This paper introduces a blockchain-based protocol for transparent and reproducible fairness evaluation of open-source large language models. The authors deploy smart contracts on the Internet Computer Protocol (ICP) to store benchmark datasets, prompts, and fairness metrics immutably on-chain, ensuring verifiable linkage between specific model versions and evaluation results. Evaluations are executed via HTTP outcalls to hosted Hugging Face model endpoints, with results logged for public auditing. The study benchmarks three models using PISA, StereoSet, and Kaleidoscope datasets, measuring fairness across English, Spanish, and Portuguese. Llama consistently outperformed other models in fairness metrics, accuracy, and multilingual robustness.

## Method Summary
The protocol deploys smart contracts on ICP to store benchmark datasets, prompts, and fairness metrics directly on-chain. Evaluations are executed via HTTP outcalls to hosted Hugging Face model endpoints, with results logged immutably for public auditing. The study benchmarks Meta Llama 3.1-8B, DeepSeek R1 Distill Llama 8B, and Mistral 7B Instruct models using PISA dataset for academic performance prediction, StereoSet for bias measurement, and Kaleidoscope dataset for multilingual fairness across English, Spanish, and Portuguese. Fairness metrics include statistical parity difference, equal opportunity difference, average odds difference, and disparate impact ratio, along with structured Context Association Test (ICAT) scores for stereotype and language modeling performance.

## Key Results
- Llama consistently outperformed other models in fairness metrics, accuracy, and multilingual robustness
- ICAT scores reached up to 65.36 with lowest error rates across languages
- The approach provides a practical, ethical framework for continuous fairness auditing of LLMs
- All code and evaluation infrastructure are open source, supporting reproducible and accountable AI development

## Why This Works (Mechanism)
The protocol leverages blockchain immutability to create an auditable trail linking specific model versions to their fairness evaluation results. By storing prompts, datasets, and metrics on-chain through ICP smart contracts, the system ensures that fairness assessments cannot be retroactively altered or manipulated. The HTTP outcall mechanism enables interaction with hosted model endpoints while maintaining on-chain verification of results. This architecture enables community-driven oversight and longitudinal tracking of model behavior, creating a transparent ecosystem for AI fairness evaluation.

## Foundational Learning
- **Internet Computer Protocol (ICP)**: Blockchain platform supporting smart contracts and canister execution; needed for decentralized storage of evaluation data; quick check: verify canister deployment on mainnet
- **Statistical Parity Difference**: Measures demographic parity across groups; needed to quantify bias in model outputs; quick check: ensure calculation matches standard definition
- **Equal Opportunity Difference**: Measures false negative rate equality across groups; needed for fairness assessment; quick check: validate against reference implementations
- **Average Odds Difference**: Combines false positive and false negative rate differences; needed for comprehensive fairness metrics; quick check: confirm mathematical formulation
- **Disparate Impact Ratio**: Ratio of favorable outcomes between groups; needed to identify systematic bias; quick check: verify calculation methodology
- **Context Association Test (ICAT)**: Measures stereotype associations and language model performance; needed for nuanced bias assessment; quick check: confirm scoring methodology

## Architecture Onboarding

**Component Map:**
Hugging Face Model Endpoints -> ICP Canister (via HTTP Outcall) -> On-chain Storage -> Public Auditing Interface

**Critical Path:**
1. Prompt submission triggers canister execution
2. HTTP outcall sends prompts to Hugging Face endpoint
3. Model responses returned to canister
4. Fairness metrics computed on-chain
5. Results stored immutably on blockchain
6. Public interface displays audit trail

**Design Tradeoffs:**
- Centralized model hosting vs. fully decentralized evaluation
- HTTP outcall dependency vs. on-chain computation limits
- Limited language coverage vs. evaluation complexity
- Binary classification metrics vs. open-ended generation nuances

**Failure Signatures:**
- HTTP outcall timeouts indicate hosting endpoint issues
- Transaction failures suggest blockchain congestion
- Inconsistent results across executions indicate prompt or metric calculation errors
- Missing language coverage reveals translation or dataset limitations

**3 First Experiments:**
1. Deploy sample canister and verify HTTP outcall functionality with test model
2. Execute fairness evaluation on single model with minimal prompt set
3. Validate on-chain storage of results through public blockchain explorer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the primary causal factors (translation bias, tokenization artifacts, or cultural assumptions in pre-training data) driving the observed cross-linguistic fairness disparities?
- **Basis in paper:** [explicit] Authors explicitly state these three factors as potential causes but do not test them: "raising important concerns about potential translation bias, tokenization artifacts, and cultural assumptions embedded in pre-training data."
- **Why unresolved:** The paper only measures disparities across English, Spanish, and Portuguese; it does not include controlled experiments to isolate root causes.
- **What evidence would resolve it:** Ablation studies using translated vs. natively-authored prompts, tokenizer analysis across languages, and correlation of fairness metrics with pre-training corpus language distribution.

### Open Question 2
- **Question:** How do fairness metrics evolve across successive model versions (e.g., Llama 3.1 → 3.2 → 3.3) within the same model family?
- **Basis in paper:** [explicit] The paper explicitly mentions "longitudinal fairness tracking across model versions" as a capability enabled by the protocol, but all reported experiments use single model snapshots.
- **Why unresolved:** Only one version per model (Llama-3.1-8B, DeepSeek R1 Distill, Mistral 7B) is evaluated.
- **What evidence would resolve it:** Repeating the same on-chain evaluation pipeline across multiple versions of each model family and analyzing metric drift over time.

### Open Question 3
- **Question:** Does the on-chain HTTP-based evaluation protocol scale efficiently to significantly larger prompt sets and models?
- **Basis in paper:** [inferred] Evaluation used relatively small prompt sets (500–4229 prompts) and 7–8B parameter models; blockchain-based HTTP outcalls and on-chain metric computation may impose latency, cost, or throughput limits at scale.
- **Why unresolved:** The paper does not report computational costs, latency, or throughput measurements.
- **What evidence would resolve it:** Benchmarks of execution time, canister cycle consumption, and success rates as prompt count and model size increase.

## Limitations
- Reliance on HTTP outcalls to hosted Hugging Face endpoints creates potential single point of failure
- Multilingual fairness assessment limited to three languages (English, Spanish, Portuguese)
- Fairness metrics focus primarily on binary classification fairness, potentially overlooking open-ended generation nuances
- Does not address potential adversarial attacks on blockchain storage mechanism

## Confidence
- **High Confidence:** Technical implementation of blockchain-based storage and smart contract deployment are well-documented and verifiable
- **Medium Confidence:** Fairness metric calculations and comparative performance analysis are methodologically sound
- **Low Confidence:** Claims about community-driven oversight and longitudinal tracking benefits are aspirational rather than empirically demonstrated

## Next Checks
1. Conduct stress test of HTTP outcall mechanism by evaluating 50+ model versions simultaneously to assess system reliability
2. Extend multilingual fairness evaluation to include at least 10 additional languages spanning different language families
3. Implement and test decentralized model hosting solution (e.g., IPFS-based) to eliminate dependency on centralized Hugging Face endpoints