---
ver: rpa2
title: 'ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision
  Language Models in Chart Understanding'
arxiv_id: '2509.17481'
source_url: https://arxiv.org/abs/2509.17481
tags:
- question
- chart
- answer
- response
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHARTHAL is the first fine-grained benchmark for evaluating hallucinations
  in Large Vision-Language Models (LVLMs) on chart understanding tasks. It introduces
  a systematic taxonomy categorizing questions by type (descriptive, reasoning, open-ended)
  and chart-question relation (irrelevant, inexistent, contradictory, normal), yielding
  12 distinct hallucination-triggering scenarios.
---

# ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding

## Quick Facts
- arXiv ID: 2509.17481
- Source URL: https://arxiv.org/abs/2509.17481
- Authors: Xingqi Wang; Yiming Cui; Xin Yao; Shijin Wang; Guoping Hu; Xiaoyu Qin
- Reference count: 27
- First fine-grained benchmark for evaluating hallucinations in LVLMs on chart understanding

## Executive Summary
ChartHal introduces the first systematic framework for evaluating hallucinations in Large Vision-Language Models (LVLMs) when interpreting charts. The benchmark employs a fine-grained taxonomy that categorizes questions by type (descriptive, reasoning, open-ended) and chart-question relation (irrelevant, inexistent, contradictory, normal), creating 12 distinct hallucination-triggering scenarios. Experiments on 15 state-of-the-art LVLMs reveal severe vulnerabilities: even top models like GPT-5 and o4-mini achieve only 34.46% and 22.79% overall accuracy respectively, with errors concentrated in unanswerable cases where models frequently fabricate content rather than recognizing unanswerability.

## Method Summary
The framework constructs a human-validated dataset of 1,062 chart-question pairs sampled from scientific charts in the CharXiv validation set, with charts restricted to 1–2 subplots at 3:2 ratio and proportional sampling across academic disciplines. QA pairs are generated using o4-mini with provided prompts across all 12 taxonomy categories, then verified by human annotators. Models are evaluated using free-form answering with temperature=0, and responses are graded by GPT-4o-2024-11-20 as binary correct/incorrect based on whether they match ground truth or explicitly recognize unanswerability. The evaluation protocol uses specific prompts to assess whether answers contain hallucinations.

## Key Results
- GPT-5 achieves only 34.46% overall accuracy on ChartHal, while o4-mini scores 22.79%
- Errors concentrate heavily in unanswerable cases: models perform near 0% on contradictory questions
- Models fail to recognize unanswerability in "inexistent" and "contradictory" categories, frequently fabricating specific answers
- Increasing reasoning effort in some models amplifies hallucination when visual perception is flawed
- Performance varies significantly across the 12 taxonomy categories, validating the framework's ability to reveal specific failure modes

## Why This Works (Mechanism)

### Mechanism 1: Unanswerability-Induced Fabrication
The paper suggests LVLMs are trained on valid QA pairs, creating a "compulsion to answer" that leads them to fabricate content when visual evidence is missing or contradictory rather than recognizing unanswerability. This generation bias stems from standard training distribution that lacks sufficient unanswerable examples.

### Mechanism 2: Reasoning Amplification of Visual Misperception
Reasoning modules operate sequentially on encoded visual features. When the vision encoder misreads chart elements (visual error), the subsequent reasoning layer treats this error as fact, constructing elaborate but incorrect justifications. Correct reasoning relies on accurate visual perception, and when visual input is misinterpreted, extra reasoning amplifies the error.

### Mechanism 3: Fine-Grained Taxonomy as a Stress Test
The intersection of Question Type and Chart-Relation acts as a high-resolution probe that isolates specific failure modes invisible to coarse benchmarks. By forcing models to handle "Open-ended" or "Contradictory" queries, the benchmark disrupts standard pattern-matching heuristics and requires semantic validation of question premises against visual states.

## Foundational Learning

- **Visual-Language Misalignment**: Understanding how vision tokens are injected into LLM context is crucial since the core failure involves the LLM ignoring Vision Encoder's signals. Quick check: Does the model have a mechanism to "abort" generation if attention weights on vision tokens are low or contradictory?

- **The "Compulsion to Answer" Bias**: Models are trained to answer, not to abstain. Understanding RLHF helps explain why models prefer confident lies over uncertain silence. Quick check: In standard instruction tuning, what is the ratio of "unanswerable" training examples to "answerable" ones?

- **Chart Semantics vs. Natural Images**: Charts are symbolic (axes, legends, data points) unlike natural photos. Hallucinations here involve non-existent data points or axis misreadings, requiring specific domain knowledge. Quick check: How does the model tokenize a line graph—is it as a raster image or does it have access to underlying data tables/OCR? (In this paper, it is image-only).

## Architecture Onboarding

- **Component map**: Input (Chart Image + Question) -> Vision Encoder (e.g., ViT) -> Projector (maps to language space) -> LLM Backbone (generates text) -> Evaluator (GPT-4o judges responses)

- **Critical path**: The Chart-Question Relation is the critical node. The system fails when the LLM backbone receives a question premise that contradicts visual features but generates an answer anyway.

- **Design tradeoffs**: Free-form vs. Multiple Choice (free-form simulates reality but requires LLM judge); Scientific vs. General Charts (scientific charts maximize stress but reduce generalizability).

- **Failure signatures**: 
  - "Confabulator": Model answers specific questions about non-existent years with precise numbers
  - "Ignorer": Model fails to correct false premises in questions
  - "Over-Thinker": Model uses high reasoning effort to justify visually incorrect values

- **First 3 experiments**:
  1. Run a capable open-source model on Normal category only vs. Inexistent category to confirm hallucination vulnerability (>40% performance drop expected)
  2. Toggle "reasoning effort" or compare base model vs. Chain-of-Thought prompted version to verify if CoT increases errors on Contradictory questions
  3. Append "If information is not in chart, say 'N/A'" to prompt and re-evaluate on Inexistent subset to test if prompting can patch answerability bias

## Open Questions the Paper Calls Out

- Can specific mitigation strategies like adversarial training or calibration effectively reduce hallucinations in unanswerable chart scenarios? The paper diagnoses behaviors without testing mitigation techniques.

- Do LVLMs exhibit similar hallucination vulnerabilities in non-scientific chart domains such as business or journalism? The dataset is limited to scientific charts from arXiv papers.

- Does increasing reasoning effort in proprietary models inherently amplify errors when visual perception is compromised? The paper observes this effect but doesn't investigate the mechanistic link between reasoning steps and visual hallucination solidification.

## Limitations

- Human verification process details are not fully specified, including number of annotators and qualification criteria
- Evaluation relies entirely on GPT-4o as judge, which may introduce systematic biases
- Dataset construction using o4-mini for initial QA generation could introduce selection bias toward questions that particular model handles well

## Confidence

- High confidence: The taxonomy framework and 12-category classification system is well-defined and methodologically sound
- Medium confidence: Reported accuracy scores for proprietary models are likely accurate but cannot be independently verified without access
- Medium confidence: The claim that reasoning amplifies hallucination when visual perception is flawed is supported by experimental evidence but requires further validation

## Next Checks

1. **Evaluator Bias Test**: Run a subset of responses through multiple LLM judges (GPT-4o, Claude-3, Gemini-1.5) to quantify agreement rates and identify systematic scoring biases

2. **Category Robustness Check**: Remove the human verification step for 100 randomly selected samples and measure how often GPT-4o assigns different category labels than original human annotations

3. **Cross-Domain Generalization**: Test a subset of models on non-scientific charts (business graphs, infographics) to evaluate whether severe hallucination vulnerabilities extend beyond academic chart domain