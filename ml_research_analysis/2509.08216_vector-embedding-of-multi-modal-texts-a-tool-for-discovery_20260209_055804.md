---
ver: rpa2
title: 'Vector embedding of multi-modal texts: a tool for discovery?'
arxiv_id: '2509.08216'
source_url: https://arxiv.org/abs/2509.08216
tags:
- query
- retrieval
- page
- vector
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of vector-based multimodal retrieval,
  powered by vision-language models, to improve discovery across multi-modal content
  in digitized computer science textbooks. Using over 3,600 digitized textbook pages,
  the study generates multi-vector representations capturing both textual and visual
  semantics using the ColPali model.
---

# Vector embedding of multi-modal texts: a tool for discovery?

## Quick Facts
- arXiv ID: 2509.08216
- Source URL: https://arxiv.org/abs/2509.08216
- Reference count: 19
- Using vector-based multimodal retrieval with vision-language models to improve discovery across digitized computer science textbooks.

## Executive Summary
This study evaluates the effectiveness of vector-based multimodal retrieval for discovering content in digitized computer science textbooks. The researchers use the ColPali model to generate multi-vector embeddings that capture both textual and visual semantics from over 3,600 textbook pages. These embeddings are stored in a vector database and evaluated using 75 natural language queries. The results demonstrate that cosine similarity is the most effective distance measure for retrieving semantically and visually relevant pages, outperforming other metrics like Dot Product, Euclidean, and Manhattan distances.

## Method Summary
The researchers employed the ColPali model (built on PaliGemma-3B) to process 3,612 digitized textbook pages at 300 DPI. Each page image was divided into 1,030 visual patches, creating a 1,030Ã—128 matrix per page. These multi-vector embeddings were stored in a Qdrant vector database with four collections configured for different distance metrics. The system used Late Interaction Scoring (ColBERT-style) to align fine-grained query tokens with specific visual features on pages. Retrieval quality was evaluated using 75 natural language queries with metrics including Precision@5, Recall@5, F1@5, Average Precision, and Mean Reciprocal Rank.

## Key Results
- Cosine similarity achieved the highest precision (0.514) and MRR (0.801) across the full benchmark
- Late interaction scoring enabled precise retrieval by aligning query tokens with specific visual features
- The approach eliminated the need for complex preprocessing pipelines involving OCR and layout detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing full page images directly with a vision-language model creates a unified representation of text, structure, and diagrams, bypassing brittle preprocessing pipelines.
- Mechanism: The ColPali model divides each page image into 1030 visual patches. The PaliGemma-3B model then processes these patches and projects them into a shared 128-dimensional embedding space. This captures textual and visual semantics in a single pass.
- Core assumption: The VLM can derive semantic meaning from visual patches that is comparable to its understanding of text tokens.
- Evidence anchors:
  - [abstract]: "Using over 3,600 digitized textbook pages... we generate multi-vector representations capturing both textual and visual semantics."
  - [section]: "Adoption of ColPali eliminates the need for complex preprocessing pipelines involving OCR, layout detection, and chunking." and "Each page image is divided into 1030 visual patches... projected into a shared 128-dimensional embedding space."
- Break condition: If visual content is not semantically aligned with its textual context (e.g., decorative, unrelated images), the unified embedding may conflate unrelated concepts.

### Mechanism 2
- Claim: Cosine similarity is the most effective distance measure for this multimodal retrieval task because it prioritizes semantic direction over vector magnitude.
- Mechanism: In the shared embedding space, cosine similarity measures the angle between the query and page vectors. This allows it to retrieve pages with strong semantic alignment (e.g., a diagram matching a text query) regardless of the magnitude of the vectors, which other metrics like Dot Product can be biased by.
- Core assumption: Semantic alignment in this VLM-created embedding space is best captured by vector direction, not length.
- Evidence anchors:
  - [abstract]: "We find that cosine similarity most effectively retrieves semantically and visually relevant pages."
  - [section]: Table II shows Cosine similarity achieving the highest precision (0.514) and MRR (0.801) on the full benchmark. The text notes that "Dot Product... may bias results toward longer vectors."
- Break condition: This finding is conditional on the ColPali/PaliGemma embedding space. Other models may produce embeddings where magnitude is a meaningful signal, making Dot Product or Euclidean distance more appropriate.

### Mechanism 3
- Claim: Late interaction scoring enables precise retrieval by aligning fine-grained query tokens with specific visual features on a page.
- Mechanism: Queries and documents are represented as sequences of vectors (token embeddings and visual patch embeddings, respectively). The relevance score is calculated by finding the maximum similarity between each query token and all document patches, then aggregating these maxima. This allows a query to be answered by the most relevant part of a page.
- Core assumption: A page's relevance to a query is often concentrated in specific regions (e.g., one diagram, one paragraph), not uniformly distributed.
- Evidence anchors:
  - [abstract]: Not explicitly mentioned.
  - [section]: "The key to our scalable approach is a technique... called Late Interaction Scoring (LI)." and "This ensures the system focuses on the most relevant parts of each page."
- Break condition: Computationally more intensive than single-vector search. Performance gains may diminish for very short queries or pages where the relevant content is not localized to a few patches.

## Foundational Learning

- Concept: **Vision-Language Models (VLMs)**
  - Why needed here: The core of this system is a VLM (PaliGemma-3B) that can "see" page images and understand them semantically. Without grasping how it fuses visual and textual modalities, its ability to retrieve diagrams from text queries is a black box.
  - Quick check question: How does a VLM differ from a standard image classifier or a text-only language model?

- Concept: **Vector Embeddings & Vector Space**
  - Why needed here: The entire retrieval strategy depends on mapping both text queries and complex page images into a shared mathematical space where "distance equals semantic dissimilarity."
  - Quick check question: In a 2D vector space, if two points are close together, what does that mean for the data they represent?

- Concept: **Late Interaction (ColBERT-style)**
  - Why needed here: This is the specific retrieval architecture used (via ColPali). It differs from common single-vector search and explains why the system is good at finding specific details on a page.
  - Quick check question: In late interaction, does the model create one vector for the whole document, or a vector for each of its parts? Why would that help with retrieval?

## Architecture Onboarding

- Component map:
  - PDFs -> PyMuPDF (page images) -> ColPali Model -> Storage (Qdrant Vector DB with 4 collections, one per distance metric)
  - User Query -> ColPali Model -> Query Embedding -> Qdrant DB (Late Interaction search) -> Top-k Retrieval -> UI/Frontend
  - Model Server: Hosts PaliGemma-3B for both ingestion (offline) and querying (online)

- Critical path:
  1. **Embedding Quality:** The pipeline's success hinges on the PaliGemma-3B model's ability to create meaningful embeddings from page patches. Poor embeddings cannot be fixed by tuning the vector database.
  2. **Late Interaction:** The scoring logic (Max-Sum Pooling) must be correctly implemented to aggregate similarity scores between query tokens and document patches.

- Design tradeoffs:
  - **Complexity vs. Performance:** Using ColPali with late interaction is more complex than a simple OCR + text embedding pipeline but enables retrieval of visual content.
  - **Control vs. Generality:** Using a single, fixed VLM (PaliGemma) for both text and images simplifies the stack but locks the system into that model's capabilities and biases.
  - **Cost:** Storing 1030 vectors per page (multi-vector embeddings) increases storage and compute costs compared to single-vector approaches.

- Failure signatures:
  - **Visually Dense but Semantically Empty Retrieval:** High scores for pages with large, complex diagrams that are only tangentially related to the query. This suggests the model is over-weighting visual complexity.
  - **Text-Only Failure:** Retrieving pages with relevant text but missing the critical diagram, suggesting a failure in the multimodal alignment of the VLM.
  - **Metric Sensitivity:** If Dot Product suddenly outperforms Cosine by a large margin, it may indicate a change in the embedding model's output normalization.

- First 3 experiments:
  1. **Baseline Reproduction:** Run the 75-query benchmark against the existing system, confirming that cosine similarity yields the best Precision@5 (~0.514). This validates the setup.
  2. **Ablation on Patch Count:** Retrain the embedding model (or adjust its input) to use fewer visual patches (e.g., 500) and measure the drop in retrieval quality for visual queries. This tests the importance of granularity.
  3. **Query-Type Segmentation:** Isolate the "Visual" (9 queries) and "Textual" (14 queries) subsets from the benchmark and run them separately. Compare metric performance (Cosine vs. Dot) to see if one metric is better for pure visual retrieval vs. pure textual retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why is Mean Reciprocal Rank (MRR) significantly lower for multi-page and conceptual query subsets compared to the full benchmark?
- Basis in paper: [explicit] The authors note the performance drop in Section V, stating, "It is also interesting that the Mean Reciprocal Rank (MRR) is lower... This needs further investigation."
- Why unresolved: The paper reports the statistical anomaly but does not determine if the cause is the metric's sensitivity to distributed content or the model's ranking behavior for abstract reasoning.
- What evidence would resolve it: Ablation studies isolating query types and a correlation analysis between MRR scores and the spatial distribution of relevant content across pages.

### Open Question 2
- Question: How does the inclusion of "relevant-but-unlabeled" pages in ground truth datasets affect the measured performance of multimodal retrieval systems?
- Basis in paper: [explicit] Section V states, "The next step in this work is strengthen the completeness and relevance of the ground truth itself," following findings that strict labels missed semantically relevant results.
- Why unresolved: Current evaluation relies on manually curated "gold standards" that failed to capture 72% of the high-scoring semantic matches found in Experiment IV.
- What evidence would resolve it: A comparative evaluation using an expanded ground truth that incorporates the serendipitous but valid results identified via GPT-4 and human validation.

### Open Question 3
- Question: Can page-level re-ranking models or context-aware strategies effectively overcome the moderate precision limitations observed with distance metrics alone?
- Basis in paper: [explicit] In Section IV, the authors conclude that "No distance measure achieved strong precision accuracy, indicating that future work could explore page-level re-ranking models."
- Why unresolved: The study isolated distance metrics as the sole variable; it did not test secondary processing layers to refine the initial vector retrieval results.
- What evidence would resolve it: Benchmarks comparing the output of the raw cosine similarity retrieval against a pipeline that includes a cross-encoder or learning-to-rank stage.

## Limitations

- The results are tightly coupled to the PaliGemma-3B/ColPali architecture and may not generalize to other VLMs or domains
- The "Visual" subset of queries is small (9 queries), potentially limiting claims about multimodal retrieval effectiveness
- Manual curation of ground truth triplets introduces potential human bias without reported inter-annotator agreement

## Confidence

- **High Confidence:** The experimental methodology is sound, the retrieval metrics are standard, and the results (cosine > dot product > Euclidean > Manhattan) are internally consistent with the ColPali architecture.
- **Medium Confidence:** The claim that late interaction enables precise retrieval is supported by the architecture description, but the paper does not provide ablation studies showing the degradation in performance if this feature is removed.
- **Low Confidence:** The paper's broader implications for "operational information retrieval" and "digital libraries" are asserted but not empirically validated beyond the specific CS textbook corpus.

## Next Checks

1. **Generalization Test:** Replicate the experiment on a non-CS corpus (e.g., medical textbooks or legal documents) to test if cosine similarity remains the optimal metric across domains.
2. **Architectural Ablation:** Modify the system to use single-vector search (averaging the 1030 patch vectors) and measure the degradation in retrieval quality, particularly for visual queries.
3. **Query-Type Analysis:** Perform a statistical analysis of retrieval performance broken down by query category (Visual, Textual, Multi-Modal) to identify if different distance metrics are optimal for different query types.