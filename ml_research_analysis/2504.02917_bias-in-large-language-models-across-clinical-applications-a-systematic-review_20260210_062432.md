---
ver: rpa2
title: 'Bias in Large Language Models Across Clinical Applications: A Systematic Review'
arxiv_id: '2504.02917'
source_url: https://arxiv.org/abs/2504.02917
tags:
- bias
- clinical
- were
- gender
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review identified pervasive bias in large language
  models (LLMs) across clinical applications, with 38 studies revealing significant
  issues stemming from both data-related and model-related sources. Bias manifested
  as allocative harm (differential treatment recommendations), representational harm
  (stereotypical associations, biased image generation), and performance disparities,
  affecting multiple attributes including race/ethnicity, gender, age, disability,
  and language.
---

# Bias in Large Language Models Across Clinical Applications: A Systematic Review

## Quick Facts
- arXiv ID: 2504.02917
- Source URL: https://arxiv.org/abs/2504.02917
- Reference count: 0
- Primary result: Systematic review identifies pervasive bias in clinical LLMs across data-related and model-related sources, manifesting as allocative and representational harm

## Executive Summary
This systematic review of 38 studies reveals that bias in large language models (LLMs) is a pervasive problem in clinical applications, stemming from both biased training data and model architecture choices. Bias manifests in three primary forms: allocative harm (differential treatment recommendations), representational harm (stereotypical associations and biased image generation), and performance disparities across patient demographics. The review found that LLMs systematically underperform or provide inequitable recommendations for marginalized groups across multiple attributes including race/ethnicity, gender, age, disability, and language. The authors call for standardized bias evaluation methods and comprehensive mitigation strategies addressing the entire LLM lifecycle.

## Method Summary
The authors conducted a systematic review following PRISMA protocol (PROSPERO: 649773), searching PubMed, OVID, and EMBASE from database inception through January 27, 2025. After screening 1,065 records, 38 studies met inclusion criteria for investigating bias in LLMs for medical/clinical applications. Data extraction followed a structured framework capturing LLM type, bias source, manifestation, affected attributes, clinical tasks, and evaluation methods. Risk of bias was assessed using a modified ROBINS-I tool across seven domains. Due to heterogeneity, a narrative synthesis approach was employed using a three-view bias classification framework examining source, manifestation, and affected attributes.

## Key Results
- Bias sources identified: data-related (35 studies), model-related (9 studies), and deployment-related (0 studies)
- Bias manifestations: allocative harm (differential treatment recommendations), representational harm (stereotypical associations, biased image generation), and performance disparities
- Affected attributes: race/ethnicity (22 studies), gender (22 studies), age (6 studies), disability (3 studies), and language (3 studies)
- Risk of bias assessment: mostly low risk across most domains, though outcome measurement and selective reporting showed moderate risk in several studies

## Why This Works (Mechanism)

### Mechanism 1: Historical Data Reflection
LLMs encode and propagate existing societal and clinical inequities because they are trained on historical data that reflects past healthcare disparities. The model learns statistical associations between demographic attributes (race, gender) and clinical outcomes present in the training corpus. If the corpus reflects historical biases (e.g., undertreatment of pain in specific groups), the model predicts outputs consistent with those historical patterns rather than current clinical best practices.

### Mechanism 2: Algorithmic Amplification via Model Design
Bias is not solely derived from data; model architecture and training processes can amplify disparities or introduce model-specific biases. Different models (e.g., Gemini vs. GPT-4) apply different weights and optimization strategies during training. These internal algorithmic choices can lead to divergent outputs for identical clinical prompts, suggesting the model structure itself acts as a confounding variable in clinical fairness.

### Mechanism 3: Representational Stereotyping in Generation
Generative models systematically underrepresent marginalized groups or depict them stereotypically, reinforcing societal hierarchies. When prompted to generate images or text of professionals (e.g., surgeons), the model retrieves the most probable association from its latent space. If the training data over-represents White males in high-status roles, the model's generative probability distribution skews heavily toward that demographic, effectively "erasing" diversity.

## Foundational Learning

- **Concept: Allocative vs. Representational Harm**
  - Why needed here: To classify whether a model's failure is causing direct resource inequality (Allocative: e.g., denying treatment) or reinforcing cultural stigma (Representational: e.g., stereotyping)
  - Quick check question: Does a model suggesting lower pain medication for a specific race constitute allocative or representational harm? (Answer: Allocative)

- **Concept: Confounding in Clinical Data**
  - Why needed here: Understanding that attributes like "race" often correlate with "social determinants of health" (e.g., income, access) in data, making it hard to isolate the source of bias
  - Quick check question: If a model predicts worse outcomes for patients from a specific zip code, is it biased against the location or the underlying socioeconomic factors?

- **Concept: Ground Truth Alignment**
  - Why needed here: Evaluating bias requires a "ground truth" (e.g., clinical guidelines). If the ground truth itself is biased, the evaluation is flawed
  - Quick check question: In the study, what served as the ground truth for evaluating opioid recommendations? (Answer: Comparison between models or clinical guidelines, though often no single standard exists)

## Architecture Onboarding

- **Component map:** Clinical Vignettes/Prompts (containing demographic variables) -> LLM (Transformer architecture, weights, training corpus) -> Comparison against Ground Truth (Guidelines or Human Expert) to detect Disparity

- **Critical path:** The prompt design is the most critical point. As seen in the study, simple "question/retrieval" prompts yield variable results. The path from *Demographic Variable in Prompt* → *Model Inference* → *Output Disparity* is the primary audit trail

- **Design tradeoffs:** Fine-tuning on local data improves performance (Context: Hasheminasab 2024 study in Table 1) but risks overfitting to local biases. General models (GPT-4) offer broad utility but lack cultural nuance

- **Failure signatures:**
  - Allocative Failure: Model recommends different treatment plans for identical clinical vignettes where only the patient's name/race is changed
  - Representational Failure: Text-to-image generation produces 100% White male figures for "Doctor" prompts

- **First 3 experiments:**
  1. Perturbation Stress Test: Run 100 identical clinical vignettes through the model while randomizing only the race/gender fields. Measure variance in treatment recommendation (Allocative Harm)
  2. Counterfactual Image Generation: Prompt the image generator for specific medical roles (e.g., "Surgeon") without demographic markers, then calculate the statistical deviation of the output demographics from the real-world population statistics (Representational Harm)
  3. Model Parity Check: Input the same clinical scenario into two different models (e.g., Gemini vs. GPT-4) and grade the "harshness" or "dosage" of recommended treatments to identify model-specific bias profiles

## Open Questions the Paper Calls Out

- How does clinician interaction with LLMs in active clinical workflows introduce or amplify deployment bias?
  - Basis: No studies assessed deployment bias; need to study real-world clinician-LLM interactions

- Can standardized metrics for bias evaluation be established to facilitate meaningful comparisons across different clinical LLMs?
  - Basis: High variability in evaluation methods prevents direct comparison between studies

- Which bias mitigation strategies are most effective across the data, model, and deployment stages of the LLM lifecycle?
  - Basis: Current strategies are nascent; need validated tools for mitigation at multiple levels

## Limitations

- The ROBINS-I adaptation for LLM bias assessment lacks detailed operational criteria, introducing subjectivity in risk-of-bias judgments
- Limited evidence on mechanistic links between training data characteristics and specific bias outcomes; most studies show correlation rather than causation
- Insufficient validation of ground truth standards used for bias evaluation across studies

## Confidence

- **High confidence**: Pervasive bias exists across clinical LLMs (supported by 38 studies)
- **Medium confidence**: Data-related and model-related sources contribute differently to bias (mechanistic evidence limited)
- **Medium confidence**: Allocative and representational harms are distinct and measurable (operational definitions vary across studies)

## Next Checks

1. Replicate ROBINS-I assessment on a subset of included studies using detailed scoring rubrics to test inter-rater reliability
2. Conduct sensitivity analysis by re-running bias assessments with different ground truth standards (clinical guidelines vs. expert consensus)
3. Perform targeted experiments testing whether bias persists after controlled data curation (removing demographic proxies) versus model fine-tuning