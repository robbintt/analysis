---
ver: rpa2
title: High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation
arxiv_id: '2508.05570'
source_url: https://arxiv.org/abs/2508.05570
tags:
- term
- where
- lemma
- proposition
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation

## Quick Facts
- **arXiv ID:** 2508.05570
- **Source URL:** https://arxiv.org/abs/2508.05570
- **Reference count:** 40
- **Primary result:** Richardson-Romberg extrapolation cancels leading linear bias in constant-step-size Markovian LSA, achieving optimal covariance scaling.

## Executive Summary
This paper analyzes the high-order error structure of constant-step-size Linear Stochastic Approximation (LSA) with Markovian noise. The authors show that Polyak-Ruppert averaging alone cannot eliminate a leading-order linear bias term, which motivates the use of Richardson-Romberg extrapolation. By combining two LSA processes with different step sizes using the same noise sequence, the method effectively cancels this bias and recovers the optimal covariance matrix typically associated with decreasing step-size schemes.

## Method Summary
The method combines two constant-step-size LSA processes with different step sizes (α and 2α) that share the same Markovian noise sequence. Both processes use Polyak-Ruppert averaging, and the final estimator is a linear combination 2θ̄^(α) - θ̄^(2α). This Richardson-Romberg extrapolation cancels the leading linear bias term while preserving the variance structure, theoretically achieving optimal error scaling.

## Key Results
- Under uniform geometric ergodicity, the bias of constant-step-size LSA contains a leading O(α) term that PR averaging cannot eliminate
- Richardson-Romberg extrapolation cancels this leading bias, leaving only higher-order error terms
- The resulting estimator achieves optimal covariance scaling matching that of decreasing-step-size methods
- The method requires careful step size selection proportional to n^(-1/2) and shared noise realizations

## Why This Works (Mechanism)

### Mechanism 1: Bias Characterization via Linearization
The paper shows that under Markovian noise, the asymptotic bias of constant-step-size LSA contains a leading term scaling linearly with α. Using linearization, the error decomposes into transient and fluctuation terms, with the fluctuation term's bias dominated by αΔ plus higher-order remainders. This requires the underlying Markov chain to be uniformly geometrically ergodic and the matrix Ā to be Hurwitz.

### Mechanism 2: Richardson-Romberg (RR) Bias Cancellation
The RR estimator 2θ̄^(α) - θ̄^(2α) cancels the leading linear bias term by exploiting linearity. If Bias(θ̄) ≈ c·α, then 2(c·α) - (c·2α) = 0, theoretically eliminating the O(α) term. This requires both sequences to share the same noise realization {Z_k} to ensure the bias structure depends only on step size.

### Mechanism 3: Recovery of Optimal Covariance
Post-extrapolation, the leading error term aligns with the asymptotically optimal covariance matrix Σ∞. By canceling the O(α) bias, the variance term (scaling as 1/n) becomes dominant, characterized by Σ∞ = Ā^(-1)Σ^(M)_ε Ā^(-T), matching the theoretical optimum.

## Foundational Learning

- **Concept: Linear Stochastic Approximation (LSA)**
  - **Why needed:** Base algorithm class for solving Āθ = b̄ with noisy samples
  - **Quick check:** How does Hurwitz stability of Ā guarantee convergence of E[θ_k] before analyzing variance?

- **Concept: Markov Chain Mixing Time (t_mix)**
  - **Why needed:** UGE assumption affects convergence speed and bias magnitude, explicitly bounded in terms of t_mix
  - **Quick check:** Why does larger t_mix necessitate smaller α to maintain stability (α ∈ (0, ... t_mix^(-1)))?

- **Concept: Polyak-Ruppert (PR) Averaging**
  - **Why needed:** Standard technique for reducing variance in LSA; paper shows it's insufficient for Markovian bias
  - **Quick check:** Would PR averaging eliminate bias differently with decreasing step sizes vs. constant α?

## Architecture Onboarding

- **Component map:** Noise Generator {Z_k} -> LSA Core (Twin Instances α and 2α) -> Averaging Buffer -> RR Combiner (2·A - B)

- **Critical path:** Coupling the two iterates to share identical noise sequence, burn-in initialization, optimal step size selection α ∝ n^(-1/2)

- **Design tradeoffs:** Memory (doubling state update cost) vs. bias reduction; aggressive step sizes improve speed but risk instability

- **Failure signatures:** Divergence if α too large relative to 1/t_mix; high variance if n small; unpredictable residuals if UGE assumption violated

- **First 3 experiments:** 1) Verify bias scales linearly with α across different step sizes, 2) Compare MSE of standard vs. RR estimator against sample size, 3) Test optimal α scaling with varying mixing times

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Analysis critically depends on Uniform Geometric Ergodicity assumption, excluding non-geometric ergodic cases
- Linearization approach assumes problem is sufficiently close to linear, may not hold for highly nonlinear dynamics
- Requires knowledge of optimal step size α ∝ n^(-1/2), which depends on unknown problem-specific constants

## Confidence
- **High Confidence:** Bias characterization mechanism is well-supported by linearization framework
- **Medium Confidence:** RR cancellation mechanism has strong theoretical justification but sensitive to implementation details
- **Medium Confidence:** Recovery of optimal covariance is theoretically sound but practical significance depends on higher-order term behavior

## Next Checks
1. Evaluate algorithm performance on Markov chains with polynomial mixing rather than geometric mixing to test bias assumption breakdown
2. Systematically vary the ratio between α and 2α to test robustness of cancellation mechanism when exact doubling relationship is perturbed
3. Conduct extensive simulations comparing empirical MSE scaling with theoretical bounds to verify transition from O(α) to O(α^(3/2)) dominance at predicted sample sizes