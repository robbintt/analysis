---
ver: rpa2
title: Kronecker-factored Approximate Curvature (KFAC) From Scratch
arxiv_id: '2507.05127'
source_url: https://arxiv.org/abs/2507.05127
tags:
- tensor
- kfac
- loss
- fisher
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This tutorial provides a comprehensive, ground-up introduction
  to Kronecker-factored approximate curvature (KFAC), a widely used curvature approximation
  in deep learning. The authors address the challenge of implementing KFAC correctly,
  which is often tedious due to its various flavors, common pitfalls in translating
  math to code, and difficulty in testing.
---

# Kronecker-factored Approximate Curvature (KFAC) From Scratch

## Quick Facts
- **arXiv ID:** 2507.05127
- **Source URL:** https://arxiv.org/abs/2507.05127
- **Reference count:** 3
- **Primary result:** Comprehensive tutorial for implementing KFAC from scratch, with step-by-step guide and validation tests.

## Executive Summary
This tutorial provides a comprehensive, ground-up introduction to Kronecker-factored approximate curvature (KFAC), a widely used curvature approximation in deep learning. The authors address the challenge of implementing KFAC correctly, which is often tedious due to its various flavors, common pitfalls in translating math to code, and difficulty in testing. The tutorial offers a step-by-step guide, combining mathematical explanations with code snippets, to ensure a reliable KFAC implementation.

## Method Summary
The tutorial implements KFAC for linear layers (`torch.nn.Linear`) without weight sharing by computing input-based factor $A = R \sum x_n x_n^\top$ and gradient-output-based factor $B = \frac{1}{N} \sum g_{n,c} g_{n,c}^\top$, then assembling the approximation as $A \otimes B$ (cvec) or $B \otimes A$ (rvec). The implementation uses PyTorch hooks to intercept layer inputs and outputs during forward and backward passes, with validation tests including exact matching against the GGN for batch size 1 and deep linear regression scenarios.

## Key Results
- Clear explanation of curvature matrices (Hessian, GGN, Fisher) and their probabilistic interpretations
- Demonstration of how Kronecker products naturally emerge in curvature matrices
- Detailed implementation and testing of KFAC for linear layers with validation against exact curvature matrices

## Why This Works (Mechanism)

### Mechanism 1: Expectation Approximation (Factor Independence)
KFAC approximates the curvature matrix by separating the sum of Kronecker products into a single Kronecker product of sums, decoupling input activations from backpropagated gradients under the assumption of statistical independence.

### Mechanism 2: Block-Diagonal Projection
Reduces the dimensionality of the inverse problem by assuming layers are independent, retaining only intra-layer curvature blocks and discarding inter-layer correlations.

### Mechanism 3: Linearization via GGN
Targets the Generalized Gauss-Newton matrix rather than the full Hessian to ensure positive semi-definiteness by discarding second-order network derivatives and keeping only curvature of the loss function.

## Foundational Learning

- **Concept: Flattening Conventions (cvec vs rvec)**
  - *Why needed here:* The order of Kronecker factors depends on whether you use row-major (rvec, PyTorch) or column-major (cvec, math) flattening
  - *Quick check:* Flattening a 2×2 matrix [[1, 2], [3, 4]] gives [1, 2, 3, 4] (rvec) or [1, 3, 2, 4] (cvec)

- **Concept: Vector-Jacobian Products (VJP)**
  - *Why needed here:* KFAC relies on VJPs to compute backpropagated vectors efficiently without materializing Jacobians
  - *Quick check:* `torch.autograd.grad(L, z)` computes the VJP of scalar loss L with respect to intermediate output z

- **Concept: Probabilistic Interpretation of Losses**
  - *Why needed here:* The backpropagated vector depends on whether you assume Gaussian (MSE) or Categorical (Cross-Entropy) distributions
  - *Quick check:* For classification, does the backpropagated vector depend on the sampled label (Type-I) or the Hessian of the loss (Type-II)?

## Architecture Onboarding

- **Component map:** Forward Hook -> Input factor A computation -> Backpropagated vector generation -> Backward Hook -> Output factor B computation -> Scaling logic -> Damping and inversion -> Precondition gradient application

- **Critical path:** Register hooks on nn.Linear layers → Run forward pass → Compute/Update A → Compute backpropagated vectors Δ → Run backward pass with Δ → Compute/Update B → Apply damping and compute inverses → Precondition gradient

- **Design tradeoffs:**
  - cvec vs rvec: Code uses rvec (B ⊗ A), math uses cvec (A ⊗ B)
  - Vector types: type-2 (exact), mc (stochastic), empirical (cheapest)
  - Reduction factor: 1/N vs 1 normalization in factor B

- **Failure signatures:**
  - Scaling mismatch: Factors off by N or 2 due to incorrect reduction
  - Swapped factors: Preconditioner ineffective due to cvec/rvec confusion
  - Divergence: Insufficient damping or violated Expectation Approximation

- **First 3 experiments:**
  1. Batch Size 1 Test: Run KFAC on single data point, should exactly equal GGN/Hessian
  2. Deep Linear Regression: Train deep linear net with square loss, KFAC should match exact curvature
  3. Flattening Invariance: Verify kron(A, B) matches cvec-GGN and kron(B, A) matches rvec-GGN

## Open Questions the Paper Calls Out

### Open Question 1
How can Eigenvalue-corrected KFAC (EKFAC) be integrated into the tutorial's framework to improve approximation quality? The current tutorial lacks the second step where a diagonal scaling term is introduced to minimize the Frobenius norm residual.

### Open Question 2
How can the KFAC implementation be extended to support the "reduce" specifier for handling weight sharing in architectures like transformers and CNNs? The current code treats shared dimensions as independent but cannot handle layers where shared dimensions are summed or averaged.

### Open Question 3
Can a functional-style KFAC implementation be achieved using torch.fx tracing to support non-modular architectures? The current implementation relies on nn.Module hooks, incompatible with functional APIs or Taylor-mode arithmetic.

## Limitations

- The Expectation Approximation lacks strong empirical validation in highly non-linear regimes
- The block-diagonal assumption may break down in very deep or tightly coupled architectures
- No clear diagnostic for when the block-diagonal assumption fails

## Confidence

- **High confidence:** Mathematical derivation of Kronecker product structure from curvature matrices
- **Medium confidence:** Implementation correctness tests are robust but real-world performance depends on approximation quality
- **Low confidence:** Behavior under stochastic optimization dynamics and comparison to other second-order methods

## Next Checks

1. **Activation Sensitivity:** Measure approximation error of Expectation Approximation across different activation functions as a function of input gradient variance
2. **Layer Coupling Stress Test:** Construct deep linear network with synthetic correlations and measure off-diagonal curvature loss under block-diagonal assumption
3. **Damping Sensitivity:** Systematically vary damping parameter and measure impact on convergence speed and stability across different loss landscapes