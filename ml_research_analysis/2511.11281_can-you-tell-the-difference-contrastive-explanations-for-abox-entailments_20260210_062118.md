---
ver: rpa2
title: Can You Tell the Difference? Contrastive Explanations for ABox Entailments
arxiv_id: '2511.11281'
source_url: https://arxiv.org/abs/2511.11281
tags:
- qdiff
- qcom
- since
- abox
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces contrastive ABox explanations (CEs) to answer
  questions like "Why is a an instance of C, but b is not?" Unlike traditional justifications
  or abductions, CEs consider both entailments jointly, focusing on relevant commonalities
  and differences. The authors formalize CEs, analyze their computational complexity
  across multiple dimensions (DLs, optimality criteria, etc.), and implement a first
  prototype for computing difference-minimal syntactic CEs.
---

# Can You Tell the Difference? Contrastive Explanations for ABox Entailments

## Quick Facts
- arXiv ID: 2511.11281
- Source URL: https://arxiv.org/abs/2511.11281
- Reference count: 40
- Primary result: Introduces contrastive ABox explanations (CEs) to answer "Why is a an instance of C, but b is not?" with a prototype for computing difference-minimal syntactic CEs.

## Executive Summary
This paper introduces contrastive explanations for ABox entailments in description logics, answering questions like "Why is Alice an instance of Interviewed, but Bob is not?" Unlike traditional justifications, CEs consider both the positive and negative entailments jointly, focusing on relevant commonalities and differences. The authors formalize CEs, analyze their computational complexity across multiple dimensions (DLs, optimality criteria, etc.), and implement a first prototype for computing difference-minimal syntactic CEs. Experiments on realistic ontologies show CEs are typically small, though computation can be slow due to large intermediate structures. Conflicts are rare in practice. The work opens avenues for more efficient CE computation and integration into ontology-based query answering.

## Method Summary
The method computes contrastive explanations for ABox entailments by constructing a "super-structure" embedding all possible explanations, then iteratively minimizing it using a modified justification algorithm that allows fixing components. The prototype takes a KB, concept C, fact individual a, and foil individual b, then outputs a contrastive explanation tuple. For the EL/EL⊥ fragments, subset difference-minimality is tractable (P), while other criteria are harder. The approach trades global optimality for practical computability by using a sequential minimization strategy.

## Key Results
- Verifying subset difference-minimality is tractable (P for EL/EL⊥, EXPTIME for ALC) via super-structure embedding
- Syntactic CEs computed on realistic ontologies are typically small (avg difference size 1.42 for EL)
- Most generated CEs have empty conflict sets and use no fresh individuals in practice
- Cardinality-based and conflict-minimality verification is coNP-hard (EL) or CoNEXPTIME-hard (ALC)

## Why This Works (Mechanism)

### Mechanism 1: Joint Commonality and Difference Analysis
- **Claim:** Contrastive explanations are more effective than isolated justifications because they explicitly compare relevant commonalities and differences between a fact and a foil.
- **Mechanism:** The system constructs a solution tuple $\langle q_{com}(\vec{x}), q_{diff}(\vec{x}), \vec{c}, \vec{d}, C \rangle$. $q_{com}$ represents ABox patterns shared by both individuals, while $q_{diff}$ represents patterns present for the fact but missing for the foil. This structured separation highlights the critical divergences responsible for different outcomes.
- **Core assumption:** Users gain better understanding from explicit comparisons than from two separate, potentially disconnected explanations.
- **Evidence anchors:** The hiring example demonstrates this: separate explanations cite different reasons (funding vs. leading), while the contrastive explanation unifies them ("Alice's publication is at a journal... and only Alice receives funding").

### Mechanism 2: ABox Pattern Super-Structure for Efficient Search
- **Claim:** A polynomially-sized "super-structure" $E_m$ can be constructed to embed all possible syntactic contrastive explanations, enabling tractable verification for subset-minimality.
- **Mechanism:** Instead of searching an infinite space of possible explanations, the algorithm builds $E_m$ using variables $x_{a',b'}$ for every pair of individuals in the ABox. This structure captures all possible mappings between fact and foil evidence.
- **Core assumption:** A meaningful syntactic explanation can be constructed using only individuals already present in the knowledge base.
- **Evidence anchors:** "Every syntactic CE for P without fresh individual names embeds into $E_m$" (Lemma 8), used to prove subset difference-minimality is in P for EL and EXPTIME for ALC.

### Mechanism 3: Modular Minimization via Fixed-Component Justification
- **Claim:** A full contrastive explanation with multiple optimality criteria can be computed by iteratively minimizing one component at a time while holding others fixed.
- **Mechanism:** The prototype uses a multi-step process (P1'-P4'). It first computes a super-structure, then iteratively minimizes it. To minimize $q_{diff}$ while holding $q_{com}$ constant, it finds a minimal subset of $q_{diff}$ that, together with the fixed $q_{com}$, still entails the concept for the foil.
- **Core assumption:** A local optimization sequence yields a practically useful explanation, even if global optimality is coNP-hard.
- **Evidence anchors:** Experiments show this method produces explanations with small differences (avg 1.42 for EL) and commonalities, validating the practical utility of the heuristic.

## Foundational Learning

- **Concept: Description Logics (DL) ABox Reasoning**
  - **Why needed here:** The entire framework is built on ABox reasoning—determining if an individual $a$ is an instance of a concept $C$ based on a knowledge base $\mathcal{K}=\langle \mathcal{T}, \mathcal{A} \rangle$. Without this, you cannot define the core problem of explaining why $\mathcal{K} \models C(a)$ and $\mathcal{K} \not\models C(b)$.
  - **Quick check question:** Given a TBox axiom $\text{Student} \sqsubseteq \text{Person}$ and an ABox assertion $\text{Student}(\text{Alice})$, can you conclude $\text{Person}(\text{Alice})$? (Answer: Yes, this is standard ABox entailment).

- **Concept: ABox Justification**
  - **Why needed here:** A justification is a minimal subset of the ABox sufficient for an entailment. This paper extends this idea; the "difference" pattern $q_{diff}(\vec{c})$ must itself be a justification. Understanding this base concept is crucial for grasping the optimality conditions (C1-C4).
  - **Quick check question:** If the ABox has $\{\text{Student}(\text{Alice}), \text{Person}(\text{Bob})\}$ and we want to explain $\text{Person}(\text{Alice})$, is the justification $\{\text{Student}(\text{Alice})\}$ or the whole ABox? (Answer: The former, as it is minimal).

- **Concept: Homomorphism and Query Embedding**
  - **Why needed here:** The theoretical analysis relies on one explanation "embedding" into another via a homomorphism (a variable mapping). This defines the partial order over explanations (e.g., $E_p$ is "better" than $E_q$ if $E_p$'s pattern maps into $E_q$'s). This formalizes the super-structure $E_m$.
  - **Quick check question:** If query $q_1 = \{A(x), B(x)\}$ and query $q_2 = \{A(x), B(x), C(x)\}$, can $q_1$ be homomorphically mapped into $q_2$? (Answer: Yes, by mapping $x \to x$, $q_1$'s atoms are a subset of $q_2$'s).

## Architecture Onboarding

- **Component map:** Input Parser -> Super-Structure Builder -> Minimization Engine -> Output Formatter
- **Critical path:** The path is `Input -> Super-Structure Builder -> Minimization Engine -> Output`. The Minimization Engine is the bottleneck, specifically the "Fixed-Component Justifier" calls which invoke a reasoner multiple times. The theoretical tractability depends on these calls being efficient.
- **Design tradeoffs:**
  - **Syntactic vs. Semantic:** The implementation focuses on *syntactic* CEs to maintain tractability. Semantic CEs are reduced to syntactic ones via pre-computation, which can be expensive.
  - **Local vs. Global Optimality:** The system uses a sequential minimization strategy. This trades the guarantee of global optimality (coNP-hard) for practical computability (polynomial with an oracle).
  - **Fresh Individuals:** The prototype allows a limited number of fresh individuals to avoid conflicts. The theoretical results show this makes conflict-minimality EXPTIME/CoNEXPTIME-hard. The design choice is to bound this set, sacrificing full conflict-minimality for termination.
- **Failure signatures:**
  - **Timeouts:** The system may timeout on large, complex ABoxes (up to ~8 minutes). This is due to the super-structure $E_m$ becoming very large, creating a huge search space for the minimizer.
  - **Empty Commonality/Trivial Difference:** On simple ABoxes, the system may produce trivial explanations with empty commonality and a single-atom difference. This is a failure of the *concept*, not the code—the KB lacks the structure for meaningful contrast.
  - **Inconsistent Super-structure:** If the initial super-structure $E_m(\vec{d})$ is inconsistent with the TBox, the algorithm must enter the repair loop (P1'). If this loop fails to find a consistent core, it may return an error or an explanation with a large conflict set.
- **First 3 experiments:**
  1. **Reproduce Running Example:** Implement the "hiring process" KB and run the system for $\langle K, \text{Interviewed}, \text{alice}, \text{bob} \rangle$. Verify the output matches either $E_1$ or $E_2$ from Example 2.
  2. **Scaling Test:** Generate synthetic ABoxes with increasing numbers of individuals and role-depth. Measure the time spent in "Super-Structure Builder" vs. "Minimization Engine" to confirm the bottleneck is the intermediate structure size.
  3. **Ablation on Commonality:** Disable the commonality-maximization step. Compare the resulting explanations with the full system's output on the provided corpora to see if explanations become harder to interpret without the shared context.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact computational complexity of verifying commonality-maximality for EL and EL⊥?
- **Basis in paper:** [explicit] Table 1 shows the complexity entry for com-max in EL⊥ as "open," indicating this case was not resolved.
- **Why unresolved:** The authors proved results for difference-minimality and conflict-minimality across all logics, but left commonality-maximality for EL/EL⊥ open in the complexity analysis.
- **What evidence would resolve it:** A proof showing CONP-completeness (matching ALC) or a different complexity class, via reduction from a known hard problem.

### Open Question 2
- **Question:** Can dedicated algorithms significantly improve CE computation compared to the prototype's super-structure approach?
- **Basis in paper:** [explicit] "In the future we want to investigate dedicated algorithms for computing CEs more efficiently."
- **Why unresolved:** The current prototype builds large intermediate super-structures, causing slow runtimes (up to ~494 seconds). The paper only establishes tractability in principle but does not optimize.
- **What evidence would resolve it:** Empirical comparison showing improved runtime on the same benchmarks with provable correctness guarantees.

### Open Question 3
- **Question:** What is the counting and enumeration complexity of contrastive explanations?
- **Basis in paper:** [explicit] "Moreover, one can address the counting and enumeration complexity for CEs."
- **Why unresolved:** The paper focuses on verification and existence problems but does not analyze how many CEs exist or how efficiently they can be enumerated.
- **What evidence would resolve it:** Complexity classifications (e.g., #P-completeness) for counting CEs, and output-polynomial enumeration algorithms or hardness proofs.

### Open Question 4
- **Question:** How should the variant of CEs with quantified variables in fact and foil vectors be formalized and computed?
- **Basis in paper:** [explicit] "We are also exploring a variant of CEs which use quantified variables in the fact and foil vectors."
- **Why unresolved:** The current definition uses concrete individuals in the evidence vectors; extending to quantified variables requires new formalization and may change complexity.
- **What evidence would resolve it:** A formal definition satisfying the same explanatory properties, with complexity analysis and a prototype implementation.

## Limitations

- The implementation relies on a sequential minimization heuristic that trades global optimality for tractability, though no formal guarantee is provided for the quality of the final CE.
- The definition of "entailed assertions" removal is underspecified, potentially affecting the KB's structure and the resulting explanations.
- Fresh individual names are allowed to resolve conflicts, but their impact on explanation interpretability and minimality is not rigorously evaluated.
- The prototype's performance bottleneck is the construction and processing of the super-structure $E_m$, which can be extremely large for complex KBs, leading to timeouts.

## Confidence

- **High:** The theoretical tractability results for subset difference-minimality (P for EL, EXPTIME for ALC) and the basic CE definition are well-supported by proofs.
- **Medium:** The experimental results showing small CE sizes in practice and the effectiveness of the multi-step minimization strategy are plausible but depend on the specific corpora and preprocessing steps.
- **Low:** The claim about the practical superiority of CEs over separate justifications for user understanding is supported only by the single hiring example and not by user studies.

## Next Checks

1. Re-run the prototype on a subset of the ORE corpora with a fixed random seed to isolate the effect of HashSet nondeterminism on justification selection.
2. Implement and test an alternative super-structure construction method that uses a bounded number of fresh individuals from the start, comparing its performance and CE quality to the current approach.
3. Conduct a small user study (e.g., with 10-15 participants) comparing comprehension and preference for CEs versus separate justifications on simple ontologies, to provide empirical support for the core usability claim.