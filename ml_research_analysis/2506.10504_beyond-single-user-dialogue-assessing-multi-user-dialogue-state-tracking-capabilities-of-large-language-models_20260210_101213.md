---
ver: rpa2
title: 'Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking
  Capabilities of Large Language Models'
arxiv_id: '2506.10504'
source_url: https://arxiv.org/abs/2506.10504
tags:
- user
- slot
- dialogue
- user2
- multi-user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have achieved strong zero-shot dialogue
  state tracking (DST) performance, but benchmarks have focused on single-user dialogues.
  This study systematically evaluates LLMs on multi-user DST by generating and inserting
  a second user's utterances into existing MultiWOZ dialogues using speech act theory.
---

# Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2506.10504
- Source URL: https://arxiv.org/abs/2506.10504
- Authors: Sangmin Song; Juhwan Choi; JungMin Yun; YoungBin Kim
- Reference count: 20
- Large language models show significant performance degradation in multi-user dialogue state tracking compared to single-user settings

## Executive Summary
This study investigates how large language models perform in multi-user dialogue state tracking (DST) compared to traditional single-user benchmarks. While LLMs have demonstrated strong zero-shot DST capabilities on datasets like MultiWOZ, real-world applications often involve multiple participants with potentially conflicting information. The researchers systematically evaluate five LLMs by introducing a second user's utterances into existing MultiWOZ dialogues using speech act theory, revealing significant performance drops across all models.

The findings reveal that current LLMs struggle to maintain accurate dialogue states in multi-user scenarios, with joint goal accuracy decreasing by 1.54-3.54% across tested models. The performance degradation stems from models' difficulties in distinguishing slot values across speakers and correctly identifying which slots belong to which user. This work highlights a critical gap in current DST evaluation practices and demonstrates the need for more robust speaker-aware approaches in multi-party dialogue systems.

## Method Summary
The researchers developed a systematic methodology to evaluate LLMs on multi-user DST by transforming single-user MultiWOZ dialogues into multi-user scenarios. They generated a second user's utterances using speech act theory, ensuring these additions would create realistic conflicts and variations in slot values. These synthetic multi-user dialogues were then used to test five different LLMs (including GPT-4o, Claude-3.5-Sonnet, and others) in zero-shot settings. The performance was compared against the original single-user dialogues to quantify the impact of multi-user complexity. Joint goal accuracy served as the primary metric, measuring the models' ability to correctly track all dialogue states across both users.

## Key Results
- GPT-4o's joint goal accuracy dropped from 57.82% to 54.28% when handling multi-user dialogues
- All five tested LLMs showed significant performance degradation in multi-user settings
- The performance drops were attributed to speaker confusion and slot misattribution issues
- Multi-user complexity created challenges in distinguishing slot values across different speakers

## Why This Works (Mechanism)
The study demonstrates that multi-user dialogue state tracking presents unique challenges that current LLMs are not adequately equipped to handle. The mechanism behind the performance degradation lies in the models' difficulty processing overlapping or conflicting information from multiple speakers. When faced with multi-user dialogues, LLMs struggle to maintain clear attribution of slot values to their respective speakers, leading to state tracking errors. The speech act theory-based generation of second-user utterances effectively simulates real-world scenarios where multiple participants contribute information that may conflict or complement each other, exposing these limitations in model performance.

## Foundational Learning

**Dialogue State Tracking (DST)**: The process of identifying and tracking user goals and intentions throughout a conversation, crucial for task-oriented dialogue systems. *Why needed*: Forms the core evaluation target for understanding model capabilities in conversational AI. *Quick check*: Can the model correctly identify when a user expresses a preference for a restaurant type?

**Speech Act Theory**: A framework for understanding how utterances function in communication, categorizing them by their intended purpose (e.g., requests, assertions, questions). *Why needed*: Provides the theoretical foundation for generating realistic second-user utterances. *Quick check*: Does the model recognize that "I need a cheap restaurant" is a request for information while "The restaurant is expensive" is an assertion?

**Zero-Shot Learning**: A setting where models make predictions on tasks they haven't been explicitly trained on, relying solely on their pre-existing knowledge. *Why needed*: Demonstrates the generalization capabilities of LLMs without task-specific fine-tuning. *Quick check*: Can the model perform DST on new dialogue domains without additional training?

**Multi-Party Dialogue**: Conversations involving more than two participants, introducing complexity through multiple information sources and potential conflicts. *Why needed*: Represents the real-world complexity often missing from traditional DST benchmarks. *Quick check*: Does the model maintain separate dialogue states for each participant when they express different preferences?

## Architecture Onboarding

**Component Map**: MultiWOZ dialogues -> Speech Act Theory-based augmentation -> Multi-user dialogue generation -> LLM inference -> Joint goal accuracy calculation

**Critical Path**: Input dialogue → LLM processing → Dialogue state extraction → Accuracy evaluation

**Design Tradeoffs**: The study chose synthetic generation of multi-user utterances over using real multi-party datasets, trading ecological validity for controlled experimental conditions that isolate the impact of multi-user complexity.

**Failure Signatures**: Performance drops primarily occur when multiple speakers express conflicting slot values, and when the model cannot correctly attribute slot information to the appropriate speaker.

**First Experiments**: 1) Test individual models on single-user vs multi-user versions of identical dialogues, 2) Analyze error patterns when speakers provide contradictory information, 3) Evaluate whether explicit speaker labeling in prompts improves performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study uses artificially generated second-user utterances rather than real multi-user dialogues, which may not fully capture authentic conversational dynamics
- Performance degradation, while statistically significant, shows varying magnitudes across models, suggesting the need to understand which architectural factors influence multi-user handling
- The analysis focuses primarily on joint goal accuracy without extensive examination of other DST metrics that could reveal more granular insights into model failures

## Confidence
- High confidence: LLMs show systematic performance degradation in multi-user DST scenarios
- Medium confidence: The primary causes (speaker confusion and slot misattribution) are correctly identified
- Medium confidence: The speech act theory-based generation method is appropriate for creating controlled test conditions

## Next Checks
1. Validate findings using real multi-user dialogue datasets rather than synthetically generated utterances to confirm whether the observed limitations persist with authentic conversational data
2. Conduct ablation studies to isolate whether performance drops are primarily due to speaker confusion versus increased dialogue complexity, by testing with single speaker using multiple speech acts
3. Test whether explicitly providing speaker identifiers in the prompt or using speaker-aware prompting strategies can recover the lost performance, establishing whether this is a solvable architectural limitation