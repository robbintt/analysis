---
ver: rpa2
title: Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous
  Vehicles
arxiv_id: '2511.17675'
source_url: https://arxiv.org/abs/2511.17675
tags:
- quantum
- trajectory
- training
- baseline
- shallow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a compact hybrid quantum architecture for multi-modal
  trajectory forecasting in autonomous driving. The model operates in an ego-centric,
  lane-aligned frame and predicts residual corrections to a kinematic baseline, using
  a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum
  feedforward stack (64 layers, ~1200 trainable angles), and a Fourier-based decoder
  that generates 16 trajectory hypotheses in a single pass.
---

# Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles

## Quick Facts
- arXiv ID: 2511.17675
- Source URL: https://arxiv.org/abs/2511.17675
- Reference count: 28
- This paper presents a compact hybrid quantum architecture for multi-modal trajectory forecasting in autonomous driving, achieving minADE of 1.94 m and minFDE of 3.56 m over a 2.0 s horizon.

## Executive Summary
This paper introduces a hybrid quantum-classical model for multi-modal trajectory forecasting in autonomous driving. The system operates in an ego-centric, lane-aligned frame and predicts residual corrections to a kinematic baseline, using a transformer-inspired quantum attention encoder, a parameter-lean quantum feedforward stack, and a Fourier-based decoder that generates 16 trajectory hypotheses in a single pass. The model is trained with Simultaneous Perturbation Stochastic Approximation (SPSA) to handle the non-analytic quantum components.

## Method Summary
The model takes 11 history time steps from the Waymo Open Motion Dataset, transforms them into an ego-centric lane-aligned frame, and computes a kinematic baseline trajectory. It then uses a quantum attention encoder (9 qubits) to process query/key/value vectors, followed by a 64-layer quantum feedforward stack with ring entanglement to extract a latent representation. A Fourier decoder applies phase offsets to generate 16 diverse trajectory hypotheses, which are scored by a spectrum-based confidence head. All parameters are trained end-to-end using SPSA with a min-over-modes loss function.

## Key Results
- Achieves minADE of 1.94 m and minFDE of 3.56 m over 2.0s horizon on Waymo Open Motion Dataset
- Outperforms kinematic baseline with reduced miss rates and strong recall
- Ablations confirm effectiveness of residual learning, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking

## Why This Works (Mechanism)

### Mechanism 1: Residual Correction in Lane-Aligned Frame
Predicting residuals to a kinematic baseline stabilizes learning and concentrates quantum capacity on structured, meter-scale deviations. The model transforms scenes into an ego-centric, lane-aligned coordinate frame and computes a map-following or CTRV baseline trajectory. The quantum circuit predicts only Δp = p_gt - b (corrections), not absolute positions. This contracts output dynamic range and aligns the learning target with the statistics of short-horizon driving.

### Mechanism 2: Phase-Superposition Multi-Modal Generation via Fourier Readout
Global phase offsets applied to a shared quantum state produce M=16 diverse, smooth trajectory hypotheses in a single forward pass at constant inference cost. The decoder encodes latent z into a quantum state, then applies phase shift R_z(Δϕ_m) to all qubits for each mode m, where Δϕ_m = (m+1)π/M. The resulting amplitude vector α^(m) maps to trajectories via truncated Fourier series (B=8 harmonics).

### Mechanism 3: SPSA Optimization Under Non-Analytic Min-Over-Modes Loss
Simultaneous Perturbation Stochastic Approximation enables stable training of shallow quantum circuits despite non-differentiable objectives and measurement-based outputs. SPSA perturbs all parameters simultaneously in random directions {±1}, estimates gradients from two loss evaluations (L+, L−), and updates with decaying step sizes.

## Foundational Learning

- **Parameterized Quantum Circuits (PQCs) and Measurement**: Understanding that rotation gates (R_x, R_y, R_z) parameterize state preparation and that entangling gates (CX) create correlations is essential. Given a 9-qubit circuit with R_y(θ_i) on each qubit followed by ring entanglement, what is the dimension of the Hilbert space and how many classical values does measuring all qubits in the Z basis produce?
- **Fourier Series Truncation as Smoothness Prior**: The decoder maps quantum amplitudes to trajectories via a truncated Fourier basis (B=8). This imposes a low-frequency smoothness bias appropriate for 2.0s horizons—high-frequency maneuvers are suppressed by design. If a ground-truth trajectory contains a sharp kink at t=0.3s, will an 8-harmonic Fourier reconstruction capture it accurately? What artifact would you expect?
- **Min-Over-Modes Loss and Multi-Modal Coverage**: The loss takes the minimum error across M predicted modes, encouraging at least one hypothesis to match the ground truth. This differs from averaging over modes and requires understanding coverage vs. ranking tradeoffs. Why does min-over-modes training not guarantee that the highest-confidence prediction is the most accurate?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Quantum attention encoder (9 qubits, L_attn=6) -> Quantum feedforward stack (9 qubits, L_ff=64) -> Quantum decoder + Fourier head -> Confidence head -> Training with SPSA
- **Critical path**: Data preprocessing correctness (lane frame, baseline, residual targets) -> Angle normalization to [-π, π] before rotation gates -> SPSA hyperparameter schedule -> Measurement-to-classical extraction at each layer
- **Design tradeoffs**: 9 qubits vs. expressivity (small Hilbert space limits representable functions); 64 feedforward layers vs. optimizability (deep circuit increases capacity but risks barren plateaus); Phase-based multi-modality vs. diversity (efficient but may not span all maneuver types); Fixed spectrum-based confidence vs. learned ranking (no extra parameters but may not surface best hypothesis)
- **Failure signatures**: Loss plateaus early with high variance; minADE/minFDE barely improves over baseline; P99 ADE remains high while P50 drops; Confidence scores uniform across modes; Validation loss diverges from training loss
- **First 3 experiments**: 
  1. Train with zero residuals (predict absolute positions) vs. residual prediction
  2. Visualize the M=16 trajectories for a held-out scenario with varying maneuver complexity
  3. Vary truncation order B ∈ {4, 8, 12, 16} and measure impact on minADE and trajectory smoothness

## Open Questions the Paper Calls Out

- **How does the model perform on real NISQ hardware under noise, decoherence, and device-specific gate errors?**: All experiments are run in classical simulation, so noise, decoherence and device-specific constraints are not captured. The reported metrics do not yet reflect real hardware behaviour.
- **Are quantum effects essential to the model's performance, or does the success derive primarily from the lane-aligned residual formulation and Fourier decoder structure?**: Ablations confirm individual component contributions, but no classical control is reported that matches the architecture without quantum superposition/entanglement.
- **Can the architecture be extended to multi-agent prediction while retaining stable SPSA optimization and low qubit counts?**: The formulation is SDV-centric with strong preprocessing and omits rich interaction patterns between agents and much of the HD-map structure beyond local lane direction.
- **Can learned confidence heads close the gap between Top-K and oracle (Best-K) performance?**: The remaining gap is largely due to ranking rather than representation, and the spectrum-derived confidences do not always surface these modes at the very top.

## Limitations
- The hybrid quantum architecture operates in a highly constrained parameter space (~1200 trainable angles), which may limit its ability to capture rare, high-variance driving scenarios
- The reliance on phase superposition for multi-modality is elegant but unproven in classical trajectory forecasting literature
- The fixed spectrum-based confidence scoring bypasses learned ranking, potentially misaligning predicted likelihoods with actual accuracy

## Confidence
- **High Confidence**: Residual learning in lane-aligned frame improves stability and performance
- **Medium Confidence**: Fourier truncation and phase superposition effectively generate smooth, diverse trajectories
- **Low Confidence**: SPSA training stability is robust across all scenarios

## Next Checks
1. **Richness of Multi-Modal Coverage**: For scenarios with diverse ground-truth futures, compute Best-K ADE across K=1-16 and compare to oracle Top-K ranking
2. **Fourier Basis Capacity**: Train models with B=4, 8, 12 harmonics and measure minADE, P50/P99 errors
3. **SPSA Stability Across Scales**: Increase dataset size or horizon length (e.g., 3.0s) and monitor loss variance and convergence rate