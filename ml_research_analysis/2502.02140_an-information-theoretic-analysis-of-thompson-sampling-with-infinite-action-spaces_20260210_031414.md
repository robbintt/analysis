---
ver: rpa2
title: An Information-Theoretic Analysis of Thompson Sampling with Infinite Action
  Spaces
arxiv_id: '2502.02140'
source_url: https://arxiv.org/abs/2502.02140
tags:
- action
- regret
- sampling
- thompson
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the information-theoretic analysis of Thompson
  Sampling (TS) to bandit problems with infinite or continuous action and parameter
  spaces. Building on Russo and Van Roy's framework, the authors adapt the rate-distortion
  approach of Dong and Van Roy to handle continuous settings.
---

# An Information-Theoretic Analysis of Thompson Sampling with Infinite Action Spaces

## Quick Facts
- arXiv ID: 2502.02140
- Source URL: https://arxiv.org/abs/2502.02140
- Reference count: 17
- Primary result: Extends information-theoretic analysis of Thompson Sampling to infinite/continuous action and parameter spaces using rate-distortion approach

## Executive Summary
This paper extends the information-theoretic analysis of Thompson Sampling (TS) to bandit problems with infinite or continuous action and parameter spaces. Building on Russo and Van Roy's framework, the authors adapt the rate-distortion approach of Dong and Van Roy to handle continuous settings. The key innovation is constructing a one-step compressed Thompson Sampling that depends on a quantized version of the optimal action rather than the parameter, allowing the analysis to work with infinite action spaces. For Lipschitz continuous reward functions, the regret bound scales with the covering number of the action space, measuring its complexity.

## Method Summary
The authors develop a compressed Thompson Sampling approach for infinite action spaces by constructing a one-step information-theoretic analysis that depends on a quantized version of the optimal action rather than the parameter itself. They introduce a partition scheme for the parameter space and define a compressed sampling function that maintains the regret guarantees while controlling information gain. The method leverages rate-distortion theory to handle the continuous setting, with the regret bound scaling with the covering number of the action space. For d-dimensional linear bandits with bounded actions, they recover a near-optimal O(d√T log T) regret bound.

## Key Results
- Extends information-theoretic analysis of TS to infinite/continuous action and parameter spaces
- Achieves O(d√T log T) regret for d-dimensional linear bandits with bounded actions
- Constructs compressed Thompson Sampling using quantized optimal actions rather than parameters
- Regret bound scales with covering number of action space for Lipschitz continuous rewards

## Why This Works (Mechanism)
The approach works by shifting the quantization target from the parameter θ to the optimal action A*(θ), which has finite cardinality in the action space. This enables the use of rate-distortion theory while maintaining the regret guarantees. The Lipschitz continuity assumption ensures that actions within the same partition cell yield similar rewards, allowing the quantization error to be controlled. The covering number of the action space captures the complexity of the problem, with smaller covering numbers leading to tighter regret bounds.

## Foundational Learning

**Thompson Sampling** - A Bayesian approach to bandit problems that samples from the posterior distribution of the parameter. Why needed: Provides the baseline algorithm that needs to be analyzed for infinite action spaces. Quick check: Can be implemented by sampling from posterior and selecting argmax action.

**Rate-Distortion Theory** - Framework for quantifying the trade-off between compression rate and distortion in information transmission. Why needed: Provides tools to analyze the information-theoretic properties of the compressed sampling process. Quick check: Can be verified by checking that mutual information bounds hold for the quantized variables.

**Covering Numbers** - Measure of the minimal number of balls needed to cover a space. Why needed: Captures the complexity of the action space and appears in the regret bound. Quick check: Can be computed or bounded for specific problem structures like linear bandits.

## Architecture Onboarding

**Component map**: Posterior distribution -> Partition construction -> Compressed sampling function -> Quantized optimal action -> Regret bound

**Critical path**: The construction of the compressed sampling function (Proposition 1) is the critical component, as it must simultaneously bound regret and control information gain.

**Design tradeoffs**: The trade-off between quantization resolution (ε) and regret performance - finer partitions reduce quantization error but increase covering number complexity.

**Failure signatures**: If the partition construction fails to satisfy the ε-closeness property for Lipschitz continuous rewards, the regret bound will not hold. If the compressed sampling function cannot be constructed to satisfy the technical conditions, the analysis breaks down.

**First experiments**:
1. Verify the covering number bounds for specific action space geometries (e.g., unit ball, hypercube)
2. Test the Lipschitz continuity requirement by applying the framework to non-smooth reward functions
3. Compare the logarithmic constants in the O(d√T log T) bound against established benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the information-theoretic analysis with compressed Thompson Sampling be extended to semi-bandit feedback problems?
- Basis in paper: [explicit] The conclusion states: "A natural direction for future work is to extend our results to other classes of bandit problems, such as the 'semi-bandit' feedback problem."
- Why unresolved: The current analysis focuses on full bandit feedback where only the reward of the selected action is observed; semi-bandit settings require different information-theoretic treatment.
- What evidence would resolve it: A regret bound derivation for combinatorial bandits with semi-bandit feedback using the compressed TS framework.

### Open Question 2
- Question: Can the O(d√T log T) regret bound be tightened to achieve the optimal O(d√T) rate by removing the logarithmic factor?
- Basis in paper: [inferred] The paper achieves a "near-optimal" rate of O(d√T log T), but the lower bound for linear bandits is O(d√T). The log factor arises from the entropy term H(A*ε) which scales with log(N(A,ρ,ε)).
- Why unresolved: The rate-distortion approach inherently introduces quantization that contributes logarithmic overhead; it remains unclear if this is fundamental to the approach.
- What evidence would resolve it: Either a refined analysis eliminating the log factor, or a lower bound showing the compressed TS approach cannot achieve O(d√T).

### Open Question 3
- Question: Can the Lipschitz continuity assumption on expected rewards be relaxed while maintaining regret guarantees?
- Basis in paper: [inferred] Corollary 1 explicitly requires L-Lipschitz continuity to control the covering number term and quantization error. The bound scales with both covering number complexity and Lipschitz constant L.
- Why unresolved: The partition construction in Equation (1) relies on the Lipschitz property to ensure ε-closeness within each partition cell; non-smooth reward functions may violate this construction.
- What evidence would resolve it: A modified partition scheme or alternative analysis that handles non-Lipschitz reward functions (e.g., Hölder continuous or discontinuous).

### Open Question 4
- Question: Can the one-to-one assumption on the optimal action mapping π* be relaxed to handle settings where multiple parameters yield the same optimal action?
- Basis in paper: [inferred] The paper assumes π* is one-to-one (Footnote 1 mentions including "duplicate versions" when this fails), which may be restrictive for problems with action equivalence or parameter symmetries.
- Why unresolved: The current construction of A*ε and the compressed TS variables depend critically on the bijection between parameters and optimal actions.
- What evidence would resolve it: An extended analysis that handles many-to-one mappings without artificial duplication, potentially using alternative statistics beyond A*ε.

## Limitations
- Restricted to Lipschitz continuous reward functions, excluding important scenarios like linear bandits with general convex reward functions
- Regret bounds rely on covering numbers of action space, which may lead to loose bounds when covering number is large
- Framework requires construction of compressed sampling function satisfying specific technical conditions

## Confidence

**High confidence**: The theoretical framework and regret bounds for Lipschitz continuous reward functions are mathematically sound and rigorously derived.

**Medium confidence**: The application to d-dimensional linear bandits correctly recovers near-optimal rates, though the improvement in logarithmic constants needs empirical verification.

**Medium confidence**: The extension of information-theoretic analysis to infinite action spaces is valid within the stated assumptions, but the generality of the approach for arbitrary problem structures remains to be seen.

## Next Checks

1. Empirically validate the claimed improvement in logarithmic constants for d-dimensional linear bandits against established benchmarks.
2. Test the framework on problems where the reward function is not Lipschitz continuous but still well-behaved (e.g., piecewise linear) to identify the boundaries of applicability.
3. Investigate whether the compressed sampling function construction can be extended to handle general convex reward functions in linear bandits, or identify fundamental barriers.