---
ver: rpa2
title: 'APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization'
arxiv_id: '2506.21655'
source_url: https://arxiv.org/abs/2506.21655
tags:
- reasoning
- training
- arxiv
- penalty
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work enhances reasoning in Multimodal Large Language Models
  (MLLMs) by introducing Asymmetric Policy Optimization (APO), which uses Difficulty-Adaptive
  Divergence Shaping (DADS) and Suboptimal Trajectory Complexity Regularization (STCR).
  DADS dynamically adjusts the KL divergence weight based on sample difficulty to
  improve training efficiency and retain knowledge, while STCR penalizes overly long
  incorrect responses to reduce overthinking and encourage concise reasoning.
---

# APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization

## Quick Facts
- **arXiv ID**: 2506.21655
- **Source URL**: https://arxiv.org/abs/2506.21655
- **Reference count**: 40
- **Primary result**: 7% average gain on reasoning benchmarks while maintaining general task performance

## Executive Summary
This paper introduces Asymmetric Policy Optimization (APO) to enhance reasoning in Multimodal Large Language Models (MLLMs). APO modifies traditional RL training through two key innovations: Difficulty-Adaptive Divergence Shaping (DADS) and Suboptimal Trajectory Complexity Regularization (STCR). DADS dynamically adjusts KL divergence weights based on sample difficulty to improve training efficiency, while STCR penalizes overly long incorrect responses to reduce overthinking. The approach achieves significant reasoning improvements without degrading general multimodal performance, a common issue in reasoning-focused fine-tuning.

## Method Summary
APO enhances MLLM reasoning through asymmetric policy optimization that combines difficulty-aware KL divergence shaping with trajectory complexity regularization. The method builds on GRPO by introducing DADS, which scales the KL penalty based on sample difficulty (d = incorrect_count/G), and STCR, which penalizes long incorrect responses to discourage overthinking. The approach is trained on ~20k curated samples from 10 reasoning datasets using a Qwen2.5-VL-3B base model, achieving reasoning gains while preserving general multimodal capabilities.

## Key Results
- View-R1-3B achieves an average 7% gain over Qwen2.5-VL-3B base model on reasoning benchmarks
- Outperforms larger MLLMs (7-11B) on MathVista, MathVerse, and MMK12
- Maintains consistent improvement on general tasks (MMStar, MMMU) unlike other reasoning-tuned MLLMs that typically degrade

## Why This Works (Mechanism)
APO addresses the fundamental tension between exploration and constraint satisfaction in RL fine-tuning. Traditional RL methods often over-constrain MLLMs, leading to catastrophic forgetting of general capabilities. DADS resolves this by making the KL divergence penalty adaptive—easy samples get lower constraint weights allowing more exploration, while difficult samples get higher weights preserving learned knowledge. STCR tackles overthinking by penalizing unnecessarily long incorrect responses, encouraging models to find concise reasoning paths rather than verbose but incorrect solutions.

## Foundational Learning

**Difficulty-based curriculum learning**
- *Why needed*: MLLMs have varying proficiency across reasoning tasks; uniform training ignores this heterogeneity
- *Quick check*: Verify accuracy distribution across difficulty levels; expect higher accuracy on easier samples

**KL divergence shaping**
- *Why needed*: Standard KL penalties can be too rigid, causing premature convergence or constraint violation
- *Quick check*: Monitor KL loss; should decrease smoothly without sudden spikes or plateaus

**Trajectory complexity regularization**
- *Why needed*: MLLMs often generate verbose incorrect answers, wasting computational resources and confusing users
- *Quick check*: Compare length distributions of correct vs incorrect responses; incorrect should be shorter after STCR

## Architecture Onboarding

**Component map**
Qwen2.5-VL-3B -> APO modifications (DADS + STCR) -> Enhanced reasoning output

**Critical path**
Input prompt → Reasoning generation → Difficulty assessment → DADS weight scaling → Reward computation (λ for correctness, μ for length penalty) → Policy update

**Design tradeoffs**
- DADS: Adaptive vs fixed KL penalty (flexibility vs stability)
- STCR: Length penalty vs solution completeness (conciseness vs thoroughness)
- Dataset curation: Coverage vs quality (diverse vs reliable samples)

**Failure signatures**
- General task degradation → KL penalty too low or DADS decay too aggressive
- Overthinking persists → STCR μ too low or length penalty not properly computed
- Slow convergence → Base β too high or insufficient exploration

**3 first experiments**
1. Implement DADS with fixed β=0.01 and verify difficulty-based scaling
2. Add STCR and test length penalty effect on incorrect responses
3. Combine both and run small-scale training to observe reasoning vs general task balance

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified base KL penalty weight β creates significant implementation uncertainty
- Difficulty-grading algorithm details are incomplete, referencing Observe-R1 without implementation specifics
- Training schedule unclear—Figure 1 suggests ~150 steps but may not represent full training
- Format reward computation remains vague regarding partial compliance handling

## Confidence

**Major Uncertainties:**
The implementation critically depends on unspecified design choices. The base KL penalty weight β in DADS is completely unspecified, significantly impacting learning dynamics. The difficulty-grading mechanism lacks implementation details, and the format reward computation is only vaguely described. The training schedule is unclear, creating uncertainty about achieving the claimed performance gains.

**Confidence Labels:**
- **High confidence** in core theoretical contributions: DADS and STCR mechanisms are mathematically well-defined
- **Medium confidence** in reproducibility: Framework is sound but missing hyperparameters create barriers
- **Low confidence** in exact performance replication: Cannot implement DADS without base β value

## Next Checks
1. Implement sensitivity analysis across β values (0.001, 0.01, 0.1) to determine optimal base weight
2. Create synthetic difficulty grading tests where G is known to verify DADS weight scaling behavior
3. Conduct ablation studies comparing full APO against variants with only DADS, only STCR, and neither