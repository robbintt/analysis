---
ver: rpa2
title: Learning Parametric Distributions from Samples and Preferences
arxiv_id: '2505.23557'
source_url: https://arxiv.org/abs/2505.23557
tags:
- preferences
- preference
- learning
- have
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies when preference feedback improves parameter
  estimation for continuous parametric distributions. The authors show that preference-based
  M-estimators achieve lower asymptotic variance than sample-only M-estimators, with
  further improvement for deterministic preferences.
---

# Learning Parametric Distributions from Samples and Preferences

## Quick Facts
- **arXiv ID:** 2505.23557
- **Source URL:** https://arxiv.org/abs/2505.23557
- **Reference count:** 40
- **Primary result:** Preference-based estimators achieve O(1/n) convergence vs O(1/√n) for sample-only methods

## Executive Summary
This paper establishes when preference feedback improves parameter estimation for continuous parametric distributions. The authors show that preference-based M-estimators achieve lower asymptotic variance than sample-only M-estimators, with further improvement for deterministic preferences. They introduce a DP MLE estimator that leverages deterministic preferences' hard constraints to achieve an accelerated O(1/n) estimation error rate, which is shown to be minimax optimal up to dimension-dependent constants. While their assumptions are restrictive, they are satisfied by notable cases like Gaussian and Laplace distributions with log-probability rewards.

## Method Summary
The paper studies parameter estimation from sample pairs (X, Y) and preferences Z indicating which sample is preferred. Three estimators are analyzed: SO MLE (sample-only maximum likelihood), SP MLE (stochastic preference MLE), and DP MLE (deterministic preference MLE with hard constraints). The key insight is that deterministic preferences enable constraint-based estimation with O(1/n) convergence, while stochastic preferences only improve variance. For Gaussian distributions with log-probability rewards, the constraint set forms a convex polytope around the true parameter.

## Key Results
- Preference-based M-estimators achieve better asymptotic variance than sample-only methods due to additive Fisher information
- Deterministic preferences enable O(1/n) convergence by imposing hard constraints on parameter space
- DP MLE achieves the best-of-both-worlds rate of O(min{d^(3/2)/n, √d/n}) and dominates SO MLE statistically
- Matching Ω(1/n) lower bound confirms acceleration is minimax optimal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preference-based M-estimators achieve lower asymptotic variance than sample-only MLE.
- **Mechanism:** The Fisher information matrix I(q_{θ⋆,h_sto}) = I(p^{⊗2}_{θ⋆}) + Δ^{SP}_{θ⋆} where Δ^{SP}_{θ⋆} is positive semidefinite. The preference gradient ∇θℓθ(X,Y) adds directional information that improves discrimination between nearby parameters, reducing variance proportional to σ(ℓθ)σ(-ℓθ)∥∇θℓθ∥².
- **Core assumption:** Preferences must be informative in all directions: P_{p^{⊗2}_{θ⋆}}(|⟨u, ∇_{θ⋆}ℓ_{θ⋆}⟩| > 0) > 0 for all u ∈ S^{k-1}.
- **Evidence anchors:** [abstract] "preference-based M-estimators achieve a better asymptotic variance than sample-only M-estimators"; [section 3.1] Lemma 3.1 proves I(q_{θ⋆,h_sto}) ⪰ I(p^{⊗2}_{θ⋆}) with definiteness under gradient spanning condition; [corpus] Limited corpus support for variance reduction magnitude; experiments show gains are empirically modest (Figure 3 shows small α_d values)
- **Break condition:** When samples have near-zero preference (≈ fair coin flip), Δ^{SP}_{θ⋆} → 0 and variance reduction vanishes.

### Mechanism 2
- **Claim:** Deterministic preferences enable O(1/n) convergence by imposing hard constraints on parameter space.
- **Mechanism:** The feasible set C_n = {θ ∈ Θ | ∀i, Z_iℓ_θ(X_i,Y_i) ≥ 0} forms a shrinking region around θ⋆. Each constraint V^{θ⋆,u}(X_i,Y_i) = ℓ_{θ⋆}(X_i,Y_i)/(-⟨u, ∇_{θ⋆}ℓ_{θ⋆}(X_i,Y_i)⟩) bounds deviation along direction u. The minimum of n such positive random variables with positive density at zero shrinks as F^{-1}_{θ⋆,u}(1/n) ≈ (n·F'_{θ⋆,u}(0))^{-1} = O(1/n).
- **Core assumption:** Assumption 4.7 requires F'_{θ⋆,u}(0) > 0 (positive density at zero) for all directions u—satisfied by Gaussian and Laplace with log-probability rewards.
- **Evidence anchors:** [abstract] "proposing an estimator achieving an estimation error scaling of O(1/n)"; [section 4.1] Theorem 4.8 proves max_{θ∈C_n} ∥θ-θ⋆∥ ≤ A_{θ⋆}·log(N(γ)/δ)/n under covering argument; [corpus] No corpus papers discuss this O(1/n) acceleration mechanism
- **Break condition:** If F'_{θ⋆,u}(0) = 0 or preferences don't span all directions (e.g., monotonic distributions like Pareto), constraints don't accumulate and rate degrades to O(1/√n).

### Mechanism 3
- **Claim:** DP MLE dominates SO MLE statistically by combining likelihood with hard constraints.
- **Mechanism:** DP MLE minimizes L^{SO}_n(θ) within C_n. For Gaussian distributions, the convex constraint set C_n intersects the likelihood ball at a point closer to θ⋆ than the unconstrained MLE, guaranteed by θ⋆ ∈ C_n and convexity.
- **Core assumption:** Preferences based on log-probability reward ensure C_n is a convex polytope (linear constraints in θ).
- **Evidence anchors:** [section 4] Lemma 4.1 proves ∥θ̂^{DP}_n - θ⋆∥_Σ ≤ ∥θ̂^{SO}_n - θ⋆∥_Σ almost surely for Gaussians; [section 6] Empirically, DP MLE outperforms all baselines, especially in high dimensions (Figure 2); [corpus] Paper "Why DPO is a Misspecified Estimator" discusses related constraint structures in preference optimization but not this specific dominance result
- **Break condition:** If θ⋆ ∉ Θ (misspecification) or preferences don't perfectly separate, C_n may be empty or θ⋆ may lie on its boundary differently.

## Foundational Learning

- **M-estimation and asymptotic normality:**
  - Why needed: The paper builds on M-estimator theory to show preference-based methods inherit asymptotic normality with modified variance.
  - Quick check: Can you derive why √n(θ̂_n - θ⋆) → N(0, I^{-1}) for MLE under regularity conditions?

- **Fisher information matrix:**
  - Why needed: Central to comparing efficiency—variance reduction appears as additive Δ terms to the Fisher information.
  - Quick check: Can you compute I(θ) = E[-∇² log p_θ] for a 1D Gaussian with known variance?

- **Minimax lower bounds via Assouad's lemma:**
  - Why needed: The Ω(1/n) lower bound uses Assouad's method since KL divergence is infinite for disagreeing deterministic preferences.
  - Quick check: Can you explain why Fano's inequality fails when KL(P||Q) = ∞ but total variation bounds still work?

## Architecture Onboarding

- **Component map:** Data: (X_i, Y_i, Z_i) pairs → Preference function ℓ_θ(X,Y) → SO MLE: θ̂^SO = argmin_θ Σ log p^{⊗2}_θ(X_i,Y_i) → SP MLE: θ̂^SP = argmin_θ L^{SO}_n(θ) + Σ log σ(Z_i·ℓ_θ(X_i,Y_i)) → Constraint set: C_n = {θ | ∀i, Z_i·ℓ_θ(X_i,Y_i) ≥ 0} → DP MLE: θ̂^DP = argmin_{θ∈C_n} L^{SO}_n(θ)

- **Critical path:** Implementing DP MLE requires: (1) computing sample-only MLE, (2) constructing C_n as linear inequality system for log-probability rewards, (3) solving constrained optimization. For Gaussian with log-probability preferences, constraints are: Z_i·⟨X_i-Y_i, θ-Σ^{-1}(X_i+Y_i)/2⟩ ≥ 0.

- **Design tradeoffs:**
  - SP MLE vs DP MLE: SP is convex unconstrained (fast); DP requires constraint satisfaction (slower but O(1/n) rate)
  - Arbitrary estimator vs DP MLE: AE picks any θ ∈ C_n (fastest); DP combines with likelihood (better constants, dominates SO MLE)
  - Stochastic vs deterministic: Stochastic is realistic but only improves variance; deterministic enables acceleration but requires noise-free preferences

- **Failure signatures:**
  - If C_n is empty → preferences contradictory or misspecified
  - If constraint gradients ∇ℓ_θ don't span → some directions unconstrained, rate degrades
  - If dim ≫ n → covering number explodes, high-dimensional constants scale as O(d^{3/2})
  - If preference model is misspecified (Bradley-Terry intransitivity) → consistency may fail

- **First 3 experiments:**
  1. **Validate O(1/n) vs O(1/√n) separation:** Sample from N(θ⋆, I_d) for d=1, compute estimation error ∥θ̂^DP_n - θ⋆∥ vs ∥θ̂^SO_n - θ⋆∥ across n ∈ [10², 10⁴], plot on log-log scale to verify slope differences (-1 vs -0.5).
  2. **Dimension scaling test:** Fix n=10⁴, vary d ∈ [1, 50], measure gap between DP MLE and SO MLE. Verify DP achieves O(min{d^{3/2}/n, √d/n}) empirically.
  3. **Constraint geometry visualization:** For d=2 Gaussian, visualize C_n as polytope intersection with likelihood contours. Confirm θ̂^DP lies closer to θ⋆ than θ̂^SO and C_n shrinks around θ⋆ as n increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dimensionality gap between the lower bound Ω(αF(k)√k/n) and upper bound O(Aθ⋆ k/n) be closed, and does it require tighter analysis or improved estimators?
- Basis in paper: [explicit] Section 5 states "there is a dimensionality gap. Closing this gap is an important direction for future work. Improvements might come from a tighter analysis, e.g., both for the upper and lower bounds, or the derivation of better estimators based on deterministic preferences."
- Why unresolved: Current analysis yields different dimension-dependence in lower vs upper bounds even for Gaussian distributions where Aθ⋆ = αF(d).
- What evidence would resolve it: Matching upper and lower bounds with identical dimension-dependence, or a provably optimal estimator with analysis achieving the minimax rate.

### Open Question 2
- Question: Can preference-based estimation benefits be quantified for discrete distributions, and what estimation rates are achievable?
- Basis in paper: [explicit] Section 8 states "a key challenge for future work is to quantify the benefits of preference-based estimation for discrete distributions. For distributions with small support, preference feedback may only localize the unknown parameter within a subset of the simplex, leading to diminishing information gains."
- Why unresolved: The current theory relies on geometric assumptions specific to continuous distributions (positive density at zero, Assumption 4.7) that may not transfer to discrete settings.
- What evidence would resolve it: Derivation of estimation rates for discrete parametric families with preference feedback, or counterexamples showing fundamental limitations.

### Open Question 3
- Question: What alternative preference functions beyond log-probability rewards satisfy the required geometric assumptions and achieve accelerated rates?
- Basis in paper: [explicit] Section 8 states "exploring alternative preference functions beyond the log-probability gap could extend the applicability of our results." Appendix B.3 shows some reward models (e.g., reference model with Gaussian) violate Assumption 4.5.
- Why unresolved: The assumptions are restrictive and characterization of valid preference functions remains incomplete.
- What evidence would resolve it: Systematic characterization of preference function classes satisfying Assumptions 4.2, 4.4, 4.5, and 4.7, with corresponding rate analysis.

### Open Question 4
- Question: Does a tractable ELBO-based optimization method exist for DP MLE under misspecification (when θ⋆ ∉ Θ)?
- Basis in paper: [inferred] Appendix B.2 discusses misspecification scenarios and states "Deriving a tractable ELBO method for this optimization is an interesting direction to obtain tractable and robust estimators."
- Why unresolved: The modified objective combining KL-divergence with classification loss may be non-convex, and computation becomes challenging without separability.
- What evidence would resolve it: An algorithm with convergence guarantees for the misspecified objective, or proof that the problem remains computationally hard.

## Limitations
- The F'_{θ⋆,u}(0) > 0 assumption is restrictive and may not hold for heavy-tailed distributions like Pareto
- O(1/n) rate requires deterministic preferences, which are rarely available in practice and sensitive to noise
- High-dimensional scaling (O(d^{3/2}/n)) may become prohibitive when d ≫ n
- Experimental validation is limited to synthetic Gaussian data with no real-world benchmarks

## Confidence
- **High confidence:** Theoretical derivation of variance reduction (Mechanism 1) and constraint geometry (Mechanism 2)
- **Medium confidence:** Empirical scaling laws in Figure 2, though only synthetic data tested
- **Low confidence:** Claims about robustness to preference model misspecification and practical performance in high-noise regimes

## Next Checks
1. Test DP MLE performance on heavy-tailed distributions (e.g., Laplace, Pareto) to verify the F'_{θ⋆,u}(0) > 0 assumption and identify failure modes when it's violated.
2. Evaluate estimators under noisy preferences (e.g., Z_i = sign(ℓ_{θ⋆} + ε_i)) to quantify the degradation from deterministic to stochastic case and validate the SP MLE framework.
3. Conduct high-dimensional experiments (d=100+) with varying n to empirically validate the d^{3/2} scaling and identify practical limits where the method becomes computationally infeasible.