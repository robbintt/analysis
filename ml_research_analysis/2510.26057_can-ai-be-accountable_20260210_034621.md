---
ver: rpa2
title: Can AI be Accountable?
arxiv_id: '2510.26057'
source_url: https://arxiv.org/abs/2510.26057
tags:
- accountability
- accountable
- forum
- agent
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores accountability in AI, emphasizing the need for
  users to be protected from the powerful, yet often inscrutable, nature of AI systems.
  It defines accountability as the ability of a forum to request information, discuss
  actions, and sanction an AI agent if necessary.
---

# Can AI be Accountable?

## Quick Facts
- arXiv ID: 2510.26057
- Source URL: https://arxiv.org/abs/2510.26057
- Reference count: 20
- Primary result: Accountability requires more than transparency—it needs enforceable forums, sanctions, and iterative updates modeled as a Markov chain process.

## Executive Summary
The paper argues that AI accountability is fundamentally a socio-technical process requiring not just transparency but enforceable mechanisms for information requests, discussion, judgment, and sanctions. It introduces the Accountable AI Markov chain to model this process as seven states (delegate, request, response, debate, judgment, sanction, update) with probabilistic transitions. The framework emphasizes that without sanctions, there is no accountability—only observation—and that audits must be designed to close information asymmetry gaps between powerful AI systems and their users.

## Method Summary
The paper presents a conceptual Markov chain model of AI accountability with seven discrete states and probabilistic transitions between them. No quantitative parameterization or empirical validation is provided; the model serves as a framework for comparing different accountability approaches, particularly against a seven-stage audit process. The methodology involves mapping existing accountability concepts (information requests, discussions, sanctions) onto Markov states and analyzing how well current audit practices cover each state.

## Key Results
- Accountability requires a complete cycle of information request, response, judgment, sanction, and system update—not just explanation
- Structured audits bridge information asymmetry by systematically gathering evidence about AI behavior
- Discussion and debate between forum and agent surface implicit values and enable iterative refinement of expectations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accountability requires a complete cycle of information request, response, judgment, sanction, and system update—not just explanation.
- Mechanism: The Accountable AI Markov chain models accountability as probabilistic transitions between discrete states (delegate → request → response → debate → judgment → sanction → update). Sanctions compel participation and changes; without them, there is no accountability, only observation.
- Core assumption: The forum has sufficient power to enforce sanctions, and the agent cannot simply ignore or evade the process.
- Evidence anchors:
  - [abstract]: "It proposes a Markov chain model to represent the AI accountability process, consisting of states such as 'Delegate task to AI agent,' 'Request to explain, justify to forum,' 'Response to forum,' 'Debate, discussion,' 'Judgment by forum,' 'Sanction by forum,' and 'Updates to AI agent.'"
  - [section 1.1.2]: "If there is no sanction, then there is no accountability. The sanction...is what can compel the actor to engage in the process, accept judgments, and make changes that support users."
  - [corpus]: Weak direct support—the multi-agent LLM pipeline paper mentions accountability but in a narrower traceability context.
- Break condition: If sanctions are unenforceable (e.g., the agent is more powerful than the forum or operates in an unregulated jurisdiction), the chain stalls at judgment with no meaningful update.

### Mechanism 2
- Claim: Structured audits bridge the information asymmetry gap by systematically gathering and analyzing evidence about AI behavior.
- Mechanism: Seven-stage audit process (harms discovery → principles/norms → information gathering → performance analysis → communication → advocacy) maps onto the Markov chain. Audits can be cooperative (agent assists) or non-cooperative (external data collection).
- Core assumption: Auditors have technical expertise and access to either transparency infrastructure (APIs, documentation) or behavioral data.
- Evidence anchors:
  - [section 1.3.1]: "Ojewale et al. (2024) conducted interviews with experts and reviewed 435 audit-related tools. The authors categorize tools as supporting one or more of seven proposed stages of the audit process."
  - [section 1.4.1]: pymetrics case study—"the audit was conducted in cooperation with pymetrics...the cooperation took the form of 'onboarding' the audit team by pymetrics employees as well as technical assistance with code."
  - [corpus]: Weak—no corpus papers directly address audit tooling.
- Break condition: If the AI is fully black-box with no transparency infrastructure and no behavioral telemetry, audits cannot progress past harms discovery.

### Mechanism 3
- Claim: Discussion and debate between forum and agent (or its developers) surface implicit values and enable iterative refinement of expectations.
- Mechanism: The "debate, discussion" state allows both parties to articulate preferences and constraints, which can loop back to new information requests or directly to updates. This is often omitted but critical for non-expert stakeholders.
- Core assumption: Both parties engage in good faith and have sufficient shared language to communicate meaningfully.
- Evidence anchors:
  - [section 1.5.2]: "Brandsma and Schillemans (2013)...point out the importance of (sometimes informal) discussions between the agent and the forum, saying that 'the principal may learn to articulate [their] preferences...and the agent may learn what its principal really wants to see accomplished.'"
  - [section 1.4.1]: pymetrics audit omitted the debate state—"the 'debate' state was skipped"—showing the chain can function without it but potentially at the cost of deeper alignment.
  - [corpus]: Limited—the online forums paper examines GenAI shaping debates but does not address accountability dialogue.
- Break condition: If stakeholders lack technical literacy or developers refuse engagement, discussion collapses into one-way communication with no mutual learning.

## Foundational Learning

- Concept: Markov chains as process models
  - Why needed here: The paper models accountability as a Markov chain with probabilistic state transitions. Understanding this representation is necessary to map audit stages and design intervention points.
  - Quick check question: Can you identify which states in the chain have multiple possible next states and what determines the transition probabilities?

- Concept: Information asymmetry in principal-agent relationships
  - Why needed here: The paper frames accountability as a solution to information asymmetry. Without this lens, you may overestimate the value of transparency alone.
  - Quick check question: In an AI deployment, who holds information that others cannot access, and what incentives do they have to share or withhold it?

- Concept: Socio-technical accountability systems
  - Why needed here: The paper argues accountability is not purely technical—it requires governing structures, enforcement, and human institutions. Technical tools (audits, explainability) are necessary but insufficient.
  - Quick check question: If you have perfect model explainability but no sanctioned forum with enforcement power, do you have accountability?

## Architecture Onboarding

- Component map:
  - **AI Agent**: Executes delegated tasks; may include model, data pipeline, and decision logic.
  - **Transparency Infrastructure**: APIs, model cards, datasheets, logging—enables information requests.
  - **Forum**: Elected body, regulator, citizen panel, or internal review board with authority to request, judge, and sanction.
  - **Audit Pipeline**: Tools and processes for harms discovery, information gathering, performance analysis, and communication.
  - **Sanction Mechanism**: Legal penalties, contract termination, public disclosure, or mandated updates.
  - **Update Loop**: Engineering process to incorporate forum judgments into model/system changes.

- Critical path:
  1. Define the forum and its authority (who can hold the AI accountable?).
  2. Implement transparency infrastructure (can the forum request and receive meaningful information?).
  3. Establish sanction enforceability (what happens if the agent refuses or fails?).
  4. Design the update pipeline (how do judgments translate into code/model changes?).

- Design tradeoffs:
  - **Cooperative vs. adversarial audits**: Cooperative audits (like pymetrics) enable deeper access but risk capture; adversarial audits preserve independence but face information limits.
  - **Transparency vs. IP/privacy**: Full disclosure supports accountability but may conflict with trade secrets or data protection.
  - **Speed vs. safety**: "Permissionless innovation" accelerates deployment but weakens pre-deployment accountability; incrementalism slows deployment but allows governance to catch up.

- Failure signatures:
  - **Audit-washing**: Audits are performed but lack enforcement power; results are used to create impression of accountability without substantive change (section 1.3.1).
  - **Sanction gap**: Forum issues judgment but cannot enforce; updates are optional or ignored (e.g., Houston teacher algorithm operated for ~a decade before court reversal).
  - **Transparency theater**: Model cards and datasheets exist but are incomplete, misleading, or disconnected from actual system behavior.

- First 3 experiments:
  1. Map an existing AI system to the Markov chain: Identify which states are implemented, which are missing, and where transition probabilities are unknown or near-zero.
  2. Design a minimal transparency interface: For a single AI decision type, specify what information the forum would need to request, how it would be exposed (API, report, log), and what format enables non-expert review.
  3. Simulate a sanction scenario: Define a concrete harm, a forum judgment, and a technically feasible update; trace whether the update path exists or would require new engineering work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "debate, discussion" phase of the AI accountability process be quantitatively evaluated?
- Basis in paper: [explicit] Page 19 notes that while crucial for defining preferences, there are very few studies providing quantitative evaluations of this phase, particularly for non-professional users.
- Why unresolved: Current research primarily focuses on information gathering and performance analysis, lacking metrics for the quality of deliberative interaction.
- What evidence would resolve it: Empirical studies defining metrics for discussion quality and measuring their correlation with accountability outcomes.

### Open Question 2
- Question: How do non-professional users differ in their accountability expectations across diverse AI interaction contexts?
- Basis in paper: [explicit] Page 19 explicitly asks, "What will these users expect of AI...?" and "How will users differ in their expectations?" in the context of increasingly varied AI use.
- Why unresolved: Users engage with AI in widely different scenarios (e.g., coding vs. driving), but it is unclear how these contexts shape their demands for explainability or sanctions.
- What evidence would resolve it: Survey or experimental data comparing user accountability requirements across distinct domains like automated vehicles, education, and creative tools.

### Open Question 3
- Question: How can AI auditing frameworks be designed to explicitly incorporate the "updates to AI agent" state?
- Basis in paper: [inferred] Page 10 highlights that existing audit stages (like communication/advocacy) do not neatly map onto the "updates" state, creating a disconnect between sanction and correction.
- Why unresolved: Audits often end with judgment or sanction, lacking a formalized mechanism to verify that the AI agent is actually updated to prevent future harm.
- What evidence would resolve it: A modified audit framework that includes a mandatory "mitigation verification" stage, demonstrating statistically significant improvements in the AI post-sanction.

## Limitations
- The Markov chain model is conceptual without quantitative parameterization, making predictive validity difficult to assess
- Audit case studies are descriptive without systematic evaluation of outcomes or counterfactuals
- The relationship between Markov states and seven-stage audit process is illustrated but not validated against independent implementations

## Confidence
- **High confidence**: The structural components of the accountability framework (information request, response, judgment, sanction, updates) and their logical necessity for accountability
- **Medium confidence**: The applicability of the Markov chain model to diverse AI accountability scenarios and the effectiveness of audit tools in practice
- **Low confidence**: Quantitative predictions about accountability outcomes, specific transition probabilities, or the model's ability to predict accountability failures

## Next Checks
1. Implement the Markov chain with baseline transition probabilities and test against real-world AI accountability case studies to identify state gaps and transition failure points
2. Conduct comparative analysis of cooperative versus adversarial audit approaches across multiple AI systems to quantify trade-offs in information access versus independence
3. Design and test a minimal transparency interface for a specific AI decision type, measuring whether non-expert forum members can meaningfully request and interpret required information