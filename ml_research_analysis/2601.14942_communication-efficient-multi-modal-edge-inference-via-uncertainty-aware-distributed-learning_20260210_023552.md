---
ver: rpa2
title: Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed
  Learning
arxiv_id: '2601.14942'
source_url: https://arxiv.org/abs/2601.14942
tags:
- communication
- semantic
- stage
- multi-modal
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of communication-efficient and
  robust multi-modal edge inference over bandwidth-limited wireless channels. The
  key difficulty is that distributed learning with multiple devices incurs high communication
  overhead, and inference robustness is limited when facing varying channels and noisy
  modalities.
---

# Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning

## Quick Facts
- arXiv ID: 2601.14942
- Source URL: https://arxiv.org/abs/2601.14942
- Reference count: 40
- Primary result: Achieves higher accuracy with ~10-20x fewer communication rounds than baselines for multi-modal edge inference

## Executive Summary
This paper tackles the challenge of communication-efficient and robust multi-modal edge inference over bandwidth-limited wireless channels. The core difficulty is that distributed learning with multiple devices incurs high communication overhead, while inference robustness is limited when facing varying channels and noisy modalities. The authors propose a three-stage framework that addresses both challenges through local pre-training, server-side fine-tuning with evidential fusion, and uncertainty-guided retransmission.

The framework achieves significant communication efficiency gains by reducing training communication rounds by an order of magnitude compared to existing approaches, while maintaining or improving accuracy. The method is validated on RGB-depth indoor scene classification, demonstrating robustness to modality degradation and channel variation. The key innovation lies in the uncertainty-aware design that selectively requests additional features only for uncertain samples, balancing accuracy and communication cost.

## Method Summary
The proposed framework consists of three stages designed to minimize communication while maximizing inference robustness. First, local multi-modal self-supervised pre-training enables devices to learn shared and modality-specific feature representations without any communication to the server. Second, the server performs supervised fine-tuning using evidential fusion to calibrate per-modality uncertainty and aggregate potentially distorted features from different devices. Third, uncertainty-guided retransmission selectively requests additional features for samples where the model exhibits high uncertainty, reducing unnecessary communication for confident predictions.

The self-supervised pre-training stage uses contrastive learning to disentangle shared and modality-specific features, while the evidential fusion component models uncertainty through Gaussian distributions. The retransmission policy uses uncertainty thresholds to determine when to request additional information from devices, creating a dynamic communication pattern that adapts to the difficulty of each inference task.

## Key Results
- Achieves ~10x fewer communication rounds compared to contrastive pre-training approaches
- Reduces communication by more than 20x compared to training from scratch methods
- Maintains higher accuracy than existing self-supervised and fully supervised baselines
- Demonstrates robustness to modality degradation and varying channel conditions

## Why This Works (Mechanism)
The framework works by addressing the fundamental tension between communication efficiency and inference robustness through uncertainty-aware design. By performing self-supervised pre-training locally on devices, the method eliminates the need for initial communication rounds while still learning useful feature representations. The evidential fusion mechanism provides calibrated uncertainty estimates that guide selective retransmission, ensuring that communication resources are allocated only where they have the greatest impact on accuracy.

The disentanglement of shared and modality-specific features during pre-training enables the model to handle missing or degraded modalities gracefully, as it can rely on the shared features when one modality is compromised. The uncertainty-guided retransmission creates a feedback loop where the model's confidence directly informs communication decisions, creating an adaptive system that automatically balances accuracy and efficiency based on the difficulty of the inference task and current channel conditions.

## Foundational Learning

1. **Multi-modal representation learning** - Needed to extract useful features from multiple data sources (RGB, depth). Quick check: Ensure models can handle missing modalities without catastrophic performance drops.

2. **Self-supervised contrastive learning** - Enables feature learning without labeled data, reducing dependence on server-device communication. Quick check: Verify learned representations transfer well to downstream tasks.

3. **Evidential deep learning** - Provides uncertainty quantification through evidential fusion of features. Quick check: Confirm uncertainty estimates correlate with prediction accuracy.

4. **Distributed optimization** - Manages parameter updates across multiple devices with limited communication. Quick check: Ensure convergence despite communication constraints.

5. **Uncertainty-guided active learning** - Selects which samples to request more information for based on model confidence. Quick check: Verify that uncertain samples are correctly identified and prioritized.

6. **Wireless communication constraints** - Accounts for bandwidth limitations and channel variability. Quick check: Confirm framework maintains performance under realistic network conditions.

## Architecture Onboarding

**Component Map**: Local Pre-training -> Server Fine-tuning -> Uncertainty Retransmission

**Critical Path**: Device features → Server aggregation → Uncertainty estimation → Selective retransmission

**Design Tradeoffs**: The framework trades off between communication frequency and inference accuracy by using uncertainty thresholds. Lower thresholds increase accuracy but also increase communication overhead, while higher thresholds reduce communication but may sacrifice accuracy on difficult samples.

**Failure Signatures**: 
- If pre-training fails, devices cannot learn useful representations without communication
- Poor uncertainty calibration leads to either excessive retransmission or missed opportunities for accuracy improvement
- Channel variations that exceed the framework's adaptability range may cause performance degradation

**First Experiments**:
1. Baseline comparison with standard federated learning approaches under identical communication budgets
2. Ablation study removing each stage to quantify individual contributions
3. Robustness testing under controlled modality degradation and channel variation scenarios

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding scalability to more than two modalities, theoretical guarantees for the self-supervised disentanglement mechanism, and the method's performance under extreme channel conditions with packet loss exceeding 50%. Additionally, the framework's adaptability to non-stationary data distributions and the characterization of hardware constraints on edge devices remain unexplored areas for future research.

## Limitations
- Theoretical guarantees for self-supervised disentanglement are not fully explored
- Method's scalability to more than two modalities is untested
- Evidential fusion assumes Gaussian-distributed uncertainty estimates that may not hold in all scenarios

## Confidence

- Multi-modal self-supervised pre-training effectiveness: High
- Evidential fusion for uncertainty calibration: Medium
- Communication efficiency gains: High
- Robustness to modality degradation: Medium
- Retransmission policy optimality: Low

## Next Checks

1. Test the framework on datasets with three or more modalities to assess scalability.
2. Conduct ablation studies to isolate the impact of each stage on overall performance.
3. Evaluate the method under varying packet loss rates to understand its robustness limits.