---
ver: rpa2
title: 'Think Outside the Policy: In-Context Steered Policy Optimization'
arxiv_id: '2510.26519'
source_url: https://arxiv.org/abs/2510.26519
tags:
- icpo
- training
- expert
- grpo
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces In-Context Steered Policy Optimization (ICPO),
  a reinforcement learning framework for Large Reasoning Models (LRMs) that addresses
  the exploration limitations of standard methods like GRPO. Instead of relying on
  external expert models or additional computational overhead, ICPO leverages the
  in-context learning capability of LRMs to generate expert-guided rollouts from existing
  datasets, enabling exploration beyond the current policy distribution.
---

# Think Outside the Policy: In-Context Steered Policy Optimization

## Quick Facts
- **arXiv ID:** 2510.26519
- **Source URL:** https://arxiv.org/abs/2510.26519
- **Authors:** Hsiu-Yuan Huang; Chenming Tang; Weijie Liu; Saiyong Yang; Yunfang Wu
- **Reference count:** 40
- **Primary result:** Introduces ICPO, a RL framework that improves mathematical reasoning by leveraging in-context learning to generate expert-guided rollouts, achieving +4.1 points over GRPO on in-distribution benchmarks

## Executive Summary
ICPO addresses exploration limitations in RLVR for Large Reasoning Models by using the model's own in-context learning capability with existing datasets as demonstrations. Rather than relying on external expert models or additional computational overhead, ICPO expands exploration beyond the current policy distribution through expert-conditioned rollouts. The method incorporates Expert Region Reject Sampling to filter noisy trajectories and Annealed Expert-Bonus Reward Shaping to balance early expert guidance with later autonomous learning. Experiments show consistent improvements over vanilla GRPO and mixed-policy variants across mathematical reasoning benchmarks.

## Method Summary
ICPO combines mixed-policy reinforcement learning with in-context learning to steer policy optimization. The method generates expert-conditioned (IEF) rollouts by prepending randomly sampled demonstrations from existing datasets to training prompts, then selectively incorporates these into the policy update using importance sampling. Key innovations include Expert Region Reject Sampling (filtering low-quality expert trajectories) and Annealed Expert-Bonus Reward Shaping (decaying expert guidance over training). The framework uses VERL with 7 on-policy and 1 off-policy rollout per prompt, trained for 400 steps on filtered OpenR1-Math datasets with Qwen3 models.

## Key Results
- ICPO consistently improves mathematical reasoning performance with maximum average gains of +4.1 points over vanilla GRPO
- Enhanced exploration demonstrated through larger inter-trajectory edit distances and higher "flip" rates from incorrect to correct answers
- Improved training stability and better out-of-distribution generalization when using reward shaping
- Performance gains maintained across multiple model scales (Qwen3-1.7B, 7B, 8B)

## Why This Works (Mechanism)

### Mechanism 1: Implicit Expert Forcing via In-Context Learning
ICL-conditioned rollouts provide diverse, high-quality expert signals that steer exploration beyond the current policy distribution. Demonstrations encode task vectors, creating an expert-induced prior that biases rollout distribution toward expert-aligned regions without explicit optimization. The model must possess sufficient ICL capability to extract useful priors from demonstrations.

### Mechanism 2: Expert Region Reject Sampling (ERRS)
Filtering low-quality expert-conditioned trajectories stabilizes training by preventing misleading gradients. Only trajectories with reward R(τ) ≥ δ contribute to policy updates, restricting expectation to trajectories within the expert region. This requires verifiable rewards to accurately signal trajectory quality.

### Mechanism 3: Annealed Expert-Bonus Reward Shaping
Time-decaying bonus balances early expert imitation with later autonomous optimization. R_shaped(τ) = R(τ) + α·γ(t) where γ(t) = 1 - t/T linearly decays, enforcing strong expert-guided shaping in early training and progressively relaxing it for autonomous optimization.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: Core mechanism for steering without external expert models; understanding how demonstrations shift output distributions is essential
  - Quick check question: How does conditioning on [D; q] differ from standard inference, and what implicit prior does it induce?

- **Concept: Importance Sampling in Off-Policy RL**
  - Why needed here: Mixed-policy GRPO requires computing importance ratios r̂(θ) = π_θ(τ|τ_<t) / π_θ^IEF(τ|τ_<t) between default and expert-conditioned policies
  - Quick check question: Why is π_θ^IEF treated as a distinct policy despite being the same model with different input context?

- **Concept: Reward Shaping with Annealing Schedules**
  - Why needed here: The α·γ(t) bonus must decay appropriately to balance guidance vs. autonomy
  - Quick check question: What happens if the bonus decays too slowly vs. too quickly relative to training convergence?

## Architecture Onboarding

- **Component map:** [Prompt Batch] → [On-Policy Rollout Generator] → N_on trajectories → [Mixed Advantage Computation] → GRPO Loss
                        ↓
                [Demo Pool D] → [IEF Rollout Generator (with ICL)] → N_off trajectories
                                                    ↓
                                    [ERRS Filter: R(τ) ≥ δ?] → Accept/Reject
                                                    ↓
                                    [Reward Shaper: +α·γ(t)] (optional)

- **Critical path:**
  1. Sample batch of B prompts
  2. Generate N on-policy rollouts per prompt from π_θ^old
  3. For each prompt, sample k demonstrations from D, form expert context x_exp
  4. Generate 1 IEF rollout per prompt from π_θ^old(·|x_exp)
  5. ERRS: If R(τ_IEF) ≥ δ, replace random on-policy rollout; else discard
  6. If RS enabled: Add α·γ(t) to accepted expert trajectory rewards
  7. Compute group-relative advantages over mixed rollout set
  8. Optimize J_ICPO via clipped objective

- **Design tradeoffs:**
  - **On/off-policy ratio:** 7:1 default; more off-policy increases exploration but may destabilize gradients
  - **Demonstration pool size:** Full pool (~7500) favors late-stage exploration; smaller pool (~10) stabilizes early training
  - **RS bonus weight α:** Higher α strengthens early guidance but risks over-reliance

- **Failure signatures:**
  - Low IEF acceptance rate (<30%): ICL not providing useful guidance; check demonstration quality
  - Early training instability: Reduce α or increase on-policy ratio
  - Late-stage performance plateau: Increase demonstration pool diversity
  - OOD degradation with RS: Expert domain may be over-weighted; reduce α or disable RS

- **First 3 experiments:**
  1. **Validate IEF quality:** Compare 0-shot vs 1-shot rollout accuracy/diversity on held-out problems before any RL training
  2. **Sweep ERRS threshold:** Test δ ∈ {0.5, 0.8, 1.0, 1.2} to calibrate filtering strength
  3. **Ablate demonstration selection:** Compare random vs difficulty-matched vs subject-matched selection strategies on a small training run

## Open Questions the Paper Calls Out

### Open Question 1
Can ICPO effectively improve reasoning in non-mathematical domains such as code generation, where verifiable rewards and solution formats differ substantially? The method has only been validated on mathematical benchmarks, and code generation requires different verifiers, demonstration formats, and potentially different reward structures that remain untested.

### Open Question 2
Can demonstration selection strategies that are tailored to individual problem characteristics systematically outperform the random sampling approach adopted in this work? The paper tested difficulty-based, length-based, and subject-based heuristics but found random selection performed comparably, suggesting the optimal selection mechanism remains unidentified.

### Open Question 3
How does ICPO behave when applied to models that lack strong in-context learning capabilities, and is there a minimum ICL capability threshold required for the method to be effective? The core mechanism relies entirely on the model's ability to leverage demonstrations, and without ICL, expert-conditioned rollouts may not provide useful guidance.

## Limitations
- The effectiveness of ERRS depends critically on the quality and calibration of the verifiable reward function, which may not generalize well across domains
- The anneal schedule's optimal parameters appear task-dependent with no clear guidance for adaptation to different reasoning domains
- Theoretical understanding of why certain demonstration contexts work better than others remains limited

## Confidence

- **High Confidence:** Empirical improvements over baselines on standard math benchmarks, with consistent +2-4 point gains across multiple model scales
- **Medium Confidence:** The mechanism by which ICL-conditioned rollouts improve exploration is supported by quantitative analysis but lacks deeper causal explanation
- **Medium Confidence:** The three-component architecture provides incremental improvements, though ablation studies show component criticality varies by evaluation setting

## Next Checks

1. **Generalization to Non-Math Domains:** Test ICPO on other reasoning-intensive tasks (code generation, commonsense reasoning, multi-step decision making) to evaluate whether the in-context steering mechanism transfers beyond mathematical problem-solving

2. **Scaling Relationship Analysis:** Systematically vary model size and demonstration pool sizes to determine whether the ICL-based steering mechanism exhibits predictable scaling behavior or threshold effects

3. **Robustness to Demonstration Quality:** Intentionally corrupt or misalign demonstration contexts to measure sensitivity to demonstration quality and validate the effectiveness of the Expert Region Reject Sampling mechanism