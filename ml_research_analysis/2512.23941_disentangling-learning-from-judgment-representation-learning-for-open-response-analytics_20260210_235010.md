---
ver: rpa2
title: 'Disentangling Learning from Judgment: Representation Learning for Open Response
  Analytics'
arxiv_id: '2512.23941'
source_url: https://arxiv.org/abs/2512.23941
tags:
- learning
- teacher
- teachers
- responses
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework to disentangle student response
  content from teacher grading biases in open-response assessment. Using de-identified
  mathematics responses from ASSISTments, it models teacher grading histories as priors
  and represents student responses with sentence embeddings.
---

# Disentangling Learning from Judgment: Representation Learning for Open Response Analytics

## Quick Facts
- arXiv ID: 2512.23941
- Source URL: https://arxiv.org/abs/2512.23941
- Reference count: 33
- Primary result: Modeling teacher grading histories as priors and isolating content features yields AUC≈0.815, outperforming content-only models (AUC≈0.626)

## Executive Summary
This paper presents a framework to disentangle student response content from teacher grading biases in open-response assessment. Using de-identified mathematics responses from ASSISTments, it models teacher grading histories as priors and represents student responses with sentence embeddings. By combining these signals and applying centering and residualization, the framework isolates content-based features from rater effects. Temporally-validated linear models show that incorporating teacher priors with content embeddings yields the strongest performance (AUC≈0.815), while content-only models are weaker (AUC≈0.626). Adjusting for rater effects also sharpens feature selection and reveals meaningful patterns of disagreement, enabling interpretable analytics that surface where student reasoning may be overlooked due to teacher grading practices.

## Method Summary
The framework combines sentence embeddings of student responses with dynamic teacher priors (historical grading averages) to predict scores. Responses and prompts are encoded as 384-dimensional vectors using SBERT-style models. Teacher priors are computed as rolling averages of past grades. The method applies centroid normalization and response-prompt differences to transform embeddings, then fits Lasso regression models. Residualization/orthogonalization isolates content signal by partialling out teacher and problem effects. Temporal 80/20 splits ensure validation without data leakage.

## Key Results
- Teacher priors alone achieve AUC=0.799, outperforming response-only models (AUC=0.626)
- Combined model with teacher priors and content embeddings achieves AUC≈0.815
- After residualization, 40 of 384 embedding dimensions remain nonzero (10.4%) vs 18 (4.7%) unadjusted
- The framework reveals interpretable disagreement patterns between content-based and prior-based predictions

## Why This Works (Mechanism)

### Mechanism 1
Teacher grading histories encode predictable rater tendencies that, when modeled as priors, explain substantial variance in assigned scores independent of response content. Dynamic teacher-level averages of past grades capture baseline leniency/severity patterns. Core assumption: Teacher grading behavior exhibits temporal stability and systematic bias that persists across different student responses. Evidence: Teacher prior only model achieves AUC=0.799, outperforming content-only models (AUC=0.626). Break condition: When teachers grade randomly, inconsistently, or when grading conditions change dramatically.

### Mechanism 2
Response embeddings capture semantic content but conflate multiple signals (reasoning quality, writing style, surface features) that teachers may weight differently. Sentence encoders map student responses to 384-dimensional vectors. Core assumption: Embeddings encode pedagogically-relevant distinctions, though not necessarily aligned with any single teacher's grading criteria. Evidence: Response-only model achieves AUC=0.626, "reliably above chance but substantially weaker." Break condition: When responses contain heavy symbolic notation, images, or domain-specific language poorly represented in pre-trained embeddings.

### Mechanism 3
Explicitly modeling rater effects through residualization sharpens content representations by reallocating shared variance to interpretable sources. After partialling out teacher priors and problem-level regularities, more embedding dimensions survive regularization (10.4% vs 4.7%). Core assumption: Variance in grades reflects additive contributions from rater tendencies and content quality; separating them does not destroy predictive information. Evidence: "After orthogonalization, 40 of 384 coordinates (10.4%) remain nonzero" vs 18 of 384 (4.7%) unadjusted. Break condition: When teacher priors and content are perfectly confounded, residualization cannot separate signals.

## Foundational Learning

- **Sentence Embeddings (SBERT-style)**: Core representation for student responses; understanding what they capture (and miss) is essential for interpreting model outputs. Quick check: Given two math responses—"The answer is 4 because I multiplied" vs "4"—would you expect similar embeddings? Why or why not?

- **Rater Effects / Grading Bias**: The paper's central claim is that teacher grading is not purely content-driven; understanding halo effects, leniency, and severity is prerequisite. Quick check: If Teacher A averages 3.2/4 and Teacher B averages 2.1/4 on identical responses, what does this imply about using raw scores for model training?

- **Residualization / Orthogonalization**: Technical method for isolating content signal by regressing out confounds; requires understanding of linear algebra intuition. Quick check: After regressing out teacher priors from grades, what does the residual represent? What assumptions must hold for this to be meaningful?

## Architecture Onboarding

- **Component map**: Data preprocessing → Teacher prior computation → Embedding generation → Centering/differencing transforms → Linear model (Lasso) → Disagreement detection → Qualitative projection

- **Critical path**: Ensure teacher IDs have sufficient grading history → Generate embeddings for all responses → Compute teacher priors from temporal training split only → Fit Lasso with cross-validated regularization → Evaluate on temporally held-out test set

- **Design tradeoffs**: Linear vs. nonlinear models (paper uses Lasso for interpretability); 10-word minimum filtering (excludes symbolic responses but stabilizes text modeling); temporal vs. random split (temporal validation is ecologically valid but may underestimate performance if grading patterns shift)

- **Failure signatures**: Teacher prior-only model outperforms combined model → content features may be misaligned with grading criteria; near-zero embedding coefficients after regularization → priors absorbing all variance; high disagreement rate with no interpretable pattern → embeddings may not capture domain-relevant semantics

- **First 3 experiments**:
  1. Reproduce teacher-only (AUC≈0.799) vs. response-only (AUC≈0.626) gap on held-out data to validate pipeline integrity
  2. Compare raw embeddings vs. centroid-normalized vs. response-prompt differences to quantify transform contributions
  3. Sample 50 cases where content-only and prior-only models disagree; manually code for conceptual vs. procedural reasoning to validate RQ3 interpretability claims

## Open Questions the Paper Calls Out

- To what extent does disentangling content from rater effects generalize across academic domains beyond middle school mathematics? The authors state "this study relies on a single dataset from a single platform, which is dominated by middle school mathematics items on ratios and proportions" and that "extending analyses to other domains and longer forms of discourse is necessary to assess generalizability."

- How can bias dimensions inherent in pre-trained sentence embeddings be systematically identified and mitigated when modeling student responses? The authors acknowledge "the embeddings used for analysis may capture biases inherent in language models (e.g., gender bias when assessing student work)" and state that "separating out relevant dimensions of bias in learned representations...is a separate but a fruitful line of future work."

- What teacher-facing analytics designs effectively support reflection on grading practices using decomposed content and prior signals? The authors state "future research should focus on translating the approaches introduced here into learning analytics capabilities that support instructional use and provide actionable guidance for teachers" and "further co-design with teachers will be needed to ensure practical and desirable designs."

## Limitations

- Assumes teacher grading behavior is temporally stable enough for prior-based modeling, but abrupt rubric changes or grading fatigue could invalidate this assumption
- Filtering criteria (10+ words, valid skill codes) may systematically exclude certain response types, potentially biasing content representation
- Reliance on ASSISTments data limits generalizability to other educational contexts or assessment types

## Confidence

- **High confidence**: Claims about AUC improvements when combining teacher priors with content embeddings (AUC ≈ 0.815) - well-supported by temporal validation results
- **Medium confidence**: Claims about interpretability gains through residualization and feature selection - supported by coefficient sparsity differences but not explicitly validated through qualitative case studies
- **Low confidence**: Claims about the specific pedagogical insights revealed by disagreement analysis - methodology described but limited empirical validation of interpretive claims

## Next Checks

1. **Temporal stability validation**: Replicate the analysis with rolling time windows to quantify how rapidly teacher priors become outdated and require recalibration
2. **Generalizability test**: Apply the same framework to a different subject domain (e.g., science open responses) to test cross-domain robustness of the disentanglement approach
3. **Interpretability audit**: Conduct blinded manual review of 50 randomly selected cases where model variants disagree to quantify the accuracy of automated disagreement categorization