---
ver: rpa2
title: 'CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large
  Language Model'
arxiv_id: '2506.13055'
source_url: https://arxiv.org/abs/2506.13055
tags:
- financial
- question
- multimodal
- mllms
- charts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFBenchmark-MM, a Chinese multimodal financial
  benchmark with over 9,000 image-question pairs covering tables, charts, and structural
  diagrams. It proposes a staged evaluation system to assess Multimodal Large Language
  Models' (MLLMs) performance in handling financial multimodal data.
---

# CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2506.13055
- Source URL: https://arxiv.org/abs/2506.13055
- Authors: Jiangtong Li; Yiyun Zhu; Dawei Cheng; Zhijun Ding; Changjun Jiang
- Reference count: 40
- Primary result: Chinese multimodal financial benchmark with 9,000+ image-question pairs evaluating MLLM performance at 52% accuracy

## Executive Summary
This paper introduces CFBenchmark-MM, a comprehensive Chinese multimodal financial benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on financial data processing tasks. The benchmark comprises over 9,000 image-question pairs spanning tables, charts, and structural diagrams, addressing a critical gap in domain-specific MLLM evaluation. Through systematic testing of leading models including GPT-4V, the study reveals that while MLLMs possess basic financial knowledge, their performance in multimodal financial contexts remains limited, with the best model achieving only 52% accuracy and 38% on specialized metrics.

The research identifies key challenges including visual content misinterpretation and financial concept misunderstanding as primary sources of error. The authors propose a staged evaluation system to assess different aspects of MLLM capabilities in financial contexts. The findings highlight significant potential for MLLMs in financial analysis while emphasizing the need for domain-specific optimization and architectural improvements to enhance robustness and efficiency in processing complex financial multimodal data.

## Method Summary
The authors developed CFBenchmark-MM through systematic collection and curation of Chinese financial documents containing tables, charts, and structural diagrams. Each visual element was paired with carefully crafted questions testing various financial reasoning capabilities. The benchmark employs a staged evaluation system that progressively assesses models' abilities from basic visual recognition to complex financial reasoning. Experiments were conducted across multiple state-of-the-art MLLMs including GPT-4V, Claude-3, and Gemini models, with performance measured using accuracy and specialized financial reasoning metrics. Error analysis was performed to categorize failure modes and identify areas for improvement.

## Key Results
- GPT-4V achieved 52% accuracy on the benchmark, with specialized metrics scoring only 38%
- MLLMs demonstrated basic financial knowledge but struggled with multimodal context processing
- Error analysis revealed visual misinterpretation and financial concept misunderstanding as primary failure modes
- Models performed worse on pure text settings compared to their corresponding LLMs, indicating knowledge loss during multimodal training
- The staged evaluation system successfully differentiated model capabilities across complexity levels

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of real-world financial document types and the systematic evaluation framework that isolates different aspects of multimodal reasoning. By combining visual and textual elements representative of actual financial analysis scenarios, the benchmark creates a realistic testing environment that reveals both the strengths and limitations of current MLLM architectures.

## Foundational Learning
- **Multimodal Financial Reasoning**: Ability to integrate visual chart/table data with textual financial concepts - needed for real-world financial analysis, quick check: successful completion of mixed-modality questions
- **Visual Financial Document Processing**: OCR and structural understanding of financial tables and charts - essential for data extraction, quick check: accurate table cell recognition and chart axis interpretation
- **Financial Domain Knowledge**: Understanding of financial terminology, metrics, and relationships - critical for meaningful analysis, quick check: correct interpretation of financial ratios and market indicators
- **Sequential Reasoning**: Ability to follow multi-step logical processes in financial contexts - required for complex analysis, quick check: accurate step-by-step problem solving
- **Error Pattern Recognition**: Identifying types of failures in multimodal processing - important for model improvement, quick check: consistent categorization of error sources

## Architecture Onboarding

**Component Map:**
Data Collection -> Annotation -> Question Generation -> Model Evaluation -> Error Analysis

**Critical Path:**
Visual Encoding -> Multimodal Fusion -> Financial Reasoning -> Answer Generation

**Design Tradeoffs:**
- General vs. domain-specific visual encoders (accuracy vs. flexibility)
- Single vs. multi-agent architectures (simplicity vs. specialization)
- Comprehensive vs. focused benchmark scope (realism vs. manageability)

**Failure Signatures:**
- Visual misinterpretation of chart elements
- Financial concept confusion
- Knowledge loss from base LLM during multimodal training
- Sequential reasoning breakdowns

**First 3 Experiments:**
1. Compare standard CLIP-based visual encoders against finance-specific alternatives
2. Test multi-agent systems vs. monolithic models on complex financial tasks
3. Evaluate regularization techniques to prevent knowledge loss during multimodal training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specialized visual alignment modules be developed to more accurately represent financial charts, surpassing the limitations of general pre-trained models like CLIP and ViT?
- **Basis in paper:** [explicit] Appendix A explicitly calls for "Developing specialized visual alignment modules" because current reliance on pre-trained models offers limited functionality in accurately representing financial charts.
- **Why unresolved:** General vision encoders are trained on natural images, which lack the high-frequency details and specific structural patterns (axes, legends, distinct data points) inherent in financial charts.
- **What evidence would resolve it:** A newly trained visual encoder specifically on financial chart datasets that achieves significantly higher OCR and structural recognition scores on CFBenchmark-MM compared to standard CLIP-based baselines.

### Open Question 2
- **Question:** Can multi-agent systems enhance the effectiveness and decision-making accuracy of Multimodal Large Language Models (MLLMs) in financial contexts?
- **Basis in paper:** [explicit] Appendix A lists "Investigating the potential of multi-agent systems" to understand how collaborative interactions can enhance model effectiveness and decision-making as a key future direction.
- **Why unresolved:** Current evaluations focus on single-agent performance, while financial analysis often requires distinct roles (e.g., data extraction, risk assessment, and verification) that might be better handled by specialized agents.
- **What evidence would resolve it:** A study demonstrating that a system of specialized financial agents outperforms a monolithic model (like GPT-4V) on complex, multi-step financial reasoning tasks within the benchmark.

### Open Question 3
- **Question:** How can multimodal integration be achieved without compromising the text-processing capabilities and financial knowledge acquired during the base LLM pre-training phase?
- **Basis in paper:** [inferred] Section C.1 reports that MLLMs perform worse than their corresponding LLMs in pure text settings, indicating that training on multimodal data leads to a loss of previously acquired financial knowledge.
- **Why unresolved:** Current MLLM architectures or training pipelines appear to suffer from "catastrophic forgetting" or interference when aligning visual features with existing linguistic knowledge.
- **What evidence would resolve it:** The development of a training methodology (e.g., specific regularization techniques or architecture adjustments) that allows an MLLM to match or exceed the text-only performance of its base LLM counterpart on financial text benchmarks.

## Limitations
- Benchmark's Chinese-language focus limits generalizability to other financial markets and multilingual applications
- Error analysis lacks detailed categorization of failure types and their relative frequencies
- Performance metrics may reflect benchmark difficulty rather than fundamental MLLM limitations
- No baseline comparisons with domain-specific financial models or human expert performance
- May not fully capture the sequential reasoning complexity of real-world financial analysis

## Confidence
- **High confidence** in establishing novel Chinese multimodal financial evaluation framework
- **Medium confidence** in claims about MLLM performance limitations (scores may reflect benchmark difficulty)
- **Low confidence** in error analysis conclusions without detailed failure mode classification

## Next Checks
1. Conduct comparative evaluations between MLLMs and specialized financial domain models to isolate whether performance gaps stem from general MLLM limitations or lack of financial domain training
2. Implement human expert benchmarking on identical tasks to establish baseline performance metrics and contextualize the MLLM accuracy scores
3. Expand error analysis methodology to include detailed error type classification and frequency distribution, particularly distinguishing between visual interpretation errors, financial knowledge gaps, and reasoning failures