---
ver: rpa2
title: 'Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning'
arxiv_id: '2508.05681'
source_url: https://arxiv.org/abs/2508.05681
tags:
- iter
- acquisition
- epoch
- samples
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first framework for clean-label backdoor
  attacks in active learning by exploiting acquisition functions as an attack surface.
  The proposed ALA method optimizes poisoned samples to exhibit high uncertainty scores,
  increasing their selection probability during active learning iterations.
---

# Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning

## Quick Facts
- **arXiv ID**: 2508.05681
- **Source URL**: https://arxiv.org/abs/2508.05681
- **Reference count**: 40
- **Primary result**: First clean-label backdoor attack framework for active learning exploiting acquisition functions as attack surface

## Executive Summary
This paper presents ALA (Acquisition-based Backdoor Attack), the first framework for clean-label backdoor attacks in active learning systems. The attack exploits acquisition functions by optimizing poisoned samples to exhibit high uncertainty scores, thereby increasing their selection probability during active learning iterations. Extensive experiments demonstrate that ALA achieves attack success rates up to 94% under low poisoning budgets (0.5%-1.0%) while maintaining clean data accuracy and human imperceptibility.

## Method Summary
ALA introduces a novel attack framework that optimizes poisoned samples to exhibit high uncertainty scores across different acquisition functions (Entropy, Margin, Least Confidence). The method works by crafting poisoned samples that appear normal to human annotators but are selected by the active learning system due to their high uncertainty scores. The attack is particularly effective with the SIG trigger method, which consistently outperforms the CL trigger across all tested settings. The framework operates by iteratively poisoning samples to maximize their selection probability while maintaining clean-label appearance.

## Key Results
- Achieved attack success rates up to 94% under low poisoning budgets (0.5%-1.0%)
- Preserved model utility on clean data while maintaining high attack effectiveness
- Demonstrated particular effectiveness with SIG trigger method over CL trigger
- Validated across three datasets (Fashion-MNIST, CIFAR-10, SVHN) and three acquisition functions

## Why This Works (Mechanism)
The attack exploits the fundamental vulnerability in active learning systems where acquisition functions select samples based on uncertainty scores. By crafting poisoned samples that maximize these uncertainty scores while appearing normal to humans, the attack increases the probability of these samples being selected for labeling and inclusion in the training set. The SIG trigger method proves particularly effective because it generates samples that simultaneously exhibit high uncertainty and maintain clean-label appearance.

## Foundational Learning
- **Active Learning Acquisition Functions**: Measure sample uncertainty to guide selection - needed to understand attack surface, quick check: verify Entropy, Margin, Least Confidence implementations
- **Clean-Label Backdoor Attacks**: Inject triggers without altering labels - needed for attack methodology, quick check: confirm trigger patterns preserve human readability
- **Poisoning Budget Optimization**: Balance attack effectiveness vs detection risk - needed for practical deployment, quick check: validate 0.5%-1.0% budget effectiveness
- **Uncertainty Score Optimization**: Craft samples to maximize selection probability - needed for attack mechanism, quick check: verify optimization converges on high-uncertainty samples
- **SIG vs CL Trigger Methods**: Different clean-label trigger generation approaches - needed for comparative analysis, quick check: confirm SIG consistently outperforms CL

## Architecture Onboarding

**Component Map**: Data Pool -> Acquisition Function -> Model Uncertainty -> Sample Selection -> Training Set -> Model Update -> Repeat

**Critical Path**: Poisoned Sample Creation → Uncertainty Score Maximization → Acquisition Function Selection → Model Training → Backdoor Activation

**Design Tradeoffs**: High uncertainty scores vs human imperceptibility, poisoning budget vs attack success rate, trigger type vs detection risk

**Failure Signatures**: Low attack success rates despite optimization, model performance degradation on clean data, human annotator detection of poisoned samples

**Three First Experiments**:
1. Test basic attack effectiveness with Entropy acquisition function on Fashion-MNIST
2. Compare SIG vs CL trigger performance under 1% poisoning budget
3. Validate clean data accuracy preservation during attack

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond tested datasets remains uncertain
- Assumes full access to model's uncertainty scores during poisoning
- Relies on qualitative human imperceptibility assessment without systematic studies
- Limited testing against ensemble-based or more sophisticated acquisition functions

## Confidence
- **High confidence**: Experimental methodology and reported results on tested datasets
- **Medium confidence**: Attack effectiveness across different active learning scenarios
- **Medium confidence**: Practical real-world applicability given assumptions about adversary capabilities

## Next Checks
1. Test attack effectiveness against ensemble-based acquisition functions that aggregate multiple uncertainty measures
2. Evaluate performance when the adversary has only partial knowledge of the model architecture or training data
3. Conduct systematic human studies with diverse annotator groups to quantify the perceptual detectability of poisoned samples across different trigger types