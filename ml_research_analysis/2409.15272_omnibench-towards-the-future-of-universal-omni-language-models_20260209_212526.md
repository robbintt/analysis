---
ver: rpa2
title: 'OmniBench: Towards The Future of Universal Omni-Language Models'
arxiv_id: '2409.15272'
source_url: https://arxiv.org/abs/2409.15272
tags:
- audio
- arxiv
- image
- data
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniBench, a benchmark for evaluating multimodal
  language models that process visual, acoustic, and textual inputs simultaneously.
  The benchmark includes 1,142 carefully curated, high-quality human-annotated questions
  across various reasoning tasks.
---

# OmniBench: Towards The Future of Universal Omni-Language Models

## Quick Facts
- arXiv ID: 2409.15272
- Source URL: https://arxiv.org/abs/2409.15272
- Reference count: 40
- Introduces OmniBench benchmark and OmniInstruct dataset for tri-modal language model evaluation

## Executive Summary
This paper introduces OmniBench, a benchmark for evaluating multimodal language models that process visual, acoustic, and textual inputs simultaneously. The benchmark includes 1,142 carefully curated, high-quality human-annotated questions across various reasoning tasks. The authors find that existing open-source models struggle with tri-modal contexts, with most baselines performing below 50% accuracy. Even proprietary models show accuracy drops when inputs are ablated. To address this gap, they introduce OmniInstruct, an 84.5K sample dataset for fine-tuning models on tri-modal understanding.

## Method Summary
The authors created OmniBench through a multi-stage process involving initial sample generation, multimodal dependency verification, iterative quality control, and balanced task categorization. They curated OmniInstruct by filtering and combining existing bimodal instruction datasets (MSRVTT-QA, AVQA, Music-AVQA2.0) with quality checks using LLaVA-1.6-34B ablation testing. For fine-tuning, they used LoRA on MiniCPM-o-2.6 with voice-filtered audio tokens, training on 8x H800 GPUs for 5 minutes per epoch.

## Key Results
- Existing open-source models struggle with tri-modal contexts, with most baselines performing below 50% accuracy
- Proprietary models show accuracy drops when inputs are ablated, indicating incomplete tri-modal integration
- Fine-tuning with OmniInstruct improves performance from 25.7% to 29.2% using voice-filtered approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on tri-modal instruction data improves omni-language model performance.
- Mechanism: The 84.5K OmniInstruct dataset provides explicit supervision signals for integrating image, audio, and text modalities, enabling models to learn cross-modal attention patterns that single-modality or bimodal training cannot provide.
- Core assumption: Tri-modal reasoning capability can emerge from combining filtered bimodal corpora (MSRVTT-QA, AVQA, Music-AVQA2.0) with quality filtering.
- Evidence anchors:
  - [abstract] "curate an instruction tuning dataset of 84.5K training samples, OmniInstruct, for training OLMs to adapt to tri-modal contexts"
  - [section 5.2] "fine-tuning with the vanilla setting improved performance to 25.7%, and the voice-filtered approach further enhanced accuracy to 29.2%"
  - [corpus] Weak direct evidence; related work (OpenOmni) discusses progressive multimodal alignment but with different methodology.

### Mechanism 2
- Claim: Enforcing strict multimodal dependency in benchmark design exposes models' inability to construct unified cross-modal representations.
- Mechanism: Each OmniBench question is validated to require information from BOTH image and audio; samples answerable from single modality are rejected through iterative human and automated inspection using LLaVA-1.6-34B ablation testing.
- Core assumption: Questions that pass multimodal-dependency checks genuinely test integration rather than superficial pattern matching.
- Evidence anchors:
  - [section 3.2] "Samples are only accepted if the model either rejected the task or made mistakes under these limited-information scenarios"
  - [section 3.2] "9.58% (121) QA pairs are defined as 'hard to recycle' and dumped"
  - [corpus] XModBench similarly evaluates cross-modal consistency but focuses on modality-invariant reasoning rather than dependency enforcement.

### Mechanism 3
- Claim: Textual approximation of modalities reveals that current models have asymmetric cross-modal grounding capabilities.
- Mechanism: Replacing audio with human transcripts improves VLM performance (average +6.42% for UnifiedIO2), while replacing images with VLM-generated captions shows mixed results, indicating stronger audio grounding deficits than visual grounding deficits.
- Core assumption: Text representations preserve sufficient semantic information to approximate their source modality for reasoning tasks.
- Evidence anchors:
  - [section 5.3] "UnifiedIO-2 models show performance gains, averagely at 6.42%, in the audio replacement setting"
  - [section 5.3] "VLMs show generally better results than ALMs even compared with open-source model with similar model size"
  - [corpus] No direct corpus evidence on textual approximation as evaluation proxy.

## Foundational Learning

- Concept: **Cross-modal attention and alignment**
  - Why needed here: Tri-modal integration requires learned attention weights between vision, audio, and text token sequences; without this, models process modalities independently.
  - Quick check question: Can you explain how a model would attend to audio features when processing a visual object reference?

- Concept: **Modality-specific encoders vs. unified tokenization**
  - Why needed here: The paper contrasts architectures using separate encoders (Qwen-Omni) with unified tokenizers (AnyGPT); understanding this distinction is critical for architecture selection.
  - Quick check question: What is the tradeoff between using Whisper/CLIP encoders versus discretizing all modalities into a shared vocabulary?

- Concept: **Instruction tuning for multimodal reasoning**
  - Why needed here: OmniInstruct's effectiveness demonstrates that instruction-following capability must be explicitly trained across all modality combinations.
  - Quick check question: Why might instruction tuning on image-text pairs fail to transfer to audio-inclusive instructions?

## Architecture Onboarding

- Component map: Image encoder (e.g., ViT) -> Audio encoder (e.g., Whisper, BEATs) -> Cross-modal attention layers -> LLM backbone -> Text generation with answer selection
- Critical path: Audio encoder → projection → cross-attention with image tokens → LLM reasoning → answer generation
- Design tradeoffs:
  - Separate encoders (better modality-specific features) vs. unified tokenizer (simpler architecture, potentially better integration)
  - Frozen LLM backbone (faster training) vs. full fine-tuning (better adaptation but higher compute)
  - Text approximation evaluation (enables dual-modal baselines) vs. native tri-modal (more realistic but limited model support)
- Failure signatures:
  - Random-guess-level accuracy (~25%) indicates modality fusion failure
  - Large performance drop on one audio type (e.g., music) suggests training data bias
  - Accuracy unchanged by ablation indicates modality is being ignored
- First 3 experiments:
  1. Evaluate existing VLM on OmniBench with audio transcript substitution to establish baseline gap between bimodal and tri-modal capability.
  2. Fine-tune a small OLM (e.g., MIO-Instruct-7B) on OmniInstruct subset (6.4K samples) with voice-filtered audio tokens to validate data quality before full training.
  3. Run ablation study (image-only, audio-only, text-only) on best-performing checkpoint to identify which modality integration is weakest.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of temporal visual dynamics (video) impact the omni-understanding and reasoning capabilities of OLMs compared to static image inputs?
- Basis in paper: [explicit] Appendix F states, "Extending the benchmark to support temporal reasoning with video and audio remains an important direction for future work."
- Why unresolved: OmniBench currently restricts visual input to static images to control for complexity and modality composition, leaving temporal reasoning between video and audio unexplored.
- What evidence would resolve it: Performance evaluations on an extended version of the benchmark incorporating video streams, specifically analyzing causal inference and temporal-spatial entity tasks.

### Open Question 2
- Question: Do universal tokenization architectures offer significant advantages over modality-specific encoders for tri-modal integration?
- Basis in paper: [inferred] Section 5.1 hypothesizes that designs using universal tokenizers (e.g., AnyGPT) "will be necessary to scale OLMs to more complex understanding and generation tasks."
- Why unresolved: While some existing baselines use specific encoders, the authors suggest unified tokenization may be required for future scalability, though this remains an unverified architectural prediction.
- What evidence would resolve it: Comparative analysis of models trained with universal tokenizers versus specific encoders on the OmniBench leaderboard, specifically on complex cross-modal reasoning tasks.

### Open Question 3
- Question: What training methodologies are required to close the performance gap between processing raw tri-modal inputs and processing textual approximations (captions/transcripts)?
- Basis in paper: [inferred] Section 5.3 notes that "the ability to construct a consistent context from text, image, and audio is often overlooked," evidenced by models performing better with textual approximations than raw data.
- Why unresolved: Current models struggle to synthesize information directly from raw audio and images, relying heavily on the stronger reasoning capabilities exhibited when modality information is pre-converted to text.
- What evidence would resolve it: The development of fine-tuning strategies (potentially using OmniInstruct) that result in raw tri-modal performance matching or exceeding textual approximation baselines.

## Limitations

- The 84.5K OmniInstruct dataset quality relies heavily on automated filtering (LLaVA-1.6-34B ablation) and InternVL-2-76B quality checks, but the specific prompts and filtering thresholds remain unspecified.
- The evaluation methodology uses textual approximations (audio transcripts, image captions) as proxies for native tri-modal inputs, which may not capture all modality-specific information needed for accurate reasoning.
- The MiniCPM-o-2.6 SFT experiments provide only preliminary results without ablation studies isolating the contribution of each training data source.

## Confidence

- **High Confidence**: The benchmark construction methodology and the observation that existing models struggle with tri-modal contexts are well-supported by systematic human annotation and evaluation procedures.
- **Medium Confidence**: The claim that instruction tuning on OmniInstruct improves performance has supporting evidence but lacks full hyperparameter specification and comprehensive ablation studies.
- **Low Confidence**: The textual approximation evaluation approach's validity as a proxy for native tri-modal reasoning remains questionable without validation that the approximations preserve task-relevant information.

## Next Checks

1. Reconstruct the exact prompts used by InternVL-2-76B for filtering OmniInstruct samples and rerun the filtering process to verify dataset quality consistency.
2. Conduct controlled experiments training separate models on each component dataset (MSRVTT-QA only, AVQA only, Music-AVQA2.0 only) to quantify their individual contributions to the full OmniInstruct performance gains.
3. Design experiments comparing model performance on tasks where the original audio/image can be preserved versus text-only approximations, measuring information loss in the approximation process across different task categories.