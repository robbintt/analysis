---
ver: rpa2
title: Human-AI Teaming Co-Learning in Military Operations
arxiv_id: '2510.01815'
source_url: https://arxiv.org/abs/2510.01815
tags:
- human
- military
- teaming
- agent
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research addresses the challenge of integrating AI into military
  operations while ensuring effective, ethical, and trustworthy human-AI teaming systems.
  The study develops a System Dynamics model incorporating four dimensions: adjustable
  autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making.'
---

# Human-AI Teaming Co-Learning in Military Operations

## Quick Facts
- arXiv ID: 2510.01815
- Source URL: https://arxiv.org/abs/2510.01815
- Reference count: 32
- One-line primary result: SD model achieves 44% positive proportionality under high uncertainty in military cyber-kinetic strike scenario

## Executive Summary
This research develops a System Dynamics model for human-AI teaming in military operations, focusing on co-learning dynamics during proportionality assessment in a cyber-kinetic strike scenario. The model integrates adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making to address the challenge of maintaining lawful operations while maximizing military advantage. Simulation results demonstrate the framework's ability to maintain positive proportionality scores for 44% of mission duration under high environmental uncertainty, with automatic autonomy retraction and workload management mechanisms.

## Method Summary
The study employs System Dynamics modeling with six stocks (human expertise, AI competence, shared situation awareness, trust calibration, AI authority level, and cognitive load) governed by six differential equations representing learning, awareness alignment, trust calibration, authority adjustment, and workload dynamics. The model is implemented iteratively in Python and tested through a 60-minute mission planning window for a cyber-kinetic strike against an adversary's Integrated Air-Defence Network. Five feedback loops capture co-learning, trust performance, adjustable autonomy, multi-layer control, and cognitive load safety. The proportionality score is computed as the balance between military advantage and collateral damage risk.

## Key Results
- The co-learning architecture maintained positive proportionality scores for 44% of the mission window under high environmental uncertainty (σ_env=0.35)
- Automatic autonomy retraction occurred when environmental uncertainty spiked, preventing invalid decisions
- Cognitive load remained below overload thresholds during the first 30 minutes of the mission
- The model successfully captured expected doctrinal behaviors including authority adjustments based on trust and awareness levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delegated authority dynamically adjusts based on trust, shared awareness, and environmental uncertainty
- Mechanism: Authority adjustment flow Ũ = k₁T + k₂S − k₃σ_env couples positive enablers with a risk brake triggered by environmental volatility
- Core assumption: Linear combination of trust, awareness, and uncertainty adequately approximates real authority transfer decisions
- Evidence anchors: Abstract mentions dynamic autonomy calibration; section 4 formalizes authority adjustment equation
- Break condition: Misestimated or delayed σ_env causes inappropriate autonomy retraction

### Mechanism 2
- Claim: Bidirectional feedback drives co-learning for both human and AI agents
- Mechanism: Human learning Ḣ = α₁F_HA − β₁C and AI learning Å = α₂F_AH − β₂σ_env model knowledge exchange with decay terms
- Core assumption: Feedback quality scales linearly with learning rate; decay terms capture interference accurately
- Evidence anchors: Abstract emphasizes bidirectional feedback loops; section 4 defines flows with feedback terms
- Break condition: Degraded feedback quality causes learning to stall or become negative

### Mechanism 3
- Claim: Cognitive load safety loop automatically suppresses autonomy and task rate under overload
- Mechanism: Loop B3 monitors cognitive load C; when C exceeds threshold, U decreases and TR is suppressed
- Core assumption: Single threshold adequately captures individual and situational variation in cognitive capacity
- Evidence anchors: Section 4 describes cognitive load safety loop; simulation shows peak load below overload thresholds
- Break condition: Incorrect threshold setting causes either unchecked overload or unnecessary autonomy suppression

## Foundational Learning

- Concept: **System Dynamics stocks and flows**
  - Why needed here: The entire model is built on SD formalism; understanding accumulation and transformation is prerequisite
  - Quick check question: Can you explain how "trust calibration" changes over time based on its inflow and outflow terms?

- Concept: **Adjustable autonomy in socio-technical systems**
  - Why needed here: Authority transfer is central to the model; engineers must grasp why static autonomy fails in dynamic military environments
  - Quick check question: What happens to delegated authority when environmental uncertainty spikes, and why?

- Concept: **Proportionality assessment in military targeting**
  - Why needed here: The use case evaluates legal balances between military advantage and collateral damage; domain literacy is required
  - Quick check question: What does a negative proportionality score imply for mission execution?

## Architecture Onboarding

- Component map: Human expertise (H) <-> AI competence (A) <-> Shared situation awareness (S) <-> Trust calibration (T) <-> AI authority level (U) <-> Cognitive load (C)

- Critical path: 1) Initialize stocks with mission-specific values, 2) Run feedback loops to update flows at each timestep, 3) Monitor proportionality score; trigger autonomy retraction or mission abort when score goes negative

- Design tradeoffs:
  - Higher k₁/k₂ (trust/awareness weights) vs. legal robustness under uncertainty
  - Aggressive delegation vs. cognitive load thresholds
  - Rich explanations (higher EQ) vs. trust calibration speed

- Failure signatures:
  - Proportionality score negative >50% of mission window
  - Trust collapse faster than autonomy retraction
  - Cognitive load sustained above threshold

- First 3 experiments:
  1. Vary σ_env from 0.1 to 0.5; observe proportionality score and autonomy retraction timing
  2. Adjust explanation quality (EQ) from 0.5 to 0.9; measure trust calibration and authority trajectories
  3. Lower cognitive load threshold; test whether autonomy suppression prevents overload but degrades mission effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed co-learning model be validated across diverse military contexts beyond the specific cyber-kinetic strike use case?
- Basis in paper: The conclusion states that "Future research should focus on... validating the model across diverse military contexts to enhance generalizability."
- Why unresolved: The current study demonstrates the model's utility using a single scenario involving a strike on an Integrated Air-Defence Network
- What evidence would resolve it: Successful simulation of the model in distinct scenarios (e.g., logistics, urban combat, or pure cyber defense) yielding consistent behavioral trends

### Open Question 2
- Question: Does coupling environmental uncertainty (σ_env) to real-time risk feeds improve the stability of the proportionality assessment?
- Basis in paper: The text suggests "future iterations should couple σ_env to a real-time risk feed" to test adaptive autonomy policies
- Why unresolved: In the current simulation, environmental uncertainty is an input parameter, and the model shows the proportionality score is fragile under high static uncertainty
- What evidence would resolve it: Simulation results where dynamic, real-time uncertainty feeds allow the system to maintain a positive proportionality score for a larger percentage of the mission window

### Open Question 3
- Question: Can embedding stochastic collateral-damage estimators extend the operational envelope where lawful decisions are possible?
- Basis in paper: The authors recommend "embed[ding] a stochastic collateral-damage estimator" to test if legal robustness can be sustained in complex environments
- Why unresolved: The current model relies on deterministic assessments, which resulted in a positive proportionality score for only 44% of the mission window under volatility
- What evidence would resolve it: New simulations showing that stochastic estimators prevent the precipitous withdrawal of authority and maintain legal compliance under high environmental volatility

## Limitations

- Specific formulations for feedback functions and proportionality scoring remain partially unspecified, limiting exact reproduction
- Cognitive load safety threshold is treated as static, though individual and situational variations could be significant in operational contexts
- Model validation relies primarily on simulation rather than empirical field testing with actual human-AI teams

## Confidence

- **Medium confidence**: The System Dynamics framework is well-established, but specific formulations remain partially unspecified and validation is primarily simulation-based

## Next Checks

1. Conduct sensitivity analysis on all coefficient values (α, β, γ, δ, θ) to determine which parameters most influence proportionality outcomes and autonomy retraction timing
2. Implement a formal verification of the System Dynamics equations to ensure conservation properties and realistic behavior across extreme parameter ranges
3. Design a human-subject experiment testing the cognitive load safety loop's effectiveness by comparing performance with static versus dynamic threshold adjustment