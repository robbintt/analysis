---
ver: rpa2
title: 'Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals''
  Subjective Text Perceptions'
arxiv_id: '2502.20897'
source_url: https://arxiv.org/abs/2502.20897
tags:
- attributes
- annotator
- degree
- annotators
- sociodemographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models (LLMs) can be
  trained to predict individuals' subjective text perceptions based on sociodemographic
  attributes. The authors curate DEMO, a dataset combining five tasks (intimacy, offensiveness,
  politeness, safety, sentiment) with standardized sociodemographic metadata.
---

# Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals' Subjective Text Perceptions

## Quick Facts
- arXiv ID: 2502.20897
- Source URL: https://arxiv.org/abs/2502.20897
- Reference count: 40
- Large language models fail to learn generalizable relationships between sociodemographics and subjective text ratings, instead using demographic profiles as unique identifiers for individual annotators.

## Executive Summary
This paper investigates whether large language models can predict individuals' subjective text perceptions using sociodemographic attributes. The authors create DEMO, a dataset combining five annotation tasks (intimacy, offensiveness, politeness, safety, sentiment) with standardized sociodemographic metadata from 2,614 annotators. They fine-tune Llama 3 8B using four input formats: content only, content with attributes, content with annotator ID, and content with both attributes and ID. Results show that while adding sociodemographics improves prediction over zero-shot prompting, annotator IDs yield significantly greater improvements. Critically, models fail to generalize to unseen annotators regardless of whether sociodemographics or IDs are provided, indicating they learn individual annotator behavior rather than demographic-behavior relationships. Further analysis reveals that sociodemographic attributes primarily serve as proxies for annotator identity when profiles are unique, rather than capturing meaningful demographic patterns.

## Method Summary
The study fine-tunes Llama 3 8B base with LoRA adapters (r=8, α=16) on five annotation tasks using the DEMO dataset containing 147,297 annotations from 2,614 annotators. Four input formats are tested: Content-Only, +Attributes, +ID, and +ID+Attributes, with minimal template formatting. Two split types are used: instance split (annotators may overlap between train and test) and annotator split (unseen annotators in test). The model predicts individual annotator ratings using a classification head on the final hidden state. Training uses bf16 precision, Adam optimizer, and learning rates selected via grid search. Macro-F1 is used as the evaluation metric to capture individual variation.

## Key Results
- Adding annotator IDs yields significantly greater performance improvements than sociodemographic attributes for predicting individual annotation behavior
- Models fail to generalize to unseen annotators regardless of whether sociodemographics or IDs are provided, indicating minimal learning of demographic-behavior relationships
- Sociodemographic attributes primarily serve as proxies for annotator identity when profiles are unique, rather than capturing generalizable demographic patterns

## Why This Works (Mechanism)

### Mechanism 1
Sociodemographic attributes improve prediction only when they serve as unique identifiers for annotators, not as generalizable demographic patterns. The model treats unique attribute combinations as de facto ID tokens rather than semantic descriptors. This is evidenced by larger gains for unique profiles versus frequent profiles.

### Mechanism 2
Annotator IDs enable learning of individual annotation patterns without requiring the model to understand underlying causal factors. The unique ID becomes associated with a learned embedding that captures specific rating tendencies, operating as a lookup table keyed by ID.

### Mechanism 3
Models fail to learn generalizable relationships between sociodemographics and annotation behavior because the annotator-split experiment removes the ID-as-proxy pathway. Without this pathway, attributes provide no signal because the model has not learned abstract demographic-to-behavior mappings.

## Foundational Learning

- **Human label variation vs. noise**
  - Why needed here: The experimental design assumes annotator disagreement reflects genuine individual perspectives rather than errors to be aggregated away.
  - Quick check question: When multiple annotators assign different labels to the same text, should you average their responses or model each annotator separately?

- **Instance split vs. annotator split**
  - Why needed here: The central finding depends on understanding why performance differs between these splits—one tests generalization to new texts from known annotators, the other tests generalization to entirely new people.
  - Quick check question: If you train on annotators A, B, C and test on annotator D, what can you conclude if sociodemographics don't help?

- **Proxy learning (ecological fallacy)**
  - Why needed here: The paper explicitly references the ecological fallacy—aggregate patterns don't necessarily apply to individuals. Models exploit unique attribute combinations as identifiers rather than learning demographic principles.
  - Quick check question: If 90% of people with attribute X prefer label Y, does that mean a specific person with attribute X will prefer Y?

## Architecture Onboarding

- **Component map**: Input formatting layer -> Base model (Llama 3 8B) -> LoRA adapters -> Prediction head
- **Critical path**: Format examples with annotator representation -> Forward pass through base model + LoRA -> Classification head predicts rating -> Cross-entropy loss against individual annotator's label -> Backprop updates LoRA weights and prediction head
- **Design tradeoffs**: ID-only vs. Attributes-only vs. Both (paper shows ID dominates); Instance split vs. annotator split (instance split inflates perceived demographic learning); Base vs. instruct model (base used since instruction-following isn't needed)
- **Failure signatures**: Near-identical performance across input formats on annotator split (no generalizable demographic learning); Large gains for unique profiles but not frequent profiles (attributes acting as IDs); High Wasserstein distance on high-entropy cases (model failing to capture disagreement patterns)
- **First 3 experiments**: 1) Replicate instance-split comparison on a single task to verify ID-dominance pattern; 2) Run annotator-split experiment to confirm attributes provide no generalization benefit; 3) Profile uniqueness analysis: partition test annotators by whether their attribute combination is unique or frequent, and compare gains from attributes across these groups

## Open Questions the Paper Calls Out

- **Cross-cultural generalizability**: The study only uses US annotators and cannot compare results across geocultural contexts due to lack of standardized sociodemographic attributes in cross-cultural datasets.

- **Dataset and architecture effects**: Prior studies show inconsistent results on whether sociodemographics outperform IDs, suggesting dataset characteristics and architectural choices may determine which approach works better.

- **Attribute granularity**: The study uses only four demographic attributes (age, gender, race, education), potentially excluding other important demographic attributes that might enable learning of generalizable demographic-behavior relationships.

## Limitations

- The four demographic attributes used may be too coarse to capture meaningful variation within demographic groups, causing models to treat unique combinations as de facto identifiers rather than learning semantic relationships.

- The DEMO dataset combines five different annotation tasks with different annotator pools and collection methods, potentially creating dataset-specific artifacts in the observed proxy-learning pattern.

- The use of Llama 3 8B may not have sufficient capacity to learn complex relationships between demographics and annotation behavior even if they exist, potentially explaining the negative results.

## Confidence

- **High confidence**: The finding that annotator IDs substantially improve performance compared to attributes, and that models fail to generalize to unseen annotators regardless of input format.
- **Medium confidence**: The interpretation that sociodemographic attributes primarily serve as proxies for annotator identity rather than capturing meaningful demographic-behavior relationships.
- **Low confidence**: The broader conclusion that LLMs cannot reliably model annotators based on sociodemographics alone, as this extends beyond the specific experimental conditions tested.

## Next Checks

1. **Alternative demographic representations** - Re-run experiments using continuous demographic variables or different categorizations to test whether the proxy-learning pattern persists with alternative attribute representations.

2. **Hierarchical modeling approach** - Implement a model architecture that explicitly models the relationship between demographics and annotator behavior through a hierarchical structure to test whether this enables learning of demographic-behavior relationships the current flat input format cannot capture.

3. **Cross-dataset generalization** - Evaluate whether models trained on DEMO demographics can predict annotation behavior in a different dataset with different annotators, tasks, and demographic distributions to test for generalizable demographic-behavior relationships.