---
ver: rpa2
title: 'Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks'
arxiv_id: '2506.22623'
source_url: https://arxiv.org/abs/2506.22623
tags:
- watermarking
- text
- temperature
- machine-generated
- paraphrasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the robustness of watermarking techniques
  for detecting machine-generated text against paraphrasing attacks. The authors propose
  a novel temperature-based watermarking approach where a unique temperature parameter
  is generated for each token using a hash of the previous context, influencing the
  model's sampling behavior.
---

# Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks

## Quick Facts
- arXiv ID: 2506.22623
- Source URL: https://arxiv.org/abs/2506.22623
- Reference count: 15
- Primary result: Novel temperature-based watermarking achieves 90% TPR at 2% FPR even after 30% paraphrasing, outperforming baseline.

## Executive Summary
This paper proposes a novel temperature-based watermarking technique to detect machine-generated text that is more robust against paraphrasing attacks than previous methods. The key innovation is using a context-dependent temperature parameter for each token, derived from a hash of previous tokens, to influence the model's sampling behavior during generation. This creates a statistical signature that can be detected even when portions of the text are paraphrased. Experiments on Vicuna-7B show the method maintains strong detection performance (90% TPR at 2% FPR) even when 30% of text is paraphrased using BERT-based masking, while baseline methods fail.

## Method Summary
The method introduces per-token temperature variation during generation by hashing the previous h tokens to create a unique temperature for each generation step. This temperature rescales the logits before softmax sampling, creating a detectable statistical signature in the generated text. During detection, the same temperature sequence is reconstructed from the text context and used to compute average token probabilities - watermarked text yields higher scores than non-watermarked text. The approach is tested against paraphrasing attacks where 30% of tokens are masked and replaced using BERT-base-uncased, demonstrating robustness where baseline methods fail.

## Key Results
- Temperature-based watermarking achieves 90% TPR at 2% FPR on clean text
- Maintains 90% TPR at 2% FPR even with 30% of text paraphrased using BERT masking
- Outperforms baseline Aaronson et al. approach which fails under paraphrasing attack
- Score distributions show clear separation between watermarked and human text for the proposed method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-dependent temperature variation creates a detectable statistical signature in generated text.
- Mechanism: A hash of the previous h tokens seeds a pseudo-random number generator that samples a unique temperature T_t for each generation step. This temperature rescales the logits before softmax (Eq. 2: P(s_t = k|...) = exp(l_k^(t)/T_t) / Σ_i exp(l_i^(t)/T_t)). Since temperature controls output entropy, the model is forced into a specific, reproducible generation path.
- Core assumption: The temperature sequence can be deterministically reconstructed during detection if the original token sequence is largely preserved.
- Break condition: If tokens are modified (e.g., paraphrasing), the hash chain breaks and subsequent temperatures cannot be correctly reconstructed, potentially degrading detection.

### Mechanism 2
- Claim: Watermarked text yields higher average token probabilities under the watermarked model than non-watermarked text.
- Mechanism: Detection computes Score = (1/N) Σ_t P(s_t|s_1,...,s_{t-1}; T_t) by running a forward pass with the same temperature sequence. Watermarked sequences were generated with these specific temperatures, so their tokens should have been sampled from higher-probability regions; human text lacks this alignment.
- Core assumption: The probability differential between watermarked and non-watermarked sequences is sufficiently large to enable discrimination.
- Break condition: If watermarked text is heavily edited, the probability scores may converge toward non-watermarked baselines.

### Mechanism 3
- Claim: Temperature variation provides robustness to paraphrasing by allowing the model to sample from broader synonym spaces at high-temperature steps.
- Mechanism: The authors hypothesize that varying temperature allows the model to occasionally sample synonyms or semantically similar tokens rather than only the most probable word. This redundancy may preserve the watermark signal even when some tokens are substituted.
- Core assumption: The watermark signal distributes across multiple tokens such that local substitutions do not catastrophically break the hash chain or score.
- Break condition: If paraphrasing exceeds ~30% of tokens or uses more aggressive rewriting models, robustness may degrade significantly.

## Foundational Learning

- Concept: **Temperature in softmax distributions**
  - Why needed here: The entire method hinges on understanding how temperature rescales logits—low temperature sharpens the distribution (more deterministic), high temperature flattens it (more random).
  - Quick check question: If T_t > T_0, does the output distribution become more or less peaked?

- Concept: **Hash-based deterministic seeding**
  - Why needed here: The watermark requires reconstructing the exact same temperature sequence during detection; this depends on the hash being a deterministic function of the context.
  - Quick check question: If token s_{t-1} is changed by paraphrasing, what happens to the hash and temperature for position t?

- Concept: **ROC curves and TPR/FPR tradeoffs**
  - Why needed here: The paper reports 90% TPR at 2% FPR; understanding this tradeoff is essential for evaluating whether the method is deployable in practical settings.
  - Quick check question: At a fixed FPR threshold, what does a higher TPR indicate about detector quality?

## Architecture Onboarding

- Component map: Hash generator -> Temperature sampler -> Logit rescaler -> Token sampler -> Detector
- Critical path:
  1. During generation: context -> hash -> temperature -> rescaled logits -> sample token
  2. During detection: text -> forward pass with reconstructed temperatures -> compute score -> threshold comparison
- Design tradeoffs:
  - Hash window size h: Larger h increases uniqueness but may slow detection; smaller h risks collisions
  - Temperature range [m, M]: Wider range creates stronger watermark signal but may degrade text quality
  - Detection threshold: Lower threshold catches more watermarked text but increases false positives
- Failure signatures:
  - Score distributions overlapping significantly between watermarked and human text (as seen in baseline)
  - Detection performance collapsing after paraphrasing attacks
  - Text quality degradation (not measured in this paper—requires separate evaluation)
- First 3 experiments:
  1. Reproduce baseline separation on clean text: Generate watermarked and human text samples, plot score distributions; confirm clear separation as in Figure 3 (top left).
  2. Ablate temperature range: Test detection performance with different (m, M) settings to find the minimal perturbation that maintains robustness.
  3. Paraphrasing intensity sweep: Instead of fixed 30%, test 10%, 20%, 40% token substitution to characterize robustness boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Aaronson baseline watermark fail to reproduce on the Vicuna-7B model despite theoretical claims of model agnosticism?
- Basis in paper: Section 4 states that "H1 is not confirmed. The reasons for this are the grounds for further investigation."
- Why unresolved: The empirical results showed overlapping score distributions for human and machine text on this specific model, contradicting previous findings.
- What evidence would resolve it: An analysis of the architectural or fine-tuning differences between LLaMA and Vicuna that cause the watermark signal to degrade.

### Open Question 2
- Question: How does the dynamic temperature watermarking impact the generation quality (e.g., perplexity) and semantic coherence of the text?
- Basis in paper: The paper focuses exclusively on detection metrics (TPR/FPR) and robustness, omitting evaluation of the text quality itself.
- Why unresolved: Forcing the model to sample with varying temperatures might lower fluency or increase grammatical errors, a common trade-off in watermarking not addressed here.
- What evidence would resolve it: Comparative perplexity scores and human evaluations of fluency between watermarked and unwatermarked outputs.

### Open Question 3
- Question: Is the proposed method robust against more sophisticated adversarial attacks, such as translation or advanced LLM-based paraphrasing?
- Basis in paper: The evaluation is limited to BERT-based masking (30% replacement), while the conclusion acknowledges that "thorough investigation... is still to be addressed."
- Why unresolved: Real-world evasion attempts often use more aggressive rewriting techniques than the synonym swapping tested in the paper.
- What evidence would resolve it: Testing detection performance against attacks using translation loops or paraphrasing by stronger models like GPT-4.

## Limitations
- Temperature parameters (T0, m, M) and context window size h are unspecified, making exact reproduction difficult
- Limited evaluation to only BERT-based paraphrasing with 30% token replacement
- No assessment of impact on generated text quality, fluency, or coherence
- Method's robustness against more sophisticated attacks (translation, advanced paraphrasing) remains untested

## Confidence

- **High confidence**: The core mechanism of temperature-based watermarking and its detection via average token probability is well-defined and reproducible in principle. The comparison to baseline Aaronson et al. and the demonstration of improved robustness against 30% paraphrasing attacks are supported by the provided results.
- **Medium confidence**: The claimed 90% TPR at 2% FPR under paraphrasing attack is credible based on the presented ROC curves, but the lack of specified parameters introduces uncertainty about exact replication.
- **Low confidence**: Claims about the method's robustness to paraphrasing rely on the assumption that temperature variation allows synonym sampling without breaking the watermark. This mechanism is described but not empirically validated beyond the specific attack tested.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary T0, m, M, and h to identify the minimum parameter settings that maintain the claimed TPR/FPR tradeoff under paraphrasing attack. Document the impact on detection performance and text quality.

2. **Attack diversity evaluation**: Test the watermark's robustness against alternative paraphrasing methods (e.g., back-translation, controlled text generation) and more aggressive token substitution rates (10%, 20%, 40%, 50%) to establish the method's failure boundaries.

3. **Cross-model transferability assessment**: Apply the watermark to a different LLM (e.g., Llama 2, MPT) and evaluate detection performance to determine if the method generalizes beyond Vicuna-7B or requires model-specific tuning.