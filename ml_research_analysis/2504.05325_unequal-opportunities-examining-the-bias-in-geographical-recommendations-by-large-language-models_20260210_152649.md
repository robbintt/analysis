---
ver: rpa2
title: 'Unequal Opportunities: Examining the Bias in Geographical Recommendations
  by Large Language Models'
arxiv_id: '2504.05325'
source_url: https://arxiv.org/abs/2504.05325
tags:
- llms
- responses
- cities
- arxiv
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how Large Language Models (LLMs) recommend
  U.S. cities for relocation, tourism, and business purposes.
---

# Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models

## Quick Facts
- arXiv ID: 2504.05325
- Source URL: https://arxiv.org/abs/2504.05325
- Reference count: 40
- Large Language Models show high internal consistency and demographic bias in recommending U.S. cities for relocation, tourism, and business

## Executive Summary
This study examines how Large Language Models (LLMs) recommend U.S. cities across three domains, revealing significant demographic biases in their outputs. The research demonstrates that LLMs consistently recommend the same set of cities with high internal similarity, while systematically underrepresenting historically underserved communities and overrepresenting affluent areas. The findings suggest that LLM training data and optimization processes perpetuate existing socioeconomic disparities through geographical recommendations.

## Method Summary
The researchers conducted a comprehensive audit of LLM geographical recommendations using 6 different models (Claude-3.5, Gemma, GPT-3.5, GPT-4o, Llama-3.1, Mistral) across 24 prompts (12 single-constraint and 12 generic). They collected 40 samples per prompt per model, generating JSON outputs with 5 cities and justifications each. The analysis employed multiple similarity metrics (Jaccard, TF-IDF, cosine, BLEU), inequality measures (concentration ratio, Theil index), and demographic skew calculations by comparing LLM outputs against U.S. Census data using t-tests.

## Key Results
- LLMs demonstrated high internal consistency with median Jaccard scores around 0.8 for city recommendations
- The same set of cities was frequently recommended across multiple samples and models
- Demographic analysis revealed consistent under-representation of historically underserved groups (racial minorities, disabled individuals, lower-income areas)
- Cities with higher education levels and financial resources were systematically over-represented in recommendations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs generate geographically concentrated outputs due to a tendency to favor high-probability token sequences, resulting in a narrow set of frequently recommended locations.
- **Mechanism:** The paper observes that the top 5 recommended locations often account for a vast majority of the probability mass (concentration ratio approaching 1.0). This suggests the underlying transformer mechanism prioritizes the most statistically dominant entities in the training data for a given prompt, effectively filtering out the "long tail" of less prominent cities.
- **Core assumption:** The mechanism assumes that the training data distribution and the model's optimization for high-likelihood completion are the primary drivers of the recommendation, rather than a logical reasoning process about the prompt's constraints.
- **Evidence anchors:**
  - [Section 5.1.1] "The concentration ratio measures the dominance of the most frequent entities... approaching 1 and, at a minimum, encompassing 0.8 of the distribution."
  - [Abstract] "Statistical training methods for LLMs have raised concerns about their representation of under-represented topics."
- **Break condition:** This mechanism breaks if the model employs a retrieval-augmented generation (RAG) strategy that explicitly samples from a uniform distribution of eligible cities or if the "temperature" parameter is set high enough to force exploration of the long tail.

### Mechanism 2
- **Claim:** Geographical recommendations serve as proxies for socio-economic status, systematically filtering out locations associated with lower income or minority populations.
- **Mechanism:** By comparing LLM outputs to U.S. Census data, the authors found that "ideal" locations are semantically associated with high education and income. The model associates the concept of a "recommended" city with attributes found in over-represented training data (wealthy/white), leading to the exclusion of cities that fail to match this latent profile.
- **Core assumption:** The mechanism assumes that "positive" sentiment or recommendation criteria in the training corpus are heavily biased toward affluent areas, creating a correlation between "good city" and "high income/low poverty."
- **Evidence anchors:**
  - [Section 5.2.3] "LLMs produce distributions that deviate from the database... It over-represented financially more affluent cities... and under-represented cities with higher unemployment rates."
  - [Abstract] "Our findings point to consistent demographic biases... which could perpetuate a 'rich-get-richer' effect."
- **Break condition:** This mechanism breaks if the training data is re-weighted to decouple location quality from socio-economic status, or if explicit negative constraints (e.g., "avoid cities with high income inequality") are applied.

### Mechanism 3
- **Claim:** Divergent model architectures converge on similar semantic justifications due to shared training corpora and alignment techniques.
- **Mechanism:** Despite different model sizes and providers (e.g., Mistral vs. GPT-4o), the paper notes high semantic similarity (Cosine Similarity) in the *reasons* provided for recommendations. This implies a convergence in how "desirable" features are defined across the model landscape, likely stemming from shared internet-scale datasets (e.g., Common Crawl).
- **Core assumption:** The mechanism assumes that the foundational "world model" for what constitutes a good city is relatively uniform across major LLM providers because they consume similar data sources.
- **Evidence anchors:**
  - [Section 6] "The similarities we observed... may stem from the underlying data... and the use of transformer-based architecture, which are likely to produce outputs with similar distributional properties."
  - [Corpus] Neighbor paper "Mitigating Bias in RAG" highlights that the corpus itself is a source of bias, supporting the idea that shared data leads to shared bias.
- **Break condition:** This mechanism breaks if models are trained on culturally distinct datasets (e.g., a model trained exclusively on rural newspapers) or specifically fine-tuned to maximize lexical diversity rather than semantic alignment.

## Foundational Learning

- **Concept: Concentration Ratio & Theil Index**
  - **Why needed here:** Standard accuracy metrics (like BLEU) don't capture the *diversity* or *inequality* of a set of recommendations. You need these economic metrics to quantify if the model is stuck in a "monoculture" of outputs.
  - **Quick check question:** If a model recommends City A 95 times and City B 5 times out of 100, how would you describe the Concentration Ratio for the top 1 entity?

- **Concept: Demographic Skewness**
  - **Why needed here:** To audit bias, you must link generated entities (cities) to demographic attributes (race, income). Skewness tells you if the model's recommendations are statistically "tilted" toward specific population profiles (e.g., positive skewness in "poverty" means the model is avoiding poor areas).
  - **Quick check question:** If the skewness for "percentage of disabled residents" is significantly negative for tourism recommendations, what does that imply about the model's inclusivity?

- **Concept: Intrinsic vs. Extrinsic Evaluation**
  - **Why needed here:** You must distinguish between the model's internal consistency (does it repeat itself?) and its external validity (does it represent the real world?). Relying only on intrinsic metrics might miss the fact that the model is consistently *wrong* about the real world.
  - **Quick check question:** Does a high Jaccard Index (internal similarity) guarantee that the recommendations are an accurate reflection of the US population distribution?

## Architecture Onboarding

- **Component map:**
  1. **Prompt Generator:** Constructs tuples of (<state>, <domain>, <constraint>) and generic variants.
  2. **Inference Layer:** Uses Langchain/OpenRouter to query 6 LLMs (e.g., Claude-3.5, GPT-4o) with temperature settings (0.6â€“0.8) to generate JSON lists of 5 cities + justifications.
  3. **Metrics Engine:**
      *   *Similarity:* Computes Jaccard (locations), TF-IDF/BLEU (lexical), and Cosine (semantic).
      *   *Inequality:* Computes Concentration Ratio and Theil Index.
  4.  **Demographic Auditor:** Joins recommended cities with U.S. Census/external databases to calculate skewness and run T-tests against the "ground truth" distribution.

- **Critical path:** The most fragile component is the **Demographic Auditor**. Mapping free-text city names to census IDs and correctly binning demographic attributes (e.g., defining "affordable" as the top quartile of below-median incomes) is the prerequisite for any bias claim.

- **Design tradeoffs:**
  - **Single-constraint vs. Generic:** Single-constraint prompts (e.g., "good for biking") lower the internal similarity (Jaccard) but may increase demographic bias by narrowing the eligible pool too aggressively.
  - **Metric Choice:** BLEU/TF-IDF capture surface form but miss semantic alignment. You must use Embedding-based Cosine Similarity to detect if the model is saying the "same thing" with different words.

- **Failure signatures:**
  - **Homogenization:** Median Jaccard scores > 0.8 across 40 samples indicate the model is collapsing the search space.
  - **Demographic Decoupling:** A T-test p-value < 0.05 (marked with ***) between the LLM distribution and the Census database for attributes like `race_native` or `disabled` indicates a statistically significant exclusion of protected groups.

- **First 3 experiments:**
  1. **Internal Repetition Test:** Run the prompt "Recommend 5 towns in Florida for relocation" 40 times on GPT-4o. Calculate the Jaccard Index. If > 0.8, you have confirmed the "monoculture" behavior.
  2. **Attribute Skew Check:** Take the top 10 most recommended cities from the previous run. Query a census database for `median_income` and `race_black`. Calculate skewness. Positive skewness on income implies a bias toward wealthy areas.
  3. **Constraint Sensitivity:** Compare the Jaccard Index of a generic prompt vs. a constrained prompt (e.g., "good for biking"). If similarity drops significantly, constraints are forcing the model to explore less optimal paths, but check if those paths still respect demographic fairness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the number of locations requested in the prompt (e.g., requesting 5 vs. 20 cities) influence the diversity and demographic distribution of the recommendations?
- Basis in paper: [explicit] The authors note that the limited set of locations observed is likely "tied to our prompt" and explicitly state they "leave further investigation of this for future work."
- Why unresolved: The current study fixed the request parameter to 5 locations per prompt, so the impact of this specific constraint on the concentration ratio remains unmeasured.
- What evidence would resolve it: Experiments varying the requested number of locations ($n_c$) and measuring the resulting distributional inequality (Theil index) and demographic skew.

### Open Question 2
- Question: Do similar geographic and socioeconomic biases in LLM recommendations exist in non-English languages and non-U.S. cultural contexts?
- Basis in paper: [explicit] The authors identify a "Limits of generalizability," noting the study is "centered on the U.S." and "conducted solely in English," which limits applicability to other cultures.
- Why unresolved: The audit used U.S. census data and English prompts; the behavior of LLMs regarding regional biases in other countries is currently unknown.
- What evidence would resolve it: Replicating the audit methodology (intrinsic and extrinsic evaluation) using census data from other nations and prompts in different languages.

### Open Question 3
- Question: Can conversational follow-up questions effectively mitigate the observed demographic skew by tailoring recommendations to specific user contexts?
- Basis in paper: [inferred] The discussion suggests "leveraging the conversational nature of LLMs" for personalization, implying a question about whether this interaction design can overcome the "limited context" bias found in single-turn responses.
- Why unresolved: The study evaluated single-turn responses and did not test if multi-turn interactions allow users to correct the under-representation of underserved groups.
- What evidence would resolve it: A comparative user study analyzing the demographic attributes of recommended cities in single-turn vs. multi-turn conversational scenarios.

## Limitations
- Analysis is constrained to U.S. cities and three specific domains (relocation, tourism, business), limiting external validity
- Relies on static demographic data from U.S. Census Bureau, which may not capture real-time economic shifts
- Use of default model temperatures (0.6-0.8) may underrepresent the full range of possible outputs

## Confidence
- **High Confidence**: Internal consistency metrics (Jaccard, TF-IDF, cosine similarity) and demographic skew calculations are methodologically sound with clear statistical significance
- **Medium Confidence**: The interpretation that observed biases stem from training data distribution rather than other factors like prompt engineering or model architecture choices
- **Low Confidence**: The causal mechanism linking specific demographic attributes to recommendation patterns without direct analysis of model weights or training data composition

## Next Checks
1. **Temporal Validation**: Repeat the analysis using 2024 census data to assess whether observed demographic biases persist with more recent population statistics
2. **Cross-Domain Testing**: Extend the prompt set to include additional domains (healthcare access, education quality, climate resilience) to determine if bias patterns are domain-specific or systemic
3. **Model Architecture Comparison**: Test whether smaller, specialized models trained on curated datasets show reduced demographic skew compared to general-purpose LLMs, isolating the role of training corpus size versus data quality