---
ver: rpa2
title: Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify
arxiv_id: '2601.02306'
source_url: https://arxiv.org/abs/2601.02306
tags:
- learning
- promotions
- podcast
- multi-task
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the cold-start problem in podcast advertising\
  \ and promotions on Spotify by unifying separate targeting models through multi-task\
  \ learning (MTL). The core idea is to jointly optimize ad and promotion objectives\u2014\
  such as streams, clicks, and follows\u2014using a shared representation of user,\
  \ content, context, and creative features."
---

# Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify

## Quick Facts
- arXiv ID: 2601.02306
- Source URL: https://arxiv.org/abs/2601.02306
- Reference count: 33
- Primary result: MTL improves ad AP by 50.2% and promo AP by 4.5% over promo-only baseline, with 22% eCPS reduction in online tests

## Executive Summary
This paper addresses cold-start targeting for podcast ads and promotions on Spotify by unifying separate models into a single multi-task learning (MTL) system. The core innovation is directional transfer: promotion data updates both promotion and ad task heads, while ad data only updates ad heads, combined with source-balanced sampling to equalize gradient contributions. Offline experiments show significant AP improvements for both tasks, and online A/B tests demonstrate up to 22% eCPS reduction and 24% higher stream rates. The approach reduces engineering overhead while improving performance for both organic promotions and paid ads.

## Method Summary
The authors propose a unified MTL architecture that jointly optimizes ad and promotion objectives through shared representations of user, content, context, and creative features. The system uses directional transfer where promotion data updates both promotion and ad heads, while ad data only updates ad heads, preventing ad-specific signals from contaminating promotion towers. Source-balanced sampling maintains equal gradient contributions from both data sources. The model includes separate task heads for ads (streams, clicks, follows) and promotions (streams, clicks, follows), with optional engagement prediction heads that further boost ad performance.

## Key Results
- Offline: Unified MTL improves Average Precision for promotions by 4.5% and for ads by 50.2% vs promotions-only baseline
- Online A/B: Up to 22% reduction in effective cost-per-stream (eCPS) and 18-24% higher stream rates
- Largest gains observed for less-streamed podcasts, demonstrating cold-start effectiveness
- Ancillary engagement heads provide additional lift for ad targeting performance

## Why This Works (Mechanism)
The directional transfer mechanism leverages the complementary nature of promotion and ad data while preventing negative transfer. Promotions tend to be organic and engagement-driven, providing cleaner signals for user-content affinity, while ads include paid placements with different engagement patterns. By allowing promotion data to inform both tasks but restricting ad data to only update ad heads, the model captures rich engagement patterns from promotions without being polluted by ad-specific biases. Source-balanced sampling ensures neither task dominates during training, maintaining stable gradient scales.

## Foundational Learning
- **Multi-Task Learning**: Training a single model on multiple related tasks to improve generalization through shared representations
  - Why needed: Separately trained models for ads and promotions led to inconsistent targeting and engineering overhead
  - Quick check: Compare MTL performance against single-task baselines on held-out data

- **Directional Transfer**: Asymmetric gradient flow where promotion updates propagate to both tasks but ad updates only affect ad heads
  - Why needed: Prevent ad-specific signals (like paid placements) from degrading promotion targeting quality
  - Quick check: Analyze gradient norms per task to verify asymmetric flow

- **Source-Balanced Sampling**: Maintaining equal representation from promotions and ads during training
  - Why needed: Prevent task imbalance from dominating gradient updates and destabilizing training
  - Quick check: Monitor task-specific loss curves during training for stability

## Architecture Onboarding
- **Component map**: User features -> Context features -> Content features -> Creative features -> Shared encoder -> Directional mask -> Task-specific heads (ads/promos) -> Engagement heads
- **Critical path**: Shared representation learning through directional transfer -> Task-specific predictions -> Balanced sampling
- **Design tradeoffs**: Unified model reduces engineering overhead but requires careful handling of task-specific signals; directional transfer adds complexity but prevents negative transfer
- **Failure signatures**: Overfitting to dominant task (imbalanced sampling), promotion signals polluting ad targeting (incorrect mask), or underfitting (insufficient task separation)
- **First experiments**: 1) Ablation study removing directional transfer to measure its contribution 2) Test different source-balanced ratios (30/70, 70/30) 3) Evaluate with only promotion data to establish baseline

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the unified MTL approach generalize to verticals beyond podcasts (e.g., music, audiobooks, video), and does cross-vertical transfer exhibit similar cold-start benefits?
- Basis in paper: The conclusion states: "While our study focuses on podcasts, the approach naturally extends to other verticals (e.g., music, audiobooks, video) where ads and organic promotions share user and content representations."
- Why unresolved: The paper validates the approach only on podcast data; user-content interaction patterns and ad formats may differ substantially across media types, potentially affecting transfer efficiency and negative transfer risks.
- What evidence would resolve it: Offline experiments and online A/B tests applying the same unified MTL architecture to music or audiobooks, reporting AP gains and eCPS reductions for cold-start items in those verticals.

### Open Question 2
- Question: What are the failure modes of directional transfer (promotion→ads but not ads→promotion), and does bidirectional gradient flow ever outperform the asymmetric masking strategy?
- Basis in paper: The directional transfer mechanism is presented as a design choice to prevent "ad-specific signals from directly shaping promotion towers," but no ablation compares it against bidirectional transfer or other masking configurations.
- Why unresolved: The intuition is that promotion data is cleaner or more aligned with organic engagement, but this assumption is not systematically tested; bidirectional transfer could theoretically improve ads further if negative transfer is manageable.
- What evidence would resolve it: Ablation experiments varying the mask matrix (e.g., full bidirectional, reverse directional, or learned task-specific masks) and measuring AP for both promotion and ad tasks, alongside gradient conflict analysis.

### Open Question 3
- Question: How does the optimal source-balanced sampling ratio (currently fixed at 50% promotions / 50% ads) change with data volume disparities or shifts in business priority between channels?
- Basis in paper: The paper notes that source-balanced sampling "keeps gradients from promotions and ads at comparable scales," but the 50/50 split is not tuned, and the sensitivity of model performance to this hyperparameter is unexplored.
- Why unresolved: Production systems often face non-stationary data volumes and shifting business objectives; a fixed ratio may become suboptimal as ad inventory grows or promotional campaigns scale.
- What evidence would resolve it: Grid search or adaptive scheduling over sampling ratios (e.g., 30/70, 70/30) in offline experiments, plus online tests measuring eCPS and stream rates under varying ratio regimes, potentially with automated adjustment based on validation metrics.

## Limitations
- Results are confined to Spotify's podcast ecosystem; generalizability to other platforms or media types is untested
- The "cold-start" framing focuses on new episodes rather than entirely new creators or listeners, limiting scope
- Potential biases from promotional content already known to the model are not addressed, nor is fairness impact across genres/demographic groups quantified

## Confidence
- **High**: Offline metric improvements (AP gains for both ads and promotions) and controlled online A/B results (eCPS reductions, stream rate increases) are clearly reported and internally consistent
- **Medium**: The directional transfer design is plausible and grounded in MTL literature, but the specific implementation details (e.g., weighting heuristics) are not fully justified or ablated
- **Low**: Generalizability of results to other platforms, podcast types, or broader cold-start scenarios (e.g., new creators) is speculative

## Next Checks
1. Conduct ablation studies isolating the impact of directional transfer vs. source-balanced sampling to quantify each component's contribution
2. Test the unified MTL approach on a held-out set of entirely new podcasts or creators to assess true cold-start performance beyond episode-level sparsity
3. Evaluate fairness and bias across podcast genres and listener demographics to ensure equitable targeting improvements