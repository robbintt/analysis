---
ver: rpa2
title: 'SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning
  for Reasoning'
arxiv_id: '2506.19767'
source_url: https://arxiv.org/abs/2506.19767
tags:
- reasoning
- minutes
- frac
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the integration of Supervised Fine-Tuning
  (SFT) and Reinforcement Learning (RL) for enhancing large language model (LLM) reasoning
  capabilities. Through a comprehensive analysis of token distributions, learning
  dynamics, and entropy-based integration mechanisms, the authors reveal that SFT
  induces coarse-grained global changes to LLM policy distributions while RL performs
  fine-grained selective optimizations.
---

# SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning

## Quick Facts
- arXiv ID: 2506.19767
- Source URL: https://arxiv.org/abs/2506.19767
- Reference count: 26
- Primary result: SRFT achieves 59.1% average accuracy on five mathematical reasoning benchmarks, outperforming zero-RL baselines by 9.0% and sequential SFT→RL approaches by 3.4%

## Executive Summary
This paper presents SRFT, a single-stage method that unifies Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for enhancing large language model reasoning capabilities. The method addresses the challenge of integrating demonstration-based learning with exploration-based optimization by using entropy as a real-time signal to dynamically balance SFT and RL objectives. Through comprehensive analysis of token distributions and learning dynamics, the authors demonstrate that SRFT achieves superior performance on mathematical reasoning tasks while maintaining stable training dynamics and better generalization to out-of-distribution problems.

## Method Summary
SRFT combines SFT and RL in a single training stage using entropy-aware weighting mechanisms. The method uses demonstrations to provide global policy guidance while self-exploration rollouts enable fine-grained optimization. Entropy serves as a dynamic signal: when policy entropy is high (indicating uncertainty), SFT influence decreases to prevent harmful distribution shifts, while RL influence increases to maintain exploration. The training integrates four loss components: SFT on demonstrations, off-policy RL on demonstrations with importance sampling, on-policy RL on positive rollouts, and standard on-policy RL on negative rollouts. Training uses binary rewards (1 for correct, 0 otherwise) and runs for 500 steps with 8 rollouts per prompt.

## Key Results
- SRFT achieves 59.1% average accuracy across five mathematical reasoning benchmarks
- Outperforms zero-RL baselines by 9.0% and sequential SFT→RL approaches by 3.4%
- Demonstrates superior generalization with 10.9% improvement on out-of-distribution tasks
- Maintains stable training dynamics with controlled entropy levels throughout training

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Aware Dynamic Weighting
SRFT uses entropy as a real-time signal to dynamically balance SFT and RL objectives during training. Two complementary entropy-aware weights operate in opposition: wSFT = 0.5 × exp(-H(πθ)) reduces SFT influence when entropy is high to prevent harmful distribution shifts from mismatched behavior policies, while wRL = 0.1 × exp(H(πθ)) increases RL weight when entropy is high to maintain exploration diversity and prevent premature convergence.

### Mechanism 2: Simultaneous Coarse-Fine Optimization
Single-stage parallel application of SFT and RL captures complementary optimization granularities that sequential approaches lose through catastrophic forgetting. SFT provides coarse-grained global policy adjustments (modifying token probabilities across entire sequences), while RL performs fine-grained selective optimization (adjusting only high-uncertainty tokens). Single-stage training preserves both simultaneously.

### Mechanism 3: Heterogeneous Advantage Amplification
Mixing expert demonstrations into on-policy rollout groups creates optimistic advantage estimates that guide exploration toward high-quality regions. Expert demonstrations typically receive higher rewards than average policy rollouts. When included in advantage computation, they raise the baseline mean, making policy's successful rollouts appear more advantageous by comparison.

## Foundational Learning

### Concept: Policy Gradient Methods with Group-Relative Advantages (GRPO)
**Why needed here**: SRFT builds on GRPO's advantage estimation; understanding how relative advantages normalize rewards across groups is essential for grasping why demonstration inclusion changes gradient magnitudes.
**Quick check question**: For a group of 8 responses with rewards [0.2, 0.4, 0.6, 0.8, 1.0, 0.0, 0.1, 0.3], what is the advantage of the response with reward 0.8 after GRPO normalization?

### Concept: Entropy as Uncertainty and Exploration Measure
**Why needed here**: SRFT's core innovation is entropy-aware weighting; understanding what high/low entropy indicates about model confidence and exploration capacity is prerequisite.
**Quick check question**: A language model assigns probability 0.8 to one token and 0.05 to each of four other tokens vs. 0.25 to each of five tokens—which distribution has higher entropy and what does this imply about the model's state?

### Concept: Off-Policy Learning and Importance Sampling
**Why needed here**: SRFT uses off-policy demonstrations with on-policy rollouts; understanding why importance sampling (πθ/πβ ratio) is needed to correct distribution mismatch is critical.
**Quick check question**: Why can't we directly use gradients from expert demonstrations without importance sampling correction, and what happens when πθ diverges significantly from πβ?

## Architecture Onboarding

### Component map
Prompt Batch → [Rollout Generator (8 samples/prompt)] → On-policy Buffer
              ↘                                          ↘
               → [Demonstration Sampler] → Demo Buffer    → [Mixed Batch Assembler]
                                                         ↘
Mixed Batch → [Reward Model (Binary Verifier)] → Rewarded Batch → [Entropy Monitor]
                                                                          ↓
                                                          [Entropy-Aware Weight Calculator]
                                                                          ↓
                                                          [Advantage Estimator (GRPO)]
                                                                          ↓
                                                          [Loss Combiner (wSFT×SFT + wRL×RL)]
                                                                          ↓
                                                          [Policy Updater (Adam + Gradient Clipping)]

### Critical path
1. **Batch preparation**: Sample prompts, generate 8 rollouts per prompt, retrieve demonstrations, verify all have valid rewards
2. **Entropy computation**: Calculate policy entropy over current batch distribution to determine wSFT and wRL
3. **Advantage estimation**: Group all samples (rollouts + demos), compute mean/std rewards, normalize advantages
4. **Loss computation**: Apply entropy-weighted SFT loss on demos + entropy-weighted positive RL loss + standard negative RL loss
5. **Policy update**: Single backward pass with combined loss, standard clipping and regularization

### Design tradeoffs
- **Demonstration ratio**: More demos improve guidance but risk overfitting to expert distribution; paper uses ~46k demos with 8 rollouts per prompt
- **Entropy weight scale**: wSFT=0.5 and wRL=0.1 coefficients chosen empirically; higher values increase entropy influence but may destabilize training
- **Rollout group size**: 8 rollouts per prompt balances advantage estimate quality vs. compute cost; smaller groups increase variance
- **Learning rate separation**: Same LR for all components; could decouple SFT vs. RL rates for finer control

### Failure signatures
- **Entropy collapse to near-zero**: Indicates overfitting/model collapse → increase wRL, reduce SFT demo ratio, or add entropy bonus
- **Training rewards plateau early with high variance**: SFT/RL gradient conflict → reduce wSFT, verify demo quality, check reward signal
- **Sudden performance drop mid-training**: Distribution shift from demos → importance sampling may be failing; verify πβ estimation
- **OOM during rollout generation**: Response lengths exploding → add length penalty or cap max generation length

### First 3 experiments
1. **Reproduce entropy ablation**: Train SRFT with fixed weights (wSFT=wRL=1.0) vs. entropy-aware weights on a small math benchmark subset (1k prompts); expect ~3-4 point gap confirming weighting mechanism
2. **Demonstration quality sensitivity**: Train SRFT with demonstrations from 3 different sources (DeepSeek-R1, GPT-4, weaker model) to establish quality requirements; expect performance scaling with demo quality
3. **Sequential baseline replication**: Implement SFT→RL and pure RL baselines on identical data to verify the reported 3.4% and 9.0% gaps; critical for validating single-stage advantage

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can more sophisticated entropy-based control mechanisms, such as adaptive entropy scheduling or multi-timescale entropy analysis, improve the integration of SFT and RL signals compared to the basic exponential weighting functions used in SRFT?
**Basis in paper**: [explicit] The Limitations section states "our current utilization of entropy dynamics remains relatively simple with basic exponential weighting functions" and suggests "future work could explore adaptive entropy scheduling or multi-timescale entropy analysis."
**Why unresolved**: The paper demonstrates that entropy is a critical indicator but only implements basic entropy-aware weighting (w_SFT = 0.5 × exp(-H(π_θ)) and w_RL = 0.1 × exp(H(π_θ))) without exploring more nuanced temporal entropy patterns.
**What evidence would resolve it**: Comparative experiments implementing adaptive entropy schedules or multi-timescale analysis, measuring both final performance and training stability across mathematical reasoning benchmarks.

### Open Question 2
**Question**: Can SRFT maintain effectiveness when trained with demonstrations of varying quality levels, or does performance degrade substantially as demonstration quality decreases?
**Basis in paper**: [explicit] The Limitations section explicitly states "our approach assumes access to high-quality demonstrations, and future research could investigate the potential for training with imperfect demonstrations to enhance the method's applicability."
**Why unresolved**: All experiments use high-quality reasoning traces from DeepSeek-R1; no ablation tests demonstration quality, and the theoretical analysis assumes demonstrations approximate optimal policies.
**What evidence would resolve it**: Experiments with systematically degraded demonstrations (introducing errors, using weaker source models, or mixing correct/incorrect responses) while measuring accuracy retention and entropy dynamics.

### Open Question 3
**Question**: Does SRFT's single-stage integration approach scale effectively to larger language models (70B+ parameters) where the balance between coarse-grained SFT adjustments and fine-grained RL modifications may shift differently during training?
**Basis in paper**: [inferred] All experiments use Qwen2.5-Math-7B exclusively; the paper shows SFT induces larger distribution shifts than RL, but this relationship may change with model scale where RL optimization dynamics differ substantially.
**Why unresolved**: The method's effectiveness is only validated on a 7B parameter model, leaving unclear whether the entropy-aware weighting mechanisms and learning dynamics observations transfer to models with fundamentally different capacity and optimization landscapes.
**What evidence would resolve it**: Scaling experiments applying SRFT to 13B, 32B, and 70B models on identical benchmarks, analyzing whether entropy-weighting coefficients require adjustment and whether the single-stage advantage over sequential SFT→RL persists at scale.

## Limitations
- All experiments focus exclusively on mathematical reasoning tasks, leaving generalization to other reasoning domains untested
- The method assumes access to high-quality demonstrations, with no investigation of effectiveness with imperfect or lower-quality demonstrations
- Implementation details for entropy computation and specific batching mechanisms are not fully specified

## Confidence
- **High Confidence**: The fundamental claim that SRFT improves reasoning performance through entropy-aware integration of SFT and RL is well-supported by the experimental results, with statistically significant improvements over multiple baselines.
- **Medium Confidence**: The causal mechanism explanations for why SRFT works (entropy-aware weighting, simultaneous coarse-fine optimization, heterogeneous advantage amplification) are theoretically sound but rely on assumptions that require further empirical validation.
- **Low Confidence**: The exact entropy calculation methodology and implementation details remain unspecified, making precise reproduction challenging.

## Next Checks
1. **Entropy Weighting Ablation with Controlled Conditions**: Implement SRFT with fixed weights (wSFT=wRL=1.0) versus entropy-aware weights on a small, controlled math benchmark subset (1k prompts). Compare performance, entropy trajectories, and training stability to isolate the contribution of the weighting mechanism.

2. **Demonstration Quality Scaling Experiment**: Train SRFT with demonstrations from three different sources: DeepSeek-R1, GPT-4, and a weaker model (e.g., Qwen2.5-7B itself). Measure performance scaling to establish the relationship between demonstration quality and SRFT effectiveness.

3. **Sequential Baseline Replication with Identical Data**: Implement SFT→RL and pure RL baselines using the exact same training data, prompts, and evaluation protocols as SRFT. This will verify the reported 3.4% and 9.0% performance gaps and isolate whether single-stage integration provides the benefit.