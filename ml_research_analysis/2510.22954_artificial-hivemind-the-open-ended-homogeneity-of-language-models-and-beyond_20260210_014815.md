---
ver: rpa2
title: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)'
arxiv_id: '2510.22954'
source_url: https://arxiv.org/abs/2510.22954
tags:
- qwen
- b-instruct
- human
- responses
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces INFINITY-CHAT, a large-scale dataset of 26K
  real-world open-ended user queries to language models. It develops the first comprehensive
  taxonomy of such queries, comprising 6 top-level categories and 17 subcategories.
---

# Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)

## Quick Facts
- **arXiv ID:** 2510.22954
- **Source URL:** https://arxiv.org/abs/2510.22954
- **Reference count:** 40
- **Key outcome:** The study introduces INFINITY-CHAT, a large-scale dataset of 26K real-world open-ended user queries to language models, and systematically analyzes mode collapse in LMs, revealing an "Artificial Hivemind" effect characterized by intra-model repetition and inter-model homogeneity.

## Executive Summary
This paper presents INFINITY-CHAT, a comprehensive dataset of 26K open-ended user queries to language models, along with the first systematic taxonomy of such queries (6 categories, 17 subcategories). Using this benchmark, the research demonstrates a pervasive "Artificial Hivemind" phenomenon: language models exhibit both intra-model repetition (identical or near-identical responses to the same query) and inter-model homogeneity (different models converging on similar semantic outputs). The study also reveals that reward models and LLM judges are less calibrated to human ratings on open-ended tasks with divergent individual preferences, highlighting fundamental challenges in current evaluation and alignment approaches.

## Method Summary
The authors curated INFINITY-CHAT by filtering 1M+ user query logs from WildChat, classifying queries using GPT-4o into open-ended (26K) and closed-ended (8K) categories. They then collected 31,250 human annotations across 31 models, using both absolute ratings and pairwise preferences to capture diverse human preferences. The dataset includes a comprehensive taxonomy and enables systematic analysis of mode collapse through semantic embedding similarity (text-embedding-3-small) and human evaluation. The evaluation framework measures both semantic (Cosine Similarity) and lexical overlap to quantify the "Artificial Hivemind" effect.

## Key Results
- **Intra-model repetition:** 79% of model responses exceed 0.8 Cosine Similarity when using standard decoding (Top-p=0.9, T=1.0)
- **Inter-model homogeneity:** Different models independently converge on similar ideas with minor variations, often sharing verbatim phrases
- **Human preference diversity:** High Shannon entropy in human annotations proves genuine disagreement on open-ended queries
- **Judge miscalibration:** Reward models and LLM judges show significantly lower correlation with human ratings on high-disagreement subsets

## Why This Works (Mechanism)

### Mechanism 1: Convergent Training Dynamics (The "Hivemind" Source)
- **Claim:** Inter-model homogeneity results from overlapping training data pipelines and convergent alignment objectives
- **Mechanism:** Models trained on similar internet slices and aligned to optimize for comparable safety/helpfulness criteria converge to map open-ended queries to the same high-probability "canonical" responses
- **Core assumption:** Different model families share significant proportions of pretraining corpora or rely on synthetic data from similar teacher models
- **Evidence anchors:** Page 5 notes possible explanations include shared data pipelines or contamination from synthetic data; Page 2 defines inter-model homogeneity as different models converging on similar ideas
- **Break condition:** Fails if strictly disjoint training corpora still produce verbatim overlaps, suggesting intrinsic linguistic attractors

### Mechanism 2: Density Traps in Stochastic Decoding
- **Claim:** Standard decoding strategies are insufficient because the model's underlying probability density is too sharply peaked around specific concepts
- **Mechanism:** Even with high temperature (T=1.0), probability mass for "high quality" answers is so concentrated that random sampling repeatedly draws from a tiny cluster of semantic variations
- **Core assumption:** Mode collapse exists in the model's learned representation/logits, not just the sampling algorithm
- **Evidence anchors:** Page 4 shows 79% of cases exceed 0.8 similarity even with Top-p=0.9, T=1.0; Min-p sampling reduces but doesn't eliminate the problem
- **Break condition:** Fails if a decoding strategy successfully forces exploration of distribution tails without incoherence

### Mechanism 3: Loss of Subjectivity in Reward Optimization
- **Claim:** Reward Models are miscalibrated on open-ended tasks because they optimize for "consensus" ground truth, discarding valid idiosyncratic preferences
- **Mechanism:** RLHF training on aggregate human preference data implicitly penalizes the long tail of valid but subjective responses
- **Core assumption:** Current alignment datasets average out individual disagreement rather than preserving it as valid signal
- **Evidence anchors:** Page 9 shows correlations between human ratings and LMs drop significantly on similar-quality subsets; High Shannon entropy proves humans genuinely disagree
- **Break condition:** Fails if an RM trained on dense preference data successfully improves calibration on disagreed subsets

## Foundational Learning

- **Concept: Semantic Embeddings (Cosine Similarity)**
  - **Why needed here:** The entire "Hivemind" argument relies on quantifying how "similar" text outputs are
  - **Quick check question:** If Model A outputs "Time is a river" and Model B outputs "Time flows like a stream," would their Cosine Similarity be closer to 0.9 or 0.1? (Answer: High (~0.9), because semantic meaning is nearly identical)

- **Concept: Mode Collapse / Density Estimation**
  - **Why needed here:** To understand why increasing temperature doesn't help - the model assigns near-zero probability to the vast space of truly creative answers
  - **Quick check question:** Why does increasing Temperature to 2.0 often lead to gibberish rather than diverse, creative answers in current LLMs?

- **Concept: Pluralistic Alignment**
  - **Why needed here:** This is the solution space the paper alludes to - moving beyond "Does the model follow instructions?" to "Does the model cater to different valid user preferences?"
  - **Quick check question:** If 25 humans rate 5 responses on an open-ended query with uniformly distributed ratings, how should a "Pluralistic" Reward Model score them differently than a standard "Helpful/Harmless" RM?

## Architecture Onboarding

- **Component map:** WildChat (1M+ logs) -> Filter (English, non-toxic, 15-200 chars) -> GPT-4o classifier (Open-ended vs Closed-ended) -> INFINITY-Chat dataset -> 25 Models -> 50 Generations/Query -> text-embedding-3-small -> Pairwise Similarity Matrix -> Heatmaps -> 25 Annotators/Item -> Absolute Ratings & Pairwise Preferences -> Entropy & Agreement Calculation

- **Critical path:** The Human Annotation Loop is the most fragile component - with 31,750 annotations serving as "ground truth," low annotator quality would corrupt the idiosyncratic preference signal

- **Design tradeoffs:** The taxonomy was constructed using GPT-4o for automatic classification rather than manual annotation, scaling the dataset but introducing classification accuracy bounds (74.7% human agreement noted in Appendix)

- **Failure signatures:** Verbatim overlaps when prompted for mottos (identical sentences across models) and semantic clustering in PCA plots showing models collapsing into just 2 clusters for the "Time" metaphor

- **First 3 experiments:**
  1. Reproduce Figure 4 (Intra-model): Take 1 model, run Top-p=0.9, T=1.0 on 10 prompts from INFINITY-Chat, compute pairwise similarity, verify >80% exceed 0.8 similarity
  2. Semantic Cluster Analysis: Prompt 5 diverse models to "Write a metaphor about time," embed outputs, run PCA, count distinct clusters (expected: <3 clusters)
  3. Judge Calibration Test: Use subset of 31K human annotations, compare GPT-4o's rating against "High Disagreement" human subset, verify correlation drop shown in Appendix Table 20

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do post-training pipelines (e.g., RLHF, SFT) versus pre-training data cause the "Artificial Hivemind" effect?
- **Basis in paper:** Appendix A.2 states: "We aim to quantify the relative contributions of different post-training pipelines... to better disentangle the respective roles of pre-training and post-training..."
- **Why unresolved:** The current study analyzed only instruction-tuned models, making it impossible to isolate whether homogeneity stems from pre-training corpus or alignment procedures
- **What evidence would resolve it:** Controlled ablation study measuring inter-model similarity using both base models and their instruction-tuned variants

### Open Question 2
- **Question:** What are the primary causal mechanisms (e.g., data contamination, shared pipelines) driving verbatim inter-model homogeneity?
- **Basis in paper:** Section 3 notes: "We highlight the need for future work to rigorously investigate the sources of such cross-model repetition... [such as] shared data pipelines... or contamination from synthetic data"
- **Why unresolved:** While the paper documents significant verbatim overlaps, exact causes remain unclear due to proprietary training data
- **What evidence would resolve it:** Controlled experiments training open-source models on distinct data mixtures to identify conditions reproducing verbatim phrase overlaps

### Open Question 3
- **Question:** How can we develop automated evaluation metrics that reliably distinguish valid alternative responses (pluralistic preferences) from lower-quality outputs?
- **Basis in paper:** Section 4.3 shows that "reward models... assign diverging scores

## Limitations
- The taxonomy's robustness across cultural contexts remains untested despite strong 74.7% human agreement
- The density trap mechanism is plausible but the paper doesn't conclusively rule out that decoding strategies simply haven't been optimized enough
- Data contamination vs. intrinsic attractor convergence remains a "chicken or egg" problem

## Confidence
- **High Confidence:** Existence of verbatim phrase overlaps across different model families, semantic clustering in PCA analysis, and systematic documentation of intra-model repetition
- **Medium Confidence:** The density trap interpretation and reward model miscalibration on subjective tasks - these mechanisms are supported but not definitively proven
- **Low Confidence:** The claim about overlapping training data pipelines being the primary cause, as the paper acknowledges this is speculation

## Next Checks
1. Test whether a decoding strategy (e.g., MCTS or diverse beam search) can successfully extract diverse responses from the same models without hallucination
2. Train a Pluralistic Reward Model specifically on the 25-annotator preference distribution from INFINITY-Chat and test if it better handles high-disagreement subsets
3. Systematically analyze the pretraining corpora overlap between models showing verbatim overlaps to determine if data contamination is the root cause