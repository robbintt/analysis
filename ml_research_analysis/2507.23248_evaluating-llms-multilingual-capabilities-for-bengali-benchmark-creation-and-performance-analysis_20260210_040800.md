---
ver: rpa2
title: 'Evaluating LLMs'' Multilingual Capabilities for Bengali: Benchmark Creation
  and Performance Analysis'
arxiv_id: '2507.23248'
source_url: https://arxiv.org/abs/2507.23248
tags:
- bengali
- datasets
- english
- across
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of standardized evaluation benchmarks
  for Bengali NLP by translating and curating eight English benchmark datasets into
  Bengali. The authors evaluate ten recent open-source multilingual LLMs on these
  translated datasets using metrics such as accuracy, Response Error Rate (RER), Response
  Adherence Rate (RAR), and LLM-Judge.
---

# Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis

## Quick Facts
- **arXiv ID:** 2507.23248
- **Source URL:** https://arxiv.org/abs/2507.23248
- **Reference count:** 5
- **Key outcome:** Translation and curation of eight English benchmark datasets into Bengali reveals significant performance gaps for multilingual LLMs, with tokenization inefficiency and limited pretraining exposure identified as primary bottlenecks.

## Executive Summary
This paper addresses the critical gap in standardized evaluation benchmarks for Bengali natural language processing by creating and releasing eight translated benchmark datasets covering reasoning, commonsense, and knowledge tasks. The authors evaluate ten recent open-source multilingual LLMs on these datasets, revealing consistent performance disparities between English and Bengali, particularly for smaller models and those with limited explicit Bengali pretraining. The study identifies tokenization efficiency as a key factor, showing that Bengali script's complex grapheme clusters lead to excessive token fragmentation that correlates with degraded model accuracy. These findings highlight the need for improved datasets, evaluation methodologies, and tokenizer optimization for underrepresented languages.

## Method Summary
The authors translated eight English benchmark datasets (MMLU, GSM8K, Hellaswag, Winogrande, CommonsenseQA, BoolQ, OpenbookQA, ARC) into Bengali using GPT-4o-mini, with post-processing to handle decoding errors and formatting inconsistencies. They evaluated ten open-source multilingual LLMs (including LLaMA, Qwen, Mistral, and DeepSeek variants) on both English and Bengali versions using accuracy, Response Error Rate (RER), Response Adherence Rate (RAR), and LLM-Judge metrics. The study also analyzed tokenization efficiency through metrics like Average Tokens Per Row (ATPR) and Average Tokens Per Word (ATPW), comparing Bengali and English tokenization patterns to establish their relationship with model performance.

## Key Results
- Performance gaps between English and Bengali are most severe for smaller models (3B/7B) on reasoning tasks, with accuracy drops of 20-30 percentage points.
- DeepSeek models demonstrated greater stability in cross-language performance compared to other families, likely due to explicit Bengali pretraining.
- Bengali inputs generate 2-3x more tokens per instance than English, with excessive tokenization correlating inversely with accuracy.
- Mistral models exhibited high RER (formatting errors) on Bengali MCQs, often outputting reasoning text instead of requested labels.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Excessive token fragmentation of Bengali script correlates with degraded downstream accuracy.
- **Mechanism:** Bengali uses an alphasyllabary script where base characters and diacritics form complex grapheme clusters. Standard subword tokenizers fragment these clusters into many small tokens, increasing ATPR and extending sequence lengths. This dilution of attention mechanism's ability to resolve semantic relationships compared to more compact English tokenization causes performance degradation.
- **Core assumption:** Performance drop is causally linked to token density/fragmentation rather than solely model capacity or translation quality.
- **Evidence anchors:** Inverse relationship between tokenization efficiency and LLM accuracy where models perform worse when inputs are excessively tokenized; Bengali ATPR significantly larger than English; SenSentencePiece confirms standard tokenizers perform poorly on Bengali script.

### Mechanism 2
- **Claim:** Pretraining data composition dictates multilingual robustness more than raw parameter count.
- **Mechanism:** Models explicitly pretrained on multilingual mixes (DeepSeek, Qwen) show higher stability when switching from English to Bengali. Models with limited Bengali exposure (LLaMA via token overlap, Mistral) suffer sharper accuracy drops or high formatting failures (RER). Specific architectural attention to multilingual alignment during pretraining allows larger models to bridge gaps that smaller, monolingually-biased models cannot.
- **Core assumption:** Performance delta is primarily driven by linguistic distribution in pretraining corpus, not architectural differences distinct to model families.
- **Evidence anchors:** DeepSeek showed more stable cross-language performance; DeepSeek/Qwen listed as having explicit Bengali in pretraining; LLaMA/Mistral marked as "Limited" or "No" exposure.

### Mechanism 3
- **Claim:** Translation-based evaluation benchmarks expose "reasoning transfer" limits in smaller models.
- **Mechanism:** Translated English reasoning benchmarks show performance gaps most severe in smaller models. These models lack the "semantic buffer" to process translated prompts (syntactically altered or noisier due to translation) and then apply abstract reasoning logic. Larger models absorb translation noise better.
- **Core assumption:** Translation process preserved enough semantic logic to make comparison valid; drop is due to model's language capability, not broken logic in translated dataset.
- **Evidence anchors:** Language gap most pronounced in math (GSM-8K) and commonsense reasoning; accuracy divergence widest for 3B/7B parameter range compared to 70B+; visual evidence showing accuracy divergence between English and Bengali.

## Foundational Learning
- **Concept:** Subword Tokenization (BPE/WordPiece) vs. Alphasyllabary Scripts
  - **Why needed here:** Standard tokenizers optimized for Latin/English space-delimited words mismatch with Bengali's complex grapheme clusters.
  - **Quick check question:** Why does splitting a Bengali word into 10 tokens hurt performance more than splitting an English word into 3 tokens?

- **Concept:** Response Error Rate (RER) & Adherence Rate (RAR)
  - **Why needed here:** Some models fail not because they don't know the answer, but because they don't follow formatting instructions in Bengali.
  - **Quick check question:** If a model answers "The answer is yes" instead of just "yes" in a BoolQ task, does RER penalize it?

- **Concept:** Cross-Lingual Transfer
  - **Why needed here:** Core comparison is between English and Bengali performance; understanding how models transfer learned logic from high-resource to low-resource languages is central to analysis.
  - **Quick check question:** Why might a model solve a math problem correctly in English but fail the exact same problem translated into Bengali?

## Architecture Onboarding
- **Component map:** English Datasets -> Translation Engine (GPT-4o-mini) -> Post-processing -> Bengali Datasets -> 10 LLMs -> Evaluation Engine -> Metrics (Accuracy, RER, RAR, LLM-Judge, Tokenization Stats)

- **Critical path:** Translation pipeline is bottleneck; multithreading skipped entries and decoding errors required manual regex fixes. Ensuring dataset loader correctly handles non-Latin JSON strings is most fragile part of setup.

- **Design tradeoffs:**
  - Translation vs. Native Creation: Machine translation ($200 cost) chosen over human curation (prohibitively expensive/time-consuming); scales fast but introduces potential translation artifacts.
  - LLM-Judge vs. Exact Match: LLM-Judge used because exact match failed to capture semantic equivalence in verbose model answers; trades precision for robustness against varied phrasing.

- **Failure signatures:**
  - Mistral 7B on Bengali: Watch for "Hallucinated Format" or "High RER"; model often outputs reasoning text instead of requested label.
  - Small Models (3B/8B) on Math: Expect near-random accuracy on GSM8K-BN.
  - Tokenization Overflow: If inputs exceed context window due to Bengali text expanding to 2-3x English token length, truncation will destroy reasoning context.

- **First 3 experiments:**
  1. Tokenization Correlation: Run tokenizer scripts on Bengali paragraph; plot ATPR vs. Accuracy for single model to verify inverse correlation.
  2. RER Sensitivity Test: Run Mistral-7B and DeepSeek-14B on BoolQ-BN subset; calculate RER specifically to confirm Mistral fails due to formatting while DeepSeek succeeds.
  3. Translation Quality Spot-Check: Randomly sample 10 rows from translated Hellaswag dataset; compare Bengali text against original English to identify potential "meaning drift."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the lack of manual validation in machine-translated benchmarks impact the quantification of performance gaps between English and Bengali?
- **Basis in paper:** Limitations section states translations were generated by GPT-4o-mini and "not manually validated," potentially introducing linguistic inaccuracies that conflate translation noise with model error.
- **Why unresolved:** Unclear if observed performance drops are caused by model's lack of Bengali capability or by model correctly identifying inconsistencies in synthetic dataset.
- **What evidence would resolve it:** Comparative study measuring model performance on translated benchmarks versus natively authored Bengali benchmarks to isolate "translation penalty."

### Open Question 2
- **Question:** To what extent do strict evaluation metrics underestimate the true reasoning capabilities of LLMs in Bengali generative tasks?
- **Basis in paper:** Section 5 acknowledges strict automated rules may "penalize valid answers that do not conform to a narrow format," limiting metric reliability across diverse model families.
- **Why unresolved:** Without semantic evaluation, models that answer correctly but with different phrasing or formatting are falsely classified as failures, masking actual understanding.
- **What evidence would resolve it:** Implementing and comparing results using more robust automatic evaluation method, such as fine-tuned semantic similarity model or "LLM-judge" calibrated for Bengali nuance.

### Open Question 3
- **Question:** Can optimizing tokenizers specifically for Bengali morphology bridge the performance gap with English, independent of increased pre-training data?
- **Basis in paper:** Section 3.4 and 4 analysis identifies inverse relationship where excessive fragmentation correlates with lower accuracy, suggesting tokenization efficiency is distinct bottleneck.
- **Why unresolved:** While correlation established, paper does not isolate whether fixing tokenizer alone is sufficient to improve performance without also addressing data scarcity.
- **What evidence would resolve it:** Ablation study replacing standard tokenizers with Bengali-optimized versions in existing models to measure accuracy gains while keeping pre-training data constant.

## Limitations
- Machine-translated benchmarks may introduce artifacts that conflate translation noise with model error, particularly affecting reasoning tasks.
- Evaluation framework relies on LLM-Judge for semantic validation, which introduces reliability concerns not fully quantified.
- Study cannot fully isolate tokenization efficiency effects from translation quality and inherent model architecture limitations.

## Confidence
**High Confidence:** Performance gaps between English and Bengali across all evaluated models are robust findings; tokenization efficiency metrics and their inverse correlation with accuracy are empirically supported.

**Medium Confidence:** Attribution of performance gaps primarily to tokenization inefficiency and limited pretraining exposure is plausible but not definitively proven; study provides strong correlative evidence but cannot fully isolate these factors.

**Low Confidence:** Specific claim that smaller models' reasoning transfer failures are primarily due to translation-induced semantic noise requires further validation; study lacks direct comparison with native Bengali reasoning datasets.

## Next Checks
1. **Translation Quality Validation:** Conduct human evaluation of 50 randomly sampled translated instances across all eight datasets to quantify semantic drift; compare model performance on verified samples against full machine-translated dataset to isolate translation effects.

2. **Tokenizer Ablation Study:** Re-evaluate same models using Bengali-optimized tokenizer (SenSentencePiece) while maintaining identical pretraining weights; measure changes in ATPR and accuracy correlation to test whether tokenization efficiency directly causes performance degradation.

3. **Native Dataset Benchmark:** Create small native Bengali subset (50-100 instances) for 2-3 reasoning tasks using human translators; compare model performance on native subset versus machine-translated versions to quantify translation artifacts' contribution to observed performance gaps.