---
ver: rpa2
title: 'IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG
  Systems'
arxiv_id: '2506.01615'
source_url: https://arxiv.org/abs/2506.01615
tags:
- retrieval
- languages
- indian
- multilingual
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IndicRAGSuite, a comprehensive resource
  suite for developing and evaluating Retrieval-Augmented Generation (RAG) systems
  in Indian languages. The key contributions are: (1) IndicMSMarco, a manually verified
  multilingual benchmark of 1,000 queries from MS MARCO translated into 13 Indian
  languages for evaluating retrieval and response generation; (2) a large-scale synthetic
  training dataset containing approximately 14 million (question, answer, relevant
  passage) triplets derived from Wikipedia articles of 19 Indian languages using large
  language models, plus translated versions of the MS MARCO training and development
  sets into 14 Indian languages.'
---

# IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems

## Quick Facts
- arXiv ID: 2506.01615
- Source URL: https://arxiv.org/abs/2506.01615
- Reference count: 19
- Key outcome: Introduces IndicMSMarco benchmark (1,000 queries × 13 Indian languages) and ~14M synthetic training triplets for Indian language RAG systems, with BGE-M3 achieving highest MRR scores (0.49-0.52) across most languages.

## Executive Summary
This paper addresses the critical shortage of evaluation benchmarks and training data for Indian language Retrieval-Augmented Generation (RAG) systems. The authors introduce IndicRAGSuite, a comprehensive resource suite containing IndicMSMarco, a manually verified multilingual benchmark derived from MS MARCO and translated into 13 Indian languages, and a large-scale synthetic training dataset of approximately 14 million (question, answer, relevant passage) triplets generated from Wikipedia articles of 19 Indian languages using large language models. The benchmark was used to evaluate several dense retrieval models, with BGE-M3 achieving the highest MRR scores across most languages, followed by Multilingual E5-large. This suite enables systematic development and comparison of retrieval models across India's linguistically diverse landscape.

## Method Summary
The method involves two primary data creation pipelines: (1) Benchmark creation using LLaMA 3.3 70B to translate 1,000 MS MARCO development queries into 13 Indian languages, followed by expert human verification for linguistic accuracy and semantic consistency; (2) Training data generation through WikiExtractor preprocessing of Wikipedia dumps from 19 Indian languages, then LLaMA 3.3 70B generating 3 diverse QA pairs with reasoning per paragraph, plus paragraph-level translation of MS MARCO train/dev sets using IndicTrans3-beta into 14 Indian languages. The resulting datasets were used to evaluate dense retrievers (BGE-M3, Multilingual E5 variants, LLM2VEC) on the IndicMSMarco benchmark using MRR as the primary metric.

## Key Results
- BGE-M3 achieved highest MRR scores across most Indian languages (0.49 in Malayalam and Tamil, 0.50 in Telugu)
- Multilingual E5-large followed as second-best performer in the evaluation
- Significant performance gap observed between high-resource languages (Hindi: 0.52 MRR) and low-resource languages (Assamese: 0.30, Odia: 0.31)
- The synthetic Wikipedia-based dataset contains approximately 14 million training triplets across 19 Indian languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated question-answer-reasoning triplets from native Wikipedia content provide scalable training data for low-resource Indian language retrieval.
- Mechanism: Wikipedia paragraphs in each target language → LLaMA 3.3 70B generates 3 diverse QA pairs with explicit reasoning per paragraph → creates contrastive training signal covering multiple question types across the full paragraph → reduces positional bias toward opening sentences.
- Core assumption: LLM-generated questions in Indian languages reflect realistic user information-seeking patterns and maintain factual grounding to source passages.
- Evidence anchors:
  - [abstract] "a large-scale synthetic training dataset containing approximately 14 million (question, answer, relevant passage) triplets derived from Wikipedia articles of 19 Indian languages using large language models"
  - [Section 4.1.4] "the triplets are crafted to cover diverse question types... and different parts of the paragraph, thereby reducing bias toward the initial lines"
  - [corpus] Weak direct validation—corpus neighbors focus on related benchmarks (BharatBBQ, BanglaMedQA) but don't validate synthetic QA quality for retrieval training.
- Break condition: If generated questions don't match real user query distributions, or reasoning contains hallucinations not grounded in source passages, downstream retrieval model generalization degrades.

### Mechanism 2
- Claim: Paragraph-level translation preserves contextual coherence and semantic alignment better than sentence-fragment translation for retrieval training data.
- Mechanism: MS MARCO passages → IndicTrans3-beta translates full paragraphs intact → maintains query-passage semantic relationships and named entity consistency → avoids context fragmentation that occurs with sentence-splitting approaches.
- Core assumption: Paragraph-level translation reduces semantic drift compared to sentence-by-sentence translation used in prior work (Indic-MARCO with NLLB-1.3B).
- Evidence anchors:
  - [abstract] "translated versions of the MS MARCO training and development sets into 14 Indian languages"
  - [Section 4.2] "our method preserves full-paragraph structure throughout translation. This approach maintains better contextual coherence, semantic alignment, and domain fidelity" compared to prior sentence-level approaches.
  - [corpus] Not directly validated in corpus neighbors.
- Break condition: If paragraph-level translation introduces systematic errors, fails on long passages, or corrupts named entities/numerical data, retrieval alignment between queries and passages degrades.

### Mechanism 3
- Claim: LLM-assisted translation with human post-editing creates reliable evaluation benchmarks by catching translation artifacts before they skew retrieval metrics.
- Mechanism: MS MARCO dev queries → LLaMA 3.3 70B initial translation → expert annotators verify (1) linguistic accuracy, (2) semantic consistency vs. original, (3) entity/domain terminology → corrections applied → standardized MRR evaluation across languages.
- Core assumption: Human verification catches semantic drift and ambiguity that would create noisy retrieval ground truth.
- Evidence anchors:
  - [abstract] "IndicMSMarco, a manually verified multilingual benchmark of 1,000 queries from MS MARCO translated into 13 Indian languages"
  - [Section 3.2] "Annotators review translations for grammatical correctness, fluency, and readability... flag and correct any instances of semantic drift, mistranslations, or ambiguous phrasing."
  - [corpus] Corpus neighbors (BharatBBQ, IndicVisionBench) similarly use culturally-adapted human-verified benchmarks for Indian language evaluation.
- Break condition: If human annotators introduce their own biases, miss domain-specific errors, or if 1,000 queries insufficiently represent query diversity, benchmark validity is compromised.

## Foundational Learning

- Concept: **Dense Retrieval / Bi-Encoder Architecture**
  - Why needed here: The entire evaluation framework tests dense retrievers (BGE-M3, E5) that encode queries and passages into shared vector space for similarity-based ranking.
  - Quick check question: Can you explain how a bi-encoder differs from a cross-encoder, and why bi-encoders are preferred for large-scale retrieval?

- Concept: **Mean Reciprocal Rank (MRR)**
  - Why needed here: MRR is the primary evaluation metric reported (0.30-0.52 across languages)—measures how high the first relevant result appears in ranked output.
  - Quick check question: Given ranks [1, 3, 2] for relevant documents across 3 queries, what is the MRR?

- Concept: **Contrastive Learning for Retrieval**
  - Why needed here: Training triplets (query, relevant passage, implicit negatives) enable contrastive loss training that pulls relevant pairs together and pushes irrelevant pairs apart.
  - Quick check question: What role do hard negatives play in contrastive retrieval training, and how might they be mined?

## Architecture Onboarding

- Component map:
  ```
  IndicRAGSuite Resources:
  ├── IndicMSMarco (Evaluation Benchmark)
  │   ├── 1,000 queries × 13 languages
  │   ├── Source: MS MARCO dev set
  │   └── Pipeline: LLaMA 3.3 70B → Human post-edit
  │
  └── Training Data (Two Sources)
      ├── Wikipedia-Generated (~14M triplets)
      │   ├── 19 Indian languages
      │   ├── Pipeline: WikiExtractor → LLaMA 3.3 70B QA generation
      │   └── Filtering: Remove too-short/excessively-long paragraphs
      │
      └── Translated MS MARCO (~12.2M total)
          ├── Train: ~778K × 14 languages
          └── Pipeline: IndicTrans3-beta paragraph-level translation
  ```

- Critical path:
  1. **Data prep**: Download Wikipedia dumps → WikiExtractor → filter paragraphs by length
  2. **Triplet generation**: LLaMA 3.3 70B generates 3 QA pairs + reasoning per paragraph
  3. **Translation**: IndicTrans3-beta translates MS MARCO (paragraph-level, not sentence-level)
  4. **Training**: Fine-tune dense retriever (BGE-M3 recommended) on combined data
  5. **Evaluation**: Compute MRR on IndicMSMarco benchmark

- Design tradeoffs:
  - Synthetic scale vs. distribution match: 14M triplets scales to 19 languages but may not reflect real user queries
  - Translation quality vs. speed: IndicTrans3-beta chosen over NLLB-1.3B for quality; slower inference
  - Benchmark size vs. reliability: 1,000 queries enables manageable human verification but limits statistical power

- Failure signatures:
  - **Resource-gap pattern**: Low MRR on Assamese (0.30) and Odia (0.31) vs. Hindi (0.52) correlates with training data scarcity
  - **Translation artifacts**: Named entity corruption or numerical errors break factual retrieval
  - **Script diversity issues**: Models struggle with languages sharing scripts but having different vocabulary

- First 3 experiments:
  1. **Baseline reproduction**: Evaluate BGE-M3 and E5-large on IndicMSMarco to confirm reported MRR scores (target: 0.49-0.52 for high-resource languages)
  2. **Data ablation**: Train retriever on (a) Wikipedia-generated only, (b) translated MS MARCO only, (c) combined—measure per-language MRR to quantify contribution of each data source
  3. **Low-resource deep-dive**: Analyze error cases in Assamese/Odia to determine if failures stem from (a) insufficient training data, (b) translation quality, or (c) tokenizer coverage issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the comparative efficacy of fine-tuning dense retrievers on the synthetic Wikipedia-based data versus the translated MS MARCO data?
- Basis in paper: [explicit] Section 4.3 states the next phase of work aims to "understand the individual and combined impact of synthetic data and real-world query-passage pairs."
- Why unresolved: The current paper establishes the dataset and provides baselines using pre-trained models, but it does not yet present results from models fine-tuned on this specific new training data.
- What evidence would resolve it: Benchmark results (MRR scores) comparing models fine-tuned solely on the synthetic set, solely on the translated set, and on a hybrid of both.

### Open Question 2
- Question: Does the IndicRAGSuite training data significantly close the performance gap between high-resource languages (e.g., Hindi) and low-resource languages (e.g., Assamese, Odia)?
- Basis in paper: [explicit] Section 4.3 notes the aim to "benchmark performance across 13 Indian languages, with special emphasis on gains in low-resource language settings."
- Why unresolved: The baseline results (Table 2) show a persistent gap where Hindi outperforms low-resource languages like Assamese by ~0.06 MRR, even with strong multilingual models like BGE-M3.
- What evidence would resolve it: A comparative analysis showing the relative percentage improvement of MRR for low-resource languages versus high-resource languages after fine-tuning on the suite.

### Open Question 3
- Question: Does the translation of English queries for the IndicMSMarco benchmark result in a lack of cultural context or "regional depth" for Indian users?
- Basis in paper: [inferred] The authors critique existing benchmarks in Section 2.1 for lacking "regional depth," yet their own benchmark is created by translating English MS MARCO queries rather than collecting native queries.
- Why unresolved: While the translations are manually verified for linguistic accuracy, the paper does not evaluate whether the underlying information needs are relevant or natural to Indian language speakers.
- What evidence would resolve it: A qualitative study or failure analysis comparing performance on translated queries versus natively sourced queries on the same topics.

## Limitations
- Data Quality Gap: Synthetic training data quality depends entirely on LLM's ability to generate realistic, factually-grounded questions without human validation on the 14M triplets
- Benchmark Coverage Constraint: 1,000-query benchmark may insufficiently represent full diversity of Indian language information-seeking patterns
- Low-Resource Language Challenge: Persistent performance gap (0.30-0.31 MRR) for Assamese and Odia vs. Hindi (0.52) with unclear root causes

## Confidence
- **High Confidence**: Creation of IndicMSMarco benchmark through LLaMA 3.3 70B translation followed by human verification
- **Medium Confidence**: BGE-M3 achieving highest MRR scores across most languages
- **Medium Confidence**: Synthetic Wikipedia-based training data improves retrieval performance
- **Low Confidence**: Paragraph-level translation preserves better contextual coherence than sentence-level approaches

## Next Checks
1. **Synthetic Data Quality Audit**: Sample 100 random triplets from Wikipedia-generated dataset across 5 Indian languages and have native speakers rate question relevance, factual accuracy, and diversity of question types.
2. **Low-Resource Language Deep-Dive**: For Assamese and Odia, conduct error analysis on 50 failed retrieval cases each to determine if failures stem from insufficient training data, translation quality degradation, tokenizer coverage gaps, or fundamental model limitations.
3. **Benchmark Representativeness Test**: Sample 200 real user queries from Indian language search logs and compare their distribution against the 1,000-query IndicMSMarco benchmark to quantify coverage gaps and potential biases.