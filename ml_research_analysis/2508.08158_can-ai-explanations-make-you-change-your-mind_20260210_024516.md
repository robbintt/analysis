---
ver: rpa2
title: Can AI Explanations Make You Change Your Mind?
arxiv_id: '2508.08158'
source_url: https://arxiv.org/abs/2508.08158
tags:
- participants
- explanations
- decision
- were
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an exploratory analysis of a user study investigating
  how AI explanations impact decision-making. Participants predicted student graduation
  outcomes with AI assistance, receiving feature importance explanations in various
  formats (highlighting, bar chart, text).
---

# Can AI Explanations Make You Change Your Mind?

## Quick Facts
- arXiv ID: 2508.08158
- Source URL: https://arxiv.org/abs/2508.08158
- Authors: Laura Spillner; Rachel Ringe; Robert Porzel; Rainer Malaka
- Reference count: 14
- Most participants spent little time on explanations, especially when AI suggestions aligned with their initial choices.

## Executive Summary
This paper presents an exploratory analysis of a user study investigating how AI explanations impact decision-making. Participants predicted student graduation outcomes with AI assistance, receiving feature importance explanations in various formats (highlighting, bar chart, text). Most participants spent little time on explanations, especially when AI suggestions aligned with their initial choices. Text and bar chart explanations, which took longer to engage with, were more effective at changing minds. Participants with AI experience were less likely to switch decisions, while those without were easily swayed, including by misleading AI suggestions. Overall, explanations improved accuracy but did not consistently help participants distinguish correct from incorrect AI suggestions. The study highlights the complexity of designing effective AI explanations and measuring their impact.

## Method Summary
The study used a "Two-Step Workflow" where participants first made an independent decision about student graduation outcomes, then received AI predictions with SHAP-based explanations in one of three formats (highlighting, bar chart, or text). A constrained linear regression model with ~73% accuracy was used to ensure sufficient uncertainty. Time spent on explanations and decision switching were logged. Participants were categorized by self-reported AI experience to examine trust patterns.

## Key Results
- Most participants spent little time on explanations, especially when AI suggestions aligned with their initial choices
- Text and bar chart explanations, which took longer to engage with, were more effective at changing minds
- Participants with AI experience were less likely to switch decisions, while those without were easily swayed, including by misleading AI suggestions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Users tend to satisfice (spend minimal time) on AI explanations when the AI suggestion confirms their initial pre-existing belief.
- **Mechanism:** Confirmation bias reduces the perceived need for systematic processing. If the AI agrees with the user's intuition ("Two-Step Workflow" first choice), the user treats the explanation as a rubber stamp rather than information to scrutinize.
- **Core assumption:** Users optimize for cognitive effort, only engaging deeply when there is a discrepancy between their internal model and the AI's output.
- **Evidence anchors:**
  - [abstract] "Most participants spent little time on explanations, especially when AI suggestions aligned with their initial choices."
  - [section] Table 1 shows that when AI agrees, time drops to ~54% compared to when it disagrees; 87% failed the attention test when it was hidden in an agreeing explanation.
  - [corpus] "Critical or Compliant?..." suggests explanations can foster confirmation bias where users assume reasoning is correct if outputs are acceptable.
- **Break condition:** If the decision task is high-stakes or legally mandated, satisficing behavior may decrease regardless of agreement.

### Mechanism 2
- **Claim:** Increasing the "interaction cost" (friction) of an explanation format forces longer engagement, which increases the likelihood of a user changing their mind.
- **Mechanism:** Formats like full text or bar charts require sequential processing (reading/scanning) compared to pre-attentive processing (highlighting). This enforced dwell time increases the cognitive impact of the content, making a switch more likely.
- **Core assumption:** Time spent viewing an explanation correlates with processing depth, rather than just navigation friction.
- **Evidence anchors:**
  - [abstract] "Text and bar chart explanations, which took longer to engage with, were more effective at changing minds."
  - [section] Table 2 indicates Text and Bar Chart explanations result in 4.9x to 6.5x higher likelihood of switching compared to no explanation, correlating with the longer decision times shown in Figure 1.
  - [corpus] Weak direct link in neighbors; "Expectations, Explanations, and Embodiment" touches on utility of explanations in failure but not specifically friction.
- **Break condition:** If the text is too complex or the visualization is confusing, increased time may indicate confusion rather than persuasion, leading to decision abandonment.

### Mechanism 3
- **Claim:** Prior AI experience functions as a skepticism filter, reducing over-reliance (overtrust) on AI suggestions but potentially increasing under-reliance (undertrust).
- **Mechanism:** Experienced users likely possess a mental model of AI fallibility ("gaming the algorithm"). They weight their own judgment higher than the AI's suggestion when conflicts arise, resisting the "sway" effect seen in novices.
- **Core assumption:** Self-reported AI familiarity correlates with actual understanding of AI limitations and error modes.
- **Evidence anchors:**
  - [abstract] "Participants with AI experience were less likely to switch decisions, while those without were easily swayed..."
  - [section] Figure 2 shows inexperienced users had 73.3% overtrust (switching to wrong AI) with text, while experienced users maintained high undertrust (sticking with their own wrong choice) across conditions.
  - [corpus] "Your Model Is Unfair..." suggests an inverse relationship between comprehension and trust in specific visualization contexts.
- **Break condition:** If the domain is highly specialized (e.g., medical imaging) where the user has *less* domain expertise than the AI, this skepticism filter might break down.

## Foundational Learning

- **Concept: Two-Step Decision Workflow**
  - **Why needed here:** The paper relies on capturing an "Initial Choice" before showing the AI to measure "Switching." Without understanding this architecture, one cannot distinguish between a user independently agreeing with the AI and a user blindly following it.
  - **Quick check question:** If a user sees the AI suggestion immediately (One-Step), can you measure "overtrust" as defined in this paper? (Answer: No, you lose the baseline of their independent judgment).

- **Concept: Feature Importance (SHAP)**
  - **Why needed here:** The explanations provided (Highlighting, Bar, Text) are all derived from feature importance weights. Understanding that these are local explanations (specific to one student) vs. global explanations is key to interpreting the user reactions.
  - **Quick check question:** Does a "green" highlighted grade mean the student will definitely graduate, or that this specific feature *pushes* the probability higher relative to the base rate? (Answer: The latter).

- **Concept: Warranted Trust vs. Overtrust**
  - **Why needed here:** The paper critiques "Switch Rate" as a metric. A high switch rate is only good if the AI is correct (Warranted Trust); if the AI is wrong, it is Overtrust. The study shows explanations failed to help users distinguish these.
  - **Quick check question:** If a user switches their decision to match the AI, and the AI is wrong, is this a failure of the user or the explanation mechanism? (Answer: Both; the explanation failed to reveal the error).

## Architecture Onboarding

- **Component map:** Student Profile -> Linear Regression Model -> SHAP Explainer -> Explanation Renderer (Highlighting/Text/Bar) -> Decision Logger
- **Critical path:**
  1. User views raw data -> Logs Initial Choice
  2. System retrieves AI prediction + generates SHAP values
  3. System renders Explanation (Format depends on experimental condition)
  4. User views AI + Explanation -> Logs Final Choice

- **Design tradeoffs:**
  - **Highlighting:** Fast to read (low friction), but users often miss it or ignore it (low engagement)
  - **Text/Bar Chart:** High engagement/switch rate, but risks "Information Overload" or blind trust in novices who assume complexity = correctness
  - **Attention Checks:** Placing them in explanations reveals non-reading behavior but contaminates the natural decision flow

- **Failure signatures:**
  - **The "Speeder" Pattern:** Users spending <5s on the second decision generally ignore the explanation
  - **The "Skeptic" Lock-in:** Experienced users taking >20s on initial choice rarely switch, even when beneficial (Undertrust)
  - **Blind Conformity:** Novices switching to the wrong answer >70% of the time when presented with authoritative text explanations

- **First 3 experiments:**
  1. **Time-Gated Explanation:** Do not show the "Submit" button for the final decision until 5 seconds have passed to test if forced exposure reduces the gap between Highlighting and Text effectiveness
  2. **Uncertainty Indicator:** Display the AI's confidence score alongside the explanation to see if it helps experienced users reduce undertrust
  3. **Adversarial Explanation:** Intentionally show a nonsensical explanation (e.g., highlighting irrelevant features) for a correct prediction to test if users are actually reading the content or just noting the presence of color

## Open Questions the Paper Calls Out

- Would users in a real-world, high-stakes scenario follow the same pattern of selectively ignoring explanations when the AI confirms their initial intuition?
- Are users with prior AI experience better at critically interpreting explanation content to distinguish correct from incorrect suggestions, or are they simply more skeptical of AI in general?
- How can researchers measure user attention to specific parts of an explanation without the measurement method itself forcing participants to engage more deeply than they naturally would?

## Limitations

- The study uses an artificial model accuracy target (~73%), raising questions about ecological validity and whether results generalize to real-world AI systems with different accuracy levels
- The binary graduation prediction task may not represent the complexity of real decision-making scenarios where stakes and consequences vary
- Self-reported AI experience may not accurately capture actual AI literacy or understanding of model limitations

## Confidence

- **High Confidence:** The core finding that explanation formats requiring more engagement time (text, bar charts) are more effective at changing decisions is well-supported by the data
- **Medium Confidence:** The relationship between AI experience and reduced switching behavior, while observed, may be influenced by self-reporting bias and doesn't establish causation
- **Low Confidence:** The claim that explanations failed to help distinguish correct from incorrect AI suggestions is based on aggregate metrics without deeper analysis of *why* users made incorrect switches

## Next Checks

1. **Ecological Validity Test:** Repeat the study with a real production AI system (varying accuracy levels) to assess if results hold when users believe the AI is fully functional
2. **Qualitative Follow-up:** Conduct think-aloud protocols or post-task interviews to understand the reasoning behind switching decisions, particularly when users follow incorrect AI suggestions
3. **Domain Transferability:** Test the same explanation formats and user experience effects in a different high-stakes domain (e.g., medical diagnosis) to evaluate generalizability of the satisficing and experience effects