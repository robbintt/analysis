---
ver: rpa2
title: Using Span Queries to Optimize for Cache and Attention Locality
arxiv_id: '2511.02749'
source_url: https://arxiv.org/abs/2511.02749
tags:
- span
- query
- figure
- cache
- vllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces span queries, a declarative intermediate
  representation for optimizing large language model inference. Span queries generalize
  non-chat workloads (RAG, inference-time scaling, agents) by expressing ordering
  constraints via commutativity.
---

# Using Span Queries to Optimize for Cache and Attention Locality

## Quick Facts
- **arXiv ID**: 2511.02749
- **Source URL**: https://arxiv.org/abs/2511.02749
- **Reference count**: 12
- **Primary result**: Introduces span queries, a declarative IR for optimizing LLM inference by improving KV cache and attention locality, showing 10-20x TTFT reductions for RAG and nested generation workloads.

## Executive Summary
This paper introduces span queries, a declarative intermediate representation that generalizes non-chat LLM workloads by expressing ordering constraints via commutativity. The authors show how span queries can be automatically optimized to improve both KV cache locality and attention locality. They implement a small modification to vLLM (492 lines) supporting span query execution. Experiments demonstrate significant performance improvements across RAG, nested generation, and attention locality tasks, with 2B optimized models outperforming unoptimized 8B models.

## Method Summary
The method introduces span queries as expression trees that capture ordering constraints through commutativity. A high-level optimizer rewrites these trees (desugaring, plus distribution), while a tokenizer serializes them with special parenthesization tokens ensuring block alignment. The vLLM scheduler is modified to suspend prefix-hash accumulation for commutative spans, and a GPU runner implements CIDRA (Concurrent In-place Duplicating ReROPE Algorithm) for efficient cache block repositioning. The system supports core operators including system prompts, assistant/user interactions, generation, and commutative/non-commutative joins.

## Key Results
- 10-20x reductions in time-to-first-token for RAG and nested generation workloads
- 2B attention-optimized span query models vastly outperform unoptimized 8B models on attention locality tasks
- High-throughput execution of reordered cache blocks (500 tokens/ms) using CIDRA algorithm
- Distinct performance benefits across multiple non-chat use cases including RAG microbenchmarks and needle-in-the-haystack attention tests

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-chat workloads can reuse KV cache blocks across requests when input segments are marked as order-independent (commutative).
- **Mechanism:** Span queries use special tokens to mark sub-trees, with the vLLM scheduler suspending prefix-hash accumulation for commutative spans. This decouples cache keys from absolute prompt positions, allowing fragment reuse regardless of prefix context.
- **Core assumption:** Application logic correctly identifies commutative inputs where reordering doesn't alter semantic outcomes.
- **Evidence anchors:** Abstract states span queries "express ordering constraints via commutativity" and improve "KV cache locality." Section 5.4 describes suspending hash accumulation for `(` tokens. LMCache (arXiv:2510.09665) supports moving caches outside GPUs for reuse.
- **Break condition:** Incorrectly labeling non-commutative inputs (e.g., sequential plot points) as commutative breaks causal dependencies and causes incoherent generation.

### Mechanism 2
- **Claim:** Restructuring generation into hierarchical tree reduction improves attention locality, mitigating the "lost-in-the-middle" phenomenon.
- **Mechanism:** High-level optimizer rewrites flat wide generation (e.g., judging 8 candidates at once) into deeper tree structures (e.g., 3 sequential 2-way judgments), keeping relevant information near attention windows' start/end.
- **Core assumption:** Models have sufficient reasoning capacity to synthesize intermediate judgments without accumulating error across tree levels.
- **Evidence anchors:** Abstract mentions span queries "can also be optimized to improve attention locality." Section 6 demonstrates transforming 8-way judgment into 3 2-way steps. Sparse Attention Remapping (arXiv:2505.05772) supports sparse attention mechanisms.
- **Break condition:** If tasks require global context from all candidates simultaneously, tree reduction discards necessary cross-candidate dependencies, degrading accuracy.

### Mechanism 3
- **Claim:** High-throughput execution of reordered cache blocks requires efficient in-place repositioning algorithm (ReRoPE) for concurrent requests with overlapping KV vectors.
- **Mechanism:** CIDRA builds dependency graphs of required block repositionings, duplicates blocks only when out-degree > 1, and uses strongly connected component analysis to batch GPU operations, minimizing memory overhead and race conditions.
- **Core assumption:** Repositioning calculation and tensor operation overhead is significantly lower than recomputing blocks from scratch.
- **Evidence anchors:** Abstract states experiments show 10-20x TTFT reductions. Section 5.5.1 details CIDRA's dependency graph handling and batching for 500 tokens/ms throughput. QUILL (arXiv:2511.13679) supports schedule-aware accelerators for irregular memory access.
- **Break condition:** If cycle size in repositioning graph exceeds GPU batch size significantly, system falls back to CPU processing, negating latency gains.

## Foundational Learning

- **Concept: Prefix Caching & Hashing**
  - **Why needed here:** Standard vLLM caches based on hash of all preceding blocks. Understanding this explains why RAG/Agents break caching (changing prefixes alter hash) and how Span Queries fix it (suspending hash chain).
  - **Quick check question:** If block B depends on hash of block A, what happens to B's cache eligibility if block A is removed or moved in new request?

- **Concept: Commutativity in Semantics**
  - **Why needed here:** Core optimization relies on algebraic property `A + B = B + A`. Must identify which prompt parts can be reordered without changing logical outcome.
  - **Quick check question:** In "Judge-Generator" flow, does order Judge reads Candidate A vs B change final judgment? If not, operation is commutative.

- **Concept: Rotary Positional Embeddings (RoPE)**
  - **Why needed here:** When cache blocks move to new sequence positions, positional encodings become invalid. Understanding RoPE explains why ReRoPE (re-computing position info) is critical systems cost for cache mobility.
  - **Quick check question:** Why can't we simply take cached KV vector from position 5 and insert into position 20 without modification?

## Architecture Onboarding

- **Component map:** Frontend (Client defines Span Query) -> High-Level Optimizer (Rewrites tree) -> Tokenizer (Serializes with `(` `)` tokens) -> vLLM Scheduler (Modified, suspends hashing) -> GPU Runner (Modified, implements CIDRA)
- **Critical path:** Serialization format (parenthesization) is contract between optimizer and vLLM scheduler. If `(` tokens not block-aligned by tokenizer, scheduler's hash suspension logic fails to trigger or misaligns with cache boundaries.
- **Design tradeoffs:** Simplicity vs Accuracy (overloading existing tokens to avoid retraining risks confusing model if special tokens clash with semantic content); Crop vs Miss (cropping trailing partial blocks to guarantee cache hits potentially loses 1-2 tokens).
- **Failure signatures:** Zero Cache Hit Rate (likely missing block alignment or incorrect hash suspension); Incoherent Output (RAG) (marking non-commutative inputs as commutative); High Latency on "Hit" (CIDRA falling back to CPU due to large cycle detection).
- **First 3 experiments:**
  1. **RAG Microbenchmark:** Vary document count (1-32) and measure TTFT delta between stock vLLM and Span Query stack (expect 10x+ improvement).
  2. **Nested Generation Stability:** Run judge/generator flow with inner fan-out 24 and temp > 0; verify p99 TTFT stability (check for degradation vs p50).
  3. **Attention Locality Check:** Run "Needle in Haystack" test with increasing "hay" length; compare 2B optimized (tree) vs 8B unoptimized (flat) accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can span queries enable nested generation to bypass the KV cache entirely by using an optimized gather/scatter scheduler?
- Basis in paper: Conclusion suggests analyzing span queries to produce a scheduler where inner/outer generates are arranged in a "gather/scatter structure (c.f. SQL)" rather than flowing through KV cache.
- Why unresolved: Current implementation optimizes for locality within existing KV cache paradigm; bypassing cache for direct data movement is proposed but not implemented.
- Evidence: Prototype scheduler that executes nested generation without intermediate KV cache storage, demonstrating reduced memory overhead or latency.

### Open Question 2
- Question: What is the optimal client-facing language design or API for defining span query expression trees?
- Basis in paper: Section 4.1 states "We defer the topic of language design to future work," noting current focus is on intermediate representation.
- Why unresolved: While semantics of span query IR are defined, user-facing syntax or library interface to construct these trees remains unspecified.
- Evidence: Proposed DSL or library specification and comparative analysis of expressiveness and usability across defined use cases (chat, RAG, agents).

### Open Question 3
- Question: How can the tail latency of span query execution be stabilized beyond the 99th percentile (p99)?
- Basis in paper: Section 5.7 notes that while median TTFT improves, "stability begins to suffer" at p99, concluding "Further work is therefore necessary to ensure stability beyond two 9's."
- Why unresolved: Current CIDRA scheduling handles median cases well but exhibits variance under high concurrency or fan-out.
- Evidence: Improved scheduling heuristic or repositioning algorithm maintaining consistent TTFT speedups up to 99.9th percentile in nested generation benchmarks.

## Limitations

- **Token Alignment Dependency**: Optimization relies heavily on block-aligned parenthesization tokens; misalignment between tokenizer and scheduler can completely break cache locality gains.
- **Commutativity Assumption Risk**: Core optimization assumes application logic correctly identifies commutative operations; limited empirical validation that real-world workloads satisfy these semantic constraints across domains.
- **Model-Specific Evaluation**: All experiments use Granite 3.3 models; claimed 2B vs 8B performance reversal requires validation across different model families and training regimes.

## Confidence

- **High Confidence**: Basic mechanism of suspending KV cache hashing for commutative spans is well-founded and vLLM integration is technically sound (492 lines modification is verifiable concrete claim).
- **Medium Confidence**: Attention locality optimization through tree reduction shows promise but accuracy preservation across different task types needs broader validation beyond needle-in-haystack scenario.
- **Medium Confidence**: CIDRA repositioning algorithm's claimed 500 tokens/ms throughput is specific but depends heavily on GPU architecture and block contention patterns not fully characterized.

## Next Checks

1. **Cross-Model Commutativity Validation**: Test Span Query system with different model families (Llama, Mistral, Qwen) on RAG tasks to verify semantic commutativity assumptions hold across training regimes and cache locality gains transfer.

2. **Error Boundary Characterization**: Systematically measure accuracy degradation when tree-reduction optimization is applied to increasingly complex reasoning tasks that may require global context, establishing clear boundaries for safe application.

3. **Multi-GPU Scaling Analysis**: Evaluate CIDRA's performance characteristics under high contention scenarios with multiple concurrent requests across multiple GPUs to identify scaling limits and potential bottlenecks not visible in single-GPU microbenchmarks.