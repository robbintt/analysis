---
ver: rpa2
title: Improving Generalization in LLM Structured Pruning via Function-Aware Neuron
  Grouping
arxiv_id: '2512.23014'
source_url: https://arxiv.org/abs/2512.23014
tags:
- pruning
- sparsity
- neurons
- neuron
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor generalization in structured
  pruning of large language models when calibration sets fail to represent the pretraining
  data distribution. The proposed solution, Function-Aware Neuron Grouping (FANG),
  groups neurons by functional roles, prunes each group independently with reweighted
  importance metrics, retains shared neurons contributing across contexts, and adaptively
  allocates sparsity based on functional complexity.
---

# Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping

## Quick Facts
- **arXiv ID:** 2512.23014
- **Source URL:** https://arxiv.org/abs/2512.23014
- **Reference count:** 40
- **Primary result:** FANG improves downstream accuracy by 1.5%-8.5% while maintaining low perplexity compared to standard pruning methods

## Executive Summary
The paper addresses the critical problem of poor generalization in structured pruning of large language models when calibration sets fail to represent the pretraining data distribution. FANG (Function-Aware Neuron Grouping) proposes grouping neurons by functional roles, pruning each group independently with reweighted importance metrics, retaining shared neurons contributing across contexts, and adaptively allocating sparsity based on functional complexity. Experiments demonstrate FANG achieves state-of-the-art results, improving downstream accuracy by 1.5%-8.5% while maintaining low perplexity when combined with pruning methods like FLAP and OBC.

## Method Summary
FANG groups FFN neurons by functional role through a pipeline: (1) cluster tokens via K-Means on PCA-reduced FFN inputs to identify semantic contexts; (2) compute cluster-neuron sensitivity scores using Taylor expansion; (3) assign neurons to functional groups via Linear Assignment Problem; (4) identify and preserve shared neurons contributing across multiple contexts; (5) reweight importance by semantic similarity using softmax over negative distances; (6) allocate sparsity adaptively based on Functional Complexity (cosine similarity between block input/output); (7) prune each group independently using OBC or FLAP variants. The method is tested on LLaMA-1/2/3.1, Qwen-2.5 (7B–70B) with 20%, 30%, 40% sparsity.

## Key Results
- FANG achieves state-of-the-art performance, improving downstream accuracy by 1.5%-8.5% over baseline pruning methods
- Maintains low perplexity while achieving higher sparsity levels compared to traditional approaches
- Effectively preserves general language modeling capability by retaining shared neurons across functional groups

## Why This Works (Mechanism)

### Mechanism 1: Functional Specialization
- **Claim:** Grouping neurons by functional roles and pruning them independently mitigates calibration bias that causes generalization loss in standard post-training pruning.
- **Mechanism:** The method uses K-Means clustering on token representations (reduced via PCA) to identify semantic context types, then formulates a Linear Assignment Problem to assign neurons to these clusters based on Taylor-expansion sensitivity scores.
- **Core assumption:** LLM neurons are functionally specialized and process distinct types of semantic context, rather than being fully distributed or polysemantic across all inputs.
- **Break condition:** If token representations in the calibration set do not form distinct semantic clusters, the "function-aware" grouping may fail to isolate relevant neurons.

### Mechanism 2: Semantic Reweighting
- **Claim:** Reweighting token importance based on semantic relevance to a neuron group preserves critical capabilities better than uniform averaging.
- **Mechanism:** During importance estimation within a functional group, tokens are reweighted based on semantic distance to the group's function using softmax over negative L2 distances.
- **Core assumption:** A neuron's importance is best measured by its behavior on inputs relevant to its assigned function, rather than its average activation across a potentially mismatched calibration set.
- **Break condition:** If the temperature parameter τ is misconfigured or if the calibration set entirely lacks data for a specific functional domain, reweighting may amplify noise.

### Mechanism 3: Shared Neuron Preservation
- **Claim:** Retaining neurons that contribute across multiple contexts and allocating sparsity based on complexity preserves general-purpose capacity.
- **Mechanism:** The framework identifies a "Shared Neuron Group" (neurons ranked highly by multiple functional clusters) and exempts them from pruning. It also allocates sparsity based on Functional Complexity (cosine similarity between block input/output).
- **Core assumption:** Blocks with low output transformation (high similarity) are functionally simpler and contain more redundancy than blocks that significantly alter representations.
- **Break condition:** If functional complexity does not correlate with layer importance, this mechanism might unnecessarily limit the compression ratio.

## Foundational Learning

- **Concept:** **Structured vs. Unstructured Pruning**
  - **Why needed here:** FANG is a structured pruning method that directly reduces model width for hardware acceleration, requiring careful neuron selection to maintain accuracy.
  - **Quick check question:** Does removing 50% of individual weights provide the same inference speedup on a standard GPU as removing 50% of neurons (channels)?

- **Concept:** **Calibration Bias in Post-Training Pruning**
  - **Why needed here:** The core problem FANG solves is that small calibration sets may not represent the diverse data the model was pre-trained on, leading to the removal of neurons critical for downstream tasks.
  - **Quick check question:** If a calibration set contains only news articles, which neurons might a standard pruning method accidentally remove, and what capability would the model lose?

- **Concept:** **Linear Assignment Problem (LAP)**
  - **Why needed here:** FANG uses LAP to optimally map neurons to functional groups, distinguishing between a "soft" clustering and the "hard" one-to-one matching used here to define functional boundaries.
  - **Quick check question:** Why would a simple "maximum activation" mapping fail compared to an optimal assignment strategy when distributing neurons among K groups?

## Architecture Onboarding

- **Component map:** Input (pretrained LLM + calibration set) -> Context Analyzer (PCA -> K-Means -> Distance Matrix) -> Neuron-Function Mapper (Taylor Scores -> Score Matrix -> LAP -> Functional Groups) -> Pruning Engine (Reweighted Importance -> Group-wise OBC/FLAP -> Adaptive Sparsity Allocation)

- **Critical path:** The Cluster-Neuron Score Matrix (S) construction and temperature (τ) setting for reweighting are the most sensitive steps. If S is noisy or τ is wrong, the "function-aware" signal is lost.

- **Design tradeoffs:**
  - Group Count (K): Higher K increases functional granularity but reduces neurons per group, potentially making importance estimates unstable
  - Shared Group Size: Larger shared groups improve robustness but reduce effective compression ratio

- **Failure signatures:**
  - Catastrophic forgetting on specific tasks: suggests clustering failed to identify relevant context or neurons were assigned to aggressive sparsity groups
  - High Perplexity, Low Accuracy: indicates general language modeling capability was sacrificed; check if Shared Neuron Group was preserved

- **First 3 experiments:**
  1. **Sanity Check:** Run FANG with K=1 vs. K=7 on LLaMA-7B to verify grouping benefit on Perplexity and Average Accuracy
  2. **Ablation:** Compare "Only-Matched" weighting vs. proposed Softmax reweighting to confirm cross-context weighting necessity
  3. **Robustness Test:** Run FANG on Code-LLM using natural language calibration set to test if reweighting preserves coding capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can FANG be effectively extended to Multi-Head Attention layers?
- **Basis:** The authors state neuron functional grouping is applied only to FFN pruning, while MHA layers use standard baseline methods
- **Why unresolved:** The proposed mechanism relies on clustering tokens based on FFN hidden features, with no comparable mechanism defined for attention heads
- **What evidence would resolve it:** A study adapting clustering and re-weighting mechanisms to attention modules, demonstrating improved retention of attention-based functional roles

### Open Question 2
- **Question:** Can the computational bottleneck of context clustering be reduced without compromising pruning accuracy?
- **Basis:** Context clustering is identified as the most time-consuming step, taking approximately 1 hour for LLaMA2-7B
- **Why unresolved:** While PCA reduces dimensionality, K-Means clustering on large token sets remains the dominant cost
- **What evidence would resolve it:** Experiments showing approximate clustering methods achieve similar downstream accuracy with substantially reduced runtime

### Open Question 3
- **Question:** How sensitive is generalization capability to the specific choice of functional groups (K)?
- **Basis:** The implementation fixes K=7 based on balancing clustering efficiency and accuracy, but provides no ablation on varying K
- **Why unresolved:** Unclear if K=7 is optimal across different model sizes or if dynamic K would better capture functional complexity
- **What evidence would resolve it:** An ablation study varying K from 3 to 20 on multiple model sizes, correlating K with downstream task accuracy

## Limitations

- The functional grouping mechanism may create arbitrary partitions rather than capturing true neuron specialization, especially with small calibration sets
- The assumption that neurons exhibit functional specialization similar to human brain regions lacks strong empirical validation in LLMs
- The "Functional Complexity" metric based on cosine similarity between block inputs/outputs is not rigorously justified and may misallocate pruning budgets

## Confidence

- **High Confidence:** Empirical results showing FANG improves downstream accuracy (1.5%-8.5%) over baseline pruning methods are well-supported
- **Medium Confidence:** The mechanism of using semantic reweighting to preserve cross-context neurons is plausible but relies heavily on the specific reweighting formula
- **Low Confidence:** The foundational assumption that LLM neurons exhibit functional specialization similar to human brain regions is not well-established in the literature

## Next Checks

1. **Functional Grouping Validation:** Run ablation studies systematically varying K from 1 to 10, measuring downstream accuracy and perplexity to identify optimal grouping granularity

2. **Polysemanticity Test:** Design an experiment where a model is fine-tuned on two distinct tasks (e.g., code generation and narrative text), apply FANG with K=2 groups, and test if groups preserve distinct capabilities

3. **Complexity Metric Correlation:** Compute Functional Complexity across all blocks in multiple models and correlate it with established importance metrics to validate the adaptive sparsity allocation mechanism