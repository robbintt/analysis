---
ver: rpa2
title: 'SpecServe: Efficient and SLO-Aware Large Language Model Serving with Adaptive
  Speculative Decoding'
arxiv_id: '2503.05096'
source_url: https://arxiv.org/abs/2503.05096
tags:
- speculative
- decoding
- length
- draft
- adaspec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaSpec addresses the challenge of achieving low inference latency
  and meeting Service Level Objectives (SLOs) for large language model (LLM) services
  under dynamic request patterns. It introduces an adaptive speculative decoding framework
  that dynamically adjusts speculative strategies based on real-time request loads
  and system configurations.
---

# SpecServe: Efficient and SLO-Aware Large Language Model Serving with Adaptive Speculative Decoding

## Quick Facts
- **arXiv ID:** 2503.05096
- **Source URL:** https://arxiv.org/abs/2503.05096
- **Reference count:** 40
- **Primary result:** AdaSpec achieves 1.14×-14.3× speedups over state-of-the-art speculative inference systems while consistently meeting SLOs under dynamic request patterns.

## Executive Summary
SpecServe (AdaSpec) introduces an adaptive speculative decoding framework that dynamically adjusts speculative strategies to achieve low inference latency and meet Service Level Objectives (SLOs) for large language model (LLM) services. The system employs an efficiency model to predict speculative decoding performance, an adaptive drafter to control speculative length at the batch level, and a confidence prior verifier to optimize token verification. Experimental results on real-world LLM traces demonstrate that AdaSpec consistently meets SLOs and achieves substantial performance improvements compared to state-of-the-art speculative inference systems.

## Method Summary
AdaSpec is built on vLLM and uses an offline analyzer to profile performance coefficients (α, γ, δ) for the target and draft model pair. The online system consists of an Adaptive Drafter with a "predict-execute-correct" loop that estimates efficiency using historical confidence scores, a Confidence Prior Verifier that prunes low-probability tokens before verification to achieve per-request speculative lengths within a batch, and an SLO-Aware Estimator that enforces time constraints by preemptively rejecting speculative steps that risk exceeding latency limits. The system dynamically adjusts speculative length based on real-time request loads and system configurations to maximize throughput while adhering to SLO constraints.

## Key Results
- Achieves 1.14×-14.3× speedups over state-of-the-art speculative inference systems
- Consistently meets SLOs (P90 TPOT constraints) under dynamic request patterns
- Demonstrates effectiveness across three distinct production traces with varying request rates and batch arrivals

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Drafter Efficiency Control
The system maximizes inference throughput by dynamically halting draft token generation when marginal efficiency gain becomes negative. The Adaptive Drafter employs a "predict-execute-correct" loop where a Speculative Controller estimates the speculative acceptance rate using historical confidence scores and a time model. If predicted throughput is lower than the current state, it stops drafting to prevent wasted computation. This relies on draft model confidence scores serving as a reliable proxy for target model acceptance probability.

### Mechanism 2: Confidence Prior Verifier
The system enables per-request speculative lengths within a single batch by pruning low-probability tokens before verification. The Confidence Prior Verifier decouples drafting from verification and iteratively identifies and removes the token with the lowest estimated acceptance rate, ensuring only high-value tokens consume verification bandwidth. This assumes the computational cost of verifying low-confidence tokens outweighs the potential benefit if serendipitously accepted.

### Mechanism 3: SLO-Aware Efficiency Estimator
The system enforces SLOs by preemptively rejecting speculative steps that statistically risk exceeding latency limits. The SLO-Aware Efficiency Estimator calculates expected time for speculative decoding steps using a linear performance model. If estimated time exceeds the SLO constraint (TPOT limit), the estimator returns negative efficiency, forcing the system to reduce speculative length or fall back to safer strategies. This assumes offline-derived performance coefficients accurately predict runtime latency.

## Foundational Learning

- **Concept: Speculative Decoding (Draft-then-Verify)**
  - **Why needed:** This is the core primitive AdaSpec optimizes, where "drafting" is fast but prone to rejection while "verification" is slow but accurate.
  - **Quick check:** If the draft model generates tokens that are 100% rejected, is the system faster or slower than standard autoregressive decoding?

- **Concept: Time Per Output Token (TPOT)**
  - **Why needed:** Paper's SLO constraints are defined specifically around TPOT, distinct from Time to First Token (TTFT). The efficiency model optimizes this specific metric.
  - **Quick check:** Does a lower TPOT guarantee lower end-to-end latency for a single request if prefill is extremely high?

- **Concept: Token Acceptance Rate vs. Confidence**
  - **Why needed:** AdaSpec relies on the mathematical link between draft model's output probability (confidence) and likelihood of target model accepting that token to function without feedback loop.
  - **Quick check:** Why does the system drop tokens with low confidence scores before sending them to target model?

## Architecture Onboarding

- **Component map:** Adaptive Drafter (Speculative Controller + Draft Model) -> Confidence Prior Verifier (Draft Token Elimination + Target Model) -> SLO-Aware Estimator (utility function for Goodput calculation)

- **Critical path:** The Adaptive Drafter Loop must predict efficiency, execute draft, and correct prediction fast enough that overhead doesn't negate speedup of parallel verification. Verification phase (Target Model forward pass) remains primary latency bottleneck.

- **Design tradeoffs:**
  - **Batch Size vs. Speculative Length:** As batch size increases, verification compute rises quadratically (attention). AdaSpec compensates by shortening speculative length, trading per-request acceleration for higher aggregate throughput.
  - **Estimation Overhead vs. Accuracy:** Efficiency model is linear for speed. More complex models might be more accurate but too slow for critical path.

- **Failure signatures:**
  - **Low Acceptance Rate:** If draft model diverges from target (low acceptance), Confidence Prior Verifier will aggressively truncate drafts, reducing speedup to near 1.0x.
  - **SLO Cascades:** If time model underestimates latency, SLO estimator will allow overly long drafts, causing consistent SLO violations.
  - **Stagnation:** If SLO constraint is too strict for hardware capability, estimator may force speculative length to 0 (autoregressive), yielding no acceleration.

- **First 3 experiments:**
  1. **Coefficient Calibration:** Run offline analyzer on your specific deployment hardware to ensure time model (Eq. 5) reflects reality. Do not use default coefficients.
  2. **Correlation Check:** Visualize relationship between draft confidence scores and actual acceptance rates on your target workload. If correlation is weak, Confidence Prior Verifier logic may need tuning.
  3. **Load Test:** Vary batch size (1 to 128) and observe if AdaSpec dynamically shortens speculative length. If length remains static, efficiency estimator or adaptive drafter logic is failing.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in a dedicated section. However, the limitations section identifies several areas requiring further investigation, particularly around correlation robustness, time model accuracy, and batch heterogeneity handling under varying system conditions.

## Limitations
- Correlation between confidence scores and acceptance rates may degrade under domain shift, model fine-tuning, or adversarial inputs, potentially leading to suboptimal speculative lengths
- Linear time model may not accurately predict real-world execution time under memory bandwidth saturation, CUDA overhead, or GPU thermal throttling
- Computational overhead of token elimination algorithm for batch heterogeneity handling is not quantified, especially for large batch sizes

## Confidence
- **High Confidence:** Overall system architecture and existence of demonstrable performance improvement over baseline speculative decoding systems
- **Medium Confidence:** Specific mechanisms (Adaptive Drafter, Confidence Prior Verifier, SLO-Aware Estimator) working as described
- **Low Confidence:** System's robustness to extreme workloads or performance on model pairs and tasks not included in evaluation set

## Next Checks
1. **Correlation Robustness Test:** Conduct systematic analysis of draft confidence score versus target acceptance rate across broader range of tasks, model sizes, and domains. Report correlation coefficient (R) and variance for each configuration.

2. **Time Model Error Analysis:** Measure prediction error of linear performance model (Eq. 5) by running controlled experiments with fixed speculative lengths and comparing estimated time to actual wall-clock time. Report mean absolute error and R² value.

3. **Overhead Quantification of Token Elimination:** Profile computational cost of Confidence Prior Verifier's token elimination algorithm and measure contribution to total inference latency. Analyze overhead as function of batch size.