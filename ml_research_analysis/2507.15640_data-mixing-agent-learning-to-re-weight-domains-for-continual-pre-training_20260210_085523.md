---
ver: rpa2
title: 'Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training'
arxiv_id: '2507.15640'
source_url: https://arxiv.org/abs/2507.15640
tags:
- data
- target
- domain
- mixing
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Data Mixing Agent, a model-based framework
  that learns to re-weight domains for continual pre-training of large language models.
  The method addresses catastrophic forgetting by using reinforcement learning to
  parameterize data mixing heuristics through trajectory sampling and evaluation feedback.
---

# Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training

## Quick Facts
- arXiv ID: 2507.15640
- Source URL: https://arxiv.org/abs/2507.15640
- Reference count: 9
- Primary result: 3.02% average improvement across 8 general and 4 math reasoning benchmarks via domain re-weighting for continual pre-training

## Executive Summary
This paper introduces Data Mixing Agent, a reinforcement learning framework that learns to re-weight domains during continual pre-training of large language models to mitigate catastrophic forgetting. The method uses proxy models to generate trajectory data, then trains an agent with offline RL (CQL) to predict optimal domain distributions. Experiments show significant improvements over strong baselines across general and math benchmarks, with good generalization to unseen domains and model sizes without retraining.

## Method Summary
Data Mixing Agent formulates domain re-weighting as a Markov Decision Process where states represent current domain distributions and actions predict the next distribution. The method samples diverse trajectories using an inductive scoring algorithm, trains small 50M-parameter proxy models on these trajectories to generate rewards, and trains a 2.1M-parameter Transformer agent using Conservative Q-Learning on the offline dataset. The agent learns generalizable heuristics that can guide large-scale training (1.4B-3B parameters) without retraining.

## Key Results
- Achieves 3.02% average improvement across 8 general and 4 math reasoning benchmarks
- Generalizes well to unseen source fields, target models, and domain spaces without retraining
- Outperforms strong baselines including random sampling and heuristic-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Proxy-Target Heuristic Transfer
Small proxy models (50M parameters) exhibit similar relative domain preferences to larger target models, allowing an agent trained on proxy trajectories to generalize to large-scale training without retraining. The framework samples data mixing trajectories and trains cheap proxy models to generate (state, action, reward) tuples, decoupling agent training complexity from target model size.

### Mechanism 2: Conservative Q-Learning (CQL) for Distribution Stability
CQL prevents the agent from overestimating the value of untested, out-of-distribution data mixes that could cause training collapse. It adds a regularization term that penalizes the agent for assigning high Q-values to actions that differ significantly from the offline dataset, ensuring conservative, validated mixing strategies.

### Mechanism 3: Semantic Domain Granularity
Structuring the action space via semantic classifiers (e.g., Science, Health) rather than raw data sources allows finer control over catastrophic forgetting. By classifying source and target data into a shared high-dimensional domain space (52 dimensions), the agent can up-sample specific "general" domains that support the target capability while down-sampling irrelevant ones.

## Foundational Learning

- **Markov Decision Process (MDP):** The core abstraction mapping "Data Mixing" to (State: current distribution, Action: next distribution, Reward: benchmark score). Why needed: provides the formal framework for learning optimal mixing strategies. Quick check: Can you identify what constitutes the "State" and "Reward" in the agent's environment?

- **Offline Reinforcement Learning (CQL):** The paper uses Conservative Q-Learning because the agent cannot interact with the live evaluation environment during training. Why needed: prevents overestimation of Q-values for out-of-distribution actions. Quick check: Why would standard Q-learning fail if trained only on a fixed dataset of proxy model runs?

- **Catastrophic Forgetting:** The fundamental problem being solved - why simply training on target data degrades source capabilities. Why needed: motivates the need for domain re-weighting instead of simple fine-tuning. Quick check: Why does re-weighting data mixtures alleviate this compared to fine-tuning only on target data?

## Architecture Onboarding

- **Component map:** Domain Classifier -> Trajectory Sampler -> Proxy Loop (trains 50M models -> Evaluation Environment -> Feedback Database) -> Agent (2.1M param Transformer Actor + Critic) -> Inference (Agent predicts domain distribution -> Megatron-LM loads batch)

- **Critical path:** The Trajectory Sampling & Proxy Training phase is the computational bottleneck, requiring training hundreds of proxy models. If this data is flawed, the CQL phase cannot recover performance.

- **Design tradeoffs:** Proxy Size vs. Fidelity (larger proxies yield better heuristics but increase data collection cost), Dimensionality (52-dim space allows precise control but increases RL optimization difficulty compared to 2-dim mix)

- **Failure signatures:** Degenerate Policy (agent collapses to 100% weight on single domain), Feedback Noise (high variance in environment rewards leads to flat Q-function)

- **First 3 experiments:** 1) Proxy Correlation Test: verify Mix A > Mix B for proxy implies same relation for large model, 2) SFT vs. CQL Ablation: compare imitation learning vs. RL optimization, 3) Domain Space Ablation: run agent on 2-dim source vs. target space vs. 52-dim space

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the scale of the proxy model used for trajectory sampling influence the optimality of the learned re-weighting policy for larger target models? The authors use a 50M parameter proxy model yet apply the agent to models with 1.4B-3B parameters, but do not analyze sensitivity to proxy model size.

### Open Question 2
How can the agent's generalization be improved for semantically distinct target domains (e.g., coding vs. math) without requiring retraining? Section 4.3 notes degradation when applying math-trained agent to code generation, indicating target-field-dependent heuristics.

### Open Question 3
Does the lightweight evaluation environment based on per-token log probability accurately proxy for complex downstream task performance? The paper assumes correlation between this proxy reward and downstream benchmarks but does not verify if this signal introduces optimization biases.

## Limitations
- Proxy model scaling relationship with target models is not empirically validated across full range of domain mixes
- Requires extensive offline proxy training (27,266 runs) which is computationally expensive
- Domain classifier accuracy in capturing semantically distinct regions remains unverified
- Evaluation focuses on math and general reasoning benchmarks, leaving questions about other downstream tasks

## Confidence

- **High Confidence:** Baseline methodology (proxy training + CQL) is technically sound and aligns with established offline RL practices. 3.02% average improvement metric is clearly reported and reproducible.
- **Medium Confidence:** Generalization claims to unseen fields and models are supported by ablation studies but lack extensive cross-domain validation. Semantic domain granularity benefit is plausible but not conclusively proven.
- **Low Confidence:** Proxy-transfer assumption across scales is the weakest link; provides theoretical justification but limited empirical scaling validation. Computational cost-benefit tradeoff is not rigorously analyzed.

## Next Checks

1. **Scaling Validation:** Train proxy models at 3 different scales (25M, 100M, 500M) and measure correlation decay in preference rankings across same trajectory sets to quantify proxy fidelity loss.

2. **Domain Classifier Ablation:** Replace the Nvidia classifier with random domain assignment and measure performance drop to isolate impact of semantic domain structure versus raw trajectory diversity.

3. **CQL Hyperparameter Sweep:** Systematically vary the CQL conservatism penalty (Î±) and measure tradeoff between exploration and stability to confirm reported settings are optimal rather than arbitrary.