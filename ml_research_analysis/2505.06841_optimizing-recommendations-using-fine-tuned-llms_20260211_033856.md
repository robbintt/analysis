---
ver: rpa2
title: Optimizing Recommendations using Fine-Tuned LLMs
arxiv_id: '2505.06841'
source_url: https://arxiv.org/abs/2505.06841
tags:
- data
- user
- synthetic
- datasets
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving movie recommendations
  by enabling more expressive and natural user queries beyond traditional keyword-based
  search. The authors generate synthetic datasets that model complex user interactions,
  allowing for queries involving mood, plot details, and thematic elements.
---

# Optimizing Recommendations using Fine-Tuned LLMs

## Quick Facts
- arXiv ID: 2505.06841
- Source URL: https://arxiv.org/abs/2505.06841
- Reference count: 19
- Primary result: Llama 3.2 3B fine-tuned with QLoRA achieves perfect Macro-F1 (1.0) for movie title extraction and 0.9935 for intent classification on synthetic conversational queries

## Executive Summary
This paper addresses the challenge of improving movie recommendations by enabling more expressive and natural user queries beyond traditional keyword-based search. The authors generate synthetic datasets that model complex user interactions, allowing for queries involving mood, plot details, and thematic elements. They fine-tune smaller, efficient LLMs (Llama 3.2 3B) using LoRA and QLoRA techniques to optimize intent classification and entity extraction tasks. The fine-tuned models achieved a perfect Macro-F1 score of 1.0 for movie title extraction and a 1.3% improvement in intent classification accuracy (0.9935 Macro-F1) compared to the base model.

## Method Summary
The authors generate 20,000 synthetic conversational prompts using Llama 3.1 405B constrained by knowledge graphs from movie datasets (TMDB, Netflix, Amazon). These prompts undergo grammar extension and rotating seed data for diversity. They fine-tune Llama 3.2 3B using QLoRA (4-bit quantization + LoRA adapters) on two tasks: binary intent classification ("rec" vs. "non_rec") and entity extraction (titles, genres, themes, plots). The fine-tuning targets q_proj, v_proj, and o_proj layers in transformer blocks, trained on single A10/L4 GPUs with JSON-formatted output templates.

## Key Results
- Fine-tuned Llama 3.2 3B achieves perfect Macro-F1 (1.0) for movie title extraction
- Intent classification Macro-F1 improves to 0.9935 (vs. base 0.9805)
- QLoRA enables training on single A10/L4 GPUs with 4-bit quantization
- Base NER models (BERT-based) perform poorly (0.22-0.33 F1) compared to generative approach

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Generation Grounded in Knowledge Graphs
Large teacher models (405B parameters) generate diverse, domain-specific conversational queries when constrained by structured knowledge graphs and grammar rules. Llama 3.1 405B with knowledge graphs produces 20,000 synthetic prompts. Grammar extension and rotating seed data introduce linguistic diversity while ontological anchors (titles, genres, plots from TMDB, Netflix, Amazon datasets) constrain outputs to realistic movie-related queries. Synthetic data generated by large models with proper constraints approximates the distribution of real user queries well enough to improve downstream task performance.

### Mechanism 2: Parameter-Efficient Fine-Tuning via QLoRA
QLoRA enables training capable models on constrained hardware while preserving task performance through 4-bit quantization and low-rank adapter matrices. QLoRA freezes the base model weights and introduces trainable rank-decomposition matrices (A and B) into transformer layers. 4-bit NormalFloat (NF4) quantization, double quantization, and paged optimizers reduce memory enough to train Llama 3B on single A10/L4 GPUs. Gradients backpropagate through frozen 4-bit model into adapters. The low-rank adaptation captures sufficient task-specific information without modifying the full parameter space.

### Mechanism 3: Task Decomposition into Intent Classification and Entity Extraction
Decomposing conversational search into discrete structured tasks (intent routing + entity extraction) enables smaller models to achieve near-perfect performance on each subtask. Two separate fine-tuning tasks: (1) binary intent classification ("rec" vs. "non_rec") and (2) entity extraction (titles, genres, themes, plots). Task-specific prompt templates with JSON output formatting ensure consistent, parseable responses. Smaller 3B model achieves 1.0 Macro-F1 on entity extraction and 0.9935 on intent classification. The recommendation pipeline can be effectively modularized into understanding (intent/entities) and retrieval (handled separately via SBERT/embeddings).

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Core technique enabling parameter-efficient fine-tuning. Without understanding rank decomposition and adapter matrices, cannot debug training issues or select appropriate target layers.
  - Quick check question: Given a transformer with hidden dimension 4096, what is the parameter reduction if LoRA uses rank r=8 for q_proj and v_proj?

- **Concept: Quantization-Aware Training**
  - Why needed here: QLoRA's 4-bit quantization is memory-critical for training on constrained GPUs. Understanding NF4, double quantization, and precision tradeoffs informs hardware selection.
  - Quick check question: Why does NF4 outperform standard 4-bit integer quantization for normally distributed weights?

- **Concept: Named Entity Recognition (NER) vs. Generative Extraction**
  - Why needed here: Paper shows generative LLMs outperform traditional NER (BERT-based) for movie title extraction (1.0 vs 0.22-0.33 F1). Understanding this gap informs architectural decisions.
  - Quick check question: Why might a generative model with structured JSON output outperform a token-classification NER model for domain-specific entity extraction?

## Architecture Onboarding

- **Component map:** [Knowledge Graphs + Movie Datasets] → [Llama 3.1 405B] → Synthetic Prompts (20K) → [Prompt Templates + System Instructions] → [Llama 3.2 3B Base] ← [QLoRA Adapters] ← [Fine-tuning on Intent/Entity Tasks] → [Intent Classifier] → Route → [Entity Extractor] → [Retrieval System (SBERT/Embeddings)]

- **Critical path:** Synthetic data quality → Template formatting consistency → QLoRA target layer selection → JSON output reliability. Errors in early stages compound; poor synthetic data cannot be fixed by fine-tuning.

- **Design tradeoffs:**
  - 405B teacher vs. cost: High-quality synthetic data but expensive generation
  - 3B student vs. accuracy: Fast inference (<500ms target) but limited contextual depth for generation
  - LoRA rank vs. expressiveness: Lower rank = faster training but may underfit complex patterns
  - Macro-F1 vs. per-class accuracy: Optimizes for balanced performance but may not reflect production class imbalance

- **Failure signatures:**
  - Hallucinated entities in synthetic data (wrong director-movie pairings)
  - Inconsistent JSON format in model outputs (base model "ignored" system instructions)
  - Training loss plateaus early (rank too low or wrong target layers)
  - Base NER models fail on movie titles (0.22-0.33 F1) → indicates domain mismatch

- **First 3 experiments:**
  1. Baseline validation: Run base Llama 3.2 3B with system prompts on 100 held-out queries; measure JSON parse success rate and entity extraction accuracy before fine-tuning.
  2. Layer ablation: Train separate LoRA adapters targeting different layer combinations (q_proj only vs. q_proj+v_proj+o_proj) to validate the grid search finding.
  3. Synthetic data quality audit: Manually inspect 50 randomly sampled synthetic prompts for hallucinations, attribute compatibility, and grammatical coherence before training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating Direct Preference Optimization (DPO) and Reward Model (RM) techniques improve the quality and alignment of synthetic data generation compared to the current SFT-only approach?
- Basis in paper: Section II notes that MAGPIE's "integration of DPO and RM has shown promising results, suggesting potential avenues for further improving synthetic data generation."
- Why unresolved: The authors used only Supervised Fine-Tuning; they did not explore DPO or RM despite acknowledging their potential.
- What evidence would resolve it: Comparative evaluation of models trained on DPO/RM-refined synthetic data versus SFT-only data, measured on intent classification and entity extraction benchmarks.

### Open Question 2
- Question: How robust is the fine-tuned model against adversarial prompts designed to exploit weaknesses in entity extraction or intent classification?
- Basis in paper: Section VII calls for "synthetic datasets that incorporate adversarial prompts" to "evaluate the robustness of the retrieval system and its ability to handle challenging queries."
- Why unresolved: Current evaluation uses benign synthetic test data; no adversarial testing was conducted.
- What evidence would resolve it: Macro-F1 scores on adversarial test sets containing ambiguous, misleading, or exploit-targeted queries.

### Open Question 3
- Question: How does the model perform on noisy, incomplete user inputs or queries containing multiple intents?
- Basis in paper: Section VII states future work should simulate "real-world scenarios where user inputs are noisy, incomplete, or contain multiple intents" to improve generalization.
- Why unresolved: All synthetic data assumes well-formed queries; no corruption or multi-intent examples were included in training or evaluation.
- What evidence would resolve it: Performance degradation curves across controlled noise levels (typos, missing fields, mixed intents) in synthetic test data.

### Open Question 4
- Question: Does high performance on synthetic benchmarks translate to real user interactions on actual streaming platforms?
- Basis in paper: The authors acknowledge "there is a lack of such datasets" because streaming companies do not yet deploy AI assistants. All training and evaluation relies entirely on synthetic data.
- Why unresolved: The perfect 1.0 entity extraction and 0.9935 intent classification scores were measured on synthetic test sets that may share distributional biases with training data.
- What evidence would resolve it: Deployment metrics from A/B testing with real users, including query success rates, recommendation acceptance, and user satisfaction scores.

## Limitations

- Synthetic data quality may not fully capture real user query distributions, lacking validation against actual user search logs
- Binary intent classification ("rec" vs. "non_rec") oversimplifies complex user queries that often contain multiple intents
- Hardware constraints and scalability remain unproven; claimed <500ms inference targets lack empirical validation

## Confidence

- **High Confidence:** QLoRA implementation with 4-bit quantization and target layers (q_proj, v_proj, o_proj) follows established practices; reported Macro-F1 scores are internally consistent
- **Medium Confidence:** Synthetic data generation effectiveness supported by results but lacks direct comparison with real user data or alternative approaches
- **Low Confidence:** Task decomposition generalizability to complex scenarios and QLoRA scalability for production inference remain unproven

## Next Checks

1. Test fine-tuned models on actual user search logs from deployed recommendation systems to measure performance degradation versus synthetic test sets
2. Design and evaluate test cases containing multiple intents to assess whether binary classification breaks down and identify failure patterns
3. Measure actual inference latency and memory usage on production hardware configurations with varying query loads to validate claimed <500ms response times and assess scalability limitations