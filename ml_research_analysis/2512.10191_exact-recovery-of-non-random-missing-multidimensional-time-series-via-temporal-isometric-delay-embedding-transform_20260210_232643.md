---
ver: rpa2
title: Exact Recovery of Non-Random Missing Multidimensional Time Series via Temporal
  Isometric Delay-Embedding Transform
arxiv_id: '2512.10191'
source_url: https://arxiv.org/abs/2512.10191
tags:
- tensor
- temporal
- time
- hankel
- recovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of exact recovery of multidimensional
  time series data with non-random missing patterns, which is a common yet challenging
  issue in various real-world domains. Existing low-rank tensor completion methods
  are primarily designed for random missing data and fail to handle non-random missingness
  effectively.
---

# Exact Recovery of Non-Random Missing Multidimensional Time Series via Temporal Isometric Delay-Embedding Transform

## Quick Facts
- **arXiv ID:** 2512.10191
- **Source URL:** https://arxiv.org/abs/2512.10191
- **Authors:** Hao Shu; Jicheng Li; Yu Jin; Ling Zhou
- **Reference count:** 40
- **Key outcome:** Introduces LRTC-TIDT method for exact recovery of multidimensional time series with non-random missing patterns using temporal Hankel transform and theoretical guarantees.

## Executive Summary
This paper addresses the challenge of recovering multidimensional time series data with non-random missing patterns, which commonly occur in real-world domains like network traffic and urban monitoring. Existing low-rank tensor completion methods primarily target random missing data and fail when large contiguous segments are missing. The authors propose a novel method called LRTC-TIDT that combines a Temporal Isometric Delay-embedding Transform (TIDT) with tensor nuclear norm minimization under the t-SVD framework.

The method constructs a Hankel tensor through TIDT that naturally exhibits low-rank structure when the underlying time series is smooth or periodic. This transformation converts problematic segment-missing scenarios into more tractable random-like missing problems within the transformed tensor. The paper establishes theoretical guarantees showing exact recovery is achievable under specific non-random sampling conditions and mild incoherence assumptions, validated through extensive simulations and real-world applications including network flow reconstruction, urban traffic estimation, and temperature field prediction.

## Method Summary
The LRTC-TIDT method addresses non-random missing patterns in multidimensional time series by first applying a Temporal Isometric Delay-embedding Transform (TIDT) to restructure the data into a Hankel tensor format. This transform leverages the inherent smoothness or periodicity of time series data to create a low-rank structure in the transformed space. The method then solves a tensor nuclear norm minimization problem using the t-SVD framework, specifically employing Alternating Direction Method of Multipliers (ADMM) with tensor singular value thresholding. The approach shifts from probabilistic sampling assumptions to deterministic metrics based on minimum temporal sampling rates, enabling exact recovery guarantees for non-random missing patterns under specific theoretical conditions.

## Key Results
- LRTC-TIDT achieves exact recovery of non-random missing multidimensional time series under prescribed sampling conditions and incoherence assumptions
- Theoretical guarantees establish that minimum temporal sampling rate must exceed bounds dependent on tensor rank, incoherence, and time length
- Method consistently outperforms existing tensor-based methods (HaLRTC, TNN) across network flow reconstruction, urban traffic estimation, and temperature field prediction tasks
- Simulation experiments validate phase transition behavior aligning with theoretical recovery bounds for various non-random missing patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Temporal Isometric Delay-embedding Transform (TIDT) mitigates non-random missing patterns by restructuring the data such that the resulting Hankel tensor is low-rank if the original time series is smooth or periodic.
- **Mechanism:** TIDT applies a delay embedding along the temporal mode, constructing a Hankel tensor $\mathcal{H}_k(M)$. Unlike standard transforms, TIDT is isometric (norm-preserving). Theoretically, Lemmas 3.1 and 3.2 (and Propositions B.1/B.2) demonstrate mathematically that temporal smoothness ($\eta(M)$) or periodicity ($\beta_\tau(M)$) bounds the rank-approximation error of the transformed tensor. This structural change converts a "segment missing" problem (fatal for standard low-rank methods) into a "random-like" missing problem within the transformed tensor slices.
- **Core assumption:** The underlying time series data possesses intrinsic temporal smoothness or periodicity.
- **Evidence anchors:**
  - [abstract] "constructs a Hankel tensor whose low-rankness is naturally induced by the smoothness and periodicity of the underlying time series."
  - [section 3.2] Defines TIDT and explicitly links smoothness to low-rankness via Propositions B.1/B.2.
  - [corpus] Related work "Guaranteed Multidimensional Time Series Prediction..." confirms the broader viability of deterministic tensor completion theories, though specific TIDT proofs are internal to this paper.
- **Break condition:** The time series lacks autocorrelation (e.g., white noise) or contains discontinuities/jumps that violate the smoothness assumption, preventing the Hankel tensor from being low-rank.

### Mechanism 2
- **Claim:** Exact recovery is achievable under non-random sampling provided the "Minimum Temporal Sampling Rate" exceeds a theoretical bound derived from the tensor's intrinsic rank.
- **Mechanism:** The paper shifts from probabilistic sampling assumptions to a deterministic metric: $\rho(\Omega)$ (the minimum sampling rate across individual time series vectors). Theorem 4.1 establishes that if $\rho(\Omega)$ satisfies condition (4.4) (dependent on rank $r$, incoherence $\mu$, and time length $t$), the missing entries are uniquely identifiable. The TIDT ensures that even if a specific time slice is fully missing, the temporal sampling rate of the underlying vector remains sufficient to reconstruct the Hankel structure.
- **Core assumption:** The temporal Hankel tensor satisfies specific incoherence conditions (Definition 4.3), ensuring the energy of singular vectors isn't concentrated in a few entries.
- **Evidence anchors:**
  - [section 4.3] Theorem 4.1 states: "If $\rho(\Omega) > 1 - k / (2\mu r(r_s+1)t)$ ... then $M$ is the unique solution."
  - [section 6.1] Simulation results in Fig. 5 show phase transitions aligning with the theoretical boundary between success and failure.
  - [corpus] "Robust Tensor Principal Component Analysis..." aligns with the deterministic modeling approach, validating the move away from random sampling assumptions.
- **Break condition:** The sampling rate drops below the theoretical threshold, or the data is "incoherent" (e.g., highly spiky or localized), causing the convex relaxation to fail.

### Mechanism 3
- **Claim:** The Tensor Nuclear Norm (TNN) within the t-SVD framework serves as a tight convex surrogate for the rank of the Hankel tensor, enabling efficient optimization via ADMM.
- **Mechanism:** The method minimizes $\|\mathcal{H}_k(X)\|_{\mathchar 1406\relax}$ (Tensor Nuclear Norm) subject to data fidelity. This norm is the dual of the tensor spectral norm in the t-SVD framework. The optimization uses an Alternating Direction Method of Multipliers (ADMM) loop, specifically employing Tensor Singular Value Thresholding (t-SVT) (Eq 5.8) to solve the sub-problems efficiently in the Fourier domain.
- **Core assumption:** The tubal-rank (t-SVD rank) is the appropriate definition of low-rankness for this data structure (as opposed to CP or Tucker rank).
- **Evidence anchors:**
  - [section 2.1] Defines the t-product and tensor nuclear norm.
  - [section 5.1] Eq 5.8 derives the closed-form solution via t-SVT.
  - [corpus] "Fourier Low-rank and Sparse Tensor..." corroborates the effectiveness of TNN in Fourier space for tensor completion.
- **Break condition:** Computational instability if the scale coefficient $k$ is set too large relative to the time dimension $t$, or if the ADMM penalty parameters are poorly tuned, leading to non-convergence.

## Foundational Learning

- **Concept: Hankel Matrix / Delay Embedding**
  - **Why needed here:** The core of LRTC-TIDT is transforming the time series into a Hankel structure. You must understand that a 1D vector $[x_1, ..., x_t]$ becomes a 2D matrix where column $j$ is $[x_j, x_{j+1}... ]$.
  - **Quick check question:** If a time series is constant ($x_i = c$), what is the rank of its Hankel matrix? (Answer: 1).

- **Concept: t-SVD and Tubal Rank**
  - **Why needed here:** This paper uses a specific definition of tensor rank (t-SVD) different from standard matrices. It relies on the "t-product" (convolution in time $\to$ multiplication in Fourier frequency).
  - **Quick check question:** How does the t-product differ from standard matrix multiplication? (Hint: It uses circular convolution).

- **Concept: Incoherence Conditions**
  - **Why needed here:** The theoretical guarantees rely on the tensor not being "spiky." Incoherence measures how spread out the information is across the tensor.
  - **Quick check question:** Why is low incoherence necessary for exact recovery guarantees? (Hint: If information is concentrated in one row and that row is missing, recovery is impossible).

## Architecture Onboarding

- **Component map:** Input -> TIDT Operator -> Hankel Tensor -> ADMM Loop (t-SVT) -> Inverse Hankel -> Output

- **Critical path:** The ADMM update steps (Eq 5.8 and 5.10) are the computational bottleneck. Specifically, the t-SVT requires an FFT along the last $d-2$ modes and matrix SVD on the frontal slices.

- **Design tradeoffs:**
  - **Window size $k$:** Larger $k$ increases the size of the transformed tensor (computational cost $O(t k \dots)$) but may improve redundancy. The paper suggests setting $k$ related to the periodicity.
  - **Symmetric Padding:** Section 6.6.1 discusses symmetric padding for non-cyclical data. This improves performance but slightly increases data size.

- **Failure signatures:**
  - **Trivial Zero Solution:** If standard LRTC is used without TIDT on segment-missing data, output is zero. If TIDT is used but sampling rate is too low, output may be garbage.
  - **Boundary Artifacts:** If data start/end points are discontinuous, TIDT (which assumes circular wrap-around) may introduce artifacts unless Symmetric Padding is used.
  - **Slow Convergence:** If ADMM parameter $\mu$ is not updated dynamically (as in Algorithm 5.1), convergence may stall.

- **First 3 experiments:**
  1. **Validation of Theorem 4.1:** Generate synthetic tensor with known rank and smoothness (Eq 6.1). Test recovery success rates against varying sampling rate $\rho$ and rank $r$ to replicate the phase transition diagram (Fig 5).
  2. **Ablation on Transform:** Compare "Standard" vs. "Symmetric Padding" on a real dataset (e.g., Abilene) to quantify the impact of boundary handling (Table 5).
  3. **Non-Random Pattern Robustness:** Test the model on "Pattern-1" (Segment Missing) vs. "Pattern-2" (Striped Missing) to verify that TIDT handles structural missingness better than baselines like HaLRTC or TNN.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the model be adapted to effectively recover time series that lack strict temporal smoothness or periodicity?
- **Basis in paper:** [explicit] The conclusion states that the temporal Hankel low-rank property relies on smoothness/periodicity, and the model "may become less effective" without them, explicitly motivating "learning-based norm minimization approaches."
- **Why unresolved:** The current theoretical bounds (Lemmas 3.1 and 3.2) explicitly link low-rankness to small smoothness ($\eta(m)$) or periodicity ($\beta_{\tau}(m)$) parameters.
- **What evidence would resolve it:** A modified norm or transform that provides theoretical recovery guarantees for chaotic or discontinuous data, along with empirical validation on non-smooth datasets.

### Open Question 2
- **Question:** Is there a theoretically optimal or data-adaptive method for selecting the scale coefficient $k$?
- **Basis in paper:** [inferred] While Theorem 4.1 provides a recovery condition dependent on $k$, the experiments (Section 6) arbitrarily select fixed values ($k=50, 20$) without a data-driven selection rule.
- **Why unresolved:** The paper does not provide a heuristic or algorithm to determine the optimal delay-embedding dimension $k$ relative to the time series length $t$ or estimated rank $r$.
- **What evidence would resolve it:** An analysis showing the sensitivity of the Minimum Temporal Sampling Rate $\rho(\Omega)$ to $k$, or a proposed algorithm that selects $k$ based on observed data properties.

### Open Question 3
- **Question:** Can the exact recovery conditions be refined to distinguish between different non-random missing geometries?
- **Basis in paper:** [inferred] Section 6.1 observes that "segment missing" (Pattern-1) poses "greater challenges" and has a smaller recovery region than other patterns, despite the theory treating all patterns equally via the Minimum Temporal Sampling Rate $\rho(\Omega)$.
- **Why unresolved:** The theoretical bound in Theorem 4.1 relies only on the scalar $\rho(\Omega)$, failing to capture why the spatial distribution of missing entries (e.g., one large gap vs. scattered points) affects difficulty.
- **What evidence would resolve it:** A refined version of Theorem 4.1 that includes a term for the maximum consecutive missing interval, providing a tighter bound for segment missing scenarios.

## Limitations
- **Smoothness assumption vulnerability:** The method's effectiveness depends critically on the underlying time series being smooth or periodic; performance may degrade significantly for discontinuous or chaotic data.
- **Computational scalability concerns:** The ADMM-based optimization involving tensor SVDs in Fourier domain may face practical limitations for very large-scale spatiotemporal tensors.
- **Rank and incoherence estimation challenges:** Theoretical guarantees require accurate estimation of tensor rank and incoherence parameters in real-world data, which is non-trivial and may lead to conservative bounds.

## Confidence
- **High Confidence:** The mechanism by which TIDT restructures data to induce low-rank Hankel tensors (Mechanism 1) is mathematically sound and supported by both theory and simulation. The use of TNN and t-SVD for optimization (Mechanism 3) is well-established in the tensor completion literature.
- **Medium Confidence:** The deterministic sampling theory (Mechanism 2) is rigorous, but its practical applicability depends on accurate estimation of rank and incoherence in real data, which is non-trivial. The phase transition results (Fig. 5) support the theory but are based on synthetic data.
- **Low Confidence:** The generalizability of the smoothness/periodicity assumption to all real-world time series (e.g., financial, event-driven data) is not demonstrated. The paper does not explore failure modes for highly non-smooth or non-stationary signals.

## Next Checks
1. **Real-World Rank/Incoherence Estimation:** Apply the method to a diverse set of real-world datasets (e.g., financial, sensor, event logs) and empirically estimate the Hankel tensor's rank and incoherence. Compare these to the theoretical bounds required for exact recovery.
2. **Stress Test on Non-Smooth Data:** Generate synthetic time series with abrupt jumps, spikes, or discontinuities. Apply LRTC-TIDT and quantify reconstruction error and boundary artifacts. Compare against baseline methods to assess robustness.
3. **Scalability Benchmark:** Scale up the problem size (e.g., 1000+ time points, high spatial resolution) and measure runtime, memory usage, and convergence behavior. Identify practical limits for deployment on large spatiotemporal datasets.