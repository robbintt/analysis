---
ver: rpa2
title: 'HCAST: Human-Calibrated Autonomy Software Tasks'
arxiv_id: '2503.17354'
source_url: https://arxiv.org/abs/2503.17354
tags:
- tasks
- task
- experience
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HCAST is a benchmark of 189 tasks across machine learning, cybersecurity,
  software engineering, and general reasoning, with human baselines totaling over
  1500 hours from 140 skilled professionals. The benchmark groups tasks into 78 families,
  with time estimates ranging from 1 minute to 8+ hours, and evaluates AI agents on
  success rates relative to human completion times.
---

# HCAST: Human-Calibrated Autonomy Software Tasks

## Quick Facts
- arXiv ID: 2503.17354
- Source URL: https://arxiv.org/abs/2503.17354
- Reference count: 40
- Key result: Current agents succeed 70-80% on tasks taking humans <1 hour, but <20% on tasks taking >4 hours

## Executive Summary
HCAST is a benchmark of 189 tasks across machine learning, cybersecurity, software engineering, and general reasoning, with human baselines totaling over 1500 hours from 140 skilled professionals. The benchmark groups tasks into 78 families, with time estimates ranging from 1 minute to 8+ hours, and evaluates AI agents on success rates relative to human completion times. Results show current frontier agents succeed 70-80% on tasks taking humans less than one hour, but less than 20% on tasks exceeding four hours. Tasks are designed to require novel solutions and multi-step reasoning, with quality assurance including human and agent QA runs. The benchmark addresses the gap between "expert-level" AI performance and real-world autonomous capability by grounding AI performance in human time estimates.

## Method Summary
The benchmark uses the Vivaria platform to host both human and agent task runs, with tasks consisting of instruction strings, containerized environments, and algorithmic scoring functions. Two agent scaffolds are employed: Modular (single command generation, ReAct-style) and Triframe (multi-command proposal with scoring). Human baselines are collected from 140 professionals who complete qualification tasks, then work via SSH with web access under time limits. Time estimates are generated as geometric means of successful baseline times, with forecasted estimates for tasks lacking successful completions. Quality assurance includes manual transcript review to catch reward hacking and illegitimate success modes.

## Key Results
- Agents achieve 70-80% success on tasks taking humans less than one hour
- Success drops below 20% for tasks taking humans more than four hours
- Sharp performance cliff exists between 1-4 hour and 4+ hour task categories
- Failure modes include premature abandonment, poor error recovery, and resource mismanagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human time calibration converts abstract benchmark scores into interpretable real-world capability estimates.
- Mechanism: By having 140 skilled humans complete tasks under identical conditions as AI agents, the benchmark maps AI success rates to the question "can an agent be trusted to complete a task that would take a human X hours?"
- Core assumption: Human completion time on these tasks correlates with real-world task difficulty and economic value.
- Evidence anchors:
  - [abstract] "Measuring the time tasks take for humans provides an intuitive metric for evaluating AI capabilities, helping answer the question 'can an agent be trusted to complete a task that would take a human X hours?'"
  - [Section 1] "To make it easier to directly draw conclusions from the scores AI agents achieve on HCAST, we have 140 people skilled in relevant domains make 563 attempts to complete the tasks."
  - [corpus] Related work "Measuring AI Ability to Complete Long Tasks" proposes similar time-horizon metrics, suggesting convergent validity for this approach.

### Mechanism 2
- Claim: Task diversity and novelty requirements reduce data contamination and reward-hacking risks.
- Mechanism: 78 task families across four domains, with most tasks having private solutions and novel problem statements, prevent models from relying on memorization. Manual transcript review catches agents succeeding through exploitation rather than legitimate problem-solving.
- Core assumption: Models cannot easily transfer memorized solutions to genuinely novel task variants.
- Evidence anchors:
  - [Section 2.1] "Require novel solutions to open-ended problems, to avoid concerns about memorization and data contamination."
  - [Section 2.3] "When agents succeed, we confirm that they succeeded for legitimate reasons, as opposed to getting lucky, or exploiting bugs in their environment or in the task's scoring logic."
  - [corpus] SWE-Bench and similar benchmarks face contamination issues where models perform better on tasks created before training cutoffs; HCAST's from-scratch task creation explicitly mitigates this.

### Mechanism 3
- Claim: Performance degradation with task duration reveals a fundamental capability cliff rather than gradual scaling.
- Mechanism: The sharp drop from 70-80% success on <1-hour tasks to <20% on >4-hour tasks suggests current agents lack reliable long-horizon planning, error recovery, and resource management—not just scaled-up versions of shorter-task skills.
- Core assumption: Task duration correlates with required planning depth, not just more of the same operations.
- Evidence anchors:
  - [abstract] "Current agents succeed 70-80% of the time on tasks that take humans less than one hour, and less than 20% of the time on tasks that take humans more than 4 hours."
  - [Section 4.2] "Agent performance consistently decreases as the task difficulty estimate increases, with a significant performance drop between the 15 minute to 1 hour bucket and the 1 to 4 hour bucket."
  - [Section D.1] Qualitative failure modes include "premature task abandonment," "error recovery and strategic adaptation" failures, and "resource management" issues on complex tasks.

## Foundational Learning

- Concept: **Agentic AI systems**
  - Why needed here: HCAST evaluates "agents," not just models—understanding the distinction between a foundation model and an agent scaffold with tool access is essential.
  - Quick check question: Can you explain why GPT-4o scores differently when placed in the Modular scaffold versus being queried directly?

- Concept: **Benchmark contamination and memorization**
  - Why needed here: The paper explicitly designs tasks to avoid contamination; understanding why memorization undermines evaluation validity clarifies the task-design choices.
  - Quick check question: Why would a model that memorized SWE-Bench solutions still struggle on HCAST's "hash collision" task?

- Concept: **Time-horizon metrics for capability evaluation**
  - Why needed here: The core innovation is grounding capability in human time; understanding this metric's interpretation is central to using HCAST results.
  - Quick check question: If an agent achieves 50% success on 2-hour tasks, what does this predict about its reliability on 6-hour tasks?

## Architecture Onboarding

- Component map: Vivaria platform -> Task structure (instruction + container + scoring) -> Agent scaffolds (Modular/Triframe) -> Human baseline pipeline -> Quality assurance

- Critical path: Task creation -> Human QA run -> Agent QA run (5x per task) -> Manual transcript review -> Human baseline collection -> Time estimate generation

- Design tradeoffs:
  - Withholding vs. releasing tasks: Withholding 178/189 tasks prevents contamination but limits community scrutiny; releasing 11 examples provides transparency without compromising evaluation integrity.
  - Baseline success conditioning: Using only successful baseline times provides cleaner estimates but may systematically bias longer-task estimates upward (harder tasks have fewer successful baselines).
  - Human web access: Allowing baseliners external web access avoids artificial hindrances but creates non-identical conditions with agents (though models likely have equivalent memorized knowledge).

- Failure signatures:
  - Reward hacking: Agent succeeds by exploiting scoring function bugs (caught via transcript review)
  - Premature abandonment: Agent submits known-incorrect answers despite remaining resources (observed in "password check" runs)
  - Looping without adaptation: Agent repeats failed approaches without strategic pivots (observed in "eliciting secret" runs)
  - Resource mismanagement: Agent exhausts token budget without reaching solution (observed when scaffold warns of budget limits)

- First 3 experiments:
  1. Run the Modular scaffold on the 11 public example tasks with 5 repetitions each; verify you can reproduce the reported action-count distributions.
  2. Implement a simple ReAct agent and compare its success-rate-by-duration curve against the reported Claude 3.7 Sonnet results; identify which failure modes dominate at each duration bucket.
  3. Select one task family with successful human baselines and one with only forecasted times; attempt both as a human baseliner to calibrate your intuition for the time-estimate methodology and its limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can improved scaffolding and prompting close the performance gap between AI agents and humans on tasks requiring over four hours?
- Basis in paper: [explicit] The authors state in Section 4.2 that "better prompting and scaffolding is likely to improve performance" and "Future work will be necessary to determine the full capabilities of current frontier models."
- Why unresolved: Current results show a sharp drop in success rates for long tasks (<20%), but it is unclear if this is a fundamental model limitation or a deficiency in the current agent scaffolds.
- What evidence would resolve it: Evaluation of frontier models using advanced, planning-heavy scaffolds specifically designed for long-horizon tasks, compared against the current baseline.

### Open Question 2
- Question: What is the correlation between the number of actions (steps) an agent takes and the probability of successful task completion?
- Basis in paper: [explicit] Section 4.3 notes that "Understanding the relationship between tasks and iteration/exploration is an exciting direction for future research."
- Why unresolved: The paper reports the distribution of action counts but does not analyze if high action counts correlate with specific failure modes or if there is an optimal "exploration" range for different task difficulties.
- What evidence would resolve it: A statistical analysis linking action counts to success rates across different task families, identifying if "over-exploration" or "under-exploration" is a primary failure mode.

### Open Question 3
- Question: Does high performance on automatically scored, solitary HCAST tasks predict success on real-world tasks that lack clear success criteria or require human collaboration?
- Basis in paper: [inferred] Section 5.1 highlights that HCAST tasks are "unusually easy to evaluate" compared to natural work and are "solitary," lacking the human interaction present in most economic work.
- Why unresolved: It is unclear if the skills measured by HCAST transfer to ambiguous, collaborative environments where "success" is not algorithmically defined.
- What evidence would resolve it: A comparative study measuring agent performance on HCAST versus performance on equivalent open-ended, human-judged professional tasks.

## Limitations

- Task Representativeness: The 189 tasks, while diverse across four domains, may not fully capture the breadth of real-world autonomous software tasks, and the geometric mean time estimation method could systematically overestimate longer tasks when few baselines succeed.
- Contamination Mitigation Effectiveness: While task novelty requirements and from-scratch creation aim to prevent contamination, the effectiveness of these measures against increasingly capable memorization and transfer learning remains uncertain.
- Performance Degradation Attribution: The sharp drop in agent performance beyond 4-hour tasks could stem from multiple factors beyond planning depth—including task-specific complexity, domain expertise requirements, or scaffold limitations rather than fundamental capability cliffs.

## Confidence

- **High Confidence**: The correlation between human time estimates and agent success rates is well-supported by the data. The benchmark's methodology for collecting human baselines under controlled conditions is clearly specified and implemented.
- **Medium Confidence**: The interpretation that performance degradation reflects planning and resource management limitations rather than task distribution artifacts. The effectiveness of contamination mitigation strategies, while plausible, cannot be fully verified given withheld tasks.
- **Low Confidence**: Predictions about agent reliability on tasks exceeding the 8-hour benchmark limit, as extrapolation beyond observed data points is highly uncertain.

## Next Checks

1. Replicate the 1-hour to 4-hour performance cliff using a different agent scaffold (e.g., implement a tree-search or planning-based agent) on the public task subset to determine if the degradation is scaffold-specific or reflects fundamental capability limits.

2. Analyze the correlation between task duration and success rate separately within each domain (ML, cybersecurity, software engineering, general reasoning) to identify whether the performance drop is uniform across domains or concentrated in specific areas.

3. Conduct a human baseline expansion study where the same 140 professionals attempt a subset of tasks multiple times under varied conditions (with/without web access, different time constraints) to quantify the sensitivity of human time estimates to baseline conditions.