---
ver: rpa2
title: 'DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron
  Merging'
arxiv_id: '2507.04517'
source_url: https://arxiv.org/abs/2507.04517
tags:
- neurons
- dotresize
- width
- pruning
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DOTResize, a novel method for compressing large
  language models (LLMs) by reducing the width of model layers through neuron merging
  rather than pruning. The method frames width reduction as a discrete optimal transport
  problem, using activation similarities to redistribute signal across fewer neurons
  while preserving model function.
---

# DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging

## Quick Facts
- arXiv ID: 2507.04517
- Source URL: https://arxiv.org/abs/2507.04517
- Reference count: 36
- Primary result: DOTResize achieves up to 3-point perplexity improvement over magnitude pruning at 30% width reduction in LLMs

## Executive Summary
DOTResize introduces a novel approach to reducing LLM width through neuron merging rather than pruning. The method formulates width reduction as a discrete optimal transport problem, using activation similarities to redistribute signal across fewer neurons while preserving model function. Unlike pruning approaches that discard neurons, DOTResize employs Sinkhorn entropy regularization to create "soft" alignments that allow each neuron to correspond to linear combinations of others. To maintain computational invariance in Transformers, the method uses QR decomposition to transform general invertible matrices into orthogonal forms compatible with RMSNorm layer normalization.

## Method Summary
DOTResize reduces LLM width by treating neuron merging as a discrete optimal transport problem. The method computes activations over calibration data, selects a subset of neurons, and solves an entropy-regularized optimal transport problem to redistribute activation signal across remaining neurons. QR decomposition transforms the transport map into orthogonal components compatible with RMSNorm. The method then absorbs transformation matrices into adjacent weight matrices. DOTResize is evaluated on multiple LLM families (Llama-3.1, Mistral, Phi-4) at 20% and 30% sparsity, showing improved perplexity and zero-shot accuracy compared to magnitude pruning and SliceGPT.

## Key Results
- On Wikitext-2, DOTResize reduces perplexity by up to 3 points compared to magnitude pruning at 30% width reduction
- On zero-shot tasks, DOTResize achieves over 10% average accuracy improvement on certain models
- DOTResize outperforms both magnitude pruning and SliceGPT across multiple LLM families and sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete optimal transport enables neuron merging that preserves more model signal than hard pruning.
- Mechanism: DOTResize treats neurons as source distribution, selects a subset as target, and computes a transport map that redistributes activation signal across remaining neurons based on pairwise similarity. This contrasts with pruning, which discards low-importance neurons entirely.
- Core assumption: Neurons in LLM layers exhibit diffuse redundancy—multiple subsets can approximate full-layer performance.
- Evidence anchors:
  - [abstract] "DOTResize redistributes the signal from all neurons to a smaller subset based on activation similarities computed over calibration data."
  - [section 1] "Instead of determining neurons unimportant to model inference, we instead focus on combining neurons into fewer according to their activation similarities."
  - [corpus] No directly comparable optimal transport-based LLM compression work found; related corpus focuses on spiking networks and delta compression.

### Mechanism 2
- Claim: QR decomposition enables invertible (non-orthogonal) transport maps to work with RMSNorm in pre-norm Transformers.
- Mechanism: DOTResize decomposes transport map T = QR, applies orthogonal Q before RMSNorm (norm-preserving), and applies R after normalization. This extends prior work that only used orthogonal transformations.
- Core assumption: RMSNorm is unweighted or can be converted to unweighted form; residual connections exist.
- Evidence anchors:
  - [section 3.2] "We use QR decomposition in order to decompose invertible transformation matrix T = QR such that the Q matrix is applied to the input vector before RMSNorm, and the R matrix is applied after the RMSNorm."
  - [section 3.2] "Our method does not converge without its application due to its role in preserving layer normalization."
  - [corpus] No corpus evidence; this appears to be a novel contribution in this paper.

### Mechanism 3
- Claim: Entropic regularization produces "soft" neuron alignments that outperform hard one-to-one mappings.
- Mechanism: Sinkhorn regularization in the OT objective allows each source neuron to map to a weighted combination of target neurons, rather than forcing sparse one-to-one assignments. This captures more nuanced redundancy.
- Core assumption: Soft alignments better approximate functional behavior of original layer.
- Evidence anchors:
  - [section 3.1] "'Soft' alignments allow for each neuron to correspond to linear combinations of other neurons, whereas non-regularized alignments are much sparser."
  - [section 5.3] "The amount of regularization used does not change downstream language modeling performance to a large extent... different values of λ can result in some perplexity differences."
  - [corpus] No corpus evidence on regularization effects for neuron merging.

## Foundational Learning

- Concept: **Discrete Optimal Transport & Sinkhorn Algorithm**
  - Why needed here: DOTResize frames neuron merging as an optimal transport problem. Sinkhorn provides efficient, differentiable solutions with entropic regularization.
  - Quick check question: Given two discrete distributions and a cost matrix, can you explain why adding entropy regularization makes the transport map "softer"?

- Concept: **QR Decomposition for Norm-Preserving Transformations**
  - Why needed here: RMSNorm is invariant to orthogonal transformations only. QR decomposition enables use of general invertible maps while preserving RMSNorm behavior.
  - Quick check question: Why does applying Q before RMSNorm preserve the norm, but applying the full T = QR would not?

- Concept: **Transformer Pre-Norm Residual Architecture**
  - Why needed here: DOTResize applies transport maps at residual junctions. Understanding where M and M_inv are absorbed into weights requires knowing the data flow.
  - Quick check question: In a pre-norm Transformer, why can a transformation be applied before and inverted after a residual connection without changing the model function?

## Architecture Onboarding

- Component map:
  1. Calibration Data Selection -> Activation Computation -> Neuron Selection -> Cost Matrix Construction -> Optimal Transport Computation -> QR Decomposition -> Weight Transformation -> Evaluation

- Critical path:
  1. Calibration data → activations → cost matrix → transport map → QR decomposition → weight absorption.
  2. Failure at any step (e.g., incorrect cost matrix, missing QR) leads to divergent perplexity or non-functional model.

- Design tradeoffs:
  - **Sparsity level**: 20% yields compute reduction; 30% risks higher perplexity degradation.
  - **PCA pre-projection**: Improves performance for some models (Llama-3.1-8B), neutral or worse for others (Mistral-7B).
  - **Regularization λ**: 0.1 works well; lower values (0.01) may hurt performance on some models.

- Failure signatures:
  - Perplexity spikes (e.g., >100 PPL at 30% sparsity with magnitude pruning alone).
  - Model fails to converge if QR decomposition is skipped.
  - Zero-shot accuracy drops >20% average if wrong neurons selected or transport map is too sparse.

- First 3 experiments:
  1. Replicate Table 1: Run DOTResize vs. Magnitude Prune on Llama-3.1-8B at 20%/30% sparsity; verify perplexity improvement (expected ~12–13 PPL reduction at 30%).
  2. Ablate QR decomposition: Run DOTResize without QR step on a small model; confirm perplexity divergence.
  3. Sensitivity to calibration data: Vary token count (65K, 131K, 262K) and measure perplexity; expect minimal sensitivity beyond 131K tokens per Figure 5.

## Open Questions the Paper Calls Out

- How does DOTResize interact with low-bit quantization methods? The paper acknowledges that while DOTResize achieves compression, it is "orthogonal to methods like quantization and should be combined."
- Does the method generalize to multilingual or multimodal Transformer architectures? The authors limit evaluation to English LLMs, stating, "Our method can be easily generalized... We leave this exploration for future work."
- Can the QR-decomposition approach effectively handle standard LayerNorm without performance degradation? Section 3.2 derives the method specifically for unweighted RMSNorm, noting it assumes or converts to unweighted forms.

## Limitations

- The method is specifically designed for pre-norm Transformer architectures with RMSNorm and may not directly apply to post-norm architectures or those using standard LayerNorm.
- DOTResize achieves width reduction through neuron merging but does not inherently reduce parameter count, though it can be combined with quantization for further compression.
- The method requires a calibration dataset and forward pass to compute activations, adding computational overhead during compression setup.

## Confidence

**High confidence** in the core claim that DOTResize outperforms magnitude pruning, supported by consistent perplexity improvements across multiple model families.

**Medium confidence** in the QR decomposition approach for preserving RMSNorm invariance, as this is a novel contribution without corpus validation, though the mathematical reasoning is sound.

**Medium confidence** in the claim that entropic regularization produces better "soft" alignments, as ablation studies show sensitivity to λ values but the optimal setting appears model-dependent.

## Next Checks

1. **Ablation of QR decomposition step**: Run DOTResize without QR decomposition on Llama-3.1-8B and measure perplexity divergence. The paper claims this causes convergence failure, which should manifest as dramatically increased perplexity (>100 PPL) or NaN outputs.

2. **Calibration data sensitivity**: Systematically vary calibration token count (65K, 131K, 262K) and measure perplexity impact. The paper suggests minimal sensitivity beyond 131K tokens, but this should be verified across model sizes.

3. **Cost matrix computation verification**: Confirm that the cost matrix uses ℓ1 distance between activations (not ℓ2) and that the uniform distributions assumption in the OT problem is implemented correctly. Incorrect distance metrics or distribution specifications would invalidate the transport solution.