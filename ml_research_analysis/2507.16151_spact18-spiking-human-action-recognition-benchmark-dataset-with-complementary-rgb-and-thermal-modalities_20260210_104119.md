---
ver: rpa2
title: 'SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary
  RGB and Thermal Modalities'
arxiv_id: '2507.16151'
source_url: https://arxiv.org/abs/2507.16151
tags:
- spiking
- spike
- data
- recognition
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPACT18, the first video action recognition
  (VAR) dataset captured using a spike camera alongside synchronized RGB and thermal
  modalities. The dataset contains 3,168 videos of 44 subjects performing 18 daily
  actions, captured at ultra-high temporal resolution (20,000 Hz) using spike cameras,
  with complementary RGB (60 FPS) and thermal imaging.
---

# SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities

## Quick Facts
- **arXiv ID**: 2507.16151
- **Source URL**: https://arxiv.org/abs/2507.16151
- **Reference count**: 40
- **One-line primary result**: First dataset with synchronized spike, RGB, and thermal video modalities for human action recognition, achieving 85.7% accuracy with thermal data

## Executive Summary
This paper introduces SPACT18, the first video action recognition dataset captured using a spike camera alongside synchronized RGB and thermal modalities. The dataset contains 3,168 videos of 44 subjects performing 18 daily actions, captured at ultra-high temporal resolution (20,000 Hz) using spike cameras, with complementary RGB (60 FPS) and thermal imaging. A novel compression algorithm is proposed to reduce spiking data latency from 100k to 10k and 1k time steps while preserving temporal information. Experiments using state-of-the-art lightweight models (X3D, UniFormer) show thermal data outperforms RGB and spiking modalities, achieving 85.7% accuracy with UniFormer. Spiking data compressed to 10k matches RGB performance, but extreme compression (1k) degrades accuracy significantly. Direct SNN training remains challenging with accuracy below 60%, while ANN-SNN conversion suffers from high latency (up to 71.35% accuracy at 2048 timesteps). Hybrid ANN-SNN models improve performance but lack energy efficiency. The dataset establishes a new benchmark for spiking video understanding and multimodal fusion research.

## Method Summary
SPACT18 was captured using a Prophesee GenX320 spike camera (250×400, 20kHz), synchronized with RGB (1920×1080, 60 FPS) and thermal (1440×1080, 8.7 Hz) cameras. The dataset includes 3,168 videos from 44 subjects performing 18 actions (e.g., walking, running, jumping) in controlled indoor environments. A compression algorithm reduces spike data from 100k to 10k or 1k time steps while preserving temporal information. Models tested include X3D (CNN-based), UniFormer (hybrid CNN-Transformer), and MC3 (ResNet-18 backbone) for ANN approaches, plus direct SNN training (STS-ResNet, MS-ResNet, TET-ResNet) and ANN-SNN conversion methods. Training uses subject-wise 80/10/10 splits with Adam/AdamW optimizers and cross-entropy loss.

## Key Results
- Thermal modality achieves highest accuracy (85.7% with UniFormer-B) outperforming RGB (85.2%) and spiking (84.7% at 10k compression)
- Compression algorithm preserves critical temporal information, with 10k compression matching RGB performance while 1k compression degrades accuracy by ~10pp
- Direct SNN training remains challenging (<60% accuracy), while ANN-SNN conversion requires high latency (≥2048 timesteps) to achieve competitive results
- Hybrid ANN-SNN models achieve highest accuracy (87.5% with thermal+spiking) but lose energy efficiency advantages on current hardware

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spike cameras capture absolute brightness at ultra-high temporal resolution (20,000 Hz) through pixel-wise integrate-and-fire mechanisms, enabling fine-grained temporal representation unavailable to standard RGB or event cameras.
- **Mechanism:** Each pixel contains a photoreceptor (converts light to electrical signal), an integrator (accumulates charge over time), and a comparator. When accumulated charge exceeds threshold θ, a spike fires and the integrator resets. This occurs asynchronously per pixel, producing binary spike streams S(x,y,k) ∈ {0,1}^{H×W×T}.
- **Core assumption:** The paper assumes this bio-inspired mechanism provides "textural spatiotemporal details" superior to event cameras, but this is stated rather than empirically validated within the paper's experiments.
- **Evidence anchors:**
  - [Page 5-6]: Mathematical formulation: V_{i,j}(t) = ∫_{t_last}^{t} α·I_{i,j}(γ)dγ, with firing when V_{i,j}(t) ≥ θ
  - [Abstract]: "spike cameras provide even finer spatiotemporal resolution" than event cameras
  - [corpus]: Weak direct evidence—related papers (Temporal-Guided SNNs, SpikeVideoFormer) focus on event cameras rather than spike cameras specifically
- **Break condition:** If lighting conditions are insufficient (indoor artificial lighting required 2000-watt lamps), the integrator may not accumulate sufficient charge, degrading output quality.

### Mechanism 2
- **Claim:** The proposed compression algorithm preserves limit spiking rates while reducing temporal resolution from 100k to 10k or 1k time steps, enabling tractable training without complete temporal information loss.
- **Mechanism:** The algorithm divides spike trains into T' = ⌊T/d⌋ non-overlapping intervals. For each interval I[i], it computes the spiking rate r[i] = (1/d)Σ_{t∈I[i]} s[t], then feeds r[i] into an integrate-and-fire neuron to generate compressed spikes. Lemma 1 proves that for constant-rate inputs, original and compressed spike trains converge to identical limit spiking rates.
- **Core assumption:** The compression preserves "critical temporal information" for action recognition, though the paper acknowledges extreme compression (1k) degrades accuracy significantly.
- **Evidence anchors:**
  - [Page 6-7]: Algorithm 1 specification and Lemma 1 with proof
  - [Page 9, Table 3]: 10k compressed spiking data achieves 84.7% accuracy (UniFormer), matching RGB performance (85.2%), while 1k drops to 74.4%
  - [corpus]: No direct corpus validation of this specific compression approach
- **Break condition:** Extreme compression (d=100, yielding 1k timesteps) causes significant accuracy degradation (~10 percentage points), suggesting temporal resolution falls below the threshold needed to distinguish similar actions (e.g., walking vs. jogging).

### Mechanism 3
- **Claim:** Thermal modality outperforms RGB and spiking modalities for action recognition by reducing background distractions and focusing on motion-relevant heat signatures.
- **Mechanism:** Thermal imaging captures infrared radiation from subjects, naturally segmenting human activity from ambient environment. This reduces "irrelevant details" that may confound models trained on RGB data with complex backgrounds.
- **Core assumption:** The paper infers this mechanism from performance results but does not directly visualize or validate attention patterns showing reduced distraction.
- **Evidence anchors:**
  - [Page 8-9, Table 3]: Thermal achieves 85.7% accuracy with UniFormer-B (highest across all modalities), RGB achieves 85.2%, Spiking 10k achieves 84.7%
  - [Page 9]: "thermal data consistently outperforms other modalities, allowing the model to focus solely on the activity and reducing distractions from irrelevant details"
  - [corpus]: EPFL-Smart-Kitchen-30 dataset paper notes complex environments challenge models, indirectly supporting thermal's advantage in clutter reduction
- **Break condition:** Thermal provides less benefit for actions with subtle differences in similar body regions (e.g., drums vs. guitar show confusion in confusion matrix, Figure 11), where fine-grained texture/color cues may be needed.

### Mechanism 4
- **Claim:** ANN-SNN conversion for video models requires high latency (≥2048 timesteps) to achieve competitive accuracy due to conversion error accumulation across deep architectures with custom layers.
- **Mechanism:** Pre-trained ANN weights are transferred to SNN by replacing ReLU activations with LIF neurons, using channel-wise maximum activations as neuron thresholds. However, video models' depth and custom layers (beyond standard ReLU) introduce approximation errors that compound across layers, requiring longer simulation to converge.
- **Core assumption:** The paper assumes the high latency is inherent to the conversion approach rather than an artifact of the specific implementation or hyperparameter choices.
- **Evidence anchors:**
  - [Page 8]: MC3 conversion achieves only 35.07% accuracy at T=1024, improving to 71.35% at T=2048
  - [Page 9, Table 5]: Shows progressive accuracy improvement with timestep increase for both 10k and 1k compressed data
  - [corpus]: Temporal-Guided SNNs paper addresses similar temporal processing challenges in event-based HAR
- **Break condition:** If latency constraints prevent running ≥1024 timesteps (as in real-time applications), conversion-based SNNs will significantly underperform ANNs.

## Foundational Learning

- **Concept: Integrate-and-Fire Neuron Dynamics**
  - **Why needed here:** Understanding how spike cameras and SNNs process information requires grasping membrane potential accumulation, threshold crossing, and spike generation as continuous-time processes discretized for computation.
  - **Quick check question:** Given a constant input current of 0.3 and threshold of 1.0, how many timesteps on average until a spike fires? (Answer: ~3-4 timesteps, as charge accumulates linearly)

- **Concept: Temporal Resolution vs. Computational Latency Trade-off**
  - **Why needed here:** The paper's compression algorithm directly addresses this trade-off. Higher temporal resolution preserves fine-grained motion but increases training/inference cost; compression reduces cost but risks losing discriminative information.
  - **Quick check question:** Why does 1k compression perform worse than 10k compression even though both use the same rate-encoding scheme for ANN training? (Answer: 1k has coarser temporal resolution—0.1 vs 0.01—losing finer temporal patterns needed to distinguish similar actions)

- **Concept: Multimodal Fusion in Video Understanding**
  - **Why needed here:** The dataset provides synchronized RGB, thermal, and spike modalities. Effective use requires understanding what each modality captures (color/texture, heat signatures, ultra-high-speed temporal dynamics) and how to fuse complementary information.
  - **Quick check question:** If deploying on an edge device with strict power constraints, which modality combination would you prioritize? (Answer: Thermal + Spiking, as thermal reduces background complexity and spiking enables event-driven low-power processing, though the paper notes hybrid models lose energy-efficiency advantages on current neuromorphic hardware)

## Architecture Onboarding

- **Component map:**
  - Spike Camera Pipeline: Raw spike stream (100k timesteps) → Compression Algorithm → Rate Encoding (for ANN training) or Binary Spikes (for SNN training)
  - Baseline ANN Models: X3D (CNN-based, 3M params) and UniFormer (hybrid CNN-Transformer, 21-50M params) process RGB, Thermal, or Rate-Encoded Spiking data
  - SNN Approaches: (1) Direct training (STS-ResNet, MS-ResNet, TET-ResNet) on compressed 10k data; (2) ANN-SNN conversion (MC3 backbone); (3) Hybrid models (Respike) fusing ANN modalities with SNN spiking data
  - Data Flow: 44 subjects × 18 actions × 2 sessions → 3,168 videos per modality → 80/10/10 subject-wise split

- **Critical path:**
  1. Data preprocessing: Select compression level (10k recommended, 1k for efficiency experiments)
  2. Modality selection: Start with Thermal (highest baseline accuracy) for proof-of-concept
  3. Model selection: UniFormer-B for accuracy, X3D-M for efficiency
  4. If pursuing SNNs: Begin with ANN-SNN conversion for rapid baseline, then explore direct training

- **Design tradeoffs:**
  - **Compression level:** 10k balances accuracy (84.7%) and computational cost; 1k sacrifices ~10pp accuracy for 10× fewer timesteps
  - **Model complexity:** UniFormer-B (50M params, 85.7% thermal) vs X3D-M (3M params, 83.4% thermal)—factor of ~16× parameters for ~2pp gain
  - **SNN approach:** Conversion provides fast baseline but requires 2048 timesteps; direct training is compute-intensive and achieves only 58% (TET-ResNet); hybrid (Respike) achieves 87.5% (thermal + spiking) but loses neuromorphic hardware compatibility

- **Failure signatures:**
  - **Direct SNN training <60%:** Likely issue with gradient vanishing through time or insufficient training epochs (paper uses 100 epochs); consider TET (Temporal Efficient Training) or surrogate gradient methods
  - **ANN-SNN conversion accuracy collapses at low timesteps:** Expected behavior—Table 5 shows 8.68% at T=16 for 10k data; must increase timesteps or use threshold normalization techniques
  - **Confusion between walking/jogging:** Figure 9-11 show systematic misclassification; these actions share similar motion patterns, requiring finer temporal discrimination or data augmentation
  - **Confusion between drums/guitar/roll:** Hand-arm actions with similar motion ranges; thermal and spiking modalities may lack texture cues to distinguish held objects

- **First 3 experiments:**
  1. **Reproduce Thermal baseline:** Train UniFormer-B on Thermal-Color data (30×224² input, 181.88G FLOPs). Target: 85.7% accuracy. Validates data pipeline and training setup. Hyperparameters: Adam, LR=10⁻², batch=8, 100 epochs, StepLR scheduler.
  2. **Compare compression levels:** Train X3D-M on Spiking 10k rate-encoded vs. 1k rate-encoded data. Expected: ~10pp gap (79.9% vs 69.4%). Analyze confusion matrices to identify which actions lose discriminability under extreme compression.
  3. **Establish SNN conversion baseline:** Train MC3 on rate-encoded spiking data, convert to SNN with channel-wise threshold normalization, evaluate at T∈{64, 256, 1024, 2048}. Map accuracy-latency curve. If T=2048 <70%, debug threshold calibration or check for non-ReLU layers in backbone.

## Open Questions the Paper Calls Out

- **Question:** How can direct SNN training algorithms be optimized to handle the computational complexity of ultra-high temporal resolution (100k time steps) data?
  - **Basis:** [explicit] The authors state that direct training on raw 100k data is computationally intensive and requires "specialized algorithms tailored for direct SNN training" (Section 6.1).
  - **Why unresolved:** Current SNN training complexity scales with time steps, making the raw dataset infeasible, while current direct training results on compressed data remain low (<60%).
  - **Evidence:** A training algorithm achieving >75% accuracy on the 10k or 100k versions without prohibitive memory consumption.

- **Question:** Can ANN-SNN conversion methods be developed for deep video models that achieve high accuracy without requiring high inference latency?
  - **Basis:** [explicit] Section 6.1 highlights the need for "more efficient ANN-SNN conversion methods specific to video models" to address the high latency (2048 timesteps) observed in experiments.
  - **Why unresolved:** The paper shows current conversion methods require very long simulation times (high latency) to reach acceptable accuracy (71.35%), negating energy efficiency.
  - **Evidence:** A conversion method achieving >70% accuracy on SPACT18 using significantly fewer timesteps (e.g., <200) than the current baseline.

- **Question:** What learning frameworks can effectively synchronize and fuse sparse spike data with dense RGB and thermal modalities within a unified SNN?
  - **Basis:** [explicit] The authors note that "synchronization and fusion of diverse modalities" remain "key obstacles" in multimodal SNN research (Section 6.1).
  - **Why unresolved:** Current hybrid approaches (e.g., Respike) rely on ANN-SNN cross-attention, which limits deployment on purely neuromorphic hardware.
  - **Evidence:** A fully spiking multimodal architecture that successfully leverages the complementary data to outperform single-modality SNN baselines.

## Limitations
- The paper's claimed superiority of spike cameras over event cameras for temporal resolution is asserted but not experimentally validated through direct comparison
- Direct SNN training results show significant accuracy gaps (<60%) compared to ANN approaches, suggesting fundamental challenges not fully explained
- Thermal modality advantage is inferred from performance gains without visualization of attention patterns or ablation of background effects

## Confidence

- **High confidence**: Thermal modality performance advantage (directly measured with consistent results across models); dataset construction methodology and statistics (3,168 videos, 44 subjects, 18 actions clearly documented); compression algorithm correctness (mathematical proof provided with empirical validation)
- **Medium confidence**: Compression algorithm's preservation of "critical temporal information" (accuracy metrics show preservation but no qualitative temporal analysis); SNN conversion latency requirements (empirically demonstrated but may depend on specific implementation details); multimodal fusion benefits (performance gains observed but fusion mechanisms not deeply analyzed)
- **Low confidence**: Spike camera superiority claims over event cameras (stated but not experimentally validated); direct SNN training effectiveness (results below 60% accuracy suggest fundamental challenges not resolved); energy efficiency comparisons (hybrid models' efficiency not benchmarked against pure ANN approaches)

## Next Checks

1. **Temporal information preservation**: Conduct ablation studies comparing compressed spike data (10k, 1k) against varying temporal subsampling of raw spike streams to quantify information loss beyond accuracy metrics.

2. **Attention visualization**: Generate Grad-CAM or similar visualizations for thermal vs RGB vs spiking modalities to empirically demonstrate thermal's background reduction claims and spiking's temporal focus patterns.

3. **Cross-dataset generalization**: Evaluate models trained on SPACT18 on standard RGB video datasets (UCF-101, HMDB-51) to assess modality-specific feature transferability and generalization beyond the controlled dataset environment.