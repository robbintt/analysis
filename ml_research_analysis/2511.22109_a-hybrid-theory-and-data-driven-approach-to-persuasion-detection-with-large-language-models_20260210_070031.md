---
ver: rpa2
title: A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large
  Language Models
arxiv_id: '2511.22109'
source_url: https://arxiv.org/abs/2511.22109
tags:
- persuasion
- features
- llms
- persuasive
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a hybrid approach combining large language\
  \ models (LLMs) and psychological theory to predict persuasive success in online\
  \ discourse. The method uses LLM-generated ratings of eight theory-derived features\u2014\
  including epistemic emotion and willingness to share\u2014as inputs to a Random\
  \ Forest classifier trained on belief change data."
---

# A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models

## Quick Facts
- **arXiv ID:** 2511.22109
- **Source URL:** https://arxiv.org/abs/2511.22109
- **Reference count:** 40
- **Primary result:** 82% accuracy in classifying persuasive vs. non-persuasive comments

## Executive Summary
This study develops a hybrid approach combining large language models (LLMs) and psychological theory to predict persuasive success in online discourse. The method uses LLM-generated ratings of eight theory-derived features—including epistemic emotion and willingness to share—as inputs to a Random Forest classifier trained on belief change data. The model achieved 82% accuracy in classifying persuasive versus non-persuasive comments, outperforming both theory-driven and data-driven baselines. Epistemic emotion and willingness to share emerged as top predictors of belief change. The results demonstrate that integrating psychological insights with LLM capabilities enhances interpretability and predictive performance in persuasion detection, with broader applications in influence detection and misinformation mitigation.

## Method Summary
The study employs a hybrid architecture combining theory-driven and data-driven components. First, LLMs (LLaMA3, Gemma2, or Mixtral) generate ratings for eight psychological features (Influential, Interesting, Interesting-If-True, Positive, Negative, Shareable, Truthfulness, Attention) on a 1-5 Likert scale. These ratings serve as inputs to both an OLS regression model (trained on the "Truth Wins" dataset to predict "Belief Update" scores) and a Random Forest classifier (trained on the "Winning Arguments" dataset). The Random Forest uses both the eight LLM ratings and the predicted Belief Update score to classify comments as persuasive or non-persuasive, achieving 82% accuracy.

## Key Results
- Hybrid model achieved 82% accuracy in classifying persuasive vs. non-persuasive comments
- Epistemic emotion ("Interesting-If-True") and willingness to share emerged as top predictors
- Outperformed both theory-driven (64% accuracy) and data-driven (54% accuracy) baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured psychological feature extraction via LLMs improves classification accuracy over zero-shot reasoning.
- **Mechanism:** Instead of asking an LLM to directly predict persuasion (a complex, opaque task), the model forces the LLM to function as a measurement instrument. By constraining the output to specific psychological ratings (e.g., "Interesting-If-True," "Shareable") on a Likert scale, the architecture transforms abstract linguistic patterns into structured, interpretable numerical features suitable for a Random Forest classifier.
- **Core assumption:** LLMs can approximate human psychological ratings for constructs like "epistemic emotion" with sufficient consistency to serve as training data.
- **Evidence anchors:**
  - [abstract] "...use LLM-generated ratings of eight psychological features... to build a random forest classification model..."
  - [section 3.3] "LLMs excel at converting linguistic features into ratings that capture statistical regularities... enhancing the performance of downstream models."
- **Break condition:** If the LLM's interpretation of the prompt features drifts or fails to align with the intended psychological definitions (e.g., conflating "Interesting" with "True"), the feature space becomes noisy, degrading classifier performance.

### Mechanism 2
- **Claim:** Epistemic curiosity and social shareability are stronger predictors of persuasion than objective truthfulness in online discourse.
- **Mechanism:** The Random Forest classifier identifies that messages rated high in "Interesting-If-True" (curiosity) and "Shareable" (social utility) correlate most strongly with the "delta" (persuasion) label. This suggests that in the "Change My View" environment, successful persuasion relies on triggering engagement and perceived value rather than strict factual accuracy.
- **Core assumption:** The "delta" award mechanism in the dataset serves as a reliable proxy for actual belief change.
- **Evidence anchors:**
  - [abstract] "Epistemic emotion and willingness to share emerged as the top predictors of belief change."
  - [section 5.4] "Truthfulness was a weaker predictor... Perceived truthfulness might be more important in environments with a wider mix of objectively true and false information."
- **Break condition:** If applied to a domain where factual verification is the primary goal (e.g., medical advice) rather than opinion exchange, the reliance on epistemic emotion over truthfulness may lead to false positives for "successful" persuasion that is actually misinformation.

### Mechanism 3
- **Claim:** Hybridization of theory-driven regression and data-driven classification captures non-linear persuasion dynamics.
- **Mechanism:** The architecture uses an OLS regression model (Theory-Driven) to generate a "Belief Update" score, which is then fed as a feature into the Random Forest (Data-Driven). This allows the final classifier to utilize the linear theoretical baseline while learning complex, non-linear interactions between the eight psychological features that the OLS model misses.
- **Core assumption:** The "Truth Wins" experimental dataset shares sufficient feature distribution similarity with the "Winning Arguments" ecological dataset for the regression weights to transfer effectively.
- **Evidence anchors:**
  - [section 3.2] "...belief update scores... are also used as a feature, contributing to a psychologically grounded signal..."
  - [table 1] Shows Hybrid models (~73-82%) outperforming Theory-Driven models (~54-64%).
- **Break condition:** If the relationship between features and belief change is strictly linear, the Random Forest adds unnecessary complexity without gain; if the training data (Truth Wins) is distributionally distinct from the target data (Winning Arguments), the regression score introduces bias.

## Foundational Learning

- **Concept:** **Random Forest Variable Importance (Permutation)**
  - **Why needed here:** The paper relies on this technique not just for prediction, but to explain *why* persuasion happens (identifying "Interesting-If-True" as top predictor). You must understand that this measures the drop in accuracy when a feature is shuffled, indicating its unique contribution.
  - **Quick check question:** If "Shareable" is permuted and the model accuracy drops by 5%, while "Truthfulness" drops it by 0.1%, which feature should be prioritized for further psychological analysis?

- **Concept:** **Likert Scales as Regression/Classification Inputs**
  - **Why needed here:** The LLM outputs ordinal text ("one" to "five") which is converted to integers. Understanding how to treat this data (as ordinal vs. continuous) is critical for the OLS and RF stages.
  - **Quick check question:** Should the "Belief Update" score (ranging -100 to 100) be treated the same way as the 1-5 Likert ratings in the Random Forest?

- **Concept:** **Jaccard Similarity for Pair Selection**
  - **Why needed here:** The authors use this to select "negative" comments that are topically similar to "positive" comments to ensure the model learns *persuasiveness* rather than just topic relevance.
  - **Quick check question:** Why is it important to control for content similarity when building a dataset to isolate the effect of persuasive style?

## Architecture Onboarding

- **Component map:** Raw text (OP Post + Comment Pairs) -> LLM Feature Extractor (8 Likert ratings) -> OLS Regression (Belief Update score) -> Random Forest Classifier (Binary Classification)
- **Critical path:** The **LLM Prompt Engineering** (Listing 2). If the prompt does not strictly enforce the JSON schema or maps the psychological definitions incorrectly (e.g., "Persuasive" vs "Influential"), the entire downstream Random Forest fails.
- **Design tradeoffs:**
  - **Accuracy vs. Interpretability:** The authors chose Random Forest over Deep Learning (e.g., BERT fine-tuning) to maintain explicit feature importance visibility, sacrificing potential accuracy gains from end-to-end learning.
  - **Cost vs. Speed:** Using LLMs for feature extraction is computationally cheaper than fine-tuning them, but requires managing API/Inference costs for large datasets.
- **Failure signatures:**
  - **Flat Rating Distribution:** LLM outputs "3" (Somewhat) for all features on all inputs (mode collapse), resulting in low classifier accuracy (~50%).
  - **Theory-Data Mismatch:** The OLS "Belief Update" score negatively correlates with the actual labels in the "Winning Arguments" dataset, acting as noise.
- **First 3 experiments:**
  1. **Sanity Check Prompt:** Run the feature extraction prompt (Listing 2) on 10 manual examples. Verify the LLM distinguishes clearly between "Interesting" and "Interesting-If-True."
  2. **Baseline Comparison:** Train the Random Forest using *only* the "Belief Update" score vs. *only* the 8 LLM features to quantify the contribution of the theory-driven vs. data-driven components.
  3. **Ablation Study:** Remove the top-ranked feature ("Interesting-If-True" or "Shareable") from the training set and measure the drop in accuracy to validate the paper's claim about epistemic emotion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hybrid persuasion model maintain high predictive accuracy in online environments where persuasive intent is implicit or absent?
- Basis in paper: [explicit] The authors note that the "Winning Arguments" dataset (Change My View) may limit generalizability because users are explicitly trying to change opinions. They suggest future research should apply the model to settings where intent is less explicit, such as product reviews or news comments.
- Why unresolved: The current model was trained and tested solely on a forum specifically dedicated to debate and belief revision, which may possess unique linguistic and interaction dynamics not found in organic social media.
- What evidence would resolve it: Testing the Random Forest classifier on diverse, non-debate datasets containing organic opinion shifts or implicit persuasion to compare performance metrics against the CMV baseline.

### Open Question 2
- Question: To what extent do LLM-generated feature ratings align with human psychological ratings for the same persuasive dimensions?
- Basis in paper: [explicit] The paper identifies the "lack of validation against human-sourced ratings" as a key limitation, specifically calling for future work to compare LLM-generated ratings with human-annotated counterparts to establish a ground truth.
- Why unresolved: While LLMs showed consistency across models, the study did not verify if the numerical ratings assigned by the LLMs accurately reflect how a human would psychologically perceive "Influence," "Interest," or "Shareability."
- What evidence would resolve it: A study collecting human annotations for the eight features on the same text samples and calculating the correlation (e.g., Spearman’s rank) between human and LLM ratings.

### Open Question 3
- Question: Does incorporating emotional arousal alongside valence significantly enhance the prediction of persuasive success?
- Basis in paper: [explicit] The authors state that their analysis "overlooks arousal, which plays a crucial role in persuasion" and explicitly call for future research to examine its role, as high-arousal emotions can amplify impact and reduce verification.
- Why unresolved: The current feature set captures emotional valence (Positive/Negative) but fails to distinguish between low and high energy states (e.g., sadness vs. anger), which previous literature suggests are critical for driving content sharing and snap judgments.
- What evidence would resolve it: Integrating an "arousal" feature into the model—either via lexicon-based scoring or LLM prompting—and measuring the resulting change in classification accuracy and feature importance rankings.

## Limitations

- **Dataset access dependency:** The "Truth Wins" dataset required to train the OLS regression component is not publicly available, creating a critical dependency for exact reproduction.
- **Feature definition drift:** The psychological constructs (particularly "Interesting-If-True" vs "Interesting") rely on subjective LLM interpretation, which may vary across model versions or prompt formulations.
- **Single domain validation:** Results are based solely on the Change My View forum dataset, limiting generalizability to other online discourse environments.

## Confidence

- **Hybrid architecture claims (82% accuracy):** Medium confidence. While the methodology is well-specified, exact replication depends on accessing proprietary datasets and may be sensitive to implementation details.
- **Top predictor identification (epistemic emotion, shareability):** Medium confidence. The permutation importance analysis is sound, but the specific feature rankings could shift with different LLM models or dataset splits.
- **Generalizability claims:** Low confidence. Results are based on a single Reddit dataset with a specific "delta" award mechanism; performance in other discourse contexts remains untested.

## Next Checks

1. **Sanity check prompt consistency:** Run the feature extraction prompt on 10 manual examples to verify the LLM correctly distinguishes between "Interesting" and "Interesting-If-True" as intended.
2. **Ablation study validation:** Remove the top-ranked feature ("Interesting-If-True" or "Shareable") from training and measure accuracy drop to confirm its contribution.
3. **Distribution alignment test:** Compare the feature distributions between the "Truth Wins" and "Winning Arguments" datasets to assess whether the OLS regression weights transfer appropriately.