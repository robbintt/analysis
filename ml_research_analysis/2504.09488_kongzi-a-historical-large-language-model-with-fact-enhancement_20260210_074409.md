---
ver: rpa2
title: 'Kongzi: A Historical Large Language Model with Fact Enhancement'
arxiv_id: '2504.09488'
source_url: https://arxiv.org/abs/2504.09488
tags:
- reasoning
- historical
- data
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Kongzi, a historical large language model designed
  to improve factual accuracy and reasoning depth for historical analysis tasks. The
  authors address the challenge of hallucinations and factual inaccuracies in longer
  reasoning chains, which are particularly problematic in historical contexts requiring
  cross-temporal correlations and coherent conclusions from fragmentary sources.
---

# Kongzi: A Historical Large Language Model with Fact Enhancement

## Quick Facts
- arXiv ID: 2504.09488
- Source URL: https://arxiv.org/abs/2504.09488
- Reference count: 25
- Key outcome: Kongzi-7B significantly outperforms DeepSeek-R1 and O3-mini on historical accuracy and reasoning tasks, with LLM-based evaluation scores exceeding 70 points and Historical Accuracy scores above 56 across different judges.

## Executive Summary
This paper presents Kongzi, a historical large language model designed to improve factual accuracy and reasoning depth for historical analysis tasks. The authors address the challenge of hallucinations and factual inaccuracies in longer reasoning chains, which are particularly problematic in historical contexts requiring cross-temporal correlations and coherent conclusions from fragmentary sources. Kongzi employs a three-stage approach: continued pre-training on curated historical corpora, two-stage supervised fine-tuning for foundational QA and chain-of-thought reasoning, and a novel fact-aware reinforcement learning framework that rewards entity-level factual accuracy.

## Method Summary
Kongzi uses Qwen2.5 base models (0.5B and 7B) with continued pre-training on ~0.2B tokens of cleaned classical Chinese historical text, followed by two-stage supervised fine-tuning on 1M QA pairs and 10K CoT examples, and fact-aware GRPO reinforcement learning with entity-level rewards. The model is evaluated on Chinese historical analysis tasks using LLM-based judges (GPT-4o, Gemini-2.5, Qwen-Max) on metrics including Historical Accuracy, Logical Reasoning, and Problem Solving.

## Key Results
- Kongzi-7B achieves answer scores of 70.64, 84.43, and 67.82 on GPT-4o, Gemini-2.5, and Qwen-Max respectively
- Historical Accuracy scores reach 58.45, 80.45, and 56.36 across the same judges
- RL training provides substantial improvements over SFT baseline, with gains of 6.4-9.7 percentage points
- The 0.5B parameter version outperforms larger distilled models from DeepSeek

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continued pre-training on curated historical corpora enhances domain-specific factual knowledge in smaller models.
- **Mechanism:** The paper exposes Qwen2.5 base models (0.5B and 7B) to ~0.2B tokens of cleaned classical Chinese historical text (including the Twenty-Four Histories). This domain injection grounds the model's internal knowledge representations before reasoning capabilities are added.
- **Core assumption:** Domain-specific pre-training transfers factual knowledge more efficiently than trying to encode it through instruction tuning alone.
- **Evidence anchors:** [abstract] "Through the integration of curated, high-quality historical data... Kongzi demonstrates strong factual alignment and sophisticated reasoning depth." [Section 3.1] "With the assistance of professionals, we effectively collected nearly 0.2B tokens of high-quality classical Chinese historical corpus, including but not limited to the Twenty-Four Histories."

### Mechanism 2
- **Claim:** Two-stage supervised fine-tuning separates basic QA acquisition from chain-of-thought reasoning development, yielding more stable capability growth.
- **Mechanism:** Stage 1 uses 1M augmented QA pairs (mixed with general SFT data) for foundational conversational ability. Stage 2 uses 10K filtered CoT examples generated by DeepSeek-R1 from contextual reasoning texts. This staged approach prevents interference between learning to answer and learning to reason.
- **Core assumption:** Reasoning capabilities require a stable factual QA foundation; training both simultaneously degrades both.
- **Evidence anchors:** [Section 3.2] "we preliminarily optimize the pre-trained model through supervised command fine-tuning... To further enable the model to acquire reasoning capabilities, we introduce high-quality Chain-of-Thought data." [Section 4.2] "This process yielded 50,000 thought chain data points, which were then screened by the LLM and manually to retain 10,000 high-quality data points."

### Mechanism 3
- **Claim:** Entity-level fact-aware reinforcement learning reduces hallucinations by rewarding correct entity generation and penalizing incorrect or missing entities.
- **Mechanism:** Built on GRPO, the reward function combines: (1) Entity recognition reward (ER = wc × CE − wi × IE), (2) Format reward, (3) Logical coherence reward, (4) Repetition penalty. Entities are extracted from outputs and compared against annotated ground truth.
- **Core assumption:** Entity-level correctness correlates with overall factual accuracy; incorrect entities are the primary hallucination signal.
- **Evidence anchors:** [Section 3.3] "By explicitly modeling entity-level factuality, this mechanism not only enhances the model's sensitivity to historically accurate content but also significantly mitigates hallucination issues." [Table 2] Kongzi-7B-RL shows +7.5 to +12.1 percentage point gains in Historical Accuracy over SFT baseline across different LLM judges.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Fine-Tuning**
  - Why needed here: Kongzi's second SFT stage relies on CoT data to teach reasoning; understanding CoT structure explains why the model generates intermediate thinking steps.
  - Quick check question: Can you explain the difference between zero-shot CoT prompting and CoT fine-tuning?

- **Concept: Proximal Policy Optimization (PPO) and GRPO**
  - Why needed here: The fact-aware RL stage uses GRPO, a variant of PPO; understanding policy gradients and clipping is essential to debug reward signal issues.
  - Quick check question: What is the purpose of the clipping parameter ε in PPO/GRPO?

- **Concept: Entity Recognition and Knowledge Grounding**
  - Why needed here: The reward mechanism depends on extracting and validating entities; NER fundamentals clarify how factual rewards are computed.
  - Quick check question: How would you handle entity ambiguity when the same name refers to multiple historical figures?

## Architecture Onboarding

- **Component map:**
  Historical Corpus 0.2B tokens → Continued Pre-training (Qwen2.5 base) → Stage 1 SFT: 1M QA pairs + general data → Stage 2 SFT: 10K CoT examples → Fact-Aware RL (GRPO + Entity Rewards) → Kongzi-7B-RL

- **Critical path:** The CoT data generation and filtering pipeline (Section 4.2) is the bottleneck—quality here directly determines reasoning capability. Manual + LLM screening reduced 50K generated samples to 10K.

- **Design tradeoffs:**
  - Smaller models (0.5B) require more aggressive CPT; larger models (7B) benefit more from RL refinement.
  - Entity-level rewards are precise but require annotated ground truth; this limits scalability to domains without structured entity knowledge bases.
  - RAG was explicitly excluded (Section 3.3) to focus on internal factuality rather than external retrieval.

- **Failure signatures:**
  - Low Historical Accuracy but high Logical Reasoning → entity reward not firing; check annotation coverage.
  - Repetitive outputs → repetition penalty weight (w4) may be too low.
  - CoT format violations → format reward not being applied; verify <begin_of_thought> markers in training data.

- **First 3 experiments:**
  1. **Ablate the RL stage:** Compare Kongzi-7B-SFT vs. Kongzi-7B-RL to isolate RL contribution (Table 2 provides baseline).
  2. **Vary entity reward weights:** Test different wc/wi ratios to find optimal balance between rewarding correct entities and penalizing incorrect ones.
  3. **Cross-domain transfer:** Apply the same pipeline (CPT → two-stage SFT → fact-aware RL) to a different knowledge-intensive domain (e.g., biomedical) to test generalizability of the architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the fact-aware reinforcement learning framework generalize to mitigating hallucinations in domains beyond Chinese history?
- **Basis in paper:** [explicit] The authors state: "we are currently unable to demonstrate that incorporating an accurate reward function is universally effective in mitigating hallucinations" and plan to "establish a broadly applicable paradigm to address hallucination across diverse scenarios."
- **Why unresolved:** The current experiments only cover ancient Chinese historical corpora; entity recognition rewards may not transfer to domains with different factuality structures or non-entity-based reasoning.
- **What evidence would resolve it:** Evaluation of Kongzi's fact-aware RL on non-historical domains (e.g., scientific, legal) or non-Chinese historical corpora, with comparable hallucination metrics.

### Open Question 2
- **Question:** How does the performance of fact-aware RL scale with increased training data and model size?
- **Basis in paper:** [explicit] The conclusion states: "we will explore the scalability of reinforcement learning methods augmented with factual data."
- **Why unresolved:** The study uses only 0.2B tokens for pre-training, 500 RL samples, and maximum 7B parameters; it is unknown whether gains persist or diminish at larger scales.
- **What evidence would resolve it:** Ablation studies varying pre-training corpus size, RL sample count, and model parameter count, reporting performance curves across these dimensions.

### Open Question 3
- **Question:** What is the relative contribution of entity-level factual rewards versus other reward components (logical coherence, format, repetition penalty) to hallucination reduction?
- **Basis in paper:** [inferred] The reward function integrates four weighted components, but the paper does not isolate the effect of the entity-level factual reward from other rewards.
- **Why unresolved:** Without ablation on individual reward terms, it remains unclear whether factual accuracy gains derive primarily from entity rewards or from confounding factors like improved formatting or reduced repetition.
- **What evidence would resolve it:** Ablation experiments training models with each reward component independently and in subsets, comparing hallucination rates and factual accuracy scores.

## Limitations
- Lack of publicly available historical corpus and training data details, including exact preprocessing steps and entity annotation methods
- Entity-level reward mechanism assumes comprehensive ground truth coverage, which may not hold for all historical queries
- LLM evaluation prompts and general SFT dataset composition are unspecified, making fair comparison difficult

## Confidence
- **High confidence:** The staged SFT architecture and entity-level reward mechanism's effectiveness are well-supported by experimental results
- **Medium confidence:** The claim that continued pre-training on historical corpora enhances factual knowledge is supported but lacks detailed transfer efficiency analysis
- **Low confidence:** The generalizability to non-historical domains and scalability of the approach remain unproven

## Next Checks
1. **Ablation study on entity reward weights:** Systematically vary wc and wi ratios in the entity recognition reward to quantify their individual contributions and identify optimal balance points
2. **Cross-domain transfer experiment:** Apply the exact pipeline (CPT → two-stage SFT → fact-aware RL) to a different knowledge-intensive domain (e.g., biomedical or legal) using domain-specific entity annotations to test architecture generalizability
3. **Ground truth coverage validation:** Measure the percentage of historical entities in the test set that are covered by the annotated ground truth used for RL rewards, and assess how missing entities affect Historical Accuracy scores