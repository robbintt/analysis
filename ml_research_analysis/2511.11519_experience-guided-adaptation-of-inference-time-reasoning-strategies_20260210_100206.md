---
ver: rpa2
title: Experience-Guided Adaptation of Inference-Time Reasoning Strategies
arxiv_id: '2511.11519'
source_url: https://arxiv.org/abs/2511.11519
tags:
- strategy
- answer
- strategies
- memory
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Experience-Guided Reasoner (EGuR), a system
  that dynamically generates complete strategies for AI problem-solving at inference
  time based on accumulated experience. Unlike existing approaches that modify inputs
  to fixed agents or require offline optimization, EGuR produces complete computational
  procedures including LLM calls, tools, sampling parameters, and control flow tailored
  to each problem.
---

# Experience-Guided Adaptation of Inference-Time Reasoning Strategies

## Quick Facts
- arXiv ID: 2511.11519
- Source URL: https://arxiv.org/abs/2511.11519
- Reference count: 40
- One-line primary result: EGuR achieves up to 14% accuracy improvements over baselines while reducing computational costs by up to 111×

## Executive Summary
This paper presents Experience-Guided Reasoner (EGuR), a system that dynamically generates complete strategies for AI problem-solving at inference time based on accumulated experience. Unlike existing approaches that modify inputs to fixed agents or require offline optimization, EGuR produces complete computational procedures including LLM calls, tools, sampling parameters, and control flow tailored to each problem. The system operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory, while a Consolidator integrates execution feedback to improve future strategy generation. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111×, with both metrics improving as the system gains experience.

## Method Summary
EGuR dynamically generates complete computational strategies at inference time without requiring offline optimization. The system uses a Guide LLM to produce k candidate strategies per problem, which are executed in parallel with their execution traces, costs, and verifier feedback collected. A Consolidator LLM processes this experience to update structured memory containing a Strategy Library of successful strategies and General Notes of high-level heuristics. The Guide then uses this memory to inform future strategy generation, enabling continual adaptation. The approach contrasts with methods that only modify input text or require offline training, instead generating full computational procedures that can adapt sampling parameters, tool configurations, and control flow for each problem.

## Key Results
- Achieves up to 14% accuracy improvements over strongest baselines on AIME 2025 and 3-SAT tasks
- Reduces computational costs by up to 111× compared to existing methods
- Both accuracy and cost metrics improve as the system gains experience through adaptive strategy refinement
- EGuR-5 (k=5 exploration) outperforms EGuR-1 (k=1) on most tasks, validating comparative strategy evaluation
- On BBEH tasks, improvements are more modest or absent, suggesting task-specific limitations

## Why This Works (Mechanism)

### Mechanism 1: Complete Strategy Specification at Inference Time
- Claim: Generating full computational procedures (not just input modifications) enables broader adaptation across strategy components.
- Mechanism: A meta-strategy (LLM-based Guide) outputs executable strategy code specifying prompts, sampling parameters, tool configurations, and control flow. This differs from prior methods that only prepend memory text to fixed agent inputs.
- Core assumption: The Guide LLM has sufficient zero-shot capability to produce valid, executable strategy code for diverse problem types.
- Evidence anchors:
  - [abstract] "generates tailored strategies—complete computational procedures involving LLM calls, tools, sampling parameters, and control logic—dynamically at inference time"
  - [Section 3.1] "The Guide generates complete strategy specifications tailored to each problem based on the current context and past experiences."
  - [corpus] Weak direct corpus support; OptScale addresses inference-time scaling but via sampling heuristics rather than strategy generation.
- Break condition: If the base Guide LLM lacks sufficient coding ability or domain knowledge, generated strategies may be syntactically invalid or semantically poor.

### Mechanism 2: Comparative Strategy Evaluation Within Episodes
- Claim: Generating multiple strategies per problem and comparing their relative performance accelerates learning of effective strategy selection.
- Mechanism: The Guide produces k candidate strategies per problem; all execute in parallel with execution traces, costs, and verifier feedback collected. The Consolidator records which strategies outperform alternatives on similar problems, building relative preference signals.
- Core assumption: Verifier feedback (binary correct/incorrect) is available and reliable; execution cost correlates with desirability.
- Evidence anchors:
  - [abstract] "the system learns the relative effectiveness of different strategies, leading to continual improvements in both accuracy and computational efficiency"
  - [Figure 5] EGUR-5 outperforms EGUR-1 on most tasks, showing comparative evaluation provides benefits beyond absolute feedback.
  - [corpus] No direct corpus neighbor addresses this comparative mechanism.
- Break condition: If the task lacks reliable verification, or if k strategies all fail, comparative learning degrades.

### Mechanism 3: Structured Memory with Selective Retention
- Claim: Maintaining a Strategy Library (successful strategies with problem characteristics) and General Notes (high-level heuristics) improves future strategy generation while preventing unbounded memory growth.
- Mechanism: The Consolidator processes experience tuples (query, answer, trace, cost, feedback) and updates Σ via selective abstraction—retaining frequently useful patterns, pruning outdated entries, and caching compiled strategies for direct reuse.
- Core assumption: The Consolidator LLM can make good retention decisions; memory stays under ~10k tokens to avoid distraction.
- Evidence anchors:
  - [Section 3.3] "The context Σ maintains two key components: a Strategy Library... and General Notes... To prevent unbounded memory growth, the Consolidator implements selective retention policies."
  - [Figure 4] EGuR maintains low and decreasing costs while Dynamic Cheatsheet's costs grow unboundedly from accumulating raw memory.
  - [corpus] ATLAS (Continual Learning, Not Training) addresses online agent adaptation but via different mechanisms; no direct parallel to structured strategy memory.
- Break condition: If memory management becomes suboptimal (too aggressive pruning or excessive retention), guidance quality degrades.

## Foundational Learning

- Concept: **Strategies as Compositions of Stateful Processes**
  - Why needed here: The paper formalizes strategies using a grammar of sequential/parallel composition, conditionals, and recursion. Understanding this compositional view is essential to grasp what EGuR generates.
  - Quick check question: Can you explain why CodeAct is classified as an "agent" (recursive) while CoT is a "pipeline" (no conditionals or recursion)?

- Concept: **Inference-Time vs. Offline Optimization**
  - Why needed here: EGuR's key distinction is adapting at inference time without requiring offline training phases. Prior methods like ADAS optimize offline then remain static.
  - Quick check question: What can EGuR modify at inference time that Dynamic Cheatsheet cannot?

- Concept: **Textual Steering vs. Complete Strategy Adaptation**
  - Why needed here: The paper contrasts methods that only modify input text (Mem0, Dynamic Cheatsheet) with EGuR's ability to change sampling parameters, remove tools, and switch paradigms.
  - Quick check question: If a problem is better solved with a single LLM call at temperature 0 rather than an agentic code interpreter loop, which approach can make that switch?

## Architecture Onboarding

- Component map:
  Guide (LLM) -> Strategy Executor -> Verifier -> Consolidator (LLM) -> Memory Store (Σ) -> Guide (LLM)

- Critical path:
  1. Question arrives → Guide generates k strategies using current Σ
  2. All k strategies execute in parallel, collecting traces and costs
  3. Verifier evaluates each answer
  4. Consolidator processes k experience tuples, updates Σ
  5. First strategy's answer returned (system learns from all k for future)

- Design tradeoffs:
  - **Exploration factor k**: Higher k enables better comparative learning but increases per-problem cost. Paper uses k=5 for Claude, k=3 for smaller models.
  - **Memory size**: Must stay under ~10k tokens (per Consolidator prompt) or memory distracts more than it helps.
  - **Verifier dependency**: System requires reliable verification; weaker signals (LLM-as-judge) are noted as future work, not yet evaluated.
  - **Guide LLM quality**: Strategy generation quality hinges on Guide's zero-shot coding ability.

- Failure signatures:
  - Generated strategy is syntactically invalid → parse error before execution
  - All k strategies produce incorrect answers → no strategy saved to memory; only negative patterns extracted
  - Memory grows beyond threshold → Consolidator prompted to delete rather than add
  - Code interpreter harms performance on non-algorithmic tasks → EGuR should learn to remove it (observed in Object Counting)

- First 3 experiments:
  1. **Ablate exploration factor**: Run EGuR-1, EGuR-3, EGuR-5 on a held-out subset of one benchmark; plot accuracy vs. cumulative cost to validate the comparative evaluation benefit shown in Figure 5.
  2. **Memory analysis**: After training on 100 samples, inspect Σ contents. Verify Strategy Library contains task-appropriate cached strategies and General Notes capture non-obvious heuristics (e.g., "remove code interpreter for non-algorithmic tasks").
  3. **Cross-task transfer**: Train on BBEH Object Counting, evaluate zero-shot on a structurally similar counting task. Check if learned strategy (single LLM call with detailed parsing instructions) transfers or overfits to training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EGuR learn effective strategies using weaker supervision signals such as LLM-based evaluation instead of ground truth verifiers?
- Basis in paper: [explicit] "Exploring whether this feedback can be replaced with weaker signals—such as LLM-based evaluation—is an important direction."
- Why unresolved: All experiments use ground-truth verifiers providing binary feedback; weaker signals may introduce noise that degrades learning.
- What evidence would resolve it: Experiments comparing LLM-based evaluators against ground-truth verifiers across the same benchmarks.

### Open Question 2
- Question: Can training or reinforcement learning improve the Guide's strategy generation for unfamiliar problem types where zero-shot generation is suboptimal?
- Basis in paper: [explicit] "The effectiveness of the system hinges on the Guide's zero-shot strategy generation capabilities, which may be suboptimal for unfamiliar problem types. In this case, it may be necessary to train or optimize the Guide through reinforcement learning or other training methods."
- Why unresolved: The current Guide relies entirely on zero-shot LLM capabilities without any fine-tuning.
- What evidence would resolve it: Comparison of fine-tuned versus zero-shot Guide performance on out-of-distribution tasks.

### Open Question 3
- Question: Can meta-learning approaches improve the Consolidator's ability to balance memory size against information utility?
- Basis in paper: [explicit] "The Consolidator relies on an LLM to manage memory, which may not optimally balance memory size against information utility, and may benefit from meta-learning approaches for more effective memory management."
- Why unresolved: Current memory management uses heuristic retention policies rather than learned optimization.
- What evidence would resolve it: Experiments comparing meta-learned memory policies against the current LLM-based approach on long-horizon tasks.

### Open Question 4
- Question: How does EGuR performance scale when the memory size approaches the 10,000-token limit mentioned, and what retrieval mechanisms might extend this constraint?
- Basis in paper: [inferred] The Consolidator prompt specifies "ensure the memory stays shorter than 10000 tokens or else it will distract more than it helps," but scaling behavior at this boundary is not analyzed.
- Why unresolved: No experiments examine performance degradation or mitigation strategies near memory capacity limits.
- What evidence would resolve it: Ablation studies varying memory size up to and beyond the current threshold with different retrieval mechanisms.

## Limitations
- Performance degrades on open-ended or weakly verified tasks where reliable verifiers are unavailable
- Memory management relies on heuristic retention rather than learned optimization, potentially suboptimal for long-term scaling
- System performance depends heavily on the quality of the base Guide LLM's zero-shot coding ability

## Confidence
- **High**: The core mechanism of complete strategy generation at inference time, and the observed improvements on AIME 2025 and 3-SAT tasks
- **Medium**: The comparative strategy evaluation benefit (k=5 vs. k=1), as the signal is present but not uniformly strong across all tasks
- **Low**: The applicability of EGuR to open-ended or weakly verified tasks, and the long-term stability of the memory system

## Next Checks
1. Run ablation on exploration factor (EGuR-1, EGuR-3, EGuR-5) on a held-out subset of one benchmark to confirm the comparative evaluation benefit shown in Figure 5
2. Inspect memory contents after training on 100 samples to verify that Strategy Library contains task-appropriate cached strategies and General Notes capture non-obvious heuristics
3. Test cross-task transfer: train on BBEH Object Counting, then evaluate zero-shot on a structurally similar counting task to check for overfitting or genuine strategy generalization