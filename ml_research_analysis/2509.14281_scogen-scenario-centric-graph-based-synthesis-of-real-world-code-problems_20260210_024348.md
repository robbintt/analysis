---
ver: rpa2
title: 'SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems'
arxiv_id: '2509.14281'
source_url: https://arxiv.org/abs/2509.14281
tags:
- domain
- knowledge
- code
- coding
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the scarcity of real-world coding problems
  that constrains the advancement of code large language models. The authors propose
  SCoGen, a framework that synthesizes realistic code problems by extracting application
  scenarios, domain knowledge, domain skills, and coding skills from real-world datasets
  like Stack Overflow and Kaggle.
---

# SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems

## Quick Facts
- arXiv ID: 2509.14281
- Source URL: https://arxiv.org/abs/2509.14281
- Reference count: 40
- Outperforms state-of-the-art open-source models on real-world benchmarks (24.32% accuracy on BigCodeBench Instruct Hard vs. 18.24% baseline)

## Executive Summary
This paper addresses the scarcity of real-world coding problems that constrains the advancement of code large language models. The authors propose SCoGen, a framework that synthesizes realistic code problems by extracting application scenarios, domain knowledge, domain skills, and coding skills from real-world datasets like Stack Overflow and Kaggle. These elements are integrated into a scenario-centric graph, and a sampling strategy controls the complexity and diversity of generated problems. Experiments show that fine-tuned models using SCoGen significantly outperform state-of-the-art open-source models on real-world benchmarks while maintaining competitive performance on algorithm-level tasks.

## Method Summary
SCoGen synthesizes realistic code problems by first curating Stack Overflow (11M→3M) and Kaggle (1.5M→0.5M) datasets with deduplication and filtering. It extracts four elements—application scenarios, domain knowledge, domain skills, and coding skills—using LLM prompts, then constructs a scenario-centric graph where scenarios serve as central nodes connected to the other elements via co-occurrence relationships. A sampling strategy traverses this graph using temperature-controlled probabilities to select feature sets of configurable complexity (1-3 feature combinations). Finally, Qwen3-32B generates problem-solution pairs from these sampled features, which are used to fine-tune code LLMs via supervised learning.

## Key Results
- SCoGen fine-tuned models achieve 24.32% accuracy on BigCodeBench Instruct Hard, outperforming 18.24% baseline
- Random sampling strategy (33.42% avg) outperforms LLM-based sampling (32.39% avg) on benchmark evaluation
- C1T3 configuration (Complexity 1, Temperature 3) yields best overall performance at 34.61% average accuracy
- Maintains competitive performance on algorithm-level tasks while excelling at real-world problems

## Why This Works (Mechanism)

### Mechanism 1: Scenario-Centric Knowledge Graph Construction
The framework structures code problems around application scenarios, creating interconnected representations that capture how real programming tasks require orchestration of multiple competencies. The graph uses scenarios as central nodes connected to domain knowledge, domain skills, and coding skills via co-occurrence edges, capturing the inherent interdependencies in real-world programming.

### Mechanism 2: Probabilistic Graph Sampling with Temperature Control
Temperature-controlled transition probabilities enable controlled diversity in generated problems by balancing coherence and exploration. As temperature increases, the distribution flattens, giving higher weight to lower-probability paths and promoting greater diversity. This creates a tradeoff where high complexity with high temperature degrades performance due to semantic incompatibility.

### Mechanism 3: Complexity Scaling via Feature Stacking
Problem difficulty correlates with the number of required competency combinations. Each "feature" is a (domain knowledge, domain skill, coding skill) triplet, with C1=1 feature, C2=2 features, and C3=3 features. Interdependencies emerge naturally from graph structure, with performance increasing from C1→C3 under fixed temperature conditions.

## Foundational Learning

- **Knowledge Graphs with Typed Edges**
  - Why needed here: The framework depends on constructing and traversing a graph with specific edge types (AS-DK, AS-CS, DK-DS, DK-DK, CS-CS)
  - Quick check question: Can you explain why second-step neighbors improve diversity over first-step neighbors alone?

- **Temperature in Probability Distributions**
  - Why needed here: Temperature T controls the exploration-exploitation tradeoff in graph traversal
  - Quick check question: What happens to transition probabilities when T=1 vs T=3?

- **Supervised Fine-Tuning (SFT) for Code LLMs**
  - Why needed here: The paper uses SFT on 500K synthetic pairs, comparing against instruction-tuned and distilled baselines
  - Quick check question: Why does SCoGen achieve competitive results with SFT alone versus models using SFT+RL?

## Architecture Onboarding

- **Component map:**
  - Seed Curation: Stack Overflow (11M→3M) + Kaggle (1.5M→0.5M) with hashing deduplication + MinHash near-duplicate detection
  - Element Extraction: LLM prompts extract AS, DK, DS, CS with detailed usage (see Appendix B, C)
  - Knowledge Graph: G=(V,E) where V=AS∪DK∪DS∪CS; five typed edge relationships
  - Sampling Strategy: Random (preferred) or LLM-based; configurable complexity (C1-C3) and temperature (T1-T3)
  - Synthesis/Answer: Qwen3-32B generates problems and solutions (no verification)

- **Critical path:**
  1. Seed quality → extraction fidelity → graph semantics
  2. Graph structure → traversal diversity
  3. Sampling config (C+T) → problem coherence vs diversity balance
  4. Prompt engineering → final problem realism

- **Design tradeoffs:**
  - Random vs LLM sampling: Random (33.42% avg) outperformed LLM-based (32.39%)—simpler is better, LLMs may introduce selection bias
  - Complexity vs coherence: Authors recommend C1T3–C2T3; avoid C3T3
  - Scale vs verification: 500K pairs without answer verification acceptable for problem synthesis; deferred for future work

- **Failure signatures:**
  - C3T3 produces incoherent problems (nine diverse elements with amplified randomness)
  - Generic scenarios like "Data Processing System" fail to constrain problem space
  - Answer verification gap may propagate incorrect solutions

- **First 3 experiments:**
  1. Reproduce C1T3 vs C3T3 comparison on Qwen2.5-Coder-7B with 50K subset to validate complexity-temperature tradeoff locally
  2. Visualize a subgraph for one domain (e.g., medical imaging from Appendix C) to inspect edge distributions and node connectivity
  3. Ablate random vs LLM-based sampling on a held-out seed set to confirm random sampling superiority isn't dataset-specific

## Open Questions the Paper Calls Out

- **Does the inclusion of a formal answer verification mechanism significantly improve the reliability of the synthetic data?**
  - Currently, answer generation relies solely on Qwen3-32B without code validation or execution testing. The paper intends to explore answer verification in future work.

- **Can the scenario-centric graph framework be effectively extended to synthesize coherent repository-level challenges?**
  - The current framework focuses on isolated code generation tasks rather than interconnected software repository structures. Repository-level benchmarks like RepoBench would be needed for validation.

- **How does the efficacy of the SCoGen framework scale when applied to models significantly larger than 8B parameters?**
  - Experiments were restricted to 1.5B-8B parameter models. The paper intends to investigate scalability for 32B+ parameter models.

- **Does LLM-based feature sampling introduce a selection bias that inherently reduces problem diversity?**
  - The paper hypothesizes that LLM-based sampling may introduce bias leading to reduced diversity, but doesn't quantitatively measure semantic diversity differences between sampling methods.

## Limitations

- Answer quality without automated verification raises concerns about propagating incorrect solutions through the training corpus
- Generic scenarios may not sufficiently constrain the problem space, leading to vague or unfocused problems
- Quality of the scenario-centric graph heavily depends on the LLM's extraction accuracy, with no systematic evaluation of extraction fidelity provided

## Confidence

- **High Confidence**: The controlled diversity mechanism via temperature scaling, the complexity scaling via feature stacking, and the comparative advantage on real-world benchmarks
- **Medium Confidence**: The scenario-centric knowledge graph construction, the random sampling strategy outperforming LLM-based sampling, and the claim that SCoGen creates more realistic problems than existing methods
- **Low Confidence**: The long-term impact on downstream model performance beyond immediate benchmarks, the generalizability across different LLM architectures for extraction, and the effectiveness of the framework for domains not represented in Stack Overflow/Kaggle

## Next Checks

1. **Reproduce the C1T3 vs C3T3 comparison** on a smaller subset (50K pairs) using Qwen2.5-Coder-7B to validate the complexity-temperature tradeoff locally and ensure it's not dataset-specific.

2. **Conduct a systematic analysis of extracted scenarios** by sampling 100 scenarios from different domains and rating their specificity and usefulness in constraining problem generation. This would validate the core assumption about scenario quality.

3. **Implement automated verification** for a subset of generated problems (e.g., using test case generation or code execution) to quantify the error rate in synthesized solutions and assess the practical impact of the current verification gap.