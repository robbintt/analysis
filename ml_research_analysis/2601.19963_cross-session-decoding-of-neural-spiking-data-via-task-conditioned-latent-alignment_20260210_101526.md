---
ver: rpa2
title: Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment
arxiv_id: '2601.19963'
source_url: https://arxiv.org/abs/2601.19963
tags:
- session
- neural
- latent
- data
- tcla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-session nonstationarity in neural activity
  recorded by implanted electrodes, which poses a major challenge for invasive brain-computer
  interfaces (BCIs). Decoders trained on one session often fail to generalize to subsequent
  sessions, particularly when only limited data are available from new sessions.
---

# Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment

## Quick Facts
- arXiv ID: 2601.19963
- Source URL: https://arxiv.org/abs/2601.19963
- Authors: Canyang Zhao; Bolin Peng; J. Patrick Mayo; Ce Ju; Bing Liu
- Reference count: 18
- Primary result: TCLA framework improves cross-session neural decoding by up to 0.386 R² in limited data scenarios

## Executive Summary
The paper addresses the critical challenge of cross-session nonstationarity in neural activity recorded by implanted electrodes, which severely limits the performance of invasive brain-computer interfaces (BCIs). When decoders trained on one recording session are applied to subsequent sessions, performance degrades significantly due to changes in neural activity patterns. The proposed Task-Conditioned Latent Alignment (TCLA) framework tackles this by leveraging abundant data from a source session to improve decoding in target sessions with limited data. TCLA learns a low-dimensional representation of neural dynamics from the source session using an autoencoder and aligns target session latent representations to the source in a task-conditioned manner, preserving the task-dependent structure of neural activity across sessions.

The framework was evaluated on three macaque motor and oculomotor center-out datasets covering both arm and eye movements. Compared to baseline methods trained solely on target-session data, TCLA consistently improved decoding performance across datasets and settings. The most significant gains (up to 0.386 R² improvement for y-coordinate velocity decoding) occurred when baseline performance was weakest, demonstrating TCLA's effectiveness in low-data scenarios where traditional approaches fail.

## Method Summary
TCLA employs a two-stage approach to address cross-session nonstationarity in neural decoding. First, it learns a low-dimensional latent representation of neural dynamics from the source session using an autoencoder architecture, which compresses high-dimensional spiking data into a compact representation that captures the essential task-related information. Second, it aligns the target session's latent representations to the source session's latent space in a task-conditioned manner, meaning the alignment preserves the inherent task-dependent structure in neural activity. This alignment is achieved through a shared latent space that ensures the task structure is maintained across sessions while adapting to session-specific variations. The framework was tested on three macaque datasets involving motor and oculomotor tasks, demonstrating consistent improvements over baseline methods that train only on target-session data.

## Key Results
- TCLA consistently improved decoding performance across three macaque motor and oculomotor datasets
- Achieved R² gains of up to 0.386 for y-coordinate velocity decoding compared to target-only baselines
- Improvements were most pronounced when baseline decoding performance was weakest
- Framework demonstrated effectiveness for both arm and eye movement decoding tasks

## Why This Works (Mechanism)
The framework works by learning a shared latent representation space that captures task-relevant neural dynamics while being invariant to session-specific variations. By first learning the latent structure from a source session with abundant data, TCLA establishes a robust representation of task-related neural patterns. The task-conditioned alignment then maps target session data into this learned space while preserving the task structure, effectively normalizing across sessions. This approach is particularly effective in low-data scenarios because it leverages the rich source data to compensate for limited target data, reducing the impact of session-specific noise and drift while maintaining task-relevant information.

## Foundational Learning
- **Neural nonstationarity**: Neural activity recorded by implanted electrodes changes across sessions due to electrode drift, tissue response, and neural adaptation. This variability is the primary challenge TCLA addresses, requiring methods that can normalize across sessions while preserving task information.
- **Autoencoder architectures**: The framework uses autoencoders to learn compressed representations of neural data. This dimensionality reduction is crucial for capturing the essential task-related structure while filtering out session-specific noise and redundancy.
- **Task-conditioned alignment**: Unlike simple alignment methods, TCLA preserves the task-dependent structure during alignment. This ensures that the learned representation maintains the relationship between neural activity and behavioral variables across sessions.

## Architecture Onboarding

**Component Map:**
Neural Spiking Data -> Autoencoder (Source) -> Latent Space -> Task-Conditioned Alignment -> Decoded Output

**Critical Path:**
Source Session Data → Autoencoder Training → Latent Space Learning → Target Session Alignment → Decoding Performance

**Design Tradeoffs:**
The framework balances between preserving task-relevant information (through task-conditioned alignment) and achieving session invariance (through shared latent space). This tradeoff is critical because overly aggressive alignment could erase task information while insufficient alignment fails to address cross-session variability.

**Failure Signatures:**
- Poor performance when source and target sessions have fundamentally different neural dynamics
- Degradation when task structure changes significantly between sessions
- Limited effectiveness in extremely low-data regimes (fewer than 5-10 trials)

**First Experiments to Run:**
1. Test TCLA on human intracranial recording datasets to validate cross-species generalizability
2. Evaluate performance degradation under simulated electrode drift to assess robustness limits
3. Conduct ablation studies removing the task-conditioned component to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to macaque motor and oculomotor datasets, restricting generalizability to human applications
- Performance improvements show variable magnitudes across movement dimensions and recording sites
- Computational overhead of autoencoder and alignment components not discussed for real-time BCI deployment
- Framework assumes source session adequately represents target session latent structure

## Confidence
- **High confidence**: Methodology for task-conditioned latent alignment is technically sound with established autoencoder architecture
- **Medium confidence**: Reported performance improvements are reproducible within tested datasets, though absolute gains vary
- **Medium confidence**: Framework's effectiveness in low-data scenarios demonstrated, but "limited data" lower bound not precisely defined

## Next Checks
1. Evaluate TCLA on human intracranial recording datasets to assess cross-species generalizability and test performance with more variable electrode configurations
2. Conduct ablation studies to quantify individual contributions of autoencoder architecture versus task-conditioned alignment components
3. Test framework's robustness to simulated electrode drift and signal degradation to understand real-world deployment limits