---
ver: rpa2
title: Gender Bias in Emotion Recognition by Large Language Models
arxiv_id: '2511.19785'
source_url: https://arxiv.org/abs/2511.19785
tags:
- emotion
- gender
- llms
- bias
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines gender bias in emotion recognition by large
  language models (LLMs). It investigates whether LLMs exhibit gender biases when
  asked to infer emotions from image captions, using a dataset with gender-swapped
  and gender-neutral versions of the same descriptions.
---

# Gender Bias in Emotion Recognition by Large Language Models

## Quick Facts
- **arXiv ID:** 2511.19785
- **Source URL:** https://arxiv.org/abs/2511.19785
- **Reference count:** 13
- **Primary result:** Fine-tuning with gender-augmented data reduces gender bias in LLM emotion predictions, while prompt-based approaches fail to mitigate bias.

## Executive Summary
This paper investigates gender bias in emotion recognition by large language models when inferring emotions from image captions. Using a dataset with gender-swapped and gender-neutral caption variants, the study tests multiple LLMs and finds significant gender biases in some models. The research evaluates debiasing strategies including prompt engineering and fine-tuning. While inference-time prompt-based approaches proved ineffective and sometimes increased bias, fine-tuning with gender-augmented data significantly mitigated gender-related patterns in emotion predictions. The findings demonstrate that training-based interventions are more effective than prompt-based approaches for achieving fairer emotion recognition in LLMs.

## Method Summary
The study used 1,000 NarraCap captions from the EMOTIC dataset, creating gender-swapped (man↔woman) and gender-neutral variants. Zero-shot inference was performed across multiple LLMs using a prompt requesting comma-separated emotions from 26 EMOTIC categories. Bias was quantified using chi-square tests comparing emotion predictions across genders, with significant deviation from a 50:50 baseline indicating bias. For debiasing, 100 additional captions were augmented to 300 pairs and used to fine-tune Mistral-7B-Instruct-v0.3 via QLoRA. The fine-tuned model was then re-evaluated on the original test set to measure bias reduction.

## Key Results
- Fine-tuning with gender-augmented data effectively mitigated gender-related biases in emotion predictions
- Prompt-based approaches (prompt engineering, in-context learning, chain-of-thought) did not significantly reduce bias and sometimes exacerbated it
- Some LLMs (GPT-4o-mini, Mistral, LLaMA) exhibited significant gender bias for at least one emotion category (p < 0.05)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with gender-augmented data reduces gender bias by training the model to treat gender as irrelevant to emotion labels. Data augmentation creates triplets (original, gender-swapped, gender-neutral) with identical ground-truth emotion labels. Fine-tuning via QLoRA on these triplets teaches the model that gender perturbations should not shift emotion predictions, effectively desensitizing the model to gender cues.

### Mechanism 2
Inference-time prompt-based debiasing does not reliably reduce gender bias because biases are embedded in model weights. Prompt-based methods attempt to override learned biases at inference time, but since biases are deeply embedded in model weights, prompting cannot durably suppress these associations. In-context examples may even reinforce stereotypical mappings by making gender salient.

### Mechanism 3
Gender bias detection via chi-square tests on paired gender-swapped captions provides quantifiable measure of model fairness using an equal-distribution baseline. By presenting identical captions with only gender changed and comparing predicted emotion distributions, significant deviations from a 50:50 baseline indicate gender-dependent predictions.

## Foundational Learning

- **Concept: Data augmentation for debiasing**
  - Why needed here: The paper's successful intervention relies on creating gender-swapped and gender-neutral caption variants with identical labels
  - Quick check: Given "The man looked anxious in the crowded room," can you generate (a) a gender-swapped version and (b) a gender-neutral version while preserving the emotion label?

- **Concept: Chi-square test for categorical association**
  - Why needed here: The paper uses chi-square tests to quantify whether predicted emotions associate with gender
  - Quick check: If a model predicts "pleasure" 40 times for women and 20 times for men, what would a significant chi-square result (p < 0.05) indicate?

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: Fine-tuning experiments used QLoRA with 4-bit quantization
  - Quick check: Why might QLoRA be preferred over full fine-tuning for a 7B-parameter model when only 100 augmented samples are available?

## Architecture Onboarding

- **Component map:** NarraCap captions -> Gender augmentation (swap/neutralize) -> Model backbone (Mistral-7B-Instruct-v0.3) -> QLoRA fine-tuning layer -> Chi-square evaluation layer

- **Critical path:** Sample 1000 NarraCap captions → Generate gender variants (3000 total) → Zero-shot inference across models → Compute chi-square statistics → Fine-tune Mistral with 300 augmented samples → Re-evaluate bias

- **Design tradeoffs:** Prompt engineering vs. fine-tuning (no weight updates vs. effective bias reduction), binary gender only (dataset limitations), small fine-tuning set (100 samples) vs. computational efficiency

- **Failure signatures:** In-context learning introducing new biases, fine-tuned models failing to predict certain emotions, models outputting emotions outside 26 EMOTIC categories

- **First 3 experiments:**
  1. Baseline bias audit: Run zero-shot inference on gender-swapped caption pairs and compute chi-square per emotion
  2. Minimal fine-tuning test: Augment 100 captions, fine-tune with QLoRA, compare pre/post chi-square statistics
  3. Generalization probe: Evaluate fine-tuned model on held-out subset with different contextual domains

## Open Questions the Paper Calls Out

### Open Question 1
How does gender bias in LLM emotion recognition manifest for non-binary or gender-diverse identities? The authors explicitly state the dataset includes only binary gender categories and future work should cover all gender diversities.

### Open Question 2
To what extent do observed gender biases generalize to multimodal or conversational settings? The study uses text captions from static visual scenes, cautioning that biases may not fully generalize to settings where tone and body language are factors.

### Open Question 3
Is the adopted "equal distribution baseline" a valid gold standard for fairness given potential true variations in emotional expression across genders? The authors acknowledge emotional expression may vary across genders but lack data to confirm or deny this.

## Limitations
- Binary gender only: The study is limited to man/woman categories and does not address non-binary identities or intersectional biases
- Small fine-tuning dataset: The 100-sample fine-tuning set raises questions about robustness and generalization to diverse contexts
- Text-based approach: The methodology relies on text captions rather than direct image analysis, limiting applicability to multimodal emotion recognition

## Confidence
- **High Confidence:** Fine-tuning with gender-augmented data reduces gender bias (clear pre/post comparison showing significant reduction)
- **Medium Confidence:** Prompt-based debiasing approaches are ineffective (results show they may increase bias, but mechanisms less thoroughly explained)
- **Medium Confidence:** Chi-square measurement methodology with 50:50 baseline (standard statistical approach but baseline assumption may be problematic)

## Next Checks
1. **Cross-domain generalization test:** Evaluate the fine-tuned model on emotion recognition tasks from different domains to verify debiasing generalizes beyond the augmentation distribution
2. **Intersectional bias audit:** Extend analysis to include intersectional factors by creating caption variants combining gender with other demographic attributes
3. **Real-world emotion alignment validation:** Compare model predictions against human-annotated emotion data that accounts for actual gender differences in emotional expression