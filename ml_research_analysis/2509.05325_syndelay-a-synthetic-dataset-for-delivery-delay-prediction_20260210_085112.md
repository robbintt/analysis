---
ver: rpa2
title: 'SynDelay: A Synthetic Dataset for Delivery Delay Prediction'
arxiv_id: '2509.05325'
source_url: https://arxiv.org/abs/2509.05325
tags:
- data
- dataset
- supply
- chain
- delivery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynDelay is a synthetic dataset for delivery delay prediction in
  supply chains, addressing the lack of open, high-quality benchmark data. It was
  generated using a generative model trained on real-world data, preserving realistic
  delivery patterns while ensuring privacy.
---

# SynDelay: A Synthetic Dataset for Delivery Delay Prediction

## Quick Facts
- arXiv ID: 2509.05325
- Source URL: https://arxiv.org/abs/2509.05325
- Reference count: 8
- Primary result: Synthetic dataset with 155k rows, 41 features for delivery delay prediction, with ensemble baselines showing modest performance

## Executive Summary
SynDelay is a synthetic dataset designed to address the lack of open, high-quality benchmark data for delivery delay prediction in supply chains. The dataset was generated using a generative model trained on real-world data, preserving realistic delivery patterns while ensuring privacy. Baseline models were evaluated using metrics such as accuracy, macro F1, weighted F1, and per-class precision/recall. Ensemble methods (Random Forest, XGBoost, CatBoost) outperformed trivial baselines, with Random Forest achieving the highest macro F1 (0.4833) and weighted F1 (0.5601). The dataset is publicly available through the Supply Chain Data Hub, supporting reproducible research and benchmarking in supply chain AI.

## Method Summary
The dataset contains 155,488 rows and 41 variables, including a three-class delivery outcome. The synthetic data was generated using a generative model that integrates LLM-based reasoning with a score-based diffusion model operating in latent space. This approach preserves statistical properties and inter-column logical relationships while ensuring privacy. Baseline models were trained using minimal transformations without domain-specific feature engineering, with a 0.8:0.1:0.1 train/val/test split and 10 independent runs reporting mean ± std.

## Key Results
- Random Forest achieved the highest macro F1 (0.4833) and weighted F1 (0.5601)
- ZeroR baseline achieved high accuracy (0.577) but very low macro F1 (0.2439)
- Ensemble methods (Random Forest, XGBoost, CatBoost) outperformed trivial baselines
- The dataset presents a challenging benchmark with modest performance across all models

## Why This Works (Mechanism)

### Mechanism 1: Latent Diffusion with Logical Constraints
The synthetic data generation pipeline likely preserves statistical fidelity and inter-column logical relationships better than standard generative models by integrating LLM-based reasoning with diffusion processes. The framework extracts inter-column relationships using an LLM, which constrains a score-based diffusion model operating in latent space. This forces the generative process to honor logical dependencies (e.g., delivery date > order date) that purely statistical models might violate. The mechanism degrades if the underlying LLM fails to infer correct logical schemas from the raw data headers, resulting in synthetic data that violates business logic.

### Mechanism 2: Ensemble Learning for Imbalanced Multi-class Targets
Ensemble methods (Random Forest, XGBoost) function as effective baselines because they handle mixed feature types and class imbalance without requiring extensive preprocessing. Boosting methods (XGBoost, CatBoost) iteratively correct errors on hard-to-predict samples, while Bagging (Random Forest) reduces variance. This allows them to navigate the trade-off between the majority class (Delayed, ~60%) and minority classes (Early, On-time) better than trivial baselines. Performance plateaus if the synthetic data contains noise or inconsistencies that obscure the decision boundary.

### Mechanism 3: Benchmarking via Lower-Bound Establishment
The dataset serves as a calibration mechanism for the field, where the gap between trivial baselines (ZeroRule) and ensemble baselines defines the "learnable" signal vs. noise floor. By fixing the dataset and publishing baseline metrics, the authors transform a vague prediction task into a constrained optimization problem. The gap between ZeroRule accuracy (0.57) and ensemble accuracy (~0.57-0.58) indicates the dataset is "hard," forcing researchers to look beyond naive feature engineering.

## Foundational Learning

- **Concept: Tabular Diffusion Models**
  - Why needed here: SynDelay is generated by a latent diffusion process. Understanding how diffusion iteratively denoises data to recover tabular structures is required to diagnose generation artifacts.
  - Quick check question: Can you explain why a diffusion model might handle sparse, high-dimensional categorical variables differently than a continuous image pixel grid?

- **Concept: Macro vs. Weighted F1 Score**
  - Why needed here: The dataset is imbalanced (Class 2 is 60%). Relying on accuracy is misleading. Macro F1 is the specific metric highlighted as "balanced," and understanding its sensitivity to minority classes is critical for beating the baselines.
  - Quick check question: If a model improves Accuracy but lowers Macro F1 compared to ZeroRule, what is it likely doing to the minority classes?

- **Concept: LLM-driven Schema Extraction**
  - Why needed here: The data generation relies on an LLM to "extract inter-column relationships." Understanding that the synthetic data's logic is bounded by an LLM's inference capability explains potential logical inconsistencies.
  - Quick check question: How might an LLM misinterpret a column header like "Type 2 Delay" compared to a deterministic schema parser?

## Architecture Onboarding

- **Component map:** Source: Real-world delivery dataset (Retail, 180k rows) -> Generator: Preprocessing -> LLM (Relationship Extractor) -> Variational Encoder -> Latent Diffusion Sampler -> Decoder -> Consumer: Benchmarking Suite (sklearn/xgboost/catboost) -> Metric Aggregator (Accuracy, F1)
- **Critical path:** The quality of the LLM Relationship Extraction step. If the LLM fails to capture that `Delivery Date` depends on `Shipping Date`, the diffusion model generates statistically valid but logically invalid samples, corrupting the benchmark.
- **Design tradeoffs:** Privacy vs. Fidelity: The synthetic data guarantees privacy but explicitly trades off "noise and inconsistencies" for clean separability. Simplicity vs. SOTA: The authors use "minimal transformations" for baselines. New architects must decide whether to invest in domain-specific feature engineering or model architecture complexity to beat the modest baseline scores.
- **Failure signatures:** High Accuracy, Low Macro F1: Model is simply predicting the majority class (Delayed). Training Instability: If attempting to re-train the generative model, instability in the latent space indicates the LLM-constraints are conflicting with the diffusion score function.
- **First 3 experiments:**
  1. Reproduce the Baseline: Run Random Forest on the raw 41 variables to verify the Macro F1 (~0.48) and confirm the difficulty of beating the ZeroRule accuracy (~0.57).
  2. Feature Correlation Audit: Calculate correlation matrices for both the Original (if accessible) and SynDelay datasets to verify if the LLM-preserved "logical relationships" (e.g., distance vs. time) hold statistically.
  3. Minority Class Over-sampling: Apply SMOTE or class weights to the XGBoost model to specifically target the Macro F1 metric, testing the hypothesis that the "modest performance" is purely a class imbalance issue rather than a data quality issue.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do models trained on SynDelay transfer to real-world delivery delay prediction tasks? The paper validates that SynDelay preserves statistical patterns from real data but does not empirically compare model performance on synthetic versus actual held-out real data. Train models on SynDelay and evaluate on a held-out portion of the original real-world dataset; compare performance gaps.

### Open Question 2
Can domain-specific feature engineering substantially improve prediction performance over the provided baselines? The authors state "targeted feature engineering and hyperparameter tuning would likely lead to further improvements" after reporting modest baseline results. Systematic feature engineering experiments using supply chain domain knowledge, reporting gains over baseline metrics.

### Open Question 3
How well do models trained on this retail-focused dataset generalize to other supply chain contexts and industries? The paper states the dataset "is currently restricted to the retail sector" and calls for "extending future datasets to a wider range of industries and contexts." Evaluate retail-trained models on delivery data from manufacturing, healthcare, or other supply chain domains.

### Open Question 4
What advanced modeling approaches beyond standard ensemble methods can better handle the dataset's class imbalance and noise? The modest Macro F1 scores (0.31–0.48) and acknowledgment of noise suggest current methods inadequately address these challenges. Apply specialized techniques (e.g., cost-sensitive learning, deep learning, noise-robust methods) and demonstrate significant improvements in minority-class performance.

## Limitations
- Data Generation Fidelity: The synthetic dataset explicitly trades off noise and inconsistencies for privacy preservation, but the extent to which these artifacts impact real-world model generalization remains unclear.
- Hyperparameter Specification: The paper does not specify exact hyperparameters for ensemble models beyond "commonly-used," which may lead to variations in baseline reproducibility.
- Single-Source Generalization: The dataset is generated from a single retail source, raising questions about whether it captures the full diversity of global supply chain scenarios.

## Confidence
- **High Confidence:** The benchmark methodology and metric definitions are clearly specified and reproducible.
- **Medium Confidence:** The synthetic generation mechanism's effectiveness in preserving logical relationships is supported by the paper's methodology but not independently verified.
- **Low Confidence:** The dataset's ability to serve as a proxy for real-world supply chain AI development across diverse retail contexts.

## Next Checks
1. Calculate and compare correlation matrices between the original (if accessible) and SynDelay datasets to verify that LLM-preserved logical relationships hold statistically.
2. Apply SMOTE or class weighting specifically to the XGBoost model to determine whether modest baseline performance stems from class imbalance versus data quality issues.
3. Create test cases that verify whether the synthetic data respects fundamental supply chain constraints (e.g., delivery date > order date, transit time bounds) that the LLM should have extracted from the schema.