---
ver: rpa2
title: 'Projectable Models: One-Shot Generation of Small Specialized Transformers
  from Large Ones'
arxiv_id: '2506.05641'
source_url: https://arxiv.org/abs/2506.05641
tags:
- image
- arxiv
- task
- large
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a technique for generating smaller, task-specialized
  Transformer models from a large foundation model by projecting its parameters based
  on task identifiers. The method uses matrix generators to map weights from a large
  source model to smaller projected models, with projections conditioned on task-specific
  information.
---

# Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones

## Quick Facts
- **arXiv ID:** 2506.05641
- **Source URL:** https://arxiv.org/abs/2506.05641
- **Authors:** Andrey Zhmoginov, Jihwan Lee, Mark Sandler
- **Reference count:** 24
- **Primary Result:** Task-specialized small Transformers generated from large foundation models outperform universal models of the same size

## Executive Summary
This paper introduces a technique for generating small, task-specialized Transformer models from a large foundation model through parameter projection based on task identifiers. The method employs matrix generators to map weights from the source model to smaller projected models, with projections conditioned on task-specific information. Experiments demonstrate that these generated specialized models outperform universal conditional models of equivalent size on image modeling tasks, with the performance gap increasing for smaller models. The approach also enables knowledge transfer across related tasks and allows generating multiple model sizes from a single foundation model without degrading the source model's performance.

## Method Summary
The paper proposes a one-shot generation approach where a large foundation model serves as the basis for creating smaller, task-specialized Transformers. The core mechanism uses matrix generators to project parameters from the large source model to smaller target models. These projections are conditioned on task-specific identifiers, allowing the generation of models tailored to particular tasks. The method enables creating multiple specialized models of different sizes from the same foundation model, with the key innovation being that this generation process does not degrade the performance of the original source model. The approach is evaluated primarily on image modeling tasks, showing that task-specialized generated models outperform universal conditional models of the same size.

## Key Results
- Task-specialized generated models outperform universal conditional models of the same size on image modeling tasks
- Performance gap increases for smaller generated models, demonstrating greater relative benefit
- Knowledge transfer is demonstrated across related tasks through the projection mechanism
- Multiple model sizes can be generated from a single foundation model without degrading source model performance

## Why This Works (Mechanism)
The approach works by leveraging the rich representations learned by large foundation models and adapting them to specific tasks through conditional parameter projection. The matrix generators act as learned mappings that can extract and specialize relevant knowledge from the source model based on task identifiers. This allows the generation of models that are both smaller and more task-adapted than traditional universal models. The conditioning on task information ensures that the projected parameters are optimized for the specific characteristics of each task, rather than trying to maintain general-purpose capabilities.

## Foundational Learning
- **Matrix Generators:** Learnable parameter mappings that transform weights from large to small models
  - *Why needed:* Enables efficient adaptation of foundation model knowledge to smaller specialized models
  - *Quick check:* Verify the generator architecture can produce valid model parameters

- **Conditional Parameter Projection:** Weight transformations conditioned on task-specific information
  - *Why needed:* Allows generation of task-specialized models rather than universal ones
  - *Quick check:* Confirm task conditioning improves performance over unconditioned projection

- **One-Shot Generation:** Creating multiple specialized models from a single large model in a single pass
  - *Why needed:* Enables efficient deployment of multiple specialized models without retraining
  - *Quick check:* Measure generation time and resource requirements

- **Foundation Model Preservation:** Ensuring the source model remains unchanged during generation
  - *Why needed:* Allows the foundation model to continue serving other purposes
  - *Quick check:* Validate source model performance before and after generation

- **Knowledge Transfer Across Tasks:** Leveraging related task information during projection
  - *Why needed:* Improves performance on new tasks by utilizing learned relationships
  - *Quick check:* Test transfer performance across task families

## Architecture Onboarding

**Component Map:**
Foundation Model -> Matrix Generators -> Task Conditioners -> Specialized Small Models

**Critical Path:**
1. Foundation model parameters are passed to matrix generators
2. Task conditioners provide task-specific conditioning information
3. Matrix generators produce projected parameters for specialized models
4. Generated models are deployed for task-specific inference

**Design Tradeoffs:**
- Model size reduction vs. task specialization capability
- Generation computational cost vs. training cost of specialized models
- Foundation model capacity vs. number of specialized models that can be effectively generated
- Task conditioning complexity vs. generalization across similar tasks

**Failure Signatures:**
- Poor task performance indicates inadequate conditioning or generator architecture
- Degraded source model performance suggests interference during projection
- Unstable generation across similar tasks points to conditioning sensitivity issues
- Excessive computational overhead during generation indicates inefficient matrix operations

**3 First Experiments:**
1. Generate two specialized models from the same foundation model and compare their performance on respective tasks
2. Measure the degradation (if any) of the foundation model after multiple generation operations
3. Compare the performance of generated models against models trained from scratch on the same tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on image modeling tasks with limited validation on other domains
- Performance improvements demonstrated on a relatively narrow set of tasks and datasets
- Scalability concerns when generating hundreds or thousands of specialized tasks
- Matrix generator overhead may offset efficiency gains from smaller model sizes

## Confidence

**High Confidence:**
- The core technical approach of using matrix generators for parameter projection is sound and well-implemented

**Medium Confidence:**
- The empirical results showing performance improvements on image tasks are reliable but may not generalize to other domains
- The claim of non-degrading the source model performance appears supported but requires more extensive validation

## Next Checks
1. Test the approach on NLP and multimodal tasks to evaluate cross-domain generalizability
2. Conduct scaling studies to measure performance as the number of specialized tasks increases from dozens to hundreds
3. Perform ablation studies on the matrix generator architecture to quantify its contribution versus simpler projection methods