---
ver: rpa2
title: Supervised Contrastive Learning for Ordinal Engagement Measurement
arxiv_id: '2505.20676'
source_url: https://arxiv.org/abs/2505.20676
tags:
- engagement
- learning
- contrastive
- student
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses student engagement measurement in virtual
  learning environments, tackling class imbalance and the ordinal nature of engagement
  levels. The authors propose a novel approach combining supervised contrastive learning
  with data augmentation techniques and ordinal classification.
---

# Supervised Contrastive Learning for Ordinal Engagement Measurement

## Quick Facts
- arXiv ID: 2505.20676
- Source URL: https://arxiv.org/abs/2505.20676
- Reference count: 40
- Key outcome: Novel approach combining supervised contrastive learning with data augmentation and ordinal classification achieves 0.6732 accuracy on DAiSEE dataset, surpassing most prior methods

## Executive Summary
This paper addresses student engagement measurement in virtual learning environments, tackling class imbalance and the ordinal nature of engagement levels. The authors propose a novel approach combining supervised contrastive learning with data augmentation techniques and ordinal classification. Affective and behavioral features are extracted from video samples and used to train ordinal classifiers within a supervised contrastive learning framework, with sequential models (LSTM and TCN) as encoders. Time-series augmentation techniques are applied to enhance model training. The method is evaluated on the DAiSEE dataset, demonstrating superior performance compared to previous approaches. The proposed method achieves a classification accuracy of 0.6732, surpassing most prior methods and showing particular strength in handling class imbalance and reducing class confusion.

## Method Summary
The method uses a two-phase training approach with sequential encoders (LSTM or TCN) to classify student engagement from video data. First, affective features (valence/arousal from EmoFAN) and behavioral features (OpenFace 2.0) are extracted from video frames, along with 256-dimensional latent affective vectors reduced to 32 dimensions. Time-series augmentation (jittering, scaling, shifting, permutation, flipping) is applied to address class imbalance, with minority classes (0 and 1) augmented 10× and majority classes (2 and 3) augmented 1.5×. In Phase 1, the encoder and projection head are trained using supervised contrastive loss to learn discriminative representations. In Phase 2, the encoder is frozen and a new ordinal classifier (3 binary classifiers for 4 classes) is trained. The method achieves 0.6732 accuracy on DAiSEE, outperforming baseline approaches.

## Key Results
- Proposed method achieves 0.6732 accuracy on DAiSEE dataset, surpassing most prior methods
- Effectively handles class imbalance through time-series augmentation (10× for minority classes, 1.5× for majority classes)
- Reduces class confusion by enforcing ordinal structure through binary decomposition approach
- TCN encoder with latent features performs best, though LSTM shows higher recall for classes 1 and 3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive learning improves class separation by learning representations where same-class samples cluster together while different-class samples are pushed apart.
- Mechanism: The supervised contrastive loss maximizes similarity between representations of samples sharing the same engagement label while minimizing similarity between samples with different labels. This directly addresses class confusion by enforcing discriminative feature boundaries in the embedding space.
- Core assumption: Engagement levels can be distinguished through separable feature representations in the learned embedding space, and class labels provide reliable supervision for learning these boundaries.
- Evidence anchors:
  - [abstract] "proposed that utilizes supervised contrastive learning for ordinal classification of engagement"
  - [section III] "In this context, the term anchor is assigned to the index i, while the positive samples are used to denote samples with identical labels, and the negative samples refer to samples that possess different labels."
  - [corpus] Limited direct corpus evidence; contrastive learning for ordinal/engagement tasks is underexplored in neighboring papers.
- Break condition: If engagement labels are noisy or subjective (annotator disagreement), contrastive loss may reinforce incorrect cluster boundaries.

### Mechanism 2
- Claim: Time-series data augmentation mitigates class imbalance by synthetically increasing minority class samples while preserving temporal structure.
- Mechanism: Augmentation techniques (jittering, magnitude scaling, time shifting, permutation, flipping) create new training samples from existing feature sequences. Minority classes (0 and 1) are augmented by a factor of 10, while majority classes (2 and 3) by 1.5×, reducing the imbalance ratio without losing temporal dependencies critical for engagement inference.
- Core assumption: The selected augmentation techniques preserve the underlying engagement level semantics while providing sufficient variation for the model to generalize.
- Evidence anchors:
  - [abstract] "A key step involves the application of diverse time-series data augmentation techniques to these feature vectors, enhancing model training."
  - [section IV-B] "These techniques were applied randomly to the samples in the training and validation sets... to increase the total number of samples in the minority classes of 0 and 1 by a factor of ten"
  - [corpus] No corpus papers directly address time-series augmentation for engagement measurement; this appears novel to this domain.
- Break condition: If augmentation parameters (e.g., jitter magnitude, shift amount) are too aggressive, they may alter the semantic meaning of engagement signals.

### Mechanism 3
- Claim: Ordinal classification framing captures the natural ordering of engagement levels, reducing prediction errors across non-adjacent classes.
- Mechanism: Instead of treating engagement as 4 independent categories, the method transforms the problem into (C-1)=3 binary classification problems (class 0 vs >0, classes 0-1 vs >1, classes 0-2 vs class 3). Probability outputs from each binary classifier are combined to yield final class probabilities, constraining predictions to respect ordinal structure.
- Core assumption: Engagement levels follow a meaningful ordinal progression where misclassification between adjacent levels (e.g., 1 vs 2) is more acceptable than between distant levels (e.g., 0 vs 3).
- Evidence anchors:
  - [abstract] "incorporating order into engagement levels rather than treating it as mere categories"
  - [section III-c] "In the ordinal framework, a C-class classification problem is transformed into (C −1) binary classification problems"
  - [corpus] Corpus paper on "Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings" supports ordinal approaches with contrastive learning but in regression contexts.
- Break condition: If engagement levels are not truly ordinal (e.g., level 2 is qualitatively different rather than "more" than level 1), the ordinal constraint may introduce bias.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - Why needed here: Standard cross-entropy loss does not explicitly enforce inter-class separation; SupCon directly optimizes the embedding space to cluster same-class samples and separate different-class samples, which is critical when classes have high overlap (class confusion).
  - Quick check question: Can you explain why SupCon uses both positive (same-label) and negative (different-label) pairs, while self-supervised contrastive learning only uses augmentations of the same sample?

- **Concept: Ordinal Classification**
  - Why needed here: Engagement levels are inherently ordered (disengaged → fully engaged); treating them as nominal categories discards this structural information, potentially causing severe misclassifications.
  - Quick check question: How does the binary decomposition approach (C-1 classifiers) differ from simply applying regression to engagement levels?

- **Concept: Time-Series Augmentation for Sequential Data**
  - Why needed here: Traditional image augmentations (rotation, crop) don't apply to temporal feature sequences; domain-specific augmentations must preserve temporal dependencies while increasing sample diversity.
  - Quick check question: Why might jittering (adding Gaussian noise) be more appropriate for engagement features than time-warping for this specific application?

## Architecture Onboarding

- **Component map:**
  Video Input → Feature Extraction (OpenFace + EmoFAN) → Feature Vector (48 dims: affect + behavioral + latent) → Data Augmentation (jittering, scaling, shifting, permutation, flipping) → Encoder (LSTM or TCN) → Projection Head (256→128 FC) → [Phase 1] Supervised Contrastive Loss → [Phase 2] Freeze Encoder, Train Classifier (256→128→num_classes) → Ordinal Classification (3 binary classifiers) OR Standard 4-class softmax

- **Critical path:**
  1. **Feature extraction must be consistent** — OpenFace and EmoFAN outputs must align temporally across frames.
  2. **Augmentation ratios** — Minority classes (0, 1) require 10× augmentation; incorrect ratios will maintain imbalance bias.
  3. **Phase transition** — Encoder must be frozen before Phase 2 classifier training; failure to freeze causes representation drift.
  4. **Ordinal decoder setup** — Each binary classifier must be trained with sigmoid activation and binary cross-entropy, not softmax.

- **Design tradeoffs:**
  - **TCN vs LSTM**: TCN handles longer sequences and avoids vanishing gradients better; LSTM may capture finer temporal dynamics for certain classes (Table III shows LSTM has higher recall for classes 1 and 3).
  - **With/Without Latent Features**: Adding 32-dim latent affective features improves performance across all configurations (compare rows with vs without latent features in Table III).
  - **Class-Weighted Loss**: Degrades performance in this specific setting due to extreme imbalance (weighted loss overcompensates for rare classes).

- **Failure signatures:**
  - **Class 0 precision/recall = 0**: Model never predicts minority class; augmentation or oversampling insufficient.
  - **High confusion between classes 2 and 3**: Ordinal structure not being learned; check binary classifier thresholds.
  - **Phase 2 accuracy drops significantly**: Encoder not properly trained in Phase 1; check contrastive loss convergence.

- **First 3 experiments:**
  1. **Baseline validation**: Replicate TCN with cross-entropy loss (no contrastive, no augmentation, no ordinal) on DAiSEE test set to establish baseline accuracy (~0.60 per Table III).
  2. **Ablation study**: Isolate each component — (a) add contrastive loss only, (b) add augmentation only, (c) add ordinal classification only — to measure individual contributions.
  3. **Augmentation sensitivity**: Test different augmentation ratios for minority classes (5×, 10×, 15×) and measure impact on class 0/1 precision/recall to find saturation point.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does incorporating audio data and contextual cues (e.g., task progress) improve the accuracy of cognitive and context-oriented engagement assessments?
  - Basis in paper: [explicit] The authors state future research will involve "incorporating audio data and contextual information... to enable comprehensive engagement measurement, encompassing cognitive and context-oriented engagement assessments."
  - Why unresolved: The current study utilizes only video-based affective and behavioral features, excluding auditory or environmental context.
  - What evidence would resolve it: Evaluation of the framework on multimodal datasets (e.g., containing speech) with ablation studies showing performance gains from adding contextual metadata.

- **Open Question 2**: Can the proposed supervised contrastive learning framework be effectively transferred to measure patient engagement in virtual rehabilitation contexts?
  - Basis in paper: [explicit] The conclusion explicitly suggests the intent to "explore other applications, such as patient engagement in virtual rehabilitation."
  - Why unresolved: The method has currently been validated exclusively on the DAiSEE dataset, which consists of students in virtual learning environments.
  - What evidence would resolve it: Successful training and testing of the model on a clinical rehabilitation dataset with ordinal engagement labels, demonstrating comparable performance to the educational domain.

- **Open Question 3**: Why does the proposed method fail to classify the minority class (Class 0, disengaged) despite the use of augmentation and ordinal contrastive learning?
  - Basis in paper: [inferred] Table III shows that the best performing models (rows e and f) yield a Precision and Recall of 0.0000 for Class 0 (disengaged), despite the paper claiming the method handles "class imbalance."
  - Why unresolved: The specific combination of time-series augmentation and contrastive loss may be insufficient for the extreme under-representation of Class 0 samples in the DAiSEE dataset.
  - What evidence would resolve it: An analysis of latent space separability for Class 0 or the application of specialized few-shot learning techniques to verify if this class is learnable with the current features.

## Limitations
- The proposed method fails to classify the minority class (Class 0, disengaged), yielding zero precision and recall despite extensive augmentation
- Hyperparameter details (learning rate, batch size, optimizer settings, epoch count) are not specified, creating significant variability in reproduction attempts
- Feature extraction methodology is partially specified, but exact preprocessing steps remain unclear

## Confidence
- **High confidence**: The core mechanism of supervised contrastive learning for ordinal engagement classification is well-established in the paper. The two-phase training approach (contrastive pre-training followed by ordinal classification) is clearly described and theoretically sound.
- **Medium confidence**: The specific time-series augmentation techniques and their implementation details are described, but the exact parameters and random seed usage are not specified, potentially affecting reproducibility.
- **Low confidence**: The exact implementation of the ordinal classification decoder (sigmoid thresholds, probability aggregation method) relies on external references and may vary in practice.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Run ablation studies varying learning rates (1e-3, 1e-4, 5e-4), batch sizes (32, 64, 128), and augmentation ratios (5×, 10×, 15×) to determine which components most affect performance and identify optimal configurations.
2. **Statistical significance testing**: Evaluate the proposed method across 5-10 random seeds to calculate mean accuracy with confidence intervals, then perform paired t-tests against baseline methods to verify reported performance improvements are statistically significant.
3. **Confusion matrix analysis**: Generate detailed per-class precision, recall, and F1-score matrices for all engagement levels to identify whether the model genuinely reduces class confusion between adjacent levels or simply shifts errors to different class pairs.