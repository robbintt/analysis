---
ver: rpa2
title: Adjusting the Output of Decision Transformer with Action Gradient
arxiv_id: '2510.05285'
source_url: https://arxiv.org/abs/2510.05285
tags:
- algorithms
- gradient
- action
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Decision Transformer (DT) for
  offline reinforcement learning, specifically its poor state-level extrapolation
  and trajectory stitching capabilities. The authors propose Action Gradient (AG),
  a method that refines the action output by DT using gradients of a Q-value with
  respect to the action.
---

# Adjusting the Output of Decision Transformer with Action Gradient

## Quick Facts
- arXiv ID: 2510.05285
- Source URL: https://arxiv.org/abs/2510.05285
- Authors: Rui Lin; Yiwen Zhang; Zhicheng Peng; Minghao Lyu
- Reference count: 9
- Primary result: RF+AG outperforms prior DT-based methods on D4RL benchmarks, reaching 757.8 normalized score on Gym tasks and 193.9 on Maze2d

## Executive Summary
This paper addresses limitations in Decision Transformer (DT) for offline reinforcement learning, specifically its poor state-level extrapolation and trajectory stitching capabilities. The authors propose Action Gradient (AG), a method that refines the action output by DT using gradients of a Q-value with respect to the action. AG searches locally around the initial DT action to find a better action via gradient ascent, using a trained critic. This approach is orthogonal to token prediction techniques and can be combined with them. Experiments on D4RL benchmarks show that RF+AG outperforms prior DT-based methods, with total normalized scores reaching 757.8 on Gym tasks and 193.9 on Maze2d. Ablation studies confirm that AG improves performance even without token prediction, and hyperparameter tuning (e.g., learning rate η and iteration count) further enhances results. The method provides a stable and effective way to enhance DT's extrapolation ability.

## Method Summary
The method trains a Decision Transformer policy using standard NLL loss on the offline dataset, while simultaneously training a Q-value critic using Implicit Q-Learning (IQL) with expectile regression. At inference time, AG takes the initial action from DT and performs gradient ascent on the critic's Q-value to find a locally optimal action. The action is iteratively updated: $a^{i+1}_t = a^i_t + \eta \nabla_{a^i_t} Q(s_t, a^i_t)$. This approach is orthogonal to token prediction techniques and can be combined with them. The critic provides a stable gradient signal for local search, while the DT provides the initial action. AG can be seen as a post-hoc refinement module that doesn't interfere with the DT training process.

## Key Results
- RF+AG outperforms prior DT-based methods on D4RL benchmarks, reaching 757.8 normalized score on Gym tasks and 193.9 on Maze2d
- AG improves performance even without token prediction, demonstrating its standalone effectiveness for state-level extrapolation
- Hyperparameter tuning (learning rate η and iteration count) significantly impacts performance, with optimal values varying by dataset
- Ablation studies confirm that AG's benefits are complementary to token prediction, not redundant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A trained critic network can guide a Decision Transformer's output toward higher-value actions via local gradient ascent.
- **Mechanism:** At inference, the DT produces an initial action $a^0_t$. A critic $Q(s, a)$ provides the gradient $\nabla_{a^i_t} Q(s_t, a^i_t)$. The action is iteratively updated: $a^{i+1}_t = a^i_t + \eta \nabla_{a^i_t} Q(s_t, a^i_t)$. This performs a local heuristic search to maximize the Q-value, enabling state-level extrapolation beyond the behavioral cloning distribution.
- **Core assumption:** The critic $Q(s,a)$ generalizes smoothly to actions near the dataset distribution, providing a reliable gradient signal toward higher-value regions.
- **Break condition:** The critic has significant estimation error or non-smooth regions, causing the gradient to point toward areas of Q-overestimation rather than true value improvement.

### Mechanism 2
- **Claim:** Decoupling action refinement (AG) from policy training (DT/TP) avoids training instability (the "deadly triad") while combining the benefits of trajectory stitching and state-level extrapolation.
- **Mechanism:** AG operates as a post-hoc module at evaluation time. Token Prediction (TP) improves trajectory-level stitching by conditioning on predicted returns. Since AG modifies only the final action and not the DT training loss, it avoids unstable interactions between policy gradient terms and transformer training.
- **Core assumption:** The benefits of better trajectory conditioning (from TP) and better local action selection (from AG) are complementary and independent.
- **Break condition:** The refined action leads to a state where the TP network's predicted return-to-go is highly inaccurate, causing a mismatch that disrupts the trajectory.

### Mechanism 3
- **Claim:** A critic trained with Implicit Q-Learning (IQL) using expectile regression provides a sufficiently stable and conservative value estimate for the gradient search.
- **Mechanism:** Instead of standard Bellman updates which risk overestimation from out-of-distribution actions, the critic is trained via IQL (Eqs. 4-5). This uses expectile regression to approximate the upper tail of the value distribution directly from dataset samples, providing a more robust target for gradient ascent.
- **Core assumption:** The expectile regression provides a gradient signal well-aligned with the true value function for nearby actions.
- **Break condition:** The dataset lacks sufficient quality transitions for the critic to learn a meaningful gradient landscape, or the expectile level $\tau$ is poorly chosen.

## Foundational Learning

- **Concept: Offline Reinforcement Learning & Extrapolation**
  - **Why needed here:** Standard DT imitates the dataset. This method is designed to "extrapolate" or select actions better than the data.
  - **Quick check question:** Why does maximizing action likelihood in a suboptimal dataset produce a suboptimal policy?

- **Concept: Decision Transformer (DT) Architecture**
  - **Why needed here:** You must understand how DT conditions on $(s_t, RTG_t)$ to predict $a_t$ and why it's purely a sequence modeling problem without an explicit critic during training.
  - **Quick check question:** How does a DT differ from a standard actor-critic RL algorithm?

- **Concept: Gradient Ascent & Q-Learning**
  - **Why needed here:** This method is gradient ascent on a function approximator. You must understand how gradients guide optimization and the risks of imperfect functions.
  - **Quick check question:** If the Q-function is poorly learned, how will the action gradient update behave? (Hint: it may optimize towards an error).

## Architecture Onboarding

- **Component map:** Train DT Policy -> Train Critic via IQL -> Inference: Get DT action -> Compute Critic Gradient -> Update Action -> Execute Action
- **Critical path:** Train DT policy (standard) -> Train Critic via IQL (standard) -> **Inference:** Get DT action -> Compute Critic Gradient -> Update Action -> Execute Action
- **Design tradeoffs:**
  - **Learning Rate ($\eta$) & Iterations ($n$):** Higher values allow more aggressive search but risk leaving the data manifold where the critic is inaccurate. Tuning is critical (Fig 3).
  - **Computation:** AG adds $n$ forward/backward passes through the critic per environment step, increasing inference cost.
- **Failure signatures:**
  1. **Overestimation Exploitation:** Agent finds an action with artificially high Q-value but low true reward. Look for actions saturating at bounds.
  2. **Cascading Errors:** An "improved" action leads to an OOD state where both the critic and TP network fail, causing rapid performance degradation.
  3. **Gradient Oscillation:** The action update fails to converge or oscillates, visible if logging the sequence $\{a^0_t, \dots, a^n_t\}$.
- **First 3 experiments:**
  1. **Gradient Visualization:** On a simple environment (like the paper's single-state example), visualize the $Q(s, \cdot)$ landscape and trace the action trajectory $a^0 \to \hat{a}$. Verify it moves toward higher true reward.
  2. **Hyperparameter Ablation:** On a D4RL medium dataset, sweep $\eta$ (e.g., [0.0005, 0.002]) and $n$ (e.g., [5, 15]). Plot normalized score to replicate the tuning sensitivity shown in Fig 3.
  3. **Critic Sensitivity:** Swap the IQL critic for one trained with standard TD learning (e.g., from TD3+BC). Compare performance to test the claim that IQL's stability is crucial.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which gradient optimization strategy is most effective for the Action Gradient (AG) framework?
- Basis in paper: [explicit] The ablation study in Section 4.4 concludes that current results for methods like Adam or momentum are inconclusive, stating "a more comprehensive experimental approach... is necessary to investigate and identify superior gradient methods further."
- Why unresolved: The paper observes that advanced gradient methods sometimes perform worse than the naive approach, likely because the critic's estimated optimal point differs from the true optimum, interfering with momentum-based overshooting.
- What evidence would resolve it: A comparative analysis of gradient estimators (e.g., SGD, Adam, RMSProp) specifically assessing their robustness to the noise and estimation errors inherent in offline RL critic functions.

### Open Question 2
- Question: Can advanced critic training techniques improve the stability and performance of Action Gradient?
- Basis in paper: [explicit] The Conclusion states that "exploration of advanced gradient methods and optimized critic training techniques have the potential to enhance AG's performance."
- Why unresolved: The current implementation relies on IQL (Implicit Q-Learning) for the critic, which may suffer from overestimation or underestimation in sparse reward regions, directly impacting the gradient direction used by AG.
- What evidence would resolve it: Empirical results showing performance gains when substituting the standard IQL critic with alternative value learning methods (e.g., CQL, ensemble critics) within the AG framework.

### Open Question 3
- Question: What is the dependency relationship between Action Gradient and Token Prediction (TP) regarding stitching capability?
- Basis in paper: [inferred] Section 4.3 notes that AG shows "limited effectiveness" on naive DT (without TP) because "scarcity of stitching ability... forces the agent to follow the existing trajectory."
- Why unresolved: It is unclear if AG can function independently as a robust solution for state-level extrapolation if the agent lacks the trajectory-level flexibility provided by TP, or if the two are fundamentally coupled for success.
- What evidence would resolve it: Experiments on datasets with specific trajectory fragmentation patterns, testing AG's ability to correct actions when the base policy is strictly constrained to offline trajectories.

## Limitations
- The paper lacks detailed implementation specifications for key hyperparameters (learning rate $\eta$ and iteration count $n$), relying instead on sensitivity analysis graphs without providing exact optimal values for different datasets.
- No direct corpus evidence validates the specific critic-gradient mechanism or the orthogonality claim between AG and token prediction techniques.
- The method's effectiveness depends heavily on the critic's ability to provide accurate gradients in regions near the behavioral cloning distribution, with no clear failure mode analysis when this assumption breaks.

## Confidence
- **High Confidence:** The core mechanism of using critic gradients to refine DT actions is clearly specified and experimentally validated on D4RL benchmarks, showing consistent improvements over baselines.
- **Medium Confidence:** The claim that AG is orthogonal to token prediction and can be combined with it is supported by experimental results but lacks theoretical justification or direct comparative analysis.
- **Medium Confidence:** The assertion that IQL-trained critics provide more stable gradient signals is reasonable given IQL's conservative estimation properties, but not empirically validated against alternatives in this paper.

## Next Checks
1. **Gradient Landscape Verification:** Implement the single-state visualization experiment to confirm that action trajectories move toward higher true reward rather than critic-induced overestimation artifacts.
2. **Hyperparameter Sensitivity Replication:** Replicate the $\eta$ and $n$ sensitivity analysis on a medium-difficulty D4RL task to verify the tuning recommendations and identify optimal values.
3. **Critic Architecture Ablation:** Replace the IQL critic with a standard TD3+BC critic and measure performance degradation to test whether expectile regression is truly crucial for stable gradient ascent.