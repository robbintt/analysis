---
ver: rpa2
title: Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading
  Comprehension?
arxiv_id: '2507.08232'
source_url: https://arxiv.org/abs/2507.08232
tags:
- grade
- student
- reading
- prompt
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether large language models (LLMs) can accurately
  simulate real students' abilities in mathematics and reading comprehension across
  grades 4, 8, and 12. Using data from the National Assessment of Educational Progress
  (NAEP), the authors apply Item Response Theory (IRT) to measure 11 diverse LLMs
  against authentic student performance patterns.
---

# Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?

## Quick Facts
- arXiv ID: 2507.08232
- Source URL: https://arxiv.org/abs/2507.08232
- Reference count: 23
- Strong general-purpose LLMs consistently outperform average students across grades without guidance

## Executive Summary
This study evaluates whether large language models can accurately simulate real students' abilities in mathematics and reading comprehension across grades 4, 8, and 12. Using NAEP data and Item Response Theory, researchers tested 11 diverse LLMs against authentic student performance patterns. The results reveal that while strong general-purpose models consistently outperform average students, grade-enforcement prompts significantly alter performance, though alignment with grade-level students remains highly model- and prompt-specific. No evaluated model-prompt pair consistently matches average student performance across subjects and grades, highlighting the need for dedicated model finetuning for faithful grade-level emulation.

## Method Summary
The authors evaluated 11 diverse LLMs using NAEP assessment items from grades 4, 8, and 12 in mathematics and reading comprehension. They applied Item Response Theory (IRT) to measure model performance against authentic student ability distributions. The study employed both unguided model responses and grade-enforced prompts designed to simulate different grade-level abilities. Performance was compared across subject areas to identify patterns of alignment with real student performance, with particular attention to how different prompting strategies affected model behavior.

## Key Results
- Strong general-purpose models consistently outperform average students at every grade without guidance
- Grade-enforcement prompts significantly alter model performance but alignment remains model- and prompt-specific
- No evaluated model-prompt pair consistently matches average student performance across subjects and grades

## Why This Works (Mechanism)
The evaluation framework leverages IRT's ability to calibrate item difficulty and student ability on the same scale, enabling meaningful comparison between LLM responses and actual student performance. Grade-enforcement prompts attempt to constrain model behavior to simulate different educational levels, though their effectiveness varies considerably by model and subject area.

## Foundational Learning
**Item Response Theory (IRT)**: A statistical framework for modeling the relationship between item characteristics and respondent abilities. *Why needed*: Provides standardized scale for comparing model and student performance. *Quick check*: Can you explain how item difficulty and discrimination parameters work?

**NAEP Assessment Framework**: The National Assessment of Educational Progress provides standardized assessment items across subjects and grades. *Why needed*: Serves as gold standard for authentic student performance data. *Quick check*: What are the key features that make NAEP assessments reliable?

**Grade-Level Proficiency Standards**: Educational benchmarks defining expected knowledge and skills at different grade levels. *Why needed*: Essential for creating realistic simulation targets. *Quick check*: How do proficiency standards vary across grades 4, 8, and 12?

## Architecture Onboarding

**Component Map**: NAEP Items -> IRT Calibration -> LLM Evaluation -> Performance Comparison -> Model Ranking

**Critical Path**: Assessment Item Selection → IRT Parameter Estimation → Model Prompting → Response Evaluation → Statistical Comparison with Student Data

**Design Tradeoffs**: 
- Synthetic grade-enforcement prompts vs. natural student behavior patterns
- Computational cost of evaluating multiple models vs. sample size requirements
- General-purpose vs. specialized model performance across subjects

**Failure Signatures**: 
- Models consistently outperforming average students indicate overgeneralization
- Inconsistent performance across prompts suggests poor grade-level alignment
- Subject-specific failures reveal domain knowledge gaps

**First Experiments**:
1. Baseline evaluation without grade-enforcement prompts
2. Cross-validation of IRT parameters with different student cohorts
3. Comparison of general-purpose vs. specialized model performance on same items

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic grade-enforcement prompts may not reflect authentic student behavior patterns
- Evaluation framework may not generalize beyond NAEP-specific contexts
- Limited to mathematics and reading comprehension subjects

## Confidence
High: Strong general-purpose models outperform average students across grades without guidance
Medium: Grade-enforced simulation results depend on prompt engineering effectiveness
Low: Cross-subject generalizability claims limited to study scope

## Next Checks
1. Conduct longitudinal validation studies tracking the same models' performance across multiple assessment cycles to verify consistency of simulation capabilities over time.

2. Implement mixed-methods validation combining quantitative IRT analysis with qualitative expert review of model responses against authentic student work samples.

3. Test model performance across additional subjects and assessment frameworks (e.g., PISA, TIMSS) to evaluate generalizability beyond NAEP-specific contexts.