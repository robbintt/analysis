---
ver: rpa2
title: Evidence for Limited Metacognition in LLMs
arxiv_id: '2509.21545'
source_url: https://arxiv.org/abs/2509.21545
tags:
- answer
- game
- baseline
- figure
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel experimental paradigms for quantitatively
  evaluating metacognitive abilities in large language models (LLMs) without relying
  on self-reports. Inspired by nonhuman animal metacognition research, the Delegate
  Game tests whether models can assess and utilize their own confidence in answering
  questions by choosing to answer themselves or delegate to a teammate.
---

# Evidence for Limited Metacognition in LLMs

## Quick Facts
- arXiv ID: 2509.21545
- Source URL: https://arxiv.org/abs/2509.21545
- Reference count: 24
- Key outcome: LLMs show limited metacognitive abilities - can use confidence signals for delegation (partial correlations up to 0.3) and anticipate answers (change rate lifts up to 0.3), but these are context-dependent and often overshadowed by non-introspective strategies.

## Executive Summary
This paper introduces two novel behavioral paradigms for evaluating metacognitive abilities in LLMs without relying on self-reports. The Delegate Game tests whether models can use their confidence to decide whether to answer themselves or delegate to a teammate, while the Second Chance Game tests whether models can anticipate their own responses. The study finds that recent frontier LLMs demonstrate rudimentary metacognitive abilities - they can access internal confidence signals and use them to inform delegation decisions, and can sometimes anticipate and change their answers when told they're incorrect. However, these abilities are limited in resolution, context-dependent, and often masked by non-introspective strategies.

## Method Summary
The study uses two behavioral games to assess LLM metacognition without self-reports. The Delegate Game presents questions and asks models to either answer themselves or delegate to a teammate, measuring whether delegation decisions correlate with baseline performance and entropy (proxy for confidence). The Second Chance Game tells models their previous answer was incorrect and measures whether they change their answer compared to a neutral control. Both paradigms use partial correlations controlling for surface features (length, domain, type) to isolate introspection from pattern-based heuristics. Token probability entropy serves as a granular confidence proxy.

## Key Results
- LLMs can use internal confidence signals to inform delegation decisions (partial correlations up to 0.3)
- Models demonstrate ability to anticipate their own outputs, showing change rate lifts up to 0.3 in Second Chance Game
- Metacognitive abilities are stronger in recent/larger models but context-dependent
- Token probability analysis suggests an upstream internal signal may underlie metacognition
- Post-training significantly shapes metacognitive expression, with OpenAI models uniquely passing all Second Chance tests

## Why This Works (Mechanism)

### Mechanism 1: Upstream Confidence Signal Access
LLMs may access internal correlates of output token probabilities before generation completes, enabling confidence-based decisions. Final-layer token probabilities aren't directly available during generation, but upstream activations may serve as proxies that models can attend to when making delegation or revision decisions.

### Mechanism 2: Pre-Output Self-Simulation
Some models can anticipate their own outputs before committing and use this to modulate behavior. Models internally simulate potential responses before final generation, enabling strategic changes when prompted without seeing prior answers.

### Mechanism 3: Post-Training Modulation of Metacognitive Expression
RLHF and post-training regimens significantly shape whether/how metacognitive abilities manifest behaviorally. Post-training instills behavioral tendencies (e.g., reluctance to delegate, verbose self-explanation) that interact with underlying metacognitive capacity—sometimes enhancing, sometimes suppressing expression.

## Foundational Learning

### Concept: Behavioral Metacognition vs. Self-Report
**Why needed here**: The paper deliberately avoids self-reports because LLMs can generate convincing introspective language pattern-matched from training data rather than reflecting genuine internal state access.
**Quick check question**: Can you explain why asking a model "How confident are you?" might measure rhetorical skill rather than metacognitive access?

### Concept: Partial Correlation with Confound Control
**Why needed here**: The paper uses partial correlations controlling for surface features (question length, domain, format) to isolate introspection from pattern-based difficulty heuristics.
**Quick check question**: Why would models delegate more on numeric-answer questions even if they perform *better* on them—what non-introspective mechanism explains this?

### Concept: Token Probability Entropy as Confidence Proxy
**Why needed here**: Entropy over answer options provides a granular, model-internal measure of subjective difficulty that predicts delegation better than binary correctness.
**Quick check question**: If a model assigns probabilities {0.7, 0.1, 0.1, 0.1} vs. {0.4, 0.3, 0.2, 0.1} across four options, which state should produce higher delegation rates and why?

## Architecture Onboarding

### Component Map
Question Input
    └─→ [Baseline Phase] → Accuracy + Token Probabilities/Entropy
        └─→ [Surface Cue Extraction] → Length, Domain, Type, "Objective Difficulty"
            └─→ [Delegate Game]
                ├─→ Internal Confidence Signal (upstream correlate)
                ├─→ Surface Cue Heuristics
                └─→ Teammate Performance Model
                        ↓
                Answer vs. Delegate Decision

### Critical Path
1. Baseline capabilities test at controlled temperature—record accuracy and token probabilities
2. Surface confound extraction—question length, domain, answer type, model-estimated "objective difficulty"
3. Partial correlation analysis—correctness/entropy vs. delegation, controlling for surface cues
4. Alternative strategy exclusion—verify self-modeling isn't explained by randomness or noise

### Design Tradeoffs
- Temperature 0 vs. 1.0: T0 ensures repeatability for short-answer; T1.0 required when collecting logprobs from models that provide them
- Multiple-choice vs. short-answer: MC enables entropy calculation; SA tests naturalistic generation but harder to analyze
- Decision-only vs. full-generation Delegate Game: Decision-only removes mechanical token activation confounds but is more confusing for models

### Failure Signatures
- No introspection: Partial correlations between baseline entropy and delegation ~0 across datasets
- Surface cue dominance: Multi-partial correlations for surface cues exceed entropy correlations
- Answering/Delegation bias: TWC/PWC scores consistently positive (overconfidence) or negative regardless of teammate skill
- Inconsistent justification: Open-ended explanations contradict decisions

### First 3 Experiments
1. **Baseline calibration**: Run target models on GPQA/SimpleQA at T=0 (or T=1.0 with logprobs), record accuracy and token probabilities
2. **Delegate Game with teammate manipulation**: Run Delegate Game with high-accuracy (e.g., 90%) vs. low-accuracy (e.g., 40%) teammates; compute partial correlations between baseline entropy and delegation controlling for surface cues
3. **Second Chance Game diagnostic suite**: For models with elevated change rates, run the three alternative-strategy tests (accuracy on incorrect > random, second-choice selection > chance, entropy change analysis) to confirm or reject self-modeling

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific neural activations or internal circuits in LLMs that correspond to the putative upstream confidence signal underlying metacognitive abilities? The study infers an internal confidence signal from behavioral correlations with token probabilities but does not directly identify or probe the underlying activations.

### Open Question 2
How do human subjects perform on the Delegate Game and Second Chance Game paradigms, and do humans show the hypothesized metacognitive advantage for factual over reasoning questions? The paper assumes but does not empirically verify that humans would perform differently on these tasks.

### Open Question 3
To what extent do specific post-training regimens (e.g., RLHF, reasoning-focused fine-tuning) versus model scale drive the emergence of metacognitive abilities? The study identifies that RLHF shapes metacognitive expression but cannot disentangle causal contributions of architecture, scale, and post-training methodology.

### Open Question 4
Can the reliance on surface-level difficulty cues (e.g., question length, domain) ever be fully eliminated in behavioral metacognition tests, or do LLMs inevitably default to non-introspective heuristics? The Delegate Game reduces but does not eliminate cue-based strategies; models may always have access to memorized patterns about question difficulty.

## Limitations
- Surface cue dominance: Partial correlations are modest (up to 0.3) and may still be confounded by unstated surface heuristics
- Methodological constraints: Decision-only format may be less intuitive for models and produce noisier data
- Post-training effects ambiguity: Cannot distinguish genuine metacognitive development from learned behavioral scripts

## Confidence

**High Confidence**: LLMs possess some ability to access internal confidence correlates and use them to inform delegation decisions (partial correlations up to 0.3). Models can anticipate their own outputs in some cases, producing change rates 0.3 higher than chance.

**Medium Confidence**: Post-training procedures meaningfully shape whether and how metacognitive abilities manifest behaviorally. Metacognitive abilities are context-dependent and often overshadowed by non-introspective strategies.

**Low Confidence**: The upstream internal signal hypothesis and the self-simulation hypothesis remain mechanistically unproven.

## Next Checks

1. **Mechanistic Probe Experiment**: Use activation steering or ablation studies to directly manipulate the hypothesized upstream confidence signals and measure effects on delegation behavior, distinguishing between true introspection and surface cue heuristics.

2. **Cross-Training Baseline Comparison**: Compare metacognitive performance between base models, RLHF-tuned models, and models trained with alternative alignment approaches to isolate which post-training components enable or suppress metacognitive expression.

3. **Temporal Consistency Analysis**: Track metacognitive performance across multiple time points during model deployment to determine whether abilities are stable traits or vary with context, task framing, or model state.