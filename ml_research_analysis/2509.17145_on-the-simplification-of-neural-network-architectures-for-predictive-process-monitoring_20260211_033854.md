---
ver: rpa2
title: On the Simplification of Neural Network Architectures for Predictive Process
  Monitoring
arxiv_id: '2509.17145'
source_url: https://arxiv.org/abs/2509.17145
tags:
- prediction
- lstm
- process
- next
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of simplifying deep learning
  architectures for predictive process monitoring (PPM), aiming to reduce computational
  costs while maintaining predictive accuracy. Specifically, the research focuses
  on pruning state-of-the-art models like LSTM and Transformer-based architectures.
---

# On the Simplification of Neural Network Architectures for Predictive Process Monitoring

## Quick Facts
- arXiv ID: 2509.17145
- Source URL: https://arxiv.org/abs/2509.17145
- Authors: Amaan Ansari; Lukas Kirchdorfer; Raheleh Hadian
- Reference count: 19
- Primary result: Reducing Transformer parameters by 85% yields only 2-3% accuracy loss in PPM tasks

## Executive Summary
This study investigates simplifying deep learning architectures for predictive process monitoring (PPM) to reduce computational costs while maintaining accuracy. The research focuses on pruning state-of-the-art LSTM and Transformer models, demonstrating that significant parameter reduction (up to 85% for Transformers) results in minimal performance degradation (2-3% accuracy loss). Experiments across five diverse event logs show that lightweight models achieve comparable performance to their full counterparts, offering a path toward more efficient and scalable PPM solutions. While Transformer models show strong resilience to pruning, LSTM models exhibit slightly higher sensitivity, particularly for waiting time prediction tasks.

## Method Summary
The study evaluates five PPM architectures: MTLFormer (multi-stream Transformer), MTLFormer light (simplified version with single linear prediction heads), Transformer simple (single-stream Transformer), LSTM, and LSTM light (shared LSTM backbone with linear heads). All models predict next activity, next role, and three time features using uncertainty-weighted multi-task learning. The experiments use five event logs (Production, BPIC2012W, P2P, Confidential 1000, Confidential 2000) with a 70/10/20 train/validation/test split. Preprocessing includes z-score normalization, prefix extraction with start/end tokens, and padding. Models are trained with grid-searched hyperparameters and selected using a composite score balancing performance and loss.

## Key Results
- Transformer parameter reduction of 85% yields only 2-3% drop in F1 performance
- LSTM architectures show slightly higher sensitivity to simplification, especially for waiting time prediction (13% MAE increase)
- MTLFormer light achieves 98-99% of original F1 performance despite 85% fewer parameters
- Transformer simple (77% fewer parameters) maintains competitive performance with only 3 F1 percentage point drop
- Lightweight models converge faster than full models in most cases

## Why This Works (Mechanism)

### Mechanism 1: Over-parameterization in Multi-Stream Attention Architectures
The original MTLFormer uses five parallel Transformer streams that provide more representational capacity than PPM tasks require. Reducing embedding dimensions, attention heads, and feed-forward dimensions removes redundant capacity without degrading task representations. PPM event logs contain structured temporal patterns that don't require the full capacity designed for NLP tasks.

### Mechanism 2: Sequential Hidden State Dependency in LSTM Architectures
LSTMs show higher sensitivity to simplification because predictions depend on accumulated sequential representations. Removing task-specific LSTM layers forces all predictions to rely directly on the shared backbone's representation, degrading performance more severely for time-based tasks requiring temporal reasoning beyond the shared representation.

### Mechanism 3: Prediction Head Complexity Reduction
Replacing deep MLP prediction heads with single linear layers preserves performance when the backbone representation is sufficiently expressive. The backbone's pooled embeddings already contain sufficient task-relevant information, making deep heads unnecessary. This works better for Transformers than LSTMs because attention mechanisms create richer sequence representations at the pooling point.

## Foundational Learning

- **Concept: Event Log Structure (Traces, Prefixes, Events)**
  - Why needed here: Understanding how event logs decompose into traces and prefixes is essential for comprehending what the models predict. Each prefix represents a partial process execution.
  - Quick check question: Given a trace with 5 events, how many training prefixes can be extracted? (Answer: 4 prefixes of lengths 1, 2, 3, and 4)

- **Concept: Multi-Head Self-Attention Mechanisms**
  - Why needed here: The Transformer variants rely on attention to capture dependencies across event sequences. Understanding how attention weights distribute relevance across events explains why pruning heads doesn't catastrophically harm performance.
  - Quick check question: How does multi-head attention differ from single-head attention in capturing different relationship types?

- **Concept: Multi-Task Learning with Uncertainty Weighting**
  - Why needed here: All models predict multiple tasks simultaneously (next activity, next role, time features). The training procedure uses uncertainty weighting to balance loss contributions across tasks with different scales.
  - Quick check question: Why would a naive sum of cross-entropy loss and MSE loss create optimization problems?

## Architecture Onboarding

- **Component map:** Input Layer (Activity + Role embeddings + Temporal features) -> Backbone (Transformer multi-stream encoders or LSTM shared layer) -> Pooling (Average pooling or final hidden state) -> Prediction Heads (3 heads for next activity, role, time values) -> Output (Categorical + Continuous)

- **Critical path:** Input preprocessing (prefix extraction + temporal feature normalization) -> Embedding concatenation -> Backbone encoding -> Pooling -> Head predictions. Errors in temporal feature normalization propagate to all time-based predictions.

- **Design tradeoffs:**
  - MTLFormer light vs. Transformer simple: MTLFormer light preserves multi-stream architecture (better for next-activity prediction, +3 F1 percentage points) but requires more complex implementation
  - LSTM light vs. Transformer simple: LSTM light uses 15% fewer parameters and slightly better next-activity prediction, but 26.8% worse remaining time prediction
  - Head complexity vs. convergence speed: Simplified heads reduce parameters but MTLFormer light often converges faster, suggesting compactness aids optimization

- **Failure signatures:**
  - Waiting time prediction degradation in LSTMs: If LSTM light shows >15% MAE increase for waiting time but not duration or remaining time
  - Activity prediction collapse in Transformer simple: If NAP F1 drops significantly on large datasets
  - Non-convergence in light models: If validation loss plateaus higher than full models

- **First 3 experiments:**
  1. Baseline replication: Implement MTLFormer light on P2P dataset to verify 85% parameter reduction with <3% F1 drop
  2. Ablation study on prediction heads: Test MTLFormer light with single linear heads vs. two-layer MLP heads
  3. Cross-architecture timing comparison: Measure training time and inference latency for LSTM light vs. Transformer simple on BPIC2012W

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do simplified architectures maintain their performance advantages when applied to longer-term prediction targets, such as suffix or outcome prediction?
- Basis in paper: The authors state that "most PPM tasks in this study are limited to next-event prediction" and suggest "exploring more longer-term targets, such as suffix or outcome prediction, could provide valuable insights."
- Why unresolved: The current study restricted its scope to next-event predictions, leaving the behavior of simplified models on complex, multi-step forecasting tasks unknown.
- What evidence would resolve it: Benchmarking the simplified LSTM and Transformer variants specifically on suffix and outcome prediction tasks using the same event logs.

### Open Question 2
- Question: Can automated techniques like Neural Architecture Search (NAS) identify more optimal trade-offs between model size and accuracy than the manual simplification methods used?
- Basis in paper: The conclusion proposes that "future work could explore architecture-aware pruning techniques or neural architecture search to further optimize the trade-off between efficiency and accuracy."
- Why unresolved: The paper relied on manual grid searches and specific simplification strategies, which may not represent the global optimum for efficiency.
- What evidence would resolve it: A comparative study where NAS algorithms design PPM architectures evaluated against the manually simplified baselines established in this paper.

### Open Question 3
- Question: Are the lightweight models robust enough to generalize effectively when applied to event logs from completely unseen domains without retraining?
- Basis in paper: The authors list "investigating the generalizability of lightweight models across unseen domains" as a specific direction for future work.
- Why unresolved: The experiments utilized specific datasets (mostly financial/procurement), and it is unclear if the reduced parameter count hinders the model's ability to transfer knowledge to structurally different processes.
- What evidence would resolve it: Zero-shot or few-shot learning experiments where models trained on the current datasets are tested on out-of-domain event logs.

## Limitations
- Findings based on five specific event logs with particular characteristics, limiting generalizability to other PPM domains
- Study does not provide detailed convergence curves or variance across runs, limiting confidence in stability of performance differences
- Analysis focuses primarily on parameter reduction rather than comprehensive computational efficiency metrics (inference time, memory usage)

## Confidence
- **High confidence:** The 85% parameter reduction in Transformer models with minimal performance loss (2-3% F1 drop) is well-supported by experimental results across multiple datasets
- **Medium confidence:** The claim that LSTM architectures are "slightly more sensitive" to simplification is supported but requires further investigation to determine if this difference is statistically significant across all tasks
- **Low confidence:** The mechanism explaining why Transformer heads are more resilient to pruning than LSTM task-specific layers lacks direct empirical validation and relies on architectural assumptions

## Next Checks
1. **Statistical significance testing:** Perform paired t-tests or Wilcoxon signed-rank tests across multiple training runs to determine if performance differences between full and light models are statistically significant, particularly for LSTM waiting time predictions
2. **Cross-domain validation:** Test the simplified architectures on additional event logs from different domains (healthcare, manufacturing, finance) to assess generalizability of the simplification benefits beyond the current datasets
3. **Ablation on head complexity:** Systematically vary the depth of prediction heads (0, 1, 2, 3 layers) in both MTLFormer light and LSTM light to isolate the impact of head complexity from backbone simplification