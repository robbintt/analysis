---
ver: rpa2
title: 'Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers'
arxiv_id: '2601.09000'
source_url: https://arxiv.org/abs/2601.09000
tags:
- loss
- training
- cooldown
- learning
- warmup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether the Warmup Stable Decay (WSD) learning
  rate scheduler's success, primarily observed in transformer-based language models,
  extends to other architectures like CNNs. The authors compare WSD's training dynamics
  on a Pythia-like language model and a small CNN trained on CIFAR10, examining loss
  landscapes, sharpness, and training directions.
---

# Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers

## Quick Facts
- arXiv ID: 2601.09000
- Source URL: https://arxiv.org/abs/2601.09000
- Reference count: 37
- Primary result: WSD's river-valley dynamics and progressive sharpening extend from transformers to CNNs

## Executive Summary
This paper investigates whether Warmup Stable Decay (WSD), a learning rate scheduler that has shown success primarily in transformer-based language models, exhibits similar effectiveness across different neural architectures. By comparing WSD's training dynamics on a Pythia-like language model and a small CNN trained on CIFAR10, the authors examine loss landscapes, sharpness, and training directions. They find that WSD induces qualitatively similar training dynamics and loss landscape characteristics in both architectures, including a "river valley" profile and increasing sharpness during cooldown. This suggests that the effectiveness of WSD is not specific to transformers but rather reflects shared geometric properties of high-dimensional nonconvex optimization problems. The study provides validation for theoretical explanations of WSD's behavior and highlights potential future research directions on the impact of overparameterization on WSD performance.

## Method Summary
The study compares WSD scheduler dynamics between a 160M parameter Pythia-like language model and a 334K parameter CNN trained on CIFAR10. Both models use AdamW optimizer with batch sizes of 256 and 128 respectively. The CNN is trained for 50 epochs with 35% cooldown, while the LM uses 20% cooldown. Key analyses include loss interpolation between checkpoints to identify river-valley profiles, Hessian spectral norm estimation for sharpness tracking during cooldown, PCA on iterate sets to identify dominant movement directions, and weak quasi-convexity diagnostics. The CNN architecture is provided in Appendix A with 4 convolutional layers.

## Key Results
- WSD induces similar "river valley" loss landscape geometry in both transformer LMs and CNNs
- Progressive sharpening occurs during cooldown in both architectures, with CNN requiring 35% decay length to match cosine annealing baseline
- Training trajectories exhibit weak quasi-convexity behavior along optimizer paths
- Sharpness increases monotonically during cooldown as learning rate decreases in both architectures

## Why This Works (Mechanism)

### Mechanism 1: River Valley Loss Landscape Navigation
WSD exploits a consistent "river valley" geometry in the loss landscape where the stable phase traverses a flat valley floor while cooldown descends the steep walls. During stable phase (high LR), stochasticity causes iterates to oscillate between valley slopes with gradual progress along the flat "river" direction. During cooldown (decaying LR), reduced step size allows iterates to settle toward the valley floor, producing rapid loss reduction. The loss landscape exhibits directional asymmetry with at least one flat direction connected to steep cross-sectional curvature.

### Mechanism 2: Progressive Sharpening Enables Cooldown Acceleration
Cooldown phase accesses higher-curvature regions that were invisible to larger learning rates, causing sharpness to increase as learning rate decreases. The early cooldown direction aligns more closely with high-curvature Hessian eigenspaces than the stable-phase direction. Smaller LR allows the optimizer to "see" and follow these sharper subspaces without divergence. High-curvature directions exist but are suppressed during high-LR phases due to implicit regularization from gradient noise and step-size constraints.

### Mechanism 3: Weak Quasi-Convexity Along Training Trajectory
The training trajectory satisfies weak quasi-convexity conditions, making convex optimization bounds surprisingly applicable to non-convex AdamW training. The loss along the optimizer's path behaves approximately convexly: gradients point toward the final solution, and AdamW updates maintain positive cosine similarity with negative gradient direction. The iterates remain in a region where local convexity-like structure holds without sharp non-convex transitions between basins.

## Foundational Learning

- **Concept: Learning Rate Scheduling as Landscape Geometry Probe**
  - Why needed here: The paper treats scheduler behavior as a diagnostic tool for understanding loss surface geometry, not just a hyperparameter
  - Quick check question: Can you explain why a constant LR might produce different trajectory geometry than a decaying LR on the same loss surface?

- **Concept: Sharpness (Spectral Norm of Hessian)**
  - Why needed here: Sharpness is the primary metric used to characterize how cooldown changes the optimizer's position in parameter space
  - Quick check question: If sharpness doubles during cooldown while loss halves, what does this imply about the geometry of the path taken?

- **Concept: Principal Component Analysis of Training Trajectories**
  - Why needed here: PCA on iterate sets is the method used to identify dominant movement directions during stable vs. decay phases
  - Quick check question: If the first PC explained 90% of variance in stable phase but only 30% in decay phase, what would this suggest about trajectory complexity?

## Architecture Onboarding

- **Component map:**
  Warmup phase (t ≤ T_w) -> Stable phase (T_w < t ≤ T_c) -> Cooldown phase (t > T_c) -> Checkpoint x̂ at T_c -> Iterate sets S (stable) and D (decay)

- **Critical path:**
  1. Implement WSD scheduler with configurable T_w, T_c, T_end
  2. Log checkpoints at minimum: 80% stable, 100% stable, cooldown start, cooldown end
  3. Compute Hessian spectral norm (sharpness) on sampled checkpoints during cooldown
  4. Perform PCA on iterate sets to verify single-direction dominance per phase
  5. Check weak quasi-convexity condition if validating against convex theory

- **Design tradeoffs:**
  - Cooldown length vs. final performance: Paper finds CNNs may require longer cooldown (35%) than typical LM practice (20%) to match cosine annealing baseline
  - Checkpoint frequency vs. compute: Sharpness and PCA require iterate snapshots; too sparse sampling may miss trajectory transitions
  - Model size vs. landscape analysis feasibility: Hessian computation scales poorly; the paper uses 334K parameter CNN and 160M parameter LM as feasible targets

- **Failure signatures:**
  - Loss plateau during stable phase without subsequent cooldown acceleration → suggests landscape lacks river-valley structure or cooldown initiated too early
  - Sharpness decrease during cooldown → contradicts expected progressive sharpening; check optimizer settings or regularization
  - Negative τ_i values → weak quasi-convexity violated; training may be crossing basin boundaries
  - Cooldown performance inferior to cosine with same compute → cooldown fraction likely insufficient (increase to 30-40%)

- **First 3 experiments:**
  1. Train small CNN on CIFAR10 with WSD (35% cooldown) and warmup cosine, plotting loss curves side-by-side to verify similar "elbow" pattern at cooldown start
  2. Compute spectral norm at 5 equally-spaced points during cooldown on both architectures; confirm monotonic increase in both cases
  3. Collect iterates from last 20% of stable phase and first 20% of cooldown; run PCA separately and compare angle between first PCs to verify phase-distinct movement directions

## Open Questions the Paper Calls Out

### Open Question 1
How does the degree of overparameterization influence the performance and dynamics of the Warmup Stable Decay (WSD) scheduler? The study compared a large language model to a small CNN, leaving the specific impact of the interpolation regime (overparameterization) on WSD dynamics unisolated.

### Open Question 2
What is the fundamental geometric origin of the "river valley" loss landscape in non-language architectures? The authors confirm the "river valley" profile exists in CNNs, contradicting previous hypotheses that this geometry stems specifically from "heterogeneity in the stochasticity of tokens" in language models.

### Open Question 3
Why do CNNs require a proportionally longer cooldown phase than Transformers to achieve competitive performance with WSD? The authors note in the appendix that the CNN required a decay length of 35% of total steps to match cosine annealing, whereas the Transformer only required 20%.

## Limitations

- Limited cross-architecture comparison to only one LM architecture and one CNN architecture
- CNN experiments deliberately stopped short of convergence, potentially masking late-stage optimization behaviors
- Sharpness estimation via Hessian top eigenvalue becomes computationally prohibitive for larger models

## Confidence

- **High Confidence:** Architecture-independent river-valley loss landscape characterization
- **Medium Confidence:** Progressive sharpening during cooldown as a general phenomenon
- **Low Confidence:** Weak quasi-convexity along training trajectories

## Next Checks

1. Test WSD dynamics on additional architectures (e.g., ViT, MLP-Mixer) to verify river-valley consistency beyond CNN and LM
2. Extend CNN training to convergence to examine whether river-valley behavior persists in final optimization phases
3. Implement power iteration or Lanczos methods to estimate sharpness for larger models, validating whether progressive sharpening holds at scale