---
ver: rpa2
title: Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction
arxiv_id: '2505.18169'
source_url: https://arxiv.org/abs/2505.18169
tags:
- emotion
- learning
- physiological
- physics
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a Multi-Task Physics-Informed Neural Network
  (PINN) that jointly predicts Electrodermal Activity (EDA) and classifies emotional
  states using the WESAD dataset. The model integrates psychological self-reports
  (PANAS, SAM) with a physics-inspired differential equation representing EDA dynamics,
  enforcing physiological constraints through a custom loss function.
---

# Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction

## Quick Facts
- arXiv ID: 2505.18169
- Source URL: https://arxiv.org/abs/2505.18169
- Reference count: 16
- This study presents a Multi-Task Physics-Informed Neural Network (PINN) that jointly predicts Electrodermal Activity (EDA) and classifies emotional states using the WESAD dataset.

## Executive Summary
This study introduces a Multi-Task Physics-Informed Neural Network (PINN) that jointly predicts Electrodermal Activity (EDA) and classifies emotional states using the WESAD dataset. The model integrates psychological self-reports with a physics-inspired differential equation representing EDA dynamics, enforcing physiological constraints through a custom loss function. Evaluated via 5-fold cross-validation, the model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919, and F1-score of 94.08%. These results outperform classical baselines (SVR, XGBoost) and ablated variants, demonstrating improved performance and interpretability. The learned physical parameters (decay rate, emotional sensitivity, time scaling) are stable and physiologically meaningful, confirming alignment with stress-response theory. This is the first multi-task PINN framework for wearable emotion recognition, offering enhanced generalization and model transparency for healthcare and human-computer interaction applications.

## Method Summary
The method employs a dual-input, dual-output PINN architecture with two hidden layers (Swish activation, dropout, batch normalization). Inputs include a scalar time proxy and 3D emotion features (PANAS_mean, SAM_valence, SAM_arousal). The model jointly predicts continuous EDA values and binary emotional states (stress vs. non-stress). A physics-informed loss enforces a first-order ODE constraint representing EDA dynamics, with learnable parameters for decay rate (α₀), emotional sensitivity (β), and time scaling (γ). Training uses 5-fold cross-validation (80/20 split), 50 epochs per fold, Adam optimizer (lr=0.001), batch size 128, and TensorFlow GradientTape for automatic differentiation. The total loss combines MSE for EDA prediction, BCE for emotion classification, and a physics residual term weighted by learnable λ_phys.

## Key Results
- Average EDA RMSE: 0.0362, Pearson correlation: 0.9919
- Emotion classification F1-score: 94.08%, Accuracy: 94.72%
- Physics parameters (α₀, β, γ) are stable across folds and align with physiological stress-response theory
- Outperforms classical baselines (SVR, XGBoost) and ablated variants

## Why This Works (Mechanism)

### Mechanism 1
Embedding a differential equation constraint for EDA dynamics improves generalization and yields physiologically plausible predictions. The physics residual term penalizes violations of a first-order ODE relating EDA decay, emotional modulation, and time scaling. This biases the network toward solutions consistent with stress-response theory while still fitting data. Core assumption: The simplified ODE meaningfully approximates true EDA dynamics and is not so reductive as to misguide learning. Evidence: Learned physical parameters are interpretable and stable across folds, aligning with known principles of human physiology. Break condition: If the ODE fails to capture dominant EDA dynamics, the physics term could introduce systematic bias rather than regularization.

### Mechanism 2
Multi-task learning (simultaneous EDA regression + emotion classification) improves both tasks via shared representations and implicit regularization. The dual-output architecture forces shared hidden layers to learn embeddings useful for both predicting continuous EDA values and discretized emotional states. This creates bidirectional synergy where emotional context informs physiological predictions and vice versa. Core assumption: EDA dynamics and emotional states share underlying latent structure; improvements in one task transfer to the other rather than causing interference. Evidence: The multi-task configuration acts as a regularizer, improving convergence and reducing overfitting. Break condition: If tasks compete for capacity in shared layers, performance on one or both tasks degrades compared to single-task baselines.

### Mechanism 3
Learnable physics parameters (α₀, β, γ, λ_phys) provide interpretable insights into individual stress physiology while remaining stable across cross-validation folds. These parameters are optimized jointly with network weights via backpropagation. Their values emerge from the data fit but are constrained by the ODE structure, yielding quantities with direct physiological meaning (e.g., decay rate = recovery speed, β = emotional sensitivity). Core assumption: Learned parameter values reflect true physiological properties rather than artifacts of optimization dynamics or overfitting to training noise. Evidence: Learned physical parameters are interpretable and stable across folds, aligning with known principles of human physiology. Break condition: If parameters exhibit high variance across folds or subjects without clear physiological rationale, interpretability claims weaken.

## Foundational Learning

- Concept: Physics-Informed Neural Networks (PINNs)
  - Why needed here: Understanding how differential equation residuals become loss terms is essential for debugging convergence and interpreting physics parameters.
  - Quick check question: Can you explain how the residual γ·(dEDA/dt) + α₀·EDA - βᵀ·e is computed using automatic differentiation and minimized?

- Concept: Multi-Task Learning with Shared Representations
  - Why needed here: The architecture relies on shared hidden layers feeding separate output heads; understanding gradient interactions is critical for diagnosing task interference.
  - Quick check question: If EDA regression loss is 10⁻³ and emotion classification loss is 10⁰, what scaling or weighting strategies prevent one task from dominating updates?

- Concept: Electrodermal Activity (EDA) and Stress Physiology
  - Why needed here: The ODE constraint encodes domain knowledge; interpreting learned α₀ and β requires understanding sympathetic nervous system response dynamics.
  - Quick check question: What does a higher decay coefficient (α₀) imply about an individual's stress recovery, and how would this manifest in raw EDA signals?

## Architecture Onboarding

- Component map:
  Input branch 1 (t) → Shared backbone → Output head A (EDA regression)
  Input branch 2 (emotion features) → Shared backbone → Output head B (emotion classification)
  Physics module: Computes dEDA/dt, forms residual, optimizes α₀, β, γ, λ_phys

- Critical path:
  1. Forward pass produces EDA prediction and emotion classification
  2. GradientTape differentiates EDA prediction w.r.t. time to obtain derivative
  3. Physics residual computed from ODE
  4. Total loss = MSE(EDA) + BCE(emotion) + λ_phys × MSE(residual, 0)
  5. Joint backpropagation updates network weights + physics parameters

- Design tradeoffs:
  - Physics weight (λ_phys): Too high → underfits EDA data; too low → ignores physiological constraint
  - Shared vs. task-specific layers: More sharing improves regularization but risks negative transfer
  - Assumption: First-order ODE is a simplification; real EDA has tonic/phasic components not explicitly modeled

- Failure signatures:
  - Physics residual does not decrease: Check GradientTape implementation, ensure t is properly differentiated
  - Emotion F1 near 0 with low EDA loss: Task imbalance or missing gradient flow to classification head
  - Learned parameters unstable across folds: Possible overfitting or insufficient physics constraint
  - EDA predictions violate monotonicity or range constraints: Physics term may be underweighted

- First 3 experiments:
  1. Replicate single-fold training with physics loss ablated (λ_phys = 0) to verify performance gap
  2. Inspect learned β weights across all 5 folds; report mean and std to validate stability claims
  3. Test on held-out subject (leave-one-subject-out) to assess cross-subject generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does incorporating multimodal physiological signals (ECG, respiration) alter the learned emotional modulation weights (β) and the stability of the physics-constrained loss? The authors state they will extend this architecture by incorporating additional physiological signals such as ECG and respiration to support multimodal emotion modelling. Unresolved because the current study isolates EDA dynamics; it is unknown how additional biological oscillations interact with or contradict the simplified differential equation used for the physics loss. Evidence: A comparative study on WESAD showing the variance of β and α₀ when trained on EDA-only versus EDA+ECG+Respiration inputs.

### Open Question 2
Can domain adaptation techniques successfully transfer the learned physical parameters (α₀, β, γ) to unseen subjects or different datasets while maintaining high F1-scores? The Conclusion notes plans to explore domain adaptation for subject-independent generalization. Unresolved because the current 5-fold cross-validation likely mixes data from the same subjects in training and validation pools, potentially inflating performance metrics relative to real-world subject-independent scenarios. Evidence: Results from Leave-One-Subject-Out (LOSO) cross-validation or tests on a separate dataset, demonstrating that the physics parameters generalize without retraining.

### Open Question 3
Does the first-order differential equation (γ·(dEDA/dt) + α₀·EDA = ...) fail to capture rapid phasic bursts compared to non-linear or deconvolution-based EDA models? The paper describes the physics component as a simplified differential model, but does not validate if this linear ODE is sufficient for complex stress responses typically modeled by deconvolution (e.g., cvxEDA). Unresolved because a linear decay model may conflate tonic and phasic components, potentially limiting the interpretability of the decay rate (α₀) during high-frequency emotional stimuli. Evidence: A signal analysis comparing the model's predicted derivatives against ground-truth phasic peaks extracted by standard signal processing techniques.

## Limitations
- Physics formulation validity: The first-order ODE simplification may inadequately capture complex EDA dynamics (tonic/phasic components, multi-timescale recovery)
- Task interference risk: Multi-task optimization could cause negative transfer if emotion and EDA signals compete for shared representations
- Parameter stability interpretation: Learned physics parameters are reported stable across folds, but variance metrics and subject-level consistency are not quantified

## Confidence

- Performance claims (RMSE, F1, Pearson): High confidence
- Interpretability claims (physio-meaningful parameters): Medium confidence
- Multi-task synergy claims: Medium confidence

## Next Checks

1. Leave-one-subject-out test: Validate cross-subject generalization beyond 5-fold within-subject splits to confirm robustness
2. Parameter variance analysis: Compute coefficient of variation for α₀, β, γ across folds and subjects; correlate with baseline EDA characteristics
3. Ablation comparison: Isolate MTL effect by comparing single-task PINN (EDA-only) vs. single-task DNN (no physics) to quantify unique contributions