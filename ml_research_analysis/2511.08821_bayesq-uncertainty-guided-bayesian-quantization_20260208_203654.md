---
ver: rpa2
title: 'BayesQ: Uncertainty-Guided Bayesian Quantization'
arxiv_id: '2511.08821'
source_url: https://arxiv.org/abs/2511.08821
tags:
- bits
- bayesq
- posterior
- quantization
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BayesQ is a post-training quantization framework that optimizes
  quantization under the posterior expected loss using a lightweight Gaussian posterior
  over weights. It employs a greedy knapsack allocator to maximize marginal expected-loss
  reduction per bit under a global budget, and designs codebooks via posterior-weighted
  distortion in whitened space.
---

# BayesQ: Uncertainty-Guided Bayesian Quantization

## Quick Facts
- arXiv ID: 2511.08821
- Source URL: https://arxiv.org/abs/2511.08821
- Reference count: 40
- Primary result: Improves low-bit PTQ accuracy by 0.3-1.5 points over GPTQ baselines via posterior-guided quantization

## Executive Summary
BayesQ reframes low-bit quantization as uncertainty-aware risk minimization by fitting a lightweight Gaussian posterior over weights and allocating precision based on marginal expected-loss reduction. The framework uses a greedy knapsack allocator to maximize expected-loss reduction per bit under a global budget, and designs codebooks in posterior-whitened space to minimize distortion. At 3.0/3.5/4.0 average bits/weight, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) by up to 1.5 percentage points.

## Method Summary
BayesQ fits a diagonal Laplace posterior over each weight block via Hutchinson probes, whitens by the posterior covariance, and designs codebooks to minimize posterior-expected distortion. It allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. The framework uses 500 calibration examples for ResNet-50 and 5,000 for BERT-base, with optional 500-step distillation. Key technical innovations include closed-form expected-loss computation, weighted Lloyd-Max in whitened space, and submodular-like greedy allocation.

## Key Results
- ResNet-50 @ 3.0 bits: +1.5 top-1 points vs GPTQ
- ResNet-50 @ 3.5 bits: +0.7 top-1 points vs GPTQ  
- BERT-base @ 3.0 bits: +1.1 GLUE points vs GPTQ
- Robust to calibration set size (50-100 examples sufficient)
- Optional distillation adds +0.3-0.5 points at low bits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing quantization under posterior-expected loss improves low-bit accuracy by allocating precision proportional to weight uncertainty.
- Mechanism: Fit a lightweight Gaussian posterior over each weight block via diagonal Laplace (or K-FAC), then quantify distortion as the expectation over this posterior rather than at a single point. High-variance directions receive quantization error scaled by their uncertainty.
- Core assumption: The Laplace approximation captures meaningful epistemic uncertainty anisotropy; the Gaussian posterior is a sufficient proxy for true weight uncertainty.
- Evidence anchors:
  - [abstract] "BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion."
  - [Section 3.2, Eq. 5-7] Defines L_b(Q_b) ≡ E_{w_b∼N(μ_b,Σ_b)}[ℓ(Q_b(w_b), w_b)] and derives closed-form MSE expressions.
  - [corpus] Related work on activation-aware and curvature-based quantization (GPTQ, AWQ) provides indirect support but does not directly validate posterior-expected objectives.
- Break condition: If the true posterior is highly non-Gaussian (multimodal, heavy-tailed) or curvature estimates are too noisy on small calibration sets, expected-loss tables misallocate bits.

### Mechanism 2
- Claim: Greedy per-bit allocation by marginal expected-loss reduction per bit yields near-optimal mixed-precision under storage constraints when per-block loss curves exhibit diminishing returns.
- Mechanism: For each block and bit-width increment m→m+1, compute Δ_b(m) = L_b(Q_b^{(m)}) − L_b(Q_b^{(m+1)}) and γ_b(m) = Δ_b(m) / (C_b(m+1) − C_b(m)). Iteratively upgrade the (b,m) with largest γ_b(m) until budget exhausted.
- Core assumption: Per-block expected-loss reductions are approximately concave in bit-width, and cross-block couplings are weak (submodular-like structure).
- Evidence anchors:
  - [abstract] "allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget."
  - [Section 3.3, Eq. 10-11] Formalizes the constrained optimization and greedy rule.
  - [Section C.1] Discusses monotonicity, diminishing returns, and (1−1/e)-style approximation under approximate submodularity.
  - [corpus] Corpus does not contain direct validation of greedy optimality for this objective; related HAWQ-style knapsacks use Hessian proxies rather than posterior-expected loss.
- Break condition: If per-block loss curves are non-concave (e.g., sharp jumps at specific bit-widths) or strong cross-block dependencies exist, greedy may misallocate.

### Mechanism 3
- Claim: Quantizing in posterior-whitened space with Gaussian-weighted codebook design reduces distortion by making quantization error isotropic before re-projecting through the covariance.
- Mechanism: Whiten coordinates via z_b = S_b^{−1}(w_b − μ_b) where Σ_b = S_b S_b^⊤. Perform uniform or Lloyd-Max quantization in z-space where z∼N(0,I), then map codepoints back to weight space via c_k = μ_b + S_b c̃_k.
- Core assumption: Whitening correctly aligns quantization granularity with posterior geometry; the high-resolution approximation (Δ²/12 per-coordinate error) holds for 2–4 bit quantizers.
- Evidence anchors:
  - [abstract] "whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion."
  - [Section 3.4, Eq. 12-14] Formulates weighted Lloyd-Max in whitened space and derives uniform step/range optimization.
  - [Section B.1, Eq. 29-34] Derives posterior-expected MSE decomposition: L(Q̃) ≈ (Δ²/12) tr(Σ) + clipping terms.
  - [corpus] Corpus papers on quantization geometry (e.g., GPTQ as Babai's algorithm) provide conceptual parallels but no direct validation of whitening-based codebook design.
- Break condition: If the covariance estimate is poorly conditioned or the posterior is highly non-Gaussian, whitening may amplify errors rather than reduce them.

## Foundational Learning

- Concept: **Laplace approximation for Bayesian neural networks**
  - Why needed here: BayesQ relies on diagonal or K-FAC Laplace to estimate posterior covariance over weights from a pretrained model using only calibration data.
  - Quick check question: Given a trained network with parameters θ̂ and empirical Fisher F, what is the diagonal Laplace posterior N(θ̂, diag(F+λI)^{−1})?

- Concept: **Lloyd-Max / vector quantization with weighted distortion**
  - Why needed here: Codebook design in BayesQ uses Gaussian-weighted Lloyd iterations in whitened space to minimize posterior-expected MSE.
  - Quick check question: For a distribution p(z) over R^d and codebook {c_k}, what are the optimal Voronoi regions and centroid update rules?

- Concept: **Knapsack optimization and greedy approximation guarantees**
  - Why needed here: Bit allocation is formulated as a budget-constrained knapsack; the paper claims greedy per-bit upgrades are near-optimal under diminishing returns.
  - Quick check question: For a monotone submodular function with knapsack constraints, what approximation ratio does the density-greedy algorithm achieve?

## Architecture Onboarding

- Component map:
  - Posterior estimator (per-block): Hutchinson probes → diagonal Hessian/Fisher → damped covariance Σ_b
  - Whitening transform: Cholesky or eigendecomposition of Σ_b → S_b such that S_b S_b^⊤ = Σ_b
  - Codebook designer: For each block and bit-width candidate, run weighted Lloyd-Max or optimize uniform range α via line search
  - Expected-loss table builder: Compute L_b(Q_b^{(m)}) via closed-form MSE (Eq. 8) or MC proxy (Eq. 9)
  - Greedy allocator: Max-heap over (b,m,γ_b(m)) entries; pop and upgrade until budget exhausted
  - Optional distillation: 500 steps of KL alignment between quantized student and posterior-predictive teacher

- Critical path:
  1. Calibration pass → collect Fisher/Hessian statistics → fit posteriors for all blocks
  2. For each block, precompute expected-loss tables at all candidate bit-widths
  3. Run greedy allocator to determine final {m_b^⋆}
  4. Export quantized weights + per-block metadata (scales, codebooks if non-uniform)

- Design tradeoffs:
  - Diagonal vs K-FAC posterior: diagonal is faster (~0.95× GPTQ time) but may miss correlations; K-FAC captures anisotropy better (~1.05×) at higher memory cost
  - Uniform vs Lloyd codebooks: uniform with optimized range is fastest; posterior-weighted Lloyd gives +0.3–0.5 points at low bits but requires iterations
  - Calibration size: 50–100 examples sufficient for stability; <10 causes whitening failures; 500–1000 gives marginal gains
  - Distillation steps: 500 steps recommended for m̄≤3.5; >500 yields diminishing returns

- Failure signatures:
  - **Noisy curvature**: If Hutchinson probe count too low or damping λ too small, variances explode → whitening unstable → large accuracy drops. Symptom: per-block expected-loss table has NaN/Inf entries.
  - **Non-concave loss curves**: If some blocks show non-monotonic Δ_b(m), greedy may early-stop on wrong blocks. Symptom: abnormally low bits on downstream-sensitive layers (e.g., attention projections).
  - **Backend incompatibility**: If target runtime does not support mixed-precision or non-uniform codebooks, gains shrink. Symptom: fallback to uniform INT8 reduces advantage to +0.3–0.7 points.

- First 3 experiments:
  1. **Posterior quality ablation**: Compare diagonal Laplace vs K-FAC vs no-posterior (magnitude heuristic) on RN50@3.0 bits; measure accuracy gap and ECE. Expected: K-FAC +0.5 points over diagonal; no-posterior −1.0 points vs GPTQ baseline.
  2. **Calibration size sensitivity**: Run BayesQ on RN50 with calibration sets of size {10, 50, 100, 500}; track accuracy and whitening stability (check for NaN covariances). Expected: stable at ≥50; severe degradation at 10.
  3. **Allocator validation**: Compare greedy vs small-scale DP oracle on a subset of 5–10 blocks at 3.5 bits; report final accuracy and per-block bit assignments. Expected: greedy within 0.1 points of DP; both match within seed variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the near-optimality of the greedy knapsack allocator be formally proven via submodularity properties?
- **Basis in paper:** [explicit] The authors state that establishing "submodularity-like properties or bounds on the greedy allocator's suboptimality" is a key theoretical direction for strengthening the framework's foundations.
- **Why unresolved:** While the paper empirically observes diminishing returns and checks against a toy Dynamic Programming (DP) oracle, it does not provide a theoretical guarantee that the greedy solution approximates the global optimum within a constant factor.
- **What evidence would resolve it:** A mathematical proof demonstrating that the marginal loss reduction function satisfies the properties of a submodular set function under the Gaussian posterior assumption.

### Open Question 2
- **Question:** Can posterior amortization eliminate the computational cost of Hutchinson probes for extremely large models?
- **Basis in paper:** [explicit] The paper suggests that "learning small adapters to predict block-level uncertainty from activations—may reduce or eliminate Hutchinson probes at scale."
- **Why unresolved:** The current method relies on Hutchinson probes to estimate the diagonal of the Hessian/Fisher, which has a complexity of O(MD) per sweep. This cost can dominate the PTQ budget for very deep transformers, making the method potentially inefficient for trillion-parameter models without amortization.
- **What evidence would resolve it:** A study showing that a learned predictor can approximate block-level posterior variances with sufficient accuracy to maintain BayesQ's accuracy gains while reducing preprocessing time significantly compared to the probe-based baseline.

### Open Question 3
- **Question:** How can the framework be adapted for joint weight–activation quantization to handle activation outliers?
- **Basis in paper:** [explicit] The conclusion lists "joint weight–activation BayesQ" as a future direction to "extend posterior guidance to activation ranges and outlier paths."
- **Why unresolved:** The current framework focuses on weight-only quantization (keeping FP16 activations). It does not model the uncertainty or non-Gaussian distributions of activations, which are critical bottlenecks for full integer (INT) inference due to outlier channels.
- **What evidence would resolve it:** An extension of BayesQ that fits posteriors over activations, optimizes activation codebooks, and maintains accuracy in a full INT8/INT4 pipeline on LLMs.

## Limitations

- Gaussian posterior approximation may not capture true epistemic uncertainty in highly non-convex, multimodal loss landscapes
- No empirical validation of greedy knapsack allocator against DP oracle or analysis of non-concave loss curves
- Whitening-based codebook design assumes well-conditioned curvature estimates; ill-conditioning or poor calibration sets can destabilize the transform

## Confidence

- Mechanism 1 (posterior-expected loss): **Medium** - Laplace approximations are standard but untested for quantization-specific anisotropy; no ablation on posterior quality.
- Mechanism 2 (greedy allocation): **Low** - Theoretical approximation bounds referenced but no empirical validation against DP oracle; no analysis of non-concave loss curves.
- Mechanism 3 (whitened codebook design): **Medium** - Geometric intuition aligns with prior quantization literature, but no ablation on whitening stability or covariance conditioning.

## Next Checks

1. **Posterior quality ablation**: Compare diagonal Laplace vs K-FAC vs magnitude-based allocation on RN50@3.0 bits; measure accuracy gap and expected calibration error. Expected: K-FAC +0.5 points over diagonal; no-posterior −1.0 points vs GPTQ baseline.
2. **Calibration size sensitivity**: Run BayesQ with 10, 50, 100, 500 calibration examples; track accuracy, whitening stability (NaN covariances), and per-block variance magnitudes. Expected: stable at ≥50; severe degradation at 10.
3. **Allocator validation**: Compare greedy vs DP oracle on a subset of 5–10 blocks at 3.5 bits; report accuracy difference and per-block bit assignments. Expected: greedy within 0.1 points of DP; both match within seed variance.