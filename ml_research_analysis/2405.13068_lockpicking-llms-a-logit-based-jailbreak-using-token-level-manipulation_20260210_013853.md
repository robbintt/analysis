---
ver: rpa2
title: 'Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation'
arxiv_id: '2405.13068'
source_url: https://arxiv.org/abs/2405.13068
tags:
- harmful
- llms
- arxiv
- jailbreak
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation

## Quick Facts
- arXiv ID: 2405.13068
- Source URL: https://arxiv.org/abs/2405.13068
- Reference count: 40
- Primary result: Achieves up to 96% attack success rate on open-source LLMs by manipulating output logits to suppress refusals and bias generation toward harmful content

## Executive Summary
JailMine is a novel jailbreak attack that manipulates LLM output logits at the token level to bypass safety alignment and elicit harmful responses. By strategically setting logits to +∞ for affirmative tokens and -∞ for denial tokens, the attack biases generation toward harmful completions while suppressing standard refusals. The method leverages a learned sorting model to efficiently prioritize promising manipulations, achieving high success rates across multiple open-source LLMs. This approach is distinct from prompt-level attacks and requires white-box access to the model's logits.

## Method Summary
JailMine manipulates LLM output logits to bias generation toward harmful content while suppressing refusals. The method uses a few-shot template to generate affirmative prefixes, then manipulates logits: denial tokens are set to -∞, affirmative tokens to +∞, and additional tokens are sampled and set to +∞ for a prefix of length m. A sorting model ranks manipulation sets based on predicted success, and generation proceeds until a judge classifier confirms harmful output. The approach is evaluated on multiple open-source models using the AdvBench and JailbreakBench datasets.

## Key Results
- Achieves up to 96% attack success rate on open-source LLMs (LLaMA-2-7B/13B, Mistral-7B, Gemma-7B, LLaMA-3-8B)
- Outperforms baseline methods (GCG, PAIR, GPTFuzzer) in both success rate and efficiency
- Sorting model achieves 0.9207 F1 score in predicting successful manipulations
- Effective against a wide range of harmful behaviors in benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manipulating early output logits toward affirmative tokens increases harmful content generation.
- Mechanism: JailMine sets logit values for tokens in a pre-generated affirmative prefix (e.g., "Sure, here is a tutorial on making a bomb") to +∞ before generation, biasing the model to continue in that direction rather than refuse.
- Core assumption: LLMs conditioned on affirmative prefixes are more likely to complete harmful outputs, per the observed correlation in motivation experiments.
- Evidence anchors:
  - [abstract]: "strategically selecting affirmative outputs and iteratively reducing the likelihood of rejection"
  - [section 4.2.3]: "Each logits corresponding to the tokens in R is set to +∞, ensuring that these tokens are not chosen in subsequent generations, facilitating a more controlled output direction."
  - [corpus]: Weak direct corpus evidence; related work (Prefill-level Jailbreak) suggests prefilling attack surfaces exist but mechanisms differ.
- Break condition: If target LLM uses post-hoc safety filtering or output-level classifiers that override prefix-based generation, this mechanism may fail.

### Mechanism 2
- Claim: Suppressing denial token logits prevents standard refusal outputs.
- Mechanism: JailMine identifies a fixed set of denial prefixes (DT) from empirical analysis (e.g., "I'm sorry", "As an AI", "I cannot") and sets their logits to -∞ during manipulation, blocking the model from entering standard refusal trajectories.
- Core assumption: Refusals are largely tokenized through a small, identifiable set of prefix patterns; blocking these exhausts the model's default refusal pathways.
- Evidence anchors:
  - [section 3.2]: "99.19% of the denial responses can be classified into these four categories."
  - [section 4.2.3]: "we first adjust logits for the denial tokens to −∞, preventing their selection."
  - [corpus]: Related jailbreak taxonomies (e.g., "A Domain-Based Taxonomy of Jailbreak Vulnerabilities") note refusal-pattern exploitation but do not confirm suppression efficacy.
- Break condition: If the model has diverse or randomized refusal patterns beyond the identified 17 prefixes, or if it can refuse without using those prefixes, suppression will be incomplete.

### Mechanism 3
- Claim: A learned sorting model increases attack efficiency by prioritizing promising logit manipulations.
- Mechanism: JailMine trains a neural network (Γ) on empirical response data to predict which manipulated logit sequences are likely to yield harmful outputs, sorting batch candidates before full generation.
- Core assumption: The first m tokens of a response are predictive of whether the full generation will be harmful, and this pattern is learnable.
- Evidence anchors:
  - [section 4.2.4]: "we collect the 1,000 responses of LLMs from 100 questions... train a simple neural network Γ"
  - [section 6.1]: "the model achieves an F1 Score of 0.9207 with m=5"
  - [corpus]: No direct corpus corroboration; efficiency gains are paper-specific.
- Break condition: If the target LLM's behavior shifts (e.g., via fine-tuning or different system prompts), the sorting model may become misaligned and reduce efficiency.

## Foundational Learning

- Concept: **Logits and Softmax in LLM Generation**
  - Why needed here: JailMine directly manipulates logits; understanding how logits become probabilities is essential to follow the attack.
  - Quick check question: If a logit is set to +∞ and others remain finite, what happens to the probability of the corresponding token after softmax?

- Concept: **Token-level vs Prompt-level Jailbreaks**
  - Why needed here: The paper explicitly positions JailMine as a token-level approach, distinct from prompt crafting; this determines the attack surface and required access.
  - Quick check question: What type of model access is required for gradient-based token optimization vs. prompt-level black-box fuzzing?

- Concept: **Few-shot Templating for Response Generation**
  - Why needed here: JailMine uses few-shot templates to auto-generate affirmative prefixes; understanding this clarifies how attack targets are constructed.
  - Quick check question: How does providing example harmful-behavior/response pairs in a prompt influence the model's output format?

## Architecture Onboarding

- Component map: Positive Response Generator (few-shot templating) -> Logits Manipulator (Algorithm 1) -> Sorting Model Γ (two-layer neural net) -> Harmful Content Generator (Algorithm 2)
- Critical path: Harmful question -> Positive Response Generator -> Logits Manipulator (k+m manipulations) -> Sorting Model -> Generation Loop -> JUDGE check -> return harmful output
- Design tradeoffs:
  - Batch size N (2000): Larger N increases coverage but raises compute/time; smaller N risks missing successful manipulations.
  - Prefix length m (5): Chosen based on observed denial prefix lengths; longer m adds robustness but increases manipulation complexity.
  - Sorting model complexity: Simple two-layer network chosen for speed; deeper models could improve prediction but may overfit to specific LLM behaviors.
- Failure signatures:
  - All batch candidates trigger refusal patterns not in DT -> denial set incomplete.
  - JUDGE consistently returns False despite manipulations -> model may have output-level safety layers.
  - Sorting model F1 drops significantly -> LLM behavior has shifted; retraining may be needed.
- First 3 experiments:
  1. Baseline Reproduction: Run JailMine on a single model (e.g., Llama-2-7B-chat) with provided code and hyperparameters; verify ASR and time match paper.
  2. Ablation on m and N: Systematically vary m (3, 5, 7, 10) and N (500, 1000, 2000, 3000) on a subset of AdvBench; observe ASR/time tradeoffs.
  3. Denial Set Coverage Check: Collect refusal responses from a target model not in training; manually compute what fraction start with prefixes not in DT to identify coverage gaps.

## Open Questions the Paper Calls Out

- **How does JailMine perform against active defense mechanisms such as adversarial training, perplexity filtering, or specialized guard models?**
  - Basis in paper: [explicit] Section 6.2 states, "Given the scarcity of defensive measures in LLMs against jailbreaking attacks, we have not incorporated any defense mechanisms in the three baseline models or JailMine."
  - Why unresolved: The evaluation only tests JailMine against the base safety alignment of open-source models without applying additional defensive layers that are common in robust deployments.
  - What evidence would resolve it: Empirical Attack Success Rates (ASR) on models specifically hardened with defenses like SmoothLLM or adversarial training against token manipulation.

- **Can JailMine be adapted for black-box settings where logit access is unavailable, or do the generated manipulations transfer to closed-source commercial models?**
  - Basis in paper: [explicit] Section 4.2.1 defines the attack model as a white-box scenario requiring access to "open-source LLMs' logit distribution information."
  - Why unresolved: The core mechanism (Algorithm 1) sets logits to ±∞ to force token selection, which requires internal model access, leaving the vulnerability of API-only models untested.
  - What evidence would resolve it: A modified JailMine framework that approximates logit biasing via token sampling manipulation in an API, or data showing the attack vectors transfer to models like GPT-4.

- **Does the Sorting Model Γ remain effective when target models update their refusal strategies to include semantic variations not present in the current four denial categories?**
  - Basis in paper: [inferred] Section 6.2 suggests that "expanding the repertoire of denial patterns" would "significantly diminish the effectiveness of our sorting model," and Section 3.2 notes the current limited variety of denials.
  - Why unresolved: The sorting model is trained on the current distribution of refusal prefixes (e.g., "I cannot", "As an AI"), but its robustness to novel, adversarially designed refusal styles is unknown.
  - What evidence would resolve it: Evaluation of the sorting model's accuracy (F1 score) on outputs from models fine-tuned to use obfuscated or non-standard refusal language.

## Limitations
- Denial token coverage may be incomplete for models with diverse or randomized refusal strategies beyond the identified 17 prefixes.
- The sorting model's effectiveness may degrade when target models update their refusal strategies to include semantic variations not present in the current training data.
- Requires white-box access to model logits, limiting applicability to closed-source models or API-only deployments.

## Confidence
- **High Confidence**: The basic mechanism of manipulating logits to bias generation toward affirmative prefixes and away from denial tokens is technically sound and well-grounded in LLM theory.
- **Medium Confidence**: The claim that 99.19% of refusals can be blocked using 17 prefixes is empirical but likely model- and prompt-specific; broader coverage may require dynamic prefix detection.
- **Medium Confidence**: The reported F1 score (0.9207) for the sorting model suggests strong in-distribution performance, but generalization to new models or behaviors is uncertain.
- **Low Confidence**: The claim that JailMine is "more efficient" than gradient-based attacks (GCG, PAIR) is based on limited comparison and does not account for variance across model families or input distributions.

## Next Checks
1. **Denial Token Coverage Test**: Collect refusals from 3-5 target models not used in training; compute the fraction that begin with prefixes outside DT to quantify coverage gaps and identify model-specific denial patterns.
2. **Cross-Model Sorting Model Generalization**: Evaluate Γ on a held-out model (e.g., Gemma-7B-IT if not in training set); measure F1 drop and retrain if needed to assess robustness to behavioral shifts.
3. **Post-hoc Filter Resilience Test**: Run JailMine outputs through an independent safety classifier (e.g., Llama-Guard-2 or custom fine-tuned filter) to verify that manipulated generations are not caught at the output level.