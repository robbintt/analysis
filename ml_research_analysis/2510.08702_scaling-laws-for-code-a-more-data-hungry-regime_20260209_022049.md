---
ver: rpa2
title: 'Scaling Laws for Code: A More Data-Hungry Regime'
arxiv_id: '2510.08702'
source_url: https://arxiv.org/abs/2510.08702
tags:
- code
- scaling
- data
- arxiv
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work conducts the first large-scale empirical study of scaling\
  \ laws for code, training 117 models ranging from 0.2B to 3.8B parameters on code\
  \ datasets of 2B to 128B tokens. The study demonstrates that the Farseer scaling\
  \ law formulation provides significantly better fit and prediction accuracy for\
  \ code than the Chinchilla law, with a mean relative error of 0.82\u2030 versus\
  \ 1.03\u2030."
---

# Scaling Laws for Code: A More Data-Hungry Regime

## Quick Facts
- **arXiv ID**: 2510.08702
- **Source URL**: https://arxiv.org/abs/2510.08702
- **Reference count**: 24
- **Key outcome**: Code scaling requires substantially higher data-to-parameter ratios than natural language, with Farseer formulation providing superior predictive accuracy.

## Executive Summary
This paper presents the first large-scale empirical study of scaling laws for code, training 117 models from 0.2B to 3.8B parameters on code datasets ranging from 2B to 128B tokens. The study demonstrates that code represents a more data-hungry regime than natural language, requiring higher data-to-parameter ratios for optimal performance. The Farseer scaling law formulation provides significantly better fit and prediction accuracy than the Chinchilla law, with mean relative errors of 0.82‰ versus 1.03‰. The research also reveals that natural language data benefits code model performance in resource-constrained scenarios but becomes detrimental at higher compute budgets.

## Method Summary
The study trained 117 decoder-only Transformer models with parameters ranging from 0.2B to 3.8B on code datasets from 2B to 128B tokens, using the OpenCoder corpus downsampled to 895.51B tokens. Models employed SwiGLU activations, Rotary Position Embeddings, and RMSNorm. Hyperparameters were derived from target parameter counts using Farseer's aspect-ratio procedure, with training following AdamW optimization and cosine learning rate decay. The resulting (N, D, Loss) tuples were fitted to the Farseer formulation to establish code-specific scaling laws, validated against held-out code datasets.

## Key Results
- Code requires substantially higher data-to-parameter (D/N) ratios than natural language for compute-optimal training
- The Farseer scaling law formulation provides significantly better fit and prediction accuracy for code than Chinchilla law
- Natural language data provides performance benefits in resource-constrained scenarios but becomes detrimental at higher compute budgets
- Code scaling with model size is robust, contrary to initial expectations of plateauing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code requires higher D/N ratios than natural language for compute-optimal training.
- Mechanism: Code corpora exhibit inherent repetitiveness—common syntactic patterns and programming idioms are mastered early, requiring massive token volumes to encounter genuinely novel application scenarios.
- Core assumption: The lower average information density of large-scale code corpora drives the data hunger, rather than task complexity per se.
- Evidence anchors: [abstract] "code represents a more data-hungry regime, requiring substantially higher data-to-parameter ratios than natural language"

### Mechanism 2
- Claim: The Farseer scaling law formulation captures code's scaling dynamics more accurately than Chinchilla.
- Mechanism: Farseer allows scaling exponents to vary with model size N, capturing the interplay where larger models learn more efficiently from data, while Chinchilla's fixed exponents underestimate code's accelerating data demand.
- Core assumption: The fitted Farseer surface generalizes beyond the 0.2B–3.8B parameter range studied.
- Evidence anchors: [Page 4] "Farseer formulation provides a better fit... lower mean relative error than the Chinchilla model (0.82‰ vs. 1.03‰)"

### Mechanism 3
- Claim: Natural language data benefits code model performance in low-resource regimes but degrades it at higher compute budgets.
- Mechanism: In small models or data-scarce settings, NL acts as a regularizer and provides transferable world knowledge, but as model capacity increases, the distributional shift cost outweighs regularization benefits.
- Core assumption: The crossover point scales predictably with N and D/N ratio.
- Evidence anchors: [abstract] "NL benefits resource-constrained scenarios, but becomes a detriment at higher compute budgets"

## Foundational Learning

- **Scaling Laws (N, D, C relationship)**:
  - Why needed here: The entire paper builds on understanding how loss L relates to model size N, data size D, and compute C = 6ND.
  - Quick check question: Given fixed compute C, if you double model size N, what happens to optimal data D?

- **Data-to-Parameter Ratio (D/N)**:
  - Why needed here: The central finding is that code's optimal D/N ratio differs from NL's and grows with compute budget.
  - Quick check question: If Chinchilla suggests D/N ≈ 20 for NL and Farseer suggests D/N ≈ 150 for code at a given compute, which model would be undertrained if trained with D/N = 20?

- **Power-Law Relationships**:
  - Why needed here: Loss scales approximately as power-law functions of N and D; recognizing log-log linearity is key to validating scaling behavior.
  - Quick check question: On a log-log plot of loss vs. model size, what does a straight line indicate?

## Architecture Onboarding

- **Component map**: OpenCoder corpus (895.51B tokens) -> 117 decoder-only Transformers (0.2B-3.8B params) -> AdamW training with cosine decay -> validation on 6.3M token held-out set

- **Critical path**: 1) Define target N and D from compute budget 2) Derive architecture config from N 3) Calculate optimal LR and batch size via StepLaw 4) Train on nested subsets of code corpus 5) Evaluate on held-out validation set

- **Design tradeoffs**: Fewer GPUs → higher MFU but longer runtime; pruning extreme D/N ratios reduces cost but limits extrapolation; downsampling high-volume languages improves balance but may underrepresent web-code patterns

- **Failure signatures**: If validation loss doesn't decrease monotonically with N or D, scaling law assumptions break; if Chinchilla-fit predictions match empirical results better than Farseer, code may not require expressive exponents

- **First 3 experiments**:
  1. **Sanity check**: Train 3 models at fixed D (10B tokens) with N = 0.5B, 1B, 2B. Verify loss decreases with N on log-log plot.
  2. **D/N validation**: At fixed compute C, train models at D/N = 20, 100, 200. Confirm code optimum is higher than NL-predicted 20.
  3. **Mixture probe**: For N = 0.3B at D/N = 30, compare pure code vs. 70/30 code-NL mixture. Expect mixture to win at this scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise optimal blend of code and natural language data across the entire mixture spectrum?
- Basis in paper: [explicit] The authors state in the Limitations section that a "more fine-grained study with additional ratios is needed to map the entire mixture spectrum," as they only tested 70/30 and 30/70 splits.
- Why unresolved: The current study only tested two distant points on the mixture spectrum to maximize observable differences, leaving the specific optimal ratio for various compute budgets undefined.
- What evidence would resolve it: A series of experiments varying the code-NL ratio in smaller increments (e.g., 10% steps) to fit a continuous function for performance relative to mixture composition.

### Open Question 2
- Question: Does the Farseer scaling law maintain predictive accuracy at the extreme compute scales characteristic of the largest industrial models?
- Basis in paper: [explicit] The authors note that their validation "did not extend to the extreme scales of the largest industrial models," leaving predictive accuracy at higher orders of magnitude unconfirmed.
- Why unresolved: The study validated predictions on models up to 3.8B parameters, while frontier models often exceed 70B+ parameters; errors may compound at these scales.
- What evidence would resolve it: Training and validating code LLMs with parameters in the 7B to 70B range and comparing the empirical loss against the values extrapolated from the 3.8B fitted curve.

### Open Question 3
- Question: Does improving code data quality specifically reduce the high data-to-parameter (D/N) ratio required for optimal training?
- Basis in paper: [inferred] The authors hypothesize in Section 4.2 that high D/N ratios are driven by the "inherent repetitiveness" of code, suggesting that enhancing data quality might be more effective than increasing quantity.
- Why unresolved: The experiments controlled for scale and mixture but did not systematically vary data quality (e.g., filtering for high-complexity code) to observe its effect on the D/N ratio.
- What evidence would resolve it: An ablation study comparing models trained on standard corpora versus "high-quality" upsampled corpora to see if the compute-optimal D/N ratio decreases.

## Limitations

- The empirical foundation is robust within the 0.2B-3.8B parameter range, but findings are extrapolated to regimes orders of magnitude larger without validation.
- The claim about code being "more data-hungry" rests on the assumption that GitHub-style corpora represent typical code distributions, but specialized domains might exhibit different scaling characteristics.
- The NL mixture experiments use unspecified "general web text" for NL data, leaving open whether domain-specific NL would show different crossover behavior.

## Confidence

- **High Confidence** (95%+): The empirical finding that code achieves optimal performance at higher D/N ratios than natural language, within the tested parameter range.
- **Medium Confidence** (75%): The claim that code scaling with model size is "robust" rather than plateauing, though extrapolation to larger models introduces uncertainty.
- **Low Confidence** (60%): The Farseer formulation's superiority for code scaling beyond the tested range, as validation on larger architectures or alternative code datasets is lacking.

## Next Checks

1. **Exponent Stability Test**: Train an additional model at 7-8B parameters and measure whether the D/N optimum continues the upward trend or plateaus.

2. **Corpus Diversity Validation**: Repeat the scaling experiments using a different code corpus (e.g., CodeSearchNet or a domain-specific codebase) to determine if the "data-hungry" regime is universal or corpus-dependent.

3. **NL Mixture Domain Sensitivity**: Repeat the NL mixture experiments using technical documentation or code-adjacent text instead of general web text to determine if the crossover point shifts.