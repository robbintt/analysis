---
ver: rpa2
title: 'Named entity recognition for Serbian legal documents: Design, methodology
  and dataset development'
arxiv_id: '2502.10582'
source_url: https://arxiv.org/abs/2502.10582
tags:
- language
- legal
- training
- entity
- serbian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a Named Entity Recognition (NER) system for\
  \ Serbian legal documents using BERT-based pre-trained models. The authors developed\
  \ a novel dataset of 75 court rulings and fine-tuned the BERTi\xB4c model for identifying\
  \ 8 named entity types (courts, dates, decisions, laws, money, official gazettes,\
  \ person names, and references)."
---

# Named entity recognition for Serbian legal documents: Design, methodology and dataset development

## Quick Facts
- arXiv ID: 2502.10582
- Source URL: https://arxiv.org/abs/2502.10582
- Authors: Vladimir Kalušev; Branko Brkljač
- Reference count: 40
- Key outcome: BERT-based NER system for Serbian legal documents achieves 0.96 macro F1 score on 75 annotated court rulings

## Executive Summary
This paper presents a Named Entity Recognition system for Serbian legal documents using fine-tuned BERT-based models. The authors developed a novel dataset of 75 court rulings and fine-tuned the BERTić model to identify 8 named entity types with high accuracy. The system demonstrates effectiveness for low-resource languages through domain-specific adaptation of pre-trained language models, achieving state-of-the-art performance with limited annotated data.

## Method Summary
The approach uses BERTić, a Serbian language model based on ELECTRA architecture, fine-tuned for token classification using BIO annotation scheme. The system processes transliterated Latin text from Cyrillic documents, applying a token-level classifier to identify 15 labels representing 8 entity types. Training uses 5-fold cross-validation with clustered partitioning to ensure balanced entity distribution, employing AdamW optimizer with specific hyperparameters (LR 2e-5, batch size 2 with gradient accumulation).

## Key Results
- Achieves mean macro F1 score of 0.96 through 5-fold cross-validation
- Successfully identifies 8 named entity types in Serbian legal documents
- Demonstrates robustness against text errors and demonstrates feasibility for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a language-specific pre-trained model enables effective NER with limited annotated data. The BERTić model, pre-trained on Serbian and South Slavic corpora using ELECTRA's replaced token detection objective, provides contextualized token representations that capture language-specific patterns. Fine-tuning adapts these representations to the legal NER task by updating model weights on a small labeled dataset (75 documents, 2172 sentences).

### Mechanism 2
BIO annotation scheme with careful entity boundary definition enables accurate multi-word entity recognition. The BIO scheme marks token positions within entities (B- for beginning, I- for inside, O- for outside), producing 15 classification labels from 8 entity types. Character-level annotation precision prevents boundary errors from whitespace and punctuation.

### Mechanism 3
Clustering-based cross-validation partitioning ensures representative entity distribution across folds. Documents are clustered by normalized histograms of NE type frequencies, then sampled without replacement to create balanced partitions. This prevents entity type imbalance between training and test sets.

## Foundational Learning

- **BERT encoder architecture and self-attention**: Understanding how BERT processes bidirectional context (up to 512 tokens) enables comprehension of why encoder-based models suit NER versus decoder models. *Quick check*: Can you explain why BERT's bidirectional attention is more suitable for NER than GPT's unidirectional attention?

- **Sequence labeling and token classification**: NER is formulated as multi-class token classification where each token receives one of 15 labels (BIO scheme × entity types). *Quick check*: Given the sentence "Presude Apelacionog suda," what labels would B-Court and I-Court tokens receive?

- **Transfer learning and fine-tuning hyperparameters**: The paper uses specific hyperparameters (learning rate 2e-5, 6 epochs, batch size 2 with gradient accumulation) that deviate from defaults. *Quick check*: Why might a smaller batch size with gradient accumulation be necessary for legal documents with long sentences?

## Architecture Onboarding

- **Component map**: Serbian legal text -> BERTić tokenizer -> BERTić ELECTRA-discriminator (110M parameters) -> Token-level linear classifier (15 output classes) -> BIO labels per token, aggregated to entity spans

- **Critical path**: 
  1. Verify input text encoding matches BERTić tokenizer expectations
  2. Confirm maximum sequence length (512 tokens) is not exceeded; implement sentence splitting if needed
  3. Load pre-trained weights from HuggingFace (`kalusev/NER4Legal_SRB`)
  4. Post-process BIO labels to extract entity spans (merge B-X and subsequent I-X tokens)

- **Design tradeoffs**: 
  - Small dataset (75 documents) vs. model capacity: Risk of overfitting mitigated by 5-fold cross-validation and early stopping
  - Latin-only processing: Original Cyrillic documents transliterated; BERTić produces more tokens for Cyrillic, potentially affecting performance
  - Macro-averaged metrics reported: Class "O" dominates numerically; macro averaging prevents inflated scores from majority class

- **Failure signatures**: 
  - B-Reference/I-Reference classes show lowest F1 (0.89-0.93): High variability in alphanumeric formats across courts
  - B-Person shows lower precision: Anonymized abbreviations (e.g., "AA") cause false positives on two-letter tokens
  - B-Official Gazette has few training examples (112 total) but achieves F1=0.93: Lower within-class variability compensates

- **First 3 experiments**: 
  1. Reproduce baseline: Run inference on held-out documents from the published dataset, verify macro F1 ≈ 0.96
  2. Robustness test: Apply synthetic noise (character swaps, deletions) to test inputs and measure performance degradation; compare against paper's Figure 6 qualitative examples
  3. Out-of-domain test: Evaluate on Serbian legal documents not from appellate courts (e.g., municipal court rulings) to characterize domain transfer limits

## Open Questions the Paper Calls Out
None

## Limitations
- Data domain specificity: Results limited to appellate court rulings on non-economic damage cases, may not generalize to other legal domains
- Transliteration impact: Converting Cyrillic to Latin script may affect performance and computational efficiency
- Entity boundary ambiguity: B-Reference and B-Person classes show systematic difficulties with variable formats and anonymized names

## Confidence
- **High Confidence**: BERTić fine-tuning approach achieves strong performance on Serbian legal NER for the specific appellate court domain tested
- **Medium Confidence**: Model demonstrates robustness to text errors, though evaluation is qualitative rather than quantitative
- **Low Confidence**: Claims about generalizability to other Serbian legal domains or courts without out-of-domain validation

## Next Checks
1. **Out-of-Domain Performance**: Evaluate the model on Serbian legal documents outside the appellate court domain (municipal courts, criminal cases, administrative rulings) to quantify domain transfer limits and identify systematic failure patterns.

2. **Error Type Analysis**: Conduct systematic testing with controlled text errors (character substitutions, deletions, insertions) across different entity types to measure robustness quantitatively rather than relying on qualitative examples.

3. **Cyrillic vs. Latin Comparison**: Train and evaluate identical models on original Cyrillic text versus transliterated Latin text to quantify the performance impact of the transliteration decision and determine optimal processing approach.