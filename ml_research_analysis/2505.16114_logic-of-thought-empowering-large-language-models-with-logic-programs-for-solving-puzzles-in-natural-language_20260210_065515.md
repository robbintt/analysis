---
ver: rpa2
title: 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for
  Solving Puzzles in Natural Language'
arxiv_id: '2505.16114'
source_url: https://arxiv.org/abs/2505.16114
tags:
- block
- puzzle
- puzzles
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Logic-of-Thought (Logot) addresses the challenge of solving puzzles
  in natural language by bridging large language models (LLMs) with logic programming.
  The method translates puzzle rules and initial states into answer set programs (ASPs)
  using LLMs, then employs ASP solvers for accurate and efficient inference.
---

# Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language

## Quick Facts
- arXiv ID: 2505.16114
- Source URL: https://arxiv.org/abs/2505.16114
- Authors: Naiqi Li; Peiyuan Liu; Zheng Liu; Tao Dai; Yong Jiang; Shu-Tao Xia
- Reference count: 40
- Primary result: Hybrid LLM+ASP method achieves near-perfect accuracy on Sudoku, Hitori, Fillomino, and Blocks World puzzles

## Executive Summary
Logic-of-Thought (Logot) bridges large language models with logic programming to solve natural language puzzles with high accuracy and efficiency. The method translates puzzle rules and initial states into answer set programs using LLMs, then employs ASP solvers for precise reasoning and search. Experiments show Logot achieves near-perfect accuracy across classic grid puzzles and dynamic planning tasks, outperforming direct LLM reasoning while maintaining cost efficiency through smaller model variants.

## Method Summary
Logot translates natural language puzzles into answer set programs (ASPs) using LLMs with few-shot examples, then employs off-the-shelf ASP solvers for accurate inference. The method consists of two translation modules: rule translation converts puzzle constraints into ASP rules, while state translation encodes initial configurations as ASP facts. The hybrid approach combines LLMs' natural language understanding with ASP solvers' systematic reasoning capabilities, achieving near-perfect accuracy on tested puzzles while maintaining computational efficiency.

## Key Results
- GPT-4o achieves 100% accuracy on all tested puzzle types (Sudoku, Hitori, Fillomino, Blocks World)
- Deepseek-V3 and GPT-4o-mini maintain high accuracy with reduced computational cost
- Standard prompting and CoT approaches achieve 0% accuracy on grid puzzles, demonstrating Logot's effectiveness
- Translation errors identified as the primary failure mode, while solver errors are rare

## Why This Works (Mechanism)

### Mechanism 1: Division of Cognitive Labor
Delegating constraint satisfaction and exhaustive search to ASP solvers yields near-perfect accuracy on combinatorial puzzles where LLMs alone fail. LLMs perform natural-language-to-logic translation (a pattern-matching task suited to their strengths), while the ASP solver handles systematic grounding and search—tasks where LLMs struggle due to error propagation and lack of guaranteed completeness.

### Mechanism 2: Analogical Transfer via Few-Shot Prompting
Providing translated examples from related puzzle domains enables LLMs to induce correct translation patterns for unseen instances without fine-tuning. The prompt concatenates example pairs showing natural-language-to-ASP mappings, plus the target input, allowing the LLM to perform analogical reasoning.

### Mechanism 3: Declarative Specification with Automatic Search
Expressing puzzles declaratively (stating what constraints must hold) rather than procedurally (how to search) enables off-the-shelf solvers to handle search complexity automatically. ASP rules encode constraints as logical implications, and Clingo grounds the program then computes stable models satisfying all constraints.

## Foundational Learning

- **Answer Set Programming (ASP) Syntax and Semantics**: To understand how constraints like "no duplicate numbers in a row" become ASP rules like `:- pos(X,Y1,N), pos(X,Y2,N), Y1!=Y2.` Quick check: Given `:- black(X,Y), black(X+1,Y).`, what does this prevent?
- **In-Context Learning / Few-Shot Prompting**: The entire translation module relies on LLMs learning from demonstration pairs in the prompt without gradient updates. Quick check: If you have 3 translation examples and a new input, how does the LLM produce output without training?
- **Declarative vs. Procedural Programming Paradigms**: Logot differs from Program-of-Thought by generating what must be true (declarative ASP) rather than how to compute it (procedural Python). Quick check: Would specifying a Sudoku solving algorithm step-by-step be declarative or procedural?

## Architecture Onboarding

- **Component map**: Input Layer → Rule Translation Module → State Translation Module → ASP Solver (Clingo) → Decoder D(·) → Output
- **Critical path**: Correct translation (both modules) → well-formed ASP program → solver finds satisfying assignment → decoder formats output. Translation errors propagate; solver errors are rare.
- **Design tradeoffs**: LLM choice (GPT-4o best accuracy but higher cost; GPT-4o-mini/Deepseek-V3 offer tradeoffs), few-shot example count/quality (more examples increase context length and cost; poorly chosen examples reduce transfer), prompt design for translation (whether to include background B, how to format examples).
- **Failure signatures**: Translation failure (ASP program syntactically valid but semantically wrong), state encoding error (initial state incorrectly transcribed), solver timeout (rare, manifests as no output within time budget).
- **First 3 experiments**: 1) Reproduce baseline comparison on Hitori: run Standard Prompting, CoT, and Logot with GPT-4o-mini on 20 instances. 2) Ablate few-shot example relevance: remove puzzle-specific examples from Dr, keeping only generic ones. 3) Stress-test translation robustness: paraphrase puzzle rules and measure translation quality degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can information retrieval techniques effectively replace manually annotated few-shot examples in the Logot framework? The current implementation relies on static, human-curated few-shot prompts to guide the LLM in translating natural language into ASP. What evidence would resolve it: Experiments demonstrating that a dynamic retriever can select contextually relevant examples that achieve comparable or superior accuracy to the static method across diverse puzzle types.

### Open Question 2
Can integrating Program-of-Thought (PoT) methods eliminate the translation errors identified in the failure analysis? The authors note that accuracy could be improved by integrating advanced prompting techniques like PoT to assist with the translation of rules and states. What evidence would resolve it: Ablation studies showing that adding a PoT intermediate step significantly reduces or removes state encoding errors in grid and dynamic puzzles.

### Open Question 3
How effectively does Logot generalize to more complex planning domains beyond the modest set of grid and Blocks World puzzles evaluated? While near-perfect on tested tasks, the framework's dependency on explicit rule translation has not been stress-tested on domains with implicit rules, vague constraints, or significantly larger action spaces. What evidence would resolve it: Successful application of the Logot framework to larger, more complex planning benchmarks without structural modification.

## Limitations
- Translation robustness depends critically on few-shot example quality and coverage, potentially brittle to significant rule variations
- Scalability boundaries inherited from both LLM translation and ASP solver components, not fully characterized on larger problem instances
- Cost-efficiency tradeoffs well-demonstrated but optimal balance between model size and accuracy needs further exploration

## Confidence
- Translation Robustness: Medium confidence - performance depends on few-shot example quality and may not generalize to structurally different puzzles
- Scalability Boundaries: Medium confidence - solver performance on larger instances and more complex constraints not fully characterized
- Cost-Efficiency Tradeoffs: High confidence - experiments clearly demonstrate smaller models maintain high accuracy while reducing computational costs

## Next Checks
1. **Cross-Domain Transfer Test**: Apply Logot to puzzle types structurally different from training examples (e.g., logic grid puzzles with different constraint patterns) and measure translation accuracy degradation.
2. **Solver Complexity Stress Test**: Systematically increase puzzle size to identify thresholds where solver grounding or search becomes intractable, documenting time and memory scaling behavior.
3. **Error Analysis on Translation Failures**: Intentionally introduce controlled variations in puzzle rule phrasing and measure corresponding changes in translation accuracy to quantify brittleness and identify problematic linguistic features.