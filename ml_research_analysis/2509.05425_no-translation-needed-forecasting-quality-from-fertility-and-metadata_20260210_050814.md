---
ver: rpa2
title: 'No Translation Needed: Forecasting Quality from Fertility and Metadata'
arxiv_id: '2509.05425'
source_url: https://arxiv.org/abs/2509.05425
tags:
- translation
- fertility
- quality
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that translation quality can be predicted\
  \ without running any translation system, using only token fertility ratios, token\
  \ counts, and basic linguistic metadata. Gradient boosting models (XGBoost) achieve\
  \ strong performance with R\xB2=0.66 for XX\u2192English and R\xB2=0.72 for English\u2192\
  XX translations across 203 languages in the FLORES-200 benchmark."
---

# No Translation Needed: Forecasting Quality from Fertility and Metadata

## Quick Facts
- arXiv ID: 2509.05425
- Source URL: https://arxiv.org/abs/2509.05425
- Reference count: 21
- XGBoost achieves R²=0.66 for XX→English and R²=0.72 for English→XX translations across 203 languages

## Executive Summary
This paper demonstrates that translation quality can be predicted without running any translation system, using only token fertility ratios, token counts, and basic linguistic metadata. Gradient boosting models achieve strong performance predicting ChrF scores for GPT-4o translations across 203 languages in FLORES-200. Feature importance analyses reveal that typological factors dominate predictions into English, while fertility plays a larger role for translations into diverse target languages. This approach offers new insights for multilingual evaluation and quality estimation, showing that translation quality is shaped by both token-level fertility and broader linguistic typology.

## Method Summary
The approach extracts nine features from FLORES-200 data: Joshi class (1-6), region (9 categories), language family, script (29 types), ISO language code, reference and candidate fertility ratios, and token counts using the o200k_base tokenizer. These features are used to train regression models predicting ChrF scores from GPT-4o translations. XGBoost outperforms linear models and Random Forest, achieving R² of 0.66 for XX→English and 0.72 for English→XX translations. The methodology separates direction-specific models to capture different predictive patterns.

## Key Results
- XGBoost achieves R²=0.66 for XX→English and R²=0.72 for English→XX translations
- Feature importance differs by direction: Region and fertility dominate XX→English, while Joshi Class dominates English→XX
- Linear models perform substantially worse (R²≈0.25-0.31), indicating strong non-linear relationships
- Fertility ratios capture morphological complexity that constrains translation quality

## Why This Works (Mechanism)

### Mechanism 1
Token fertility ratios encode morphological complexity signals that predictably constrain translation quality. Fertility (tokens per word) captures how efficiently a tokenizer handles a language's morphology. High-fertility languages require more tokens to express equivalent content, increasing sequence length, attention complexity, and error propagation. When source and target fertility diverge significantly, the model must implicitly compress or expand information across mismatched token boundaries, creating systematic quality degradation.

### Mechanism 2
Typological metadata proxies for training data distribution and resource availability in multilingual models. Language family, script, and region correlate with how much training data existed for each language during GPT-4o's pretraining. High-resource families (Indo-European) and regions (Europe) have more web-scale training data, leading to better representations. The model has seen more examples of these languages, learned better character-level patterns, and calibrated its output distributions more accurately.

### Mechanism 3
Feature interactions are highly non-linear, requiring tree ensembles to capture conditional dependencies between typological and token-level factors. The prediction requires learning thresholds and interactions (e.g., "high fertility only hurts when script is non-Latin AND Joshi class is 5+"). Linear models cannot represent these conjunctions. XGBoost's sequential gradient boosting focuses on residual errors, learning which features matter for which subsets of languages, while Random Forest's bagging captures distributed patterns across diverse tree structures.

## Foundational Learning

- **ChrF (Character n-gram F-score)**: The target variable being predicted. Understanding that ChrF measures character-level overlap (not semantic equivalence) explains why surface-level features like fertility and script can predict it. *Quick check*: Why would ChrF favor languages with Latin script over those with complex morphology, even if semantic quality were identical?

- **Token Fertility**: The core predictive feature. You must understand that fertility = tokens/word, and that it varies by language due to morphological complexity and tokenizer design, not translation quality itself. *Quick check*: A language with fertility 4.0 vs. 1.5—what does this imply about its relationship to the tokenizer's vocabulary?

- **Feature Importance in Tree Ensembles**: The paper's conclusions rest on interpreting why Joshi Class dominates English→XX but Region dominates XX→English. You need to distinguish between "this feature predicts well" vs. "this feature causes the outcome." *Quick check*: If Region has high importance, does that mean geography causally affects translation, or that it correlates with the true causal factors?

## Architecture Onboarding

- **Component map**: FLORES-200 parallel corpus (203 languages) → Feature extraction pipeline (o200k_base tokenizer → token counts + fertility ratios; Language metadata lookup → Joshi class, family, script, region) → Training data assembly (9 features, ChrF labels) → Model comparison layer (Linear, Random Forest, XGBoost, MLP) → Interpretation layer (feature importance rankings, marginal quality plots)

- **Critical path**: Extract fertility ratios correctly (tokens per word using consistent tokenizer) → Map language codes to complete metadata (Joshi class, family, script, region) → Ensure ChrF scores are computed consistently across all language pairs → Train/test split must prevent language leakage

- **Design tradeoffs**: XGBoost vs. Random Forest: XGBoost gives higher R² but concentrates importance on fewer features (less interpretable distribution). Random Forest spreads importance more evenly—better for discovering multiple contributing factors. Feature granularity: Using ISO language codes as features (high cardinality) vs. family/region groupings. Codes capture language-specific effects but risk overfitting; groupings generalize better but lose detail.

- **Failure signatures**: Linear models matching tree performance → features are nearly independent with linear relationships (unexpected given results). Feature importance concentrated 90%+ on single feature → likely data leakage or label encoding issue. Large train/test R² gap (>0.1) → overfitting to training languages, not generalizing to held-out languages.

- **First 3 experiments**: 1) Ablation study: Remove fertility features entirely, retrain XGBoost. Compare R² drop to quantify how much unique variance fertility explains beyond typology. 2) Cross-direction transfer: Train on XX→English data, test on English→XX (and reverse). Low transfer would confirm that different mechanisms govern each direction. 3) Language family holdout: Train excluding all Niger-Congo languages, then evaluate on held-out Niger-Congo. This tests whether the model generalizes to unseen language families or merely memorizes family-level patterns.

## Open Questions the Paper Calls Out

1. **Do fertility and metadata features predict translation quality equally well for systems other than GPT-4o?** The study exclusively evaluates GPT-4o translations from FLORES-200, leaving untested whether the observed relationships generalize across diverse MT architectures.

2. **Would the predictive power of fertility and metadata hold for sentence-level quality estimation rather than aggregate language-level scores?** The paper predicts ChrF scores aggregated across the FLORES-200 benchmark but does not test whether the approach works for individual sentence predictions.

3. **What explains the 28-34% of variance in translation quality that remains unaccounted for by fertility and metadata?** The best models achieve R² of 0.66-0.72, meaning substantial variance is unexplained.

4. **Why does target-side fertility dominate predictions for English→XX while source-side typology dominates for XX→English?** Feature importance analyses reveal different predictors for each translation direction, but the underlying mechanism remains unexplained.

## Limitations

- Results are specific to GPT-4o translations and ChrF metric, limiting generalizability to other MT systems
- Fertility calculations depend on o200k_base tokenizer, making results tokenizer-specific
- Approximately 28-34% of quality variance remains unexplained by the current feature set
- Categorical metadata features may contain systematic biases or incomplete coverage for low-resource languages

## Confidence

**High Confidence (Mechanistic Understanding):**
- Token fertility ratios encode morphological complexity signals that constrain translation quality
- Typological metadata proxies for training data distribution and resource availability

**Medium Confidence (Model Behavior):**
- Feature interactions are highly non-linear, requiring tree ensembles
- Separate direction-specific models are justified

**Low Confidence (Generalizability):**
- Results generalize to other translation models and quality metrics
- Missing features don't substantially improve predictions

## Next Checks

1. **Ablation study on fertility features**: Remove all fertility ratio features from the XGBoost model and retrain. Compare the R² drop to the unexplained variance (0.28-0.34) to quantify how much unique signal fertility provides beyond typology.

2. **Cross-direction transfer experiment**: Train separate XGBoost models on XX→English data only, then evaluate on English→XX (and vice versa). Low transfer performance would confirm that different mechanisms govern each translation direction.

3. **Alternative quality metric validation**: Repeat the entire analysis pipeline using COMET-QE scores instead of ChrF for the same GPT-4o translations. Compare feature importance rankings and model performance to determine whether the observed patterns are specific to ChrF's surface-level focus or reflect deeper quality determinants.