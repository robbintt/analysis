---
ver: rpa2
title: 'Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents'
arxiv_id: '2506.14852'
source_url: https://arxiv.org/abs/2506.14852
tags:
- caching
- plan
- cache
- arxiv
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high costs and latency of large language
  model (LLM) agents by introducing Agentic Plan Caching (APC), a novel approach that
  extracts, stores, adapts, and reuses structured plan templates from completed agent
  executions. Unlike traditional caching methods designed for chatbots, APC focuses
  on task-level caching by identifying keywords that capture the higher-level intent
  of queries, matching them against a cache of plan templates, and using lightweight
  models to adapt these templates for new tasks.
---

# Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents

## Quick Facts
- arXiv ID: 2506.14852
- Source URL: https://arxiv.org/abs/2506.14852
- Reference count: 40
- Key outcome: Reduces costs by 50.31% and latency by 27.28% while maintaining 96.61% of optimal accuracy across diverse agent workloads

## Executive Summary
Agentic Plan Caching (APC) introduces a novel approach to reduce the high costs and latency of LLM agents by caching and reusing structured plan templates from completed executions. Unlike traditional caching methods designed for chatbots, APC extracts task-level templates using keyword-based intent capture and adapts them for new queries using lightweight models. This approach enables significant cost savings and performance improvements while maintaining high accuracy across diverse agent workloads, including data-intensive reasoning and multi-step tasks.

## Method Summary
APC operates by extracting structured plan templates from completed agent executions, identifying keywords that capture the higher-level intent of queries, and storing these templates in a cache. When new queries arrive, the system matches them against cached templates using the extracted keywords and adapts the matched templates for the current task using lightweight models. This process allows APC to bypass expensive LLM calls for repetitive task patterns while maintaining task completion quality through intelligent template adaptation.

## Key Results
- Reduces operational costs by 50.31% on average across tested workloads
- Decreases latency by 27.28% while maintaining high task completion accuracy
- Maintains 96.61% of optimal accuracy compared to uncached approaches

## Why This Works (Mechanism)
APC works by recognizing that LLM agents often perform similar task patterns across different queries. By extracting the structural essence of completed plans rather than just their outputs, APC captures reusable templates that can be adapted for semantically similar but not identical tasks. The keyword-based intent capture allows the system to match new queries to appropriate templates even when surface-level details differ, while the lightweight adaptation models ensure the cached plans remain relevant to the specific context of new requests.

## Foundational Learning
- Plan template extraction: The process of distilling completed agent executions into reusable structural blueprints; needed to identify common patterns in agent behavior that can be reused
- Keyword-based intent capture: Using specific keywords to represent the higher-level intent of queries rather than exact text matching; needed to enable semantic matching across varied query phrasings
- Lightweight adaptation models: Small models that modify cached templates for new contexts without full LLM recomputation; needed to maintain accuracy while avoiding expensive re-execution
- Template matching algorithms: Methods for efficiently finding the best cached template for a new query; needed to ensure fast retrieval and appropriate template selection
- Cache staleness management: Strategies for handling outdated templates in evolving domains; needed to maintain relevance as task patterns change over time

## Architecture Onboarding
Component map: Query Parser -> Keyword Extractor -> Template Matcher -> Adaptation Model -> Execution Engine
Critical path: New query → keyword extraction → template matching → adaptation → execution
Design tradeoffs: Speed vs accuracy (adaptation model complexity), storage vs freshness (cache size and staleness), keyword specificity vs matching coverage
Failure signatures: Poor keyword extraction leading to template mismatches, adaptation model errors causing incorrect plan modifications, cache misses increasing latency
First experiments:
1. Test keyword extraction accuracy on a diverse query set to validate intent capture quality
2. Measure template matching precision and recall across different workload types
3. Evaluate adaptation model performance on edge cases with semantically similar but structurally different tasks

## Open Questions the Paper Calls Out
The paper acknowledges but does not fully address cache staleness concerns in rapidly evolving domains, nor does it explore how keyword extraction performance scales with more complex or ambiguous queries. The adaptation model's robustness to semantically similar but structurally different tasks remains underexplored, potentially limiting effectiveness in nuanced use cases.

## Limitations
- Experimental scope limited to 4 agent workloads and 6 datasets, raising generalizability concerns
- Cache staleness impact in domains with evolving terminology and task structures not fully explored
- Adaptation model performance in nuanced semantic variations not thoroughly validated

## Confidence
High confidence in the core technical contribution of keyword-based plan template extraction and adaptation as a viable approach for LLM agent caching. Medium confidence in the reported performance metrics due to limited experimental scope and lack of extensive real-world validation. Low confidence in the scalability claims for production environments with high query volume variability and domain drift.

## Next Checks
1. Evaluate APC performance across a broader range of real-world agent workloads, including open-ended and highly variable query patterns, to assess robustness beyond the current 4 workload types.
2. Conduct long-term deployment experiments to measure cache staleness impact and adaptation accuracy decay over time in domains with evolving terminology and task structures.
3. Benchmark APC's keyword extraction and plan template matching accuracy against human-annotated ground truth in complex multi-step reasoning tasks to validate semantic understanding quality.