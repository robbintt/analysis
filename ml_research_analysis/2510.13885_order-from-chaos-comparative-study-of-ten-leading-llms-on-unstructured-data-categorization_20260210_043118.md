---
ver: rpa2
title: 'Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data
  Categorization'
arxiv_id: '2510.13885'
source_url: https://arxiv.org/abs/2510.13885
tags:
- categorization
- text
- llms
- taxonomy
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates ten leading large language models (LLMs) on
  unstructured text categorization using the IAB 2.2 taxonomy, a hierarchical industry
  standard with 690 categories. Each model was tested on 8,660 human-annotated documents
  using a uniform zero-shot prompting strategy, and performance was measured with
  both classic metrics (accuracy, precision, recall, F1-score) and LLM-specific indicators
  (hallucination ratio, inflation ratio, categorization cost).
---

# Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization

## Quick Facts
- arXiv ID: 2510.13885
- Source URL: https://arxiv.org/abs/2510.13885
- Authors: Ariel Kamen
- Reference count: 9
- Primary result: Despite advances in LLM capabilities, classic performance remains modest (average F1 ≈ 41%) on large-scale multi-label text categorization; ensemble methods substantially improve accuracy and eliminate hallucinations

## Executive Summary
This study evaluates ten leading LLMs on unstructured text categorization using the IAB 2.2 taxonomy (690 categories) with 8,660 human-annotated documents. The results reveal that even state-of-the-art models struggle with this task, achieving only modest performance (F1 ≈ 41%) and consistently over-generating categories compared to human annotations. All models demonstrated hallucination ratios between 0.7% and 5.9%, with inflation ratios averaging 209%. The study introduces an ensemble-based approach where multiple LLMs act as independent experts, which completely eliminated hallucinations and improved accuracy, suggesting that coordinated orchestration rather than scale alone may be the most effective path forward for large-scale text categorization.

## Method Summary
The study employed a uniform zero-shot prompting strategy across ten LLMs (Claude 3.5, Gemini 1.5/2.0 Flash, LLaMA 3 8B/3.3 70B, Mistral, Grok, DeepSeek, GPT-20B/120B) using 8,660 human-annotated documents from open news corpora. Evaluation metrics included classic measures (accuracy, precision, recall, F1) plus LLM-specific indicators: hallucination ratio (categories outside taxonomy), inflation ratio (predicted vs. expert count), and token cost. The IAB 2.2 hierarchical taxonomy with 690 categories and 4 tiers served as the categorization framework. The authors developed an ensemble-based approach where multiple LLMs act as independent experts, substantially improving accuracy and eliminating hallucinations.

## Key Results
- Classic performance remains modest: average accuracy of 34%, precision of 42%, recall of 45%, and F1-score of 41%
- All models consistently overproduce categories compared to human annotations, with hallucination ratios ranging from 0.7% to 5.9% and inflation ratios averaging 209%
- Ensemble approach eliminated hallucinations entirely and improved accuracy, suggesting coordinated orchestration of models is more effective than scale alone

## Why This Works (Mechanism)
The study demonstrates that LLMs face fundamental architectural constraints when compressing unstructured text into sparse hierarchical taxonomies. The task requires simultaneously maintaining semantic understanding while adhering to rigid category boundaries and hierarchical relationships. Models tend to over-generate categories due to their inherent design for completion and generation rather than sparse classification. The ensemble approach works by leveraging multiple independent expert models, where each model's unique biases and strengths can compensate for others' weaknesses, creating a more robust and accurate categorization system through coordinated orchestration.

## Foundational Learning

**IAB 2.2 Taxonomy** - A hierarchical industry standard with 690 categories across 4 tiers used for digital advertising classification. Why needed: Provides standardized, comprehensive framework for evaluating multi-label text categorization. Quick check: Verify taxonomy has exactly 690 categories and 4 hierarchical levels.

**Zero-shot prompting** - Technique where models categorize without task-specific training, using only provided examples or instructions. Why needed: Enables direct comparison across models without training bias. Quick check: Confirm all models receive identical prompt templates.

**Hallucination ratio** - Metric measuring percentage of predicted categories outside the valid taxonomy. Why needed: Critical for evaluating model reliability in constrained classification tasks. Quick check: Calculate ratio by dividing out-of-taxonomy predictions by total predictions.

**Parent Exclusion Rule** - Hierarchical deduplication method preventing parent categories from being counted when child categories are present. Why needed: Ensures accurate evaluation in hierarchical taxonomies where children imply parents. Quick check: Verify parent categories are excluded when their children appear in predictions.

## Architecture Onboarding

**Component Map:** Document Corpus -> IAB 2.2 Taxonomy -> LLM Inference Engine -> Evaluation Metrics -> Results Analysis

**Critical Path:** Data preprocessing → Uniform zero-shot prompting → Category prediction → Hierarchical deduplication → Metric calculation → Performance analysis

**Design Tradeoffs:** Zero-shot approach enables fair model comparison but limits performance optimization; ensemble approach improves accuracy but increases computational cost and complexity.

**Failure Signatures:** High hallucination ratio indicates models outputting free-text instead of exact labels; inflated category counts suggest prompt constraints being ignored; low F1 scores reveal fundamental mismatch between model capabilities and task requirements.

**First Experiments:** 1) Run single model inference to establish baseline performance and identify failure modes; 2) Implement ensemble approach with 3-4 diverse models to test performance improvements; 3) Vary prompt templates systematically to identify optimal configuration for hierarchical categorization.

## Open Questions the Paper Calls Out
None

## Limitations
- Single dataset evaluation limits generalizability to other domains or text types
- Fixed zero-shot prompting may not represent optimal configuration for any specific model
- Hallucination metrics depend on exact label matching, potentially missing semantic equivalences between categories

## Confidence

**Performance metrics and model rankings:** High - clear experimental protocol, publicly available dataset, and reproducible evaluation metrics

**Claims about architectural limitations:** Medium - supported by results but speculative regarding underlying model mechanisms

**Ensemble effectiveness:** Medium - strong results on single dataset, but generalization and scalability not demonstrated

## Next Checks
1. Replicate experiments on at least one additional multi-label text categorization dataset to test generalizability across different domains
2. Systematically vary prompt templates and hyperparameters to establish upper bounds of zero-shot performance
3. Conduct ablation studies on ensemble composition to determine optimal model combinations and reduce computational costs