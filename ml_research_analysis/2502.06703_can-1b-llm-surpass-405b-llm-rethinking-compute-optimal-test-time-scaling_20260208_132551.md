---
ver: rpa2
title: Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling
arxiv_id: '2502.06703'
source_url: https://arxiv.org/abs/2502.06703
tags:
- compute-optimal
- qwen2
- arxiv
- policy
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how policy models, process reward models
  (PRMs), and problem difficulty influence test-time scaling (TTS) for large language
  models. The authors conduct comprehensive experiments on MATH-500 and AIME24, showing
  that the compute-optimal TTS strategy depends heavily on the choice of policy model,
  PRM, and problem difficulty.
---

# Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling

## Quick Facts
- arXiv ID: 2502.06703
- Source URL: https://arxiv.org/abs/2502.06703
- Reference count: 40
- Demonstrates that compute-optimal test-time scaling enables a 1B model to outperform a 405B model on MATH-500

## Executive Summary
This paper investigates how policy models, process reward models (PRMs), and problem difficulty influence test-time scaling (TTS) for large language models. The authors show that the compute-optimal TTS strategy depends heavily on the choice of policy model, PRM, and problem difficulty. With a reward-aware compute-optimal TTS approach, they demonstrate that extremely small models (e.g., 1B parameters) can outperform much larger ones (e.g., 405B parameters) on mathematical reasoning tasks. For example, a 3B model surpasses a 405B model, and a 7B model beats both o1 and DeepSeek-R1 on these tasks while achieving higher inference efficiency.

## Method Summary
The paper evaluates three TTS methods—Best-of-N, Beam Search, and DVTS—across different policy model sizes (0.5B-72B) and PRMs on MATH-500 and AIME24 benchmarks. The compute-optimal strategy is formulated as maximizing expected reward: θ*(x, y*, ℛ)(N) = argmax E[y∼Target(θ,N,x,ℛ)][1(y=y*)]. The authors test with compute budgets of {4, 16, 64, 256} generations and evaluate performance using Pass@1 accuracy and FLOPS analysis. The study reveals that optimal TTS method varies systematically with model size and problem difficulty, with small models benefiting from search-based methods while larger models favor parallel exploration.

## Key Results
- A 1B Llama-3.2 model exceeds 405B Llama-3.1 on MATH-500 using compute-optimal TTS
- A 7B model beats both OpenAI's o1 and DeepSeek-R1 on the same tasks
- Compute-optimal TTS achieves 100-1000× FLOPS reduction compared to baseline 405B inference
- Optimal TTS method varies by difficulty: BoN for easy problems, beam search for hard problems

## Why This Works (Mechanism)

### Mechanism 1: Reward-Aware Compute-Optimal Scaling
The optimal TTS strategy must account for the specific PRM being used, as different PRMs exhibit distinct biases and generalization behaviors. The paper reformulates compute-optimal TTS to include the reward function ℛ as an explicit parameter. PRMs trained on different policy models' outputs (off-policy vs. on-policy) exhibit out-of-distribution issues that affect search trajectory quality. Some PRMs bias toward shorter responses while others allow longer exploration, directly affecting both answer correctness and token expenditure.

### Mechanism 2: Adaptive Method Selection by Model Size and Problem Difficulty
The optimal TTS method varies systematically with policy model size and problem difficulty. Small models (<7B parameters) benefit from step-by-step verification via search-based methods, while larger models (≥72B) benefit more from parallel exploration. For problem difficulty: easy problems favor BoN; hard problems favor beam search. The paper uses absolute Pass@1 accuracy thresholds to categorize difficulty.

### Mechanism 3: Test-Time Compute as Performance Amplifier
Additional inference compute, when optimally allocated, can compensate for smaller model capacity—enabling 1B models to match or exceed 405B model performance on mathematical reasoning. TTS enables small models to sample or search across many solution trajectories, with PRM-based selection recovering the correct answer from a larger solution space.

## Foundational Learning

- **Process Reward Models (PRMs)**: Step-level verification models that score intermediate reasoning steps. Needed to understand how PRM quality and alignment with policy model outputs affects TTS effectiveness. Quick check: Given a PRM trained on Mistral-7B outputs, what failure modes should you expect when using it with a Qwen-2.5 policy model?

- **Markov Decision Process (MDP) Formulation for Reasoning**: Frames reasoning as states (partial solutions), actions (next reasoning steps), transitions (appending steps), and rewards (PRM scores). Needed to understand why different search strategies explore the solution space differently. Quick check: In the MDP formulation, what does the transition function P: S × A → S represent for a mathematical reasoning task?

- **Best-of-N vs. Beam Search vs. DVTS**: Three core TTS methods with distinct compute-accuracy tradeoffs. Best-of-N generates N independent responses then selects; beam search performs step-by-step selection with branching; DVTS partitions into independent subtrees for diversity. Quick check: For a 0.5B model on hard problems, which method would you expect to perform best and why?

## Architecture Onboarding

- **Component map**: Policy Model -> TTS Strategy Selector -> Process Reward Model -> Voting Methods -> Final Answer
- **Critical path**: 1. Classify problem difficulty, 2. Select TTS method based on (model size, PRM, difficulty), 3. Generate candidate solutions with budget N, 4. Score candidates with PRM, 5. Aggregate via voting method to select final answer
- **Design tradeoffs**: On-policy PRM (trained on same policy model's outputs) → more accurate rewards but requires per-model training; Off-policy PRM → cheaper but risks OOD errors; Larger compute budget → better pass rates but diminishing returns for strong models
- **Failure signatures**: Over-criticism (PRM assigns low scores to correct steps), Error neglect (PRM assigns high scores to incorrect steps), Length bias (PRM favors short/long responses), OOD mismatch (PRM trained on different policy model)
- **First 3 experiments**:
  1. **PRM-Policy Compatibility Test**: Run Llama-3.1-8B-Instruct with all seven PRMs on MATH-500 subset (50 problems) using beam search (N=64)
  2. **Method-by-Difficulty Grid**: For Qwen2.5-7B-Instruct, test all three TTS methods at all three difficulty levels with budgets N∈{16, 64, 256}
  3. **Scaling Frontier Identification**: Fix a PRM (Qwen2.5-Math-PRM-7B), fix difficulty (medium), vary policy model size (0.5B→72B) and compute budget (4→256)

## Open Questions the Paper Calls Out

- **Domain Generalization**: Can compute-optimal TTS strategies generalize effectively to domains beyond mathematical reasoning, such as coding, chemistry, and open-ended reasoning tasks? The paper only tested on MATH-500 and AIME24.

- **PRM Improvement**: How can PRMs be improved to address identified failure modes—over-criticism, error neglect, error localization bias, and scoring bias—that distort the search process? The paper documents these problems but doesn't propose solutions.

- **Weak-to-Strong Supervision**: Can a "weak-to-strong" supervision paradigm enable smaller PRMs to effectively guide larger policy models, overcoming the current "strong-to-weak" limitation? Current PRM training assumes the verifier is at least as capable as the policy.

## Limitations
- The compute-optimal TTS strategy is highly sensitive to PRM-policy model alignment, but the paper doesn't provide a robust method for PRM selection when multiple PRMs are available
- The absolute Pass@1 thresholds for difficulty classification (easy: 50-100%, medium: 10-50%, hard: 0-10%) may not generalize to other problem domains beyond math
- While the paper demonstrates dramatic size reduction (135× model size, 100-1000× FLOPS), the practical implementation cost of running multiple TTS generations with PRM scoring is not fully characterized

## Confidence
- **High Confidence**: The empirical demonstration that compute-optimal TTS enables small models to outperform much larger ones on MATH-500 and AIME24
- **Medium Confidence**: The mechanism explaining why PRM quality and policy model size determine optimal TTS method selection
- **Low Confidence**: The generalizability of the absolute difficulty thresholds and the practical latency implications of TTS deployment

## Next Checks
1. **PRM Generalization Test**: Run Llama-3.1-8B-Instruct with all seven PRMs on a MATH-500 subset (50 problems) using beam search (N=64) to identify which PRMs generalize well to your policy model
2. **Method-by-Difficulty Grid**: For Qwen2.5-7B-Instruct, test all three TTS methods at all three difficulty levels (using Pass@1 proxy from small sample) with budgets N∈{16, 64, 256} to replicate the finding that optimal method varies by difficulty
3. **Scaling Frontier Identification**: Fix a PRM (Qwen2.5-Math-PRM-7B), fix difficulty (medium), vary policy model size (0.5B→72B) and compute budget (4→256) to identify where compute scaling yields diminishing returns for your task distribution