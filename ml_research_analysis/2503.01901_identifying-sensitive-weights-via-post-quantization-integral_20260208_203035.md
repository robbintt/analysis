---
ver: rpa2
title: Identifying Sensitive Weights via Post-quantization Integral
arxiv_id: '2503.01901'
source_url: https://arxiv.org/abs/2503.01901
tags:
- quantization
- weights
- https
- zhang
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate sensitivity metrics
  in post-training quantization of large language models. The authors identify that
  existing gradient and Hessian-based metrics are inaccurate due to the small convergence
  radius of local second-order approximations, leading to underestimation of quantization's
  impact on loss by orders of magnitude.
---

# Identifying Sensitive Weights via Post-quantization Integral

## Quick Facts
- **arXiv ID:** 2503.01901
- **Source URL:** https://arxiv.org/abs/2503.01901
- **Reference count:** 38
- **One-line primary result:** ReQuant improves perplexity by up to 2.66 and enhances MATH few-shot performance by nearly 3% compared to state-of-the-art baselines on Llama 3.2 models.

## Executive Summary
This paper addresses the challenge of accurate sensitivity metrics in post-training quantization of large language models. The authors identify that existing gradient and Hessian-based metrics are inaccurate due to the small convergence radius of local second-order approximations, leading to underestimation of quantization's impact on loss by orders of magnitude. They propose Post-quantization Integral (PQI), a novel sensitivity metric that accurately estimates element-wise importance by decomposing the path from original to quantized weights into small fragments and considering both weight states. Leveraging PQI, they introduce ReQuant, a simple yet powerful framework using dense-and-sparse detach components for self-adaptive outlier selection and step-wise significant weight detachment. When applied to Llama 3.2 models, ReQuant improves perplexity by up to 2.66 and enhances MATH few-shot performance by nearly 3% compared to state-of-the-art baselines.

## Method Summary
The paper proposes ReQuant, a post-training quantization framework that uses a novel sensitivity metric called Post-quantization Integral (PQI). The method first pre-quantizes the model using existing techniques (AWQ, SqueezeLLM, or QTIP) to obtain draft quantized weights. It then computes PQI by numerically integrating gradients along the path from original to quantized weights, which accurately captures element-wise sensitivity. Using this metric, ReQuant performs two-stage refinement: first adaptively selecting outlier weights per layer, then greedily detaching significant weights that contribute most to accuracy. The final model consists of quantized dense weights plus sparse high-precision corrections.

## Key Results
- PQI accurately predicts quantization impact, while gradient/Hessian metrics underestimate loss changes by orders of magnitude
- ReQuant improves Llama 3.2 1B perplexity from 13.86 to 13.30 (SqueezeLLM baseline) and MATH score from 11.28 to 14.18
- Outperforms state-of-the-art baselines by up to 2.66 perplexity points and 3% MATH accuracy
- Works across multiple quantization approaches including AWQ, SqueezeLLM, and QTIP

## Why This Works (Mechanism)

### Mechanism 1: Failure of Local Taylor Approximations
- Claim: Existing gradient and Hessian-based sensitivity metrics systematically underestimate quantization impact on loss by orders of magnitude.
- Mechanism: The Taylor series expansion (first- and second-order terms) used to predict loss change ΔF assumes the quantized weights ̃w lie within a small convergence radius around original weights w. In LLMs with billions of parameters, the cumulative quantization distance ||̃w - w|| is large, causing the Taylor approximation to become highly inaccurate.
- Core assumption: The relationship between weight perturbation and loss change is approximately quadratic only in a small neighborhood around w.
- Evidence anchors:
  - [abstract] "...gradient and Hessian-based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation"
  - [Section 4.1, Table 1] Shows that first- and second-order Taylor terms sum to values much smaller (or even negative) compared to actual ΔF across all layers
  - [Section 4.2, Table 2] Demonstrates that as interpolation parameter λ decreases (reducing distance), Taylor approximation becomes more accurate
- Break condition: If quantization step sizes were sufficiently small such that ||̃w - w|| remained within the Taylor convergence radius for all parameters, local approximations might remain accurate.

### Mechanism 2: Post-quantization Integral (PQI) via Path Decomposition
- Claim: Integrating gradients along the path from original to quantized weights provides an accurate, fine-grained sensitivity metric.
- Mechanism: PQI decomposes the path from w to ̃w into N small segments where local linear approximations hold. By numerically integrating gradients at intermediate points (Equation 7), PQI captures the cumulative sensitivity without assuming a small global perturbation. The resulting metric vPQI can predict element-wise contribution to ΔF.
- Core assumption: The gradient ∇F is continuous along the path from w to ̃w, allowing numerical integration with sufficient segments.
- Evidence anchors:
  - [Section 5, Eq. 4-7] Formal definition of PQI as path integral of gradients
  - [Table 3] Shows that with N≥32 intervals, predicted ΔFPQI matches actual ΔF (0.1024) with error ~0.1%
  - [Table 4] Element-wise average ΔFPQI values across layers and sublayers show consistent, meaningful variations
- Break condition: If the loss landscape contained discontinuous regions along the path, the integral approximation would fail. The paper assumes local continuity is achievable.

### Mechanism 3: Dense-and-Sparse Decomposition via PQI-Guided Selection
- Claim: Using PQI to guide outlier selection and significant weight detachment in a two-stage quantization pipeline improves perplexity and task performance.
- Mechanism: ReQuant first pre-quantizes the model to obtain a draft ̃w, then uses PQI to compute layer-wise sensitivity. Outlier ratio per layer is adaptively set based on each layer's ΔFPQI (Algorithm 1), and significant weights are greedily detached based on vPQI ⊙ |̃w - w| (Algorithm 2). This creates a final model: quantized dense weights + sparse high-precision corrections.
- Core assumption: (1) Layers with higher ΔFPQI benefit more from additional preserved precision; (2) Iterative detachment identifies weights whose preservation yields greatest accuracy recovery.
- Evidence anchors:
  - [Section 6.1, Algorithm 1] Temperature-based grid search for layer-adaptive outlier ratios
  - [Section 6.2, Algorithm 2] Greedy search for significant weight detachment
  - [Table 7] Llama 3.2 1B: SqueezeLLM+ReQuant achieves 13.30 PPL (vs 13.86 baseline) and 14.18 MATH score (vs 11.28)
  - [corpus] Related work (FGMP, Squeeze10-LLM) explores mixed-precision but does not specifically validate PQI; corpus evidence for PQI mechanism is weak or indirect
- Break condition: If the overhead of sparse matrix storage and computation negates accuracy benefits, or if PQI identifies spurious sensitive weights due to calibration set mismatch, performance gains would diminish.

## Foundational Learning

- Concept: Taylor Series Approximation for Loss Prediction
  - Why needed here: Understanding why gradient/Hessian metrics fail requires knowing the assumptions (small perturbation radius) behind Taylor expansion.
  - Quick check question: Why does a large ||̃w - w|| break the validity of the second-order Taylor approximation for ΔF?

- Concept: Numerical Integration along a Parameter Path
  - Why needed here: PQI's core operation is integrating gradients along a path; knowing rectangle approximation and error bounds is essential.
  - Quick check question: In Equation 7, how does increasing N affect the trade-off between approximation accuracy and computational cost?

- Concept: Sensitivity Metrics in Quantization (gradient, activation, Hessian)
  - Why needed here: The paper builds on and critiques these prior metrics; understanding their definitions and limitations contextualizes PQI's contribution.
  - Quick check question: What is the key difference between activation-based sensitivity (AWQ) and Hessian-based sensitivity (SqueezeLLM), and why does PQI outperform both according to the paper?

## Architecture Onboarding

- Component map:
  1. Pre-quantization module: Runs a standard PTQ method (AWQ/SqueezeLLM/QTIP) to obtain draft ̃w
  2. PQI computation: Numerical integration (Eq. 7) using calibration data to compute vPQI and layer-wise ΔFPQI
  3. Outlier ratio search: Algorithm 1 grid searches temperature t to allocate sparsity budget across layers
  4. Significant weight search: Algorithm 2 greedily detaches rs% of weights based on vPQI ⊙ |̃w - w|
  5. Final assembly: Dense quantized weights (from step 1, adjusted) + sparse outliers wo + sparse significant weights ws

- Critical path:
  Calibration data → Pre-quantize → PQI computation → Outlier ratio search → Re-quantize with wo → Significant weight search → Final model (Eq. 8)

- Design tradeoffs:
  - Sparsity budget (ro%, rs%): Higher sparsity improves accuracy but increases storage (indices) and may slow inference due to sparse operations
  - Number of integration intervals N: Higher N improves accuracy but increases GPU hours (Table 11)
  - Search steps (α, β): Finer grid search for outlier ratio and more iterative detachment improve results but add computation

- Failure signatures:
  - Calibration set mismatch: If calibration data does not represent deployment distribution, PQI sensitivities may be misaligned
  - Over-detachment: Setting rs% too high can cause memory bloat and inference slowdown without proportional accuracy gains
  - Sparse multiplication bottleneck: cuSPARSE operations can be slower than dense quantized kernels, partially offsetting speed benefits (Table 9)

- First 3 experiments:
  1. Reproduce Table 1: Quantize a single layer of Llama 3.2 1B with SqueezeLLM, compute first-order, second-order terms and actual ΔF to validate Taylor approximation failure
  2. Implement PQI with N=32 on a small model (e.g., OPT-125M), compare predicted ΔFPQI against actual perplexity change for a held-out set
  3. Integrate ReQuant with AWQ on Llama 3.2 1B using Pile calibration, ablate wo-only vs wo+ws vs random selection, reporting WikiText-2 PPL and MATH scores (mirroring Table 8)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PQI be integrated directly into the quantization process to iteratively tune weight distributions, eliminating the need for storing sparse weights?
- Basis in paper: [explicit] Section 8 states, "Combining the PQI analysis into the quantization process is left as future work," suggesting one could iteratively compute PQI to avoid sparse weights.
- Why unresolved: The current ReQuant pipeline is a post-processing step that necessitates Dense-and-Sparse decomposition, which introduces storage overhead.
- What evidence would resolve it: A modified quantization algorithm that uses PQI to guide weight rounding dynamically, achieving comparable perplexity without storing sparse matrices.

### Open Question 2
- Question: How can the storage overhead and inference latency caused by sparse matrix indices in the Dense-and-Sparse decomposition be mitigated?
- Basis in paper: [explicit] Appendix A identifies that sparse indices require significant memory (at least 16 bits each) and sparse matrix multiplication is a bottleneck.
- Why unresolved: The method relies on this decomposition to detach outliers and significant weights, trading accuracy for memory/compute overhead.
- What evidence would resolve it: Optimization techniques that compress sparse indices or kernel optimizations that reduce the latency gap between dense and sparse matrix multiplication in this context.

### Open Question 3
- Question: Can an accurate sensitivity metric be derived without requiring a pre-computed "draft" quantized model (w̃) as a starting point?
- Basis in paper: [inferred] Section 5 notes PQI cannot be used to predict ahead of quantization because it requires w̃ to construct the integration path.
- Why unresolved: This constraint forces a multi-stage pipeline (draft then refine) rather than a single-pass quantization solution.
- What evidence would resolve it: A predictive approximation of the integration path or a closed-form estimation of sensitivity that does not depend on the final quantized state.

## Limitations

- The paper's empirical validation is limited to Llama 3.2 models, with unclear generalization to other LLM architectures
- PQI requires N=32 gradient evaluations per weight, creating computational overhead that may limit scalability to trillion-parameter models
- The sparse matrix operations for storing outliers and significant weights could introduce inference slowdowns that partially offset accuracy gains

## Confidence

- **High Confidence:** The mathematical framework of PQI (Eq. 4-7) is internally consistent, and the failure of local Taylor approximations is well-demonstrated in Table 1. The algorithmic steps for ReQuant (Algorithm 1-2) are clearly specified.
- **Medium Confidence:** The empirical performance improvements (WikiText-2 PPL, MATH scores) are convincing for Llama 3.2 1B/3B, but the paper lacks ablation studies on the critical hyperparameters (temperature step α, significant weight step β, N) that would validate robustness.
- **Low Confidence:** The generalization of PQI's superiority to non-LLaMA architectures and the practical inference overhead of sparse operations are not thoroughly evaluated.

## Next Checks

1. **Ablation Study on Integration Intervals:** Systematically vary N in PQI (e.g., N=4, 8, 16, 32, 64) and measure the trade-off between ΔFPQI accuracy and computational cost (GPU hours) on Llama 3.2 1B. This would validate the claimed optimal N=32.

2. **Cross-Architecture Validation:** Apply ReQuant to a non-LLaMA model (e.g., OPT-125M or Mistral-7B) and report WikiText-2 PPL and few-shot accuracy. This would test the generalization of PQI's sensitivity estimation.

3. **Inference Overhead Measurement:** Benchmark the wall-clock latency and memory usage of the ReQuant model (with sparse outliers and significant weights) versus a dense quantized baseline on a representative inference task (e.g., batch size 32, sequence length 2048). This would quantify the practical cost of the sparse representation.