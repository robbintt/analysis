---
ver: rpa2
title: 'Finnish SQuAD: A Simple Approach to Machine Translation of Span Annotations'
arxiv_id: '2501.05963'
source_url: https://arxiv.org/abs/2501.05963
tags:
- dataset
- finnish
- translation
- squad
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a simple method for machine translating extractive
  QA datasets with span annotations using the DeepL MT service's ability to preserve
  formatted text. The method was used to create a Finnish version of SQuAD2.0, which
  preserved 97.2% of original question-answer pairs compared to 66-93% in other MT
  approaches.
---

# Finnish SQuAD: A Simple Approach to Machine Translation of Span Annotations

## Quick Facts
- arXiv ID: 2501.05963
- Source URL: https://arxiv.org/abs/2501.05963
- Authors: Emil Nuutinen; Iiro Rastas; Filip Ginter
- Reference count: 6
- Primary result: 97.2% preservation of QA pairs in Finnish SQuAD translation

## Executive Summary
This paper presents a straightforward method for machine translating extractive QA datasets with span annotations using DeepL's ability to preserve formatted text. The approach was applied to create a Finnish version of SQuAD2.0, achieving 97.2% preservation of original question-answer pairs compared to 66-93% in previous MT approaches. The resulting dataset enabled Finnish QA models to achieve 73.7 F1 score, outperforming prior Finnish SQuAD translations by 11.8 F1 points. Manual evaluation showed 87.2% perfect answer transfers with only 2.2% completely lost.

## Method Summary
The approach leverages DeepL's capability to preserve formatted text when translating span-annotated QA datasets. The method involves translating both questions and context while maintaining the original span annotations, allowing the preservation of answer locations across languages. This simple technique avoids the complexity of re-annotating answers after translation, which typically results in significant data loss. The method was specifically applied to translate SQuAD2.0 from English to Finnish, creating the Finnish SQuAD dataset.

## Key Results
- 97.2% preservation of original question-answer pairs compared to 66-93% in other MT approaches
- Finnish QA models trained on translated dataset achieved 73.7 F1 score
- Outperformed previous Finnish SQuAD translations by 11.8 F1 points
- Backtranslation showed approximately 5pp EM and 2.5pp F1 degradation compared to original data
- Manual evaluation of 321 questions found 87.2% perfect answer transfers with only 2.2% completely lost

## Why This Works (Mechanism)
The method works by exploiting DeepL's ability to preserve formatted text during translation, maintaining the relationship between questions, contexts, and answer spans across languages. This preserves the extractive QA format without requiring complex re-annotation processes that typically degrade data quality. The simplicity of the approach reduces implementation overhead while achieving superior preservation rates compared to more complex methods.

## Foundational Learning
- **Extractive QA format**: Understanding the structure of question-answer pairs with span annotations is essential for developing translation methods that preserve answer locations.
- **Machine translation evaluation metrics**: EM (Exact Match) and F1 scores are standard metrics for evaluating QA model performance and translation quality.
- **Backtranslation methodology**: Comparing translated data back to original helps quantify translation-induced degradation and validate preservation quality.
- **Cross-lingual transfer learning**: The approach enables leveraging high-quality English QA datasets for low-resource languages through translation.

## Architecture Onboarding

Component Map:
English SQuAD dataset -> DeepL translation with formatted text preservation -> Finnish SQuAD dataset -> QA model training -> Evaluation

Critical Path:
The core pipeline flows from the original English dataset through DeepL translation while maintaining span annotations, then directly to model training without intermediate re-annotation steps.

Design Tradeoffs:
The approach trades off some translation accuracy for simplicity and preservation of span annotations. While backtranslation shows degradation, the method avoids the complexity and data loss of re-annotating answers post-translation.

Failure Signatures:
Potential failures include translation errors that break the relationship between questions and answer spans, domain-specific terminology that doesn't translate well, and loss of contextual meaning that affects answer extraction.

First Experiments:
1. Apply the method to translate a small subset of SQuAD and manually verify span preservation rates
2. Train a basic QA model on the translated subset to establish baseline performance
3. Compare preservation rates and model performance against a re-annotation approach on the same subset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on automated metrics (EM/F1) and limited manual sampling (321 questions)
- Comparison to previous Finnish SQuAD translations uses only two cited works
- 5pp EM and 2.5pp F1 degradation from backtranslation lacks deeper error analysis
- Generalizability to other languages and domain-specific content remains untested

## Confidence
- Preservation rates: High confidence in 97.2% claim
- Model performance: High confidence in 73.7 F1 result
- Degradation analysis: Medium confidence in 5pp EM and 2.5pp F1 figures
- Generalizability: Low confidence without broader testing

## Next Checks
1. Expand manual evaluation to 1000+ questions with systematic error categorization by type (factual, syntactic, semantic)
2. Test the approach on additional low-resource languages and domain-specific QA datasets (e.g., medical or legal)
3. Conduct human evaluation of model outputs to assess if translation-induced degradation affects answer quality or usability in practice