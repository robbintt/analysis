---
ver: rpa2
title: 'LangMark: A Multilingual Dataset for Automatic Post-Editing'
arxiv_id: '2511.17153'
source_url: https://arxiv.org/abs/2511.17153
tags:
- translation
- post-editing
- dataset
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LangMark is a multilingual human-annotated dataset for automatic
  post-editing (APE) of neural machine translation (NMT) outputs, comprising over
  200,000 triplets across seven languages (English to Brazilian Portuguese, French,
  German, Italian, Japanese, Russian, and Spanish). The dataset includes source segments,
  NMT outputs from a proprietary system, and human post-edited translations by expert
  linguists.
---

# LangMark: A Multilingual Dataset for Automatic Post-Editing

## Quick Facts
- arXiv ID: 2511.17153
- Source URL: https://arxiv.org/abs/2511.17153
- Reference count: 13
- Over 200,000 human-annotated APE triplets across seven languages

## Executive Summary
LangMark is a multilingual human-annotated dataset for automatic post-editing of neural machine translation outputs, containing over 200,000 triplets across seven language pairs. The dataset includes source segments, NMT outputs from a proprietary system, and human post-edited translations by expert linguists. Experiments demonstrate that large language models with few-shot prompting can effectively perform APE and improve upon both commercial and proprietary NMT systems. The dataset highlights the challenge of determining when edits are necessary, as models with high precision but lower recall often outperform those with higher recall but lower precision.

## Method Summary
The LangMark dataset was constructed using a proprietary neural machine translation system to generate baseline translations across seven language pairs (English to Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish). Source segments were filtered to include only those with at least one NMT error, ensuring the dataset focuses on necessary edits. Expert linguists then performed human post-editing to create the final reference translations. The dataset comprises over 200,000 triplets of source text, NMT output, and human-edited translations. Evaluation experiments used large language models with few-shot prompting, comparing their performance against commercial translation systems like Google Translate and DeepL, as well as the proprietary NMT system used to generate the baseline translations.

## Key Results
- GPT-4o with few-shot prompting achieved 86.7 BLEU on the English-to-Spanish LangMark test set, outperforming both DeepL (80.3 BLEU) and Google Translate (77.6 BLEU)
- LLMs demonstrated that high precision with lower recall (targeted edits) often outperforms high recall with lower precision (extensive changes) in APE tasks
- The dataset reveals that determining when edits are necessary represents a fundamental challenge, with human editors typically making minimal changes only when needed

## Why This Works (Mechanism)
The success of LLMs in APE tasks stems from their ability to understand context and make targeted corrections rather than wholesale rewrites. Few-shot prompting provides sufficient examples for models to learn the editing pattern without extensive fine-tuning. The dataset's focus on necessary edits rather than comprehensive corrections aligns with how human editors naturally approach post-editing tasks, preferring minimal interventions when possible.

## Foundational Learning
- Neural Machine Translation (NMT) basics: Understanding how NMT systems generate translations and common error patterns
  - Why needed: To comprehend baseline outputs and identify edit-worthy errors
  - Quick check: Can distinguish between fluency, adequacy, and adequacy-related errors in translations

- Automatic Post-Editing (APE) concepts: The task of correcting NMT output errors automatically
  - Why needed: Core task that LangMark dataset enables
  - Quick check: Understand triplet structure (source, NMT output, reference) and evaluation metrics

- Large Language Model (LLM) prompting techniques: Few-shot learning and instruction-based prompting
  - Why needed: Primary methodology for evaluating APE performance
  - Quick check: Can design effective few-shot prompts for translation correction tasks

## Architecture Onboarding

Component Map:
Source Text -> NMT System -> APE Model (LLM) -> Post-Edited Output -> Evaluation Metrics

Critical Path:
The critical path involves generating NMT output, applying LLM-based APE with few-shot prompting, and evaluating against human references using automatic metrics (BLEU, COMET).

Design Tradeoffs:
The dataset prioritizes quality over quantity by filtering for segments with errors, which ensures relevance but may not reflect real-world distributions. Using proprietary NMT systems provides controlled baselines but limits reproducibility.

Failure Signatures:
Poor APE performance typically manifests as either over-editing (low precision, high recall) or under-editing (high precision, low recall). Models struggle particularly with semantic errors and when the source text contains ambiguity.

First Experiments:
1. Evaluate LLM performance on clean NMT outputs to establish baseline capabilities
2. Test different few-shot prompt formulations to optimize APE performance
3. Analyze error type distribution to identify which NMT errors are most challenging for APE models

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The dataset uses a proprietary NMT system for baseline translations, limiting reproducibility and comparability
- No examples of error-free NMT outputs are included, as the source corpus was filtered to include only segments with at least one NMT error
- Evaluation methodology may favor precision over recall, potentially not reflecting true translation quality improvements

## Confidence
- High confidence: Dataset construction methodology and basic statistics (200k+ triplets across 7 languages, expert human post-editing)
- Medium confidence: Claims about LLM performance improvements over commercial NMT systems, given the proprietary nature of baseline systems
- Medium confidence: Conclusions about precision-recall trade-offs reflecting human editing behavior, pending further validation

## Next Checks
1. Replicate key experiments using open-source NMT systems as baselines to verify performance claims are not system-specific
2. Conduct human evaluation studies to validate whether LLM-generated edits actually improve translation quality compared to commercial systems
3. Analyze the impact of different error types on LLM performance to determine if certain error categories remain challenging across all tested models