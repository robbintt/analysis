---
ver: rpa2
title: Training Language Models with homotokens Leads to Delayed Overfitting
arxiv_id: '2601.02867'
source_url: https://arxiv.org/abs/2601.02867
tags:
- arxiv
- language
- data
- training
- canonical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces homotokens\u2014meaning-preserving alternative\
  \ subword tokenizations of the same word\u2014as a form of data augmentation for\
  \ language model training. The authors formalize homotokens as strictly label-preserving\
  \ augmentations that preserve exact lexical identity while inducing different internal\
  \ computational representations."
---

# Training Language Models with homotokens Leads to Delayed Overfitting

## Quick Facts
- arXiv ID: 2601.02867
- Source URL: https://arxiv.org/abs/2601.02867
- Authors: Adrian Cosma; Stefan Ruseti; Emilian Radoi; Mihai Dascalu
- Reference count: 37
- Primary result: Homotoken augmentation delays overfitting and improves generalization under data-constrained LM pretraining with R≥16 repetitions.

## Executive Summary
This paper introduces homotokens—meaning-preserving alternative subword tokenizations of the same word—as a form of data augmentation for language model training. The authors formalize homotokens as strictly label-preserving augmentations that preserve exact lexical identity while inducing different internal computational representations. They propose a lightweight architecture with an auxiliary causal encoder that processes homotokens and injects their representations into the main decoder via block-causal cross-attention, all while preserving the standard next-token prediction objective. In data-constrained pretraining with repeated data exposure (R≥16), training with homotokens consistently delays overfitting and improves generalization across diverse evaluation datasets compared to standard transformers. The approach is compatible with other activation-level perturbations like attention dropout and Gaussian noise. However, the effectiveness depends on tokenizer quality: gains are strongest when the tokenizer produces highly compressed canonical tokens and diminish when input is already over-fragmented.

## Method Summary
The method trains language models with homotokens—alternative BPE tokenizations of the same lexical items that decode to identical strings but induce different computational representations. An auxiliary causal encoder processes homotoken sequences with a block causal mask, and cross-attention fuses these representations into the main decoder at the first block. Training uses the standard next-token prediction objective, with homotokens sampled as the top 5 longest subtokens (by character count) excluding the canonical tokenization. The architecture includes learned intra-token and inter-token positional embeddings for the encoder branch. Models are trained on repeated data exposure (R≥16) with scaling from 88M to 244M parameters, using AdamW optimization and cosine learning rate decay.

## Key Results
- Homotoken augmentation consistently delays overfitting under repeated data exposure (R≥16) and improves generalization across diverse evaluation datasets.
- When data is repeated more than 16 times, training with homotokens offers clear advantages: the overfitting point is later in training, and the best loss is lower.
- Using the GPT-2 tokenizer, which is geared towards English, does not benefit multilinguality, while using the aya-23 tokenizer, which is specifically designed to handle multiple languages, shows definite benefits.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Homotoken augmentation delays overfitting by inducing tokenization invariance through exposure to multiple valid segmentations of the same lexical item.
- Mechanism: The model receives alternative subword decompositions (e.g., "dinosaur" → "dino + saur" vs. "d + inosaur") that decode to identical strings. This forces the model to learn representations that are robust to surface-level token variation while preserving semantic identity, effectively increasing training signal diversity without changing labels.
- Core assumption: Models trained on single canonical tokenizations overfit to specific subword patterns rather than learning truly lexical representations.
- Evidence anchors:
  - [abstract]: "homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets"
  - [section 5]: "when the data is repeated more than 16 times, training with homotokens offers clear advantages: the overfitting point is later in training, and the best loss is lower"
  - [corpus]: Related work (Zheng et al. 2025, "Broken Tokens?") shows LLMs can handle non-canonical tokenizations, suggesting latent capacity for tokenization robustness exists but is underutilized in standard training.
- Break condition: When R < 16 (low data repetition), the paper shows no significant benefit—suggesting the mechanism requires sufficient training pressure from data scarcity to manifest.

### Mechanism 2
- Claim: The auxiliary causal encoder with block-causal cross-attention provides structured fusion of fine-grained subtoken information into the canonical token stream.
- Mechanism: The homotoken sequence is processed separately under a block causal mask ensuring all subtokens of token $s_t$ attend only to subtokens of tokens $1, \ldots, t$. Cross-attention then allows canonical tokens to selectively attend to relevant subtoken representations, enabling the model to learn task-dependent weighting rather than uniform aggregation.
- Core assumption: The model benefits from explicit access to subtoken structure rather than learning it implicitly from canonical tokens alone.
- Evidence anchors:
  - [section 3.3]: "Cross-attention, by contrast, lets the model learn task-dependent weights over the homotoken representations, deciding for each context how much to use or disregard each subtoken"
  - [section A.1]: Rejected designs (additive fusion) explicitly noted as inferior because they "force all subtokens to contribute uniformly"
  - [corpus]: No direct corpus evidence on cross-attention vs. additive fusion for tokenization; this is an architectural hypothesis specific to this paper.
- Break condition: If the auxiliary encoder capacity is too small or the cross-attention is placed at an inappropriate depth, fusion may be insufficient—the paper uses a fixed 1M parameter encoder (Section A.2).

### Mechanism 3
- Claim: Effectiveness is conditional on tokenizer compression quality—gains are strongest when canonical tokens are highly compressed and diminish when input is already over-fragmented.
- Mechanism: Highly compressed canonical tokens (e.g., from multilingual tokenizers like aya-23) have more subdivision opportunities, yielding meaningful homotoken variants. Over-fragmented tokenizers leave little room for alternative segmentations, making the auxiliary encoder process nearly identical sequences to the main trunk.
- Core assumption: Tokenizer quality determines the "augmentation budget" available through homotoken sampling.
- Evidence anchors:
  - [section 5]: "using the GPT-2 tokenizer, which is geared towards English, does not benefit multilinguality... using the aya-23 tokenizer, which is specifically designed to handle multiple languages, shows definite benefits"
  - [section 5]: "gains are strongest when the input text is less fragmented by the tokenizer"
  - [corpus]: Petrov et al. (2023) and related work confirm tokenizer fairness issues across languages, supporting the premise that tokenizer quality varies significantly.
- Break condition: When using poorly-matched tokenizers (e.g., English-centric tokenizer on multilingual data), homotokens provide no benefit or may even harm performance.

## Foundational Learning

- Concept: **Subword Tokenization (BPE)**
  - Why needed here: The entire method builds on the observation that BPE tokenization is non-unique—many token sequences decode to the same string. Understanding that canonical "longest-prefix" tokenization is a convention, not a necessity, is prerequisite.
  - Quick check question: Can you explain why "dinosaur" could be tokenized as either [dinosaur] or [d, inosaur] under the same BPE vocabulary?

- Concept: **Cross-Attention Mechanisms**
  - Why needed here: The architecture uses cross-attention to fuse homotoken representations into the main decoder. Understanding how query/key/value projections create learned alignment between sequences is essential for debugging the block-causal mask design.
  - Quick check question: How does cross-attention differ from self-attention in terms of which sequence provides keys/values vs. queries?

- Concept: **Data Augmentation as Regularization**
  - Why needed here: Homotokens are framed as strictly label-preserving augmentations. Understanding why augmentation helps in data-constrained regimes (increasing effective dataset diversity without changing supervision) contextualizes the R≥16 finding.
  - Quick check question: Why would augmenting at the tokenization level preserve meaning more reliably than synonym replacement or back-translation?

## Architecture Onboarding

- Component map:
  Main trunk (standard causal decoder) -> Auxiliary encoder (single Transformer block) -> Cross-attention fusion (block-causal) -> Position embeddings (intra-token + inter-token) -> Next-token prediction loss

- Critical path:
  1. Tokenize input canonically → main trunk embeddings
  2. Sample homotoken variants (top 5 longest subtokens by character count, excluding canonical) → auxiliary encoder
  3. Apply block causal mask M to auxiliary encoder attention
  4. Cross-attend from canonical token positions to corresponding homotoken representations using M_cross
  5. Continue standard causal LM training with canonical next-token prediction loss

- Design tradeoffs:
  - **Auxiliary encoder size**: Paper uses single block with 1M parameters—sufficient for proof-of-concept but scaling implications unknown for >1B models
  - **Homotoken sampling strategy**: Top-5 by character length excludes canonical token; alternative sampling strategies (random, shortest) not explored
  - **Injection depth**: Cross-attention at first block; deeper injection not tested
  - **Tokenizer choice**: Must match language/data distribution; English tokenizer on multilingual data yields no gains

- Failure signatures:
  - No improvement when R < 16 (insufficient data repetition pressure)
  - Degraded performance with mismatched tokenizers (e.g., GPT-2 on multilingual fine-tuning)
  - Auxiliary encoder processing sequences identical to main trunk (over-fragmented tokenization)
  - Position embedding misalignment causing cross-attention to attend to wrong subtokens

- First 3 experiments:
  1. **Reproduction on small scale**: Train 88M parameter model on 4B tokens (R=16) with GPT-2 tokenizer; compare validation loss curves between standard transformer and homotoken variant to verify overfitting delay.
  2. **Tokenizer quality ablation**: Pretrain identical models with GPT-2 vs. aya-23 tokenizer on multilingual AYA dataset subset; measure gap in homotoken benefit to confirm tokenizer dependency.
  3. **Auxiliary encoder depth probe**: Move cross-attention injection from block 1 to blocks 2, 4, 8 in separate runs; observe whether early vs. late fusion affects generalization on AlpacaEval or grammar correction benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness tightly coupled to dataset repetition (R≥16) and tokenizer quality, creating narrow conditions for consistent gains.
- Enumeration algorithm for valid homotoken segmentations is underspecified, potentially leading to inconsistent implementations.
- Auxiliary encoder architecture (single block, 1M parameters) and cross-attention injection depth appear empirically motivated but lack systematic ablation.

## Confidence

- **High confidence**: The claim that homotokens delay overfitting under repeated data exposure (R≥16) is directly supported by validation loss curves and downstream evaluations across diverse tasks. The architectural description of the auxiliary encoder and cross-attention fusion is precise and reproducible given the masks and embedding design.

- **Medium confidence**: The assertion that gains depend on tokenizer compression quality is supported by comparisons between GPT-2 and aya-23 tokenizers, but the exact threshold for "over-fragmented" tokenization is not quantified. The block-causal mask design is clearly described, but its optimality versus alternative causal schemes is not explored.

- **Low confidence**: The enumeration algorithm for homotoken variants is only sketched; without the full BPE merge tree traversal procedure, reproducing the exact set of sampled tokens is uncertain. The generalization of results from 88M and 244M parameter models to larger scales is untested.

## Next Checks

1. **Tokenization quality ablation at scale**: Train homotoken models using both high-compression (aya-23) and over-fragmented (GPT-2 on multilingual data) tokenizers on the same 4B-token FineWeb-Edu subset. Quantify the gap in validation loss improvement and downstream benchmark gains to establish a clear tokenizer quality threshold.

2. **Auxiliary encoder depth and capacity scaling**: Systematically vary the auxiliary encoder depth (1, 2, 4 blocks) and parameter count (0.5M, 1M, 2M) while keeping cross-attention injection at block 1. Measure overfitting delay and generalization on AlpacaEval to identify optimal architecture scaling laws.

3. **Cross-attention injection depth probe**: Fix the auxiliary encoder at 1 block, 1M parameters, and move the cross-attention fusion point to decoder blocks 2, 4, and 8 in separate runs. Evaluate whether early fusion (block 1) is optimal or if deeper injection yields better representation learning on grammar correction and code completion tasks.