---
ver: rpa2
title: Teaching Language Models To Gather Information Proactively
arxiv_id: '2507.21389'
source_url: https://arxiv.org/abs/2507.21389
tags:
- information
- questions
- proactive
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces proactive information gathering as a new
  task for language models, where models must identify and strategically elicit missing
  contextual information through targeted questions. The authors design a synthetic
  conversation engine using the DOLOMITES dataset to generate partially specified
  real-world tasks with masked information, then train a Qwen-2.5-7B model using reinforcement
  learning with a reward signal that encourages questions reaching beyond the provided
  context.
---

# Teaching Language Models To Gather Information Proactively
## Quick Facts
- arXiv ID: 2507.21389
- Source URL: https://arxiv.org/abs/2507.21389
- Reference count: 7
- Primary result: Language model trained to proactively seek missing information outperforms baseline by 18% on automatic metrics

## Executive Summary
This paper introduces proactive information gathering as a novel task for language models, where models must identify and strategically elicit missing contextual information through targeted questions. The authors design a synthetic conversation engine using the DOLOMITES dataset to generate partially specified real-world tasks with masked information, then train a Qwen-2.5-7B model using reinforcement learning with a reward signal that encourages questions reaching beyond the provided context. Experiments show the trained model significantly outperforms o3-mini by 18% on automatic evaluation metrics, while human evaluation reveals a 42% preference for clarification questions and 28% preference for final outlines generated by their approach.

## Method Summary
The authors create a synthetic conversation engine that generates partially specified tasks from the DOLOMITES dataset by masking critical information. They train a Qwen-2.5-7B model using reinforcement learning with a custom reward function that encourages asking questions to fill knowledge gaps. The reward signal is designed to promote questions that successfully elicit information not present in the original context. The model is evaluated on both automatic metrics comparing information coverage and human preference studies assessing the quality of clarification questions and final task outlines.

## Key Results
- Qwen-2.5-7B model trained on proactive information gathering outperforms o3-mini by 18% on automatic evaluation metrics
- Human evaluators preferred the model's clarification questions 42% of the time compared to baseline
- Human evaluators preferred the model's final outlines 28% more often than baseline outputs

## Why This Works (Mechanism)
The approach works by explicitly training language models to recognize information gaps and strategically formulate questions to fill those gaps. By using reinforcement learning with a reward function that values questions extending beyond the provided context, the model learns to balance between asking clarifying questions and progressing toward task completion. The synthetic conversation engine provides controlled training scenarios where the exact information needs are known, allowing for precise reward shaping during training.

## Foundational Learning
- Reinforcement learning from human feedback (RLHF): why needed - to train models to optimize for human-aligned behaviors; quick check - verify reward function captures desired information-seeking behavior
- Synthetic data generation: why needed - provides controlled environments for training and evaluation; quick check - validate generated conversations match real-world task patterns
- Information gap identification: why needed - models must recognize when they lack necessary context; quick check - measure accuracy of gap detection in test scenarios

## Architecture Onboarding
Component map: DOLOMITES dataset -> Synthetic conversation engine -> Masked task generator -> Qwen-2.5-7B model -> Reinforcement learning training loop -> Evaluation metrics

Critical path: Task generation → Model inference → Question selection → Information gathering → Task completion

Design tradeoffs: Synthetic data provides control but may lack real-world complexity; RL reward shaping requires careful tuning to avoid degenerate behaviors

Failure signatures: Model may ask redundant questions, fail to recognize critical information gaps, or focus on irrelevant clarification

First experiments:
1. Test model's ability to identify missing information in simple masked tasks
2. Evaluate question quality on progressively complex scenarios
3. Compare information coverage between trained and baseline models

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data generation may not capture real-world interaction complexity
- Evaluation primarily uses automatic metrics and human preference studies rather than extensive real-world testing
- Reinforcement learning approach requires careful reward engineering and may not generalize well to tasks outside training distribution

## Confidence
- High confidence in synthetic conversation engine's ability to generate controlled test scenarios
- Medium confidence in RLHF training methodology and its effectiveness on Qwen-2.5-7B model
- Medium confidence in reported performance improvements against o3-mini, given synthetic nature of evaluation
- Low confidence in generalizability to completely unseen real-world tasks and domains

## Next Checks
1. Test trained model on diverse, real-world task datasets (customer service logs, technical support conversations) to evaluate generalization beyond synthetic data
2. Conduct longitudinal studies to assess whether proactive questioning behavior persists across extended conversations and multiple task types
3. Implement ablation studies comparing different reward structures and question-selection strategies to isolate which components contribute most to performance gains