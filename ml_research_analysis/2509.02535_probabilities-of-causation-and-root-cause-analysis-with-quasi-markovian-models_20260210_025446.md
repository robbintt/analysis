---
ver: rpa2
title: Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models
arxiv_id: '2509.02535'
source_url: https://arxiv.org/abs/2509.02535
tags:
- causal
- variables
- probabilities
- root
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses computational challenges in calculating probabilities
  of causation under partial identifiability and latent confounding, proposing algorithmic
  simplifications that significantly reduce complexity. The authors extend quasi-Markovian
  models to streamline counterfactual graph reductions, lowering the degree of multilinear
  programs used for tight bounds on PN, PS, and PNS.
---

# Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models

## Quick Facts
- arXiv ID: 2509.02535
- Source URL: https://arxiv.org/abs/2509.02535
- Reference count: 0
- This paper proposes algorithmic simplifications for calculating probabilities of causation under partial identifiability and latent confounding, extending quasi-Markovian models to streamline counterfactual graph reductions and demonstrating that necessity-based metrics consistently identify correct root causes in synthetic microservice models.

## Executive Summary
This paper addresses computational challenges in calculating probabilities of causation (PN, PS, PNS) under partial identifiability and latent confounding in quasi-Markovian structural causal models. The authors propose algorithmic simplifications that significantly reduce complexity by streamlining counterfactual graph reductions, lowering the degree of multilinear programs used for tight bounds. A novel Root Cause Analysis (RCA) methodology leverages these probabilities to rank causal paths in DAGs rather than isolated variables. Experiments on synthetic microservice models demonstrate that necessity-based metrics (PN and weak-PN) consistently identify correct root causes under limited observability, while sufficiency-based metrics underperform.

## Method Summary
The method computes tight bounds for probabilities of causation through counterfactual graph reduction in quasi-Markovian models, then applies these bounds to rank causal paths for root cause analysis. The process involves constructing counterfactual graphs (twin networks), applying graph reduction theorems to remove unnecessary variables, formulating multilinear optimization problems with linear constraints from c-component factorization, and solving these programs to derive bounds. The RCA component implements a pruned-DFS algorithm that scores entire causal paths using linear path-significance scores and stops when causal influence drops abruptly, enabling more accurate tracing of cascading failure paths in complex systems.

## Key Results
- Necessity-based metrics (PN and weak-PN) consistently identify correct root causes in synthetic microservice models with cascading failures
- Sufficiency-based metrics (PS and weak-PS) consistently underperform in identifying root causes under limited observability
- Graph reduction techniques significantly lower multilinear program degree from number of exogenous variables to reduced graph exogenous variables
- Path-based scoring methodology outperforms isolated variable ranking for causal narrative identification

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Graph Reduction for Computational Tractability
- Claim: Reducing counterfactual graphs lowers the degree of multilinear programs required to compute tight bounds on probabilities of causation (PN, PS, PNS).
- Mechanism: By identifying and removing exogenous variables that are d-separated from target counterfactual variables by intermediate endogenous variables (Z), the multilinear objective function's degree—originally equal to the number of exogenous variables—is reduced to the number of exogenous variables in the reduced graph. The marginal distribution of the separating variables (Z) is fixed to the observed input distribution.
- Core assumption: Quasi-Markovian structural causal models (SCMs), where each endogenous variable has at most one exogenous parent, and the existence of a set Z that d-separates specific ancestors from counterfactual targets given factual observations (Theorem 1, Theorem 2).
- Evidence anchors:
  - [abstract] "algorithmic simplifications that significantly reduce complexity... extend quasi-Markovian models to streamline counterfactual graph reductions, lowering the degree of multilinear programs"
  - [Section 3, Theorem 1] "Then the computation of lower and upper versions of PN and of PS are not affected if all ascendants of Z (including exogenous variables) are removed from the counterfactual graph, and the marginal distribution of Z is then be set at Pr̂(Z)"
  - [corpus] Related work "Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models" investigates similar complexity reduction for quasi-Markovian models, supporting the viability of this approach.
- Break condition: If the SCM is not quasi-Markovian (multiple exogenous parents per endogenous variable), or if no suitable separating set Z exists, the reductions do not apply, potentially returning the problem to the original, higher degree of multilinear programming.

### Mechanism 2: Necessity-Based Metrics for Root Cause Ranking
- Claim: Necessity-based causal metrics, specifically Probability of Necessity (PN) and its interventional proxy weak-PN (w-PN), more effectively identify correct root causes in systems with cascading failures and latent confounders than sufficiency-based metrics (PS, w-PS).
- Mechanism: PN captures the counterfactual query: "Would the effect have occurred if the cause had not?" This aligns with the diagnostic goal of RCA: determining if removing a candidate cause would prevent the observed incident. A pruning-DFS algorithm uses these metrics to score and rank entire causal paths, stopping when the causal strength (change in PC score) drops abruptly.
- Core assumption: Assumption: Necessity is a more appropriate measure than sufficiency for identifying root causes in systems where failures propagate (remedial vs. preventive analysis). Also assumes the provided DAG structure is correct.
- Evidence anchors:
  - [abstract] "Experiments on synthetic microservice models demonstrate that necessity-based metrics (PN and weak-PN) consistently identify correct root causes under limited observability, while sufficiency-based metrics underperform."
  - [Section 4.3] "metrics derived from necessity (PN and w-PN) consistently identifies the correct root cause pathways... sufficiency-based metrics (PS and w-PS) consistently underperformed"
  - [corpus] Corpus evidence does not directly compare necessity vs. sufficiency for RCA; this paper provides the primary evidence.
- Break condition: If the goal is preventive (what would cause an incident if present?), sufficiency metrics might be more relevant. If the causal DAG is significantly misspecified, the causal narratives derived will be incorrect regardless of the metric.

### Mechanism 3: Path-Based Scoring for Causal Narrative Identification
- Claim: Ranking entire causal paths (narratives) rather than isolated variables using a linear path-significance score improves root cause identification.
- Mechanism: Algorithm 1 traverses the causal graph upstream from an incident. It computes a local PC score for each variable relative to the target. It prunes branches where causal influence (PC score) drops precipitously. A path score S(π) aggregates the PC scores along the surviving path, weighting the root node more heavily (w ≥ 1). The highest-scoring path identifies the root cause and its propagation chain.
- Core assumption: Assumption: A true root cause is the head of a coherent causal chain where causal influence is relatively stable or increasing upstream, and a significant drop in PC score indicates a transition away from the true causal narrative.
- Evidence anchors:
  - [abstract] "A novel Root Cause Analysis (RCA) methodology leverages these probabilities to rank causal paths in DAGs rather than isolated variables."
  - [Section 4.2] "For every surviving path π = ⟨Xr, . . . , X1, Y⟩ we define a linear path-significance score S(π) = w·PC(Xr, Y) + Σ Pr−1 j=1 PC(Xj, Y)..."
  - [corpus] Corpus evidence does not directly compare path vs. node scoring; this is a novel methodological contribution.
- Break condition: If causal influence does not monotonically decrease away from the root cause (e.g., due to complex feedback or masking effects), the pruning rule may cut the true path prematurely. The linear aggregation assumes contributions are additive, which may not hold in all cases.

## Foundational Learning

- Concept: **Quasi-Markovian Structural Causal Models (SCMs)**
  - Why needed here: The central theorems (1 & 2) for computational simplification rely entirely on the quasi-Markovian assumption (each endogenous variable has at most one exogenous parent). Without understanding this constraint, one cannot judge when the method applies.
  - Quick check question: Given a causal graph with two latent confounders (U1, U2) both pointing to the same endogenous variable V, can Theorem 1 be directly applied to simplify PN computations for V?

- Concept: **Counterfactual Graphs and Twin Networks**
  - Why needed here: The computational improvements for PN/PS/PNS are framed as reductions on counterfactual graphs. To understand what is being "reduced," one must grasp that counterfactual queries create copies of the original graph (a twin network) representing hypothetical interventions.
  - Quick check question: In a twin network for computing PN = Pr(Y_x=0 = 0 | X=1, Y=1), which nodes from the original SCM are duplicated, and which are shared?

- Concept: **Probabilities of Causation (PN, PS, PNS)**
  - Why needed here: The paper's entire RCA framework is built on choosing between these metrics. Distinguishing between the semantics of "necessity" (PN), "sufficiency" (PS), and their combination (PNS) is critical to interpreting the experimental results and deciding which metric to use in practice.
  - Quick check question: Consider a cause X=1 and effect Y=1. What does a high Probability of Necessity (PN) but low Probability of Sufficiency (PS) imply about the relationship between X and Y?

## Architecture Onboarding

- Component map:
  SCM Canonicalizer -> Counterfactual Graph Builder & Reducer -> Multilinear Optimizer -> Path-Ranking Engine (Algorithm 1)

- Critical path:
  Model Definition -> (Choice of PC Metric: PN/PS/PNS) -> Counterfactual Graph Reduction -> Multilinear Optimization (bound calculation) -> Path-Scoring Algorithm (DFS + Pruning). The critical computational bottleneck is the optimizer; the critical methodological choice is the metric (necessity vs. sufficiency).

- Design tradeoffs:
  1.  **PN vs. w-PN:** PN provides tighter, more discriminative bounds but requires solving a multilinear program. w-PN (Pr(Y=0|do(X=0))) is an interventional proxy, often identifiable and simpler to compute, offering a faster but potentially less precise alternative.
  2.  **Graph Fidelity:** A more detailed causal graph with more nodes may improve accuracy but will increase the degree of the multilinear objective (before reduction), raising computational cost. The quasi-Markovian restriction limits model expressiveness for tractability.
  3.  **Pruning Thresholds (α, w):** The pruning factor (α) and root weight (w) in the path-ranking algorithm control sensitivity vs. noise. A small α may prune too aggressively, missing causes; a large w may overweight an incorrect root if the local score is noisy.

- Failure signatures:
  1.  **High-Degree Blow-up:** If Theorem 1/2 cannot apply (no separating Z), the multilinear program degree equals the number of exogenous variables, leading to exponential complexity. Signature: optimizer runtimes grow intractably with graph size.
  2.  **Empty or Trivial Paths:** If pruning is too aggressive (α too small), the algorithm may return no paths or only single-node paths. Signature: output list is empty or contains only the incident node's immediate parents.
  3.  **Misspecified Metric:** Using PS/w-PS in a remedial RCA context will consistently point to incorrect causes (as shown in experiments). Signature: identified root causes do not align with domain knowledge of failure propagation.

- First 3 experiments:
  1.  **Baseline Reproduction (Table 1/2):** Replicate one of the synthetic microservice experiments (e.g., Model 1, Narrative 1). Verify that using PN or w-PN correctly identifies "MemLeak" as the root cause, while PS fails. This validates the entire pipeline.
  2.  **Ablation on Graph Reduction:** Take a more complex graph and manually disable Theorem 1/2 reductions. Compare the runtime of the Multilinear Optimizer for PN bounds with and without reduction to quantify the computational gains.
  3.  **Sensitivity to α & w:** Run the Path-Ranking Engine on a synthetic dataset with a known ground-truth causal path. Vary the pruning factor α and root weight w to find optimal ranges that balance recall of the true path against precision (avoiding spurious long paths).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed counterfactual graph reduction techniques and RCA methodology be generalized to discrete and continuous variables without requiring binarization?
- Basis in paper: [explicit] The conclusion suggests extending "methods to recently formalized definitions of probabilities of causation for discrete and continuous variables" to avoid the "need for data binarization."
- Why unresolved: The current algorithms and experiments (Section 4.3) rely on binary representations of service-level metrics, limiting granularity in real-world continuous data.
- What evidence would resolve it: Successful derivation of multilinear program simplifications for non-binary structural causal models and validation on continuous microservice datasets.

### Open Question 2
- Question: Can the computational complexity of bounds for probabilities of causation be managed if the quasi-Markovian assumption is relaxed?
- Basis in paper: [inferred] The authors note in Section 2 that relaxing the quasi-Markovian assumption makes optimization "much harder, and often practically intractable," yet this assumption restricts the class of models.
- Why unresolved: The paper's theoretical contributions (Theorems 1 and 2) explicitly depend on the graph being quasi-Markovian to reduce multilinear degrees.
- What evidence would resolve it: Development of approximation algorithms or bounded relaxations that perform efficiently on general non-Markovian graphs with multiple exogenous parents per node.

### Open Question 3
- Question: What is the optimal heuristic for collapsing interval estimates of causation probabilities (PN, PS) into scalar scores for path ranking?
- Basis in paper: [inferred] Section 4.3 mentions using strategies like Minimum, Maximum, Mean, and Midpoint to handle interval estimates but does not evaluate which strategy yields the most robust root cause identification.
- Why unresolved: The choice of scalar reduction is currently heuristic; the sensitivity of the final path ranking to this choice remains unquantified.
- What evidence would resolve it: Comparative analysis of root cause recovery rates across various synthetic models using different interval-to-scalar reduction methods.

## Limitations
- The quasi-Markovian assumption limits applicability to systems where each endogenous variable has at most one exogenous parent, restricting model expressiveness for tractability
- Experimental validation relies on synthetic microservice models without real-world deployment examples, raising questions about generalizability to actual system failures
- The methodology depends on accurate causal DAG specification, and misspecification can lead to incorrect root cause identification regardless of metric choice

## Confidence
- **High Confidence:** The theoretical framework for counterfactual graph reduction and multilinear optimization is mathematically sound and well-established in causal inference literature
- **Medium Confidence:** The experimental results showing necessity-based metrics (PN, w-PN) outperforming sufficiency-based metrics are internally consistent but rely on synthetic data with known ground truth
- **Medium Confidence:** The path-ranking methodology (Algorithm 1) is novel but depends on hyperparameter tuning (α, w) that may not generalize across different failure modes or system architectures

## Next Checks
1. **Ablation Study on Graph Reduction:** Implement the same multilinear optimization without applying Theorem 1/2 reductions on a complex graph to empirically measure the computational savings and verify the claimed complexity reduction
2. **Real-World Deployment Test:** Apply the RCA methodology to a real-world incident dataset (e.g., from a distributed system monitoring platform) and compare the identified causal narratives against domain expert analysis to validate practical effectiveness
3. **Metric Sensitivity Analysis:** Systematically vary the pruning factor α and root weight w across multiple synthetic scenarios with different failure propagation patterns to determine optimal hyperparameter ranges and assess robustness to noise in causal influence estimates