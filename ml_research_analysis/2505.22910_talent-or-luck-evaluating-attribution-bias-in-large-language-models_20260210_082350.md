---
ver: rpa2
title: Talent or Luck? Evaluating Attribution Bias in Large Language Models
arxiv_id: '2505.22910'
source_url: https://arxiv.org/abs/2505.22910
tags:
- failure
- attribution
- success
- bias
- asian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a cognitively grounded framework to evaluate
  social biases in large language models (LLMs) using Attribution Theory from social
  psychology. The framework probes how LLMs assign internal (effort, ability) versus
  external (difficulty, luck) causes to success and failure outcomes across social
  identities including gender, nationality, race, and religion in 10 societal scenarios.
---

# Talent or Luck? Evaluating Attribution Bias in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.22910
- **Source URL**: https://arxiv.org/abs/2505.22910
- **Reference count**: 40
- **Primary result**: LLMs show significant attribution asymmetries, with marginalized groups receiving less credit for success and more blame for failure compared to dominant groups.

## Executive Summary
This work introduces a cognitively grounded framework to evaluate social biases in large language models (LLMs) using Attribution Theory from social psychology. The framework probes how LLMs assign internal (effort, ability) versus external (difficulty, luck) causes to success and failure outcomes across social identities including gender, nationality, race, and religion in 10 societal scenarios. Experiments on three LLMs—AYA-EXPANSE-8B, QWEN-32B, and LLAMA-3.3-70B—reveal significant attribution asymmetries. Marginalized groups receive less credit for success and more blame for failure compared to dominant groups. Smaller models like AYA-EXPANSE-8B rely more on external attributions, while larger models like QWEN-32B and LLAMA-3.3-70B show stronger internal attribution biases. Attribution patterns also vary by domain, with education and workplace outcomes more often attributed to effort, while humanities domains show stronger reliance on luck. Observer identity and reasoning significantly influence attributions, especially for failure outcomes, highlighting the contextual and comparative nature of bias in model reasoning.

## Method Summary
The study constructs 400 prompt templates spanning 10 societal scenarios, incorporating 4 identity dimensions with 5 male and 5 female names per group. For each template, binary outcomes (success/failure) are paired with 4 attribution options (effort, ability, difficulty, luck). The evaluation runs across three settings: single-actor, actor-actor, and actor-observer, generating 140k total prompts. Models tested include AYA-EXPANSE-8B, QWEN-32B, and LLAMA-3.3-70B with temperature=0.7, top_p=0.95, max_tokens=200. Internal-External effect size d = (p_effort + p_ability) - (p_difficulty + p_luck) measures attribution preference per identity/outcome, while attribution gap Δd = d_X - d_Y quantifies cross-identity differences. Statistical tests include one-sample t-tests for single-actor and Wilcoxon signed-rank tests for paired comparisons.

## Key Results
- Marginalized groups receive less credit for success and more blame for failure compared to dominant groups
- Smaller models (AYA-EXPANSE-8B) rely more on external attributions (difficulty, luck), while larger models (QWEN-32B, LLAMA-3.3-70B) prefer internal attributions (effort, ability)
- Cross-identity comparisons and observer-mediated settings amplify attribution biases, especially for failure outcomes
- Attribution patterns vary by domain, with education and workplace outcomes more often attributed to effort, while humanities domains show stronger reliance on luck

## Why This Works (Mechanism)

### Mechanism 1: Training Data Encodes Human-Like Attribution Patterns
- Claim: LLMs internalize cognitive attribution patterns from human-authored training corpora, producing asymmetries that mirror social psychology findings.
- Mechanism: Training on narrative text exposes models to how humans explain outcomes (e.g., crediting dominant groups' success to ability, marginalizing groups' success to luck). These patterns surface when models complete attribution tasks.
- Core assumption: Attributions in training data reflect societal stereotypes; models learn these correlations.
- Evidence anchors:
  - [abstract] "marginalized groups receive less credit for success and more blame for failure"
  - [Page 2] "trained on massive human corpora, they may internalize not just associations but also cognitive patterns that underlie human social bias"
  - [corpus] Weak direct evidence; neighbor papers focus on bias detection, not attribution mechanisms specifically.
- Break condition: If training corpora were debiased for attribution patterns or models trained on synthetic data lacking social narratives, this mechanism would weaken.

### Mechanism 2: Model Scale Shifts Attribution Strategy
- Claim: Larger models prefer internal attributions (effort, ability); smaller models favor external ones (difficulty, luck).
- Mechanism: Scale increases capacity to learn complex internal-cause associations (effort/ability correlate with outcomes in training narratives), while smaller models default to safer external explanations.
- Core assumption: Scale increases sensitivity to statistical patterns linking personal traits to outcomes.
- Evidence anchors:
  - [Page 5] "Smaller models rely on external attributions while larger models prefer internal attributions"
  - [Page 5] "AYA attributes both success and failure to task difficulty and luck more than other factors... QWEN and LLAMA rely most on effort"
  - [corpus] No scale-attribution comparison found in neighbors.
- Break condition: If smaller models were fine-tuned on attribution-balanced data, or architectural differences (not scale) drive the pattern.

### Mechanism 3: Comparative Context Amplifies Attribution Asymmetry
- Claim: Presenting multiple identities or an observer shifts model attributions, amplifying biases beyond single-actor baselines.
- Mechanism: Cross-identity comparison activates relative judgment heuristics; observer attributions provide social cues that models follow, especially for failure outcomes.
- Core assumption: Models are sensitive to contextual framing and social comparison structures.
- Evidence anchors:
  - [abstract] "Cross-identity comparisons and observer-mediated settings amplify these biases"
  - [Page 7-8] "observer comments significantly sway the model's attributions, especially in failure cases... contrary to what we found in single-actor"
  - [corpus] Neighbor "Source framing triggers systematic evaluation bias" supports framing sensitivity.
- Break condition: If models were trained with context-invariant attribution objectives or explicit debiasing for comparative settings.

## Foundational Learning

- Concept: **Attribution Theory (Internal vs. External Causes)**
  - Why needed here: Core framework; understanding effort/ability (internal) vs. difficulty/luck (external) is essential to interpret model outputs.
  - Quick check question: If a model explains "Maria failed the exam" as "the questions were unusually hard," is this internal or external attribution?

- Concept: **Actor-Observer Asymmetry**
  - Why needed here: Explains why third-party attributions differ from self-attributions; directly relevant to actor-observer setting.
  - Quick check question: Why might an observer blame a person's failure on low effort while the person blames the test difficulty?

- Concept: **I-E Differential (Effect Size d)**
  - Why needed here: Primary metric; quantifies internal vs. external attribution preference per identity/outcome.
  - Quick check question: If d = +0.15 for success, does the model favor internal or external causes?

## Architecture Onboarding

- Component map:
  - Prompt templates -> 400 templates × identities × outcomes -> 140k prompts
  - Evaluation settings: Single-actor, Actor-Actor, Actor-Observer
  - Metric: I-E differential d = (p_effort + p_ability) - (p_difficulty + p_luck); attribution gap Δd for comparisons
  - Models tested: AYA-EXPANSE-8B, QWEN-32B, LLAMA-3.3-70B

- Critical path:
  1. Construct templates with identity markers, scenarios, outcomes, and 4 attribution options
  2. Run model inference to get probability distribution over options
  3. Compute d per identity/outcome; aggregate across scenarios
  4. For comparisons, compute Δd between identity pairs; test significance (Wilcoxon signed-rank)

- Design tradeoffs:
  - Closed-ended (4 options) enables controlled comparison but limits attribution diversity
  - Names as identity proxies are practical but imperfect (acknowledged in Limitations)
  - No ground truth for attributions; analysis focuses on asymmetry, not correctness

- Failure signatures:
  - Near-zero d across all identities → model may not engage with task or options not distinct enough
  - Inconsistent d across replications → check temperature/randomness; lower temperature for reproducibility
  - Observer condition reverses single-actor pattern → expected (paper documents this)

- First 3 experiments:
  1. Replicate single-actor setting on a new model; verify d values correlate with reported patterns (internal for dominant groups, external for marginalized).
  2. Run actor-actor on a single scenario (e.g., education success-failure) with identity pairs; confirm Δd direction matches paper (dominant groups favored).
  3. Test actor-observer with one observer identity (e.g., American) across multiple actors; verify observer influence is stronger for failure than success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attribution biases manifest in open-ended, free-form generation where LLMs construct explanations without predefined categories?
- Basis in paper: [explicit] The authors state: "Our current study focuses on closed-ended prompts with predefined attribution options... open-ended, free-form reasoning... may reveal richer and more implicit forms of bias. As part of future work, we plan to extend our framework to open-ended attribution generation and scoring."
- Why unresolved: The four-category taxonomy constrains possible attributions and may miss subtler forms of reasoning beyond effort, ability, difficulty, and luck.
- What evidence would resolve it: Extending the framework to unconstrained text generation and developing scoring methods for open-ended attributions across identities.

### Open Question 2
- Question: How do attribution biases transfer to real-world downstream applications such as hiring decisions, performance evaluations, or educational assessments?
- Basis in paper: [inferred] The paper notes LLMs increasingly mediate decisions with real-world consequences, but the evaluation uses constructed scenarios rather than deployed systems.
- Why unresolved: The study measures biases in controlled prompt templates, not in actual decision-making pipelines where stakes and context differ.
- What evidence would resolve it: Evaluating attribution patterns in deployed LLM applications or domain-specific simulations with realistic task structures.

### Open Question 3
- Question: What mechanisms cause larger models (e.g., 70B) to favor internal attributions while smaller models (e.g., 8B) rely on external factors?
- Basis in paper: [explicit] The authors observe: "Smaller models rely on external attributions while larger models prefer internal attributions" but do not explain why this scaling pattern emerges.
- Why unresolved: The paper documents the pattern across model sizes but does not investigate whether it stems from training data, model capacity, or emergent reasoning behaviors.
- What evidence would resolve it: Controlled experiments varying model scale while holding training data and architecture constant, or probing internal representations linked to attribution choices.

## Limitations

- Framework relies on predefined attribution categories that may not capture the full spectrum of human causal reasoning
- Name-based identity proxies introduce potential confounds, as names can carry multiple intersecting identities and cultural associations
- Closed-ended prompt design may artificially constrain model responses compared to open-ended attribution generation
- Statistical significance is established but effect sizes are sometimes modest (e.g., I-E differentials around 0.1-0.2), raising questions about practical impact

## Confidence

- **High confidence**: Scale-attribution relationship (smaller models favor external attributions, larger models favor internal attributions) - supported by direct probability comparisons across models
- **Medium confidence**: Cross-identity attribution asymmetries - consistent patterns observed but names as proxies introduce measurement uncertainty
- **Medium confidence**: Observer influence on attributions - effect documented but contextual variability suggests caution in generalization

## Next Checks

1. Replicate the single-actor setting with an additional name set from a different source to test robustness to name-based identity proxies
2. Conduct an open-ended attribution study (no predefined options) to compare forced-choice vs. generative attribution patterns
3. Test model performance on attribution tasks with explicitly balanced training corpora to isolate whether patterns reflect genuine bias vs. learned statistical correlations