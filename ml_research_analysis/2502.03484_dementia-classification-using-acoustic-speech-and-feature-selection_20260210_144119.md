---
ver: rpa2
title: Dementia Classification Using Acoustic Speech and Feature Selection
arxiv_id: '2502.03484'
source_url: https://arxiv.org/abs/2502.03484
tags:
- features
- mfcc
- feature
- speech
- dementia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses early diagnosis of dementia by analyzing acoustic
  features from spontaneous speech. Using the ADReSS 2020 dataset, which contains
  recordings of individuals describing a kitchen scene, the authors extracted 6,923
  acoustic features (eGeMAPS, EmoBase, and ComParE) from entire recordings without
  segmentation.
---

# Dementia Classification Using Acoustic Speech and Feature Selection

## Quick Facts
- arXiv ID: 2502.03484
- Source URL: https://arxiv.org/abs/2502.03484
- Reference count: 40
- This study achieved 87.8% classification accuracy using acoustic features from spontaneous speech recordings

## Executive Summary
This study addresses early diagnosis of dementia by analyzing acoustic features from spontaneous speech. Using the ADReSS 2020 dataset, which contains recordings of individuals describing a kitchen scene, the authors extracted 6,923 acoustic features (eGeMAPS, EmoBase, and ComParE) from entire recordings without segmentation. Three machine learning models—Ridge regression, Extreme Minimal Learning Machine (EMLM), and Linear Support Vector Machine (L-SVM)—were used to compute feature importance scores. Ridge regression achieved the highest classification accuracy of 87.8% in Leave-One-Subject-Out cross-validation, while EMLM proved most effective on a separate test dataset with 79.2% accuracy. The study identified frequency-domain spectrogram and MFCC features as most important for distinguishing dementia patients from healthy controls.

## Method Summary
The study used the ADReSS 2020 dataset containing 156 subjects (108 training, 48 test) who described a kitchen scene. Audio recordings were preprocessed with noise reduction, downsampled to 16 kHz, and normalized to EBU R 128 standard. The authors extracted 6,923 acoustic features using openSMILE configurations (eGeMAPS, EmoBase, and ComParE) from full recordings without segmentation. Feature selection was performed using permutation testing with 100 runs and 5-fold cross-validation, employing Wilcoxon signed-rank tests to identify significant features. Three models were evaluated: Ridge regression (Glmnet with coordinate descent), Extreme Minimal Learning Machine (EMLM) with all training samples as reference points, and Linear SVM (scikit-learn SMO). Regularization parameters were optimized via grid search, and final models were evaluated using Leave-One-Subject-Out cross-validation and a separate test set.

## Key Results
- Ridge regression achieved 87.8% classification accuracy in Leave-One-Subject-Out cross-validation
- EMLM achieved 79.2% accuracy on the separate test dataset
- Frequency-domain spectrogram and MFCC features were identified as most important for dementia classification
- Results rank among top approaches using only acoustic features for dementia diagnosis

## Why This Works (Mechanism)
The effectiveness of this approach likely stems from the ability of acoustic features to capture subtle changes in speech patterns associated with cognitive decline. Frequency-domain spectrogram features can reveal alterations in vocal tract resonances and speech timing, while MFCC features capture spectral envelope characteristics that may be affected by cognitive impairment. The combination of these features with robust feature selection and non-linear classifiers (EMLM) enables the model to identify patterns that distinguish dementia patients from healthy controls, even when using only spontaneous speech recordings without segmentation.

## Foundational Learning
- **Leave-One-Subject-Out cross-validation**: Why needed - prevents data leakage when subjects appear in both train and test sets; Quick check - verify each fold contains unique subjects
- **Permutation testing for feature selection**: Why needed - establishes statistical significance of features while controlling for multiple comparisons; Quick check - compare feature importance distributions with and without label shuffling
- **Extreme Minimal Learning Machine**: Why needed - non-linear classifier that uses all training samples as reference points, potentially capturing subtle acoustic patterns; Quick check - verify reference points are correctly indexed from training data

## Architecture Onboarding

**Component Map:**
ADReSS 2020 audio -> Preprocessing (noise reduction, downsampling, normalization) -> openSMILE feature extraction (eGeMAPS + EmoBase + ComParE) -> Feature deduplication and variance filtering -> Permutation-based feature selection -> Model training (Ridge/EMLM/L-SVM) -> Cross-validation and testing

**Critical Path:**
The most critical sequence is audio preprocessing → feature extraction → feature selection → model training. Each step builds on the previous, with feature selection being particularly sensitive to permutation testing parameters and model-specific importance scoring.

**Design Tradeoffs:**
The study chose full recording analysis over segmentation, trading temporal resolution for reduced preprocessing complexity and avoiding potential boundary artifacts. This decision likely preserved natural speech patterns but may have diluted temporally-localized acoustic markers.

**Failure Signatures:**
Overfitting is indicated by the performance drop from 87.8% (LOSO) to 79.2% (test set). Data leakage could occur if normalization is fit on the entire dataset rather than per-fold. Using segmented audio instead of full recordings would yield different feature distributions and potentially invalid comparisons.

**First 3 Experiments to Run:**
1. Verify Min-Max normalization is fit only on training folds, not the full dataset
2. Re-run feature selection with shuffled labels to establish baseline importance distributions
3. Test model performance on segmented audio to compare with full-recording approach

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit: How do these acoustic markers compare to other biomarkers (e.g., neuroimaging, biomarkers in CSF)? Can these features generalize across different languages and cultural contexts? What is the relationship between the identified acoustic features and specific cognitive domains affected in dementia?

## Limitations
- The 87.8% LOSO accuracy drops to 79.2% on the test set, suggesting potential overfitting to training data
- Full recordings introduce variability in feature extraction timing and duration normalization
- The study relies on a single dataset (ADReSS 2020) with limited demographic diversity
- High-dimensional feature space (6,923 features) relative to sample size may affect generalization
- The approach focuses solely on acoustic features, potentially missing complementary information from linguistic content or multimodal sources

## Confidence
- **High confidence**: Acoustic feature extraction methodology using openSMILE configurations is well-established and reproducible
- **Medium confidence**: Classification performance metrics are reported transparently, but generalization concerns exist due to LOSO vs test set discrepancy
- **Medium confidence**: Frequency-domain and MFCC features as most important is plausible but requires further validation in the 6,923-dimensional space

## Next Checks
1. Replicate the feature selection process with shuffled labels to establish baseline importance score distributions and verify statistical significance
2. Conduct ablation studies by systematically removing top-ranked features (frequency-domain and MFCC) to assess their actual contribution to classification performance
3. Test model generalization on an independent dataset with similar acoustic tasks to evaluate whether the 87.8% LOSO accuracy translates to real-world clinical settings