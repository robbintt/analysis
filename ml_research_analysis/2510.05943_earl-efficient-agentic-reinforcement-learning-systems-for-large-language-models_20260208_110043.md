---
ver: rpa2
title: 'EARL: Efficient Agentic Reinforcement Learning Systems for Large Language
  Models'
arxiv_id: '2510.05943'
source_url: https://arxiv.org/abs/2510.05943
tags:
- context
- length
- training
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EARL addresses scalability bottlenecks in agentic RL training by
  dynamically adapting parallelism configurations and optimizing data dispatch strategies.
  It employs a Parallelism Selector that adjusts tensor parallelism degrees based
  on context length and system load, and a Data Dispatcher that uses layout-aware,
  decentralized communication to reduce data movement overhead.
---

# EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models

## Quick Facts
- arXiv ID: 2510.05943
- Source URL: https://arxiv.org/abs/2510.05943
- Authors: Zheyue Tan; Mustapha Abdullahi; Tuo Shi; Huining Yuan; Zelai Xu; Chao Yu; Boxun Li; Bo Zhao
- Reference count: 37
- One-line result: EARL improves throughput by up to 11.2x in data dispatch latency and prevents OOM failures during long-context agentic RL training

## Executive Summary
EARL addresses scalability bottlenecks in agentic reinforcement learning for large language models by dynamically adapting parallelism configurations and optimizing data dispatch strategies. The system employs a Parallelism Selector that adjusts tensor parallelism degrees based on context length and system load, and a Data Dispatcher that uses layout-aware, decentralized communication to reduce data movement overhead. Experimental results show that EARL significantly improves throughput while preventing out-of-memory failures during long-context training, enabling stable large-scale training of agentic LLMs without hard context limits.

## Method Summary
EARL implements two key optimizations on top of the ROLL framework for agentic RL training. The Parallelism Selector profiles throughput across different tensor parallelism (TP) degrees and context lengths at startup, then dynamically switches configurations during training when entering new context ranges. The Data Dispatcher replaces centralized all-gather-and-scatter communication with decentralized all-to-all operations, routing intermediate tensors directly from computation origins to target workers based on layout patterns. The system was tested on Qwen2.5-72B-Instruct model using Connect Four game environment with context lengths from 1K to 32K tokens, employing REINFORCE advantage estimation.

## Key Results
- Data dispatch latency reduced by 9.8× to 11.2× across 8K-32K context lengths
- Throughput improvement of up to 31% at short contexts with TP=4 vs baseline
- OOM prevention during long-context training (32K) where fixed configurations failed
- Training stability maintained without hard context limits that introduce truncated reasoning

## Why This Works (Mechanism)

### Mechanism 1
Dynamic tensor parallelism selection based on context length improves throughput and prevents OOM failures during long-context training. The Parallelism Selector profiles throughput at startup across different TP degrees and context lengths, maintaining optimal configurations per context range. During training, it monitors average context length and switches TP configuration when entering a new range before the next Rollout stage. Core assumption: optimal parallelism configuration for a given context length range remains relatively stable during training.

### Mechanism 2
Replacing centralized all-gather-and-scatter with decentralized all-to-all communication reduces data dispatch latency. The Data Dispatcher sends intermediate tensors (log-probabilities, rewards, returns) directly from computation origins to target workers, eliminating the bottleneck of aggregating all data on a single node before redistribution. Core assumption: intermediate tensors have predictable layout patterns based on parallelism configuration, enabling direct routing without centralized coordination.

### Mechanism 3
Avoiding hard context limits preserves model performance potential during agentic RL training. By dynamically adapting system resources to growing context, EARL eliminates the need for truncation or length penalties that introduce "low-quality" truncated reasoning into rollouts. Core assumption: longer contexts during training correlate with improved reasoning capabilities, and truncation degrades learning signals.

## Foundational Learning

- **Tensor Parallelism (TP)**
  - Why needed: EARL dynamically adjusts TP degree (splitting model layers across GPUs) based on context length. Understanding TP is essential to interpret why TP=4 works for short contexts while TP=8 becomes necessary for long contexts.
  - Quick check: Can you explain why increasing TP degree reduces per-GPU memory usage but may increase communication overhead?

- **All-to-All vs. All-Gather Communication Patterns**
  - Why needed: The Data Dispatcher optimization relies on replacing centralized patterns with decentralized all-to-all. Understanding collective communication primitives is required to evaluate when this optimization applies.
  - Quick check: What is the difference in data movement complexity between all-gather (O(n×p) where p is number of processes) and all-to-all (O(n) per process)?

- **Agentic RL Training Loop**
  - Why needed: EARL intervenes at specific stages (Rollout, Experience Preparation, Model Update). Understanding the data flow between policy/reference/value/reward models is necessary to identify dispatch bottlenecks.
  - Quick check: In a standard PPO-style RL loop, which tensors must be exchanged between the rollout phase and the training update phase?

## Architecture Onboarding

- **Component map:**
  - Parallelism Selector -> Rollout Workers (dynamic TP configuration)
  - Experience Preparation Workers -> Data Dispatcher -> Model Update Workers (all-to-all routing)
  - Rollout Workers generate trajectories with configurable TP
  - Experience Preparation Workers run reference, value, and reward models with independent TP configuration

- **Critical path:**
  1. Monitor average context length during Rollout
  2. Before next iteration, Parallelism Selector checks if context range changed → trigger TP reconfiguration if needed
  3. Experience Preparation generates intermediate tensors (log-probs, rewards, returns)
  4. Data Dispatcher determines layout-aware routing based on current parallelism
  5. All-to-all transfer sends tensors directly to Model Update workers
  6. Gradient update proceeds with received batches

- **Design tradeoffs:**
  - Profiling overhead vs. adaptability: Pre-profiling all TP configurations adds startup cost but enables fast runtime decisions
  - TP granularity vs. reconfiguration cost: More context ranges mean more potential reconfigurations (current prototype uses TP=4 and TP=8 only)
  - Decentralization vs. dependency handling: All-to-all works for independent tensors (log-probs) but requires aggregation for advantage estimation tensors

- **Failure signatures:**
  - OOM during Rollout at high context lengths: TP degree too low for current context; Selector should have triggered switch
  - Stale profile data: If model or environment changes significantly, pre-profiled TP configs may be suboptimal
  - Dispatch deadlock: Incorrect layout metadata causing all-to-all misrouting (ensure consistent parallelism metadata across stages)
  - Throughput regression after TP switch: Reconfiguration overhead exceeds benefits; may indicate too-frequent switching

- **First 3 experiments:**
  1. Baseline comparison: Measure tokens-per-GPU-per-second (TGS) with fixed TP=4 across 1K-32K context lengths on Qwen2.5-72B; document OOM threshold
  2. Dynamic TP validation: Enable Parallelism Selector with TP={4,8}; verify context-range transitions trigger correctly and measure throughput speedup vs. baseline at each range
  3. Dispatch latency isolation: Profile data transfer time between Experience Preparation and Model Update at 8K, 16K, 32K contexts; compare all-gather-scatter vs. all-to-all on identical hardware

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic parallelism strategies be jointly optimized for both the memory-intensive Rollout stage and the compute-intensive Model Update stage without incurring prohibitive reconfiguration overheads? The authors note they have optimized only the Rollout stage without extending to the training stage, and achieving joint optimization requires a more comprehensive design.

### Open Question 2
How can dependency-heavy tensors required for advantage estimation (rewards and returns) be aggregated in a distributed manner to eliminate the centralized communication bottleneck? The current Data Dispatcher optimizes only for tensors with minimal inter-stage dependencies, while rewards and returns are aggregated centrally for advantage estimation.

### Open Question 3
Does integrating replay buffers and asynchronous scheduling into EARL further alleviate data dispatch bottlenecks by decoupling rollout generation from model updates? The authors list designing fully asynchronous RL systems and integrating replay buffers into off-policy training as specific directions for future work.

## Limitations
- Dynamic TP optimization limited to Rollout stage, not extended to training phase
- All-to-all optimization doesn't fully address aggregation requirements for advantage estimation
- Performance validation limited to single model architecture and game environment

## Confidence
- **High Confidence**: Fundamental problem identification (context length explosion causing OOM, data dispatch bottlenecks) is well-established and clearly demonstrated
- **Medium Confidence**: Mechanism designs (dynamic TP switching, all-to-all dispatch) are sound in principle and supported by experimental results
- **Medium Confidence**: Performance improvements demonstrated but only for specific experimental setup; broader applicability remains to be validated

## Next Checks
1. **Context Distribution Stability Test**: Run EARL with varying context length distributions during training to verify that pre-profiled TP configurations remain optimal under non-stationary conditions.

2. **Cross-Environment Generalization**: Evaluate EARL on a different agentic RL task to assess whether TP switching thresholds and all-to-all optimization generalize beyond Connect Four, particularly focusing on advantage estimation aggregation.

3. **Edge Case Communication Analysis**: Systematically test the Data Dispatcher with different tensor sizes and shapes at various parallelism degrees to identify potential deadlocks or performance regressions, particularly at TP switching boundaries.