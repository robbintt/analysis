---
ver: rpa2
title: 'M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical
  Imaging'
arxiv_id: '2502.20301'
source_url: https://arxiv.org/abs/2502.20301
tags:
- files
- dataset
- json
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M3Builder, a multi-agent system for automating
  machine learning workflows in medical imaging. The system uses four specialized
  agents (Task Manager, Data Engineer, Module Architect, and Model Trainer) that collaborate
  within a structured workspace containing datasets, code templates, and interaction
  tools.
---

# M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging

## Quick Facts
- arXiv ID: 2502.20301
- Source URL: https://arxiv.org/abs/2502.20301
- Reference count: 40
- Success rate: 94.29% task completion with Claude-3.7-Sonnet across 14 datasets

## Executive Summary
M3Builder is a multi-agent system that automates end-to-end machine learning workflows for medical imaging tasks. The system employs four specialized agents (Task Manager, Data Engineer, Module Architect, and Model Trainer) that collaborate within a structured workspace containing datasets, code templates, and interaction tools. Evaluated on M3Bench benchmark with 14 datasets covering five anatomies and three imaging modalities, the system demonstrates state-of-the-art performance with 94.29% success rate, significantly outperforming existing agentic systems through iterative auto-debugging and role-specialized error isolation.

## Method Summary
M3Builder uses a sequential four-agent architecture built on LangGraph framework. The Task Manager selects datasets and decomposes requirements, the Data Engineer generates JSON indices, the Module Architect creates data loaders, and the Model Trainer executes training with auto-debugging. The system provides eight interaction tools (file operations, script execution, directory previewing) and operates within a workspace containing datacards and code templates based on nnU-Net and Transformers. Agents communicate through structured messages and iteratively refine code based on execution feedback until successful model training.

## Key Results
- 94.29% success rate on 14 medical imaging tasks using Claude-3.7-Sonnet
- 39.29% success rate for single-agent system without multi-agent collaboration
- 21.43% success rate without auto-debugging capabilities

## Why This Works (Mechanism)

### Mechanism 1: Role-Specialized Error Isolation
Decomposing the medical imaging ML pipeline into distinct roles (Data Engineer, Architect, Trainer) reduces the error surface compared to monolithic single-agent systems. Each agent receives a specialized system prompt and restricted toolset, localizing errors so that if training fails, the Model Trainer attempts a fix without rewriting the entire data processing logic, preventing cascading hallucinations.

### Mechanism 2: Iterative Auto-Debugging via Compiler Feedback
The system's performance relies heavily on the feedback loop between code execution errors and LLM self-correction. The Model Trainer executes scripts and, if an error traceback occurs, reads the error, reflects, and uses edit_files to patch the code. This converts a generation problem into a refinement problem, with ablation showing catastrophic performance drops (21.43%) without debugging.

### Mechanism 3: Context Grounding via Structured Workspace
Providing agents with a workspace containing explicit dataset descriptions (datacards) and code templates constrains the solution space, reducing hallucinations about data formats. Instead of assuming data formats, agents use tools to inspect real metadata. The workspace enforces a standard interface, creating a "lingua franca" between agents that ensures compatibility with template assumptions.

## Foundational Learning

- **LangGraph / State Machine Orchestration**: The paper utilizes a sequential flow with langgraph. Understanding how to pass state between nodes is required to modify the agent topology. *Quick check: Can you trace how the train.json path generated by the Data Engineer is passed to the Module Architect?*

- **Medical Imaging Data Loaders (Monai/Nibabel)**: The Module Architect must generate code to load 3D NIfTI or 2D PNGs. Understanding dimension ordering is critical for debugging "shape mismatch" errors. *Quick check: If a 3D CT volume loads as (512, 512, 100), does the agent need to permute axes for a PyTorch model expecting (Batch, Channel, Depth, Height, Width)?*

- **Tool Use / Function Calling**: The agents interact with the OS via 8 distinct tools. Distinguishing when to preview_files (read partial) vs read_files (read full) is a key agent capability. *Quick check: Which tool should the agent use to verify the train.sh execution output?*

## Architecture Onboarding

- **Component map**: User Requirement → Task Manager → Data Engineer → Module Architect → Model Trainer → Successful Training
- **Critical path**: Dataset Selection (Task Manager parses descriptions.json) → Index Generation (Data Engineer generates train.json with correct paths) → Dataloader Validation (Module Architect produces script without IndexError) → Training Convergence (Model Trainer handles CUDA OOM and runs to completion)
- **Design tradeoffs**: Sequential vs. Parallel (strictly sequential simplifies state management but prevents parallelization), Template vs. Scratch (templates ensure stability but limit novel architecture generation), Token Limit (preview tools trade full context for token efficiency)
- **Failure signatures**: Infinite Debug Loops (Model Trainer edits same file repeatedly with 0 actions succeeding), Path Hallucination (agent writes code with literal paths instead of workspace variables), Dimension Mismatch (repeated shape errors in model_arch.py)
- **First 3 experiments**: 1) Baseline Validation: Run M3Builder on COVID19 dataset to verify 5/5 success rate, 2) Debug Ablation: Disable edit_files tool to confirm 0% performance without auto-debugging, 3) LLM Swap: Replace Claude-3.7-Sonnet with Llama-3.3-70B on BTCV task to measure tool-calling sensitivity

## Open Questions the Paper Calls Out
- Can integrating visual processing capabilities into agents improve performance to better approximate clinical expertise?
- Can the M3Builder framework effectively generalize to non-imaging medical tasks?
- To what extent can automated dataset preparation eliminate the need for human-curated datacards?

## Limitations
- Missing implementation details for code templates and model architectures make exact reproduction difficult
- System relies on human-generated datacards, limiting full autonomy
- Comparison to baselines lacks detailed failure mode analysis

## Confidence
- **High confidence**: Multi-agent architecture well-specified with strong ablation support (39.29% and 21.43% drops)
- **Medium confidence**: Generalizability claim supported by 14 datasets but limited by missing error analysis
- **Low confidence**: Baseline comparisons lack detailed explanation of why other systems failed completely

## Next Checks
1. **Template Compatibility Test**: Run M3Builder on 4D time-series data to verify graceful failure or need for workspace updates
2. **LLM Model Sensitivity**: Replace Claude-3.7-Sonnet with Llama-3.3-70B on BTCV task to measure tool-calling sensitivity
3. **Silent Error Detection**: Test system's ability to detect and correct logic errors that don't raise compiler exceptions