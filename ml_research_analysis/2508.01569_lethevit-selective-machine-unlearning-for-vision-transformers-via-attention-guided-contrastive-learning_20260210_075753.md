---
ver: rpa2
title: 'LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided
  Contrastive Learning'
arxiv_id: '2508.01569'
source_url: https://arxiv.org/abs/2508.01569
tags:
- samples
- unlearning
- data
- lethevit
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selective machine unlearning
  in Vision Transformers (ViTs) under privacy regulations like GDPR and CCPA, where
  specific data samples must be forgotten while preserving model performance on retained
  data. The authors propose LetheViT, a contrastive unlearning method that leverages
  attention-guided masking to identify and obscure critical image regions, guiding
  the model to forget targeted details while retaining class-level information.
---

# LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning

## Quick Facts
- arXiv ID: 2508.01569
- Source URL: https://arxiv.org/abs/2508.01569
- Reference count: 16
- Primary result: Achieves state-of-the-art selective unlearning with 0.57% average gap vs retraining

## Executive Summary
This paper addresses selective machine unlearning in Vision Transformers, where specific data samples must be forgotten while maintaining model performance on retained data. The authors propose LetheViT, a method that uses attention-guided masking combined with contrastive learning to selectively erase sample-specific information. The approach leverages attention maps to identify critical image regions, then uses masked and original images to generate positive and negative logits respectively, optimizing a contrastive loss to achieve targeted forgetting. Experiments demonstrate superior performance compared to existing methods, with particularly strong results in resisting membership inference attacks.

## Method Summary
LetheViT employs a two-stage approach for selective machine unlearning in Vision Transformers. First, it uses attention-guided masking to identify and obscure critical regions of target images based on attention maps. These masked images are then used to generate positive logits, while the original images generate negative logits. A contrastive loss function is optimized to encourage the model to produce similar outputs for masked versions while maintaining distinct outputs for original images. This process effectively guides the model to forget specific sample details while preserving class-level information and overall model performance on retained data.

## Key Results
- Achieves average gap of 0.57% compared to full retraining, significantly outperforming SalUn and â„“1-sparse methods
- Demonstrates superior performance in membership inference attack (MIA) suppression
- Shows effectiveness across multiple datasets and ViT variants, validating method robustness
- Maintains high performance on retained data while successfully forgetting targeted samples

## Why This Works (Mechanism)

The mechanism works through attention-guided contrastive learning that exploits the dual nature of visual information - both local details and global semantics. By masking attention-weighted regions identified as most critical for sample identification, LetheViT forces the model to rely on less distinctive features. The contrastive framework then reinforces this forgetting by training the model to produce similar predictions for masked (forgetting-targeted) and original images, effectively erasing sample-specific details while maintaining class-level information. This approach is particularly effective for Vision Transformers because attention mechanisms naturally highlight the most informative regions for classification, making them ideal targets for selective information removal.

## Foundational Learning

**Attention Mechanisms**: Essential for identifying which image regions contain the most discriminative information for sample identification. Understanding attention maps is crucial for the masking strategy. Quick check: Verify attention maps highlight semantically meaningful regions for the given task.

**Contrastive Learning**: Provides the framework for forcing similarity between masked and original representations while maintaining class distinctions. Needed to guide the forgetting process without explicit negative sample pairs. Quick check: Ensure contrastive loss effectively reduces sample-specific information while preserving class semantics.

**Vision Transformer Architecture**: Understanding ViT's self-attention mechanism and patch embedding process is critical since the method operates directly on these components. Quick check: Confirm the attention-guided masking respects ViT's patch structure and attention patterns.

## Architecture Onboarding

**Component Map**: Input Images -> Attention Map Extraction -> Critical Region Masking -> Contrastive Loss Optimization -> Updated Model Parameters

**Critical Path**: The most critical components are the attention map extraction and the contrastive loss optimization. The attention maps must accurately identify forget-worthy regions, and the contrastive loss must effectively guide the model toward forgetting those regions while preserving performance.

**Design Tradeoffs**: The method trades computational overhead during unlearning for reduced need for full retraining. The attention-guided approach may miss distributed information that isn't captured by attention weights, but provides more targeted forgetting than global approaches.

**Failure Signatures**: Poor attention map quality leads to ineffective masking and incomplete forgetting. If the contrastive loss is improperly weighted, the model may either fail to forget or lose too much general knowledge. Performance degradation on retained data indicates over-aggressive forgetting.

**3 First Experiments**:
1. Verify attention maps correctly identify semantically important regions by visualizing masked vs original image regions
2. Test contrastive loss behavior with varying temperature parameters to find optimal forgetting vs preservation balance
3. Evaluate forgetting efficacy on a small subset before full-scale unlearning to validate the approach

## Open Questions the Paper Calls Out
None

## Limitations
- May not perform optimally when unlearning targets share semantic similarities with retained data
- Assumes attention maps reliably identify all critical information for forgetting, which may not hold for distributed features
- Computational overhead during unlearning process not thoroughly characterized for practical deployment
- Evaluation framework may not fully represent real-world scenarios with non-IID data distributions or continuous unlearning requests

## Confidence

**Methodology Soundness**: High
**Experimental Coverage**: Medium
**Practical Applicability**: Medium

## Next Checks

1. Test LetheViT's performance on non-IID data distributions where unlearning samples are semantically similar to retained samples to evaluate robustness against semantic overlap.

2. Implement a continuous unlearning scenario where samples are forgotten incrementally over time rather than in batch mode to assess real-world applicability.

3. Conduct a comprehensive ablation study varying the attention masking threshold and contrastive loss weighting to understand the sensitivity of the method to hyperparameter choices.