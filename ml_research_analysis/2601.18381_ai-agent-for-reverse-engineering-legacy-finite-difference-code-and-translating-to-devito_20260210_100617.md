---
ver: rpa2
title: AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating
  to Devito
arxiv_id: '2601.18381'
source_url: https://arxiv.org/abs/2601.18381
tags:
- code
- devito
- retrieval
- knowledge
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops an AI agent system to automate the translation
  of legacy Fortran finite difference code into the modern Devito framework. The system
  integrates Retrieval-Augmented Generation (RAG) with open-source large language
  models within a multi-stage iterative workflow built on the LangGraph architecture.
---

# AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito
## Quick Facts
- arXiv ID: 2601.18381
- Source URL: https://arxiv.org/abs/2601.18381
- Reference count: 34
- Key outcome: AI agent system translates legacy Fortran finite-difference code to Devito with 76.9% Grade-A success rate and 0.964 Precision@5 in retrieval

## Executive Summary
This paper presents an AI agent system that automates the translation of legacy Fortran finite-difference code into the modern Devito framework. The system integrates Retrieval-Augmented Generation (RAG) with open-source large language models within a multi-stage iterative workflow built on the LangGraph architecture. A Devito knowledge graph, constructed from documentation and source code, is leveraged by GraphRAG for efficient retrieval. The agent employs static analysis of Fortran code to generate targeted queries, retrieves relevant Devito examples, and synthesizes new code under Pydantic-structured output constraints. Quality validation combines conventional static analysis with LLM-based evaluation (G-Eval), covering execution correctness, structural integrity, API compliance, parameter consistency, and mathematical fidelity. The system demonstrates high retrieval accuracy (Precision@5 = 0.964, Recall@5 = 0.930) and strong translation performance (Grade-A success rate = 76.9%) across 13 test cases, achieving near-perfect scores in execution, structure, and API usage.

## Method Summary
The system integrates Retrieval-Augmented Generation (RAG) with open-source large language models within a multi-stage iterative workflow built on the LangGraph architecture. A Devito knowledge graph, constructed from documentation and source code, is leveraged by GraphRAG for efficient retrieval. The agent employs static analysis of Fortran code to generate targeted queries, retrieves relevant Devito examples, and synthesizes new code under Pydantic-structured output constraints. Quality validation combines conventional static analysis with LLM-based evaluation (G-Eval), covering execution correctness, structural integrity, API compliance, parameter consistency, and mathematical fidelity. The system demonstrates high retrieval accuracy (Precision@5 = 0.964, Recall@5 = 0.930) and strong translation performance (Grade-A success rate = 76.9%) across 13 test cases, achieving near-perfect scores in execution, structure, and API usage. The agent workflow supports concurrent processing and dynamic routing for iterative refinement, advancing beyond static translation toward adaptive analytical behavior.

## Key Results
- Retrieval performance: Precision@5 = 0.964, Recall@5 = 0.930
- Translation success: Grade-A rate = 76.9% across 13 test cases
- Quality metrics: Near-perfect scores in execution correctness, structural integrity, and API compliance

## Why This Works (Mechanism)
The system's effectiveness stems from integrating GraphRAG with a Devito-specific knowledge graph, enabling precise retrieval of relevant examples from documentation and source code. Static analysis of Fortran code generates targeted queries that improve retrieval relevance. Pydantic-structured output constraints ensure syntactically valid Devito code generation. The multi-stage iterative workflow with concurrent processing and dynamic routing allows for adaptive refinement of translations. LLM-based G-Eval provides comprehensive quality assessment beyond traditional static analysis, validating mathematical fidelity and parameter consistency alongside structural and execution correctness.

## Foundational Learning
- **GraphRAG**: Graph-based retrieval augmented generation that improves over traditional RAG by leveraging knowledge graph structure for more precise information retrieval
  - Why needed: Standard RAG struggles with complex domain-specific queries requiring relationships between concepts
  - Quick check: Verify retrieval accuracy improves when using knowledge graph relationships vs. keyword matching alone

- **LangGraph architecture**: Framework for building multi-agent workflows with stateful execution and dynamic routing capabilities
  - Why needed: Enables complex multi-stage translation workflows with concurrent processing and iterative refinement
  - Quick check: Confirm agent state transitions follow expected workflow paths

- **Pydantic-structured output**: Schema validation for generated code ensuring syntactic and structural correctness
  - Why needed: Guarantees generated code adheres to Devito API requirements and Python syntax
  - Quick check: Validate all generated code passes Pydantic schema validation

- **G-Eval (LLM-based evaluation)**: AI-powered quality assessment that evaluates code correctness, structure, and mathematical fidelity
  - Why needed: Provides comprehensive validation beyond traditional static analysis tools
  - Quick check: Compare G-Eval scores with ground truth execution results

## Architecture Onboarding
- **Component map**: Fortran Code -> Static Analyzer -> Query Generator -> GraphRAG -> Knowledge Graph -> LLM Generator -> Pydantic Validator -> G-Eval -> Output
- **Critical path**: Static analysis → query generation → GraphRAG retrieval → code synthesis → validation → output
- **Design tradeoffs**: Open-source LLMs vs. proprietary models (cost vs. performance), GraphRAG complexity vs. retrieval accuracy, concurrent processing overhead vs. translation speed
- **Failure signatures**: Low retrieval precision indicates knowledge graph gaps, validation failures suggest schema mismatches, poor G-Eval scores indicate semantic errors
- **First experiments**:
  1. Test retrieval accuracy with varying query complexity to establish GraphRAG performance baseline
  2. Validate Pydantic schema coverage against diverse Devito code patterns
  3. Benchmark G-Eval evaluation time and accuracy against traditional static analysis tools

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 13 test cases of modest complexity, with unverified performance on large-scale industrial legacy codes
- Reliance on open-source LLMs and GraphRAG may not generalize across all scientific computing domains or codebases
- Pydantic-structured output may be insufficient for complex transformations requiring deeper semantic understanding beyond the knowledge graph

## Confidence
- High: Retrieval metrics (Precision@5, Recall@5) and execution/structure/API compliance results
- Medium: Grade-A success rate (76.9%) given small sample size and potential selection bias
- Low: Claims about adaptability to other legacy codebases or scientific domains without external validation

## Next Checks
1. Test the agent on a larger, more diverse corpus of legacy Fortran codes, including industrial-scale examples, to assess scalability and robustness
2. Perform ablation studies to quantify the impact of GraphRAG vs. standard RAG, and open-source vs. proprietary LLMs, on translation accuracy and efficiency
3. Validate the system's performance on non-Fourier finite-difference codes (e.g., Navier-Stokes, Maxwell's equations) to confirm domain generalizability