---
ver: rpa2
title: 'Position: Foundation Models Need Digital Twin Representations'
arxiv_id: '2505.03798'
source_url: https://arxiv.org/abs/2505.03798
tags:
- representations
- arxiv
- token
- digital
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies inherent limitations of current foundation
  models (FMs) using token representations, including poor semantic coherence across
  modalities, inability to capture fine-grained spatial-temporal dynamics, and limited
  causal reasoning capabilities. These limitations stem from token representations
  fragmenting continuous real-world data into discrete tokens, forcing FMs to learn
  physical relationships through statistical correlation rather than explicit domain
  knowledge.
---

# Position: Foundation Models Need Digital Twin Representations

## Quick Facts
- arXiv ID: 2505.03798
- Source URL: https://arxiv.org/abs/2505.03798
- Authors: Yiqing Shen; Hao Ding; Lalithkumar Seenivasan; Tianmin Shu; Mathias Unberath
- Reference count: 40
- Primary result: Foundation models using token representations suffer from poor semantic coherence across modalities, inability to capture fine-grained spatial-temporal dynamics, and limited causal reasoning capabilities

## Executive Summary
This position paper argues that current foundation models (FMs) built on token representations have fundamental limitations in capturing the continuous, causally structured nature of real-world processes. The authors identify key shortcomings including fragmented semantic coherence across modalities, inability to capture fine-grained spatial-temporal dynamics, and poor causal reasoning capabilities. They propose digital twin (DT) representations as an alternative paradigm where representations are outcome-driven digital replicas that explicitly encode task-specific entities, interactions, and physical constraints. DT representations preserve the continuous nature of real-world processes and maintain domain-specific knowledge, enabling better generalization, improved causal reasoning, and enhanced interpretability compared to token-based approaches.

## Method Summary
The paper proposes a new representation paradigm R = Φ(P, S) = Φₑ(S, Φd(P)), where Φd is outcome-driven representation design and Φₑ is representation extraction. Unlike token representations that uniformly partition data, DT representations are constructed according to task-specific requirements and explicitly encode physical constraints, geometric relationships, and causal mechanisms. The framework separates representation design from extraction, allowing foundation models to operate directly on physically grounded representations rather than rediscovering established principles from data. The authors demonstrate through various examples how DT representations can bridge the sim-to-real gap, enable more practical data synthesis at scale, and provide unified semantic representations across modalities while reducing hallucination in FMs.

## Key Results
- Token representations fragment continuous real-world data into discrete tokens, forcing FMs to learn physical relationships through statistical correlation rather than explicit domain knowledge
- DT representations preserve continuous causal chains and physical processes, maintaining cause-and-effect relationships that token discretization typically fragments
- Unified semantic space in DT representations enables counterfactual reasoning by ensuring generated scenarios remain physically consistent
- Explicit domain knowledge encoding in DT representations reduces learning complexity compared to statistical pattern discovery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit domain knowledge encoding reduces learning complexity compared to statistical pattern discovery
- **Mechanism:** DT representations encode task-specific properties (identification, geometric properties, physical constraints) through a layered framework where geometric relationships become spatial transforms and semantic knowledge becomes graphs with defined predicates. The processing pipeline R = Φ(P, S) separates outcome-driven representation design (Φd) from representation extraction (Φe), enabling FMs to operate directly on physically grounded representations rather than rediscovering established principles from data.
- **Core assumption:** Domain-specific physical constraints and causal relationships can be explicitly and accurately formalized into representation structures.
- **Evidence anchors:**
  - [abstract] "DT representations preserve the continuous nature of real-world processes and maintain domain-specific knowledge"
  - [Section 4.1] "AlphaGeometry had to rediscover millions of theorems and proofs that were already known to geometry literature"
  - [corpus] Weak direct evidence—corpus focuses on FM applications rather than representation alternatives
- **Break condition:** If physical constraints cannot be reliably formalized for complex domains, or if extraction algorithms fail to accurately capture task-specific entities from raw sensor data

### Mechanism 2
- **Claim:** Outcome-driven representation design enables better generalization through physically valid state spaces
- **Mechanism:** Unlike token representations that uniformly partition data (e.g., ViT patches may contain no entities or multiple entities), DT representations are constructed according to task-specific requirements. This preserves the continuous nature of causal chains and physical processes, maintaining cause-and-effect relationships that token discretization typically fragments. The unified semantic space enables counterfactual reasoning by ensuring generated scenarios remain physically consistent.
- **Core assumption:** Outcome-driven design can systematically outperform uniform tokenization across diverse task structures
- **Evidence anchors:**
  - [abstract] "representations are outcome-driven digital replicas that explicitly encode task-specific entities, interactions, and physical constraints"
  - [Section 4.3] "DT representations enable counterfactual reasoning by maintaining physically valid state spaces"
  - [corpus] [89620] describes world models for Physical AI using digital twins, supporting the paradigm
- **Break condition:** If outcome-driven extraction proves computationally prohibitive at scale, or if task-specific customization prevents transfer learning benefits

### Mechanism 3
- **Claim:** Continuous representations maintain cross-modal semantic coherence that discrete tokens fragment
- **Mechanism:** Token representations create artificial boundaries between modalities (visual patches vs. text tokens differ in semantic levels and granularities). DT representations provide a unified semantic space grounded in physical reality—encoding information through shared physical and geometric properties rather than learned statistical correlations. Methods like NeRFs demonstrate unified 3D representations maintaining consistency across views and modalities.
- **Core assumption:** Modality semantics can converge to shared physical properties rather than requiring modality-specific processing
- **Evidence anchors:**
  - [abstract] "poor semantic coherence across modalities" identified as key limitation of token representations
  - [Section 4.4] "visual tokens capture low-level pixel patterns while text tokens encode linguistic structures, with no inherent mechanism to maintain their interconnections"
  - [corpus] [79028] discusses AI integration with digital twins but doesn't directly validate unified representations
- **Break condition:** If unified representations lose critical modality-specific information, or if convergence to shared physical properties proves impossible for abstract domains

## Foundational Learning

- **Tokenization paradigms in Transformers**
  - Why needed here: Understanding how current FMs fragment continuous data into discrete sequences (BPE for text, patches for images, frames for video) reveals why DT representations propose an alternative encoding strategy
  - Quick check question: Can you explain why patch-based image tokenization may mix multiple entities or capture no entities in a single patch?

- **Digital Twin architecture patterns**
  - Why needed here: The paper distinguishes DTs from digital models and digital shadows by requiring automatic cyclic data flows—understanding this distinction clarifies what makes DT representations "outcome-driven"
  - Quick check question: What is the difference between a digital model, digital shadow, and digital twin in terms of data flow?

- **Causal inference fundamentals (correlation vs. causation)**
  - Why needed here: The paper argues tokens force FMs to learn causal relationships from observational data alone, violating fundamental causal inference principles
  - Quick check question: Why does learning from observational data violate the principle that "correlation does not imply causation"?

## Architecture Onboarding

- **Component map:**
  Physical Process P → Sensors → Raw Data S
  Outcome-driven representation design (Φd) → Representation extraction (Φe) → DT representations R
  Foundation Model Θ → Outputs T
  Feedback loop: DT model sends feedback to physical process

- **Critical path:**
  1. Define task-specific requirements (what entities, interactions, constraints matter?)
  2. Design representation schema (geometric, semantic, physical properties)
  3. Implement extraction algorithms (perception modules, sensor fusion)
  4. Validate physical consistency (do representations maintain domain constraints?)
  5. Integrate with FM architecture (modify input layers to accept DT representations)

- **Design tradeoffs:**
  - Computational overhead of extraction vs. reduced FM training data requirements
  - Task-specific optimization vs. general-purpose applicability
  - Explicit physical modeling vs. learned abstractions
  - Interpretability gains vs. implementation complexity

- **Failure signatures:**
  - Extraction algorithms fail to identify task-specific entities accurately
  - DT representations still lose critical physical relationships
  - Sim-to-real gap persists despite physically grounded representations
  - Causal reasoning degrades on out-of-distribution scenarios

- **First 3 experiments:**
  1. **Domain comparison experiment:** Implement DT representations for a robotics manipulation task (e.g., pouring). Compare against token-based FM on physical consistency metrics (mass conservation, trajectory validity). Use the peg transfer example from [23] as reference.
  2. **Synthetic data validation:** Generate synthetic scenarios using DT representations and token-based generators. Measure physical validity and sim-to-real transfer performance. Reference TWICE dataset [62] methodology for adverse weather simulation.
  3. **Causal reasoning benchmark:** Design counterfactual reasoning tasks where DT representations encode explicit causal mechanisms. Compare against token-based FMs on causal inference tasks. Use Jin et al. [36] evaluation approach showing "almost random performance on pure causal inference tasks" as baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can efficient frameworks be developed to construct and maintain digital twin representations at scale?
- Basis in paper: [explicit] The conclusion explicitly lists "the development of efficient frameworks for constructing and maintaining DT representations at scale" as a future research direction.
- Why unresolved: The paper acknowledges in the "Alternative Views" section that the computational overhead of maintaining and processing outcome-driven representations could outweigh their benefits compared to the simplicity of token representations.
- What evidence would resolve it: The development of an automated pipeline that extracts DT representations from raw data with computational efficiency comparable to current tokenization methods, while successfully handling massive, diverse multimodal datasets.

### Open Question 2
- Question: What are the theoretical foundations regarding the generalization and physical consistency properties of digital twin representations?
- Basis in paper: [explicit] The conclusion calls for "theoretical foundations for understanding their properties," specifically regarding how they encode physical constraints compared to statistical patterns.
- Why unresolved: The paper relies on "Assumptions" (1-5) to argue that DT representations enable better generalization and causal reasoning, but currently lacks formal mathematical proof that these representations guarantee physical validity better than scaled token models.
- What evidence would resolve it: Formal proofs demonstrating that outcome-driven representations mathematically preserve specific invariants (e.g., conservation laws, geometric relationships) during model processing, unlike token embeddings.

### Open Question 3
- Question: Can hybrid architectures successfully combine the scalability of token representations with the physical grounding of digital twin representations?
- Basis in paper: [explicit] The conclusion identifies "hybrid architectures that can combine the strengths of both DT and token representations" as a necessary area of future exploration.
- Why unresolved: It is currently unclear how to interface continuous, physics-aware DT representations with the discrete, high-dimensional latent spaces used by current Large Language Models without creating semantic mismatches or losing the benefits of standard pre-training.
- What evidence would resolve it: The implementation of a hybrid model that maintains the linguistic fluency of standard LLMs while significantly outperforming them on benchmarks requiring spatial-temporal dynamics and physical reasoning.

### Open Question 4
- Question: What specialized evaluation benchmarks are required to validate the specific advantages of digital twin representations over token representations?
- Basis in paper: [explicit] The paper explicitly notes the need for "specialized evaluation benchmarks" in the conclusion to assess the unique capabilities of this paradigm.
- Why unresolved: Current benchmarks focus largely on semantic understanding and pattern recognition; they fail to penalize models for violations of physical laws or causal inconsistencies, making it difficult to quantify the proposed advantages of DT representations.
- What evidence would resolve it: The creation and adoption of benchmark datasets that specifically test for fine-grained causal reasoning, counterfactual prediction, and physical consistency, where token-based models are shown to fail while DT-based models succeed.

## Limitations
- No concrete implementation of Φd (representation design) or Φₑ (extraction) algorithms provided—only conceptual descriptions
- Absence of specific foundation model architecture, training procedure, loss functions, or hyperparameters for validating DT representations
- No evaluation benchmarks or datasets explicitly defined for comparing DT vs. token representations

## Confidence
- **High confidence:** The identification of current token representation limitations (semantic fragmentation, poor causal reasoning, modality incoherence) is well-supported by existing literature and the logical consequences of discrete tokenization
- **Medium confidence:** The theoretical benefits of DT representations (preserved continuity, explicit domain knowledge encoding, unified semantics) follow logically from the proposed mechanism but lack empirical validation
- **Low confidence:** The practical feasibility of implementing DT representations at scale remains uncertain due to unquantified computational costs and challenges in formalizing physical constraints for complex domains

## Next Checks
1. **Cross-domain generalization experiment:** Implement DT representations for two distinct domains (e.g., robotics manipulation and autonomous driving). Measure whether representations from one domain transfer to related tasks in another domain, addressing the task-specific optimization vs. general-purpose applicability tradeoff.

2. **Computational overhead benchmark:** Quantify the real-time processing overhead of DT representation extraction compared to standard tokenization. Measure whether the claimed FM training efficiency gains offset the extraction costs, particularly for edge computing scenarios.

3. **Abstract domain feasibility test:** Attempt to apply DT representations to an inherently abstract domain (e.g., financial time series or social network analysis) where physical constraints are less obvious. This validates whether the representation paradigm extends beyond physically grounded domains or breaks down when causal mechanisms are difficult to formalize.