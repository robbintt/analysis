---
ver: rpa2
title: 'MLLMs Know Where to Look: Training-free Perception of Small Visual Details
  with Multimodal LLMs'
arxiv_id: '2502.17422'
source_url: https://arxiv.org/abs/2502.17422
tags:
- visual
- image
- mllms
- vicrop
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLLMs exhibit a significant sensitivity to the size of visual concepts
  in images, with smaller details leading to notably lower accuracy. This effect is
  shown to be causal through an intervention study using visual cropping.
---

# MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs

## Quick Facts
- arXiv ID: 2502.17422
- Source URL: https://arxiv.org/abs/2502.17422
- Reference count: 20
- Small visual details significantly degrade MLLM accuracy on VQA tasks, and this effect is causal.

## Executive Summary
MLLMs exhibit a strong sensitivity to the size of visual concepts in images, with smaller details leading to notably lower accuracy on VQA tasks. Through intervention studies using visual cropping, the authors show this effect is causal, not merely correlational. Crucially, analysis of attention maps reveals that MLLMs consistently know where to look—even when answering incorrectly—indicating the issue lies in perceiving details rather than localizing objects. To address this, the authors propose three training-free visual cropping methods that leverage the model's internal attention and gradient maps to focus on relevant regions, significantly improving accuracy on detail-sensitive tasks without requiring any training.

## Method Summary
The paper introduces ViCrop, a training-free visual cropping framework that improves MLLM perception of small visual details by leveraging the model's own internal knowledge. Three methods are proposed: rel-att (attention-based relative importance), grad-att (gradient-weighted attention), and pure-grad (gradient-based pixel importance). These methods extract importance maps from the MLLM's attention or gradient signals, crop the highest-importance region, re-encode it at full resolution, and concatenate it with original image tokens. Applied to two popular MLLMs across seven VQA benchmarks, ViCrop methods significantly improve accuracy on detail-sensitive tasks without any parameter updates.

## Key Results
- MLLM accuracy on VQA tasks decreases significantly as the relative size of visual concepts decreases.
- Ground-truth cropping intervention reverses the size-accuracy relationship, improving small-partition accuracy by 31-44 percentage points.
- rel-att and grad-att cropping methods improve TextVQA accuracy by 7.37-11.64 percentage points on LLaVA-1.5 and 11.96-17.96 points on InstructBLIP.
- MLLMs consistently attend to question-relevant regions even when answering incorrectly, indicating perception failure rather than localization failure.

## Why This Works (Mechanism)

### Mechanism 1: Visual Concept Size Causally Determines Perception Accuracy
When visual details occupy fewer image patches after ViT encoding, their representation is distributed across fewer tokens with lower spatial resolution. The connector then compresses these already-sparse signals into a fixed number of image tokens for the LLM, causing small objects to be under-represented in the final embedding space. Ground-truth cropping intervention reverses this effect, improving small-partition accuracy by 31-44 percentage points.

### Mechanism 2: Localization-Precision Separation (MLLMs Attend Correctly but Perceive Incorrectly)
Cross-attention layers in both the connector and LLM aggregate spatial information progressively. Early layers localize relevant patches based on semantic similarity to the question embedding, but later layers fail to extract sufficient fine-grained features from those patches to discriminate among similar visual concepts. Attention weights reflect where the model "intends" to look, but do not guarantee sufficient feature resolution at those locations.

### Mechanism 3: Internal State-Guided Cropping Amplifies Perceptual Resolution Without Training
Extracting importance maps from an MLLM's own attention or gradients, then cropping to high-importance regions and re-encoding at full resolution, improves accuracy on detail-sensitive tasks without any parameter updates. The importance map identifies regions the model already "wants" to attend to. By cropping and re-encoding these regions at the model's native input resolution, small objects now occupy proportionally more patches, receiving higher-resolution representations.

## Foundational Learning

- **Vision Transformer (ViT) Patch Embedding**
  - Why needed here: The entire size-sensitivity phenomenon stems from how ViT divides images into fixed-size patches (N×N). Understanding that smaller objects = fewer patches = less information is essential for diagnosing the perception bottleneck.
  - Quick check question: If an image is 336×336 and patch size is 14×14, how many patches does a 28×28 object occupy? (Answer: 2×2 = 4 patches)

- **Cross-Attention in Multimodal Transformers**
  - Why needed here: The paper's attention analysis relies on extracting cross-attention weights between answer tokens and image tokens. Without understanding query-key-value mechanics, the "attention ratio" metric is opaque.
  - Quick check question: In cross-attention, what does the query come from and what do key/value come from when computing answer-to-token attention? (Answer: query from answer token; key/value from image tokens)

- **Gradient-Based Saliency (Grad-CAM Intuition)**
  - Why needed here: The grad-att and pure-grad methods compute gradients of the output logit w.r.t. attention weights or input pixels. Understanding why gradients indicate "importance" is critical for interpreting these methods.
  - Quick check question: Why are negative gradients suppressed in grad-att? (Answer: They correspond to tokens that, if attended to more, would reduce model confidence—often distractors.)

## Architecture Onboarding

- **Component map:**
  [Image] → ViT Encoder (N×N patches) → [ViT output tokens] → [Connector: Transformer or MLP] → [Image tokens for LLM] ← concatenated with [Question tokens] → [LLM backbone] → [Answer tokens] → [Cross-attention layers] → [Answer-to-token attention (Ast) + Token-to-image attention (Ati)] → [Importance map] → [Crop selector] → [Cropped image] → [Re-encoded and concatenated]

- **Critical path:** The two-stage forward pass: (1) initial pass computes importance map from attention/gradients, (2) cropped image is re-encoded and concatenated with original tokens for final answer generation. The crop quality depends entirely on layer selection for attention extraction.

- **Design tradeoffs:**
  - rel-att vs. grad-att vs. pure-grad: rel-att requires a second forward pass with generic instruction for normalization; grad-att requires backward pass for gradients but no second forward; pure-grad is architecture-agnostic but requires edge-filtering heuristics.
  - Single-crop vs. multi-crop: Current implementation crops one region; questions requiring multiple objects (counting, relations) may be harmed. Paper notes this as a limitation.
  - Layer selection: Selective layer (chosen via held-out validation) slightly outperforms averaging across layers for grad-att; rel-att is robust to either.

- **Failure signatures:**
  - Accuracy degrades on questions about spatial relations ("left of", "behind")—cropping loses global context.
  - Questions about counting may fail if multiple instances of the target object exist outside the crop.
  - For InstructBLIP (connector-only training), gains are smaller than LLaVA-1.5—LLM hasn't learned to leverage additional cropped tokens.

- **First 3 experiments:**
  1. **Baseline size-sensitivity reproduction:** Run TextVQA partitioned by bbox size (small/medium/large) on your target MLLM without cropping. Confirm accuracy drops as size decreases.
  2. **Attention ratio sanity check:** For 20 manually inspected failure cases, visualize relative attention maps (Arel) at layers m=14 (LLaVA-1.5) or m=15, k=2 (InstructBLIP). Verify that high-attention regions overlap ground-truth bboxes even when answers are wrong.
  3. **ViCrop ablation on held-out set:** Implement rel-att with sliding-window crop selection. Compare: (a) no cropping, (b) random cropping, (c) rel-att cropping. Measure accuracy delta on TextVQA-small and GQA (to check for global-context degradation).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can visual cropping methods be extended to handle questions that require reasoning about multiple regions simultaneously, such as spatial relations or counting?
  - Basis in paper: The authors state in the Limitations section that "questions concerning relations and counting are particularly difficult for ViCrop methods" because the method "can only focus on one region."
  - Why unresolved: The current implementation selects a single bounding box with the highest attention/gradient sum, effectively discarding other relevant visual information required for multi-object reasoning.
  - What evidence would resolve it: A modified ViCrop mechanism that identifies and fuses multiple disjoint image regions, showing improved performance on relation-heavy or counting-specific VQA benchmarks compared to the single-crop approach.

- **Open Question 2:** Can the distinct visual cropping methods (rel-att, grad-att, pure-grad) be combined effectively, for instance by utilizing prediction uncertainty?
  - Basis in paper: The paper notes: "We have observed that the proposed methods tend to have some complementary benefits, and therefore exploring ways to combine them (for example based on the prediction uncertainty) is also a very interesting direction."
  - Why unresolved: The study evaluates the three methods individually against baselines but does not explore an ensemble approach or a dynamic selection strategy to leverage their specific strengths.
  - What evidence would resolve it: A unified model or meta-classifier that selects the optimal cropping method per sample, demonstrating higher aggregate accuracy than any single method in isolation.

- **Open Question 3:** Can the inference-time overhead of ViCrop be optimized using techniques like the Matryoshka Query Transformer (MQT) or weight quantization without losing perceptual gains?
  - Basis in paper: The authors identify a limitation regarding "time overhead and the addition of visual tokens," suggesting that "it can be significantly optimized... by utilizing lower precision, and weight quantization" or MQT.
  - Why unresolved: While the current overhead is deemed reasonable (1-2 seconds on GPU), the feasibility and impact of these specific optimization techniques on the cropping mechanism remain untested.
  - What evidence would resolve it: Benchmarks showing that a quantized or MQT-integrated ViCrop implementation maintains the accuracy improvements of the full model while reducing latency to a negligible fraction of the standard inference time.

## Limitations

- ViCrop focuses on single regions, making it ineffective for questions requiring spatial relations or counting across multiple objects.
- The inference-time overhead (1-2 seconds on GPU) may be prohibitive for real-time applications without optimization techniques.
- Accuracy gains are smaller for InstructBLIP (connector-only training) because its frozen LLM doesn't effectively utilize additional cropped image tokens.

## Confidence

- **High confidence**: The size-accuracy correlation is robustly demonstrated across seven benchmarks and three MLLM architectures. The ground-truth cropping intervention provides strong causal evidence that smaller objects are harder to perceive.
- **Medium confidence**: The attention analysis correctly identifies that MLLMs localize question-relevant regions, but the claim that this proves "perception failure rather than localization failure" assumes attention weights are functionally meaningful rather than epiphenomenal.
- **Medium confidence**: ViCrop methods achieve consistent accuracy gains, but the selective layer choice (m=14 vs. m=15, k=2) is based on held-out validation without reporting selection criteria or variance, making it unclear how robust this choice is across different datasets or MLLM versions.

## Next Checks

1. **Attention perturbation test**: For 50 failure cases, occlude the top-20% attended image patches identified by rel-att and re-run inference. If accuracy drops significantly compared to random occlusion, this confirms attention weights are causally linked to perception rather than rationalization.

2. **Multi-object robustness test**: Create a synthetic VQA dataset with questions requiring counting or spatial relations (e.g., "How many red objects are behind the blue cube?"). Apply ViCrop and measure accuracy degradation relative to the full-image baseline to quantify the single-crop limitation.

3. **Architectural ablation**: Repeat the size-sensitivity analysis on an MLLM using a variable-resolution backbone (e.g., Swin Transformer) instead of standard ViT. If the size-accuracy correlation weakens or disappears, this would confirm that the perception bottleneck is primarily an artifact of fixed patch size rather than an inherent limitation of MLLM design.