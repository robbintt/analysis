---
ver: rpa2
title: Exploring State Tracking Capabilities of Large Language Models
arxiv_id: '2511.10457'
source_url: https://arxiv.org/abs/2511.10457
tags:
- person
- position
- state
- moves
- swap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates state tracking capabilities of large language
  models (LLMs) on three carefully designed tasks: LinearWorld (tracking entity positions
  after swaps), HandSwap (tracking object exchanges), and Lights (tracking light states
  after switch presses). The authors assess performance across different depths of
  update sequences, comparing models with and without Chain-of-Thought (CoT) prompting.'
---

# Exploring State Tracking Capabilities of Large Language Models

## Quick Facts
- **arXiv ID:** 2511.10457
- **Source URL:** https://arxiv.org/abs/2511.10457
- **Reference count:** 40
- **Primary result:** LLMs struggle with state tracking through sequential updates unless provided explicit reasoning mechanisms

## Executive Summary
This paper systematically evaluates how well large language models can track entity states through sequential updates using three carefully designed tasks: LinearWorld (tracking entity positions after swaps), HandSwap (tracking object exchanges), and Lights (tracking light states after switch presses). The authors assess performance across different depths of update sequences and compare models with and without Chain-of-Thought (CoT) prompting. Results demonstrate that newer models maintain accurate state tracking at higher depths when given CoT, while older and smaller models degrade significantly after 2-3 steps.

The study reveals a fundamental limitation in LLM state tracking capabilities, showing that models struggle to maintain state through sequential updates unless provided explicit reasoning mechanisms. CoT substantially improves performance by allowing models to use their input window as temporary memory. This finding highlights the importance of memory integration and reasoning capabilities for state tracking tasks, suggesting that current LLMs require architectural enhancements or prompting strategies to handle complex sequential reasoning effectively.

## Method Summary
The authors designed three synthetic tasks to evaluate state tracking: LinearWorld involves tracking entity positions after swaps in a linear arrangement, HandSwap tests object exchange tracking, and Lights evaluates light state tracking after switch presses. Each task includes sequences of updates at varying depths, from simple single-step changes to complex multi-step transformations. Models are evaluated with and without Chain-of-Thought prompting, where CoT allows step-by-step reasoning. Performance is measured by accuracy in tracking final states after update sequences, with results compared across different model families including GPT-3.5, GPT-4o, Llama3-8B, Llama3-70B, and Mixtral.

## Key Results
- Newer models (GPT-4o, Llama3-70B with CoT) maintain accurate state tracking even at higher depths
- Older models (GPT-3.5, Mixtral) and smaller models (Llama3-8B) degrade significantly after 2-3 steps
- CoT substantially improves performance by allowing models to use input window as temporary memory

## Why This Works (Mechanism)
State tracking requires maintaining and updating representations of entity states across sequential transformations. Without explicit reasoning mechanisms, LLMs appear to compress or lose intermediate state information, leading to error accumulation. Chain-of-Thought prompting provides a workaround by externalizing intermediate reasoning steps into the input context, effectively creating a working memory buffer that preserves state information throughout the sequence.

## Foundational Learning
- **Sequential state updates**: Understanding how entity states change through deterministic rules is essential for tracking transformations over time
- **Memory limitations in LLMs**: Current models have constrained context windows that limit their ability to maintain long-term state information
- **Chain-of-Thought reasoning**: Explicit step-by-step reasoning helps models maintain intermediate states by externalizing working memory
- **Depth-dependent performance**: State tracking accuracy degrades predictably as sequence length increases
- **Model size correlation**: Larger models with CoT generally outperform smaller models and those without reasoning steps
- **Task generalization**: Performance on synthetic state tracking tasks may not directly transfer to complex real-world scenarios

## Architecture Onboarding

**Component Map:** Input sequence → LLM (with/without CoT) → State prediction → Accuracy evaluation

**Critical Path:** The evaluation pipeline processes sequential update instructions through the LLM, which either directly predicts final states or generates intermediate reasoning steps before producing predictions.

**Design Tradeoffs:** CoT prompting improves accuracy but increases computational cost and token usage; synthetic tasks provide controlled evaluation but may not reflect real-world complexity; focusing on discrete state spaces simplifies measurement but limits generalizability.

**Failure Signatures:** Performance degradation at sequence depths beyond 2-3 steps indicates working memory limitations; inconsistent predictions across similar task types suggest model-specific reasoning challenges; CoT-dependent improvements reveal lack of intrinsic state tracking capability.

**First Experiments:** 1) Test zero-shot generalization with novel state transitions not seen during training; 2) Evaluate interference effects when tracking multiple state variables simultaneously; 3) Compare CoT performance against memory-augmented architectures for the same tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic tasks with discrete state spaces may not reflect real-world complexity involving continuous variables
- Single-turn updates examined rather than iterative dialogue or multi-turn reasoning contexts
- Fixed prompt templates may not represent optimal strategies for each model
- Limited exploration of zero-shot generalization capabilities

## Confidence
- Core finding on depth-dependent degradation: High
- CoT prompting improvements: High
- Specific depth thresholds: Medium

## Next Checks
1. Test state tracking performance on mixed-domain scenarios with multiple simultaneous state variables
2. Evaluate zero-shot generalization by introducing novel state transitions not seen during prompt examples
3. Compare against dedicated state tracking architectures like memory-augmented networks