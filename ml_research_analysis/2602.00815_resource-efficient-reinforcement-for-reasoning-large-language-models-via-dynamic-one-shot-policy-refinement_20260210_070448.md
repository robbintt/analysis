---
ver: rpa2
title: Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic
  One-Shot Policy Refinement
arxiv_id: '2602.00815'
source_url: https://arxiv.org/abs/2602.00815
tags:
- reasoning
- policy
- training
- dopr
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational and data demands of
  reinforcement learning with verifiable rewards (RLVR) for training reasoning-capable
  large language models. It first establishes a theoretical lower bound on the required
  sample complexity, showing that near-optimal reasoning performance can be achieved
  with surprisingly few training examples.
---

# Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement

## Quick Facts
- arXiv ID: 2602.00815
- Source URL: https://arxiv.org/abs/2602.00815
- Reference count: 26
- Demonstrates RLVR can achieve near-optimal reasoning performance with surprisingly few training examples through dynamic sample selection

## Executive Summary
This paper addresses the high computational and data demands of reinforcement learning with verifiable rewards (RLVR) for training reasoning-capable large language models. It first establishes a theoretical lower bound on the required sample complexity, showing that near-optimal reasoning performance can be achieved with surprisingly few training examples. Based on this insight, the authors propose Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient RL strategy that dynamically selects a single most informative training sample per batch for policy updates, guided by reward volatility and exploration-aware scoring. DoPR reduces rollout overhead by nearly an order of magnitude while maintaining competitive reasoning accuracy. Experiments on multiple mathematical reasoning benchmarks demonstrate that DoPR achieves performance comparable to standard GRPO while requiring significantly fewer samples and rollouts, confirming its efficiency and scalability for reasoning-intensive LLM applications.

## Method Summary
The authors propose Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient RL strategy that leverages theoretical insights about minimal sample complexity to dramatically reduce computational overhead. DoPR operates by dynamically selecting a single most informative training sample per batch for policy updates, using a scoring mechanism that combines reward volatility (to identify high-learning-potential samples) and exploration-awareness (to ensure policy coverage). This approach maintains competitive reasoning accuracy while reducing rollout requirements by nearly an order of magnitude compared to standard GRPO, as validated across multiple mathematical reasoning benchmarks including MATH, Minierva-MATH, AIME24, AMC, OlympiadBench, and GSM8K.

## Key Results
- DoPR achieves near-optimal reasoning performance with as few as 16 training samples, establishing a theoretical lower bound on sample complexity
- Maintains competitive accuracy on mathematical reasoning benchmarks while reducing rollout overhead by nearly an order of magnitude
- Sample efficiency suggests RLVR functions more as a "capability activator" than a "performance booster" for pre-trained models

## Why This Works (Mechanism)
The mechanism works by exploiting the theoretical insight that near-optimal reasoning performance requires surprisingly few training examples. DoPR's dynamic sample selection identifies high-value training instances through reward volatility scoring, focusing policy updates on the most informative samples. This approach leverages the observation that pre-trained models already possess reasoning capabilities that RLVR activates rather than creates, allowing efficient sample usage. The exploration-aware component ensures the policy explores diverse solution paths while the volatility-based selection targets samples with high learning potential, creating an efficient feedback loop that maximizes learning per computational step.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - Provides the framework for training models on tasks with verifiable outputs; Quick check - Does the task have clear success/failure criteria?
- **Polyak–Łojasiewicz condition**: Why needed - Theoretical foundation for sample complexity bounds; Quick check - Is the loss function smooth and strongly convex in relevant regions?
- **Policy gradient methods**: Why needed - Core algorithm for updating model parameters based on rewards; Quick check - Does the model learn to improve reasoning traces over time?
- **Exploration-exploitation tradeoff**: Why needed - Balances discovering new solution strategies vs refining known ones; Quick check - Does the selection mechanism promote both coverage and high-reward paths?
- **Reward volatility analysis**: Why needed - Identifies samples with highest learning potential; Quick check - Are volatile samples correlated with policy improvement?
- **Sample complexity theory**: Why needed - Establishes theoretical limits on data efficiency; Quick check - Does the empirical performance align with theoretical bounds?

## Architecture Onboarding

**Component map:**
Pre-trained LLM -> DoPR Selector -> Reward Estimator -> Policy Updater -> Optimized Reasoning Policy

**Critical path:**
Input problem → Dynamic sample selection → Rollout generation → Reward evaluation → Policy update → Output reasoning model

**Design tradeoffs:**
DoPR trades computational efficiency for sample quality, using dynamic selection to reduce rollouts while maintaining performance. This contrasts with standard RLVR's uniform sampling approach. The exploration-aware scoring balances exploitation of high-reward paths with discovery of new strategies, while the volatility-based selection focuses updates on high-learning-potential samples. This design accepts increased selection complexity for reduced overall computational cost.

**Failure signatures:**
- Poor sample diversity leading to policy collapse on narrow solution paths
- Over-reliance on reward volatility causing instability with noisy rewards
- Exploration mechanism getting stuck in local optima
- Dynamic selection overhead negating computational savings
- Threshold selection for sample quality scoring being too conservative or aggressive

**First experiments to run:**
1. Ablation study: Compare DoPR against uniform sampling with varying rollout counts to isolate efficiency gains
2. Cross-domain validation: Apply DoPR to non-mathematical reasoning tasks (code generation, logical inference)
3. Scalability analysis: Test DoPR performance across different model scales (7B, 13B, 70B parameters)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the minimal sample complexity finding generalize beyond mathematical reasoning to other reasoning domains such as code generation, logical inference, or scientific problem-solving?
- Basis in paper: [inferred] All experiments are conducted exclusively on mathematical reasoning benchmarks (MATH, Minierva-MATH, AIME24, AMC, OlympiadBench, GSM8K), with no evaluation on other reasoning types.
- Why unresolved: The theoretical derivation makes domain-agnostic assumptions, but empirical validation is limited to mathematics. The sample complexity characteristics may differ in domains with different verification structures or reasoning patterns.
- What evidence would resolve it: Systematic experiments applying DoPR to non-mathematical reasoning tasks (e.g., code completion with unit tests, logical reasoning benchmarks) demonstrating comparable sample efficiency.

### Open Question 2
- Question: What determines the critical threshold between 8 and 16 training samples, and is this threshold dependent on model scale, architecture, or data diversity?
- Basis in paper: [explicit] "when the training set is reduced to only 8 samples, a consistent drop in performance is observed across all benchmarks. This suggests a critical threshold below which the diversity and quantity of training data are insufficient."
- Why unresolved: The paper identifies the threshold empirically but does not provide theoretical or mechanistic explanation for why 16 samples suffice while 8 fail, nor whether this scales with model size.
- What evidence would resolve it: Ablation studies varying data diversity at fixed sample sizes, cross-model analysis of threshold sensitivity, and theoretical characterization of the minimum diversity requirements.

### Open Question 3
- Question: What is the mechanistic explanation for RLVR functioning as a "capability activator" rather than a "performance booster" in pre-trained models?
- Basis in paper: [explicit] "RL supervision serves more as a capability activator rather than a performance booster, and that the reasoning ability of the model is largely constrained by its pre-training rather than the volume of RL data."
- Why unresolved: The paper provides empirical evidence for this phenomenon but lacks mechanistic analysis of what RLVR actually modifies in the model's representations or decision-making processes.
- What evidence would resolve it: Representational analysis (e.g., probing classifiers, attention pattern studies) comparing pre-training, post-SFT, and post-RLVR model states to identify what capabilities are being "activated."

### Open Question 4
- Question: How does the quality and domain-alignment of pre-training affect the theoretical sample complexity bounds and practical performance of DoPR?
- Basis in paper: [inferred] The theoretical analysis assumes pretraining/SFT creates a "favorable local manifold" satisfying the Polyak–Łojasiewicz condition, but experiments only use Qwen2.5-Math (already mathematics-specialized) and LLaMA3.1.
- Why unresolved: It remains unclear whether the minimal sample requirements hold for more general-purpose models or models with different pre-training distributions.
- What evidence would resolve it: Experiments comparing DoPR across models with varying degrees of domain-specific pre-training, measuring how pre-training alignment affects sample efficiency.

## Limitations
- Limited generalizability to non-mathematical reasoning domains due to exclusive focus on mathematical benchmarks
- Computational savings measured only against standard GRPO, lacking comprehensive comparison across RLVR variants
- Dynamic selection mechanism overhead not explicitly quantified relative to rollout savings
- Sample complexity threshold (8 vs 16 samples) identified empirically without theoretical explanation
- Assumes clean, verifiable reward signals without testing robustness to noisy rewards

## Confidence

**Confidence labels:**
- Theoretical sample complexity bound: High
- Computational efficiency claims: Medium
- Performance comparison with GRPO: Medium
- Generalizability across reasoning tasks: Low

## Next Checks

1. Test DoPR on non-mathematical reasoning tasks (e.g., code generation, commonsense reasoning) to evaluate domain transfer
2. Conduct ablation studies comparing dynamic selection against static sampling strategies across multiple RLVR implementations
3. Measure and report the wall-clock time and memory overhead of the dynamic selection mechanism relative to standard GRPO