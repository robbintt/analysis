---
ver: rpa2
title: Arabic Hate Speech Identification and Masking in Social Media using Deep Learning
  Models and Pre-trained Models Fine-tuning
arxiv_id: '2507.23661'
source_url: https://arxiv.org/abs/2507.23661
tags:
- hate
- speech
- arabic
- offensive
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two problems: Arabic hate speech detection
  and hate speech masking in social media. For detection, the authors experiment with
  various deep learning and transformer models, including RNN, CNN, and pre-trained
  Arabic language models like QARiB, MARBERT, and multi-dialect BERT.'
---

# Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning

## Quick Facts
- **arXiv ID:** 2507.23661
- **Source URL:** https://arxiv.org/abs/2507.23661
- **Reference count:** 38
- **Primary result:** 92% Macro F1 score for hate speech detection using QARiB with AraBERT preprocessing

## Executive Summary
This paper addresses two interconnected problems in Arabic social media: hate speech detection and hate speech masking. For detection, the authors compare traditional deep learning models (CNN, RNN) with transformer-based approaches (QARiB, MARBERT) and find that fine-tuned transformers achieve superior performance. For masking, they reframe the task as machine translation, training a transformer encoder-decoder to replace offensive words with asterisks. The best detection model achieves 92% Macro F1 and 95% accuracy, outperforming previous SemEval-2020 results, while the masking model reaches 30% BLEU score.

## Method Summary
The authors experiment with multiple approaches for Arabic hate speech detection, including RNNs (BiLSTM 128â†’64 units), CNNs (128 filters, kernel 7), and hybrid CNN-RNN models trained from scratch, as well as fine-tuning pre-trained transformers (QARiB, MARBERT, multi-dialect BERT) with AraBERT preprocessing. For masking, they treat the problem as sequence-to-sequence translation, creating a parallel corpus where offensive words are replaced with stars, then training a transformer encoder-decoder architecture. The detection experiments use the SemEval-2020 Arabic offensive language dataset (7,000 train / 1,000 dev / 2,000 test), while the masking experiments use a manually created parallel corpus of 4,383 sentence pairs.

## Key Results
- QARiB with AraBERT preprocessing achieves 92% Macro F1 and 95% accuracy for hate speech detection, outperforming the best SemEval-2020 results
- CNN with AraVec embeddings achieves only 51% Macro F1, demonstrating the superiority of transformer-based approaches
- Masking transformer model achieves 30% BLEU score (1-gram), considered good compared to state-of-the-art MT systems
- Class imbalance (1,991 offensive vs 8,000 not offensive) significantly impacts traditional DL model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contextual embeddings from pre-trained transformers significantly outperform static embeddings for Arabic hate speech detection.
- **Mechanism:** Models like QARiB and MARBERT, pre-trained on large Arabic corpora (including tweets), capture dialectal nuances and context that static models (like Word2Vec/AraVec) miss. This allows the classifier to distinguish offensive content even when orthography varies.
- **Core assumption:** The semantic relationships learned during pre-training transfer effectively to the specific domain of offensive language detection.
- **Evidence anchors:**
  - [abstract] Mentions using "transformers to determine the best model."
  - [section 3.5.1] Shows QARiB + AraBert achieving 92% F1 vs. CNN + AraVec achieving only 51% F1.
  - [corpus] Neighbor papers (e.g., "Advancing Hate Speech Detection with Transformers") corroborate the shift toward transformer architectures for this task.
- **Break condition:** Performance degrades if the pre-training data (e.g., formal Arabic) differs significantly from the inference data (e.g., heavy slang/dialects not seen during pre-training).

### Mechanism 2
- **Claim:** Treating hate speech masking as a sequence-to-sequence (Seq2Seq) translation task allows for context-aware sanitization.
- **Mechanism:** By framing "cleaning" as a translation from "dirty text" to "masked text," the Transformer decoder learns to identify toxic spans and generate asterisks while preserving the non-toxic sentence structure.
- **Core assumption:** The model can learn a strict alignment between specific input tokens (bad words) and output tokens (stars) without losing the surrounding context.
- **Evidence anchors:**
  - [abstract] "we consider it as a machine translation task."
  - [section 4.4] Describes the Transformer Encoder-Decoder architecture where the source is dirty and target is masked.
  - [corpus] Evidence is weak; provided neighbors focus on detection rather than masking/sanitization tasks.
- **Break condition:** The model hallucinates (reorders words) or fails to mask novel variations of hate speech not present in the parallel training corpus.

### Mechanism 3
- **Claim:** Orthographic normalization is a prerequisite for stabilizing performance in Arabic NLP pipelines.
- **Mechanism:** Normalizing variant characters (e.g., converting different forms of Alef to a single form) reduces vocabulary size and sparsity, ensuring that hate speech keywords match their labeled representations in the model's vocabulary.
- **Core assumption:** The semantic meaning of hate speech terms is preserved despite character reduction.
- **Evidence anchors:**
  - [section 3.2] Details specific normalization steps (letter normalization, removing diacritics).
  - [section 3.5.1] Notes that QARiB with "AraBert preprocessing" (which includes normalization) achieved superior results.
  - [corpus] Not explicitly detailed in neighbor abstracts, though implied in standard Arabic NLP workflows.
- **Break condition:** Over-normalization merges distinct words into the same token class, potentially increasing false positives.

## Foundational Learning

- **Concept: WordPiece Tokenization**
  - **Why needed here:** Arabic is a morphologically rich language. Standard tokenization fails on out-of-vocabulary (OOV) words. WordPiece (used by BERT/AraBERT) breaks complex words into sub-words, allowing the model to recognize toxic roots even in unfamiliar words.
  - **Quick check question:** How does the model handle a toxic word spelled with a typo that isn't in the static dictionary?

- **Concept: Class Imbalance**
  - **Why needed here:** The dataset (Section 3.1) is heavily imbalanced (1991 offensive vs 8000 not offensive). Without understanding how this skews "accuracy," one might incorrectly evaluate a model that simply guesses "not offensive" every time.
  - **Quick check question:** Why is Macro F1-score prioritized over simple Accuracy in the experimental results?

- **Concept: Encoder-Decoder Attention**
  - **Why needed here:** This is the engine of the masking mechanism. You must understand how the decoder "attends" to specific tokens in the encoder (the bad words) to decide when to output stars versus copying text.
  - **Quick check question:** In the masking task, which part of the architecture identifies *where* the bad word is, and which part generates the stars?

## Architecture Onboarding

- **Component map:** Raw Text -> Normalization -> Tokenization -> (Branch A: Detection Model OR Branch B: Seq2Seq Model)
- **Critical path:** Raw Text -> Normalization -> Tokenization -> (Branch A: Detection Model OR Branch B: Seq2Seq Model)
- **Design tradeoffs:**
  - *Detection:* Fine-tuning existing models (QARiB) is computationally cheaper and yields higher performance (92% F1) than training CNN/RNNs from scratch (51% F1), despite the latter being architecturally simpler.
  - *Masking:* Using a generative Seq2Seq model is flexible but risks grammatical errors compared to a simple dictionary lookup-and-replace method.
- **Failure signatures:**
  - High accuracy but Low Recall on the "Offensive" class (Section 3.5.1 results), indicating the model is biased toward the majority "Not Offensive" class.
  - Low BLEU scores (0.30) in masking, indicating the model struggles to reproduce the non-offensive parts of the sentence verbatim.
- **First 3 experiments:**
  1. **Baseline Detection:** Train a CNN + AraVec model to establish a lower bound and verify the preprocessing pipeline.
  2. **State-of-the-Art Comparison:** Fine-tune QARiB vs. MARBERT on the SemEval-2020 data to identify the most robust encoder for dialectal Arabic.
  3. **Masking Feasibility:** Train the Transformer Seq2Seq model on the small parallel corpus (3000+ pairs) and manually inspect output to see if stars align with the correct word positions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a machine translation model effectively paraphrase Arabic hate speech into non-offensive text while preserving semantic meaning, rather than simply masking offensive words with stars?
- **Basis in paper:** [explicit] The Conclusion states: "In additional, build a model for hate speech paraphrasing using a machine translation model."
- **Why unresolved:** The current study only addresses "cleaning" by replacing words with stars, which removes information rather than reconstructing it.
- **What evidence would resolve it:** A machine translation model trained to output semantically equivalent non-offensive sentences, evaluated on semantic similarity and style transfer metrics.

### Open Question 2
- **Question:** To what extent does mitigating class imbalance improve the recall and Macro F1-scores of deep learning models (CNN/RNN) trained from scratch?
- **Basis in paper:** [inferred] The authors attribute the poor performance of their DL models (Macro F1 ~51%) explicitly to the "imbalance in data" (1991 offensive vs 8000 not offensive) but do not apply correction techniques.
- **Why unresolved:** The experiments were conducted on the imbalanced dataset without employing balancing methods (e.g., oversampling, class weights) to validate this hypothesis.
- **What evidence would resolve it:** Re-training the RNN and CNN baselines using class weighting or data augmentation and comparing the resulting Macro F1-scores against the current baseline.

### Open Question 3
- **Question:** Does the BLEU metric correlate with the actual accuracy of masking offensive words in context, or does it primarily measure structural preservation?
- **Basis in paper:** [inferred] The authors acknowledge the task is "very challenging" and use BLEU (0.30 score), which measures n-gram overlap; however, this metric may not penalize the model sufficiently if it misses masking an offensive word or masks a neutral word.
- **Why unresolved:** The paper does not provide an error analysis on the masking output (e.g., precision/recall of the masking function itself) to verify if the 0.30 BLEU score reflects accurate censorship.
- **What evidence would resolve it:** A manual or token-level evaluation of the generated "clean" sentences to calculate the precision and recall of the masking mechanism independent of the BLEU score.

## Limitations
- The masking task corpus was manually created by a single annotator without inter-annotator agreement metrics, raising reliability concerns
- The study lacks comparison to the most recent transformer-based approaches in Arabic NLP that emerged after 2020
- The paper doesn't evaluate model performance on zero-shot scenarios with unseen dialects or emerging slang

## Confidence
**High Confidence:** The claim that transformer-based models (QARiB, MARBERT) significantly outperform traditional DL approaches (CNN/RNN with static embeddings) for Arabic hate speech detection is well-supported by the 92% vs 51% F1-score comparison and aligns with the broader literature on transformer effectiveness in NLP tasks.

**Medium Confidence:** The assertion that the best detection model "outperforms the best results from the SemEval-2020 shared task" is credible given the reported metrics, but the lack of explicit comparison to published SemEval-2020 results makes this claim partially dependent on external validation.

**Low Confidence:** The claim that the masking task can be effectively treated as a machine translation problem is the least validated, as there's minimal literature on hate speech sanitization as MT, and the reported BLEU score (0.30) is relatively low for any MT task.

## Next Checks
1. **Reproduce the detection pipeline with the official SemEval-2020 test set:** Download the exact dataset, apply the specified preprocessing (AraBERT normalization), and fine-tune QARiB with the stated hyperparameters (batch 16, LR 5e-5, epochs 8) to verify the 92% Macro F1 and 95% accuracy claims on the official split.

2. **Analyze per-class performance on the imbalanced dataset:** Calculate and report precision, recall, and F1 for both "Offensive" and "Not Offensive" classes separately to identify whether the high Macro F1 is driven by the majority class, which would indicate potential deployment bias.

3. **Benchmark the masking model against a dictionary-based baseline:** Implement a simple rule-based system that masks known offensive words and compare its output quality (using BLEU) and computational efficiency against the transformer-based approach to determine if the complexity is justified.