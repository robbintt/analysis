---
ver: rpa2
title: Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label
  Feature Selection
arxiv_id: '2511.12462'
source_url: https://arxiv.org/abs/2511.12462
tags:
- feature
- selection
- attention
- features
- redundancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of feature selection in multi-view
  multi-label data, where existing attention-based methods neglect inter-view complementarity
  and feature redundancy. The proposed method, RMAN-MMFS, employs multi-head attention
  to model intra-view feature-label relationships and cross-attention to capture inter-view
  feature complementarity.
---

# Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection

## Quick Facts
- arXiv ID: 2511.12462
- Source URL: https://arxiv.org/abs/2511.12462
- Authors: Yuzhou Liu; Jiarui Liu; Wanfu Gao
- Reference count: 13
- Outperforms six state-of-the-art methods on six datasets

## Executive Summary
This paper introduces RMAN-MMFS, a novel attention-based framework for multi-view multi-label feature selection that addresses limitations in existing methods. The approach employs multi-head attention to capture intra-view feature-label relationships, cross-attention to model inter-view feature complementarity, and dual redundancy optimization terms to minimize feature redundancy. Extensive experiments demonstrate superior performance across multiple evaluation metrics on six real-world datasets, with RMAN-MMFS outperforming existing methods in five of six cases.

## Method Summary
RMAN-MMFS employs a multi-head attention architecture where each head processes one view's features. View-self attention computes feature-label relationships using scaled dot-product attention, while cross-attention captures inter-view complementarity. The method introduces both static redundancy optimization (using mean absolute correlation coefficient for intra-view redundancy) and dynamic redundancy optimization (using mutual information between selected and candidate features across views). Features are iteratively selected based on combined attention scores and redundancy penalties, with performance evaluated using a MLKNN classifier.

## Key Results
- RMAN-MMFS outperforms six state-of-the-art methods on five of six datasets
- Superior performance in Average Precision, AUC, Coverage Error, and Ranking Loss metrics
- Ablation study confirms contribution of each component (cross-attention, static/dynamic redundancy)
- Corr+MI combination for redundancy optimization outperforms alternative combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-head attention with one head per view captures intra-view feature-label relationships more effectively than single-view or concatenated approaches.
- Mechanism: Each attention head independently computes view-self attention using labels as Query (Q) and normalized features as Key/Value (K/V). The softmax-weighted scores produce initial importance rankings per view, allowing heterogeneous data distributions to be modeled separately rather than conflated.
- Core assumption: Features within the same view share statistical properties that differ from other views, and label relevance can be captured via scaled dot-product similarity between feature and label matrices.
- Evidence anchors: [abstract] "we employ each individual attention head to model intra-view feature relationships"; [page 3] "Each attention head corresponds to one view to avoid overfitting from direct raw data computation"; [corpus] Weak direct validation; neighbor papers (MAFS) apply multi-head attention to single-view data, not multi-view per-head assignment
- Break condition: If views are highly correlated or derived from the same source, separate heads may learn redundant patterns, reducing marginal benefit.

### Mechanism 2
- Claim: Cross-attention between heads captures inter-view feature complementarity that view-self attention alone would miss.
- Mechanism: For each view v, all other views are concatenated into a context key matrix K_context. The label query Q attends to K_context, producing cross-view attention weights α_cross, which are then mapped back to the current view's feature space. High weights indicate features in other views that complement the current view's discriminative information.
- Core assumption: Complementary information exists across views (e.g., text "jade-green" complements image "eye" feature), and this can be quantified via attention weights between label queries and cross-view features.
- Evidence anchors: [abstract] "use the cross-attention mechanisms between different heads to capture inter-view feature complementarity"; [page 3] "This supplements view-self attention weights with cross-view feature complementarity, providing a robust global representation"; [page 6, ablation] RMAN1 (without cross-attention) shows lower AP across all six datasets vs. full RMAN; [corpus] Neighbor papers on multi-view learning emphasize cross-view complementarity but lack cross-attention implementations for comparison
- Break condition: If views are independent with no shared semantic structure, cross-attention may assign near-uniform weights, adding noise without benefit.

### Mechanism 3
- Claim: Dual redundancy optimization (static intra-view + dynamic global) reduces feature redundancy more effectively than either term alone.
- Mechanism: Static term uses Mean Absolute Correlation Coefficient (MACC) to penalize highly correlated features within each view. Dynamic term uses Mutual Information (MI) between candidate features and already-selected features across all views, penalizing candidates that duplicate information already captured. These are subtracted from attention scores with penalty coefficients λ and β.
- Core assumption: Redundancy manifests both linearly (within-view correlation) and nonlinearly (cross-view mutual information), and sequential selection should avoid repeating information already selected.
- Evidence anchors: [abstract] "the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features"; [page 4] "Combining linear (MACC) and nonlinear (MI) metrics balances efficiency and precision"; [page 6, ablation] RMAN2 (no static penalty) and RMAN3 (no dynamic penalty) both underperform full RMAN; [page 6, Table 5] Corr+MI combination outperforms MI+MI, MI+Corr, and Corr+Corr alternatives; [corpus] Weak; neighbor papers on multi-view feature selection mention redundancy but don't compare static/dynamic decomposition
- Break condition: If features have high feature-label correlations but also high inter-feature correlations (as in Mfeat dataset), the redundancy penalty may over-penalize discriminative features, reducing performance.

## Foundational Learning

- Concept: **Scaled Dot-Product Attention (Q, K, V)**
  - Why needed here: The entire RMAN architecture builds on mapping features to K/V and labels to Q. Without understanding how attention weights are computed via softmax(Q^T K / sqrt(d_k)), the intra-view and cross-view scoring mechanisms are opaque.
  - Quick check question: Given feature matrix X (n×d) and label matrix Y (n×c), can you compute the attention weight matrix W = softmax(Y^T X / sqrt(d)) and explain what each entry W_ij represents?

- Concept: **Multi-View Learning Complementarity vs. Redundancy**
  - Why needed here: RMAN explicitly models both complementarity (cross-attention) and redundancy (penalty terms). Distinguishing these is critical: complementarity means different views provide non-overlapping useful information; redundancy means different features (within or across views) provide overlapping information.
  - Quick check question: For two views of an image (HOG features and color histograms), give an example of complementary information and an example of redundant information.

- Concept: **Correlation vs. Mutual Information for Redundancy**
  - Why needed here: RMAN uses correlation (linear) for static redundancy and MI (nonlinear) for dynamic redundancy. Understanding when each captures distinct relationships determines why both are needed.
  - Quick check question: If feature A = X and feature B = X^2 (where X is uniform on [-1,1]), what would Pearson correlation return? What would MI return? Why does this matter for feature selection?

## Architecture Onboarding

- Component map: Input {X^(v)} for v=1..H views → Normalization → Per-view head (Q=Y, K^(v)=X_norm^(v), V^(v)=X^(v)) → View-self attention → Cross-view attention → Static redundancy penalty → Dynamic redundancy penalty → Final score → Selection

- Critical path: 1. Normalize all views independently; 2. For each view v: compute view-self attention → compute cross-view attention → compute static redundancy penalty → combine to get preliminary scores; 3. If selected set S is non-empty, compute dynamic redundancy penalty for all candidates; 4. Apply dynamic penalty, compute L2 norm of final scores, select top features; 5. Iterate across views until desired feature count reached

- Design tradeoffs: Static vs. Dynamic redundancy: Static (O(d²)) is faster but only captures intra-view linear redundancy. Dynamic (O(ndk)) captures cross-view nonlinear redundancy but scales with selection iterations. Both needed for comprehensive coverage. Per-head vs. shared parameters: One head per view avoids overfitting but prevents parameter sharing that could help when views are correlated. Correlation vs. MI: Correlation is O(nd²) and fast; MI estimation is O(nd log n) and captures nonlinear relationships but requires discretization or density estimation. L2 norm as final aggregation: Squaring before norm emphasizes large scores but may amplify noise.

- Failure signatures: Over-penalization on high-correlation datasets: On Mfeat (where feature-label correlations are unusually high), RMAN underperforms MLSMFS because redundancy penalty reduces discriminative feature scores too aggressively (page 5: "causing excessive penalization of discriminative features"). Cross-attention noise: If views are independent, cross-view attention weights become near-uniform, adding noise without complementary signal. Monitor α_cross distributions; high entropy indicates weak complementarity. Dynamic redundancy cold start: On first iteration, S is empty, so dynamic penalty is zero. Early selections may include redundant features that static penalty misses if redundancy is cross-view rather than intra-view.

- First 3 experiments: 1. Ablation by component: Run RMAN, RMAN1 (no cross-attention), RMAN2 (no static penalty), RMAN3 (no dynamic penalty) on a single dataset (e.g., SCENE). Plot AP vs. number of selected features. Confirm each component contributes positively and identify which component degrades most when removed. 2. Redundancy metric swap: Replace Corr+MI with MI+Corr, MI+MI, Corr+Corr (as in Table 5). Run on three datasets with varying inter-view correlation levels. Verify that Corr (static) + MI (dynamic) is consistently optimal or identify dataset characteristics where alternatives perform better. 3. Cross-view attention diagnosis: For each view v, compute the entropy of α_cross^(v) distribution across other views. Low entropy (sparse attention) indicates strong complementarity focus; high entropy (uniform attention) suggests views are independent or redundant. Correlate entropy values with per-dataset performance gaps between RMAN and RMAN1 to quantify cross-attention's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the relationships between labels be explicitly modeled to enhance the feature selection capability of the attention mechanism?
- Basis in paper: [explicit] The Conclusion states, "Future work will focus on exploring relationships between labels to further enhance the method’s performance."
- Why unresolved: The current RMAN-MMFS framework focuses exclusively on feature-feature and feature-label relationships, treating labels as independent queries without modeling the correlations or dependencies between the labels themselves.
- What evidence would resolve it: An extension of the proposed model that incorporates a label-correlation module (e.g., label co-occurrence matrices or label-graph attention) demonstrating improved Average Precision over the baseline RMAN-MMFS.

### Open Question 2
- Question: Can the redundancy optimization terms be made adaptive to prevent the over-penalization of discriminative features in datasets with naturally high feature-label correlations?
- Basis in paper: [inferred] The paper notes that RMAN underperforms on the Mfeat dataset because "This overlap complicates redundancy avoidance, weakening optimization adjustment and causing excessive penalization of discriminative features."
- Why unresolved: The current static ($\lambda$) and dynamic ($\beta$) penalty coefficients appear rigid; they struggle to distinguish between detrimental redundancy and beneficial high correlation (relevance) when feature-label overlap is high.
- What evidence would resolve it: An adaptive weighting mechanism or a modified mutual information metric that maintains or improves performance specifically on the Mfeat dataset (where the current method fails against MLSMFS) without degrading performance on others.

### Open Question 3
- Question: How does the computational complexity of the dynamic redundancy term scale with extreme feature dimensionality, and can it be approximated for large-scale data?
- Basis in paper: [inferred] The complexity analysis lists the dynamic redundancy term as $O(ndk)$, and the experiments utilize datasets with relatively low dimensionality (max 712 features).
- Why unresolved: While described as "lightweight," the dynamic term requires calculating mutual information against all selected features for every candidate feature; this iterative computation may become a bottleneck for high-dimensional, large-scale multi-view data not represented in the current benchmarks.
- What evidence would resolve it: A complexity analysis or runtime experiment on a dataset with features numbering in the tens of thousands, potentially comparing exact mutual information calculation against a sampled or approximated alternative.

## Limitations
- Hyperparameter sensitivity: The penalty coefficients λ and β show extreme sensitivity (0.001-1000 range) with no principled method for setting defaults, creating a reproducibility barrier.
- Dynamic redundancy scalability: As the selected set grows, mutual information computation between all candidates and selected features becomes computationally expensive, potentially limiting application to high-dimensional problems.
- Performance inconsistency: RMAN underperforms on Mfeat dataset due to over-penalization of discriminative features when feature-label correlations are high.

## Confidence
- **High confidence**: The individual mechanisms (multi-head attention, cross-attention, redundancy penalties) are well-specified and their contributions are validated through ablation studies. The theoretical framework is coherent.
- **Medium confidence**: The empirical superiority claims are well-supported within the tested datasets, but the sensitivity to hyperparameters and the failure on Mfeat suggest performance is not universally robust.
- **Low confidence**: The paper doesn't address how to handle the cold-start problem in dynamic redundancy (first iteration with empty selected set) or provide guidance on when cross-attention adds noise rather than signal.

## Next Checks
1. **Hyperparameter sensitivity mapping**: Run RMAN across the full λ, β parameter space on SCENE dataset; plot performance heatmaps to identify robust operating regions and quantify sensitivity.
2. **Cross-attention entropy analysis**: For each dataset, compute entropy of cross-attention weight distributions; correlate with performance gaps between RMAN and RMAN1 to quantify when cross-attention helps vs. hurts.
3. **Scalability benchmark**: Measure runtime and memory usage of dynamic redundancy computation as selected set size increases from 2% to 20% of features; identify breaking points and test approximation strategies (e.g., random sampling of selected features for MI computation).