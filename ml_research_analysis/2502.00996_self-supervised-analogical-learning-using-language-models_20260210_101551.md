---
ver: rpa2
title: Self-supervised Analogical Learning using Language Models
arxiv_id: '2502.00996'
source_url: https://arxiv.org/abs/2502.00996
tags:
- question
- questions
- answer
- programs
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the reasoning inconsistency problem in large
  language models (LLMs), where models fail on seemingly similar problems due to memorization
  of final answers rather than understanding underlying reasoning processes. The authors
  propose SAL (Self-supervised Analogical Learning), a framework that trains models
  to explicitly transfer high-quality symbolic solutions from familiar cases to rare
  ones.
---

# Self-supervised Analogical Learning using Language Models

## Quick Facts
- arXiv ID: 2502.00996
- Source URL: https://arxiv.org/abs/2502.00996
- Authors: Ben Zhou, Sarthak Jain, Yi Zhang, Qiang Ning, Shuai Wang, Yassine Benajiba, Dan Roth
- Reference count: 7
- Primary result: Improves reasoning accuracy on StrategyQA, GSM8K, HotpotQA by 2-20% through self-supervised analogical learning

## Executive Summary
This paper addresses the reasoning inconsistency problem in large language models (LLMs), where models fail on seemingly similar problems due to memorization of final answers rather than understanding underlying reasoning processes. The authors propose SAL (Self-supervised Analogical Learning), a framework that trains models to explicitly transfer high-quality symbolic solutions from familiar cases to rare ones. SAL employs two extraction methods: conceptualization, which finds similar questions with identical reasoning paths and extracts their programmatic solutions, and simplification, which decomposes math questions into easier subproblems. The method improves performance on reasoning benchmarks (StrategyQA, GSM8K, HotpotQA) by 2% to 20% compared to chain-of-thought and seed-only baselines. SAL also demonstrates better generalization and interpretability through its programmatic inference scheme, with ablation studies showing that models trained with SAL are less likely to simply repeat original questions in their reasoning.

## Method Summary
SAL addresses LLM reasoning inconsistency by training models to transfer symbolic solutions from familiar cases to rare ones. The framework uses two extraction methods: conceptualization, which generates similar questions to find high-quality programs, and simplification, which decomposes math questions into subproblems. The approach trains on Mistral-7B using LoRA (r=32, lr=2e-4) for 5 epochs with seed supervision from 228 StrategyQA and 265 GSM8K questions. Programs are generated as Python code with helper functions for comparisons and lookups, enabling explicit reasoning chains that can be executed and verified.

## Key Results
- Improves StrategyQA accuracy by 14% over chain-of-thought baseline
- Increases GSM8K performance by 20% compared to seed-only training
- Achieves 2-8% gains on HotpotQA, demonstrating cross-domain effectiveness
- Shows models trained with SAL are less likely to repeat original questions in reasoning chains

## Why This Works (Mechanism)
SAL works by explicitly training LLMs to recognize and transfer reasoning patterns rather than memorizing answers. The conceptualization method finds questions with identical reasoning paths but different surface forms, allowing extraction of generalizable solution programs. The simplification method breaks down complex math problems into manageable subproblems, creating supervision from easier cases. By using Python programs as intermediate representations, SAL makes reasoning explicit and verifiable, reducing the tendency to memorize specific question-answer pairs.

## Foundational Learning
- Python programming with helper functions: Needed for programmatic reasoning representation; Quick check: Can generate valid Python programs for simple arithmetic reasoning
- Question similarity metrics: Required for finding analogous problems; Quick check: Can compute semantic similarity between question pairs
- Chain-of-thought reasoning: Baseline method for comparison; Quick check: Can generate coherent reasoning steps for multi-hop problems
- LoRA fine-tuning: Efficient parameter-efficient adaptation method; Quick check: Can successfully fine-tune a small model with LoRA
- Abstraction generation: Creates simplified versions of complex questions; Quick check: Can generate 5+ meaningful simplifications of a given problem

## Architecture Onboarding

**Component map**: Question → Abstraction → Similar Question Generation → Program Extraction → LoRA Fine-tuning → Improved Model

**Critical path**: The most important sequence is: seed question → abstraction generation → conceptualization extraction → program generation → training, as this creates the core self-supervision signal.

**Design tradeoffs**: The framework trades computational cost of generating multiple similar questions against quality of supervision signals. Using Python programs provides interpretability but requires helper functions for real-world operations.

**Failure signatures**: Low-quality self-supervision occurs when generated programs simply repeat the original question via ask_llm calls. Math simplification fails when the 9/10 program agreement threshold cannot be reached within iteration limits.

**First experiments**: 1) Generate 20 similar questions for a seed question and measure similarity distribution; 2) Run the math simplification pipeline on a GSM8K question and check convergence rate; 3) Train a small LoRA model on seed-only data to establish baseline performance.

## Open Questions the Paper Calls Out
1. **Single Base Model**: The authors only experimented with Mistral-7B due to computational constraints, leaving unclear whether the 2-20% gains would persist in larger models (e.g., 70B+ parameters).
2. **Binary Question Limitation**: The current pipeline relies on binary verification to filter high-quality supervision signals, which does not translate to generative tasks like open-ended biography generation.
3. **Hyperparameter Sensitivity**: The agreement thresholds (e.g., 9/10 consensus) and iteration limits were hand-picked without ablation studies to test their robustness across different domains.

## Limitations
- Only tested on binary questions, limiting scalability to free-form reasoning tasks
- Relies on manual tuning of convergence hyperparameters without systematic ablation
- Computational constraints limited experiments to Mistral-7B, leaving scalability to larger models unknown

## Confidence
- Performance improvements on benchmarks: High confidence (clear baseline comparisons, ablation studies)
- Better generalization and interpretability: High confidence (demonstrated through programmatic inference)
- Reduced memorization tendencies: Medium confidence (shown through ablation, not direct memorization tests)

## Next Checks
1. Verify LoRA fine-tuning implementation matches specifications by reproducing seed-only baseline results on GSM8K
2. Test conceptualization pipeline's filtering criteria by measuring actual similarity scores between original and generated questions
3. Validate math simplification process by checking program agreement rates across all iterations to understand failure patterns