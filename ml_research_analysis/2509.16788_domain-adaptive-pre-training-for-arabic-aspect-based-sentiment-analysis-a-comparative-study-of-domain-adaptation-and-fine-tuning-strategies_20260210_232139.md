---
ver: rpa2
title: 'Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A
  Comparative Study of Domain Adaptation and Fine-Tuning Strategies'
arxiv_id: '2509.16788'
source_url: https://arxiv.org/abs/2509.16788
tags:
- arabic
- sentiment
- in-dapt
- fine-tuning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores domain-adaptive pre-training for Arabic aspect-based
  sentiment analysis, focusing on two tasks: aspect sentiment classification (ASC)
  and opinion target expression (OTE) extraction. It evaluates seven Arabic BERT-based
  models using various fine-tuning strategies, including full fine-tuning, feature
  extraction, and adapter-based methods, across domain-specific corpora.'
---

# Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies

## Quick Facts
- arXiv ID: 2509.16788
- Source URL: https://arxiv.org/abs/2509.16788
- Reference count: 40
- Primary result: Adapter-based fine-tuning with In-DAPT improves Arabic ABSA performance while reducing parameters by 98.6%, with focal loss addressing class imbalance

## Executive Summary
This study investigates domain-adaptive pre-training (DAPT) and fine-tuning strategies for Arabic aspect-based sentiment analysis (ABSA), focusing on aspect sentiment classification (ASC) and opinion target expression (OTE) extraction. The authors evaluate seven Arabic BERT-based models using various adaptation techniques, finding that in-domain adaptive pre-training modestly improves performance when vocabulary overlap is low, while adapter-based fine-tuning offers significant computational efficiency with competitive accuracy. Error analysis reveals systematic challenges with contrastive sentiment structures, positivity bias, and multi-word expression handling, highlighting the need for syntax- and semantics-aware architectures.

## Method Summary
The study employs a three-stage approach: (1) select top Arabic BERT models (CAMeLBERT-MSA, QARiB, MARBERTv2), (2) apply domain-adaptive pre-training via MLM on unlabeled in-domain corpora (HARD, LABR), and (3) fine-tune on SemEval-2016 Arabic hotel reviews using full fine-tuning, adapters, or feature extraction. For ASC, the sentence-pair format ([CLS] review [SEP] aspect [SEP]) is used, while OTE employs sequence labeling with softmax or CRF heads. Focal loss with inverse-frequency class weighting addresses class imbalance. Performance is measured via accuracy, macro-F1 (ASC), and micro-F1/precision/recall (OTE), with statistical significance testing.

## Key Results
- Adapter-based fine-tuning reduces parameters by 98.6% with only ~2.15% accuracy drop compared to full fine-tuning
- In-DAPT improves performance when base model vocabulary overlap with domain is low (<10%), but Out-DAPT degrades performance
- Focal loss increases Recall from 73.33% to 78.67% and Macro-F1 from 75.16% to 78.63% for ASC
- CAMeLBERT-MSA with In-DAPT + adapter fine-tuning achieves the best overall performance

## Why This Works (Mechanism)

### Mechanism 1: In-Domain Vocabulary Acquisition
In-domain adaptive pre-training (In-DAPT) improves performance primarily when the base model has low vocabulary overlap with the target domain, allowing it to learn domain-specific semantics through MLM prediction of rare tokens. This lexical adaptation is more effective than fine-tuning existing knowledge, though gains are modest (0.69-2.19% accuracy) and Out-DAPT consistently degrades performance.

### Mechanism 2: Parameter-Efficient Knowledge Preservation
Adapter-based fine-tuning achieves competitive performance by isolating task-specific learning to small bottleneck layers, preventing catastrophic forgetting while drastically reducing trainable parameters (98.6% reduction). This approach strikes a balance between efficiency and accuracy, though it may underperform full fine-tuning for complex syntactic patterns requiring extensive adaptation.

### Mechanism 3: Bias Correction via Loss Regularization
Focal loss combined with class weighting shifts the decision boundary to improve sensitivity to minority classes by reducing loss contribution from easy, majority-class examples. This regularization effectively addresses the positivity bias in imbalanced datasets, significantly improving recall and macro-F1 metrics without sacrificing overall accuracy.

## Foundational Learning

- **Concept: Subword Tokenization (WordPiece)**
  - Why needed here: Arabic is morphologically rich, and error analysis shows over-segmentation creates noise in OTE extraction. Understanding tokenizer behavior is critical for diagnosing failures on multi-word expressions.
  - Quick check question: How does the model handle the Arabic word for "uncleanliness" if it is not in the vocabulary? (It splits it, potentially losing semantic meaning).

- **Concept: Sentence-Pair Classification (SPC) Input Format**
  - Why needed here: ASC requires associating specific aspects with sentences. The format [CLS] Sentence [SEP] Aspect [SEP] enables the model to differentiate between "The food was good" (Food aspect) and "The food was good but service was bad" (Service aspect).
  - Quick check question: How does the model differentiate between "The food was good" (Food aspect) and "The food was good but service was bad" (Service aspect)?

- **Concept: Macro-F1 vs. Accuracy**
  - Why needed here: The dataset is imbalanced (mostly Positive), so high accuracy can be achieved by ignoring minority classes. Macro-F1 ensures the model works across all sentiment polarities.
  - Quick check question: If a model predicts "Positive" for every input, it achieves ~59% accuracy. Why is this considered a failure in this context? (Because Macro-F1 would effectively be zero for the other classes).

## Architecture Onboarding

- **Component map:** Arabic Text + Aspect Category -> CAMeLBERT-MSA Backbone -> Pfeiffer/Houlsby Adapters -> Linear Classifier Head
- **Critical path:** 1) Strict preprocessing (diacritics, elongation removal), 2) Vocabulary overlap check (<10% triggers In-DAPT), 3) Focal Loss application for imbalance (IR > 5)
- **Design tradeoffs:** Full Fine-Tuning vs. Adapters (89.67% accuracy vs. 97% parameter reduction), CRF vs. Softmax for OTE (Softmax better for precision, Bi-GRU+Softmax better for recall)
- **Failure signatures:** Contrastive Misses (misreading "but/however"), Positivity Bias (defaulting to Positive), Multi-word Expression Fragmentation (Idafa constructions breaking)
- **First 3 experiments:** 1) Baseline: CAMeLBERT-MSA full fine-tuning on ASC, 2) Efficiency: Pfeiffer Adapters on same task, 3) Imbalance: Focal Loss with gamma=2 on best configuration

## Open Questions the Paper Calls Out

### Open Question 1
Can syntax- and semantics-aware graph architectures (e.g., dependency-aware GCNs such as ARGCN or SD-GCN) systematically reduce error rates on long-distance contrastive markers, multi-word Idafa spans, and aspect-opinion misalignments in Arabic ABSA?

### Open Question 2
How robust is the In-DAPT benefit when the adaptation corpus is subsampled to the size of TAPT, across domains beyond hotel reviews and across Arabic dialects?

### Open Question 3
Do alternative parameter-efficient methods (Prefix-Tuning, LoRA) improve over Pfeiffer/Houlsby adapters on the performance-efficiency frontier for Arabic ASC and OTE?

### Open Question 4
Can instruction-tuned Arabic LLMs (e.g., AceGPT-7B) with retrieval-augmented generation match or exceed the current best supervised fine-tuned performance on ASC and OTE without domain-adaptive pre-training?

## Limitations

- Performance improvements from In-DAPT are modest (0.69-2.19%) and highly dependent on vocabulary overlap between base model and target domain
- Out-DAPT consistently degrades performance (-0.11% to -2.16%), suggesting domain mismatch causes negative transfer
- Error analysis reveals fundamental limitations with contrastive sentiment structures, positivity bias, and multi-word expression handling
- Adapter-based methods, while efficient, show slightly lower performance than full fine-tuning for complex syntactic patterns

## Confidence

**High Confidence:**
- Adapter-based fine-tuning achieves competitive performance with 98.6% parameter reduction
- In-DAPT improves performance when vocabulary overlap is low (<10%)
- Feature extraction underperforms compared to full fine-tuning or adapters
- Out-DAPT causes performance degradation due to domain mismatch

**Medium Confidence:**
- Focal loss effectively addresses class imbalance in sentiment classification
- Sentence-pair format with aspect categories improves ASC performance
- CRF vs. softmax trade-offs for OTE extraction are task-dependent

**Low Confidence:**
- The specific mechanism of vocabulary overlap driving In-DAPT gains (needs more controlled experiments)
- Generalization of error patterns across different Arabic dialects
- Long-term stability of adapter-based approaches for evolving domain data

## Next Checks

1. **Vocabulary Overlap Experiment**: Systematically test In-DAPT effectiveness across models with varying vocabulary overlap percentages (0%, 5%, 10%, 15%, 20%) using controlled subsets of the HARD corpus to validate the hypothesis that gains occur primarily when overlap is below 10%.

2. **Syntax-Aware Fine-Tuning**: Implement a syntactic dependency-aware fine-tuning approach where the model explicitly learns to handle contrastive structures and multi-word expressions, comparing performance against the current adapter-based methods on the same SemEval-2016 dataset.

3. **Cross-Dialect Robustness Test**: Evaluate the best-performing configurations (CAMeLBERT-MSA with In-DAPT + adapters) on a mixed-dialect Arabic ABSA dataset to assess generalization beyond MSA hotel reviews, measuring degradation in accuracy and error pattern shifts.