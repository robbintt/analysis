---
ver: rpa2
title: Evaluation of LLM-based Explanations for a Learning Analytics Dashboard
arxiv_id: '2511.11671'
source_url: https://arxiv.org/abs/2511.11671
tags:
- learning
- explanations
- dashboard
- condition
- provided
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated large language model (LLM)-generated explanations
  for a Learning Analytics Dashboard against no explanations and human teacher explanations.
  Twelve university-level educators participated in a within-subject study where they
  evaluated three dashboard conditions: standalone, human-generated explanations,
  and LLM-generated explanations (using Qwen 32B via BIKI interface).'
---

# Evaluation of LLM-based Explanations for a Learning Analytics Dashboard

## Quick Facts
- arXiv ID: 2511.11671
- Source URL: https://arxiv.org/abs/2511.11671
- Reference count: 8
- LLM-generated explanations significantly outperformed both standalone dashboards and human teacher explanations (p < 0.01, d = 0.96) for skill states and course recommendations

## Executive Summary
This study evaluates large language model (LLM)-generated explanations for a Learning Analytics Dashboard (LAD) using a within-subject design with 12 university-level educators. Participants evaluated three conditions: standalone dashboard, human-generated explanations, and LLM-generated explanations (using Qwen 32B via BIKI interface) across two simulated student examples. The study measured explanation quality using TOAST questionnaire and adjusted explanation satisfaction scales. Results showed that LLM-generated explanations were significantly more favored than both the standalone dashboard and human teacher explanations, particularly for skill state interpretation and course recommendations.

## Method Summary
The study employed a within-subject design where 12 university-level educators evaluated three dashboard conditions in randomized order. Each condition presented two simulated student examples with different learning performances. The LLM explanations were generated using Qwen 32B via the BIKI interface, with prompts and analysis code available in the git repository. Participants completed pre/post questionnaires on teaching experience and self-regulated learning strategies, plus three evaluation questionnaires (TOAST and two adjusted explanation satisfaction scales). Statistical analysis used pairwise t-tests with Cohen's d effect sizes to compare conditions.

## Key Results
- LLM-generated explanations significantly outperformed both standalone dashboard and human explanations for skill states (p < 0.01)
- LLM explanations showed large effect size (d = 0.96) compared to human explanations for performance evaluation
- Participants rated LLM explanations more favorably for course recommendations interpretation

## Why This Works (Mechanism)
The LLM-generated explanations succeeded by providing more comprehensive, consistent, and pedagogically sound interpretations of dashboard data compared to both the raw dashboard view and human-generated explanations. The Qwen 32B model's ability to synthesize complex performance data into coherent narratives about skill development and course recommendations resonated with educators' needs for actionable insights.

## Foundational Learning
- Learning Analytics Dashboard (LAD) concepts: Understanding how dashboard data represents student learning trajectories and skill development
  - Why needed: Forms the basis for evaluating explanation quality
  - Quick check: Can identify key metrics displayed in educational dashboards
- TOAST questionnaire methodology: Trust in automated systems measurement framework
  - Why needed: Provides standardized evaluation of explanation trustworthiness
  - Quick check: Can administer and score TOAST questionnaire items
- Performance Factors Analysis (PFA) model: Educational data modeling approach
  - Why needed: Underlies the simulated student data used in evaluation
  - Quick check: Understands basic PFA model assumptions and outputs

## Architecture Onboarding
- Component map: Educators -> Dashboard Conditions (Standalone, Human, LLM) -> Qwen 32B -> BIKI Interface -> TOAST & Satisfaction Scales -> Statistical Analysis
- Critical path: Dashboard data → LLM prompt → Qwen 32B generation → BIKI interface delivery → educator evaluation → questionnaire response → statistical comparison
- Design tradeoffs: Simulated student data provides controlled evaluation but may not capture real-world complexity
- Failure signatures: Small sample size (N=12) may lead to underpowered results; order effects despite randomization
- First experiments: 1) Test BIKI interface access and Qwen 32B connectivity; 2) Verify prompt templates produce coherent outputs; 3) Pilot TOAST questionnaire administration

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (N=12) limits statistical power and generalizability of observed effect sizes
- Use of simulated student data may not capture the complexity of real student performance scenarios
- Findings are specific to university-level educators in a particular institutional context, limiting transferability

## Confidence
- High confidence: Study successfully demonstrates LLM-generated explanations can be meaningfully evaluated against baseline conditions using established metrics
- Medium confidence: Finding that LLM explanations outperform both standalone dashboards and human explanations for skill states and course recommendations
- Medium confidence: Within-subject design methodology with randomized conditions is appropriate for this exploratory evaluation

## Next Checks
1. Replicate the study with a larger sample size (n ≥ 30) to verify observed effect sizes and establish statistical power for detecting meaningful differences
2. Test LLM explanations with actual student performance data rather than simulated examples to assess real-world applicability
3. Conduct a longitudinal study to evaluate whether observed preference for LLM explanations translates into improved teaching outcomes and student learning gains over time