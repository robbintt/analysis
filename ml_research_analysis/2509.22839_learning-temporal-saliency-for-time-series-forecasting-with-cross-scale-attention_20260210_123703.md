---
ver: rpa2
title: Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention
arxiv_id: '2509.22839'
source_url: https://arxiv.org/abs/2509.22839
tags:
- attention
- temporal
- time
- saliency
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CrossScaleNet, a novel architecture for time
  series forecasting that combines patch-based cross-attention with multi-scale processing
  to achieve both high performance and enhanced temporal explainability. The method
  addresses the challenge of identifying temporal saliency in time series forecasting,
  where understanding which time steps are most important for predictions is critical
  for transparency and decision-making.
---

# Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention

## Quick Facts
- arXiv ID: 2509.22839
- Source URL: https://arxiv.org/abs/2509.22839
- Reference count: 40
- Key outcome: CrossScaleNet achieves competitive forecasting accuracy while providing intrinsic interpretability through attention maps, outperforming state-of-the-art transformer models on both synthetic saliency detection and real-world benchmarks.

## Executive Summary
This paper presents CrossScaleNet, a novel architecture for time series forecasting that combines patch-based cross-attention with multi-scale processing to achieve both high performance and enhanced temporal explainability. The method addresses the challenge of identifying temporal saliency in time series forecasting, where understanding which time steps are most important for predictions is critical for transparency and decision-making. By incorporating cross-attention between scales and using different key representations for patch and local attention (derived from predictions and seasonal components), the model effectively captures temporal dependencies while providing interpretable attention maps.

## Method Summary
CrossScaleNet operates by decomposing time series into multiple scales, each processed through separate encoders for trend and seasonal components. The key innovation is cross-patch attention that operates between scales using prediction outputs as semantic keys rather than raw input values. For each scale beyond the first, patch attention uses the highest-scale prediction as keys to capture global temporal relationships, while local attention uses the highest-scale seasonal prediction to emphasize periodic patterns within patches. The model fuses these attention outputs and learns scale-specific weights through sigmoid activation, ultimately combining all scale outputs through a final fully connected layer. This design enables the model to maintain competitive forecasting accuracy while providing intrinsic interpretability through attention maps that highlight important temporal features.

## Key Results
- CrossScaleNet outperforms state-of-the-art transformer-based models including iTransformer, PatchTST, and LMSAutoTSF on both temporal saliency detection and forecasting accuracy
- On synthetic datasets, CrossScaleNet achieves lower MSE and MAE values compared to baselines while correctly identifying temporal saliency patterns
- The model successfully identifies temporal saliency patterns, focusing on both recent and distant time points as appropriate for different datasets
- CrossScaleNet maintains competitive performance on real-world forecasting tasks while providing intrinsic interpretability through its attention mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-scale attention improves interpretability by grounding attention in semantically meaningful predictions rather than raw input correlations.
- Mechanism: Instead of self-attention comparing positions within the same scale, cross-patch attention compares temporal patches at lower scales (m > 1) against abstracted predictions from the highest scale (y₁) and its seasonal component (y₁_seasonal). This acts as a relevance filter—only components of Xₘ that align with the abstract, generalizable structure in y₁ contribute significantly to attention scores.
- Core assumption: Prediction outputs from the highest scale encode meaningful temporal abstractions (trends, periodicities) that can serve as reliable semantic keys for lower-scale attention.
- Evidence anchors:
  - [abstract] "By incorporating cross-attention between scales and using different key representations for patch and local attention (derived from predictions and seasonal components), the model effectively captures temporal dependencies while providing interpretable attention maps."
  - [Section 4.2.1] "The key representation for Patch Attention (Xₚ,ₖ) is derived from the prediction output from the highest-scale y₁... This design choice ensures that the global dependencies across patches are captured using the most comprehensive temporal representation."
  - [Section 4.3] "From an information-theoretic perspective, this cross-scale conditioning acts as a form of relevance filtering via a soft information bottleneck... I(Xₘ; yₘ | y₁) < I(Xₘ; yₘ)"
- Break condition: If highest-scale predictions are noisy or uninformative (e.g., highly non-stationary series with no stable patterns), the semantic key degenerates and attention may amplify noise rather than signal.

### Mechanism 2
- Claim: Dual-key attention (separate keys for patch vs. local attention) better captures both global dependencies and fine-grained seasonal patterns.
- Mechanism: Patch attention uses y₁ (full prediction) as key to capture global temporal relationships across patches. Local attention uses y₁_seasonal as key to emphasize periodic/seasonal patterns within patches. The outputs are fused: C = Cₚ + Cₗ.
- Core assumption: Seasonal and trend components require different attention strategies—global patches benefit from trend-aware keys, while local timesteps benefit from seasonality-aware keys.
- Evidence anchors:
  - [Section 4.2.2] "The key representation for Local Attention (Xₗ,ₖ) is derived from the prediction output from the highest-scale seasonal component y₁_seasonal... which captures the periodic patterns within the sequence."
  - [Table 3] Ablation shows Patch-crosskey* (different keys for patch/local) vs. Patch-crosskey (shared key) have different performance profiles across SYN datasets, with the dual-key approach better capturing temporal saliency on some datasets.
  - [corpus] AWGformer paper uses wavelet decomposition for multi-resolution, suggesting scale-specific processing is a recognized pattern, but no direct evidence on dual-key attention exists in corpus.
- Break condition: When series lack strong seasonality, the seasonal-derived key becomes weakly informative, and local attention may not add value over patch attention alone.

### Mechanism 3
- Claim: Synthetic datasets with known saliency ground truth enable validation of temporal saliency detection that real-world benchmarks cannot provide.
- Mechanism: Eight synthetic datasets (SYN1-8) are generated with pre-defined important features and important lags. The target y is constructed using specific features at specific lags with controlled noise. This allows direct comparison of learned attention maps against ground truth saliency.
- Core assumption: Synthetic temporal dependencies with known importance generalize to real-world saliency detection scenarios.
- Evidence anchors:
  - [Section 3] "Each dataset contains a unique combination of Important Features... Important Lags... and Noise Level... The saliency maps of datasets named SYN1, SYN2, SYN3, SYN4, SYN5, SYN6, SYN7, and SYN8 exhibit distinct temporal focusing patterns."
  - [Table 1] Shows specific lag ranges (e.g., SYN5: lags 71-77, distant; SYN1: lags 1-15, recent) enabling evaluation of whether models correctly attend to recent vs. distant time points.
  - [corpus] No corpus papers address synthetic saliency validation; this is a gap in external evidence.
- Break condition: If synthetic saliency patterns are too simplistic (linear lag dependencies) compared to real-world complex interactions, validation results may not transfer to actual forecasting tasks.

## Foundational Learning

- Concept: **Time series decomposition (trend-seasonal)**
  - Why needed here: CrossScaleNet decomposes inputs at each scale into seasonal and trend components processed by separate encoders. Understanding decomposition is essential to grasp why different keys (y₁ vs. y₁_seasonal) are used.
  - Quick check question: Given a time series with daily temperature data over a year, what would the trend component capture vs. the seasonal component?

- Concept: **Cross-attention vs. self-attention**
  - Why needed here: The core innovation is cross-patch attention where queries/values come from one scale but keys come from predictions at a different scale. This differs from standard self-attention.
  - Quick check question: In self-attention, Q, K, V all derive from the same input. What changes when K comes from a different source?

- Concept: **Patch-based sequence processing**
  - Why needed here: CrossScaleNet segments time series into patches before attention, enabling efficient processing and global-local attention separation.
  - Quick check question: If a sequence has 96 timesteps and patch size is 16, how many patches are formed and what dimensionality does patch attention operate on?

## Architecture Onboarding

- Component map: Input X -> Multi-scale decomposition (trend/seasonal) -> Scale encoders -> Cross-patch attention (scales m > 1) -> Attention fusion (C = Cₚ + Cₗ) -> Scale weighting (yₘ = yₘ · σₘ) -> Output aggregation (y = FC([y₁, y₂, ..., yₘ]))

- Critical path:
  1. Scale 1 (highest) processes input → produces y₁ and y₁_seasonal
  2. For each subsequent scale m > 1: cross-attention uses y₁/y₁_seasonal as keys
  3. All scale outputs weighted and concatenated through final FC layer
  4. Attention maps interpolated and aggregated for interpretability

- Design tradeoffs:
  - Accuracy vs. interpretability: CrossScaleNet achieves competitive accuracy (Table 6: avg MSE 0.342 vs. LMSAutoTSF 0.332) while providing intrinsic saliency maps, unlike LMSAutoTSF/TimeMixer
  - Computational cost: 4.663s/epoch vs. iTransformer's 2.012s/epoch, but adds interpretability (Table 8)
  - Patch size selection: Smaller patches capture finer local patterns but increase computational cost; paper does not specify hyperparameter tuning details

- Failure signatures:
  - Attention maps show uniform weighting across all timesteps → keys may be uninformative; check scale 1 prediction quality
  - Performance degrades significantly on non-seasonal datasets → seasonal key may be misguiding local attention; consider disabling local attention or using trend-derived key
  - Saliency maps highlight wrong timesteps on synthetic data with known ground truth → cross-scale conditioning not working; verify interpolation alignment between scales

- First 3 experiments:
  1. **Ablation on attention type**: Compare self-attention, patch-attention, and cross-patch attention on SYN1-8 to isolate contribution of cross-scale design (replicate Table 3)
  2. **Saliency validation**: Train CrossScaleNet on SYN5-8 (distant saliency) and visualize attention maps against ground truth; measure sufficiency/comprehensiveness metrics
  3. **Real-world benchmark**: Run on ETTh1 with look-back 96, horizon 96; compare MSE/MAE against iTransformer and PatchTST while plotting attention maps for interpretability assessment

## Open Questions the Paper Calls Out
- Would explicitly separating the processing of target and non-target features within the attention mechanism improve temporal saliency estimation and forecasting performance?
- Can the effectiveness of using seasonal component outputs as keys for local attention be theoretically justified, or is this design choice only empirically validated?
- How well does attention-based saliency from CrossScaleNet generalize to domains where ground truth importance is unknown or differs from synthetic test conditions?

## Limitations
- The synthetic saliency evaluation may not generalize to real-world complexity with more complex temporal dependencies
- The method's performance on irregularly sampled time series is not evaluated
- Several critical hyperparameters (number of scales, patch size, attention heads, encoder depth) are unspecified

## Confidence
- Cross-scale attention mechanism design: **High** (well-specified with clear mathematical formulation)
- Synthetic saliency dataset construction: **High** (explicit ground truth generation procedure)
- Performance claims vs. baselines: **Medium** (competitive results but missing hyperparameter details)
- Saliency interpretation validity: **Medium** (synthetic validation strong but real-world generalizability uncertain)
- Computational efficiency claims: **Medium** (runtime comparisons provided but ablation on complexity missing)

## Next Checks
1. **Saliency transfer validation**: Apply CrossScaleNet trained on synthetic data to real-world datasets (ETTh1, ETTh2) and compare attention patterns against domain expert knowledge to assess whether detected saliency aligns with expected important time points.

2. **Scale weight sensitivity analysis**: Systematically vary the number of scales M and analyze how attention distribution changes across scales, particularly examining whether scale weights σm learn to down-weight noisy scales and if this correlates with improved performance.

3. **Dual-key ablation completeness**: Conduct comprehensive ablation testing all key combinations (shared key, seasonal-only, trend-only, dual-key) across all eight synthetic datasets to fully characterize when dual-key attention provides benefits and under what data conditions it degrades performance.