---
ver: rpa2
title: 'Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for
  Invoice Processing'
arxiv_id: '2509.04469'
source_url: https://arxiv.org/abs/2509.04469
tags:
- native
- document
- processing
- docling
- gemini-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks eight multi-modal large language models
  from three families (GPT-5, Gemini 2.5, and Gemma 3) on three diverse invoice datasets
  using zero-shot prompting. The study compares two processing strategies: direct
  image processing using multi-modal capabilities and a structured parsing approach
  that converts documents to markdown first using Docling.'
---

# Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing

## Quick Facts
- **arXiv ID:** 2509.04469
- **Source URL:** https://arxiv.org/abs/2509.04469
- **Reference count:** 40
- **Primary result:** Native image processing consistently outperforms markdown conversion across all tested models and datasets

## Executive Summary
This paper benchmarks eight multi-modal large language models on invoice processing tasks, comparing direct image analysis against a structured parsing approach using document conversion. The study evaluates GPT-4o, GPT-4o-mini, Gemini 2.0 Flash, Gemini 2.5 Pro, and Gemma 3 variants across three diverse invoice datasets. Results demonstrate that native multi-modal processing significantly outperforms the markdown conversion strategy for extracting key invoice information, with Gemini 2.5 Pro achieving the highest overall accuracy. The research provides practical guidance for selecting appropriate models and processing strategies in automated document processing systems.

## Method Summary
The study evaluates eight multi-modal LLMs from three families (GPT-5, Gemini 2.5, and Gemma 3) using zero-shot prompting on three invoice datasets: scanned receipts, clean digital invoices, and scanned invoices. Two processing strategies are compared: direct image processing using each model's native multi-modal capabilities, and a structured parsing approach that first converts documents to markdown using Docling before processing with text-only models. Performance is measured across accuracy metrics for extracted invoice fields including total amounts, dates, vendor information, and structured data like IBANs.

## Key Results
- Native image processing consistently outperforms markdown conversion across all models and datasets
- Gemini 2.5 Pro achieves highest overall accuracy: 87.46% on scanned receipts, 96.50% on clean invoices, and 92.71% on scanned invoices
- Smaller models like gemma-3-4b-it struggle with direct image analysis
- Unstructured fields like IBANs present particular challenges due to OCR-related errors

## Why This Works (Mechanism)
Multi-modal LLMs leverage their native vision capabilities to process visual layout, formatting cues, and spatial relationships directly from invoice images. This direct processing preserves contextual information that gets lost during document conversion to markdown format. The models can interpret visual hierarchies, recognize patterns in document structure, and extract information while maintaining spatial context. Direct processing also avoids errors introduced during the intermediate conversion step, particularly for complex layouts or low-quality scans where markdown conversion may introduce artifacts or lose critical formatting information.

## Foundational Learning

**Multi-modal LLM Architecture** - Why needed: Understanding how vision and language models integrate for document processing. Quick check: Verify model supports both image and text input simultaneously.

**Document Layout Analysis** - Why needed: Recognizing how visual structure impacts information extraction accuracy. Quick check: Assess model's ability to identify table structures and hierarchical information.

**OCR Quality Impact** - Why needed: Understanding how text extraction quality affects downstream processing. Quick check: Compare performance on high vs. low quality document scans.

**Prompt Engineering for Vision Tasks** - Why needed: Optimizing how visual information is presented to language models. Quick check: Test different prompt formats for same visual input.

**Zero-shot vs Few-shot Learning** - Why needed: Determining baseline performance without task-specific training. Quick check: Measure performance difference with and without few-shot examples.

## Architecture Onboarding

**Component Map:** Document -> Multi-modal LLM (direct) OR Document -> Docling -> Markdown -> Text-only LLM

**Critical Path:** Direct Image Processing: Document acquisition → Multi-modal LLM → Field extraction → Output validation
Structured Parsing: Document acquisition → Docling conversion → Markdown generation → Text-only LLM → Field extraction → Output validation

**Design Tradeoffs:** Direct processing preserves visual context but requires more computational resources; structured parsing is faster but loses spatial information. Zero-shot approach provides baseline generalization but may underperform fine-tuned alternatives.

**Failure Signatures:** Structured parsing fails on complex layouts due to markdown conversion artifacts; direct processing struggles with low-resolution images and smaller models; both approaches have difficulty with unstructured fields requiring high OCR accuracy.

**3 First Experiments:**
1. Compare performance on same invoice using both processing strategies
2. Test model accuracy across different invoice quality levels
3. Measure processing time and resource usage for each approach

## Open Questions the Paper Calls Out
None

## Limitations
- Only three model families and eight specific versions tested, limiting generalizability
- Evaluation datasets represent limited sample of real-world invoice variability
- Zero-shot prompting may not reflect optimized performance achievable through few-shot learning
- Assumes Docling provides optimal markdown conversion without exploring alternatives

## Confidence
- Native image processing superiority: High confidence within tested model-dataset combinations, Medium for generalization
- Gemini 2.5 Pro performance ranking: High confidence for tested invoices, Medium for other document types
- Smaller models' direct image analysis struggles: High confidence within tested models, Low for extrapolation

## Next Checks
1. Test benchmarked models on invoices containing non-Latin characters and right-to-left scripts
2. Compare zero-shot performance against few-shot prompting using 5-10 exemplars per dataset
3. Evaluate alternative document conversion tools (e.g., PyMuPDF, Tika) against Docling