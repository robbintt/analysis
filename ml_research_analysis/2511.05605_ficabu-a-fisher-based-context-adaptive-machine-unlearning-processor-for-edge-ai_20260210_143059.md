---
ver: rpa2
title: 'FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for
  Edge AI'
arxiv_id: '2511.05605'
source_url: https://arxiv.org/abs/2511.05605
tags:
- unlearning
- dampening
- edge
- ficabu
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient on-device machine
  unlearning for edge AI systems, where privacy regulations require removing the influence
  of specific data from pre-trained models. The proposed solution, FiCABU, combines
  Context-Adaptive Unlearning (starting edits from back-end layers and halting when
  target forgetting is reached) with Balanced Dampening (scaling dampening strength
  by layer depth).
---

# FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI

## Quick Facts
- arXiv ID: 2511.05605
- Source URL: https://arxiv.org/abs/2511.05605
- Reference count: 40
- Key outcome: Achieves random-guess forget accuracy while matching SSD baseline on retain accuracy, with up to 87.52% MAC reduction on ResNet-18 and 71.03% on ViT, and energy consumption reduced to 6.48% (CIFAR-20) and 0.13% (PinsFaceRecognition) of baseline.

## Executive Summary
This paper addresses the challenge of efficient on-device machine unlearning for edge AI systems, where privacy regulations require removing the influence of specific data from pre-trained models. The proposed solution, FiCABU, combines Context-Adaptive Unlearning (starting edits from back-end layers and halting when target forgetting is reached) with Balanced Dampening (scaling dampening strength by layer depth). The method is realized in a full RTL RISC-V edge AI processor integrating Fisher estimation and dampening IPs into a GEMM-centric streaming pipeline, validated on FPGA and synthesized in 45 nm.

## Method Summary
FiCABU implements Fisher-based machine unlearning on edge devices through a context-adaptive approach that edits back-end layers first and halts when target forgetting is achieved. It uses balanced dampening with depth-aware scaling of hyperparameters and integrates specialized hardware IPs for Fisher estimation and parameter dampening into a GEMM-centric streaming pipeline. The processor is evaluated on ResNet-18 and ViT models using CIFAR-20 and PinsFaceRecognition datasets, demonstrating significant reductions in computation and energy consumption while maintaining unlearning quality.

## Key Results
- Achieves random-guess forget accuracy while maintaining baseline SSD retain accuracy
- Reduces MAC operations by up to 87.52% (ResNet-18) and 71.03% (ViT)
- Energy consumption drops to 6.48% (CIFAR-20) and 0.13% (PinsFaceRecognition) of SSD baseline on INT8 hardware

## Why This Works (Mechanism)

### Mechanism 1: Context-Adaptive Unlearning (Back-End-First Early Stopping)
- Claim: Starting unlearning from back-end layers and halting when target forgetting is achieved reduces computation while maintaining unlearning quality.
- Mechanism: The method iterates from layer l=1 (back-end, near classifier) to l=L (front-end, near input), computing diagonal-Fisher importance and applying dampening layer-by-layer. At user-defined checkpoints, cached activations from the initial forward pass enable partial inference to evaluate forget accuracy. When forget accuracy drops below threshold τ (random-guess level), the process terminates, leaving remaining front-end layers untouched.
- Core assumption: Fine-grained, class-specific features concentrate in back-end layers; early layers contribute less to forget-set predictions.
- Evidence anchors:
  - [abstract] "Context-Adaptive Unlearning, which begins edits from back-end layers and halts once the target forgetting is reached... reducing computation by up to 87.52 percent (ResNet-18) and 71.03 percent (ViT)."
  - [Section III-A, Table I] ResNet-18/CIFAR-20 achieves average 87.52% MAC reduction; ViT/CIFAR-20 achieves 71.03% average reduction while reaching random-guess forget accuracy.
  - [corpus] Weak direct validation. Related work on quantized unlearning (arXiv:2602.00567) addresses edge constraints but does not test back-end-first early stopping specifically.
- Break condition: If forget accuracy plateau occurs before reaching τ (e.g., class features distributed across all layers), front-end editing may be required; early stopping could fail to achieve target forgetting.

### Mechanism 2: Balanced Dampening (Depth-Aware Scaling)
- Claim: Depth-aware scaling of selection threshold α and dampening strength λ preserves retain accuracy better than uniform hyperparameters.
- Mechanism: Instead of fixed (α, λ), the method applies S(l)·(α, λ) where S(l) is a sigmoid function smaller at back-end layers (l small) and larger at front-end layers (l large). This assigns stronger dampening to back-end layers where class-specific detail resides, while protecting front-end representations.
- Core assumption: Back-end layers encode more discriminative, forget-class-specific information; front-end layers encode general features that should be preserved.
- Evidence anchors:
  - [Section III-B, Table II] Balanced Dampening achieves positive Retain Preservation Rate (RPR) across all cases: RN/CIFAR-20 avg. RPR=6.42%; ViT/CIFAR-20 avg. RPR=15.60%; RN/PinsFaceRecognition RPR=15.79%.
  - [Section III-B] "Depth-aware scaling maintains unlearning efficacy while better safeguarding retain-set performance."
  - [corpus] No direct validation found. Related unlearning methods (GUARD, CoUn) address retain preservation but use different mechanisms (data attribution, contrastive learning).
- Break condition: If class-specific features are not depth-localized (e.g., distributed architectures), the sigmoid profile S(l) may mismatch actual parameter attribution, reducing RPR gains.

### Mechanism 3: GEMM-Rate Streaming Pipeline with Lightweight IPs
- Claim: Specialized FIMD and Dampening IPs integrated into a GEMM-centric pipeline hide latency and achieve unlearning with minimal hardware overhead.
- Mechanism: The FIMD module computes diagonal Fisher via LOAD→SQUARE→ACCUMULATE→STORE pipeline (11.7× speedup vs. core execution). The Dampening module performs LOAD→COMPARE→βCALC→MULTIPLY→STORE (7.9× speedup). Both use double buffering and operate at GEMM patch rate, forming a continuous three-stage pipeline.
- Core assumption: GEMM operations dominate unlearning workload; patch-level streaming can hide IP latency within GEMM processing windows.
- Evidence anchors:
  - [Section IV-A, Table III] Specialized IPs consume only 2,185 LUTs (3.1%), 785 FFs (2.2%), and 0.81 mW (0.44%) of system power.
  - [Section IV-B, Table IV] FiCABU processor reduces energy to 6.48% (CIFAR-20) and 0.13% (PinsFaceRecognition) of SSD baseline on INT8 hardware.
  - [corpus] No direct validation of hardware streaming pipeline; edge unlearning work (arXiv:2503.02312) addresses efficiency but via gradient orthogonalization, not hardware co-design.
- Break condition: If non-GEMM operations (e.g., attention mechanisms in transformers) dominate, patch-level alignment may break; latency hiding depends on GEMM-centric workload balance.

## Foundational Learning

- Concept: **Fisher Information Matrix (Diagonal Approximation)**
  - Why needed here: Core to SSD and FiCABU—provides per-parameter importance scores from squared gradients without full Hessian computation.
  - Quick check question: Can you explain why diagonal Fisher suffices for sensitivity estimation in retraining-free unlearning?

- Concept: **Layer-Wise Feature Distribution in CNNs/ViTs**
  - Why needed here: Motivates back-end-first unlearning—class-specific features concentrate in later layers, general features in earlier layers.
  - Quick check question: In a ResNet-18, which layers would you expect to contain more class-discriminative information for a "Rocket" class?

- Concept: **GEMM Accelerator Architecture and Patch-Level Processing**
  - Why needed here: FiCABU's streaming pipeline aligns FIMD/Dampening IPs with GEMM patch cadence; understanding this is essential for integration.
  - Quick check question: How does patch-level (tile) processing enable streaming between GEMM and auxiliary IPs?

## Architecture Onboarding

- Component map: RISC-V Rocket Core -> GEMM Accelerator (VTA) -> FIMD Module -> Dampening Module -> Memory Subsystem (64 KB SRAM, DDR controller, DMA) -> System Interconnect (AXI/APB µNoC)

- Critical path: Forward pass on forget batch → cache activations at checkpoints → back-end layer iteration with: (1) diagonal Fisher computation via FIMD, (2) selection via comparison to α·I_D, (3) dampening via β-generator, (4) checkpoint evaluation via partial inference → early stop if A_forget ≤ τ.

- Design tradeoffs:
  - Checkpoint frequency: More checkpoints increase verification overhead but enable finer-grained early stopping (Table I uses 6 checkpoints for ResNet-18, 4 for ViT).
  - Sigmoid midpoint c_m: Centering at parameter distribution mid-value balances front/back-end protection; misalignment reduces RPR.
  - INT8 quantization: Required for edge deployment but may affect unlearning precision vs. FP32 simulation.

- Failure signatures:
  - Forget accuracy stuck above τ: Class features may be front-end distributed; reduce checkpoint spacing or apply uniform dampening.
  - Retain accuracy drops excessively: Over-dampening front-end layers; increase b_r to strengthen front-end protection.
  - Pipeline stalls: FIMD or Dampening IP latency exceeds GEMM patch window; verify patch size alignment and double-buffer operation.

- First 3 experiments:
  1. **Baseline SSD on FiCABU processor without specialized IPs**: Measure MACs, forget/retain accuracy, and energy to establish reference against Table IV SSD baseline.
  2. **Context-Adaptive Unlearning with single mid-point checkpoint**: Verify early stopping behavior and MAC reduction on CIFAR-20/ResNet-18; compare to 87.52% reduction claim.
  3. **Balanced Dampening with varying b_r (1, 5, 10)**: Measure RPR sensitivity on PinsFaceRecognition to validate depth-aware scaling benefits; target positive RPR per Table II.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FiCABU's Context-Adaptive Unlearning perform under sequential or cascaded unlearning requests, where multiple forget sets are processed over time?
- Basis in paper: [inferred] The paper evaluates single unlearning events per class but does not analyze whether repeated early-stopping and partial layer updates accumulate errors or degrade retain accuracy across multiple requests.
- Why unresolved: Edge devices in practice face ongoing data deletion requests; understanding long-term stability is critical for deployment.
- What evidence would resolve it: Experiments with sequential unlearning requests (e.g., 10+ forget sets applied sequentially) measuring retain accuracy drift, forget accuracy consistency, and MIA robustness over time.

### Open Question 2
- Question: How sensitive is Balanced Dampening's sigmoid schedule (parameters $c_m$ and $b_r$) to different model architectures, datasets, and forget-set sizes?
- Basis in paper: [inferred] Section III-B states $c_m$ is centered at the mid-value between smoothed extrema and $b_r = 10$ for all experiments, but does not explore sensitivity or provide adaptive selection criteria.
- Why unresolved: If hyperparameters require per-model tuning, edge deployment becomes less practical; automated or theoretical guidance is needed.
- What evidence would resolve it: Ablation studies varying $c_m$ and $b_r$ across architectures (e.g., MobileNet, EfficientNet), datasets with different inter-class similarities, and varying forget-set sizes (e.g., 1%, 5%, 10% of training data).

### Open Question 3
- Question: Does FiCABU maintain its efficiency and effectiveness when applied to larger models (e.g., ResNet-50, ViT-Large) or modalities beyond vision (e.g., NLP, audio)?
- Basis in paper: [explicit] The conclusion states FiCABU demonstrates back-end-first, depth-aware unlearning is practical "for resource-constrained edge AI devices," but experiments are limited to ResNet-18 and ViT on CIFAR-20 and PinsFaceRecognition.
- Why unresolved: Edge AI is evolving toward more diverse and capable models; scalability to larger or different architectures remains unverified.
- What evidence would resolve it: Experiments on ImageNet-scale datasets with ResNet-50/ViT-Large, and on NLP models (e.g., BERT variants for text classification unlearning), reporting MACs reduction, energy savings, and retain/forget accuracy.

### Open Question 4
- Question: How does the approximation $I_D \approx I_{D_r}$ affect unlearning quality when forget sets become large relative to the training corpus?
- Basis in paper: [explicit] Section II notes SSD uses $I_D$ rather than $I_{D_r}$ to avoid recomputation and states "the difference between $I_D$ and $I_{D_r}$ is practically negligible" when $|D_f| \ll |D|$, but this assumption may not hold for larger forget sets.
- Why unresolved: In scenarios where users request deletion of substantial data portions, the approximation could introduce unlearning errors or retain degradation.
- What evidence would resolve it: Systematic study varying $|D_f|$ from 1% to 20% of $|D|$, comparing unlearning metrics when using $I_D$ vs. explicitly computed $I_{D_r}$.

## Limitations
- The paper's central claims rely on the untested assumption that class-specific features concentrate in back-end layers across all architectures, which may not hold for distributed feature representations.
- Hardware validation is limited to a single 45 nm technology node, and the FPGA prototype may not fully capture ASIC performance.
- INT8 quantization could introduce precision loss that affects unlearning efficacy, though this is not thoroughly explored.

## Confidence
- **High Confidence**: The Context-Adaptive Unlearning mechanism achieving 87.52% MAC reduction on ResNet-18, supported by Table I results and the core claim about early termination effectiveness.
- **Medium Confidence**: The Balanced Dampening mechanism improving retain accuracy, as the RPR gains are reported but lack direct comparison to alternative hyperparameter tuning methods.
- **Medium Confidence**: The GEMM-rate streaming pipeline achieving the claimed energy reduction (6.48% for CIFAR-20, 0.13% for PinsFaceRecognition), though hardware validation is limited to a single prototype.

## Next Checks
1. **Architecture-Agnostic Feature Distribution**: Test Context-Adaptive Unlearning on architectures known for distributed feature representations (e.g., Vision Transformers with deeper layers) to verify that back-end-first editing remains effective when class-specific features are not depth-localized.

2. **Hyperparameter Sensitivity Analysis**: Conduct ablation studies varying the sigmoid midpoint (c_m) and scaling range (b_r) across multiple datasets to determine whether Balanced Dampening consistently outperforms uniform hyperparameter tuning.

3. **Cross-Technology Hardware Verification**: Synthesize FiCABU in both 45 nm and 7 nm technology nodes to quantify scaling effects on power consumption and area, validating whether the 0.44% power overhead claim holds across process nodes.