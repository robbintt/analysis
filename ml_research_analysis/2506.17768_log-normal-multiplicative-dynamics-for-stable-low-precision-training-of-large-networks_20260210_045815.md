---
ver: rpa2
title: Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large
  Networks
arxiv_id: '2506.17768'
source_url: https://arxiv.org/abs/2506.17768
tags:
- multiplicative
- weight
- learning
- training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Log-Normal Multiplicative Dynamics (LMD),
  a new optimizer that mimics biological synaptic dynamics by using multiplicative
  updates and log-normal noise injection. LMD is derived from a Bayesian learning
  rule with log-normal posterior distributions over weights, enabling stable training
  under low-precision forward operations.
---

# Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks

## Quick Facts
- arXiv ID: 2506.17768
- Source URL: https://arxiv.org/abs/2506.17768
- Reference count: 27
- The paper introduces Log-Normal Multiplicative Dynamics (LMD), a new optimizer that mimics biological synaptic dynamics by using multiplicative updates and log-normal noise injection

## Executive Summary
This paper presents Log-Normal Multiplicative Dynamics (LMD), an optimizer designed to enable stable training of large networks under low-precision forward operations. LMD is derived from a Bayesian learning rule with log-normal posterior distributions over weights, incorporating multiplicative updates that naturally mimic biological synaptic dynamics. The method is easy to implement, requiring only one additional vector compared to AdamW, and effectively addresses the exponential weight growth issue seen in standard multiplicative updates.

The authors demonstrate LMD's effectiveness through experiments on Vision Transformer and GPT-2, showing that it achieves stable and accurate training-from-scratch even when forward passes are performed in the low-precision Microscaling (MX) data format. In ViT training, LMD significantly outperforms AdamW and Madam, while in GPT-2 training it achieves better perplexity and more stable learning dynamics under low-precision forward passes. The key innovation is the multiplicative noise injection, which is crucial for maintaining stability when using low-precision representations during forward propagation.

## Method Summary
LMD is an optimizer that uses multiplicative weight updates with log-normal noise injection, derived from Bayesian learning rules with log-normal posterior distributions. The method employs the EG± trick to maintain weights near their initialization through multiplicative weight decay. A key feature is the log-normal noise injection during training, which helps maintain stability when using low-precision forward operations. The implementation requires only one additional vector compared to AdamW, making it practical for large-scale training. The optimizer uses parameters including learning rate η, noise scale σ, momentum terms β₁ and β₂, and weight decay mr, with specific values optimized for each architecture.

## Key Results
- LMD achieves 77.06% test accuracy on ViT-B/16 trained from scratch with 200 epochs, significantly outperforming AdamW and Madam baselines
- In GPT-2 training, LMD achieves better perplexity and more stable learning dynamics compared to AdamW when using low-precision forward passes
- LMD maintains stable weight norms throughout training, preventing the exponential weight growth that occurs with standard multiplicative updates

## Why This Works (Mechanism)
LMD works by maintaining weight distributions that are approximately log-normal, which naturally constrains weights to remain positive and prevents unbounded growth. The multiplicative noise injection serves as a regularization mechanism that maintains stability during low-precision forward operations by preventing the optimizer from exploiting quantization errors. The EG± trick ensures that weights stay near their initialization through multiplicative weight decay, which is particularly important when using low-precision representations where weight values can become unstable. The Bayesian foundation provides a principled way to derive the update rules while the log-normal assumption matches the natural constraints of multiplicative dynamics.

## Foundational Learning
- **Bayesian learning rules**: Framework for deriving optimization algorithms from probabilistic principles; needed to connect log-normal posteriors to multiplicative dynamics; quick check: verify posterior update matches Eq. 6
- **Multiplicative weight updates**: Weight updates that scale weights rather than add to them; needed for biological plausibility and low-precision stability; quick check: confirm weight norm remains bounded
- **Log-normal distributions**: Distributions where logarithm is normally distributed; needed to maintain positive weights and derive stable dynamics; quick check: verify weight histograms appear log-normal
- **EG± trick**: Technique for maintaining weights near initialization using separate positive/negative weight parameters; needed to prevent exponential weight growth; quick check: confirm weight initialization matches Eq. 12
- **Low-precision training challenges**: Quantization effects that can destabilize optimization; needed context for why multiplicative noise helps; quick check: verify MXFP6 emulation works correctly
- **Multiplicative weight decay**: Weight decay applied multiplicatively rather than additively; needed to maintain scale stability; quick check: confirm weight norm converges to initial value

## Architecture Onboarding

**Component Map**
Input data -> Forward pass (low-precision) -> Loss computation -> Backward pass (bfloat16) -> LMD update (multiplicative with log-normal noise) -> Updated weights

**Critical Path**
The critical path for LMD involves: 1) Sampling log-normal noise ε ~ LogN(0, σ²I), 2) Computing multiplicative update with momentum and weight decay, 3) Applying EG± trick to maintain weight scale, 4) Ensuring weights stay positive through multiplicative operations.

**Design Tradeoffs**
- LMD trades additional noise sampling computation for improved stability in low-precision regimes
- The method requires maintaining additional state vectors (m, v) similar to AdamW but with multiplicative dynamics
- Log-normal noise injection adds randomness but prevents optimizer exploitation of quantization errors
- EG± trick adds implementation complexity but prevents exponential weight growth

**Failure Signatures**
- Excessive weight growth indicates missing or incorrect multiplicative weight decay
- Training instability with MXFP6 suggests noise injection is disabled or σ is too small
- Degraded accuracy may indicate temperature τ initialization is incorrect
- NaN or Inf values suggest numerical instability in log-normal sampling

**First Experiments**
1. Verify LMD implementation matches Algorithm 1 with correct EG± trick application
2. Train ViT-B/16 on ImageNet for 10 epochs with η=0.005, σ=0.125 to check weight norm stability
3. Compare LMD vs AdamW training curves on a subset of ImageNet to validate performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that multiplicative noise injection is "crucial" for low-precision stability is based on visual inspection rather than quantitative metrics comparing noisy vs. mean-only training
- The specific temperature parameter τ in the weight initialization equation remains unspecified despite being critical for deriving mr
- The paper's primary claims rely on a single ViT experiment and one GPT-2 experiment, limiting generalizability

## Confidence

**High confidence**: LMD implementation details and the EG± trick are clearly specified with pseudocode (Algorithm 1). The ViT training procedure (hyperparameters, architecture details, MXFP6 emulation) is reproducible as described.

**Medium confidence**: The theoretical derivation connecting log-normal posteriors to multiplicative dynamics is sound, but the practical impact of the temperature parameter τ on weight initialization is unclear without explicit values.

**Low confidence**: The claim that multiplicative noise injection is "crucial" for low-precision stability is based on visual inspection of Figure 4 rather than quantitative metrics comparing noisy vs. mean-only training.

## Next Checks
1. **Temperature parameter validation**: Systematically vary τ in weight initialization and measure its impact on final ViT accuracy and weight norm stability to determine if the heuristic initialization works across different settings.
2. **Ablation study quantification**: Implement a controlled experiment comparing LMD with noise injection disabled (pure multiplicative updates) versus enabled, measuring both accuracy and weight norm trajectories on ViT to quantify the exact contribution of noise to stability.
3. **Computational overhead measurement**: Profile LMD training to measure the additional computational cost from log-normal noise sampling and compare wall-clock training time against AdamW baseline on identical hardware.