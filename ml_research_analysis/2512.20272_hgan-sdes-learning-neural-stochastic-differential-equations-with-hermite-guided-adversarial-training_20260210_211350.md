---
ver: rpa2
title: 'HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided
  Adversarial Training'
arxiv_id: '2512.20272'
source_url: https://arxiv.org/abs/2512.20272
tags:
- neural
- hermite
- functions
- stochastic
- sdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HGAN-SDEs, a GAN framework for learning Neural
  Stochastic Differential Equations (Neural SDEs) that uses Neural Hermite functions
  as the discriminator. This approach addresses the instability and computational
  inefficiency of existing SDE GANs that rely on Neural Controlled Differential Equations
  (CDEs) for trajectory discrimination.
---

# HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training

## Quick Facts
- arXiv ID: 2512.20272
- Source URL: https://arxiv.org/abs/2512.20272
- Authors: Yuanjian Xu; Yuan Shuai; Jianing Hao; Guang Zhang
- Reference count: 31
- Primary result: Achieves state-of-the-art performance in learning Neural Stochastic Differential Equations using Hermite functions as discriminators, reducing computational cost by 8.3x while improving stability

## Executive Summary
This paper introduces HGAN-SDEs, a GAN framework for learning Neural Stochastic Differential Equations (Neural SDEs) that replaces computationally expensive Controlled Differential Equation (CDE) discriminators with Neural Hermite functions. The method leverages Hermite functions as an orthogonal polynomial basis to efficiently approximate SDE transition densities while maintaining training stability and reduced computational complexity. The framework demonstrates universal approximation properties and strong theoretical convergence guarantees for a broad class of SDE-driven distributions. Empirical results show HGAN-SDEs significantly outperforms competing methods including linear models, CNNs, transformers, RNNs, and prior SDE GANs across synthetic benchmarks (GBM, OU, CIR, Polynomial Drift) and real-world datasets (Stock-AAL, Stock-ADBE, Traffic).

## Method Summary
HGAN-SDEs employs a generator based on Neural SDEs with Lipschitz drift and diffusion networks, paired with a discriminator that uses truncated Hermite polynomial expansions. The Hermite basis provides an orthogonal, convergent approximation to transition densities in L² space, avoiding the computational burden of solving CDEs. The method uses a Wasserstein GAN objective with 3-4 Hermite terms and 3-layer MLPs for coefficient networks, achieving both theoretical guarantees (Theorem 2) and practical efficiency gains through reduced gradient variance and computational overhead.

## Key Results
- Achieves 8.3x speedup (10.0→1.2 GPU-hours) compared to CDE-based SDE GANs while maintaining or improving performance
- Outperforms linear models, CNNs, transformers, RNNs, and prior SDE GANs on synthetic benchmarks (GBM, OU, CIR, Polynomial Drift)
- Demonstrates superior MISE metrics on real-world datasets (Stock-AAL, Stock-ADBE, Traffic) with only 3-4 Hermite terms required
- Shows improved training stability with reduced gradient variance compared to CDE-based discriminators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hermite functions provide a convergent basis for approximating SDE transition densities, enabling discriminators to capture path-level distributions without solving expensive differential equations.
- **Mechanism:** Hermite functions {ψₙ(x)} form a complete orthonormal basis in L²(ℝ, dx). The paper shows that under Lipschitz and bounded-moment assumptions, stationary transition densities can be approximated as p(x) ≈ Σcₙψₙ(x), with convergence in L² as N→∞ (Theorem 2). This transforms the discriminator from solving CDEs (computationally expensive) to evaluating a truncated polynomial series.
- **Core assumption:** The SDE drift fθ and diffusion gθ are Lipschitz continuous with bounded moments (Assumption 1); transition densities are square-integrable.
- **Evidence anchors:**
  - [Section 3.2]: "Hermite functions form a complete orthonormal basis in the Hilbert space L²(ℝ, dx)... guarantee[ing] that any square-integrable function... can be approximated arbitrarily well"
  - [Section 3.2, Theorem 2]: Convergence proof under Assumption 1
  - [corpus]: Related work on Neural SDEs confirms computational challenges with CDE-based discriminators (Kidger et al., 2021 cited in paper)
- **Break condition:** If drift/diffusion violate Lipschitz conditions, or if transition densities are not square-integrable (e.g., heavy-tailed distributions beyond L²), convergence guarantees may not hold.

### Mechanism 2
- **Claim:** Replacing CDE discriminators with Hermite-based discriminators reduces gradient variance during adversarial training, improving stability.
- **Mechanism:** CDE discriminators require backpropagation through continuous-time dynamics, introducing high gradient variance (as noted: "exacerbate the instability of adversarial training"). Hermite discriminators compute fixed polynomial basis functions, yielding well-conditioned gradients. The orthonormality property (Theorem 1) prevents basis function correlation that could amplify gradient noise.
- **Core assumption:** The GAN training dynamics benefit from lower-gradient-variance discriminators; the WGAN objective provides meaningful gradient signals.
- **Evidence anchors:**
  - [Abstract]: "Hermite functions provide an expressive yet lightweight basis... enabling both reduced runtime complexity and improved training stability"
  - [Section 4.2]: "GAN-SDEs with CDE-based discriminators further suffer from high gradient variance, causing unstable training and performance collapse"
  - [corpus]: Weak direct evidence—related papers discuss SDE training challenges but don't specifically analyze Hermite gradient properties
- **Break condition:** If adversarial training instability stems primarily from generator-discriminator capacity imbalance rather than discriminator gradient properties, Hermite replacement may not resolve instability.

### Mechanism 3
- **Claim:** Truncating Hermite expansion to 3-4 terms preserves discriminative power while minimizing overfitting and computational cost.
- **Mechanism:** Higher-order Hermite coefficients decay exponentially for well-behaved SDEs (demonstrated for OU process: cₙ(t) = cₙ(0)e^{-θnt}). The paper argues most information is captured in low-order terms. Table 4 and Figure 2 empirically validate that 3-4 terms achieve optimal MISE.
- **Core assumption:** Target SDEs have sufficiently regular transition densities that low-frequency Hermite components dominate.
- **Evidence anchors:**
  - [Section 3.2]: "higher-order components decay exponentially, leading to increasingly regular trajectories"
  - [Section 4.3, Figure 2]: "three to four terms suffice"
  - [corpus]: No direct external validation of truncation sufficiency
- **Break condition:** For SDEs with sharp transitions, multi-modal densities, or path-dependent discontinuities, more Hermite terms may be required; the 3-4 term heuristic may underfit.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs)**
  - **Why needed here:** The generator is fundamentally an SDE; understanding drift (deterministic trend) and diffusion (stochastic perturbation) terms is essential for debugging generated trajectories.
  - **Quick check question:** Can you explain why dx(t) = fθdt + gθ∘dw(t) produces a distribution over paths rather than a single trajectory?

- **Concept: Orthogonal Polynomial Bases / Hilbert Space Approximation**
  - **Why needed here:** The discriminator's power derives from Hermite orthonormality; grasping why basis orthogonality enables stable, expressive function approximation clarifies the architecture's advantage.
  - **Quick check question:** Why does orthonormality (⟨ψₙ, ψₘ⟩ = δₙₘ) help prevent gradient amplification in neural network training?

- **Concept: Wasserstein GAN (WGAN) Training Dynamics**
  - **Why needed here:** HGAN-SDEs use the WGAN objective; understanding critic/discriminator training, gradient penalty concepts, and convergence behavior helps diagnose training failures.
  - **Quick check question:** In WGAN, why does the discriminator output an unbounded real value rather than a probability?

## Architecture Onboarding

- **Component map:**
  Generator (Neural SDE) -> Noise input v ~ N(0, I_v) -> Initial state network h_θ(v) -> Drift network f_θ(t, x) [Lipschitz MLP, 2-3 layers] -> Diffusion network g_θ(t, x) [Lipschitz MLP, 2-3 layers] -> SDE solver (numerical integration)
  
  Discriminator (Neural Hermite) -> Hermite basis functions ψ_0(x), ..., ψ_N(x) [N=3-4] -> Basis coefficient networks [3-layer MLP, hidden=128] -> Linear combination: D_ϕ(x) = Σ c_ϕ,n · ψ_n(x)

- **Critical path:**
  1. Sample noise batch → Generator produces trajectory batch via SDE integration
  2. Sample real trajectory batch from dataset
  3. Compute Hermite basis evaluations for all trajectories
  4. Discriminator scores real vs. generated via linear combination
  5. Update discriminator (maximize real score - generated score)
  6. Update generator (minimize negative discriminator score on generated)
  7. Repeat with gradient penalty (Assumption: WGAN-GP variant implied)

- **Design tradeoffs:**
  - **Hermite terms (N):** More terms → better approximation but higher compute/risk of overfitting. Paper shows N=3-4 is sweet spot.
  - **SDE solver precision vs. speed:** Finer integration steps → more accurate paths but slower training. Not explicitly tuned in paper.
  - **Discriminator MLP size:** 3-layer, 128-hidden is default; larger may improve discrimination but slows convergence.

- **Failure signatures:**
  - Mode collapse (generated paths lack diversity): Check if diffusion network g_θ outputs near-zero; may need to constrain diffusion scale.
  - Discriminator too weak (MMD plateaus): Increase Hermite terms or MLP capacity; verify Lipschitz constraints.
  - Training instability (loss oscillates): May indicate CDE-compatibility issue—ensure you're using Hermite discriminator, not CDE; reduce learning rate.
  - Poor tail fit (TD metric high): Hermite truncation may be insufficient; increase N or inspect data for heavy tails.

- **First 3 experiments:**
  1. **Reproduce OU process benchmark** with default settings (N=3, 3-layer MLP, hidden=128). Target: MISE < 1.0 per Table 1. This validates implementation correctness on a process with known Hermite coefficients.
  2. **Ablate Hermite term count (N=1,2,3,4,5)** on GBM dataset. Plot MISE vs. N to confirm elbow at N=3-4. This tests the truncation assumption on your implementation.
  3. **Compare discriminator architectures** (Hermite vs. MLP vs. LSTM) using identical Neural SDE generator on Polynomial Drift dataset. Measure MISE, GPU-hours, and training stability (loss variance). Target: Hermite achieves MISE < 0.1 and GPU-hours < 2 per Table 4. This validates the core architectural claim.

## Open Questions the Paper Calls Out
None

## Limitations
- Tail behavior uncertainty: The method assumes transition densities are square-integrable (L²), which may not hold for heavy-tailed distributions. No empirical tests on such cases are provided.
- Generalization beyond synthetic benchmarks: While synthetic OU, GBM, CIR, and Polynomial Drift processes are well-approximated, real-world financial and traffic data may exhibit path dependencies or regime switches not captured by the tested SDE classes.
- Hyperparameter sensitivity: The paper uses fixed Hermite term count (N=3-4) and MLP architectures across all datasets. It's unclear how sensitive performance is to these choices for data with different statistical properties.

## Confidence
- **High confidence:** Synthetic benchmark performance (OU, GBM, CIR, Polynomial Drift). These controlled experiments validate the core mechanism with known ground truth and clear theoretical justification (Theorem 2).
- **Medium confidence:** Computational efficiency claims. The 8.3x speedup (10.0→1.2 GPU-hours) is demonstrated, but ablation studies on solver precision and hardware variability are absent.
- **Medium confidence:** Real-world dataset generalization. Stock and traffic data results show improvement, but the underlying data-generating processes are not verified SDEs, making it difficult to assess whether gains reflect better SDE learning or other factors.

## Next Checks
1. **Stress test on heavy-tailed data:** Evaluate HGAN-SDEs on synthetic SDEs with Cauchy-like or α-stable noise (transition densities outside L²). Measure MISE degradation compared to baseline methods to quantify the limits of the Hermite approximation.
2. **Cross-dataset hyperparameter transferability:** Train HGAN-SDEs with identical hyperparameters (N=3, 3-layer MLP) on an out-of-distribution dataset (e.g., medical time series or weather data). Compare MISE to a model with dataset-specific tuning to assess robustness.
3. **Ablate SDE solver precision:** For the real-world datasets, systematically vary the numerical integration step size (e.g., 10⁻³ to 10⁻¹) while measuring MISE and GPU-hours. This isolates the contribution of solver accuracy to the claimed efficiency gains.