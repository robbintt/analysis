---
ver: rpa2
title: 'LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers'
arxiv_id: '2601.10155'
source_url: https://arxiv.org/abs/2601.10155
tags:
- attention
- quantization
- compression
- lookat
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LOOKAT addresses the memory-bandwidth bottleneck in KV-cache compression
  for large language models on edge devices. It recognizes that attention scoring
  is equivalent to inner-product similarity search and applies product quantization
  and asymmetric distance computation from vector databases.
---

# LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers

## Quick Facts
- arXiv ID: 2601.10155
- Source URL: https://arxiv.org/abs/2601.10155
- Authors: Aryan Karmore
- Reference count: 21
- Primary result: Achieves 64× KV-cache compression with 95.7% fidelity on GPT-2 while maintaining rank correlation ρ > 0.95

## Executive Summary
LOOKAT addresses the memory-bandwidth bottleneck in KV-cache compression for large language models on edge devices. It recognizes that attention scoring is equivalent to inner-product similarity search and applies product quantization and asymmetric distance computation from vector databases. This approach decomposes key vectors into subspaces, learns codebooks via K-means clustering, and computes attention scores using lookup tables instead of dequantization. The method achieves 64× compression with 95.7% output fidelity and 32× compression with 95.0% fidelity on GPT-2 while maintaining rank correlation ρ > 0.95. Theoretical analysis confirms rank correlation degrades as O(dk/mK), validated across sequence lengths up to 1024 tokens.

## Method Summary
LOOKAT compresses KV-cache by recognizing attention scoring as inner-product similarity search, enabling transfer of vector database compression techniques. The method decomposes key vectors into m subspaces, learns K=256 centroids per subspace via K-means clustering on calibration keys, and encodes each key as m uint8 indices. For each query, it precomputes m lookup tables (LUTs) containing dot products between the query subspace and codebook centroids. Attention scores are computed by summing LUT lookups for each compressed key, eliminating dequantization. The approach requires no architectural changes or retraining, transforms attention from memory-bound to compute-bound, and uses only 32 KB of codebook storage per layer.

## Key Results
- Achieves 64× compression with 95.7% fidelity (m=2) and 32× compression with 95.0% fidelity (m=4) on GPT-2
- Maintains Spearman rank correlation ρ > 0.95 across all compression settings
- Transforms attention from memory-bound to compute-bound, reducing memory bandwidth by 32-64×
- Theoretical rank correlation degradation bound of O(dk/mK) validated empirically across sequence lengths up to 1024 tokens

## Why This Works (Mechanism)

### Mechanism 1
Attention scoring is mathematically equivalent to inner-product similarity search, enabling direct transfer of compression techniques from vector databases. Softmax over attention scores preserves rank ordering rather than requiring absolute values, matching how approximate nearest neighbor systems operate—both need correct relative rankings, not exact distances. The core assumption is that attention mechanisms are rank-preserving operations where top-k attended tokens matter more than precise score magnitudes.

### Mechanism 2
Decomposing key vectors into subspaces with independent codebooks preserves structure while achieving 32–64× compression. Splitting head dimension d_k into m subspaces and learning K=256 centroids per subspace via K-means on calibration keys enables encoding each key as m uint8 indices. For d_k=64, m=4: 128 bytes → 4 bytes (32×). The core assumption is that key vectors exhibit low intrinsic dimensionality with correlated subspace structure amenable to joint quantization.

### Mechanism 3
Asymmetric Distance Computation via precomputed lookup tables eliminates dequantization, transforming attention from memory-bound to compute-bound. Query subspaces precompute dot products with codebooks → LUTs, then for each compressed key, attention score = sum of m LUT lookups. No key reconstruction needed. The core assumption is that the O(m) lookup cost plus O(m·256) precomputation is negligible compared to O(L·d_k) memory bandwidth.

## Foundational Learning

- **Product Quantization (PQ)**: Core compression technique—must understand subspace decomposition, codebook learning, and encoding. *Quick check*: For a 64-dimensional key vector split into 4 subspaces with 256 centroids each, what is the compression ratio and bytes per key?
- **Asymmetric Distance Computation (ADC)**: Enables score computation without dequantization—query stays FP16, database stays compressed. *Quick check*: Why does ADC achieve the same ranking as full-precision dot products despite never reconstructing vectors?
- **Spearman's Rank Correlation (ρ)**: Primary metric for attention quality—softmax depends on rankings, not magnitudes. *Quick check*: Why is ρ > 0.95 sufficient for attention preservation even when cosine similarity < 0.96?

## Architecture Onboarding

- **Component map**: Calibration Data → K-Means Clustering → Codebooks (32 KB/layer) → Key Encoder (m uint8 indices) → LUT Precomputer (m×256 FP16 values) → Score Aggregator (Σ LUT lookups) → Softmax + Value Weighted Sum
- **Critical path**: Offline: Collect calibration data → fit K-means per subspace → store codebooks per layer/head; Inference: Precompute m LUTs → for each key, m lookups + (m-1) adds → softmax → value aggregation
- **Design tradeoffs**: m=2: Highest compression (64×), lowest compute, but coarser subspaces; surprisingly achieves 95.7% fidelity; m=4: Paper's "sweet spot"—32× compression with 95.0% fidelity; Higher m: More codebook storage, marginally better quality, less compression
- **Failure signatures**: Domain shift (codebooks trained on prose may degrade on code/technical text); Long-context degradation (cosine similarity drops from 0.999 to 0.903 at L=1024); Top-5 accuracy gap (78–80% vs 100% baseline); Calibration sensitivity (codebook quality depends on representative calibration data)
- **First 3 experiments**: 1) Replicate compression-fidelity Pareto: Test LOOKAT-{2,4,8,16} on GPT-2 layer 0, measure cosine similarity, Spearman ρ, KL divergence, Top-5 accuracy; 2) Sequence length scaling validation: Run LOOKAT-4 on L∈{64,128,256,512,1024}, plot degradation curve; 3) Memory bandwidth profiling: On target edge hardware, measure actual DRAM transfers for INT4 keys vs LOOKAT indices + LUT lookups

## Open Questions the Paper Calls Out

1. **Value Compression Extension**: Can product quantization be extended to value compression while preserving the weighted sum computation in attention, and what accuracy trade-offs would result? The paper notes this is non-trivial because of the weighted sum requirements.

2. **Architectural Compatibility**: Does LOOKAT maintain fidelity when combined with multi-query or grouped-query attention architectures that share KV heads? The paper suggests testing with GQA/MQA to measure memory savings.

3. **Large-Scale Scaling**: How does LOOKAT scale to modern LLMs with larger head dimensions (e.g., d_k = 128) and longer contexts (L > 1024)? The theoretical bound suggests sensitivity to d_k and mK that requires empirical validation on larger models.

## Limitations

- Calibration data generalization across extreme domains (mathematical reasoning, legal text, low-resource languages) remains unproven
- Hardware-specific performance claims lack empirical validation on target edge platforms
- Asymmetric compression (keys compressed, values not) may limit total memory savings and introduce compounding errors

## Confidence

- **High Confidence**: Core attention-as-similarity-search equivalence is mathematically proven with O(dk/mK) rank correlation bound
- **Medium Confidence**: 32-64× compression with 95%+ fidelity demonstrated on GPT-2, but limited to one model and moderate sequence lengths
- **Low Confidence**: Memory-bandwidth transformation claims lack hardware profiling; assumed performance gains on actual edge devices remain speculative

## Next Checks

1. **Hardware-Aware Validation**: Profile LOOKAT on target edge hardware (ARM Cortex-A78, Apple Neural Engine, or NPU) measuring actual DRAM bandwidth reduction and inference latency compared to INT4 baseline and uncompressed attention.

2. **Domain Robustness Testing**: Evaluate LOOKAT across diverse text domains including mathematical reasoning, legal documents, and low-resource languages, measuring calibration data sensitivity by training codebooks on different text types.

3. **Long-Context Stress Test**: Extend validation to sequences beyond 1024 tokens (e.g., L=2048, 4096) and specialized long-context tasks, tracking quality degradation patterns to identify whether exponential decay in rank correlation becomes problematic.