---
ver: rpa2
title: 'False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual
  Language Models'
arxiv_id: '2509.18750'
source_url: https://arxiv.org/abs/2509.18750
tags:
- overlap
- english
- language
- tokens
- high-sim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether overlapping tokens across languages
  in multilingual tokenizers help or hinder cross-lingual transfer. To control for
  confounders like token frequency and segmentation granularity, the authors train
  bilingual autoregressive models on six language pairs under four systematically
  varied vocabulary overlap settings: full, high-similarity, low-similarity, and no
  overlap.'
---

# False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models

## Quick Facts
- arXiv ID: 2509.18750
- Source URL: https://arxiv.org/abs/2509.18750
- Reference count: 40
- Primary result: Token overlap improves cross-lingual transfer, with high-similarity overlap yielding strongest gains

## Executive Summary
This study systematically investigates whether overlapping tokens across languages in multilingual tokenizers help or hinder cross-lingual transfer. The authors control for confounding factors like token frequency and segmentation granularity by training bilingual autoregressive models on six language pairs under four vocabulary overlap settings: full, high-similarity, low-similarity, and no overlap. Results show that token overlap enables embedding spaces to capture cross-lingual semantic relationships, with stronger effects for high-similarity overlap. Models with any overlap outperform those with disjoint vocabularies in downstream tasks (XNLI and XQuAD), and transfer performance generally improves as overlap increases.

## Method Summary
The authors train 24 bilingual GPT-2-style autoregressive Transformers (85M non-embedding parameters) on CCMatrix parallel texts for six language pairs (English with Spanish, German, Turkish, Chinese, Arabic, and Swahili). They create four vocabulary overlap settings by offsetting L2 token indices for non-shared tokens, then rank overlap tokens using XLM-R layer-5 contextual embeddings computed from 100 occurrences per token. Models are pretrained for 100K steps with AdamW optimization, then fine-tuned on MultiNLI and SQuAD with hyperparameter sweeps. Zero-shot transfer is evaluated on XNLI and XQuAD for the target languages.

## Key Results
- Any vocabulary overlap (even low-similarity) outperforms no overlap in cross-lingual transfer
- High-similarity overlap yields the strongest gains, particularly for distant language pairs
- Full overlap performs well but high-similarity overlap is often comparable or better
- Embedding spaces with overlap better capture cross-lingual semantic relationships

## Why This Works (Mechanism)
Vocabulary overlap provides shared semantic anchors that allow embedding spaces to learn cross-lingual correspondences. When tokens are semantically similar across languages, the model can use these shared representations to transfer knowledge more effectively. High-similarity overlap provides stronger anchors for distant language pairs where semantic relationships are less obvious, enabling better cross-lingual mapping.

## Foundational Learning
- **Vocabulary overlap definition**: The set of tokens shared between two languages' tokenizers. Why needed: This is the core experimental variable being manipulated.
- **Contextual embedding similarity**: Using XLM-R layer embeddings to measure semantic similarity between tokens across languages. Why needed: Provides a principled way to categorize overlap into high vs low similarity.
- **Cross-lingual transfer**: The ability of a model trained on one language to perform well on another language without additional training. Why needed: The ultimate evaluation metric for multilingual models.
- **Zero-shot evaluation**: Testing a model on a task in a language it wasn't explicitly fine-tuned on. Why needed: Standard method for assessing cross-lingual capabilities.
- **Index offset technique**: Creating disjoint vocabularies by offsetting token indices in one language. Why needed: Allows precise control over overlap amount while maintaining other factors constant.

## Architecture Onboarding

**Component map**: Tokenizer (XLM-R SP) -> Bilingual Corpus (CCMatrix) -> Autoregressive Transformer (GPT-2 style) -> Fine-tuning (MultiNLI/SQuAD) -> Zero-shot Evaluation (XNLI/XQuAD)

**Critical path**: Token overlap selection → Vocabulary construction → Bilingual pretraining → Fine-tuning on source language → Zero-shot transfer evaluation

**Design tradeoffs**: 
- High overlap enables better transfer but increases vocabulary size and memory requirements
- Low overlap reduces memory but sacrifices cross-lingual generalization
- Semantic quality of overlap matters more than quantity for distant language pairs

**Failure signatures**: 
- No Overlap models failing to transfer indicates vocabulary sharing is crucial
- Similar performance between High and Full overlap suggests diminishing returns beyond semantic quality
- Better transfer for similar language pairs indicates overlap helps more when languages are distant

**Three first experiments**:
1. Verify overlap partitioning by computing average cosine similarity of O_hi vs O_lo tokens
2. Check effective vocabulary sizes match Table 2 calculations
3. Replicate XNLI zero-shot accuracy results for one language pair

## Open Questions the Paper Calls Out
1. Do the benefits of semantic vocabulary overlap generalize to non-English-centric language pairings?
2. Can models with disjoint vocabularies achieve comparable transfer performance given significantly extended training or larger parameter budgets?
3. How does the semantic quality of overlap interact with tokenizers of varying design choices and compression rates?

## Limitations
- Results from controlled bilingual settings may not generalize to massively multilingual tokenizers
- Study only examines transfer from English to other languages, not symmetric transfer
- Uses XLM-R-based semantic similarity which may not reflect autoregressive model behavior

## Confidence
- **High confidence**: Any overlap improves cross-lingual transfer; high-similarity overlap particularly effective for distant pairs
- **Medium confidence**: Specific performance magnitudes across overlap settings; relative effectiveness of high-similarity vs full overlap
- **Low confidence**: Generalization to massively multilingual settings; accuracy of semantic similarity measure for autoregressive models

## Next Checks
1. Verify overlap partitioning methodology by computing average cosine similarity within O_hi versus O_lo
2. Confirm effective vocabulary sizes match Table 2 calculations using N_eff = N_L1 + N_L2 - N_overlap
3. Replicate downstream task results by running zero-shot inference on XNLI and XQuAD and comparing to reported scores