---
ver: rpa2
title: 'RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing'
arxiv_id: '2508.18642'
source_url: https://arxiv.org/abs/2508.18642
tags:
- writing
- reward
- quality
- constraint
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing subjective writing
  quality and objective constraint adherence in creative writing tasks. Existing single-reward
  and fixed-weight mixed-reward approaches fail to optimize both aspects simultaneously.
---

# RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing

## Quick Facts
- **arXiv ID:** 2508.18642
- **Source URL:** https://arxiv.org/abs/2508.18642
- **Reference count:** 30
- **Primary result:** RLMR achieves consistent improvements across multiple model scales (8B-72B parameters), outperforming baseline methods on automated benchmarks: writing quality improves from 6.14 to 7.93 on WritingBench, and instruction following increases from 83.36% to 86.65% on IFEval.

## Executive Summary
This paper addresses the challenge of balancing subjective writing quality and objective constraint adherence in creative writing tasks. Existing single-reward and fixed-weight mixed-reward approaches fail to optimize both aspects simultaneously. The authors propose RLMR, a dynamic mixed-reward GRPO framework that combines a writing reward model for evaluating subjective quality with a constraint verification model for assessing objective compliance. The key innovation is dynamically adjusting constraint reward weights based on actual constraint compliance within sampled groups, ensuring constraint-violating samples receive negative advantages during training. RLMR achieves consistent improvements across multiple model scales (8B-72B parameters), outperforming baseline methods on automated benchmarks: writing quality improves from 6.14 to 7.93 on WritingBench, and instruction following increases from 83.36% to 86.65% on IFEval. Manual expert evaluation shows a 72.75% win rate on the real-world WriteEval benchmark. The framework demonstrates effective multi-dimensional optimization for creative writing tasks.

## Method Summary
RLMR combines two reward models: a writing reward model (trained on 200K human-labeled preference pairs using Pointwise Bradley-Terry) and a constraint verification model (Qwen2.5-72B-Instruct, binary all-or-nothing). The framework applies GRPO with dynamic penalty adjustment: for constraint-violating samples, it subtracts a calculated δ to ensure negative advantages, where δ ≥ (n·r_max^vio + n·γ - Σr_i) / (n-k). Groups are filtered using DAPO-style criteria (all rewards above/below thresholds, or all violate constraints), with resampling if insufficient data remains. Training uses 128 H20 GPUs, 1 epoch (68 steps), LR=1e-6, batch=128, 8 samples/query, temp=1.0, max_tokens=14K.

## Key Results
- WritingBench scores improve from 6.14 to 7.93 across all model scales
- IFEval accuracy increases from 83.36% to 86.65% across all model scales
- Manual expert evaluation shows 72.75% win rate on WriteEval benchmark
- Single-reward models generate excessively long outputs (>1400 tokens) and show declining IFEval scores
- Fixed-weight approaches fail to effectively penalize high-quality constraint violators

## Why This Works (Mechanism)
The framework dynamically adjusts constraint reward weights based on actual compliance within sampled groups. By calculating a penalty δ that ensures violating samples receive negative advantages, RLMR prevents the reward model from being dominated by high-quality but non-compliant outputs. The DAPO-style filtering removes degenerate groups where all samples share the same reward pattern, ensuring meaningful gradient signals. This approach allows the model to learn a balance between creative expression and constraint adherence that static weighting cannot achieve.

## Foundational Learning

**GRPO (Generalized Reward Policy Optimization)**: An RL algorithm that optimizes policies using reward signals without value function estimation. Needed because it directly optimizes the generation policy using the combined writing and constraint rewards. Quick check: Verify policy gradients are computed correctly from the adjusted rewards.

**Dynamic Reward Adjustment**: The mechanism of calculating δ to ensure constraint-violating samples have negative advantages. Needed because fixed-weight approaches cannot adaptively balance competing objectives. Quick check: Confirm δ calculation ensures r'_j < mean - γ for all violating samples.

**DAPO (Diverse and Adaptive Policy Optimization)**: A filtering strategy that removes groups with uniform reward patterns. Needed because gradient updates from identical samples provide no learning signal. Quick check: Monitor filtering rate and verify it removes degenerate groups while preserving diverse examples.

**Bradley-Terry Preference Model**: A pairwise comparison model used to train the writing reward model. Needed because it can learn relative quality judgments from preference data. Quick check: Validate that the model correctly ranks writing samples according to human preferences.

**Constraint Verification**: Binary all-or-nothing evaluation of whether responses meet specified requirements. Needed because creative writing tasks often have strict formatting or content constraints. Quick check: Test the verifier on known constraint violations to ensure accuracy.

## Architecture Onboarding

**Component Map:** Seed Data -> Self-Instruct Expansion -> Reward Model Training (Writing RM, Constraint Verifier) -> GRPO Training (RLMR) -> Evaluation (Automated + Manual)

**Critical Path:** The most important sequence is Seed Data → Self-Instruct Expansion → Reward Model Training → GRPO Training. The quality of the reward models directly determines the effectiveness of the RL optimization.

**Design Tradeoffs:** Dynamic weight adjustment vs. fixed weights (flexibility vs. simplicity), all-or-nothing constraint verification vs. partial credit (clear signals vs. nuanced feedback), binary filtering vs. continuous adjustment (computational efficiency vs. precision).

**Failure Signatures:** Reward hacking (excessive output length, declining IFEval scores), gradient vanishing (zero advantages from uniform groups), fixed-weight ineffectiveness (high-quality violators still rewarded).

**Three First Experiments:**
1. Train baseline GRPO with fixed constraint weight (e.g., 0.5) to establish performance floor
2. Implement dynamic penalty with γ=1.0 and verify δ ensures negative advantages for violators
3. Add DAPO filtering with simple percentile-based thresholds and measure impact on training stability

## Open Questions the Paper Calls Out
None

## Limitations
- Dynamic penalty calculation depends critically on the minimum gap parameter γ, which is only shown as 1 in Figure 2 without clear specification across all experiments
- DAPO-style filtering thresholds for high/low rewards and resampling conditions remain underspecified, potentially affecting training stability
- The writing reward model training methodology beyond the loss function is not fully detailed, making it difficult to assess whether reported improvements are primarily due to the dynamic mixing strategy

## Confidence
- **High confidence**: The core architectural innovation of dynamically adjusting constraint reward weights based on actual compliance is technically sound and addresses a genuine problem in mixed-objective RL training
- **Medium confidence**: The automated benchmark results (WritingBench, IFEval, ComplexBench) appear robust, though the evaluation methodology for constraint verification could benefit from more detailed validation
- **Low confidence**: The manual evaluation win rates, particularly the 72.75% on WriteEval, would benefit from more transparent methodology regarding evaluator selection, inter-rater reliability, and potential bias mitigation

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary γ and the filtering thresholds across multiple runs to quantify their impact on final performance and identify optimal configurations for different model scales
2. **Ablation on reward model quality**: Train RLMR with simplified or lower-quality reward models to determine whether the framework's effectiveness depends critically on having highly accurate writing and constraint models
3. **Cross-dataset generalization**: Apply the trained RLMR models to creative writing tasks with different constraint types and complexity levels to assess whether the learned balancing strategy generalizes beyond the training distribution