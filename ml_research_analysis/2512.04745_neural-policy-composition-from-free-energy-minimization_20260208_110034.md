---
ver: rpa2
title: Neural Policy Composition from Free Energy Minimization
arxiv_id: '2512.04745'
source_url: https://arxiv.org/abs/2512.04745
tags:
- gatemod
- softmax
- gateframe
- gateflow
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GateMod addresses the challenge of understanding how natural agents
  compose previously acquired skills to plan and execute complex behaviors, linking
  gating mechanisms to the underlying decision-making task and neural circuit architecture.
  The core method introduces a theoretically grounded computational model that casts
  policy gating into the minimization of free energy.
---

# Neural Policy Composition from Free Energy Minimization

## Quick Facts
- **arXiv ID:** 2512.04745
- **Source URL:** https://arxiv.org/abs/2512.04745
- **Reference count:** 40
- **Primary result:** GateMod provides a theoretically grounded model for policy gating that consistently recovers natural behaviors and better explains human decision-making than established models.

## Executive Summary
GateMod addresses the challenge of understanding how natural agents compose previously acquired skills to plan and execute complex behaviors. The framework links gating mechanisms to the underlying decision-making task and neural circuit architecture through free energy minimization. It provides interpretable insights into behavioral modulation while demonstrating better explanatory power than established models in both collective animal behavior and human decision-making tasks.

## Method Summary
GateMod introduces a three-component framework: GateFrame formulates primitives gating as a free energy optimization problem combining task performance and behavioral diversity; GateFlow provides continuous-time energy-based dynamics provably converging to the optimal solution; and GateNet implements this dynamics through a neural circuit with interpretable elements. The model casts policy gating into minimizing a functional that balances statistical complexity and entropy, with the softmax gating rule emerging naturally from the optimization.

## Key Results
- GateMod consistently recovers well-known collective behaviors in multi-agent systems, such as polarization and leader-guided navigation
- The model provides interpretable insights into the modulation of social forces during collective behavior
- In human decision-making tasks, GateMod better explains data than established models and reveals structured patterns in exploration/exploitation balance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Policy gating emerges from minimizing a free energy functional that balances task performance and behavioral diversity
- **Mechanism:** The framework defines a cost function combining KL divergence between the agent's policy and a generative model with an entropic regularizer
- **Core assumption:** The task can be accurately modeled by a generative probability distribution and the agent's policy is a linear mixture of available primitives
- **Evidence anchors:** Abstract mentions free energy minimization; GateFrame section describes the complexity term and entropic regularizer
- **Break condition:** Poor generative model leads to suboptimal policy convergence

### Mechanism 2
- **Claim:** The softmax gating rule emerges as the proximal operator of the entropic barrier in the optimization problem
- **Mechanism:** The dynamics are derived by formulating the constrained optimization as a proximal gradient flow
- **Core assumption:** The optimization landscape is convex or can be treated with proximal methods
- **Evidence anchors:** GateFlow section states softmax emerges from the proximal operator of the entropic term
- **Break condition:** Zero temperature parameter removes entropic barrier, causing argmax behavior and potential local optima

### Mechanism 3
- **Claim:** The neural circuit converges to optimal policy because its dynamics are strictly contracting
- **Mechanism:** The system implements a two-timescale recurrent circuit with fast gradient computation and slow weight integration
- **Core assumption:** Separation of timescales holds and neural variables can represent required activations
- **Evidence anchors:** Abstract mentions exponential convergence from contractivity; GateNet section describes the two-unit architecture
- **Break condition:** Violated timescale separation causes oscillatory behavior

## Foundational Learning

- **Concept: KL Divergence & Free Energy Principle**
  - **Why needed here:** The core objective function is built around minimizing "statistical complexity" (KL divergence) against a generative model
  - **Quick check question:** If the generative model q is a uniform distribution, what does minimizing KL divergence force the agent's policy to do?

- **Concept: Proximal Gradient Descent**
  - **Why needed here:** The paper relies on "proximal flows" to derive continuous-time dynamics
  - **Quick check question:** Why does standard gradient descent fail if applied directly to constraints of a probability simplex, requiring a proximal step instead?

- **Concept: Contraction Theory**
  - **Why needed here:** The paper guarantees convergence via contraction, not standard Lyapunov stability
  - **Quick check question:** Does a contracting system require knowledge of the equilibrium point to guarantee stability?

## Architecture Onboarding

- **Component map:** Current State $x_{k-1}$ -> GateFrame -> GateFlow -> GateNet (Fast Unit + Slow Unit) -> Policy $p^*_u$

- **Critical path:** The calculation of the gradient vector in the Fast Unit is the computational bottleneck

- **Design tradeoffs:**
  - Temperature ($\epsilon$): Low $\epsilon$ mimics "argmax" (sparse selection, risk of local optima); High $\epsilon$ encourages uniformity (exploration, but reduced task efficiency)
  - Timescale constants: Faster dynamics improve reaction time but may compromise stability if gradient computation lags

- **Failure signatures:**
  - Weight Collapse: Weights converging to a one-hot vector indicates $\epsilon$ is too low or entropy regularization is insufficient
  - Oscillations: Weights failing to settle indicate violation of time-scale separation assumption

- **First 3 experiments:**
  1. **Polarization Recovery:** Simulate a flock of agents and verify if GateMod weights evolve to uniform values, recovering collective polarization
  2. **Leader Navigation:** Introduce informed agents with goal-directed generative model and check if GateMod allows leaders to steer the flock without breaking cohesion
  3. **Bandit Task Fitting:** Fit GateMod to human decision-making dataset and compare Protected Exceedance Probability against Hybrid/UCB models

## Open Questions the Paper Calls Out

- **Open Question 1:** How can GateMod be integrated with an inference and learning framework to autonomously acquire primitives rather than relying on given ones?
- **Open Question 2:** Does introducing competition among primitives lead to robust diversity or degenerate "rich-get-richer" effects?
- **Open Question 3:** Can alternative gating rules emerge from normative frameworks using different regularizers?

## Limitations
- The specific derivation of softmax as the proximal operator of entropy is not validated by the provided corpus
- The contracting neural dynamics and its robustness to parameter variations require empirical validation
- The model assumes pre-defined primitives and does not address how agents autonomously acquire them

## Confidence
- **High:** GateFrame as a free energy minimization problem
- **Medium:** GateFlow convergence to the softmax rule
- **Medium:** GateNet stability via contraction

## Next Checks
1. Reproduce the softmax derivation to verify the proximal operator of negative entropy yields the softmax function
2. Test parameter sensitivity by systematically varying temperature and timescale constants in GateNet simulation
3. Compare to ablation baselines by running flocking and bandit experiments with hard-max gating rule instead of softmax