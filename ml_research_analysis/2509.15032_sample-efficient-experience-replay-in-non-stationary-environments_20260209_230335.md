---
ver: rpa2
title: Sample Efficient Experience Replay in Non-stationary Environments
arxiv_id: '2509.15032'
source_url: https://arxiv.org/abs/2509.15032
tags:
- replay
- learning
- arxiv
- experience
- deer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of sample efficiency in reinforcement
  learning (RL) for non-stationary environments, where changing dynamics and rewards
  render past experiences quickly outdated. Traditional experience replay methods,
  particularly those based on TD-error prioritization, struggle to differentiate between
  policy updates and environmental changes, leading to inefficient learning.
---

# Sample Efficient Experience Replay in Non-stationary Environments

## Quick Facts
- arXiv ID: 2509.15032
- Source URL: https://arxiv.org/abs/2509.15032
- Reference count: 38
- Primary result: DEER improves RL performance by 11.54% over state-of-the-art methods on Mujoco benchmarks

## Executive Summary
This paper addresses the challenge of sample efficiency in reinforcement learning for non-stationary environments where dynamics and rewards change over time. Traditional experience replay methods struggle because they cannot differentiate between policy updates and environmental changes, leading to inefficient learning from outdated experiences. The authors propose Discrepancy of Environment Prioritized Experience Replay (DEER), which introduces a principled metric called Discrepancy of Environment (DoE) to isolate the effects of environmental shifts on value functions. DEER employs a binary classifier to detect environmental changes and applies distinct prioritization strategies for transitions before and after detected shifts, using both policy updates and environmental changes to guide sampling.

## Method Summary
DEER introduces a novel approach to prioritized experience replay specifically designed for non-stationary environments. The method relies on a Discrepancy of Environment (DoE) metric that quantifies how much an environmental change affects the value function, independent of policy updates. A binary classifier is trained to detect when environmental transitions occur by analyzing state-action-value triplets from the replay buffer. When a transition is classified as an environmental change, DEER applies different prioritization strategies: it samples pre-change transitions more frequently to maintain performance on previous task versions, while post-change transitions are sampled based on their contribution to learning the new environment. This dual prioritization mechanism allows the agent to balance exploitation of known strategies with adaptation to new conditions.

## Key Results
- DEER improves performance of off-policy algorithms by 11.54% compared to best state-of-the-art experience replay methods
- Under extreme non-stationary settings (200% shift), DEER achieves 22.53% additional performance improvement for offline policy algorithms
- Demonstrated effectiveness across four non-stationary Mujoco benchmark tasks

## Why This Works (Mechanism)
DEER works by fundamentally changing how experience replay prioritizes transitions in non-stationary environments. Traditional methods use TD-error, which conflates policy learning progress with environmental changes. DEER's DoE metric isolates environmental effects by measuring the divergence between value functions before and after potential environmental changes. The binary classifier acts as a change detector, creating a temporal boundary in the replay buffer. This boundary enables DEER to apply different sampling strategies to pre-change and post-change transitions, ensuring that the agent doesn't forget how to behave in previous environment states while efficiently adapting to new ones.

## Foundational Learning

**Reinforcement Learning in Non-stationary Environments**
- Why needed: Most RL assumes stationary MDPs, but real-world applications frequently involve changing dynamics
- Quick check: Can the agent maintain performance across multiple environment shifts?

**Prioritized Experience Replay**
- Why needed: Not all experiences are equally valuable for learning; prioritization improves sample efficiency
- Quick check: Does prioritization improve learning speed compared to uniform sampling?

**Change Detection in Sequential Data**
- Why needed: Identifying when environmental changes occur is crucial for adapting learning strategies
- Quick check: Can the classifier accurately detect environmental transitions in synthetic data?

## Architecture Onboarding

**Component Map**
Environment -> Agent (with Replay Buffer) -> DEER Prioritizer -> Classifier (Change Detector)

**Critical Path**
State-Action-Reward-State (SARS) transitions → Replay Buffer → DEER Prioritizer → Sampled Batch → Agent Update

**Design Tradeoffs**
- Classifier accuracy vs. computational overhead
- Frequency of classifier updates vs. change detection latency
- Pre-change vs. post-change sampling ratios

**Failure Signatures**
- Poor classifier performance leading to incorrect prioritization
- Over-prioritization of outdated transitions causing performance degradation
- Insufficient exploration after environmental changes

**First 3 Experiments**
1. Test DEER on a simple non-stationary gridworld with synthetic environment shifts
2. Compare DEER's performance against standard PER on Mujoco HalfCheetah with scheduled changes
3. Evaluate classifier accuracy in detecting environmental transitions under varying noise levels

## Open Questions the Paper Calls Out
None

## Limitations
- Classifier-based change detection may fail in complex or noisy environments
- Results are limited to Mujoco continuous control benchmarks, limiting generalizability
- Computational overhead of DEER compared to standard experience replay is not thoroughly analyzed

## Confidence

**Classifier Dependency**: High confidence in theoretical framework, Medium confidence in empirical results, Low confidence in scalability

**Generalization Claims**: High confidence in Mujoco results, Low confidence in claims beyond tested domains

**Computational Efficiency**: Medium confidence in reported results, Low confidence in claims about real-time applicability

## Next Checks

1. Test DEER's performance on Atari benchmark suite to assess generalization beyond continuous control tasks
2. Evaluate classifier robustness under varying levels of observation noise and partial observability
3. Measure computational overhead of DEER compared to standard experience replay methods across different batch sizes and replay buffer capacities