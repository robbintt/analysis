---
ver: rpa2
title: 'Fantastic Features and Where to Find Them: A Probing Method to combine Features
  from Multiple Foundation Models'
arxiv_id: '2512.01405'
source_url: https://arxiv.org/abs/2512.01405
tags:
- combo
- probing
- adapter
- multiple
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively combining diverse
  representations from multiple foundation models (FMs) for downstream tasks. The
  authors propose Combined backBones (ComBo), a probing-based adapter that compresses
  activations from multiple FM layers into compact token-wise representations and
  processes them with a lightweight transformer for task-specific prediction.
---

# Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models

## Quick Facts
- arXiv ID: 2512.01405
- Source URL: https://arxiv.org/abs/2512.01405
- Reference count: 40
- This paper proposes ComBo, a probing-based adapter that efficiently combines features from multiple foundation models for downstream tasks without backpropagation through backbones.

## Executive Summary
This paper addresses the challenge of effectively combining diverse representations from multiple foundation models (FMs) for downstream tasks. The authors propose Combined backBones (ComBo), a probing-based adapter that compresses activations from multiple FM layers into compact token-wise representations and processes them with a lightweight transformer for task-specific prediction. Crucially, ComBo does not require backpropagation through backbone models or dataset-specific hyperparameter tuning. The method is evaluated on the 19 tasks of the VTAB-1k benchmark, where ComBo outperforms previous probing methods and matches or surpasses more expensive alternatives like distillation-based model merging. The authors also introduce a mechanism to evaluate each backbone's task-relevance, enabling efficient model selection and improved performance through selective adaptation.

## Method Summary
ComBo is a probing-based adapter that extracts features from frozen foundation models without backpropagation. It works by stacking feature maps from selected layers of one or more models at each spatial token position, applying a learned affine projection to compress these high-dimensional features into compact 128-dimensional token representations, and processing them through a lightweight 6-layer transformer with a learnable class token. The method preserves spatial information by interpolating feature maps to consistent token counts and avoids pooling operations that degrade structured task performance. During training, L2 regularization on backbone-specific projection weights provides a proxy for task-relevance, enabling efficient model selection.

## Key Results
- ComBo outperforms previous probing methods (Head2Toe, SMP) and matches or surpasses more expensive alternatives like distillation-based RADIOv2.5 on VTAB-1k tasks
- Using task-relevance estimation through L2 regularization, selective adaptation with top-2 models achieves 78.1% accuracy versus 78.6% for exhaustive search
- ComBo requires only 1.7M parameters from timm (compared to 3.5M for LoRA and 6.2M for Adapter+) while avoiding backpropagation through backbone models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressing multi-layer, multi-model features via a learned affine projection preserves spatial information while enabling scalability.
- Mechanism: For each spatial token position i, features from all selected layers of all models are stacked into a high-dimensional vector S_i, then projected to a compact dimension D′ via a shared affine transformation Λ = {W, b}. This allows the model to learn which layer/model combinations are most relevant while maintaining the spatial token structure.
- Core assumption: Relevant information is distributed across layers and models but can be linearly projected to a lower-dimensional space without critical loss.
- Evidence anchors:
  - [abstract] "ComBo compresses activations from layers of one or more FMs into compact token-wise representations"
  - [Page 5] "This projection is shared across all token positions, allowing a first selection of the most relevant features from all layers and models considered while preserving the spatial layout of feature maps."
  - [corpus] No direct corpus evidence on this specific compression mechanism.
- Break condition: If tasks require fine-grained spatial relationships that cannot be captured in the compressed embedding dimension, performance may degrade.

### Mechanism 2
- Claim: A lightweight transformer can dynamically attend to task-relevant features across spatial locations and models.
- Mechanism: After compression, the token representations are processed by a 6-layer transformer with learned class token. Self-attention enables the model to identify and weight relevant features across different spatial positions and backbone sources without manual pooling or pooling-region tuning.
- Core assumption: The compressed token representations retain sufficient information for the transformer to learn task-relevant attention patterns.
- Evidence anchors:
  - [Page 4] "We build ComBo around a transformer architecture for its proven ability to dynamically attend to relevant information through its self-attention mechanism, making it ideal for focusing on task-relevant features from and across pre-trained models."
  - [Page 6, Table 6] Replacing transformer with linear head drops accuracy from 73.8% to 65.1%; using MLP from SMP achieves 67.5%.
  - [corpus] Corpus evidence on probing representations for downstream tasks (e.g., Probing Preference Representations) supports the general viability of learned probing mechanisms.
- Break condition: If the downstream task requires reasoning beyond what 128-dim embeddings and 6-layer transformer can capture, expressivity limits may emerge.

### Mechanism 3
- Claim: L2 regularization on backbone-specific projection weights provides a proxy for task-relevance, enabling efficient model selection.
- Mechanism: During training with regularization, importance score s_k for each backbone M_k is computed as the ℓ2 norm of projection weight columns corresponding to that model. The regularized loss L_total = L_task + λ Σ s_k encourages sparse backbone usage. Higher learned weights indicate greater task-relevance.
- Core assumption: Models with more task-relevant features will have their corresponding projection weights grow larger during optimization.
- Evidence anchors:
  - [Page 5] "For each backbone M_k, we compute an importance score s_k (measuring task-relevance) as the ℓ2 norm of the projection weights associated with all its layers"
  - [Page 9, Table 5] "Best per-task top-n" selection using importance scores achieves 78.1% average accuracy, approaching the 78.6% upper bound from exhaustive search.
  - [corpus] No corpus evidence on this specific regularization-based selection method.
- Break condition: If multiple models provide redundant useful features, regularization may arbitrarily suppress some relevant backbones.

## Foundational Learning

- Concept: **Vision Transformer (ViT) token structure**
  - Why needed here: ComBo operates on token-level features from ViT backbones. Understanding that images are partitioned into patches and processed as token sequences is essential for grasping how spatial positions are preserved and interpolated.
  - Quick check question: Can you explain why ComBo interpolates feature maps to a consistent spatial resolution before stacking?

- Concept: **Probing vs. Fine-tuning**
  - Why needed here: ComBo is a probing-based adapter—it extracts features from frozen models without backpropagating through them. This is the key computational advantage over tuning methods like Adapter+ or LoRA.
  - Quick check question: Why does ComBo require significantly less GPU memory than Multi-Adapter+ when combining four backbones?

- Concept: **Feature map normalization across models**
  - Why needed here: Different FMs produce activations at different scales. ComBo normalizes feature maps via mean/std computed across token and embedding dimensions before stacking.
  - Quick check question: What happens to ComBo performance if feature map normalization is removed? (Answer in Table 6: performance drops, especially on structured tasks.)

## Architecture Onboarding

- Component map:
  - Feature extraction -> Spatial interpolation -> Normalization -> Stacking -> Affine projection -> Transformer -> Classification head

- Critical path: Feature extraction → interpolation → normalization → projection → transformer → classifier. The projection and transformer are the only trainable components.

- Design tradeoffs:
  - **Projection dimension**: Smaller D′ reduces parameters but may lose information; D′=128 worked well for VTAB-1k
  - **Transformer depth**: 6 blocks used; fewer blocks (1-2) hurt structured task performance; more (12) slightly hurts overall
  - **Layer selection**: Probing all 12 blocks outperforms subsets; for pre-tuned models, final layer alone suffices (Table 4)

- Failure signatures:
  - **Structured task underperformance with pooling**: Previous methods (Head2Toe, SMP) use average pooling, degrading spatial information needed for tasks like dSprites
  - **Dominated by weak backbone**: Including consistently poor models (e.g., SAM on VTAB) can drag down combined performance; use task-relevance estimation to filter
  - **Inconsistent token counts**: Models with different patch sizes (e.g., ViT-B/14 vs ViT-B/16) require interpolation; failure to align degrades performance

- First 3 experiments:
  1. **Single-backbone validation**: Apply ComBo to one FM (e.g., ViT-B/16 ImageNet-21K) on VTAB-1k. Compare against linear probing, Head2Toe, SMP. Expect ComBo to excel on structured tasks due to preserved spatial information.
  2. **Multi-backbone combination**: Probe DFN CLIP, DINOv2, SAM, SigLIP simultaneously. Compare against individually probed models and distilled RADIOv2.5 baseline. Expect combined ComBo to match or exceed best single model.
  3. **Task-relevance estimation**: Train with regularization (λ=0.01), extract importance scores, select top-2 models per task, retrain without regularization. Compare against all-models baseline and random selection. Expect selective adaptation to improve average performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal number of backbone models be automatically determined for a given downstream task without exhaustive evaluation?
- Basis in paper: [explicit] "it remains unclear how to automatically determine the optimal number of models to maximise performance without evaluating each possible option"
- Why unresolved: The current task-relevance scoring mechanism successfully ranks models but requires manual selection or per-task evaluation to identify the ideal ensemble size. Table 12 shows optimal model count varies from 1 to 4 across tasks.
- What evidence would resolve it: A principled thresholding or stopping criterion for the importance scores that predicts the optimal ensemble size a priori, validated across diverse tasks.

### Open Question 2
- Question: Can ComBo's performance and efficiency be maintained when scaling to larger foundation models and dense prediction tasks?
- Basis in paper: [explicit] "our experiments focus on ViT-B architectures and VTAB-1k's low-data classification tasks. Evaluation on larger models, higher-resource settings, and dense prediction tasks would help to establish the broader generalisability of ComBo."
- Why unresolved: All experiments used ViT-B backbones; larger models (e.g., DINOv2-g) were only mentioned as feasible to probe, not systematically evaluated.
- What evidence would resolve it: Systematic benchmarking of ComBo on larger backbone architectures (ViT-L/G) and tasks requiring dense outputs (segmentation, detection).

### Open Question 3
- Question: Would hierarchical or layer-specific projection mechanisms improve ComBo's scalability as more model layers are incorporated?
- Basis in paper: [explicit] "using a single affine projection for all layers may limit scalability as we want to include more model layers. More flexible or hierarchical projection mechanisms could improve scalability and representational capacity."
- Why unresolved: The current design uses one shared projection across all layers and models, which may become a bottleneck with more inputs.
- What evidence would resolve it: An ablation comparing single vs. hierarchical projection designs with increasing numbers of layers/models, measuring performance and parameter efficiency.

## Limitations
- Performance gains are modest and task-dependent, suggesting sensitivity to task characteristics
- Task-relevance estimation mechanism lacks theoretical grounding for why regularization-based selection correlates with true downstream utility
- Method's efficacy on regression, segmentation, or generative tasks remains untested

## Confidence
- **High confidence**: The computational efficiency claims (no backpropagation through backbones, memory savings) are directly verifiable from the method description and align with probing-based adaptation literature.
- **Medium confidence**: The claim that ComBo "matches or surpasses" more expensive alternatives is supported by VTAB-1k results, but the margin of victory is modest and task-dependent.
- **Low confidence**: The task-relevance estimation mechanism's general applicability beyond VTAB-1k is not established, as the regularization-based selection approach lacks theoretical justification and has not been validated on diverse benchmarks.

## Next Checks
1. **Ablation on compression dimension**: Systematically vary D' (e.g., 32, 64, 128, 256) to quantify the information loss trade-off and identify the minimum viable dimension for maintaining performance across task types.
2. **Cross-dataset transferability**: Evaluate ComBo on a non-VTAB dataset (e.g., CIFAR-100 or a medical imaging benchmark) to test whether task-relevance scores learned on one distribution generalize to others, and whether selective adaptation improves robustness.
3. **Failure case analysis on structured tasks**: Conduct detailed error analysis on dSprites and similar tasks to identify whether spatial information degradation from projection or interpolation is the limiting factor, and test whether higher D' or additional spatial attention mechanisms can recover lost accuracy.