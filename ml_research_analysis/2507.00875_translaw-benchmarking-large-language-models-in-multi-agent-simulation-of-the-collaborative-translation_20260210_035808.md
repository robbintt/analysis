---
ver: rpa2
title: 'TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of
  the Collaborative Translation'
arxiv_id: '2507.00875'
source_url: https://arxiv.org/abs/2507.00875
tags:
- translation
- legal
- evaluation
- hong
- kong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents TransLaw, a multi-agent system that addresses
  challenges in Hong Kong legal judgment translation by combining three specialized
  agents: Translator, Annotator, and Proofreader. The framework integrates a Hong
  Kong legal glossary database, Retrieval-Augmented Generation (RAG), and iterative
  feedback to produce translations with high legal accuracy, stylistic fidelity, and
  structural coherence.'
---

# TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation

## Quick Facts
- **arXiv ID**: 2507.00875
- **Source URL**: https://arxiv.org/abs/2507.00875
- **Reference count**: 40
- **Key outcome**: TransLaw achieves 88.45% average Legal ACS score vs 72.65% for single-agent GPT-4o, reducing translation costs by nearly 4,000x

## Executive Summary
TransLaw introduces a multi-agent framework for translating Hong Kong legal judgments from English to Traditional Chinese. The system employs three specialized modules (Translator, Annotator, Proofreader) with seven distinct agents working in an iterative feedback loop. By integrating a Hong Kong legal glossary database with Retrieval-Augmented Generation (RAG) and structured inter-agent communication, TransLaw significantly outperforms single-agent baselines on automated and human evaluation metrics. The approach achieves superior legal semantic accuracy, structural coherence, and stylistic fidelity while reducing translation costs by nearly 4,000 times compared to professional human translation.

## Method Summary
TransLaw implements a seven-agent multi-agent system designed specifically for Hong Kong legal judgment translation. The framework consists of Translation Command (ACom), Translation Execution (ATerm with RAG, ATrans), and Expert Review (AAlign, ATermR, ACita, AStyleP) modules. The system processes bilingual judgments from the HKCFA Judgement 97-22 dataset (344 judgments, 11,099 sentence pairs) using a combined DOJ Glossaries of Legal Terms database. Agents communicate through structured JSON messages following detailed prompt templates, with iterative feedback loops for quality improvement. The system was evaluated across 13 open-source and commercial LLMs, with GPT-4o serving as the primary baseline, using automated metrics (xCOMET-XL, wmt22-unite-da) and human evaluation via the Legal ACS metric (Accuracy α=0.6, Coherence β=0.3, Style γ=0.1).

## Key Results
- TransLaw achieves 88.45% average Legal ACS score versus 72.65% for single-agent GPT-4o
- Automated metrics show significant improvements: xCOMET-XL and wmt22-unite-da scores exceed single-agent baselines
- Translation cost reduced by nearly 4,000 times compared to professional human translation
- Human evaluation confirms superior legal semantic accuracy and structural coherence, though still trails human experts in complex terminology contextualization and stylistic naturalness

## Why This Works (Mechanism)
The multi-agent architecture works by distributing specialized translation tasks across agents with distinct expertise. The Translator module handles initial translation with RAG-powered terminology lookup, the Annotator module performs alignment and citation formatting, and the Proofreader module ensures stylistic consistency and legal accuracy. Iterative feedback between agents allows for error correction and quality refinement that single-agent systems cannot achieve. The structured communication protocol ensures consistency while the specialized glossary database provides domain-specific accuracy that general-purpose LLMs lack.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Combines document retrieval with language model generation to provide context-specific information. Why needed: Ensures accurate legal terminology translation using official glossaries. Quick check: Verify glossary entries are correctly retrieved and integrated into translations.
- **Multi-agent orchestration**: Manages communication and task distribution between specialized agents. Why needed: Enables collaborative translation workflow mimicking human expert teams. Quick check: Test JSON message passing between agents maintains task context.
- **Legal domain adaptation**: Customizes translation for Hong Kong legal system requirements. Why needed: Standard translation tools lack jurisdiction-specific legal terminology. Quick check: Validate translations against official HKCFA citation formats.
- **Iterative feedback loops**: Enables quality improvement through multiple refinement passes. Why needed: Single-pass translation insufficient for complex legal documents. Quick check: Measure quality improvement across feedback iterations.
- **Legal ACS evaluation**: Domain-specific metric for legal translation quality. Why needed: Standard translation metrics don't capture legal accuracy requirements. Quick check: Apply ACS metric to benchmark translations.
- **Structured prompt engineering**: Uses detailed templates for consistent agent behavior. Why needed: Ensures reproducible translation workflows across different models. Quick check: Verify prompt templates produce consistent outputs.

## Architecture Onboarding

**Component Map**: ACom -> ATerm(ATermR) -> ATrans -> AAlign -> ACita -> AStyleP (iterative feedback loop)

**Critical Path**: Translation Command receives source text → ATerm performs RAG lookup for terminology → ATrans executes translation → AAlign verifies alignment → ACita formats citations → AStyleP applies stylistic polish → ACom evaluates and iterates if needed

**Design Tradeoffs**: Multi-agent complexity vs. quality gains; RAG retrieval latency vs. terminology accuracy; iterative refinement vs. translation speed; specialized agents vs. model parameter efficiency

**Failure Signatures**: Legal terminology mistranslation (RAG failure), citation format errors (ACita malfunction), inconsistent terminology (ACom memory issues), hallucinated legal terms (ATermR validation failure)

**Three First Experiments**:
1. Implement single-agent baseline with GPT-4o on FACC 1/2021 test set (200 paragraphs) using xCOMET-XL evaluation
2. Add RAG component to terminology agent and measure improvement in legal term accuracy
3. Implement two-agent workflow (translation + proofreading) and compare against single-agent baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details unclear: RAG retrieval parameters (embedding model, chunk size, top-k) and iteration threshold K unspecified
- No statistical significance testing reported for performance differences between TransLaw and baselines
- Reliance on proprietary models (GPT-4o, Gemini Pro, Claude) limits reproducibility and raises data privacy concerns
- Claims about superiority in complex terminology contextualization lack confidence intervals and statistical validation

## Confidence

**High confidence**: Automated metric improvements (xCOMET-XL, wmt22-unite-da scores), multi-agent architecture design, and dataset quality (HKCFA Judgement 97-22, DOJ glossary)

**Medium confidence**: Human evaluation results, cost reduction claims, and qualitative assessments of stylistic fidelity

**Low confidence**: Claims about superiority in "complex terminology contextualization" and "stylistic naturalness" due to lack of statistical validation

## Next Checks

1. **Statistical Validation**: Perform paired t-tests or bootstrap confidence intervals on all metric differences between TransLaw and baselines to establish statistical significance of claimed improvements.

2. **RAG Implementation Audit**: Implement and test multiple RAG configurations (different embedding models, chunk sizes, retrieval strategies) to determine sensitivity of translation quality to retrieval parameters.

3. **Cost-Performance Tradeoff Analysis**: Benchmark TransLaw against fine-tuned specialized legal translation models (e.g., BLOOMZ, GPT-Neo) to validate the claimed 4,000x cost reduction while maintaining quality parity.