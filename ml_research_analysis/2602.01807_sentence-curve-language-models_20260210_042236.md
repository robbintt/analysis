---
ver: rpa2
title: Sentence Curve Language Models
arxiv_id: '2602.01807'
source_url: https://arxiv.org/abs/2602.01807
tags:
- sentence
- curve
- language
- word
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces sentence curves, a continuous representation
  of sentences defined as spline curves in embedding space, where control points affect
  multiple words. This approach extends diffusion language models (DLMs) to predict
  sentence curves instead of static word embeddings.
---

# Sentence Curve Language Models

## Quick Facts
- arXiv ID: 2602.01807
- Source URL: https://arxiv.org/abs/2602.01807
- Reference count: 28
- Key outcome: Sentence curves as continuous representations enable state-of-the-art diffusion language models on MT and LM tasks

## Executive Summary
This paper introduces sentence curves, a novel continuous representation for sentences defined as spline curves in embedding space where control points influence multiple words. The approach addresses a fundamental limitation in diffusion language models (DLMs) where target words are treated as context-independent, leading to overfitting on local word-level structure while neglecting global sentence structure. By predicting sentence curves instead of static word embeddings, the method theoretically induces regularization that favors global structure modeling. Empirically, Sentence Curve Language Models (SCLM) achieve state-of-the-art performance among DLMs on IWSLT14 and WMT14 machine translation benchmarks, demonstrate stable training without knowledge distillation, and show competitive performance on LM1B language modeling.

## Method Summary
The core innovation involves representing sentences as continuous spline curves in embedding space, where each control point affects multiple consecutive words. This creates a smooth trajectory through the embedding space that captures global sentence structure. The model is trained to predict these sentence curves using diffusion-based objectives, extending the standard DLM framework. The continuous nature of the representation means that local changes to the curve affect multiple words simultaneously, theoretically encouraging the model to learn more global structural patterns rather than focusing on individual word predictions.

## Key Results
- Achieves state-of-the-art performance among diffusion language models on IWSLT14 and WMT14 machine translation benchmarks
- Demonstrates stable training without requiring knowledge distillation, a common requirement for DLMs
- Shows competitive performance on LM1B language modeling task
- Theoretically induces regularization favoring global structure modeling through sentence curve prediction

## Why This Works (Mechanism)
The mechanism leverages the mathematical property that control points in spline curves affect multiple consecutive words, creating a dependency that spans beyond local word-level structure. When the model learns to predict these curves, it must reason about the global trajectory of the sentence rather than optimizing individual word predictions in isolation. This continuous representation naturally encourages the model to capture sentence-level patterns and coherence, addressing the tendency of standard DLMs to overfit to local structures.

## Foundational Learning
- **Diffusion Language Models**: Why needed - baseline framework being extended; Quick check - understand how standard DLMs predict static word embeddings
- **Spline curves and control points**: Why needed - core mathematical representation; Quick check - understand how control points affect multiple words
- **Regularization in neural networks**: Why needed - theoretical motivation; Quick check - grasp how continuous representations can induce implicit regularization
- **Embedding space geometry**: Why needed - sentence curves exist in this space; Quick check - understand how sentences can be represented as continuous trajectories

## Architecture Onboarding
- **Component Map**: Input text -> Sentence curve predictor -> Diffusion decoder -> Output sentence
- **Critical Path**: Text encoding → Curve prediction → Curve diffusion → Sentence reconstruction
- **Design Tradeoffs**: Continuous vs. discrete representations; curve complexity vs. model capacity; global structure capture vs. local precision
- **Failure Signatures**: Poor global coherence in generated text; instability in training without curves; reduced performance on sentence-level tasks
- **First Experiments**: 1) Train standard DLM on same tasks for baseline comparison; 2) Vary curve complexity (number of control points); 3) Test on sentence-level coherence evaluation tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on idealized assumptions about curve configurations
- Experimental comparison with standard DLMs is somewhat narrow
- Claims about global structure modeling need more rigorous empirical validation

## Confidence
- **Medium Confidence**: Claims about improved global structure modeling due to sentence curves
- **High Confidence**: Claims about state-of-the-art DLM performance on IWSLT14 and WMT14
- **Medium Confidence**: Claims about stable training without knowledge distillation

## Next Checks
1. Conduct systematic ablation studies comparing sentence curves against alternative continuous representations to isolate the specific contribution of the curve-based approach

2. Design experiments that directly measure global vs. local structure learning through attention pattern analysis or probing tasks for sentence-level understanding

3. Test the approach on additional benchmarks beyond MT and LM, particularly tasks where global sentence structure is known to be crucial (e.g., document-level MT, summarization)