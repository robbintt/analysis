---
ver: rpa2
title: Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with
  a Billion-Parameter Instruction-Tuned Model
arxiv_id: '2509.08381'
source_url: https://arxiv.org/abs/2509.08381
tags:
- data
- json
- extraction
- samples
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ETLCH, a 1B-parameter LLaMA-based model fine-tuned
  with low-rank adaptation (LoRA) on only 100-1000 samples per task for structured
  information extraction. The model was evaluated on JSON extraction, knowledge graph
  extraction, and named entity recognition tasks, outperforming larger baseline models
  including Qwen2.5-7B and Breeze-7B across most metrics.
---

# Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model

## Quick Facts
- arXiv ID: 2509.08381
- Source URL: https://arxiv.org/abs/2509.08381
- Reference count: 13
- A 1B-parameter LLaMA-based model fine-tuned with LoRA on 100-1000 samples per task outperforms larger baselines on structured information extraction

## Executive Summary
This paper introduces ETLCH, a 1B-parameter LLaMA-based model fine-tuned with low-rank adaptation (LoRA) on only 100-1000 samples per task for structured information extraction. The model was evaluated on JSON extraction, knowledge graph extraction, and named entity recognition tasks, outperforming larger baseline models including Qwen2.5-7B and Breeze-7B across most metrics. ETLCH achieved statistically significant improvements in JSON and knowledge graph tasks, with winning rates exceeding 70% against baselines. The model demonstrated exceptional data efficiency, showing rapid performance gains with minimal training data and plateauing after approximately 300 samples. These results demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, making them viable for resource-constrained environments.

## Method Summary
ETLCH uses Llama-3.2-1B-Instruct as the base model with LoRA fine-tuning (rank 32, α 64) for multi-task structured information extraction. The approach trains jointly on three tasks: JSON extraction, knowledge graph extraction, and named entity recognition using synthetic data generated by gpt-4o-mini. The model employs aggressive regularization (dropout 0.4, batch size 2) and trains for 100 epochs with a learning rate of 1e-7. The multi-task setup creates cross-task consistency constraints where outputs from one task inform and constrain the others, improving overall extraction quality. The model is evaluated using ROUGE-L, cosine similarity, and JSON parseability metrics against larger baseline models.

## Key Results
- ETLCH outperforms Qwen2.5-7B and Breeze-7B across most metrics with only 1B parameters
- JSON parseability reaches 48% vs. 27-45% for baselines when trained on 300 samples
- Statistical significance (p < 0.05) achieved in JSON and knowledge graph tasks
- Performance plateaus around 300 samples, demonstrating rapid data efficiency gains

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adaptation in Constrained Parameter Space
LoRA fine-tuning with rank 32 enables effective adaptation for structured extraction tasks with 100-300 samples by modifying a low-dimensional subspace of the pretrained weight space. Rather than updating full weight matrices, LoRA adds trainable low-rank decomposition matrices (A×B where rank r=32, α=64). This constrains parameter updates to a smaller hypothesis space, reducing overfitting risk when data is scarce while preserving the base model's linguistic knowledge.

### Mechanism 2: Multi-Task Semantic Consistency via Cross-Task Constraints
Joint training on NER → KGE → JSON creates implicit mutual verification that reduces error accumulation in the extraction reasoning chain. When entities from NER differ from subjects in KGE triplets, or when JSON outputs don't match extracted entities, the composite loss penalizes inconsistency. This forces the model to learn internally consistent representations across tasks rather than treating each task independently.

### Mechanism 3: Small-Model Specialization with Regularization for Low-Data Regimes
Aggressive regularization (dropout 0.4, batch size 2) combined with small model capacity prevents overfitting while forcing learning of transferable extraction patterns. Small models (1B parameters) have limited capacity relative to 7B+ models, which acts as implicit regularization. Explicit regularization via high dropout and noisy small-batch training forces the model to learn robust patterns rather than memorize training examples.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper's entire approach depends on LoRA for efficient fine-tuning. Understanding that LoRA decomposes weight updates into low-rank matrices (W' = W + BA where B and A are rank-r matrices) is essential to grasp why 100-300 samples suffice.
  - Quick check question: If LoRA rank is 32 and the original weight matrix is 4096×4096, how many trainable parameters does LoRA add per weight matrix?

- Concept: **ROUGE-L (Longest Common Subsequence F1)**
  - Why needed here: The paper uses ROUGE-L as the primary metric. Unlike ROUGE-1/2 (n-gram overlap), ROUGE-L captures sentence-level structure via LCS, making it more sensitive to output format correctness—critical for structured extraction.
  - Quick check question: Why would ROUGE-L be more appropriate than ROUGE-1 for evaluating JSON extraction quality?

- Concept: **Multi-Task Learning with Shared Representations**
  - Why needed here: The paper's innovation partly comes from training NER, KGE, and JSON jointly. Understanding that shared hidden representations can benefit from cross-task gradient signals helps explain the performance gains.
  - Quick check question: What happens to multi-task learning benefits if the optimal representations for two tasks are orthogonal rather than aligned?

## Architecture Onboarding

- Component map:
Llama-3.2-1B-Instruct (base, frozen) -> LoRA adapters (rank=32, α=64, trainable) -> Multi-task training head (shared) -> NER/KGE/JSON tasks -> Composite loss

- Critical path:
  1. Data preparation: Generate instruction-output pairs using GPT-4o-mini (context → instruction → gold output). Each sample has all three task annotations.
  2. LoRA attachment: Initialize LoRA matrices with rank-32 decomposition to base model.
  3. Joint training: 100 epochs, batch size 2, lr=1e-7, gradient clipping=0.1, dropout=0.4.
  4. Evaluation: Test on held-out samples using ROUGE-L, cosine similarity, and JSON parseability.

- Design tradeoffs:
  - Rank 32 vs. higher: Paper chose r=32; lower ranks may underfit schemas, higher ranks may overfit with limited data. No ablation provided.
  - Batch size 2: Introduces noise to prevent overfitting, but may slow convergence. Paper shows plateau at ~300 samples anyway.
  - No quantization: 1B model fits in memory without quantization; larger models would require it.
  - 100 epochs: Many epochs on small data risks overfitting; dropout mitigates this but training could be more efficient with early stopping.

- Failure signatures:
  - JSON parse failures: If model outputs malformed JSON (e.g., trailing commas, unescaped quotes), check if training data had inconsistent schemas.
  - Entity inconsistency across tasks: If NER entities don't match KGE subjects, the cross-task constraint may not be activating—verify composite loss weighting.
  - Plateau too early: If performance plateaus before 300 samples, regularization may be too aggressive; try reducing dropout to 0.2-0.3.
  - Catastrophic forgetting: If base instruction-following degrades, learning rate may be too high; paper used 1e-7.

- First 3 experiments:
  1. Replicate with 100 samples: Train ETLCH-100 on your domain data. Verify JSON parse rate > 45% (paper: 144/300 = 48%). If lower, check instruction format alignment with training data.
  2. Ablate multi-task training: Train three single-task models with same total sample budget. Compare ROUGE-L to joint model. Expect 5-15% degradation based on paper's implicit cross-task benefit claims.
  3. Test generalization to new schema: Create held-out test with unseen JSON schema structure (different field names). If parse rate drops significantly, model may have overfit to specific schemas rather than learning general extraction patterns.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- All results are reported on a single Chinese-language dataset, limiting cross-lingual and cross-domain generalization
- LoRA configuration and regularization parameters were selected without ablation studies
- Evaluation focuses heavily on format compliance (JSON parseability) rather than semantic correctness
- Synthetic data generation using gpt-4o-mini may introduce bias toward instruction-tuned output patterns

## Confidence

**High Confidence:**
- ETLCH outperforms larger baseline models (Qwen2.5-7B, Breeze-7B) on most metrics
- Data efficiency is exceptional, with performance plateauing around 300 samples
- JSON parseability rates are significantly higher than baselines (48% vs. 27-45%)

**Medium Confidence:**
- Statistical significance claims (p < 0.05) for JSON and KGE tasks
- The multi-task training mechanism provides consistent benefits across all three tasks
- Low-Rank Adaptation with rank 32 is optimal for this use case

**Low Confidence:**
- Generalization to non-Chinese languages and different document types
- The model's performance on complex nested JSON structures beyond simple lists
- Long-term stability of fine-tuned parameters with extended inference use

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate ETLCH on English-language structured extraction tasks (e.g., clinical notes, financial reports) using the same 100-300 sample training regime. Compare ROUGE-L and JSON parseability against the Chinese results to quantify language/domain transfer limits.

2. **LoRA Configuration Ablation**: Systematically vary LoRA rank (8, 16, 32, 64) and dropout rates (0.2, 0.4, 0.6) while holding other parameters constant. Measure impact on both training efficiency and final performance to identify optimal regularization levels for low-data regimes.

3. **Semantic vs. Format Correctness Analysis**: Manually annotate 100 predictions from each baseline and ETLCH to distinguish between JSON parse failures and semantically incorrect extractions. Calculate precision/recall for entity and relation extraction separately to determine whether format improvements correlate with content accuracy.