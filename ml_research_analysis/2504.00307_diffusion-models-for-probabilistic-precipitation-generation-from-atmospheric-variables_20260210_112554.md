---
ver: rpa2
title: Diffusion models for probabilistic precipitation generation from atmospheric
  variables
arxiv_id: '2504.00307'
source_url: https://arxiv.org/abs/2504.00307
tags:
- precipitation
- era5
- gfdl
- unet
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a novel machine learning approach to improve\
  \ precipitation representation in Earth system models (ESMs). The proposed method\
  \ combines a deterministic UNet regression model with a conditional diffusion model\
  \ to generate high-resolution (0.25\xB0) global daily precipitation fields from\
  \ a small set of prognostic atmospheric variables."
---

# Diffusion models for probabilistic precipitation generation from atmospheric variables

## Quick Facts
- arXiv ID: 2504.00307
- Source URL: https://arxiv.org/abs/2504.00307
- Reference count: 40
- Primary result: ML method reduces precipitation bias from 0.424 mm/d to 0.091 mm/d while generating ensemble predictions

## Executive Summary
This study presents a novel machine learning approach to improve precipitation representation in Earth system models (ESMs). The proposed method combines a deterministic UNet regression model with a conditional diffusion model to generate high-resolution (0.25°) global daily precipitation fields from a small set of prognostic atmospheric variables. Unlike traditional column-based parameterizations, this framework efficiently produces ensemble predictions, capturing uncertainties in precipitation while mitigating spatial biases. Trained exclusively on ERA5 reanalysis data, the model demonstrates significant improvements over GFDL ESM precipitation.

## Method Summary
The framework consists of two stages: (1) a UNet regression model predicts 1° precipitation from atmospheric variables (specific humidity, wind components, sea level pressure), and (2) a conditional diffusion model downscales to 0.25° resolution while correcting small-scale biases. The method uses noise injection during training to force the model to learn small-scale reconstruction, which is then applied during inference to remove ESM biases. Quantile Delta Mapping aligns ESM input distributions to ERA5 training data. The model is trained on ERA5 data (1980-2018) and evaluated on both ERA5 (2018-2020) and GFDL-ESM4 data.

## Key Results
- Mean absolute bias reduced from 0.424 mm/d to 0.091 mm/d compared to ERA5
- Diffusion model successfully downscales from 1° to 0.25° resolution, reproducing accurate small-scale spatial patterns
- Model preserves large-scale climate trends under future warming scenarios while correcting small-scale biases
- Ensemble predictions are well-calibrated, with CRPS of 0.56 mm/day compared to 0.73 mm/day for ERA5 upsampling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling large-scale deterministic prediction from small-scale stochastic generation appears to improve the fidelity of high-resolution precipitation fields.
- **Mechanism:** The architecture separates the task into two stages: a UNet regression model first predicts the coarse (1°) precipitation mean state from atmospheric variables, and a conditional diffusion model subsequently infers the high-resolution (0.25°) details. This partition allows the regression model to focus on large-scale atmospheric-dynamics coupling while the diffusion model specializes in texture and small-scale variability reconstruction.
- **Core assumption:** The large-scale precipitation field predicted by the regression model is sufficiently accurate to serve as a valid conditioning signal for the generative stage.
- **Evidence anchors:** [abstract] "integrates a conditional diffusion model with a UNet architecture to generate accurate, high-resolution (0.25°) global daily precipitation fields..." [Page 4] "The framework consists of (I) a global deterministic regression module (UNet)... and (II) a conditional diffusion model... that generates global fields at 0.25° resolution."

### Mechanism 2
- **Claim:** Noise injection into the conditioning input likely acts as an implicit bias-correction mechanism for small-scale features.
- **Mechanism:** During training, noise is added to low-resolution inputs to destroy small-scale information, forcing the model to learn how to reconstruct these scales consistent with high-resolution ERA5 data. During inference, adding similar noise to the (biased) ESM-derived low-resolution input removes the ESM's flawed small-scale structures, allowing the diffusion model to regenerate unbiased details consistent with the large-scale state.
- **Core assumption:** The noise level is calibrated to mask the specific spatial scale of ESM errors while preserving the large-scale signal.
- **Evidence anchors:** [Page 4] "Adding noise to the conditional-input removes the small-scale variability and allows the model... to learn how to reconstruct small-scales..." [Page 18] "...we add noise to the fields in order to remove the biased small-scale spatial patterns in the regression model output."

### Mechanism 3
- **Claim:** Training exclusively on reanalysis data (ERA5) enables cross-model generalization without retraining.
- **Mechanism:** By learning the mapping from atmospheric variables to precipitation solely from observation-constrained ERA5 data, the model avoids inheriting the specific structural biases of any single ESM. Quantile Delta Mapping (QDM) is used at inference to align the ESM input distribution to the ERA5 training distribution, bridging the domain gap.
- **Core assumption:** The relationship between the input atmospheric variables and precipitation learned from ERA5 holds for ESM atmospheric states, even under future climate scenarios (stationarity of the learned physical relationship).
- **Evidence anchors:** [Page 15] "A key advantage... is that both our models are trained exclusively on ERA5 reanalysis data. This independence from any particular ESM means that our method can be applied to any ESM without any retraining." [Page 14] "The spatial trend... reveals that applying the DM on the UNet precipitation preserves the spatial patterns of the trend."

## Foundational Learning

- **Concept:** Denoising Diffusion Probabilistic Models (DDPMs)
  - **Why needed here:** This is the core engine for generating stochastic, high-resolution outputs. Understanding the forward/reverse process is necessary to grasp how the model generates "new" information.
  - **Quick check question:** How does the model generate a sample? (Answer: By iteratively reversing a noising process starting from pure noise).

- **Concept:** Quantile Delta Mapping (QDM)
  - **Why needed here:** This statistical method is the bridge allowing a model trained on ERA5 to accept ESM inputs. It corrects distributional biases while preserving trends.
  - **Quick check question:** Why use QDM instead of simple mean subtraction? (Answer: It corrects biases across the entire probability distribution, essential for extremes, and preserves climate change signals).

- **Concept:** Spatial Power Spectral Density (PSD)
  - **Why needed here:** The paper uses PSD to prove that the model successfully restores small-scale variability (spatial texture) that traditional ESMs smear out.
  - **Quick check question:** What does a higher amplitude at short wavelengths (small scales) in the PSD indicate? (Answer: More spatial variability/fine detail).

## Architecture Onboarding

- **Component map:** 4 Atmospheric Variables (Humidity, Wind U/V, Sea Level Pressure) at 1° -> UNet regression -> QDM bias correction -> Noisy condition -> Diffusion model -> 0.25° Precipitation

- **Critical path:** The **noise injection calibration** (Section 4.2) is the most sensitive step. It determines exactly which spatial scales are "regenerated" by the diffusion model versus passed through from the condition.

- **Design tradeoffs:**
  - **Resolution vs. Physical Constraints:** The model generates highly realistic 0.25° fields but does not explicitly enforce physical conservation laws (e.g., mass conservation), relying instead on implicit learning.
  - **Stochasticity vs. Determinism:** The diffusion model provides valuable uncertainty estimates (ensembles) but introduces variance that may be undesirable for deterministic forecasting applications.

- **Failure signatures:**
  - **Mode Collapse:** If the diffusion model is under-trained or the noise level is wrong, it might revert to generating the mean precipitation climatology (blurry outputs) rather than sharp events.
  - **Domain Shift Artifacts:** If applied to an ESM with a very different climatology than ERA5, the QDM may fail to align distributions, resulting in negative precipitation values or suppressed extremes.

- **First 3 experiments:**
  1. **Zero-noise ablation:** Run the diffusion model at inference with `noise_level = 0` to quantify how much of the bias correction is due to the noise destruction mechanism vs. the diffusion process itself.
  2. **ESM Transfer Test:** Apply the model (without retraining) to a different ESM (e.g., MPI-ESM or CESM) to test the claim that it generalizes across models without retraining.
  3. **Conservation Check:** Calculate global total precipitation mass over time for the generated ensemble vs. the ESM input to verify if the model conserves water mass implicitly or drifts.

## Open Questions the Paper Calls Out
- **Question:** Can the diffusion model-based parameterization maintain stability and accuracy when integrated directly into a fully coupled climate system?
  - **Basis in paper:** [explicit] The authors state, "An immediate extension is to integrate our DM-based parameterization directly into a climate model and evaluate its stability in a fully coupled system."
  - **Why unresolved:** The current study evaluates the model in an "offline" or post-processing mode using pre-computed atmospheric variables, without testing feedback loops between the generated precipitation and the atmospheric dynamics.
  - **What evidence would resolve it: Running the model within an ESM's dynamical core to test for stability and emergent behaviors over long climate simulations.

## Limitations
- The model requires re-running the entire diffusion process to generate each ensemble member, which may be computationally expensive compared to analytical sampling methods
- The approach assumes the relationship between atmospheric variables and precipitation is stationary across climate scenarios, which may not hold for extreme warming conditions
- The model only conditions on four atmospheric variables, potentially missing important precipitation drivers like topography and land surface properties
- No explicit physical constraints (e.g., mass conservation) are enforced, relying entirely on learned patterns

## Confidence
- **Confidence level:** High for current results
- **Evidence:** The model demonstrates consistent performance across multiple metrics (bias reduction, spatial fidelity, ensemble calibration) when evaluated on both ERA5 and GFDL-ESM4 data
- **Key uncertainty:** The assumption that the learned atmospheric-precipitation relationship generalizes to future climate states and different ESMs remains untested

## Next Checks
- Verify the model's performance on extreme precipitation events and compare against other bias correction methods
- Test the model's sensitivity to different noise levels and their impact on small-scale feature reconstruction
- Evaluate whether the model maintains performance when applied to ESMs with significantly different climatologies than GFDL-ESM4
- Check if the generated precipitation fields satisfy basic physical constraints (e.g., no negative precipitation, reasonable spatial continuity)