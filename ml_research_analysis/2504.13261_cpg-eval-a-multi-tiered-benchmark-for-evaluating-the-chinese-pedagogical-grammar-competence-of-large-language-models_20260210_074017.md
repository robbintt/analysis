---
ver: rpa2
title: 'CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical
  Grammar Competence of Large Language Models'
arxiv_id: '2504.13261'
source_url: https://arxiv.org/abs/2504.13261
tags:
- grammar
- language
- llms
- evaluation
- pedagogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CPG-EVAL, the first benchmark for assessing
  large language models' (LLMs) competence in pedagogical grammar within Chinese language
  teaching. Grounded in a validated pedagogical grammar framework, the benchmark comprises
  five task types evaluating grammar recognition, fine-grained distinction, categorical
  discrimination, and resistance to linguistic interference.
---

# CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models

## Quick Facts
- arXiv ID: 2504.13261
- Source URL: https://arxiv.org/abs/2504.13261
- Authors: Dong Wang
- Reference count: 7
- Primary result: First benchmark evaluating LLMs' pedagogical grammar competence in Chinese teaching contexts

## Executive Summary
CPG-EVAL introduces a comprehensive framework for assessing large language models' competence in pedagogical grammar within Chinese language teaching. The benchmark comprises five task types that evaluate grammar recognition, fine-grained distinction, categorical discrimination, and resistance to linguistic interference. Testing 13 models revealed that smaller models struggle with negative instance identification and multiple-instance tasks, while larger models demonstrate better stability but still have room for accuracy improvement. The study highlights the need for improved instructional alignment and rigorous benchmarks to guide LLM deployment in educational contexts.

## Method Summary
CPG-EVAL evaluates LLMs on pedagogical grammar competence through five zero-shot tasks using 739 grammar items from the Chinese Grammar Learning Manual (CGLM) and 6,651 synthetic sentences. Tasks include binary classification (SINGLE, BATCH), similarity-based multiple choice (SIM-GRA), category-based multiple choice (CAT-GRA), and confusing instance discrimination (CON-INS). Models must output strictly "T"/"F" or option letters, with accuracy calculated at the sub-question level. Evaluation uses regex scoring to extract structured outputs, enabling fine-grained diagnosis of model performance across different complexity levels and interference conditions.

## Key Results
- Smaller models show systematic false-positive bias, with some falling below random baseline on negative instance tasks
- Larger models maintain better stability across task complexity transitions but still have accuracy gaps
- Average performance across all tasks was 81%, with task-specific accuracies ranging from 73% to 95%
- Interference from linguistically similar instances significantly degrades performance, especially for smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale predicts performance stability across task complexity transitions, but not simple task accuracy.
- Mechanism: Larger models maintain performance when task complexity increases (SINGLE→BATCH), while smaller models show pronounced degradation, particularly on negative instance identification where they fall below random baseline.
- Core assumption: The performance gap reflects differences in capacity to maintain structured reasoning over extended contexts rather than fundamental grammar knowledge.
- Evidence anchors:
  - [abstract] "smaller models struggle with complex tasks and negative instance identification, while larger models show better stability"
  - [section 5.1] "InternLM2.5-7B demonstrates an especially large decrease, falling from 74.2% on BATCH-T to 31.6% on BATCH-F, significantly below the baseline of 50%"
  - [corpus] Limited direct evidence; AraLingBench (Arabic linguistic benchmark) shows similar multi-category evaluation but doesn't address scale-complexity interactions directly.
- Break condition: If fine-tuning smaller models on pedagogical grammar corpora closes the gap, the mechanism may reflect training data distribution rather than scale-dependent reasoning capacity.

### Mechanism 2
- Claim: LLMs exhibit systematic false-positive bias in grammar recognition, misclassifying negative instances as grammar matches.
- Mechanism: Models trained on next-token prediction associate surface linguistic forms with grammar descriptions without verifying functional correspondence, leading to affirmative judgments even when grammar items are absent.
- Core assumption: The bias stems from statistical co-occurrence patterns in training data rather than deliberate response strategies.
- Evidence anchors:
  - [abstract] "smaller models often fall below random baseline in negative instance tasks"
  - [section 5.1] "performance significantly declines for most LLMs when handling negative instances (SINGLE-F)... certain models displaying significant false-positive issues"
  - [corpus] GRILE benchmark for Romanian grammar reasoning shows similar challenges in linguistic competence evaluation but doesn't isolate false-positive rates.
- Break condition: If prompt engineering (e.g., explicit negative examples) eliminates the bias, the mechanism may reflect instruction-following limitations rather than fundamental grammar comprehension gaps.

### Mechanism 3
- Claim: Interference from linguistically similar but grammatically distinct instances degrades performance proportionally to confusion density.
- Mechanism: Models rely partially on surface-form matching; when confusing instances share lexical/phonological features with target grammar items, this activates competing representations that overwhelm discriminative capacity in smaller models.
- Core assumption: The interference operates at the representation level rather than pure reasoning failure.
- Evidence anchors:
  - [section 5.3] "smaller-scale models... achieve accuracy rates only slightly above the level of random guessing... internlm2_5-7b (35.9%) and glm-4-9b (36.8%) is substantially lower than the random baseline"
  - [section 5.3] "reduction in the number of confusing instances significantly influences the models' robustness... all models demonstrating clear gains in accuracy as task difficulty decreases"
  - [corpus] Irish-BLiMP minimal pairs benchmark tests similar linguistic interference but focuses on acceptability judgments rather than pedagogical grammar categorization.
- Break condition: If attention visualization shows models correctly attend to relevant grammatical features but output incorrectly, the mechanism may involve decision boundary issues rather than representation-level interference.

## Foundational Learning

- Concept: **Pedagogical Grammar Pattern Recognition (P-GPR)**
  - Why needed here: The benchmark's core formalization—determining whether a language instance exemplifies a grammar item—differs from standard NLP grammar evaluation. P-GPR requires meta-linguistic abstraction: mapping "nice and friendly" → "FORM: COMBINING TWO ADJECTIVES WITH 'AND'".
  - Quick check question: Given the sentence "She studies what interests her" and grammar item "什么...都/也 construction for universal quantification," would P-GPR return 0 or 1?

- Concept: **Zero-shot Evaluation Protocol**
  - Why needed here: The paper evaluates "intrinsic knowledge" without task-specific examples. Understanding this constraint explains why performance gaps emerge—models cannot adapt to the pedagogical grammar framing during evaluation.
  - Quick check question: If you provided 3 exemplar SINGLE-T questions before evaluation, would this still be zero-shot? What threat would this pose to the benchmark's validity claim?

- Concept: **Confusing Instance Taxonomy**
  - Why needed here: The paper distinguishes (a) identical form/different meaning ("会" = modal verb vs. "会议" = meeting) from (b) partial form overlap ("A着A着" fixed pattern vs. standard progressive "着"). Understanding this distinction is prerequisite to designing interference-resistant evaluations.
  - Quick check question: Classify: The confusing instance "他在图书馆里看书" for grammar item "[在...看来]" (in someone's opinion)—is this type (a) or (b) interference?

## Architecture Onboarding

- Component map:
  CGLM Grammar Items (739) → SINGLE-T/F (binary classification, single instance)
  → BATCH-T/F (binary classification, 9 instances)
  → SIM-GRA (4-way selection, semantic distractors)
  → CAT-GRA (up to 56-way selection, category distractors)
  → CON-INS-F10/T5F5 (interference resistance, 10 instances)
  Synthetic Sentences (6,651) → Confusing Instance Generator (314 grammar items × synthetic)

- Critical path:
  1. Grammar item selection from CGLM (validates pedagogical relevance)
  2. Synthetic sentence generation with human verification (ensures grammar-item correspondence)
  3. Question construction with distractor generation (creates difficulty tiers)
  4. Zero-shot evaluation with regex scoring (extracts structured outputs)
  5. Sub-question level accuracy aggregation (enables fine-grained diagnosis)

- Design tradeoffs:
  - **Synthetic vs. authentic data**: Synthetic sentences enable scale (6,651) and controlled grammar-item mapping, but may not reflect learner error patterns or natural discourse contexts.
  - **Binary vs. multi-way discrimination**: SINGLE/BATCH use binary output (simpler scoring) while SIM-GRA/CAT-GRA use multi-way selection (better discriminative validity but higher complexity).
  - **Interference density**: CON-INS-F10 maximizes interference (10 confusing instances) for stress-testing but may underrepresent realistic teaching scenarios where confusion is sparser.

- Failure signatures:
  - **Scale-dependent collapse**: Accuracy on BATCH-F drops below 50% random baseline (InternLM2.5-7B: 31.6%)—indicates systematic false-positive bias under context load.
  - **Category confusion**: CAT-GRA accuracy (76.0% avg) significantly below SIM-GRA (87.7% avg) with small models scoring 35-54%—indicates structured grammatical knowledge is poorly differentiated from category-level associations.
  - **Interference susceptibility**: CON-INS-F10 scores near or below random for small models (35.9-58.2%)—indicates surface-form matching dominates over grammatical function analysis.

- First 3 experiments:
  1. **Baseline replication**: Run SINGLE-T/F on your target model to establish grammar recognition baseline. If SINGLE-F accuracy is <85%, the model has fundamental false-positive issues unsuitable for grammar teaching applications without mitigation.
  2. **Scale sensitivity test**: Compare BATCH-T vs. BATCH-F accuracy delta. If delta > 20%, the model lacks stability for multi-sentence instructional scenarios; consider batching limits in deployment.
  3. **Interference threshold mapping**: Test CON-INS with varying confusing instance counts (2, 5, 10) to identify the point where accuracy crosses 70% threshold. This defines safe confusion density for real-world task design.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CPG-EVAL framework be effectively generalized to evaluate pedagogical grammar competence in languages other than Chinese?
- Basis in paper: [explicit] The conclusion explicitly emphasizes the necessity to refine benchmark designs regarding "target instructional language" and "knowledge structure" in future work.
- Why unresolved: The current study relies exclusively on the Chinese Grammar Learning Manual (CGLM), limiting the validation of the framework to Chinese language teaching contexts.
- What evidence would resolve it: Successful implementation and validation of the five CPG-EVAL task types using a different language standard, such as the English Grammar Profile.

### Open Question 2
- Question: Does high performance on CPG-EVAL recognition tasks predict an LLM's ability to generate accurate grammatical explanations or correct student errors?
- Basis in paper: [inferred] The paper defines competence strictly as "Pedagogical Grammar Pattern Recognition" (P-GPR), formalized as a mapping function $f(Gi, Li) \rightarrow \{0, 1\}$, rather than generative teaching ability.
- Why unresolved: The benchmark measures the model's ability to identify correspondence between rules and instances, but does not assess the ability to articulate rules or provide feedback.
- What evidence would resolve it: A correlational study comparing CPG-EVAL scores with human evaluations of LLMs' generative performance in simulated teaching scenarios.

### Open Question 3
- Question: Can specific fine-tuning strategies effectively mitigate the "affirmative judgment strategy" (false positives) observed in smaller models when identifying negative instances?
- Basis in paper: [explicit] The analysis notes a "pervasive difficulty" in identifying negative instances and highlights that smaller models frequently fall below the random baseline due to false-positive tendencies.
- Why unresolved: The paper identifies and diagnoses this limitation but does not propose or test technical interventions to correct this bias in smaller-scale models.
- What evidence would resolve it: A comparative evaluation of smaller models before and after fine-tuning on datasets specifically balanced to penalize affirmative biases.

## Limitations

- Synthetic sentence generation may not capture authentic learner production patterns or contextual factors influencing grammar acquisition
- Zero-shot evaluation protocol may underestimate models' practical teaching capabilities with instructional scaffolding
- Benchmark focuses exclusively on recognition tasks rather than production, limiting assessment of models' ability to generate corrections or explanations

## Confidence

- **High confidence** in empirical findings regarding scale-dependent performance degradation and systematic false-positive bias in smaller models
- **Medium confidence** in generalizability of interference mechanism findings across different grammar item sets or languages
- **Low confidence** in benchmark's ability to predict real-world teaching effectiveness without conversational context or instructional scaffolding

## Next Checks

1. **Cross-linguistic generalization test**: Adapt the benchmark framework to evaluate LLMs on pedagogical grammar for another language (e.g., Spanish or French) using established teaching materials. Compare whether scale-dependent patterns and interference effects replicate across linguistic systems.

2. **Authentic data validation**: Replace synthetic sentences with authentic learner production data (errors and corrections) from language learning corpora. Re-run the benchmark to determine whether synthetic-data performance predicts success on real learner language.

3. **Instructional scaffolding experiment**: Design a "teaching mode" variant of the benchmark where models receive explicit pedagogical context (target learner level, learning objectives) before evaluation. Compare zero-shot versus scaffolding-informed performance to identify practical deployment strategies.