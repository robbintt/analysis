---
ver: rpa2
title: Nearly Tight Bounds for Cross-Learning Contextual Bandits with Graphical Feedback
arxiv_id: '2502.04678'
source_url: https://arxiv.org/abs/2502.04678
tags:
- algorithm
- regret
- feedback
- probability
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cross-learning contextual bandit problem
  with graphical feedback, where pulling an arm reveals losses for neighboring arms
  across all contexts. The key theoretical question is whether an algorithm with $\tilde{O}(\sqrt{\alpha
  T})$ regret exists, where $\alpha$ is the independence number of the feedback graph.
---

# Nearly Tight Bounds for Cross-Learning Contextual Bandits with Graphical Feedback

## Quick Facts
- arXiv ID: 2502.04678
- Source URL: https://arxiv.org/abs/2502.04678
- Authors: Ruiyuan Huang; Zengfeng Huang
- Reference count: 40
- Achieves $\tilde{O}(\sqrt{\alpha T})$ regret for cross-learning contextual bandits with graphical feedback

## Executive Summary
This paper resolves a fundamental question in contextual bandit theory by demonstrating that $\tilde{O}(\sqrt{\alpha T})$ regret bounds are achievable for cross-learning problems with graphical feedback. The work bridges a gap between theory and practice by showing that information revealed through feedback graphs can be leveraged to achieve near-optimal performance without requiring context-dependent arm selection. The algorithms work for both known and unknown context distributions, handling both stochastic and adversarial loss settings.

## Method Summary
The paper presents two algorithms: one for known context distributions using importance-weighted estimators with a standard FTRL subroutine, and another for unknown distributions using epoch-based sampling with rejection sampling procedures. Both algorithms leverage the stochastic nature of contexts while applying adversarial bandit techniques. The core approach involves decomposing the regret and using concentration inequalities to handle the graphical feedback structure. The analysis demonstrates that the stochastic case follows as a natural corollary of the more general adversarial framework.

## Key Results
- Achieves minimax $\tilde{O}(\sqrt{\alpha T})$ regret bound for both stochastic and adversarial contexts
- Provides affirmative answer to whether such bounds are achievable in cross-learning contextual bandits
- Demonstrates algorithms work for both known and unknown context distributions

## Why This Works (Mechanism)
The mechanism exploits the graphical feedback structure by decomposing the overall regret into components that can be bounded separately. The algorithms use importance-weighted estimators to handle the cross-learning aspect, where pulling one arm reveals information about neighboring arms. By leveraging the stochastic nature of contexts while applying adversarial bandit techniques, the approach achieves tighter bounds than would be possible with purely stochastic or purely adversarial methods.

## Foundational Learning
- **Contextual Bandits**: Multi-armed bandit framework where each arm's reward depends on observable context - needed for understanding the cross-learning problem setup; quick check: can identify the difference between context-free and contextual bandits
- **Graphical Feedback**: Information revelation structure where pulling an arm reveals losses for neighboring arms - essential for understanding the feedback mechanism; quick check: can explain how feedback graphs reduce exploration cost
- **Importance-weighted Estimators**: Technique for unbiased estimation when sampling from non-uniform distributions - crucial for handling known context distributions; quick check: can derive the unbiasedness property
- **FTRL (Follow-the-Regularized-Leader)**: Online optimization algorithm for sequential decision making - core subroutine for the known distribution case; quick check: can state the regret bound for FTRL
- **Concentration Inequalities**: Probabilistic tools for bounding deviations - used extensively in the regret analysis; quick check: can apply Hoeffding's inequality to bounded random variables
- **Epoch-based Learning**: Algorithm design pattern using time-based phases - key for handling unknown context distributions; quick check: can explain why epoch-based approaches help with exploration

## Architecture Onboarding
**Component Map**: Context Sampling -> Arm Selection -> Feedback Processing -> Regret Estimation -> Update Rule

**Critical Path**: Context distribution estimation → Epoch scheduling → Rejection sampling → Arm selection → Loss feedback → Importance weighting → Parameter update

**Design Tradeoffs**: The algorithm trades computational complexity for tighter regret bounds. Using epochs and rejection sampling adds overhead but enables handling unknown distributions. The importance-weighted approach requires exact context probabilities for known distributions, limiting practical applicability.

**Failure Signatures**: 
- High variance in context distribution estimates leads to poor rejection sampling efficiency
- Dense feedback graphs reduce the advantage of the $\sqrt{\alpha T}$ bound
- Mis-specification of context distribution causes bias in the known distribution algorithm
- Small epoch sizes cause excessive overhead from rejection sampling

**First Experiments**:
1. Test regret scaling on synthetic data with varying independence numbers
2. Compare performance against non-cross-learning baselines under different graph densities
3. Evaluate robustness to imperfect context distribution estimates

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Results are purely theoretical with no empirical validation
- Importance-weighted estimator requires exact context probabilities, limiting practical use
- Rejection sampling in the unknown distribution case may be computationally intensive
- Analysis relies heavily on concentration inequalities that may not hold in practice

## Confidence
- Theoretical regret bounds: High
- Algorithmic practicality: Medium
- Applicability to real-world scenarios: Low

## Next Checks
1. Implement the proposed algorithms and measure empirical regret performance on benchmark contextual bandit datasets
2. Test the robustness of the epoch-based approach when context distribution estimates are imperfect
3. Evaluate computational complexity and runtime scaling with increasing number of actions and contexts