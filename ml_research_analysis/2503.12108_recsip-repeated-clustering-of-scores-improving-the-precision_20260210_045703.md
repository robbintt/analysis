---
ver: rpa2
title: 'RECSIP: REpeated Clustering of Scores Improving the Precision'
arxiv_id: '2503.12108'
source_url: https://arxiv.org/abs/2503.12108
tags:
- recsip
- responses
- llms
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the reliability challenge in Large Language
  Models (LLMs) by proposing a framework that reduces incorrect responses through
  repeated clustering of LLM outputs. The core method, RECSIP, queries multiple LLMs
  in parallel, evaluates their responses using language scores (ROUGE, METEOR, and
  cross-encoder), clusters similar responses, and iteratively generates callback questions
  when models disagree until consensus is reached or models cannot agree further.
---

# RECSIP: REpeated Clustering of Scores Improving the Precision

## Quick Facts
- **arXiv ID**: 2503.12108
- **Source URL**: https://arxiv.org/abs/2503.12108
- **Reference count**: 35
- **One-line primary result**: Achieves 83.5% accuracy on MMLU-Pro by querying and clustering responses from three LLMs, outperforming the best single model by 5.8 percentage points.

## Executive Summary
RECSIP addresses the reliability challenge in Large Language Models (LLMs) by reducing incorrect responses through repeated clustering of LLM outputs. The core method queries multiple LLMs in parallel, evaluates their responses using language scores (ROUGE, METEOR, and cross-encoder), clusters similar responses, and iteratively generates callback questions when models disagree until consensus is reached or models cannot agree further. When evaluated on the MMLU-Pro benchmark using GPT-4o, Claude, and Gemini, RECSIP achieved an overall accuracy of 83.5%, outperforming the best single model by 5.8 percentage points. The approach effectively filters wrong responses by leveraging model disagreement, though it requires more computational resources and depends on individual model performance. The framework is modular and adaptable for various applications, including automated design and software development.

## Method Summary
RECSIP queries three LLMs (GPT-4o, Claude, Gemini) in parallel on multiple-choice questions, scores responses using ROUGE, METEOR, and a cross-encoder, and clusters them based on semantic similarity. If multiple clusters remain, it constructs a callback multiple-choice prompt with one representative answer per cluster and re-queries all models. This iterative process continues until either a single cluster emerges (consensus) or no reduction in cluster count occurs (disagreement). The system returns the most frequent answer from the largest cluster if consensus is reached, otherwise reports that models could not agree.

## Key Results
- Achieved 83.5% overall accuracy on MMLU-Pro, outperforming the best single model (GPT-4o at 77.6%) by 5.8 percentage points
- Improved precision on specific categories: History (77.5%), Computer Science (84.5%), and Mathematics (81.8%)
- Demonstrated that iterative clustering and callback questions can correct initial wrong answers, with some models switching votes during the process

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating responses from diverse models and clustering them isolates the most probable correct answer by identifying convergence across independent reasoning paths.
- **Mechanism:** RECSIP extends the "Self-Consistency" paradigm from a single model to multiple heterogeneous models (e.g., GPT-4o, Claude, Gemini). It assumes that if models with different training data and architectures converge on the same answer, the likelihood of that answer being correct increases, while hallucinations remain uncorrelated and fail to cluster.
- **Core assumption:** Correct answers are semantically convergent across diverse models, whereas errors are stochastic and divergent.
- **Evidence anchors:**
  - [Abstract]: "...asking multiple models in parallel, scoring and clustering their responses to ensure a higher reliability on the response."
  - [Section 3.1]: "As the LLMs are trained on different data sets and some even have a slightly different architecture, it is expected that we receive more different reasoning paths this way."
  - [Corpus]: *Learning to Trust the Crowd* supports the view that heterogeneous LLMs can be leveraged for instance-level reliability via consensus.
- **Break condition:** If all polled models share a common bias or hallucination (e.g., outdated data), they may agree on a wrong answer, which the system cannot detect (Section 4.3: "all three models answered wrongly").

### Mechanism 2
- **Claim:** Iterative feedback via multiple-choice "callback" prompts resolves disagreements by forcing models to evaluate and select between distinct cluster representatives.
- **Mechanism:** When clusters disagree, the system constructs a multiple-choice prompt containing one representative answer from each cluster. This shifts the task from "generation" (open-ended) to "selection" (discriminative), which is generally easier for LLMs. This loop allows models to switch their vote based on the reasoning provided by peers (via the representative answers).
- **Core assumption:** Models possess the capability to recognize a correct answer when presented with it (recognition over generation), and can be swayed by the reasoning of peer models.
- **Evidence anchors:**
  - [Section 3.2]: "For the callback prompt, we construct a multiple-choice question using the original question, picking one random response from each cluster and formulate them as possible answers..."
  - [Section 4.3]: Mentions cases where "exactly one model was initially correct but switched later," implying the selection mechanism actively changes model states.
  - [Corpus]: *The Avengers* paper suggests uniting smaller models via similar collaborative recipes, reinforcing the selection/aggregation approach.
- **Break condition:** The loop terminates if models refuse to switch votes (cluster count stabilizes), resulting in a "disagreement" rather than a forced consensus (Section 3.2).

### Mechanism 3
- **Claim:** System precision is improved by abstaining from answering when consensus fails, effectively filtering out low-confidence predictions.
- **Mechanism:** RECSIP introduces a third output state ("The models could not agree") alongside Correct/Wrong. By refusing to answer questions where models fundamentally disagree, the system artificially inflates precision (accuracy on answered questions) at the cost of coverage (Section 4.2).
- **Core assumption:** In high-risk environments, an "unknown" response is preferable to a hallucination.
- **Evidence anchors:**
  - [Section 3.1]: "...notice the user that the language models did not find an agreement which means that we do not have a trustful answer."
  - [Section 4.2]: "The performance difference is mostly reached by filtering results where the models do not find an agreement."
  - [Corpus]: Weak/No direct corpus support for this specific abstention mechanism in the provided neighbors, though general reliability literature supports precision-over-recall trade-offs.
- **Break condition:** If the rate of disagreement (disunity) is too high (e.g., 47% in Law), the system becomes computationally expensive and operationally useless for automated pipelines.

## Foundational Learning

- **Concept: Self-Consistency in Chain-of-Thought (CoT)**
  - **Why needed here:** RECSIP is explicitly described as an extension of Self-Consistency. Understanding that "multiple reasoning paths lead to the same answer = higher probability of truth" is the theoretical bedrock of this paper.
  - **Quick check question:** How does RECSIP differ from standard Self-Consistency regarding the source of reasoning paths?

- **Concept: Semantic Textual Similarity (N-grams vs. Cross-Encoders)**
  - **Why needed here:** The system relies on clustering responses using ROUGE/METEOR and a cross-encoder. Engineers must understand why simple overlap (ROUGE) is insufficient (it misses synonyms/semantic equivalence) and why the cross-encoder is used to penalize length differences or verify content similarity.
  - **Quick check question:** Why does the implementation use ROUGE/METEOR in addition to a cross-encoder?

- **Concept: Ensemble Diversity**
  - **Why needed here:** The mechanism fails if models are not diverse (Section 4.4). One must understand that combining GPT-4o, Claude, and Gemini works because they have different "strengths" and training sets.
  - **Quick check question:** What happens to the error correction capability if two of the three models share the exact same training data cutoff and biases?

## Architecture Onboarding

- **Component map:** Orchestrator -> Model Pool -> Evaluator -> Clusterer -> Callback Generator -> Model Pool (iterative)
- **Critical path:** The **Similarity Thresholds** in the Clusterer. If the cross-encoder threshold is too low, distinct answers merge incorrectly (False Positive clustering). If too high, semantic variations of the same answer fail to merge (False Negative clustering).
- **Design tradeoffs:**
  - **Precision vs. Coverage:** The system trades a ~28% "disunity" rate (refusal to answer) for higher accuracy on the remaining 72%.
  - **Cost vs. Reliability:** Requires 3x+ the inference cost per question (parallel calls) plus iterative callback rounds.
- **Failure signatures:**
  - **"Wrong Similarity Detection":** The cross-encoder scores distinct answers as similar (Section 4.3). Suggests tuning the threshold or enforcing strict Regex matches for structured data.
  - **"All Models Wrong":** High agreement score but the answer is factually incorrect (Section 4.3). This is undetectable by the architecture without external tools (RAG/Knowledge Graphs).
  - **"Format Interpretation":** The benchmark or regex matcher captures a secondary answer (e.g., in a list) rather than the primary one (Section 4.3).
- **First 3 experiments:**
  1. **Threshold Calibration:** Run a subset of MMLU-Pro and sweep the cross-encoder similarity threshold. Plot the trade-off between "Disunity" rate and Accuracy to find the optimal operating point.
  2. **Ablation on Callbacks:** Compare performance (Accuracy & Cost) between a "Single-Pass" version (just cluster once and pick largest) vs. the full "Iterative Callback" version.
  3. **Error Analysis of Disunity:** Analyze the 47% disunity rate in "Law" (Table 3). Determine if this is due to model weakness or ambiguous question formulation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does implementing dynamic response weighting based on the domain-specific strengths of individual models (e.g., prioritizing Gemini for Physics) yield higher precision than the current unweighted consensus mechanism?
- **Basis in paper:** [explicit] Section 4.2 states, "An approach for future research would be the analysis of the impact of weighing the responses of the models according to their specific strengths," noting that different models excel in different categories.
- **Why unresolved:** The current implementation treats all models equally during clustering and selection; the paper notes that adding weights introduces a potential point of failure if categorization fails, which has not yet been tested.
- **What evidence would resolve it:** A comparative evaluation on MMLU-Pro where model votes are weighted by their historical accuracy scores per category, measured against the baseline unweighted RECSIP implementation.

### Open Question 2
- **Question:** How can the framework be augmented to detect or mitigate "unanimous hallucinations" where all participating LLMs generate the same incorrect answer?
- **Basis in paper:** [explicit] Section 4.3 and Section 5 explicitly identify this as a limitation, stating, "These cases are not detectable by RECSIP by principle and need further research."
- **Why unresolved:** RECSIP relies on agreement between models to verify truth; if the initial option pool contains only incorrect answers, the logic forces an agreement on one of them, making the system reliant on the individual models' baseline performance.
- **What evidence would resolve it:** Successful integration of an external verification method (e.g., uncertainty estimation or knowledge graphs) that flags low-reliability responses even when model agreement is high.

### Open Question 3
- **Question:** Can the scoring and clustering methodology be effectively adapted for non-natural language outputs, such as code generation or formal logic, without losing semantic precision?
- **Basis in paper:** [explicit] Section 5 recommends the "exploration of additional scoring architectures and their combinations" to evaluate outputs beyond natural language, such as code generation or formal languages.
- **Why unresolved:** The current implementation relies on n-gram metrics (ROUGE, METEOR) and a cross-encoder optimized for natural language, which may not correctly cluster functional code or logic where syntax differs but semantic intent is identical.
- **What evidence would resolve it:** A study applying RECSIP to a coding benchmark (e.g., HumanEval) using code-specific similarity metrics, comparing performance against the natural language configuration.

## Limitations

- **Detection of unanimous hallucinations:** The system cannot identify cases where all models share a common hallucination, as high agreement is misinterpreted as high confidence in correctness.
- **Computational cost and coverage trade-off:** The iterative clustering and callback mechanism requires 3x+ the inference cost and results in a 28% refusal rate, limiting practical deployment in automated pipelines.
- **Dependence on model diversity:** The error correction capability relies on the assumption that diverse models will generate different wrong answers; shared biases across models render the mechanism ineffective.

## Confidence

- **High:** The core mechanism of using clustering and iterative callback to improve precision via consensus is well-supported by results and theoretical alignment with Self-Consistency.
- **Medium:** Claims regarding cost-effectiveness, generalizability to other tasks, and robustness to model diversity are plausible but not thoroughly validated in the paper.
- **Low:** Assertions that the system can always identify and filter out wrong responses are not supported, as common biases and hallucinations remain undetected.

## Next Checks

1. **Threshold Sweep and Trade-off Analysis:** Conduct a systematic sweep of the cross-encoder similarity threshold and analyze the trade-off between "disagreement" rate and overall accuracy. Identify the optimal operating point and assess the sensitivity of performance to threshold choice.

2. **Ablation Study of the Callback Mechanism:** Compare the full iterative callback approach to a single-pass clustering baseline (just cluster once and pick the largest cluster) in terms of both accuracy and computational cost. This will clarify the marginal benefit of the iterative process.

3. **Error Analysis of Persistent Disagreements:** For categories with high disagreement rates (e.g., Law at 47%), analyze the nature of the disagreements: Are they due to model weaknesses, ambiguous questions, or inherent uncertainty in the answers? This will help determine whether the disagreement signal is useful or if it indicates a limitation of the approach.