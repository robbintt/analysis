---
ver: rpa2
title: 'Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic
  Workflows'
arxiv_id: '2506.03332'
source_url: https://arxiv.org/abs/2506.03332
tags:
- judge
- feedback
- arxiv
- judges
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines vulnerabilities in agentic workflows that rely
  on feedback mechanisms, focusing on the reliability of judges. The authors introduce
  a two-dimensional taxonomy characterizing judge behavior along axes of intent (constructive
  vs.
---

# Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows

## Quick Facts
- **arXiv ID:** 2506.03332
- **Source URL:** https://arxiv.org/abs/2506.03332
- **Reference count:** 40
- **Primary result:** Even state-of-the-art agentic workflows degrade significantly under misleading feedback, with grounded-knowledge judges causing over 50% accuracy drops.

## Executive Summary
This paper investigates vulnerabilities in feedback-based agentic workflows where a judge critiques a generator's answers. The authors introduce a two-dimensional taxonomy characterizing judge behavior along intent (constructive, hypercritical, malicious) and knowledge access (no-knowledge, parametric, external retrieval). They construct WAFER-QA, a benchmark with adversarial critiques grounded in retrieved web evidence, to evaluate robustness against factually supported deceptive feedback. Experiments reveal that models are highly susceptible to feedback authority exploitation, with reasoning models showing greater resilience. Multi-round interactions induce oscillatory behavior in non-reasoning models, while grounded-knowledge judges cause the largest performance drops.

## Method Summary
The study evaluates agentic workflows through a generator-judge feedback loop across multiple datasets (ARC-Challenge, Winogrande, GPQA Diamond, SimpleQA) and the WAFER-QA benchmark. Judges are configured with varying intent and knowledge access levels, providing critiques that generators must critically assess and potentially revise. The framework measures accuracy after K rounds (Acc@RK), recovery rates (Srec@RK), and acknowledgment rates. Parametric-knowledge judges use strategic or persuasive prompting styles, while grounded-knowledge judges retrieve real web evidence supporting alternative answers. Multi-round interactions reveal oscillatory patterns in non-reasoning models.

## Key Results
- GPT-4o drops from 96.5% to 76.0% accuracy on ARC-Challenge with template-based critiques
- o4-mini shows 14.4% drop on GPQA-Diamond with parametric-knowledge judges
- Most high-end models suffer over 50% accuracy drops from Acc@R0 to Acc@R1 with grounded-knowledge judges
- Non-reasoning models exhibit oscillatory behavior (alternating correct/incorrect answers) across multiple rounds

## Why This Works (Mechanism)

### Mechanism 1: Feedback Authority Exploitation
- **Claim:** LLMs are highly susceptible to switching correct answers when judges present critiques with apparent authority (confidence, citations, or fabricated evidence).
- **Mechanism:** Models appear to prioritize alignment with external feedback signals over internal parametric knowledge. When feedback is presented confidently or backed by sources (even fabricated ones), models exhibit higher rates of answer reversal, suggesting a tendency to defer to perceived authority.
- **Core assumption:** The generator model interprets confident, source-backed feedback as more reliable than its own reasoning.
- **Evidence anchors:** [abstract] "...even top-performing agents are highly vulnerable to misleading feedback—particularly when critiques are grounded in evidence—often switching correct answers after a single round." [section] "Non-reasoning models struggle to detect fabricated statistics or studies embedded in strategic feedback." (Section 4.3, Table 2)

### Mechanism 2: Evidence-Grounded Persuasion
- **Claim:** Critiques backed by web-retrieved evidence (even for incorrect alternatives) cause the largest performance drops, as models struggle to dismiss verifiable-looking sources.
- **Mechanism:** A grounded-knowledge judge retrieves real passages supporting an alternative answer. The generator encounters a conflict between its internal knowledge and external evidence. Most models tend to accept the external evidence, leading to a switch from correct to incorrect answers.
- **Core assumption:** Models are biased to trust retrieved, citable evidence over parametric memory when both are present.
- **Evidence anchors:** [abstract] "...WAFER-QA, a new benchmark featuring adversarial critiques backed by web-retrieved evidence." [section] "Most high-end models (except the latest o4-mini) suffer performance drops by over 50% from Acc@R0 to Acc@R1, with malicious judges causing the steepest declines." (Section 4.4, Figure 4)

### Mechanism 3: Oscillatory Instability Under Multi-Round Feedback
- **Claim:** Non-reasoning models exhibit oscillatory behavior (alternating correct/incorrect answers) across multiple rounds of feedback, while reasoning models show more stable trajectories.
- **Mechanism:** Non-reasoning models may lack a stable internal "confidence anchor," causing them to repeatedly flip answers when critiqued. Reasoning models, trained for step-by-step verification, appear to maintain consistency even under repeated pressure.
- **Core assumption:** Reasoning-trained models have a more robust internal verification process that resists external perturbation.
- **Evidence anchors:** [abstract] "Multi-round analysis uncovers oscillatory behavior in non-reasoning models, while reasoning models show greater stability." [section] "GPT-4o and Qwen-2.5 share similar oscillatory patterns—most notably ✓×✓×✓—indicating that the model changes its answer back and forth across rounds." (Section 5.1, Figure 6)

## Foundational Learning

- **Concept: Generator-Judge Feedback Loops**
  - **Why needed here:** The entire vulnerability framework depends on understanding how one model (generator) produces an answer and another (judge) critiques it, forming a loop that can be exploited.
  - **Quick check question:** Can you diagram a single generator-judge round, labeling where adversarial feedback is injected?

- **Concept: Parametric vs. Retrieval-Augmented Knowledge**
  - **Why needed here:** The paper's taxonomy hinges on whether judges rely on internal model weights (parametric) or external sources (retrieval). This determines feedback persuasiveness and attack surface.
  - **Quick check question:** What is the difference between a parametric-knowledge judge and a grounded-knowledge judge in terms of information access and potential for fabrication?

- **Concept: Reasoning vs. Non-Reasoning Model Architectures**
  - **Why needed here:** The paper shows reasoning models (e.g., o3-mini, o4-mini) are more robust to deceptive feedback. Understanding what makes them "reasoning" models is key to interpreting results and designing defenses.
  - **Quick check question:** Name one architectural or training difference that might explain why reasoning models are more stable under multi-round feedback.

## Architecture Onboarding

- **Component map:** Generator Agent -> Judge Agent -> Feedback Interface -> Generator Agent (loop)
- **Critical path:**
  1. Load QA example from benchmark
  2. Generator produces initial answer (R0)
  3. Judge retrieves or generates critique based on intent/knowledge configuration
  4. Generator receives feedback and revises answer (R1)
  5. Repeat for multi-round (R2, R3, R4) or evaluate final accuracy
  6. Log accuracy, recovery rate, and oscillation patterns

- **Design tradeoffs:**
  - Judge strength vs. evaluation realism: Stronger judges provide more coherent critiques but may exaggerate vulnerabilities
  - Single-round vs. multi-round: Multi-round exposes instability but increases cost and complexity
  - Prompting for critical evaluation: Instructing generator to "only revise if warranted" adds robustness but may reduce responsiveness to genuine errors

- **Failure signatures:**
  - Single-round drop: Acc@R1 significantly lower than Acc@R0, especially under grounded-knowledge judges
  - Oscillation pattern: Repeated ✓×✓× sequences in multi-round logs
  - Low recovery rate: High Srec@R1 only on easy tasks; near-zero on challenging datasets

- **First 3 experiments:**
  1. **Baseline robustness:** Run a no-knowledge judge on ARC-Challenge for GPT-4o and o3-mini. Compare single-round accuracy drop.
  2. **Grounded attack:** Use WAFER-QA (N) to test a grounded-knowledge malicious judge. Measure performance drop and acknowledgment rate.
  3. **Multi-round oscillation:** Run a hypercritical parametric judge for 4 rounds on Winogrande. Visualize correctness trajectories for GPT-4o and o4-mini.

## Open Questions the Paper Calls Out

- **Question:** How do vulnerabilities in feedback-based agentic workflows manifest in domains beyond QA tasks, such as interactive planning, code generation, and computer use?
- **Basis:** [explicit] Limitations section states: "agentic workflows span a broader range of domains, such as interactive planning, code generation, and computer use—where the nature of feedback and error propagation may differ. Extending our framework to such settings is an important direction for future research."
- **Question:** How does memory-augmented judge behavior affect deceptive feedback dynamics in multi-round interactions?
- **Basis:** [explicit] Limitations section notes: "Our current analysis also assumes that judges are memoryless—that is, they act independently of prior interaction history. Modeling judge behavior in fully interactive or memory-augmented environments may uncover new feedback dynamics."
- **Question:** What mechanisms enable reasoning models (o3-mini, o4-mini) to resist oscillatory behavior under repeated adversarial feedback, and can these mechanisms be transferred to non-reasoning models?
- **Basis:** [explicit] Section 5.1 observes "reasoning models like o4-mini are significantly more stable, suggesting they 'know what they know' and are less perturbed by repeated critical feedback," while non-reasoning models show "pronounced zigzag trajectory" and patterns like "✓×✓×✓".

## Limitations

- WAFER-QA benchmark uses synthetic adversarial critiques generated by GPT-4.1 rather than human adversaries, potentially limiting realism
- Study focuses exclusively on English-language QA tasks, leaving cross-domain and multilingual robustness unexplored
- Does not explore defensive strategies or mitigations, leaving a gap in practical guidance for practitioners

## Confidence

- **High confidence:** The observation that grounded-knowledge judges cause the largest performance drops (>50% accuracy reduction) is well-supported by experimental data across multiple models and datasets
- **Medium confidence:** The oscillatory behavior pattern in non-reasoning models under multi-round feedback is observed but may depend on specific prompting strategies that are not fully disclosed
- **Medium confidence:** The superiority of reasoning models in resisting deceptive feedback is demonstrated, though the exact architectural reasons are not deeply explored
- **Low confidence:** The generalizability of WAFER-QA's findings to real-world deployment scenarios, as the benchmark uses synthetic adversarial critiques rather than human-generated attacks

## Next Checks

1. **Cross-task validation:** Test the same judge configurations on non-QA tasks (e.g., code generation, multi-step reasoning) to assess generalizability of vulnerability patterns
2. **Human vs. synthetic critiques:** Compare performance drops when using human-generated adversarial feedback versus GPT-4.1-generated critiques to validate benchmark realism
3. **Defense mechanism exploration:** Implement and evaluate simple defensive strategies (e.g., confidence thresholding, source verification prompts) to measure potential for robustness improvement