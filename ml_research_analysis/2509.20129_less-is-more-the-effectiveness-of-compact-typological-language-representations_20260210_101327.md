---
ver: rpa2
title: 'Less is More: The Effectiveness of Compact Typological Language Representations'
arxiv_id: '2509.20129'
source_url: https://arxiv.org/abs/2509.20129
tags:
- feature
- features
- language
- selection
- typological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high dimensionality and sparsity
  in typological feature datasets like URIEL+, which limit the effectiveness of distance
  metrics for cross-lingual NLP. The authors propose a pipeline combining feature
  selection and imputation to create compact, interpretable language representations.
---

# Less is More: The Effectiveness of Compact Typological Language Representations

## Quick Facts
- arXiv ID: 2509.20129
- Source URL: https://arxiv.org/abs/2509.20129
- Reference count: 25
- Primary result: Feature selection + imputation pipeline produces compact typological representations that outperform full URIEL+ in distance alignment (ρ up to 0.358) and cross-lingual transfer tasks.

## Executive Summary
This paper tackles the challenge of high dimensionality and sparsity in typological feature datasets like URIEL+, which limit the effectiveness of distance metrics for cross-lingual NLP. The authors propose a pipeline combining feature selection and imputation to create compact, interpretable language representations. Using four feature selection methods—Variance, PCA Loadings, Laplacian Score, and Correlation-based Feature Selection—alongside SoftImpute for missing value imputation, they produce reduced-size typological feature subsets. These subsets outperform the full URIEL+ feature space in linguistic distance alignment with a known similarity measure (Spearman correlation ρ up to 0.358 for Laplacian Score at 300 features). Downstream task results show consistent improvements: up to 7.8% gain in cross-lingual transfer ranking (LangRank NDCG@3), 1.4% reduction in language model performance prediction error (ProxyLM RMSE), and comparable accuracy in linguistic regularization tasks (LinguAlchemy), demonstrating that smaller, well-chosen feature sets yield more informative language distances and improve multilingual NLP applications.

## Method Summary
The authors address URIEL+'s high dimensionality (800 features, 87% sparsity) by combining feature selection and imputation to create compact, interpretable language representations. Four feature selection methods are applied: Variance ranking, PCA Loadings, Laplacian Score (which preserves manifold structure by measuring feature smoothness across similar languages), and Correlation-based Feature Selection (CFS) emphasizing phylogenetic relevance. Each method produces subsets of size k ∈ {100, 200, ..., 700}. SoftImpute is then applied to complete missing values by approximating a low-rank matrix. Angular distances are computed between language vectors, and performance is evaluated through alignment with linguistic distance measures and three downstream tasks: LangRank (cross-lingual transfer ranking), ProxyLM (language model performance prediction), and LinguAlchemy (linguistic regularization).

## Key Results
- Laplacian Score subset of 300 features achieves highest linguistic distance alignment with modified Gower coefficient (ρ = 0.358)
- LangRank NDCG@3 improves by up to 7.8% over baseline using optimal feature subsets
- ProxyLM RMSE reduces by up to 1.4% with selected features
- Downstream tasks consistently outperform full URIEL+ representation while using fewer features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing feature dimensionality via principled selection improves typological distance quality by removing redundant and noisy features.
- **Mechanism:** The 800-feature URIEL+ typological space contains strongly overlapping features from multiple sources. Feature selection methods identify statistically informative features while eliminating those that add noise without discriminative value. Lower-dimensional spaces improve separability and reduce overfitting risk.
- **Core assumption:** Not all typological features are equally informative for capturing cross-lingual relationships; redundancy exists and harms distance metrics.
- **Evidence anchors:**
  - [abstract] "high dimensionality and sparsity... limit the effectiveness of distance metrics"
  - [Page 1] "many of which overlap or correlate strongly... 87% of values are still missing"
  - [Page 3] "All feature selection methods, at the right subset size, outperforms the full URIEL+ feature space"
- **Break condition:** If downstream tasks require features that are individually low-variance but collectively critical (e.g., rare but diagnostic features), aggressive selection may discard useful signal.

### Mechanism 2
- **Claim:** Laplacian Score preserves manifold structure better than variance-based selection for typological data.
- **Mechanism:** Laplacian Score ranks features by how smoothly they vary across similar languages (k-nearest neighbors in feature space). Features encoding intrinsic typological patterns remain stable across linguistically related languages, receiving lower scores. This graph-based criterion captures locality that variance and PCA miss.
- **Core assumption:** Meaningful typological features should exhibit consistency across genetically/typologically similar languages.
- **Evidence anchors:**
  - [Page 2] "meaningful features should vary smoothly across similar languages"
  - [Page 3] "Laplacian Score strategy achieves the strongest Gd alignment, with ρ=0.358... using only 300 features"
  - [corpus] Related work (Oncevay et al., 2020) fuses typological vectors with task-learned embeddings—suggesting structure preservation matters.
- **Break condition:** If the k-NN graph poorly reflects true linguistic relationships (due to sparsity), locality preservation may reinforce artifacts rather than genuine structure.

### Mechanism 3
- **Claim:** SoftImpute enables robust distance computation by completing sparse matrices while preserving global structure.
- **Mechanism:** SoftImpute approximates a low-rank matrix by spectral regularization, distributing information from observed entries to infer missing ones. This allows computing angular distances between any language pair, including low-resource languages with sparse feature coverage.
- **Core assumption:** The true language-feature matrix is approximately low-rank; missing values can be inferred from observed patterns.
- **Evidence anchors:**
  - [Page 2] "SoftImpute fills unknown typological values... by approximating a low-rank matrix"
  - [Page 3] "consistent performance gains further indicate that our combined pipeline... can effectively trim noisy and redundant features"
  - [corpus] Khan et al. (2025) found SoftImpute strongest among imputation methods in URIEL+ evaluations.
- **Break condition:** If missingness is not random but systematic (e.g., entire feature categories missing for certain families), low-rank assumptions may introduce systematic bias.

## Foundational Learning

- **Concept: URIEL/URIEL+ knowledge base**
  - Why needed here: The paper operates on URIEL+'s 4,555 languages × 800 features matrix; understanding what typological/phylogenetic/geographic features encode is prerequisite.
  - Quick check question: Can you explain why 87% sparsity arises even after aggregating across 14 data sources?

- **Concept: Feature selection methods (filter-based)**
  - Why needed here: The four methods (Variance, PCA Loadings, Laplacian Score, CFS) use different statistical criteria; choosing appropriately requires understanding their biases.
  - Quick check question: Why does Laplacian Score rank features in ascending order while Variance uses descending order?

- **Concept: Cross-lingual transfer and language distance**
  - Why needed here: The downstream tasks (LangRank, ProxyLM, LinguAlchemy) all use typological distances to predict or improve transfer—mechanistic understanding requires grasping how distances inform transfer decisions.
  - Quick check question: What does Spearman correlation ρ between derived distances and Gd measure, and why is higher ρ desirable?

## Architecture Onboarding

- **Component map:**
  URIEL+ raw (4555 × 800) → Preprocessing (union aggregation) → Feature Selection (Var/PCA/LS/CFS) → Imputation (SoftImpute) → Distance Computation (angular) → Downstream Tasks

- **Critical path:**
  1. Load URIEL+ typological subset (union-aggregated across sources)
  2. Choose feature selection method and subset size k ∈ {100, 200, ..., 700}
  3. Compute feature scores, select top-k
  4. Apply SoftImpute to complete missing values
  5. Compute angular distances between language vectors
  6. Evaluate on alignment (Gd correlation) or downstream tasks

- **Design tradeoffs:**
  - **Subset size:** Smaller k (100-300) favors LS and CFS; larger k (500-700) favors Variance and PCA. Task-dependent.
  - **Method selection:** PCA favors global variance; CFS emphasizes phylogenetic relevance; LS preserves manifold locality.
  - **Imputation-only vs. selection+imputation:** Paper shows selection adds value beyond imputation alone (baseline uses imputation without selection).

- **Failure signatures:**
  - Distance alignment drops below baseline (ρ < 0.260) → likely over-aggressive pruning or method mismatch
  - Downstream task degrades → check if selected features omit category critical for that task (e.g., syntax for LinguAlchemy)
  - High variance in cross-validation folds → sparsity may be too severe for reliable imputation in that subset

- **First 3 experiments:**
  1. **Reproduce alignment result:** Compute Laplacian Score subset at k=300, apply SoftImpute, compute angular distances for the 28 Isthmo-Colombian language pairs, verify ρ ≈ 0.358 against Gd.
  2. **Ablate selection method:** Run all four methods at k=200 and k=500 on LangRank DEP subtask; confirm LS/CFS outperform at low k, Variance/PCA improve at high k.
  3. **Probe feature type bias:** For each method, compute proportion of retained features by type (syntax, phonology, inventory) against expected uniform distribution; verify inventory features are under-retained (Figure 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid or ensemble feature selection strategy yield more balanced and effective typological representations?
- Basis in paper: [explicit] The authors state in the Limitations section that "selected subsets may emphasize certain linguistic properties over others. A hybrid or ensemble-based selection strategy may yield more balanced representations."
- Why unresolved: The current study evaluates four distinct methods (Variance, PCA, Laplacian, CFS) individually, finding that each excels in different tasks or subset sizes.
- What evidence would resolve it: An experimental comparison showing that a combined selection approach outperforms individual methods across alignment and downstream tasks.

### Open Question 2
- Question: How robust are these compact representations when evaluated on a broader, typologically diverse set of low-resource languages?
- Basis in paper: [explicit] The authors note the "evaluation focuses on a relatively small set of languages... with an emphasis on high-resource languages," and suggest future work should "incorporate broader benchmarks, including... typologically varied evaluation sets."
- Why unresolved: The current findings are based on limited evaluation sets (e.g., 19 languages for alignment), limiting generalizability across different language families and resource levels.
- What evidence would resolve it: Replicating the pipeline experiments on a large-scale benchmark specifically designed for diverse, low-resource languages.

### Open Question 3
- Question: Do alternative imputation techniques like random forests or autoencoders improve results for highly sparse languages compared to SoftImpute?
- Basis in paper: [explicit] The authors acknowledge, "alternative techniques... could better capture linguistic relationships and offer complementary benefits, particularly for highly sparse languages."
- Why unresolved: The pipeline relies solely on SoftImpute based on prior URIEL+ findings, leaving the potential benefits of other imputation methods within this specific compact framework untested.
- What evidence would resolve it: Ablation studies substituting SoftImpute with autoencoder-based methods, measuring performance gains in downstream tasks for languages with high missing data rates.

## Limitations

- **Downstream task benchmarks are sensitive to evaluation setup.** LANGRANK results vary across subtasks (morphology, syntax, phonology), and gains are modest (<8%). This suggests the typological feature pipeline is beneficial but not transformative—its impact is contingent on the task and feature subset size.

- **Missingness handling is a potential confounder.** The paper does not clarify whether feature selection is applied before or after imputation. If scores are computed on incomplete data, the selection may be biased toward features with more observed values rather than more informative ones.

- **Low-resource language coverage is untested in downstream tasks.** While the method enables distance computation for low-resource languages, the paper does not report how performance varies when such languages are source vs. target in transfer tasks.

## Confidence

- **High confidence** in the core claim that dimensionality reduction improves linguistic distance alignment. This is supported by direct correlation metrics and multiple selection methods.
- **Medium confidence** in downstream gains. Gains are task-dependent and relatively small; they are consistent but not uniform.
- **Low confidence** in the robustness of method comparisons. Ablation results show variance across tasks and subset sizes, and no statistical significance testing is reported.

## Next Checks

1. **Verify Laplacian Score ranking order.** Confirm that the method ranks features in ascending order and that the best alignment (ρ = 0.358 at k = 300) is reproducible with correct graph construction and heat kernel parameters.

2. **Test imputation order sensitivity.** Run the pipeline with imputation before and after feature selection on a subset of languages; compare downstream performance to detect bias from computing statistics on incomplete data.

3. **Analyze feature retention by type.** Compute the distribution of retained features (syntax, phonology, inventory, etc.) for each selection method at k = 200 and k = 500; verify the pattern that inventory features are under-retained, especially by Laplacian Score and CFS.