---
ver: rpa2
title: 'Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology
  Reporting with Quality Control'
arxiv_id: '2512.02814'
source_url: https://arxiv.org/abs/2512.02814
tags:
- report
- radiology
- liver
- quality
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Radiologist Copilot, an agentic AI assistant
  designed to automate radiology reporting with quality control. The system leverages
  large language models as a reasoning backbone to autonomously select and orchestrate
  tools for the complete radiology reporting process, including region localization,
  region analysis planning, strategic template selection, quality assessment, and
  feedback-driven refinement.
---

# Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control

## Quick Facts
- arXiv ID: 2512.02814
- Source URL: https://arxiv.org/abs/2512.02814
- Reference count: 26
- One-line primary result: Training-free agentic AI system achieving BLEU-1 of 0.4025 on liver radiology reporting with integrated quality control

## Executive Summary
Radiologist Copilot is an agentic AI system that automates radiology reporting through orchestrated tool selection and quality control. The system uses a large language model as a reasoning backbone to autonomously select and coordinate specialized tools for the complete radiology workflow, including region localization, analysis planning, template selection, and quality assessment. Experimental results on liver radiology reporting demonstrate significant performance improvements over state-of-the-art methods, with the system showing strong robustness across different 3D medical vision-language models.

## Method Summary
The system employs a training-free approach using OctoTools framework with Qwen3-32B as the reasoning backbone. It processes 3D CT images through a sequential workflow: segmentation using TotalSegmentator produces organ and lesion masks, Region Analysis Planning (RAP) generates structured analysis prompts based on these masks, Hulu-Med 3D VLM performs region-specific analysis, Strategic Template Selection (STS) adapts relevant report templates to findings, and a Quality Controller iteratively refines reports until they meet clinical standards. The complete process is limited to 10 steps and 500 seconds per case.

## Key Results
- Achieved BLEU-1 of 0.4025, ROUGE-L of 0.3222, METEOR of 0.4560, BERTScore of 0.7024 on AMOS-MM dataset
- Demonstrated strong robustness across different 3D medical vision-language models (Hulu-Med, RadFM, CT-CHAT)
- Ablation studies confirm critical importance of Region Analysis Planning and Strategic Template Selection components

## Why This Works (Mechanism)

### Mechanism 1: Orchestrated Tool Selection via LLM Reasoning Backbone
The LLM acts as a reasoning core to autonomously select and coordinate specialized tools without task-specific training. Using OctoTools framework, an action planner analyzes queries, determines intermediate goals, and selects appropriate tools while an action executor generates commands and captures outcomes. Memory tracks action history to verify task completion and enable coherent multi-step reasoning. The core assumption is that the LLM possesses sufficient domain knowledge to distinguish when to invoke segmentation vs. analysis vs. quality control.

### Mechanism 2: Region Analysis Planning (RAP) with Think-with-Image Paradigm
RAP improves VLM analysis quality by providing explicit, context-aware prompts based on segmented regions. After segmentation produces organ and lesion masks, RAP dynamically determines whether lesion-related analysis should proceed and defines specific analysis items covering organ and lesion characteristics. These items become structured prompts for the 3D medical VLM to analyze extracted region-of-interest images. The core assumption is that segmentation masks accurately localize organs/lesions and the VLM can reliably analyze cropped region images with structured prompts.

### Mechanism 3: Strategic Template Selection (STS) with Quality Control Feedback Loop
STS combines template-guided generation with iterative quality assessment to produce clinically compliant reports. The system clusters training reports via BioBERT embeddings and K-means to create template reports, then the LLM selects the most relevant template based on analysis results to generate findings and impression sections. The Quality Controller assesses format, content, language, and expression, triggering feedback-driven regeneration for unqualified reports. The core assumption is that template clustering produces representative report structures and LLM-based quality assessment reliably detects clinical deficiencies.

## Foundational Learning

- **Medical Vision-Language Models (VLMs) for 3D Imaging**
  - Why needed here: The Analyzer Tool depends on Hulu-Med (or alternative 3D VLMs) to interpret CT region images and generate analysis results
  - Quick check question: Can you explain how a 3D medical VLM differs from a 2D VLM in handling volumetric CT data, and what prompt structures typically improve its analysis accuracy?

- **Agentic Frameworks with Tool Orchestration**
  - Why needed here: The system is built on OctoTools and requires understanding action planning, execution, memory management, and termination conditions
  - Quick check question: How does an action planner differ from an action executor in an agentic system, and what role does memory play in determining task completion?

- **Segmentation-to-Analysis Pipelines**
  - Why needed here: The Segmentator outputs (organ/lesion masks) directly feed into RAP and region extraction
  - Quick check question: If a segmentation model produces a lesion mask with 70% confidence, how should that uncertainty be represented or handled in downstream analysis planning?

## Architecture Onboarding

- **Component map**:
  Input Layer (CT image + query) -> Reasoning Core (LLM backbone) -> Action Planner -> Action Executor -> Tool Library (Segmentator, Analyzer, Report Generator, Quality Controller) -> Memory Module -> Output (Qualified report)

- **Critical path**:
  Query → Action Planner (plan: segment) → Segmentator → Action Planner (plan: analyze) → Analyzer (RAP + VLM) → Action Planner (plan: generate) → Report Generator (STS) → Action Planner (plan: assess quality) → Quality Controller → [if unqualified: feedback → regenerate] → [if qualified: return report]

- **Design tradeoffs**:
  - Training-free vs. fine-tuned: System operates without additional training (flexibility, rapid deployment) but may underperform specialized fine-tuned models on narrow tasks
  - Template-based vs. free-form generation: STS provides structure and consistency but may miss novel findings not represented in templates
  - LLM-based QC vs. rule-based QC: LLM assessment captures nuanced errors but introduces subjectivity and potential hallucination in feedback

- **Failure signatures**:
  - Segmentation failure: Missing lesions in masks → RAP skips lesion analysis → report omits critical findings. Detect via mask coverage audits.
  - Template mismatch: STS selects irrelevant template → report structure incongruent with findings. Detect via template-query similarity thresholds.
  - QC false negative: Quality Controller passes report with factual errors. Detect via random human audits or secondary validation tools.
  - Infinite loop: Feedback-driven refinement cycles without reaching "qualified" state. Mitigate with max iteration limits (current: 10 steps).

- **First 3 experiments**:
  1. Tool ablation baseline: Run system with each tool removed individually to confirm component contributions on held-out validation set
  2. VLM substitution test: Replace Hulu-Med with RadFM and CT-CHAT to assess robustness across VLM backbones
  3. QC threshold calibration: Vary quality assessment strictness and measure precision/recall of flagged errors against human radiologist review

## Open Questions the Paper Calls Out

- **Generalization to other anatomical structures**: The system shows feasibility for chest or abdomen radiology reports, but current validation is restricted to liver task on AMOS-MM dataset. The system's ability to handle distinct morphological characteristics and reporting standards of other organs remains unproven.

- **STS vs GREEN score trade-off**: Removing Strategic Template Selection slightly improves GREEN scores despite degrading other metrics. The specific mechanism causing this trade-off between lexical similarity and clinical error notation remains unclear.

- **Computational latency for clinical deployment**: The sequential process involving segmentation, VLM analysis, and LLM reasoning with 500-second timeout per case raises questions about viability for real-time clinical deployment compared to single-pass models.

## Limitations
- Training-free orchestration may limit performance on rare or complex cases where specialized fine-tuning would be beneficial
- LLM-based quality control introduces potential subjectivity and hallucination risks that are difficult to quantify without extensive human validation
- Template clustering depends on representative training data; unrepresented clinical scenarios may lead to suboptimal report structures

## Confidence
- **High confidence**: Core mechanism of orchestrated tool selection via LLM reasoning backbone (supported by ablation showing 0.0425 BLEU-1 drop when removed)
- **Medium confidence**: Region Analysis Planning effectiveness (strong ablation evidence but limited external validation)
- **Medium confidence**: Strategic Template Selection with iterative QC (ablation shows significant impact but quality assessment validation is limited)

## Next Checks
1. **Human evaluation study**: Conduct blinded radiologist review of 100 randomly selected reports comparing Radiologist Copilot outputs against human-generated reports
2. **Cross-dataset generalization test**: Evaluate system performance on held-out external radiology dataset from different institution to assess robustness to institutional variation
3. **Failure mode analysis**: Systematically document and categorize system failures across 50 cases where quality control triggered regeneration, identifying failure sources