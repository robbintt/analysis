---
ver: rpa2
title: On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion
  Management
arxiv_id: '2504.13314'
source_url: https://arxiv.org/abs/2504.13314
tags:
- agent
- perturbation
- system
- robustness
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a framework to assess the robustness and resilience\
  \ of reinforcement learning (RL) agents in real-time congestion management, addressing\
  \ a gap in the European Union\u2019s AI Act, which mandates robustness and resilience\
  \ but lacks assessment methodologies. The approach uses the Grid2Op digital environment,\
  \ where perturbation agents simulate natural and adversarial disruptions by altering\
  \ AI input data without modifying the actual system state."
---

# On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management

## Quick Facts
- arXiv ID: 2504.13314
- Source URL: https://arxiv.org/abs/2504.13314
- Reference count: 19
- Primary result: A framework for assessing robustness and resilience of RL agents in power grid congestion management using perturbation agents

## Executive Summary
This paper proposes a framework to assess the robustness and resilience of reinforcement learning (RL) agents in real-time congestion management, addressing a gap in the European Union's AI Act, which mandates robustness and resilience but lacks assessment methodologies. The approach uses the Grid2Op digital environment, where perturbation agents simulate natural and adversarial disruptions by altering AI input data without modifying the actual system state. Robustness is evaluated through stability and reward impact metrics, while resilience is quantified by recovery from performance degradation. Experimental results on the IEEE-14 bus system show that the framework effectively identifies vulnerabilities, with RL-based perturbation agents causing significant performance drops (below 30% in key metrics), whereas random and gradient-based perturbations had less severe impacts. The methodology enables quantitative certification of AI agents for safe operation in critical power grid applications.

## Method Summary
The framework uses Grid2Op's IEEE-14 bus environment to evaluate RL agents under three perturbation types: Random (RPA), Gradient-based (GEPA), and Reinforcement Learning (RLPA). Perturbations are bounded at 10% and applied to state observations (generator outputs, loads, power flows) without altering the true environment state. Robustness is measured via total reward, survival steps, action changes, action similarity, and reward per action. Resilience is quantified through degradation/restoration times, area between reward curves, and cosine similarity to unperturbed states. The RLPA agent uses a DQN with greedy action-space reduction heuristic to optimize attack strategies.

## Key Results
- RLPA attacks caused total reward and survival time to drop below 30% of baseline performance
- GEPA attacks showed moderate impact with long degradation and restoration times (~476 and ~467 steps avg)
- RPA attacks maintained >90% performance but reduced reward per action ratio
- The framework successfully identified input vulnerabilities and quantified recovery capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perturbation agents can systematically identify input vulnerabilities in RL-based power grid controllers without requiring access to model internals or environment modification.
- **Mechanism:** A perturbation agent intercepts the AI system's input observations and applies controlled modifications (zeroing values, scaling via lognormal noise, or gradient-based adversarial examples) while the true environment state remains unchanged. This decouples attack simulation from actual grid operations, enabling safe stress-testing of pre-trained agents.
- **Core assumption:** Input perturbations—rather than reward or action-space attacks—are the primary realistic threat vector for operational AI assistants, since reward functions and models reside in protected IT systems while sensor data is more exposed.
- **Evidence anchors:**
  - [abstract]: "perturbation agents simulate natural and adversarial disruptions by perturbing the input of AI systems without altering the actual state of the environment"
  - [Section II]: "focus is placed on state space perturbations due to: a) reward functions and AI models being secured in cyber-safe IT systems... b) action space operations being linked to OT systems with high cyber-security standards"
  - [corpus]: Weak direct corpus support; neighbor papers focus on traffic/multi-agent systems rather than power grid adversarial testing.
- **Break condition:** If the AI agent has access to redundant state verification (e.g., cross-checked sensors or physics-based state estimation), input perturbations alone may fail to induce meaningful policy deviations.

### Mechanism 2
- **Claim:** Robustness can be quantified as a combination of performance preservation (reward maintenance) and output stability (action consistency) under perturbation.
- **Mechanism:** The framework computes: (1) total reward difference between perturbed and unperturbed trajectories (Eq. 3), (2) action change frequency (Eq. 4), (3) action similarity scores combining shared substation modifications (Eq. 5–7), and (4) survival time before grid failure (Eq. 8). These align with ISO/IEC 24029-2 stability and reachability properties.
- **Core assumption:** Stability is operationally defined relative to an unperturbed baseline; deviation from this baseline indicates fragility even if the alternative action is functionally adequate.
- **Evidence anchors:**
  - [abstract]: "Robustness is measured through stability and reward impact metrics"
  - [Section III-B]: "The metrics defined in Eq. 4–7 allow the verification of the stability property... This property evaluates whether the system's output remains consistent despite variations in the input"
  - [Section IV-B, Fig. 2]: "the AI system performs well under both the RPA and GEPA, as the total reward and number of steps remain above 90% in most cases"
  - [corpus]: Limited corpus alignment; neighbor papers address resilience conceptually but lack comparable metric frameworks.
- **Break condition:** If multiple actions have equivalent optimality (degenerate action space), action change counts may overstate vulnerability when the agent is simply selecting among equally valid alternatives.

### Mechanism 3
- **Claim:** Resilience manifests as measurable degradation-recovery cycles in both reward trajectories and grid state divergence.
- **Mechanism:** Resilience is quantified by: (1) area between perturbed and unperturbed reward curves (Eq. 11), (2) degradation time from perturbation onset to minimum reward (Eq. 12), (3) restoration time from minimum to maximum recovery (Eq. 13), and (4) cosine similarity between perturbed and unperturbed grid states over time (Eq. 16). Recovery to near-baseline indicates resilient adaptation.
- **Core assumption:** The AI agent's policy contains implicit recovery behaviors (learned during training) that can re-stabilize the system if perturbation effects are transient or bounded.
- **Evidence anchors:**
  - [abstract]: "resilience quantifies recovery from performance degradation"
  - [Section III-C, Fig. 1]: Visual definition of degradation and restorative states
  - [Section IV-B, Fig. 5]: "It can be seen that the AI agent can recover the state to the unperturbed state perfectly in about 20 steps after the first drop-off"
  - [corpus]: Neighbor paper "Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks" addresses resilience but in a multi-agent accountability context rather than single-agent recovery measurement.
- **Break condition:** If perturbations persist indefinitely or compound across episodes, recovery may not occur; the framework measures resilience under finite-duration perturbation exposure.

## Foundational Learning

- **Concept: Reinforcement Learning Policy and Value Functions**
  - **Why needed here:** The perturbation agents target the AI agent's policy outputs (action selection) and value estimates (Q-values). Understanding how policy π(a|s) and value Q(s,a) interact is essential to interpret why gradient-based attacks minimize action values.
  - **Quick check question:** If an attacker minimizes Q(s, a*) for the optimal action a*, what happens to the probability of selecting a* under a greedy policy?

- **Concept: Projected Gradient Descent (PGD) and FGSM Adversarial Attacks**
  - **Why needed here:** GEPA uses PGD with gradient estimation (Algorithm 2) and RLPA uses FGSM (Eq. 2). These require understanding how iterative perturbation refinement differs from single-step methods.
  - **Quick check question:** Why does PGD typically produce stronger adversarial examples than FGSM, and what is the computational tradeoff?

- **Concept: Power Grid Topology Actions and Congestion**
  - **Why needed here:** The action similarity metric (Eq. 5–6) requires understanding what topology changes mean (bus bar switching, line reconnection). Congestion management involves rerouting power flow to prevent line overloads.
  - **Quick check question:** If action a1 switches line L1 to bus bar 2 and action a2 switches line L2 to bus bar 2 at the same substation, would Eq. 6 assign a high or low similarity score?

## Architecture Onboarding

- **Component map:**
Environment emits true state s → Perturbation agent transforms to sadv → AI agent selects action aadv → Action executed in environment → Reward R, next state s' computed → Metrics computed comparing perturbed vs unperturbed trajectories

- **Critical path:**
1. Environment emits true state s
2. Perturbation agent intercepts and transforms to sadv
3. AI agent selects action aadv based on sadv
4. Action executed in environment, yielding reward R and next state s'
5. Metrics computed by comparing (R, aadv, s') against unperturbed baseline

- **Design tradeoffs:**
- **RPA vs. GEPA vs. RLPA:** RPA is computationally cheap but low-efficacy (90%+ robustness). GEPA requires gradient estimation queries (2 queries per dimension per iteration) but produces moderate attacks. RLPA requires full training but achieves <30% robustness—most effective for vulnerability discovery.
- **Perturbation budget (ξ = 10%):** Chosen based on AC state estimation detection thresholds [10]; larger budgets detectable, smaller budgets ineffective.
- **Episode length (8064 steps):** Long episodes required to observe multiple degradation-recovery cycles; short episodes may miss resilience patterns.

- **Failure signatures:**
- RLPA exposure: Total reward and survival time drop below 30% of baseline (Fig. 2); action changes exceed 50 per 1000 steps (Table I).
- GEPA exposure: Long degradation and restoration times (~476 and ~467 steps avg, Table II) but moderate reward impact.
- RPA exposure: Low reward-per-action ratio (more corrective actions needed), stable overall performance.

- **First 3 experiments:**
1. **Baseline robustness profiling:** Run the pre-trained AI agent under RPA at p = 20%, 40%, 60%, 80%, 100% perturbation probability. Plot total reward and survival time as percentages of unperturbed performance. Identify the p threshold where performance drops below 90%.
2. **Vulnerability heatmap generation:** Apply GEPA with W=10 iterations and ξ=10%. Compute Eq. 10 for each input dimension to identify which grid components (generators, loads, lines) have highest action-change sensitivity. Visualize as in Fig. 3.
3. **Recovery dynamics analysis:** Expose the agent to RLPA for 100 episodes. Compute degradation time, restoration time, and cosine similarity curves (Eq. 16). Identify whether recovery occurs and the typical timestep lag to return to similarity >0.95.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning algorithms be advanced to create more intelligent adversarial agents that optimize attack strategies by incorporating features such as attack location and perturbation budget into the reward function?
- **Basis in paper:** [explicit] The Conclusion states future work involves "developing more intelligent adversarial agents, e.g., based on an RL algorithm, that includes features (e.g., where to attack, perturbation budget) in the reward function."
- **Why unresolved:** The current RL-based perturbation agent (RLPA) uses a simplified single-objective approach with a fixed upper bound and a greedy heuristic for action space reduction, which limits its ability to optimize complex attack strategies.
- **What evidence would resolve it:** A new RL agent framework that dynamically selects attack vectors and budgets, demonstrating higher efficacy in degrading the AI agent's performance compared to the current RLPA.

### Open Question 2
- **Question:** What specific methodology can be used to co-identify thresholds for robustness and resilience metrics with human decision-makers to certify an AI system for safe operation?
- **Basis in paper:** [explicit] The Conclusion lists "developing a methodology to co-identify thresholds (with the decision-maker) for the metrics proposed in this paper and determine if a certain AI-based system is certified for safe operation" as a future step.
- **Why unresolved:** While the paper defines metrics for robustness (e.g., reward impact, stability) and resilience (e.g., recovery time), it does not establish acceptable ranges or "pass/fail" thresholds for these metrics to guide certification.
- **What evidence would resolve it:** A proposed set of threshold values or a decision framework linking specific metric ranges to safety certification standards for power grid operation.

### Open Question 3
- **Question:** How does the computational complexity and efficacy of the RL-based Perturbation Agent (RLPA) scale when applied to high-dimensional grid environments beyond the IEEE-14 bus system?
- **Basis in paper:** [inferred] The RLPA utilizes a "greedy action space reduction heuristic" to manage the "exponentially growing number of possible perturbations," and the case study is limited to the small IEEE-14 bus system.
- **Why unresolved:** The reliance on a greedy heuristic to manage complexity suggests the method may struggle to find optimal attacks or become computationally prohibitive in realistic, large-scale grids without further optimization.
- **What evidence would resolve it:** Performance benchmarks of the RLPA (attack success rate and computation time) applied to larger standard grid environments, such as IEEE-118 or the L2RPN WCCI 2020 dataset.

### Open Question 4
- **Question:** Are the proposed robustness and resilience metrics effective for evaluating the AI system against perturbations that alter the actual physical state of the environment, rather than just the observed input?
- **Basis in paper:** [inferred] The methodology restricts the perturbation agent to altering the "input of the AI system without altering the actual state of the environment," excluding physical manipulations or action-space attacks.
- **Why unresolved:** Real-world adversarial scenarios (e.g., cyber-physical attacks) may alter the grid state directly; the current metrics focus on perception errors, so their utility for physical state recovery is unproven.
- **What evidence would resolve it:** A modified simulation where perturbations affect the environment's physical state, showing that the proposed resilience metrics (like cosine similarity of state) still accurately reflect recovery capabilities.

## Limitations
- The framework's perturbation budget (ξ = 10%) is based on AC state estimation detection thresholds, but actual grid detection sensitivity may vary by topology and sensor configuration. The IEEE-14 bus system may not capture behaviors in larger, more complex grids.
- The curriculum agent baseline performance and training methodology are referenced but not fully specified, making direct replication dependent on external sources. RLPA training convergence criteria are not provided.
- Resilience metrics assume finite-duration perturbations; the framework does not address persistent or compounding perturbations that could cause permanent degradation.

## Confidence
- **High confidence:** The perturbation agent architecture and metric definitions (Eq. 3-16) are clearly specified and mathematically sound.
- **Medium confidence:** The effectiveness ranking of perturbation agents (RPA < GEPA < RLPA) is supported by results but depends on training success of RLPA.
- **Low confidence:** Generalization of resilience metrics to larger grid systems and different congestion management objectives.

## Next Checks
1. Test the perturbation framework on a larger grid system (e.g., IEEE-118 bus) to assess scalability and metric sensitivity to system complexity.
2. Implement a cross-validation experiment where perturbed states are fed to a separate state estimator to verify that 10% perturbations remain undetected in practice.
3. Run ablation studies removing individual metric components (e.g., action similarity vs. reward difference) to determine which best predict operational risk in congestion management.