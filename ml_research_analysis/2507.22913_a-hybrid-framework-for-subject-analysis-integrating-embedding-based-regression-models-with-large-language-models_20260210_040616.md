---
ver: rpa2
title: 'A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression
  Models with Large Language Models'
arxiv_id: '2507.22913'
source_url: https://arxiv.org/abs/2507.22913
tags:
- terms
- subject
- lcsh
- llms
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hybrid framework integrating embedding-based
  ML models with LLMs to improve subject analysis for library cataloging. It addresses
  LLM limitations in determining optimal label counts and reducing hallucinations
  by using ML models to predict the number of subject headings and post-edit outputs.
---

# A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models

## Quick Facts
- **arXiv ID:** 2507.22913
- **Source URL:** https://arxiv.org/abs/2507.22913
- **Reference count:** 40
- **Key outcome:** Hybrid framework improves recall from 43% to 63% and precision from 8% to 26% for library subject heading generation

## Executive Summary
This paper presents a hybrid framework that combines embedding-based machine learning models with large language models (LLMs) to improve subject analysis for library cataloging using Library of Congress Subject Headings (LCSH). The framework addresses two key LLM limitations: determining optimal label counts and reducing hallucinations. It uses ML models to predict the number of subject headings and post-edits LLM outputs to ensure they conform to the controlled vocabulary. The approach demonstrates significant improvements in both accuracy and standardization of generated subject headings while maintaining computational efficiency.

## Method Summary
The framework operates through a multi-stage pipeline. First, it trains regression models (Linear Regression, Random Forest, XGBoost) on BERT embeddings of title and abstract to predict the optimal number of LCSH labels. Second, it generates candidate subject headings using Llama-3.1-8B with Chain-of-Thought prompting and explicit quantity constraints based on the predicted count. Third, it post-processes hallucinated terms by embedding them, reducing dimensions via PCA, and finding nearest neighbors in the LCSH vocabulary using FAISS. The system was evaluated on 78,260 book samples from the UNT Library Catalog, achieving improved precision, recall, and F1 scores while reducing the average number of generated terms.

## Key Results
- **Precision improvement:** From 8% to 26% when constraining LLM output based on predicted label counts
- **Recall improvement:** From 43% to 63% using Chain-of-Thought with post-processing
- **Output volume reduction:** Average terms decreased from 14.89 to 3.14 while maintaining quality
- **Hallucination reduction:** Post-processing step successfully replaced invalid terms with semantically similar LCSH terms

## Why This Works (Mechanism)

### Mechanism 1: LLM Output Quantity Constraint
Constraining LLM output quantity via ML-predicted label counts improves precision by reducing over-generation. Lightweight regression models predict the optimal number N of subject headings, which is injected into the LLM prompt as a hard constraint ("Predict exactly N labels"), forcing selective generation rather than unconstrained production. Evidence shows N constraints reduced average terms from 14.89 to 3.14, precision improved from 0.08 to 0.21, and F1 from 0.135 to 0.244.

### Mechanism 2: Post-hoc Semantic Mapping
Post-hoc semantic mapping to a controlled vocabulary reduces hallucinations and improves recall. Generated terms not found in LCSH are embedded, reduced via PCA, then matched to the nearest valid LCSH term using FAISS nearest neighbor search. This corrects hallucinated terms without retraining the LLM. Evidence shows post-processing improved recall from 51%→63% (CoT) and 43%→52% (zero-shot).

### Mechanism 3: Multi-round Chain-of-Thought Prompting
Multi-round Chain-of-Thought prompting increases recall by encouraging diverse aspect coverage. The LLM is prompted iteratively (3 rounds): first N labels, then 2N additional "different aspects," then final补充. Each round conditions on prior outputs, reducing redundancy and broadening topical coverage. Evidence shows CoT V1 achieved recall 0.51 vs. zero-shot 0.43.

## Foundational Learning

- **Multi-Label Classification with Controlled Vocabulary**
  - Why needed: Subject analysis assigns multiple standardized terms from LCSH (~318K vocabulary). Unlike open-ended generation, outputs must belong to a fixed label set.
  - Quick check: Can you explain why standard multi-class classification fails when each document needs 1–6 labels from a 300K+ vocabulary?

- **Embedding-Based Semantic Similarity**
  - Why needed: Both label count prediction and hallucination correction rely on representing text as dense vectors and computing similarity.
  - Quick check: Given two subject headings "Climate change" and "Global warming," would you expect high cosine similarity in a sentence embedding space? Why might this be problematic for disambiguation?

- **Precision-Recall Tradeoff in Generation Tasks**
  - Why needed: The framework explicitly tunes this tradeoff via N constraints (higher N → higher recall, lower precision).
  - Quick check: If a library prioritizes comprehensive discovery (recall) over cataloger efficiency (precision), should you set constraints to N, 2N, or 3N?

## Architecture Onboarding

- **Component map:** Input Layer (Title+Abstract → BERT embeddings) → Label Count Predictor (embeddings → Linear Regression → predicted N) → LLM Generator (Prompt + N constraint → Llama-3.1-8B → candidate LCSH terms) → Post-Processor (Invalid terms → all-mpnet-base-v2 embedding → PCA → FAISS NNS → valid LCSH replacement) → Controlled Vocabulary Store (Pre-computed LCSH embeddings)

- **Critical path:** Embedding quality → count prediction accuracy → LLM constraint effectiveness → LCSH vocabulary completeness → post-processing coverage → prompt design → LLM compliance

- **Design tradeoffs:**
  - **LoRA vs. SFT fine-tuning:** LoRA requires 155MB vs. 55GB, trains 4x faster (3h vs. 12h/epoch), achieves comparable recall (0.52) — prefer LoRA unless full domain adaptation is required.
  - **N vs. 2N vs. 3N constraints:** N maximizes F1 (0.244), 3N maximizes recall (0.41) — choose based on downstream use case (precision-critical cataloging vs. recall-critical discovery).
  - **PCA reduction dimension:** 50 dims chosen for efficiency; higher dims may improve matching accuracy but slow FAISS queries.

- **Failure signatures:**
  - **LLM ignores N constraint:** Output terms >> N → check prompt formatting, model instruction-following capability
  - **Post-processing introduces irrelevant terms:** Hallucinated term semantically distant from valid LCSH → consider thresholding similarity scores
  - **Count predictor systematically off:** Avg difference 1.09 terms → may need domain-specific features beyond title+abstract

- **First 3 experiments:**
  1. **Baseline reproduction:** Run zero-shot Llama-3.1-8B on 100 samples per LCC category; verify ~14.89 terms avg, recall ~0.43, precision ~0.08.
  2. **Label count ablation:** Train Linear Regression on BERT embeddings; compare N, 2N, 3N constraints on precision/recall/F1.
  3. **Post-processing validation:** Generate terms with CoT, run FAISS mapping; manually inspect 20 random hallucination corrections to verify semantic appropriateness.

## Open Questions the Paper Calls Out

- **Human expert evaluation:** How does the relevance and appropriateness of generated subject headings compare when evaluated by human experts versus the automatic metrics used in this study?
- **Proprietary model extension:** Does the hybrid framework's constraint and post-processing pipeline yield similar performance improvements when applied to state-of-the-art proprietary models like GPT-4 or DeepSeek-R1?
- **RAG alternative:** Can Retrieval-Augmented Generation (RAG) utilizing LCSH as an external knowledge base outperform the current semantic mapping approach in reducing hallucinations?
- **Ethical implications:** What is the impact of the automated subject analysis framework on the perpetuation of harmful biases present in the training data or the devaluation of professional cataloging labor?

## Limitations

- **Vocabulary coverage:** The LCSH vocabulary (~318,500 terms) may have coverage gaps for emerging topics or specialized domains, limiting post-processing effectiveness.
- **Metadata reliance:** The framework achieves only modest improvement in predicting optimal label counts, with an average difference of 1.09 terms between predictions and ground truth.
- **Evaluation scope:** The study focuses primarily on quantitative metrics without extensive qualitative analysis of the corrected subject headings' semantic appropriateness.

## Confidence

- **High Confidence:** The core mechanism of constraining LLM output quantity via predicted label counts is well-supported by experimental results (precision improved from 0.08 to 0.21).
- **Medium Confidence:** The post-processing hallucination correction mechanism shows promise but has limited external validation; the semantic similarity assumption needs more rigorous testing.
- **Low Confidence:** The multi-round Chain-of-Thought prompting approach lacks strong external validation; its specific application to controlled vocabulary generation differs from standard CoT reasoning tasks.

## Next Checks

1. **Vocabulary Coverage Analysis:** Evaluate LCSH vocabulary coverage for contemporary topics (AI, climate change, social media) to quantify post-processing limitations when encountering emerging concepts not present in the controlled vocabulary.

2. **Human Evaluation Study:** Conduct expert librarian assessment of 100 randomly selected corrected subject headings to evaluate semantic appropriateness and determine whether nearest-neighbor substitution introduces relevant or misleading terms.

3. **Cross-Domain Generalization:** Test the framework on subject heading systems from different domains (medical MeSH terms, scientific keywords) to assess whether the embedding-based post-processing approach generalizes beyond library science applications.