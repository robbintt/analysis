---
ver: rpa2
title: 'Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case
  Study on Intent and Topic Classification in German Dialects'
arxiv_id: '2510.07890'
source_url: https://arxiv.org/abs/2510.07890
tags:
- data
- german
- whisper
- text
- cascaded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Research on cross-dialectal transfer from a standard to a non-standard
  dialect variety has typically focused on text data. However, dialects are primarily
  spoken, and non-standard spellings are known to cause issues in text processing.
---

# Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects

## Quick Facts
- **arXiv ID**: 2510.07890
- **Source URL**: https://arxiv.org/abs/2510.07890
- **Reference count**: 40
- **Primary result**: Speech-only models outperform cascaded systems on dialect data while text-only models work best on standard data.

## Executive Summary
This paper investigates cross-dialectal transfer from standard German to Bavarian and Swiss German dialects across three experimental setups: text-only models, speech-only models, and cascaded systems (ASR + text classifier). The authors release the first dialectal audio intent classification dataset and conduct experiments on intent and topic classification tasks. They find that speech-only models provide the best results on dialect data while text-only models excel on standard German data. The cascaded systems perform relatively well on dialectal data when the ASR generates normalized, standard-like output, though they lag behind text-only models for standard German.

## Method Summary
The study compares three transfer approaches across German dialects: text-only (fine-tuning text encoders like mBERT on German MASSIVE training data), speech-only (fine-tuning speech encoders like Whisper on German Speech-MASSIVE), and cascaded (using ASR models to transcribe dialect audio, then applying text classifiers). The authors evaluate on two tasks: intent classification (German→Bavarian) and topic classification (German→Swiss German). They use pre-trained encoders with classification heads, training with batch size 32, learning rate {1e-5, 5e-5, 1e-4}, and weight decay 0.01. Evaluation uses classification accuracy as primary metric, with WER/CER for ASR quality.

## Key Results
- Speech-only models consistently outperform cascaded systems on Bavarian dialect intent classification
- Text-only models achieve the highest accuracy on standard German data across both tasks
- Cascaded performance improves significantly when ASR output is normalized to standard German
- Whisper large-v3 ASR model shows best normalization performance for Bavarian dialects
- Speech models demonstrate greater robustness to dialectal variation than text models

## Why This Works (Mechanism)
The superiority of speech models on dialect data likely stems from their ability to process continuous acoustic signals that encode phonetic similarity directly, bypassing the subword tokenization issues that plague text models when handling non-standard spellings. Text models struggle with dialectal orthographic variations, while speech models can map phonetically similar inputs to similar representations regardless of how they're spelled. The cascaded systems' success when ASR produces normalized output suggests that the transfer gap is primarily at the representation level rather than in the classifier itself.

## Foundational Learning

**Dialectal variation and transfer learning**: Understanding how language varieties differ from standard forms and how models trained on one variety transfer to others is crucial for evaluating cross-dialectal performance. Quick check: Review phonological and lexical differences between Bavarian/Swiss German and standard German.

**Subword tokenization**: Text models rely on subword tokenizers (e.g., BPE, WordPiece) that can fragment non-standard spellings into meaningless units. Quick check: Examine how dialectal words are tokenized by mBERT's tokenizer versus their standard German equivalents.

**Acoustic modeling in speech**: Speech models process continuous audio signals through convolutional and transformer layers to extract phonetic features. Quick check: Review how Whisper and XLS-R encode spectrograms into contextual representations.

## Architecture Onboarding

**Component map**: MASSIVE/Speech-MASSIVE (training data) -> Encoder models (BERT, Whisper, etc.) -> Classification head -> Evaluation on xSID/SwissDial (test data)

**Critical path**: Training data → Model fine-tuning → Evaluation metrics → Performance comparison across setups

**Design tradeoffs**: Text models offer efficiency and interpretability but struggle with orthographic variation; speech models handle phonetic similarity better but require more computational resources; cascaded systems balance both but depend heavily on ASR quality.

**Failure signatures**: Speech models diverging during training (learning rate too high), cascaded systems failing on dialect audio (ASR WER >80%), text models underperforming on dialect due to tokenization errors.

**First experiments**: 1) Fine-tune mBERT on German MASSIVE and evaluate on standard German text, 2) Fine-tune Whisper on German Speech-MASSIVE and evaluate on Bavarian audio, 3) Run XLS-R ASR on Bavarian audio and compare cascaded vs. speech-only performance.

## Open Questions the Paper Calls Out

**Open Question 1**: Do the findings regarding the superiority of speech-only models for dialectal data transfer generalize to instruction-tuned Large Language Models (LLMs) and audio LLMs? The authors explicitly exclude these from their experiments and classify this as interesting follow-up work.

**Open Question 2**: Do the observed transfer trends hold for non-Germanic dialects or languages with different relationships between orthography and phonology? The study is restricted to German and its dialects, and the authors hope their work encourages similar datasets in additional languages.

**Open Question 3**: To what extent is the robustness of speech models in dialectal transfer attributable to phonetic similarity rather than the avoidance of subword tokenization errors? The paper hypothesizes about continuous signal processing but doesn't isolate this mechanism from other factors.

## Limitations

- The xSID audio recordings (main evaluation data) are not yet publicly released, preventing exact reproduction
- Relatively small dialect evaluation sets (412 utterances per language) may limit generalizability
- The study focuses only on German and its dialects, limiting claims about other language families

## Confidence

**Primary claims**:
- Speech-only models outperform cascaded systems on dialect data: **Medium** (depends on unreleased audio data)
- Text-only models work best on standard German data: **High** (standard data is available)
- Cascaded performance improves with normalized ASR output: **Medium** (needs validation on Bavarian data)

## Next Checks

1. Verify ASR transcript quality by computing WER/CER on Bavarian test audio once xSID data is released, confirming the >80% WER observations
2. Replicate the cascaded vs. speech-only comparison using the same three ASR models (XLS-R 300M DE, MMS 1B-all, Whisper large-v3) to confirm the normalization effect
3. Test model sensitivity by varying the learning rate schedule and observing impact on dev set performance, particularly for Whisper medium/large variants