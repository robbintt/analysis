---
ver: rpa2
title: 'Demo: A Practical Testbed for Decentralized Federated Learning on Physical
  Edge Devices'
arxiv_id: '2505.08033'
source_url: https://arxiv.org/abs/2505.08033
tags:
- training
- testbed
- devices
- learning
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating decentralized federated
  learning (DFL) on resource-constrained edge devices by building a physical testbed
  using heterogeneous hardware like Raspberry Pi and Jetson Nano. The testbed extends
  the NEBULA DFL platform with real-time power monitoring via JT-TC66C USB multimeters,
  enabling energy-aware experimentation.
---

# Demo: A Practical Testbed for Decentralized Federated Learning on Physical Edge Devices

## Quick Facts
- arXiv ID: 2505.08033
- Source URL: https://arxiv.org/abs/2505.08033
- Reference count: 7
- This work builds a physical testbed for decentralized federated learning using heterogeneous edge devices with real-time power monitoring.

## Executive Summary
This work presents a physical testbed for evaluating decentralized federated learning (DFL) on resource-constrained edge devices. The testbed extends the NEBULA DFL platform to run on heterogeneous hardware including Raspberry Pi and Jetson Nano, with real-time power monitoring via USB multimeters. Experiments demonstrate that fully connected topologies achieve the best model performance (81-82% F1-score) while sparser topologies trade off performance for reduced bandwidth and energy consumption. The physical deployment achieves performance comparable to virtualized baselines, validating its realism for studying DFL scalability and efficiency on real hardware.

## Method Summary
The testbed extends the NEBULA DFL platform to support physical edge devices with heterogeneous capabilities. Each node runs the DFL algorithm with power consumption monitored in real-time using JT-TC66C USB multimeters. The setup supports multiple communication topologies including fully connected, ring, star, and tree structures. Experiments use MNIST and FashionMNIST datasets with 4-8 nodes, measuring both model performance (F1-score) and resource utilization (CPU usage 25-29%, power draw 3-3.5W, energy consumption 1250-1400J per run).

## Key Results
- Fully connected topologies achieve the highest F1-scores (81-82%) for both MNIST and FashionMNIST datasets
- Sparser topologies (ring, star, tree) show reduced performance but consume less bandwidth and energy
- Physical deployments match virtualized baseline performance, validating testbed realism
- CPU usage ranges from 25-29% with power draw of 3-3.5W per device

## Why This Works (Mechanism)
The physical testbed enables realistic evaluation of DFL by exposing the algorithm to real-world constraints like heterogeneous hardware capabilities, network topology limitations, and power consumption patterns. The extension of NEBULA to physical devices allows researchers to validate virtualized findings under actual hardware conditions, while the real-time power monitoring provides empirical data on energy efficiency trade-offs between different communication topologies.

## Foundational Learning
The work demonstrates that DFL topology design involves fundamental trade-offs between model accuracy, communication efficiency, and energy consumption. Fully connected topologies provide optimal performance but at the cost of higher bandwidth and energy usage, while sparser topologies like rings and stars reduce resource consumption at the expense of model accuracy. These relationships between topology structure and learning performance appear to be consistent across both MNIST and FashionMNIST datasets in the tested configuration.

## Architecture Onboarding
The testbed architecture follows a client-server pattern where each physical edge device acts as an independent DFL client. Nodes are configured to run the extended NEBULA DFL framework, with communication topology defined at startup. The JT-TC66C USB multimeters are integrated via software interfaces to collect power metrics in real-time. Each node independently processes local data, communicates with neighbors according to the defined topology, and aggregates model updates, with the entire system coordinated through a centralized experiment controller.

## Open Questions the Paper Calls Out
- How do DFL topology-performance tradeoffs scale when moving from 4-8 nodes to hundreds or thousands of edge devices?
- What are the precise energy costs attributable specifically to DFL computations versus baseline system operations?
- How does the testbed perform under heterogeneous data distributions and non-IID scenarios across devices?
- Can the observed performance and efficiency patterns be generalized to other federated learning algorithms and datasets?

## Limitations
- Small-scale experiments (4-8 nodes) may not capture scalability challenges of larger deployments
- Power measurements aggregate across all system components rather than isolating DFL-specific energy costs
- Fixed 100-iteration training horizon may not represent convergence points for all federated learning scenarios
- Limited to MNIST and FashionMNIST datasets, which may not represent complexity of real-world applications
- Hardware heterogeneity limited to Raspberry Pi and Jetson Nano platforms

## Confidence
High confidence in testbed implementation and topology-performance tradeoffs within tested scope
Medium confidence in energy efficiency claims and scalability projections
Low confidence in generalization to production edge deployments with variable conditions

## Next Checks
1. Scale experiments to 16-32 nodes to evaluate how topology-performance tradeoffs change with network size
2. Isolate DFL computational overhead from baseline system power consumption for precise energy metrics
3. Test with heterogeneous data distributions and non-IID scenarios across devices to assess robustness
4. Evaluate performance with more complex datasets and federated learning algorithms
5. Investigate energy consumption patterns during different phases of DFL training (local computation vs. communication)