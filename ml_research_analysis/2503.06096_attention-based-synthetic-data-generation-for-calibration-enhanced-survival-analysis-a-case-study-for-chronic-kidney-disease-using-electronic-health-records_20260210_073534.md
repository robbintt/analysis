---
ver: rpa2
title: 'Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival
  Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records'
arxiv_id: '2503.06096'
source_url: https://arxiv.org/abs/2503.06096
tags:
- data
- calibration
- synthetic
- mice
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Masked Clinical Modelling (MCM), an attention-based
  framework for generating synthetic healthcare data that enhances calibration in
  survival analysis. Unlike existing methods such as SMOTE and VAEs, MCM preserves
  hazard ratios and improves stratified calibration by balancing realism, utility,
  and practicality.
---

# Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records

## Quick Facts
- **arXiv ID:** 2503.06096
- **Source URL:** https://arxiv.org/abs/2503.06096
- **Reference count:** 40
- **Primary result:** MCM reduced overall calibration loss by 15% and achieved a 9% reduction in mean calibration loss across 10 clinically stratified subgroups

## Executive Summary
This study introduces Masked Clinical Modelling (MCM), an attention-based framework for generating synthetic healthcare data that enhances calibration in survival analysis. Unlike existing methods such as SMOTE and VAEs, MCM preserves hazard ratios and improves stratified calibration by balancing realism, utility, and practicality. Tested on a chronic kidney disease dataset, MCM reduced overall calibration loss by 15% and achieved a 9% reduction in mean calibration loss across 10 clinically stratified subgroups, outperforming 15 alternative methods. MCM supports both standalone dataset synthesis and conditional augmentation, making it adaptable for diverse healthcare research scenarios while ensuring equitable representation across patient populations.

## Method Summary
MCM is an attention-based generative model that learns feature dependencies through masked reconstruction. The architecture consists of two blocks, each containing an attention filter followed by an MLP. During training, features are randomly masked (10-95% ratio) and the model learns to reconstruct them using unmasked context. The model is trained on preprocessed EHR data (Box-Cox transformed numerics, normalized to [0,1]) using MSE loss over 500 epochs with Adam optimizer. For conditional augmentation, the pre-trained model generates synthetic patients from stratified subsets to improve calibration in underrepresented groups. The approach is evaluated using 5x2 cross-validation, measuring calibration slope and error across 10 clinical subgroups.

## Key Results
- Reduced overall calibration loss by 15% compared to baseline methods
- Achieved 9% mean reduction in calibration loss across 10 clinically stratified subgroups
- Preserved hazard ratios while improving stratified calibration performance
- Outperformed 15 alternative synthetic data generation methods including SMOTE, VAEs, and diffusion models

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Feature Dependency Learning
MCM learns contextual relationships between clinical covariates through attention weights, enabling accurate reconstruction of masked features based on unmasked context. Linear attention scores are computed for all features, then masked positions are set to -∞ before softmax normalization, ensuring reconstruction depends only on observed context. The attention-weighted features feed into MLP blocks for reconstruction. Core assumption: Clinical features have learnable inter-dependencies that remain stable across patients with similar characteristics.

### Mechanism 2: Dynamic Masking Ratio for Robust Reconstruction
Variable masking ratios (10-95%) during training force the model to learn both local and global feature dependencies, enabling flexible generation without curriculum learning. Each batch samples a masking ratio uniformly, ensuring the model reconstructs features from varying amounts of context. This prevents over-reliance on any subset of features. Core assumption: The underlying data distribution can be recovered from partial observations across varying sparsity levels.

### Mechanism 3: Conditional Augmentation for Stratified Calibration
Pre-trained MCM can generate subgroup-specific synthetic patients without retraining, improving calibration for underrepresented strata by augmenting training data conditionally. During cross-validation, training data is filtered by stratification condition, synthetic patients are generated, then appended to training set before fitting CoxPH. The 5-iteration loop provides error bars. Core assumption: Synthetic patients generated from stratified subsets preserve the conditional distributions of outcomes relative to covariates.

## Foundational Learning

- **Concept:** Cox Proportional Hazards Model
  - **Why needed here:** The entire evaluation framework relies on fitting CoxPH models and comparing hazard ratios and calibration between real and synthetic data.
  - **Quick check question:** If the proportional hazards assumption is violated in the real data, would MCM-generated synthetic data preserve or mask this violation?

- **Concept:** Calibration Slope and Calibration Error
  - **Why needed here:** The paper's primary metric is calibration loss (|1 - S|), where S is the slope relating predicted risk to observed outcomes.
  - **Quick check question:** A calibration slope of 0.77 indicates the model is overconfident or underconfident in its risk predictions?

- **Concept:** Masked Language Modelling (BERT-style)
  - **Why needed here:** MCM adapts the MLM paradigm from NLP to tabular clinical data.
  - **Quick check question:** Why does MLM-style training avoid the posterior collapse issues seen in VAEs for this application?

## Architecture Onboarding

- **Component map:** Input (N×D) → [Block 1: Masked Attention → MLP → LayerNorm + ReLU] → [Block 2: Residual Attention → MLP → Sigmoid] → Output (N×D)

- **Critical path:**
  1. Preprocessing: Box-Cox transform numerics, binarize categorical → normalize all to [0,1]
  2. Training loop: Sample mask ratio → apply mask → forward pass → MSE loss on masked positions only
  3. Generation: Start with copy of preprocessed data → mask 50% per instance → reconstruct → postprocess

- **Design tradeoffs:**
  - Single-head attention (not multi-head): Simpler, but may miss multi-faceted feature relationships
  - Hidden dim = 64: Lightweight but may underfit complex datasets
  - No auxiliary losses (correlation, projection): Relies entirely on reconstruction; may not enforce global distributional properties
  - Masking during generation: 50% masking ratio trades novelty vs. fidelity

- **Failure signatures:**
  - Posterior collapse equivalent: If attention weights become uniform across features, reconstructions will be population-averaged rather than patient-specific
  - Calibration degradation at short durations: Paper reports worse calibration at 25th percentile with MCM (slope 1.12 vs. ideal 1.0)
  - MICE integration instability: Appendix F.3 shows MCM+MICE combinations are "highly unstable and less reliable overall"

- **First 3 experiments:**
  1. **Sanity check:** Train MCM on a small subset (N=100), generate synthetic data, and verify that KM curves and hazard ratios match ground truth within confidence intervals.
  2. **Ablation on masking ratio:** Compare generation quality (distributional fidelity via Kolmogorov-Smirnov tests, calibration loss) at generation-time masking ratios of 25%, 50%, 75%.
  3. **Stratified calibration stress test:** Apply MCM augmentation to the most imbalanced subgroup (CVD at 13.85%) and measure whether calibration improves or degrades compared to no augmentation.

## Open Questions the Paper Calls Out
None

## Limitations
- Single dataset (491 patients) limits generalizability to larger or more diverse clinical populations
- Absence of multi-head attention and small hidden dimension (H=64) may limit performance on complex datasets
- Dynamic masking strategy could fail at extreme ratios (>95%), leading to mode-averaged reconstructions that lose patient-specific patterns

## Confidence

**High Confidence:** Core attention-based reconstruction mechanism is well-specified and theoretically sound; standard evaluation metrics; 15% calibration loss reduction is robust within studied dataset.

**Medium Confidence:** Conditional augmentation approach is supported but performance on severely underrepresented subgroups remains uncertain; hazard ratio preservation demonstrated but needs cross-dataset validation.

**Low Confidence:** Claim of outperforming 15 methods lacks detailed comparative analysis; MCM+MICE instability noted without clear guidance on appropriate usage.

## Next Checks

1. **Cross-dataset validation:** Test MCM on a larger, more diverse clinical dataset (N > 1000) to verify 15% calibration loss reduction generalizes beyond the CKD cohort.

2. **Extreme subgroup analysis:** Systematically evaluate MCM's performance on most imbalanced strata (e.g., rare comorbidities at 13.85%) to quantify calibration improvement bounds and identify conditions where synthetic augmentation may amplify sampling noise.

3. **Architecture scaling study:** Compare MCM performance with and without multi-head attention and with increased hidden dimensions (H=128, H=256) to determine if current architecture is near-optimal.