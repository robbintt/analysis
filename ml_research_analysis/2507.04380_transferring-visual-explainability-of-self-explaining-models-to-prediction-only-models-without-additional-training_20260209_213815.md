---
ver: rpa2
title: Transferring Visual Explainability of Self-Explaining Models to Prediction-Only
  Models without Additional Training
arxiv_id: '2507.04380'
source_url: https://arxiv.org/abs/2507.04380
tags:
- explainability
- target
- classification
- explanation
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes transferring visual explainability from self-explaining
  models to prediction-only models without additional training, using task arithmetic.
  The core idea is to define an "explainability vector" as the difference between
  parameters of a self-explaining model and its prediction-only counterpart, then
  transfer this vector to prediction-only models in target domains.
---

# Transferring Visual Explainability of Self-Explaining Models to Prediction-Only Models without Additional Training

## Quick Facts
- arXiv ID: 2507.04380
- Source URL: https://arxiv.org/abs/2507.04380
- Reference count: 40
- Key outcome: Transferring visual explainability from self-explaining models to prediction-only models without additional training, improving E-RMSE by ~9% while maintaining 88.6% accuracy on 10 target datasets

## Executive Summary
This paper proposes a novel method to transfer visual explainability from self-explaining models to prediction-only models without requiring additional training on the target domain. The core innovation is defining an "explainability vector" as the parameter difference between a self-explaining model and its prediction-only counterpart, then applying this vector to prediction-only models in target domains. Experiments demonstrate that explainability learned on ImageNet+X transfers effectively to 10 diverse target datasets, achieving explanation quality within 9% of lower-bound while maintaining classification accuracy within 4% of upper-bound. The method produces explanations in a single forward pass, outperforming Kernel SHAP with 300+ perturbations on most metrics.

## Method Summary
The method uses task arithmetic to transfer explainability capability across domains. First, train two models on the source domain: one with classification only (α=1) and one with classification plus explanation supervision (α<1). The explainability vector is computed as the parameter difference between these models. This vector is then added to prediction-only models trained on target domains to create self-explaining models without additional training. The approach leverages ViT architectures with CLIP-based text embeddings for the classification head, allowing explainability to be encoded purely in the feature extractor parameters. The transferred models produce patch-level attributions in a single forward pass that match or exceed the quality of computationally expensive perturbation-based methods.

## Key Results
- E-RMSE reduced by ~9% compared to baseline prediction-only models across 10 target datasets
- Classification accuracy maintained within 4% of upper-bound (88.6% vs 90.9%)
- Single-forward-pass explanations outperform Kernel SHAP with 300+ perturbations on most metrics
- Explainability vectors transfer effectively from ImageNet+X to diverse domains including Cars, GTSRB, SVHN, and CIFAR100
- Transferred models produce high-quality patch attributions without requiring expensive perturbation-based computation

## Why This Works (Mechanism)

### Mechanism 1: Explainability Vector as Isolated Capability Representation
The parameter difference between self-explaining and prediction-only models isolates explanation capability as a transferable vector in parameter space. This works because the explanation supervision (MSE between predicted and ground-truth patch attributions) creates a distinct capability that's separable from domain-specific classification knowledge in ViT parameters.

### Mechanism 2: Task Analogy via Parameter Arithmetic
Adding the source-domain explainability vector to target-domain prediction parameters creates a self-explaining model for the target domain. The task analogy "prediction-only is to prediction-with-explanation in source domain as prediction-only is to prediction-with-explanation in target domain" enables cross-domain transfer through parameter arithmetic.

### Mechanism 3: Shared Domain-Specific Head for Zero-Shot Attribution
The domain-specific head using fixed CLIP text embeddings can produce both classification logits and patch attributions because it projects both [CLS] and patch embeddings through the same fixed matrix. This enables attribution generation without requiring additional training or specialized architectures.

## Foundational Learning

- **Task Arithmetic**: The mathematical foundation enabling parameter-space editing. Understanding equation 1 (θ = θbase + λΣτm) and equation 2 (task analogy) is essential to grasp how capabilities transfer. Quick check: Given task vectors for tasks A, B, C, how would you construct parameters for task D assuming A:B :: C:D?

- **Vision Transformer (ViT) Patch Embeddings**: The method operates on M=49 patches per image (32×32 patches for 224×224 images). Understanding how ViT produces [CLS] and patch tokens is required to interpret the attribution mechanism. Quick check: How does applying the same linear projection to [CLS] vs. patch tokens yield classification vs. attribution outputs?

- **Shapley Values / Patch Attribution**: The ground-truth supervision uses Kernel SHAP (500 perturbations per image). Understanding what these values represent (patch contribution to classification) is needed to evaluate explanation quality metrics (E-RMSE, IoU@K, Rank Correlation). Quick check: Why does the proposed single-forward-pass method outperform Kernel SHAP with 300+ perturbations on most metrics?

## Architecture Onboarding

- **Component map**: Feature Extractor fθ (ViT) -> Domain-Specific Head gW -> Classification/Attribution Outputs. The feature extractor produces M+1 feature vectors (one [CLS], M patches). The domain-specific head is a fixed linear projection using CLIP text embeddings.

- **Critical path**: 1) Train θS_ft on source with classification only (α=1) 2) Train θS_ft* on source with classification + explanation (α=0.8) 3) Compute explainability vector: τS* = θS_ft* - θS_ft 4) Apply transfer: θ̃T_ft* = θbase + λ₁(θT_ft - θbase) + λ₂τS*

- **Design tradeoffs**: λ₂ scaling improves explanation but risks accuracy drop (tune on validation set). α balance controls classification vs. explanation during source training. Source dataset choice affects transfer quality (larger/diverse sources transfer better).

- **Failure signatures**: Large accuracy drop suggests τS* aligned with high-eigenvalue eigenvectors of HT (reduce λ₂). Poor attribution quality indicates source-target domain mismatch. Consistent with observations: MNIST→Cars transfer failed due to dissimilar domains.

- **First 3 experiments**: 1) Verify explainability vector isolation by training θS_ft and θS_ft* on small source dataset 2) Cross-domain transfer sanity check using ImageNet+X to one target dataset 3) Domain similarity analysis by correlating cosine similarity of τS_ft and τT_ft with E-RMSE

## Open Questions the Paper Calls Out

1. **Universal explainability vector construction**: Can a universal explainability vector be constructed using datasets significantly larger than ImageNet to enable zero-shot transfer to diverse downstream tasks? The paper plans to explore constructing such vectors using larger-scale datasets.

2. **Adaptation for non-VLM architectures**: Can the explainability transfer method be adapted for architectures that do not rely on fixed vision-language model embeddings, such as standard ResNets or randomly initialized Transformers? This requires modification since the method depends on CLIP's fixed text embeddings.

3. **Negative transfer prediction and mitigation**: How can negative transfer (accuracy degradation) be automatically predicted and mitigated when applying explainability vectors to target domains that are semantically distant from the source? The paper identifies the risk but relies on tuning λ₂, which may require validation sets.

## Limitations

- **Architectural specificity**: Method's effectiveness tied to ViT's patch embedding structure and CLIP's fixed text embeddings, not validated beyond tested datasets
- **Hyperparameter sensitivity**: Critical training hyperparameters unspecified; λ₂ scaling factor sensitivity not systematically explored
- **Ground truth attribution quality**: Method depends on Kernel SHAP as ground truth without addressing potential biases or limitations

## Confidence

**High confidence**: Core mathematical framework for parameter arithmetic and explainability vector transfer is well-defined and internally consistent. E-RMSE improvement of ~9% is robust.

**Medium confidence**: Claim that single-forward-pass explanations outperform Kernel SHAP with 300+ perturbations is supported but computational efficiency needs more detailed analysis.

**Low confidence**: Theoretical interpretation of τS* as optimal perturbation lacks empirical validation. Claim that explainability is "disentangled" from domain-specific features is asserted rather than proven.

## Next Checks

1. **Cross-domain similarity analysis**: For each target dataset, compute cosine similarity between τS_ft and τT_ft, then correlate with E-RMSE improvements to validate whether r=-0.32 correlation holds across all source-target pairs.

2. **Hyperparameter sensitivity sweep**: Systematically vary λ₂ ∈ {0.4, 0.6, 0.8, 1.0, 1.2} on a subset of target datasets to determine stability of accuracy-explanation tradeoff curves.

3. **Alternative architecture validation**: Test explainability transfer mechanism on non-ViT architecture (e.g., ResNet) with patch-based attention mechanisms to verify whether effectiveness is architecture-specific or generalizes to other transformer-based models.