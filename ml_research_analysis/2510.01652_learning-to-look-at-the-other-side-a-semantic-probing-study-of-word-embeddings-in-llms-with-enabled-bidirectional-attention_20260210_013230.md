---
ver: rpa2
title: 'Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings
  in LLMs with Enabled Bidirectional Attention'
arxiv_id: '2510.01652'
source_url: https://arxiv.org/abs/2510.01652
tags:
- mntp
- task
- word
- tasks
- verb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how enabling bidirectional attention in
  autoregressive language models affects word-level semantic representations. The
  authors test Llama models with and without bidirectional attention, along with contrastive
  learning techniques, on five lexical semantic probing tasks.
---

# Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention

## Quick Facts
- arXiv ID: 2510.01652
- Source URL: https://arxiv.org/abs/2510.01652
- Reference count: 32
- Enabling bidirectional attention in autoregressive models improves subsequent context representation but weakens previous context utilization, while contrastive learning helps maintain both abilities.

## Executive Summary
This paper investigates how enabling bidirectional attention in autoregressive language models affects word-level semantic representations. The authors test Llama models with and without bidirectional attention, along with contrastive learning techniques, on five lexical semantic probing tasks. Results show that bidirectional attention improves representation of subsequent context but weakens previous context utilization, while contrastive learning helps maintain both abilities. Interestingly, bidirectional attention increases anisotropy across all layers, contradicting expectations that it would reduce this issue. Despite this, supervised contrastive learning generally produces the best performance across tasks. The study demonstrates that autoregressive models can achieve performance comparable to bidirectional models in semantic probing tasks when properly trained, challenging the assumption that bidirectional attention is always necessary for word-level semantic understanding.

## Method Summary
The study employs Llama language models with and without bidirectional attention enabled, comparing their performance on five lexical semantic probing tasks. The authors implement supervised contrastive learning to enhance semantic representations and analyze how bidirectional attention affects context utilization across model layers. They measure performance improvements and changes in anisotropy, examining the trade-offs between bidirectional and unidirectional attention mechanisms in autoregressive models.

## Key Results
- Bidirectional attention improves subsequent context representation but weakens previous context utilization
- Contrastive learning techniques help maintain both previous and subsequent context abilities
- Bidirectional attention increases anisotropy across all layers, contrary to expectations
- Supervised contrastive learning produces the best overall performance across semantic probing tasks

## Why This Works (Mechanism)
Enabling bidirectional attention allows the model to access both left and right context during token prediction, fundamentally changing how information flows through the transformer architecture. This modification affects the attention mechanism's ability to capture semantic relationships, particularly for subsequent context words. Contrastive learning helps by creating more discriminative representations that can better preserve semantic information from both directions, even when bidirectional attention is limited.

## Foundational Learning
- **Bidirectional vs Unidirectional Attention**: Understanding the difference between accessing context from both directions versus only future tokens is crucial for interpreting the study's findings about context utilization trade-offs.
- **Anisotropy in Word Embeddings**: Knowledge of how word vectors cluster in high-dimensional space helps explain why increased anisotropy is significant and counterintuitive in bidirectional models.
- **Contrastive Learning in NLP**: Familiarity with contrastive objectives and their role in creating discriminative representations is essential for understanding how they improve semantic probing performance.
- **Lexical Semantic Probing**: Understanding the specific tasks used to evaluate semantic representations provides context for interpreting the performance metrics and their implications.

## Architecture Onboarding

**Component Map:**
Llama Encoder -> Bidirectional Attention Layer -> Contrastive Learning Module -> Semantic Probing Tasks

**Critical Path:**
Input tokens → Embedding layer → Transformer blocks with bidirectional attention → Contrastive loss computation → Output representations → Probing task evaluation

**Design Tradeoffs:**
The study balances the benefits of bidirectional context access against the increased computational complexity and potential degradation of previous context encoding. The choice of contrastive learning techniques represents a tradeoff between maintaining semantic richness and avoiding over-specialization to specific tasks.

**Failure Signatures:**
Increased anisotropy suggests potential loss of semantic diversity in representations. Weakened previous context utilization indicates that bidirectional attention may create an imbalance in how the model processes historical versus future information.

**First 3 Experiments:**
1. Compare baseline Llama performance with bidirectional attention disabled against the same model with bidirectional attention enabled
2. Evaluate the impact of different contrastive learning objectives on semantic probing task performance
3. Analyze layer-wise changes in anisotropy and semantic representation quality as bidirectional attention is enabled

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to five specific lexical semantic probing tasks
- Findings may not generalize to other transformer architectures or training regimes
- Focus on word-level semantic representations may miss higher-level linguistic abstractions

## Confidence

**Major Claim Clusters and Confidence:**
- **Bidirectional Attention Effects** (Medium Confidence): Interpretation of effects remains somewhat ambiguous
- **Contrastive Learning Benefits** (High Confidence): Consistently supported by experimental results
- **Anisotropy Findings** (Medium Confidence): Contradicts expectations but needs further investigation

## Next Checks
1. Test whether observed effects replicate across different LLM architectures (GPT, OPT, Mistral) to establish generalizability
2. Extend semantic probing tasks to multilingual settings to evaluate cross-linguistic validity
3. Implement layer-wise and head-wise analysis of attention patterns to better understand context encoding mechanisms