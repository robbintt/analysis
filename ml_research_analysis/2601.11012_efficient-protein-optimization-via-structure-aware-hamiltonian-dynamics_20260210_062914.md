---
ver: rpa2
title: Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics
arxiv_id: '2601.11012'
source_url: https://arxiv.org/abs/2601.11012
tags:
- protein
- fitness
- sequence
- structure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HADES, a Bayesian optimization method for protein
  engineering that combines Hamiltonian dynamics with structure-aware modeling. The
  key idea is to use Hamiltonian Monte Carlo sampling in a continuous sequence space
  while leveraging structural information as prior knowledge to guide exploration.
---

# Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics

## Quick Facts
- **arXiv ID:** 2601.11012
- **Source URL:** https://arxiv.org/abs/2601.11012
- **Reference count:** 12
- **Primary result:** HADES outperforms state-of-the-art baselines on GB1 and PhoQ datasets, achieving perfect (1.00±0.00) and near-perfect (0.80±0.25) maximum fitness scores respectively.

## Executive Summary
This paper introduces HADES, a Bayesian optimization method for protein engineering that leverages Hamiltonian dynamics and structure-aware modeling. The method uses a two-stage encoder-decoder framework to learn structure-function relationships, with a structure decoder predicting RMSD relative to wild-type before a fitness decoder fine-tunes with frozen encoder. HADES employs Hamiltonian Monte Carlo sampling in a continuous sequence space, using structure information as prior knowledge to guide exploration and uncertainty estimation to balance exploration and exploitation.

## Method Summary
HADES is a Bayesian optimization framework for protein directed evolution that treats the problem as black-box optimization. It uses a two-stage encoder-decoder to first predict structural perturbations (RMSD) relative to wild-type, then fine-tune on fitness labels with frozen encoder. The method employs Hamiltonian Monte Carlo sampling with virtual barriers to maintain valid probability vectors and Metropolis-Hastings correction for discretization error. Structure information from ESMFold serves as prior knowledge to guide exploration, while ensemble uncertainty estimation balances exploration and exploitation via UCB acquisition.

## Key Results
- On GB1 dataset: HADES achieves maximum fitness scores of 1.00±0.00 compared to 0.93±0.14 for second-best method
- On PhoQ dataset: HADES achieves maximum fitness scores of 0.80±0.25 compared to 0.72±0.24 for second-best method
- HADES demonstrates superior functional diversity (fDiv) scores while maintaining stable performance across different query sizes

## Why This Works (Mechanism)

### Mechanism 1
Structure-aware pre-training creates a smoother fitness landscape approximation, reducing the likelihood of getting trapped in local optima caused by epistasis. The two-stage encoder-decoder first learns to predict structural perturbations (RMSD) relative to the wild type before fine-tuning on sparse fitness labels. By freezing the structure-aware encoder for the fitness prediction stage, the model imposes a structural constraint on the gradient, effectively filtering out high-fitness predictions that would require physically implausible structural deviations. This works because the distribution of structural deviations among mutants serves as a valid prior for functional viability.

### Mechanism 2
Hamiltonian dynamics enables efficient long-range exploration of the sequence space by converting the discrete optimization problem into a continuous dynamics simulation with momentum. By treating sequence probability vectors as position q and introducing auxiliary momentum p, the system simulates a "particle" moving on the energy surface defined by the surrogate model. The momentum allows the sampler to traverse low-probability "valleys" that would stop a standard random walk or greedy ascent, facilitating jumps to distant, diverse sequence candidates.

### Mechanism 3
A "virtual barrier" and Metropolis-Hastings (MH) correction bridge the gap between continuous optimization dynamics and discrete biological sequences. To maintain valid probability vectors (0 ≤ q ≤ 1), the dynamics simulate elastic collisions (virtual barriers) at boundaries. To correct for the error introduced by discretizing the continuous vector to a one-hot sequence, an MH step accepts or rejects the discrete proposal based on the energy difference, ensuring the final samples approximate the target distribution.

## Foundational Learning

**Bayesian Optimization (BO)**: HADES is fundamentally a BO loop. Understanding the trade-off between "exploration" (uncertainty) and "exploitation" (predicted high fitness) is critical to interpret the Upper Confidence Bound (UCB) selection logic. Quick check: If the surrogate model is overconfident but wrong, how does the UCB acquisition function fail?

**Markov Chain Monte Carlo (MCMC) & Hamiltonian Monte Carlo (HMC)**: The core sampling engine is HMC. Understanding the leapfrog integrator and the role of momentum is critical to debugging the proposal generation mechanism. Quick check: Why does HMC generally achieve higher acceptance rates in high-dimensional spaces compared to standard Random Walk Metropolis?

**Sequence-Structure Mapping (RMSD)**: The method relies on ESMFold and RMSD as a proxy for structural viability. Understanding RMSD is necessary to interpret the "Structure Decoder" outputs and the "fDiv" metric. Quick check: Does a low RMSD between a mutant and wild type always guarantee high fitness? (Consider active site mutations).

## Architecture Onboarding

**Component map**: Input (One-hot Sequence) -> Encoder (3-layer Self-Attention) -> Stage 1 Decoder (Predicts RMSD) -> Freeze Encoder -> Stage 2 Decoder (Predicts Fitness) -> Sampler (HMC loop) -> Acquisition (UCB Score)

**Critical path**:
1. Pre-train: Train Encoder + Structure Decoder on ESMFold-generated RMSD labels
2. Freeze: Lock Encoder weights
3. Fine-tune: Train Fitness Decoder on available wet-lab fitness data
4. Sample: Run HMC using the Fitness Decoder's energy function to generate continuous candidates
5. Discretize & Select: Convert to sequences, filter via MH, select Top-K via UCB

**Design tradeoffs**:
- Continuous vs. Discrete: The method smooths the landscape via continuous relaxation but risks high discretization error if step sizes are too aggressive
- Encoder Freezing: Freezing the encoder after Stage 1 prevents overfitting to sparse fitness data but limits the model's ability to learn fitness features not captured by structural perturbation

**Failure signatures**:
- Low Acceptance Rate: HMC proposals are consistently rejected. Diagnosis: Step size ε too high or surrogate landscape too jagged
- Mode Collapse: High fitness scores but low diversity (low fDiv). Diagnosis: Uncertainty estimation (ensemble) is underestimating variance; UCB is strictly exploiting
- Structure Over-constraint: Performance lags baselines on tasks where function requires large structural changes. Diagnosis: The structure prior (Stage 1) is too dominant

**First 3 experiments**:
1. Surrogate Validation: Train the two-stage model on a subset of GB1/PhoQ and plot the correlation between predicted fitness and ground truth to verify the structure-aware prior aids fitness prediction
2. Sampler Dynamics Check: Visualize the trajectory of the continuous variable q during HMC to verify virtual barrier triggers and monitor MH acceptance rate
3. Ablation on Prior: Compare HADES against a version without the Structure Decoder to isolate the gain provided by the structure-aware prior on maximum fitness score

## Open Questions the Paper Calls Out

**Open Question 1**: Can the HADES framework be effectively adapted for multi-objective protein optimization? The authors state future efforts will focus on designing proteins that optimize multiple objectives. The current implementation optimizes a single fitness score and lacks mechanisms for handling trade-offs between competing properties.

**Open Question 2**: Does enhancing the structural modeling component improve generalization to proteins where ESMFold exhibits high error? The conclusion proposes enhancing protein structure modeling to improve generalization ability, while the ablation study notes the structure module's modest impact was possibly due to generalization error of ESMFold.

**Open Question 3**: Do the protein sequences proposed by HADES retain high fitness and structural stability in wet-lab experiments? The paper reports results exclusively from in-silico evaluations using static datasets as the black-box oracle instead of physical experiments.

## Limitations
- The method's reliance on ESMFold for structure prediction may limit applicability to proteins where accurate structural models are unavailable
- The discretization step from continuous probability vectors to discrete sequences introduces an approximation that may accumulate error in highly non-smooth fitness landscapes
- Exact ensemble size and UCB coefficient parameters are not specified, making precise reproduction difficult

## Confidence

**High Confidence**: The core mechanism of using Hamiltonian dynamics for efficient exploration and the two-stage structure-aware training framework are well-specified and theoretically grounded.

**Medium Confidence**: The experimental results show strong performance on GB1 and PhoQ, but the lack of specification for ensemble size and UCB parameters creates uncertainty about the exact selection behavior.

**Low Confidence**: The claim that structure-aware pre-training always creates a smoother fitness landscape is not fully validated; it may fail for fitness functions that are decorrelated from structural similarity.

## Next Checks

1. **Surrogate Model Validation**: Train the two-stage model on a subset of GB1/PhoQ data and compare the correlation between predicted fitness and ground truth against a model without the structure-aware prior to isolate the benefit.

2. **Sampler Dynamics Visualization**: Visualize the trajectory of the continuous variable `q` during HMC to verify that the virtual barrier is correctly constraining the search space and that the Metropolis-Hastings acceptance rate is stable.

3. **Prior Ablation Study**: Compare HADES against a version where the structure decoder is initialized randomly (or not pre-trained) to quantify the exact contribution of the structure-aware prior to the final maximum fitness score.