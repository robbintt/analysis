---
ver: rpa2
title: Zero-Shot Reinforcement Learning Under Partial Observability
arxiv_id: '2506.15446'
source_url: https://arxiv.org/abs/2506.15446
tags:
- learning
- arxiv
- reinforcement
- zero-shot
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zero-shot reinforcement learning (RL) under
  partial observability, where the Markov state is only partially observed via noisy
  or incomplete observations. Standard zero-shot RL methods fail under partial observability
  due to state misidentification and task misidentification.
---

# Zero-Shot Reinforcement Learning Under Partial Observability

## Quick Facts
- arXiv ID: 2506.15446
- Source URL: https://arxiv.org/abs/2506.15446
- Reference count: 17
- Primary result: Memory-based behavior foundation models (FB-M) with GRU memory significantly outperform memory-free baselines in zero-shot RL under partial observability.

## Executive Summary
This paper addresses zero-shot reinforcement learning under partial observability, where Markov states are only partially observed through noisy or incomplete observations. Standard zero-shot RL methods fail because they cannot reliably identify the true state or task when observations are insufficient. The authors propose memory-based behavior foundation models that condition the forward model, backward model, and policy on trajectories of observation-action pairs rather than single states. Their approach uses GRU memory models to compress trajectory histories into hidden states that approximate belief states, enabling better generalization across unseen tasks.

## Method Summary
The authors propose memory-based behavior foundation models (FB-M) that extend standard forward-backward models with GRU memory components. The architecture processes L-length trajectories of observation-action pairs through separate GRUs for the forward model F, backward model B, and policy π. Each GRU compresses the trajectory history into a hidden state that conditions the respective component instead of single observations. The method is evaluated on ExORL benchmarks with partially observed states (noisy and flickering) and dynamics changes (interpolation and extrapolation). Training involves 1M steps with Adam optimizer, and task inference computes embeddings via weighted averages of backward model outputs.

## Key Results
- FB-M achieves 511±85 normalized score on Walker with flickering states vs. 76±32 for standard FB
- GRU memory models outperform transformers and S4, which fail to train stably at scale
- FB-M approaches oracle performance in some settings, with 478±19 vs. 555±89 on Walker extrapolation
- Memory-free methods fail completely on partially observed tasks while FB-M maintains reasonable performance

## Why This Works (Mechanism)

### Mechanism 1: State Misidentification
Conditioning forward models on partial observations causes unreliable long-run dynamics prediction, leading to inaccurate Q-functions and suboptimal policies.

### Mechanism 2: Task Misidentification
Backward models receiving partial observations cannot reliably encode task-relevant state features, resulting in incorrect task embeddings when rewards depend on unobserved state components.

### Mechanism 3: Memory-Based Recovery
GRU memory models compress trajectory histories into hidden states that approximate the Markov state, mitigating both state and task misidentification failures.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: The entire problem formulation relies on POMDPs where observations only partially reveal the true state. Quick check: Can you determine the true state from only the current observation? If not, what additional information helps?
- **Successor Measures and Forward-Backward Representations**: FB models approximate successor measures, which capture cumulative discounted state visitation. Quick check: How does a successor measure differ from a Q-function?
- **Gated Recurrent Units (GRUs)**: Understanding GRU gating mechanisms explains their stability advantages over transformers and S4. Quick check: How does a GRU's update gate balance new input versus previous state?

## Architecture Onboarding

- **Component map**: Preprocessor MLP -> GRU memory (f_F, f_B, f_π) -> Forward/Backward/Policy MLPs
- **Critical path**: Pre-train F, B, π on 1M steps → Compute task embeddings via backward model → Evaluate policy with learned embedding and GRU state
- **Design tradeoffs**: GRUs chosen over transformers/S4 for stability; context length L=32 balances performance and compute; separate memory models per component
- **Failure signatures**: Training collapse with non-GRU memory when both F and B use alternative architectures; performance degradation with observation stacking
- **First 3 experiments**: (1) Ablate memory placement across components on Walker-flickering, (2) Sweep context length L on Quadruped-noisy, (3) Test transformer/S4 memory alternatives on single domain

## Open Questions the Paper Calls Out

1. **Why do transformer and S4 models cause training collapse when used jointly?** The authors note this as important future work but don't identify the specific mechanism causing instability versus GRUs.

2. **How does FB-M performance depend on pre-training dataset homogeneity?** The current results use exploratory RND datasets, leaving robustness to narrow, real-world datasets untested.

3. **Why does FB-M occasionally outperform the oracle in dynamics extrapolation?** The authors observe this counter-intuitive result but only hypothesize it relates to dataset expressivity without verification.

## Limitations

- Memory models may not scale to more complex environments requiring richer representations
- Performance depends heavily on the coverage and diversity of pre-training datasets
- Inconsistent results on dynamics extrapolation suggest limited reliability for out-of-distribution changes

## Confidence

- **High Confidence**: Failure modes of state and task misidentification are well-supported theoretically and empirically
- **Medium Confidence**: Trajectory memory approximation of belief states works empirically but lacks formal guarantees
- **Low Confidence**: Extrapolation performance is inconsistent across domains, indicating unreliable generalization

## Next Checks

1. Systematically test which components (F, B, π) require memory by training variants with memory on only one or two components
2. Evaluate performance across different context lengths L to identify optimal tradeoff between computational cost and performance
3. Investigate why transformers and S4 fail to train stably when both F and B use non-GRU memory through gradient analysis