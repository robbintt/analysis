---
ver: rpa2
title: 'SafeVid: Toward Safety Aligned Video Large Multimodal Models'
arxiv_id: '2505.11926'
source_url: https://arxiv.org/abs/2505.11926
tags:
- safety
- video
- alignment
- arxiv
- safevid-350k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeVid addresses the safety generalization gap in video large
  multimodal models (VLMMs), where textual safety alignments fail to transfer to dynamic
  video contexts. The core method leverages detailed textual video descriptions as
  an interpretive bridge, enabling LLM-based rule-driven safety reasoning.
---

# SafeVid: Toward Safety Aligned Video Large Multimodal Models

## Quick Facts
- arXiv ID: 2505.11926
- Source URL: https://arxiv.org/abs/2505.11926
- Reference count: 40
- One-line primary result: SafeVid achieves up to 42.39% improvement in safety rate for VLMMs through video-specific textual description bridging and DPO alignment

## Executive Summary
SafeVid addresses the safety generalization gap in video large multimodal models (VLMMs), where textual safety alignments fail to transfer to dynamic video contexts. The core method leverages detailed textual video descriptions as an interpretive bridge, enabling LLM-based rule-driven safety reasoning. This is implemented through a closed-loop system: (1) generation of SafeVid-350K, a 350,000-pair video-specific safety preference dataset synthesized using detailed descriptions and a structured safety taxonomy; (2) targeted alignment of VLMMs using Direct Preference Optimization (DPO) on SafeVid-350K; and (3) comprehensive evaluation via SafeVidBench, a benchmark suite with 2,760 adversarial queries. Alignment with SafeVid-350K significantly enhances VLMM safety, with models like LLaVA-NeXT-Video demonstrating substantial improvements (e.g., up to 42.39% on SafeVidBench-Base). SafeVid demonstrates that leveraging textual descriptions as a conduit for safety reasoning markedly improves the safety alignment of VLMMs.

## Method Summary
SafeVid employs a three-stage approach to bridge the modality gap between textual safety alignment and video content. First, it curates a video corpus from InternVid-10M-FLT, filters for diverse scene categories, and generates high-fidelity textual descriptions using multiple VLMMs reconciled by GPT-4. Second, it creates SafeVid-350K by generating adversarial queries through a hierarchical safety taxonomy (3H principles with subcategories) and synthesizing preference pairs where GPT-4 generates safe responses and LLaVA-NeXT-Video generates unsafe ones. Third, it aligns VLMMs using DPO on SafeVid-350K with specific hyperparameters (β=0.1, 1 epoch, frozen vision tower). The framework is evaluated on SafeVidBench, comprising Base and Challenge sets with 2,760 adversarial queries, plus OOD benchmarks like VLBreakBench and MM-SafetyBench.

## Key Results
- SafeVid-350K contains 350,000 video-specific query-response preference pairs synthesized using detailed textual descriptions and a structured safety taxonomy
- LLaVA-NeXT-Video + SafeVid-350K achieves 96.38% safety rate on SafeVidBench-Base (up from 53.99%), with 42.39% improvement over baseline
- DPO alignment on SafeVid-350K maintains or improves general capabilities, with LLaVA-NeXT-Video showing minimal degradation on MMBench-Video (1.12% drop)
- SafeVidBench-Challenge, human red-teamed, shows robust safety improvements across all 7 harmful categories with no significant over-refusal

## Why This Works (Mechanism)

### Mechanism 1: Textual Description as Cross-Modal Bridge
- **Claim:** Detailed textual video descriptions enable transfer of textual safety alignment capabilities to the video modality by making video content amenable to text-based safety reasoning.
- **Mechanism:** Multi-model description synthesis (LLaVA-NeXT-Video, Qwen2.5-VL, InternVL2.5) → GPT-4 reconciliation → high-fidelity textual narratives → LLM-based safety reasoning on descriptions → preference pair synthesis conditioned on both video context and text-grounded safety principles.
- **Core assumption:** Textual descriptions sufficiently capture the safety-relevant visual-temporal information in videos for alignment purposes.
- **Evidence anchors:**
  - [abstract]: "employing detailed textual video descriptions as an interpretive bridge, facilitating LLM-based rule-driven safety reasoning"
  - [Section 3.1]: "A core tenet of our approach is bridging the modality gap by translating rich video information into detailed textual narratives, making video content amenable to text-based safety reasoning."
  - [corpus]: Limited direct validation. Related work (SEA, Security Tensors) explores alternative cross-modal alignment approaches but does not evaluate textual bridging specifically.
- **Break condition:** If video content contains safety-critical visual-temporal nuances that resist textual description (e.g., subtle motion patterns, frame-level details), the bridge mechanism degrades. The paper acknowledges this limitation: "highly subtle visual-temporal safety nuances... may still present nuanced edge cases."

### Mechanism 2: Adversarial Query Generation via Structured Safety Taxonomy
- **Claim:** Hierarchical safety taxonomy (3H principles with subcategories) guides generation of contextually grounded adversarial queries that expose video-specific safety vulnerabilities.
- **Mechanism:** 30 scene categories → iterate through safety subcategories → Gemini 2.0 Pro generates candidate questions conditioned on video description + scene category + subcategory definition → re-evaluation for adversarial strength → SafeVid-350K contains 350K contextually grounded preference pairs.
- **Core assumption:** The taxonomy coverage is sufficient to capture real-world safety risks in video contexts.
- **Evidence anchors:**
  - [Section 3.1]: "Our question generation is guided by a hierarchical safety framework... inspired by the 3H (Helpful, Honest, Harmless) principles and integrating insights from existing safety taxonomies"
  - [Figure 2]: Shows structured safety dimensions organized under 3H with subcategories (Hate Speech, Copyright Infringement, Factuality Hallucination, etc.)
  - [corpus]: No corpus papers validate this specific taxonomy design for video safety.
- **Break condition:** If adversarial queries do not reflect real-world attack distributions, alignment may not generalize. SafeVidBench-Challenge attempts to address this via human red-teaming.

### Mechanism 3: DPO for Safety Preference Optimization
- **Claim:** Direct Preference Optimization on SafeVid-350K preference pairs aligns VLMM behavior with video-specific safety requirements without explicit reward modeling.
- **Mechanism:** Preference pairs (chosen safe response, rejected unsafe response) → DPO loss optimizes policy πθ to increase likelihood of chosen responses and decrease likelihood of rejected responses relative to reference model → implicit reward margin via β parameter.
- **Core assumption:** Preference pairs accurately encode the safety-behavior distinction; DPO objective sufficiently captures safety alignment without overfitting to refusal patterns.
- **Evidence anchors:**
  - [Section 3.2]: Defines DPO loss function with β=0.1, sigmoid loss, 1 epoch training
  - [Table 2]: LLaVA-NeXT-Video + SafeVid-350K achieves 96.38% safety rate (up from 53.99%)
  - [corpus]: DPO for VLMMs is explored in related work (Direct Preference Optimization from Language Model Reward), but corpus does not provide independent validation of DPO for video safety.
- **Break condition:** If preference pairs are noisy or if DPO causes over-refusal, helpfulness degrades. Table 3 shows Helpful Rate remains high or improves (e.g., 100% on StrongReject), suggesting no severe over-refusal.

## Foundational Learning

- **Concept: Mismatched Generalization in VLMMs**
  - **Why needed here:** Understanding the core problem—safety training on static modalities fails to transfer to dynamic video contexts—is essential before implementing the solution.
  - **Quick check question:** Can you explain why a VLMM might reject a harmful text query but comply when the same query accompanies a relevant video?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** DPO is the alignment algorithm used; understanding how it optimizes policy from preference pairs without explicit reward models is critical for implementation.
  - **Quick check question:** What does the β hyperparameter control in DPO, and what happens if β is set too high?

- **Concept: Cross-Modal Transfer via Language**
  - **Why needed here:** The core insight is using text as a bridge modality; understanding cross-modal transfer principles helps evaluate when this approach will succeed or fail.
  - **Quick check question:** What properties must textual descriptions have for them to serve as an effective bridge for safety reasoning?

## Architecture Onboarding

- **Component map:** InternVid-10M-FLT → filtering → 30-scene classification taxonomy → VideoCLIP centroid selection → 12,377 curated videos → multi-model description synthesis → GPT-4 reconciliation → high-fidelity textual descriptions → Gemini 2.0 Pro adversarial question generation → SafeVid-350K preference pairs → DPO training → SafeVidBench evaluation

- **Critical path:** Video corpus curation → description synthesis → adversarial query generation → preference pair construction → DPO training → evaluation. Description quality directly affects all downstream stages.

- **Design tradeoffs:**
  - **Description granularity vs. computational cost:** Multi-model synthesis improves fidelity but increases cost.
  - **Taxonomy breadth vs. data concentration:** 30 scenes × multiple safety subcategories provides coverage but may spread data thinly; consider data scale experiments (Figure 3 shows 10-20% data achieves significant gains).
  - **Automation vs. human oversight:** SafeVidBench-Base is automated; Challenge set adds human refinement for harder cases.

- **Failure signatures:**
  - **Low safety improvement after DPO:** Check description quality (hallucination rate), preference pair consistency, or data scale sufficiency.
  - **High over-refusal (low Helpful Rate):** Examine chosen response guidelines—may be too conservative.
  - **Poor OOD generalization:** Taxonomy may not cover target distribution; consider expanding scene categories or safety subcategories.

- **First 3 experiments:**
  1. **Baseline safety assessment:** Run unaligned VLMM on SafeVidBench-Base to establish baseline safety rate and identify failure categories.
  2. **Data scale ablation:** Train with 1%, 10%, 50%, 100% of SafeVid-350K to determine minimum effective data scale for your target VLMM (replicate Figure 3 methodology).
  3. **Description quality validation:** Manually inspect 50-100 synthesized descriptions for hallucination rate and safety-relevant detail coverage before full pipeline execution.

## Open Questions the Paper Calls Out

- **Question:** How does the "interpretive bridge" of textual description fail when faced with safety violations dependent on non-textual, subtle visual-temporal dynamics?
  - **Basis in paper:** [explicit] The Limitations section concedes that efficacy is linked to textual fidelity and may fail on "highly subtle visual-temporal safety nuances that are exceptionally challenging to capture exhaustively in text."
  - **Why unresolved:** The framework assumes text is a sufficient proxy for video content, but no ablation is provided to measure safety performance when video descriptions are intentionally lossy or fail to capture implicit temporal threats.
  - **What evidence would resolve it:** An evaluation on a benchmark specifically designed with non-semantic temporal safety risks (e.g., dangerous pacing, flashing lights, or subtle context shifts) where textual descriptions are intentionally insufficient.

- **Question:** To what extent does the static, hierarchical safety taxonomy limit the framework's ability to detect novel or rapidly evolving video-based misuse patterns?
  - **Basis in paper:** [explicit] The authors explicitly note that "rapidly evolving misuse patterns not yet fully encapsulated by our current safety taxonomy" may still present edge cases.
  - **Why unresolved:** The SafeVid-350K dataset is generated using a fixed taxonomy, meaning the model is only aligned to known categories of harm defined at the time of creation.
  - **What evidence would resolve it:** A longitudinal study testing the aligned models against newly emergent adversarial strategies (e.g., new "jailbreak" styles) developed after the dataset's cutoff date.

- **Question:** Do hallucinations or inconsistencies in the multi-model video description generation propagate into the preference data, leading to incorrect safety reasoning?
  - **Basis in paper:** [inferred] While the paper mentions using GPT-4 to reconcile discrepancies, the description generation relies on imperfect VLMMs (LLaVA, Qwen, InternVL) which are prone to hallucination, potentially biasing the "Chosen" response synthesis.
  - **Why unresolved:** The "Chosen" responses are grounded in the generated descriptions; if the description misrepresents the video, the safety logic (e.g., "factuality hallucination") may be applied to a false premise.
  - **What evidence would resolve it:** A human evaluation of the SafeVid-350K "Chosen" responses measuring the frequency of reasoning errors caused by faulty video descriptions compared to a ground-truth description baseline.

## Limitations

- **Limited validation of textual description bridging mechanism:** The paper assumes textual descriptions sufficiently capture safety-relevant visual-temporal information without empirical validation across diverse video content types.
- **Static safety taxonomy constraints:** The hierarchical safety taxonomy may not capture rapidly evolving or novel video-based misuse patterns that emerge after dataset creation.
- **Potential hallucination propagation:** Multi-model video description generation using imperfect VLMMs could introduce hallucinations that propagate into preference data and affect safety reasoning quality.

## Confidence

- **Safety improvement claims:** Medium confidence - Strong empirical support from SafeVidBench but limited OOD validation and no comparison to state-of-the-art safety alignment methods.
- **Textual description bridging mechanism:** Medium confidence - Mechanistically sound but limited empirical validation of description quality and coverage of safety-relevant visual-temporal nuances.
- **DPO alignment effectiveness:** Medium confidence - Standard method with reported success but limited hyperparameter sensitivity analysis and no ablation on alternative alignment approaches.

## Next Checks

1. **Description quality audit:** Manually evaluate 200 randomly sampled video descriptions for hallucination rate, completeness of safety-relevant information, and consistency across the three synthesis models (LLaVA-NeXT-Video, Qwen2.5-VL, InternVL2.5).

2. **Adversarial query robustness:** Take the top 50 highest-scoring adversarial queries from SafeVidBench-Base and modify them with subtle semantic variations (e.g., synonym substitution, rephrasing) to test whether the model's safety responses are robust to query manipulation.

3. **Temporal safety nuance evaluation:** Create a targeted test set of 100 videos containing subtle temporal safety cues (e.g., gradual escalation, context-dependent harm) that require frame-by-frame or motion understanding to assess whether textual descriptions adequately capture these safety-relevant temporal dynamics.