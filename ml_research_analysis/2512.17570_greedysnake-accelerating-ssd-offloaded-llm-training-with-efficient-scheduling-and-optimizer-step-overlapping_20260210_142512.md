---
ver: rpa2
title: 'GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling
  and Optimizer Step Overlapping'
arxiv_id: '2512.17570'
source_url: https://arxiv.org/abs/2512.17570
tags:
- layer
- training
- memory
- optimizer
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GreedySnake is an SSD-offloaded training system for large language
  models that uses vertical scheduling to improve throughput. Instead of processing
  all layers for one micro-batch before moving to the next (horizontal scheduling),
  it executes all micro-batches for one layer before proceeding to the next layer.
---

# GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping

## Quick Facts
- arXiv ID: 2512.17570
- Source URL: https://arxiv.org/abs/2512.17570
- Reference count: 39
- Key outcome: GreedySnake achieves 1.96x-2.53x higher saturated training throughput than ZeRO-Infinity for GPT-65B and GPT-175B models on A100 GPUs

## Executive Summary
GreedySnake is an SSD-offloaded training system for large language models that fundamentally rethinks execution order to improve throughput. Instead of processing all layers for one micro-batch before moving to the next (horizontal scheduling), it executes all micro-batches for one layer before proceeding to the next layer. This vertical scheduling reduces parameter loading traffic and increases the amount of GPU computation that can overlap with optimizer steps. The system also overlaps part of the optimizer step with the forward pass of the next iteration, achieving significant performance gains over existing systems.

## Method Summary
GreedySnake implements vertical gradient accumulation with pipelined execution, using Adam optimizer and mixed precision training with activation checkpointing at layer boundaries. The system uses an LP-based configuration search to determine optimal micro-batch count and delay ratio for overlapping optimizer steps with forward passes. It's implemented in approximately 5K lines of Python on top of PyTorch, supporting GPT-style models from Megatron-LM with various sizes and configurations.

## Key Results
- Achieves 1.96x to 2.53x higher saturated training throughput compared to ZeRO-Infinity for GPT-65B and GPT-175B models
- Reduces parameter loading traffic from 2×M×m_s to 2×m_s through layer-wise parameter reuse
- Successfully overlaps optimizer step with backward passes of multiple micro-batches, increasing overlap capacity from (N-1) layers to M×(N-1) layers

## Why This Works (Mechanism)

### Mechanism 1: Vertical Parameter Reuse
- Claim: Processing all micro-batches for a layer before moving to the next layer reduces parameter loading traffic compared to sequential micro-batch processing.
- Mechanism: In standard horizontal scheduling, parameters are loaded from offloaded memory for every micro-batch. GreedySnake's vertical scheduling loads a layer's parameters once, performs the forward and backward passes for all M micro-batches on that layer, and then discards them. This reduces parameter traffic from 2×M×m_s to 2×m_s.
- Core assumption: Activation checkpoint traffic (which increases under vertical scheduling) creates less bottleneck than parameter traffic. The paper assumes parameter size scales quadratically with hidden dimension while activation size scales linearly.
- Evidence anchors:
  - [Abstract] "...executes all microbatches of a layer before proceeding to the next. Compared to existing systems... GreedySnake achieves higher training throughput..."
  - [Section 3.4] "...improving parameter reuse is more critical... the number of elements in each LLM layer scales quadratically... activation checkpoint only scales linearly..."
  - [Corpus] Weak direct support; related work (AutoHete, AsyncHZP) focuses on general heterogeneous parallelism but does not explicitly validate this specific vertical trade-off.

### Mechanism 2: Backward-Optimizer Overlap Scaling
- Claim: Vertical scheduling increases the computable surface area available to overlap with the optimizer step, preventing the optimizer from becoming a serial bottleneck.
- Mechanism: The optimizer step is IO-heavy (loading/updating states from SSD). In horizontal scheduling, overlap is limited to the backward pass of the final micro-batch. Vertical scheduling allows the optimizer to run concurrently with the backward passes of all micro-batches (except the last layer), increasing overlap capacity from (N-1) layers to M×(N-1) layers.
- Core assumption: The GPU compute time for the backward pass is sufficient to hide the SSD access latency of the optimizer step.
- Evidence anchors:
  - [Abstract] "...increases the amount of GPU computation that can overlap with optimizer steps."
  - [Section 3.4] "...vertical scheduling substantially increases the opportunity to overlap the optimizer step with the backward pass... overlapped computation becomes approximately M×(N-1) layers..."
  - [Corpus] *AsyncHZP* supports the general need for asynchronous/hierarchical scheduling to mask communication overhead, aligning with the need to hide IO.

### Mechanism 3: Delayed Optimizer Step via Memory Reclamation
- Claim: Delaying a portion (α) of the optimizer step to the next iteration's forward pass reduces peak IO demand without increasing memory footprint.
- Mechanism: Instead of forcing all optimizer updates to finish before the next forward pass, GreedySnake delays updates for specific parameter chunks. It reclaims CPU memory buffers from obsolete parameters and checkpoints (which are no longer needed after their specific layer's backward pass) to store the gradients required for these delayed updates.
- Core assumption: The memory reclaimed from processed layers is sufficient to buffer the delayed gradient data.
- Evidence anchors:
  - [Abstract] "...overlaps part of the optimization step with the forward pass of the next iteration."
  - [Section 4.4] "...reuse the CPU memory allocated for offloaded parameters and checkpoints... Layer by layer... the corresponding data in such memory becomes obsolete gradually."
  - [Corpus] No direct corroboration found in the provided corpus for this specific memory reclamation strategy.

## Foundational Learning

- Concept: **Gradient Accumulation (Micro-batching)**
  - Why needed here: GreedySnake fundamentally alters the execution order of micro-batches. Understanding that gradients are summed over M steps before an optimizer step is required to see why vertical scheduling aggregates gradients in GPU memory rather than swapping them.
  - Quick check question: How does vertical scheduling change when the gradient accumulation buffer is written to CPU memory compared to horizontal scheduling?

- Concept: **Activation Checkpointing (Rematerialization)**
  - Why needed here: Vertical scheduling forces the system to swap activation checkpoints between layers because it switches micro-batches rather than layers. You must understand that activations are discarded and recomputed to grasp the increased checkpoint traffic cost.
  - Quick check question: In vertical scheduling, why must the output activation of Layer i for Micro-batch 0 be offloaded before Layer i+1 starts?

- Concept: **Roofline Model**
  - Why needed here: The paper uses a roofline model to define "ideal" throughput versus "IO-bound" throughput. This is critical for understanding the goal: shifting the system from an IO-roofline constraint to a compute-roofline constraint.
  - Quick check question: Does increasing the number of micro-batches move the system closer to the IO-roofline or the Compute-roofline in the GreedySnake model?

## Architecture Onboarding

- Component map: Inter-layer Tensor Coordinator -> Parameter Coordinator -> Optimizer Step Coordinator
- Critical path:
  1. **Prefetch:** Load Layer i params (SSD -> CPU -> GPU)
  2. **Forward Vertical:** Compute Layer i for Micro-batches 0..M. Offload activations
  3. **Backward Vertical:** Recompute + Backward Layer i for Micro-batches 0..M. Accumulate gradients in GPU
  4. **Offload & Optimize:** Move accumulated gradients to CPU. Trigger optimizer step (partially delayed)
  5. **Reclaim:** Free CPU memory from Layer i params/checkpoints to prepare for delayed optimizer data
- Design tradeoffs:
  - Pipeline Granularity: Parameters move in micro-batch-sized chunks (better bandwidth utilization) vs. Checkpoints move in layer-granularity (simpler SSD IO)
  - Delay Ratio (α): High α improves forward overlap but requires holding stale gradients longer. The LP solver determines this.
- Failure signatures:
  - SSD Bandwidth Starvation: Throughput saturates well below the compute roofline even with high micro-batch counts (indicates IO is not being hidden)
  - CPU OOM during Backward: Occurs if the memory reclamation strategy fails to free buffers fast enough for delayed gradient storage
  - Stale Parameters: Numerical divergence or loss spikes if the delayed optimizer step (α fraction) is not correctly applied before the corresponding layer's forward pass in the next iteration
- First 3 experiments:
  1. **Traffic Baseline:** Run GPT-65B with Horizontal vs. Vertical scheduling; measure total GB moved across PCIe and SSD interfaces to validate Figure 5
  2. **Overlap Efficiency:** Profile the backward pass timeline; verify that the optimizer step (CPU) execution time is fully covered by the backward computation (GPU) duration
  3. **Alpha Sweep:** Vary the delay ratio α from 0.0 to 0.5 on a constrained memory setup to find the point where memory reclamation fails (OOM) or throughput degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GreedySnake's vertical scheduling scale to multi-node distributed training with network communication overhead?
- Basis in paper: [inferred] The evaluation is limited to single-node configurations with up to 4 GPUs using FSDP. The paper does not address inter-node communication patterns or how vertical scheduling interacts with distributed training across multiple nodes.
- Why unresolved: Multi-node training introduces network bandwidth constraints and synchronization patterns that may affect the overlapping opportunities that vertical scheduling provides.
- What evidence would resolve it: Experiments with multi-node clusters showing throughput scaling efficiency and communication overlap analysis.

### Open Question 2
- Question: Is vertical scheduling effective for non-decoder-only transformer architectures such as encoder-only (BERT) or encoder-decoder (T5) models?
- Basis in paper: [inferred] The paper exclusively evaluates GPT-style decoder-only models. These architectures have different computation graphs and activation patterns that may affect the checkpoint-vs-parameter tradeoff analysis in Section 3.4.
- Why unresolved: The quadratic vs. linear scaling argument for parameters vs. checkpoints may not apply identically across different architectures with different layer structures.
- What evidence would resolve it: Comparative experiments across encoder-only, encoder-decoder, and mixture-of-experts architectures.

### Open Question 3
- Question: How can the LP-based configuration search adapt dynamically to variable SSD bandwidth in shared cloud environments?
- Basis in paper: [explicit] Section 6.2 states: "the cloud cluster is shared by multiple users, and as a result, the SSD bandwidth tends to diverge more from the performance model due to the sharing and contention of its bandwidth."
- Why unresolved: The current approach relies on static benchmarking, but SSD bandwidth variability causes the performance model predictions to diverge from actual throughput.
- What evidence would resolve it: An adaptive configuration system with online bandwidth monitoring and dynamic reconfiguration demonstrating improved throughput stability.

## Limitations
- Memory Reclamation Assumption: The effectiveness relies on CPU memory freed from processed layers being sufficient to buffer delayed gradient data, which may fail under memory-constrained scenarios
- Architecture Generalization: The quadratic vs. linear scaling argument for parameters vs. checkpoints may not hold for all transformer architectures or extreme sequence lengths
- Hardware Dependency: Reported performance improvements are measured on A100 GPUs and may not generalize to other GPU architectures or SSD configurations

## Confidence
**High Confidence** (supported by multiple evidence anchors):
- Vertical scheduling reduces parameter loading traffic from 2×M×m_s to 2×m_s through layer-wise parameter reuse
- The optimizer step bottleneck can be partially hidden by overlapping with backward passes of multiple micro-batches
- GreedySnake achieves 1.96x-2.53x higher throughput than ZeRO-Infinity on GPT-65B and GPT-175B models

**Medium Confidence** (supported by mechanism description but limited empirical validation):
- The delayed optimizer step strategy successfully reduces peak IO demand without increasing memory footprint
- The activation checkpoint traffic increase does not negate parameter reuse gains across typical LLM architectures

**Low Confidence** (mechanism described but no direct empirical validation):
- The LP-based configuration search consistently finds near-optimal configurations across diverse model architectures
- Memory reclamation from processed layers is always sufficient to buffer delayed gradient data

## Next Checks
1. **Memory Reclamation Validation**: Systematically test GreedySnake on GPT-65B with varying CPU memory constraints (128GB, 256GB, 512GB) while monitoring memory usage patterns. Measure the ratio of reclaimed memory to required delayed gradient buffer across different micro-batch counts to identify the breaking point where memory reclamation fails.

2. **Architecture Generalization**: Evaluate GreedySnake on diverse LLM architectures beyond GPT-style models, including transformer variants with different layer widths, sequence lengths, and attention mechanisms. Compare parameter vs. activation traffic ratios to validate the core assumption about quadratic vs. linear scaling.

3. **Hardware Architecture Sensitivity**: Benchmark GreedySnake on multiple GPU architectures (A100, H100, V100) and SSD configurations (PCIe 3.0 vs 4.0, different NVMe models). Measure how throughput improvements scale with hardware capabilities and identify the hardware thresholds where vertical scheduling benefits diminish.