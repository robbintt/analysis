---
ver: rpa2
title: Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement
  Learning
arxiv_id: '2511.11607'
source_url: https://arxiv.org/abs/2511.11607
tags:
- learning
- cowm
- layer
- policy
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-stationarity in reinforcement
  learning, where environments change over time, leading to low sample efficiency.
  The authors introduce the Clustering Orthogonal Weight Modified (COWM) layer, which
  can be integrated into the policy network of any RL algorithm to mitigate non-stationarity
  effectively.
---

# Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.11607
- Source URL: https://arxiv.org/abs/2511.11607
- Authors: Guoqing Ma; Yuhan Zhang; Yuming Dai; Guangfu Hao; Yang Chen; Shan Yu
- Reference count: 40
- One-line primary result: Introduces COWM layer that improves RL sample efficiency by 9-12.6% through gradient orthogonalization.

## Executive Summary
This paper addresses non-stationarity in reinforcement learning by introducing the Clustering Orthogonal Weight Modified (COWM) layer. The COWM layer can be integrated into any RL algorithm's policy network to stabilize learning by using clustering techniques and projection matrices to minimize interference with previously learned skills. The method demonstrates significant improvements in both vision-based and state-based DMControl benchmarks, achieving 9% and 12.6% gains respectively, while showing robustness across various algorithms and tasks.

## Method Summary
The COWM layer replaces standard linear layers in RL policy networks. During forward passes, it stores principal components of input activations in a buffer. Using K-means clustering (typically with 2 clusters), it identifies distinct sub-policies in the state space. During backpropagation, it projects weight updates onto the null space of previously learned policy inputs, effectively constraining gradients to preserve old skills while learning new ones. The method requires replacing linear layers in the actor network and maintaining a buffer of historical input directions.

## Key Results
- Achieves 9% improvement in vision-based DMControl benchmarks
- Achieves 12.6% improvement in state-based DMControl benchmarks
- Outperforms state-of-the-art methods including DreamerV3, CURL, SAC, PPO, and D4PG
- Demonstrates robustness and generality across various algorithms and tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning input activations via clustering isolates distinct sub-policies (e.g., "standing" vs. "walking") inherent in a single RL task, allowing selective protection of old skills.
- **Mechanism:** The COWM layer maintains a buffer F of past input principal components. It applies K-means clustering to find centers U. When new data arrives, it identifies the nearest cluster j and defines the "preserved directions" A as the set of all other cluster centers. This ensures that the gradient constraint protects inputs dissimilar to the current input.
- **Core assumption:** The state space of a single task can be decomposed into relatively distinct sub-distributions corresponding to different sub-policies or phases of learning.
- **Evidence anchors:** [abstract] "employing clustering techniques... to mitigate non-stationarity"; [page 3, Eq. 2] Definition of A_l; [page 6, Fig. 5] Visualization of differentiated representations.
- **Break condition:** If the state distribution is uniform or sub-policies are heavily overlapping, clustering will fail to isolate "old" vs. "new" memories.

### Mechanism 2
- **Claim:** Projecting the gradient update onto the input space of previously learned skills minimizes interference (catastrophic forgetting) without explicitly storing old data.
- **Mechanism:** The method calculates a projection matrix P derived from the preserved directions A. During backpropagation, the standard gradient is modified by subtracting the component that aligns with the subspace spanned by A.
- **Core assumption:** The input features stored in the buffer accurately span the functional space required for old policies, and the local gradient estimate is sufficient for the projection to be effective.
- **Evidence anchors:** [page 3, Eq. 4] The weight update formula; [page 4, Eq. 18] Mathematical proof showing expected interference equals zero.
- **Break condition:** If the network width is too small to support orthogonal subspaces for multiple policies, the projection might overly constrain the network.

### Mechanism 3
- **Claim:** Stabilizing the non-stationary state distribution in RL improves sample efficiency by reducing the need to relearn forgotten sub-policies.
- **Mechanism:** By enforcing the gradient constraint, the agent avoids the "sawtooth" pattern of learning a new skill and immediately losing an old one. This reduces the variance of the learning process and accelerates convergence.
- **Core assumption:** The overhead of calculating the projection matrix is negligible compared to the savings gained from not relearning forgotten behaviors.
- **Evidence anchors:** [page 1, Fig 1a] Illustration of the "Forget stand center" cycle; [page 6, Fig 3/6] Training curves showing COWM achieves higher reward faster.
- **Break condition:** In environments with extremely high-frequency non-stationarity or chaotic dynamics where "old" policies are never revisited, the computational cost may outweigh the benefits.

## Foundational Learning

- **Concept:** **Null Space Projection**
  - **Why needed here:** The core mathematical operation of COWM is projecting gradients into the null space of the input correlation matrix of old memories.
  - **Quick check question:** If matrix A represents the input features of old tasks, why does projecting a weight update ΔW into the null space of A preserve the output for inputs in A?

- **Concept:** **K-means Clustering in High Dimensions**
  - **Why needed here:** The method relies on K-means to dynamically partition the representation space.
  - **Quick check question:** How does the "curse of dimensionality" affect distance metrics in K-means, and why might the paper limit the cluster count c to small numbers (e.g., 2)?

- **Concept:** **Catastrophic Forgetting in Neural Networks**
  - **Why needed here:** The paper frames its solution as a fix for forgetting in continual/single-task RL.
  - **Quick check question:** Why does standard Stochastic Gradient Descent (SGD) inherently overwrite weights relevant to old data distributions when training on a new distribution?

## Architecture Onboarding

- **Component map:** Input Buffer -> Clustering Module -> Projection Calculator -> Modified Linear Layer

- **Critical path:**
  1. Forward pass processes batch; input vector x_{l-1} is added to buffer.
  2. During backward pass, K-means runs (or updates) on the buffer to find A.
  3. Gradient ∂L/∂W is intercepted.
  4. Projection P is calculated.
  5. Gradient is modified: g' = g - g P A^T x.
  6. Optimizer steps with g'.

- **Design tradeoffs:**
  - **Buffer Size (F) vs. Memory:** Larger buffers capture more history but increase memory usage and clustering time.
  - **Cluster Count (c) vs. Granularity:** The paper suggests c=2 works best. Higher c might fragment the policy space unnecessarily.
  - **Clustering Iterations (k):** Too few (k<10) leads to unstable centers; too many yields diminishing returns.

- **Failure signatures:**
  - **Gradient Collapse:** Reward stays flat; gradients might be zero if the projection matrix is overly aggressive.
  - **Cluster Collapse:** All inputs map to one cluster, meaning no directions are "preserved," and the method degenerates to standard RL.
  - **Slow Training:** Excessive time per step due to K-means convergence checks.

- **First 3 experiments:**
  1. **Toy Continual Learning:** Implement COWM on a simple MLP solving Permuted MNIST (sequentially). Verify if accuracy on Task 0 remains high after training on Task 1 compared to standard SGD.
  2. **Ablation on Cluster Count (c):** Run DMControl "Walker-Walk" with c={1, 2, 5, 10}. Verify the paper's claim that c=2 is optimal and analyze why performance drops at c=1 (no protection) or high c.
  3. **Gradient Norm Analysis:** Log the norm of the standard gradient vs. the projected gradient during training. If the ratio is consistently ≈ 0, the protection is too strong. If ≈ 1, it is inactive.

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that a single RL task's state space naturally decomposes into distinct sub-distributions is not rigorously validated across all tested environments.
- Buffer size F, k-means update frequency, and regularization for matrix inversion are unspecified, making exact reproduction challenging.
- The claim of 9-12.6% improvements is based on aggregate benchmarks; per-task variance and statistical significance are not detailed.

## Confidence

**High:** COWM improves learning speed and sample efficiency in DMControl benchmarks; the orthogonalization mechanism is mathematically sound.

**Medium:** The clustering-based weight orthogonalization is effective in mitigating interference; the method generalizes across algorithms.

**Low:** The specific claim that non-stationarity occurs in single-task settings is plausible but not definitively proven across all environments.

## Next Checks

1. Implement COWM on a simple continual learning task (e.g., Permuted MNIST) to verify if accuracy on old tasks is preserved compared to standard SGD.

2. Conduct an ablation study varying cluster count c on DMControl tasks to confirm the optimal value and understand performance drop at c=1 and high c.

3. Log and analyze the ratio of standard gradient norm to projected gradient norm during training to detect overly aggressive or inactive protection.