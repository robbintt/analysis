---
ver: rpa2
title: Deep Generative Clustering with VAEs and Expectation-Maximization
arxiv_id: '2501.07358'
source_url: https://arxiv.org/abs/2501.07358
tags:
- clustering
- data
- cluster
- each
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel deep clustering method that integrates
  Variational Autoencoders (VAEs) with the Expectation-Maximization (EM) algorithm.
  The approach models each cluster's distribution with a separate VAE and alternates
  between updating VAE parameters via ELBO maximization and refining cluster assignments.
---

# Deep Generative Clustering with VAEs and Expectation-Maximization

## Quick Facts
- arXiv ID: 2501.07358
- Source URL: https://arxiv.org/abs/2501.07358
- Reference count: 26
- Primary result: Achieves 94.6% clustering accuracy on MNIST using VAEs with EM optimization, outperforming existing VAE-based methods

## Executive Summary
This paper introduces a novel deep clustering approach that combines Variational Autoencoders (VAEs) with the Expectation-Maximization (EM) algorithm. The method models each cluster with a separate VAE and alternates between updating VAE parameters via ELBO maximization and refining cluster assignments. Unlike previous VAE-based clustering methods, this approach eliminates the need for Gaussian Mixture Model priors or additional regularization techniques. The framework achieves state-of-the-art clustering performance on MNIST and FashionMNIST while also providing generative capabilities that traditional clustering methods lack.

## Method Summary
The method treats clustering as a generative problem where each of K clusters is modeled by its own VAE. During the E-step, soft cluster assignments are computed by normalizing the ELBO scores across all K VAEs for each data point. In the M-step, each VAE is trained on its weighted data points using the computed assignments. This alternating optimization continues until convergence. The approach uses fully connected VAE architectures with 20-dimensional latent spaces and trains for 300 EM iterations with 20 epochs per M-step. Unlike VaDE and GMVAE, which use a single VAE with a GMM prior in the latent space, this method directly models each cluster's distribution in observation space using separate VAEs.

## Key Results
- Achieves 94.6% clustering accuracy on MNIST and 59.1% on FashionMNIST
- Outperforms state-of-the-art VAE-based methods VaDE (88.1%) and GMVAE (85.4%) on MNIST
- Demonstrates ability to generate new samples from each learned cluster distribution
- Shows clustering accuracy remains stable across different random initializations (88.4±5.2% on MNIST)

## Why This Works (Mechanism)

### Mechanism 1
Modeling each cluster with a separate VAE eliminates the need for GMM priors or explicit regularization. Instead of forcing all clusters to share a single latent space with a GMM prior, each cluster k gets its own VAE with parameters α_k = (φ_k, θ_k). The cluster-specific decoder p_θ_k(x|z) directly models the data distribution for that cluster in observation space, avoiding constraints on latent geometry. This specialization allows VAEs to learn distinct generative models for each cluster without artificial regularization. The method assumes cluster distributions are sufficiently distinct that separate generative models can specialize without collapsing, and that soft assignments provide enough signal to train K VAEs in parallel.

### Mechanism 2
Using ELBO as the assignment score provides a principled likelihood proxy for soft cluster membership. The E-step computes u_i = softmax([ELBO(α_k; x_i)]_{k=1}^K). Since ELBO lower-bounds log-likelihood log p_α_k(x_i), samples achieving higher ELBO under cluster k's VAE are more likely generated by that cluster. The softmax normalizes across clusters to produce valid probability distributions. This mechanism assumes the ELBO gap (KL divergence between approximate and true posterior) is roughly consistent across clusters, so ELBO comparisons reflect relative likelihoods. The approach relies on the mathematical property that minimizing the weighted assignment objective is equivalent to minimizing a KL divergence, leading to the softmax solution.

### Mechanism 3
Alternating EM optimization allows mutual refinement between cluster assignments and generative parameters. The M-step trains each VAE on weighted samples (weighted by u_{i,k}), improving cluster-specific distributions. The E-step recomputes assignments using improved VAEs. This bootstrapping continues until convergence—better assignments lead to better VAEs, which lead to better assignments. The method assumes the local optimum reached by this procedure corresponds to meaningful clustering, and that initializing K VAEs randomly doesn't trap all models in poor local minima. This generalized EM approach only requires ELBO improvement rather than exact maximization, making it computationally tractable.

## Foundational Learning

- **Concept: Evidence Lower Bound (ELBO)**
  - Why needed: The entire method hinges on using ELBO as a tractable proxy for cluster likelihood. Understanding the decomposition ELBO = E[log p(x|z)] - KL(q(z|x) || p(z)) is essential for debugging assignment quality.
  - Quick check: Given two VAEs with equal reconstruction error but different KL terms, which will receive higher assignment probability for a sample? (Answer: The one with lower KL term, since ELBO = reconstruction - KL)

- **Concept: Softmax over Log-Space Values**
  - Why needed: Cluster assignments u_i are computed via softmax of ELBO values. Understanding temperature scaling and numerical stability (log-sum-exp trick) is critical for implementation.
  - Quick check: What happens to assignments if all ELBO values are large negative numbers (e.g., -500)? (Answer: Assignments become more uniform as the softmax temperature effect dominates)

- **Concept: Generalized EM (GEM)**
  - Why needed: The M-step uses gradient descent (Adam) rather than exact maximization, falling under GEM which only requires ELBO improvement, not maximization.
  - Quick check: Why does the paper use multiple training epochs per M-step (Table 1: 20 epochs) rather than a single gradient update? (Answer: To ensure sufficient ELBO improvement and stable VAE updates)

## Architecture Onboarding

- **Component map:**
  - K VAE encoders E_φ_k: Map input x ∈ R^{784} to latent parameters (μ_k(x), σ_k^2(x)) ∈ R^{20}
  - K VAE decoders D_θ_k: Map latent z ∈ R^{20} to reconstruction x̂ ∈ R^{784}
  - Assignment module: Computes softmax over K ELBO values per sample
  - EM scheduler: Alternates M-step (VAE updates) and E-step (assignment updates)

- **Critical path:**
  1. Forward pass: Compute ELBO for sample x_i under all K VAEs (requires encoder + decoder + reparameterization)
  2. E-step: Softmax over ELBOs → assignment vector u_i
  3. M-step: Weighted ELBO loss ∑_i u_{i,k} · ELBO(α_k; x_i) per VAE
  4. Repeat for 300 EM iterations (MNIST)

- **Design tradeoffs:**
  - Memory vs. specialization: K separate VAEs require K× memory but avoid GMM prior constraints
  - MC samples in E-step: Paper uses 10 samples for ELBO estimation (accuracy vs. speed)
  - M-step epochs: 20 epochs per EM iteration provides stable VAE updates but increases training time
  - Assumption: Fixed K known a priori—no automatic cluster detection

- **Failure signatures:**
  - Cluster collapse: >70% assignments to single cluster → check assignment entropy, consider assignment temperature
  - Mode overlap: Generated samples from different clusters look identical → VAEs converged to same solution, reinitialize
  - ELBO divergence: Assignments become deterministic (0 or 1) early → ELBO gaps too large, normalize or calibrate

- **First 3 experiments:**
  1. **Sanity check on synthetic data**: Replicate the 5 half-moons experiment (Figure 1) with known ground truth to verify cluster separation emerges correctly
  2. **Ablation on MC samples**: Compare 1, 5, 10 MC samples for ELBO estimation in E-step to quantify variance-accuracy tradeoff
  3. **Assignment initialization sensitivity**: Test random vs. uniform vs. k-means-based initial assignments to assess convergence robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generative EM framework be extended to normalizing flows, enabling exact log-likelihood computation without ELBO approximation while preserving clustering accuracy?
- Basis in paper: The conclusion states: "Future work could extend this generative EM framework to other models, such as normalizing flows, which allow for easy computation of log-likelihoods without relying on lower bounds."
- Why unresolved: Normalizing flows provide tractable likelihoods but require invertible architectures, which may conflict with the current per-cluster model structure and the iterative EM assignment updates.
- What evidence would resolve it: A successful implementation replacing cluster-specific VAEs with normalizing flows, demonstrating comparable clustering metrics while computing exact likelihoods.

### Open Question 2
- Question: How does clustering performance scale to higher-resolution, more complex image datasets beyond 28×28 grayscale images?
- Basis in paper: Experiments are limited to MNIST, FashionMNIST, and a 2D synthetic dataset; no evaluation on benchmarks like CIFAR-10, STL-10, or natural images.
- Why unresolved: The fully connected architecture and simple image datasets may not reflect performance on data with complex textures, higher dimensionality, or greater intra-class variability.
- What evidence would resolve it: Systematic evaluation on diverse datasets (CIFAR-10/100, STL-10) with analysis of computational cost and accuracy trends.

### Open Question 3
- Question: What causes the high variance in clustering accuracy across random runs (e.g., ±5.2% on MNIST), and can principled initialization strategies reduce this instability?
- Basis in paper: Table 2 shows substantial variance (MNIST: 88.4±5.2%, FashionMNIST: 59.1±2.9%) with random initialization of both soft assignments and model weights.
- Why unresolved: The paper does not analyze sensitivity to initialization or propose methods to ensure consistent convergence.
- What evidence would resolve it: Ablation study comparing random, pretraining-based, and k-means-guided initializations, demonstrating reduced variance.

## Limitations

- The method requires K (number of clusters) to be known a priori, limiting applicability in real-world unsupervised settings where cluster count is unknown
- Memory and computational cost scale linearly with K due to separate VAE models for each cluster, making it less efficient than single-model alternatives
- Performance on complex, high-dimensional datasets beyond simple 28×28 grayscale images remains unproven and may not generalize well

## Confidence

- **High Confidence**: The EM framework with alternating updates is theoretically sound and well-established in statistics
- **Medium Confidence**: Empirical results on MNIST/FashionMNIST show improvement over baselines, but the margin is modest and needs replication
- **Medium Confidence**: The mechanism of using ELBO as assignment scores is principled but depends on the assumption that ELBO gaps are comparable across clusters
- **Low Confidence**: Claims about scalability and performance on complex datasets are unsupported by current evidence

## Next Checks

1. **Synthetic Cluster Validation**: Replicate the 5 half-moons experiment with ground truth to verify cluster separation emerges correctly and assess sensitivity to initialization
2. **ELBO Assignment Calibration**: Test whether ELBO gaps across clusters are consistent by measuring assignment entropy and clustering accuracy under varying temperature scaling
3. **Memory-Complexity Analysis**: Benchmark memory usage and runtime against VaDE/GMVAE on MNIST to quantify the cost of K separate VAEs versus shared-latent-space alternatives