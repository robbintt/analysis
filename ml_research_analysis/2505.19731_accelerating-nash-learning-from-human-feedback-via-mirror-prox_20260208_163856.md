---
ver: rpa2
title: Accelerating Nash Learning from Human Feedback via Mirror Prox
arxiv_id: '2505.19731'
source_url: https://arxiv.org/abs/2505.19731
tags:
- policy
- learning
- preference
- have
- nash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Nash Mirror Prox (NashMP), a new algorithm
  for finding Nash equilibria in regularized preference games arising in Nash Learning
  from Human Feedback (NLHF). Unlike traditional RLHF methods that rely on reward
  models (e.g., Bradley-Terry), NLHF directly models pairwise preferences as a game,
  seeking a stable mixed policy (von Neumann winner) without requiring transitive
  preferences.
---

# Accelerating Nash Learning from Human Feedback via Mirror Prox

## Quick Facts
- arXiv ID: 2505.19731
- Source URL: https://arxiv.org/abs/2505.19731
- Reference count: 40
- Primary result: Introduces Nash Mirror Prox (NashMP) achieving O((1+2β)^(-N/2)/β) convergence rate, improving upon polynomial O((β2N)^(-1)) of prior methods

## Executive Summary
This paper introduces Nash Mirror Prox (NashMP), a new algorithm for finding Nash equilibria in regularized preference games arising in Nash Learning from Human Feedback (NLHF). Unlike traditional RLHF methods that rely on reward models, NLHF directly models pairwise preferences as a game, seeking a stable mixed policy without requiring transitive preferences. NashMP leverages the Mirror Prox optimization scheme to achieve last-iterate linear convergence to the β-regularized Nash equilibrium, significantly improving upon the polynomial convergence rate of the prior NashMD algorithm. The method shows competitive performance on both synthetic matrix games and LLM fine-tuning tasks, outperforming existing methods like Online DPO, Online IPO, and NashMD in aligning large language models with human preferences.

## Method Summary
NashMP is a two-step Mirror Prox algorithm that finds Nash equilibria in β-regularized preference games for LLM alignment. It uses a target network with exponential moving average to approximate the proximal point method without solving inner problems exactly. The algorithm combines preference REINFORCE gradients with KL regularization toward both a reference policy and the current iterate. The practical implementation uses stochastic policy gradients in deep learning settings, with hyperparameters including learning rate 1e-6, β=1e-4, mirror prox rate η=0.1, and EMA rate κ=0.1.

## Key Results
- Achieves linear convergence rate of (1+2β)^(-N/2)/β, improving upon polynomial O((β2N)^(-1)) of NashMD
- Finds ε-VNW with query complexity eO(1/ε), matching state-of-the-art results with last-iterate guarantees
- Outperforms Online DPO, Online IPO, and NashMD on LLM fine-tuning tasks with 2-5% win rate improvements
- Demonstrates robustness to preference intransitivity that defeats traditional reward-based approaches

## Why This Works (Mechanism)

### Mechanism 1
Two-step extrapolation-update structure accelerates convergence over single-step mirror descent. NashMP first computes an "improved" opponent policy via mirror descent against the current policy, then updates by performing another mirror descent step against this improved opponent. This anticipates opponent response rather than reacting to stale information. Core assumption: The preference game is bilinear with Lipschitz gradients; extrapolation reduces oscillation around equilibrium. Break condition: If inner optimization is too noisy or inaccurate, extrapolation may amplify errors rather than reduce them.

### Mechanism 2
Dual KL regularization (toward reference πref AND previous iterate πk) stabilizes learning while maintaining tractability. Each step minimizes a combination of preference loss + β·KL(π||πref) + (β/η)·KL(π||πk). The first term anchors to prior knowledge; the second provides proximal smoothing between iterations. Core assumption: KL divergence is the appropriate geometry for policy space; β is small enough that regularization doesn't dominate the preference signal. Break condition: If β is too large relative to preference signal strength, policy collapses toward reference regardless of human preferences.

### Mechanism 3
Target network with exponential moving average approximates the proximal point method without solving inner problems exactly. Maintain θ_target updated as θ_target ← κθ + (1-κ)θ_target. This soft update approximates n ≈ 1/κ gradient steps per proximal update, balancing approximation quality with computational cost. Core assumption: The target network provides a sufficiently stable opponent for the online policy to play against. Break condition: If κ is too large (target updates too fast), training becomes unstable; if too small, convergence slows dramatically.

## Foundational Learning

- **Mirror Descent in Probability Simplex**: Why needed here: NashMP iterates are mirror descent updates with KL divergence as the Bregman divergence. Understanding why this geometry is natural for distributions is essential. Quick check question: Why is KL divergence preferred over Euclidean distance when optimizing over probability distributions?

- **Two-Player Zero-Sum Game Theory**: Why needed here: NLHF frames preference learning as finding a Nash equilibrium of a symmetric zero-sum game. The von Neumann winner is the equilibrium policy. Quick check question: What is the difference between a Condorcet winner and a von Neumann winner, and why does intransitivity require the latter?

- **Proximal Point Method and its Approximations**: Why needed here: NashMP is motivated as an approximation to the proximal point method. Understanding this connection explains why the algorithm has good convergence properties. Quick check question: How does Mirror Prox approximate the proximal point method, and what is the computational tradeoff?

## Architecture Onboarding

- Component map: Online policy network πθ -> Target policy network π_target (EMA of online) -> Reference policy π_ref (fixed SFT) -> Preference model P (external judge)

- Critical path: 1. Sample prompts → generate response pairs (y, y') from online policy 2. Query preference model for P(y ≻ y') 3. Compute contrastive REINFORCE-style gradient with KL regularization 4. Update online policy via gradient step 5. Soft-update target: θ_target ← (1-κ)θ_target + κθ_online

- Design tradeoffs:
  - κ (soft update rate): Lower κ = more stable but slower convergence; empirical results suggest κ ∈ {0.01, 0.1} for long training
  - η (mirror prox learning rate): Theory suggests η = 2β, but practical experiments used η = 0.1 >> 2β because the effective regularization is β(1 + 1/η)
  - β (KL strength): Controls closeness to reference; too high ignores preferences, too low risks divergence

- Failure signatures:
  - Policy collapse to reference: Check if win rate against reference stays near 50% while KL(π||π_ref) remains very small
  - Unstable training (oscillating loss): κ may be too large; try reducing by 10x
  - No improvement over baselines: Preference model may be weak or β too conservative; verify preference model correlation with human judgment first

- First 3 experiments:
  1. Sanity check on synthetic matrix game: Reproduce Figure 1 results with r=2, Y=100, β=0.01. Verify NashMP outperforms Online IPO after ~1000 steps.
  2. κ sensitivity sweep: Fix all other hyperparameters, vary κ ∈ {0.5, 0.1, 0.01, 0.001} on a small LLM (e.g., 500M params). Plot win rate vs. SFT reference over training steps.
  3. Preference model ablation: Compare using the trained preference model vs. random preferences vs. a Bradley-Terry-compatible synthetic preference model. This tests whether NashMP's gains come from handling intransitivity specifically.

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal convergence rates for finding the β-regularized Nash equilibrium in NLHF, and can lower bounds be established for this setting? Basis: "However, determining the optimal rates remains an open question, as we are not aware of any established lower bound for this specific setting, to the best of our knowledge." Why unresolved: NashMP achieves a linear rate of O((1+2β)^−N/2/β), but without lower bounds it remains unclear whether this rate is optimal or can be further improved. What evidence would resolve it: Establishing information-theoretic lower bounds for the regularized preference game setting, or developing an algorithm with provably faster convergence.

### Open Question 2
Can the memory overhead from maintaining a target network in NashMP be reduced while preserving convergence guarantees? Basis: "We notice that using the target network increases the memory footprint of the model, which is a limitation of our method." Why unresolved: The target network parameters are essential to the two-step Mirror Prox structure and the soft update mechanism, but require storing a second copy of model parameters. What evidence would resolve it: Developing a variant that achieves similar convergence properties without explicitly storing target network parameters, or proving that equivalent performance cannot be achieved without such overhead.

### Open Question 3
How can the gap between theoretically optimal learning rates (η = 2β) and practically effective rates (η = 0.1) be explained and bridged? Basis: The paper notes that the practical choice η = 0.1 is "much larger than the theoretical value η = 2β = 2·10^−4," and that η = β resulted in "overly conservative policy due to extremely strong regularization." Why unresolved: The theoretical analysis assumes exact proximal steps, while practical implementation uses single-gradient approximations, creating a mismatch between theoretical predictions and empirical optimal hyperparameters. What evidence would resolve it: Extending the theoretical analysis to account for inexact proximal step approximations in the deep learning setting, providing guidance on learning rate selection under practical constraints.

## Limitations

- The use of a target network increases memory footprint, requiring storage of both online and target policy parameters
- Theoretical convergence guarantees assume exact gradients and infinite compute, while practical implementation uses stochastic gradients with finite iterations
- All empirical comparisons use the same preference model as judge, potentially introducing bias in win rate evaluations

## Confidence

**Medium**: Theoretical convergence claims assume exact gradients and infinite compute, while practical implementation uses stochastic gradients that may degrade rates.

**Low**: Optimal hyperparameter settings show significant gap between theory (η=2β) and practice (η=0.1), with sensitivity to κ that likely depends on model scale and training duration.

**Medium**: Empirical superiority claims demonstrate statistically significant improvements (2-5% win rate gains) but use same preference model as judge, and computational overhead versus simpler methods needs consideration.

## Next Checks

1. **Convergence rate validation**: Run NashMP on synthetic matrix games with varying sizes (r ∈ {2, 5, 10}, Y ∈ {50, 200, 500}) and plot actual KL divergence vs. theoretical rate (1+2β)^(-N/2)/β. Measure whether empirical convergence matches theoretical predictions and how stochasticity affects the rate.

2. **Hyperparameter sensitivity on larger models**: Test NashMP with varying κ ∈ {0.5, 0.1, 0.01} on a 1B-3B parameter model rather than 9B judge model. This would validate whether the κ=0.1 setting generalizes beyond the specific experimental setup and identify scaling relationships.

3. **Preference model robustness test**: Replace the learned preference model with synthetic Bradley-Terry preferences plus controlled intransitivity noise. Measure whether NashMP's advantages over reward-based methods (Online DPO) persist when preference intransitivity is the primary variable, isolating this effect from other factors like model architecture or training dynamics.