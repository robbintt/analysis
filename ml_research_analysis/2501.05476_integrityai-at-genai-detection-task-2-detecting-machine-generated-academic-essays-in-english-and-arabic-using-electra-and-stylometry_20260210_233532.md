---
ver: rpa2
title: 'IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic
  Essays in English and Arabic Using ELECTRA and Stylometry'
arxiv_id: '2501.05476'
source_url: https://arxiv.org/abs/2501.05476
tags:
- english
- arabic
- training
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting machine-generated
  academic essays in English and Arabic. It proposes using ELECTRA and AraELECTRA
  transformer-based models fine-tuned with stylometric features.
---

# IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic Essays in English and Arabic Using ELECTRA and Stylometry

## Quick Facts
- arXiv ID: 2501.05476
- Source URL: https://arxiv.org/abs/2501.05476
- Reference count: 6
- Primary result: ELECTRA and AraELECTRA models with stylometric features achieved F1-scores of 99.7% (English) and 98.4% (Arabic), ranking 1st-2nd in the competition

## Executive Summary
This study presents a high-performance approach for detecting machine-generated academic essays in English and Arabic using ELECTRA and AraELECTRA transformer models fine-tuned with stylometric features. The system achieved exceptional results in the GenAI Detection Task 2 competition, ranking 2nd out of 26 teams in English detection (99.7% F1-score) and 1st out of 23 teams in Arabic detection (98.4% F1-score). The combination of transformer-based architectures with stylometric features demonstrates a robust solution for maintaining academic integrity in educational contexts. The findings validate the effectiveness of this hybrid approach for AI-generated text detection across multiple languages.

## Method Summary
The approach combines ELECTRA (English) and AraELECTRA (Arabic) transformer models with stylometric features to detect machine-generated academic essays. Custom models were fine-tuned using benchmark datasets specifically curated for the competition. The methodology leverages the pre-trained language understanding capabilities of transformer architectures while incorporating stylometric features that capture writing style patterns. This hybrid approach enables the system to identify subtle linguistic markers that distinguish human-written from AI-generated text. The models were evaluated under controlled competition conditions, demonstrating superior performance compared to other participating teams.

## Key Results
- Achieved 99.7% F1-score in English detection, ranking 2nd out of 26 teams
- Achieved 98.4% F1-score in Arabic detection, ranking 1st out of 23 teams
- Demonstrated the effectiveness of combining transformer models with stylometric features for AI-generated text detection
- Validated the approach across two linguistically distinct languages

## Why This Works (Mechanism)
The effectiveness stems from ELECTRA's pre-training objective that distinguishes real from fake input data, making it particularly suited for detection tasks. The model learns to identify subtle linguistic patterns that differentiate human and machine-generated text. AraELECTRA extends this capability to Arabic by incorporating language-specific pre-training. The integration of stylometric features captures writing style characteristics such as sentence complexity, vocabulary diversity, and structural patterns that are difficult for AI models to replicate authentically. This combination addresses both semantic and stylistic dimensions of text generation, providing complementary signals for accurate detection.

## Foundational Learning
- **Transformer architectures**: Essential for understanding the base model's capability to process sequential data and capture contextual relationships; quick check: verify ELECTRA's token masking and reconstruction mechanism
- **Stylometric feature extraction**: Critical for capturing writing style patterns; quick check: confirm feature extraction covers vocabulary diversity, sentence structure, and syntactic complexity
- **Fine-tuning methodology**: Required for adapting pre-trained models to specific detection tasks; quick check: validate training data balance and validation strategy
- **Cross-linguistic model adaptation**: Important for understanding how pre-trained models transfer across languages; quick check: compare English and Arabic model architectures and training approaches
- **Evaluation metrics**: Necessary for interpreting performance results; quick check: verify F1-score calculation accounts for class imbalance

## Architecture Onboarding

Component Map:
Raw Text -> Pre-processing -> Stylometric Feature Extraction -> ELECTRA/AraELECTRA Model -> Classification Output

Critical Path:
The critical path flows from raw input text through pre-processing and stylometric feature extraction into the transformer model, where features are combined with learned representations for final classification. The most computationally intensive step is the transformer inference, while the most critical decision point is the integration of stylometric features with model embeddings.

Design Tradeoffs:
The approach balances model complexity with detection accuracy by leveraging pre-trained transformers rather than training from scratch. The tradeoff involves increased computational requirements during inference versus achieving state-of-the-art performance. Stylometric feature engineering adds preprocessing overhead but provides complementary detection signals that pure transformer models might miss.

Failure Signatures:
Potential failures include overfitting to benchmark datasets, reduced performance on out-of-domain text, and sensitivity to input preprocessing variations. The model may struggle with hybrid texts containing both human and AI-generated sections, and performance could degrade with non-academic writing styles or highly specialized domain content.

First Experiments:
1. Test model performance on a held-out validation set with varying text lengths and complexity levels
2. Evaluate detection accuracy on texts with mixed human-AI authorship
3. Measure computational efficiency and inference time under different hardware configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation was conducted only on benchmark datasets from the competition, limiting generalizability to real-world scenarios
- The approach focuses exclusively on English and Arabic languages, with unknown performance on other linguistic contexts
- The specific contribution of stylometric features versus transformer models to overall performance is not explicitly quantified
- Computational resource requirements for fine-tuning and inference may pose practical constraints for some institutions

## Confidence
- High confidence: The reported F1-scores and competition rankings are verifiable through the shared task results
- Medium confidence: The methodology is sound, but the generalizability to broader contexts needs validation
- Medium confidence: The effectiveness of the stylometric feature integration is demonstrated but not fully isolated

## Next Checks
1. Evaluate the models on out-of-distribution datasets that include diverse writing styles, topics, and quality levels to assess robustness beyond benchmark data
2. Conduct cross-linguistic testing by applying the English-trained models to other languages or testing on multilingual datasets to determine transferability
3. Implement a pilot deployment in an actual academic setting to measure real-world detection accuracy, false positive rates, and computational efficiency under practical constraints