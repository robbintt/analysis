---
ver: rpa2
title: 'Privacy-Preserving Transformers: SwiftKey''s Differential Privacy Implementation'
arxiv_id: '2505.05648'
source_url: https://arxiv.org/abs/2505.05648
tags:
- data
- training
- transformer
- size
- typing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a differential privacy (DP)-trained transformer
  model for language modeling in SwiftKey, achieving small but consistent gains in
  next-word-prediction accuracy compared to the production GRU with only a modest
  increase in memory and speed requirements. The authors scaled down a GPT2 architecture
  to fit on-device constraints, trained a seed model on general data, and then DP-finetuned
  it on user typing data to address privacy concerns.
---

# Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation

## Quick Facts
- **arXiv ID**: 2505.05648
- **Source URL**: https://arxiv.org/abs/2505.05648
- **Reference count**: 5
- **Primary result**: DP-trained transformer achieves small but consistent gains in next-word-prediction accuracy versus production GRU with modest memory/speed increases

## Executive Summary
This paper presents a differential privacy (DP)-trained transformer model for language modeling in SwiftKey, achieving small but consistent gains in next-word-prediction accuracy compared to the production GRU with only a modest increase in memory and speed requirements. The authors scaled down a GPT2 architecture to fit on-device constraints, trained a seed model on general data, and then DP-finetuned it on user typing data to address privacy concerns. The transformer was integrated using ONNX for flexibility and efficiency. Offline experiments showed that the transformer model achieved slightly better performance than the production GRU, with improvements in NWP and accuracy, particularly for newer users.

## Method Summary
The authors implemented a two-stage training approach: first pre-training a scaled-down GPT-2 decoder (4 layers, 4 heads, 512 hidden, 128 embedding) on general web/social data, then DP fine-tuning on anonymized user typing data using DP-SGD with ε=14, δ=10⁻⁸. They used relaxed author sampling to prevent heavy users from dominating gradients, applied 1-byte quantization for ~6MB model size, and deployed via ONNX integration with the Fluency engine. The approach addressed privacy concerns while maintaining competitive accuracy on next-word prediction tasks.

## Key Results
- Transformer with 4 layers outperforms production GRU in offline NWP accuracy tests
- DP fine-tuning causes measurable but acceptable accuracy degradation versus non-DP training
- Model size remains within 6MB on-device budget with 1-byte quantization
- Relaxed author sampling improves DP training loss compared to per-user sampling
- 4-layer architecture provides good latency tradeoff; 12-layer version too slow for production

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training (pre-training on general data, then DP fine-tuning on user typing data) preserves privacy while maintaining competitive accuracy.
- Mechanism: A seed model learns general language patterns from public web/twitter data. DP-SGD then fine-tunes on private typing data with gradient clipping and calibrated noise injection. The noise scale is σ = C × √(2log(1.25/δ))/ε, where C is the clipping norm. This bounds the influence of any single user's data on the final model.
- Core assumption: Typing data distribution is sufficiently related to general language data that transfer learning remains effective despite DP noise.
- Evidence anchors:
  - [abstract] "two stage training process that builds a seed model on general data and DP finetunes it on typing data"
  - [section 3.4.2] DP parameters fixed at δ = 10⁻⁸, target ε = 14; Figure 2 shows DP vs non-DP loss curves with clear but acceptable degradation
  - [corpus] Related work on DP for clinical LLMs (arXiv:2511.14936) confirms DP fine-tuning pipeline viability but notes domain-specific tuning is required
- Break condition: If ε target is too strict (<1) or user data is extremely heterogeneous, DP noise may overwhelm signal, making pre-training knowledge irretrievable.

### Mechanism 2
- Claim: Scaling down GPT-2 to 4 layers with constrained embedding dimension yields transformer performance superior to same-size GRU while staying within 6MB on-device budget.
- Mechanism: The architecture uses 4 layers × 4 heads × 512 hidden dim with 128-dim embeddings (vs 256). Embedding matrix dominates model size; constraining it has minimal accuracy impact. One-byte quantization further compresses. Transformer's parallel attention captures longer-range dependencies better than sequential GRU hidden states.
- Core assumption: On-device latency budget can tolerate modest transformer overhead (no KV-cache currently).
- Evidence anchors:
  - [section 3.3] "We build a 4-layer model with 4 attention heads and a hidden dimension of 512... very small difference in performance between embedding size of 128 and 256"
  - [section 3.5, Figure 3] 4-layer transformer shows graceful mean inference time increase vs GRU; 12-layer is dramatically slower
  - [corpus] Corpus evidence on attention optimization (arXiv:2503.05840 "Slim attention") suggests further memory reduction is possible but not evaluated here
- Break condition: If context lengths grow beyond ~25 tokens without KV-cache, inference time degrades non-gracefully (95th percentile spikes in Figure 3).

### Mechanism 3
- Claim: Relaxed author sampling (uniform sentence probability, user appears ≤once per batch) improves DP fine-tuning by preventing heavy users from dominating gradient updates.
- Mechanism: Raw user data is highly skewed (some users: 67K sentences; others: 20). Standard per-user sampling lets heavy users dominate batches, effectively training on fewer distinct users—worse privacy-utility tradeoff. Relaxed sampling ensures gradient noise reflects diverse user distribution.
- Core assumption: User-level DP (not sample-level) is the correct privacy boundary for keyboard data.
- Evidence anchors:
  - [section 4.2] "sampling using the user distribution will result in batches that are dominated by a limited number of users... relaxed author sampling... reduced the evaluation loss and improved hit rate as shown in Figure 7"
  - [abstract] Implicitly referenced via DP training on user typing data
  - [corpus] No direct corpus comparison found; this is a domain-specific engineering contribution
- Break condition: If users have highly distinct dialects/behaviors and data per user is sparse, per-user single-sample constraint may slow convergence excessively.

## Foundational Learning

- **Concept**: Differential Privacy (ε, δ)-guarantees
  - Why needed here: Understanding how ε controls privacy-utility tradeoff is essential for tuning DP-SGD hyperparameters (Section 4).
  - Quick check question: If ε is reduced from 14 to 1, would noise increase or decrease, and what happens to model accuracy?

- **Concept**: GPT-2-style decoder-only transformer
  - Why needed here: The architecture is a scaled-down GPT-2; understanding attention, layer norm, and positional embeddings is prerequisite for modifications.
  - Quick check question: Why does a decoder-only architecture suit next-word prediction better than encoder-decoder?

- **Concept**: ONNX model deployment
  - Why needed here: The production integration path relies on ONNX conversion for cross-platform inference (Section 3.5).
  - Quick check question: What information is lost when exporting a PyTorch transformer to ONNX without KV-cache support?

## Architecture Onboarding

- **Component map**:
  - Pre-training data (common crawl + twitter) -> Vocabulary construction (20K word-based) -> Seed model pre-training
  - Anonymized user typing data (with entity placeholders) -> DP fine-tuning (ε=14, δ=10⁻⁸) -> ONNX export
  - ONNX model -> Fluency engine integration -> On-device inference

- **Critical path**:
  1. Vocabulary construction (Section 3.2) gates both pre-training and fine-tuning
  2. Seed model pre-training convergence (Figure 1) determines transfer quality
  3. DP fine-tuning hyperparameter search (Section 4) determines final accuracy
  4. ONNX conversion + latency validation before flight

- **Design tradeoffs**:
  - Vocabulary size (10K vs 20K): 20K reduces OOV but increases model size ~20-25%
  - Batch size vs ε budget: Larger batches improve loss but exhaust ε faster (Figure 6)
  - Clipping norm: Lower = less noise but gradient distortion; tuned to 0.01 (Figure 8)
  - Layers: 4L is sweet spot; 12L improves accuracy but latency unacceptable (Figure 3)

- **Failure signatures**:
  - Edit rate gap between offline and flight (Section 3.6.2): Dynamic user model overrides static LM gains for established users—slice by user age to detect
  - Inference latency spike at 95th percentile: Indicates missing KV-cache for longer contexts
  - DP/non-DP accuracy gap too large (Figure 2): Check ε target, data volume, or sampling strategy

- **First 3 experiments**:
  1. **Vocabulary ablation**: Train 4L transformer with 10K vs 20K vocab on same data; measure OOV rate and NWP on both snippet and 50K BUS test sets (replicate Table 2).
  2. **DP hyperparameter sweep**: Fix architecture, vary (batch_size, learning_rate, clipping_norm) combinations; plot eval loss vs ε consumed to find Pareto frontier (replicate Figures 6, 8).
  3. **Latency profiling**: Deploy 4L and 12L transformers via ONNX to Fluency; measure mean and 95th-percentile inference time on 630-sample typing dataset; confirm 4L stays within production GRU +20% overhead (replicate Figure 3).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can subword vocabulary (e.g., BPE) significantly reduce the OOV rate and improve next-word prediction accuracy compared to the current 20K word-based vocabulary, while maintaining on-device memory constraints?
  - Basis in paper: [explicit] "One way to overcome this, that we will explore in future work, is to use a sub-word vocabulary."
  - Why unresolved: The paper only experiments with word-based vocabularies, which show high OOV rates (23-24%) even with backoff unigram models.
  - What evidence would resolve it: Experiments comparing subword vocabularies against word-based approaches on the same test sets, measuring NWP, accuracy, and quantized model size.

- **Open Question 2**: Can knowledge distillation from larger transformer models (e.g., 12-layer) to smaller ones (4-layer) capture the performance gains observed when scaling up, while meeting on-device deployment constraints?
  - Basis in paper: [explicit] "Areas of future work include training larger models and distilling them into the required size since we observed significant gains by increasing the model size."
  - Why unresolved: The paper demonstrates 12-layer transformers outperform 4-layer models but are too slow for production; distillation is proposed but not tested.
  - What evidence would resolve it: Comparative experiments showing distilled 4-layer models versus directly trained 4-layer models on NWP and accuracy metrics.

- **Open Question 3**: What training techniques or privacy mechanisms can effectively reduce the performance gap between DP-finetuned and non-DP-finetuned transformer models?
  - Basis in paper: [explicit] "Areas of future work include... bridging the gap between the non-DP and DP training."
  - Why unresolved: Figure 2 shows clear performance degradation from DP finetuning; optimizations like gradient clipping and relaxed author sampling only partially mitigate this.
  - What evidence would resolve it: Novel techniques that narrow the DP/non-DP gap while maintaining formal (ε, δ)-differential privacy guarantees.

- **Open Question 4**: Does relative positional encoding provide measurable benefits over absolute positional encoding for context lengths exceeding 25 tokens in on-device keyboard language modeling?
  - Basis in paper: [inferred] Section 5 reports "no differences in results between relative and absolute positional encoding when testing with context lengths up to 25 words," but typical SwiftKey sessions may involve longer contexts.
  - Why unresolved: The experiments were limited to 25-word contexts; the paper notes FIRE encoding shows robustness to position shifts, suggesting potential benefits at longer lengths.
  - What evidence would resolve it: Benchmarking FIRE or similar relative encodings against absolute encodings on longer test sequences (25-100+ tokens).

## Limitations

- Production deployment benefits remain unverified due to longer experimentation timelines
- DP fine-tuning shows measurable accuracy degradation versus non-DP training
- No KV-cache implementation could cause latency degradation for longer contexts
- Complex interactions with existing production systems may limit static LM improvements for established users

## Confidence

- **High Confidence**: The architectural scaling-down of GPT-2 to 4 layers with constrained parameters is well-specified and the memory/speed calculations are straightforward. The transformer architecture itself is a proven approach for language modeling.
- **Medium Confidence**: The DP-SGD implementation using standard frameworks (Opacus/dp-transformers) follows established patterns. The hyperparameter choices (ε=14, δ=10⁻⁸, clipping=0.01) are reasonable though not extensively justified. The relaxed author sampling shows clear offline improvements but lacks theoretical grounding.
- **Low Confidence**: Claims about production deployment benefits are speculative. The edit rate improvements for newer users versus established users suggest complex interactions with existing systems that aren't fully characterized. The paper doesn't address potential failure modes in diverse linguistic contexts or with different user populations.

## Next Checks

1. **Production Flight Validation**: Deploy the DP-trained transformer model to a subset of users and measure actual edit rate improvements versus the production GRU baseline across different user cohorts (new vs established users, different languages, usage patterns). This should include A/B testing with statistical significance analysis.

2. **DP Privacy-Utility Frontier Analysis**: Systematically vary ε from 1 to 20 while keeping other parameters fixed, measuring both accuracy metrics and estimated privacy loss. Plot the Pareto frontier to identify optimal operating points and characterize sensitivity to ε changes.

3. **KV-Cache Implementation Impact**: Implement and benchmark KV-cache support in the ONNX deployment to evaluate latency improvements for longer contexts (50-100 tokens). Measure the trade-off between memory usage increase and inference time reduction, particularly for the 95th percentile latency cases.