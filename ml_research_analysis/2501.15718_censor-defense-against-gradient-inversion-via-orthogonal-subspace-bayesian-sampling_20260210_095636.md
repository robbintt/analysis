---
ver: rpa2
title: 'CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian
  Sampling'
arxiv_id: '2501.15718'
source_url: https://arxiv.org/abs/2501.15718
tags:
- gradient
- attacks
- censor
- gradients
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CENSOR, a defense against gradient inversion
  attacks in federated learning that leverages orthogonal subspace Bayesian sampling
  to protect client data privacy. The method works by sampling gradients from a subspace
  orthogonal to the original gradient, then selecting the optimal gradient using cold
  posteriors to minimize loss while preserving privacy.
---

# CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling

## Quick Facts
- arXiv ID: 2501.15718
- Source URL: https://arxiv.org/abs/2501.15718
- Reference count: 40
- Primary result: CENSOR achieves 0.0507 MSE↑, 0.7610 LPIPS↑, 13.32 PSNR↓, and 0.009 SSIM↓ against GIFD attack on ImageNet while maintaining model accuracy

## Executive Summary
This paper presents CENSOR, a defense against gradient inversion attacks in federated learning that leverages orthogonal subspace Bayesian sampling to protect client data privacy. The method works by sampling gradients from a subspace orthogonal to the original gradient, then selecting the optimal gradient using cold posteriors to minimize loss while preserving privacy. The core idea is that sampling from an orthogonal subspace prevents attackers from recovering the original gradient direction, while cold posteriors ensure the selected gradient maintains model utility.

Experimental results show CENSOR outperforms state-of-the-art defenses across multiple metrics. Against five gradient inversion attacks (IG, GI, GGL, GIAS, GIFD) on ImageNet, FFHQ, and CIFAR-10 datasets, CENSOR achieves significant improvements in privacy protection while maintaining model accuracy. The method remains effective across different batch sizes and training rounds, and demonstrates robustness against adaptive attacks using Expectation Over Transformation (EOT).

## Method Summary
CENSOR operates at the client side in federated learning by first computing the original gradient G₀, then generating T candidate gradients in an orthogonal subspace through Gram-Schmidt orthogonalization and layer-wise normalization. Each candidate gradient is evaluated for its loss impact, with the gradient minimizing loss selected for submission. The method uses cold posterior sampling (temperature M → 0) to concentrate selection around low-loss solutions without gradient-based optimization, providing defense against gradient inversion attacks while maintaining model convergence.

## Key Results
- CENSOR shows 0.0507 MSE↑, 0.7610 LPIPS↑, 13.32 PSNR↓, and 0.009 SSIM↓ against GIFD attack on ImageNet
- Maintains model accuracy ≈ 86% on CIFAR-10 non-i.i.d. conditions matching vanilla training
- Outperforms four baseline defenses (Noise, Clipping, Sparsification, Soteria) across all metrics
- Demonstrates robustness against EOT adaptive attacks, with metrics remaining stable compared to non-EOT cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting gradients onto a subspace orthogonal to the original gradient prevents attackers from recovering the original gradient direction.
- Mechanism: For each layer, sample a random gradient g_r ~ N(0, εI), then compute the orthogonal component g_o = g_r - proj_{g_l}(g_r) = g_r - (⟨g_r, g_l⟩/⟨g_l, g_l⟩)g_l, where g_l is the original layer gradient. This ensures ⟨g_o, g_l⟩ = 0.
- Core assumption: The model has sufficiently high dimensionality (m parameters) such that the orthogonal subspace (m-k dimensions for k training instances) provides enough degrees of freedom to find loss-reducing directions.
- Evidence anchors:
  - [abstract] "perturb gradients within a subspace orthogonal to the original gradient"
  - [Section IV-A] "S⊥_k = span{∀w ∈ V : ⟨w, ∇l(f(Xj,k, θ)), c)⟩ = 0}"
  - [corpus] SVDefense uses SVD-based orthogonal decomposition for similar defense purposes, suggesting orthogonal projection is a viable strategy.
- Break condition: If the model is severely underparameterized (m ≈ k), the orthogonal subspace collapses and may not contain useful gradient directions.

### Mechanism 2
- Claim: Cold posterior sampling over the orthogonal subspace selects gradients that minimize training loss while maintaining privacy protection.
- Mechanism: Sample T candidates from the orthogonal subspace, evaluate each with acceptance criterion P(accept) = min(1, P(D_k|θ_τ + G)^M · P(θ_τ + G) / P(D_k|θ_τ)^M · P(θ_τ)), where M ≈ 0 is the cold temperature. Select G* = argmax_{G∈{G_1,...,G_T}} P(D_k|θ_τ + G).
- Core assumption: Loss landscapes of overparameterized neural networks contain multiple descent directions; at least one orthogonal direction reduces loss sufficiently.
- Evidence anchors:
  - [abstract] "cold posteriors ensure the selected gradient maintains model utility"
  - [Section IV-A] "P_M(θ|D) ∝ P(D|θ)^M P(θ) is highly concentrated around the MAP solution"
  - [corpus] Weak direct corpus evidence for cold posteriors in FL defense; this appears novel to CENSOR.
- Break condition: If all orthogonal directions increase loss substantially (e.g., near sharp minima with limited curvature), the selected gradient may harm convergence.

### Mechanism 3
- Claim: Layer-wise normalization preserves gradient magnitude while changing direction, maintaining optimization dynamics.
- Mechanism: After orthogonal projection, normalize g_o^l = g_o^l · (||g_l||_2 / ||g_o^l||_2) for each layer, ensuring the protected gradient has the same L2 norm as the original.
- Core assumption: Gradient magnitude per layer is important for learning rate scaling and batch normalization statistics.
- Evidence anchors:
  - [Algorithm 1, Lines 31-36] Explicit normalization procedure
  - [Table VII] Layer-wise operation slightly outperforms entire-gradient application
  - [corpus] No direct corpus comparison; layer-wise vs whole-model perturbation remains an empirical design choice.
- Break condition: If layer-wise normalization disrupts cross-layer gradient correlations critical for certain architectures (e.g., batch normalization layers), convergence may degrade.

## Foundational Learning

- Concept: Federated Learning (FL) and gradient aggregation
  - Why needed here: CENSOR operates at the client side before gradient submission; understanding FL's aggregation step (θ_{τ+1} = θ_{τ} - η · Σ_k G_k^τ) clarifies where defense is applied.
  - Quick check question: Can you explain why the server never sees raw data but can still reconstruct it from gradients?

- Concept: Gradient Inversion Attacks (stochastic optimization and GAN-based)
  - Why needed here: Defense design is attack-aware; understanding that attackers minimize D(F(X'), g) helps explain why orthogonal gradients break this optimization.
  - Quick check question: What is the difference between IG/GI (stochastic) and GGL/GIFD (GAN-based) attacks in terms of prior knowledge required?

- Concept: Bayesian posterior sampling and cold posteriors
  - Why needed here: CENSOR's selection mechanism is motivated by Bayesian principles; cold posteriors (M → 0) concentrate sampling near MAP solutions without gradient-based optimization.
  - Quick check question: Why does setting temperature M close to zero make the posterior "sharper" around good solutions?

## Architecture Onboarding

- Component map:
  Original Gradient G_0 → [Orthogonal Projection Layer-wise] → G_t
                         ↓
  Random Sample g_r ~ N  → [Gram-Schmidt: g_o = g_r - proj(g_r, g_l)]
                         ↓
                    [Normalization: g_o · ||g_l||/||g_o||] → G_t^N
                         ↓
  For T trials → [Loss Evaluation: L(f_{θ-η·G_t^N}, D_k)] → Select G* with min loss
                         ↓
                    Protected Gradient G* → Submit to server

- Critical path:
  1. Compute original gradient G_0 = ∇_θ f(D_k, θ_τ)
  2. For t=1 to T trials: Generate orthogonal + normalized candidate G_t^N
  3. Evaluate loss for each candidate
  4. Return G* with lowest loss

- Design tradeoffs:
  - T (number of trials): Higher T improves loss reduction but increases client-side compute. Paper finds T=20 sufficient (Figure 9).
  - Temperature M: Colder posteriors (M → 0) favor lower-loss gradients but may approach original gradient direction; paper uses selection rather than explicit M tuning.
  - Layer-wise vs whole-model: Layer-wise provides slightly better protection (Table VII) but adds implementation complexity.

- Failure signatures:
  - Label leakage: If orthogonal gradient still correlates with final FC layer gradients, label inference (via sign of ∇W_FC^i) may succeed. CENSOR corrupts this correlation.
  - EOT adaptive attack: Averaging over multiple trials cannot recover original gradient because each trial samples from high-dimensional orthogonal subspace (m-1 dimensions).

- First 3 experiments:
  1. **Sanity check**: On CIFAR-10 with ResNet-18, batch size 1, epoch 0, run CENSOR with T=20 against IG attack. Verify MSE increases from ~0.002 (no defense) to ~0.094 (Table I).
  2. **Convergence test**: Train FL for 2000 rounds on CIFAR-10 non-i.i.d. with 100 clients, apply CENSOR. Verify final accuracy ≈ 86% matches vanilla training (Figure 7).
  3. **Adaptive attack robustness**: Apply GIFD + EOT against CENSOR on ImageNet. Verify metrics remain similar to non-EOT case (Table III), confirming orthogonal subspace dimensionality prevents gradient averaging attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can formal differential privacy guarantees be established for CENSOR's orthogonal subspace sampling approach, and if so, what privacy-utility trade-offs would emerge?
- Basis in paper: [inferred] The paper provides strong empirical privacy protection (e.g., 0.0507 MSE, 0.7610 LPIPS on ImageNet against GIFD) but offers no formal privacy bounds. The discussion of differential privacy (Definition II.1) frames existing methods' limitations but CENSOR itself lacks such theoretical grounding.
- Why unresolved: The orthogonal subspace mechanism introduces randomness fundamentally different from standard DP noise injection, making existing DP analysis frameworks potentially inapplicable.
- What evidence would resolve it: A theoretical analysis bounding the mutual information between original and protected gradients, or empirical privacy accounting across varying subspace dimensions.

### Open Question 2
- Question: How does CENSOR perform on non-image federated learning tasks, such as natural language processing or tabular data domains?
- Basis in paper: [explicit] Appendix E states: "it can be explored in the future" regarding GAN-based adaptive attacks, and all experiments are limited to image datasets (ImageNet, FFHQ, CIFAR-10) with ResNet-18.
- Why unresolved: The defense relies on gradient properties in high-dimensional spaces and cold posterior sampling; these mechanisms may interact differently with text models (transformers) or tabular networks where gradient structures differ substantially.
- What evidence would resolve it: Experiments applying CENSOR to federated NLP tasks (e.g., next-word prediction with transformer models) or healthcare tabular data, measuring both inversion resistance and model utility.

### Open Question 3
- Question: Can adaptive attacks that train specialized GANs to invert CENSOR's orthogonal projection overcome the defense?
- Basis in paper: [explicit] Appendix E explicitly states: "Although it is non-trivial to employ the gradient as a latent space to train a GAN to reconstruct the original gradients effectively in the context of CENSOR, it can be explored in the future."
- Why unresolved: Current adaptive attacks (EOT) average over random transformations but don't learn the specific structure of CENSOR's orthogonal subspace sampling. A learned inversion model might exploit statistical patterns in the protected gradients.
- What evidence would resolve it: Development and evaluation of a GAN trained to map CENSOR-protected gradients back toward original gradient directions, tested across multiple sampling trials and temperature settings.

### Open Question 4
- Question: What is the optimal selection strategy for the cold posterior temperature parameter M across different model architectures and training stages?
- Basis in paper: [inferred] The paper mentions "typically 0 < M ≪ 1" and uses cold posteriors for gradient selection, but provides no systematic study of temperature effects or adaptive selection criteria. Ablation studies focus on trial count (T), not temperature.
- Why unresolved: The temperature controls the concentration around MAP solutions, creating a privacy-utility trade-off, but its optimal value may depend on model dimensionality, gradient magnitude, and training progress.
- What evidence would resolve it: A parameter sweep across temperature values with analysis of resulting privacy metrics (MSE, LPIPS) and model convergence rates, potentially with an adaptive schedule based on gradient statistics.

## Limitations

- Theoretical foundation gaps: The paper lacks formal privacy guarantees and theoretical analysis of why orthogonal subspace sampling provides privacy
- Limited attack evaluation: Only GIFD + EOT is tested for adaptive attacks; other combinations and stronger strategies remain unexplored
- Dataset/task constraints: All experiments use image datasets with ResNet-18; performance on other architectures, modalities, and tasks is unknown

## Confidence

- High confidence in privacy metrics (MSE↑, LPIPS↑, PSNR↓, SSIM↓): Experimental results are comprehensive and show consistent improvements across multiple attacks and datasets
- Medium confidence in convergence preservation: While Table I and Figure 7 show accuracy retention, the convergence speed and final accuracy trade-offs under non-i.i.d. conditions need more extensive validation
- Low confidence in adaptive attack robustness: Table III shows EOT resistance, but only GIFD + EOT is tested; other attack combinations and stronger adaptive strategies (e.g., Bayesian optimization over sampling parameters) remain unexplored

## Next Checks

1. **Dimensionality Stress Test**: Evaluate CENSOR on severely underparameterized models (e.g., ResNet-18 on CIFAR-10 with B=100) to verify orthogonal subspace retains useful directions. Measure loss increase and convergence degradation.

2. **Cross-Layer Dependency Analysis**: Apply CENSOR to architectures with batch normalization and layer normalization layers. Monitor training stability and final accuracy compared to layer-wise vs whole-model perturbation.

3. **Adaptive Attack Extension**: Implement EOT + Bayesian optimization against CENSOR, optimizing both attack parameters and sampling trial count T. Measure whether attacker can recover original gradients or significantly reduce privacy metrics.