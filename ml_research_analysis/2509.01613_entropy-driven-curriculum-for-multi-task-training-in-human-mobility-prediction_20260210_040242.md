---
ver: rpa2
title: Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction
arxiv_id: '2509.01613'
source_url: https://arxiv.org/abs/2509.01613
tags:
- mobility
- prediction
- human
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified training framework that integrates
  entropy-driven curriculum learning with multi-task learning for human mobility prediction.
  The method quantifies trajectory predictability using normalized Lempel-Ziv entropy
  and organizes training from simple to complex trajectories, achieving up to 2.92-fold
  faster convergence.
---

# Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction

## Quick Facts
- **arXiv ID**: 2509.01613
- **Source URL**: https://arxiv.org/abs/2509.01613
- **Reference count**: 40
- **Primary result**: Achieves GEO-BLEU of 0.354 and DTW of 26.15 on YJMob100K with 2.92× faster convergence using entropy-driven curriculum

## Executive Summary
This paper introduces a unified training framework that combines entropy-driven curriculum learning with multi-task learning for human mobility prediction. The method quantifies trajectory predictability using normalized Lempel-Ziv entropy to organize training from simple to complex trajectories, achieving up to 2.92-fold faster convergence. The framework jointly optimizes location prediction with auxiliary distance and direction estimation tasks, leveraging complementary supervision signals. Experimental results demonstrate state-of-the-art performance on the YJMob100K dataset while maintaining strong zero-shot generalization capability across different cities with significantly fewer parameters than competing methods.

## Method Summary
The approach uses a MoBERT transformer architecture (8-layer encoder-only, 8 heads, 256-dim) with multi-task learning objectives for location, distance, and direction prediction. Training follows an entropy-driven curriculum: trajectories are sorted by normalized Lempel-Ziv entropy and trained in three progressive stages (H<0.4 for 3 days, H<0.65 for 7 days, then all trajectories for 15 days). The curriculum accelerates convergence by 2.92× compared to standard training. The model uses a feature interaction module with multi-head self-attention over stacked feature embeddings and achieves superior performance through joint optimization of primary and auxiliary tasks with specific weighting parameters (λ₁=0.5, λ₂=0.8).

## Key Results
- Achieves GEO-BLEU of 0.354 and DTW of 26.15 on YJMob100K dataset
- Demonstrates 2.92-fold faster convergence to validation loss ≤2.1
- Shows strong zero-shot generalization capability across different cities
- Uses significantly fewer parameters than competing methods while maintaining state-of-the-art performance

## Why This Works (Mechanism)
The entropy-driven curriculum works by quantifying trajectory predictability through normalized Lempel-Ziv entropy, which measures the complexity and regularity of movement patterns. By training on increasingly complex trajectories, the model first learns general movement patterns on simpler data before tackling harder cases, reducing catastrophic forgetting and improving generalization. The multi-task learning component provides complementary supervision signals through distance and direction prediction, which helps the model better understand spatial relationships and movement dynamics beyond just location coordinates.

## Foundational Learning
- **Normalized Lempel-Ziv entropy**: Quantifies trajectory predictability by measuring compressibility of movement sequences; needed to sort trajectories by complexity for curriculum learning
- **Multi-task learning weighting**: Balances primary location prediction with auxiliary distance/direction tasks; needed to prevent auxiliary tasks from overwhelming main objective
- **Curriculum scheduling**: Three-stage progression based on entropy thresholds; needed to gradually increase task difficulty during training
- **Feature interaction module**: Uses multi-head self-attention over stacked embeddings; needed to capture complex relationships between user, time, location, and POI features
- **Distance/direction discretization**: Converts continuous movement metrics into classification bins; needed to align with dataset structure and training framework
- **Zero-shot transfer**: Evaluating trained model on unseen cities without fine-tuning; needed to assess generalization beyond training distribution

## Architecture Onboarding

**Component Map**: Input features -> Feature Interaction Module (MHSA) -> Transformer Encoder (8 layers) -> Multi-task Heads (Location/Distance/Direction)

**Critical Path**: Feature embeddings → MHSA → Transformer layers → Prediction heads → Loss computation

**Design Tradeoffs**: Discrete classification for auxiliary tasks vs. regression (simplicity vs. precision), fixed curriculum vs. adaptive pacing (control vs. responsiveness), parameter efficiency vs. model capacity

**Failure Signatures**: Curriculum doesn't accelerate convergence (check entropy distribution and augmentation), MTL degrades location accuracy (adjust task weights), poor GEO-BLEU with good DTW (check grid cell generation and evaluation protocol)

**First Experiments**:
1. Implement normalized LZ entropy estimator and verify 70% user concentration between 0.4-0.65 entropy values matches reported Gaussian-like distribution
2. Build MoBERT with feature interaction module and test with different FFN dimension configurations (256→512→256, 256→1024→256)
3. Train curriculum stages on augmented data and validate 2.92× faster convergence to validation loss ≤2.1

## Open Questions the Paper Calls Out
- **Fixed curriculum comparison**: How does the manually defined three-stage curriculum compare to adaptive or self-paced learning strategies that could better align with the model's dynamic learning capacity?
- **Auxiliary task formulation**: Would formulating distance and direction tasks as regression problems rather than discrete classification improve spatial prediction accuracy by preserving precise movement dynamics?
- **Multi-city aggregation**: Can the entropy-driven curriculum be extended to effectively aggregate training data from multiple heterogeneous cities to avoid conflicting patterns that plague competing methods?

## Limitations
- Critical architectural details missing for feature interaction module (FFN dimensions, positional encoding, dropout)
- Results specific to YJMob100K dataset with particular grid resolution (500m) and time slot configuration
- Fixed, manually defined curriculum may not optimally align with model's dynamic learning capacity

## Confidence
- **High Confidence**: Entropy-driven curriculum accelerates convergence, Multi-task learning improves performance, Zero-shot generalization capability
- **Medium Confidence**: State-of-the-art performance claims, Parameter efficiency advantages
- **Low Confidence**: None identified

## Next Checks
1. Replicate entropy distribution analysis (Figure 6) to verify 70% user concentration between 0.4-0.65 entropy values before implementing curriculum stages
2. Implement feature interaction module with multiple FFN dimension configurations and validate against reported convergence behavior and final GEO-BLEU scores
3. Train on YJMob100K subset from one city and evaluate on unseen city's data to verify strong generalization capability while maintaining identical preprocessing