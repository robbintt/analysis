---
ver: rpa2
title: 'Analogical Reasoning Inside Large Language Models: Concept Vectors and the
  Limits of Abstraction'
arxiv_id: '2503.03666'
source_url: https://arxiv.org/abs/2503.03666
tags:
- concepts
- representations
- prompt
- task
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) encode
  invariant conceptual representations. The authors compare Function Vectors (FVs),
  derived via activation patching, with Concept Vectors (CVs), identified using Representational
  Similarity Analysis (RSA).
---

# Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction

## Quick Facts
- arXiv ID: 2503.03666
- Source URL: https://arxiv.org/abs/2503.03666
- Authors: Gustaw Opiełka; Hannes Rosenbusch; Claire E. Stevenson
- Reference count: 5
- Key outcome: Concept Vectors (CVs) are more invariant than Function Vectors (FVs) and can causally steer model behavior, but LLMs lack invariant linear representations for abstract concepts like "previous" and "next."

## Executive Summary
This paper investigates whether Large Language Models (LLMs) encode invariant conceptual representations by comparing Function Vectors (FVs) and Concept Vectors (CVs). FVs, derived via activation patching, capture task attributes rather than pure concepts, clustering by format changes (open-ended vs. multiple-choice) rather than underlying meaning. In contrast, CVs—identified using Representational Similarity Analysis (RSA)—are more invariant and function as feature detectors independent of final outputs. However, for abstract concepts like "previous" and "next," no invariant linear representations emerge, suggesting LLMs lack reusable abstractions for certain relational concepts, limiting their analogical reasoning capabilities.

## Method Summary
The authors extract and compare FVs and CVs from Llama-3.1 models using activation patching and RSA respectively. FVs are constructed by summing mean activations from attention heads ranked by Causal Indirect Effect (CIE) across corrupted/clean prompts. CVs are identified by computing Spearman correlations between representational similarity matrices (RSMs) and binary design matrices for task attributes, selecting top heads by RSA alignment scores (Φconcept), and summing their mean activations. Causal interventions add scaled CVs (10x) at early-to-mid layers (14 for 8B, 31 for 70B) to test portability and steering ability across different input distributions.

## Key Results
- FVs encode task attributes (task_type, response_type) rather than pure concepts, clustering by format (MC vs. open-ended) instead of underlying meaning
- CVs for verbal concepts (antonym, categorical, translation) are more invariant across input transformations and function as early-layer feature detectors
- No invariant linear representations emerge for abstract concepts like "previous" and "next," correlating with poor generalization in letter-string analogy tasks
- CVs can causally steer behavior with better cross-format portability than FVs but weaker zero-shot performance

## Why This Works (Mechanism)

### Mechanism 1
CVs function as early-layer feature detectors that encode verbal concepts invariantly across input format changes. A small set of attention heads in early-to-mid layers (identified via RSA alignment scores Φconcept) sum their outputs to form a CV. This representation remains stable across transformations like language changes (EN→FR) but degrades with format shifts (open-ended→MC, similarity drops from 0.8 to 0.7). The Linear Representation Hypothesis holds—concepts are encoded as directions in activation space that can be captured via linear methods.

### Mechanism 2
Activation patching localizes causally-important components but fails to isolate latent conceptual information. Patching computes Causal Indirect Effect (CIE) by transplanting clean activations into corrupted runs. This identifies heads that affect output probability but captures task attributes (task_type, response_type) rather than pure concepts—FVs cluster by format (MC vs. open-ended) not by underlying concept. The assumption is that heads with high causal effect on output carry the most task-relevant information.

### Mechanism 3
CVs can causally steer behavior when added to hidden states, with better cross-format portability than FVs but weaker zero-shot performance. Adding CVs (scaled 10x) to hidden states at specific layers (14 for 8B, 31 for 70B) during AmbiguousICL tasks increases probability of concept-consistent outputs. CVs extracted from different distributions (EN vs. FR) transfer better than FVs when similarity ≥0.8. The assumption is that the intervention layer corresponds roughly to where concept-encoding heads emerge (early-to-mid layers).

## Foundational Learning

- **In-Context Learning (ICL)**: All experiments use ICL prompts (N-shot examples) to elicit concept representations. Understanding how models process exemplar pairs is prerequisite.
  - Quick check question: Given "hot→cold, big→small, fast→?", would you expect the model to output "slow"? What internal representation might enable this?

- **Representational Similarity Analysis (RSA)**: RSA is the core method for localizing CVs—comparing pairwise similarity matrices of activations against binary design matrices for task attributes.
  - Quick check question: If two prompts share the same concept but different formats, what would you expect their RSM correlation to be if representations are truly invariant?

- **Attention Head Output Composition**: Both FVs and CVs are constructed by summing outputs from selected attention heads. The residual stream structure (h^ℓ = h^{ℓ-1} + MLP + Σa_j) determines where interventions apply.
  - Quick check question: Why might early-to-mid layer heads encode concepts while later layers don't?

## Architecture Onboarding

- **Component map**: Input layer -> Tokenized ICL prompts (5-shot standard) -> Attention heads a^ℓ_j -> CV extraction (Top 3 heads by Φconcept, summed mean activations) -> FV extraction (Top 20/100 heads by AIE, summed mean activations) -> Intervention layers (Layer 14/31 for CVs, L/3 for FVs)

- **Critical path**: 1. Generate ICL prompts across multiple low-level manifestations (language, format) 2. Extract attention head outputs for each prompt 3. Compute RSMs per head, correlate with design matrices → Φconcept scores 4. Select top heads, sum mean activations → CV 5. For intervention: Add scaled CV to hidden states at target layer

- **Design tradeoffs**: Fewer heads (CV): More invariant, more portable, weaker zero-shot causal effect; More heads (FV): Stronger zero-shot effect, but encodes format/task_type noise; RSA vs. Patching: RSA finds latent representations; patching finds causal effectors—they don't overlap much

- **Failure signatures**: CV similarity <0.7 across distributions: Intervention degrades toward baseline; No heads with Φconcept >0.2: Concept likely not linearly represented (as with "previous"/"next"); High task accuracy but low Φconcept: Model using memorization/alternative strategies, not abstraction

- **First 3 experiments**: 1. Replicate FV non-invariance: Extract FVs from antonym tasks in open-ended EN vs. MC format; compute pairwise RSM. Expect format-based clustering. 2. CV extraction for verbal concept: Run RSA across translation/antonym/categorical tasks; verify top Φconcept heads emerge in layers 8-20 (for 8B model). 3. Abstract concept probe: Attempt CV extraction for "next" across Item-in-List, Abstract-Letter, and Letter-String tasks. Confirm absence of heads with Φconcept >0.2 and task-specific clustering in RSM.

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs encode abstract concepts like "previous" and "next" using non-linear representations that current linear methods fail to capture? The authors state in Limitations: "Our LLMs might still encode 'Next' and 'Previous' nonlinearly but our methods fail to capture it." This remains unresolved because the study exclusively focused on linear representations aligned with the Linear Representation Hypothesis, and did not test for non-linear encoding. Applying non-linear representational analysis methods (e.g., probing with neural networks, manifold analysis) to the same tasks would resolve this.

### Open Question 2
How can LLMs be trained or modified to develop invariant abstract representations necessary for human-like generalization in analogical reasoning? The Discussion states: "Investigating whether LLMs exhibit traces of such abstract knowledge, and how to develop it, is critical for achieving human-level artificial reasoning systems." The paper demonstrates the absence of these representations but does not propose or test interventions to create them. Training experiments with curricula or objectives designed to promote abstract concept invariance, followed by RSA analysis showing emergent invariant CVs, would resolve this.

### Open Question 3
What determines whether a model leverages correct internal concept representations (CVs) to produce correct outputs versus ignoring them? Section 4.2 shows "the model sometimes forms accurate CVs even when it predicts the incorrect answer," suggesting a gap between representation and utilization. The paper identifies the phenomenon but does not isolate the mechanisms controlling whether CVs influence downstream predictions. Layer-wise causal intervention studies tracking when and how CV information is routed (or blocked) through later transformer layers to the output would resolve this.

## Limitations
- The findings assume RSA can reliably detect invariant conceptual representations through linear methods, potentially missing nonlinear encodings
- FVs conflate causal effects with latent conceptual information, making it unclear whether they truly fail to encode concepts or simply fail to isolate them from task attributes
- Experimental scope is limited to Llama-3.1 models, leaving open questions about generalizability across architectures

## Confidence

- **High Confidence**: FVs cluster by format (MC vs. open-ended) rather than concept, demonstrating their encoding of task attributes over pure conceptual information. This is directly observable through RSM visualization and supported by consistent results across multiple verbal concept datasets.

- **Medium Confidence**: CVs for verbal concepts (antonym, categorical, translation) are more invariant than FVs and function as feature detectors. While the RSA evidence is compelling, the exact nature of what these vectors encode remains somewhat ambiguous given the potential for nonlinear representations.

- **Low Confidence**: The absence of invariant linear representations for abstract concepts ("previous," "next") indicates LLMs lack reusable abstractions for these concepts. This conclusion assumes RSA's linear probing is exhaustive, which the authors themselves question in their limitations.

## Next Checks

1. **Nonlinear RSA Probe**: Apply nonlinear probing methods (e.g., small MLPs trained on hidden states) to verify whether the absence of high Φconcept scores for abstract concepts reflects true non-invariance or simply the limitations of linear RSA.

2. **Cross-Architecture Comparison**: Replicate the CV extraction and intervention experiments on alternative LLM architectures (e.g., GPT-3.5, Claude) to determine whether the observed patterns are specific to Llama-3.1 or represent a broader phenomenon in transformer-based models.

3. **Latent Concept Isolation**: Develop a method to separate causal effects from latent conceptual information in activation patching (e.g., by systematically ablating task attributes while preserving concept-related features) to determine whether FVs could be modified to capture invariant concepts rather than merely encoding task attributes.