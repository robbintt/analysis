---
ver: rpa2
title: 'Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs
  on Real-World Tables'
arxiv_id: '2511.17238'
source_url: https://arxiv.org/abs/2511.17238
tags:
- table
- reasoning
- pairs
- tables
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MirageTVQA, a new large-scale benchmark designed
  to evaluate vision-language models (VLMs) on table reasoning under realistic conditions.
  Unlike previous benchmarks that use clean, English-only tables, MirageTVQA features
  nearly 60,000 QA pairs across 24 languages and includes visually noisy table images
  to simulate real-world document conditions.
---

# Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables

## Quick Facts
- arXiv ID: 2511.17238
- Source URL: https://arxiv.org/abs/2511.17238
- Reference count: 18
- Primary result: VLMs fail significantly on visually noisy and multilingual tables

## Executive Summary
This paper introduces MirageTVQA, a large-scale benchmark designed to evaluate vision-language models on table reasoning under realistic conditions. Unlike previous benchmarks using clean, English-only tables, MirageTVQA features nearly 60,000 QA pairs across 24 languages and includes visually noisy table images. The benchmark construction pipeline translates and renders tables from multiple sources, then generates and validates question-answer pairs with human annotators and LLMs.

The evaluation reveals two major failure modes in current VLMs: a severe performance drop exceeding 35% when faced with visual noise, and a consistent English-first bias where reasoning abilities fail to transfer to other languages. These findings highlight the need for more robust, multilingual, and visually resilient models for real-world table understanding.

## Method Summary
MirageTVQA was constructed through a pipeline that first translated tables from multiple sources into 24 languages, then rendered them as images while injecting various types of visual noise. Question-answer pairs were generated using a combination of human annotators and large language models, with validation through multiple review stages. The benchmark includes diverse noise patterns such as blur, compression artifacts, and misalignment to simulate real-world document conditions.

## Key Results
- VLMs show over 35% performance drop when processing visually noisy tables compared to clean versions
- Consistent English-first bias observed across all tested models, with reasoning abilities failing to transfer to other languages
- Nearly 60,000 QA pairs across 24 languages reveal systematic weaknesses in current vision-language models

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic simulation of real-world table conditions. By combining multilingual content with synthetic visual noise, it exposes fundamental limitations in how VLMs handle degraded visual inputs and cross-lingual reasoning. The controlled injection of noise patterns allows precise measurement of visual robustness, while the multilingual setup reveals whether models can transfer learned reasoning patterns across languages or rely on language-specific heuristics.

## Foundational Learning
- Visual noise impact: Models struggle with degraded visual inputs, affecting OCR and spatial reasoning
  - Why needed: Real-world documents contain various noise patterns
  - Quick check: Test models on progressively noisier versions of same table
- Multilingual reasoning transfer: Models trained primarily on English data fail to generalize reasoning across languages
  - Why needed: Global applications require cross-lingual understanding
  - Quick check: Compare performance on translated vs original language tables
- Table structure parsing: Visual models must extract and interpret tabular layouts from images
  - Why needed: Tables organize information in complex spatial arrangements
  - Quick check: Test models on tables with varying structures and complexity

## Architecture Onboarding
- Component map: Image input -> Visual encoder -> Feature fusion -> Language model -> Answer generation
- Critical path: Visual noise handling and multilingual reasoning transfer
- Design tradeoffs: Synthetic noise vs natural document degradation patterns
- Failure signatures: Performance drops on noisy inputs, language-specific reasoning failures
- First experiments:
  1. Test baseline performance on clean English tables
  2. Measure degradation across different noise types
  3. Compare cross-lingual performance against English baseline

## Open Questions the Paper Calls Out
- How do naturally occurring noise patterns in real documents compare to synthetic noise in terms of model performance degradation?
- What architectural modifications could enable better transfer of reasoning capabilities across languages?
- Can specialized visual features for table structure improve robustness to both noise and language variation?
- How do different levels of visual degradation interact with language complexity in affecting model performance?

## Limitations
- Synthetic noise injection may not capture all real-world document degradation patterns
- Benchmark construction relies on translation and rendering tools that could introduce artifacts
- Human validation process lacks detailed metrics on inter-annotator agreement
- Limited evaluation of model-specific architectural differences in handling noise and language transfer

## Confidence
- Visual noise degradation: High confidence (controlled experiments, measurable drops)
- English-first bias: Medium confidence (extensive multilingual coverage but potential annotation artifacts)
- Real-world inadequacy claim: Supported but requires additional validation

## Next Checks
1. Test VLMs on naturally occurring noisy table images from scanned documents and enterprise data to verify synthetic noise findings transfer to authentic degradation patterns
2. Conduct cross-linguistic studies with native speakers evaluating model outputs in each language to assess whether performance gaps reflect true reasoning failures or annotation artifacts
3. Evaluate specialized table parsing models and multimodal architectures against MirageTVQA to determine if architectural innovations can overcome the identified failure modes