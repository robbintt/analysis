---
ver: rpa2
title: 'Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed
  Preferences?'
arxiv_id: '2506.00751'
source_url: https://arxiv.org/abs/2506.00751
tags:
- preference
- preferences
- prompt
- principle
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Large Language Models (LLMs) exhibit
  consistent behavior between their stated preferences (reported alignment with general
  principles) and revealed preferences (decisions in contextualized scenarios). It
  introduces a formal framework to measure this deviation using KL divergence and
  absolute probability differences.
---

# Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?

## Quick Facts
- arXiv ID: 2506.00751
- Source URL: https://arxiv.org/abs/2506.00751
- Reference count: 31
- Key outcome: LLMs exhibit significant deviation between stated and revealed preferences, with minor prompt changes causing substantial shifts in choices.

## Executive Summary
This study investigates whether Large Language Models (LLMs) maintain consistent behavior between their stated preferences (what they report aligning with general principles) and revealed preferences (what they actually choose in contextualized scenarios). The researchers developed a formal framework using KL divergence and absolute probability differences to measure these deviations. Testing GPT and Gemini across 23 carefully crafted prompt pairs spanning moral, risk, fairness, and reciprocal preference domains, they discovered that even minor contextual changes can cause significant shifts in model choices. GPT showed greater internal reasoning sensitivity to context, while Gemini displayed more behavioral variability in specific domains. The findings reveal that LLMs lack consistent decision-making competence, raising critical concerns for their deployment in high-stakes applications where reliable alignment between stated principles and actual behavior is essential.

## Method Summary
The researchers introduced a formal framework to quantify deviation between stated and revealed preferences in LLMs. They crafted 23 base prompts paired with contextualized variants across four domains: moral, risk, fairness, and reciprocal preferences. For each domain, they measured preference deviation using both KL divergence (capturing probabilistic divergence in reasoning paths) and absolute probability difference (measuring behavioral consistency). The study compared two state-of-the-art models - GPT and Gemini - using a binary-choice experimental design. Context variation was carefully designed to simulate real-world scenarios where stated preferences might be tested against practical decision-making situations.

## Key Results
- Minor prompt changes caused significant shifts in LLM choices, with some prompts showing over 30% deviation between stated and revealed preferences
- GPT demonstrated higher KL divergence (internal reasoning sensitivity to context) compared to Gemini, while Gemini showed greater behavioral variability in specific domains
- Across all domains, the study found systematic inconsistency between what LLMs claim to prefer and what they actually choose when context changes

## Why This Works (Mechanism)
The framework works by quantifying the gap between stated and revealed preferences through formal information-theoretic measures. KL divergence captures how much the reasoning process changes when moving from abstract principle statements to concrete scenarios, while absolute probability difference measures the actual behavioral shift. This dual-measure approach reveals that LLMs can maintain consistent stated principles while exhibiting highly variable revealed preferences when context changes, indicating that their internal representations of preferences are not robust to situational framing.

## Foundational Learning
1. **Preference Consistency Theory** - Understanding how stated preferences should align with revealed preferences in rational agents; needed to establish the baseline expectation that LLMs should maintain consistency across contexts; quick check: verify that human subjects show similar consistency patterns
2. **Information-Theoretic Measures in NLP** - KL divergence and probability difference as tools for measuring semantic and behavioral shifts; needed to quantify the degree of preference inconsistency; quick check: validate measures against known semantic similarity benchmarks
3. **Contextual Prompt Engineering** - How subtle changes in prompt phrasing and scenario framing affect model outputs; needed to understand the sensitivity of LLMs to contextual variation; quick check: test multiple prompt variations to establish sensitivity thresholds

## Architecture Onboarding
**Component Map**: Prompt Generator -> Preference Extractor -> Consistency Measurer -> Domain Comparator
**Critical Path**: Base prompt creation → Contextual variant generation → Preference extraction → KL divergence and probability difference calculation → Domain-specific analysis
**Design Tradeoffs**: Binary choices simplify analysis but may miss nuanced preferences; small sample size enables controlled experiments but limits generalizability; focus on two models enables depth but constrains breadth
**Failure Signatures**: High KL divergence indicates reasoning instability; large probability differences signal behavioral inconsistency; domain-specific patterns suggest areas of particular vulnerability
**First Experiments**:
1. Test consistency framework on a third LLM model to validate generalizability
2. Expand prompt pairs to include multi-option choices and longer-term consequence scenarios
3. Measure human consistency on same prompts to establish baseline comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on only two LLM models (GPT and Gemini), limiting generalizability across the broader landscape of language models
- Analysis uses a relatively small sample of 23 prompt pairs, potentially missing complex preference consistency issues
- Focus on binary-choice scenarios may not capture more nuanced preference expressions that occur in real-world applications

## Confidence
- **High Confidence**: Core finding that stated and revealed preferences can diverge significantly in LLMs is well-supported by empirical results and formal framework
- **Medium Confidence**: Comparative analysis between GPT and Gemini's consistency patterns is reliable but limited by small sample size and model selection
- **Medium Confidence**: Implications for real-world deployment are reasonable but require further validation across broader contexts

## Next Checks
1. Cross-Model Validation: Test the consistency framework across a broader range of LLM architectures (e.g., Claude, LLaMA, open-source models) to assess whether observed patterns are universal or model-specific
2. Scenario Complexity Expansion: Develop and test a larger, more diverse set of prompt pairs including multi-option choices, longer-term consequences, and more complex ethical dilemmas
3. Real-World Application Testing: Deploy LLMs in controlled real-world applications (e.g., customer service, educational tutoring) to measure how stated-revealed preference gaps manifest in actual use cases and impact user trust and outcomes