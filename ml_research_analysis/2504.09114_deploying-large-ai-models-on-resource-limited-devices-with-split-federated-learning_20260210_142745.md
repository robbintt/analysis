---
ver: rpa2
title: Deploying Large AI Models on Resource-Limited Devices with Split Federated
  Learning
arxiv_id: '2504.09114'
source_url: https://arxiv.org/abs/2504.09114
tags:
- devices
- quantization
- device
- learning
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SFLAM, a framework for deploying large AI
  models on resource-limited devices using split federated learning. SFLAM partitions
  model training between edge devices and a server, incorporating quantization management,
  power control, and bandwidth allocation to reduce computational and communication
  overhead.
---

# Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning

## Quick Facts
- arXiv ID: 2504.09114
- Source URL: https://arxiv.org/abs/2504.09114
- Reference count: 40
- This paper introduces SFLAM, a framework for deploying large AI models on resource-limited devices using split federated learning

## Executive Summary
This paper introduces SFLAM, a framework designed to deploy large AI models on resource-constrained edge devices using split federated learning. The approach partitions model training between edge devices and a central server, incorporating quantization management, power control, and bandwidth allocation to minimize computational and communication overhead. Through theoretical analysis and simulations, SFLAM demonstrates faster convergence and higher accuracy compared to conventional federated learning methods, achieving test accuracies of 94%, 95%, and 96% on CIFAR-10 under different Dirichlet distributions. The framework effectively reduces memory requirements below 100MB, enabling efficient operation of large-scale models on devices with limited resources.

## Method Summary
SFLAM is a split federated learning framework that partitions large AI model training between edge devices and a central server to reduce computational and communication overhead. The framework incorporates quantization management, power control, and bandwidth allocation to optimize resource usage on constrained devices. By splitting the model architecture and distributing computation, SFLAM achieves faster convergence and higher accuracy compared to conventional federated learning approaches, with test accuracies of 94%, 95%, and 96% on CIFAR-10 under different Dirichlet distributions.

## Key Results
- SFLAM achieves test accuracies of 94%, 95%, and 96% on CIFAR-10 under different Dirichlet distributions
- Memory requirements reduced below 100MB for large-scale models
- Faster convergence compared to conventional federated learning methods

## Why This Works (Mechanism)
SFLAM works by partitioning large AI models between edge devices and a central server, reducing the computational burden on resource-limited devices. The framework uses quantization to compress model parameters and gradients, power control to manage device energy consumption, and bandwidth allocation to optimize communication efficiency. This split architecture allows edge devices to handle only a portion of the model, while the server manages the remainder, enabling large models to be trained effectively in federated settings with constrained resources.

## Foundational Learning

1. **Split Federated Learning**: Dividing model training between edge devices and a central server to reduce computational overhead. Why needed: Large models exceed memory and processing capabilities of resource-limited devices. Quick check: Verify model partitioning reduces device memory usage by at least 50%.

2. **Quantization Management**: Compressing model parameters and gradients to reduce communication overhead. Why needed: High-bandwidth communication is often unavailable or costly on edge networks. Quick check: Confirm communication volume reduction by measuring transmitted bytes before and after quantization.

3. **Bandwidth Allocation**: Dynamically adjusting communication resources based on network conditions and model requirements. Why needed: Network heterogeneity affects training efficiency and convergence. Quick check: Monitor training time under varying bandwidth constraints.

## Architecture Onboarding

**Component Map**: Edge devices -> Local model partition -> Quantization -> Communication -> Server -> Global model update -> Parameter synchronization -> Edge devices

**Critical Path**: Model partitioning -> Local training with quantization -> Communication of compressed gradients -> Global aggregation -> Parameter synchronization

**Design Tradeoffs**: 
- Accuracy vs. resource constraints: Partitioning reduces device load but may impact convergence
- Communication efficiency vs. model fidelity: Quantization reduces bandwidth but may lose information
- Power consumption vs. training speed: Power control extends device battery life but slows convergence

**Failure Signatures**:
- Convergence stalls: Check quantization levels and bandwidth allocation
- Accuracy degradation: Verify model partitioning and gradient compression
- Device overload: Monitor memory usage and adjust partitioning strategy

**First 3 Experiments**:
1. Measure memory usage on actual edge devices with varying model partition sizes
2. Evaluate training time and accuracy under different network bandwidth constraints
3. Test power consumption during training with different power control strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on simulations rather than real hardware deployment
- Specific memory usage metrics and device constraints not detailed
- Performance under varying network conditions and heterogeneous device capabilities unexplored

## Confidence
- Theoretical analysis and convergence guarantees: High
- Test accuracy results on CIFAR-10: Medium
- Memory reduction claims: Low

## Next Checks
1. Deploy SFLAM on actual edge devices with limited computational resources to verify claimed memory usage and performance.
2. Conduct experiments under varying network conditions to assess the framework's robustness and adaptability.
3. Evaluate the framework's performance across diverse datasets and model architectures to validate generalizability.