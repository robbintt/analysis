---
ver: rpa2
title: 'HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films'
arxiv_id: '2512.24946'
source_url: https://arxiv.org/abs/2512.24946
tags:
- frames
- film
- restoration
- methods
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HaineiFRDM proposes a diffusion-based framework for restoring degraded
  old films with fast-moving content. It addresses the limitations of existing open-source
  methods, which struggle with high-resolution data, content distortion, and fast
  motion due to noisy optical flows and synthetic training data.
---

# HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films

## Quick Facts
- arXiv ID: 2512.24946
- Source URL: https://arxiv.org/abs/2512.24946
- Reference count: 39
- Primary result: Diffusion-based framework for restoring degraded old films with fast-moving content on consumer GPUs

## Executive Summary
HaineiFRDM addresses the challenge of restoring degraded old films with fast-moving content using a diffusion-based approach. Unlike existing methods that struggle with high-resolution data and fast motion due to noisy optical flows and synthetic training data, this framework introduces a patch-wise strategy enabling restoration on consumer GPUs while maintaining spatial coherence through global-local fusion modules. The method achieves superior defect restoration while preserving original textures and avoiding content structure erasure, outperforming prior approaches on both synthetic and real film data.

## Method Summary
HaineiFRDM employs a two-stage training approach where a pretrained video diffusion model is conditioned through a trainable Restoration-Guidance Network (ControlNet-style). The framework processes high-resolution film frames by splitting them into overlapping 3D patches, with each patch receiving global context through CLIP-based fusion modules (textual scene descriptions and visual tokens from downsampled frames). A frequency-domain texture reconstruction module preserves original film grain while removing defects. During inference, a global residual fusion module provides consistency across patches, with a cosine-decreasing weight schedule balancing global coherence and local detail across denoising timesteps.

## Key Results
- Outperforms prior methods on synthetic defect restoration with better handling of fast motion and fewer artifacts
- Successfully restores real degraded films while preserving original textures and avoiding content structure erasure
- Achieves high-resolution restoration (2K/4K) on consumer GPUs with 24GB VRAM through patch-wise processing
- Demonstrates improved quantitative metrics (PSNR, SSIM, LPIPS) and qualitative expert assessment on fast-motion scenes

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models can restore film defects without explicit defect detection modules by leveraging learned priors about natural image structure. Instead of detecting scratches via optical flow (which fails on fast motion), the model learns to distinguish defects from content through denoising training on paired degraded-restored data. The Restoration-Guidance Network injects spatial features into a frozen pretrained UNet, relying on the model's intrinsic real-world awareness to semantically distinguish defects from legitimate content.

### Mechanism 2
Patch-wise processing with global context injection enables high-resolution restoration on consumer GPUs while maintaining spatial coherence. Frames split into overlapping 3D patches, with Global Frame Fusion providing position-aware CLIP visual tokens from downsampled full frames via cross-attention, and Global Prompt Fusion providing textual scene descriptions. This prevents local patches from being misinterpreted when cropped content is ambiguous.

### Mechanism 3
Frequency-domain texture reconstruction preserves original film grain and detail while restoring defects. 3D-FFT transforms features to frequency domain, with high-frequency components from degraded input fused with model features via cross-attention, then inverse FFT. Low-frequency global frame features maintain color/lightness consistency. This approach targets defects primarily manifesting in specific frequency bands distinguishable from legitimate texture.

## Foundational Learning

- **Concept: Latent Diffusion Models & ControlNet** - Why needed: The architecture builds on a pretrained video diffusion UNet, adding a trainable Restoration-Guidance Network that injects conditional features without modifying the frozen base model. Quick check: Can you explain why ControlNet-style conditioning preserves the pretrained model's generation priors while learning task-specific features?

- **Concept: Patch-based High-Resolution Generation** - Why needed: The inference pipeline uses global residuals from low-resolution restoration to guide patch-wise high-resolution generation, directly inspired by DemoFusion. Quick check: How does a cosine-decreasing weight for global residuals balance global consistency vs. local detail across denoising timesteps?

- **Concept: Frequency-Domain Image Processing** - Why needed: The Texture-Reconstruction Module operates in 3D frequency space to separate texture preservation from defect removal. Quick check: Why would 3D-FFT (spatial + temporal) provide different information than 2D-FFT per frame for video restoration?

## Architecture Onboarding

- **Component map**: Degraded frames → Preprocess Module → VAE encoder → Frozen UNet + Restoration-Guidance Network (Self-attention → Global Prompt Fusion → Global Frame Fusion → Texture Reconstruction → Temporal Module) → VAE decoder → RGB restored frames

- **Critical path**: Low-resolution full-frame restoration provides global structure/color reference → Per-timestep: fuse previous restoration with global residual (cosine-weighted), patchify, denoise each patch with KV-cache from prior patches → Texture Reconstruction Module ensures frequency-domain consistency at each denoising step

- **Design tradeoffs**: Patch size vs. VRAM (smaller patches reduce memory but increase ambiguity), Global residual weight schedule (higher weight preserves consistency but may suppress detail restoration), Temporal module freezing (saves VRAM but limits learned temporal consistency)

- **Failure signatures**: Blocky artifacts at patch boundaries (insufficient overlap or global residual weighting issue), Content erasure on fast motion (global fusion not resolving ambiguity), Texture mismatch (Frequency module not receiving adequate high-frequency input), Slow inference (inherent to diffusion)

- **First 3 experiments**: Ablate Global Frame Fusion (train/inference without CLIP global frame injection), Vary Global Residual Weight Schedule (test constant weight vs. cosine schedule), Frequency Module Ablation (disable 3D-FFT path and use LPIPS/texture metrics)

## Open Questions the Paper Calls Out

- **Cross scene information**: Cross scene information have not been explored! The current architecture uses Global Frame Fusion for within-scene context, but scene transitions present different challenges for maintaining narrative coherence in restored films.

- **Temporal length extension**: Temporal length is more limited than previous works that employ traditional methods. Diffusion models have high memory requirements that limit the number of frames that can be processed jointly, affecting long-range temporal consistency.

- **Additional degradation types**: The authors currently only consider dust and scratch removal, but there are many other degradations in human restoration process that haven't been explored, like color adjustment, dye fading, and frame loss.

- **Constant line scratches**: The diffusion model still has trouble recognizing and repainting some constant line scratches. These may be semantically similar to legitimate film content (lines, edges), making them difficult to distinguish from actual scene elements.

## Limitations

- The pretrained diffusion backbone architecture and VideoMamba configuration are not specified, requiring assumptions for reproduction
- Limited quantitative validation on real degraded film data; most results are on synthetic defects
- Inference speed remains slow despite patch-wise approach, with no reported runtime metrics
- No ablation study on the relative importance of the three fusion modules

## Confidence

- **High Confidence**: The core diffusion-based architecture with patch-wise processing and ControlNet-style conditioning is technically sound and reproducible
- **Medium Confidence**: The frequency-domain texture reconstruction module's effectiveness, given weak direct corpus support and no quantitative texture preservation metrics beyond PSNR/SSIM
- **Low Confidence**: Claims about superiority on real degraded films, as validation relies heavily on synthetic data and expert qualitative assessment

## Next Checks

1. **Ablate Global Frame Fusion**: Train/inference without CLIP global frame injection; measure content distortion rate on fast-motion patches vs. full model
2. **Vary Global Residual Weight Schedule**: Test constant weight vs. cosine schedule; visualize patch boundary artifacts at 2K resolution
3. **Frequency Module Ablation**: Disable 3D-FFT path; use LPIPS and texture metrics (not just PSNR/SSIM) to quantify detail preservation on synthetic data with known ground truth