---
ver: rpa2
title: Towards Quantum Machine Learning for Malicious Code Analysis
arxiv_id: '2508.19381'
source_url: https://arxiv.org/abs/2508.19381
tags:
- quantum
- qcnn
- qmlp
- malware
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Quantum Multilayer Perceptron (QMLP) and Quantum
  Convolutional Neural Network (QCNN) models for malware classification across five
  datasets. QMLP encodes malware features using angle embedding and processes them
  through a two-layer quantum circuit with full qubit measurements and data re-uploading,
  while QCNN applies quantum convolution and pooling layers to reduce active qubits.
---

# Towards Quantum Machine Learning for Malicious Code Analysis

## Quick Facts
- arXiv ID: 2508.19381
- Source URL: https://arxiv.org/abs/2508.19381
- Reference count: 35
- Primary result: QMLP achieves 95-96% binary classification accuracy on API-Graph dataset

## Executive Summary
This study evaluates Quantum Multilayer Perceptron (QMLP) and Quantum Convolutional Neural Network (QCNN) models for malware classification across five datasets. Both models utilize angle embedding to encode malware features into quantum states, with QMLP capturing complex patterns through full qubit measurement and data re-uploading, while QCNN offers faster training via pooling layers. The research demonstrates that QMLP outperforms QCNN in multiclass classification scenarios, particularly when dealing with higher class counts, though at increased computational cost.

## Method Summary
The study employs hybrid quantum-classical models where malware feature vectors (1,159–2,439 dimensions) are first normalized and reduced to 16 principal components via PCA. QMLP uses 16-qubit circuits with angle embedding, two layers of Rot gates and CRX entanglement, data re-uploading, and full-qubit measurement. QCNN applies similar quantum operations but includes pooling layers to reduce active qubits from 16 to 4. Both models are trained using PennyLane's default.qubit simulator with PyTorch integration, employing Adam optimizer for 20 epochs with batch size 64.

## Key Results
- QMLP achieves 95-96% accuracy in binary classification on API-Graph dataset
- QCNN offers faster training but slightly reduced accuracy compared to QMLP
- In multiclass settings, QMLP demonstrates superior performance with better accuracy and F1 scores, particularly in complex scenarios with higher class counts

## Why This Works (Mechanism)

### Mechanism 1
- Angle embedding enables classical malware features to be processed as quantum states through RX rotation gates mapping normalized input features to qubit Bloch sphere positions, preserving feature magnitude information while making it quantum-compatible. Core assumption: PCA-reduced 16-dimensional representation captures sufficient malware discriminative information.

### Mechanism 2
- CRX-based ring entanglement captures cross-feature correlations analogous to classical weight interactions through controlled-RX gates connecting each qubit to its neighbor in ring topology, allowing the model to learn relationships between adjacent principal components. Core assumption: Ring topology is sufficient for malware feature correlations.

### Mechanism 3
- Data re-uploading increases model expressivity by injecting input data multiple times with independent parameters through repeating encoding-rotation-entanglement sequence twice with different trainable parameters, creating deeper quantum circuits that learn more complex decision boundaries. Core assumption: Re-uploading does not cause gradient vanishing at this circuit depth.

## Foundational Learning

- **Parameterized Quantum Circuits (PQCs)**: Both QMLP and QCNN are PQCs—quantum circuits with trainable rotation angles analogous to neural network weights. Quick check: Can you explain how a Rot(θ, φ, λ) gate differs from a fixed quantum gate?

- **PCA for Quantum Dimensionality Reduction**: Input features must be reduced to 16 to match qubit count; PCA preserves maximum variance. Quick check: What percentage of variance is retained after PCA reduction to 16 components?

- **Hybrid Quantum-Classical Training Loops**: Quantum circuits generate feature representations while classical PyTorch layers handle final classification and backpropagation coordinates gradients through PennyLane's autodiff. Quick check: How does PennyLane's TorchLayer interface enable gradient flow from classical loss back through quantum parameters?

## Architecture Onboarding

- **Component map**: Input preprocessing: Min-Max normalization → PCA (16 components) → Quantum layer: 16-qubit circuit with angle embedding → Rot gates → CRX entanglement → (QCNN: pooling 16→8→4) or (QMLP: data re-upload, full measurement) → Classical post-processing: Linear layer → Log-Softmax → class probabilities

- **Critical path**: Verify PCA retains >85% variance, confirm PennyLane default.qubit simulator is active for noise-free baseline, monitor gradient magnitudes during training—values <10⁻⁶ suggest barren plateau risk

- **Design tradeoffs**: QMLP offers higher accuracy (full 16-qubit measurement) vs. slower training (more parameters, measurements); QCNN provides faster training (4-qubit output, pooling) vs. accuracy loss in complex multiclass (25–50% drop at 23 classes); 16 qubits balance practical simulation runtime vs. information loss from aggressive PCA

- **Failure signatures**: Accuracy drops sharply as class count increases (QCNN: 92%→42% from 4→23 classes on AZ-Class), high FPR/FNR on AZ-Domain (~31%) suggests dataset-specific difficulty, low precision/recall despite high accuracy (API-Graph 23-class: 4.2% F1 vs. 91.6% accuracy) indicates class imbalance issues

- **First 3 experiments**: Replicate binary classification on API-Graph with 16 qubits targeting 95–96% accuracy; ablate data re-uploading in QMLP (single layer vs. two layers) measuring accuracy drop and training time difference; test QCNN with all-to-all entanglement replacing ring with full connectivity comparing accuracy vs. runtime on AZ-Class 14-class configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameters such as learning rates and optimizer configurations are not specified, which can significantly impact model performance
- PCA dimensionality reduction to 16 components may result in information loss, particularly for complex multiclass scenarios
- Multiclass performance degrades substantially as class count increases, suggesting scalability limitations

## Confidence
- High confidence: Basic mechanism of angle embedding and quantum circuit construction
- Medium confidence: Reported accuracy metrics and comparative performance between QMLP and QCNN
- Low confidence: Generalization claims across diverse malware datasets and real-world applicability

## Next Checks
1. **Ablation study on PCA dimensionality**: Systematically vary PCA components (8, 16, 32) to quantify information loss impact on classification accuracy across all five datasets

2. **Hyperparameter sensitivity analysis**: Test multiple learning rates (0.001, 0.01, 0.1) and optimizer configurations to establish performance bounds and identify optimal training conditions

3. **Scalability evaluation**: Extend testing to datasets with 30+ classes to validate claims about QMLP's superior performance in complex multiclass scenarios and identify breaking points for both models