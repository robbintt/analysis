---
ver: rpa2
title: 'The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering for
  LLM Pretraining'
arxiv_id: '2510.00866'
source_url: https://arxiv.org/abs/2510.00866
tags:
- data
- quality
- figure
- dataset
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Classifier-based Quality Filtering (CQF) is widely used in large
  language model pretraining to improve downstream performance by selecting high-quality
  documents. The method trains a binary classifier to distinguish between a high-quality
  (HQ) set and a large low-quality pretraining set, then ranks and filters documents
  from the pretraining set based on classifier scores.
---

# The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering for LLM Pretraining

## Quick Facts
- arXiv ID: 2510.00866
- Source URL: https://arxiv.org/abs/2510.00866
- Authors: Thiziri Nait Saada; Louis Bethune; Michal Klein; David Grangier; Marco Cuturi; Pierre Ablin
- Reference count: 40
- Primary result: Classifier-based Quality Filtering (CQF) improves downstream accuracy but degrades or fails to improve language modeling on the HQ set itself.

## Executive Summary
Classifier-based Quality Filtering (CQF) is widely used to improve LLM pretraining by selecting high-quality documents. This study reveals a paradox: while CQF improves downstream accuracy, it does not improve language modeling on the HQ set itself and sometimes degrades it. The key insight is that CQF implicitly filters the HQ set by upweighting data far from the low-quality set, so models perform well only on a higher-quality subset of HQ data, not the whole set. CQF thus emphasizes data that are both similar to HQ and dissimilar to LQ, rather than truly mimicking the HQ distribution. When compared to importance sampling methods, CQF yields worse language modeling on the HQ set but better downstream performance. The study introduces "data conditioning" as a measure of whether training on cleaner data improves performance on dirtier data, finding that CQF fails this property, suggesting its notion of quality relates more to stylistic or domain similarity than a universal measure of data quality.

## Method Summary
The method trains a binary classifier to distinguish between a high-quality (HQ) set and a large low-quality pretraining set, then ranks and filters documents from the pretraining set based on classifier scores. Documents are embedded using sBert (or FastText), a logistic regression classifier is trained on these embeddings, and the LQ set is scored and filtered to retain the top-k% highest-scoring documents. Models are then trained on these filtered subsets and evaluated on both downstream benchmarks (ARC-Easy, MMLU, Reward-Bench) and language modeling loss on the full HQ set. The key experiment varies k across [1%, 5%, 25%, 100%] to reveal the U-shaped HQ loss curve while downstream accuracy monotonically increases.

## Key Results
- CQF improves downstream accuracy (ARC-Easy) while HQ set language modeling loss follows a U-curve.
- The loss on top-scoring HQ data is monotonic with k, while loss on bottom-scoring HQ data rises sharply as k decreases.
- CQF does not satisfy "data conditioning"—training on higher-ranked CQF data does not improve performance on lower-ranked subsets.
- CQF emphasizes documents that are both likely under HQ and unlikely under LQ, rather than approximating the HQ distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CQF improves downstream performance by selecting documents with high likelihood ratio (pHQ/pLQ), not by mimicking the HQ distribution.
- Mechanism: A Bayes-optimal classifier outputs s(x) = pHQ(x)/(pHQ(x) + pLQ(x)), which orders documents by the density ratio. This upweights documents that are both likely under HQ and unlikely under LQ—effectively "removing the bad" rather than "imitating the good."
- Core assumption: The classifier approximates Bayes-optimality; the LQ set is not uniformly distributed.
- Evidence anchors:
  - [abstract] "CQF implicitly filters the high-quality set—favoring data far from the low-quality set."
  - [section 4] "Since in general the LQ set is not uniformly distributed, CQF does not select samples that are most likely to come from the HQ set only."
  - [corpus] Related work (Li et al., 2024; Xie et al., 2023) supports classifier-based selection effectiveness but does not directly verify the likelihood-ratio interpretation.
- Break condition: If the classifier is poorly calibrated or the LQ distribution is near-uniform, the ranking approximates pHQ only and the mechanism reverts to importance sampling behavior.

### Mechanism 2
- Claim: CQF's downstream gains arise from implicit filtering of the HQ set itself, aligning selected data with evaluation benchmarks.
- Mechanism: By emphasizing HQ examples distant from the LQ manifold, CQF selects a higher-quality subset of HQ that correlates with downstream tasks. Models trained on this subset show monotonic loss reduction on top-scoring HQ deciles even as overall HQ loss follows a U-curve.
- Core assumption: Evaluation benchmarks share structure with the top-scoring portion of the chosen HQ set.
- Evidence anchors:
  - [section 4.2] "The loss on the top-scoring HQ data is monotonic with k, while the loss on the bottom-scoring HQ data rises sharply as k decreases."
  - [figure 5] Distance from HQ decile barycenters to ARC-Easy correlates with downstream performance.
  - [corpus] No direct corpus evidence for implicit HQ filtering; related papers focus on filtering LQ only.
- Break condition: If the HQ set is small, homogeneous, or poorly aligned with target benchmarks, the implicit filtering may not confer additional benefit.

### Mechanism 3
- Claim: CQF does not satisfy the "data conditioning" property; training on higher-ranked CQF data does not universally improve performance on lower-ranked subsets.
- Mechanism: Data conditioning requires that training on cleaner data eases optimization on dirtier distributions (due to imperfect optimization). Semi-synthetic quality axes (e.g., token-permuted documents) exhibit this; CQF-based axes do not, suggesting CQF captures stylistic/domain similarity rather than intrinsic quality.
- Core assumption: Curriculum-like benefits require true quality gradations, not just domain shift.
- Evidence anchors:
  - [abstract] "CQF does not exhibit this property: training on 'higher-quality' CQF-selected data does not improve performance on 'lower-quality' subsets."
  - [section 6.2] "For the Perm quality axis, we observe a mostly upper-triangular structure... In contrast, CQF-based axes fail to show this ordering."
  - [corpus] Weak/missing; related work does not frame quality in terms of data conditioning.
- Break condition: If optimization dynamics change (e.g., different architectures or training algorithms), data conditioning effects may differ.

## Foundational Learning
- Concept: KL divergence links HQ loss to distributional distance.
  - Why needed here: Interpreting HQ loss as KL(DHQ || DCQF) explains why reduced k can increase loss even as downstream performance improves.
  - Quick check question: If a model trained on filtered data achieves lower loss on HQ than one trained on HQ itself, what does that imply about the filtered distribution relative to HQ?

- Concept: Binary classification yields posterior probabilities proportional to likelihood ratios.
  - Why needed here: CQF scores are classifier outputs; understanding their probabilistic basis clarifies why they rank by pHQ/pLQ rather than pHQ alone.
  - Quick check question: If pLQ(x) is nearly uniform, does the classifier ranking approximate importance sampling?

- Concept: Importance sampling vs. classification-based selection.
  - Why needed here: The paper contrasts CQF with CRISP (importance sampling); knowing when each applies informs pipeline choices.
  - Quick check question: Which method should you use if your goal is to minimize loss on a specific HQ corpus rather than maximize downstream task accuracy?

## Architecture Onboarding
- Component map: Document embedder (sBert/FastText) -> Binary classifier (Logistic Regression) -> Scoring module -> Threshold estimator -> Selection filter
- Critical path: Embedding quality and classifier calibration determine whether scores reflect meaningful quality distinctions vs. spurious features (e.g., document length). Validate on held-out data before large-scale filtering.
- Design tradeoffs:
  - Embedding choice (sBert vs. FastText): Higher fidelity vs. scalability
  - k selection: Smaller k improves downstream performance but risks data scarcity and distributional drift from HQ
  - HQ set selection: Determines inductive bias; no single HQ set dominates across tasks
- Failure signatures:
  - U-shaped HQ loss curve as k decreases → indicates distributional drift, not HQ approximation
  - Classifier biases (e.g., length) → selected data diverge from intended HQ characteristics
  - No data conditioning ordering → suggests CQF is capturing domain shift, not universal quality
- First 3 experiments:
  1. Run CQF with two different HQ sets (e.g., OpenOrca vs. KnowledgePile) on the same LQ corpus; compare downstream benchmarks to identify task-specific alignment
  2. Train models on top-k subsets (k = 100%, 20%, 5%, 1%) and plot both HQ loss and downstream accuracy to verify the U-curve pattern
  3. Inspect classifier-learned features (e.g., via SHAP or by correlating scores with document length) to detect spurious biases; retrain with balanced sampling if needed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal selection fraction (k) vary with respect to model size, training compute, and the specific choice of the high-quality reference set?
- Basis in paper: [explicit] Appendix B explicitly asks "How to chose the optimal k?" and states that results were "inconclusive" and showed "no clear trend" once noise was accounted for, diverging from concurrent work.
- Why unresolved: The ablations across various model sizes (125M to 1.3B) and FLOP budgets failed to reveal a stable scaling law for the optimal filtering threshold, suggesting high variance or sensitivity to the specific dataset.
- What evidence would resolve it: A large-scale study deriving a scaling law that correlates the optimal k with compute budget and model dimensions, specifically identifying if a predictable trend emerges beyond the noise.

### Open Question 2
- Question: Can CQF classifiers be effectively regularized or re-engineered to prevent them from learning spurious features, such as sequence length or formatting artifacts, as proxies for quality?
- Basis in paper: [explicit] Appendix C investigates "Do classifiers used in CQF exhibit undesired biases?" and identifies sequence length as a confounding variable that persists even after some debiasing attempts.
- Why unresolved: The paper demonstrates that classifiers often rely on undesirable artifacts (e.g., sequence length in OpenOrca) rather than semantic quality, and simple subsampling strategies do not fully eliminate these biases.
- What evidence would resolve it: A training methodology that successfully decouples classifier scores from document length or stylistic markers while maintaining or improving alignment with the target benchmark embeddings.

### Open Question 3
- Question: Does a data selection method exist that satisfies the "data conditioning" property while simultaneously achieving the downstream performance gains provided by CQF?
- Basis in paper: [inferred] Section 6 introduces "data conditioning" (training on clean data improves performance on dirty data) as a desirable property, but shows CQF fails to exhibit it.
- Why unresolved: The paper establishes "data conditioning" as a theoretical definition of quality but highlights that CQF operates differently (capturing stylistic similarity), leaving the development of a method that bridges this gap unstated.
- What evidence would resolve it: A filtering algorithm that produces a dataset where training strictly improves loss on out-of-distribution or lower-quality subsets compared to training on those subsets directly, without sacrificing benchmark accuracy.

## Limitations
- The study's conclusions depend on the specific HQ sets and downstream benchmarks used; results may not generalize to all domains or tasks.
- The classifier's tendency to learn spurious features (e.g., document length) can introduce biases that are difficult to fully eliminate.
- The data conditioning analysis uses semi-synthetic quality axes that may not fully represent real-world quality variations.

## Confidence
- CQF paradox observation: High
- Mechanism 1 (likelihood ratio ranking): Medium
- Mechanism 2 (implicit HQ filtering): Medium
- Mechanism 3 (data conditioning failure): High
- Generalizability to other domains/tasks: Low

## Next Checks
1. Replicate the CQF paradox using a different embedding method (FastText) and verify whether results hold, particularly checking for length bias artifacts.
2. Test whether the U-shaped HQ loss curve persists when using smaller LQ corpora to assess data scarcity effects.
3. Evaluate CQF with an HQ set that is explicitly disjoint from downstream benchmarks to test whether implicit filtering still provides benefits.