---
ver: rpa2
title: Cross-Modal Alignment via Variational Copula Modelling
arxiv_id: '2511.03196'
source_url: https://arxiv.org/abs/2511.03196
tags:
- copula
- modalities
- learning
- missing
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel copula-driven multimodal learning\
  \ framework (CM2) to tackle the joint fusion paradigm from a probabilistic perspective.\
  \ The core idea is to interpret the copula model as an effective tool of distribution\
  \ alignment, guaranteed by Sklar\u2019s theorem."
---

# Cross-Modal Alignment via Variational Copula Modelling

## Quick Facts
- **arXiv ID:** 2511.03196
- **Source URL:** https://arxiv.org/abs/2511.03196
- **Reference count:** 40
- **Primary result:** CM2 achieves 0.858 AUROC and 0.527 AUPR for in-hospital mortality prediction on MIMIC-IV, outperforming state-of-the-art methods by 1.2-3.3% in AUROC and 1.9-4.6% in AUPR.

## Executive Summary
This paper introduces CM2, a novel copula-driven multimodal learning framework that tackles the joint fusion paradigm from a probabilistic perspective. The core innovation is using copulas to separate marginal distributions from dependency structures, enabling precise alignment of heterogeneous modalities. By assuming Gaussian mixture distributions for each modality and learning a copula model on the joint distribution, CM2 can generate accurate representations for missing modalities. Extensive experiments on public MIMIC datasets demonstrate superior performance over competitors, with significant gains in both in-hospital mortality and readmission prediction tasks.

## Method Summary
CM2 employs a probabilistic framework that learns Gaussian Mixture Models (GMMs) for each modality's latent features and models their joint distribution using a parametric copula. The model uses modality-specific encoders (ResNet34, LSTM, TinyBERT) to extract latent representations, then estimates GMM parameters (mixture weights, means, covariances) for each modality. A copula layer captures the dependency structure between modalities, while a fusion module (LSTM + MLP) combines information for classification. The model is trained via Evidence Lower Bound (ELBO) optimization, balancing task performance with distributional alignment. For missing modalities, CM2 samples from the learned GMMs to generate plausible representations, preserving the joint distribution geometry better than zero-imputation.

## Key Results
- CM2 achieves 0.858 AUROC and 0.527 AUPR for in-hospital mortality prediction on MIMIC-IV dataset
- Outperforms state-of-the-art methods by 1.2-3.3% in AUROC and 1.9-4.6% in AUPR
- Ablation analysis confirms effectiveness of copula in modality alignments and robustness to variations
- Superior performance maintained under different missing data patterns and ratios

## Why This Works (Mechanism)

### Mechanism 1: Copula-Driven Dependency Separation
The framework utilizes Sklar's theorem to decompose joint distributions into marginal CDFs and a copula function capturing dependency. By learning the copula parameter, CM2 isolates the interaction structure from individual feature distributions, allowing more precise alignment than simple concatenation or Kronecker products. This separation enables the model to capture complex relationships between heterogeneous modalities while maintaining their individual characteristics.

### Mechanism 2: Flexible Marginal Modeling via GMM
Assuming Gaussian Mixture Models for latent features enables representation of complex, multi-modal data distributions like diverse patient types in EHR. Instead of forcing unimodal Gaussian latent spaces, the model predicts mixture weights and component parameters for multiple clusters, allowing the latent space to form distinct clusters per modality before alignment. This flexibility captures the inherent heterogeneity in clinical data more effectively than single Gaussian assumptions.

### Mechanism 3: Probabilistic Imputation for Missing Modalities
Sampling from learned marginal GMMs allows CM2 to hallucinate plausible representations for missing modalities, preserving joint distribution geometry better than zero-imputation. When a modality is missing, the model samples a latent vector from the learned GMM and passes it to the fusion module. This probabilistic approach maintains the integrity of the joint distribution and enables training on partially observed data without introducing artificial zeros that could bias learning.

## Foundational Learning

- **Sklar's Theorem & Copulas**: Mathematical foundation guaranteeing any joint distribution can be decomposed into marginals and a dependency structure. *Why needed*: Enables separation of individual modality characteristics from their relationships. *Quick check*: Can you explain why a copula allows modeling correlation between variables regardless of their individual distributions?
- **Variational Inference (ELBO)**: Training methodology maximizing Evidence Lower Bound. *Why needed*: Balances task performance against distributional constraints during optimization. *Quick check*: What happens to latent space geometry if the copula regularization weight is set to zero?
- **Gaussian Mixture Models (GMM)**: Statistical model assuming data comes from multiple Gaussian components. *Why needed*: Captures the multi-modal nature of clinical data distributions. *Quick check*: How does the reparameterization trick apply to sampling from a discrete mixture distribution versus a continuous Gaussian?

## Architecture Onboarding

- **Component map**: Encoders -> Latent z -> GMM Marginal Estimator -> Copula Layer -> Fusion Module -> Classifier
- **Critical path**: Gradient Preserve Sampling (GPS) is essential when sampling z for missing modalities. Ensure gradients flow back to GMM parameters so hallucinated data improves over time.
- **Design tradeoffs**: Copula Family selection (Frank captures negative/positive correlations; Gumbel captures only positive); Mixture Count K (higher K fits complex data but risks overfitting).
- **Failure signatures**: Mode Collapse (GMM collapses to single component); Zero Gradients in Copula (if lambda_cop is too small, alignment term vanishes).
- **First 3 experiments**: 1) Sanity Check with full data to prove copula alignment adds value; 2) Missing Modality Stress Test with 50% random missingness to validate generative imputation; 3) Hyperparameter Sensitivity sweep of K mixtures and lambda_cop to find optimal balance.

## Open Questions the Paper Calls Out

The paper identifies several limitations and future research directions. One key limitation is that using neural networks to learn the copula parameter may be insufficient due to potential non-convexity in the joint log-likelihood. The authors suggest exploring alternative updating algorithms, such as partial likelihood, to ensure convexity and stable convergence. Another open question is the development of automated mechanisms to select optimal copula families for specific datasets without manual comparison, as the best-performing copula varies across different tasks. Finally, the robustness of imputation performance when the "Missing At Random" assumption is violated remains unexplored, particularly in real-world clinical settings where data is often Missing Not At Random.

## Limitations

- Non-convex optimization may prevent finding global optimum when learning copula parameters via neural networks
- Performance relies on MAR assumption for missing modalities, which may not hold in real clinical settings
- Manual selection of copula family required, with no automated mechanism for optimal choice

## Confidence

- **High**: Mathematical foundation (Sklar's theorem) is sound and well-established
- **Medium**: Superiority of copula-based alignment demonstrated, but specific copula family impact needs investigation
- **Medium**: Generative imputation effective, but robustness to informative missingness untested

## Next Checks

1. Test sensitivity of performance to copula family choice by re-running experiments with different families and analyzing learned dependency parameters
2. Validate imputation mechanism under different missingness patterns, particularly Missing Not At Random scenarios where modality absence correlates with patient severity
3. Analyze learned GMM components by visualizing parameters for subsets of data to ensure meaningful cluster discovery rather than mode collapse or overfitting