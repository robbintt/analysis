---
ver: rpa2
title: Large Language Model Integration with Reinforcement Learning to Augment Decision-Making
  in Autonomous Cyber Operations
arxiv_id: '2509.05311'
source_url: https://arxiv.org/abs/2509.05311
tags:
- uni00000013
- action
- agent
- uni00000003
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of slow and inefficient training
  in autonomous cyber operations (ACO) where reinforcement learning (RL) agents must
  learn from scratch, often taking undesirable actions before learning their consequences.
  The proposed method integrates a large language model (LLM) pretrained on cybersecurity
  data as a teacher to guide the RL agent's decision-making during early training.
---

# Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations

## Quick Facts
- arXiv ID: 2509.05311
- Source URL: https://arxiv.org/abs/2509.05311
- Reference count: 40
- Primary result: LLM-guided PPO agent achieves >2x early rewards and converges ~4,500 episodes faster than baseline in CybORG cyber defense

## Executive Summary
This paper addresses the challenge of slow training in autonomous cyber operations (ACO) where reinforcement learning agents must explore many detrimental actions before learning optimal defenses. The authors propose integrating a large language model (LLM) pretrained on cybersecurity data as a teacher to guide PPO-based RL agents defending a network in CybORG Cage Challenge 2. By combining action masking with an auxiliary loss signal, the guided agent achieves significantly faster convergence while maintaining comparable final performance to agents trained solely through independent RL.

## Method Summary
The method combines inference-only action masking with an auxiliary loss term during PPO training. The LLM processes environment states and outputs recommended actions or distributions, which are used to bias the agent's action sampling and provide a teacher loss. Both the masking influence and auxiliary loss decay by 0.25 every 8 episodes starting at episode 32, with entropy increasing by 5e-4 per decay interval. The approach maps LLM textual outputs to probability distributions via KL divergence rather than single deterministic actions, preventing overconfident policies.

## Key Results
- Guided agent achieves over twice the rewards during early training compared to baseline
- Converges to favorable policy approximately 4,500 episodes faster than baseline PPO agent
- Final performance comparable to baseline despite faster convergence

## Why This Works (Mechanism)

### Mechanism 1
Constraining the agent's action space via LLM-driven action masking significantly improves early-stage rewards by filtering out "obviously unfavorable" exploratory actions. The LLM processes the environment state and outputs recommendations, with an inference-only mask biasing the agent's sampling distribution toward these suggestions. This prevents wasting timesteps on actions known to be detrimental by the pretrained model. Core assumption: LLM possesses sufficient domain knowledge to recommend at least sub-optimal rather than catastrophic actions.

### Mechanism 2
Integrating teacher guidance as an auxiliary loss signal facilitates smoother transition from guided exploration to independent reinforcement learning than masking alone. An auxiliary loss term is added to the PPO actor loss, scaled by a decaying coefficient. This forces the RL agent's internal policy network to update weights to align with the LLM's logic, gradually shifting authority from LLM to the agent's own learned value function. Core assumption: Agent architecture can converge on a policy mimicking the teacher's output distribution without destabilizing primary RL objective gradients.

### Mechanism 3
Mapping the LLM's textual output to a probability distribution rather than a single deterministic action stabilizes learning by preventing overconfident, peaked policies. Instead of forcing a specific action, the system extracts the LLM's softmax probabilities for all valid tokens and trains the RL agent to match this distribution. This prevents the agent's policy from collapsing to a single high-probability action, which creates high variance when the agent eventually explores alternatives. Core assumption: LLM's token probabilities meaningfully correlate with relative utility of actions in the environment.

## Foundational Learning

**Concept:** Proximal Policy Optimization (PPO)
- Why needed here: The paper uses PPO as the baseline RL algorithm. Understanding the balance between actor (policy) and critic (value) is essential to seeing where the auxiliary loss fits in.
- Quick check question: How does the clipping function in PPO prevent the policy from changing too drastically in a single update?

**Concept:** Kullback-Leibler (KL) Divergence
- Why needed here: This is the specific loss function used to align the RL agent's distribution with the LLM's distribution in the final implementation.
- Quick check question: Does minimizing KL divergence force the agent to match the probability of every action, or just the most likely one?

**Concept:** Inference-only Action Masking
- Why needed here: The paper distinguishes between masking during training (affecting gradients) vs. inference (affecting only the action sampled for the environment).
- Quick check question: If an action mask is applied only during inference, how does the agent eventually learn to choose that action without the mask?

## Architecture Onboarding

**Component map:** Environment (CybORG) outputs raw state → Prompt Generator creates text → LLM (Frozen) generates tokens → Logit Extractor creates distribution → PPO Agent uses this for Action Masking (inference) and Auxiliary Loss (training)

**Critical path:** The extraction of the probability distribution from the LLM (Fig. 11). A failure here (e.g., extracting the wrong token probability or mapping "Enterprise1" to the wrong host ID) breaks the alignment mechanism.

**Design tradeoffs:** Latency vs. Sample Efficiency. The paper notes the LLM takes ~3.69s per step vs. milliseconds for the RL agent. This is only viable if the "faster convergence" (4,500 fewer episodes) outweighs the per-step latency cost.

**Failure signatures:**
- **Policy Collapse:** The agent's probability for a single action spikes to >99% early in training, causing high variance later.
- **Prompt Drift:** The LLM ignores the instruction to use generic labels and hallucinates specific IP addresses or non-existent hosts.

**First 3 experiments:**
1. **Verify Logit Extraction:** Feed a controlled state to the LLM and manually verify that the extracted "Distribution" matches the expected optimal defense (e.g., Isolate infected host).
2. **Decay Schedule Ablation:** Run a sweep on the auxiliary loss decay parameter (0.25 vs 0.5) to identify when the agent can "let go" of the teacher without performance dipping.
3. **Prompt Stability Test:** Stress test the Prompt Generator with varying network sizes (hosts) to ensure the "prioritized ordering" logic prevents the LLM from ignoring critical hosts.

## Open Questions the Paper Calls Out

**Open Question 1:** Can encoder-only LLM architectures achieve comparable guidance quality to decoder-only models while reducing computational overhead in ACO environments? The authors state this as future work, noting current decoder-only models require 3.69 seconds per inference on a DGX-H100 GPU.

**Open Question 2:** Does fine-tuning an LLM on environment-specific data produce superior alignment and training efficiency compared to using pretrained models with carefully engineered prompts? The authors list this as a direction for future work, as current prompt engineering compensates for misalignment but wasn't tested against fine-tuning.

**Open Question 3:** Can the peaked distribution instability observed during the teacher-guided phase be mitigated through alternative loss functions or regularization techniques? The paper documents extreme peaked policies (99.96% on one action) causing high variance, but the underlying mechanism and potential solutions remain unexplored.

**Open Question 4:** How does the LLM-guided approach perform against adversarial attacks that specifically target the LLM's decision-making process? The authors identify this as critical future work, as current evaluation assumes a benign environment with no testing of whether adversarial red agents could exploit predictable LLM behavior.

## Limitations
- Results depend on assumption that LLM's action recommendations are consistently aligned with optimal defensive actions, but domain-specific accuracy isn't extensively validated
- PPO hyperparameters are not fully specified, requiring replication from external sources and introducing potential variability
- Computational cost of LLM inference (3.69s per step) may be prohibitive for real-time deployment despite faster convergence

## Confidence

**High Confidence:** The claim that action masking and auxiliary loss significantly improve early-training rewards is well-supported by experimental results and established teacher-guided RL literature.

**Medium Confidence:** The assertion that auxiliary loss facilitates smoother transition from guided to independent learning is supported by ablation results but depends on specific decay schedule as a hyperparameter choice.

**Medium Confidence:** The conclusion that LLM-guided agent achieves "comparable final performance" to baseline is based on convergence to a "favorable policy" within 10,000 episodes, but absolute performance levels aren't explicitly stated for direct comparison.

## Next Checks

1. **PPO Hyperparameter Validation:** Conduct hyperparameter sensitivity analysis for PPO baseline (learning rate, batch size, network architecture) to ensure results aren't contingent on specific, unreported configuration.

2. **LLM Domain Accuracy Test:** Create environment-specific test set of states and ground-truth optimal actions for CybORG Cage Challenge 2. Evaluate LLM's action recommendations on this set to quantify domain-specific accuracy before using as teacher.

3. **Transferability Test:** Apply LLM-guided training method to different network topology within CybORG (e.g., 5-host or 20-host scenario) to test robustness of approach and generality of decay schedule.