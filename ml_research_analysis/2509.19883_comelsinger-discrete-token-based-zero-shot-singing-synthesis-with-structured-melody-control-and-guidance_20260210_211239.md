---
ver: rpa2
title: 'CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured
  Melody Control and Guidance'
arxiv_id: '2509.19883'
source_url: https://arxiv.org/abs/2509.19883
tags:
- pitch
- singing
- synthesis
- speech
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMelSinger, a zero-shot singing voice synthesis
  framework that addresses prosody leakage in prompt-based discrete token systems.
  The core innovation is a coarse-to-fine contrastive learning strategy that disentangles
  pitch from timbre by aligning acoustic tokens with explicit melody conditions while
  suppressing prompt-induced prosodic interference.
---

# CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance

## Quick Facts
- arXiv ID: 2509.19883
- Source URL: https://arxiv.org/abs/2509.19883
- Reference count: 40
- Zero-shot singing synthesis with F0-RMSE of 0.042 and MCD of 4.17 on seen singers

## Executive Summary
CoMelSinger addresses prosody leakage in prompt-based singing synthesis by disentangling pitch from timbre through a coarse-to-fine contrastive learning strategy. The method aligns acoustic tokens with explicit melody conditions while suppressing prompt-induced prosodic interference, enabling zero-shot synthesis for unseen singers. A lightweight singing voice transcription module provides frame-level pitch supervision, achieving state-of-the-art pitch accuracy and timbre consistency on both seen and unseen singers.

## Method Summary
CoMelSinger is a two-stage framework built on MaskGCT, consisting of a singing voice transcription (SVT) module and a fine-tuned S2A module. The SVT module learns to predict discrete pitch tokens from acoustic codes using a 4-layer Transformer with cross-entropy, segment transition, and soft duration losses. The S2A module is fine-tuned with contrastive learning objectives that align acoustic tokens with melody conditions while suppressing prompt-induced prosody. A coarse-to-fine strategy combines sequence-level symmetric contrastive loss with frame-level cosine similarity regression, implemented using LoRA adapters for efficiency.

## Key Results
- F0-RMSE of 0.042 and MCD of 4.17 on seen singers in M4Singer and Opencpop datasets
- SingMOS of 4.32 and SECS of 0.912 demonstrating strong zero-shot generalization
- Outperforms baselines in pitch accuracy, timbre consistency, and zero-shot performance

## Why This Works (Mechanism)
The coarse-to-fine contrastive learning strategy effectively disentangles pitch from timbre by explicitly aligning acoustic tokens with melody conditions while suppressing prompt-induced prosodic interference. The two-stage approach first establishes reliable pitch prediction through supervised SVT training, then refines the synthesis model with contrastive objectives that enforce pitch-timbre separation. The regulated pitch tokens from SVT provide frame-level supervision that guides the model toward accurate melodic reproduction without being influenced by the prosodic characteristics of the acoustic prompts.

## Foundational Learning
- **Contrastive Learning**: Why needed - To align acoustic tokens with melody conditions while suppressing prompt-induced prosody. Quick check - Verify temperature parameter τ is properly tuned to balance positive and negative sample distances.
- **Pitch Token Discretization**: Why needed - To convert continuous pitch information into discrete tokens that can be predicted and controlled. Quick check - Ensure pitch token vocabulary size C matches the SVT classifier head dimensions.
- **MaskGCT Codecs**: Why needed - To convert audio into discrete semantic and acoustic tokens for processing by Transformer modules. Quick check - Validate that pre-trained MaskGCT weights are properly loaded and compatible with the framework.
- **LoRA Fine-tuning**: Why needed - To efficiently adapt the pre-trained S2A module with minimal parameter updates. Quick check - Confirm LoRA adapter ranks and scaling factors match the specified configuration.
- **Sequence-to-Atom Architecture**: Why needed - To enable efficient parallel decoding by converting sequence-level conditioning to frame-level synthesis. Quick check - Verify the transition from T2S to S2A maintains conditioning consistency.
- **Pitch Embedding Integration**: Why needed - To incorporate explicit pitch information into the semantic embeddings for guided synthesis. Quick check - Confirm element-wise addition of pitch embeddings to semantic embeddings is correctly implemented.

## Architecture Onboarding
- **Component Map**: Audio → MaskGCT Codecs → Semantic/Acoustic Tokens → SVT Module → Pitch Tokens → S2A with LoRA → Synthesized Audio
- **Critical Path**: Pitch extraction → SVT training → S2A fine-tuning with contrastive loss → Parallel iterative decoding
- **Design Tradeoffs**: The coarse-to-fine contrastive strategy balances computational efficiency with pitch-timbre disentanglement accuracy, while LoRA fine-tuning enables efficient adaptation with minimal storage overhead.
- **Failure Signatures**: Pitch drift or jittery contours indicate SVT training issues; timbre leakage from prompts suggests insufficient prosody suppression; poor zero-shot performance reveals conditioning implementation problems.
- **First 3 Experiments**: 1) Train SVT module and validate pitch prediction accuracy on held-out data. 2) Fine-tune S2A with contrastive objectives and evaluate F0-RMSE on validation set. 3) Test zero-shot synthesis on unseen singers and measure SECS and MOS-Q.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters including temperature τ for contrastive loss, margin δ for segment transition, and pitch token vocabulary size C remain unspecified
- Zero-shot generalization claims lack ablations to isolate improvements from prompt design versus model architecture
- Perceptual quality improvements reported without statistical significance testing for subjective metrics

## Confidence
- **High**: Pitch accuracy (F0-RMSE) and timbre consistency (MCD) on seen singers
- **Medium**: Zero-shot generalization claims without ablations for prompt conditioning
- **Low**: Perceptual quality improvements without statistical validation

## Next Checks
1. Replicate pitch token prediction accuracy on validation set before S2A fine-tuning to isolate SVT module efficacy.
2. Test model with prompt length variations (L/8 to L/2) to quantify prosody leakage sensitivity.
3. Compare SECS/MOS-Q on unseen singers using same-singer vs. different-singer prompts to validate prosodic disentanglement.