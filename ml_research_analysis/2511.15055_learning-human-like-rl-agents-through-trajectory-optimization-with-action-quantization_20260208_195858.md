---
ver: rpa2
title: Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization
arxiv_id: '2511.15055'
source_url: https://arxiv.org/abs/2511.15055
tags:
- human
- human-like
- rlpd
- agents
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing human-like reinforcement
  learning agents that not only achieve high task performance but also exhibit natural,
  human-like behaviors. The authors propose Macro Action Quantization (MAQ), a framework
  that distills human demonstrations into macro actions via Vector-Quantized Variational
  Autoencoder (VQ-VAE).
---

# Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization

## Quick Facts
- arXiv ID: 2511.15055
- Source URL: https://arxiv.org/abs/2511.15055
- Reference count: 40
- This paper proposes Macro Action Quantization (MAQ), a framework that distills human demonstrations into macro actions via Vector-Quantized Variational Autoencoder (VQ-VAE) to produce RL agents that are both high-performing and human-like in behavior.

## Executive Summary
This paper addresses the challenge of designing human-like reinforcement learning agents that not only achieve high task performance but also exhibit natural, human-like behaviors. The authors propose Macro Action Quantization (MAQ), a framework that distills human demonstrations into macro actions via Vector-Quantized Variational Autoencoder (VQ-VAE). These macro actions constrain the agent to operate within a human-like behavioral space while optimizing for task success.

MAQ was evaluated on four Adroit tasks from D4RL using three different RL algorithms (IQL, SAC, RLPD). Results show that MAQ significantly improves trajectory similarity to human demonstrations, with DTW and Wasserstein distance scores increasing by up to 0.84 and 0.95 respectively. In human evaluation studies, MAQ+RLPD achieved a 39% win rate in Turing tests, outperforming all baseline methods, and received 71% of votes as most human-like in direct comparisons. These findings demonstrate that MAQ successfully produces agents that are both effective at task completion and convincingly human-like in behavior.

## Method Summary
MAQ works by first training a conditional VQ-VAE on human demonstration data to learn a discrete codebook of macro actions. The VQ-VAE encoder maps state-action pairs to latent vectors, which are quantized to the nearest codebook entries. The RL policy then operates over this discrete codebook rather than primitive actions, selecting macro actions that are sequences of H primitive actions. During execution, the agent commits to each macro action for H timesteps before replanning, creating smoother, more human-perceivable trajectories. The policy is trained using standard RL algorithms (IQL, SAC, or RLPD) but with the modified action space, optimizing for both task success and behavioral similarity to human demonstrations.

## Key Results
- MAQ significantly improves trajectory similarity to human demonstrations, with DTW and Wasserstein distance scores increasing by up to 0.84 and 0.95 respectively
- In human Turing tests, MAQ+RLPD achieved a 39% win rate, outperforming all baseline methods
- MAQ+RLPD received 71% of votes as most human-like in direct comparisons with baseline methods
- MAQ consistently improves human-likeness across all four Adroit tasks (Door, Hammer, Pen, Relocate) and all three RL algorithms tested

## Why This Works (Mechanism)

### Mechanism 1: Action Space Constriction via Learned Codebook
Limiting action selection to a discrete codebook of human-derived macro actions improves human-likeness scores without sacrificing task success. The VQ-VAE encoder maps state-action pairs to latent vectors, which are quantized to nearest codebook entries. This forces the agent to select from K discrete macro actions (K=8-32 in experiments), each representing a sequence of H primitive actions (H=1-9). The codebook acts as a behavioral manifold—any policy output maps to a human-consistent trajectory segment. Core assumption: Human demonstrations contain sufficient behavioral diversity to cover task-relevant states. Evidence anchors: [abstract] "distills human demonstrations into macro actions via Vector-Quantized Variational Autoencoder (VQ-VAE)"; [Section 4] "MAQ significantly reduces the macro action space... a vast number of these macro actions can be disregarded in practice since they do not appear in human demonstration". Break condition: If human demonstrations are sparse relative to state space coverage, the codebook will lack viable actions for novel states, causing execution failures or forced suboptimal actions.

### Mechanism 2: Temporal Commitment via Receding-Horizon Execution
Executing macro actions over H timesteps before replanning produces smoother, more human-perceivable trajectories. Rather than replanning at each timestep, MAQ commits to H-step action sequences. The paper frames this as Human-like Receding-Horizon Control (HRC): "executing a larger prefix j of the H-step action sequence generates longer, uninterrupted human-style motion" (Section 3). Longer H values correlate with higher similarity scores—H=9 outperforms H=1 across metrics. Core assumption: Human motor control operates at similar temporal abstraction levels; short-horizon reactive control appears "glitchy" to observers. Evidence anchors: [Section 3] "HRC first proposes a complete H-step sequence... executes all H actions, observes the resulting state st+H, and then replans"; [Section 5.2.3] "As H increases, both trajectory similarity and success rate improve". Break condition: In environments requiring rapid reactive adaptation (e.g., adversarial settings), H>1 may cause catastrophic lag between observation and response.

### Mechanism 3: Dual-Objective Optimization via SMDP Reward Accumulation
Framing human-likeness as trajectory optimization with reward maximization jointly improves both metrics. MAQ converts the MDP to a Semi-MDP where macro actions yield cumulative rewards R(st, mt) = Σγ^i·r(si, ai). The policy selects codebook indices to maximize this while implicitly staying on the human manifold. The discrete action space prevents reward hacking through inhuman micro-adjustments. Core assumption: The human behavioral manifold contains sufficiently high-reward trajectories that the agent need not exit it to succeed. Evidence anchors: [Section 3, Eq. 5] "m*t = arg max mt∈H E[R(st, mt)]" — explicit dual objective; [Section 5.2.3] "longer macro actions not only enhance human-likeness similarity score but also contribute to more effective task execution". Break condition: If optimal task reward requires behaviors absent from demonstrations (e.g., superhuman precision), the constrained policy may plateau below unconstrained RL baselines.

## Foundational Learning

- **Semi-Markov Decision Processes (SMDPs)**: MAQ operates over temporally extended actions; standard MDP formulations assume single-timestep actions. Understanding how value functions compose over macro actions is essential. Quick check question: If a macro action spans 5 timesteps with rewards [1, 2, 1, 3, 1] and γ=0.9, what is the macro-action return?

- **Vector Quantization and VQ-VAE Architecture**: The codebook is the core representational bottleneck. Understanding commitment loss, codebook updating, and reconstruction fidelity determines whether MAQ captures human behavior meaningfully. Quick check question: In VQ-VAE, why does the stop-gradient operation matter for training stability?

- **Trajectory Similarity Metrics (DTW, Wasserstein Distance)**: The paper's claims rest on these metrics improving. DTW handles temporal misalignment; Wasserstein captures distributional similarity. Misinterpreting these leads to wrong conclusions about human-likeness. Quick check question: If two trajectories have identical action distributions but different temporal orderings, which metric will penalize this more?

## Architecture Onboarding

- **Component map:** Conditional VQ-VAE (offline) -> Encoder(s_t, m_t) → latent e → quantize to e_k → Decoder(s_t, e_k) → reconstructed macro action. Stores codebook of size K. -> Policy Network (online): π_θ(s) → logits over K codebook indices → sample k → retrieve e_k → Decoder → execute H-step macro action. -> Environment: Standard RL environment, but receives H primitive actions per policy call.

- **Critical path:** 1. Collect human demonstrations (paper uses 25 trajectories per task) 2. Extract macro actions via sliding window over action trajectories 3. Train VQ-VAE to convergence (paper: 100 episodes, ~5 minutes) 4. Initialize policy with random weights over K actions 5. Train policy using chosen RL algorithm (IQL/SAC/RLPD) with modified action space

- **Design tradeoffs:** **H (macro action length):** Higher H → better similarity scores, worse reactivity. Paper shows H=9 optimal for Adroit, but this is environment-dependent. **K (codebook size):** Larger K → more behavioral coverage, slower inference. Paper uses K=8-32; variance across K is small (Appendix B.1). **Base RL algorithm:** RLPD performs best (52% avg success), SAC struggles (28%)—offline data quality matters for MAQ initialization.

- **Failure signatures:** **Codebook collapse:** If VQ-VAE training fails to distribute codebook entries, only a subset of K will be used → reduced behavioral diversity. **Distribution shift:** If online policy visits states far from demonstration states, codebook lookup returns inappropriate macro actions. **Success-human-likeness gap:** Appendix B.3 shows MAQ+RLPD success on Hammer drops from 72% (horizon 450) to 56% (horizon 200)—human-style behavior may be slower than episode allows.

- **First 3 experiments:** 1. **VQ-VAE reconstruction sanity check:** Train on demonstrations, verify decoder can reconstruct held-out macro actions with low MSE. If reconstruction error is high, codebook is insufficient—increase K or H. 2. **Baseline algorithm comparison:** Train MAQ+SAC, MAQ+IQL, MAQ+RLPD on a single task (e.g., Door). Compare success rates and DTW scores. Confirm RLPD advantage holds. 3. **Macro length ablation:** Sweep H ∈ {1, 3, 5, 7, 9} with fixed K=16. Plot similarity score vs. H to validate paper's finding that longer sequences improve human-likeness. Check if success rate degrades at high H.

## Open Questions the Paper Calls Out
The paper acknowledges that "one limitation is its reliance on human demonstrations to distill macro actions. The quality of these demonstrations can affect the effectiveness of MAQ." Appendix B.2 shows limited experiments with suboptimal demonstrations, testing random subsets and lowest-reward subsets, but does not address noisy, inconsistent, or adversarial demonstrations, nor does it propose automatic quality assessment mechanisms.

## Limitations
- Robustness to distribution shift: MAQ's behavioral manifold may be insufficient for states not covered by human demonstrations, leading to execution failures or unnatural workarounds
- Codebook capacity constraints: With K=8-32 codebook entries, MAQ constrains the agent to a finite behavioral vocabulary that may lack coverage for certain state-action regions
- Time efficiency trade-off: Human-like (slower) movements may conflict with strict time limits, as shown by MAQ+RLPD's success rate on Hammer improving from 56% to 72% when episode horizon was extended from 200 to 450 steps

## Confidence
- **High Confidence:** The VQ-VAE training pipeline and macro action execution framework are well-specified and reproducible. The discrete action space formulation is standard.
- **Medium Confidence:** The trajectory similarity metrics (DTW, Wasserstein) and their implementation details. While the metrics themselves are standard, implementation choices (normalization, solver parameters) can significantly affect scores.
- **Low Confidence:** Generalization to tasks beyond Adroit and the scalability of human-likeness as a design constraint. The paper's ablation studies focus on Adroit; performance on more complex or safety-critical tasks remains untested.

## Next Checks
1. **Distribution Shift Stress Test:** Evaluate MAQ on tasks where human demonstrations are incomplete or suboptimal (e.g., intentionally add noise to demonstrations). Measure success rate and similarity score degradation to quantify robustness.

2. **Codebook Coverage Analysis:** For each task, compute the fraction of VQ-VAE codebook entries used during policy execution. Identify states where the policy falls back to less-used entries and analyze whether these correspond to novel states.

3. **Human-Likeness vs. Task Performance Pareto Frontier:** Systematically vary H and K to map the trade-off between similarity score and success rate. Identify whether human-likeness can be tuned independently of task performance or if they are inherently coupled.