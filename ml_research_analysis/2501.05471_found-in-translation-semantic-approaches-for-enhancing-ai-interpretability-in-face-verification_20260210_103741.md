---
ver: rpa2
title: 'Found in Translation: semantic approaches for enhancing AI interpretability
  in face verification'
arxiv_id: '2501.05471'
source_url: https://arxiv.org/abs/2501.05471
tags:
- similarity
- semantic
- concepts
- explanations
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study advances explainable AI for face verification by integrating
  semantic concepts inspired by human cognitive processes. The approach combines global
  concept extraction using context-aware methods (LIME, MAGE) with local similarity
  mapping based on user-defined facial landmarks.
---

# Found in Translation: semantic approaches for enhancing AI interpretability in face verification

## Quick Facts
- arXiv ID: 2501.05471
- Source URL: https://arxiv.org/abs/2501.05471
- Reference count: 40
- Key outcome: Semantic explanations (78% preference) outperform pixel heatmaps in face verification interpretability, with LIME excelling in concept extraction and LLM-generated text improving accessibility.

## Executive Summary
This study advances explainable AI for face verification by integrating semantic concepts inspired by human cognitive processes. The approach combines global concept extraction using context-aware methods (LIME, MAGE) with local similarity mapping based on user-defined facial landmarks. Quantitative experiments show LIME outperforms other methods in identifying globally important semantic regions. User studies (n=61) demonstrate strong preference for semantic explanations (78%) over traditional pixel-based heatmaps, with 76% finding the explanations clear. The most detailed semantic set (SET 2) was preferred by non-technical users (61%) while technical users valued precision and completeness. Large language models were integrated to generate accessible textual explanations, with Beagle14-7B being most preferred. The framework successfully bridges the comprehension gap between AI model decisions and human understanding, fostering trust in critical face verification applications.

## Method Summary
The framework extracts semantic features from face verification models by defining facial regions using Mediapipe landmarks. Three semantic sets (13, 13, and 30 features) aggregate landmarks into meaningful regions. XAI methods (LIME, MAGE, KernelSHAP) compute region importance per image, with BORDA count aggregating rankings across images to produce global ordering. Single removal algorithm generates per-pair similarity/dissimilarity maps weighted by global importance. Large language models translate numerical contribution tables into natural language explanations, with Beagle14-7B preferred for structured output. The approach was validated on FERET dataset using FaceNet models trained on CasiaWebFace and VGGFace2.

## Key Results
- LIME outperformed other XAI methods in occlusion experiments, identifying globally important semantic regions with highest fidelity
- 78% of users preferred semantic explanations over pixel heatmaps, with 76% finding explanations clear
- Non-technical users preferred detailed semantic set (SET 2) at 61%, while technical users valued precision and completeness
- Beagle14-7B LLM generated most preferred textual explanations with well-organized structure
- Occlusion-based validation showed LIME maintains lower embedding distance compared to random concept removal

## Why This Works (Mechanism)

### Mechanism 1
Mapping pixel-level attributions to user-defined semantic regions improves human comprehension of face verification decisions. Mediapipe landmarks define facial regions (e.g., "left eye," "nose"), which serve as aggregation units for XAI explanations. Instead of showing raw pixel heatmaps, the system attributes similarity/dissimilarity scores to named semantic concepts. Core assumption: Users' mental models of face comparison align with semantically segmented regions rather than arbitrary pixel clusters. Evidence: "using semantic features defined by user-selected facial landmarks to generate similarity maps and textual explanations." Break condition: If users define semantically incoherent regions (e.g., random pixel groupings), the explanation's interpretability advantage disappears.

### Mechanism 2
Global concept importance rankings derived from aggregated local explanations better identify model-relevant facial regions than single-image explanations. XAI methods compute region importance per image. BORDA count aggregates rankings across images to produce a global ordering. This global ordering weights the contribution of each semantic region in local similarity maps. Core assumption: Aggregated rankings generalize across the dataset; the model consistently relies on specific semantic regions. Evidence: "the best method for this type of occlusion-based experiment is LIME for all the models and sets." Break condition: If the test distribution diverges significantly from the aggregation set, global rankings may misweight irrelevant regions.

### Mechanism 3
LLMs can translate numerical contribution tables into accessible textual explanations preferred by users over raw tables or heatmaps. A structured prompt provides the cosine similarity score, a contribution table with semantic region names and values, and color-map context. LLMs generate natural-language summaries. Beagle14-7B was most preferred for structured, descriptive output. Core assumption: The LLM faithfully interprets numerical data without hallucinating unsupported claims. Evidence: "Beagle14-7B is the most preferred transcription style, praised for its well-organized text structure." Break condition: If prompts lack constraints, LLMs may produce verbose or factually inconsistent outputs.

## Foundational Learning

- **Concept: LIME and KernelSHAP for image attribution**
  - Why needed here: These methods identify which input regions most influence model output. The paper compares them for concept extraction.
  - Quick check question: Can you explain why LIME uses a local surrogate model while KernelSHAP uses Shapley values?

- **Concept: Face verification embeddings and cosine similarity**
  - Why needed here: The explanation framework interprets similarity scores derived from embedding vectors. Understanding how embeddings encode identity is essential.
  - Quick check question: Given two 512-dimensional embeddings, how would you compute their cosine similarity?

- **Concept: Semantic segmentation vs. superpixel segmentation**
  - Why needed here: The paper contrasts human-defined semantic regions with traditional superpixel-based heatmaps.
  - Quick check question: What is the key difference between a superpixel region and a semantically labeled region like "left eye"?

## Architecture Onboarding

- **Component map:** Semantic definition layer (Mediapipe landmarks → region masks) → Concept extraction layer (XAI methods compute region importance) → Aggregation layer (BORDA count produces global ranking) → Similarity map generator (Single removal algorithm with global weights) → LLM transcription layer (Prompt + contribution table → textual explanation)

- **Critical path:** Semantic definition → XAI-based concept extraction → global aggregation → weighted similarity map → LLM transcription. Errors in landmark projection or mask creation propagate downstream.

- **Design tradeoffs:**
  - Granularity vs. interpretability: SET 2 (30 features) provides more detail but may overwhelm non-technical users; SET 0 (13 features) is simpler but less precise
  - LIME vs. KernelSHAP vs. MAGE: LIME outperformed in occlusion experiments, but MAGE may excel with more concepts occluded
  - LLM selection: Beagle14-7B preferred for structure; CodeLlama offers practical examples; Zephyr is concise. Trade-off between detail and brevity

- **Failure signatures:**
  - LLM outputs that misattribute negative values as similarities
  - Low occlusion impact curves indicating concept extraction failure
  - User confusion over table values on mobile displays

- **First 3 experiments:**
  1. Reproduce occlusion experiment: Apply successive occlusion of top-ranked concepts; verify LIME outperforms random ordering using Euclidean distance
  2. Semantic set sensitivity test: Run pipeline with SET 0, SET 1, and SET 2; compare similarity map coherence and user preference
  3. Prompt ablation for LLM: Remove one constraint at a time; measure output length and factual consistency

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed semantic-based XAI framework maintain its fidelity and user preference ratings when applied to computer vision tasks other than face verification? The authors state in the Conclusion that "Expanding the framework to include more diverse datasets and testing its applicability across different AI tasks will also be crucial steps in advancing the utility and acceptance of explainable AI." This is unresolved because the current study validated the methodology exclusively on face verification using the FERET, CasiaWebFace, and VGGFace2 datasets; it is unknown if the semantic extraction methods perform similarly on non-facial data or different model architectures. Quantitative experiments applying the framework to other domains (e.g., medical imaging or autonomous driving) showing similar concept extraction fidelity and user study results would resolve this.

### Open Question 2
How can the factual consistency of LLM-generated textual explanations be guaranteed to align strictly with the input similarity data? Appendix A.3.3 details a methodological limitation where "outputs generated by the models sometimes contain statements that do not align with the input data," citing instances where Zephyr misinterpreted negative values as similarities. While the authors note that prompt engineering reduces errors, the inherent stochasticity of LLMs still leads to occasional hallucinations that contradict the provided numerical tables. A comparative evaluation of different LLMs or prompt strategies using a metric for factual consistency would resolve this.

### Open Question 3
Does the strong user preference for detailed semantic explanations (Set 2) persist across broader, more demographically diverse populations? The Conclusion notes that the "survey sample was limited to 61 people, and there may be a 'bubble' effect due to the similarity in age and background between the authors and some participants." The user study demographic was skewed (72% aged 25-34; 55% holding Master's degrees), which may have inflated the preference for high granularity (Set 2) over simpler options, as non-technical users in the sample were a minority. A large-scale user study (n > 200) with stratified sampling across age, education level, and technical expertise would validate if the 78% preference holds true.

## Limitations
- Weighting term $W(A,B)_n$ in similarity map equation is referenced but not mathematically defined, creating ambiguity in reproduction
- Specific hyperparameters for XAI methods (LIME sample count, KernelSHAP kernel width) are not reported, potentially affecting reproducibility
- LLM faithfulness is highly prompt-dependent, with observed hallucinations in Zephyr output indicating reliability concerns

## Confidence

- **High Confidence:** User preference for semantic explanations over pixel heatmaps (78% preference, n=61) and clarity ratings (76%) are directly measured and reported with statistical backing
- **Medium Confidence:** The superiority of LIME in occlusion experiments is supported by quantitative results, though exact experimental protocol depends on unresolved hyperparameter choices
- **Low Confidence:** Claims about LLM faithfulness are qualified by the authors themselves; observed hallucinations in Zephyr output and the need for structured prompts indicate reliability is highly prompt-dependent

## Next Checks

1. **Reproduce occlusion experiment** with a held-out image set to verify LIME's superiority over random ordering using Euclidean distance in embedding space
2. **Run semantic set sensitivity test** comparing SET 0, SET 1, and SET 2 on identical image pairs to evaluate impact on similarity map coherence and user preference
3. **Conduct prompt ablation study** for LLM text generation by removing constraints one at a time to measure impact on output length, factual consistency, and user comprehension