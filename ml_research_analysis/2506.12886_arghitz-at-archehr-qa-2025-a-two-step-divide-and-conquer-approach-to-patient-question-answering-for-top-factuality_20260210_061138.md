---
ver: rpa2
title: 'ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient
  Question Answering for Top Factuality'
arxiv_id: '2506.12886'
source_url: https://arxiv.org/abs/2506.12886
tags:
- clinical
- question
- sentences
- patient
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents three approaches to the ArchEHR-QA 2025 shared
  task on patient question answering. The methods include an end-to-end prompt-based
  baseline and two two-step approaches that first extract essential sentences from
  clinical text and then generate answers.
---

# ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality

## Quick Facts
- arXiv ID: 2506.12886
- Source URL: https://arxiv.org/abs/2506.12886
- Reference count: 37
- Two-step re-ranker approach achieves best factuality (0.44 overall score, rank 8/30, top in factuality)

## Executive Summary
This work presents three approaches to the ArchEHR-QA 2025 shared task on patient question answering using Electronic Health Records (EHRs). The methods include an end-to-end prompt-based baseline and two two-step approaches that first extract essential sentences from clinical text and then generate answers. The two-step re-ranker approach performs best, achieving an overall score of 0.44, ranking 8th out of 30 on the leaderboard, and securing the top position in overall factuality. The results highlight the importance of selecting the right approach for each subtask, showing that dividing the task and using tailored methods improves performance. Notably, the best system surpasses the organizers' zero-shot baseline without relying on external knowledge.

## Method Summary
The best-performing system employs a two-step re-ranker approach. First, a Jina reranker scores sentence relevance given the patient narrative and technical question, classifying sentences as essential via a threshold determined using ROC curves and the Youden index on the dev set. Second, Aloe 8B generates the answer using only the selected essential sentences, without citations in the prompt. Post-processing enforces the 75-word limit and adds citations by matching each output sentence to the most similar essential sentence. This approach isolates evidence selection from answer generation, allowing each subtask to use methods optimized for its specific objective.

## Key Results
- The two-step re-ranker approach achieves the highest overall score of 0.44, ranking 8th out of 30 teams
- The system secures the top position in overall factuality with a strict F1 score of 0.605
- The best system outperforms the organizers' zero-shot baseline without using external knowledge
- Factuality scores (0.605 strict F1) significantly exceed relevance scores (0.276 test set), indicating generation quality as the bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition improves factuality by isolating evidence selection from answer generation.
- Mechanism: The two-step approach separates (1) identifying essential EHR sentences from (2) synthesizing patient-facing responses. This allows each subtask to use methods optimized for its specific objective—re-ranking models for relevance detection, LLMs for language generation.
- Core assumption: Evidence quality drives factuality more than generation quality; errors in citation can be minimized independently of fluency.
- Evidence anchors:
  - [abstract] "Results indicate that the re-ranker based two-step system performs best, highlighting the importance of selecting the right approach for each subtask."
  - [Section 7] "more critical than merely dividing the task into smaller and simpler subtasks, is the choice of an appropriate method for each subtask"
  - [corpus] UTSA-NLP at ArchEHR-QA 2025 confirms two-step LLM approaches for this task with similar decomposition.

### Mechanism 2
- Claim: Re-ranking models outperform LLM prompting for binary relevance classification in clinical text.
- Mechanism: Re-rankers (Jina, BAAI BGE, Alibaba GTE) score query-sentence pairs using learned similarity, then threshold scores (via ROC/Youden index) to classify sentences as essential. This bypasses LLM reasoning overhead and format compliance issues.
- Core assumption: Sentence-level relevance can be captured via semantic similarity to a combined query (patient narrative + clinician question) without multi-hop reasoning.
- Evidence anchors:
  - [Section 6.2.1.2] "determine sentence relevance... by the output scores provided by the reranker and establish a threshold"
  - [Table 4] Jina achieves strict F1=0.535 vs. best prompting F1=0.50 (Table 2)
  - [corpus] No direct corpus comparison of re-rankers vs. prompting for clinical sentence selection—limited external validation.

### Mechanism 3
- Claim: Smaller domain-adapted models (8B) can outperform larger general-purpose models (70B) on clinical QA with structured prompts.
- Mechanism: Aloe 8B (healthcare-fine-tuned) achieves 0.388 overall vs. Llama 3.3 70B's 0.340 in end-to-end evaluation (Table 1). Domain adaptation + one-shot prompting compensates for scale.
- Core assumption: Clinical QA benefits more from domain vocabulary alignment than raw parameter count.
- Evidence anchors:
  - [Section 6.1] "the smaller Aloe Beta (8B) model outperforms larger models in both Factuality and Relevance"
  - [Section 7] "larger models do not necessarily outperform smaller ones in complex tasks requiring not only medical expertise but also argumentative, summarization, and rewriting skills"
  - [corpus] Corpus papers do not directly compare Aloe vs. Llama scale effects—weak external corroboration.

## Foundational Learning

- Concept: **Re-ranking for Retrieval**
  - Why needed here: Core to the best-performing system; understanding cross-encoder vs. bi-encoder architectures, scoring functions, and threshold calibration is essential.
  - Quick check question: Given a query and 50 sentences, how would you determine which 5 are most relevant? What tradeoffs exist between precision and recall in threshold selection?

- Concept: **Prompt Engineering for Structured Output**
  - Why needed here: Both end-to-end and two-step systems rely on prompts with role, CoT, and one-shot examples to enforce citation format (|ID|) and word limits.
  - Quick check question: Design a prompt that ensures an LLM outputs exactly one sentence per line with a citation. What failure modes would you anticipate?

- Concept: **Evaluation Metrics for Grounded Generation**
  - Why needed here: The task uses strict/lenient F1 for factuality (sentence-level) and BLEU/ROUGE/BERTScore/AlignScore/Medcon for relevance (generation quality).
  - Quick check question: Why might a system score high on fluency metrics but low on strict F1? How do these metrics interact?

## Architecture Onboarding

- Component map: Patient narrative + clinician question → Jina reranker → essential sentences → Aloe 8B generation → citation matching → 75-word limit → final output
- Critical path: Re-ranker scoring → threshold application → filtered sentences → LLM generation → citation matching → truncation. Errors in threshold calibration cascade directly to factuality scores.
- Design tradeoffs:
  - Re-ranker vs. LLM extraction: Re-ranker faster and more precise; LLM extraction enables reasoning but suffers from format errors and lower F1.
  - Strict vs. lenient evaluation: Strict optimizes for essential-only citations; lenient allows supplementary sentences, improving recall but potentially diluting precision.
  - Model size vs. domain adaptation: 8B domain-adapted (Aloe) outperforms 70B general (Llama) on this task; larger models add latency without proportional gains.
- Failure signatures:
  - Citation format errors: Missing vertical bars, incorrect delimiters—requires post-processing regex fixes.
  - Word count overflow: LLMs often exceed 75 words; truncation may sever citations mid-sentence.
  - Threshold overfitting: ROC-determined threshold on dev set (20 documents) may not generalize to test distribution—Table 5 shows dev→test factuality drops in some configurations.
  - Low relevance despite high factuality: Best system ranked #1 in factuality but Relevance lagged (0.276 test vs. 0.605 factuality F1)—suggests answer quality is bottlenecked by generation, not evidence.
- First 3 experiments:
  1. Re-ranker ablation: Compare Jina, BAAI BGE, Alibaba GTE with identical thresholds on held-out split; measure strict/lenient F1 gap.
  2. Threshold sensitivity analysis: Vary decision threshold ±0.1 around Youden-optimal point; plot precision-recall curves to identify robust operating range.
  3. Generation model swap: Replace Aloe 8B with Mistral 7B or Gemma 9B in second step while keeping re-ranker fixed; isolate contribution of generation quality to Relevance metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative response formulation methods, such as text-to-text models like T5, significantly improve the Relevance score without compromising the high Factuality achieved by the current system?
- Basis in paper: [explicit] The authors state in the Limitations section: "there is still room for improvement in order to enhance the Relevance overall score—for instance, by trying text-to-text models like T5."
- Why unresolved: The current two-step approach prioritizes factuality through sentence extraction but results in lower Relevance scores compared to the end-to-end baseline.
- What evidence would resolve it: Experimental results comparing the current Aloe-based generation step against T5 or similar generative models on the ArchEHR-QA test set.

### Open Question 2
- Question: Would incorporating Retrieval Augmented Generation (RAG) to introduce external medical knowledge improve the system's performance over the strictly internal EHR-based approach?
- Basis in paper: [explicit] The authors note that "Information Retrieval methods such as Retrieval Augmented Generation (RAG) are not employed... We leave these methods for future work."
- Why unresolved: The study deliberately restricted the input context to the provided EHR notes to assess performance without external data, leaving the potential benefit of external knowledge untested.
- What evidence would resolve it: A comparison of the current baseline against a RAG-augmented variant on the shared task leaderboard.

### Open Question 3
- Question: To what extent would fine-tuning or increasing the number of few-shot examples improve performance compared to the current one-shot prompt engineering approach?
- Basis in paper: [explicit] The authors acknowledge: "Due to the lack of training samples, we do not extend the prompt examples and neither perform any finetuning."
- Why unresolved: The current results rely on zero-shot or one-shot in-context learning; the impact of weight updates or richer prompting context remains unknown due to data constraints.
- What evidence would resolve it: Performance metrics from a fine-tuned model or a many-shot prompting setup, should more training data become available.

### Open Question 4
- Question: Does the threshold optimization method used for the re-ranker generalize effectively from the development set to the test set?
- Basis in paper: [inferred] The paper states regarding the re-ranker threshold: "a potential limitation of this method is that the threshold is determined based on the dev set and may not generalize well to the test set if the data distribution is different."
- Why unresolved: The authors calculate the optimal cutoff using the Youden index on the dev set but do not verify if this specific threshold remains optimal under the distribution of the hidden test set.
- What evidence would resolve it: An analysis of the score distributions on the test set or the use of dynamic thresholding techniques to verify stability across datasets.

## Limitations
- The dev set contains only 20 documents, making threshold calibration potentially overfit and limiting generalization to the test set
- The strict/lenient F1 gap indicates potential inconsistencies in what constitutes "essential" evidence across annotators
- The factuality-relevance trade-off is pronounced - the best system achieves top factuality but lags in relevance metrics, suggesting generation quality remains a bottleneck

## Confidence

- High Confidence: Task decomposition improves factuality through isolated evidence selection (supported by strong F1 differences between approaches)
- Medium Confidence: Re-rankers outperform LLM prompting for sentence classification (supported by Table 4 but limited external corpus validation)
- Medium Confidence: Domain adaptation compensates for model size (Aloe 8B vs. Llama 70B results are clear but lack broader comparative context)

## Next Checks
1. **Threshold Generalization Test**: Apply the Jina reranker threshold determined on dev set to a held-out validation split from the same dataset and measure factuality drop to assess overfitting risk
2. **Citation Matching Method Audit**: Implement the similarity-based citation matching with multiple similarity metrics (cosine, sentence-BERT) and measure how different methods affect strict F1 scores
3. **Generation Model Swap Experiment**: Replace Aloe 8B with a different domain-adapted model (e.g., Meditron 7B) in the second step while keeping re-ranker fixed to isolate the contribution of generation quality to relevance metrics