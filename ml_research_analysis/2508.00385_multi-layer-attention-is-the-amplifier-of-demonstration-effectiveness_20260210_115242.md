---
ver: rpa2
title: Multi-Layer Attention is the Amplifier of Demonstration Effectiveness
arxiv_id: '2508.00385'
source_url: https://arxiv.org/abs/2508.00385
tags:
- gradient
- demonstrations
- flow
- demonstration
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes why some demonstrations fail to improve in-context
  learning (ICL) performance. Through gradient flow analysis in linear self-attention
  models, it shows that ineffective demonstrations occur when the model has already
  learned their information or they are irrelevant to the user query.
---

# Multi-Layer Attention is the Amplifier of Demonstration Effectiveness

## Quick Facts
- **arXiv ID**: 2508.00385
- **Source URL**: https://arxiv.org/abs/2508.00385
- **Reference count**: 40
- **Primary result**: Multi-layer transformers amplify the effectiveness gap between demonstrations, with GRAD S achieving 6.8% relative improvement over strongest baselines.

## Executive Summary
This paper analyzes why some demonstrations fail to improve in-context learning (ICL) performance. Through gradient flow analysis in linear self-attention models, it shows that ineffective demonstrations occur when the model has already learned their information or they are irrelevant to the user query. The key finding is that multi-layer transformers amplify the effectiveness gap between demonstrations - as layers increase, the gradient flow disparity widens, causing models to focus more on effective demonstrations. Based on this, the authors propose GRAD S, a gradient-flow-based demonstration selection method that ensures selected demonstrations provide substantial information contribution. Experiments across four LLMs and five datasets show GRAD S achieves 6.8% relative improvement over strongest baselines by effectively selecting demonstrations that maximize gradient flow with the query.

## Method Summary
The paper proposes GRAD S, a demonstration selection method based on gradient flow analysis. The method computes the gradient of the predicted answer with respect to each demonstration, using this as a proxy for the demonstration's information contribution. Demonstrations are selected based on their gradient flow magnitude relative to the query, ensuring they provide both relevance and unlearned knowledge. The theoretical framework uses linear self-attention models to derive that gradient flow disparity between demonstrations amplifies with increasing layers, leading to focus on effective demonstrations.

## Key Results
- Multi-layer transformers amplify effectiveness gaps between demonstrations as layer depth increases
- GRAD S achieves 6.8% relative improvement over strongest baselines across four LLMs and five datasets
- Ineffective demonstrations occur when information is either already learned or irrelevant to the query
- The gradient flow magnitude serves as a valid metric for demonstration effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Gradient Flow as a Proxy for Information Contribution
The magnitude of gradient flow from a demonstration to the predicted answer serves as a valid metric for the demonstration's effectiveness. By defining effectiveness via gradient flow, the paper posits that a demonstration is ineffective if the gradient is zero, meaning the model output is insensitive to the demonstration input.

### Mechanism 2: Dual Determinants of Ineffectiveness
Ineffectiveness is caused by two distinct factors: low relevance to the query or high redundancy with the model's existing knowledge base. Equation 2 decomposes the gradient into terms dependent on query similarity and model knowledge, with demonstrations failing if they add no new knowledge or have no relevance.

### Mechanism 3: Layer-Wise Amplification of Effectiveness Disparity
Multi-layer transformer architectures act as amplifiers, widening the gap between effective and ineffective demonstrations as depth increases. Theorem 1 states that the ratio of gradient flows between two demonstrations increases with the number of layers, causing effective demonstrations to dominate signal propagation in deeper layers.

## Foundational Learning

**Linear Self-Attention (LSA)**: The theoretical claims rely entirely on approximating the Transformer as a linear system to derive closed-form gradient expressions. Quick check: Can you explain how replacing the softmax activation with a linear function allows for the analytical derivation of gradient flow?

**Chain Rule in Deep Networks**: Understanding the "amplifier" mechanism requires comprehending how gradients compound multiplicatively across layers. Quick check: How does the product of layer-wise gradients lead to the amplification of disparity?

**In-Context Learning (ICL) Setup**: The method specifically targets the selection of demonstrations to boost ICL performance. Quick check: What is the specific input structure used in the 1-shot setting for this analysis?

## Architecture Onboarding

**Component map**: Input Encoder -> Theoretical Core (LSA) -> Selection Metric -> Retrieval System

**Critical path**: Offline Encoding (Encode all demos) → Online Gradient Calculation (Compute Eq 2 for query) → Top-K Selection

**Design tradeoffs**: The method sacrifices exact modeling of non-linear attention for computational tractability and theoretical clarity. It assumes the final layer's gradient is the most discriminative signal, which may miss intermediate-layer reasoning shortcuts.

**Failure signatures**: Performance degrades on models where residual connections significantly dampen the amplification effect, or if the model is so small that the knowledge base is insufficient to utilize the demonstrations.

**First 3 experiments**:
1. Validation of Eq 2: Replicate Figure 2 on a target model to confirm performance correlates with both Relevance and Unlearned Knowledge
2. Amplification Check: Plot gradient flow ratios across layers to verify the "amplifier" hypothesis for your specific model architecture
3. Ablation on Layers: Test GradS performance using gradient flows from intermediate layers vs. the final layer to find optimal selection depth

## Open Questions the Paper Calls Out

### Open Question 1
How do the factors of "unlearned knowledge" and "query relevance" quantitatively interact when they conflict? The current formalization defines effectiveness only when both conditions are met, leaving the trade-off scenario undefined theoretically.

### Open Question 2
How do non-linear architectural components, such as Softmax normalization and MLP layers, specifically alter the amplification of demonstration effectiveness in standard Transformers? The theoretical proofs assume linear attention and do not account for dampening effects of non-linearities.

### Open Question 3
Does the amplification of demonstration effectiveness persist in many-shot settings where interactions between multiple demonstrations occur? The theoretical derivation models the gradient flow of a single demonstration; it is unclear if the amplification dynamic holds when the model must integrate competing gradient signals from many shots.

## Limitations

- The theoretical foundation relies on linear self-attention models, a significant simplification of actual transformer architectures
- The amplification effect appears to depend on model capacity, potentially not scaling uniformly across all model sizes
- The method assumes model parameters remain static during inference, treating learned knowledge as a fixed reference point

## Confidence

**High Confidence**: The empirical observation that demonstration selection improves ICL performance (6.8% relative improvement) across four models and five datasets

**Medium Confidence**: The theoretical gradient flow framework as a proxy for demonstration effectiveness, though generalization to complex transformer architectures remains uncertain

**Medium Confidence**: The dual-determinant model of ineffectiveness (relevance vs. learned knowledge), which may oversimplify complex interactions in actual transformer reasoning

## Next Checks

1. **Non-Linear Attention Validation**: Replicate the gradient flow analysis using the actual softmax-based attention mechanism rather than the linear approximation

2. **Cross-Model Scalability Test**: Evaluate GRAD S on models significantly smaller (< 7B parameters) and larger (> 100B parameters) than those tested

3. **Dynamic Knowledge Baseline**: Implement a variant of GRAD S that tracks gradient flow changes during the actual ICL task rather than using static pre-computed values