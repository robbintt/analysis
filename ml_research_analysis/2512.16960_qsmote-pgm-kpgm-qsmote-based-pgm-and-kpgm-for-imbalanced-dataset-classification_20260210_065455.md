---
ver: rpa2
title: 'QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification'
arxiv_id: '2512.16960'
source_url: https://arxiv.org/abs/2512.16960
tags:
- qsmote
- copies
- kpgm
- quantum
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses imbalanced dataset classification by proposing
  three novel Quantum Synthetic Minority Oversampling Technique (QSMOTE) variants:
  KNN-based, Fidelity-based, and Margin-based. These methods integrate quantum-inspired
  principles such as fidelity weighting, centroid-driven generation, and margin-aware
  filtering to generate balanced training data.'
---

# QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification

## Quick Facts
- **arXiv ID**: 2512.16960
- **Source URL**: https://arxiv.org/abs/2512.16960
- **Reference count**: 23
- **Primary result**: PGM with stereo encoding and n_copies=2 achieves highest overall accuracy (0.8512) and F1-score (0.8234) on Telco Customer Churn dataset

## Executive Summary
This paper addresses imbalanced dataset classification by proposing three novel Quantum Synthetic Minority Oversampling Technique (QSMOTE) variants: KNN-based, Fidelity-based, and Margin-based. These methods integrate quantum-inspired principles such as fidelity weighting, centroid-driven generation, and margin-aware filtering to generate balanced training data. The oversampled data is then used to train quantum-inspired classifiers, Pretty Good Measurement (PGM) and kernelized PGM (KPGM), which leverage Hilbert-space geometry for classification. Experiments on the Telco Customer Churn dataset show that both PGM and KPGM classifiers consistently outperform the Random Forest baseline, with PGM demonstrating superior performance when using stereo encoding and n_copies=2.

## Method Summary
The method combines three QSMOTE oversampling variants with quantum-inspired PGM and kPGM classifiers. QSMOTE variants include KNN-based interpolation with k=5 neighbors, Fidelity-based sampling using KMeans centroids with fidelity weighting, and Margin-based filtering using logistic regression confidence scores. The oversampled data is then encoded into quantum density matrices using either amplitude or stereographic encoding, reduced to 16 dimensions via PCA, and classified using PGM (with measurement operators) or kPGM (with Gram matrix formulation). The classifiers are evaluated on the Telco Customer Churn dataset with 5-fold cross-validation and compared against a Random Forest baseline.

## Key Results
- PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), with recall of 0.7865
- KPGM demonstrates competitive and stable performance across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude)
- All QSMOTE variants combined with PGM and kPGM classifiers consistently outperform the Random Forest baseline
- KNN-based QSMOTE shows consistently lower performance across all configurations, suggesting potential issues with sparse minority regions

## Why This Works (Mechanism)
The effectiveness stems from quantum-inspired oversampling that generates synthetic samples aligned with the quantum geometry of the feature space, followed by classification using density matrix-based operators that capture quantum correlations. The fidelity-based weighting in QSMOTE ensures synthetic samples are generated in regions with high quantum information content, while the PGM classifier leverages Hilbert-space geometry through measurement operators to make decisions. The kPGM variant further improves scalability by reducing computational complexity from feature dimension to sample size.

## Foundational Learning
- **Quantum fidelity weighting**: Measures similarity between quantum states; needed to ensure synthetic samples preserve quantum geometric properties; quick check: verify fidelity values between synthetic and real samples fall within expected range [0,1]
- **Density matrix encoding**: Represents feature vectors as quantum states; needed for quantum-inspired classification; quick check: confirm density matrices are positive semi-definite and trace-normalized
- **Hilbert-space measurement operators**: Used for quantum classification decisions; needed to map quantum states to class labels; quick check: verify measurement operators are Hermitian and properly normalized
- **Kernel trick in quantum space**: Allows efficient computation without explicit feature mapping; needed for scalability in kPGM; quick check: confirm Gram matrix is positive semi-definite
- **Quantum margin-aware filtering**: Identifies reliable synthetic samples based on classifier confidence; needed to avoid noisy oversampling; quick check: verify margin threshold correctly filters ambiguous samples
- **Tensor power constructions**: Enables n-copies quantum states for enhanced representation; needed for boosting classification performance; quick check: verify tensor products maintain proper normalization

## Architecture Onboarding
- **Component map**: QSMOTE variants (KNN, Fidelity, Margin) -> Density matrix encoding (Amplitude, Stereographic) -> PCA dimensionality reduction (16D) -> PGM/kPGM classification -> 5-fold CV evaluation
- **Critical path**: QSMOTE oversampling → Quantum encoding → PCA → Classifier training → Cross-validation
- **Design tradeoffs**: PGM offers better accuracy but higher computational cost (cubic in feature dimension), while kPGM trades some accuracy for scalability (linear in sample size)
- **Failure signatures**: KNN-QSMOTE produces noisy samples causing low recall (<0.2); n_copies>2 causes exponential memory growth; numerical instability in σ^(-1/2) computation
- **First experiments**: 1) Implement QSMOTE variants and verify synthetic sample distribution; 2) Test density matrix encoding with simple 2D inputs; 3) Run PGM with n_copies=1 on small dataset subset

## Open Questions the Paper Calls Out
- How do PGM and kPGM classifiers perform across diverse imbalanced datasets beyond the single Telco Customer Churn benchmark tested?
- What is the optimal number of quantum copies (n_copies) beyond the tested values of 1 and 2, and how does performance saturate or degrade?
- What are the actual computational time and memory requirements of PGM versus kPGM at scale, and does kPGM's theoretical efficiency advantage materialize in practice?
- Why does KNN-based QSMOTE consistently underperform across all classifier configurations, and can modifications improve its effectiveness?

## Limitations
- Encoding formulas for amplitude and stereographic mappings are not fully specified in the paper
- The interaction between PCA dimensionality reduction and quantum encoding is unclear
- The paper reports 5-fold CV but doesn't specify whether hyperparameter tuning was performed within each fold
- Specific backend configurations for Qiskit AerSimulator beyond 'automatic' and 1024 shots are not documented

## Confidence
- **High confidence**: The general methodology of combining QSMOTE oversampling with quantum-inspired PGM/kPGM classifiers is sound and the reported improvements over Random Forest baseline are plausible
- **Medium confidence**: The specific numerical results are likely reproducible but depend critically on implementation details of the encoding and PCA steps
- **Low confidence**: The relative performance differences between the three QSMOTE variants and between PGM vs kPGM implementations

## Next Checks
1. Verify the density matrix construction for both amplitude and stereographic encodings by implementing test cases with known input vectors and comparing against theoretical expectations
2. Confirm the PCA application point in the pipeline by testing both pre-encoding and post-encoding scenarios on a small dataset subset
3. Validate the numerical stability of σ^(-1/2) computation by testing with regularized pseudoinverses on matrices with near-zero eigenvalues and comparing classification outcomes