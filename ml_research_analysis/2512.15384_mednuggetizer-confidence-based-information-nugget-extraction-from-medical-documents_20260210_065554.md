---
ver: rpa2
title: 'MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical
  Documents'
arxiv_id: '2512.15384'
source_url: https://arxiv.org/abs/2512.15384
tags:
- https
- information
- nuggets
- prostate
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedNuggetizer addresses reproducibility challenges in LLM-based
  evidence synthesis by performing repeated extraction runs and clustering information
  nuggets based on confidence scores. The system extracts query-relevant information
  nuggets from medical documents using LLM-powered extraction, then groups them through
  repeated runs (n=5) with a confidence threshold (conf=0.8), followed by BERTopic
  clustering.
---

# MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents

## Quick Facts
- arXiv ID: 2512.15384
- Source URL: https://arxiv.org/abs/2512.15384
- Reference count: 35
- Primary result: Confidence-based LLM extraction with repeated runs achieves 4.0-4.8/5 expert ratings for medical evidence synthesis

## Executive Summary
MedNuggetizer addresses reproducibility challenges in LLM-based evidence synthesis by performing repeated extraction runs and clustering information nuggets based on confidence scores. The system extracts query-relevant information nuggets from medical documents using LLM-powered extraction, then groups them through repeated runs (n=5) with a confidence threshold (conf=0.8), followed by BERTopic clustering. Domain experts evaluated the system on antibiotic prophylaxis before prostate biopsy using four guidelines and ten recent studies, producing 155 clusters and 406 nuggets. Expert ratings averaged 4.0-4.8 (out of 5) for both cluster coherence and nugget relevance, with urologists noting the tool efficiently retrieves highly relevant information while effectively distinguishing multiple layers of evidence.

## Method Summary
The system extracts query-relevant information nuggets from medical documents using LLM-powered extraction (Gemini 2.5 Flash), then groups them through repeated runs (n=5) with a confidence threshold (conf=0.8), followed by BERTopic clustering. Domain experts evaluated the system on antibiotic prophylaxis before prostate biopsy using four guidelines and ten recent studies, producing 155 clusters and 406 nuggets. Expert ratings averaged 4.0-4.8 (out of 5) for both cluster coherence and nugget relevance, with urologists noting the tool efficiently retrieves highly relevant information while effectively distinguishing multiple layers of evidence.

## Key Results
- Expert ratings averaged 4.0-4.8 (out of 5) for cluster coherence and nugget relevance
- System processed 4 guidelines and 10 recent studies, producing 155 clusters and 406 nuggets
- Urologists reported efficient retrieval of highly relevant information while distinguishing multiple evidence layers
- Confidence-based filtering (conf=0.8, n=5) successfully addressed LLM output non-determinism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated extraction runs with confidence thresholding mitigates LLM output non-determinism for more reproducible evidence extraction.
- Mechanism: The system runs nugget extraction n times per document and requires nuggets to appear in at least conf × n runs to be retained. With n=5 and conf=0.8, a nugget must appear in ≥4 of 5 runs, filtering stochastic outputs while preserving consistently extracted information.
- Core assumption: Information that appears repeatedly across independent LLM runs is more likely to be valid and relevant than single-occurrence extractions.
- Evidence anchors:
  - [abstract] "performs repeated extractions of information nuggets that are then grouped to generate reliable evidence"
  - [section] "variations in outputs can occur due to stochastic sampling, temperature settings, or batch variance... we increase the reliability of the extracted information nuggets by providing two parameters: n... and conf"
  - [corpus] Weak direct evidence; related work (GINGER) uses nugget extraction but does not specifically validate confidence-thresholding as a reproducibility mechanism.
- Break condition: If LLM exhibits systematic bias rather than random variance, repeated runs may amplify errors rather than filter them.

### Mechanism 2
- Claim: Two-stage BERTopic clustering organizes extracted nuggets first within documents (cross-run consolidation), then across documents (evidence aggregation).
- Mechanism: Stage 1 clusters nuggets from repeated runs of the same PDF to create unified nuggets via LLM summarization. Stage 2 clusters unified nuggets from different documents to identify semantically similar evidence, where larger clusters indicate stronger supporting evidence.
- Core assumption: Semantic similarity in embedding space corresponds to evidential relevance and coherence for clinical decision-making.
- Evidence anchors:
  - [section] "clusters identified in this stage form the final output... clusters indicate stronger supporting evidence for the information expressed by these nuggets"
  - [section] Expert ratings: cluster coherence averaged 4.0-4.84 (out of 5); "clusters effectively distinguishing multiple layers of information such as context, current evidence, recommendations"
  - [corpus] GINGER framework validates BERTopic for nugget clustering in RAG contexts, but medical-domain validation remains limited to this single evaluation.
- Break condition: If topics are highly interdependent or nuanced (e.g., subtle contraindications), BERTopic may conflate distinct clinical concepts into single clusters.

### Mechanism 3
- Claim: Query-guided extraction focuses LLM attention on user-specified clinical questions, improving relevance over generic summarization.
- Mechanism: User provides a natural language query (e.g., "antibiotic prophylaxis before prostate biopsy"); the LLM extracts only nuggets relevant to that query from uploaded PDFs, reducing noise from unrelated medical content.
- Core assumption: LLMs can accurately judge query relevance during extraction without explicit relevance training for medical domains.
- Evidence anchors:
  - [section] "The query formulated by the user about the uploaded documents; it guides the information nugget extraction process"
  - [section] Nugget relevance ratings averaged 4.0-4.75 across five queries; urologists noted "highly relevant information nuggets"
  - [corpus] No direct corpus validation for query-guided medical extraction specifically.
- Break condition: Ambiguous or overly broad queries may yield unfocused extractions; overly narrow queries may miss contextual evidence.

## Foundational Learning

- Concept: **Information Nuggets** (atomic, query-relevant information units)
  - Why needed here: The entire system is built around extracting, clustering, and consolidating these minimal information units rather than full sentences or paragraphs.
  - Quick check question: Can you explain why nuggets are preferred over full-text extraction for evidence synthesis?

- Concept: **LLM Non-Determinism** (stochastic output variation across identical prompts)
  - Why needed here: Understanding why repeated runs are necessary requires grasping that LLMs do not produce identical outputs even with temperature=0 in some implementations.
  - Quick check question: What factors besides temperature can cause output variation in LLM inference?

- Concept: **Topic Modeling / BERTopic** (clustering documents via embedding similarity and class-based TF-IDF)
  - Why needed here: The system relies on BERTopic for both within-document and cross-document clustering stages.
  - Quick check question: How does BERTopic differ from traditional LDA topic modeling in its use of embeddings?

## Architecture Onboarding

- Component map:
  1. **Frontend**: Flask web app → PDF upload, query input, parameter controls (n, conf)
  2. **Extraction Layer**: Gemini 2.5 Flash → query-guided nugget extraction (n runs per PDF)
  3. **Intra-Document Clustering**: BERTopic → groups nuggets across runs; LLM generates unified nuggets per cluster
  4. **Inter-Document Clustering**: BERTopic → groups unified nuggets across files; generates cluster headings
  5. **Output**: Clustered evidence with coherence scores and source traceability

- Critical path: PDF ingestion → n× extraction runs → confidence filtering (≥conf threshold) → intra-document BERTopic clustering → LLM unification → inter-document BERTopic clustering → display

- Design tradeoffs:
  - Higher n increases reproducibility but also cost and latency (paper uses n=5 as default)
  - Higher conf threshold improves precision but may miss valid evidence (paper uses conf=0.8)
  - Gemini 2.5 Flash chosen for cost efficiency; accuracy tradeoffs vs. larger models not evaluated
  - BERTopic minimum cluster size = n × conf (4 for default settings)

- Failure signatures:
  - Empty clusters: conf threshold too high for extraction consistency
  - Overlapping clusters: semantic ambiguity in query or documents
  - Undefined abbreviations/missing context: extraction does not preserve document-level context (noted by expert evaluators)
  - Method-focused noise: nuggets include study methodology rather than findings

- First 3 experiments:
  1. **Parameter sweep**: Run same documents with n ∈ {3, 5, 7} and conf ∈ {0.6, 0.8, 1.0} to measure stability of cluster counts and expert ratings.
  2. **Ablation on clustering**: Skip intra-document clustering and feed all n× nuggets directly to inter-document clustering; compare coherence scores.
  3. **Query sensitivity test**: Run identical documents with progressively narrower queries to identify extraction scope boundaries and relevance degradation points.

## Open Questions the Paper Calls Out
- How does the selection of different backend LLMs and clustering algorithms affect the extraction consistency and cluster coherence compared to the current implementation?
- What is the optimal trade-off between the number of extraction runs (n) and the confidence threshold (conf) to maximize recall while minimizing computational cost?
- Can post-processing techniques or refined prompting strategies mitigate the reported issues of undefined abbreviations and partial overlap between clusters?

## Limitations
- Single-domain validation on antibiotic prophylaxis for prostate biopsy with only 14 documents and 2 urologists limits generalizability
- BERTopic clustering performance for medical evidence aggregation lacks comparative validation against alternative approaches
- System does not preserve document-level context, leading to missing context and undefined abbreviations

## Confidence
- **High Confidence**: The core mechanism of repeated LLM runs with confidence thresholding to address non-determinism is technically sound and directly addresses a documented problem in LLM-based extraction.
- **Medium Confidence**: Expert ratings (4.0-4.8/5) suggest positive reception, but the small sample size and single-domain focus limit generalizability claims.
- **Low Confidence**: The assumption that BERTopic clustering reliably distinguishes multiple layers of evidence (context, recommendations, current evidence) is supported by qualitative feedback but lacks rigorous quantitative validation.

## Next Checks
1. **Cross-Domain Reproducibility**: Apply the system to a different medical evidence synthesis task (e.g., diabetes management guidelines) with at least 20 documents and 3 domain experts to assess generalization.
2. **Ablation of Confidence Thresholding**: Run extraction with conf=0 and conf=1.0 alongside the standard conf=0.8 to quantify the impact on cluster coherence, nugget relevance, and output variability.
3. **Alternative Clustering Comparison**: Replace BERTopic with a supervised evidence classification model trained on labeled medical evidence types to evaluate whether semantic clustering improves precision in distinguishing evidence layers.