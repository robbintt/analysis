---
ver: rpa2
title: 'VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset
  for Legal Question Answering'
arxiv_id: '2507.19995'
source_url: https://arxiv.org/abs/2507.19995
tags:
- legal
- question
- articles
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLQA is the first comprehensive, large-scale, and expert-annotated
  Vietnamese dataset for legal question answering, addressing the scarcity of legal
  NLP resources in low-resource languages. The dataset comprises 3,129 real-world
  legal questions with detailed answers and citations to 59,636 statutory articles,
  verified by legal experts.
---

# VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering

## Quick Facts
- **arXiv ID:** 2507.19995
- **Source URL:** https://arxiv.org/abs/2507.19995
- **Reference count:** 40
- **Primary result:** VLQA is the first comprehensive, large-scale, and expert-annotated Vietnamese dataset for legal question answering, addressing the scarcity of legal NLP resources in low-resource languages.

## Executive Summary
VLQA introduces the first comprehensive, large-scale, and expert-annotated Vietnamese dataset for legal question answering. The dataset comprises 3,129 real-world legal questions with detailed answers and citations to 59,636 statutory articles, verified by legal experts. Baseline models for legal article retrieval and question answering were evaluated, including both traditional sparse models (e.g., BM25) and modern dense retrieval methods (e.g., BGE-m3, fine-tuned mBERT). Retrieval experiments revealed that dense models significantly outperform sparse approaches, with BGE-m3 achieving the highest recall. For question answering, generative models (e.g., BARTpho, ViT5) demonstrated superior contextual understanding, while open-weight and commercial LLMs (e.g., GPT-4o-mini, Qwen2.5-14B) achieved strong performance but occasionally produced incomplete or hallucinated responses. Human evaluation highlighted the need for further improvements in factual accuracy and reasoning capabilities. VLQA is publicly released to advance research in Vietnamese legal NLP and support the development of practical, trustworthy legal assistance tools.

## Method Summary
The VLQA dataset was constructed through automated extraction of legal questions and references from online forums, followed by expert validation. For legal article retrieval, a two-stage approach was employed: BM25 for candidate generation followed by fine-tuned mBERT for reranking. Question answering experiments included both extractive approaches (PhoBERT, mBERT) and generative approaches (BARTpho, ViT5, GPT-4o-mini, Qwen2.5-14B). The retrieval models were evaluated using Recall@k, Precision@2, F2@2, MAP@2, and MRR@2 metrics, while QA models were assessed using ROUGE-1/2/L and BERTScore, supplemented by manual expert evaluation for factual accuracy.

## Key Results
- Fine-tuned mBERT achieved the best retrieval performance with Recall@2 of 0.62, significantly outperforming zero-shot BGE-m3 (0.54) and BM25 (0.46)
- Generative models like GPT-4o-mini and BARTpho demonstrated superior contextual understanding compared to extractive approaches
- Manual evaluation revealed that high automatic evaluation scores (ROUGE, BERTScore) often masked factual errors and hallucinations in LLM outputs
- All retrieval models showed precision scores below 0.40, indicating substantial room for improving accuracy in the large 59,636-article corpus

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised fine-tuning on domain-specific data significantly improves legal article retrieval over zero-shot lexical or dense baselines.
- **Mechanism:** Generic pre-trained embeddings fail to capture the precise semantic relationships in Vietnamese statutory language. Fine-tuning a cross-encoder (mBERT) on in-domain (question, article) pairs aligns the model's attention mechanisms with legal terminology.
- **Core assumption:** The training split of VLQA is sufficiently representative of the test distribution to create a robust ranker.
- **Evidence anchors:** "The fine-tuned mBERT retriever achieved the best performance among retrieval models" and "the fine-tuned mBERT dense retriever... substantially outperforms strong zero-shot baselines."

### Mechanism 2
- **Claim:** High automatic evaluation scores (ROUGE, BERTScore) in legal QA mask factual errors and hallucinations that are only detectable via expert review.
- **Mechanism:** LLMs prioritize the generation of fluent, contextually probable natural language. In legal tasks, this leads to "plausible" answers that are semantically similar to references (high BERTScore) but contain subtle statutory misinterpretations or fabricated citations (hallucinations).
- **Core assumption:** The 100 randomly selected samples for manual evaluation are representative of the model's general failure modes.
- **Evidence anchors:** "Manual evaluation revealed limitations in factual accuracy" and "GPT-4o-mini demonstrates notable strengths in producing coherent... answers. However, most of the generated outputs are incomplete or logically incorrect..."

### Mechanism 3
- **Claim:** Automated regex-based extraction of legal references from forum data creates noisy "silver labels" that require expert verification to be viable for training.
- **Mechanism:** Real-world legal questions often cite repealed laws or irrelevant articles. A two-stage process—initial automated collection followed by a clearness/validity/fluency check by legal experts—filters out the noise caused by non-expert forum posts and changing legislation.
- **Core assumption:** Senior law students under supervision provide a sufficiently accurate proxy for ground truth legal logic.
- **Evidence anchors:** "Having legal experts validate the answers and references" and "7.19% of samples required changes to both the answer and supporting articles..."

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The paper explicitly evaluates LLMs using provided context (top-k articles). Understanding the distinction between parametric knowledge (what the model memorized) and non-parametric knowledge (the retrieved articles) is critical to diagnosing the hallucination issues reported.
  - **Quick check question:** Can you explain why a model might score high on BERTScore but fail a manual fact check in a RAG setup? (Hint: Fluency vs. Faithfulness).

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** The paper benchmarks BM25 (sparse, keyword-based) against Bi-Encoders and Cross-Encoders (dense, semantic). You must understand why BM25 remains a strong baseline for legal term matching while dense models handle semantic similarity.
  - **Quick check question:** Why might a lexical model like BM25 perform well on "Lexical Matching" questions but struggle with "Semantic Interpretation" questions?

- **Concept: Cross-Encoder vs. Bi-Encoder Architecture**
  - **Why needed here:** The paper uses Bi-Encoders (SBERT) for zero-shot retrieval and Cross-Encoders (mBERT-ft) for supervised re-ranking/finetuning.
  - **Quick check question:** Why is a Cross-Encoder generally more accurate but slower than a Bi-Encoder for re-ranking a set of legal documents?

## Architecture Onboarding

- **Component map:** Corpus Builder -> BM25 Candidate Generator -> Fine-tuned mBERT Reranker -> PhoBERT/mBERT/BARTpho/ViT5/GPT-4o-mini Generator -> ROUGE/BERTScore/Human Evaluator
- **Critical path:** Retrieval Accuracy. If the Retriever fails to surface the correct statutory article (Recall@k is low), the Reader/Generator cannot produce a correct answer, regardless of its reasoning capability.
- **Design tradeoffs:**
  - **Extractive vs. Generative:** Extractive models (PhoBERT) are safer (grounded in text) but less fluent. Generative models (GPT-4o-mini) are fluent but prone to hallucination.
  - **Zero-shot vs. Fine-tuning:** Zero-shot (BGE-m3) is cheaper to deploy but lower performance (R@2 ~0.54) vs. Fine-tuned (mBERT) which requires labeled data but performs better (R@2 ~0.62).
- **Failure signatures:**
  - **The "Fluent Lie":** High BERTScore (>0.80) combined with Logical Incorrectness in manual review. Look for models that "invent" law names or merge article clauses incorrectly.
  - **Retrieval Collapse:** The retriever returns articles with shared keywords but opposite legal implications (e.g., old vs. new Decrees).
- **First 3 experiments:**
  1. **Retrieval Baseline:** Implement BM25 and measure Recall@2 on the test set to establish a floor. Compare against a zero-shot BGE-m3 run.
  2. **Hallucination Stress Test:** Run GPT-4o-mini on the test set with "distractor" articles (irrelevant laws) included in the context to see if it still generates factually correct answers.
  3. **Silver vs. Gold Comparison:** Train a simple classifier on the "Silver" (regex-extracted) data vs. the "Gold" (expert-verified) data to quantify the impact of the expert annotation noise reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an end-to-end framework be developed to jointly optimize retrieval and answer generation for long-form legal QA?
- **Basis in paper:** The conclusion states the aim "to develop a robust end-to-end framework capable of handling lengthy statutory texts, complex legal queries, and real-world application needs."
- **Why unresolved:** Current baselines use separate, pipelined components (retrieval then QA), which may propagate errors and fail to leverage global context for complex multi-article reasoning.
- **What evidence would resolve it:** A unified model architecture that jointly retrieves and generates answers, evaluated on VLQA with improvements in recall, answer correctness, and factual grounding over pipelined baselines.

### Open Question 2
- **Question:** What techniques can significantly improve retrieval precision for Vietnamese statutory law without sacrificing recall?
- **Basis in paper:** Results show "all models show precision scores smaller than 0.40" and conclude there is "substantial room for enhancing the performance of retrievers in terms of accuracy."
- **Why unresolved:** The large corpus (59,636 articles) and domain-specific language create a challenging retrieval scenario; current neural and lexical models return many irrelevant articles.
- **What evidence would resolve it:** A retriever achieving >0.50 Precision@2 on the VLQA test set while maintaining or improving Recall@2 compared to the fine-tuned mBERT baseline.

### Open Question 3
- **Question:** How can automatic evaluation metrics be aligned with expert judgments of factual accuracy in legal QA?
- **Basis in paper:** The paper notes a "significant disparity between superficial language generation and robust legal reasoning" as LLMs achieve high ROUGE/BERTScore but contain "factual inaccuracies or hallucinated elements" in human evaluation.
- **Why unresolved:** Current metrics measure lexical or semantic overlap but fail to penalize plausible-sounding but legally incorrect information.
- **What evidence would resolve it:** A new metric or evaluation protocol that correlates strongly (e.g., Pearson r > 0.7) with human expert scores for factual accuracy and completeness on a held-out set of LLM outputs.

## Limitations
- Dataset access remains a primary limitation as VLQA is stated to be released "soon" without a specific URL, making full reproducibility dependent on external availability.
- The high manual annotation cost (7.19% of samples required changes) highlights scalability concerns for expanding the dataset.
- Current evaluation metrics (ROUGE, BERTScore) are insufficient for measuring true legal reasoning capability, as evidenced by the gap between high automatic scores and factual errors in manual evaluation.

## Confidence
- **High confidence:** The superiority of fine-tuned dense retrievers over zero-shot baselines for legal article retrieval is well-supported by experimental results (R@2: 0.62 vs. 0.54).
- **Medium confidence:** The claim that automated regex extraction creates noisy "silver labels" requiring expert verification is supported by the annotation process description but relies on the representativeness of the sampled corrections.
- **Medium confidence:** The observation that high automatic evaluation scores mask factual errors is based on manual inspection of 100 samples, which provides suggestive but not exhaustive evidence of model limitations.

## Next Checks
1. **Dataset accessibility verification:** Confirm VLQA dataset availability and complete data preprocessing pipeline using provided specifications.
2. **Hallucination stress test replication:** Run GPT-4o-mini with distractor articles to quantify hallucination frequency and compare against reported 100-sample manual evaluation results.
3. **Negative sampling impact analysis:** Experiment with different negative sampling strategies (random vs. hardness-based) from BM25 top-200 to measure impact on fine-tuned mBERT retrieval performance.