---
ver: rpa2
title: Mechanism of Task-oriented Information Removal in In-context Learning
arxiv_id: '2509.21012'
source_url: https://arxiv.org/abs/2509.21012
tags:
- attention
- index
- head
- layer
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel perspective on In-context Learning
  (ICL) by interpreting it as a task-oriented information removal process, where demonstrations
  help Language Models (LMs) filter out irrelevant information from queries. The authors
  propose a systematic evaluation framework that traces this information removal using
  geometric metrics like eccentricity and covariance flux on hidden states.
---

# Mechanism of Task-oriented Information Removal in In-context Learning

## Quick Facts
- **arXiv ID:** 2509.21012
- **Source URL:** https://arxiv.org/abs/2509.21012
- **Reference count:** 40
- **Primary result:** Introduces task-oriented information removal perspective on ICL, identifying key attention heads responsible for filtering irrelevant information from queries

## Executive Summary
This paper presents a novel geometric perspective on In-context Learning (ICL) by interpreting it as a task-oriented information removal process rather than information addition. The authors propose a systematic evaluation framework that traces information removal using geometric metrics like eccentricity and covariance flux on hidden states. Through experiments on LLaMA-7B models, they demonstrate that ICL implicitly performs information removal, and that injecting low-rank filters into zero-shot hidden states can steer outputs toward intended tasks. The study identifies specific "Denoising Heads" in attention layers that are responsible for this removal process, with ablation experiments showing significant accuracy degradation when these heads are removed, particularly in unseen label scenarios.

## Method Summary
The authors develop a geometric framework to quantify information removal in ICL by analyzing hidden state representations. They use eccentricity (normalized principal eigenvalue) and covariance flux to measure information removal patterns, then validate their approach through synthetic filter injection experiments where low-rank filters are added to zero-shot hidden states to steer outputs. The framework traces how demonstrations help language models filter out irrelevant information from queries. To identify critical components, they conduct head ablation experiments targeting specific attention heads, revealing that certain "Denoising Heads" are particularly important for task-oriented information removal. The evaluation spans multiple tasks including sentiment analysis, NLI, and relation extraction.

## Key Results
- Geometric metrics (eccentricity and covariance flux) effectively quantify task-oriented information removal in ICL
- Synthetic filter injection experiments show that steering zero-shot hidden states toward demonstration-like patterns improves task performance
- Denoising Heads, identified through ablation studies, are critical for ICL accuracy, especially in unseen label scenarios
- Information removal patterns correlate strongly with task success across multiple evaluation tasks

## Why This Works (Mechanism)
The mechanism works because ICL fundamentally involves filtering out irrelevant information from input queries to focus on task-relevant features. Demonstrations provide reference patterns that help the model identify which information to preserve and which to remove. The geometric approach captures this filtering process by measuring changes in the distribution of hidden states - when information is removed, the covariance structure becomes more concentrated (lower eccentricity) and flux patterns emerge that indicate directional filtering. The identified Denoising Heads implement this filtering at the attention level, selectively attending to task-relevant information while suppressing noise. This process allows models to adapt to new tasks without parameter updates by leveraging their pretraining to recognize and remove irrelevant information based on demonstration context.

## Foundational Learning

**Geometric representation of hidden states** - Understanding how to represent and analyze high-dimensional hidden states using eigenvalues and covariance matrices. *Why needed:* The geometric metrics form the core analytical framework for quantifying information removal. *Quick check:* Verify that eccentricity decreases when irrelevant information is filtered out by examining synthetic examples.

**Attention mechanism dynamics** - How attention heads process and filter information through weighted combinations of key, query, and value vectors. *Why needed:* Essential for understanding how Denoising Heads implement information removal. *Quick check:* Confirm that attention weights become more selective after demonstrations are provided.

**Covariance analysis in representation spaces** - Methods for analyzing the spread and correlation structure of high-dimensional representations. *Why needed:* Covariance flux provides a quantitative measure of directional information removal. *Quick check:* Ensure that covariance matrices become more structured (lower rank) when task-relevant information is isolated.

## Architecture Onboarding

**Component map:** Input tokens -> Embedding layer -> Transformer blocks (with Multi-Head Attention) -> Denoising Heads (critical subset) -> Output layer -> Task prediction

**Critical path:** Demonstrations are processed through the same architecture as the query, creating reference patterns. The query then passes through Denoising Heads which perform task-oriented information removal by comparing against demonstration patterns, ultimately producing task-adapted outputs.

**Design tradeoffs:** The architecture leverages pretraining to enable zero-shot adaptation through information removal rather than addition, trading off explicit task-specific parameters for implicit filtering capabilities. This creates efficiency but may limit the depth of task-specific feature extraction compared to fine-tuning approaches.

**Failure signatures:** ICL performance degrades significantly when Denoising Heads are ablated, particularly for unseen labels. Models may retain too much irrelevant information from queries or fail to extract task-relevant patterns from demonstrations. Geometric metrics show reduced information removal (lower covariance flux, higher eccentricity) when these heads are removed.

**First experiments:**
1. Compare eccentricity and covariance flux patterns between successful and failed ICL attempts on the same task
2. Test whether synthetic filter injection can rescue performance in ablated models
3. Examine attention weight distributions in Denoising Heads versus regular attention heads during ICL

## Open Questions the Paper Calls Out

The paper identifies several open questions for future research, including whether the geometric perspective on information removal can be extended to larger models and more complex tasks. The authors question how pretraining objectives influence the emergence of Denoising Heads and whether these mechanisms generalize across different model architectures. They also highlight the need to understand how information removal patterns vary across different types of tasks and whether the framework can be applied to multimodal inputs or non-English languages.

## Limitations

- The study focuses primarily on LLaMA-7B models, limiting generalizability to larger architectures
- Geometric metrics provide intuitive visualization but require empirical validation of their direct relationship to task performance
- The information removal perspective may oversimplify the complete ICL mechanism by not fully accounting for information addition processes
- Experiments are primarily English-centric, with unclear applicability to multilingual or multimodal scenarios

## Confidence

| Claim | Confidence |
|-------|------------|
| Geometric metrics effectively quantify information removal | High |
| Denoising Heads are critical for ICL performance | High |
| Synthetic filter injection can steer outputs toward target tasks | Medium |
| Information removal is the primary ICL mechanism | Medium |
| Framework generalizes across model sizes and tasks | Low |

## Next Checks

1. Replicate the eccentricity and covariance flux analyses across multiple model sizes (1B, 13B, 70B) and architectures to establish generalizability of the geometric metrics

2. Test the synthetic filter injection approach on non-English tasks and multimodal inputs to evaluate cross-domain applicability

3. Conduct controlled experiments comparing information removal patterns in models with varying pretraining objectives (causal vs masked language modeling) to isolate the impact of pretraining on ICL mechanisms