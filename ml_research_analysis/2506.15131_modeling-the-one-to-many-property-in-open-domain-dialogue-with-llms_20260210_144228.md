---
ver: rpa2
title: Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs
arxiv_id: '2506.15131'
source_url: https://arxiv.org/abs/2506.15131
tags:
- response
- responses
- dialogue
- context
- dressing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-stage approach to modeling the one-to-many
  property in open-domain dialogue for large language models. The method decomposes
  dialogue response generation into multi-response generation (MRG) and preference-based
  selection (PS), using a new dataset o2mDial to capture semantically and lexically
  diverse responses.
---

# Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs

## Quick Facts
- arXiv ID: 2506.15131
- Source URL: https://arxiv.org/abs/2506.15131
- Authors: Jing Yang Lee; Kong-Aik Lee; Woon-Seng Gan
- Reference count: 40
- Key outcome: Two-stage framework (MRG + PS) improves response diversity and coherence by up to 90% for smaller LLMs

## Executive Summary
This work introduces a two-stage approach to modeling the one-to-many property in open-domain dialogue for large language models. The method decomposes dialogue response generation into multi-response generation (MRG) and preference-based selection (PS), using a new dataset o2mDial to capture semantically and lexically diverse responses. MRG is implemented through in-context learning and instruction-tuning, while PS leverages a fine-tuned human preference model. Empirical results show that this framework improves response diversity and contextual coherence by up to 90%, elevating smaller LLMs to match the quality of larger models while maintaining lower computational demands.

## Method Summary
The paper proposes a two-stage framework for open-domain dialogue generation. First, Multi-Response Generation (MRG) produces n=5 diverse responses using either in-context learning (few-shot, chain-of-thought, or prompt chaining) or instruction-tuning. Second, Preference-based Selection (PS) uses a fine-tuned human preference model to choose the best response. The approach is trained and evaluated on a new dataset, o2mDial, containing 500 training and 100 test samples from DailyDialog, with 5 responses per context generated by different LLMs and human preference annotations for all response pairs.

## Key Results
- The two-stage framework achieves up to 90% improvement in response diversity and contextual coherence compared to baseline methods
- Prompt Chaining in MRG produces more semantically diverse responses than Multiple Inference, while maintaining lexical diversity
- Fine-tuning the preference model with hard negative samples improves selection quality across all LLM sizes tested
- Smaller LLMs (7B-13B) using this framework match the response quality of larger models (70B) in open-domain dialogue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing response generation into sequential subtasks improves both semantic and lexical diversity in generated response sets.
- Mechanism: Prompt Chaining (PC) generates responses one-by-one, where each subsequent prompt explicitly conditions on previously generated responses and instructs the LLM to produce semantically and lexically distinct outputs. This reduces the tendency toward semantic convergence seen in standard sampling approaches.
- Core assumption: Smaller LLMs struggle with generating diverse responses in a single inference pass due to deterministic logits and limited capacity, but can follow explicit differentiation instructions when given prior context.
- Evidence anchors:
  - [abstract] "decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS)"
  - [section 3.1] "we use PC to guide the LLM in generating a set of n unique responses one by one... subsequent prompts instructs the LLM to generate contextually coherent responses that differ semantically and lexically from every response generated by the previous prompts"
  - [corpus] Related work on multi-dimensional prompt chaining (arxiv 2601.01037) supports prompt decomposition for dialogue quality, though focused on different dimensions.

### Mechanism 2
- Claim: Hard negative sampling during preference model fine-tuning improves selection quality by forcing the model to discriminate between similar-quality responses.
- Mechanism: The ODRP model is fine-tuned on 50% of samples where the base preference model failed (assigned similar scores to both responses or ranked the rejected response higher). This contrastive learning signal sharpens the model's ability to distinguish nuanced quality differences.
- Core assumption: The base preference model captures general preferences but lacks domain-specific discrimination for open-domain dialogue nuances.
- Evidence anchors:
  - [abstract] "PS leverages a fine-tuned human preference model"
  - [section 4] "we introduce a variant of the ODRP model finetuned on a subset of the corpus selected via hard negative sampling... we apply the base preference model to the dataset and deliberately extracted samples on which the base model performed the worst"
  - [section 6.1] "fine-tuning the ODRP model with hard negative samples leads to a noticeable improvement in the diversity and coherence of the selected responses across all LLMs"
  - [corpus] No direct corpus evidence on hard negative sampling for dialogue preference; related work focuses on persona consistency rather than preference discrimination.

### Mechanism 3
- Claim: Separating semantic and lexical diversity evaluation enables targeted optimization of response set quality.
- Mechanism: The paper introduces two distinct metrics: d_sem uses pairwise BERTScore for semantic similarity, while d_lex uses pairwise Jaccard similarity for lexical overlap. Lower scores indicate greater diversity. This separation reveals that Multiple Inference produces lexically diverse but semantically similar responses.
- Core assumption: Semantic and lexical diversity are orthogonal dimensions that should be optimized independently; traditional metrics like Distinct-n conflate these.
- Evidence anchors:
  - [section 3.2] "we define two separate metrics each accounting for either inter-response semantic or lexical diversity respectively"
  - [section 6.2] "Response sets generated via MI tend to be semantically similar despite relatively high lexical diversity, as shown by low inter-response semantic scores and comparably higher lexical diversity scores"
  - [corpus] Related work on dialogue evaluation metrics exists but doesn't explicitly separate semantic/lexical diversity at the response-set level.

## Foundational Learning

- Concept: **One-to-Many (o2m) Property in Dialogue**
  - Why needed here: The entire framework is built on the premise that multiple valid responses exist for any dialogue context, and explicitly modeling this improves diversity without sacrificing coherence.
  - Quick check question: Given "How are you?", can you articulate why "I'm doing well, thanks!" and "Pretty tired, honestly" are both valid but why "The capital of France is Paris" is not, despite being coherent text?

- Concept: **Contrastive Learning / Pairwise Preference Optimization**
  - Why needed here: The ODRP model is trained using pairwise comparisons with a contrastive loss, requiring understanding of how preference signals differ from absolute quality scores.
  - Quick check question: If response A is preferred over B, and B is preferred over C, does this guarantee A is preferred over C? What does this imply for training data construction?

- Concept: **In-Context Learning Variants (Few-Shot, CoT, Prompt Chaining)**
  - Why needed here: MRG is implemented through multiple ICL strategies, each with different computational costs and quality trade-offs.
  - Quick check question: For generating 5 diverse responses, why might Prompt Chaining produce better results than Few-Shot prompting, even though it requires 5× more inference calls?

## Architecture Onboarding

- Component map:
  Dialogue Context → MRG Module (ICL or IT) → Response Set R_n (n=5) → ODRP_HN Model → Preference Scores → Final Response r_f

- Critical path:
  1. o2mDial construction requires 5 distinct LLMs (paper uses gpt-3.5-turbo, llama2-70b, mixtral-8x22b, Stable-Vicuna13b, Flan-T5-xxl)
  2. MRG implementation: PC or IT performs best; PC requires n inference calls, IT requires QLoRA fine-tuning (rank=16, alpha=32, lr=2e-4, 4 epochs)
  3. PS requires extending o2mDial with human preference labels (n choose 2 pairs per sample), then fine-tuning deberta-v3-large via QLoRA

- Design tradeoffs:
  - PC vs. IT: PC has higher inference cost (n calls) but no training; IT has single-pass inference but requires fine-tuning infrastructure
  - n=5 responses: Paper fixes this; increasing n could improve diversity but increases MRG cost and annotation burden for PS
  - Temperature=0.7: Fixed across experiments; higher values degrade coherence, lower values reduce diversity

- Failure signatures:
  - Insufficient responses: LLM generates <n responses (notably TinyLlama)
  - Redundancy: Responses are semantically or lexically similar despite instructions
  - Over-extension: LLM continues dialogue beyond the requested response
  - Semantic convergence in MI: Multiple Inference produces responses with high lexical diversity but low semantic diversity due to deterministic logits

- First 3 experiments:
  1. **Baseline reproduction**: Implement Few-Shot MRG on TinyLlama with the provided prompt template (Figure 3), compute d_sem and d_lex on 20 dialogue contexts to establish baseline metrics.
  2. **PC vs. MI comparison**: For the same contexts, implement Prompt Chaining and Multiple Inference (n=5, temp=0.7), compare semantic diversity scores to confirm PC produces lower d_sem (higher semantic diversity).
  3. **Preference model sanity check**: Fine-tune the OpenAssistant preference model on a small subset (50 samples) of pairwise preferences from o2mDial, verify it assigns higher scores to human-preferred responses on a held-out set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of candidate responses ($n$) in the Multi-Response Generation (MRG) stage that maximizes diversity before hitting a point of diminishing returns?
- Basis in paper: [explicit] The Conclusion states that future work involves systematically increasing $n$ (beyond the fixed value of 5 used in the study) to "identify the optimal point of diminishing returns."
- Why unresolved: The authors fixed $n=5$ for all experiments based on resource constraints and did not perform an ablation study to see if generating 10 or 20 responses would significantly improve the final selection quality or simply add noise.
- What evidence would resolve it: An ablation study measuring the proposed diversity metrics ($d_{sem}$, $d_{lex}$) and final selection quality across varying values of $n$ (e.g., 5, 10, 15, 20) to identify the saturation point.

### Open Question 2
- Question: Does integrating the dialogue context directly into the Preference-based Selection (PS) model improve the system's ability to reject contextually incoherent responses?
- Basis in paper: [explicit] The Conclusion suggests that a potential avenue for additional research is "integrating dialogue context into the evaluation process to act as a safeguard against contextually incoherent responses."
- Why unresolved: The current Preference-based Selection (PS) model (ODRP) is trained to prefer one response over another, but it may not explicitly verify consistency with the dialogue history, potentially allowing a "preferred" but irrelevant response to be selected.
- What evidence would resolve it: Developing a context-aware ODRP variant and measuring its false positive rate on incoherent responses compared to the context-agnostic baseline proposed in the paper.

### Open Question 3
- Question: Can the proposed two-stage framework yield performance improvements for state-of-the-art LLMs (larger than 70B parameters), or is the benefit limited to smaller models ($\le$ 13B)?
- Basis in paper: [inferred] While the paper focuses on "elevating smaller LLMs," the Limitations section notes the use of smaller models due to resource constraints. It is unstated if applying this computationally heavy framework to already highly capable models (like GPT-4 or Llama-3-70B) provides statistically significant gains over standard inference.
- Why unresolved: The paper demonstrates that smaller models + framework $\approx$ larger model baselines, but it does not verify if larger model + framework $>$ larger model baseline.
- What evidence would resolve it: Applying the MRG and PS framework to a baseline model like Llama-2-70b and measuring if the diversity and coherence scores improve significantly over the baseline's standard zero-shot generation.

## Limitations

- Dataset Generalization: The o2mDial dataset is built on DailyDialog, a relatively formal corpus. Results may not transfer to more casual domains like Reddit or Twitter, where response patterns differ significantly.
- Computational Trade-offs: While the framework claims lower computational demands than larger models, Prompt Chaining requires n inference calls (n=5) compared to single-pass generation.
- Evaluation Scope: Diversity metrics (d_sem, d_lex) are response-set level, but the paper doesn't analyze diversity at the utterance level within individual responses.

## Confidence

- **High Confidence**: The core decomposition approach (MRG + PS) and its implementation via Prompt Chaining. The empirical results showing improved diversity metrics are robust within the DailyDialog domain.
- **Medium Confidence**: Cross-domain generalization claims and computational efficiency comparisons. These rely on extrapolation from the DailyDialog experiments.
- **Low Confidence**: Claims about handling "any LLM size" - the TinyLlama results show clear limitations that aren't adequately addressed.

## Next Checks

1. **Domain Transfer Test**: Implement the same framework on a different open-domain corpus (e.g., OpenSubtitles or Reddit conversations) and compare diversity/coherence metrics to the DailyDialog results. This will validate the domain-agnostic claims.

2. **Computational Cost Analysis**: Measure total inference time and GPU memory usage for PC vs. MI methods across different model sizes (TinyLlama, Llama2-7b, Llama2-13b) on identical hardware. Include both MRG and PS stages in the calculation.

3. **Human Preference Validation**: Conduct human preference studies comparing the final selected responses from the full framework against baseline methods, focusing on both quality and diversity dimensions. Use the same annotation protocol as the o2mDial preference collection to ensure consistency.