---
ver: rpa2
title: Language Models Improve When Pretraining Data Matches Target Tasks
arxiv_id: '2507.12466'
source_url: https://arxiv.org/abs/2507.12466
tags:
- data
- arxiv
- betr
- benchmark
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes benchmark-targeted ranking (BETR), a data selection
  method that explicitly aligns pretraining data with target benchmarks by ranking
  documents based on their similarity to benchmark training examples. BETR embeds
  benchmark examples and pretraining documents in a shared space, scores documents
  by similarity to benchmarks, and trains a classifier to predict these scores for
  the full corpus.
---

# Language Models Improve When Pretraining Data Matches Target Tasks

## Quick Facts
- arXiv ID: 2507.12466
- Source URL: https://arxiv.org/abs/2507.12466
- Reference count: 40
- Key outcome: BETR achieves 1.8-2.8x compute multipliers over state-of-the-art baselines across all scales, improving performance on 9 out of 10 tasks

## Executive Summary
This paper introduces benchmark-targeted ranking (BETR), a data selection method that explicitly aligns pretraining data with target benchmarks by ranking documents based on their similarity to benchmark training examples. BETR achieves 1.8-2.8x compute multipliers over state-of-the-art baselines across all scales, improving performance on 9 out of 10 tasks. The method reveals that optimal filtering strategies vary with model scale: smaller models benefit from aggressive filtering (top 3%) while larger models require less strict filtering (top 30%).

## Method Summary
BETR works by embedding benchmark training examples and pretraining documents in a shared vector space, scoring documents by similarity to benchmarks, and training a classifier to predict these scores for the full corpus. The method samples 10M documents from the pretraining pool, embeds them with Arctic-Embed L 2.0 along with all benchmark examples, computes cosine similarity, and ranks documents by their maximum similarity to any benchmark example. A FastText classifier is trained to distinguish the top 10% from the bottom 90% of scored documents, which is then applied to the full corpus to retain the top 10% of tokens.

## Key Results
- BETR achieves 2.1x compute multiplier over DCLM-Baseline across all model scales
- Targeting diverse "Noncore" benchmarks avoids Goodhart's Law and generalizes better to held-out tasks
- Optimal filtering rate scales with model compute: Fopt = 4 × 10^-5 C^0.25, shifting from 3% for small models to 30% for large models

## Why This Works (Mechanism)

### Mechanism 1
Explicitly aligning pretraining data to target benchmarks via semantic similarity yields higher compute efficiency than implicit or heuristic quality filtering. BETR embeds benchmark training examples and a sample of pretraining documents in a shared vector space, scores documents by similarity to benchmarks, and trains a classifier to predict these scores for the full corpus. The core assumption is that benchmark training examples are valid proxies for target capabilities and semantic similarity correlates with functional relevance.

### Mechanism 2
Optimal data filtering aggressiveness scales inversely with model size/compute; smaller models require narrow, high-quality data while larger models benefit from broader data diversity. Smaller models have limited capacity and easily overfit to noise, requiring aggressive filtering (top 3%). Larger models possess the capacity to extract signal from noisier, more diverse data, making aggressive filtering counterproductive as it limits the data reservoir too severely.

### Mechanism 3
Targeting a diverse set of benchmarks generalizes better to held-out tasks than targeting the specific evaluation set. When targeting specific evaluation benchmarks (Target-Core), the selection over-optimizes for the specific patterns in those tests, degrading performance on unrelated tasks. Targeting a diverse, disjoint set (Target-Noncore) forces the selection of "broadly useful" text, capturing a wider capability manifold without explicitly memorizing evaluation patterns.

## Foundational Learning

### Concept: Scaling Laws & Compute Multipliers
- **Why needed here:** The paper evaluates success via "compute multipliers" (how much compute is saved to achieve the same result). You must understand the power-law relationship between FLOPs, Model Size, and Data Size to interpret the gains.
- **Quick check question:** If Method A gives a 2x compute multiplier over Method B, does it mean Method A is twice as accurate, or that it requires half the compute to match Method B's accuracy?

### Concept: Embedding Space & Similarity Search
- **Why needed here:** BETR relies on embedding documents and benchmarks into the same vector space to calculate relevance. Understanding that "closeness" in this space equates to "semantic similarity" is crucial for the scoring mechanism.
- **Quick check question:** Why might max-aggregation (closest to *any* example) be preferred over mean-aggregation (average distance to *all* examples) when scoring documents for a specific benchmark?

### Concept: Goodhart's Law in ML
- **Why needed here:** The paper explicitly cites this law: "When a measure becomes a target, it ceases to be a good measure." Understanding this explains why optimizing directly for the test set (Target-Core) degrades generalization.
- **Quick check question:** Why does the paper suggest targeting "Noncore" benchmarks to improve performance on "Core" benchmarks, rather than targeting "Core" directly?

## Architecture Onboarding

### Component map:
Target Definition -> Embedding Engine -> Scoring Function -> Filter Model -> Application

### Critical path:
1. Sampling the pretraining pool effectively (must represent the full distribution)
2. Training the FastText classifier. *Note:* The paper finds higher classifier accuracy does *not* always correlate with better downstream performance (FastText beat larger LMs)
3. Determining the filtering threshold (top 3% vs. 30%) based on your target compute scale

### Design tradeoffs:
- **FastText vs. LM Classifier:** FastText is faster and cheaper but less expressive. The paper paradoxically found FastText performed better than 302M LM classifiers despite lower validation accuracy, suggesting "harder" learning tasks (top 10 vs bottom 90) induce better features than "easy" binary tasks (benchmark vs. web)
- **Evaluation-Aware vs. Blind:** Targeting the evaluation set (EA) maximizes the score but risks overfitting/capability loss elsewhere. Evaluation-Blind (EB) trades peak score for robustness

### Failure signatures:
- **Data Starvation:** If you filter too aggressively (e.g., top 1%) for a large compute run, you may run out of unique tokens, forcing repeated epochs which degrades performance
- **Goodharting:** Exceptional performance on targeted benchmarks coupled with surprisingly poor performance on held-out tasks indicates overfitting to the selection proxy

### First 3 experiments:
1. **Sanity Check (7B-1x):** Implement the "Target-Core" pipeline on a small, clean subset of data. Verify that training on the top 10% selected tokens outperforms training on a random 10% shuffle
2. **Ablation on Scoring:** Compare `max(1/rank)` vs. mean-aggregation. Verify that rewarding "near any example" improves performance on specialized knowledge tasks compared to "near all examples"
3. **Generalization Test:** Train two small models: one targeting Core, one targeting Noncore. Evaluate *both* on Core. Check if Noncore-targeting stays within the error margin of Core-targeting while potentially offering broader capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BETR generalize to multilingual and code data, where tokenization and quality notions may differ substantially from English text?
- Basis in paper: [explicit] The authors explicitly state: "We experiment only with English text, excluding multilingual and code data, where BETR may behave differently."
- Why unresolved: FastText's whitespace tokenization is suboptimal for code and morphologically rich languages; "quality" in code requires syntactic/semantic correctness rather than semantic similarity to benchmarks
- What evidence would resolve it: Apply BETR to multilingual corpora using BPE tokenization and evaluate on multilingual benchmarks; test on code corpora targeting coding benchmarks like HumanEval

### Open Question 2
- Question: Do BETR's pretraining improvements persist through post-training stages such as supervised finetuning and reinforcement learning from human feedback?
- Basis in paper: [explicit] The authors state: "We do not verify if pretraining improvements persist after post-training."
- Why unresolved: Post-training can significantly reshape model capabilities; gains from targeted pretraining data might be washed out or amplified by alignment procedures
- What evidence would resolve it: Take models pretrained with BETR vs. baselines, apply identical post-training pipelines, and compare final benchmark performance

### Open Question 3
- Question: Why does a simple FastText classifier with 75.4% accuracy outperform a 302M parameter language model classifier with 82.4% accuracy for document scoring?
- Basis in paper: [explicit] The authors state: "FastText's strong performance despite lower classification accuracy than LM alternatives remains unexplained, indicating gaps in our understanding of data selection."
- Why unresolved: Classification accuracy appears disconnected from downstream model quality, suggesting the scoring task captures features relevant to data selection differently than validation metrics indicate
- What evidence would resolve it: Analyze what features FastText vs. LM classifiers learn; probe whether higher-accuracy classifiers overfit to spurious patterns that don't transfer to pretraining utility

## Limitations
- The method's effectiveness is untested on multilingual and code data where tokenization and quality notions differ substantially
- BETR assumes benchmark training examples are valid proxies for target capabilities, which may not hold for emergent reasoning tasks
- The scaling law for filtering rates (Fopt = 4 × 10^-5 C^0.25) is empirically fitted but lacks theoretical grounding

## Confidence
- **High Confidence:** The empirical demonstration that BETR outperforms random and heuristic baselines (DCLM-Baseline, Max-Avg) across 9/10 tasks, and the observed trend of scale-dependent filtering rates
- **Medium Confidence:** The mechanism that targeting diverse "Noncore" benchmarks avoids Goodhart's Law and generalizes better to held-out tasks, as this is shown for one held-out task (TruthfulQA) and may not generalize across all domains
- **Low Confidence:** The precise functional form of the scaling law for filtering rates and its applicability beyond the tested compute range, due to limited data points and lack of theoretical justification

## Next Checks
1. **Domain Transfer Test:** Apply BETR to a different pretraining corpus (e.g., C4, The Pile) and a distinct set of benchmarks (e.g., BIG-bench, biomedical QA). Measure compute multipliers and generalization to held-out tasks. This tests the robustness of the similarity-based selection mechanism to domain shifts.

2. **Embedding Space Ablation:** Replace `Arctic-Embed L 2.0` with a different encoder (e.g., Sentence-BERT, RoBERTa) and compare downstream performance and classifier accuracy. This isolates the impact of the embedding space on the quality of the similarity proxy and tests the sensitivity to the embedding choice.

3. **Aggregation Function Sweep:** Systematically compare `max(1/rank)`, `mean(1/rank)`, and `sum(1/rank)` across a range of tasks (specialized knowledge vs. general reasoning). This validates the assumption that rewarding "near any example" is superior to "near all examples" and identifies tasks where different aggregations may be optimal.