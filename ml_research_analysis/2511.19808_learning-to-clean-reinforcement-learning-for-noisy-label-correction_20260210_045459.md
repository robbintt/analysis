---
ver: rpa2
title: 'Learning to Clean: Reinforcement Learning for Noisy Label Correction'
arxiv_id: '2511.19808'
source_url: https://arxiv.org/abs/2511.19808
tags:
- label
- learning
- noisy
- noise
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates noisy label correction as a reinforcement
  learning problem and proposes RLNLC, a novel method that uses an actor-critic framework
  with a policy network based on deep feature representations. The approach incorporates
  a reward function that evaluates label corrections using k-nearest neighbor mechanisms
  to ensure label consistency and alignment between noisy and clean labels.
---

# Learning to Clean: Reinforcement Learning for Noisy Label Correction

## Quick Facts
- arXiv ID: 2511.19808
- Source URL: https://arxiv.org/abs/2511.19808
- Reference count: 40
- Primary result: RLNLC outperforms state-of-the-art methods on CIFAR10-IDN, CIFAR100-IDN, Animal-10N, and Food-101N datasets

## Executive Summary
This paper introduces RLNLC, a novel method for noisy label correction that formulates the problem as a reinforcement learning task. The approach uses an actor-critic framework with a policy network based on deep feature representations and k-nearest neighbor attention. A composite reward function encourages both label consistency and alignment between noisy and clean labels. Experimental results demonstrate consistent improvements over existing methods, particularly under high noise conditions, with up to 1.9% accuracy gains over the second-best approach.

## Method Summary
RLNLC models noisy label correction as a Markov Decision Process where the agent iteratively decides whether to correct or retain each instance's label. The method employs two separate feature extractors: one for the policy network (updated during training) and another fixed backbone for reward evaluation. The policy network uses k-NN attention to compute correction probabilities, while the reward function combines global label consistency and noisy-to-clean label alignment metrics. Training proceeds through multiple RL trajectories, with the final policy applied to generate corrected labels for model training.

## Key Results
- RLNLC achieves up to 1.9% test accuracy improvement over second-best methods across various noise rates
- Under 90% symmetric noise on CIFAR100, RLNLC reaches 44.2% accuracy, significantly outperforming DivideMix's 31%
- Ablation studies confirm the importance of both the noisy label alignment reward and label consistency reward components
- The method shows consistent performance improvements across multiple benchmark datasets including CIFAR10-IDN, CIFAR100-IDN, Animal-10N, and Food-101N

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Formulating label correction as sequential decision-making enables adaptive, non-myopic noise handling that outperforms single-step correction methods.
- **Mechanism:** The paper models label cleaning as an MDP where states = current dataset with (potentially corrected) labels, actions = binary decisions per instance (correct or keep), and a composite reward evaluates resulting label quality. An actor-critic policy gradient learns to maximize cumulative reward across correction trajectories.
- **Core assumption:** Noisy label correction benefits from exploring multiple correction paths and learning from long-term consequences rather than making isolated, greedy corrections.
- **Evidence anchors:**
  - [abstract] "conceptualizes noisy label correction as a reinforcement learning (RL) problem... defines a comprehensive state space... action space... reward mechanism"
  - [section 3.2] Full MDP formulation with S, A, P, R, γ tuple; Section 3.2.4 details actor-critic optimization
  - [corpus] Weak direct support; neighboring papers focus on meta-learning (Set a Thief), forward correction (FedEFC), and information bottleneck approaches rather than RL-based correction
- **Break condition:** If label noise is extremely sparse (<5%) or purely class-conditional (not instance-dependent), simpler methods may suffice; RL overhead may not justify marginal gains.

### Mechanism 2
- **Claim:** k-nearest neighbor attention in learned embedding space provides a reliable signal for detecting instance-level label noise.
- **Mechanism:** A pre-trained feature extractor fθ computes embeddings. For each instance xi, k-nearest neighbors N(xi) are identified. Attention-weighted aggregation of neighbor labels produces predicted label ȳi. Disagreement between ȳi and current label drives the stochastic policy: higher disagreement → higher probability of correction action (ai=1).
- **Core assumption:** Instances that are similar in feature space should have similar labels; label inconsistency with local neighborhood structure indicates probable noise.
- **Evidence anchors:**
  - [abstract] "policy network based on deep feature embeddings and k-nearest neighbor attention"
  - [section 3.2.2] Equations 1-3 formalize attention computation and policy probability πθ(st)i = p(aᵢ=1)
  - [corpus] Related work (Is the Information Bottleneck Robust Enough?) discusses representation vulnerability to label noise, indirectly supporting the need for noise-resilient embeddings
- **Break condition:** If embedding space is poorly structured (e.g., insufficient pre-training, extreme class imbalance), k-NN signals become unreliable. High-dimensional, sparse data may also degrade neighbor quality.

### Mechanism 3
- **Claim:** Combining global label consistency with noisy-to-clean alignment in the reward function provides complementary supervision for effective correction.
- **Mechanism:** Two sub-rewards: (1) RLCR measures KL divergence between each label and its k-NN predicted label across the full dataset (global consistency); (2) RNLA measures alignment between corrected "noisy" subset labels and their neighbors in the unchanged "clean" subset. Rewards are combined as R = exp(RLCR + λ·RNLA), bounded to (0,1].
- **Core assumption:** A fixed, pre-trained backbone fω (separate from policy network) provides stable feature representations for reward evaluation; decoupling policy and reward networks prevents feedback instability.
- **Evidence anchors:**
  - [abstract] "rewards designed to encourage label consistency and noisy-label alignment"
  - [section 3.2.3] Equations 5-7 formalize RLCR, RNLA, and composite reward; ablation (Table 5) shows 2-3% drops when removing either component
  - [corpus] Limited corpus support; no directly comparable dual-reward formulations found
- **Break condition:** If clean subset becomes too small (high noise rates) or reward network fω is biased, RNLA signal degrades. Ablation shows performance drops when fω = fθ (coupled networks).

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The entire RLNLC framework rests on casting label correction as sequential decision-making under uncertainty.
  - **Quick check question:** Can you explain why the transition function here is deterministic (given state + action → next state is fixed by label replacement rule)?

- **Concept: Policy Gradient & Actor-Critic Methods**
  - **Why needed here:** RLNLC uses actor-critic (not Q-learning or DQN) because the action space is high-dimensional (N binary decisions per dataset) and stochastic policies enable exploration.
  - **Quick check question:** Why does the paper use SARSA-style TD updates for the critic instead of off-policy Q-learning?

- **Concept: KL Divergence for Distribution Comparison**
  - **Why needed here:** Both reward functions use KL divergence to measure mismatch between current labels and k-NN predicted labels.
  - **Quick check question:** Why is negative KL divergence a sensible reward signal? What happens when labels perfectly match predictions?

## Architecture Onboarding

- **Component map:** fθ (policy feature extractor, updated) -> k-NN attention -> per-instance correction probability -> Bernoulli sampling -> label replacement; fω (fixed reward feature extractor) -> RLCR + RNLA -> composite reward; Qφ (critic) -> state binning -> value estimation

- **Critical path:**
  1. Pre-train fθ and fω on noisy data with cross-entropy (warmup)
  2. For each epoch: randomize initial state → run T=10 RL steps → update policy (θ) via gradient, critic (φ) via TD error
  3. Deploy trained policy for T'=25 steps on training data → extract "cleaned" labels
  4. Fine-tune final prediction model (hψ ∘ fθ) on cleaned labels

- **Design tradeoffs:**
  - Separate fω vs. shared: Ablation shows coupled networks (fω ← fθ) drop ~2% accuracy; decoupling stabilizes reward signals
  - Trajectory length T: T=10 optimal; T=25 during training causes overfitting/label drift
  - Binning granularity Nb: More bins (100-500) improve critic precision but with diminishing returns

- **Failure signatures:**
  - Accuracy plateaus or degrades during policy training → check if fω is frozen, reduce T, verify reward scaling
  - High variance across runs → increase k (neighbor count), check initial state randomization
  - Poor performance on high noise (>50%) → RNLA signal weakens; consider increasing λ or adjusting clean/noisy subset thresholds

- **First 3 experiments:**
  1. **Sanity check:** Reproduce CIFAR10-IDN results at 30% noise with default hyperparameters (k=10, Nb=100, λ=0.5, T=10). Expect ~97% accuracy within 0.5% of reported.
  2. **Ablation priority:** Remove RNLA reward (set λ=0) on CIFAR100-IDN at 40% noise. Expect ~1.5-2% drop vs. full model.
  3. **Hyperparameter sweep:** Vary k ∈ {5, 10, 20} on CIFAR100-IDN at 50% noise. Expect plateau around k=10-20, confirming sensitivity analysis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RLNLC scale to datasets with millions of samples, given the computational overhead of k-nearest neighbor attention mechanisms and iterative RL training?
- Basis in paper: [inferred] Experiments are limited to moderate-sized datasets (CIFAR10/100: 50k, Food-101N: 310k images). The k-NN attention requires computing distances across the dataset, and the policy is trained for 500 epochs with trajectory length T=10, raising scalability concerns.
- Why unresolved: No experiments or discussion address computational complexity or scaling behavior on large-scale datasets like ImageNet-1K or web-scale data.
- What evidence would resolve it: Runtime and memory analysis on datasets exceeding 1M samples, with comparison to baselines' scalability; potential approximations (e.g., approximate nearest neighbors) could be evaluated.

### Open Question 2
- Question: What causes the performance decline when trajectory length T exceeds 10 during label correction deployment, and can this "label drift" be mitigated?
- Basis in paper: [inferred] The sensitivity analysis (Figure 3) shows accuracy peaks at T=10 but declines at T=25, with the authors speculating this may stem from "overfitting to noisy data or excessive refinement causing label drift."
- Why unresolved: The phenomenon is observed but not investigated; the mechanism underlying this degradation remains unclear.
- What evidence would resolve it: Analysis of label correction trajectories over time, examining whether errors accumulate; evaluation of early stopping criteria or regularization to prevent drift.

### Open Question 3
- Question: Can RLNLC be combined with complementary robust learning techniques (e.g., robust loss functions, advanced data augmentation, or semi-supervised frameworks) for further gains?
- Basis in paper: [inferred] The related work describes orthogonal approaches (robust losses, semi-supervised methods, data augmentation), but RLNLC uses only cross-entropy loss after label cleaning. The method is not evaluated in combination with these techniques.
- Why unresolved: No experiments explore hybrid approaches; potential synergies remain untested.
- What evidence would resolve it: Experiments combining RLNLC with methods like DivideMix's MixMatch augmentation or generalized cross-entropy loss, reporting whether performance improves beyond RLNLC alone.

## Limitations

- Computational Overhead: RLNLC requires significantly more training epochs (e.g., 240 vs 120 for DivideMix on CIFAR10-IDN) due to the RL optimization process, which may limit scalability to larger datasets or real-time applications.
- Hyperparameter Sensitivity: The method depends on several hyperparameters (e.g., k, Nb, λ, T) that are tuned per dataset, potentially reducing generalizability without extensive validation.
- Feature Extractor Dependency: Performance relies heavily on the quality of the pre-trained feature extractor; poor embeddings could degrade k-NN-based corrections and reward signals.

## Confidence

- **High Confidence:** The claim that RLNLC outperforms state-of-the-art methods is supported by consistent experimental results across multiple benchmarks (e.g., CIFAR10-IDN, CIFAR100-IDN, Animal-10N, Food-101N) and ablation studies.
- **Medium Confidence:** The effectiveness of the k-NN attention mechanism and dual-reward formulation is well-supported, but limited corpus evidence for similar approaches makes it harder to contextualize within the broader literature.
- **Low Confidence:** The scalability and generalizability of RLNLC to datasets with extreme class imbalance or high-dimensional sparse features remain untested.

## Next Checks

1. **Scalability Test:** Evaluate RLNLC on larger datasets (e.g., ImageNet) to assess computational feasibility and performance degradation.
2. **Noise Rate Robustness:** Test RLNLC on datasets with varying noise rates (<5% and >90%) to confirm its effectiveness across the full noise spectrum.
3. **Feature Extractor Generalization:** Replace the pre-trained backbone with a weaker or domain-specific feature extractor to test the method's robustness to embedding quality.