---
ver: rpa2
title: A Video-grounded Dialogue Dataset and Metric for Event-driven Activities
arxiv_id: '2501.18324'
source_url: https://arxiv.org/abs/2501.18324
tags:
- answer
- does
- dialogue
- reference
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VDAct, a video-grounded dialogue dataset focused
  on event-driven activities, featuring longer and more complex videos requiring advanced
  contextual understanding. Alongside this, VDEval, a new session-based evaluation
  metric, is proposed to better assess dialogue responses by incorporating dialogue
  history and video content summaries.
---

# A Video-grounded Dialogue Dataset and Metric for Event-driven Activities

## Quick Facts
- arXiv ID: 2501.18324
- Source URL: https://arxiv.org/abs/2501.18324
- Reference count: 40
- A new video-grounded dialogue dataset and evaluation metric show strong correlation with human judgment

## Executive Summary
This paper introduces VDAct, a video-grounded dialogue dataset featuring longer, complex event-driven activities from simulated videos. Alongside the dataset, VDEval is proposed as a session-based evaluation metric that leverages dialogue history and video summaries to better assess response quality. Experiments show that state-of-the-art models struggle with VDAct's challenging temporal and explanatory questions, while VDEval demonstrates significantly higher correlation with human assessments compared to existing metrics.

## Method Summary
The method combines simulated videos from VirtualHome with crowdsourced dialogues to create VDAct. Activity programs are converted to Knowledge Graphs, which are then summarized via templates and refined with GPT-4o-mini. Baselines are fine-tuned with LoRA (r=128, α=256) using 8-16 sampled frames per video. VDEval employs GPT-4o-mini to score responses (1-3 scale) using session context (dialogue history + refined video summaries). Evaluation compares Kendall's rank correlation against human judgments.

## Key Results
- VDAct features 1,000 scenarios with 3,000 dialogues covering descriptive, temporal, explanatory, and quantitative question types
- State-of-the-art models show limited performance, especially on temporal and explanatory questions (avg scores ~25%)
- VDEval achieves 0.72 Kendall correlation with human judgments versus 0.38 for turn-based metrics like LA VE

## Why This Works (Mechanism)

### Mechanism 1: Context-Enriched LLM Evaluation
Integrating full dialogue history and video content summaries into the evaluation prompt significantly improves correlation with human judgment compared to turn-based metrics. VDEval uses an LLM to process session-based context, allowing verification of semantic correctness rather than just lexical overlap.

### Mechanism 2: Knowledge Graph Linearization for Grounding
Template-based text summaries derived from Knowledge Graphs effectively ground the evaluation in precise visual facts without requiring the model to process raw video. This provides verifiable "ground truth" narratives focused on specific events and state changes.

### Mechanism 3: Event-Driven Complexity Stress Testing
Sequencing multiple distinct activities in simulation videos exposes temporal reasoning failures in Vision-Language Models. The low performance on temporal and explanatory questions suggests VLMs struggle to maintain state coherence over long sequences.

## Foundational Learning

- **Concept: Video-grounded Dialogue (VGD)**
  - Why needed: Distinct from Video QA, VGD requires maintaining conversation history where subsequent questions depend on previous answers and video context
  - Quick check: Can a model answer "What did he do next?" without re-watching the video or seeing previous Q&A pairs? (No)

- **Concept: Knowledge Graphs (KGs) in Simulation**
  - Why needed: VDAct uses KGs to define "Ground Truth." Understanding how nodes (objects/agents) and edges (actions/states) relate is crucial for interpreting the dataset structure
  - Quick check: In a triplet `(event1, action, grab)`, what represents the relationship? (The predicate 'action')

- **Concept: Correlation Metrics (Kendall's Tau)**
  - Why needed: To validate that VDEval is better than BLEU or METEOR, one must understand how we measure agreement between an algorithm and human annotators
  - Quick check: If a metric rates a bad response highly (low human score, high metric score), is the correlation positive or negative? (It negatively impacts the correlation coefficient)

## Architecture Onboarding

- **Component map:** VirtualHome2KG -> Activity KGs -> Scenario KGs -> Template Summary -> LLM Refiner -> Refined Summary -> VDEval Prompt
- **Critical path:** The prompt construction for the evaluation step is critical. If the "Refined Summary" or "Dialogue History" is missing or incorrectly formatted, the VDEval mechanism fails.
- **Design tradeoffs:** Simulation vs. Real Video (traded visual realism for perfect ground-truth labeling via KGs); Template vs. LLM Summaries (LLM refinement increases cost but improves correlation)
- **Failure signatures:** Temporal Hallucination (VLM answers "cleaning sink" when video shows "drying dishes"); Reference Miss (VDEval gives score 1 to valid answer not in Summary/Reference)
- **First 3 experiments:**
  1. Sanity Check (Metric Correlation): Run VDEval on test set with provided baselines; verify Kendall's Tau ≈ 0.72
  2. Ablation (Context): Run VDEval with only turn-based context; confirm correlation drops to ~0.38
  3. Error Analysis (Question Types): Isolate T-SEQ questions; measure VDEval score to confirm reported performance dip (avg ~1.49)

## Open Questions the Paper Calls Out

- **Integration of KGs:** Future work can explore a model architecture that utilizes KGs for video-grounded dialogue, as current baselines do not explicitly leverage graph structures
- **Simulation-to-Reality Transfer:** The extent to which training on simulation-based datasets transfers to real-world video understanding tasks remains untested
- **Temporal Reasoning Enhancement:** It's unclear whether increasing frame sampling density or implementing specialized temporal modules would improve performance on temporal reasoning questions

## Limitations

- Grounding Quality: VDEval's effectiveness depends on the LLM's ability to accurately summarize video content without hallucinations
- Simulation vs. Reality Gap: The dataset's use of VirtualHome simulations may not fully capture real-world video complexity
- Metric Robustness: While VDEval shows high correlation with human judgment, its reliance on GPT-4o-mini introduces potential bias

## Confidence

- High Confidence: Core contribution of introducing VDAct and VDEval is well-supported by experimental results
- Medium Confidence: VDEval's session-based evaluation approach is supported by correlation metrics but needs real-world validation
- Low Confidence: Long-term impact of simulation-based datasets on real-world video-grounded dialogue systems is uncertain

## Next Checks

1. Test VDEval's performance on diverse real-world videos to assess generalizability and identify failure modes
2. Conduct human study to validate VDEval's scores align with human judgments across different question types
3. Evaluate models trained on VDAct when applied to real-world video-grounded dialogue tasks to assess simulation-to-reality transfer gap