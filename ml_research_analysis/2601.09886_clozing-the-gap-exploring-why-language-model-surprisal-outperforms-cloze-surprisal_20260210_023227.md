---
ver: rpa2
title: 'Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze
  Surprisal'
arxiv_id: '2601.09886'
source_url: https://arxiv.org/abs/2601.09886
tags:
- cloze
- surprisal
- both
- gpt2-h
- probabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares human cloze-based and LM-based predictability
  estimates for modeling reading times, finding that GPT2 surprisal generally outperforms
  cloze surprisal. Three hypotheses for this advantage are tested through targeted
  manipulations of GPT2 probabilities: resolution (sampling to match cloze response
  counts), semantic clustering (reducing fine-grained word distinctions), and frequency
  constraints (limiting probabilities to high-frequency tokens).'
---

# Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal

## Quick Facts
- **arXiv ID**: 2601.09886
- **Source URL**: https://arxiv.org/abs/2601.09886
- **Reference count**: 40
- **Key outcome**: GPT2 surprisal generally outperforms cloze surprisal in predicting reading times; resolution, semantic granularity, and frequency constraints explain LM advantage through targeted probability manipulations.

## Executive Summary
This study investigates why language model (GPT2) surprisal better predicts reading times than traditional cloze-based predictability estimates. The authors test three hypotheses explaining LM superiority: resolution (higher probability granularity), semantic clustering (fine-grained word distinctions), and frequency constraints (accurate low-frequency word assignment). By manipulating GPT2 probabilities to simulate cloze limitations, they demonstrate that all three factors significantly reduce GPT2's predictive power. The findings suggest that cloze studies could benefit from higher response counts and more ecologically valid designs, while also revealing that LM advantages stem from aspects of prediction that differ from human processes.

## Method Summary
The study compares GPT2-small surprisal with cloze surprisal for modeling reading times across four English datasets (BK21 SPR, Provo ET, UCL SPR/ET). Standard cloze smoothing with S=200 and squared transformation is applied. Baseline LME models include word length, position, unigram surprisal (KenLM on OpenWebText), and preceding fixation (ET only). Three targeted manipulations test the hypotheses: H1 samples N words from GPT2 to match cloze response counts; H2 applies k-means clustering (k=80) on token embeddings and sums cluster probabilities; H3 masks low-frequency tokens (<10^4/billion) and renormalizes. Models are evaluated via 10-fold cross-validation with paired permutation tests (Bonferroni-corrected).

## Key Results
- GPT2 surprisal consistently outperforms cloze surprisal in predicting reading times across multiple datasets and measures.
- Resolution manipulation (H1) significantly reduces predictive power, confirming the resolution advantage hypothesis.
- Semantic clustering manipulation (H2) significantly reduces fit, supporting the semantic granularity mechanism.
- Frequency constraint manipulation (H3) also significantly reduces performance, validating the low-frequency word hypothesis.

## Why This Works (Mechanism)

### Mechanism 1: Resolution Advantage
LMs provide higher-resolution probability estimates than cloze studies by computing conditional probabilities over their full vocabulary (~50K tokens) rather than being limited to observed response counts. This allows LMs to assign non-zero probabilities to words that might not appear in cloze responses due to practical data collection limits. The H1 manipulation sampling N tokens from GPT2 to match cloze counts significantly reduced predictive power, confirming this advantage.

### Mechanism 2: Semantic Granularity
LMs make fine-grained distinctions between semantically similar words (e.g., "couch" vs "sofa") based on contextual patterns in training data, while human predictions are influenced by shared semantic features causing cloze responses to cluster around semantic categories. The H2 manipulation using k-means clustering (k=80) on token embeddings and assigning cluster probabilities significantly reduced fit to reading times, validating this mechanism.

### Mechanism 3: Low-Frequency Word Probability Assignment
LMs accurately assign non-zero probabilities to low-frequency words that may be underrepresented in cloze responses due to sampling limitations. The H3 manipulation masking tokens below 10^4/billion frequency and renormalizing significantly reduced predictive performance, confirming that LMs' accurate handling of low-frequency words contributes to their advantage.

## Foundational Learning

**Language Model Surprisal**: The negative log probability of a word given its context, used as a predictor of processing difficulty. *Why needed*: Core metric for comparing LM and cloze predictability estimates. *Quick check*: P(w_t | w_{1..t-1}) = exp(-surprisal).

**Cloze Task**: Participants predict upcoming words in sentences, with response frequencies used to estimate predictability. *Why needed*: Standard psycholinguistic method for measuring word predictability. *Quick check*: P(w) = (C_w + 1)/(Î£C + S) with smoothing.

**Mixed-Effects Models**: Statistical models with both fixed and random effects to account for participant and item variability. *Why needed*: Standard approach for analyzing reading time data with hierarchical structure. *Quick check*: Random intercepts for subjects and items.

**Token Embeddings**: Vector representations of words learned by LMs that capture semantic and syntactic relationships. *Why needed*: Used for clustering semantically similar words in H2 manipulation. *Quick check*: Similar words have high cosine similarity in embedding space.

**Cross-Validation**: Technique for evaluating model performance by partitioning data into training and test folds. *Why needed*: Ensures robust model evaluation across datasets. *Quick check*: 10-fold CV with consistent fold assignment.

## Architecture Onboarding

**Component Map**: GPT2-small -> probability computation -> H1 sampling/H2 clustering/H3 frequency masking -> LME model fitting -> RT prediction

**Critical Path**: Word sequence -> GPT2 probability assignment -> Manipulated probability calculation -> Mixed-effects model with RT data -> Log-likelihood evaluation

**Design Tradeoffs**: Resolution vs. data collection effort (higher N improves cloze but is costly), semantic granularity vs. human prediction processes (fine distinctions may not reflect human expectations), frequency accuracy vs. model complexity (handling low-frequency words improves prediction but requires large training data).

**Failure Signatures**: Performance drops after manipulations indicate which mechanisms contribute to LM advantage; clustering artifacts may arise from suboptimal k parameter; sampling variance in H1 requires multiple runs for stable results.

**First Experiments**: 1) Compare GPT2 surprisal vs. cloze surprisal on a single dataset with standard smoothing; 2) Apply H1 manipulation with varying N values to find threshold where performance matches cloze; 3) Test H2 clustering with different k values to optimize semantic grouping.

## Open Questions the Paper Calls Out

**Open Question 1**: Do human predictions distinguish between semantically related or low-frequency words with the same fine-grained sensitivity as LMs? The study manipulated LM probabilities to simulate human limitations but did not verify if the underlying human cognitive process actually makes these distinctions during real-time comprehension.

**Open Question 2**: Can timed or speeded cloze tasks provide predictability estimates that better capture real-time processing than untimed tasks? The authors suggest timed versions may help control for conscious reflection that decouples cloze responses from rapid prediction mechanisms measured by reading times.

**Open Question 3**: Do the advantages of LM surprisal regarding resolution, semantics, and frequency generalize to non-English languages and diverse model architectures? The analysis was restricted to GPT-2 and English datasets, leaving cross-linguistic and cross-architectural validity untested.

## Limitations

The three mechanisms identified are not proven to be exclusive or exhaustive explanations for LM superiority. The study cannot definitively separate these effects from other potential factors such as LMs' superior handling of syntactic dependencies, world knowledge, or discourse-level information. Additionally, the focus on English datasets limits generalizability to other languages with different morphological complexity or corpus characteristics.

## Confidence

**High confidence**: GPT2 surprisal outperforms cloze surprisal in predicting reading times; resolution advantage confirmed through sampling manipulation; semantic granularity mechanism validated through clustering manipulation.

**Medium confidence**: Frequency constraint mechanism is supported but evidence is less definitive; relative contribution of each mechanism to overall LM advantage remains unclear.

**Low confidence**: Claims about implications for cloze study design improvements and the ecological validity of LMs versus human prediction processes; the study does not directly measure whether LMs are better models of human language processing or simply better predictors of reading behavior.

## Next Checks

1. **Replicate with larger cloze response counts**: Collect cloze data with 200-500 responses per context rather than typical 40-100 to directly test whether resolution differences persist at higher data collection volumes.

2. **Cross-linguistic validation**: Test whether the three mechanisms hold across languages with different typological features (morphologically rich languages, different word order) to assess generalizability.

3. **Direct comparison of semantic similarity judgments**: Collect human similarity ratings for word pairs that GPT2 distinguishes but cloze responses cluster together to empirically validate whether humans actually treat these words as semantically equivalent in prediction contexts.