---
ver: rpa2
title: 'IG-Pruning: Input-Guided Block Pruning for Large Language Models'
arxiv_id: '2511.02213'
source_url: https://arxiv.org/abs/2511.02213
tags:
- pruning
- mask
- sparsity
- language
- ig-pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IG-Pruning, a novel input-aware block-wise
  pruning method that dynamically selects layer masks at inference time. The method
  discovers diverse mask candidates through semantic clustering and L0 optimization,
  then implements efficient dynamic pruning without additional training.
---

# IG-Pruning: Input-Guided Block Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2511.02213
- Source URL: https://arxiv.org/abs/2511.02213
- Reference count: 23
- One-line primary result: Input-aware block-wise pruning method that dynamically selects layer masks at inference time, outperforming static depth pruning across sparsity levels.

## Executive Summary
IG-Pruning introduces a novel input-adaptive depth pruning method for large language models that dynamically selects layer masks at inference time based on input semantic similarity. The method discovers diverse mask candidates through semantic clustering and L0 optimization, then implements efficient dynamic pruning without additional training. By training only mask parameters while keeping model weights frozen, IG-Pruning enables rapid adaptation with minimal computational overhead.

## Method Summary
IG-Pruning employs a two-stage approach: (1) K-means clustering on calibration embeddings to group semantically similar inputs, followed by training N cluster-specific masks via hard concrete distribution with L0 regularization (model weights frozen, only mask parameters trained); (2) At inference, encode input → find nearest cluster via Euclidean distance → apply corresponding binary mask. FFN blocks are fully skipped; Attention blocks skip QK computation but preserve KV cache for autoregressive consistency. The method trains masks in approximately 15 minutes on H800 GPUs.

## Key Results
- Llama-3-8B at 25% sparsity preserves 87.18% of dense model performance, surpassing best baseline by 10.86 percentage points
- Qwen-3-8B at 13.9% sparsity maintains 96.01% of dense model performance compared to 90.37% for best baseline
- Dynamic pruning consistently outperforms state-of-the-art static depth pruning methods across different sparsity levels and model architectures

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Aware Dynamic Routing
Mapping inputs to pre-computed pruning masks based on semantic similarity preserves task-specific capabilities better than a single static mask. An external encoder embeds the input, which is matched to a cluster centroid via Euclidean distance. This cluster ID indexes a specific binary mask that skips selected transformer blocks. The core assumption is that semantic similarity in the embedding space correlates with structural redundancy in the transformer.

### Mechanism 2: Hard Concrete Mask Optimization
Optimizing layer masks via the hard concrete distribution allows global, differentiable learning of discrete skip decisions. Instead of magnitude pruning, the method uses a continuous relaxation to approximate binary masks. The L0 penalty enforces sparsity while allowing the loss function to account for layer coupling. The core assumption is that layers are not independent; the optimal set of layers to skip is a combinatorial problem best solved via gradient descent.

### Mechanism 3: KV-Cache Consistent Attention Skipping
Attention layers can be computationally skipped without breaking autoregressive generation if key/value projections are preserved. When a mask targets an attention block, the scaled dot-product attention is bypassed, but the linear projections are still computed to append to the KV cache. The core assumption is that the dominant compute cost in attention is the softmax interaction, not the maintenance of the cache itself.

## Foundational Learning

- **Concept: Hard Concrete Distribution / L0 Regularization**
  - Why needed: This is the mathematical engine allowing the model to "learn" which binary gates to close. Standard backpropagation cannot traverse discrete step functions; the hard concrete distribution provides a differentiable approximation.
  - Quick check: Can you explain how "stretching" a sigmoid function allows gradients to flow through a gate that effectively outputs 0 or 1 during inference?

- **Concept: Semantic Clustering (K-Means on Embeddings)**
  - Why needed: The routing mechanism relies on partitioning the input space. If you don't understand how K-Means partitions vector space or how sentence embeddings capture intent, you cannot debug why specific inputs are routed to specific masks.
  - Quick check: If two distinct tasks (e.g., math reasoning and creative writing) map to the same cluster centroid, what failure mode would you expect in the pruned model?

- **Concept: Transformer Residual Stream**
  - Why needed: Depth pruning works by skipping layers. You must understand that in a Pre-LN or Post-LN transformer, skipping a layer is equivalent to passing the residual stream or just the input.
  - Quick check: In a residual network, if you skip a transformer block entirely, does the output dimensionality remain valid for the next layer?

## Architecture Onboarding

- **Component map:** Calibration Data -> Sentence Encoder -> K-Means Clustering -> Cluster Data -> LLM with Hard Concrete Gates -> L0 Loss Optimization -> Discrete Masks -> Input -> Encoder -> Nearest Centroid Lookup -> Apply Mask -> Accelerated LLM
- **Critical path:** The Mask Training phase (Section 3.1, Eq 8). The efficiency of the system relies on the masks converging quickly (~15 mins on H800s) without tuning model weights W.
- **Design tradeoffs:**
  - Cluster Count (N): Higher N increases mask diversity and performance but increases offline training time and mask storage
  - Pruning Granularity: Block-level (separate Attention/FFN) outperforms Layer-level at ~20% sparsity, but gap narrows at higher sparsity
  - Calibration Data: High-quality diverse data is strictly better than narrow data, unlike static baselines which are data-agnostic
- **Failure signatures:**
  - Routing Collapse: If all inputs map to a single mask, the system degrades to a static pruned model
  - Autoregressive Divergence: If Attention skipping is implemented without KV cache updates, generation will produce gibberish
  - Accuracy Cliff: At >30% sparsity, performance drops significantly, suggesting a hard limit to redundancy removal
- **First 3 experiments:**
  1. Run inference on distinct task types and log cluster IDs selected to verify different tasks trigger different masks
  2. Retrain masks using a small, domain-specific dataset and measure performance drop on general tasks to validate diverse data requirement
  3. Measure wall-clock time per token at 12.5% vs 25% sparsity to verify actual speedups from KV cache preservation

## Open Questions the Paper Calls Out
- How does dynamic block pruning affect model factuality and hallucination rates in LLMs? The authors note they do not investigate this impact, though removing computational blocks risks eliminating components critical for factual recall.
- What is the optimal number of semantic clusters, and what are the trade-offs between cluster count, memory overhead, and routing accuracy? Only three cluster configurations (4, 8, 16) were tested without exploring upper bounds or marginal returns.

## Limitations
- Evaluation scope remains narrow, not extending to supervised fine-tuning, multi-turn dialogue, or non-English benchmarks
- L0 optimization relies on fixed Lagrangian schedule without exploring adaptive tuning across model scales
- Reliance on external sentence encoder introduces additional inference cost not factored into reported latency gains

## Confidence
- **High confidence:** The dynamic routing mechanism based on semantic clustering is technically sound and reproducible
- **Medium confidence:** Empirical gains over baselines are robust within tested zero-shot regime, but generalizability to other settings remains unproven
- **Low confidence:** KV-cache consistency mechanism and implications for long-form generation correctness are underspecified

## Next Checks
1. For each task, log cluster assignment distribution over validation set to confirm routing doesn't collapse
2. Retrain masks using narrow domain corpus and measure zero-shot performance drop on general tasks
3. Generate sequences longer than 512 tokens under 25% sparsity and compare perplexity against dense model to detect causal masking issues