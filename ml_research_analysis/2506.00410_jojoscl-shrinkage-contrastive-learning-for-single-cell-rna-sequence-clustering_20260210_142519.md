---
ver: rpa2
title: 'JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering'
arxiv_id: '2506.00410'
source_url: https://arxiv.org/abs/2506.00410
tags:
- clustering
- data
- learning
- contrastive
- jojoscl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents JojoSCL, a self-supervised contrastive learning\
  \ framework designed to improve single-cell RNA sequencing (scRNA-seq) clustering\
  \ by addressing high dimensionality and sparsity challenges. The method introduces\
  \ a shrinkage estimator based on hierarchical Bayesian estimation, which refines\
  \ gene expression estimates toward cluster centroids using Stein\u2019s Unbiased\
  \ Risk Estimate (SURE) to reduce intra-cluster dispersion."
---

# JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering

## Quick Facts
- arXiv ID: 2506.00410
- Source URL: https://arxiv.org/abs/2506.00410
- Reference count: 37
- This paper presents JojoSCL, a self-supervised contrastive learning framework that consistently outperforms five leading clustering methods, achieving average improvements of 26% in Adjusted Rand Index and 15% in Normalized Mutual Information on ten scRNA-seq datasets.

## Executive Summary
JojoSCL addresses the challenges of high dimensionality and sparsity in single-cell RNA sequencing (scRNA-seq) data by integrating shrinkage estimation with contrastive learning. The method employs a hierarchical Bayesian estimator based on Stein's Unbiased Risk Estimate (SURE) to refine gene expression estimates toward cluster centroids, reducing intra-cluster dispersion. By optimizing both instance-level and cluster-level contrastive learning objectives with this shrinkage approach, JojoSCL enhances feature representation and cluster separation. Extensive evaluation on ten scRNA-seq datasets demonstrates consistent superiority over five leading clustering methods, with significant improvements in clustering accuracy metrics.

## Method Summary
JojoSCL is a self-supervised contrastive learning framework for scRNA-seq clustering that combines data augmentation (random gene masking + Gaussian noise), dual encoder architecture with momentum updates, and a shrinkage estimator based on hierarchical Bayesian estimation. The method calculates temporal cluster assignments using K-means on encoder features and optimizes a combined loss function consisting of SURE-based shrinkage loss, instance-level contrastive loss, and cluster-level contrastive loss. The framework processes scRNA-seq vectors through augmentation, dual encoders (query and momentum-updated key), projection MLPs, and a shrinkage module that estimates cluster-specific priors for the SURE loss calculation.

## Key Results
- JojoSCL achieves average improvements of 26% in Adjusted Rand Index (ARI) and 15% in Normalized Mutual Information (NMI) compared to five baseline clustering methods
- The method demonstrates superior robustness to noise and effectiveness under data downsampling conditions
- Ablation studies confirm the critical role of the shrinkage estimator in improving pairwise similarity and clustering accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shrinkage estimation reduces the mean squared error (MSE) of gene expression estimates compared to standard Maximum Likelihood Estimation (MLE), provided the data conforms to specific distributional assumptions.
- **Mechanism:** The framework models gene expression using a hierarchical Bayesian structure ($X \sim Normal(\theta, \sigma^2)$ with prior $\theta \sim Normal(\mu, \tau^2)$). By employing a Maximum A Posteriori (MAP) estimator, it "shrinks" noisy observations toward cluster centroids rather than treating raw observations as ground truth. This utilizes the bias-variance tradeoff to lower estimation risk.
- **Core assumption:** The scRNA-seq data points are normally distributed around cluster centroids with a shared covariance structure.
- **Evidence anchors:**
  - [abstract] "...shrinkage estimator based on hierarchical Bayesian estimation... refines gene expression estimates toward cluster centroids..."
  - [section II] "MSE($\hat{\theta}_{JS}$) = MSE($\hat{\theta}_{MLE}$) - ... suggesting that $\theta_{JS}$ is guaranteed to have a lower MSE..."
  - [corpus] Related work (e.g., scSiameseClu, Hypergraph methods) addresses noise, but this specific SURE-based shrinkage mechanism is distinct in the provided text.
- **Break condition:** If the gene expression data deviates significantly from normality or clusters are not spherical (violating shared covariance assumptions), the shrinkage may bias estimates incorrectly.

### Mechanism 2
- **Claim:** Minimizing Stein's Unbiased Risk Estimate (SURE) acts as a regularizer that tightens intra-cluster distance, thereby amplifying the signal available to contrastive loss functions.
- **Mechanism:** The SURE loss term ($L_{SURE}$) penalizes dispersion from the cluster centroid. By minimizing this alongside contrastive losses, the model forces embeddings to occupy tighter regions in the latent space. This effectively increases the cosine similarity gap between positive pairs (same cluster) and negative pairs (different clusters).
- **Core assumption:** Reducing the Euclidean distance between an embedding and its centroid directly translates to improved cosine similarity discrimination for the contrastive loss.
- **Evidence anchors:**
  - [abstract] "...reduce intra-cluster dispersion... enhances feature representation and cluster separation."
  - [section 3.3] "LSURE refines instance-level contrastive learning by aligning embeddings closer to their centroids... increases it [Euclidean distance] between dissimilar ones."
  - [section 4.5] "LSURE increases the mean of the difference in cosine similarity between positive and negative pairs in 9 out of 10 datasets."
- **Break condition:** If the initial centroids are poorly estimated (e.g., via the preliminary K-means step), the SURE optimization might collapse distinct biological cell types into a single, incorrect cluster.

### Mechanism 3
- **Claim:** Dynamic estimation of prior parameters ($\mu_k, \tau_k$) allows the shrinkage intensity to adapt to the specific geometry of different clusters.
- **Mechanism:** Rather than shrinking toward a static origin (as in James-Stein), JojoSCL calculates cluster-specific means ($\hat{\mu}_k$) and variances ($\hat{\tau}^2_k$) using K-means assignments on the encoder features. This allows the "shrinkage factor" ($\frac{\tau^2}{\tau^2+\sigma^2}$) to vary per cluster, adapting to the density of different cell types.
- **Core assumption:** The K-means algorithm provides sufficiently accurate "temporal cluster labels" to estimate the prior distribution parameters during training.
- **Evidence anchors:**
  - [section III-B] "...we run the K-means algorithm on the features $h^a_i$ to assign temporal cluster label k... we estimate $\mu_k$ component-wise..."
  - [corpus] Weak/missing. Corpus focuses on embeddings/LLMs, not the specific statistical dynamics of K-means priors in this context.
- **Break condition:** If clusters are highly imbalanced, the Central Limit Theorem (CLT) approximation for $\tau_k$ may fail for smaller clusters, leading to unstable shrinkage factors.

## Foundational Learning

- **Concept:** **James-Stein Estimator & Shrinkage**
  - **Why needed here:** The core innovation is modifying standard contrastive learning with a statistical shrinkage estimator. Understanding that "shrinking" estimates toward a central point reduces variance at the cost of bias (lowering total MSE) is non-negotiable for interpreting the $L_{SURE}$ term.
  - **Quick check question:** In high dimensions, why would a biased estimator (James-Stein) outperform an unbiased estimator (MLE) in terms of Mean Squared Error?

- **Concept:** **Contrastive Learning (Instance vs. Cluster-level)**
  - **Why needed here:** JojoSCL optimizes two distinct contrastive objectives simultaneously. You must understand how instance-level loss pulls augmented views of the same cell together, while cluster-level loss pushes all cells toward distinct cluster prototypes.
  - **Quick check question:** What constitutes a "positive pair" vs. a "negative pair" in the instance-level loss formulation used here?

- **Concept:** **Momentum Encoders**
  - **Why needed here:** The architecture uses a momentum-based encoder ($f_k$) to stabilize training. Understanding how $\theta_k$ evolves as a moving average of $\theta_q$ helps explain how consistent feature representations are maintained for contrastive learning.
  - **Quick check question:** How does the update rule $\theta_k = m\theta_k + (1-m)\theta_q$ differ from standard backpropagation, and why does it help prevent oscillation in contrastive learning?

## Architecture Onboarding

- **Component map:** Input -> Augmentation (masking + noise) -> Dual Encoders (query + momentum key) -> Projection MLPs (instance + cluster) -> K-means assignment -> Shrinkage module (SURE loss) -> Combined loss aggregation

- **Critical path:** The **K-means assignment inside the training loop**. The calculation of the SURE loss depends entirely on the "temporal cluster labels" derived from K-means running on the current batch's features. If this step fails or produces degenerate clusters, the shrinkage loss has no valid reference point.

- **Design tradeoffs:**
  - **Statistical Rigor vs. Stability:** The paper assumes normality and uses K-means for prior estimation. This adds statistical power but introduces instability if clusters change drastically between epochs.
  - **Noise Injection:** The paper adds Gaussian noise during augmentation. While generally beneficial for robustness, Section IV-C shows this actually lowered performance on the "Mouse" dataset (Negative difference), suggesting it is not universally optimal for all data distributions.

- **Failure signatures:**
  - **Mode Collapse:** If $L_{SURE}$ dominates, all cells might shrink toward a global mean or a few dominant centroids, erasing biological nuance.
  - **Divergence:** If the K-means centroids fluctuate wildly between epochs, the SURE loss will chase a moving target, preventing convergence.

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the model with $\alpha=1, \beta=1$ but $L_{SURE}$ disabled (系数=0). Verify that the baseline contrastive learning works before introducing the complex shrinkage logic.
  2. **Hyperparameter Sensitivity:** Test different values for the momentum coefficient $m$ (default often 0.999 or 0.99) to ensure the Key Encoder updates smoothly enough to provide stable centroids for the Shrinkage module.
  3. **Visual Inspection of Centroids:** Extract the cluster assignments after 10, 50, and 100 epochs. Plot the distance between estimated centroids ($\mu_k$) to ensure they are separating rather than collapsing into a single point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the violation of the normality assumption affect the reliability of the SURE optimization for highly sparse, non-Gaussian scRNA-seq data?
- **Basis in paper:** [inferred] Section II explicitly assumes data points are "normally distributed around cluster centroids" to derive the shrinkage estimator, despite real scRNA-seq data being zero-inflated count data.
- **Why unresolved:** The mathematical derivation of the James-Stein and MAP estimators relies on Gaussian properties (Eq. 4–5), and the paper does not validate the estimator's behavior when these distributional assumptions are violated by biological noise.
- **What evidence would resolve it:** A theoretical analysis of SURE robustness under Zero-Inflated Negative Binomial (ZINB) distributions or empirical testing on simulated data with varying dropout rates.

### Open Question 2
- **Question:** Can the computational complexity be reduced to allow JojoSCL to scale to atlas-level datasets?
- **Basis in paper:** [inferred] Section IV.A states the computational complexity is $O(E \cdot (N^2))$, which creates a significant bottleneck for the large sample sizes ($N$) common in modern scRNA-seq studies.
- **Why unresolved:** The quadratic complexity stems from the pairwise similarity calculations required for contrastive learning, and the paper does not propose efficiency improvements like memory banks or approximate nearest neighbors.
- **What evidence would resolve it:** A modified algorithm utilizing efficient contrastive learning techniques that maintains clustering accuracy while reducing the complexity to linear time.

### Open Question 3
- **Question:** How sensitive is the convergence of the SURE loss to the instability of the preliminary K-means cluster assignments?
- **Basis in paper:** [inferred] Section III.B notes that the shrinkage targets ($\mu_k$, $\tau_k$) depend on "temporal cluster labels" generated by K-means, which is known to be sensitive to initialization and local optima.
- **Why unresolved:** If the K-means assignments are erroneous early in training, the estimated intra-cluster variances (Eq. 18) and SURE calculations (Eq. 21) may optimize towards incorrect centroids, creating a feedback loop of errors.
- **What evidence would resolve it:** An ablation study analyzing performance variance when initializing the temporal labels with spectral clustering or ground truth labels versus standard K-means.

## Limitations
- The K-means-based prior estimation for shrinkage introduces potential instability if cluster assignments fluctuate significantly between epochs
- The assumption of normal distribution for gene expression around cluster centroids may not hold for all datasets
- The optimal noise injection parameters for augmentation appear dataset-dependent

## Confidence

- **High Confidence:** The experimental results demonstrating superior ARI/NMI performance (26%/15% average improvements) and the ablation study showing the critical role of the shrinkage estimator are well-supported by the data.
- **Medium Confidence:** The theoretical justification for why shrinkage reduces MSE in high-dimensional spaces is sound, but the practical implementation details (exact hyperparameters, encoder architecture) require careful tuning.
- **Low Confidence:** The generalizability of the momentum encoder framework to extremely large datasets (>100k cells) or its behavior with highly imbalanced cluster distributions remains unclear from the current evaluation.

## Next Checks
1. **Centroid Stability Analysis:** Track the movement of K-means centroids across training epochs to quantify the stability of the temporal cluster assignments used for shrinkage estimation.
2. **Distribution Assumption Test:** Apply the Kolmogorov-Smirnov test to gene expression data within clusters to empirically verify the normality assumption underlying the shrinkage estimator.
3. **Noise Sensitivity Grid Search:** Systematically vary the noise level and masking ratio parameters to identify optimal augmentation strategies for each dataset and confirm whether the current settings are globally optimal or require dataset-specific tuning.