---
ver: rpa2
title: 'Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT
  Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation
  Study'
arxiv_id: '2511.13107'
source_url: https://arxiv.org/abs/2511.13107
tags:
- consort
- reporting
- trials
- items
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated 16 large language models (LLMs) on their ability
  to identify adherence to CONSORT 2010 reporting guidelines in randomized controlled
  trials. Using a gold-standard dataset of 150 RCTs, the models were tested in a zero-shot
  setting to classify each CONSORT item as Compliant, Non-Compliant, or Not Applicable.
---

# Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study

## Quick Facts
- arXiv ID: 2511.13107
- Source URL: https://arxiv.org/abs/2511.13107
- Reference count: 40
- Primary result: Best-performing models achieved macro-averaged F1-score of 0.634 in zero-shot CONSORT assessment

## Executive Summary
This study evaluated 16 large language models on their ability to identify adherence to CONSORT 2010 reporting guidelines in randomized controlled trials. Using a gold-standard dataset of 150 RCTs, the models were tested in a zero-shot setting to classify each CONSORT item as Compliant, Non-Compliant, or Not Applicable. The best-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved a macro-averaged F1-score of 0.634, indicating only fair agreement with human experts.

While LLMs performed well in identifying Compliant items, they struggled significantly with Non-Compliant and Not Applicable items, with F1-scores rarely exceeding 0.400. This suggests that LLMs are not yet reliable for autonomous use in assessing trial quality but may serve as preliminary screening tools to assist human reviewers.

## Method Summary
The study employed a zero-shot evaluation approach where 16 different LLMs were tasked with classifying CONSORT items from 150 randomized controlled trials. Each CONSORT item was categorized as Compliant, Non-Compliant, or Not Applicable based on the reporting quality. The evaluation used a gold-standard dataset with human expert annotations as the reference standard. Performance was measured using F1-scores, precision, and recall metrics for each category, with macro-averaging used to account for class imbalance.

## Key Results
- Gemini-2.5-Flash and DeepSeek-R1 achieved the highest macro-averaged F1-score of 0.634
- Models showed strong performance in identifying Compliant items but poor performance on Non-Compliant and Not Applicable items (F1 rarely > 0.400)
- Overall performance indicates LLMs are not yet reliable for autonomous CONSORT assessment

## Why This Works (Mechanism)
The evaluation leverages LLMs' natural language understanding capabilities to assess structured reporting guidelines. The zero-shot approach tests the models' ability to apply CONSORT criteria without task-specific training, demonstrating both the potential and current limitations of using LLMs for systematic review quality assessment.

## Foundational Learning
**CONSORT guidelines** - Standardized reporting framework for randomized controlled trials
*Why needed*: Provides the evaluation criteria for assessing trial quality
*Quick check*: Verify familiarity with CONSORT 2010 checklist items

**Zero-shot learning** - Model evaluation without task-specific training
*Why needed*: Establishes baseline performance capabilities
*Quick check*: Confirm understanding of prompt-based evaluation without fine-tuning

**Macro-averaged F1-score** - Performance metric accounting for class imbalance
*Why needed*: Provides balanced assessment across Compliant, Non-Compliant, and Not Applicable categories
*Quick check*: Calculate weighted vs. macro-averaged scores for imbalanced datasets

**Gold-standard dataset** - Expert-annotated reference standard
*Why needed*: Provides ground truth for model performance evaluation
*Quick check*: Review inter-rater reliability among human experts

**Systematic review quality assessment** - Process of evaluating trial reporting completeness
*Why needed*: Contextualizes the practical application of the research
*Quick check*: Outline typical workflow for human reviewers assessing CONSORT adherence

## Architecture Onboarding

**Component Map**: RCT abstracts/documents -> LLMs (16 models) -> CONSORT classification -> Performance metrics comparison

**Critical Path**: Document input → LLM processing → CONSORT item classification → F1-score calculation → Performance comparison

**Design Tradeoffs**: Zero-shot evaluation provides generalizability but may underestimate potential performance; single dataset limits representativeness but ensures consistency

**Failure Signatures**: Low F1-scores on Non-Compliant items suggest models struggle with identifying reporting deficiencies; poor performance on Not Applicable items indicates difficulty with contextual judgment

**3 First Experiments**:
1. Test few-shot learning approach with the same dataset to assess performance improvement
2. Evaluate model performance on RCTs from different medical specialties
3. Compare human reviewer performance with LLM assistance versus human-only assessment

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot evaluation may underestimate LLM potential with task-specific fine-tuning
- Single gold-standard dataset may not represent full diversity of reporting quality
- Particularly poor performance on Non-Compliant items suggests fundamental interpretation challenges

## Confidence
**High confidence**: LLMs are currently unreliable for autonomous CONSORT assessment
**Medium confidence**: LLMs may serve as preliminary screening tools
**Low confidence**: Generalizability of specific F1-score thresholds to other guideline frameworks

## Next Checks
1. Evaluate LLM performance using few-shot learning or task-specific fine-tuning to determine if performance improves beyond zero-shot baseline
2. Test model performance across multiple gold-standard datasets from different medical specialties to assess generalizability
3. Conduct workflow simulation comparing human reviewer performance with and without LLM assistance