---
ver: rpa2
title: 'Hookpad Aria: A Copilot for Songwriters'
arxiv_id: '2502.08122'
source_url: https://arxiv.org/abs/2502.08122
tags:
- music
- hookpad
- aria
- users
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hookpad Aria is a generative AI system integrated into Hookpad,
  a web-based editor for composing Western pop songs as lead sheets. The system supports
  non-sequential composition workflows including left-to-right generation, fill-in-the-middle,
  and bidirectional melody-harmony generation.
---

# Hookpad Aria: A Copilot for Songwriters

## Quick Facts
- arXiv ID: 2502.08122
- Source URL: https://arxiv.org/abs/2502.08122
- Reference count: 0
- Hookpad Aria is a generative AI system integrated into Hookpad that supports non-sequential composition workflows including left-to-right generation, fill-in-the-middle, and bidirectional melody-harmony generation.

## Executive Summary
Hookpad Aria is a generative AI system integrated into Hookpad, a web-based editor for composing Western pop songs as lead sheets. The system supports non-sequential composition workflows including left-to-right generation, fill-in-the-middle, and bidirectional melody-harmony generation. Built on the Anticipatory Music Transformer fine-tuned on 50k lead sheets from TheoryTab, Aria offers users control over selected regions while conditioning on global song attributes like meter, key, and tempo. Since its March 2024 release, Aria has generated 318k suggestions for 3k users, with 74k suggestions accepted into songs.

## Method Summary
Hookpad Aria fine-tunes the Anticipatory Music Transformer (360M parameters) on 50k lead sheets from TheoryTab. The system encodes functional harmony and melody into MIDI with a click track (one note per beat), then partitions notes into events (e) and controls (c) based on the requested generation capability. Controls are shifted forward by 5 seconds to enable conditioning on future context. For each training example, the system randomly samples a time span and capability, then partitions melody (M), harmony (H), and click track (C) accordingly. The model generates suggestions autoregressively conditioned on controls, which are decoded back to functional harmony and streamed to the Hookpad UI.

## Key Results
- Hookpad Aria has generated 318k suggestions for 3k users since March 2024
- Users have accepted 74k suggestions into their songs (23% acceptance rate)
- User interviews (n=8) revealed that Aria facilitates creative ideation while preserving user agency through short reusable suggestions and seamless integration

## Why This Works (Mechanism)

### Mechanism 1: Anticipatory Control Sequence for Bidirectional Generation
- Claim: Partitioning notes into events and time-shifted controls enables fill-in-the-middle generation without requiring a separate bidirectional architecture.
- Mechanism: MIDI notes are split into two sequences—events e and controls c—where controls are shifted forward by 5 seconds and interleaved with events. The model learns Pθ(e | c), conditioning generation on both preceding and subsequent context by assigning future notes to the control sequence.
- Core assumption: The 5-second anticipation window captures sufficient future context for musical coherence in lead sheet generation.
- Evidence anchors:
  - [section] "In the Anticipatory Music Transformer, MIDI notes... are partitioned into two sequences: events e and controls c. Controls are shifted forward in time by 5 seconds."
  - [section] "By partitioning training examples such that notes in subsequent context are assigned to controls, the Anticipatory Music Transformer learns to fill-in-the-middle."
- Break condition: If musical structure requires context beyond 5 seconds (e.g., long-range thematic development), anticipation window may prove insufficient.

### Mechanism 2: Randomized Capability-Specific Fine-Tuning
- Claim: Fine-tuning on randomly constructed examples with capability-specific event/control partitions enables a single model to support multiple generation modes.
- Mechanism: For each training example, the system (1) selects a random time span, (2) randomly chooses a target capability (left-to-right, fill-in-middle, harmony-to-melody, melody-to-harmony), and (3) partitions melody, harmony, and click track notes according to capability-specific rules in Table 1.
- Core assumption: Random sampling across capabilities during training produces a model that generalizes to user-specified capabilities at inference time.
- Evidence anchors:
  - [section] "For each song in the Theorytab data, we construct random fine tuning examples by: (1) selecting a random time span... (2) randomly choosing a control capability... (3) partitioning the song into events and controls."
  - [section] "Our backbone is the result of fine tuning on many of these random examples per song."
- Break condition: If capability distributions are imbalanced during training or inference, model may favor frequently-sampled capabilities.

### Mechanism 3: Data Flywheel from Implicit Feedback
- Claim: Logging user accept/reject decisions on suggestions creates a scalable feedback signal for future model alignment and personalization.
- Mechanism: Users select regions, audition endless alternatives, and either accept (74k instances) or ignore suggestions. This implicit preference signal is logged for future research in RLHF-style alignment and A/B testing.
- Core assumption: Acceptance into a song correlates with user preference and musical quality; ignored suggestions represent negative signal.
- Evidence anchors:
  - [abstract] "Hookpad Aria is also a scalable data flywheel for music co-creation—since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs."
  - [section] "We log whether suggestions are accepted or ignored as implicit feedback data—74k have been accepted to date."
- Break condition: If acceptance is driven by convenience rather than quality (e.g., users accept first suggestion to avoid searching), feedback signal will be noisy.

## Foundational Learning

- Concept: Autoregressive language modeling for symbolic music
  - Why needed here: Understanding how the Anticipatory Music Transformer extends standard left-to-right LLM generation to bidirectional infilling.
  - Quick check question: Can you explain why shifting controls forward enables conditioning on future context without changing the model architecture?

- Concept: Functional harmony representation
  - Why needed here: Hookpad uses a proprietary functional harmony encoding (Roman numeral analysis) rather than raw MIDI, requiring custom encoding/decoding.
  - Quick check question: How would you represent a V7 chord resolving to I in functional harmony vs. raw MIDI pitch numbers?

- Concept: Click track as temporal anchor
  - Why needed here: The system adds a synthetic "click track instrument" to convert beat-indexed time to absolute time for the model.
  - Quick check question: Why can't the model directly use beat indices without this conversion?

## Architecture Onboarding

- Component map:
  Hookpad editor -> measure selection -> global attributes (meter, key, tempo) -> encoding layer -> Anticipatory Music Transformer -> decoding layer -> Hookpad rendering -> feedback layer

- Critical path:
  1. User selects span in Hookpad (measures + generation mode)
  2. System encodes surrounding context and global attributes
  3. Partition notes into events/controls based on requested capability
  4. Model generates suggestions autoregressively conditioned on controls
  5. Decode and stream suggestions to UI for audition
  6. Log acceptance decision when user clicks "accept"

- Design tradeoffs:
  - **Anticipation window (5s)**: Longer windows capture more context but increase latency and memory; shorter windows may miss structural coherence.
  - **Capability randomization**: Unified model simplifies deployment but may underperform specialized single-task models.
  - **Implicit vs. explicit feedback**: Implicit signals scale better but are noisier than explicit ratings.

- Failure signatures:
  - Suggestions ignored by user but not logged as negative signal (partial observability)
  - Model generates suggestions incompatible with surrounding harmonic context (partitioning error or insufficient context)
  - Click track drift causing misaligned rhythm in generated output

- First 3 experiments:
  1. **A/B test anticipation windows**: Compare 3s vs. 5s vs. 10s windows on acceptance rate and user-reported coherence.
  2. **Capability-specific evaluation**: Measure acceptance rates by generation mode (left-to-right vs. fill-in-middle vs. melody-to-harmony) to identify underperforming capabilities.
  3. **Feedback signal validation**: Correlate implicit acceptance with explicit user ratings on a subset to validate flywheel assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can implicit user feedback (acceptance/rejection rates) be effectively utilized to align generative music models with songwriter preferences?
- Basis in paper: [explicit] The authors identify "aligning models to user feedback" as a primary research opportunity enabled by their data flywheel.
- Why unresolved: While the system collects acceptance data (74k suggestions), the methodology for converting this implicit feedback into model updates (e.g., via RLHF) is not yet implemented or detailed.
- What evidence would resolve it: A study demonstrating that a model fine-tuned on user acceptance data achieves higher acceptance rates in production compared to the baseline model.

### Open Question 2
- Question: Can high-level semantic controls for genre, emotional tone, and structural elements be integrated into symbolic generation without disrupting musical coherence?
- Basis in paper: [explicit] User interviews revealed that participants desired additional control beyond the current global attributes (meter, key, tempo), specifically requesting genre, emotional tone, and intended instruments.
- Why unresolved: The current system conditions only on basic musical attributes; semantic conditioning remains an unimplemented feature request.
- What evidence would resolve it: A functional interface allowing text-based or categorical control over style that results in statistically significant changes to the generated output as validated by user listening tests.

### Open Question 3
- Question: How do different generative backbones compare in terms of user engagement and creative utility when evaluated in an in-the-wild deployment?
- Basis in paper: [explicit] The authors propose "A/B testing different generative models for in-the-wild evaluation" as a specific avenue for future work.
- Why unresolved: The paper evaluates only a single model configuration (Anticipatory Music Transformer fine-tuned on TheoryTab) without comparing it against alternative architectures or baselines.
- What evidence would resolve it: Comparative analytics from a randomized control trial where users are assigned different model backends, measuring metrics such as session duration and suggestion acceptance rate.

## Limitations
- The relationship between acceptance rate (23%) and musical quality remains unproven without explicit quality ratings
- The 5-second anticipation window may be insufficient for capturing long-range musical dependencies
- The unified multi-capability model may underperform specialized single-task architectures

## Confidence

**User Impact Claims (High)**: Qualitative findings from user interviews are specific and internally consistent, though small sample size limits generalizability.

**Technical Architecture Claims (Medium)**: The Anticipatory Music Transformer's bidirectional generation mechanism is theoretically sound but lacks direct empirical validation beyond usage metrics.

**Data Flywheel Claims (Low)**: The claim that implicit feedback creates a scalable data flywheel is forward-looking and currently unverified.

## Next Checks

1. **A/B Test Anticipation Windows**: Systematically compare 3s vs. 5s vs. 10s anticipation windows on acceptance rate and user-reported coherence to determine optimal context window size for musical generation.

2. **Capability-Specific Performance Analysis**: Measure acceptance rates by generation mode (left-to-right vs. fill-in-middle vs. melody-to-harmony) and conduct targeted user studies to identify underperforming capabilities and their root causes.

3. **Implicit Feedback Validation Study**: Correlate implicit acceptance decisions with explicit user ratings on a subset of suggestions to validate whether the acceptance rate serves as a reliable quality signal for model improvement.