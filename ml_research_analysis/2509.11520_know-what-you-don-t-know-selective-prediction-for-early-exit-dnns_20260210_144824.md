---
ver: rpa2
title: 'Know What You Don''t Know: Selective Prediction for Early Exit DNNs'
arxiv_id: '2509.11520'
source_url: https://arxiv.org/abs/2509.11520
tags:
- layer
- samples
- confidence
- sample
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPEED, a selective prediction framework for
  early exit deep neural networks (EEDNNs) that addresses overconfidence issues. SPEED
  uses Deferral Classifiers (DCs) at each layer to detect hard samples likely to cause
  hallucinations, deferring them to experts rather than relying solely on confidence
  scores.
---

# Know What You Don't Know: Selective Prediction for Early Exit DNNs

## Quick Facts
- arXiv ID: 2509.11520
- Source URL: https://arxiv.org/abs/2509.11520
- Reference count: 40
- Key outcome: SPEED reduces wrong predictions by 50% while achieving 2.05x speedup over standard inference

## Executive Summary
This paper introduces SPEED, a selective prediction framework for early exit deep neural networks (EEDNNs) that addresses overconfidence issues. SPEED uses Deferral Classifiers (DCs) at each layer to detect hard samples likely to cause hallucinations, deferring them to experts rather than relying solely on confidence scores. The method trains DCs using samples labeled as easy or hard based on their confidence patterns across layers, enabling early detection of overconfidence and underconfidence cases.

## Method Summary
SPEED introduces Deferral Classifiers (DCs) at each layer of early exit DNNs to identify samples that should not be predicted by the current layer. The framework trains DCs using samples labeled as easy (low confidence variation across layers) or hard (high confidence variation), allowing detection of fake confidence cases where models are highly confident but incorrect. The method significantly improves trustworthiness by reducing wrong predictions while maintaining inference speed advantages over standard approaches.

## Key Results
- Wrong predictions reduced by 50% compared to standard inference
- 2.05x speedup achieved over standard inference
- Outperforms existing baselines like BERT-SR, SelectiveNet, and calibration-based approaches

## Why This Works (Mechanism)
SPEED works by detecting overconfidence and underconfidence patterns through deferral classifiers trained on confidence variation patterns across layers. The framework identifies hard samples that would otherwise be incorrectly predicted with high confidence, deferring them to subsequent layers or expert models. This selective approach maintains the efficiency benefits of early exits while improving reliability.

## Foundational Learning

**Confidence calibration**: Understanding how model confidence relates to actual correctness probability is crucial for identifying miscalibrated predictions. Quick check: Compare confidence scores against empirical accuracy on validation data.

**Early exit architectures**: Familiarity with multi-exit neural networks where predictions can be made at different depths. Quick check: Review how intermediate classifiers are structured in standard EEDNNs.

**Selective prediction**: Knowledge of frameworks that choose when to make predictions versus deferring to experts. Quick check: Understand the trade-off between early prediction benefits and error risks.

## Architecture Onboarding

**Component map**: Input -> Early exit DNN layers -> Deferral Classifiers (one per layer) -> Decision: predict now OR defer to next layer/expert

**Critical path**: Sample confidence scores across layers → Difficulty labeling → DC training → Inference-time deferral decisions

**Design tradeoffs**: Early prediction speed vs. deferral accuracy, number of DCs vs. computational overhead, confidence threshold selection vs. deferral rate

**Failure signatures**: DCs failing to detect overconfidence cases, incorrect difficulty labeling during training, confidence patterns not indicative of true sample difficulty

**3 first experiments**:
1. Validate DC performance on distinguishing easy vs. hard samples using synthetic confidence patterns
2. Test baseline EEDNN performance with and without deferral mechanism
3. Evaluate impact of different confidence threshold settings on deferral rates

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Assumes confidence patterns across layers reliably indicate sample difficulty, which may not generalize to all architectures
- Additional deferral classifiers increase model complexity and training time
- Performance heavily depends on quality of difficulty labels derived from potentially miscalibrated confidence scores

## Confidence
High: 50% reduction in wrong predictions and 2.05x speedup are well-supported by evaluation on multiple datasets
Medium: Generalization claims across domains supported by three datasets but may not extend to all possible domains
Low: Theoretical analysis conditions for risk minimization not fully characterized in practical scenarios

## Next Checks
1. Test SPEED's performance on adversarial examples and out-of-distribution samples to evaluate robustness
2. Conduct ablation studies on different difficulty labeling strategies and their impact on DC training
3. Evaluate computational overhead and memory requirements of adding deferral classifiers across various EEDNN architectures