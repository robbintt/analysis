---
ver: rpa2
title: 'RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across
  Domains'
arxiv_id: '2511.02331'
source_url: https://arxiv.org/abs/2511.02331
tags:
- instances
- rome
- learning
- domains
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoME, a domain-robust Mixture-of-Experts
  framework for cross-domain MILP solution prediction. RoME addresses the limitation
  of existing single-domain learning approaches by training a unified model across
  multiple MILP domains using a two-level distributionally robust optimization strategy.
---

# RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains

## Quick Facts
- **arXiv ID**: 2511.02331
- **Source URL**: https://arxiv.org/abs/2511.02331
- **Reference count**: 40
- **Primary result**: Achieves 67.7% average improvement over strong baselines across five MILP domains

## Executive Summary
This paper introduces RoME, a domain-robust Mixture-of-Experts framework designed to address the critical limitation of existing single-domain learning approaches for Mixed Integer Linear Programming (MILP) solution prediction. By training a unified model across multiple MILP domains, RoME employs a two-level distributionally robust optimization strategy to enhance cross-domain generalization. The framework combines shared graph encoding with domain-specific expert networks and dynamic routing mechanisms, demonstrating significant performance gains particularly in zero-shot settings on unseen MIPLIB instances where traditional approaches fail to generalize effectively.

## Method Summary
RoME addresses the challenge of learning effective MILP solution prediction models that can generalize across multiple domains. The framework employs a two-level distributionally robust optimization approach, combining intra-domain robustness through embedding perturbations with inter-domain robustness via group-level DRO. At its core, RoME uses a shared graph encoder to process problem instances, multiple expert networks for domain-specific specialization, and a task decoder with dynamic routing based on task embeddings. This architecture enables the model to leverage knowledge from multiple domains while maintaining the ability to specialize for specific problem types, resulting in improved performance across diverse MILP domains.

## Key Results
- Achieves 67.7% average improvement over strong baselines across five diverse MILP domains
- Demonstrates significant performance gains in zero-shot settings on MIPLIB instances where existing approaches struggle to generalize
- Shows robust cross-domain generalization capabilities through distributionally robust optimization mechanisms

## Why This Works (Mechanism)
The framework's effectiveness stems from its two-level distributionally robust optimization strategy. Intra-domain robustness is achieved through embedding perturbations that help the model handle variability within each domain, while inter-domain robustness via group-level DRO ensures generalization across domain boundaries. The mixture-of-experts architecture allows for both shared learning across domains through the common graph encoder and specialized processing through domain-specific experts. The dynamic routing mechanism based on task embeddings enables the model to adaptively select appropriate expert networks for different problem instances, facilitating effective knowledge transfer while maintaining domain-specific performance.

## Foundational Learning
- **Distributionally Robust Optimization (DRO)**: A framework that optimizes performance under worst-case distributional shifts, essential for cross-domain generalization in MILP problems
  - Why needed: Ensures model performance across different MILP domains and prevents overfitting to specific distributions
  - Quick check: Verify that the group-level DRO implementation correctly partitions data and computes worst-case distributions

- **Mixture-of-Experts (MoE) Architecture**: A neural network design where multiple specialized networks (experts) are combined with a gating mechanism to handle different input types
  - Why needed: Enables both shared representation learning and domain-specific specialization for MILP problems
  - Quick check: Confirm that expert networks are properly activated based on task embeddings and routing decisions

- **Graph Neural Networks for MILP**: Neural architectures designed to process combinatorial optimization problems represented as graphs
  - Why needed: MILP instances have inherent graph structures that can be effectively captured through GNNs
  - Quick check: Validate that the graph encoder properly captures problem structure and relationships between variables and constraints

## Architecture Onboarding

**Component Map**: Input Instance -> Shared Graph Encoder -> Task Embedding -> Dynamic Routing -> Expert Networks -> Task Decoder -> Solution Prediction

**Critical Path**: The primary computation flow involves processing the MILP instance through the shared graph encoder, generating task embeddings, using dynamic routing to select appropriate expert networks, and producing solution predictions through the task decoder.

**Design Tradeoffs**: The framework balances between shared representation learning (through the common graph encoder) and domain specialization (through expert networks). The two-level DRO approach trades computational complexity for improved generalization, while the dynamic routing mechanism adds routing overhead but enables adaptive expert selection.

**Failure Signatures**: Performance degradation may occur when domain boundaries are unclear or when domains share significant structural similarities, potentially causing routing confusion. The model might also struggle with highly novel domains that differ substantially from training domains, and computational overhead could become prohibitive for very large MILP instances.

**First Experiments**:
1. Validate individual expert network performance on their respective domains before integrating the full RoME architecture
2. Test dynamic routing decisions on held-out validation sets to ensure appropriate expert selection
3. Evaluate cross-domain transfer performance with and without DRO mechanisms to quantify their impact

## Open Questions the Paper Calls Out
None

## Limitations
- The substantial 67.7% improvement claim requires careful scrutiny of baseline selection and comparison methodology
- Cross-domain generalization results on MIPLIB need independent validation to confirm robustness across truly unseen domains
- The framework's applicability to other combinatorial optimization problems beyond MILP remains untested

## Confidence

**High**: Technical framework design and core methodology demonstrate sound theoretical foundations and practical implementation

**Medium**: Performance improvements on evaluated domains show significant gains, though baseline comparison methodology needs verification

**Low**: Zero-shot generalization claims and cross-domain robustness under distribution shifts require additional independent validation

## Next Checks
1. Replicate the zero-shot performance evaluation on a separate, independently collected MIPLIB dataset to verify generalization claims
2. Conduct ablation studies isolating the impact of intra-domain versus inter-domain robustness mechanisms on different domain pairs
3. Test framework performance when domain boundaries are deliberately blurred or when domains share significant structural similarities to assess true cross-domain capability