---
ver: rpa2
title: 'From Real to Synthetic: Synthesizing Millions of Diversified and Complicated
  User Instructions with Attributed Grounding'
arxiv_id: '2506.03968'
source_url: https://arxiv.org/abs/2506.03968
tags:
- instructions
- questions
- data
- instruction
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for synthesizing large-scale,
  high-quality instruction data for language model alignment. The core idea is "attributed
  grounding," which attributes real user instructions to real-world documents, users,
  and motivations, then uses this to synthesize new grounded instructions from web
  documents.
---

# From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding

## Quick Facts
- arXiv ID: 2506.03968
- Source URL: https://arxiv.org/abs/2506.03968
- Reference count: 40
- Creates 1 million synthetic instructions that outperform existing open-source datasets on alignment benchmarks

## Executive Summary
This paper introduces a novel framework for synthesizing large-scale, high-quality instruction data for language model alignment. The core idea is "attributed grounding," which attributes real user instructions to real-world documents, users, and motivations, then uses this to synthesize new grounded instructions from web documents. This approach produces more diverse and complex instructions than existing methods by grounding synthetic data in real-world contexts. The authors create a dataset of 1 million instructions (SynthQuestions) and demonstrate that models trained on it achieve state-of-the-art performance on alignment benchmarks compared to other open-source datasets.

## Method Summary
The proposed framework synthesizes diverse and complex instruction data by leveraging real user instructions and web documents. It begins by extracting attributed instructions from real user queries, then grounds these instructions to specific documents, user contexts, and motivations. This attribution information guides the synthesis process, where new instructions are generated from web documents while maintaining diversity and complexity. The method involves selecting diverse web corpora, generating synthetic instructions through large language models guided by the attribution information, and creating comprehensive training data. The framework addresses limitations of existing synthetic data methods by incorporating richer contextual information and more sophisticated grounding mechanisms.

## Key Results
- Models trained on SynthQuestions achieve 19.15% win rate on Alpaca Eval 2.0 and 15.4% win rate on Arena Hard
- Performance comparable to models trained on 10x more data with preference optimization
- Scaling effect shows continual improvement with more web corpora, though limited to 1M instructions in experiments
- Effectiveness demonstrated across different model sizes (Qwen and LLaMA)

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to ground synthetic instructions in real-world contexts through attributed grounding. By linking synthetic instructions to specific documents, user contexts, and motivations, the method creates more diverse and complex instructions that better reflect real-world usage patterns. This grounding approach allows the synthetic data to capture the nuances and complexities of actual user needs, leading to improved model alignment and instruction-following capabilities.

## Foundational Learning
- **Attributed Grounding**: The process of linking synthetic instructions to real-world documents, users, and motivations. This is needed to create contextually rich synthetic data that reflects real user needs. Quick check: Verify that each synthetic instruction can be traced back to specific grounding elements.
- **Instruction Synthesis**: The generation of new instructions from web documents guided by attribution information. This is needed to create diverse and complex instruction sets at scale. Quick check: Assess the diversity of generated instructions across different topics and complexity levels.
- **Data Scaling**: The relationship between the amount of web corpora used and the quality of synthesized instructions. This is needed to understand the optimal data size for training. Quick check: Plot performance against different scales of training data.

## Architecture Onboarding

**Component Map:** Real User Instructions -> Attribution Extraction -> Web Corpus Selection -> Instruction Synthesis -> SynthQuestions Dataset -> Model Training -> Performance Evaluation

**Critical Path:** The most critical path is from Attribution Extraction through Instruction Synthesis to the final SynthQuestions dataset. The quality of attribution directly impacts the diversity and complexity of synthesized instructions, which in turn determines the effectiveness of the final model training.

**Design Tradeoffs:** The framework balances between synthetic data volume and quality through attributed grounding. While generating more instructions is possible, the focus on quality through proper attribution ensures better alignment outcomes. The tradeoff between specialized domains (like math/code) and general web content is managed through careful corpus selection.

**Failure Signatures:** Potential failures include: synthetic instructions that lack proper grounding to real-world contexts, insufficient diversity in generated instructions, or degradation in instruction complexity when scaling up data volume. The framework may also struggle with highly specialized domains where attribution information is limited.

**3 First Experiments:**
1. Test the framework's ability to generate instructions across different domains by varying the web corpus composition
2. Evaluate the impact of different attribution strategies on instruction diversity and complexity
3. Compare performance of models trained on attributed-grounded data versus traditional synthetic instruction datasets

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the positive scaling effect of SynthQuestions continue linearly or logarithmically beyond the 1 million instruction limit tested in this study?
- Basis: [explicit] The authors state in the Limitations section that "while scaling curve shows the potential to further improve model performance, we do not test data scale larger than 1M."
- Why unresolved: The experiments only verified scaling up to 1M tokens; the saturation point where more data yields diminishing returns remains unidentified.
- Evidence: Training LLaMA or Qwen models on 2M, 5M, and 10M instruction subsets and plotting the performance trajectory on Alpaca Eval 2.0 and Arena Hard.

### Open Question 2
- Question: What is the optimal selection strategy and distribution ratio for web corpora (e.g., FineWeb vs. MathPILE) to maximize performance across both alignment and reasoning tasks?
- Basis: [explicit] The Limitations section calls for "a more thorough study about the optimal selection and distribution of web corpora used for synthesizing."
- Why unresolved: The current implementation mixes general web data with math/code documents somewhat heuristically to boost reasoning (Section 4.2), without systematically optimizing the mixture.
- Evidence: A comprehensive ablation study varying the proportions of specialized domains (math, code) versus general web text and measuring the impact on disparate benchmarks like GSM8K vs. Alpaca Eval.

### Open Question 3
- Question: To what extent does training on SynthQuestions induce hallucination or factual inconsistency compared to datasets with human verification?
- Basis: [explicit] The authors note in the Limitations that "the dataset has not been assessed in terms of hallucination, which may lead language models to output false or unfaithful contents."
- Why unresolved: While the data is "grounded" in documents, the instruction/response pairs are synthetically generated by LLMs, which are prone to fabrication.
- Evidence: Evaluating models fine-tuned on SynthQuestions using faithfulness metrics (e.g., FACTSCORE) or human evaluation to determine if outputs remain factual relative to the grounding documents.

## Limitations
- Limited testing of data scale beyond 1 million instructions, leaving the saturation point of scaling effects unknown
- No systematic optimization of web corpus selection and distribution ratios across different domains
- No assessment of hallucination or factual inconsistency in the synthetic dataset

## Confidence
- High confidence in the effectiveness of attributed grounding for generating diverse instructions from web documents
- Medium confidence in the scalability claims, given limited testing across diverse domains
- Medium confidence in the comparative performance metrics, as they rely on specific benchmark suites

## Next Checks
1. Evaluate the framework's performance on multilingual and specialized technical datasets to assess generalizability beyond English web corpora
2. Conduct human evaluations with real users to validate the quality and usability of synthesized instructions in practical applications
3. Test the attributed grounding approach with different types of web content (e.g., academic papers, social media posts) to determine its effectiveness across varied source materials