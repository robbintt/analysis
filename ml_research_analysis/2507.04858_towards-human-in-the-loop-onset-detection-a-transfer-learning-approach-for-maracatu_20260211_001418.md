---
ver: rpa2
title: 'Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for
  Maracatu'
arxiv_id: '2507.04858'
source_url: https://arxiv.org/abs/2507.04858
tags:
- onset
- music
- detection
- learning
- beat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transfer learning approach for musical onset
  detection in the Afro-Brazilian Maracatu tradition, which features complex rhythmic
  patterns challenging conventional models. The study adapts two Temporal Convolutional
  Network architectures - one pre-trained for onset detection and another for beat
  tracking - using only 5-second annotated snippets per instrument to fine-tune the
  models for five traditional percussion instruments.
---

# Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu

## Quick Facts
- arXiv ID: 2507.04858
- Source URL: https://arxiv.org/abs/2507.04858
- Authors: António Sá Pinto
- Reference count: 0
- Primary result: Achieves F1 scores up to 0.998 on Afro-Brazilian Maracatu percussion instruments using only 5-second annotated snippets per instrument

## Executive Summary
This paper addresses the challenge of musical onset detection in underrepresented musical traditions by presenting a transfer learning approach for Afro-Brazilian Maracatu percussion instruments. The methodology leverages pre-trained Temporal Convolutional Network architectures, adapting them using minimal annotation data (5-second snippets per instrument) to achieve significant performance improvements over baseline models. The study systematically evaluates both intra-task (onset→onset) and cross-task (beat→onset) transfer learning scenarios across five traditional percussion instruments, demonstrating the effectiveness of instrument-specific adaptation strategies.

## Method Summary
The approach uses two pre-trained TCN architectures (TCNv1 for onset detection, TCNv2 for beat tracking) and fine-tunes them using 5-second annotated snippets from each instrument. The method tests 15 different layer-freezing configurations (from freezing only early convolutional layers to freezing up to the final TCN block, or no freezing at all) across both intra-task and cross-task transfer scenarios. Models are fine-tuned for 50 epochs with a learning rate set to one-quarter of the original, and evaluated using F1 score with 25ms tolerance window on held-out files from each instrument.

## Key Results
- Intra-task transfer achieves F1 scores up to 0.998, with improvements over 50 percentage points in best-case scenarios
- Cross-task adaptation proves particularly effective for time-keeping instruments where onsets naturally align with beat positions
- Optimal fine-tuning configuration varies by instrument, demonstrating the importance of instrument-specific adaptation strategies
- All five instruments (cuica, gonge-lo, tarol, mineiro, tambor-hi) benefit from the transfer learning approach

## Why This Works (Mechanism)

### Mechanism 1: Minimal-Data Domain Adaptation via Fine-Tuning
- Claim: Pre-trained onset detection models can be adapted to underrepresented musical traditions using only 5 seconds of annotated data per instrument.
- Mechanism: The TCN base model learns general onset features (spectral changes, transient patterns) from Western training data. Fine-tuning recalibrates these representations to instrument-specific acoustic characteristics without catastrophic forgetting, as frozen layers preserve general features while updated layers specialize.
- Core assumption: General onset detection features transfer across musical traditions and instrument types.
- Evidence anchors:
  - [abstract]: "Using only 5-second annotated snippets per instrument, we fine-tune these models... achieving F1 scores up to 0.998 in the intra-task setting and improvements of over 50 percentage points"
  - [section 3.2]: "cuica showing a 52 p.p. gain" from baseline ~0.477 to 0.985; all instruments benefit from adaptation
  - [corpus]: Limited direct evidence—corpus papers focus on larger datasets (GigaMIDI) or different tasks; no comparable 5-second transfer studies found
- Break condition: If target instrument's acoustic characteristics fundamentally differ from pre-training distribution (e.g., non-percussive onsets, extremely soft attacks), minimal data may be insufficient.

### Mechanism 2: Cross-Task Feature Transfer (Beat-to-Onset)
- Claim: Beat-tracking models can be repurposed for onset detection when temporal structure aligns between source task priors and target instrument characteristics.
- Mechanism: Beat-tracking models encode rhythmic periodicity and tempo priors. Time-keeping instruments (cuica, gonge-lo) produce onsets that coincide with beat positions. Maracatu's 165-180 BPM tempo corresponds to inter-beat intervals of 333-363ms, which approximately match these instruments' waveform spans (384-428ms for cuica, 376-400ms for gonge-lo), creating structural alignment.
- Core assumption: Task similarity enables cross-task transfer when structural properties align.
- Evidence anchors:
  - [abstract]: "The cross-task adaptation proves particularly effective for time-keeping instruments where onsets naturally align with beat positions"
  - [section 3.3]: "gonbe-lo exhibits a clearly higher baseline F1 accuracy in the beat-to-onset setting compared to its onset-to-onset counterpart (0.892 vs. 0.508)"
  - [corpus]: No direct corpus evidence for beat-to-onset transfer; this appears to be a novel approach
- Break condition: Voicing instruments with dense, non-beat-aligned onsets show 11-32 percentage point gaps vs. intra-task transfer, indicating limited cross-task benefit without structural alignment.

### Mechanism 3: Instrument-Specific Layer Freezing Strategies
- Claim: Optimal fine-tuning depth varies by instrument; no universal freezing configuration maximizes performance across all instruments.
- Mechanism: TCN layers encode features at different temporal scales—early layers capture local transients (~50ms receptive field), deeper dilated layers capture longer-range temporal context (up to ~1024ms). Instruments with different onset densities and waveform characteristics require different abstraction levels recalibrated.
- Core assumption: Layer depth correlates meaningfully with feature abstraction level; different instruments require different levels of adaptation.
- Evidence anchors:
  - [abstract]: "The optimal fine-tuning configuration varies by instrument, highlighting the importance of instrument-specific adaptation strategies"
  - [section 3.2]: "ftTcn16 model achieves the highest accuracy for cuica and mineiro... for tambor-hi, the best performance is obtained with both ftTcn1024 and ft... for tarol, the highest F1 score (0.997) is achieved with ftConv3"
  - [corpus]: No corpus evidence on layer-wise transfer learning for MIR tasks
- Break condition: Full-network fine-tuning (ft) degrades performance for time-keeping instruments but ranks among top configurations for voicing instruments—suggesting overfitting risk varies by instrument type.

## Foundational Learning

- Concept: **Temporal Convolutional Networks (TCNs) with Dilated Convolutions**
  - Why needed here: TCNs use progressively dilated convolutions to expand receptive fields without parameter explosion, capturing both local transients and longer-range rhythmic context essential for onset detection.
  - Quick check question: Can you explain why dilation rates of 1, 2, 4, 8...1024 matter for detecting onsets at different temporal densities?

- Concept: **Transfer Learning: Feature Extraction vs. Fine-Tuning**
  - Why needed here: The paper systematically tests layer freezing configurations to balance preserving pre-trained features vs. adapting to new domains—a core transfer learning tension.
  - Quick check question: What determines whether freezing early layers vs. later layers will help or hurt adaptation?

- Concept: **Onset Detection Evaluation: F1 Score with Temporal Tolerance**
  - Why needed here: F1 with 25ms tolerance window is the standard metric, but the paper notes annotation precision issues (especially for mineiro) complicate evaluation—understanding metric limitations is critical.
  - Quick check question: Why might a 25ms tolerance window be inadequate for microtiming analysis applications?

## Architecture Onboarding

- Component map:
Audio Input → Log Mel Spectrogram
    ↓
Conv1 (3×3, 16 filters) → MaxPool → Dropout
Conv2 (3×3, 16 filters) → MaxPool → Dropout
Conv3 (1×8 or 3×3) → Dropout
    ↓
TCN Block: 11 Dilation Levels (1, 2, 4...1024)
Each level: Dilated Conv (kernel=5) + Residual
    ↓
Output Layer → Onset Activation Function (10ms resolution)
    ↓
Peak Picking → Onset Times

- Critical path:
  1. Obtain pre-trained TCN model (TCNv1 for onset, TCNv2 for beat)
  2. Extract 5-second annotated snippet per instrument
  3. Freeze layers according to configuration (ftConv1 through ftTcn1024, or ft for full)
  4. Fine-tune for 50 epochs with learning rate = 0.25 × original
  5. Evaluate on held-out files using F1@25ms

- Design tradeoffs:
  - TCNv1 (21,890 params) vs TCNv2 (116,302 params): Smaller model converges faster with less data; larger model has higher capacity but may overfit
  - Intra-task vs cross-task: Intra-task more reliable across all instruments; cross-task effective only when temporal structure aligns
  - Layer freezing depth: Deeper fine-tuning helps voicing instruments, hurts time-keeping instruments (overfitting risk)

- Failure signatures:
  - Low baseline + minimal improvement after fine-tuning → Acoustic characteristics too divergent from pre-training
  - High variance across layer configurations → Instrument may require hyperparameter search beyond freezing strategy
  - Cross-task transfer underperforms intra-task by >20 p.p. → Temporal alignment absent; use intra-task model instead
  - Note: mineiro shows anomalous patterns potentially due to annotation precision issues in ground truth

- First 3 experiments:
  1. **Baseline characterization**: Run both TCNv1 and TCNv2 on all 5 instruments without fine-tuning to establish per-instrument baseline F1 scores and identify which instruments have largest room for improvement.
  2. **Intra-task grid search**: For the worst-performing instrument (likely a time-keeping instrument), test all layer-freezing configurations systematically to identify optimal depth before expanding to other instruments.
  3. **Cross-task validation on time-keeping instruments**: Test beat-to-onset transfer specifically on cuica and gonge-lo, comparing against intra-task results to verify the structural alignment hypothesis holds for your specific audio samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed layer-wise fine-tuning strategy generalize effectively to other underrepresented musical traditions and instrument datasets?
- Basis in paper: [explicit] The authors state future work includes "extending the analysis to other datasets and underrepresented instruments."
- Why unresolved: The current study is limited to the specific acoustic characteristics of the five instruments within the Maracatu tradition.
- What evidence would resolve it: Replication of the methodology on distinct non-Western musical datasets demonstrating comparable data efficiency and performance gains.

### Open Question 2
- Question: Can specific hyperparameter optimizations yield significant performance improvements over the conservative training settings used in this study?
- Basis in paper: [explicit] The discussion notes that "greater improvements might be possible through hyperparameter optimization" as a "conservative approach" was maintained for fair baseline comparison.
- Why unresolved: The experiment fixed learning rates and epochs to ensure comparability, potentially leaving performance gains unrealized.
- What evidence would resolve it: Comparative experiments varying learning rates, batch sizes, and epoch counts specifically for the cross-task adaptation scenario.

### Open Question 3
- Question: How does the adaptive onset detection model perform under stricter temporal tolerance windows required for microtiming analysis?
- Basis in paper: [explicit] The conclusion suggests "Evaluating our adaptive approach using stricter tolerance windows would provide deeper insights into temporal precision."
- Why unresolved: The study relied on the standard 25 ms tolerance window, which may mask fine-grained timing errors critical for analyzing expressive instruments.
- What evidence would resolve it: Evaluation of F1 scores using tolerance windows tighter than the standard 25 ms (e.g., 10 ms).

## Limitations
- Extremely small fine-tuning dataset (5 seconds per instrument) may not generalize to longer or more variable performances
- Cross-task transfer mechanism remains partially theoretical without direct comparative evidence from other musical traditions
- Annotation quality issues, particularly for the mineiro instrument, introduce evaluation uncertainty
- Lack of hyperparameter optimization beyond layer freezing suggests results may be suboptimal
- Reliance on specific pre-trained TCN architectures means results may not transfer to other model families

## Confidence
- **High Confidence**: Intra-task transfer learning effectiveness (validated by substantial F1 improvements across all instruments)
- **Medium Confidence**: Cross-task transfer mechanism and structural alignment hypothesis (supported by data but lacks broader comparative evidence)
- **Medium Confidence**: Instrument-specific freezing strategies (data shows variation but doesn't explain underlying acoustic reasons)

## Next Checks
1. **Temporal Generalization Test**: Evaluate fine-tuned models on progressively longer audio segments (10s, 30s, full files) to verify 5-second adaptation scales to real-world usage.
2. **Cross-Cultural Transfer Validation**: Apply the same fine-tuning protocol to onset detection for another underrepresented tradition (e.g., Indian tabla or Indonesian gamelan) to test generalizability of the minimal-data approach.
3. **Ablation Study on Annotation Quality**: Systematically degrade the 5-second fine-tuning annotation quality (add noise, reduce precision) to determine minimum viable annotation standards for effective transfer.