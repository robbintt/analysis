---
ver: rpa2
title: 'Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW
  Defense and Million Scale Dataset'
arxiv_id: '2504.11707'
source_url: https://arxiv.org/abs/2504.11707
tags:
- nsfw
- image
- multimodal
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of preventing the generation of
  Not-Safe-For-Work (NSFW) content in AI image generation models, which are vulnerable
  to multimodal adversarial attacks that bypass existing text and image filters. To
  tackle this, the authors create a large-scale dataset (NSFWCorpus) containing over
  1 million text-image pairs, including both real and adversarial NSFW examples.
---

# Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset

## Quick Facts
- arXiv ID: 2504.11707
- Source URL: https://arxiv.org/abs/2504.11707
- Reference count: 19
- Primary result: Multimodal defense reduces multimodal attack success from ~80% to <4% while improving detection accuracy

## Executive Summary
This work addresses the vulnerability of AI image generation models to multimodal adversarial attacks that bypass existing text and image filters to produce Not-Safe-For-Work (NSFW) content. The authors propose a comprehensive solution combining a large-scale NSFWCorpus dataset containing over 1 million text-image pairs and a novel multimodal defense model. The defense leverages cross-attention between text and image embeddings to jointly classify content as safe or NSFW, significantly outperforming existing single-modality approaches. The research demonstrates that integrating both modalities with context-aware filtering provides robust protection against sophisticated attack strategies.

## Method Summary
The authors created the NSFWCorpus dataset by combining existing NSFW datasets with synthetically generated adversarial examples. They developed a multimodal defense model that extracts text and image embeddings from pre-trained encoders, then applies cross-attention mechanisms to capture contextual relationships between modalities. The model processes both modalities jointly rather than independently, allowing it to detect attacks that manipulate either text prompts or image inputs to bypass traditional filters. The defense is evaluated against various attack types including text-based, image-based, and multimodal adversarial attacks, with comprehensive testing on both synthetic attacks and human-generated prompts.

## Key Results
- Multimodal defense reduces Attack Success Rate (ASR) for multimodal attacks from ~80% to below 4% in most cases
- The approach improves accuracy, recall, and F1-score on human-generated prompts compared to single-modality defenses
- Cross-attention between text and image embeddings provides superior contextual understanding for NSFW detection
- The defense maintains effectiveness across different attack strategies while handling edge cases more robustly

## Why This Works (Mechanism)
The multimodal defense works by capturing contextual relationships between text prompts and generated images through cross-attention mechanisms. When text and image embeddings are processed jointly, the model can detect inconsistencies and manipulations that single-modality approaches miss. For example, text-based attacks that use coded language or image-based attacks that embed hidden NSFW content become detectable when both modalities are analyzed together. The cross-attention mechanism allows the model to weigh the importance of different features from each modality based on their relevance to NSFW classification, creating a more robust defense that adapts to various attack strategies.

## Foundational Learning

Cross-modal learning: Understanding how information from different modalities (text and images) can be integrated for better classification. Why needed: NSFW content often requires understanding both visual and textual context. Quick check: Verify that the model correctly identifies NSFW content when only one modality is available.

Adversarial attack detection: Techniques for identifying attempts to bypass content filters through manipulation. Why needed: Many existing defenses fail against sophisticated attacks that target specific vulnerabilities. Quick check: Test the model against known attack patterns to verify detection capability.

Cross-attention mechanisms: Neural network components that allow different modalities to interact and influence each other's representations. Why needed: Simple concatenation or averaging of embeddings misses important contextual relationships. Quick check: Compare performance with and without cross-attention to validate its contribution.

Text-image alignment: Ensuring that generated content matches the intended prompt and doesn't contain hidden or unexpected elements. Why needed: Many attacks exploit misalignment between text prompts and generated images. Quick check: Measure alignment scores between prompts and outputs for both clean and attacked examples.

Zero-shot classification: The ability to classify new types of content without specific training examples. Why needed: NSFW content evolves continuously, requiring models to generalize to new scenarios. Quick check: Test the model on novel NSFW content types not present in training data.

## Architecture Onboarding

Component map: Text encoder -> Image encoder -> Cross-attention module -> Classification layer -> Output (NSFW/Safe)

Critical path: The model processes text and image embeddings in parallel, applies cross-attention to create fused representations, then passes these through classification layers to determine NSFW probability. The cross-attention step is critical as it enables contextual understanding between modalities.

Design tradeoffs: The authors chose pre-trained encoders for efficiency rather than training from scratch, which limits fine-tuning capability but enables faster deployment. They also opted for a single multimodal model rather than separate models for each attack type, prioritizing unified defense over specialized performance.

Failure signatures: The model may struggle with highly abstract or culturally-specific NSFW content that requires nuanced understanding. It might also have difficulty with attacks that use sophisticated steganography or multiple layers of obfuscation across modalities.

First experiments: 1) Test the defense against single-modality attacks to establish baseline performance. 2) Evaluate multimodal attack resistance by measuring Attack Success Rate across different attack types. 3) Assess generalization by testing on human-generated prompts not present in the training data.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain for future investigation based on the limitations discussed, including real-world deployment challenges, long-term robustness against evolving attack strategies, and extension to additional modalities beyond text and image.

## Limitations

- Evaluation focuses on controlled adversarial attack scenarios using known attack types, which may not represent real-world attack diversity
- The NSFWCorpus dataset relies on synthetic adversarial prompts rather than naturally occurring user attempts to bypass filters
- Limited cross-model validation means generalizability to different T2I architectures beyond Stable Diffusion is uncertain

## Confidence

High confidence: Cross-attention between modalities provides superior defense based on controlled experiments
Medium confidence: Claim of being the first million-scale dataset for this task, as similar efforts may exist but weren't explicitly compared
Low confidence: Generalizability of the defense model to different T2I architectures due to limited cross-model validation

## Next Checks

1. Test the defense model against real-world user prompts collected from production systems to assess practical effectiveness
2. Evaluate the model's performance across multiple T2I architectures (e.g., DALL-E, Midjourney, Imagen) to verify architectural independence
3. Conduct a long-term robustness analysis by simulating evolving attack strategies over time to measure defense stability