---
ver: rpa2
title: 'MillStone: How Open-Minded Are LLMs?'
arxiv_id: '2509.11967'
source_url: https://arxiv.org/abs/2509.11967
tags:
- llms
- issues
- arguments
- position
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MILLSTONE, a benchmark to measure how large
  language models change their stances on controversial issues in response to external
  arguments. It applies MILLSTONE to nine leading LLMs, systematically varying argument
  configurations (no arguments, one-sided, clear-and-convincing, balanced) and measuring
  stance shifts.
---

# MillStone: How Open-Minded Are LLMs?
## Quick Facts
- arXiv ID: 2509.11967
- Source URL: https://arxiv.org/abs/2509.11967
- Reference count: 40
- Models respond to external arguments on controversial topics with measurable stance shifts

## Executive Summary
This paper introduces MILLSTONE, a benchmark designed to measure how large language models change their stances on controversial issues when presented with external arguments. The authors systematically test nine leading LLMs using varying argument configurations (no arguments, one-sided, clear-and-convincing, balanced) and measure stance shifts. The study reveals that all models demonstrate open-mindedness to external persuasion, with Grok 3 showing the highest responsiveness. The research highlights the importance of argument quality and source selection in influencing model outputs, raising important questions about the potential for manipulation in LLM-based information retrieval systems.

## Method Summary
The MILLSTONE benchmark employs a systematic methodology where each LLM is presented with a stance on a controversial topic followed by varying argument configurations. The authors use a random baseline approach where half the queries present the same stance as the original, and half present the opposite stance. For each topic and argument configuration, the LLM's response is compared to the baseline response using cosine distance between embedding vectors to measure stance shifts. The benchmark covers nine controversial topics and tests models across four argument conditions: no arguments, one-sided arguments, clear-and-convincing arguments, and balanced arguments from both sides.

## Key Results
- All nine tested LLMs showed measurable open-mindedness to external arguments
- Grok 3 demonstrated the highest open-mindedness with a 39.2% higher score than the median model
- Models generally shifted their stances toward the direction of presented arguments, though some showed counter-intuitive shifts
- The same arguments were often persuasive across different models, especially within the same family
- Claude Opus 4 uniquely refused to answer controversial topics unless arguments were provided

## Why This Works (Mechanism)
The paper doesn't provide a detailed mechanism for why LLMs exhibit open-mindedness to external arguments. The observed behavior appears to stem from the models' training on diverse internet text and their ability to process and incorporate new information during inference.

## Foundational Learning
**Language Model Basics**: Understanding how LLMs generate text based on probabilistic predictions is essential for interpreting stance shifts. Quick check: Can the model maintain coherent responses while incorporating new argumentative information.

**Embedding Spaces**: Knowledge of how models represent semantic similarity through vector embeddings is crucial for understanding the cosine distance metric used. Quick check: Do stance changes correlate with changes in embedding proximity to argument-related vectors.

**Fine-tuning Effects**: Awareness of how post-training fine-tuning affects model behavior helps explain differences between model families. Quick check: Do models from the same family show more similar open-mindedness patterns.

**Prompt Engineering**: Understanding how argument presentation format affects model responses is key to interpreting the results. Quick check: Does changing argument formatting alter the magnitude of stance shifts.

## Architecture Onboarding
**Component Map**: User Query -> Argument Injection -> Model Processing -> Stance Embedding Generation -> Cosine Distance Calculation -> Open-mindedness Score

**Critical Path**: The critical path involves presenting the controversial stance, injecting arguments, generating model response, computing embedding vectors, and measuring cosine distance to determine stance shift.

**Design Tradeoffs**: The benchmark trades comprehensiveness (testing many topics) for depth (detailed analysis of nine topics). Automated argument generation prioritizes scalability over argument quality and nuance.

**Failure Signatures**: Models that refuse to engage with controversial topics, models that show no stance change regardless of argument quality, or models that exhibit contradictory shifts when presented with balanced arguments.

**First Experiments**: 1) Test baseline stance consistency across multiple runs, 2) Compare human vs. automated argument persuasiveness, 3) Vary argument length to determine optimal persuasion threshold.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research.

## Limitations
- Benchmark focuses on only nine predefined controversial topics, limiting generalizability
- Automated argument generation may introduce framing biases that affect results
- Cosine distance metric may not capture nuanced changes in reasoning or contextual understanding

## Confidence
**Primary Finding Confidence**:
- All models are open-minded: High
- Grok 3 is most responsive: Medium
- Cross-model persuasion effects: Low

## Next Checks
1. Replicate the study using human-generated arguments instead of automated ones to assess whether persuasion patterns hold with more nuanced arguments.

2. Test the same models on a broader set of controversial topics (50+ topics spanning different domains) to validate whether open-mindedness patterns are robust.

3. Conduct ablation studies where arguments are presented in different formats (numbered lists, natural prose, bullet points) to determine whether persuasion effects are format-dependent.