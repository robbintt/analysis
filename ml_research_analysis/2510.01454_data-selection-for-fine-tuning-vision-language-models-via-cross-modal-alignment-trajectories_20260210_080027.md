---
ver: rpa2
title: Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment
  Trajectories
arxiv_id: '2510.01454'
source_url: https://arxiv.org/abs/2510.01454
tags:
- data
- target
- training
- alignment
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XMAS is the first principled data selection method for fine-tuning
  Large Vision-Language Models (LVLMs) that outperforms random selection. It clusters
  training examples based on cross-modal alignment trajectories extracted from a small
  proxy LVLM, then samples a balanced subset from these clusters to eliminate redundancy.
---

# Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories

## Quick Facts
- **arXiv ID:** 2510.01454
- **Source URL:** https://arxiv.org/abs/2510.01454
- **Reference count:** 40
- **Primary result:** Discards 50% of LLaVA-665k data while preserving full performance, achieving 30% more reduction than best baseline

## Executive Summary
XMAS is the first principled data selection method for fine-tuning Large Vision-Language Models (LVLMs) that outperforms random selection. It clusters training examples based on cross-modal alignment trajectories extracted from a small proxy LVLM, then samples a balanced subset from these clusters to eliminate redundancy. On LLaVA-665k, XMAS discards 50% of the data while fully preserving LLaVA-1.5-7B performance on 10 downstream benchmarks, achieving 30% more data reduction than the best baseline. On Vision-Flan, it can drop 85% of the data while maintaining full performance. The method speeds up training by 1.2x and provides theoretical convergence guarantees.

## Method Summary
XMAS extracts cross-modal attention trajectories from a small proxy LVLM (TinyLLaVA-2B) fine-tuned on full data, clusters examples by trajectory similarity, and selects a balanced subset by sampling stable examples from each cluster. The method computes top-5 singular values of cross-modal attention matrices across multiple checkpoints to capture alignment patterns, then uses K-means clustering to group examples with similar trajectories. For a given budget, it samples the most stable (lowest oscillation) examples from each cluster, ensuring both redundancy elimination and gradient-pattern coverage during target model fine-tuning.

## Key Results
- Achieves 50% data reduction on LLaVA-665k while maintaining full 95.4% ARP
- Outperforms best baseline (Passive Selection) by 30% more data reduction
- Achieves 85% data reduction on Vision-Flan while preserving full performance
- Speeds up training by 1.2x through reduced data volume

## Why This Works (Mechanism)

### Mechanism 1: Cross-modal attention trajectories as gradient proxies
XMAS leverages the theoretical result that pairwise gradient distances during instruction tuning can be upper-bounded by distances between cross-modal attention matrices. The method extracts the text-to-image attention block from all decoder layers, sums them, and computes top-5 singular values as alignment scores. Clustering examples based on these trajectory scores groups examples with similar gradient effects, enabling redundancy elimination.

### Mechanism 2: Trajectory clustering for temporal gradient similarity
Single-checkpoint attention similarity is insufficient since gradients evolve during training. By tracking top-5 singular values across T=7 checkpoints, XMAS captures how alignment patterns change over time. The method proves that under small Hessian assumptions (reasonable for fine-tuning), gradient distance at any intermediate point is bounded by the maximum attention distance at observed checkpoints.

### Mechanism 3: Instability-aware balanced sampling
Within each gradient-similar cluster, some examples are more representative than others. The instability score (sum of absolute differences in alignment scores across checkpoints) identifies stable examples—likely cluster centers rather than noisy outliers. Balanced sampling across clusters ensures gradient-pattern coverage, while instability-based selection picks the most representative examples per cluster.

## Foundational Learning

- **Concept: Attention mechanism and cross-modal attention**
  - **Why needed here:** Understanding Q-K-V attention is essential. Cross-modal attention specifically comes from the bottom-left block of attention matrices where text tokens attend to image tokens—this is what XMAS extracts.
  - **Quick check question:** Can you sketch how attention scores are computed and why the text-to-image block captures vision-language alignment?

- **Concept: Singular Value Decomposition (SVD) for dimensionality reduction**
  - **Why needed here:** XMAS uses top-5 singular values as compact alignment scores. SVD captures primary modes of variation, reducing noise while preserving the most important alignment information.
  - **Quick check question:** Why would top singular values of an attention matrix capture key alignment patterns better than using the full matrix?

- **Concept: Gradient similarity and redundancy in SGD**
  - **Why needed here:** The theory rests on the principle that examples with similar gradients have similar effects on model parameters during gradient descent—making them redundant for training.
  - **Quick check question:** During SGD, why would two examples with similar gradients be considered redundant?

## Architecture Onboarding

- **Component map:** Proxy trainer -> Attention extractor -> SVD computer -> Trajectory builder -> K-means clusterer -> Stability scorer -> Balanced sampler
- **Critical path:** Proxy training + trajectory extraction (~44% of total time at 50% budget) -> clustering -> target model training
- **Design tradeoffs:**
  - **Checkpoints (T):** T=7 optimal; T=1 saves time but drops ~2% ARP; T=11 increases cost without benefit
  - **Clusters (K):** K=1000 works best; K=500 loses granularity; K=2000 over-fragments
  - **Singular values:** Top-5 reduces SVD from 51.7ms to 1.77ms with only 0.2% ARP loss
  - **Proxy size:** 2B outperforms 0.5B; larger may help but increases overhead
- **Failure signatures:**
  - Large proxy-target architecture gaps reduce attention transferability
  - Already-non-redundant datasets yield poor gradient clustering
  - Budgets <10% may miss entire clusters
- **First 3 experiments:**
  1. **Proxy-target trajectory validation:** Fine-tune both models on small subset; verify per-cluster Euclidean distance < 0.25
  2. **Checkpoint ablation:** Test T ∈ {1, 3, 7, 11} on 10% LLaVA-665k; expect peak at T=7
  3. **Cluster size ablation:** Test K ∈ {500, 800, 1000, 2000} on 10% subset; expect peak near K=1000

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does increasing checkpoints beyond 7 degrade XMAS performance, and what determines the optimal number of checkpoints?
- **Basis in paper:** Ablation study shows 11 checkpoints reduce ARP to 94.1% from 95.4% at 7 checkpoints, but this counterintuitive finding is not explained.
- **Why unresolved:** The paper hypothesizes checkpoints should capture training dynamics, yet more temporal resolution hurts—suggesting noise accumulation or trajectory fragmentation effects are not understood.
- **What evidence would resolve it:** Analysis of trajectory cluster stability vs. checkpoint count; visualizations of how cluster assignments shift with additional checkpoints.

### Open Question 2
- **Question:** What is the minimum proxy model size required for XMAS to maintain effectiveness, and how does this scale with target model size?
- **Basis in paper:** The paper states "If the proxy model is small-enough, training the proxy... will be faster than instruction tuning on the full data" but only tests TinyLLaVA-0.5B and 2.0B with a 7B target.
- **Why unresolved:** Theoretical bounds depend on proxy-target attention distance (T), but the practical relationship between proxy capacity and transferability remains uncharacterized.
- **What evidence would resolve it:** Systematic evaluation across proxy sizes (0.1B to 7B) targeting various model scales (7B, 13B, 30B+).

### Open Question 3
- **Question:** Does XMAS generalize to LVLM architectures beyond LLaVA, particularly those with different attention mechanisms or projector designs?
- **Basis in paper:** All experiments use LLaVA-1.5 variants with identical architecture; no evaluation on alternative LVLMs (e.g., MiniGPT-4, InstructBLIP, Qwen-VL).
- **Why unresolved:** Cross-modal attention patterns may be architecture-dependent—the claim that "cross-modal attention matrices are transferrable" is only verified within the LLaVA family.
- **What evidence would resolve it:** Experiments applying XMAS to diverse LVLM architectures with different attention patterns and projection strategies.

### Open Question 4
- **Question:** What explains the large gap in data reduction potential between LLaVA-665k (50%) and Vision-Flan (85%), and can this be predicted a priori?
- **Basis in paper:** Paper reports dramatically different reduction rates achieving full performance but offers no analysis of dataset characteristics driving this difference.
- **Why unresolved:** Understanding this could guide data collection and indicate whether XMAS effectiveness depends on dataset composition, task diversity, or annotation quality.
- **What evidence would resolve it:** Analysis of cluster distributions, intra-cluster variance, and trajectory diversity metrics across datasets; controlled experiments varying dataset composition.

## Limitations
- The core theoretical claims depend heavily on the proxy-target attention similarity bound T, which is assumed but not empirically validated across the full training set
- The instability heuristic for selecting stable examples lacks theoretical grounding and may inadvertently filter out challenging but valuable examples
- The method's effectiveness for datasets with already-high redundancy is untested

## Confidence

- **High Confidence:** The data reduction results (50% on LLaVA-665k, 85% on Vision-Flan) and corresponding ARP maintenance are well-supported by Table 1a and Table 1b with clear statistical differences from baselines.
- **Medium Confidence:** The convergence guarantees (Theorem 4.1 and 4.2) are mathematically sound under stated assumptions, but the practical tightness of the bounds and the validity of assumptions in real fine-tuning scenarios require further validation.
- **Low Confidence:** The instability-based sampling heuristic lacks theoretical justification, and the assumption that stable trajectories indicate representative examples needs empirical verification across different dataset characteristics.

## Next Checks

1. **Proxy-target trajectory validation:** Fine-tune both proxy (TinyLLaVA-2B) and target (LLaVA-1.5-7B) models on a small 10% subset of LLaVA-665k. Compute per-cluster Euclidean distances between their trajectories and verify that distances are consistently below 0.25 (the claimed similarity threshold). This directly tests the transferability assumption.

2. **Dataset redundancy dependence:** Test XMAS on a dataset known to have high redundancy (e.g., LLaVA-1.0 with 9 examples per image) and compare performance to a diverse dataset. If XMAS shows minimal improvement on redundant data, this confirms the method's dependence on initial dataset diversity.

3. **Instability heuristic ablation:** Create three variants of XMAS: (a) instability-based sampling, (b) random sampling within clusters, and (c) selecting highest-instability examples. Run all three on LLaVA-665k with 50% budget and compare ARP scores. This would empirically validate whether stability selection provides meaningful benefits or if it's an unnecessary heuristic.