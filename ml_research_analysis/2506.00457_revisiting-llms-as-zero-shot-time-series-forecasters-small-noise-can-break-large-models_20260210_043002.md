---
ver: rpa2
title: 'Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break
  Large Models'
arxiv_id: '2506.00457'
source_url: https://arxiv.org/abs/2506.00457
tags:
- forecasting
- noise
- llms
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as effective zero-shot time-series forecasters. Experiments on real-world
  and synthetic datasets reveal that LLM-based forecasters are highly sensitive to
  noise, leading to significant performance degradation compared to state-of-the-art
  domain-specific models and even simple single-shot linear models.
---

# Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models

## Quick Facts
- **arXiv ID:** 2506.00457
- **Source URL:** https://arxiv.org/abs/2506.00457
- **Reference count:** 40
- **Primary result:** LLMs are highly sensitive to noise in time-series data, making them ineffective as zero-shot forecasters compared to simple linear models.

## Executive Summary
This paper investigates whether large language models (LLMs) can serve as effective zero-shot time-series forecasters. Experiments on real-world and synthetic datasets reveal that LLM-based forecasters are highly sensitive to noise, leading to significant performance degradation compared to state-of-the-art domain-specific models and even simple single-shot linear models. Despite exploring strategies like increasing input length and applying noise filtering, robustness remains limited. The study concludes that LLMs are not viable zero-shot forecasters for noisy time-series data and suggests focusing on fine-tuning LLMs for numerical sequence processing rather than emphasizing zero-shot approaches.

## Method Summary
The authors evaluated LLM-based time-series forecasting using zero-shot prompting on both synthetic (Function) and real-world (Monash, electrical, exchange rate, traffic, weather) datasets. They compared GPT-4, Qwen2.5-7B-Instruct, and Llama-3.2-1B-Instruct against single-shot linear models (DLinear-S, RLinear-S) and domain-specific models (TimeMixer, iTransformer). Experiments tested performance under varying noise levels (Gaussian noise), different input lengths, and with noise filtering (Gaussian smoothing, EMA). All evaluations used standard metrics (MAE, MSE) and focused on univariate forecasting.

## Key Results
- LLM performance degrades significantly with minimal noise (σ=0.001), while linear models remain robust
- Increasing input sequence length provides only marginal noise robustness improvement for LLMs
- Noise filtering (Gaussian smoothing, EMA) yields at best marginal improvements for LLM forecasters
- LLMs underperform simple single-shot linear models on noisy time-series data across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs amplify noise through token-based numerical encoding rather than reflecting actual numerical differences.
- Mechanism: Numerical values are converted to text tokens, which distort small variations in ways that obscure underlying patterns, making extrapolation harder when noise is present.
- Core assumption: Tokenization granularity mismatches numerical precision requirements for time-series patterns.
- Evidence anchors:
  - [Page 5] "A potential reason for LLMs' susceptibility to noise lies in their token-based encoding, which amplifies distortions in representation rather than reflecting actual numerical differences."
  - [Page 4, Figure 2] GPT-4 performs perfectly on clean Function data but degrades significantly with Gaussian noise at standard deviation 0.001 (within 0-1 input scale).
  - [corpus] Related work on noise injection for LLM forecasters (arXiv:2512.20140) suggests this sensitivity is a known limitation.
- Break condition: Noise levels below tokenization precision threshold, or fine-tuning that aligns token embeddings with numerical semantics.

### Mechanism 2
- Claim: Increasing input sequence length provides only marginal noise robustness improvement for LLMs, unlike linear models.
- Mechanism: LLMs cannot leverage additional context to disambiguate noise from signal, while linear models with L1/L2 loss statistically average out noise across more samples.
- Core assumption: In-context pattern recognition does not generalize to numerical noise filtering without explicit training.
- Evidence anchors:
  - [Page 4, Figure 3] LLMs show "only slight improvements" with more input periods, while DLinear-S gains significantly from longer sequences.
  - [Page 4] Linear models "exhibit greater robustness to noise... due to the statistical properties of their loss functions. The L1 loss function maintains a constant sum of losses under clean and noisy conditions."
  - [corpus] Corpus lacks direct mechanism validation; related papers focus on zero-shot transfer rather than noise robustness mechanisms.
- Break condition: Input sequences with noise structured in ways LLMs have seen during pre-training (untested).

### Mechanism 3
- Claim: Pre-processing noise filtering (Gaussian smoothing, EMA) provides at best marginal improvement for LLM forecasters.
- Mechanism: Filtering removes noise but also distorts the original dynamics; LLMs trained on natural text cannot compensate for the residual mismatch.
- Core assumption: The bottleneck is representation alignment, not just noise presence.
- Evidence anchors:
  - [Page 4, Table 2] After applying Gaussian and EMA filtering, "LLM performance improves only slightly or remains unchanged."
  - [Page 4] "Both methods struggle with non-stationary data, where patterns and noise characteristics change over time."
  - [corpus] No corpus papers directly address filtering as a mitigation strategy.
- Break condition: Fine-tuning on filtered numerical sequences to learn the mapping from smoothed inputs to true outputs.

## Foundational Learning

- Concept: **Zero-shot forecasting**
  - Why needed here: The paper evaluates whether pre-trained LLMs can forecast without domain-specific training—a core claim under scrutiny.
  - Quick check question: Can you explain why zero-shot forecasting requires no training data, and what tradeoffs this introduces?

- Concept: **Tokenization of numerical sequences**
  - Why needed here: The hypothesized mechanism for noise sensitivity stems from how numbers become tokens; understanding this is critical for diagnosing failures.
  - Quick check question: How does converting a floating-point number to text tokens differ from raw numerical embedding?

- Concept: **Noise robustness in regression models**
  - Why needed here: The paper contrasts LLMs with linear models; understanding why L1/L2 loss confers robustness clarifies the baseline comparison.
  - Quick check question: Why does L1 loss maintain consistent optimization behavior under Gaussian noise?

## Architecture Onboarding

- Component map:
  - Time-series input (length I) -> Optional noise filtering -> Tokenization to text representation -> LLM inference with forecasting prompt -> Output sequence extraction (length O) -> Evaluation via MAE/MSE

- Critical path:
  1. Time-series input (length I)
  2. Optional noise filtering
  3. Tokenization to text representation
  4. LLM inference with forecasting prompt
  5. Output sequence extraction (length O)
  6. Evaluation via MAE/MSE

- Design tradeoffs:
  - Zero-shot flexibility vs. accuracy: No training required, but underperforms simple baselines
  - Computational cost: LLM inference often slower than training+inference for linear models
  - Noise handling: No built-in robustness; filtering adds complexity with limited benefit

- Failure signatures:
  - Sudden accuracy collapse when noise is added to previously clean data
  - Marginal or no improvement from longer input contexts
  - Consistent underperformance vs. linear models trained on single sequences

- First 3 experiments:
  1. **Reproduce noise sensitivity**: Take Function dataset, add Gaussian noise at varying σ levels; confirm MAE spike as in Figure 2.
  2. **Compare input length scaling**: Run LLMTime and DLinear-S with increasing input lengths; verify that linear models improve while LLMs plateau.
  3. **Test noise filtering**: Apply Gaussian and EMA filtering to Monash dataset; confirm limited improvement per Table 2, then attempt a single-shot linear baseline on the same filtered data to isolate whether the bottleneck is filtering or LLM processing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms, such as tokenization strategies or embedding distortions, cause LLMs to be highly sensitive to noise in time-series data?
- Basis in paper: [explicit] The authors state in the Limitations section that "the exact mechanisms behind this issue, such as the impact of tokenization and encoding, deserve deeper investigation."
- Why unresolved: The paper empirically demonstrates that noise degrades performance but only hypothesizes that token-based encoding amplifies distortions rather than reflecting actual numerical differences.
- What evidence would resolve it: An ablation study comparing different numerical tokenization methods (e.g., character-level vs. scientific notation) on synthetic data with controlled noise injection to isolate the encoding bottleneck.

### Open Question 2
- Question: Can fine-tuning LLMs on numerical sequences effectively mitigate their sensitivity to noise, unlike zero-shot prompting?
- Basis in paper: [explicit] The study concludes that "a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences rather than emphasizing zero-shot approaches."
- Why unresolved: The experiments were strictly confined to zero-shot settings; the authors did not test whether updating model weights via fine-tuning resolves the identified robustness issues.
- What evidence would resolve it: A comparative analysis evaluating the performance of fine-tuned LLMs (e.g., using LoRA) against single-shot linear models on the same noisy datasets.

### Open Question 3
- Question: Does incorporating textual or multimodal context alongside numerical data enhance the robustness of LLM-based forecasters?
- Basis in paper: [explicit] The authors note in the Limitations that "the potential benefits of integrating textual or multimodal data could offer valuable insights for future studies."
- Why unresolved: The current evaluation protocol strips away context to treat time series purely as numerical sequences, potentially ignoring the reasoning capabilities LLMs might use to filter noise.
- What evidence would resolve it: Experiments using datasets with auxiliary metadata (e.g., the CiK benchmark) to test if textual context helps the model distinguish signal from noise.

## Limitations
- Experiments limited to univariate forecasting, leaving multivariate generalization untested
- Zero-shot setting only—fine-tuning potential remains unexplored
- Focus on English LLMs without examining specialized numerical tokenizers

## Confidence

**High Confidence (3 claims):**
1. LLMs underperform domain-specific and linear models on noisy time-series data
2. Zero-shot forecasting via LLMs remains significantly less accurate than fine-tuned alternatives
3. Noise filtering provides limited robustness improvement for LLM-based forecasting

**Medium Confidence (2 claims):**
1. Tokenization-induced representation distortion is the primary mechanism for noise sensitivity
2. In-context learning cannot effectively leverage longer sequences for noise filtering

**Low Confidence (1 claim):**
1. LLMs are fundamentally unsuitable for zero-shot time-series forecasting under any noise conditions

## Next Checks

1. **Tokenization Ablation**: Repeat the Function dataset experiments using models with different tokenization strategies (e.g., BPE vs. byte-level, or numerical tokenizers like NumbBERT) to isolate whether tokenization granularity is the primary source of noise sensitivity.

2. **Multivariate Extension**: Apply the same experimental protocol to multivariate time-series datasets (e.g., exchange rate data or weather patterns) to determine whether the observed limitations generalize beyond univariate forecasting.

3. **Fine-tuning Validation**: Fine-tune a small LLM on a synthetic noise-to-clean mapping task using filtered time-series data, then evaluate whether this approach bridges the performance gap with linear models, addressing whether the limitation is architectural or representational.