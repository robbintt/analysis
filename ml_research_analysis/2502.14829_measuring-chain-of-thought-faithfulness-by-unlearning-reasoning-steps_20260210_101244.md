---
ver: rpa2
title: Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps
arxiv_id: '2502.14829'
source_url: https://arxiv.org/abs/2502.14829
tags:
- reasoning
- unlearning
- steps
- faithfulness
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for measuring the faithfulness
  of language model chain-of-thought reasoning by intervening on model parameters
  rather than just context. The proposed Faithfulness by Unlearning Reasoning steps
  (FUR) method uses preference optimization-based unlearning to remove information
  from reasoning steps and measures how this affects model predictions.
---

# Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps

## Quick Facts
- **arXiv ID:** 2502.14829
- **Source URL:** https://arxiv.org/abs/2502.14829
- **Reference count:** 40
- **Primary result:** Introduces a parameter-level intervention framework to measure chain-of-thought faithfulness by unlearning reasoning steps and observing prediction changes

## Executive Summary
This paper presents a novel framework for evaluating the faithfulness of chain-of-thought (CoT) reasoning in language models by intervening directly on model parameters rather than just context. The proposed Faithfulness by Unlearning Reasoning steps (FUR) method uses preference optimization-based unlearning to remove information from reasoning steps and measures how this affects model predictions. Experiments across four language models and five multi-choice question answering datasets demonstrate that unlearning key reasoning steps frequently changes model predictions, indicating parametrically faithful reasoning. The work highlights the distinction between parametric and contextual faithfulness in CoTs and provides a method for detecting when language models genuinely rely on their verbalized reasoning.

## Method Summary
The FUR framework measures chain-of-thought faithfulness by intervening on model parameters rather than context. It uses preference optimization-based unlearning to selectively remove information from individual reasoning steps, then measures how these interventions affect model predictions. The method treats CoT steps as latent knowledge to be unlearned, applying preference optimization techniques to suppress the model's reliance on specific reasoning components. By comparing predictions before and after unlearning each step, the framework quantifies how parametrically faithful the CoT is - whether the model genuinely relies on the reasoning it verbalizes rather than accessing alternative reasoning pathways.

## Key Results
- Unlearning key reasoning steps frequently changes model predictions across four language models and five multi-choice QA datasets, demonstrating parametric faithfulness
- The approach achieves high efficacy in unlearning while maintaining model specificity and general capabilities
- FUR identifies steps that are parametrically important but not considered plausible by humans (Pearson correlation of 0.15 between faithfulness and human supportiveness ratings)
- Models verbalize different reasoning when key steps are unlearned, supporting different answers than the original prediction

## Why This Works (Mechanism)
The framework works by directly intervening on the model's internal representations of reasoning steps rather than manipulating input context. By using preference optimization to unlearn specific reasoning components, FUR can determine whether the model truly depends on the verbalized chain of thought or accesses alternative reasoning pathways. This parameter-level intervention provides a more fundamental assessment of faithfulness compared to context-only methods, revealing whether the model's predictions are grounded in the reasoning it produces or in other latent processes.

## Foundational Learning
- **Preference optimization for unlearning**: A technique for suppressing specific knowledge in models by fine-tuning on preference data that distinguishes between desired and undesired outputs. Why needed: Enables selective removal of reasoning step information without catastrophic forgetting. Quick check: Verify unlearning effectiveness by measuring prediction changes on related tasks.
- **Parametric faithfulness**: The extent to which a model's predictions depend on parameters representing specific reasoning steps rather than alternative pathways. Why needed: Distinguishes between models that genuinely use their verbalized reasoning versus those that rely on hidden processes. Quick check: Compare prediction stability before and after step unlearning.
- **Contextual vs parametric faithfulness**: Two distinct dimensions of reasoning evaluation - whether reasoning is present in context versus whether the model actually relies on it parametrically. Why needed: Explains why CoTs can be plausible but unfaithful, or faithful but implausible. Quick check: Measure both metrics separately on the same reasoning examples.

## Architecture Onboarding

**Component Map**: Input prompt -> CoT generation -> Step extraction -> Individual step unlearning (via preference optimization) -> Prediction evaluation -> Faithfulness scoring

**Critical Path**: The most time-consuming component is the iterative unlearning process, where each reasoning step requires a full preference optimization fine-tuning cycle. This creates computational bottlenecks when dealing with long reasoning chains spanning thousands of tokens.

**Design Tradeoffs**: The framework trades computational efficiency for measurement fidelity. While context-level interventions are faster, parameter-level unlearning provides more fundamental insights into model reasoning. The preference optimization approach balances between complete forgetting and maintaining general capabilities.

**Failure Signatures**: 
- Low faithfulness scores despite human-verified reasoning indicate potential issues with the unlearning process or model architecture
- High computational costs for long reasoning chains suggest the need for selective intervention strategies
- Disagreements between parametric and contextual faithfulness reveal limitations in current evaluation approaches

**First Experiments**:
1. Apply FUR to a small language model with a known faithful reasoning task to establish baseline performance
2. Compare FUR results with traditional context-only faithfulness metrics on the same dataset
3. Test the framework's sensitivity by introducing controlled unfaithfulness into otherwise faithful reasoning chains

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can language models be aligned so that reasoning steps identified as parametrically faithful by unlearning methods are also considered plausible and supportive by humans?
- **Basis in paper:** The human study in Section 6.4 reveals a weak Pearson correlation of 0.15 between the faithfulness metric FF-SOFT and human ratings of supportiveness, leading the authors to explicitly state a need for "specialized alignment to obtain CoTs that are both plausible and faithful."
- **Why unresolved:** Current models appear to rely on internal heuristics or reasoning pathways that humans do not find intuitive or supportive, creating a disconnect between what the model relies on (faithfulness) and what satisfies a human reader (plausibility).
- **What evidence would resolve it:** The development of a training objective that jointly optimizes for high scores on the FUR metric (faithfulness) and high human approval ratings (plausibility) without degrading task performance.

### Open Question 2
- **Question:** How can the computational complexity of parametric faithfulness frameworks be reduced to efficiently handle reasoning chains spanning thousands of tokens?
- **Basis in paper:** In Section 7, the authors note that applying FUR to models with long reasoning chains (like o1 or DeepSeek R1) is complex because "intervening on each CoT step can be time-consuming" and fully fine-tuning layers involves updating many parameters.
- **Why unresolved:** The current FUR implementation requires unlearning individual steps, which becomes computationally prohibitive as the number of reasoning steps grows significantly.
- **What evidence would resolve it:** A proposed method that utilizes "thought anchors" or verifiers to selectively target only the most salient reasoning steps for intervention, rather than processing every step individually.

### Open Question 3
- **Question:** Can parametric faithfulness metrics be adapted to distinguish between genuinely unfaithful reasoning and the existence of multiple redundant internal reasoning paths?
- **Basis in paper:** In Section 4.3, the authors acknowledge that FUR represents a lower bound because "it is possible that a faithful explanation, even when unlearned from model parameters, will not tangibly affect the models' prediction" due to alternative explanations or reasoning paths.
- **Why unresolved:** The current methodology cannot differentiate between a model ignoring the reasoning chain (unfaithfulness) and a model simply utilizing a second, redundant valid reasoning path that was not unlearned.
- **What evidence would resolve it:** An extension of the framework that maps or isolates multiple internal reasoning circuits to verify if the verbalized chain is one of several independent paths used to reach the conclusion.

## Limitations
- Computational complexity increases significantly with reasoning chain length, making the method impractical for models with extensive CoTs
- The framework may underestimate faithfulness when models have multiple redundant reasoning paths that can compensate for unlearned steps
- Human evaluation of reasoning plausibility shows weak correlation with parametric faithfulness, suggesting potential misalignment between model behavior and human expectations

## Confidence
- **Detecting faithful reasoning**: High confidence - empirical results consistently show prediction changes after unlearning key steps
- **Generalizability to other tasks**: Medium confidence - limited evaluation to five multi-choice QA datasets
- **Computational feasibility**: Low confidence - significant resource requirements not fully characterized

## Next Checks
1. Test FUR on open-ended reasoning tasks beyond multi-choice questions to evaluate generalizability
2. Conduct ablation studies to determine which components of the unlearning process are essential for faithfulness detection
3. Compare FUR's faithfulness measurements against alternative evaluation frameworks to establish relative effectiveness and identify potential biases