---
ver: rpa2
title: Tropical Bisectors and Carlini-Wagner Attacks
arxiv_id: '2503.22653'
source_url: https://arxiv.org/abs/2503.22653
tags:
- tropical
- layer
- then
- attacks
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the robustness of tropical convolutional neural
  networks against Carlini-Wagner attacks. The authors first prove an upper bound
  on the number of linear segments in tropical bisectors, which form the decision
  boundary of these networks.
---

# Tropical Bisectors and Carlini-Wagner Attacks

## Quick Facts
- arXiv ID: 2503.22653
- Source URL: https://arxiv.org/abs/2503.22653
- Reference count: 16
- Primary result: Modified Carlini-Wagner attack improves success rates from 61% to 72% for tropical networks

## Executive Summary
This paper analyzes the robustness of tropical convolutional neural networks (CNNs) against Carlini-Wagner (CW) attacks. The authors prove an upper bound on the number of linear segments in tropical bisectors, which form the decision boundary of these networks. They demonstrate that the non-differentiability of tropical layers creates sparse gradients that cause gradient descent to oscillate, hindering successful attacks. To address this, they propose a modified CW attack that penalizes all coordinates exceeding a threshold, not just the maximum. Computational experiments with MNIST using LeNet5 show that the modified attack improves success rates from 61% to 72% for tropical networks, compared to only slight improvements for linear networks.

## Method Summary
The paper combines theoretical analysis with computational experiments to study tropical CNN robustness. The method involves: (1) proving an upper bound on linear segments in tropical bisectors (C(b) ≤ 4·((d+1)/4) + 2·((d+1)/3) + 2·((d)/2) + 1), (2) demonstrating that tropical distance computation via max/min operations creates sparse gradients with only 4 non-zero entries, leading to oscillation during gradient descent, and (3) developing a modified CW attack that replaces max(z) with Σ(zi - τ)+ and min(z) with Σ(zi + τ)− in tropical distance computation. The computational experiments use MNIST with LeNet5 and ModifiedLeNet5 architectures, comparing standard vs. modified CW attacks with and without multiple starting points.

## Key Results
- Proved upper bound on linear segments in tropical bisectors: C(b) ≤ 4·((d+1)/4) + 2·((d+1)/3) + 2·((d)/2) + 1
- Standard CW attacks achieve only 61% success rate on tropical networks vs 97% on linear networks
- Modified CW attack improves success rates to 72% for tropical networks (vs 63% for linear)
- Multiple starting points further improve success to 98% for modified tropical attack

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The piecewise linear structure of tropical decision boundaries limits attack surface through bounded combinatorial complexity.
- Mechanism: The paper proves an upper bound on linear segments in tropical bisectors (C(b) ≤ 4·((d+1)/4) + 2·((d+1)/3) + 2·((d)/2) + 1), constraining decision boundary geometry.
- Core assumption: That fewer linear segments translates to fewer "easy" crossing points for adversarial perturbations (paper proves the bound but does not directly prove reduced attack surface).
- Evidence anchors: [abstract] "We prove an upper bound on the number of linear segments the decision boundary of a tropical CNN can have." [section] Theorem 3.8 provides explicit combinatorial bound; computational experiments (Table 1) confirm bound is tight for sampled generic points. [corpus] Weak direct evidence; corpus papers discuss adversarial robustness without explicit boundary segment analysis.
- Break condition: If attacks can enumerate all linear segments and target them directly, boundary complexity no longer impedes optimization.

### Mechanism 2
- Claim: Non-differentiability creates sparse gradients (only 4 non-zero entries), causing gradient descent to oscillate rather than converge.
- Mechanism: Tropical distance involves max/min operations; gradients depend only on coordinates achieving these values. When descent reduces one coordinate, ignored coordinates can increase, inducing oscillation—illustrated with h(x) = (x₁² + 2x₂, x₁² - 2.2x₂) example (Figure 7).
- Core assumption: Assumption: oscillation is the primary cause of attack failure on tropical networks.
- Evidence anchors: [abstract] "non-differentiability of tropical layers creates sparse gradients that can cause gradient descent to oscillate, hindering successful attacks" [section] Page 17-18: "gradient... depends on a relatively low proportion of input coordinates. This may produce oscillatory effects and obstructs efficient optimization." [corpus] No corpus papers analyze tropical gradient sparsity explicitly.
- Break condition: If an attack penalizes multiple coordinates simultaneously (not just max/min), oscillation should diminish.

### Mechanism 3
- Claim: Modified Carlini-Wagner attack recovers success by penalizing all coordinates exceeding a threshold τ, not just the maximum.
- Mechanism: Replace max(z) with Σ(zi - τ)+ and min(z) with Σ(zi + τ)− in tropical distance computation. This controls multiple coordinates per step, reducing oscillation and improving convergence.
- Core assumption: Assumption: τ can be tuned appropriately (initialized as 7th highest value in |x - wc|, decreased by 0.9 when all entries fall below threshold).
- Evidence anchors: [abstract] "modified Carlini-Wagner attack... improves success rates from 61% to 72% for tropical networks" [section] Equation 42-43 define modified distance; Table 2 shows improvement from 61% → 72% (LeNet5) and 42% → 63% (ModifiedLeNet5). [corpus] Carlini-Wagner (2017) foundational but corpus lacks tropical adaptations.
- Break condition: If τ is too low, too many coordinates penalized (weak signal); if too high, degenerates to standard attack.

## Foundational Learning

- Concept: Tropical semiring (max-plus algebra)
  - Why needed here: Tropical layer uses max/min instead of standard arithmetic; essential for computing tropical distances and gradients.
  - Quick check question: What is tropical sum a ⊕ b and product a ⊙ b for a = 3, b = 5?

- Concept: Tropical distance metric
  - Why needed here: Core non-differentiable operation in tropical embedding layer: dtr(x, y) = max(xi - yi) - min(xi - yi).
  - Quick check question: Compute dtr for x = (1, 3, 2), y = (0, 2, 4).

- Concept: Gradient sparsity from max/min
  - Why needed here: Understanding why max/min gradients are sparse (only depend on extremal coordinates) explains oscillation mechanism.
  - Quick check question: For f(x) = max(x₁, x₂, x₃), what is ∇f at (2, 2, 1)?

## Architecture Onboarding

- Component map: Input → Conv/Pool layers → Flatten → Tropical Embedding Layer (zⱼ = dtr(-wⱼ, x)) → Softmin → Class
- Critical path:
  1. Build standard CNN base (LeNet5/ModifiedLeNet5, accuracy >99% on MNIST)
  2. Replace final layer with tropical embedding layer
  3. Apply softmin classification: argmaxⱼ e^(-dtr(-wⱼ,x)) / Σₖ e^(-dtr(-wₖ,x))
- Design tradeoffs:
  - Robustness vs. attackability: tropical 61% vs. linear 97% under standard CW
  - Modified attack recovers 72% (still below linear), showing partial robustness remains
  - MSP + modified descent reaches 98% (LeNet5) but requires architecture-specific adaptation
- Failure signatures:
  - Standard CW oscillates near boundary (Figure 7)
  - Sparse gradient (4 non-zero entries) causes back-and-forth without convergence
  - Attack appears to make local progress but fails globally
- First 3 experiments:
  1. Replicate LeNet5 + tropical layer on MNIST; verify >99% accuracy and standard CW ~61% success
  2. Implement modified gradient descent with threshold τ; tune initialization (7th highest value) and observe 61% → 72% improvement
  3. Add multiple starting points (10 random, MSP) to modified descent; target 98% success per Table 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the upper bound on the number of linear segments in tropical bisectors tight for all dimensions $d$?
- Basis in paper: [explicit] The authors state, "Based on computational results... we conjecture that the bound in Theorem 3.8 is tight."
- Why unresolved: While computational experiments achieve the bound for vectors sampled in lower dimensions, a general mathematical proof or a counter-example for arbitrary dimensions is not provided.
- What evidence would resolve it: A formal proof establishing the tightness of the formula $C(b) \leq \frac{d^4}{6} + \frac{5d^2}{6} - d + 1$ or an example where the maximum is strictly lower.

### Open Question 2
- Question: Do the robustness properties of tropical CNNs and the efficacy of the modified attack scale to high-dimensional datasets like CIFAR-10 or ImageNet?
- Basis in paper: [inferred] All computational experiments are restricted to the MNIST dataset using LeNet5, leaving the behavior on more complex, higher-dimensional data untested.
- Why unresolved: The dynamics of gradient oscillation and the geometric structure of the decision boundary may differ significantly in more complex data domains, potentially altering attack success rates.
- What evidence would resolve it: Experimental results applying the modified Carlini-Wagner attack to tropical CNNs trained on datasets with higher input dimensionality and class complexity.

### Open Question 3
- Question: Can tropical CNNs be adapted to defend against the modified gradient descent attack proposed in this work?
- Basis in paper: [inferred] The results show the modified attack achieves up to 98% success on LeNet5, effectively breaking the tropical defense, but no remedial defensive measures are investigated.
- Why unresolved: The paper focuses on the theoretical vulnerability and successful attack methodology rather than exploring iterative defense mechanisms like adversarial training.
- What evidence would resolve it: Experiments combining tropical layers with adversarial training specifically using the modified gradient descent method to see if robustness can be restored.

## Limitations

- Theoretical gap between boundary complexity bound and actual attack difficulty remains unproven
- Modified attack's hyperparameter sensitivity (τ initialization and decay) not theoretically justified
- All computational experiments limited to MNIST dataset with small network architectures

## Confidence

- **High confidence**: The combinatorial bound on tropical bisector segments (Theorem 3.8) and its computational verification across sampled points.
- **Medium confidence**: The modified attack's improved success rates (61% → 72%) under specified conditions, given reproducible implementation.
- **Low confidence**: The claim that tropical boundary complexity inherently provides robustness beyond the specific oscillation mechanism observed.

## Next Checks

1. **Boundary Enumeration Test**: Implement an algorithm to explicitly enumerate all linear segments of tropical bisectors for small dimensions and verify whether attacks targeting these segments can achieve higher success rates than standard gradient descent.

2. **Threshold Sensitivity Analysis**: Systematically vary τ initialization and decay parameters across a wider range to determine whether the 72% success rate is robust to hyperparameter choices or specific to the reported configuration.

3. **Architecture Transfer Test**: Apply the modified attack to tropical networks trained on datasets beyond MNIST (e.g., Fashion-MNIST or CIFAR-10) to verify whether the 61% → 72% improvement generalizes across different data distributions and network architectures.