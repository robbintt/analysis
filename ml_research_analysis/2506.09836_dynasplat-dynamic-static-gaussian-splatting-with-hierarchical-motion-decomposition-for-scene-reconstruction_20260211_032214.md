---
ver: rpa2
title: 'DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition
  for Scene Reconstruction'
arxiv_id: '2506.09836'
source_url: https://arxiv.org/abs/2506.09836
tags:
- dynamic
- motion
- gaussian
- gaussians
- opacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DynaSplat tackles dynamic scene reconstruction by integrating dynamic-static
  separation, hierarchical motion modeling, and physically-based opacity computation
  into Gaussian Splatting. The dynamic-static separation classifies scene elements
  by combining deformation offset statistics with 2D motion flow consistency, allowing
  focused modeling where motion occurs.
---

# DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction

## Quick Facts
- arXiv ID: 2506.09836
- Source URL: https://arxiv.org/abs/2506.09836
- Reference count: 39
- Key outcome: Outperforms state-of-the-art on dynamic scene reconstruction, achieving 34.39 PSNR on D-NeRF and 31.68 PSNR on N3DV

## Executive Summary
DynaSplat addresses dynamic scene reconstruction by integrating dynamic-static separation, hierarchical motion modeling, and physically-based opacity computation into Gaussian Splatting. The method classifies scene elements into static and dynamic sets using a fusion of 3D deformation offset statistics and 2D motion flow consistency, enabling focused modeling where motion occurs. Hierarchical motion modeling decomposes motion into coarse global and fine local components, while physically-motivated opacity improves depth ordering and occlusion handling. Extensive experiments show DynaSplat surpasses state-of-the-art methods on D-NeRF and N3DV datasets and demonstrates robustness in specialized applications like dynamic head avatar reconstruction.

## Method Summary
DynaSplat extends 3D Gaussian Splatting by introducing dynamic-static separation, hierarchical motion modeling, and physically-based opacity computation. The method trains a unified DeformNet for initial optimization, then splits the canonical Gaussian set into static and dynamic subsets based on deformation offset variance and 2D motion flow consistency. Static Gaussians are modeled with a lightweight 3-layer MLP, while dynamic Gaussians use Adaptive Motion Networks with four blended motion modes. Motion is decomposed hierarchically into coarse neighbor-averaged components and fine MLP-refined residuals. Physically-motivated opacity computation based on camera distance and viewing angle improves occlusion handling, with temporal importance filtering pruning low-contribution Gaussians.

## Key Results
- Achieves 34.39 PSNR on D-NeRF dataset (vs. 33.12 without dynamic-static separation)
- Achieves 31.68 PSNR on N3DV dataset (vs. 31.00 without dynamic-static separation)
- Demonstrates 33.24 PSNR and 0.0281 LPIPS on dynamic head avatar reconstruction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining 3D deformation offset statistics with 2D motion flow consistency yields more reliable dynamic-static classification than either signal alone.
- Mechanism: Computes per-Gaussian position offset variance across frames (Var(Δµ)), identifying candidates where variance exceeds threshold τ. These candidates are verified by projecting positions onto 2D motion flow maps and checking if they fall within regions of significant motion (>50% of frames). The intersection of both criteria defines the final dynamic set.
- Core assumption: Dynamic elements exhibit consistent motion signatures across both 3D geometric deformation and 2D image-space flow; static elements show neither.
- Evidence anchors:
  - [abstract] "classifies scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency"
  - [section III-B] Equations 6-12 detail the variance threshold (τ=0.01), flow magnitude threshold (ϵ=1.0), and confirmation ratio (γ=0.5)
  - [corpus] SDD-4DGS independently validates static-dynamic decoupling improves 4D reconstruction, though using different separation criteria
- Break condition: Scenes with camera-only motion or severe occlusions causing flow estimation failures may misclassify Gaussians.

### Mechanism 2
- Claim: Hierarchical coarse-to-fine decomposition captures both global rigid transformations and local non-rigid deformations more effectively than single-level motion models.
- Mechanism: For dynamic Gaussians, coarse motion is extracted by averaging deformation parameters (Δµ, ΔR, ΔS) over spatial neighbors. A separate MLP then predicts fine residuals conditioned on this coarse estimate and a learnable feature vector. The final motion is the sum: ΔX = ΔX_coarse + ΔX_fine. Additionally, Adaptive Motion Networks blend M=4 motion modes via learned parameters {βm}.
- Core assumption: Neighboring Gaussians on the same object share similar global motion; local deformations are residual deviations from this shared baseline.
- Evidence anchors:
  - [abstract] "hierarchical motion modeling decomposes motion into coarse global and fine local components"
  - [section III-C, Equation 13] "This formulation enables the network to capture complex dynamic behaviors through a flexible combination of different motion modes"
  - [corpus] HEIR (graph-based motion hierarchies) provides external validation that hierarchical motion decomposition improves complex dynamics modeling
- Break condition: Objects with highly independent part motions may violate the neighbor-averaging assumption if neighbors span different motion groups.

### Mechanism 3
- Claim: Physically-motivated opacity computation based on camera distance and viewing angle improves depth ordering and occlusion handling.
- Mechanism: Opacity αg = α0 · cos(θg) / ||Pg - Pcamera||², where θg is the angle between Gaussian normal and viewing direction. This makes opacity decay with squared distance and diminish at grazing angles. Temporal Importance Filtering (Equation 15) prunes Gaussians whose maximum contribution across all views falls below threshold τ=0.02.
- Core assumption: Real surfaces become less visually prominent with distance and at oblique viewing angles; floaters exhibit uniformly low importance across temporal views.
- Evidence anchors:
  - [section III-D, Equation 14] Explicit opacity formula with physical motivation
  - [section IV-D, ablation] Removing this component drops PSNR from 34.39→33.65 (D-NeRF) and 31.68→31.00 (N3DV)
  - [corpus] No direct external validation found; physically-based opacity is relatively novel in Gaussian Splatting
- Break condition: Highly reflective or emissive materials that don't follow inverse-square falloff may render incorrectly.

## Foundational Learning

- Concept: 3D Gaussian Splatting fundamentals (primitive representation, differentiable rasterization, alpha blending)
  - Why needed here: DynaSplat extends 3DGS; without understanding base primitives (mean, covariance, opacity, SH colors), the deformation and opacity modifications won't make sense.
  - Quick check question: Given Equation 2's alpha blending formula, what happens to pixel color if all Gaussian opacities are low (σi → 0)?

- Concept: Deformation field / canonical space representation
  - Why needed here: The method uses a canonical Gaussian set plus time-conditioned offsets; understanding this disentanglement is essential for grasping dynamic-static separation.
  - Quick check question: In Equations 3-5, what does Δµt represent geometrically, and why compute it per-timestamp rather than storing independent Gaussians per frame?

- Concept: Optical flow and its limitations
  - Why needed here: 2D motion flow consistency is the verification signal for dynamic classification; knowing flow's failure modes (occlusions, textureless regions) helps predict where separation may break.
  - Quick check question: Why might optical flow estimation fail at object boundaries, and how could this affect the dynamic-static separation verification step?

## Architecture Onboarding

- Component map:
  - Multi-view video + camera poses → COLMAP point cloud initialization
  - → Canonical Gaussians (static set G_static + dynamic set G_dynamic post-separation)
  - → DeformNet (lightweight 3-layer MLP for static; Adaptive Motion Networks for dynamic)
  - → Hierarchical motion (neighbor averaging for coarse → MLP refinement for fine → summed output)
  - → Opacity module (physical formula + Temporal Importance Filtering)
  - → Rendering (standard 3DGS differentiable splatting with modified opacity)

- Critical path:
  1. Train unified DeformNet for 3k iterations (no separation)
  2. At 6k iterations: compute offset variance, verify with motion flow, split G_static / G_dynamic
  3. Static Gaussians → freeze, use lightweight MLP; Dynamic → hierarchical modeling with Adaptive Motion Networks
  4. At 20k iterations: enable Temporal Importance Filtering
  5. Train to 150k iterations total (~2 hours on RTX 3090)

- Design tradeoffs:
  - **Separation timing**: Earlier separation (before 6k) risks misclassification; later delays focused dynamic modeling. Paper's 6k is empirically chosen.
  - **Number of motion modes (M=4)**: More modes increase capacity but risk overfitting; fewer modes may underfit complex motions.
  - **Neighbor definition for coarse motion**: Paper doesn't specify (likely k-nearest based on Gaussian positions); too large a neighborhood smooths away meaningful local motion.

- Failure signatures:
  - **Static elements flickering**: Dynamic-static separation misclassified them; check flow quality at those regions.
  - **Floaters persisting**: Temporal Importance Filtering threshold (τ=0.02) may be too low; increase pruning aggressiveness.
  - **Blurry dynamic regions**: Hierarchical motion underfitting; check if M=4 modes are being used (βm distribution) or if MLP capacity is bottleneck.

- First 3 experiments:
  1. **Reproduce ablation on single D-NeRF scene**: Train full model, then retrain with dynamic-static separation disabled. Verify ~1.3 PSNR drop (33.12 vs 34.39) to confirm implementation.
  2. **Threshold sensitivity sweep**: Vary τ ∈ {0.005, 0.01, 0.02} and γ ∈ {0.3, 0.5, 0.7} on a scene with known ground-truth dynamic regions. Measure classification F1 against manual labels.
  3. **Neighbor count ablation for coarse motion**: Test k ∈ {4, 8, 16, 32} for neighbor averaging. Expect small k to overfit noise, large k to over-smooth; identify optimal range.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dynamic-static separation mechanism be extended to distinguish and model multiple independently interacting dynamic objects?
- **Basis in paper:** [explicit] The conclusion states: "future work could explore handling multiple interacting dynamic objects..."
- **Why unresolved:** The current method classifies Gaussians into a binary set $G_{dynamic}$ (Eq. 12) without segmenting individual instances, potentially conflating separate moving entities into a single deformation field.
- **Evidence to resolve:** Demonstration of the method successfully reconstructing scenes where multiple objects occlude one another or move in opposing directions without motion artifacts or blending.

### Open Question 2
- **Question:** How can explicit physics-based constraints be integrated into the hierarchical motion modeling to ensure physically plausible deformations?
- **Basis in paper:** [explicit] The conclusion identifies "physics-based constraints" as a specific avenue for future work.
- **Why unresolved:** The current motion model (Eq. 13) relies on learned MLPs and blending parameters, which may hallucinate motions that are visually plausible but geometrically or physically impossible (e.g., incorrect mass distribution).
- **Evidence to resolve:** A modified loss function or architecture that enforces constraints (e.g., elasticity, inertia) resulting in accurate motion extrapolation under unseen forces.

### Open Question 3
- **Question:** How robust is the dynamic-static classification when the initial off-the-shelf optical flow estimation fails due to textureless regions or lighting changes?
- **Basis in paper:** [inferred] Section III.B.c notes that motion flow maps are obtained using an "off-the-shelf method," and Section III.B.b relies on these maps for the consistency check $F_g$.
- **Why unresolved:** The classification of dynamic Gaussians depends on 2D flow consistency (Eq. 9); however, the paper does not analyze how noise or errors in the optical flow estimation propagate to the final dynamic mask.
- **Evidence to resolve:** An ablation study or quantitative analysis showing classification accuracy and reconstruction quality when using noisy or lower-quality flow estimators as input.

## Limitations

- **Architecture specification gaps**: Key hyperparameters like MLP layer sizes and neighbor count for coarse motion are unspecified, preventing exact reproduction
- **Empirical timing choices**: Dynamic-static separation at 6k and TIF at 20k iterations appear empirically tuned without ablation justification
- **Limited external validation**: Physical opacity computation lacks direct comparison to alternative approaches, making it unclear if gains are model-specific or dataset-dependent

## Confidence

- **Dynamic-static separation PSNR gains**: Medium confidence - gains depend on precise threshold tuning (τ=0.01, γ=0.5) that may not generalize
- **Hierarchical motion modeling**: Medium confidence - architectural details like neighbor definition and MLP capacity are unspecified
- **Physical opacity computation**: Low confidence - no direct external validation of this novel approach

## Next Checks

1. **Separation threshold sensitivity**: Run the dynamic-static classification on 3 scenes with τ ∈ {0.005, 0.01, 0.02} and γ ∈ {0.3, 0.5, 0.7}. Compare classification F1 scores against manual ground truth and observe PSNR impact.

2. **Motion mode capacity ablation**: Train models with M=2, M=4 (reported), and M=8 motion modes. Track PSNR, training stability, and motion blending parameter distributions to identify overfitting vs. underfitting regimes.

3. **Neighbor averaging radius study**: Systematically vary the neighborhood size for coarse motion computation (k ∈ {4, 8, 16, 32} Gaussians). Measure PSNR impact and visually inspect motion smoothness vs. detail preservation on complex articulated motion.