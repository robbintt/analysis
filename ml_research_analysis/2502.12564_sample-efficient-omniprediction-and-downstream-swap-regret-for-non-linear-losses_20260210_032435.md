---
ver: rpa2
title: Sample Efficient Omniprediction and Downstream Swap Regret for Non-Linear Losses
arxiv_id: '2502.12564'
source_url: https://arxiv.org/abs/2502.12564
tags:
- decision
- functions
- loss
- regret
- swap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of making online predictions that
  are useful to all downstream agents, even when agents have non-linear loss functions
  over multi-dimensional outcomes. The authors introduce "decision swap regret," which
  generalizes both omniprediction and swap regret, and develop algorithms to achieve
  it for arbitrary multi-dimensional Lipschitz loss functions in online adversarial
  settings.
---

# Sample Efficient Omniprediction and Downstream Swap Regret for Non-Linear Losses

## Quick Facts
- arXiv ID: 2502.12564
- Source URL: https://arxiv.org/abs/2502.12564
- Reference count: 40
- Key outcome: Introduces decision swap regret and achieves polynomial sample-complexity bounds for omniprediction with Lipschitz loss functions, with improved bounds for specific economically-relevant loss functions

## Executive Summary
This paper addresses the challenge of making online predictions that remain useful to downstream agents with arbitrary non-linear loss functions over multi-dimensional outcomes. The authors introduce a new concept called "decision swap regret" that generalizes both omniprediction and swap regret, and develop algorithms that can guarantee low regret for all downstream agents simultaneously. By constructing basis functions that uniformly approximate non-linear loss functions, the problem is transformed into one with linear losses in a higher-dimensional space, enabling the use of decision calibration techniques in adversarial online settings.

The key contribution is achieving exponentially improved sample complexity bounds compared to previous work, with polynomial dependence on the error parameter for Lipschitz loss functions. The paper provides specific improved bounds for economically-relevant loss functions including CES, Cobb-Douglas, and Leontief, though general bounds still scale exponentially with outcome space dimension. The work also extends results to continuous action spaces and provides an online-to-batch reduction for offline settings.

## Method Summary
The core approach involves constructing a set of basis functions that can uniformly approximate any Lipschitz loss function from a given family. This allows transforming the problem of omniprediction with non-linear losses into a higher-dimensional linear loss problem. The algorithm then applies decision calibration techniques in the online adversarial setting, using these basis functions to maintain low swap regret for all downstream agents simultaneously. The authors prove that this approach achieves polynomial sample complexity bounds for Lipschitz loss functions, dramatically improving upon previous exponential bounds. For specific loss function families (CES, Cobb-Douglas, Leontief), the algorithm provides even better bounds by exploiting their structural properties.

## Key Results
- First polynomial sample-complexity bounds for omniprediction with Lipschitz loss functions (previous bounds scaled exponentially with error parameter)
- First algorithm achieving swap regret bounds for all downstream agents with non-linear multi-dimensional loss functions
- General bounds scale exponentially with dimension but provide improved bounds for specific loss functions:
  * CES: polynomial in both dimension and error parameter
  * Cobb-Douglas: pseudo-polynomial (exponential in log of inverse error parameter)
  * Leontief: exponential in dimension but polynomial in error parameter

## Why This Works (Mechanism)
The algorithm works by transforming