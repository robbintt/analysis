---
ver: rpa2
title: Few-shot Implicit Function Generation via Equivariance
arxiv_id: '2501.01601'
source_url: https://arxiv.org/abs/2501.01601
tags:
- equivariant
- weight
- weights
- generation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Few-shot Implicit Function Generation, a new problem
  setup that aims to generate diverse yet functionally consistent INR weights from
  only a few examples. This is challenging because even for the same signal, the optimal
  INRs can vary significantly depending on their initializations.
---

# Few-shot Implicit Function Generation via Equivariance

## Quick Facts
- arXiv ID: 2501.01601
- Source URL: https://arxiv.org/abs/2501.01601
- Authors: Suizhi Huang; Xingyi Yang; Hongtao Lu; Xinchao Wang
- Reference count: 40
- Key outcome: Introduces Few-shot Implicit Function Generation via EQUI GEN, achieving FID scores of 121.24 and 164.14 on MNIST-INRs and CIFAR-10-INRs with LPIPS scores of 0.4133 and 0.4926, and MMD scores of 3.4, 3.5, and 4.2 for ShapeNet-INRs airplane, car, and chair categories with COV scores of 35%, 31%, and 41%.

## Executive Summary
This paper tackles the problem of generating diverse yet functionally consistent Implicit Neural Representation (INR) weights from only a few examples. The key insight is that functionally similar networks can be transformed into one another through weight permutations, forming an equivariance group. By projecting these weights into an equivariant latent space, the proposed EQUI GEN framework enables diverse generation within these groups even with limited data, effectively addressing the challenge that optimal INRs for the same signal can vary significantly based on initialization.

## Method Summary
EQUI GEN is a three-stage framework: (1) An equivariant encoder is pre-trained via contrastive learning with smooth augmentation to learn canonical weight orderings and cluster functionally equivalent weights in a shared latent space, (2) An equivariance-guided diffusion model generates new weights conditioned on these features with explicit equivariance regularization, and (3) For few-shot adaptation, the model extracts equivariant features from target samples, applies controlled perturbations in the latent space, and denoises to produce diverse yet functionally consistent weights. The approach is validated on 2D image (MNIST-INRs, CIFAR-10-INRs) and 3D shape (ShapeNet-INRs) datasets.

## Key Results
- Achieves FID scores of 121.24 and 164.14 on MNIST-INRs and CIFAR-10-INRs respectively
- Achieves MMD scores of 3.4, 3.5, and 4.2 for ShapeNet-INRs airplane, car, and chair categories
- Demonstrates successful few-shot generation with COV scores of 35%, 31%, and 41% across ShapeNet categories
- Shows effective trade-off between diversity and quality through controlled subspace perturbations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Functionally equivalent INR weights form permutation orbits that can be collapsed into a unified latent representation
- **Mechanism:** For a shallow network with weights $W_1$ and $W_2$, applying permutation $P$ to produce $PW_1$ and $W_2P^T$ yields identical input-output behavior because pointwise activations commute with permutation: $P\sigma(x) = \sigma(Px)$. The equivariant encoder learns to map all weights in the same permutation orbit to similar latent codes
- **Core assumption:** INR weight spaces exhibit structured symmetry that can be exploited for sample-efficient learning; smooth augmentation aligns random initializations to a canonical ordering
- **Evidence anchors:** [abstract] "functionally similar networks can be transformed into one another through weight permutations, forming an equivariance group"; [section 3.2] formalizes equivariance with Eq. (1-2); corpus evidence is weak

### Mechanism 2
- **Claim:** Contrastive pre-training with smooth augmentation produces equivariant features that cluster by functional equivalence rather than element-wise weight similarity
- **Mechanism:** The encoder $E_\phi$ is trained via SimCLR-style contrastive loss where positive pairs are generated through smooth augmentation (finding permutation $P^*$ minimizing total variation via Hamiltonian Path optimization) and INR-based augmentations (rotation, translation, scaling). This forces the encoder to learn invariance to both permutation and signal-level transformations
- **Core assumption:** Minimizing total variation across weight matrices produces a canonical ordering that aids generalization; INR-based augmentations preserve class semantics
- **Evidence anchors:** [section 4.2] describes smooth augmentation as optimization into "Shortest Hamiltonian Path problems" solved with "2.5-opt local search"; [Figure 7/8] t-SNE visualization shows tighter clustering with smooth augmentation; corpus does not provide comparable evidence

### Mechanism 3
- **Claim:** Diffusion conditioned on equivariant features with explicit equivariance regularization generates diverse weights within the same functional class
- **Mechanism:** The denoising transformer $G_\theta$ predicts weights directly, conditioned on equivariant features via cross-attention. The composite loss $L_{total} = L_{recon} + \lambda L_{eq}$ includes equivariance loss forcing generated weights to map to the same equivariant subspace. During inference, controlled perturbations $\tilde{\psi}'_i = \psi'_i + \gamma\epsilon$ in the equivariant subspace enable diverse sampling
- **Core assumption:** The equivariant subspace is smooth enough that small perturbations yield functionally consistent but diverse weights; the trade-off between diversity ($\gamma$) and quality is acceptable
- **Evidence anchors:** [section 4.3] Eq. (5-7) formalize the equivariance-regularized diffusion; [Figure 8] shows COV increases with $\gamma$ while MMD degrades; [corpus] HyperDiffusion cited as baseline but lacks equivariant regularization

## Foundational Learning

- **Equivariance to Permutations:**
  - Why needed here: The entire framework rests on the mathematical fact that permuting neurons in one layer and reversing the permutation in the next preserves network function
  - Quick check question: Given a 2-layer MLP with weights $[W_1, W_2]$, if you swap columns 1 and 3 of $W_1$, what must you do to $W_2$ to preserve the function?

- **Contrastive Learning (SimCLR paradigm):**
  - Why needed here: The encoder is trained to maximize similarity between augmented views of the same weight sample while pushing apart different samples
  - Quick check question: In contrastive learning, what defines a "positive pair" vs. a "negative pair," and what does the temperature parameter $\tau$ control?

- **Diffusion Models (DDPM/DDIM):**
  - Why needed here: The generative backbone uses diffusion to model weight distributions
  - Quick check question: What is the difference between predicting noise vs. predicting the denoised sample directly, and why might the latter be preferred for weight spaces?

## Architecture Onboarding

- **Component map:** Raw weights -> Smooth Augmentation Module (finds $P^*$ via Hamiltonian Path solver) -> Equivariant Encoder $E_\phi$ (4 equivariant layers, 128-dim features) -> INR-based Augmentation (rotation, translation, scaling, color jittering, bias perturbation) -> Contrastive Pre-training (SimCLR loss) -> Diffusion Denoiser $G_\theta$ (Transformer with cross-attention) -> Equivariance Regularizer ($L_{eq}$) -> Subspace Perturbation (adding $\gamma\epsilon$)

- **Critical path:** 1) Pre-train encoder on source domain weights with contrastive learning (smooth + INR augmentations), 2) Train diffusion model on smoothed weights, conditioned on equivariant features, with equivariance loss, 3) For few-shot target: extract equivariant features from $k$ target samples, apply subspace perturbation, denoise

- **Design tradeoffs:**
  - $\gamma$ (perturbation intensity): Higher $\to$ more diversity, lower quality. Default 0.3
  - $\lambda$ (equivariance loss weight): Higher $\to$ stronger functional consistency, may limit expressiveness. Default 0.1
  - Encoder depth: 4 equivariant layers used; deeper may over-regularize, shallower may under-capacity
  - Diffusion timesteps: 1000 used with DDIM sampling; fewer steps trade quality for speed

- **Failure signatures:**
  - Mode collapse: Generated samples cluster around one or few modes; likely $\lambda$ too high or insufficient contrastive pre-training
  - Off-class generation: Generated weights render incorrect class; likely $\gamma$ too large or encoder not converged
  - Poor reconstruction: High FID/MMD; check smooth augmentation convergence, diffusion training stability

- **First 3 experiments:**
  1. Ablate smooth augmentation: Train encoder without smooth augmentation, visualize t-SNE of equivariant features - expect less compact clusters
  2. Sweep $\gamma$ on held-out class: Generate samples with $\gamma \in \{0.1, 0.3, 0.5, 0.7\}$, plot COV vs. MMD - expect trade-off curve
  3. Compare conditioning strategies: Run (a) unconditional diffusion, (b) class-label conditioning, (c) equivariant conditioning - expect equivariance is critical

## Open Questions the Paper Calls Out
None

## Limitations
- Core equivariance assumption may break down with normalization layers, skip connections, or tied weights common in modern INR architectures
- Smooth augmentation optimization via 2.5-opt local search for Hamiltonian Path problems is computationally intensive and may not scale to deeper networks
- Diffusion model's effectiveness for weight generation is unproven territory given discrete, high-dimensional structure of weight matrices

## Confidence

**High Confidence:** The mathematical foundation of permutation equivariance for standard MLPs; The experimental results on ShapeNet-INRs showing consistent MMD/COMV patterns across categories

**Medium Confidence:** The smooth augmentation optimization actually finding useful canonical orderings; The contrastive pre-training learning meaningful semantic clusters in the equivariant latent space

**Low