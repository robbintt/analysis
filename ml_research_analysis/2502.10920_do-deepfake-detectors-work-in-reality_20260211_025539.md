---
ver: rpa2
title: Do Deepfake Detectors Work in Reality?
arxiv_id: '2502.10920'
source_url: https://arxiv.org/abs/2502.10920
tags:
- deepfake
- real-world
- detection
- dataset
- super-resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the gap between academic deepfake detection
  methods and real-world performance, particularly when deepfakes undergo post-processing
  like super-resolution. The authors create the first real-world faceswap dataset
  by collecting over 800 images from popular online face-swap tools, with face pairs
  matched by race, gender, and age.
---

# Do Deepfake Detectors Work in Reality?

## Quick Facts
- arXiv ID: 2502.10920
- Source URL: https://arxiv.org/abs/2502.10920
- Reference count: 28
- State-of-the-art detectors drop from >0.92 to ~0.53 AUROC on real-world deepfakes

## Executive Summary
This study reveals a critical gap between academic deepfake detection performance and real-world effectiveness. State-of-the-art detectors that achieve over 0.92 AUROC on academic benchmarks like FaceForensics++ drop to near-random performance (~0.53 AUROC) when tested on real-world faceswap images collected from popular online tools. The authors demonstrate that post-processing super-resolution techniques, commonly applied to deepfakes before distribution, introduce distribution shifts that obscure the artifacts detectors rely on, degrading performance from >0.9 to ~0.7 AUROC even on academic datasets.

## Method Summary
The authors create the first real-world faceswap dataset (RWFS) with 847 images from 8 popular online tools, using face pairs matched by race, gender, and age for improved realism. They evaluate two state-of-the-art detectors (EfficientNet-B4 and Self-Blended Images) on both academic benchmarks and the RWFS dataset. To quantify super-resolution impact, they apply GFPGAN and CodeFormer to FaceForensics++ and measure performance degradation. Self-swap testing reveals hidden post-processing pipelines in online tools by comparing outputs when source and target faces are identical.

## Key Results
- Academic detectors achieve >0.92 AUROC on FF++ but only ~0.53 on RWFS
- Super-resolution degrades FF++ performance from >0.9 to ~0.7 AUROC
- Self-swap testing reveals post-processing differences across all tested online tools
- Performance gap suggests SR post-processing is a primary cause of real-world detection failure

## Why This Works (Mechanism)

### Mechanism 1
Post-processing super-resolution (SR) significantly reduces deepfake detector accuracy by altering or removing the artifacts detectors rely on. Detectors trained on academic datasets learn to identify subtle generation artifacts, but when SR models reconstruct facial regions, they overwrite or mask those artifacts and introduce new SR-specific patterns, causing a distribution shift. Core assumption: learned artifact representations do not generalize to SR-modified imagery.

### Mechanism 2
Higher realism in real-world deepfakes creates a train–test distribution gap that lowers detector generalization. Academic datasets often include low-quality swaps with visible mismatches that produce detectable artifacts, while real-world tools use higher-fidelity generators and match face pairs on race, gender, and age, reducing trivial cues and producing images closer to natural face distribution.

### Mechanism 3
Self-swap testing reveals hidden post-processing pipelines in online faceswap tools. Swapping a face with itself should yield near-identical output; observed differences indicate additional processing. These differences align with SR artifacts, suggesting SR is a standard pipeline step.

## Foundational Learning

- **AUROC and class imbalance**: Why needed - performance claims underpin conclusions; Quick check - if a detector scores 0.5 AUROC on a balanced dataset, what does that imply about its discrimination ability?
- **Super-resolution models (GFPGAN, CodeFormer)**: Why needed - central to proposed failure mechanism; Quick check - how does a blind face restoration SR model generate high-frequency facial details from low-quality input?
- **Distribution shift and covariate shift**: Why needed - core issue is shift from academic to real-world/SR-processed images; Quick check - when train and test distributions differ in input characteristics but not labeling behavior, what type of shift is this?

## Architecture Onboarding

- **Component map**: Input image → Online faceswap generator → (possible SR post-processor) → Output image → Deepfake detector (EfficientNet-B4, self-blended images) → Real/Fake score
- **Critical path**: Validate baseline detector performance on academic data; apply SR to academic data and measure degradation; evaluate on RWFS dataset; use self-swap to confirm post-processing presence
- **Design tradeoffs**: Accuracy vs robustness (models tuned to specific artifacts may lose robustness under SR); dataset realism vs cost (collecting real-world matched data is expensive but more representative); matching vs diversity (race-gender-age matching improves realism but may reduce artifact diversity exposure)
- **Failure signatures**: Near-random AUROC (~0.53) on RWFS despite >0.92 on academic sets; self-swap visual diffs showing enhanced eye region details; performance drop from >0.9 to ~0.7 AUROC when SR is applied to FF++
- **First 3 experiments**: 1) Replicate baseline: Evaluate EfficientNet-B4 and SBI on FF++ and RWFS; 2) SR intervention: Apply GFPGAN/CodeFormer to FF++; 3) Self-swap diagnostic: Run self-swap on online tools and visualize differences

## Open Questions the Paper Calls Out

1. **Can detectors be trained to remain robust against SR while maintaining accuracy?** The authors highlight this as a critical avenue for enhancing robustness, but the study only evaluates existing detectors without exploring adversarial training or data augmentation approaches.

2. **What factors beyond SR contribute to the remaining AUROC gap?** The paper notes that SR alone doesn't fully explain the gap between SR-applied FF++ (~0.70) and RWFS (~0.53), suggesting other differences like generator architectures and face-matching quality play roles.

3. **Do different SR methods leave distinct, learnable artifact signatures?** The study observes that performance degradation varies by SR algorithm, raising the question of whether SR-specific artifacts could enable SR-aware detection or whether universal SR-robust features exist.

## Limitations

- Dataset representativeness: 847 images from 8 tools may not capture full diversity of real-world deepfake generation methods
- Super-resolution attribution: Unquantified contribution of SR versus other post-processing steps like compression or filtering
- Cross-detector generalizability: Only two detector architectures evaluated; vulnerability to SR may not be universal

## Confidence

- **High Confidence**: Academic detectors achieve >0.92 AUROC; same detectors achieve ~0.53 on RWFS; SR degrades FF++ from >0.9 to ~0.7 AUROC
- **Medium Confidence**: SR is primary cause of real-world degradation; real-world deepfakes create distribution shifts; self-swap testing reliably identifies post-processing
- **Low Confidence**: All current detectors are fundamentally vulnerable to SR; performance gap will persist as technology evolves; current evaluation methodologies are completely inadequate

## Next Checks

1. Test additional detector architectures (Xception, EfficientNet-B7, Capsule networks) on both academic datasets with SR and RWFS to determine if SR vulnerability is architecture-specific or universal.

2. Systematically isolate and quantify impact of different post-processing steps (SR, compression, color correction, sharpening) by applying them individually and in combination to academic datasets.

3. Train detectors on SR-augmented academic datasets and evaluate performance on both SR-modified academic data and RWFS to determine whether data augmentation can bridge the distribution gap.