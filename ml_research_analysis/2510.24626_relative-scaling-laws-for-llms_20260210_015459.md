---
ver: rpa2
title: Relative Scaling Laws for LLMs
arxiv_id: '2510.24626'
source_url: https://arxiv.org/abs/2510.24626
tags:
- scaling
- relative
- laws
- compute
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Relative scaling laws track how performance gaps between test distributions
  evolve with model scale, separating initial disparities from differences in improvement
  rate. The framework measures relative error as the ratio of treatment to baseline
  loss, fit as a power law in compute, revealing whether gaps narrow, persist, or
  widen.
---

# Relative Scaling Laws for LLMs

## Quick Facts
- **arXiv ID:** 2510.24626
- **Source URL:** https://arxiv.org/abs/2510.24626
- **Reference count:** 27
- **Key outcome:** Relative scaling laws track how performance gaps between test distributions evolve with model scale, separating initial disparities from differences in improvement rate. The framework measures relative error as the ratio of treatment to baseline loss, fit as a power law in compute, revealing whether gaps narrow, persist, or widen. Across 255 decoder-only Transformers trained under matched compute budgets from 10^18 to 10^20 FLOPs on three distinct datasets, relative scaling laws uncovered diverse trajectories: academic domains on MMLU converge toward parity regardless of training corpus; regional English dialects scale at rates correlated with online speaker population, with some regions diverging; AI risk clusters split, with capability- and influence-related risks increasing relative to self-improvement while adversarial risks do not. These results demonstrate that scale is not a universal equalizer, and highlight the importance of measuring relative alongside absolute scaling laws to better prioritize robustness challenges. The full model suite is released to enable reproducible study of both traditional and relative scaling laws.

## Executive Summary
Relative scaling laws extend the classic framework by comparing performance across different test distributions rather than a single one, quantifying how performance gaps evolve with model scale. This is achieved by computing relative error as the ratio of treatment to baseline loss, then fitting a power law in compute. Applied to 255 decoder-only Transformers spanning 10^18 to 10^20 FLOPs on three datasets, the method revealed that academic domains on MMLU converge toward parity, regional English dialects scale at rates correlated with online speaker population, and AI risk clusters diverge, with some risks worsening relative to others. The results show that scale does not uniformly close performance gaps, emphasizing the need to measure relative alongside absolute scaling laws to prioritize robustness challenges. The full model suite is released for reproducible study.

## Method Summary
The method measures relative scaling laws by computing the ratio of test loss on a treatment distribution to that on a baseline distribution, yielding a relative error metric. This metric is then modeled as a power law in training compute, isolating whether performance gaps narrow, persist, or widen with scale. Experiments use 255 decoder-only Transformers trained under matched compute budgets from 10^18 to 10^20 FLOPs on three datasets: academic domains (MMLU), regional English dialects, and AI risk clusters. The approach leverages controlled scaling to disentangle the effects of model size from training data and distribution shifts.

## Key Results
- Academic domains on MMLU converge toward parity regardless of training corpus.
- Regional English dialects scale at rates correlated with online speaker population, with some regions diverging.
- AI risk clusters split, with capability- and influence-related risks increasing relative to self-improvement while adversarial risks do not.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Scaling laws:** Describe how model performance improves with increased compute, parameters, or data; needed to predict resource requirements and understand learning dynamics; quick check: does loss decrease predictably with compute?
- **Relative error:** Ratio of treatment to baseline loss; isolates how gaps between distributions change with scale; quick check: is relative error <1 indicating convergence?
- **Power-law fitting:** Models the relationship between relative error and compute; captures scaling trends; quick check: is the fitted exponent statistically significant?

## Architecture Onboarding
**Component map:** Data preparation -> Model training -> Loss computation -> Relative error calculation -> Power-law fitting -> Analysis
**Critical path:** Compute allocation → Model training → Evaluation on treatment/baseline → Relative error computation → Scaling law fit
**Design tradeoffs:** Matched compute budgets ensure fair comparison but limit exploration of extreme scales; fixed architecture simplifies analysis but may miss scale-dependent effects
**Failure signatures:** Non-monotonic relative error curves suggest unstable training or data issues; poor power-law fits indicate scaling regime changes
**First experiments:** 1) Verify relative error decreases with compute on a simple benchmark; 2) Test sensitivity to baseline choice; 3) Confirm power-law fit quality improves with more compute points

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not extrapolate to models much larger than ~10B parameters.
- MMLU domain granularity may mask persistent gaps; alternative benchmarks could yield different results.
- Regional English dialect analysis depends on dataset representativeness, which could confound speaker population correlations.

## Confidence
- High: Methodological framework and implementation across 255 models, power-law fits within compute range
- Medium: Claims about academic domains converging, regional English dialect trajectories (benchmark and data sampling limitations)
- Low: Assertions about AI risk cluster behaviors (dependence on cluster definitions, lack of external validation)

## Next Checks
1. Evaluate the same relative scaling framework on models exceeding 10B parameters to test extrapolation of current trends.
2. Repeat the regional English dialect analysis with multiple, independently constructed training datasets to assess robustness of speaker population correlations.
3. Re-run AI risk relative scaling experiments using alternative clustering schemes or risk taxonomies to determine sensitivity of the observed relative dynamics.