---
ver: rpa2
title: 'Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need'
arxiv_id: '2510.21312'
source_url: https://arxiv.org/abs/2510.21312
tags:
- logt
- regret
- bound
- nash
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the problem of minimizing regret in stochastic
  multi-armed bandits while incorporating fairness considerations. The authors focus
  on the p-mean regret, which generalizes traditional regret measures and captures
  a spectrum of fairness objectives.
---

# Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need

## Quick Facts
- arXiv ID: 2510.21312
- Source URL: https://arxiv.org/abs/2510.21312
- Reference count: 40
- This paper demonstrates that a simple UCB algorithm with data-adaptive exploration achieves near-optimal fairness-aware regret bounds without strong assumptions.

## Executive Summary
This paper addresses the problem of minimizing regret in stochastic multi-armed bandits while incorporating fairness considerations through the p-mean regret objective. The authors show that a standard UCB algorithm, preceded by a data-adaptive uniform exploration phase, can achieve near-optimal regret bounds across the entire spectrum of fairness parameters (p ≥ -1). Their approach improves upon prior work by relaxing strong assumptions about reward distributions and optimal arm means, providing tighter bounds especially for negative values of p.

## Method Summary
The Welfarist-UCB algorithm operates in two phases. Phase I uses uniform exploration in random permutations until a data-adaptive termination condition is met for at least one arm, ensuring each arm is pulled Θ(1/(μ*)²) times. Phase II then runs a standard UCB algorithm for the remaining time horizon. The key innovation is showing that this simple approach achieves near-optimal p-mean regret without requiring strong assumptions on reward distributions or bounds on the optimal reward. The analysis carefully bounds the welfare contributions from both phases and uses linearization techniques for negative p values.

## Key Results
- Achieves order-optimal regret of O(σ√(k log T log k / T)) for p ≥ 0
- Achieves regret of O(σk^|p|+1/2√(log T / T)·max(1,|p|)) for p < 0
- Improves upon prior work by relaxing strong assumptions on reward distributions
- Numerical simulations validate practical effectiveness across different fairness parameters

## Why This Works (Mechanism)

### Mechanism 1: Data-Adaptive Exploration Termination
A uniform exploration phase with data-dependent termination allows standard UCB to achieve near-optimal fairness-aware regret without strong assumptions. The algorithm terminates when an arm's empirical mean is sufficiently large and its pull count is large relative to this mean, ensuring confidence intervals are a small fraction of the optimal reward.

### Mechanism 2: Reduction of Nash/p-mean Regret to UCB Analysis
The analysis decomposes total welfare into contributions from Phase I and Phase II. Phase I's welfare is lower-bounded using uniform exploration, while Phase II's welfare is bounded by proving arms pulled are "near-optimal" due to the UCB selection rule.

### Mechanism 3: Linearization for Negative p
For negative p values, the non-linear p-mean regret is bounded by linearizing the inverse term (1-x)^(-q). This transformation converts the problem into a solvable form by guaranteeing the error term is small enough for the linearization to hold.

## Foundational Learning

- **Concentration Inequalities (Hoeffding Bound)**: The algorithm relies on sample means concentrating around true means. Quick check: If a reward distribution has much higher variance than σ, how will this affect the frequency of the "good" event E?

- **Social Welfare Functions (Generalized Power Mean)**: Understanding what p represents (utilitarian, Nash, Rawlsian fairness) is essential for framing the problem. Quick check: For prioritizing worst-off users, should p be positive or negative?

- **Explore-Then-Commit (ETC) Strategy**: The two-phase structure requires understanding the exploration-exploitation trade-off. Quick check: What's the primary risk of making initial exploration too short?

## Architecture Onboarding

- **Component map**: Phase I Loop -> Phase II Loop
- **Critical path**: The termination condition of Phase I is most critical. A bug here causes premature exit or excessively long exploration.
- **Design tradeoffs**: Exploration duration vs. confidence in welfare guarantees. Longer Phase I provides stronger guarantees but incurs higher cumulative regret.
- **Failure signatures**: Runaway Phase I if μ* is very small; high variance blow-up if σ is underestimated; no-free-lunch for extreme fairness when |p| is large.
- **First 3 experiments**:
  1. Replicate paper experiments comparing against NCB and Explore-then-UCB on Bernoulli and Gaussian distributions
  2. Run across p values (1, 0.5, 0, -0.5, -1, -2) on fixed Gaussian instance
  3. Vary gap sizes to test sensitivity to μ* and Phase I duration

## Open Questions the Paper Calls Out
- Can matching lower bounds be established for p-mean regret when p < -1 to formalize the "no-free-lunch" principle?
- Is the √k gap between upper and lower bounds for p ∈ [-1, 0) inherent to the problem?
- Can the algorithm achieve instance-dependent logarithmic regret rates, or is the O(1/√T) rate unavoidable due to fairness constraints?

## Limitations
- Phase I duration scales as 1/μ*², becoming impractical when optimal mean is very small
- Linearization technique for negative p has tight error bounds that may not hold in practice
- Assumes known σ parameter, which is restrictive for real-world applications

## Confidence

- **High**: Core algorithmic mechanism (UCB + adaptive exploration) is well-specified with mathematically rigorous theoretical bounds
- **Medium**: Practical performance depends heavily on problem parameters (μ*, σ, k) that may not align with theoretical assumptions
- **Low**: Linearization technique for negative p values may have limited practical applicability due to tight error constraints

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary μ*, σ, and k to map performance boundaries and identify impractical regimes
2. **Distribution Robustness Test**: Evaluate on heavy-tailed distributions where sub-Gaussian assumption breaks down
3. **Competitive Benchmark Expansion**: Compare against modern fairness-aware bandit algorithms beyond NCB and Explore-then-UCB