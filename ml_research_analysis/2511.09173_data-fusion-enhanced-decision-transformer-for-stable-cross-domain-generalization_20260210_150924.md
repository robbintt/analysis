---
ver: rpa2
title: Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization
arxiv_id: '2511.09173'
source_url: https://arxiv.org/abs/2511.09173
tags:
- medium
- target
- dfdt
- source
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles cross-domain generalization for Decision Transformers
  by proposing a two-level data filtering and fusion framework. It uses MMD-based
  state-structure alignment and OT-based action feasibility weighting to select stitchable
  source fragments, then fuses them with scarce target data.
---

# Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization

## Quick Facts
- arXiv ID: 2511.09173
- Source URL: https://arxiv.org/abs/2511.09173
- Reference count: 40
- Primary result: DFDT improves returns by 22.8-63.1% over strong baselines on gravity, kinematic, and morphology shifts

## Executive Summary
This paper addresses cross-domain generalization for Decision Transformers by proposing a two-level data filtering and fusion framework. It uses MMD-based state-structure alignment and OT-based action feasibility weighting to select stitchable source fragments, then fuses them with scarce target data. Advantage-conditioned tokens replace brittle RTG signals, and a Q-guided regularizer suppresses junction discontinuities. Theoretically, stitchability radii bound performance gaps; empirically, DFDT significantly outperforms strong baselines on MuJoCo tasks under various domain shifts.

## Method Summary
DFD (Data Fusion-Enhanced Decision Transformer) improves cross-domain generalization by fusing scarce target data with selectively trusted source fragments. The method uses a two-level filter: MMD mismatch for state-structure alignment and OT deviation for action feasibility. Source fragments are weighted by their feasibility scores and fused with target data to create a weighted distribution. Advantage-conditioned tokens replace Return-to-Go tokens for more stable sequence modeling. A Q-guided regularizer is applied to suppress value and action jumps at trajectory junctions. The method is evaluated on MuJoCo locomotion tasks under gravity, kinematic, and morphology shifts.

## Key Results
- DFDT improves normalized returns by 22.8-63.1% over strong baselines across 15 cross-domain tasks
- Two-level filtering (MMD+OT) consistently outperforms single-filter variants
- Advantage-conditioned tokens provide better stability than RTG tokens across domain shifts
- Q-regularizer effectively suppresses junction discontinuities, with optimal α varying by task type

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing Return-to-Go (RTG) tokens with advantage-conditioned tokens stabilizes sequence semantics across domains.
- **Mechanism:** Advantage tokens (A = Q - V) are computed from value functions trained on the fusion distribution, providing a reward- and horizon-agnostic conditioning signal. This avoids the brittleness of RTG, which becomes incomparable when rewards or horizons shift between source and target domains.
- **Core assumption:** Value functions trained on the feasibility-weighted fusion distribution generalize sufficiently across domains to provide reliable advantage estimates.
- **Evidence anchors:** [abstract] "replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence"; [section 3.2] "A(s_i, a_i) = Q_ψ(s_i, a_i) - V_φ(s_i). These weighted advantage values replace the original (unavailable or inconsistent) RTG signals."
- **Break condition:** If the value functions Q and V are poorly estimated due to insufficient target data or misaligned fusion distribution, advantage tokens may misguide the policy, degrading performance compared to RTG.

### Mechanism 2
- **Claim:** Two-level fragment filtering (MMD for state structure + OT for action feasibility) bounds stitchability radii and improves cross-domain data fusion.
- **Mechanism:** MMD in a latent space selects source fragments with state transitions similar to the target manifold, while OT-based costs assign higher weights to source transitions with actions plausible under target dynamics. This creates a feasibility-weighted fusion distribution that reduces state-structure mismatch and action-jumps at stitch junctions.
- **Core assumption:** MMD and OT distances computed on limited target data accurately reflect true structural and feasibility gaps.
- **Evidence anchors:** [abstract] "fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility"; [section 3.1.3] "Define the weighted source and target–source data fusion distributions as P^w_S(u) ∝ w̃(u) P_S(u) and P^w_mix = (1-β)P^T + βP^w_S"
- **Break condition:** If the target dataset is too small to estimate reliable MMD/OT distances, or if the latent encoder fails to capture domain-invariant features, filtering may discard useful fragments or retain misaligned ones.

### Mechanism 3
- **Claim:** A Q-guided regularizer suppresses value and action jumps at trajectory junctions, improving policy stability.
- **Mechanism:** The training objective includes a term that maximizes the Q-value of actions under the fusion distribution, penalizing sharp transitions at fragment boundaries. This smooths token sequences and improves local Bellman consistency.
- **Core assumption:** The Q-function learned on the fusion distribution provides valid guidance for target-domain actions.
- **Evidence anchors:** [abstract] "applies a Q-guided regularizer to suppress junction value and action jumps"; [section 3.3] "L_π = L^w_DT - αE_{P^w_mix}[Q_φ(s, π(s))]"
- **Break condition:** If Q-values are overestimated due to distribution shift or insufficient data coverage, the regularizer may encourage over-optimistic actions, leading to instability.

## Foundational Learning

### Concept: Maximum Mean Discrepancy (MMD)
- **Why needed here:** MMD measures the distance between source and target state distributions in a latent space, enabling selection of structurally aligned fragments.
- **Quick check question:** Can you explain how MMD differs from KL divergence for comparing distributions?

### Concept: Optimal Transport (OT)
- **Why needed here:** OT provides a cost-based measure of how well source actions align with target dynamics, used to weight fragment feasibility.
- **Quick check question:** How does the Wasserstein distance handle support mismatch compared to KL divergence?

### Concept: Decision Transformer (DT)
- **Why needed here:** DFDT builds on DT, which frames RL as conditional sequence modeling using RTG tokens; understanding DT is essential to grasp the token-level modifications.
- **Quick check question:** How does DT differ from actor-critic methods in its use of sequence modeling?

## Architecture Onboarding

### Component map:
Two-level filter (MMD + OT) -> Critic networks (V_φ, Q_ψ) -> Command network C_ω -> Transformer policy π_θ

### Critical path:
1. Pre-compute MMD and OT distances on source data (Alg. 1, lines 3-4)
2. Train V_φ and Q_ψ on P^w_mix to generate advantage tokens (Alg. 1, lines 6-7)
3. Train C_ω to predict command tokens from advantages (Alg. 1, line 9)
4. Train π_θ with weighted loss and Q-regularizer (Alg. 1, lines 16-21)
5. Inference uses C_ω for command tokens and π_θ for actions (Alg. 2)

### Design tradeoffs:
- **MMD threshold ξ:** Lower ξ conservatively filters fragments but may discard useful data; default 50% balances data retention and alignment
- **Q-regularizer α:** Higher α suppresses junction jumps more aggressively but may over-constrain the policy; task-dependent tuning needed (e.g., α=3.5 works well for kinematic shifts, α=0 for some gravity shifts; Table 7)
- **Context length K:** Longer K captures more temporal dependencies but increases computational cost; K=5–20 based on task complexity (Table 3)
- **OT cost function:** Cosine cost is default and robust, but Euclidean may suit gravity shifts (Table 11)

### Failure signatures:
- **High action jumps (J_a > 0.2):** Indicates poor stitchability; check MMD/OT filtering or increase α
- **Q-value spikes (J_Q > 20):** Suggests overestimation; reduce α or improve critic training
- **Low normalized scores (< random):** Check data fusion quality (β, ξ) or advantage token reliability

### First 3 experiments:
1. **Ablate two-level filtering:** Compare DFDT (MMD+OT) vs. MMD-only and OT-only on a morphology shift task (e.g., hopp-m-r→medium) to verify both filters contribute (Table 5)
2. **Ablate advantage tokens:** Compare advantage-conditioned vs. RTG tokens on a kinematic shift task (e.g., hopp-m→medium-expert) to confirm stability gains (Table 6)
3. **Tune Q-regularizer α:** Sweep α (0, 1.0, 3.5, 5.0) on a challenging shift (e.g., hopp-m-e→medium morphology) to find task-optimal regularization (Table 7)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DFDT be effectively applied to real-world robotic control or high-dimensional visual domains where observation noise and partial observability are prevalent?
- Basis in paper: [explicit] The authors state in the "Limitations" section that the evaluation focuses on simulated benchmarks (D4RL-style MuJoCo) and lacks "real-world or larger-scale transfer experiments."
- Why unresolved: The method relies on precise MMD alignment in latent space and OT-based action feasibility, which may be sensitive to the high variance and complex manifolds inherent in visual or real-world data compared to low-dimensional state vectors.
- What evidence would resolve it: Successful adaptation results on standard real-world robotics benchmarks (e.g., Franka Kitchen) or vision-based control suites (e.g., Meta-World with image observations).

### Open Question 2
- Question: Can the feasibility-weighted fusion framework be successfully integrated into non-sequence-model-based offline RL algorithms, such as IQL or CQL, to improve their cross-domain performance?
- Basis in paper: [explicit] The authors note in the "Limitations" section that the fusion distribution P^w_mix is "in principle compatible with TD-based offline RL methods" and identify "exploring such TD-based variants" as a specific direction for future work.
- Why unresolved: While the theory suggests compatibility, the implementation currently relies on the sequence modeling architecture of Decision Transformers to stitch fragments; it is unclear if the fusion weights alone suffice for TD-based value updates.
- What evidence would resolve it: An implementation of IQL or TD3+BC modified to sample from the feasibility-weighted fusion distribution P^w_mix, showing performance gains over standard cross-domain baselines.

### Open Question 3
- Question: How does DFDT perform in the zero-shot transfer regime where the target dataset D_tar is completely unavailable (|D_tar|=0)?
- Basis in paper: [inferred] The problem setup (Section 2) explicitly defines the setting as using a "much smaller" but existing target dataset D_tar. The filtering mechanism relies on computing MMD and OT relative to this data (Eq. 2, Eq. 4), leaving the behavior in the absolute absence of target data uncharacterized.
- Why unresolved: Without target samples to ground the MMD and OT calculations, the stitchability radii cannot be computed using the current formulas, potentially causing the fusion to fail or require a fallback to the raw source distribution.
- What evidence would resolve it: Ablation studies varying the target dataset size down to zero samples to observe the performance degradation curve and determine if the method retains any advantage over source-only training.

## Limitations

- Scalability concerns with extremely scarce target data (5,000 transitions may be insufficient for reliable MMD/OT estimation)
- Q-regularizer effectiveness is highly task-dependent with limited systematic analysis of optimal α values
- Limited evaluation to low-dimensional MuJoCo benchmarks without validation on real-world or high-dimensional visual domains

## Confidence

- **High Confidence:** The empirical performance improvements (22.8-63.1% over baselines) are well-supported by experimental results across multiple MuJoCo tasks and domain shifts. The theoretical stitchability radius bounds provide reasonable justification for the filtering approach.
- **Medium Confidence:** The mechanism by which advantage tokens stabilize sequence semantics is plausible but relies on strong assumptions about value function generalization. While the paper shows RTG tokens are problematic across domains, the alternative advantage-based conditioning requires further validation on more diverse reward structures.
- **Low Confidence:** The Q-guided regularizer's impact is highly task-dependent and lacks systematic analysis. The paper reports α=3.5 works for kinematic shifts but α=0 for gravity shifts without explaining the underlying reasons or providing guidance for selecting α in new tasks.

## Next Checks

1. **Ablation on critic generalization:** Train DFDT with critics restricted to target data only (no fusion) versus critics trained on the full fusion distribution. Compare advantage token quality and junction stability metrics to quantify how much critic generalization contributes to the observed improvements.

2. **Scaling analysis with target data scarcity:** Systematically vary target dataset size (100, 500, 5,000, 20,000 transitions) and measure performance degradation. This would reveal whether the two-level filtering remains effective as target data becomes extremely scarce, addressing the core limitation of the approach.

3. **Latent space sensitivity analysis:** Replace the 256-dim MLP encoder with alternative architectures (CNN, transformer-based) or distance metrics (Wasserstein, energy distance) for MMD computation. Evaluate whether the filtering performance is robust to these choices or critically depends on specific encoder design.