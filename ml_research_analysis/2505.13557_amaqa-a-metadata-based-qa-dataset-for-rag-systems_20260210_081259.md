---
ver: rpa2
title: 'AMAQA: A Metadata-based QA Dataset for RAG Systems'
arxiv_id: '2505.13557'
source_url: https://arxiv.org/abs/2505.13557
tags:
- metadata
- dataset
- data
- documents
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMAQA is a metadata-based QA dataset for RAG systems, featuring
  1.1 million Telegram messages with metadata such as timestamps, topics, emotions,
  and toxicity labels. It includes 450 high-quality QA pairs, enabling evaluation
  of metadata-driven question answering.
---

# AMAQA: A Metadata-based QA Dataset for RAG Systems

## Quick Facts
- **arXiv ID:** 2505.13557
- **Source URL:** https://arxiv.org/abs/2505.13557
- **Reference count:** 13
- **Primary result:** Dataset of 1.1M Telegram messages with metadata boosts RAG accuracy from 0.12 to 0.75

## Executive Summary
AMAQA introduces a metadata-based QA dataset specifically designed for Retrieval-Augmented Generation (RAG) systems. The dataset contains 1.1 million Telegram messages enriched with metadata including timestamps, topics, emotions, and toxicity labels, along with 450 high-quality QA pairs. The study demonstrates that incorporating metadata into RAG systems significantly improves question answering accuracy, achieving a substantial increase from 0.12 to 0.75 through iterative context expansion and noise injection techniques.

## Method Summary
The AMAQA dataset was constructed from Telegram messages, with each message annotated with metadata including timestamps, topics, emotions, and toxicity labels. The dataset contains 1.1 million messages and 450 high-quality question-answer pairs. The evaluation methodology involved testing RAG systems with and without metadata utilization, measuring accuracy improvements through iterative context expansion and noise injection techniques. The study provides open-source access to the dataset for community use and benchmarking.

## Key Results
- RAG accuracy improves from 0.12 to 0.61 when leveraging metadata
- Further improvements to 0.75 achieved through iterative context expansion and noise injection
- Dataset publicly available with 1.1 million Telegram messages and 450 QA pairs

## Why This Works (Mechanism)
Metadata provides additional contextual information that helps RAG systems better understand the semantic relationships between questions and documents. Timestamps offer temporal context, topics provide semantic categorization, emotions capture sentiment and intent, and toxicity labels filter potentially harmful content. This enriched context enables more precise retrieval and generation, reducing ambiguity and improving relevance matching.

## Foundational Learning
- **Metadata enrichment** - Why needed: Provides additional context beyond raw text for better retrieval; Quick check: Verify metadata accuracy and coverage across dataset
- **Iterative context expansion** - Why needed: Allows progressive refinement of search space; Quick check: Measure performance gains at each iteration step
- **Noise injection** - Why needed: Improves robustness to real-world data variations; Quick check: Test system stability with varying noise levels
- **Temporal context** - Why needed: Enables time-aware question answering; Quick check: Validate timestamp accuracy and relevance
- **Sentiment analysis** - Why needed: Captures emotional tone for better understanding; Quick check: Cross-validate emotion labels with multiple annotators
- **Toxicity filtering** - Why needed: Ensures safe and appropriate responses; Quick check: Test filtering effectiveness across different toxicity levels

## Architecture Onboarding
- **Component map:** User Query -> Metadata Retrieval -> Document Retrieval -> Generation -> Response
- **Critical path:** Metadata extraction and filtering must complete before document retrieval for optimal performance
- **Design tradeoffs:** Rich metadata improves accuracy but increases computational overhead and storage requirements
- **Failure signatures:** Poor metadata quality leads to irrelevant document retrieval and decreased accuracy
- **First experiments:** 1) Test baseline RAG without metadata, 2) Add single metadata type (e.g., timestamps), 3) Combine multiple metadata types incrementally

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focus on Telegram messages may limit generalizability to other platforms
- No statistical significance testing provided for accuracy improvements
- Metadata types may not capture all useful contextual information for RAG systems

## Confidence
- **High:** Core dataset construction methodology and basic performance metrics
- **Medium:** Reported accuracy improvements (0.12 to 0.75) without statistical validation
- **Low:** Claims about establishing new benchmark and generalizability across domains

## Next Checks
1. Conduct statistical significance testing on reported accuracy improvements to establish robustness
2. Test AMAQA dataset with different RAG architectures and models
3. Evaluate dataset effectiveness on different communication platforms (e.g., Reddit, Twitter, Slack)