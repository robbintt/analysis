---
ver: rpa2
title: 'SoK: Measuring What Matters for Closed-Loop Security Agents'
arxiv_id: '2510.01654'
source_url: https://arxiv.org/abs/2510.01654
tags:
- security
- agents
- https
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for a principled framework to evaluate
  closed-loop autonomous security agents by introducing CLASP (Closed-Loop Autonomous
  Security Performance), which aligns security lifecycle stages with agentic capabilities
  and defines the Closed-Loop Capability (CLC) Score. The authors conduct a systematic
  survey of 21 works, mapping their capabilities and performance across reconnaissance,
  exploitation, root cause analysis, patching, and validation using structured rubrics.
---

# SoK: Measuring What Matters for Closed-Loop Security Agents

## Quick Facts
- arXiv ID: 2510.01654
- Source URL: https://arxiv.org/abs/2510.01654
- Reference count: 40
- Primary result: Introduces CLASP framework to evaluate closed-loop autonomous security agents, showing planning and reasoning are critical for success across all security stages.

## Executive Summary
This paper introduces CLASP (Closed-Loop Autonomous Security Performance), a principled framework for evaluating closed-loop autonomous security agents. The authors conduct a systematic survey of 21 security agent papers, mapping their capabilities and performance across the security lifecycle stages: reconnaissance, exploitation, root cause analysis, patching, and validation. The key contribution is the Closed-Loop Capability (CLC) Score, which balances efficacy (end-to-end completion, correctness, cycle efficiency) with efficiency (parsimonious capability use). Through structured rubrics, the study identifies that planning and reasoning are critical for success across all stages, tool use synergizes best with planning, and error handling and reflection enable recovery.

## Method Summary
The authors developed CLASP through three phases: (1) Survey - querying academic databases with specific agent anchors and function terms, filtering by Agent Evidence Score ≥3; (2) Scoring - applying CLASP rubrics to assign 1-5 levels for 5 security functions and 6 agentic capabilities; (3) Calculation - aggregating scores using the CLC formula with exponential penalties for overkill capability use. The framework defines 5 ordinal levels for each security function (reconnaissance through validation) and 6 agentic capabilities (planning, reasoning, memory, tool use, perception, reflection). The CLC Score multiplies S_Efficacy (weighted sum of completion rate, fix effectiveness, cycle efficiency) by S_Efficiency (weighted average of dimensional efficiency with exponential penalty for overkill).

## Key Results
- Planning and reasoning capabilities (score >3) are necessary for high performance across all security stages.
- Tool use synergizes best with planning, with planning providing coordination that unlocks tool value.
- Reflection enables recovery from execution errors, with agents lacking reflection halting or degrading on unexpected outputs.
- The proposed CLC Score provides a single scalar measure balancing efficacy and efficiency for closed-loop performance.

## Why This Works (Mechanism)

### Mechanism 1: Planning–Reasoning Co-Activation
High performance across security stages conditionally depends on planning AND reasoning both exceeding threshold (score >3). Planning maps belief/state to action sequences under constraints while reasoning derives warranted conclusions via deductive/abductive inference. Together, planning determines what to try while reasoning validates why it should work, reducing wasted exploration. Core assumption: Security tasks are sufficiently decomposable that symbolic-style search plus logical inference provide coverage where brute-force tool invocation cannot.

### Mechanism 2: Tool Use Requires Planning Orchestration
Tool capability without planning leads to inefficient or stuck execution; planning provides coordination that unlocks tool value. Planning modules sequence tool calls, manage dependencies, and enforce stop conditions. Without planning, agents execute tools reactively, over-focusing on single tasks or looping. With planning, tools are selected to match subgoals with pre/postcondition checks. Core assumption: Tool spaces are large enough that greedy or random selection underperforms structured selection.

### Mechanism 3: Reflection Enables Recovery from Execution Errors
Agents with reflection (metacognition) recover from failures; agents without reflection halt or degrade on unexpected outputs. Reflection monitors execution, detects mismatches between expected and actual outcomes, and triggers strategy updates (retry, replan, escalate). This closes the loop at the cognitive level, not just the action level. Core assumption: Errors are detectable and the agent has sufficient slack in budget/control to replan rather than simply abort.

## Foundational Learning

- **Concept: Closed-Loop vs Open-Loop Systems**
  - Why needed: CLASP evaluates agents that observe, act, and revise within a loop, not just predict once. Understanding feedback dynamics is prerequisite to interpreting CLC scores.
  - Quick check: Can you articulate why an agent that succeeds on first attempt but cannot recover from a failure is not "closed-loop" under this framework?

- **Concept: Agentic Capabilities as Orthogonal Dimensions**
  - Why needed: The framework separates planning, reasoning, memory, tool use, perception, and reflection into distinct rubrics scored 1-5. Designing or evaluating agents requires knowing which capability is bottlenecking performance.
  - Quick check: Given an agent that enumerates assets thoroughly but cannot prioritize attack vectors, which CLASP capability is likely underdeveloped?

- **Concept: Parsimony in Agentic Efficiency**
  - Why needed: The CLC Score multiplies efficacy by efficiency, where efficiency penalizes both under- and over-deployment of capability relative to task requirements.
  - Quick check: If an agent uses planning level 5 on a task that only requires level 2, does S_Efficiency increase, decrease, or stay the same?

## Architecture Onboarding

- **Component map:**
  - Security-function rubrics (Tables I–II): RECON.1–5, EXPL.1–5, RCA.1–5, PATS.1–5, FIXV.1–5
  - Agentic-capability rubrics (Table III): PLAN.1–5, TOOL.1–5, MEMO.1–5, REAS.1–5, PERC.1–5, REFL.1–5
  - CLC Score (Section VI): CLC = S_Efficacy × S_Efficiency

- **Critical path:**
  1. For each target/scenario, estimate C_req per capability from CLASP sheets and stage logs
  2. Instrument agent runs to log capability deployments and map to C_dep
  3. Compute Eff_i per capability using Eq. (6); aggregate to S_Efficiency via Eq. (7)
  4. Compute S_Efficacy from outcomes; multiply for CLC

- **Design tradeoffs:**
  - Choosing w_i: weight by gating frequency vs. policy priorities
  - Choosing β_i: steeper β penalizes overkill more aggressively
  - Rubric granularity: 5 levels balance resolution vs. inter-coder reliability

- **Failure signatures:**
  - High tool use + low planning → agent stuck in loops or over-focused on one subtask
  - High reasoning + low perception → agent hallucinates causal chains without grounding in telemetry
  - Low reflection → brittle on first error; no recovery path

- **First 3 experiments:**
  1. Baseline capability profiling: Run existing agent on benchmark; code outputs per CLASP rubrics to establish C_dep profile
  2. Ablation by capability: Systematically downgrade one capability and measure CLC delta
  3. Efficiency sensitivity sweep: Vary β_i for critical capability across range; verify CLC rankings are stable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed benchmark requirements (R1–R5) be operationalized to support longitudinal memory and cross-stage artifact continuity?
- Basis in paper: The authors outline requirements for a benchmark to address the "episodic resets" of current evaluations but stop short of implementation, inviting the community to "co-develop a shared... benchmark."
- Why unresolved: Existing benchmarks evaluate isolated functions and lack the infrastructure to maintain state or pass artifacts between stages.
- What evidence would resolve it: Release of a functional benchmark suite that successfully scores agents on end-to-end loops with persistent memory.

### Open Question 2
- Question: What are the empirically optimal weightings (w_i) and penalty factors (β_i) for the CLC Score to ensure robustness without overfitting?
- Basis in paper: Section VI defines the CLC Score formula but leaves the specific parameters configurable, noting that future work must "report a small grid sweep" to establish stability.
- Why unresolved: The balance between penalizing complexity and rewarding efficacy is theoretically defined but not validated against a wide range of agent behaviors.
- What evidence would resolve it: Sensitivity analysis demonstrating that CLC Score rankings remain stable across different valid parameter configurations.

### Open Question 3
- Question: Do the agentic drivers identified in isolated stages generalize to prevent failures at stage handoffs?
- Basis in paper: The survey identifies key drivers for specific functions, but notes that "failures often cluster at these handoffs" in enterprise pipelines, a gap current benchmarks do not measure.
- Why unresolved: The study maps capabilities within single functions but lacks data on whether these capabilities prevent error propagation across the security lifecycle.
- What evidence would resolve it: A study measuring the correlation between high CLASP capability scores and reduced error rates at stage transitions.

## Limitations
- The CLC Score framework depends on subjective rubric application with moderate inter-coder reliability (κ=0.73 before calibration, 0.82 after).
- Key hyperparameters (capability weights w_i, penalty factors β_i) are not fixed but estimated from benchmark policies, introducing variability in CLC rankings.
- The survey corpus (21 papers) represents a snapshot; as security agent capabilities evolve rapidly, the framework may require recalibration to remain discriminative.

## Confidence

- **High confidence:** Identification of planning and reasoning as critical drivers; observation that tool use synergizes with planning; importance of reflection for recovery from errors.
- **Medium confidence:** Proposed CLC Score as unified measure; specific weightings and penalty structures used in calculations.
- **Low confidence:** Generalizability to agents outside surveyed corpus; framework's performance on novel, unseen security tasks.

## Next Checks

1. **Inter-coder validation:** Have two independent annotators apply CLASP rubrics to 10 randomly selected agent papers and compute Cohen's κ for each rubric; verify it meets or exceeds the reported 0.82.

2. **Framework generalizability:** Apply CLASP to a held-out set of 5 security agent papers published after the survey period; assess whether the framework still discriminates performance and identifies capability bottlenecks.

3. **Hyperparameter stability:** Perform a sensitivity analysis by varying weights w_i and penalties β_i across a reasonable range; verify that CLC rankings remain stable (no rank inversions) and that conclusions about critical capabilities are robust.