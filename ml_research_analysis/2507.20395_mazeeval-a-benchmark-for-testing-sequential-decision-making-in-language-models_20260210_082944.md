---
ver: rpa2
title: 'MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models'
arxiv_id: '2507.20395'
source_url: https://arxiv.org/abs/2507.20395
tags:
- spatial
- reasoning
- maze
- navigation
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MazeEval, a benchmark for evaluating spatial
  reasoning in LLMs using coordinate-based maze navigation tasks without visual cues.
  The benchmark tests models' ability to maintain spatial state and make sequential
  decisions using only coordinate feedback and distance-to-wall information.
---

# MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models

## Quick Facts
- **arXiv ID**: 2507.20395
- **Source URL**: https://arxiv.org/abs/2507.20395
- **Authors**: Hafsteinn Einarsson
- **Reference count**: 0
- **Primary result**: OpenAI's O3 solved mazes up to 30×30 perfectly, while other models failed catastrophically beyond 9×9 due to excessive looping

## Executive Summary
MazeEval is a benchmark designed to evaluate spatial reasoning and sequential decision-making in large language models using coordinate-based maze navigation tasks without visual cues. The benchmark tests models' ability to maintain spatial state and make decisions using only numerical coordinate feedback and distance-to-wall information. Eight state-of-the-art LLMs were evaluated on mazes ranging from 5×5 to 15×15 grids in both English and Icelandic. Results reveal a stark performance gap: while OpenAI's O3 achieved perfect navigation across all maze sizes up to 30×30, other models failed catastrophically beyond 9×9 mazes, with 100% of failures attributed to excessive looping behavior. Cross-linguistic analysis showed significant performance degradation in Icelandic, suggesting spatial reasoning capabilities in LLMs are influenced by linguistic resources rather than being language-agnostic.

## Method Summary
The benchmark uses a function-calling interface where models navigate mazes by invoking `move(direction)` commands. Mazes are generated using depth-first search with fixed seeds, ranging from 5×5 to 15×15 grids. At each step, models receive their current (x,y) position, goal coordinates, distance-to-wall in four directions, and full navigation history. The evaluation terminates when the goal is reached, a cell is visited 10+ times (excessive looping), or the step limit (3n²) is exceeded. Success rates, efficiency metrics, backtracking counts, and wall hits are tracked. Eight models were tested including OpenAI's O3, Google's Gemini 1.5 Pro, and Meta's Llama 3.1 models, across English and Icelandic languages.

## Key Results
- OpenAI's O3 achieved perfect navigation across all maze sizes up to 30×30 grids
- Most other models failed catastrophically beyond 9×9 mazes, with 100% of failures due to excessive looping (revisiting cells ≥10 times)
- Cross-linguistic analysis revealed a significant performance degradation in Icelandic, with models solving 3-4 maze sizes smaller than in English
- The performance gap between English and Icelandic was statistically significant (W = 273, p < 0.001, Cohen's d = 0.50)

## Why This Works (Mechanism)

### Mechanism 1: Coordinate-Based Spatial State Maintenance
- Claim: Models with robust spatial reasoning can maintain coherent internal maps using only numerical coordinate feedback and distance-to-wall information, but most current LLMs fail catastrophically at this when complexity exceeds working memory capacity.
- Mechanism: The benchmark provides (x, y) current position, goal coordinates, and distance-to-wall in each cardinal direction per step. Models must integrate this into a persisting spatial representation across sequential decisions. O3 succeeds; others fail at scale.
- Core assumption: Transformer architectures can maintain spatial state across long sequences when appropriately trained/architectured; failure indicates this capability is not universal.
- Evidence anchors:
  - [abstract]: "models navigate mazes of increasing complexity using only coordinate feedback and distance-to-wall information, testing their ability to maintain spatial awareness through purely numerical data"
  - [section 4.5]: "100% of failures were due to excessive cell visits, where models revisited the same cell 10 or more times...reveals a fundamental limitation: models lack the ability to track visited locations despite having access to their navigation history"
  - [corpus]: AlphaMaze (arXiv:2502.14669) shows GRPO-based training can enhance spatial intelligence, suggesting mechanism is trainable but not emergent
- Break condition: When maze size exceeds model's effective spatial working memory (appears to be ~9×9 for most models, 30×30 for O3), looping behavior becomes inevitable regardless of step budget.

### Mechanism 2: Linguistic Resource → Spatial Capability Transfer
- Claim: Spatial reasoning in LLMs is mediated by linguistic training resources, not language-agnostic cognitive modules; lower-resource languages show degraded spatial performance.
- Mechanism: Models trained primarily on English develop spatial reasoning patterns tied to English linguistic structures. When operating in Icelandic (morphologically rich, limited training data), the same models solve mazes 3-4 sizes smaller.
- Core assumption: Spatial reasoning emerges from statistical patterns in language corpora rather than from dedicated architectural components.
- Evidence anchors:
  - [abstract]: "suggesting spatial reasoning capabilities in LLMs are influenced by linguistic resources rather than being language-agnostic"
  - [section 5.2]: "The performance gap likely reflects the vast difference in training data availability between English and Icelandic...spatial intelligence in LLMs emerges not from explicit spatial reasoning mechanisms but rather through the interaction of linguistic patterns"
  - [corpus]: Limited direct corpus support for cross-linguistic spatial reasoning transfer; related work (Lai et al. 2023, cited in paper) shows multilingual performance gaps but not specifically for spatial tasks
- Break condition: If spatial reasoning were language-agnostic, performance would be statistically equivalent across languages with identical maze configurations. Data show it is not (W = 273, p < 0.001, Cohen's d = 0.50).

### Mechanism 3: Function-Calling Interface for Clean Sequential Decision Evaluation
- Claim: A function-calling interface isolates spatial reasoning from natural language generation/interpretation errors, providing cleaner measurement of sequential decision-making capability.
- Mechanism: Models invoke `move(direction)` functions rather than generating natural language responses. This eliminates parsing ambiguity and ensures consistent action space across models.
- Core assumption: Function calling reflects real-world agent deployment patterns and removes confounds from language generation quality.
- Evidence anchors:
  - [section 3.3]: "This approach eliminates parsing ambiguities from natural language responses, ensures consistent action space across all models, and mirrors real-world agent deployments"
  - [section 3.3]: "By using function calling, we can precisely measure the model's intended actions without the confounding factor of natural language generation or interpretation errors"
  - [corpus]: Toolformer (Schick et al., 2023, cited) demonstrates function-calling as general LLM-agent interface pattern
- Break condition: If models cannot reliably invoke functions or if function interface adds cognitive load exceeding spatial reasoning demands, evaluation validity degrades.

## Foundational Learning

- Concept: **Sequential decision-making under partial observability**
  - Why needed here: Maze navigation requires integrating state over time with limited sensory input (only local distance information, no global map). Understanding partial observability vs. full observability is prerequisite.
  - Quick check question: Can you explain why MazeEval tests "pure spatial reasoning" rather than visual pattern recognition?

- Concept: **Spatial working memory capacity**
  - Why needed here: The 9×9 performance cliff for most models suggests a spatial working memory limit. Understanding how transformers maintain state across sequences (attention, context window) is essential.
  - Quick check question: What happens to a model's navigation behavior when maze size exceeds its effective spatial working memory?

- Concept: **Cross-linguistic transfer in LLMs**
  - Why needed here: The English-Icelandic gap (3-4 maze size levels) reveals that "reasoning" capabilities may be language-dependent. Understanding training data distribution effects is critical.
  - Quick check question: Why does O3 show no language gap while other models do? What might explain this difference?

## Architecture Onboarding

- Component map: Maze Generator -> State Provider -> LLM Agent -> Validator -> Metrics Collector

- Critical path:
  1. Generate reproducible mazes (5×5 to 15×15) with DFS algorithm
  2. At each step, provide model with: (x,y) position, goal position, distance-to-wall in 4 directions, full navigation history
  3. Model calls `move(north|south|east|west)` via function interface
  4. Terminate if: goal reached, cell visited 10+ times, or step limit (3n²) exceeded
  5. Track success, efficiency (optimal/actual steps), backtracking, wall hits

- Design tradeoffs:
  - Distance-to-wall feedback vs. binary wall detection: More informative but still requires mental map construction
  - 10-visit loop threshold: Generous enough for exploration but catches degenerate behavior
  - Full history provided: Tests whether models can utilize perfect memory (most cannot effectively integrate it)
  - No visual input: Isolates pure spatial reasoning but differs from real-world multimodal deployment

- Failure signatures:
  - **Excessive looping (100% of failures)**: Model revisits same cell 10+ times despite having full history
  - **Sharp 9×9 threshold**: Most models fail catastrophically beyond this size (suggests working memory limit)
  - **Language degradation**: 3-4 maze size reduction in Icelandic (suggests linguistic resource dependency)
  - **Low wall collision rate (0.8 average)**: Models understand local constraints but cannot maintain global spatial coherence

- First 3 experiments:
  1. **Baseline replication**: Run English maze evaluation (5×5 to 12×12) on 2-3 models from different providers (e.g., GPT-4o, Claude Sonnet, Gemini Pro). Confirm 9×9 cliff and looping failure mode.
  2. **History ablation**: Test whether removing navigation history from prompts affects performance. If models don't effectively use history (likely), this validates the "spatial working memory" hypothesis.
  3. **Cross-linguistic probe**: Run identical mazes in English and a second language (Icelandic or another lower-resource language). Measure success rate gap and statistical significance using Wilcoxon signed-rank test with Bonferroni correction.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark abstracts away from multimodal sensory integration, using only coordinate-based feedback rather than visual input
- Cross-linguistic comparison is limited to only one lower-resource language (Icelandic), limiting generalizability
- Performance beyond 15×15 to 30×30 relies on extrapolation from smaller mazes rather than direct testing

## Confidence
- **High Confidence**: The existence of a sharp performance cliff at ~9×9 maze sizes and the attribution of 100% of failures to excessive looping behavior. This is directly measurable from the experimental data.
- **Medium Confidence**: The claim that spatial reasoning emerges from linguistic patterns rather than language-agnostic mechanisms. While the cross-linguistic gap is statistically significant, the causal mechanism remains speculative.
- **Medium Confidence**: The interpretation that the 9×9 threshold represents a spatial working memory limit. Alternative explanations (e.g., architectural constraints, training data patterns) cannot be ruled out with current evidence.

## Next Checks
1. **Architecture-specific probing**: Test the same maze tasks on different transformer architectures (e.g., standard decoder, encoder-decoder, MoE variants) to determine whether the 9×9 threshold is architecture-dependent or universal across LLM designs.

2. **Multimodal extension**: Introduce visual maze representations alongside coordinate data to test whether models can integrate spatial information across modalities, and whether this mitigates the looping failure mode.

3. **Training intervention study**: Train models with explicit spatial reasoning objectives (similar to AlphaMaze) and retest on MazeEval to quantify whether spatial reasoning capabilities are fundamentally learnable or architecturally constrained.