---
ver: rpa2
title: 'DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation'
arxiv_id: '2512.19012'
source_url: https://arxiv.org/abs/2512.19012
tags:
- evaluation
- script
- format
- dimensions
- conflict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DramaBench introduces a six-dimensional evaluation framework for
  drama script continuation, addressing the gap in existing benchmarks that fail to
  comprehensively assess screenplay-specific requirements. The framework uses a novel
  LLM Labeling + Statistical Analysis methodology, where LLMs act as structured data
  annotators rather than direct judges, ensuring reproducibility and interpretability.
---

# DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation

## Quick Facts
- arXiv ID: 2512.19012
- Source URL: https://arxiv.org/abs/2512.19012
- Authors: Shijian Ma; Yunqi Huang; Yan Lin
- Reference count: 16
- Primary result: Introduces a six-dimensional framework for drama script continuation evaluation using LLM labeling methodology

## Executive Summary
DramaBench addresses the gap in existing benchmarks by providing a comprehensive six-dimensional evaluation framework specifically designed for drama script continuation. The framework uses a novel LLM Labeling + Statistical Analysis methodology where LLMs act as structured data annotators rather than direct judges, ensuring reproducibility and interpretability. Across 1,103 scripts and 8 SOTA models, no single model excels universally: GPT-5.2 leads in overall robustness, Qwen3-Max in emotional depth, and Gemini-3-Pro in conflict handling. All models achieve perfect format compliance, but logic consistency shows significant variance (2-5% error rates). Rigorous statistical validation confirms dimension independence and substantial human-LLM agreement on 3/5 dimensions.

## Method Summary
The DramaBench framework evaluates drama script continuations across six dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. It uses an LLM Labeling + Statistical Analysis approach where Qwen3-Max extracts categorical labels at various granularities (event-level, dialogue-level, scene-level, fact-level, global-level) which are then converted to objective metrics via statistical formulas. The framework includes a scene-boundary-aware splitting algorithm that searches for natural scene breaks within ±20% of the midpoint, ensuring continuations begin at realistic narrative points. Human validation was performed on 188 scripts to verify evaluator reliability, with correlation analysis confirming dimension independence (mean |r|=0.014).

## Key Results
- GPT-5.2 achieves highest overall robustness across all six dimensions
- Qwen3-Max excels in emotional depth while Gemini-3-Pro leads in conflict handling
- All models achieve perfect format compliance but show significant variance in logic consistency (2-5% error rates)
- No single model dominates across all dimensions, highlighting the importance of multi-dimensional evaluation
- Dimension independence is confirmed with mean correlation |r|=0.014 between metrics

## Why This Works (Mechanism)

### Mechanism 1
Using LLMs as structured data annotators (labeling categorical attributes) rather than direct quality scorers improves reproducibility and enables actionable feedback. The framework defines annotation units, categorical label sets, and structured prompts. LLMs extract labels like "Driver/Static/Redundant" for narrative beats, which are converted to objective metrics via statistical formulas. This decouples subjective judgment from metric calculation.

### Mechanism 2
Six independent evaluation dimensions capture distinct quality aspects that aggregate scores mask. Each dimension targets a separable capability: Format Standards (rule-based), Narrative Efficiency (plot progression), Character Consistency (persona adherence), Emotional Depth (arc dynamics), Logic Consistency (factual coherence), Conflict Handling (dramatic structure). Independence is validated through Spearman correlations.

### Mechanism 3
Scene-boundary-aware splitting creates more realistic continuation tasks than arbitrary midpoints. The algorithm searches within ±20% of the 50% midpoint for scene heading markers (INT./EXT.). If found (69.5% of cases), it splits at the boundary; otherwise uses the midpoint. This ensures continuations begin at natural narrative breaks.

## Foundational Learning

- **Fountain Screenplay Format**: Why needed - Format Standards dimension uses rule-based validation against Fountain specification. Quick check - Can you identify which lines are scene headings vs. action vs. dialogue?
- **LLM-as-a-Judge vs. LLM Labeling**: Why needed - Framework positions against direct scoring approaches. Quick check - If an LLM outputs "Score: 7/10" vs. "Label: Driver beat," which approach is being used?
- **Cohen's Kappa and Inter-Annotator Agreement**: Why needed - Human-LLM agreement is reported using Cohen's κ (0.42-0.53). Quick check - What does κ=0.53 indicate about the relationship between human and LLM annotations?

## Architecture Onboarding

- **Component map**: Dataset Layer (1,103 scripts) -> Scene-boundary-aware splitter -> Context/Continuation pairs -> Generation Layer (8 models) -> Evaluation Layer (Format analyzer + LLM Labeler) -> Validation Layer (Mann-Whitney U tests, human validation)
- **Critical path**: 1) Understand six dimensions and annotation units 2) Implement labeling prompts 3) Validate label extraction reproducibility 4) Apply statistical metrics 5) Run significance testing
- **Design tradeoffs**: Single evaluator vs. ensemble (increases cost but reduces bias), binary vs. multi-class labels (richer signal but complex prompts), scene-boundary vs. midpoint splits (30.5% use midpoint)
- **Failure signatures**: Logic consistency failures (spatial contradictions, state resets), character consistency outliers (12.9% rate with extreme skew), evaluator bias on 2/5 dimensions
- **First 3 experiments**: 1) Reproduce labeling reproducibility on 10 scripts 2) Test dimension independence on new model 3) Ablate scene-boundary splitting vs. midpoint

## Open Questions the Paper Calls Out

- Can multi-evaluator ensemble voting mitigate systematic biases observed in single-LLM evaluation for Narrative Efficiency and Character Consistency dimensions?
- Do the six evaluation dimensions generalize to full-length feature films, comedy, and experimental screenplay formats?
- How does evaluation performance change when using human-annotated examples in evaluation prompts to reduce LLM evaluator bias?
- Can the extracted categorical labels effectively serve as training data for Direct Preference Optimization to improve model performance on specific dimensions?

## Limitations

- Reliance on single LLM evaluator (Qwen3-Max) introduces potential systematic bias on 2/5 dimensions
- Human validation sample represents only ~17% of total corpus, limiting generalizability
- Novel scene-boundary-aware splitting strategy lacks comparative validation against simpler alternatives
- Framework effectiveness on longer or more complex scripts remains untested

## Confidence

- **High Confidence**: Format Standards dimension (rule-based), dimension independence validation, overall ranking robustness
- **Medium Confidence**: Human-LLM agreement for 3/5 dimensions, six-dimension framework capturing distinct quality aspects
- **Low Confidence**: Human-LLM agreement for Narrative Efficiency and Character Consistency, absolute metric values where evaluator bias may exist

## Next Checks

1. Reproduce labeling reproducibility: Run labeling pipeline on 20 randomly selected scripts across 3 independent trials to measure inter-run consistency of categorical labels.
2. Cross-evaluator validation: Implement ensemble of 3 different LLM evaluators for 5 non-format dimensions and compare dimension rankings.
3. Alternative splitting strategy comparison: Evaluate same 8 models on 200 scripts using simple midpoint splitting and perform paired statistical tests.