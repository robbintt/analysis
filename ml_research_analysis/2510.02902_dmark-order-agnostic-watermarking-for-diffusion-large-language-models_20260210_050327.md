---
ver: rpa2
title: 'DMark: Order-Agnostic Watermarking for Diffusion Large Language Models'
arxiv_id: '2510.02902'
source_url: https://arxiv.org/abs/2510.02902
tags:
- watermarking
- dllms
- text
- generation
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DMark, the first watermarking framework designed
  for diffusion-based large language models (dLLMs). The key innovation addresses
  the fundamental incompatibility between traditional watermarking methods and dLLMs'
  non-sequential generation process.
---

# DMark: Order-Agnostic Watermarking for Diffusion Large Language Models

## Quick Facts
- arXiv ID: 2510.02902
- Source URL: https://arxiv.org/abs/2510.02902
- Reference count: 40
- Primary result: Achieves 92.0-99.5% detection rates at 1% false positive rate across multiple dLLMs

## Executive Summary
DMark introduces the first watermarking framework specifically designed for diffusion-based large language models (dLLMs). Traditional watermarking methods fail on dLLMs because their non-sequential generation process breaks the context dependencies these methods rely on. DMark overcomes this by introducing three complementary strategies: predictive watermarking that infers missing context tokens, bidirectional watermarking that exploits both forward and backward dependencies unique to diffusion decoding, and their combination for maximum detection strength. Experiments show DMark achieves detection rates of 92.0-99.5% at 1% false positive rate, significantly outperforming naive adaptations of existing methods.

## Method Summary
DMark implements order-agnostic watermarking for dLLMs by addressing the fundamental incompatibility between traditional watermarking and non-sequential generation. The method uses predictive context inference when neighboring tokens are missing, bidirectional green list constraints that exploit both forward and backward dependencies, and their combination for maximum coverage. The framework precomputes a bit matrix encoding forward and backward green lists for O(1) lookup, then applies bias to logits based on whether tokens belong to these lists. Detection uses z-score calculation comparing observed green token proportions against expected values under the secret key.

## Key Results
- DMark achieves 92.0-99.5% detection rates at 1% false positive rate across multiple dLLMs
- Outperforms naive adaptations of existing methods (49.6-71.2% TPR) by 20-40 percentage points
- Maintains text quality with recommended parameters (γ=0.5, δ=2.0) achieving perplexity <6.5
- Demonstrates vulnerability to paraphrasing attacks (51.2% TPR for GPT-5-nano, 72.6% for Dipper)

## Why This Works (Mechanism)

### Mechanism 1: Predictive Context Inference
dLLMs compute logits for all positions simultaneously during each denoising step, including unfilled positions. When a token's predecessor doesn't exist, the algorithm predicts it via argmax over position i-1's logits, then uses this prediction to construct the green list for watermarking position i. Early denoising steps with highly uncertain logits may produce incorrect predictions, weakening watermark signal at those positions.

### Mechanism 2: Bidirectional Green List Constraints
dLLMs' bidirectional attention means token relationships are exploitable in both directions. Define backward green list G′i as tokens at position i that would make the subsequent token xi+1 green-listed. Detection objective maximizes either forward matches (xi ∈ Gi) or backward matches (xi ∈ G′i). Isolated tokens without neighbors in either direction receive no bias under pure bidirectional watermarking.

### Mechanism 3: Predictive-Bidirectional Combination
Combining prediction with bidirectional constraints maximizes watermark coverage across all generation scenarios. Always construct both forward green list Ĝi (using actual or predicted xi-1) and backward green list Ĝ′i (using actual or predicted xi+1). Apply bias δ·(1[v∈Ĝi] + 1[v∈Ĝ′i]) to all tokens, ensuring injection at every position regardless of generation order. Paraphrasing attacks degrade detection by disrupting statistical accumulation of green tokens.

## Foundational Learning

- **Concept: Diffusion Language Model Decoding** - dLLMs generate through iterative denoising from masked sequences, computing all positions simultaneously—fundamentally different from AR sequential generation. Quick check: Can you explain why position i might be finalized before position i-1 in dLLM generation, and what this implies for watermark context availability?

- **Concept: Green/Red List Watermarking (KGW Framework)** - DMark builds on the KGW paradigm where a secret key partitions vocabulary into green (favored) and red lists based on context hash. Detection uses z-scores from green token proportion. Quick check: Given context window w=1 and hash function h(s, xi-1), how would you determine if token xi is watermarked during detection?

- **Concept: Detection-Generation Tradeoffs in Watermarking** - Stronger bias δ improves detection but degrades perplexity. DMark recommends δ=2.0, γ=0.5 for practical deployment, achieving >92% TPR while preserving quality. Quick check: If you need to watermark short texts (L<50 tokens), what parameter changes would be necessary and what quality tradeoff would you expect?

## Architecture Onboarding

- **Component map:** Precomputation module -> Context checker -> Prediction module -> Bias applicator -> Detector
- **Critical path:** Token generation → Context availability check → (Prediction if needed) → Bidirectional green list construction → Bias application → Sampling → Detection via z-score
- **Design tradeoffs:** Higher δ (≥5.0) guarantees detection but PPL exceeds 20; δ=2.0 balances detectability with quality (PPL <6.5). Smaller γ (0.25) improves detection for moderate δ; larger ratios require stronger δ to compensate. Short texts require stronger parameters; long texts (L>150) achieve robust detection with conservative settings.
- **Failure signatures:** Low context availability drops TPR to 49.6%; prediction errors during early denoising lead to wrong green lists; paraphrasing attacks reduce best method to 51.2% TPR; excessive δ causes PPL degradation making output impractical.
- **First 3 experiments:** 1) Baseline validation: Run direct KGW adaptation on LLaDA with ELI5, measure context availability rate and TPR—expect ~67% context availability and ~65% TPR at 1% FPR. 2) Ablation across strategies: Compare Predictive vs. Bidirectional vs. Predictive-Bidirectional on C4 dataset with LLaDA-1.5-8B, holding δ=2.0, γ=0.5 constant—expect progressive improvement: 76.8% → 87.6% → 96.8% TPR. 3) Parameter sweep for deployment configuration: Test γ∈{0.25, 0.5, 0.75} × δ∈{1.0, 2.0, 5.0} combinations, measuring both TPR@1%FPR and PPL to identify practical operating point—expect γ=0.5, δ=2.0 to achieve target >92% TPR with acceptable PPL.

## Open Questions the Paper Calls Out

### Open Question 1
How can Dmark be modified to maintain detection robustness against sophisticated semantic paraphrasing attacks? The authors report detection rates drop significantly to 51.2% TPR when facing GPT-5-nano paraphrasing, identifying it as a well-known limitation. This remains unresolved because the current method relies on exact token sequences and green list membership, which are easily disrupted by semantic rewriting. A modified framework incorporating semantic invariants or continuous embeddings to sustain >90% TPR under similar paraphrasing conditions would resolve this.

### Open Question 2
What are the theoretical impacts of prediction errors on the false positive rate (FPR) guarantees? The predictive method relies on argmax predictions to generate seeds for green lists when context is missing, introducing potential noise into the hash function. While the paper empirically validates low FPR, it does not quantify how approximation errors in the predicted tokens theoretically bias the expected z-score distribution. A formal analysis deriving bounds on FPR inflation given a specific probability of prediction error would resolve this.

### Open Question 3
Does the Predictive-Bidirectional framework generalize to dLLMs with non-parallel decoding or strictly iterative refinement schedules? The method is predicated on the observation that dLLMs compute logits for all positions simultaneously. It is unclear if the strategy fails or degrades on diffusion models that do not offer full parallel logit access during the denoising step. Evaluation results applying DMark to alternative diffusion architectures lacking simultaneous full-sequence logit computation would resolve this.

## Limitations
- Significant vulnerability to paraphrasing attacks (51.2% TPR for GPT-5-nano, 72.6% TPR for Dipper)
- Parameter recommendations (γ=0.5, δ=2.0) validated only on specific model-task combinations
- Cross-dLM architecture generalization claims not experimentally verified across diverse architectures

## Confidence

**High Confidence (Experimental Evidence Strong):**
- Detection rates of 92.0-99.5% TPR at 1% FPR for Predictive-Bidirectional on tested datasets
- Quality degradation patterns with increasing δ (PPL scaling clearly demonstrated)
- Context availability limitations of direct KGW adaptation (~67% context availability)

**Medium Confidence (Experimental Evidence Moderate):**
- Comparative advantage over direct KGW adaptation (49.6-71.2% vs 92.0-99.5% TPR)
- Bidirectional mechanism effectiveness (87.6% vs 76.8% TPR for Predictive alone)
- Parameter recommendations for practical deployment (γ=0.5, δ=2.0 achieving target TPR with acceptable PPL)

**Low Confidence (Experimental Evidence Limited or Missing):**
- Cross-dLM architecture generalization claims
- Robustness against diverse attack vectors beyond tested paraphrasing methods
- Long-term stability of watermark detection under model fine-tuning or distribution shift

## Next Checks

1. **Cross-Architecture Generalization Test:** Evaluate DMark across at least three distinct dLM architectures (different diffusion formulations, training objectives, or token prediction mechanisms) using identical parameter settings. Measure whether the 92.0-99.5% TPR range holds or if significant architecture-specific tuning is required.

2. **Adversarial Attack Suite:** Systematically test DMark against a comprehensive adversarial attack portfolio including adversarial prefixing (targeted perturbations at generation start), watermark transfer attacks (attempting to migrate watermarks between models), and controlled paraphrasing with varying degrees of semantic preservation. Map the full degradation curve of TPR vs attack intensity.

3. **Parameter Transferability Analysis:** Conduct a formal parameter sensitivity study across the full γ-δ space for each tested model-task combination, then test whether parameters optimized for one combination transfer effectively to others. Quantify the prediction error and detection degradation when using cross-combination parameters versus locally optimized settings.