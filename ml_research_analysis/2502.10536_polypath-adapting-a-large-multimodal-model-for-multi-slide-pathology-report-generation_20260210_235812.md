---
ver: rpa2
title: 'PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology Report
  Generation'
arxiv_id: '2502.10536'
source_url: https://arxiv.org/abs/2502.10536
tags:
- report
- slides
- text
- original
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the ability to generate accurate pathology
  report text from multiple high-magnification whole-slide images using a large multimodal
  model with a long context window. By fine-tuning Gemini 1.5 Flash on tissue patches
  from up to 50 slides per case, the model produces part-level diagnostic findings
  that expert pathologists judged to be equivalent to or preferred over the original
  report in 68% of multi-slide cases with up to 5 slides.
---

# PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology Report Generation

## Quick Facts
- arXiv ID: 2502.10536
- Source URL: https://arxiv.org/abs/2502.10536
- Reference count: 40
- Primary result: Gemini 1.5 Flash fine-tuned on up to 50 whole-slide images per pathology part generates part-level reports preferred to original in 68% of 2-5 slide cases

## Executive Summary
This work presents PolyPath, a system that adapts a large multimodal model (LMM) to generate pathology report text from multiple whole-slide images (WSIs) per specimen part. The approach fine-tunes Gemini 1.5 Flash with a long context window to process tissue patches from up to 50 slides per case, producing part-level diagnostic findings. Expert pathologists judged the generated reports equivalent to or preferred over original reports in 68% of multi-slide cases containing 2-5 slides. Performance decreased for cases with more than 5 slides, likely due to lower training data availability for these complex cases.

## Method Summary
PolyPath adapts Gemini 1.5 Flash for multi-slide pathology report generation through LoRA fine-tuning on tissue patches from whole-slide images. The model processes up to 50 slides per part at 10X magnification, extracting 768x768 patches with a frozen medical-specialized patch encoder. Each patch's 256 visual tokens are mean-pooled to a single token to manage the 1-million token context window. Patches are arranged in row-major order per slide. The model predicts both structured labels and free-text findings. Training uses ROUGE-L and METEOR metrics for checkpoint selection, with final evaluation including NLG metrics and expert pathologist review.

## Key Results
- Generated reports preferred to or equivalent with original reports in 68% of 2-5 slide cases
- Performance drops for cases with >5 slides, attributed to training data scarcity
- System handles up to 50 slides per part using mean-pooling to fit 1M token context
- Demonstrates feasibility of LMMs for complex multi-slide pathology report generation

## Why This Works (Mechanism)
The approach leverages Gemini 1.5 Flash's long context window to process multiple whole-slide images simultaneously, treating each patch as a visual token. Mean-pooling from 256 to 1 token per patch enables fitting more patches within the context limit. LoRA fine-tuning allows efficient adaptation without full model retraining. The frozen medical-specialized patch encoder provides domain-specific visual feature extraction while the LMM handles the language generation component.

## Foundational Learning
- **Whole-slide image processing**: Breaking large pathology slides into smaller patches enables LMM processing; needed because WSIs are too large for direct model input
- **Context window management**: Mean-pooling 256 visual tokens to 1 token per patch preserves spatial information while fitting within 1M token limit
- **LoRA fine-tuning**: Parameter-efficient adaptation of pre-trained models; needed to avoid full fine-tuning costs while achieving task-specific performance
- **Part-level vs case-level modeling**: Current approach generates per-part reports that are aggregated; case-level could enable more holistic analysis
- **Multi-slide pathology analysis**: Complex cases require examining multiple slides per specimen; traditional approaches process slides individually
- **Expert evaluation in medical AI**: Human expert review provides gold-standard assessment beyond automated metrics

## Architecture Onboarding

**Component map:** WSIs -> Tissue patches -> Patch encoder -> Mean-pooled tokens -> Gemini 1.5 Flash (LoRA-tuned) -> Report text

**Critical path:** Input WSIs → Patch extraction (768x768 at 10X) → Medical patch encoder → Mean-pooling (256→1 token) → Gemini 1.5 Flash → Text generation

**Design tradeoffs:** Mean-pooling preserves context window but may lose fine-grained visual details; part-level modeling is simpler but may miss case-level patterns; LoRA enables efficient fine-tuning but adds inference overhead

**Failure signatures:** Performance degrades significantly on cases with >5 slides; context overflow errors if token count exceeds 1M; poor generalization to external datasets

**3 first experiments:**
1. Test with single-slide cases to establish baseline performance
2. Gradually increase slide count (2, 5, 10) to identify context window limits
3. Compare mean-pooling vs. alternative token compression methods

## Open Questions the Paper Calls Out
- Does the model generalize to pathology cases from different hospital systems or geographical regions?
- Can performance improve by using more visual tokens or better spatial encoding than mean-pooling?
- Is the performance degradation in 6+ slide cases due to training data scarcity or context window limits?
- Can case-level models that jointly process all slides outperform the current part-level approach?

## Limitations
- Proprietary dataset prevents independent verification of results
- Performance drops significantly for cases with more than 5 slides
- Limited to colon polyps, raising generalizability questions to other tissue types

## Confidence
- **High confidence** in general methodology and framework for multi-slide pathology report generation using long-context LMMs
- **Medium confidence** in reported performance metrics due to lack of open data/code and unknown hyperparameters
- **Medium confidence** in clinical relevance given single-tissue-type focus and small expert panel size

## Next Checks
1. Replicate the pipeline using an open pathology dataset (e.g., TCGA) to verify methodology works outside proprietary data
2. Test model performance scaling with number of input slides beyond 10 to characterize true context window limits
3. Evaluate the model on a different tissue type (e.g., breast or lung pathology) to assess generalizability beyond colon polyps