---
ver: rpa2
title: Single-to-mix Modality Alignment with Multimodal Large Language Model for Document
  Image Machine Translation
arxiv_id: '2507.07572'
source_url: https://arxiv.org/abs/2507.07572
tags:
- image
- translation
- m4doc
- mllm
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M4Doc, a framework that leverages multimodal
  large language models (MLLMs) to enhance document image machine translation (DIMT)
  performance. The core idea is a "single-to-mix modality alignment" strategy, where
  an image-only encoder is aligned with MLLM's multimodal representations during training,
  enabling a lightweight DIMT model to capture visual-textual correlations without
  the computational cost of using MLLMs at inference.
---

# Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation

## Quick Facts
- arXiv ID: 2507.07572
- Source URL: https://arxiv.org/abs/2507.07572
- Reference count: 40
- Primary result: M4Doc achieves state-of-the-art translation quality on document images, outperforming baselines by +4.30 BLEU in-domain and +5.54 BLEU in cross-domain zero-shot settings.

## Executive Summary
This paper introduces M4Doc, a framework that leverages multimodal large language models (MLLMs) to enhance document image machine translation (DIMT) performance. The core innovation is a "single-to-mix modality alignment" strategy where an image-only encoder is aligned with MLLM's multimodal representations during training. This enables a lightweight DIMT model to capture visual-textual correlations without the computational cost of using MLLMs at inference. Experiments show M4Doc achieves state-of-the-art translation quality while maintaining high inference efficiency.

## Method Summary
M4Doc aligns an image-only encoder with the multimodal representations of a frozen MLLM during training. The alignment encoder processes only the document image to produce representations that match the MLLM's output when given both image and ground-truth text. This alignment is achieved through a cosine similarity loss. During inference, the MLLM is discarded, and the trained alignment encoder works with a translation decoder to produce the final translation. The framework uses Swin Transformer encoders for both alignment and image processing, with a Transformer decoder handling cross-attention to both encoder outputs.

## Key Results
- Achieves state-of-the-art translation quality, outperforming cascade and end-to-end baselines by significant margins (+4.30 BLEU over DIMTDA in-domain)
- Maintains high inference efficiency by bypassing the MLLM during inference while benefiting from its multimodal knowledge during training
- Demonstrates improved generalization in challenging scenarios like long contexts and complex layouts
- Shows superior cross-domain performance with +10 BLEU improvement on Political Reports in zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1: Single-to-Mix Representation Alignment
The image-only encoder learns to approximate the rich multimodal representations of a Large Language Model (MLLM) through training. During training, a frozen MLLM processes both the document image and ground-truth source text to produce a reference representation $H_{MLLM}$. Simultaneously, a lightweight Alignment Encoder processes only the image to produce $H_{Align}$. The model minimizes the distance (cosine similarity loss) between these representations, forcing the Alignment Encoder to extract visual features that implicitly encode textual content and layout structure.

### Mechanism 2: Inference-Time Decoupling
The system achieves efficiency by distilling the MLLM's capability into a specialized encoder during training, allowing the MLLM to be completely discarded during inference. While the MLLM is required to generate the target $H_{MLLM}$ for the alignment loss during backpropagation, it is not used in the forward pass for the translation decoder during inference.

### Mechanism 3: Cross-Domain Generalization via MLLM Priors
Aligning with an MLLM pre-trained on massive document datasets imparts generalization capabilities that small models trained on limited DIMT data cannot achieve alone. The Alignment Encoder learns from the MLLM's robust feature space, which has presumably learned to map "visual layout complexity" to "semantic structure" across millions of documents.

## Foundational Learning

- **Concept: Cosine Similarity Loss**
  - Why needed here: Unlike standard MSE, cosine similarity measures the alignment of vectors in terms of direction rather than magnitude, crucial for aligning semantic "content" between the MLLM and the student encoder without enforcing strict value matching.
  - Quick check question: Why does the paper choose cosine similarity over MSE for the alignment loss (Eq 4)?

- **Concept: Cross-Attention in Transformers**
  - Why needed here: The Translation Decoder must "read" the visual features extracted by the Alignment Encoder. Cross-attention allows the text generation step to attend to specific parts of the visual representation.
  - Quick check question: In the M4Doc architecture, does the Decoder attend to the Image Encoder, the Alignment Encoder, or both?

- **Concept: Representation Distillation**
  - Why needed here: This is the broader category M4Doc falls into. It differs from "output distillation" (matching logits) by matching internal hidden states ($H_{MLLM}$), often required when student and teacher architectures differ significantly.
  - Quick check question: Why is the MLLM frozen during this process?

## Architecture Onboarding

- **Component map:** Image → Alignment Encoder → Translation Decoder → Text; Image → Image Encoder → Translation Decoder; Image + Ground Truth Text → Frozen MLLM → Reference Representation

- **Critical path:** Training: Image → Alignment Encoder → Loss (compare with MLLM output); Inference: Image → Alignment Encoder → Translation Decoder → Text

- **Design tradeoffs:** The paper uses separate Alignment Encoder and Image Encoder (ablation shows forcing a single encoder degrades performance); document-specialized MLLM (Textmonkey) yields better results than general MLLM (Llava-next); balancing translation loss vs. alignment loss is sensitive (α=1.0 optimal).

- **Failure signatures:** Modality mismatch if MLLM is trained on natural images but applied to documents; text ignorance if alignment encoder cannot resolve small text; context truncation on very long contexts (>750 words).

- **First 3 experiments:** 1) Run "Base" model vs. M4Doc on DoTA validation set to confirm alignment loss adds value (+3 BLEU expected); 2) Implement "w/o Alignment Encoder" setup to verify necessity of separate Swin Transformer; 3) Generate T-SNE plots for Alignment Encoder outputs vs. MLLM outputs to check cluster overlap.

## Open Questions the Paper Calls Out
- The authors plan to explore integrating user prompts to translate text in specific regions of the image, as the current architecture generates entire translated text in a single output pass without region-specific focus.

## Limitations
- The approach shows performance degradation on very long contexts (>750 words), indicating the alignment encoder cannot fully resolve scalability limitations.
- The method assumes perfect OCR-like input from the MLLM during training, which may not hold for noisy or low-resolution document images.
- Success heavily depends on the MLLM being pre-trained on document data, suggesting the alignment is more "feature hijacking" than general multimodal reasoning.

## Confidence
- **High Confidence:** The empirical results showing M4Doc's superior BLEU/COMET scores over cascade and end-to-end baselines are robust and well-documented across multiple datasets and languages.
- **Medium Confidence:** The claim that single-to-mix alignment captures "crucial visual-textual correlations" is supported by ablation studies and latent space visualizations, but the exact nature of what is being aligned remains ambiguous.
- **Low Confidence:** The assertion that the Alignment Encoder can "fully" simulate the MLLM's multimodal reasoning is overstated, as the method still fails on long contexts and complex layouts beyond a threshold.

## Next Checks
1. **Error Pattern Analysis:** Conduct detailed failure case analysis on M4Doc outputs, categorizing errors by type (hallucination, missing text, layout corruption, long-context truncation) and comparing against Base model.

2. **Alignment Objective Ablation:** Replace cosine similarity loss with alternative objectives (MSE, KL divergence, contrastive loss) while keeping all other components fixed to measure performance differences.

3. **MLLM Capacity Sensitivity:** Systematically vary the capacity of the frozen MLLM teacher (smaller/shallow variants of Textmonkey or Llava-next) while measuring the student's performance to test dependency on teacher's representational richness.