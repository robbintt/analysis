---
ver: rpa2
title: 'Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized
  Guidelines'
arxiv_id: '2511.21214'
source_url: https://arxiv.org/abs/2511.21214
tags:
- safety
- prompts
- harmful
- guidelines
- sgasa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SGASA, a synthesized guideline-based adaptive
  safety alignment framework that improves reasoning models' robustness against adversarial
  jailbreak prompts by internalizing model-generated safety guidelines. The framework
  generates synthetic safety guidelines and augmented prompts from limited jailbreak
  examples, then applies supervised fine-tuning and direct preference optimization
  to internalize these guidelines.
---

# Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines

## Quick Facts
- **arXiv ID**: 2511.21214
- **Source URL**: https://arxiv.org/abs/2511.21214
- **Reference count**: 16
- **Primary result**: Adaptive safety alignment framework using synthesized guidelines improves reasoning models' robustness against adversarial jailbreak prompts

## Executive Summary
This paper introduces SGASA (Self-Guided Adaptive Safety Alignment), a framework that enhances reasoning models' safety by generating synthetic safety guidelines and augmented prompts from limited jailbreak examples. The method applies supervised fine-tuning and direct preference optimization to internalize these guidelines, improving robustness against adversarial attacks while maintaining reasoning performance. Experiments demonstrate significant safety improvements across multiple datasets and model scales, with up to 34.5% improvement on malicious content detection.

## Method Summary
SGASA generates synthetic safety guidelines and augmented prompts from limited jailbreak examples, then applies supervised fine-tuning and direct preference optimization to internalize these guidelines into reasoning models. The framework processes adversarial prompts through safety filters and reasoning modules, producing outputs that balance safety compliance with task completion. The approach is tested on DeepSeek-R1-Distill models across three datasets, showing improved safety performance while maintaining reasoning capabilities.

## Key Results
- SGASA (DPO) achieves up to 34.5% improvement on MaliciousEducator dataset for harmful content detection
- Strong cross-dataset generalization with consistent safety improvements across different reasoning tasks
- Maintains performance across different model scales (7B and 8B parameters)
- Effective balance between safety enhancement and reduced unnecessary refusals

## Why This Works (Mechanism)
The framework works by internalizing model-generated safety guidelines through fine-tuning, allowing models to develop robust safety responses without requiring extensive human-labeled safety data. The synthetic guideline generation process creates diverse adversarial examples that expose models to various attack patterns, while the preference optimization ensures the model learns to prioritize safety decisions. The combination of supervised fine-tuning and direct preference optimization creates a dual learning mechanism that reinforces safety behaviors through both explicit instruction following and preference-based learning.

## Foundational Learning
- **Adversarial jailbreak prompts**: Malicious inputs designed to bypass safety filters - needed to train models to recognize and resist attacks
- **Direct preference optimization (DPO)**: Reinforcement learning method that optimizes model preferences based on human feedback - provides stable fine-tuning without complex reward modeling
- **Supervised fine-tuning (SFT)**: Standard fine-tuning method using labeled examples - establishes baseline safety behaviors before preference optimization
- **Synthetic data generation**: Creating training examples through model generation rather than human annotation - enables scalable safety training without extensive human effort
- **Safety alignment**: Process of teaching models to refuse harmful requests while maintaining task performance - core objective of the framework
- **Cross-dataset generalization**: Model performance consistency across different evaluation datasets - validates robustness beyond specific training scenarios

## Architecture Onboarding

**Component Map**: Jailbreak examples → Synthetic guideline generator → Augmented prompt generator → SFT module → DPO module → Safety-enhanced reasoning model

**Critical Path**: The critical path flows from synthetic guideline generation through both SFT and DPO modules to produce the final safety-enhanced model. The DPO module provides the primary safety improvements while SFT establishes foundational safety behaviors.

**Design Tradeoffs**: The framework balances between safety enhancement and reasoning capability preservation, using dual fine-tuning approaches to avoid over-refusal while maintaining strong safety responses. The synthetic generation approach trades off some realism for scalability and diversity in training data.

**Failure Signatures**: Potential failures include over-refusal of legitimate requests, failure to recognize novel attack patterns not present in synthetic training data, and performance degradation on complex reasoning tasks due to safety constraints.

**First 3 Experiments**:
1. Safety improvement evaluation on WildJailbreak dataset comparing SGASA vs baseline models
2. Cross-dataset generalization test measuring performance consistency across MathPrompt and MaliciousEducator
3. Model scale analysis examining safety performance differences between 7B and 8B parameter models

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies entirely on public jailbreak datasets that may not represent all real-world adversarial strategies
- Safety improvement claims assume dataset scoring criteria align with actual harmful content detection without external validation
- Performance degradation claims based on model-generated evaluations rather than human expert assessment
- Comparison with Alpaca-Guard uses different model scales (1.1B vs 7B/8B), raising fairness concerns

## Confidence
- **High confidence**: Safety improvements are statistically significant and methodologically sound based on reported metrics
- **Medium confidence**: Claims about negligible performance degradation and strong generalization require additional validation through human evaluation
- **Low confidence**: Unsupervised nature claim and optimal safety-utility trade-off assertions need further substantiation

## Next Checks
1. Conduct human evaluation studies to verify that safety improvements correspond to actual harmful content detection rather than just dataset-specific patterns
2. Test the framework on larger frontier models (100B+ parameters) to assess scalability and performance consistency
3. Perform ablation studies to quantify the individual contributions of supervised fine-tuning versus direct preference optimization components to the overall safety improvement