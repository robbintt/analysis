---
ver: rpa2
title: How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and
  Failure Scenarios of Various LLMs in Agentic Simulations
arxiv_id: '2512.07497'
source_url: https://arxiv.org/abs/2512.07497
tags:
- file
- tool
- name
- type
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes 900 agentic execution traces across three models
  to understand how LLMs fail in tool-use scenarios. While DeepSeek V3.1 shows superior
  reliability through systematic verification and recovery, scale alone doesn't predict
  agentic robustness - Llama 4 Maverick's larger size yields only marginal improvements
  over Granite 4 Small.
---

# How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations

## Quick Facts
- arXiv ID: 2512.07497
- Source URL: https://arxiv.org/abs/2512.07497
- Authors: JV Roig
- Reference count: 40
- Models compared: DeepSeek V3.1, Llama 4 Maverick, Granite 4 Small

## Executive Summary
This study analyzes 900 agentic execution traces across three models to understand how LLMs fail in tool-use scenarios. While DeepSeek V3.1 shows superior reliability through systematic verification and recovery, scale alone doesn't predict agentic robustness - Llama 4 Maverick's larger size yields only marginal improvements over Granite 4 Small. The analysis reveals four recurring failure archetypes: premature action without grounding, over-helpfulness substituting missing entities, vulnerability to context pollution, and fragile execution under load. Recovery capability, not initial correctness, best predicts overall success. These findings suggest enterprise deployment requires deliberate design choices reinforcing verification, constraint discovery, and source-of-truth adherence rather than simply relying on stronger models.

## Method Summary
The study employs qualitative analysis of 900 execution traces from three LLMs (DeepSeek V3.1, Llama 4 Maverick, Granite 4 Small) across 10 KAMI v0.1 benchmark scenarios. Each model runs 30 samples per scenario with 29 tools spanning filesystem, Python, SQLite, git, and web categories. The analysis uses emergent coding to identify behavioral patterns, focusing on how models use tools, recover from errors, and handle missing information. Key constraints include temperature 0.4, single-tool-per-round execution, and 20-round maximum inference.

## Key Results
- DeepSeek V3.1 achieves 88.3% overall success rate through systematic verification and error recovery
- Llama 4 Maverick (97B) shows only marginal improvement over Granite 4 Small (2.2B), challenging scale-as-proxy assumptions
- Four failure archetypes identified: premature action, over-helpfulness substitution, context pollution vulnerability, fragile execution under load
- Recovery capability, not initial correctness, best predicts agentic success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recovery capability, not initial correctness, predicts agentic success.
- Mechanism: Models that interpret error feedback, diagnose root causes (e.g., wrong column names), and iteratively refine tool use convert failures into corrections. DeepSeek V3.1's dominance stems from this feedback-driven adaptation rather than raw scale.
- Core assumption: Error messages are sufficiently informative and models have inference budget to retry.
- Evidence anchors:
  - [abstract] "Recovery capability, not initial correctness, best predicts overall success."
  - [section 5] "DeepSeek V3.1's superiority derives not from never failing, but from consistently recognizing errors... and iteratively refining tool use."
  - [corpus] Related work on failure detection in robotic systems (Guardian paper) supports error-feedback loops as critical for robustness, though specific LLM recovery mechanisms remain underexplored.
- Break condition: When error feedback is ambiguous, or when max inference rounds are exhausted before recovery completes (observed in Granite 4 Small Q201).

### Mechanism 2
- Claim: Systematic verification before action reduces schema-guessing failures.
- Mechanism: Proactive grounding—inspecting schemas, sampling categorical values, validating assumptions before querying—prevents silent failures from incorrect column names or filter values. DeepSeek V3.1 frequently executes `SELECT DISTINCT` queries to verify region codes exist before filtering.
- Core assumption: Verification tools are available and their invocation cost is acceptable.
- Evidence anchors:
  - [abstract] "Enterprise deployment requires deliberate design choices reinforcing verification, constraint discovery, and source-of-truth adherence."
  - [section 4.2] DeepSeek V3.1 Q501: "model is heavily biased towards verifying which codes actually exist in the database... occurring in 27 out of 30 successful samples."
  - [corpus] Weak direct evidence; related agent benchmarks (APTBench) focus on skill acquisition rather than verification behaviors.
- Break condition: When distractor information creates "Chekhov's gun" effects, causing models to incorporate irrelevant data despite verification (observed across all models in Q503).

### Mechanism 3
- Claim: Over-helpfulness alignment causes autonomous substitution of missing entities.
- Mechanism: Models tuned for conversational helpfulness sometimes substitute plausible alternatives when requested entities don't exist (e.g., replacing a missing company with a similar one), violating task fidelity. This appears linked to alignment training over-prioritizing completion fluency.
- Core assumption: The behavior stems from post-training alignment rather than architectural factors.
- Evidence anchors:
  - [abstract] "Over-helpfulness substituting missing entities" identified as a recurring failure archetype.
  - [section 5] "This behavior appears to stem from alignment tuning that over-optimizes for helpfulness and completion fluency, not precision under uncertainty."
  - [corpus] No direct corpus evidence; agentic misalignment research (Anthropic paper cited) discusses related but distinct insider-threat scenarios.
- Break condition: When explicit instructions override helpfulness defaults (Q602 success rates jumped to 87.5% from 52.9% with added hint about missing data handling).

## Foundational Learning

- Concept: **Agentic execution traces**
  - Why needed here: The paper's methodology relies on analyzing complete tool-invoke/result sequences to identify behavioral patterns invisible in aggregate metrics.
  - Quick check question: Can you trace how a model would recover from a malformed JSON tool call across multiple inference rounds?

- Concept: **Grounding vs. inference**
  - Why needed here: The core failure mode distinction is whether models use tools to discover constraints (grounding) or guess based on priors (inference).
  - Quick check question: Given a database with unknown schema, what's the first tool call a grounded agent should make?

- Concept: **Context pollution / Chekhov's gun effect**
  - Why needed here: Explains why larger models still fail when distractor tables are present—they treat all context as signal.
  - Quick check question: If an agent has access to 6 database tables but only needs 3, how might you design the context to prevent distraction?

## Architecture Onboarding

- Component map:
  - Tool layer: 29 tools across filesystem, Python, SQLite, git, web categories (~2K tokens in context)
  - Execution engine: Single-tool-per-round constraint, 20-round max, temperature 0.4
  - Trace logger: Captures full conversation history for post-hoc analysis
  - Verification checkpoint (recommended): Explicit schema/value validation before dependent actions

- Critical path:
  1. Schema discovery → 2. Value sampling → 3. Action execution → 4. Output validation → 5. Error recovery loop
  - Models that skip steps 1-2 show highest failure rates (Granite in Q401-Q403)

- Design tradeoffs:
  - **Context breadth vs. precision**: More available tools/data increases distractor risk
  - **Recovery budget vs. latency**: More inference rounds enable recovery but increase response time
  - **Temperature**: 0.4 chosen to reduce stochasticity while preserving exploration; effects not fully characterized

- Failure signatures:
  - **Schema guessing loop**: Repeated queries with hallucinated column names
  - **Generation loop**: Repetitive token patterns indicating coherence collapse (Maverick Q402)
  - **Silent substitution**: Plausible outputs with wrong filter values (all models in Q502)
  - **Resignation after schema retrieval**: Declaring task impossible despite having correct schema (Granite Q503)

- First 3 experiments:
  1. **Schema-first prompt injection**: Add explicit instruction "Begin by examining schema before any queries" and measure Q502→Q602-style improvement on your task.
  2. **Distractor ablation**: Run same task with/without irrelevant tables/files to quantify context pollution sensitivity for your model.
  3. **Recovery budget calibration**: Vary max inference rounds (10/20/30) to find minimum budget where your model achieves acceptable recovery rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do varying temperature settings quantitatively and qualitatively affect agentic performance and error recovery rates?
- Basis in paper: [explicit] The authors state in Section 6: "Future work will explore the effects of model temperature on agentic performance more explicitly."
- Why unresolved: The study utilized a fixed temperature of 0.4 to balance reduced stochasticity with problem-solving flexibility, leaving the impact of this hyperparameter untested.
- What evidence would resolve it: Comparative benchmarking of the same models across a range of temperature settings (e.g., 0.0 to 1.0) on the KAMI v0.1 scenarios.

### Open Question 2
- Question: Can specialized AI-augmented techniques be developed to reliably analyze large-scale agentic execution traces without hallucinating findings?
- Basis in paper: [explicit] Section 6 notes that current AI attempts (Claude, Qwen) "proved to be too unreliable" and that "We plan to address this topic in future work."
- Why unresolved: Execution traces are too voluminous for purely manual analysis, yet general LLMs fail to synthesize these long contexts accurately, often fabricating success/failure patterns.
- What evidence would resolve it: A validated automated pipeline that can categorize failure archetypes in raw traces with precision and recall comparable to the manual analysis presented in the paper.

### Open Question 3
- Question: Which specific post-training reinforcement learning mechanisms are causally responsible for the superior recovery behaviors observed in DeepSeek V3.1?
- Basis in paper: [inferred] The authors attribute DeepSeek's reliability to post-training RL but note in Section 6 that "Proprietary post-training does not allow attribution of observed behaviors to specific methods."
- Why unresolved: Because DeepSeek V3 and V3.1 share an architecture but differ in performance, the exact training data or reward structures that enable better error diagnosis remain unknown.
- What evidence would resolve it: Controlled ablation studies on open-weights models comparing the effects of different RL strategies (e.g., process supervision vs. outcome supervision) on agentic recovery metrics.

## Limitations

- The qualitative coding methodology introduces subjectivity in failure classification, potentially missing or conflating distinct behaviors
- Results are limited to structured tool-use tasks with predetermined tool sets, may not generalize to open-ended scenarios
- Model-specific behaviors (DeepSeek V3.1's verification patterns) may reflect benchmark-specific optimization rather than fundamental architectural advantages
- Temperature setting of 0.4 and single-tool-per-round constraint may not reflect real-world agent deployments

## Confidence

- **High confidence**: The four failure archetypes are clearly identifiable across all models and scenarios; DeepSeek V3.1 demonstrates superior recovery rates in scenarios requiring iterative refinement (Q201-Q203, Q501).
- **Medium confidence**: The claim that recovery capability predicts success better than initial correctness - supported by pattern observation but not rigorously quantified through controlled experiments comparing recovery rates vs. initial accuracy.
- **Low confidence**: The assertion that over-helpfulness stems specifically from alignment training rather than architectural factors - this mechanism is inferred from behavior without direct experimental validation or ablation studies.

## Next Checks

1. **Controlled recovery experiment**: Design scenarios where models face identical initial errors but vary recovery budgets (10, 20, 30 inference rounds) to quantify the relationship between recovery capability and overall success rate across all three models.

2. **Context pollution quantification**: Systematically vary the number and relevance of distractor tools/tables in identical tasks to measure the precise impact of context breadth on success rates for each model, isolating the "Chekhov's gun" effect.

3. **Verification behavior ablation**: Create two prompt variants for identical tasks - one with explicit schema-first instructions and one without - to measure how much DeepSeek V3.1's verification advantage derives from model capability versus prompt conditioning.