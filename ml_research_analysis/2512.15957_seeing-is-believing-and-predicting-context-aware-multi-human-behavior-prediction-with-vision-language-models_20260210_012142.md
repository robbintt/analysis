---
ver: rpa2
title: 'Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction
  with Vision Language Models'
arxiv_id: '2512.15957'
source_url: https://arxiv.org/abs/2512.15957
tags:
- human
- prediction
- behavior
- scene
- camp-vlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting multi-human behaviors
  from third-person perspectives in complex environments, which is critical for mobile
  robots operating in human-populated spaces. The authors propose CAMP-VLM (Context-Aware
  Multi-human behavior Prediction), a Vision Language Model-based framework that integrates
  visual inputs with Scene Graphs to enhance prediction of human-scene interactions.
---

# Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models

## Quick Facts
- arXiv ID: 2512.15957
- Source URL: https://arxiv.org/abs/2512.15957
- Reference count: 40
- Primary result: CAMP-VLM achieves up to 66.9% improvement in multi-human behavior prediction accuracy using synthetic data and Vision Language Models

## Executive Summary
This paper addresses the critical challenge of predicting multi-human behaviors from third-person perspectives in complex environments, which is essential for mobile robots operating in human-populated spaces. The authors propose CAMP-VLM (Context-Aware Multi-human behavior Prediction), a Vision Language Model-based framework that integrates visual inputs with Scene Graphs to enhance prediction of human-scene interactions. The method employs a two-stage fine-tuning process combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to improve prediction accuracy.

Since existing datasets lack suitable multi-human behavior data from observer views, the authors generate synthetic data using a photorealistic simulator and also collect real-world videos. CAMP-VLM achieves substantial performance improvements over baselines across various metrics including accuracy, cosine similarity, and edit distance in both synthetic and real-world scenarios.

## Method Summary
CAMP-VLM addresses multi-human behavior prediction through a Vision Language Model-based framework that integrates visual inputs with Scene Graphs. The approach uses a two-stage fine-tuning process: first applying Supervised Fine-Tuning (SFT) on available data, followed by Direct Preference Optimization (DPO) to refine predictions. Due to the lack of existing datasets capturing multi-human behaviors from third-person observer perspectives, the authors generate synthetic data using a photorealistic simulator and complement this with real-world video collection. The framework combines these diverse data sources to train a model capable of predicting human-scene interactions in complex environments.

## Key Results
- CAMP-VLM achieves up to 66.9% improvement in prediction accuracy over the best-performing baseline
- Model outperforms baselines across multiple metrics (accuracy, cosine similarity, edit distance) in both synthetic and real-world scenarios
- Demonstrates consistent performance improvements in multi-human behavior prediction tasks

## Why This Works (Mechanism)
The framework works by leveraging Vision Language Models' ability to understand both visual and textual contexts simultaneously. By integrating Scene Graphs, the model can capture complex spatial relationships between humans and their environment. The two-stage fine-tuning process (SFT followed by DPO) allows the model to first learn from available data and then refine its predictions based on preference-based feedback, leading to more accurate behavior predictions.

## Foundational Learning
- **Scene Graphs**: Represent spatial relationships and interactions between objects and humans in a scene; needed to capture context for behavior prediction; quick check: verify graph completeness and accuracy
- **Vision Language Models**: Process both visual and textual information simultaneously; needed for multimodal understanding of human behaviors; quick check: test on diverse visual-textual pairs
- **Supervised Fine-Tuning (SFT)**: Adapts pre-trained models to specific tasks using labeled data; needed as initial adaptation step; quick check: monitor training loss convergence
- **Direct Preference Optimization (DPO)**: Refines model outputs based on preference learning; needed to improve prediction quality; quick check: evaluate preference ranking accuracy
- **Synthetic Data Generation**: Creates artificial training data when real data is scarce; needed to address dataset limitations; quick check: compare synthetic vs real data statistics
- **Photorealistic Simulation**: Generates realistic visual environments for training; needed to create believable synthetic scenarios; quick check: conduct human perception studies

## Architecture Onboarding
**Component Map**: Visual Input -> Scene Graph Extraction -> VLM Processing -> SFT Fine-tuning -> DPO Fine-tuning -> Behavior Prediction

**Critical Path**: The most critical path is the integration of Scene Graphs with VLM processing, as this provides the contextual understanding necessary for accurate behavior prediction. Any degradation in Scene Graph quality directly impacts prediction accuracy.

**Design Tradeoffs**: The authors chose synthetic data generation over waiting for real-world datasets, trading potential realism for immediate availability and control over data characteristics. This enables rapid development but may limit generalization.

**Failure Signatures**: Performance degradation is likely when Scene Graphs fail to capture relevant contextual information, when synthetic data poorly represents real-world scenarios, or when fine-tuning hyperparameters are suboptimal. Models may also struggle with rare or unexpected human behaviors not well-represented in training data.

**First Experiments**:
1. Baseline VLM performance without Scene Graph integration
2. Single-stage vs two-stage fine-tuning comparison
3. Synthetic data ablation study (varying ratios of synthetic vs real data)

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on synthetic data generation due to lack of suitable real-world datasets
- Performance improvements evaluated primarily on author-generated data, raising concerns about dataset bias
- Limited exploration of model sensitivity to fine-tuning hyperparameters

## Confidence
| Claim | Confidence |
|-------|------------|
| 66.9% improvement over baselines | Medium |
| Consistent performance across synthetic and real-world scenarios | Medium |
| Two-stage fine-tuning process effectiveness | Medium |

## Next Checks
1. Conduct cross-dataset validation using independently collected multi-human behavior datasets to verify generalizability beyond the authors' synthetic and collected data
2. Perform ablation studies to quantify the contribution of each component (Scene Graphs, SFT vs DPO, synthetic vs real data) to overall performance
3. Test the model's robustness across diverse environmental conditions and social scenarios not represented in the training data to assess real-world deployment readiness