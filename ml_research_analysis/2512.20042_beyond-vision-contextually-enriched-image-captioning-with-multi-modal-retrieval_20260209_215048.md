---
ver: rpa2
title: 'Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieval'
arxiv_id: '2512.20042'
source_url: https://arxiv.org/abs/2512.20042
tags:
- arxiv
- image
- visual
- captions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal pipeline for generating context-enriched
  image captions by combining visual retrieval, semantic search, and a fine-tuned
  language model. The approach retrieves semantically similar images using BEIT-3
  and SigLIP2, re-ranks them with ORB and SIFT feature matching, extracts relevant
  textual context from news articles, and integrates it with visual descriptions via
  a QLoRA-fine-tuned DeepSeek-Qwen3 model.
---

# Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieval

## Quick Facts
- **arXiv ID**: 2512.20042
- **Source URL**: https://arxiv.org/abs/2512.20042
- **Authors**: Nguyen Lam Phu Quy; Pham Phu Hoa; Tran Chi Nguyen; Dao Sy Duy Minh; Nguyen Hoang Minh Ngoc; Huynh Trung Kiet
- **Reference count**: 40
- **Primary result**: CLIPScore of 0.748 (+12.8% vs. best baseline) and CIDEr score of 0.195 (10× higher than most baselines)

## Executive Summary
This paper introduces a multimodal pipeline for generating context-enriched image captions by combining visual retrieval, semantic search, and a fine-tuned language model. The approach retrieves semantically similar images using BEIT-3 and SigLIP2, re-ranks them with ORB and SIFT feature matching, extracts relevant textual context from news articles, and integrates it with visual descriptions via a QLoRA-fine-tuned DeepSeek-Qwen3 model. Evaluated on the OpenEvents v1 dataset, the system significantly outperforms prior methods, achieving a CLIPScore of 0.748 (+12.8% vs. best baseline) and a CIDEr score of 0.195 (10× higher than most baselines). These results demonstrate the effectiveness of incorporating external knowledge for richer, more informative event-aware captions.

## Method Summary
The proposed method employs a multi-stage pipeline: first, an ensemble of BEIT-3 and SigLIP2 encoders retrieves semantically similar images; second, ORB and SIFT feature matching with RANSAC re-ranks candidates to ensure geometric alignment; third, relevant textual context is extracted from news articles using semantic chunk selection with sliding-window segmentation; finally, a DeepSeek-Qwen3 model fine-tuned with QLoRA integrates the retrieved context with visual descriptions via a structured prompt. The entire system is trained and evaluated on the OpenEvents v1 dataset, producing enriched captions that combine factual information from articles with visual content descriptions.

## Key Results
- Achieved CLIPScore of 0.748, representing a 12.8% improvement over the best baseline
- Obtained CIDEr score of 0.195, approximately 10× higher than most baselines
- Demonstrated Retrieval Recall@1 of 0.994, indicating highly accurate semantic retrieval
- Outperformed existing methods including CONCAP (FMR 0.555) and Knowledge Completes the Vision (FMR 0.549)

## Why This Works (Mechanism)

### Mechanism 1
Multi-encoder semantic retrieval followed by geometric verification reduces false positive matches compared to embedding-only approaches. BEIT-3 and SigLIP2 produce overlapping but distinct candidate pools via different pretraining objectives (masked image modeling vs. sigmoid-based alignment). ORB/SIFT feature matching with RANSAC then filters candidates where semantic similarity does not correspond to shared visual structure. The core assumption is that semantically similar images describing the same event share both embedding proximity and geometric keypoints.

### Mechanism 2
Semantic chunk selection with sliding-window segmentation provides stronger supervision signal than full-article context for LLM fine-tuning. All-MiniLM-L12-v2 ranks sentence chunks by cosine similarity to the base caption. Top-5 chunks plus structural markers (first 3, last 2 sentences) form a condensed context window, reducing noise during gradient updates. The core assumption is that caption-relevant information clusters in specific article regions rather than being uniformly distributed.

### Mechanism 3
Prompt-anchored QLoRA fine-tuning enables the LLM to prioritize news context over visual description while maintaining factual grounding. A structured prompt (70% article info, 30% visual; explicit WHO/WHAT/WHY/WHEN/WHERE instructions) is concatenated with retrieved chunks and base caption. QLoRA (rank 256, 8-bit) updates a small adapter while preserving base model capabilities. The core assumption is that the prompt formulation during training generalizes to test-time queries with similar structure.

## Foundational Learning

- **Multiway Transformer Architecture (BEIT-3)**:
  - Why needed here: Understanding how a single backbone processes images, text, and image-text pairs via modality-specific adapters clarifies why BEIT-3 can embed both modalities into a shared space.
  - Quick check question: How does a Multiway Transformer differ from using separate encoders for vision and language?

- **Sigmoid vs. Contrastive Loss for Image-Text Alignment**:
  - Why needed here: SigLIP2 uses sigmoid-based binary classification rather than CLIP's contrastive objective. This affects how semantic similarity scores should be interpreted during ensemble weighting.
  - Quick check question: Why might sigmoid loss handle imbalanced positive/negative pairs differently than contrastive loss?

- **QLoRA Parameter Efficiency**:
  - Why needed here: The paper uses rank-256 adapters with 8-bit quantization. Understanding low-rank adaptation helps diagnose whether fine-tuning capacity is sufficient for the target domain.
  - Quick check question: What determines the minimum adapter rank needed to capture domain-specific knowledge without overfitting?

## Architecture Onboarding

- **Component map**:
Input Image → [BEIT-3 + SigLIP2 Encoders] → Candidate Pool → [ORB + SIFT Reranker] → InstructBLIP → Base Caption → [all-MiniLM Chunk Selector] → DeepSeek-Qwen3 + QLoRA → Context-Enriched Caption

- **Critical path**: Image → retrieval candidates → geometric reranking → chunk selection → LLM fusion. If reranking confidence < 0.4 or inliers < 8, the system may fall back to embedding-only ranking.

- **Design tradeoffs**:
  - Ensemble retrieval increases robustness but adds latency (3 model forward passes + ORB/SIFT matching)
  - Semantic chunking reduces context noise but may omit distributed information
  - QLoRA rank 256 balances capacity and memory; rank 512 showed slower convergence in ablation

- **Failure signatures**:
  - Low Recall@1 with high Recall@10: Reranking too aggressive, filtering correct matches
  - High CLIPScore but low CIDEr: Captions semantically aligned but phrasing differs from human references
  - Hallucinated named entities: LLM generates facts not grounded in retrieved chunks

- **First 3 experiments**:
  1. **Ablate geometric reranking**: Disable ORB/SIFT stage, compare Recall@1 and CIDEr to quantify verification contribution
  2. **Vary chunk count**: Test top-3 vs. top-7 chunks to find optimal context window size for the target dataset's article structure
  3. **Prompt perturbation**: Remove specific prompt constraints (e.g., 70/30 ratio) at inference to measure sensitivity to training-time prompt engineering

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative vision-language models with stronger OCR and entity recognition capabilities (e.g., Qwen-VL 2.5) substantially reduce hallucinations and improve factual grounding in the final enriched captions? The current pipeline relies on InstructBLIP Vicuna-7B for base caption generation, which lacks robust entity extraction, creating a "visual bottleneck" for the downstream LLM.

### Open Question 2
How can the relatively low CIDEr score (0.195) be improved while maintaining the high CLIPScore (0.748), given the apparent trade-off between semantic novelty and reference alignment? The enrichment process prioritizes adding external context (70% article, 30% visual) which may diverge from reference caption styles in the OpenEvents dataset.

### Open Question 3
Would dynamic model selection among diverse LLMs (based on image type or domain) improve caption quality compared to a single fine-tuned model? The current single-model approach cannot adapt to varying content types (e.g., political events vs. sports vs. natural disasters) that may benefit from different pretraining priors.

### Open Question 4
Can post-processing techniques (retrieval-based re-ranking, fluency polishing, learned constraints) meaningfully improve CIDEr and METEOR scores without sacrificing the contextual richness gained from retrieval? Current outputs are generated in a single pass without refinement, potentially including verbose or stylistically inconsistent portions.

## Limitations
- Geometric reranking assumes distinctive keypoints exist in query images; performance may degrade for abstract or heavily cropped inputs where ORB/SIFT matches fail
- Optimal chunk window size (3 sentences, stride 1) was not systematically ablated across datasets with varying article structures
- QLoRA rank-256 capacity adequacy was inferred from faster convergence curves but not validated against higher ranks

## Confidence
- **High**: The retrieval + reranking architecture significantly improves Recall@1 (0.994) over embedding-only methods, supported by clear performance gains and geometric verification principles
- **Medium**: The semantic chunking + prompt-based fine-tuning approach improves caption quality metrics, but the relative contribution of each component (chunk count, prompt ratio, adapter rank) remains under-specified
- **Low**: The claimed 10× CIDEr improvement over most baselines appears inflated when compared to CONCAP (0.325) and KCV (0.413) results from the corpus, suggesting potential metric sensitivity or evaluation protocol differences

## Next Checks
1. **Ablate geometric verification**: Disable ORB/SIFT reranking while keeping all other components fixed; measure change in Recall@1 and CIDEr to isolate verification contribution
2. **Vary chunk window size**: Test 2-sentence vs. 4-sentence sliding windows on OpenEvents to identify optimal context granularity for news articles
3. **Prompt perturbation study**: At inference, systematically remove or alter prompt constraints (70/30 ratio, WHO/WHAT/WHY/WHERE structure) to measure sensitivity to training-time prompt engineering