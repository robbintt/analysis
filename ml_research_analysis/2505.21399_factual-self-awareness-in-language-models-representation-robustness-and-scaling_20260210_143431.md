---
ver: rpa2
title: 'Factual Self-Awareness in Language Models: Representation, Robustness, and
  Scaling'
arxiv_id: '2505.21399'
source_url: https://arxiv.org/abs/2505.21399
tags:
- entity
- known
- accuracy
- forgotten
- gemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether language models encode an internal
  signal indicating their ability to recall factual information at the time of generation,
  rather than after the fact. The authors construct a factual recall dataset covering
  football players, movies, cities, and songs, and annotate samples as "known" or
  "forgotten" based on whether the model's top-k predictions include the correct tokens.
---

# Factual Self-Awareness in Language Models: Representation, Robustability, and Scaling

## Quick Facts
- **arXiv ID**: 2505.21399
- **Source URL**: https://arxiv.org/abs/2505.21399
- **Reference count**: 40
- **Primary result**: Models encode a linear signal in intermediate layers indicating correct vs incorrect factual recall at generation time.

## Executive Summary
This work investigates whether language models maintain an internal signal indicating their ability to recall factual information during generation, rather than only after the fact. The authors construct a factual recall dataset across four domains and annotate samples as "known" or "forgotten" based on whether correct tokens appear in top-k predictions. Using linear probes and sparse autoencoders, they demonstrate that models linearly separate correct from incorrect factual recall in intermediate layers (6-14), with the signal emerging early in training and being more pronounced in larger models. The findings suggest a form of factual self-awareness that could enable early hallucination detection.

## Method Summary
The authors create a factual recall dataset covering football players, movies, cities, and songs, then annotate samples based on whether the model's top-k predictions include correct tokens. They employ linear probes and sparse autoencoders to detect internal representations that distinguish correct from incorrect factual recall. The analysis spans multiple model families and sizes, examining representations at different layers and training stages. They test robustness to formatting changes and analyze scaling behavior across model sizes and training epochs.

## Key Results
- Linear probes achieve up to 31.1% accuracy improvement over random baseline in detecting correct vs incorrect factual recall
- Strongest internal correctness signal appears in intermediate layers (6-14) across model families
- Signal emerges early in training and is more pronounced in larger models, though not strictly proportional to parameter count
- Signal shows robustness to minor formatting changes but sensitivity to structural semantics

## Why This Works (Mechanism)
The work identifies an internal representation mechanism where language models maintain differentiable signals about their own factual recall accuracy during generation. This self-awareness manifests as linearly separable representations in intermediate transformer layers, suggesting the model's processing includes metacognitive components about factual knowledge state.

## Foundational Learning
- **Transformer architecture**: Why needed - to understand where self-awareness signals appear across layers; Quick check - verify intermediate layer prominence
- **Linear probe methodology**: Why needed - to test for linearly separable internal representations; Quick check - confirm probe performance exceeds random baseline
- **Top-k prediction analysis**: Why needed - to establish ground truth for "known" vs "forgotten" states; Quick check - validate annotation scheme captures genuine knowledge gaps
- **Sparse autoencoder interpretation**: Why needed - to provide complementary analysis beyond linear probes; Quick check - compare findings with probe results
- **Model scaling principles**: Why needed - to understand relationship between model size and self-awareness strength; Quick check - analyze correlation between parameters and accuracy gains
- **Factual recall task design**: Why needed - to create controlled environment for testing self-awareness; Quick check - verify dataset covers diverse domains and difficulty levels

## Architecture Onboarding

**Component map**: Dataset -> Annotation -> Probe Training -> Layer Analysis -> Scaling Experiments

**Critical path**: Factual recall generation → Top-k annotation → Linear probe training → Layer-wise representation analysis → Cross-model comparison

**Design tradeoffs**: 
- Linear vs nonlinear probes (simplicity vs potential performance)
- Static vs dynamic annotation (consistency vs capturing uncertainty)
- Layer selection range (coverage vs computational cost)
- Model diversity vs controlled comparison (breadth vs depth)

**Failure signatures**:
- Poor probe performance suggests weak or absent self-awareness signal
- Layer misalignment indicates domain-specific rather than general mechanism
- Inconsistent scaling patterns suggest confounding factors beyond model size
- Robustness failures reveal sensitivity to semantic rather than syntactic changes

**First experiments**:
1. Replicate probe experiments with deeper MLP architectures to test linear separability ceiling
2. Conduct semantic perturbation ablation studies to quantify robustness boundaries
3. Test intervention capability by using signal for early stopping in generation

## Open Questions the Paper Calls Out
None

## Limitations
- Annotation scheme conflates model uncertainty with genuine knowledge gaps through top-k coverage metric
- Linear separability findings lack ablation showing whether deeper probes would yield different results
- Robustness tests against formatting changes may not capture full spectrum of semantic perturbations
- Scaling analysis conflates model size with training duration, making it unclear whether improvements stem from architectural capacity or optimization dynamics

## Confidence

**High confidence**: The existence of intermediate-layer representations that linearly separate correct/incorrect factual recall is well-supported by probe performance metrics and cross-model consistency.

**Medium confidence**: Claims about the signal's emergence during training and its relative strength across model sizes are plausible but limited by the small set of checkpoints and models tested.

**Medium confidence**: The assertion that the signal could enable early hallucination detection is speculative, as the work does not demonstrate practical utility in real-world generation scenarios.

## Next Checks
1. Replicate the probe experiments using larger, more expressive architectures (e.g., MLPs with multiple hidden layers) to test whether linear separability is a ceiling rather than a floor for detection performance.
2. Conduct targeted ablation studies where factual recall is tested under controlled semantic perturbations (e.g., paraphrasing, entity substitution) to quantify the robustness boundary more precisely.
3. Test whether the internal correctness signal can be used to intervene during generation (e.g., early stopping or confidence-based re-ranking) and measure downstream impact on hallucination rates in open-ended generation tasks.