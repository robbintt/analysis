---
ver: rpa2
title: Evaluating Large Language Models for Evidence-Based Clinical Question Answering
arxiv_id: '2509.10843'
source_url: https://arxiv.org/abs/2509.10843
tags:
- evidence
- clinical
- accuracy
- question
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for answering
  clinical questions using evidence from systematic reviews and guidelines. A multi-source
  benchmark was created, including Cochrane reviews and AHA guidelines, with GPT-4o-mini
  and GPT-5 tested under different retrieval-augmented settings.
---

# Evaluating Large Language Models for Evidence-Based Clinical Question Answering

## Quick Facts
- arXiv ID: 2509.10843
- Source URL: https://arxiv.org/abs/2509.10843
- Reference count: 16
- Primary result: GPT-5 achieves 67.8% accuracy on systematic review questions, improving to 93.2% with gold abstract and 75.2% with top-3 PubMed abstracts

## Executive Summary
This study evaluates large language models for answering clinical questions using evidence from systematic reviews and guidelines. A multi-source benchmark was created, including Cochrane reviews and AHA guidelines, with GPT-4o-mini and GPT-5 tested under different retrieval-augmented settings. GPT-5 achieved 67.8% accuracy on systematic review questions, improving to 93.2% when provided the correct source abstract, and 75.2% with top-3 PubMed abstracts. GPT-4o-mini showed similar gains, rising from 60.3% to 79.9% with retrieved abstracts. Accuracy was highest (90%) on structured AHA guidelines and lowest (60-70%) on narrative sources. Model performance correlated with citation counts, suggesting reliance on well-cited evidence. Retrieval-augmented prompting notably improved factual accuracy, highlighting the importance of context and targeted retrieval for reliable clinical QA.

## Method Summary
The study evaluated GPT-4o-mini and GPT-5 on clinical QA tasks using a multi-source benchmark of Cochrane systematic reviews (8,533 abstracts), AHA guidelines (2,581 recommendations), and narrative guidelines (289 documents). Questions were generated via GPT-4o using structured prompts, and models were evaluated under four conditions: no context, gold abstract, top-3 PubMed abstracts, and random abstracts. Accuracy was measured as exact-match classification across three answer categories (Yes/No/No Evidence) with auxiliary labels for evidence quality and discrepancies.

## Key Results
- GPT-5 accuracy: 67.8% baseline → 93.2% with gold abstract → 75.2% with top-3 PubMed abstracts
- GPT-4o-mini accuracy: 60.3% baseline → 79.9% with top-3 PubMed abstracts
- Structured AHA guidelines yielded 90% accuracy vs. 60-70% for narrative sources
- Citation count correlation: each doubling increases correct answer odds by ~30%
- "No evidence" classification F1: 0.33 (GPT-5), 0.18 (GPT-4o-mini) - models under-call uncertainty

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Context Enables Knowledge Access Beyond Parametric Encoding
- **Claim:** Providing relevant source material to LLMs significantly improves clinical QA accuracy, with effect size dependent on retrieval precision.
- **Mechanism:** In-context learning allows models to access evidence that is not reliably encoded in model parameters, bypassing limitations of pretraining data coverage and recency.
- **Core assumption:** The retrieved context is semantically relevant and contains the information needed to answer the question correctly.
- **Evidence anchors:** "providing the gold-source abstract raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed abstracts...improves accuracy to 0.23"; Table 7 shows GPT-5 accuracy improved from 67.8% (baseline) to 93.2% with correct abstract, 75.2% with PubMed top-3; GPT-4o-mini improved from 60.3% to 79.9%.
- **Break condition:** Random abstracts reduced accuracy slightly (GPT-5: 65.1% vs 67.8% baseline), indicating noise tolerance limits; irrelevant context provides no benefit.

### Mechanism 2: Source Structure and Format Modulate Knowledge Access Reliability
- **Claim:** Highly structured evidence formats (AHA guidelines with explicit COR/LOE classifications) yield substantially higher accuracy than unstructured narrative text.
- **Mechanism:** Structured schemas align with patterns common in model training data; explicit categorical classifications reduce semantic ambiguity and reasoning burden.
- **Core assumption:** Models encode structured medical taxonomies and hierarchical evidence frameworks more reliably than free-text clinical reasoning.
- **Evidence anchors:** "accuracy is highest on structured guideline recommendations (90%) and lower on narrative guideline and systematic review questions (60–70%)"; GPT-4o-mini achieved 94.0% on AHA guidelines; errors clustered in LOE C-LD/C-EO and COR 2B categories (Figure 3).
- **Break condition:** Narrative guidelines with double negations or complex hedging cause systematic failures (31.6% accuracy on "No" answers; Table 6 example).

### Mechanism 3: Pretraining Exposure Distribution Shapes Parametric Knowledge Quality
- **Claim:** Evidence prominence (proxied by citation count) correlates with baseline model accuracy even without retrieval augmentation.
- **Mechanism:** Frequently cited evidence appears more often in pretraining corpora, leading to more reliable parametric encoding of findings from influential studies.
- **Core assumption:** Citation count serves as a reasonable proxy for pretraining corpus frequency and evidence visibility.
- **Evidence anchors:** "each doubling of citations is associated with roughly a 30% increase in the odds of a correct answer"; "For GPT-4o-mini, accuracy increased from about 50% for reviews with fewer than 10 citations to nearly 80% for those with more than 100 (p<0.001)".
- **Break condition:** Reviews published in 2025 show slight accuracy decline, consistent with training data temporal cutoff; publication year analysis showed no monotonic increase with age, suggesting citation—not recency—drives the effect.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) with Ranked Context**
  - **Why needed here:** The paper demonstrates RAG transforms clinical QA accuracy from ~60-70% to ~80-93%, but effect depends critically on retrieval quality (gold vs. top-3 vs. random).
  - **Quick check question:** If gold-source abstract improves accuracy to 93.2% but random abstracts yield 65.1% (near baseline), what does this imply about the margin for error in your retrieval pipeline?

- **Concept: Clinical Evidence Grading Hierarchies (COR/LOE)**
  - **Why needed here:** AHA guidelines use Class of Recommendation (I=strong, IIa=moderate, IIb=weak, III=harm) and Level of Evidence (A=high-quality RCTs, B=moderate, C=expert opinion). Model errors cluster in weak categories (C-LD, C-EO, IIb).
  - **Quick check question:** If a model achieves 94% accuracy overall but 81% of errors occur in LOE C-LD/C-EO categories, should you trust its predictions on rare diseases with limited trial evidence?

- **Concept: Citation Signal as Pretraining Exposure Proxy**
  - **Why needed here:** The 30% odds increase per citation doubling suggests parametric knowledge quality varies systematically with evidence prominence—critical for deployment decisions on rare conditions.
  - **Quick check question:** For a systematic review with 8 citations (low prominence) vs. 200+ citations, how should your confidence and retrieval strategy differ?

## Architecture Onboarding

- **Component map:** Cochrane reviews (8,533 abstracts) -> QA generation (GPT-4o) -> Structured questions (Yes/No/No Evidence) -> Model evaluation (GPT-4o-mini, GPT-5) -> Four conditions (no-context, gold abstract, top-3 PubMed, random) -> Accuracy assessment
- **Critical path:** Source curation and metadata extraction (DOI, PubMed ID, citation counts) -> QA generation with GPT-4o (structured prompts, constrained vocabularies) -> Baseline evaluation (no context; temperature=0.2 for GPT-4o-mini, reasoning_effort=medium for GPT-5) -> Context ablation on misclassified subset (gold abstract, top-3 PubMed, random) -> Stratified analysis (citation count bins, clinical domain, publication year)
- **Design tradeoffs:** Categorical outputs (Yes/No/No Evidence) enable scalable evaluation but may oversimplify clinical nuance; Cochrane/AHA focus ensures high methodological rigor but excludes emerging evidence and grey literature; citation count is an imperfect proxy; actual pretraining corpus composition is unknown
- **Failure signatures:** "No evidence" classification: F1=0.33 (GPT-5), F1=0.18 (GPT-4o-mini)—models under-call uncertainty; Discrepancy detection ("Yes" discrepancy): F1=0.13—models miss conflicting evidence between RCTs and observational studies; Double negation in narrative text causes affirmative bias (Table 6: "provides no added benefit" → model answers "Yes"); 2025 publications show slight accuracy decline, likely reflecting training data cutoff
- **First 3 experiments:**
  1. Replicate retrieval ablation on a low-performing specialty (e.g., Neonatal Care: 50.5% GPT-4o-mini baseline) to test whether top-3 PubMed retrieval closes the gap proportionally or reveals domain-specific retrieval challenges.
  2. Test citation-stratified performance on narrative guidelines to verify whether the citation effect generalizes beyond systematic reviews to unstructured evidence sources.
  3. Implement explicit uncertainty calibration prompting (require confidence scores or "insufficient evidence" reasoning) and measure F1 improvement on the "No evidence" and discrepancy detection tasks.

## Open Questions the Paper Calls Out

- **How does LLM accuracy shift when evaluating emerging clinical evidence or "grey literature" that has not yet been synthesized into systematic reviews?** The current benchmark is restricted to established, high-quality reviews, leaving the performance gap for newer, less formalized, or contradictory evidence unknown.

- **Does the restriction to categorical answer labels mask the model's ability to provide nuanced clinical justifications or detect subtle contradictions?** The study optimized for evaluation efficiency, so the trade-off between the validity of forced-choice answers and the utility of free-text clinical reasoning remains unquantified.

- **Is the strong correlation between citation count and model accuracy driven primarily by training data memorization or by the structural clarity of highly-cited research?** It is unclear if models fail on low-cited papers simply because they are absent from training data, or because they lack the reasoning capacity to interpret novel findings.

## Limitations

- Retrieval quality dependency: Substantial accuracy gains from retrieval augmentation critically depend on semantic relevance of retrieved abstracts, but retrieval methodology is not specified
- Citation count proxy validity: Assumes citation frequency accurately reflects pretraining corpus exposure, but actual pretraining data composition is unknown
- Temporal generalization uncertainty: 2025 publications show slight accuracy decline, but whether this represents systematic degradation or random variation is unclear

## Confidence

- **High Confidence:** Retrieval augmentation effectiveness with gold context (93.2% accuracy), structured vs. narrative source performance differences (90% vs 60-70%), and overall pattern of model errors clustering in low-evidence categories
- **Medium Confidence:** Citation count correlation with accuracy (30% odds increase per doubling) and general magnitude of retrieval-augmented improvements (60-70% → 80-93%), pending clarification of retrieval methodology
- **Low Confidence:** Extrapolation of citation-based findings to pretraining data distribution, and whether retrieval augmentation effects would hold with alternative retrieval models or in clinical subdomains not represented in the benchmark

## Next Checks

1. Replicate retrieval ablation on a clinical subdomain with systematically lower baseline accuracy (e.g., Neonatal Care at 50.5% for GPT-4o-mini) to determine whether top-3 PubMed retrieval closes the accuracy gap proportionally or reveals domain-specific retrieval challenges.

2. Verify whether the citation count correlation with accuracy extends to narrative guidelines, testing if citation prominence predicts performance across structured and unstructured evidence sources.

3. Implement explicit uncertainty prompting requiring confidence scores or "insufficient evidence" reasoning, then measure F1 improvement on the poorly performing "No evidence" classification (F1=0.33 for GPT-5) and discrepancy detection tasks (F1=0.13).