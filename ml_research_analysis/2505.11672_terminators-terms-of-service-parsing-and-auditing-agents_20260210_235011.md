---
ver: rpa2
title: 'Terminators: Terms of Service Parsing and Auditing Agents'
arxiv_id: '2505.11672'
source_url: https://arxiv.org/abs/2505.11672
tags:
- terms
- term
- source
- service
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Terminators, a modular agentic framework
  that uses large language models (LLMs) to parse and audit Terms of Service (ToS)
  documents. The approach breaks down the task into three interpretable steps: term
  extraction, verification, and accountability planning.'
---

# Terminators: Terms of Service Parsing and Auditing Agents

## Quick Facts
- arXiv ID: 2505.11672
- Source URL: https://arxiv.org/abs/2505.11672
- Reference count: 26
- LLM-based multi-agent framework parses ToS documents with reduced hallucinations and improved auditability

## Executive Summary
Terminators is a modular agentic framework that uses large language models to parse and audit Terms of Service documents through a three-step process: term extraction, verification, and accountability planning. The framework processes OpenAI's ToS document with GPT-4o, demonstrating that structuring the task into these steps reduces hallucinations and improves auditability. Experiments show that section-by-section parsing enhances term coverage and accuracy, while the verifier reliably flags unsupported terms. The approach supports real-world queries about data usage and compliance, offering a practical foundation for automated policy audits.

## Method Summary
The framework uses GPT-4o in three sequential agentic steps. First, the Term Parser extracts terms from the ToS with explicit source citations and applicability metadata. Second, the Term Verifier checks each extracted term against its cited source span, labeling it as Supported, Contradicted, or Unverifiable, and triggers re-parsing for non-Supported terms. Third, the Accountability Planner generates scenario-specific verification instructions conditioned on a user's concrete use case. The approach requires preprocessing the ToS with explicit line numbers and chunking the document into semantic sections before parsing.

## Key Results
- Section-by-section parsing improves term coverage and accuracy compared to whole-document prompting
- Verifier reliably flags unsupported terms, enabling corrective re-parsing to reduce hallucinations
- Framework generates actionable accountability checks when conditioned on realistic user scenarios

## Why This Works (Mechanism)

### Mechanism 1: Section-by-Section Parsing Reduces Information Loss
Processing ToS documents in smaller, semantically coherent chunks improves term extraction coverage compared to whole-document prompting. The parser extracts more terms when given individual paragraphs rather than the full document, aligning with findings that LLMs struggle with long-context retrieval.

### Mechanism 2: Source Citation Requirement Enables Verification and Correction
Requiring the parser to cite source locations allows a downstream verifier agent to detect hallucinations and trigger corrective re-parsing. The verifier cross-checks each term against its cited span, labeling it as Supported, Contradicted, or Unverifiable.

### Mechanism 3: Scenario Conditioning Improves Accountability Check Relevance
Conditioning the accountability planner on a concrete user scenario yields more actionable and context-appropriate verification instructions. The planner takes a user scenario description alongside verified terms, then generates accountability checks tailored to what that user could realistically observe or test.

## Foundational Learning

- **LLM Hallucination in Document Tasks**
  - Why needed here: The entire framework is structured to mitigate hallucination risks when extracting terms from legal documents
  - Quick check question: Can you explain why an LLM might generate a plausible-sounding term that isn't actually in the source document?

- **Chunking Strategies for Long Documents**
  - Why needed here: The parser's effectiveness depends on how the ToS is segmented before processing
  - Quick check question: What are the tradeoffs between fixed-length chunking vs. semantic/section-based chunking for legal documents?

- **Verification/Fact-Checking Agents**
  - Why needed here: The verifier agent is the quality gate that catches parser errors
  - Quick check question: What information must a verification agent have access to in order to assess whether a claim is supported by a source?

## Architecture Onboarding

- **Component map:**
```
[ToS Document] → [Chunking Layer] → [Parser Agent] → [Term + Source Output]
                                              ↓
                                    [Verifier Agent] ←→ [Re-parser if needed]
                                              ↓
                              [Verified Terms] → [Accountability Planner] → [Actionable Checks]
                                                              ↑
                                               [User Scenario Context]
```

- **Critical path:**
  1. Preprocess ToS with explicit line numbers
  2. Chunk by semantic sections (paragraphs work better than full document)
  3. Run parser per chunk with structured output format (term, source, applicable_to)
  4. Pass each extracted term to verifier with original ToS as reference
  5. For non-Supported terms, trigger re-parsing or discard
  6. Feed verified terms + user scenario to accountability planner

- **Design tradeoffs:**
  - Chunk size: Smaller chunks improve coverage but increase API calls and may fragment multi-sentence clauses
  - Verification strictness: "Unverifiable" is a conservative label; relaxing it may retain more terms at hallucination risk
  - Aspect filtering: Specifying target aspects focuses output but may miss unexpected but relevant terms

- **Failure signatures:**
  - Parser cites wrong line numbers → verifier returns "Unverifiable" → term discarded or re-parsed
  - Aspect prompt too vague → noisy overgeneralized terms
  - Scenario misalignment → accountability checks that are untestable or irrelevant to user context

- **First 3 experiments:**
  1. **Chunking ablation**: Run the parser on the same ToS using (a) full-document prompt, (b) fixed-size chunks, (c) paragraph-level chunks. Measure term count, source accuracy, and coverage of known clauses.
  2. **Line number sensitivity**: Compare parser source-citation accuracy with and without explicit line numbers in the input document. Use verifier disagreement rate as the metric.
  3. **Verifier precision check**: Manually annotate a sample of parser outputs as ground truth. Run verifier and compute precision/recall for Supported/Contradicted/Unverifiable labels.

## Open Questions the Paper Calls Out
1. How does the Terminators framework perform when applied to a diverse corpus of Terms of Service documents beyond the OpenAI case study?
2. What are the quantitative precision and recall rates of the term extraction and verification agents when evaluated against a human-annotated ground truth?
3. Can the verified terms be reliably organized into machine-interpretable structures like knowledge graphs to support automated reasoning?

## Limitations
- Performance validated only on a single ToS document (OpenAI's), limiting generalizability to other policy documents with different structures or legal language styles
- No quantitative comparison against baseline approaches (single-prompt vs. multi-agent) beyond qualitative observation of improved term coverage
- The accountability planner's outputs are not systematically evaluated for practical utility or user comprehension

## Confidence
- **High confidence**: Section-by-section parsing and source citation requirements are mechanistically beneficial
- **Medium confidence**: Verifier's reliability, as the paper demonstrates its function but lacks systematic evaluation against ground truth
- **Medium confidence**: Scenario-conditioned accountability planning, as the concept is well-motivated but not empirically validated

## Next Checks
1. **Multi-Document Generalization Test**: Apply the framework to ToS documents from 10+ different companies and measure term extraction coverage and source accuracy across domains
2. **Ablation Study on Verification**: Compare term quality and hallucination rates with and without the verification loop to quantify its impact on accuracy
3. **User Study on Accountability Checks**: Have real users attempt to verify generated accountability checks against actual ToS documents to assess practical utility and comprehension