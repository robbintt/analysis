---
ver: rpa2
title: 'ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection'
arxiv_id: '2511.05862'
source_url: https://arxiv.org/abs/2511.05862
tags:
- anomaly
- logs
- target
- system
- zerolog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of log-based anomaly detection
  when no labeled logs are available in the target system. Existing methods require
  labeled target logs for transfer learning or meta-learning, limiting their applicability
  to cold-start scenarios.
---

# ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection

## Quick Facts
- arXiv ID: 2511.05862
- Source URL: https://arxiv.org/abs/2511.05862
- Reference count: 40
- Achieves >80% F1-score in zero-label settings across three public log datasets

## Executive Summary
ZeroLog addresses the challenge of log-based anomaly detection when no labeled logs are available in the target system. Unlike existing methods that require labeled target logs for transfer learning or meta-learning, ZeroLog eliminates this requirement through a novel combination of unsupervised domain adaptation and meta-learning. The method learns system-agnostic feature representations that can generalize to new systems without any target labels, achieving comparable performance to state-of-the-art methods that use labeled target data. ZeroLog demonstrates strong cross-system generalization capabilities while maintaining performance on source systems.

## Method Summary
ZeroLog combines unsupervised domain adaptation with meta-learning to learn system-agnostic feature representations for log-based anomaly detection. The method uses adversarial training between source and target domains to extract domain-invariant features, then applies meta-learning to generalize these representations to the target system without labels. The approach consists of three modules: log embedding using semantic vectors, system-agnostic representation meta-learning, and a ZeroLog network architecture with GRU and attention mechanisms. Meta-tasks sample support sets with labeled source and unlabeled target data, and query sets for evaluation. The feature extractor is optimized via inner-loop gradient descent on combined classification and adversarial losses, then meta-optimized on query sets.

## Key Results
- Achieves over 80% F1-score in zero-label settings across HDFS, BGL, and OpenStack datasets
- Improves F1-score by up to 33% compared to transfer learning baselines
- Improves F1-score by up to 84% compared to meta-learning baselines without target labels
- Demonstrates strong generalization capabilities while maintaining source system performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial domain adaptation extracts system-agnostic feature representations by forcing domain confusion.
- **Mechanism:** A domain classifier is trained to distinguish source from target domain features while the feature extractor is optimized adversarially via gradient reversal to produce indistinguishable features between domains.
- **Core assumption:** Source and target systems share latent common structures despite surface distribution shifts.
- **Evidence anchors:** Abstract mentions adversarial training between source and target domains to learn system-agnostic general feature representations; Section III-B describes domain classifier purpose and adversarial training; related work LogAction uses active domain adaptation.
- **Break condition:** If source and target domains have no shared anomaly semantics, adversarial alignment will not help and may degrade performance.

### Mechanism 2
- **Claim:** Meta-learning enables fast adaptation to target systems without labels by learning a generalizable feature extractor initialization.
- **Mechanism:** Meta-tasks sample support sets and query sets; feature extractor adapts via inner-loop gradient descent on combined classification and adversarial loss, then meta-optimizes on query set.
- **Core assumption:** Learned initialization transfers to unseen target systems; inner-loop adaptation on unlabeled target data provides sufficient signal for generalization.
- **Evidence anchors:** Abstract mentions meta-learning generalizes representations to target system without labels; Section III-B describes meta-optimization goal; related work suggests meta-learning progression.
- **Break condition:** If meta-tasks are not representative of target system distributions or adaptation requires more than a few gradient steps, generalization will fail.

### Mechanism 3
- **Claim:** Joint optimization of classification loss and domain adversarial loss balances discriminative and transferable features.
- **Mechanism:** Meta-task loss combines source classification and domain alignment; hyperparameters β and γ control trade-off between adaptation and classification performance.
- **Core assumption:** There exists a Pareto-optimal balance between discriminative power and domain invariance.
- **Evidence anchors:** Section III-B mentions hyperparameters control trade-off; Section IV-C parameter sensitivity studies show F1 degrades when β or γ are too large/small.
- **Break condition:** If source classification signals are weak or domain shift is extreme, no hyperparameter setting may yield both transferable and discriminative features.

## Foundational Learning

- **Concept: Unsupervised Domain Adaptation (UDA)**
  - **Why needed here:** ZeroLog relies on UDA to align source and target domains without target labels.
  - **Quick check question:** Can you explain why the domain classifier is trained to maximize domain distinction while the feature extractor tries to minimize it?

- **Concept: Model-Agnostic Meta-Learning (MAML)**
  - **Why needed here:** ZeroLog's meta-learning draws from MAML-style bi-level optimization.
  - **Quick check question:** In meta-learning, what is the difference between the support set and the query set, and why is the outer loop optimized on the query set?

- **Concept: Sequence Modeling with GRU + Attention**
  - **Why needed here:** ZeroLog network uses GRU for sequential dependencies and attention for event importance weighting.
  - **Quick check question:** How does an attention mask help the model focus on specific log events within a sequence?

## Architecture Onboarding

- **Component map:**
  Drain parser -> Parsed log events -> Semantic embedding (300-dim GloVe vectors) -> Feature extractor (GRU + Attention) -> Anomaly classifier + Domain classifier

- **Critical path:**
  1. Parse logs with Drain to extract event templates
  2. Generate semantic embeddings for events using pre-trained word vectors
  3. Construct meta-tasks: sample support (labeled source, unlabeled target) and query sets
  4. Train domain classifier to maximize domain distinction and anomaly classifier to minimize BCE loss
  5. Inner-loop adaptation: update feature extractor on support set using combined loss
  6. Outer-loop meta-optimization: update feature extractor on query set
  7. Inference: pass target sequences through feature extractor to anomaly classifier

- **Design tradeoffs:**
  - β (domain adversarial weight): Higher β improves domain alignment but may reduce anomaly discrimination; start with β=2.0
  - γ (classification weight): Higher γ improves anomaly detection but may hurt transfer; start with γ=2.5
  - Source/target data ratio: More source data improves feature learning; more target unlabeled data improves domain alignment; test with 80-100% of both
  - Meta-learning rates: α=1.0 (meta-step size), δ=3e-3 (inner-loop); control adaptation speed vs. stability

- **Failure signatures:**
  - Domain classifier accuracy near 50%: domain alignment succeeded; if anomaly F1 is low, check classification loss
  - Domain classifier accuracy near 100%: domain alignment failed; increase β or check target data distribution
  - Meta-training loss does not decrease: check meta-task sampling, learning rates, or gradient flow through feature extractor
  - High precision, low recall on target: model is too conservative; consider adjusting anomaly classifier threshold or γ

- **First 3 experiments:**
  1. Baseline sanity check: Train ZeroLog on HDFS→BGL with β=γ=1.0; verify F1 > 70%
  2. Ablation study: Remove meta-learning entirely; expect significant F1 drop (Table III shows ~53-66 F1)
  3. Hyperparameter sweep: Fix γ=2.5, sweep β ∈ {1.0, 2.0, 3.0, 4.0, 5.0} on HDFS→BGL; plot F1 vs. β

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does ZeroLog generalize to log datasets from systems with fundamentally different architectures, such as microservices, IoT devices, or real-time streaming platforms?
- Basis in paper: The conclusion states "In future research, we will further test the zero-label generalization performance of ZeroLog on more types of log datasets and explore its practical applicability."
- Why unresolved: Experiments only cover three datasets (HDFS, BGL, OpenStack) from traditional distributed computing or cloud infrastructure contexts.
- What evidence would resolve it: Evaluation on datasets from additional system types demonstrating comparable F1-scores.

### Open Question 2
- Question: Can ZeroLog extend to true zero-shot generalization where the target system has no unlabeled logs available during training?
- Basis in paper: The conclusion states "Going further, our ultimate goal is to address the most challenging zero-shot generalization scenarios."
- Why unresolved: ZeroLog currently requires unlabeled target logs for adversarial domain adaptation; zero-shot would need domain-invariant features learned without any target data access.
- What evidence would resolve it: A modified ZeroLog variant achieving >70% F1-score on a held-out target system with zero exposure to its logs during training.

### Open Question 3
- Question: What is the relative contribution of unsupervised domain adaptation versus meta-learning to ZeroLog's zero-label performance?
- Basis in paper: The ablation study only removes meta-learning entirely, showing degraded performance, but does not isolate the UDA component's contribution.
- Why unresolved: Both components are trained jointly; it remains unclear whether domain-invariant features from UDA or the meta-optimization procedure drives most gains.
- What evidence would resolve it: Ablation experiments with UDA disabled but meta-learning retained, and vice versa, across all four transfer scenarios.

## Limitations

- The adversarial domain adaptation mechanism assumes sufficient semantic overlap between source and target systems, but lacks validation of performance degradation when systems have fundamentally different failure modes.
- Meta-learning hyperparameters were tuned on specific dataset pairs but may not generalize to other system combinations, with performance drops outside narrow parameter ranges.
- The claim of "zero-label" applicability is limited by the requirement for labeled source data and the assumption that meta-tasks adequately represent target system characteristics.

## Confidence

- High confidence: The experimental methodology and results showing >80% F1 on zero-label settings are well-documented and reproducible with specified datasets.
- Medium confidence: The mechanism of adversarial domain adaptation for log anomaly detection is theoretically sound but lacks independent validation beyond the proposed framework.
- Medium confidence: The meta-learning approach for zero-label generalization is plausible given related work, but specific implementation details are underspecified.

## Next Checks

1. **Domain Shift Robustness:** Test ZeroLog on system pairs with minimal semantic overlap to quantify adversarial alignment's effectiveness boundaries.
2. **Meta-Task Distribution Sensitivity:** Systematically vary the meta-task sampling distribution to identify whether performance correlates with task diversity or similarity to target systems.
3. **Real-World Cold-Start Evaluation:** Deploy ZeroLog on a new, unseen log dataset without any target labels and measure F1-score degradation compared to controlled experimental setting.