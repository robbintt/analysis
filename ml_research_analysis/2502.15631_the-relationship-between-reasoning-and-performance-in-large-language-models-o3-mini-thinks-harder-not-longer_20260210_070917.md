---
ver: rpa2
title: The Relationship Between Reasoning and Performance in Large Language Models
  -- o3 (mini) Thinks Harder, Not Longer
arxiv_id: '2502.15631'
source_url: https://arxiv.org/abs/2502.15631
tags:
- reasoning
- o3-mini
- arxiv
- accuracy
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether higher-performing reasoning models
  generate longer reasoning chains to achieve better accuracy. By analyzing OpenAI's
  o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH benchmark, we find that o3-mini
  (m) achieves superior accuracy without using longer reasoning chains than o1-mini,
  suggesting more effective reasoning.
---

# The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer

## Quick Facts
- arXiv ID: 2502.15631
- Source URL: https://arxiv.org/abs/2502.15631
- Reference count: 40
- Key outcome: Higher-performing models achieve better accuracy without longer reasoning chains, and accuracy generally declines with reasoning chain length across all models.

## Executive Summary
This study investigates whether higher-performing reasoning models generate longer reasoning chains to achieve better accuracy. By analyzing OpenAI's o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH benchmark, we find that o3-mini (m) achieves superior accuracy without using longer reasoning chains than o1-mini, suggesting more effective reasoning. Across all models and compute settings, accuracy generally declines as reasoning chain length increases, even after controlling for question difficulty. This accuracy drop is significantly smaller in more proficient models, indicating they use test-time compute more effectively. However, o3-mini (h) achieves only marginal accuracy gains over o3-mini (m) by allocating substantially more reasoning tokens across all problems, including those already solved by o3-mini (m).

## Method Summary
The study evaluates mathematical reasoning performance on Olympiad-level problems across OpenAI reasoning models (gpt-4o, o1-mini, o3-mini variants) using the Omni-MATH benchmark (4428 problems with metadata). Models are queried via OpenAI Batch API with a consistent prompt structure, and reasoning tokens are extracted from API responses. Accuracy is assessed using Omni-Judge, an automated evaluation model. The analysis employs logistic regression with Average Marginal Effects to quantify the relationship between token usage and accuracy while controlling for difficulty tier and domain. The study examines token distribution patterns, accuracy-token correlations, and conditional error rates across models and compute settings.

## Key Results
- o3-mini (m) achieves higher accuracy than o1-mini without generating longer reasoning chains
- Accuracy generally declines as reasoning chain length increases across all models and compute settings
- The accuracy drop with longer chains is significantly smaller in more proficient models
- o3-mini (h) achieves only marginal accuracy gains over o3-mini (m) despite substantially longer reasoning chains

## Why This Works (Mechanism)
The study reveals that more capable reasoning models achieve better performance through more effective reasoning strategies rather than simply generating longer chains. This suggests that these models have learned to allocate test-time compute more efficiently, focusing on productive reasoning paths while avoiding error accumulation in extended chains. The finding that accuracy declines with longer reasoning chains across all models indicates a fundamental challenge in maintaining correctness over extended reasoning sequences, which more proficient models have partially overcome through better reasoning strategies rather than brute-force token generation.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The paper's core analysis measures reasoning via CoT token counts; understanding CoT is essential to interpret token-accuracy relationships.
  - Quick check question: Can you explain how CoT differs from standard prompting in LLMs?

- Concept: Test-Time Compute Scaling
  - Why needed here: The study evaluates models that allocate variable compute at inference; knowing how test-time scaling works clarifies why token usage varies.
  - Quick check question: What is the difference between training-time and test-time compute scaling in LLMs?

- Concept: Logistic Regression for Effect Size Estimation
  - Why needed here: The paper uses logistic regression with Average Marginal Effects to quantify the token-accuracy relationship; basic familiarity aids critical reading.
  - Quick check question: Why might Average Marginal Effects be preferred over raw logistic regression coefficients for interpretation?

## Architecture Onboarding

- Component map: Models evaluated (gpt-4o, o1-mini, o3-mini (m), o3-mini (h)) → Omni-MATH benchmark (problems, metadata) → Inference via OpenAI Batch API → Omni-Judge model for answer correction → Token and accuracy analysis (histograms, regression).

- Critical path: Ensure prompt consistency, token limit settings, and automated judge validation; errors in judge alignment or token counting propagate to all downstream analyses.

- Design tradeoffs: Using Omni-Judge (model-based evaluation) enables scale but introduces potential divergence from human judgment; setting max_completion_tokens limits inference cost but may truncate reasoning.

- Failure signatures: If Omni-Judge parsing fails, accuracy metrics exclude those samples; if token limits are too low, models may not complete reasoning, biasing results toward shorter chains.

- First 3 experiments:
  1. Replicate the token distribution comparison between o1-mini and o3-mini (m) on a subset of Omni-MATH, confirming similar distributions for correctly answered problems.
  2. Stratify accuracy vs. token usage by difficulty tier to verify that the negative correlation persists within tiers.
  3. Run a controlled test with reduced max_completion_tokens on o1-mini to observe if accuracy degradation accelerates compared to o3-mini (m).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed accuracy decline with longer reasoning chains stem from models disproportionately allocating tokens to unsolvable problems, or from an intrinsic error accumulation in extended reasoning sequences?
- Basis in paper: [explicit] The authors state: "A possible hypothesis for this accuracy drop is that models tend to reason more on problems they cannot solve. Another possibility is that longer reasoning chains inherently have a higher probability of leading to a wrong final solution, highlighting the need for mathematical benchmarks with reference reasoning templates."
- Why unresolved: The current study cannot disentangle these two mechanisms because it lacks reference reasoning templates that would enable analysis of where errors accumulate.
- What evidence would resolve it: Benchmarks with annotated reference reasoning chains would allow comparing model reasoning paths to gold-standard reasoning at each step, isolating whether errors cluster at specific reasoning depths.

### Open Question 2
- Question: Does the finding that stronger models achieve higher accuracy without longer reasoning chains generalize beyond the OpenAI o-series to other reasoning model families (e.g., DeepSeek-R1, rStar-Math)?
- Basis in paper: [inferred] The study exclusively analyzes OpenAI models (o1-mini, o3-mini variants), and the authors acknowledge their "prompting strategy employed here may not generalize to alternative approaches." The generalization across model families remains untested.
- Why unresolved: Different training procedures and architectures may exhibit different relationships between capability and reasoning efficiency.
- What evidence would resolve it: Replicating the same token-accuracy analysis on identical benchmark problems across diverse reasoning model families would establish whether "thinking harder, not longer" is a general principle or model-family-specific.

### Open Question 3
- Question: How does the interaction between prompting strategies and test-time compute scaling affect the token-accuracy relationship in reasoning models?
- Basis in paper: [explicit] The authors note: "our prompting strategy employed here may not generalize to alternative approaches or more constrained prompt settings, and their interaction with test-time compute warrants further investigation."
- Why unresolved: The study uses a single vanilla prompt format; the effect of prompt engineering on reasoning token efficiency remains unexplored.
- What evidence would resolve it: A systematic comparison of token-accuracy relationships across multiple prompting strategies (zero-shot, few-shot, tree-of-thought, constrained formats) on identical model-benchmark combinations would quantify prompting effects.

### Open Question 4
- Question: To what extent do Omni-Judge evaluations diverge from human evaluations for o3-mini model outputs?
- Basis in paper: [explicit] The authors state: "Omni-Judge has only been validated for data leakage checks on o1-mini; extending these checks to o3-mini remains future work, though we assume minimal overlap."
- Why unresolved: The automated evaluator's validity for newer model generations is assumed but not empirically verified, introducing potential measurement error.
- What evidence would resolve it: A human annotation study on a sample of o3-mini responses comparing Omni-Judge verdicts against human expert judgments would quantify any systematic divergence.

## Limitations
- Judge reliability concerns: Omni-Judge validation was only performed for o1-mini models, not o3-mini variants, creating uncertainty about evaluation accuracy.
- Token usage measurement limitations: The analysis depends on token limit settings (25K vs 100K) that may influence observed reasoning chain lengths and truncate genuine reasoning.
- Domain-specific statistical power: Several domains have limited sample sizes, potentially affecting the reliability of domain-specific effect estimates.

## Confidence
- High Confidence: General Token-Accuracy Negative Correlation
- Medium Confidence: o3-mini (m) More Efficient Than o1-mini
- Low Confidence: o3-mini (h) Marginal Gains Represent Inefficient Scaling

## Next Checks
- Check 1: Judge Reliability Cross-Validation - Replicate accuracy analysis using human-annotated subset to verify differences persist under human judgment.
- Check 2: Token Limit Sensitivity Analysis - Run o3-mini-m with varying max_completion_tokens to test if efficiency is intrinsic or constraint-driven.
- Check 3: Qualitative Reasoning Pattern Analysis - Manually examine stratified samples where o3-mini-h uses more tokens but achieves similar accuracy to identify verbose versus deep reasoning.