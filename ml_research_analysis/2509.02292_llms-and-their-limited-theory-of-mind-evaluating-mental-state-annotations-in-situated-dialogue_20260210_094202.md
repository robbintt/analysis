---
ver: rpa2
title: 'LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations
  in Situated Dialogue'
arxiv_id: '2509.02292'
source_url: https://arxiv.org/abs/2509.02292
tags:
- searcher
- belief
- dialogue
- llms
- beliefs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a two-step framework to evaluate large language
  models' (LLMs) ability to infer and track shared mental models (SMMs) in situated
  dialogue. LLMs were used both as annotators of belief, commitment, and goal states
  in the CReST corpus, and as discrepancy detectors when compared to human and ground-truth
  annotations.
---

# LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue

## Quick Facts
- **arXiv ID**: 2509.02292
- **Source URL**: https://arxiv.org/abs/2509.02292
- **Authors**: Katharine Kowalyshyn; Matthias Scheutz
- **Reference count**: 35
- **Primary result**: LLMs show surface-level Theory of Mind capabilities but struggle with spatial reasoning and grounding beliefs in environmental context.

## Executive Summary
This study introduces a two-step framework to evaluate large language models' ability to infer and track shared mental models in situated dialogue. The framework uses LLMs both as annotators of belief, commitment, and goal states, and as discrepancy detectors when compared to human and ground-truth annotations. Results show that while LLMs demonstrate some Theory of Mind capabilities, they systematically err in scenarios requiring spatial reasoning or disambiguation of prosodic cues, with Claude Sonnet 4 exhibiting the highest discrepancy rates while o3-mini and Gemma 8.5B performed more conservatively.

## Method Summary
The method employs a two-step framework: first, LLMs annotate belief, commitment, and goal states from situated dialogue using a structured JSON format; second, the same or different LLMs detect discrepancies between these annotations and ground truth. The evaluation framework identifies four discrepancy types (belief contradictions, omissions, unsupported beliefs, and false beliefs) and normalizes scores to assess annotation quality. The CReST corpus provided six dialogues with 1,142 utterances, with three annotation sets compared: LLM annotations (Claude Sonnet 4, o3-mini, Gemma 8.5B), naive human annotations (2 annotators), and ground truth annotations (2 annotators with video access).

## Key Results
- LLMs demonstrate surface-level Theory of Mind capabilities but struggle with spatial reasoning and grounding beliefs in environmental context
- Claude Sonnet 4 exhibited the highest discrepancy rates, whereas o3-mini and Gemma 8.5B performed more conservatively
- Naive human annotators produced fewer unsupported beliefs but comparable error rates in other categories
- The weighted discrepancy scoring system effectively differentiated model performance, with scores ranging from 0.104-0.229 for Claude to 0.770-1.000 for o3-mini

## Why This Works (Mechanism)

### Mechanism 1: Sequential SMM Annotation and Discrepancy Detection
LLMs can be operationalized to infer shared mental models (SMMs) and subsequently critique those inferences against ground truth when provided a structured taxonomy of errors. A primary LLM parses situated dialogue into structured belief/goal states (JSON), while a secondary LLM acts as a "discrepancy detector" comparing these states to ground truth labels. This decouples the generation of ToM inferences from their evaluation, assuming LLMs possess sufficient meta-cognitive capability to recognize and classify errors like "belief contradiction" vs. "unsupported belief."

### Mechanism 2: Linguistic Pattern Matching vs. Grounded Spatial Reasoning
LLMs fail to ground mental states in physical context when relying solely on text, defaulting to linguistic priors which results in "unsupported beliefs" or "hallucinations." The model predicts likely continuations based on semantic similarity in dialogue history rather than maintaining a dynamic world model. Without visual input, the LLM infers beliefs that are linguistically plausible but factually incorrect regarding the searcher's location.

### Mechanism 3: Severity-Weighted Coherence Scoring
A weighted normalization of error types allows for a single "coherence score" that reflects the functional risk of an LLM's ToM limitations in a team setting. The framework assigns arbitrary weights to discrepancy types (e.g., Belief Contradictions ranked more severe than Omissions), sums these, normalizes by dialogue length, and scales to [0,1]. This translates abstract annotation errors into a comparable performance metric.

## Foundational Learning

- **Concept: Shared Mental Models (SMMs)**
  - Why needed: This is the target variable the LLM is trying to predict. Without understanding that SMMs involve overlapping beliefs, goals, and commitments between team members, the annotation task is incomprehensible.
  - Quick check: How does a "Common Belief" differ from a "2nd order belief" (e.g., "Searcher believes Director believes")?

- **Concept: Theory of Mind (ToM) in AI**
  - Why needed: To interpret results not just as "text generation errors" but as failures in cognitive emulation. The paper frames annotation accuracy as a proxy for ToM capability.
  - Quick check: Why is a "False Belief" task considered a higher bar for ToM than a simple knowledge query?

- **Concept: Situated Dialogue & Grounding**
  - Why needed: The paper's core finding is that LLMs struggle here. You must understand that "situated" means language is dependent on a physical environment, and "grounding" connects symbols (words) to that reality.
  - Quick check: Why does removing access to the "ground truth" (video feed) specifically increase "Unsupported Beliefs" rather than just "Omissions"?

## Architecture Onboarding

- **Component map**: CReST Corpus (Utterance + Speaker + Timestamp) -> State Tracker (LLM) -> JSON of belief/goal states -> Ground Truth (Human-annotated) -> Evaluator (LLM) -> Scorer (Weighted discrepancy score)

- **Critical path**: The prompt engineering for the State Tracker is the highest leverage point. If the prompt does not strictly enforce valid JSON or specific verbs (e.g., "at", "near"), the downstream Evaluator will fail to parse comparisons.

- **Design tradeoffs**:
  - Gemma 8.5B (Conservative): Near-zero omissions, but higher risk of forcing an inference (False Belief). Good for extracting explicit info, bad for reading between lines.
  - Claude Sonnet 4 (Speculative): High recall of implied states, but massive "hallucination" rate (Unsupported Beliefs). Dangerous in safety-critical teaming.
  - o3-mini (Balanced): Higher omissions (lazy), but fewer critical errors.

- **Failure signatures**:
  - JSON Schema Drift: LLM invents new keys (e.g., "Searcher feels") breaking the parser
  - Modal Confusion: Annotator attributes the Searcher's visual knowledge to the Director who is remote
  - Looping: The State Tracker gets stuck repeating "no change" or the same belief for every turn

- **First 3 experiments**:
  1. Run a Zero-Shot Baseline: Use the provided prompt on a single CReST dialogue without examples to establish a baseline error rate for your chosen model.
  2. Weight Sensitivity Analysis: Re-score the results using extreme weights (e.g., womission = 10, others = 1) to see which model architecture is robust to different task priorities.
  3. Context Window Ablation: Truncate the dialogue history to only the last 3 turns to see if "Omissions" are caused by forgetting long-term goals or just local negligence.

## Open Questions the Paper Calls Out

### Open Question 1
How does the evaluation framework's accuracy generalize across more diverse dialogue contexts and larger model ensembles? The current study was constrained to only six dialogues from the CReST corpus and three specific LLMs due to cost and environmental considerations. Applying the discrepancy framework to other situated dialogue corpora and evaluating a broader range of state-of-the-art models would resolve this.

### Open Question 2
Can LLMs effectively intervene to correct human misrepresentations and reestablish team coherence in real-time? This study only assessed LLMs as passive annotators and discrepancy detectors; it did not evaluate their ability to act as active team participants. Empirical trials where LLMs attempt verbal interventions based on detected discrepancies, followed by measurements of team performance and coherence recovery, would resolve this.

### Open Question 3
Does integrating cognitively grounded architectures for spatial reasoning reduce the high rates of false and unsupported beliefs in LLMs? The paper identifies spatial reasoning as a specific failure mode causing discrepancies but does not test architectural solutions to this problem. Comparative evaluations of standard LLMs versus those augmented with symbolic spatial reasoners or embodied grounding modules on the discrepancy detection task would resolve this.

## Limitations
- The framework's reliance on a single LLM as both annotator and evaluator introduces potential circular reasoning
- Severity weights are arbitrary and may not reflect actual team performance impact in real-world scenarios
- Spatial reasoning limitations appear systematic but may be partially mitigated by providing multimodal input, which wasn't tested

## Confidence

- **High confidence**: LLM performance differentiation (Claude vs o3-mini vs Gemma), basic discrepancy classification framework, overall trend of ToM limitations in spatial reasoning
- **Medium confidence**: Weighted scoring methodology's practical utility, the specific severity rankings assigned to discrepancy types
- **Low confidence**: Whether the observed limitations represent fundamental Theory of Mind deficits versus architectural constraints that could be addressed with different prompting or model architectures

## Next Checks

1. Test the framework with multimodal models (text + video) to determine if spatial reasoning limitations persist when visual context is available
2. Conduct ablation studies varying the severity weights to identify which error types most impact downstream team performance metrics
3. Compare LLM discrepancy detection against human expert analysis on a subset of dialogues to establish the accuracy ceiling for automated evaluation