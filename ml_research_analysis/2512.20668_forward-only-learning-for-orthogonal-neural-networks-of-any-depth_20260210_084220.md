---
ver: rpa2
title: Forward Only Learning for Orthogonal Neural Networks of any Depth
arxiv_id: '2512.20668'
source_url: https://arxiv.org/abs/2512.20668
tags:
- foton
- pepita
- networks
- layers
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational and memory bottlenecks of
  backpropagation (BP) in training deep neural networks by introducing FOTON, a forward-only
  training algorithm. FOTON eliminates the need for backward passes or storing the
  computation graph, significantly reducing memory overhead.
---

# Forward Only Learning for Orthogonal Neural Networks of any Depth

## Quick Facts
- arXiv ID: 2512.20668
- Source URL: https://arxiv.org/abs/2512.20668
- Reference count: 40
- Primary result: FOTON matches backpropagation accuracy on shallow networks and scales to 50-layer MLPs where prior forward-only methods fail

## Executive Summary
This paper introduces FOTON, a forward-only training algorithm that eliminates backpropagation's memory overhead while maintaining competitive accuracy. By enforcing orthogonal weight constraints and using a two-pass forward architecture, FOTON computes gradient updates without storing the computation graph or performing backward passes. The method achieves exact gradient equivalence to backpropagation in linear orthogonal networks and provides valid descent directions in non-linear networks. Empirical results demonstrate that FOTON matches backpropagation on shallow fully connected networks, scales to 50-layer architectures where previous forward-only methods fail, and successfully trains convolutional networks for the first time in this framework.

## Method Summary
FOTON replaces backpropagation with a two-pass forward architecture under orthogonal weight constraints. The method computes standard activations in Pass 1, then computes error-modulated activations in Pass 2 using perturbed input x - Fe, where F is the error-projection matrix derived from current weights. The difference between these passes encodes gradient information, enabling local weight updates via δWℓ = -η(hℓ - h^err_ℓ)h^⊤_{ℓ-1}. Orthogonalization via the Björck algorithm maintains the constraint, and for convolutions, Block Convolution Orthogonal Parametrization (BCOP) constructs orthogonal kernels with transposed convolution for updates. The approach eliminates memory overhead from storing activations while maintaining competitive accuracy.

## Key Results
- FOTON achieves 98.5% accuracy on 3-layer MNIST MLPs, matching backpropagation and exceeding PEPITA's 97.9%
- Successfully trains 50-layer MLPs on CIFAR-10 (75.06% accuracy), where PEPITA fails beyond 3 layers
- First forward-only training of convolutional networks, achieving 90.69% on 2-layer MNIST convnets
- Maintains gradient cosine similarity >0.1 with backpropagation across all layers in non-linear networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal weight constraints enable forward-only passes to compute gradients equivalent to backpropagation in linear networks, and provide meaningful descent directions in non-linear networks.
- Mechanism: When weights are left-orthogonal (W^⊤_ℓ W_ℓ = I), the transpose operation becomes equivalent to the inverse. This allows the second forward pass to propagate error information "backwards" through the network using only forward computation. The update δh^FO_ℓ = h_ℓ - h^err_ℓ equals δh^BP_ℓ exactly in the linear orthogonal case, and approximates it (rescaled by λ^(2ℓ-L+1)) in non-linear networks.
- Core assumption: Weight matrices remain approximately orthogonal throughout training (enforced via Björck algorithm or similar).
- Evidence anchors:
  - [abstract] "FOTON operates under an orthogonal weight constraint, enabling exact gradient equivalence to backpropagation in the linear regime"
  - [Section 3.2.1] Shows derivation: δh^FO_ℓ = W^⊤_{ℓ+1} ··· W^⊤_L e = δh^BP_ℓ for orthogonal linear networks
  - [Section 4.3.2] Empirical validation: cosine similarity between FOTON and BP gradients remains >0.1 across all 10 layers in non-linear networks, vs. ~0 for PEPITA
  - [corpus] Weak direct validation; related work (PEPITA, Forward-Forward) lacks orthogonal constraint and fails beyond shallow depths
- Break condition: Orthogonality drifts significantly between re-orthogonalization steps (ablation shows 50+ layer networks collapse if orthogonalization interval exceeds ~50 steps)

### Mechanism 2
- Claim: A two-pass forward architecture (clean pass + error-modulated pass) replaces the backward pass entirely.
- Mechanism: Pass 1 computes standard activations h_ℓ = σ_ℓ(W_ℓ h_{ℓ-1}). Pass 2 computes modulated activations using perturbed input x - Fe, where F is the error-projection matrix and e = ∇_L L. The difference h_ℓ - h^err_ℓ encodes the gradient direction at layer ℓ, enabling local weight updates via δW_ℓ = -η(h_ℓ - h^err_ℓ)h^⊤_{ℓ-1}.
- Core assumption: The error-projection matrix F stays aligned with forward weights (F = W^⊤_1 ··· W^⊤_L), enabling coherent error transmission.
- Evidence anchors:
  - [abstract] "eliminates the backward pass and memory overhead of backpropagation"
  - [Section 3.2, Eq. 8-9] Formal update rules showing how two-pass difference replaces BP's δh^BP_ℓ
  - [corpus] Forward Target Propagation paper validates forward-only credit assignment concept but uses local losses rather than global error modulation
- Break condition: F matrix not updated frequently enough; ablation shows monotonically decreasing accuracy as F-update frequency drops

### Mechanism 3
- Claim: Block Convolution Orthogonal Parametrization (BCOP) extends FOTON to convolutional architectures while preserving orthogonality.
- Mechanism: Standard convolutions don't admit straightforward orthogonalization. BCOP constructs convolutional kernels as products of Householder transformations, ensuring the corresponding Toeplitz matrix is orthogonal. The convolutional update uses transposed convolution (adjoint operator): δk_ℓ = conv^★_{h_{ℓ-1}}(h_ℓ - h^err_ℓ).
- Core assumption: Average pooling (not max pooling) is used; max pooling's non-differentiability is incompatible with forward-only error modulation.
- Evidence anchors:
  - [Section 3.2.3] "FOTON's convolutional update is derived by mimicking backpropagation... exact weight gradient is given by applying the adjoint convolution"
  - [Table 3] FOTON achieves 90.69% on 2-layer conv MNIST vs. PEPITA failure; first forward-only training of multi-layer convnets
  - [corpus] No direct corpus validation for BCOP+FOTON combination; this is a novel contribution
- Break condition: Max pooling operations break gradient coherence; only average pooling currently supported

## Foundational Learning

- Concept: **Orthogonal matrices and their properties** (W^⊤ W = I, norm preservation, W^⊤ = W^-1)
  - Why needed here: Core to understanding why orthogonality enables forward passes to simulate backward passes
  - Quick check question: Why does orthogonality prevent vanishing/exploding gradients in deep networks?

- Concept: **Credit assignment problem in neural networks**
  - Why needed here: FOTON is fundamentally a solution to global credit assignment without backward propagation
  - Quick check question: How does backpropagation solve credit assignment, and what constraint does FOTON relax?

- Concept: **Transposed/adjoint convolution**
  - Why needed here: Convolutional FOTON requires understanding that convolutions have adjoint operations (implemented as `conv2d_input` gradients)
  - Quick check question: In PyTorch, what operation computes the gradient w.r.t. input for a conv layer?

## Architecture Onboarding

- Component map:
  Forward Pass 1 (standard feedforward) -> Error Projection Matrix F computation -> Forward Pass 2 (modulated) -> Orthogonalization Module (Björck) -> Update Computation -> Weight Update

- Critical path:
  1. Initialize weights with orthogonal constraint
  2. Compute F matrix from current weights
  3. Run two forward passes (can be parallelized)
  4. Compute layer-wise updates without storing computation graph
  5. Apply updates
  6. Re-orthogonalize weights (frequency depends on depth)

- Design tradeoffs:
  - Orthogonalization frequency vs. speed: Deep networks require frequent re-orthogonalization (~every 50 steps); shallow networks can use sparse updates
  - F-update frequency vs. accuracy: Per-step F updates are optimal but costly; per-epoch updates still work for shallow networks
  - Expressivity vs. stability: Orthogonal constraints limit representational capacity but enable depth scaling
  - Memory vs. computation: Eliminates activation storage but adds orthogonalization overhead

- Failure signatures:
  - Training collapse after N epochs: Orthogonalization interval too large for network depth
  - Near-zero cosine similarity with true gradient: F matrix not aligned (not updating frequently enough)
  - Conv layer divergence: Using max pooling instead of average pooling
  - Dying ReLU in very deep networks: Switch to Tanh activation for 50+ layers

- First 3 experiments:
  1. **Shallow MLP validation**: Train 2-3 layer MLP on MNIST with FOTON vs. PEPITA vs. BP; verify FOTON matches BP accuracy (target: ~98.5%)
  2. **Depth scaling test**: Train 10-layer and 50-layer MLPs on CIFAR-10; ablate orthogonalization frequency (1, 10, 50, 200 steps) to find collapse threshold
  3. **Convolutional extension**: Implement BCOP orthogonalization + transposed convolution update; train 2-layer conv net on MNIST (target: >90%)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can FOTON be adapted to effectively train modern architectures like ResNets and Transformers that rely on skip connections or attention mechanisms?
- **Basis in paper:** [explicit] The authors state in the Discussion that the ideas presented "readily extend to convolutional layers... opening the door to forward-only training of modern architectures, with a clear avenue being scaling to ResNets and Transformers."
- **Why unresolved:** The paper only validates FOTON on fully connected networks and simple convolutional networks (up to 2 layers). Architectures with complex topology (skip connections) or specific module requirements (attention layers usually requiring backward passes) have not been tested.
- **What evidence would resolve it:** Successful training of a ResNet or Vision Transformer on a standard benchmark (e.g., ImageNet) using FOTON, demonstrating that the orthogonal constraint and forward-only updates are compatible with these structural complexities.

### Open Question 2
- **Question:** Can a formal theoretical guarantee be established for gradient alignment in deep non-linear networks?
- **Basis in paper:** [explicit] The Discussion notes that while empirical results are positive, "a formal non-linear theory of gradient alignment also remains a clear direction for future work."
- **Why unresolved:** Theoretical equivalence to backpropagation is currently proven only for linear or piecewise-orthogonal (e.g., GroupSort) networks. The paper relies on a first-order approximation argument for non-linear cases (ReLU/Tanh), which lacks formal convergence guarantees.
- **What evidence would resolve it:** A formal proof bounding the deviation of FOTON's update direction from the true gradient in standard non-linear regimes, or empirical evidence showing consistent convergence rates matching theoretical bounds in deep non-linear settings.

### Open Question 3
- **Question:** How can non-differentiable but standard operations like max-pooling be integrated into the FOTON framework?
- **Basis in paper:** [explicit] The authors acknowledge in the Conclusion that "standard techniques like max-pooling would require a specific adaptation as a forward-only version is not straightforward to define."
- **Why unresolved:** Max-pooling is data-dependent and non-linear in a way that disrupts the norm-preserving orthogonal signal propagation required by FOTON. The paper was forced to substitute average pooling, which is often less performant in vision tasks.
- **What evidence would resolve it:** A derived update rule or modulation mechanism for max-pooling that fits the forward-only, orthogonal framework, demonstrated by improved accuracy on vision benchmarks (like CIFAR) compared to the current average-pooling baseline.

## Limitations

- Orthogonal weight constraints fundamentally limit representational capacity compared to unconstrained networks
- Requirement for average pooling (not max pooling) in convolutional networks is a significant architectural constraint
- Björck orthogonalization algorithm's computational overhead scales with network depth, potentially offsetting some memory efficiency gains in very deep networks

## Confidence

- **High confidence:** The theoretical mechanism for gradient equivalence in linear orthogonal networks (Mechanism 1) is mathematically sound and directly validated in the paper's ablation studies
- **Medium confidence:** The approximation quality for non-linear networks (cosine similarity >0.1) is empirically demonstrated but lacks theoretical bounds on approximation error as depth increases
- **Medium confidence:** The memory efficiency claims are valid for shallow networks but may diminish in very deep architectures due to frequent orthogonalization requirements

## Next Checks

1. **Orthogonality drift measurement:** Track Frobenius norm of (W^⊤ W - I) across training epochs for networks of varying depth to quantify how quickly orthogonality degrades and validate the claimed need for frequent re-orthogonalization

2. **Gradient approximation bounds:** Theoretically derive and experimentally validate the upper bound on gradient approximation error ||∇^FO - ∇^BP||/||∇^BP|| as a function of depth and non-linearity strength

3. **Memory complexity analysis:** Measure peak memory usage and training time per epoch for FOTON vs. BP across networks of increasing depth (1, 5, 10, 25, 50 layers) to quantify actual memory efficiency gains vs. computational overhead trade-offs