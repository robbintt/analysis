---
ver: rpa2
title: Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant
  Negative Data
arxiv_id: '2506.05721'
source_url: https://arxiv.org/abs/2506.05721
tags:
- loss
- class
- instances
- negative
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel loss function for multi-label classification
  (MLC) that explicitly models the likelihood of any class being present, aiming to
  address the challenge of abundant negative data. The proposed approach redefines
  standard MLC losses (BCE and Focal) by incorporating an "any-class presence likelihood,"
  formulated as a normalized weighted geometric mean of predicted class probabilities.
---

# Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data

## Quick Facts
- **arXiv ID**: 2506.05721
- **Source URL**: https://arxiv.org/abs/2506.05721
- **Reference count**: 40
- **Primary result**: Introduces any-class presence likelihood loss for MLC that improves F1 by 6.01, F2 by 8.06, and mAP by 3.11 on datasets with abundant negative data

## Executive Summary
This paper addresses the challenge of multi-label classification with abundant negative data by introducing a novel loss function that explicitly models the likelihood of any class being present. The proposed approach redefines standard MLC losses (BCE and Focal) by incorporating an "any-class presence likelihood," formulated as a normalized weighted geometric mean of predicted class probabilities. This formulation encourages the model to better distinguish positive instances from negatives. The method also includes a class-balanced reweighting scheme to handle label imbalance. Extensive experiments on three large-scale datasets (SewerML, modified COCO, and ChestX-ray14) with diverse networks demonstrate consistent improvements in MLC performance without additional parameters or computational complexity.

## Method Summary
The proposed method introduces an "any-class presence likelihood" computed as a normalized weighted geometric mean of predicted class probabilities. This likelihood is combined with standard per-class BCE or focal loss terms to create a new loss function. The method also extends class-balanced reweighting to explicitly handle negative instances by treating them as an additional pseudo-class. The overall approach encourages the model to maintain collective awareness of positive instance presence while improving discrimination between positive and negative examples. The loss can be applied to any MLC architecture without architectural modifications, requiring only the addition of the any-class probability computation and loss term.

## Key Results
- Achieves up to 6.01 percentage points improvement in F1 score compared to standard BCE loss
- Improves F2 score by up to 8.06 percentage points, indicating better recall for positive instances
- Increases mean average precision (mAP) by up to 3.11 percentage points
- Demonstrates consistent performance across three diverse datasets and multiple network architectures
- Ablation studies confirm effectiveness of λ regularization (optimal at 0.01-0.05) and class-balanced negative instance weighting

## Why This Works (Mechanism)

### Mechanism 1: Any-Class Presence Likelihood as Contrastive Signal
The proposed loss creates an explicit optimization objective that contrasts positive instances from negative ones by synthesizing a collective "any-class presence" probability. For each instance, the method computes $p_a$ using a normalized weighted geometric mean of all predicted class probabilities. During backpropagation, each output neuron receives gradient $(p_a - y_a)$ in addition to standard per-class gradients. For positive instances ($y_a=1$), all neurons are encouraged to contribute toward higher $p_a$; for negative instances ($y_a=0$), all neurons are pushed toward lower $p_a$. This geometric mean formulation maintains gradient flow even when individual probabilities are near zero.

### Mechanism 2: Absent-Class Regularization via λ Parameter
Introducing a small contribution from absent classes to the any-class presence probability in positive instances acts as controlled regularization, improving the model's ability to discriminate positive from negative instances. When $\lambda > 0$ for positive instances, absent class predictions influence $p_a$ calculation. This creates pressure for the model to keep absent-class probabilities lower even when computing the "presence" signal, forcing sharper distinction between present and absent classes. Small λ values (0.01-0.05) provide beneficial regularization without diluting the primary presence signal from actually-present classes.

### Mechanism 3: Class-Balanced Reweighting with Negative Instance Awareness
Extending class-balanced loss to explicitly weight negative instances prevents them from being ignored during reweighting, maintaining learning signal across the full label distribution. Standard class-balanced loss assigns zero weight to fully-negative instances. The method treats negative instances as an additional "class" with its own effective number $n_e^{neg}$ and weight $w_{cb}^{neg}$, ensuring negative instances contribute meaningfully to the loss. This approach improves the model's calibration on the negative-positive boundary without requiring architectural changes.

## Foundational Learning

- **Concept**: Multi-label classification (MLC) with sigmoid-based independent binary predictions
  - **Why needed here**: The entire method builds on MLC formulation where each class is predicted independently via sigmoid; understanding that MLC differs from multi-class (mutually exclusive) classification is essential for grasping why negative instances are problematic
  - **Quick check question**: Given 3 classes with predictions [0.8, 0.3, 0.1] and threshold 0.5, what labels would be assigned in MLC vs. multi-class classification?

- **Concept**: Geometric mean properties and gradient behavior
  - **Why needed here**: The method relies on geometric mean (rather than product or arithmetic mean) to aggregate probabilities; geometric mean maintains sensitivity when individual probabilities are small and provides stable gradients
  - **Quick check question**: Why would a simple product of probabilities $\prod_j p_j$ be problematic for optimization compared to geometric mean $(\prod_j p_j)^{1/M}$ when many probabilities are near zero?

- **Concept**: Class-balanced loss and effective number of samples
  - **Why needed here**: The paper extends class-balanced loss to handle negative instances; understanding how effective number $n_e = \frac{1-\beta}{1-\beta^{n}}$ reweights rare classes is necessary to follow the negative-instance extension
  - **Quick check question**: If class A has 100 positive instances and class B has 10, and β=0.999, which class receives higher weight in class-balanced loss, and why?

## Architecture Onboarding

- **Component map**: Standard MLC network (any backbone) producing M raw logits -> Sigmoid activation producing M class probabilities -> Any-class module (new) computing $p_a$ via normalized weighted geometric mean -> Loss combiner adding any-class term to standard BCE/focal loss

- **Critical path**:
  1. Forward pass: logits → sigmoid → class probabilities
  2. Any-class computation: $p_a = \frac{(\prod p_j^{w_j})^{1/\sum w_j}}{(\prod p_j^{w_j})^{1/\sum w_j} + (\prod (1-p_j)^{w_j})^{1/\sum w_j}}$
  3. Loss calculation: $J = \tilde{w}_{cb} \left[ \sum_j -\log(p_j^t) - \alpha \log(p_a^t) \right]$ (BCE variant)
  4. Backward pass: Standard per-class gradients + any-class gradient distributed to all neurons

- **Design tradeoffs**:
  - **α (any-class contribution)**: Paper sets α=1 for equal contribution; lower α reduces any-class influence but may under-utilize the contrastive signal
  - **λ (absent-class weight)**: 0.01-0.05 recommended for typical datasets; higher values (0.2) may help with extreme negatives (>80%) but risk diluting present-class signal
  - **β (class-balancing)**: Paper uses 0.9999; lower values reduce balancing strength
  - **Focal γ**: Set to 2; standard focal loss parameter for down-weighting easy examples

- **Failure signatures**:
  - **F1-Neg drops significantly (>5%)**: λ may be too low; model is over-optimizing for positive instances at expense of negative detection
  - **mAP improves but F1/F2 don't**: Classification thresholds may need adjustment; model is ranking correctly but threshold 0.5 is suboptimal
  - **No improvement over baseline**: Check that α>0 (any-class term is active) and negative instances exist in dataset; method is designed for negative-heavy datasets
  - **Training instability with ViT**: Learning rate may need reduction (paper uses 1e-4 for MaxViT on ChestX-ray14 vs. 1e-3 for other datasets)

- **First 3 experiments**:
  1. **Baseline comparison**: Train your chosen backbone with standard class-balanced BCE and with $J^{cb}_{any|bce}$ (α=1, λ=0.02, β=0.9999) on your dataset; compare F1, F2, mAP, and F1-Neg to verify improvement pattern matches paper
  2. **λ sensitivity**: On validation set, sweep λ ∈ {0, 0.01, 0.02, 0.05, 0.1, 0.2} while holding other parameters fixed; plot F1 and F1-Neg to find optimal balance point for your negative-instance ratio
  3. **Focal variant test**: If your dataset has long-tail label distribution, repeat experiments with focal variants ($J^{cb}_{any|focal}$, γ=2) to determine whether focal + any-class provides additional benefit over BCE + any-class

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the any-class presence likelihood effectively improve performance in multi-label classification tasks with weak or ambiguous annotations?
- **Open Question 2**: Is there a theoretical or heuristic relationship between the proportion of negative data in a dataset and the optimal value for the regularization parameter $\lambda$?
- **Open Question 3**: Does the any-class presence likelihood remain effective when applied to discrete classification heads rather than a shared multi-label output layer?

## Limitations
- Method relies on geometric mean aggregation, which may not be optimal across all MLC scenarios
- Effectiveness of λ regularization and class-balanced negative instance weighting may vary with dataset characteristics beyond evaluated benchmarks
- Paper does not address computational overhead for very large label spaces (>1000 classes)
- Potential sensitivity to label noise not thoroughly evaluated

## Confidence
- **High Confidence**: Performance improvements on evaluated datasets (6.01 F1, 8.06 F2, 3.11 mAP) and ablation study demonstrating λ sensitivity (0.01-0.05 optimal range)
- **Medium Confidence**: Generalization to extreme negative ratios (>80%) and long-tail distributions beyond SewerML and modified COCO
- **Medium Confidence**: Architectural agnosticism across TresNet-L, ViT-B16, and MaxViT-S for all MLC applications

## Next Checks
1. Test on a dataset with extreme negative ratio (>80%) using λ=0.2 as suggested in Appendix A.4 to verify performance maintenance
2. Evaluate on a long-tail MLC dataset (e.g., JFT-300M subset) with focal loss variants to confirm extended effectiveness
3. Assess computational overhead and gradient stability on a 1000+ class MLC benchmark to validate scalability claims