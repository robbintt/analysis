---
ver: rpa2
title: Exploring Federated Pruning for Large Language Models
arxiv_id: '2505.13547'
source_url: https://arxiv.org/abs/2505.13547
tags:
- pruning
- layer
- iterative
- one-shot
- comparison
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of pruning large language models
  (LLMs) in privacy-sensitive domains where access to public calibration data is restricted.
  The authors propose FedPrLLM, a federated pruning framework that allows multiple
  clients to collaboratively prune an LLM while preserving data privacy.
---

# Exploring Federated Pruning for Large Language Models

## Quick Facts
- **arXiv ID**: 2505.13547
- **Source URL**: https://arxiv.org/abs/2505.13547
- **Reference count**: 40
- **Primary result**: Layer comparison, no weight scaling, and one-shot pruning yield optimal federated LLM pruning

## Executive Summary
This paper introduces FedPrLLM, a federated pruning framework that enables multiple clients to collaboratively prune large language models while preserving data privacy. The method allows clients to compute local pruning masks based on their private calibration data and share only binary masks with a central server, which aggregates them to derive a final pruning mask for the global model. Through extensive experiments across six open-source LLMs, three sparsity ratios, and three datasets, the study evaluates three core design choices and identifies an optimal approach: one-shot pruning with layer comparison and no weight scaling.

## Method Summary
FedPrLLM operates by having each client compute a local pruning mask matrix using the Wanda method (importance = |W|·||X||₂ with row comparison) based on their private calibration data. Clients send only binary masks to the central server, which aggregates them via element-wise sum. The server then selects top-k values using layer comparison to create the final pruning mask. The method is evaluated on LLaMA 7B/13B/30B, LLaMA-2 7B/13B, and LLaMA-3 8B models using 128 C4 calibration samples split across 64 clients, with perplexity measured on WikiText2, C4, and PTB datasets.

## Key Results
- Layer comparison at the server outperforms row and column comparison despite its simplicity
- Weight scaling degrades performance rather than improving it, contrary to expectations
- One-shot pruning matches iterative pruning's effectiveness while being more communication-efficient
- The optimal configuration is one-shot pruning with layer comparison and no weight scaling

## Why This Works (Mechanism)
FedPrLLM works by leveraging the distributed nature of federated learning to aggregate local pruning decisions without sharing raw data. Each client computes importance scores based on their local data distribution, and the server combines these through majority voting at the layer level. This approach preserves privacy while capturing diverse pruning insights across clients. The element-wise sum aggregation followed by top-k selection creates a consensus mask that reflects the most important weights across all clients' perspectives.

## Foundational Learning
- **Wanda pruning**: Importance score calculation using |W|·||X||₂ - needed for local mask computation; quick check: verify importance scores correlate with weight magnitude
- **Federated learning aggregation**: Element-wise sum of binary masks - needed for privacy-preserving collaboration; quick check: ensure masks align dimensionally before aggregation
- **Top-k selection**: Choosing most important weights across layers - needed for final mask creation; quick check: validate k matches desired sparsity ratio
- **Layer-wise sparsity**: Applying different sparsity levels per layer - needed for balanced model compression; quick check: confirm total pruned weights match target sparsity
- **Unstructured pruning**: Individual weight removal rather than structured components - needed for fine-grained compression; quick check: verify masks are binary per weight
- **Perplexity evaluation**: Language model performance metric - needed for quantitative assessment; quick check: compare against baseline unpruned model

## Architecture Onboarding

**Component Map**: Clients -> Server -> Global Model

**Critical Path**: Client forward pass → Local mask computation → Mask transmission → Server aggregation → Top-k selection → Final mask application → Model evaluation

**Design Tradeoffs**: 
- Privacy vs. performance: Binary masks preserve privacy but may lose gradient information
- Communication cost vs. accuracy: One-shot reduces communication but may miss refinement opportunities
- Local computation vs. server complexity: Simple element-wise sum keeps server light but may not capture complex relationships

**Failure Signatures**:
- Column comparison yields exploded perplexity (>1000x worse) - occurs when local pruning comparison group conflicts with server aggregation
- Weight scaling degrades performance - occurs when applying (m-M̂)/m scaling to retained parameters
- Poor convergence - may indicate insufficient calibration data or extreme sparsity levels

**First Experiments**:
1. Verify mask alignment by checking dimensions match across all 64 clients before aggregation
2. Test tie-breaking behavior by introducing deliberate ties in importance scores
3. Compare layer vs row vs column aggregation using small subset of clients and models

## Open Questions the Paper Calls Out

**Open Question 1**: How does non-IID data distribution across clients affect the aggregation of pruning masks in FedPrLLM? The experimental setup assumes IID distribution, but real-world scenarios may have heterogeneous data distributions that could skew importance criteria.

**Open Question 2**: Do the optimal design choices hold when using calibration-free or Hessian-based local pruning methods? The current framework relies exclusively on Wanda pruning, and results may depend on its specific properties.

**Open Question 3**: Can FedPrLLM be adapted for structured pruning where clients must agree on removing entire architectural components? The current work focuses on unstructured pruning, leaving structured compression unexplored.

## Limitations
- Uses only 128 calibration samples total (2 per client), which may not represent realistic federated scenarios
- All experiments use GPT-2 tokenizer variants; results may not transfer to other tokenization schemes
- The aggregation mechanism lacks theoretical analysis of convergence properties

## Confidence
- **High confidence**: Layer comparison superiority, weight scaling harmfulness, one-shot vs iterative parity
- **Medium confidence**: Generalization across all sparsity levels, optimal k-selection mechanism
- **Low confidence**: Theoretical underpinnings for weight scaling failure, potential dataset-specific effects

## Next Checks
1. Test tie-breaking behavior by introducing deliberate ties in importance scores and verifying consistent selection
2. Replicate experiments using different tokenization schemes (e.g., SentencePiece) to assess tokenizer dependency
3. Scale up calibration samples per client (e.g., 10x) to evaluate performance under more realistic federated conditions