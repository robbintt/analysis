---
ver: rpa2
title: Logit Reweighting for Topic-Focused Summarization
arxiv_id: '2507.05235'
source_url: https://arxiv.org/abs/2507.05235
tags:
- summary
- topical
- scores
- quality
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating topic-focused
  summaries from language models without resource-intensive fine-tuning. The authors
  propose a lightweight method that directly reweights the logits of topic-relevant
  tokens during generation, enabling control over topical content while preserving
  summary quality.
---

# Logit Reweighting for Topic-Focused Summarization

## Quick Facts
- **arXiv ID:** 2507.05235
- **Source URL:** https://arxiv.org/abs/2507.05235
- **Reference count:** 8
- **Primary result:** Threshold Selection logit reweighting improves topical focus without compromising summary quality, offering a lightweight alternative to fine-tuning.

## Executive Summary
This paper introduces a lightweight method for controlling topical content in language model summaries without resource-intensive fine-tuning. The approach directly reweights logits of topic-relevant tokens during generation, using three techniques: Constant Shift (fixed additive bias), Factor Scaling (multiplicative amplification), and Threshold Selection (conditional boosting above probability threshold). Experiments on the NEWTS dataset with Gemma-2B and Llama-3-8B show that Threshold Selection particularly excels, improving topical focus while maintaining linguistic quality - a key advantage over other approaches that often sacrifice fluency for control. The method provides precise, resource-efficient control over thematic content in generated text.

## Method Summary
The method uses LDA topic models to identify topic-relevant vocabulary (top 25 words per topic), which is expanded into token variants through morphological processing. During generation, a custom HuggingFace LogitsProcessor intercepts and modifies token logits based on three strategies: Constant Shift adds a fixed value to topic token logits, Factor Scaling multiplies them by a scaling factor (direction depends on model's logit sign), and Threshold Selection boosts only tokens exceeding a probability threshold to the maximum logit plus an encouragement factor. The approach uses nucleus sampling (top-p=0.95, top-k=50) or beam search, with summary length fixed at 80-90 tokens.

## Key Results
- Threshold Selection method improves topical focus without compromising summary quality, unlike Constant Shift which degrades fluency
- The method effectively increases topical vocabulary usage in generated summaries across both Gemma-2B and Llama-3-8B models
- Threshold Selection shows robustness across models with different logit distributions, requiring less hyperparameter tuning than other approaches

## Why This Works (Mechanism)

### Mechanism 1: Probability Distribution Reshaping via Logit Offset
Adding a constant to topic-relevant token logits increases their sampling probability without altering relative ordering among those tokens. The softmax function is monotonic; adding a fixed value c to selected logits increases their probability mass while proportionally decreasing mass for unmodified tokens. This creates a controllable bias toward topic vocabulary. Break condition: If c exceeds ~10, summaries degrade into word salad as non-topic tokens become nearly impossible to sample.

### Mechanism 2: Relative Amplification via Multiplicative Scaling
Scaling logits by a factor α amplifies or attenuates token preferences proportionally to their original magnitude. For negative logits (Gemma), α < 1 brings logits closer to zero (more probable); for positive logits (Llama), α > 1 increases probability. Effect direction inverts if you don't account for logit sign; quality degrades faster than with Constant Shift for equivalent topical gains.

### Mechanism 3: Conditional Boosting via Threshold Gating
Selectively boosting only tokens that already exceed a probability threshold improves topic focus while preserving fluency by avoiding forced insertion of low-probability tokens. Tokens with baseline probability ≥ θ are boosted to max(logit) + β, making them the dominant candidates. This exploits the model's existing context-aware predictions rather than fighting them. Break condition: If θ is set too low (< 0.005), too many tokens get boosted, reducing selectivity.

## Foundational Learning

- **Logits and Softmax Probability**: Why needed - All three methods manipulate logits to change sampling probabilities. Without understanding that logits are unnormalized scores converted to probabilities via softmax, you can't reason about why additive vs. multiplicative changes differ. Quick check: If a token has logit 2.0 and you add 3.0, does its probability increase by exactly 3%? (Answer: No—softmax is nonlinear; probability depends on all logits.)

- **Autoregressive Token Sampling**: Why needed - Interventions occur at each generation step. Understanding that next-token prediction is conditioned on all prior tokens explains why threshold selection respects context while constant shift doesn't. Quick check: If you boost topic tokens at every step regardless of context, what failure mode would you expect? (Answer: Grammatical breakdown—the model can't insert topic words naturally into any syntactic position.)

- **Topic Models (LDA) and Bag-of-Words Representation**: Why needed - The method uses LDA-derived topic word lists to identify which tokens to boost. Understanding that LDA produces word distributions over topics explains why the approach relies on token matching rather than semantic similarity. Quick check: Why might stemming and lemmatization be necessary when mapping topic words to tokens? (Answer: A topic word "running" won't match token "run" without normalization; morphology varies.)

## Architecture Onboarding

- **Component map:** Source Article + Topic ID → LDA Topic Model → Top 25 Topic Words → Token Expander (stem, lemma, case variants) → LogitsProcessor (intercepts during generation) → Three Reweighting Strategies → Modified Logits → Softmax → Sampling → Generated Summary

- **Critical path:** 1) Identify topic-relevant tokens from LDA (top 25 words → expanded to 3-5 token variants each), 2) Register custom LogitsProcessor with HuggingFace generate(), 3) At each decoding step, reweight logits for tokens in the topic set, 4) Apply sampling parameters (top-p, top-k) to modified distribution

- **Design tradeoffs:** Threshold Selection vs. Constant Shift - Threshold is more robust across models (doesn't require tuning to logit scale) but may provide less fine-grained control at extreme settings. Beam Search vs. Sampling - Beam search amplifies reweighting effects (cumulative across beams) but costs ~4× compute. Topic Vocabulary Size - 25 words is a design choice; more words increase coverage but dilute specificity.

- **Failure signatures:** Fluency collapse - Summaries read like keyword lists (cause: Shift constant > 10 or threshold too low). No topical improvement - Scores unchanged (cause: Token expansion missed variants, or scaling factor direction wrong for model's logit sign). Model instruction leakage - Summary repeats prompt instructions (cause: Instruction-tuned models without logit intervention).

- **First 3 experiments:** 1) Validate token expansion: Generate summaries with no reweighting; manually check if topic words appear in source but not in output. 2) Calibrate Threshold Selection on 10 articles: Start with θ=0.01, β=1.0. Measure topical score vs. ROUGE-L. Adjust θ downward until topical score increases ~2× without ROUGE-L drop > 5%. 3) A/B test against prompt engineering baseline: Compare prompt-based topic direction vs. threshold reweighting. Expect: similar topical gains, but reweighting should show higher quality scores (no instruction leakage).

## Open Questions the Paper Calls Out

**Open Question 1:** Can a dynamic intervention strategy, which adjusts logit bias strength based on the syntactic role of the next predicted token (e.g., content vs. function words), outperform the static Threshold Selection method? Basis: Authors suggest "topical steering is unnecessary when the model is about to output function words... a more substantial bias toward topic-relevant alternatives could improve control." Unresolved because current Threshold Selection applies uniform logic based on probability thresholds, ignoring linguistic category.

**Open Question 2:** How does the efficacy of logit reweighting compare to intermediate activation steering methods for controlling topical focus in abstractive summarization? Basis: Authors identify "steering methods that intervene on intermediate model activations" as "promising lightweight approach" but note their efficacy for free-form text generation is "still largely untested." Unresolved because paper focuses exclusively on logit manipulation without benchmarking against activation-based interventions.

**Open Question 3:** Does the Threshold Selection method maintain its effectiveness on non-instruction-tuned base models, or does it rely on the alignment behaviors of instruction-tuned models? Basis: Authors note in Limitations that "Results were obtained with one 2B and one 8B parameter instruction-tuned language model, and behavior may differ for... base models." Unresolved because instruction-tuned models may adhere to prompt constraints differently than base models.

## Limitations

- Method relies on exact token matching from LDA topic words without semantic similarity consideration, creating issues with polysemy and out-of-vocabulary terms
- Evaluation uses LDA-derived topics rather than human judgments, creating potential circularity in validation
- Only tested on two models (Gemma-2B, Llama-3-8B) and one dataset, limiting generalizability claims
- Resource efficiency claims don't account for full lifecycle costs including LDA model training and preprocessing overhead

## Confidence

**High Confidence:** The basic mechanism of logit reweighting works as described - adding/subtracting values from logits does change sampling probabilities, and the softmax function is monotonic. The empirical observation that threshold selection outperforms constant shift for balancing topic focus and quality is well-supported by presented results.

**Medium Confidence:** The claim that threshold selection is "model-independent" and "parameter-free" is overstated. While the method shows robustness across Gemma and Llama, it still requires tuning of the threshold value θ and encouragement factor β. The assertion that this approach is "practical and resource-efficient" compared to fine-tuning needs more careful cost-benefit analysis including preprocessing overhead.

**Low Confidence:** Generalization claims to other domains, tasks, or model architectures lack empirical support. The method's behavior on multilingual text, longer documents, or non-news domains is unknown. Claims about avoiding "resource-intensive fine-tuning" don't account for the full lifecycle costs of maintaining topic models and preprocessing pipelines.

## Next Checks

**Validation Check 1: Semantic Generalization Test** - Evaluate the method on a dataset with human-annotated topic relevance (rather than LDA-derived topics) to verify that token-based boosting captures semantic topicality. Test whether synonym substitution in topic words affects performance, and measure false positive rates when topic words appear in unrelated contexts.

**Validation Check 2: Cross-Architecture Robustness** - Test the threshold selection method on models with different logit distributions (e.g., models that output logits centered around zero vs. those with different scaling). Systematically vary the encouragement factor β and threshold θ across at least five diverse architectures to identify when the "model-independent" claim breaks down.

**Validation Check 3: Resource Efficiency Analysis** - Quantify the total computational cost including LDA model training, topic word extraction, token expansion preprocessing, and hyperparameter tuning across all topics. Compare this to fine-tuning costs for equivalent performance across multiple topics, and analyze the break-even point in terms of number of topics and document volume where each approach becomes more efficient.