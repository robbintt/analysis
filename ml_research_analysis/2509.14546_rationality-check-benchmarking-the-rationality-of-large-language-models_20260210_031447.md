---
ver: rpa2
title: Rationality Check! Benchmarking the Rationality of Large Language Models
arxiv_id: '2509.14546'
source_url: https://arxiv.org/abs/2509.14546
tags:
- uni00000013
- uni00000011
- uni00000010
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first comprehensive benchmark for evaluating
  the rationality of large language models (LLMs), covering six domains: psychology,
  cognitive science, decision-making, economics, game theory, and collective rationality.
  The benchmark employs a wide range of questionnaires and tests adapted from human
  rationality studies, assessing both theoretical and practical aspects.'
---

# Rationality Check! Benchmarking the Rationality of Large Language Models

## Quick Facts
- arXiv ID: 2509.14546
- Source URL: https://arxiv.org/abs/2509.14546
- Reference count: 40
- Key outcome: First comprehensive benchmark evaluating LLM rationality across six domains shows advanced models outperform humans in collective rationality and cognitive tasks, with larger models and RLHF showing higher rationality scores

## Executive Summary
This paper introduces the first comprehensive benchmark for evaluating the rationality of large language models across six domains: psychology, cognitive science, decision-making, economics, game theory, and collective rationality. The benchmark adapts human rationality assessment tools to evaluate LLMs through questionnaires and tests, examining both theoretical understanding and practical application of rational principles. Experiments with 21 leading LLMs reveal that while advanced models generally outperform humans in collective rationality and many cognitive tasks, they show mixed results in economics and game theory domains. The study finds that larger models and those with reinforcement learning from human feedback tend to exhibit higher rationality scores, providing a foundational tool for developers to assess and improve LLM rationality.

## Method Summary
The researchers constructed a benchmark by adapting established human rationality assessment tools from academic literature across six domains. They selected questionnaires and tests that measure rational decision-making, cognitive biases, economic reasoning, strategic thinking, and collective behavior. The benchmark was applied to 21 leading LLMs, comparing their performance against human baselines where available. The evaluation included both multiple-choice and open-ended questions, with responses scored based on correctness and rational consistency. The study also examined correlations between model size, training methodology (particularly RLHF), and rationality scores across different domains.

## Key Results
- Advanced LLMs generally outperform humans in collective rationality and cognitive tasks
- Larger models and those with reinforcement learning from human feedback show higher rationality scores
- Performance varies significantly across domains, with mixed results in economics and game theory compared to strengths in collective rationality
- The benchmark provides a comprehensive toolkit for assessing LLM rationality across multiple dimensions

## Why This Works (Mechanism)
The benchmark works by applying established human rationality assessment tools to language models, assuming that rational decision-making principles are domain-general and transferable across cognitive architectures. By measuring how well models align with normative standards of rationality established in human psychology and economics, the benchmark provides quantitative metrics for comparing different models and tracking improvements over time.

## Foundational Learning
**Human Rationality Concepts**
- Why needed: Provides theoretical foundation for what constitutes "rational" behavior
- Quick check: Understanding cognitive biases and normative decision theory

**LLM Architecture Fundamentals**
- Why needed: Essential for interpreting model responses and limitations
- Quick check: Knowledge of transformer architecture and attention mechanisms

**Reinforcement Learning from Human Feedback**
- Why needed: Critical for understanding why RLHF models show higher rationality scores
- Quick check: Understanding alignment techniques and preference learning

## Architecture Onboarding

**Component Map**
Human Rationality Assessment Tools -> Benchmark Framework -> LLM Evaluation Pipeline -> Performance Analysis

**Critical Path**
Questionnaires/Tests Selection -> Adaptation for LLM format -> Model Response Generation -> Scoring Algorithm -> Comparative Analysis

**Design Tradeoffs**
The benchmark prioritizes comprehensiveness across domains over depth in any single area, choosing breadth of rationality coverage versus specialized assessment of particular rational capabilities.

**Failure Signatures**
- Models may appear rational through pattern matching rather than genuine reasoning
- Human-designed rationality tests may not capture AI-specific forms of rationality
- Benchmark results may reflect training data biases rather than inherent model capabilities

**3 First Experiments**
1. Test the same benchmark on multiple model sizes from the same family to verify scaling relationships
2. Apply the benchmark to models with different training objectives (RLHF vs non-RLHF) to isolate the effect of alignment
3. Compare model performance on closed-form questions versus open-ended reasoning tasks to identify response pattern differences

## Open Questions the Paper Calls Out
None

## Limitations
- Transferability of human rationality measures to AI systems remains theoretically uncertain
- Benchmark may not capture AI-specific forms of rationality or limitations
- Performance improvements may reflect pattern matching rather than genuine rational reasoning

## Confidence

**High Confidence**
- This is the first comprehensive benchmark for LLM rationality across multiple domains

**Medium Confidence**
- Larger models and RLHF models exhibit higher rationality scores
- Mixed performance across domains reflects genuine differences in model capabilities

## Next Checks
1. Conduct controlled human-AI comparison studies analyzing response patterns, not just final answers, to validate benchmark construct validity

2. Apply the benchmark to non-LLM AI systems to test generalizability of rationality measures across different AI architectures

3. Implement longitudinal testing of the same models over time to distinguish genuine rationality improvements from evaluation artifacts