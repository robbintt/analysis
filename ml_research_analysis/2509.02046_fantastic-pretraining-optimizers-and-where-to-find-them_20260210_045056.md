---
ver: rpa2
title: Fantastic Pretraining Optimizers and Where to Find Them
arxiv_id: '2509.02046'
source_url: https://arxiv.org/abs/2509.02046
tags:
- loss
- chinchilla
- hyperparameter
- data
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically benchmarks 11 optimizers for LLM pretraining\
  \ across 4 model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8\xD7\
  \ Chinchilla optimum). The authors identify two critical methodological shortcomings\
  \ in prior work: insufficient hyperparameter tuning and limited evaluation setups."
---

# Fantastic Pretraining Optimizers and Where to Find Them

## Quick Facts
- **arXiv ID**: 2509.02046
- **Source URL**: https://arxiv.org/abs/2509.02046
- **Reference count**: 40
- **Primary result**: Systematic benchmarking reveals claimed optimizer speedups are inflated by poor baselines and weak hyperparameter tuning

## Executive Summary
This paper provides the first systematic benchmarking of 11 optimizers for LLM pretraining across multiple model scales (0.1B-1.2B parameters) and data-to-model ratios. The authors identify two critical methodological flaws in prior work: insufficient hyperparameter tuning and narrow evaluation setups that artificially inflate speedup claims. Through rigorous coordinate-descent hyperparameter sweeps, they demonstrate that many claimed 1.4-2× speedups over AdamW largely reflect weak baselines rather than optimizer superiority. The actual speedups of matrix-based optimizers (Muon, Soap, Kron) over well-tuned AdamW are much smaller than claimed and decrease with model scale—from 1.4× for 0.1B parameter models to merely 1.1× for 1.2B parameter models.

## Method Summary
The authors conduct a comprehensive benchmarking study across 4 model scales (0.1B, 0.3B, 0.6B, and 1.2B parameters) and 4 data-to-model ratios (1×, 2×, 4×, and 8× Chinchilla optimum). They evaluate 11 optimizers including AdamW (scalar-based), Lion (scalar-based), and matrix-based optimizers (Muon, Soap, Kron, Muon-MoE, Kron-MoE). For each configuration, they perform coordinate-descent hyperparameter sweeps with 60-110 trials per setup, optimizing learning rate, beta values, and weight decay. The evaluation protocol tracks validation loss throughout training, measuring the number of steps to reach specific loss thresholds. The study uses the same 200B token dataset for all experiments, with data ratios determined by multiplying this fixed dataset size by the ratio values.

## Key Results
- Many claimed optimizer speedups (1.4-2× over AdamW) were inflated due to insufficient hyperparameter tuning and narrow evaluation setups
- Matrix-based optimizers (Muon, Soap, Kron) consistently outperform scalar-based ones (AdamW, Lion) for small models, but their advantage diminishes at larger scales
- The optimal optimizer choice shifts with data-to-model ratios, with Muon being overtaken by Soap and Kron at high ratios (8× Chinchilla)
- Early-stage loss curves can be misleading as rankings change during training due to learning rate decay

## Why This Works (Mechanism)
The diminishing advantage of matrix-based optimizers at larger scales likely stems from their increased computational overhead becoming less justified as models grow. Matrix preconditioning becomes more expensive relative to the parameter count, while the benefits may saturate due to the increased model capacity. The shifting optimizer rankings with data-to-model ratios suggest that different optimizers have varying sensitivities to data scarcity or abundance, possibly related to their adaptive learning rate mechanisms and how they handle gradient statistics across different training regimes.

## Foundational Learning
- **Coordinate descent optimization**: Iterative hyperparameter tuning where one parameter is optimized while others are held fixed; needed to efficiently explore high-dimensional hyperparameter spaces without combinatorial explosion; quick check: verify sweep convergence by examining parameter sensitivity patterns
- **Data-to-model ratio scaling**: The ratio of training tokens to model parameters relative to Chinchilla optimum; crucial for understanding how optimizer performance changes under different data constraints; quick check: compare loss curves across ratios to identify breaking points
- **Matrix preconditioning**: Using second-moment information to adapt learning rates per parameter; fundamental to understanding why matrix-based optimizers initially outperform scalar methods; quick check: analyze preconditioner conditioning across training stages
- **Learning rate decay scheduling**: Exponential decay of learning rate during training; essential for interpreting why early rankings differ from final ones; quick check: plot optimizer rankings against learning rate schedule
- **Validation loss tracking**: Monitoring model performance on held-out data throughout training; critical for fair optimizer comparison; quick check: verify loss plateau detection is consistent across runs
- **Small-scale proxy evaluation**: Using smaller models to approximate larger model behavior; necessary for tractable experimentation; quick check: verify scaling trends are monotonic across model sizes

## Architecture Onboarding
**Component map**: Data → Tokenizer → Model (Transformer) → Loss Function → Optimizer → Parameter Updates → Validation Loop

**Critical path**: Token generation → Embedding lookup → Multi-head attention → Feed-forward network → Layer normalization → Gradient computation → Optimizer update → Validation evaluation

**Design tradeoffs**: Matrix-based optimizers offer better adaptation but at 2-3× computational cost; scalar optimizers are faster but less adaptive; the choice depends on available compute budget versus convergence speed requirements

**Failure signatures**: Early convergence followed by plateau suggests poor learning rate scheduling; inconsistent rankings across training stages indicate optimizer sensitivity to decay schedules; widening performance gaps at small scales indicate matrix preconditioning benefits are scale-dependent

**First experiments**: 1) Replicate AdamW baseline with thorough hyperparameter tuning to establish proper reference point; 2) Run matrix-based optimizers with identical hyperparameter search space to ensure fair comparison; 3) Track validation loss at multiple training stages to capture ranking changes over time

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on relatively small models (0.1B-1.2B parameters), limiting generalizability to larger models where pretraining economics are more critical
- Experiments conducted on a single dataset type (standard language modeling), leaving open the possibility of task-dependent optimizer performance
- The 1.2B parameter upper bound may still be too small to observe asymptotic behavior of matrix-based optimizers

## Confidence
- **Inflated speedup claims due to poor baselines**: High confidence - well-supported by systematic hyperparameter sweeps
- **Diminishing matrix optimizer advantage at larger scales**: Medium confidence - observed trend but largest model may be insufficient
- **Changing optimizer rankings during training**: Medium confidence - supported by data but needs additional verification

## Next Checks
1. Extend experimental framework to 10B+ parameter models to verify whether diminishing advantages of matrix-based optimizers continue as predicted
2. Conduct experiments across diverse pretraining tasks (code generation, mathematical reasoning, multilingual modeling) to assess task-dependent optimizer performance
3. Perform ablation studies isolating effects of specific optimizer components (momentum scaling, weight decay coupling, preconditioning) to understand performance drivers