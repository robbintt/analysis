---
ver: rpa2
title: Does UMBRELA Work on Other LLMs?
arxiv_id: '2507.09483'
source_url: https://arxiv.org/abs/2507.09483
tags:
- umbrela
- relevance
- llms
- prompt
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reproduces the UMBRELA LLM-based relevance assessment
  framework across different LLM families to evaluate its generalizability. The study
  compares GPT-4o, DeepSeek V3, LLaMA-3.3-70B, LLaMA-3-8B, and FLAN-T5-large using
  the same UMBRELA prompt across TREC Deep Learning datasets.
---

# Does UMBRELA Work on Other LLMs?

## Quick Facts
- **arXiv ID:** 2507.09483
- **Source URL:** https://arxiv.org/abs/2507.09483
- **Reference count:** 13
- **Key outcome:** UMBRELA framework generalizes across LLM families, but per-label agreement requires larger models while rank correlation remains robust.

## Executive Summary
This paper evaluates the generalizability of the UMBRELA framework across different LLM families (GPT-4o, DeepSeek V3, LLaMA-3.3-70B, LLaMA-3-8B, FLAN-T5-large) using the same prompt across TREC Deep Learning datasets. While all models produce leaderboards closely matching human rankings, per-label agreement metrics vary significantly with model scale. Larger models demonstrate better consistency across datasets and are more reliable for detailed relevance judgments, with GPT-4o and DeepSeek V3 achieving the highest agreement (Cohen's Œ∫ ‚âà 0.3-0.5).

## Method Summary
The study reproduces UMBRELA's zero-shot relevance assessment framework across five different LLMs using the exact "Zero-shot Bing" prompt from the UMBRELA GitHub repository. Researchers run inference on TREC Deep Learning datasets (DL 2019, 2020, 2023) with temperature=0, implementing a regex-based parser to extract scores from model outputs. The evaluation compares Spearman's œÅ and Kendall's œÑ for leaderboard correlation against human rankings, and Cohen's Œ∫ for per-label agreement against human relevance labels. The study uses Together API for larger models and local Hugging Face inference for smaller ones.

## Key Results
- All models produce leaderboard rankings closely matching human assessments (Spearman's œÅ typically >0.9)
- Per-label agreement varies dramatically: GPT-4o and DeepSeek V3 achieve Œ∫ ‚âà 0.3-0.5, while smaller models show Œ∫ dropping to 0.06-0.2
- FLAN-T5-large shows only 0.06-0.18 scale Cohen's Œ∫ but maintains leaderboard correlations
- Model choice should align with evaluation goals: smaller models suffice for ranking systems, larger models needed for accurate document-level assessments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The UMBRELA framework's reliability is conditional on the LLM's capacity for strict instruction adherence, specifically regarding output formatting.
- **Mechanism:** The prompt requires a structured output ("##final score: [score]") without reasoning text. Larger models suppress auxiliary output, whereas smaller models often leak internal reasoning states or fail to aggregate scores.
- **Core assumption:** Degradation in smaller models is partially a failure of formatting compliance rather than purely relevance reasoning.
- **Evidence anchors:** [Section 3.6] reports 1.59‚Äì1.87% invalid outputs for LLaMA-3-8B vs 0.00% for DeepSeek V3. [Table 4] shows format adherence differences across models.
- **Break condition:** If the parser is upgraded to handle component scores directly, the performance gap for smaller models may narrow.

### Mechanism 2
- **Claim:** Leaderboard rank correlation metrics are robust to model scale because they rely on relative system ordering, whereas per-label agreement metrics are sensitive because they require exact semantic precision.
- **Mechanism:** Rank correlation (Spearman, Kendall) is resilient to noise; as long as the judge ranks System A > System B correctly on average, the score remains high. Per-label agreement requires distinguishing fine-grained relevance (e.g., score 1 vs. 2), a capability that diminishes with parameter count.
- **Core assumption:** The goal of evaluation is ranking systems, not generating ground-truth training data.
- **Evidence anchors:** [Abstract] states "leaderboard rank correlations remain robust across model scales, accurate per-label judgments require larger models." [Section 3.2] shows per-label agreement benefits more from model scale than rank correlation measures.
- **Break condition:** If evaluating systems with very similar performance, the robustness of rank correlation in smaller models may fail.

### Mechanism 3
- **Claim:** DeepSeek V3 achieves comparability with GPT-4o potentially due to training data lineage (distillation) rather than just parameter count.
- **Mechanism:** DeepSeek V3 is trained on data generated by GPT-4o, creating alignment between the UMBRELA prompt (designed for GPT-4o) and the open-source model's output distribution.
- **Core assumption:** The UMBRELA prompt is overfitted to GPT-4o's specific reasoning patterns, favoring models distilled from it.
- **Evidence anchors:** [Section 2.5] notes DeepSeek V3 "known to be trained with data generated by GPT-4o, making it a strong publicly available contender."
- **Break condition:** If the prompt is rewritten to be model-agnostic, the advantage of DeepSeek V3 over other 70B+ models might disappear.

## Foundational Learning

- **Concept: Rank Correlation vs. Per-Label Agreement**
  - **Why needed here:** To understand why UMBRELA "works" on small models (high rank correlation) while simultaneously showing it "fails" (low Cohen's Kappa).
  - **Quick check question:** If a judge swaps labels 2 and 3 for every document, does the rank correlation change? Does the per-label agreement change?

- **Concept: Zero-Shot DNA Prompting**
  - **Why needed here:** The UMBRELA prompt structures the task into Descriptive, Narrative, and Aspects (DNA) and forces a multi-step internal process (M, T, O scores).
  - **Quick check question:** Why does asking the model to "split this problem into steps" potentially hurt smaller models that struggle to suppress the output of those steps?

- **Concept: Cohen's Kappa (Œ∫)**
  - **Why needed here:** The paper uses Œ∫ to measure "agreement," correcting for chance.
  - **Quick check question:** Why might "binary" Œ∫ (lumping 0/1 and 2/3) be higher than "scale" Œ∫ (0,1,2,3) for smaller models?

## Architecture Onboarding

- **Component map:** TREC DL Dataset -> Prompt Constructor -> LLM Inference -> Parser -> Evaluator
- **Critical path:** The **Parser** is the fragile edge. The paper notes that simply checking for "##final score" is insufficient for smaller models. You must implement the fallback logic described in Section 3.6 to handle "O: X" or raw numbers, or the pipeline will return 0 (default) for 1.87% of inputs, skewing results.
- **Design tradeoffs:**
  - **Cost vs. Granularity:** Using FLAN-T5-large is cheap and preserves leaderboard rankings but is useless for generating fine-grained training labels.
  - **Prompt Complexity:** The complex "Bing" prompt helps large models but adds parsing complexity for small models that cannot follow format constraints.
- **Failure signatures:**
  - **Format Drift:** Model outputs "M: 2 \n T: 3 \n O: 2" instead of "##final score: 2".
  - **Label Collapse:** Smaller models may output "3" too frequently (hallucinating relevance) or default to "0" if parsing fails.
- **First 3 experiments:**
  1. **Parser Stress Test:** Run LLaMA-3-8B on 100 samples and verify the parser extracts the correct integer from the "O: X" fallback format vs. the standard format.
  2. **Metric Divergence Check:** Evaluate DeepSeek V3 vs. LLaMA-3.3-70B on a subset of DL 2023. Verify that while Rank Correlation (œÅ) is similar, the Cohen's Œ∫ (scale) drops significantly for the non-GPT-distilled model.
  3. **Prompt Ablation:** Swap the complex "Bing" prompt for the "Basic" prompt on the smallest model (FLAN-T5) to see if performance recovers due to reduced instruction complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the performance degradation in small-scale models caused by inherent capacity limitations or by the UMBRELA prompt's design?
- **Basis in paper:** [explicit] Section 3.3 states that future research should explore whether the lower performance on small-scale models is inherent to their capacity or if "the UMBRELA prompt is simply not well-suited to smaller LLMs."
- **Why unresolved:** The study applied the original zero-shot UMBRELA prompt uniformly across all models without modifying the prompt complexity to suit smaller architectures.
- **What evidence would resolve it:** Testing simplified or few-shot prompt variations on LLaMA-3-8B and FLAN-T5-large to see if performance converges with larger models.

### Open Question 2
- **Question:** Can UMBRELA maintain its effectiveness when applied to conversational or multi-turn evaluation tasks?
- **Basis in paper:** [inferred] Section 2.3 notes that applying the approach to "conversational relevance or multi-turn QA" requires adapting the prompt, but the experimental scope was restricted to single-turn TREC Deep Learning datasets.
- **Why unresolved:** The paper does not evaluate the framework's generalizability beyond ad-hoc retrieval, leaving its utility in modern RAG or conversational settings unconfirmed.
- **What evidence would resolve it:** Evaluating DeepSeek V3 and LLaMA variants on multi-turn benchmarks (e.g., TREC CAsT) using adapted prompts.

### Open Question 3
- **Question:** Can instruction-tuning or few-shot prompting mitigate the output format adherence issues observed in mid-scale models?
- **Basis in paper:** [inferred] Section 3.6 highlights that LLaMA-3-8B produced invalid outputs (1.59‚Äì1.87%) due to a failure to follow the strict "##final score" format, necessitating complex parsing logic.
- **Why unresolved:** The authors relied on zero-shot prompts and rule-based error handling but did not investigate if providing examples (few-shot) improves the reliability of the output format.
- **What evidence would resolve it:** A comparison of zero-shot vs. few-shot performance for LLaMA-3-8B to measure the reduction in parsing errors and improvement in Cohen's ùúÖ.

## Limitations

- The study relies on the assumption that DeepSeek V3's strong performance stems from GPT-4o training data lineage, though this causal relationship isn't definitively proven.
- The parsing fallback mechanism may not fully capture the range of output formats from smaller models.
- The evaluation focuses on TREC DL datasets, which may not generalize to other IR tasks or domains.

## Confidence

- **High confidence:** The core finding that larger models achieve substantially better per-label agreement than smaller models is well-supported by the Œ∫ statistics and consistent across datasets.
- **Medium confidence:** The interpretation that DeepSeek V3's performance advantage is due to GPT-4o distillation is plausible but not definitively proven.
- **Medium confidence:** The claim that leaderboard correlations remain robust across model scales is supported by rank correlation metrics, but edge cases aren't tested.

## Next Checks

1. **Parser Robustness Test:** Run LLaMA-3-8B on 1000 samples with the current parser and measure the exact distribution of extracted scores vs. human labels to quantify format drift impact.
2. **Distillation Ablation:** Replace the "Bing" prompt with a model-agnostic prompt and re-run DeepSeek V3 vs. LLaMA-3.3-70B to test whether the performance gap narrows.
3. **Edge Case Correlation:** Select a subset of TREC DL 2023 systems with minimal nDCG@10 differences and re-compute rank correlations for smaller models to test the robustness claim under tight competition.