---
ver: rpa2
title: 'Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions'
arxiv_id: '2511.15830'
source_url: https://arxiv.org/abs/2511.15830
tags:
- park
- react
- gpt-5
- guests
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mini Amusement Parks (MAPs) is a benchmark designed to evaluate
  AI systems on the integrated challenges of real-world decision-making, such as business
  management. It combines open-ended optimization, long-horizon planning, active world-model
  learning, spatial reasoning, and stochastic reasoning under uncertainty.
---

# Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions

## Quick Facts
- **arXiv ID**: 2511.15830
- **Source URL**: https://arxiv.org/abs/2511.15830
- **Reference count**: 40
- **Primary result**: Benchmark exposing LLM limitations in real-world decision-making (6.5× human gap on easy difficulty)

## Executive Summary
Mini Amusement Parks (MAPs) is a benchmark designed to evaluate AI systems on the integrated challenges of real-world decision-making, such as business management. It combines open-ended optimization, long-horizon planning, active world-model learning, spatial reasoning, and stochastic reasoning under uncertainty. The environment simulates running an amusement park where agents must maximize park value by managing rides, shops, staff, and guest behavior. Evaluation against state-of-the-art LLMs shows humans outperform AI by 6.5× on easy and 9.8× on medium difficulty. Analysis reveals AI struggles with long-horizon planning, sample-efficient learning, spatial reasoning, and robust decision-making under stochasticity. The sandbox mode for active world-model learning provides minimal benefit, and spatial heuristics significantly improve AI performance. World modeling with learned models fails, while oracle models yield up to 4× improvement. Overall, MAPs exposes persistent gaps in AI decision-making and provides a foundation for future research in adaptable, real-world AI agents.

## Method Summary
The MAPs benchmark uses a 20×20 grid simulator where agents manage an amusement park to maximize Park Value over 50-100 days. Agents interact via JSON observations and Python function action strings using a ReAct (Reasoning + Acting) loop architecture. The evaluation compares multiple LLM models (GPT-5, GPT-5 Nano, Grok-4, Sonnet 4.5, Gemini 2.5 Pro) against human baselines across Easy and Medium difficulties. Performance is measured relative to human baseline (100% = human parity). The study tests various augmentations including spatial heuristics, sandbox exploration for active learning, and world models (oracle vs. learned).

## Key Results
- Humans outperform AI by 6.5× on easy difficulty and 9.8× on medium difficulty
- Spatial heuristics improve AI performance by 3× by offloading coordinate selection
- Oracle world models provide up to 4× improvement, while learned models fail
- Sandbox exploration mode provides minimal benefit and can degrade performance through context dilution
- AI agents frequently choose "wait" actions (>40% of turns) due to myopia or fear of bankruptcy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing LLM spatial reasoning with a hard-coded heuristic significantly improves agent performance.
- **Mechanism**: LLMs struggle to understand path connectivity and proximity relationships in grid environments. By offloading coordinate selection to a deterministic rule (density + water adjacency + path intersection), the LLM is relieved of a low-level failure mode, allowing its high-level strategy to execute successfully.
- **Core assumption**: Spatial placement is a distinct capability that can be modularized and solved independently of resource planning.
- **Evidence anchors**: Abstract states "spatial heuristics significantly improve AI performance"; Section 4.4 shows "providing this simple heuristic improves performance across all three models... parks with the heuristic are more compact, rides are more frequently by water."
- **Break condition**: If the environment requires dynamic spatial reconfiguration where static rules fail, this mechanism would degrade.

### Mechanism 2
- **Claim**: Access to a ground-truth "Oracle" world model allows Model Predictive Control to overcome stochasticity, whereas learned models fail.
- **Mechanism**: The environment has high variance in guest behavior. LLMs cannot reliably predict these outcomes. An oracle model simulates future states perfectly, allowing a search algorithm to select actions that maximize expected value over a 4-step lookahead, effectively bypassing the LLM's poor probabilistic reasoning.
- **Core assumption**: The bottleneck in stochastic environments is the accuracy of the prediction model, not the search algorithm depth.
- **Evidence anchors**: Abstract states "World modeling with learned models fails, while oracle models yield up to 4× improvement"; Section 4.5 shows "Oracle WM boosts performance with a >4x improvement... WALL-E... is ineffective."
- **Break condition**: If the horizon required for planning exceeds the feasible rollout depth of the MPC, short-horizon oracle rollouts may fail to capture long-term dependencies.

### Mechanism 3
- **Claim**: Unsupervised "Sandbox" exploration degrades performance because LLMs generate spurious causal rules that pollute the context window.
- **Mechanism**: Agents are given a sandbox to generate "learnings." However, they conflate correlation with causation and fail to distill generalizable rules. When these verbose, often incorrect notes are injected into the evaluation prompt, they distract the model or introduce false constraints ("context dilution").
- **Core assumption**: The failure is not due to lack of data, but the inability to filter noise and form correct hypotheses from limited interaction.
- **Evidence anchors**: Abstract states "The sandbox mode... provides minimal benefit"; Section 4.3 analyzes how "Learnings... parrot the same concepts... fail to capture true causal relationships... merely diluting the context."
- **Break condition**: If a filtering mechanism or a separate "critic" model is introduced to validate learnings before injection, this negative effect might be reversed.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Loop**
  - **Why needed here**: This is the baseline agentic architecture used for evaluation. You must understand how the LLM interleaves "Thought" traces with "Action" calls to diagnose where the loop breaks.
  - **Quick check question**: Can you trace a single turn in the paper where the agent decides to "Wait" and explain why the "Thought" process leading to it was likely myopic?

- **Concept: Stochasticity & Coefficient of Variation (CV)**
  - **Why needed here**: The paper evaluates performance under uncertainty. Understanding CV (σ/μ) is required to interpret Figure 2 and why deterministic strategies fail when revenue variance is high.
  - **Quick check question**: If revenue has a high CV but Park Value has a low CV, what does that imply about the reliability of the evaluation metric vs. the daily feedback signal?

- **Concept: World Models (Predictive Simulators)**
  - **Why needed here**: Section 4.5 compares "Oracle" vs. "Wall-E" (learned) world models. You need to distinguish between a perfect simulator and a trained approximator to understand why the latter failed to aid planning.
  - **Quick check question**: Why does the paper conclude that world modeling is "unstudied" in stochastic environments like MAPs, despite the existence of prior work on deterministic world models?

## Architecture Onboarding

- **Component map**: Environment (MAPS) -> JSON observation -> LLM (ReAct) -> Python function actions -> Environment (step) -> Park Value evaluation
- **Critical path**:
  1. Initialize: Load layout & difficulty (Easy/Medium)
  2. Observe: LLM receives JSON state + Documentation
  3. Reason: LLM generates "Thought" and "Action" (e.g., `place(x=5, y=5, ...)`)
  4. Step: Environment executes action, simulates guest day (stochastic), returns new state
  5. Terminate: End of horizon (50 or 100 days) -> Calculate Value

- **Design tradeoffs**:
  - Documentation vs. Few-Shot: Documentation provides priors but is ignored if context is too long; Sandbox allows learning but generates noise
  - ReAct vs. MPC: ReAct is fast/cheap (1 LLM call/step). MPC is slow/expensive (k rollouts * n steps lookahead) but offers higher potential performance *if* the world model is accurate

- **Failure signatures**:
  - The "Wait" Loop: Agent chooses `wait` action >40% of the time due to myopia or fear of bankruptcy
  - Spatial Clustering: Placing rides adjacent in grid space but disconnected by path topology
  - Context Dilution: Agent output quality drops after ~30 steps as history accumulates, or immediately after Sandbox "Learnings" are injected

- **First 3 experiments**:
  1. Baseline Reproduction: Run `GPT-5-Nano` on "Easy" mode with only the documentation prompt. Verify you get ~0.36% relative performance.
  2. Spatial Ablation: Implement the 3-rule heuristic (density, water, intersections) in a `SpatialHeur` wrapper. Run the same model and confirm the boost to ~0.96%.
  3. Sandbox Analysis: Run the Sandbox mode, save the generated "Learnings" text to a file, and manually annotate how many are "Spurious" vs. "Actionable" to validate Section 4.3's qualitative analysis.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can agents be enabled to extract actionable, generalizable causal insights from sparse exploration data (active world-model learning) rather than memorizing specific transitions or parroting the manual? The paper's sandbox analysis shows agents' "learnings" were often "overly specific to the current transition," "parrot the same concepts provided in the manual," or failed to capture "true causal relationships," resulting in no performance gain or even degradation.

- **Open Question 2**: Can scalable learned world models be developed to effectively handle stochastic transitions and long-horizon dependencies without prohibitive computational costs? The paper concludes that "world modeling holds promise, but work addressing accurate stochastic modelling, inference speed, and search speed is required," noting that the learned WALL-E model degraded performance while an oracle model yielded 4× improvement.

- **Open Question 3**: Can LLMs develop spatial reasoning capabilities that surpass simple heuristics to effectively manage complex topological constraints (e.g., path connectivity) and dynamic crowd flow? The paper states that "spatial reasoning of LLMs currently underperforms a simple heuristic" and notes that agents "struggle with spatial understanding of paths," frequently placing rides on disconnected tiles or failing to account for walking distances.

## Limitations

- The benchmark's generalizability is constrained by its specific amusement park domain, which may not translate to other business contexts
- The "human baseline" comparison lacks detailed methodology - it's unclear whether humans had access to the same information or different training procedures
- The study does not explore hybrid approaches combining multiple AI architectures or more sophisticated learning mechanisms
- Environmental stochasticity, while present, may not capture the full complexity of real-world business uncertainty

## Confidence

- **High confidence**: The performance gap between humans and AI (6.5× easy, 9.8× medium) is well-documented with concrete measurements across multiple models and layouts
- **Medium confidence**: The specific mechanisms (spatial heuristics, oracle world models, sandbox failure) are supported by ablation studies, though the causal interpretation of sandbox failure remains somewhat qualitative
- **Low confidence**: The claim that MAPs "exposes persistent gaps in AI decision-making" is somewhat overstated, as the benchmark may primarily highlight limitations in current LLM architectures rather than fundamental AI limitations

## Next Checks

1. **Replicate the spatial heuristic ablation**: Implement the three-rule placement heuristic independently and verify the reported 3× performance improvement across all tested models
2. **Test alternative world model approaches**: Evaluate whether more sophisticated learned world models (e.g., neural simulators or ensemble methods) can match or exceed oracle performance, addressing the claim that world modeling is "unstudied" in stochastic environments
3. **Analyze sandbox learning quality**: Manually annotate a sample of sandbox-generated learnings to quantify the spurious vs. actionable ratio, providing more rigorous support for the context dilution hypothesis