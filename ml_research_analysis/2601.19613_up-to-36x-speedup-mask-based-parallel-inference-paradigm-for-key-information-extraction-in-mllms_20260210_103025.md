---
ver: rpa2
title: 'Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information
  Extraction in MLLMs'
arxiv_id: '2601.19613'
source_url: https://arxiv.org/abs/2601.19613
tags:
- mask
- inference
- parallel
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in Key Information
  Extraction (KIE) from visually-rich documents by proposing a parallel inference
  paradigm for Multi-Modal Large Language Models (MLLMs). The core method, PIP (Parallel
  Inference Paradigm), reformulates KIE by replacing target values with "[mask]" tokens,
  enabling simultaneous generation of all outputs in a single forward pass.
---

# Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs

## Quick Facts
- arXiv ID: 2601.19613
- Source URL: https://arxiv.org/abs/2601.19613
- Reference count: 0
- Achieves 5-36x inference speedup with negligible performance degradation in KIE tasks

## Executive Summary
This paper addresses the efficiency bottleneck in Key Information Extraction (KIE) from visually-rich documents by proposing a parallel inference paradigm for Multi-Modal Large Language Models (MLLMs). The core method, PIP (Parallel Inference Paradigm), reformulates KIE by replacing target values with "[mask]" tokens, enabling simultaneous generation of all outputs in a single forward pass. To enable this approach, the authors develop a mask pre-training strategy and construct large-scale supervised datasets. Experiments demonstrate that PIP-Models achieve significant inference speedup with minimal performance loss compared to autoregressive baselines, setting new state-of-the-art results on SROIE and CORD datasets.

## Method Summary
The paper introduces a parallel inference paradigm (PIP) that reformulates KIE as a mask prediction task rather than sequential autoregressive generation. The approach replaces target values in key-value pairs with "[mask]" tokens, allowing the model to predict all values simultaneously in a single forward pass. This requires two key innovations: a mask pre-training strategy to adapt MLLMs to this new paradigm, and the construction of large-scale supervised datasets for training. The method is validated across different base models and scales, demonstrating consistent efficiency gains with limited memory overhead while maintaining competitive performance on benchmark datasets.

## Key Results
- Achieves 5-36x inference speedup compared to autoregressive baselines
- Sets new state-of-the-art results: SROIE (97.0 ANLS) and CORD (97.3 ANLS)
- Maintains competitive performance on FUNSD (79.3 ANLS)
- Demonstrates consistent efficiency gains across different base models and scales

## Why This Works (Mechanism)
The parallel inference paradigm works by fundamentally changing how MLLMs approach KIE tasks. Instead of generating outputs sequentially through autoregressive decoding, the model predicts all target values simultaneously by treating them as masked tokens. This leverages the MLLM's ability to process the entire document context in parallel, eliminating the sequential dependency that creates bottlenecks in traditional approaches. The mask pre-training strategy enables the model to learn effective representations for masked value prediction, while the large-scale supervised datasets provide diverse training examples that improve generalization across different document layouts and field types.

## Foundational Learning

**Mask Token Prediction**
- Why needed: Enables parallel generation by converting sequential task into simultaneous prediction
- Quick check: Verify that masked tokens can be accurately predicted given full document context

**Multi-Modal Integration**
- Why needed: Combines visual layout information with textual content for accurate field extraction
- Quick check: Ensure visual features are properly aligned with text tokens in the input representation

**Supervised Pre-training**
- Why needed: Provides large-scale training data for the mask prediction paradigm
- Quick check: Validate dataset diversity across different document types and layouts

**Parallel Decoding Optimization**
- Why needed: Maximizes GPU utilization by eliminating sequential dependencies
- Quick check: Measure actual speedup versus theoretical maximum based on sequence length

## Architecture Onboarding

**Component Map**
Document Encoder -> Mask Predictor -> Parallel Output Generator -> Loss Computation

**Critical Path**
Input processing -> Visual-textual feature fusion -> Mask token prediction -> Output generation

**Design Tradeoffs**
- Memory vs Speed: Parallel prediction increases memory usage but eliminates sequential decoding overhead
- Pre-training vs Fine-tuning: Mask pre-training requires large datasets but enables efficient inference
- Model Size vs Performance: Larger models provide better accuracy but reduce speedup benefits

**Failure Signatures**
- Poor performance on nested/hierarchical key-value structures
- Degraded accuracy with incomplete document context
- Memory overflow with very long documents or many target fields

**3 First Experiments**
1. Baseline comparison: Measure speedup and accuracy degradation versus autoregressive approach
2. Ablation study: Test impact of mask pre-training strategy on final performance
3. Scalability test: Evaluate performance across different model sizes and base architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on having complete document context available upfront, limiting streaming applications
- Mask pre-training requires constructing large-scale supervised datasets, challenging for low-resource domains
- Performance on complex layouts with nested or hierarchical key-value structures remains unclear

## Confidence

**High Confidence:**
- Reported speedup measurements (5-36x) are technically sound and well-validated through controlled experiments

**Medium Confidence:**
- Claim of "negligible performance degradation" holds for tested datasets but may not generalize to all KIE scenarios
- Scalability claims across different base models are demonstrated but primarily validated on similar architectural families

## Next Checks
1. Test the PIP approach on documents with hierarchical or nested key-value structures to evaluate robustness beyond flat field extraction scenarios
2. Evaluate performance degradation when applied to streaming scenarios where document context is incomplete or arrives incrementally
3. Conduct ablation studies on the mask pre-training strategy to quantify the contribution of dataset size and quality to final performance, particularly in low-resource settings