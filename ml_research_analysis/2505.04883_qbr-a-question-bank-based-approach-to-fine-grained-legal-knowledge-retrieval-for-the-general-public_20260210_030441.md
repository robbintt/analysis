---
ver: rpa2
title: 'QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval
  for the General Public'
arxiv_id: '2505.04883'
source_url: https://arxiv.org/abs/2505.04883
tags:
- legal
- retrieval
- document
- user
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of legal knowledge retrieval
  for the general public, who often struggle with technical legal documents. The proposed
  QBR method uses a Question Bank (QB) to bridge this knowledge gap by deriving training
  samples that enhance the embedding of knowledge units within documents, enabling
  fine-grained knowledge retrieval.
---

# QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public

## Quick Facts
- arXiv ID: 2505.04883
- Source URL: https://arxiv.org/abs/2505.04883
- Authors: Mingruo Yuan; Ben Kao; Tien-Hsuan Wu
- Reference count: 9
- Primary result: 54% Recall@1 for document retrieval vs 25% BM25; 83.7% scope accuracy vs 48.7% SBERT

## Executive Summary
QBR addresses the challenge of legal knowledge retrieval for the general public by using a Question Bank (QB) to bridge the knowledge gap between technical legal documents and layperson queries. The method employs a two-stage process: first retrieving relevant documents, then identifying the specific scope within those documents that answers the query. QBR achieves significant performance improvements over traditional methods, demonstrating 54% Recall@1 for document retrieval and 83.7% accuracy for scope identification. The approach has been deployed on a public platform and shows positive social impact by helping users resolve legal issues through more accessible question-answer pairs rather than full documents.

## Method Summary
QBR uses a Question Bank containing question-document-scope triples to enhance knowledge unit embeddings through contrastive learning. The method first matches user queries to the most relevant question-scope pairs in the QB, then fine-tunes the MPNet embedding function using InfoNCE loss to distinguish between different scopes within the same document. The training process includes GPT-generated synthetic user inputs to handle hard cases. During inference, QBR retrieves documents based on QB question matches and then re-ranks scopes within those documents using the fine-tuned embeddings. This approach enables fine-grained knowledge retrieval while maintaining explainability through question-answer pairs.

## Key Results
- Document retrieval: QBR achieves 54% Recall@1 compared to 25% for BM25
- Scope identification: 83.7% accuracy versus 48.7% for SBERT baseline
- QBR outperforms state-of-the-art models in both stages of retrieval
- The method demonstrates improved efficiency and explainability over traditional document-based retrieval

## Why This Works (Mechanism)
QBR works by bridging the semantic gap between layperson queries and technical legal documents through the Question Bank. The QB provides a structured mapping between natural language questions and specific knowledge units (scopes) within documents. By training on these question-scope pairs using contrastive learning, QBR learns to map user queries to the most relevant knowledge units, even when the queries use different terminology than the source documents. The two-stage approach allows for efficient document retrieval followed by precise scope identification, while the use of synthetic data through GPT augmentation helps handle ambiguous or complex queries.

## Foundational Learning
- Contrastive Learning: Used to train the embedding function to distinguish between similar but different scopes within documents. Why needed: To handle cases where multiple scopes in a document are semantically close. Quick check: Verify InfoNCE loss implementation and hard negative sampling.
- Knowledge Unit Segmentation: Documents are divided into paragraph-level scopes for fine-grained retrieval. Why needed: To enable precise identification of relevant information within documents. Quick check: Ensure consistent scope boundaries across the corpus.
- Question Bank Construction: Building a comprehensive mapping between questions and document scopes. Why needed: To bridge the semantic gap between user queries and technical documents. Quick check: Validate QB coverage of document knowledge units.

## Architecture Onboarding

**Component Map:** User Query -> MPNet(T) -> Question Bank -> Document Retrieval -> MPNet(T') -> Scope Identification -> Answer

**Critical Path:** User query embedding → QB matching → document retrieval → scope re-ranking → answer selection

**Design Tradeoffs:** The two-stage approach trades some computational efficiency for improved accuracy and explainability. Using question-answer pairs instead of full documents improves accessibility but requires maintaining a comprehensive Question Bank.

**Failure Signatures:**
- Low Recall@1 despite QB: Check cosine similarity normalization and QB coverage
- Scope accuracy stuck near 50%: Verify positive/negative pair construction and hard negative sampling
- Poor generalization to new queries: Check GPT augmentation effectiveness and training data diversity

**3 First Experiments:**
1. Validate document retrieval performance using synthetic queries on the test set
2. Test scope identification accuracy within retrieved documents
3. Evaluate model performance on real user queries from the deployed platform

## Open Questions the Paper Calls Out

**Open Question 1:** How robust is QBR when applied to professional domains with different terminology densities, such as medicine or finance? The paper mentions extending the approach to medical data but states detailed results are in the appendix, implying the main claim requires broader verification. The paper primarily evaluates legal data; performance metrics on other distinct professional domains are not detailed in the main text. Benchmarks of QBR on standard medical or financial retrieval datasets (e.g., PubMed) demonstrating similar scope identification accuracy would resolve this.

**Open Question 2:** How does the inclusion of synthetic user inputs via GPT-augmentation impact the fairness or bias of the retrieval model? The paper relies heavily on GPT to generate synthetic user inputs for contrastive learning, but does not analyze if these synthetic inputs introduce linguistic biases. The evaluation focuses on accuracy (Recall/MRR) but does not measure whether the model learns spurious correlations from the synthetic data. An error analysis comparing model performance on real-world user queries versus GPT-generated test queries to identify systematic bias would resolve this.

**Open Question 3:** What mechanism allows QBR to efficiently update its embeddings when source legislation or knowledge documents are amended? The paper assumes a static document collection and Question Bank, but legal corpora are dynamic. The proposed Contrastive Learning requires training on the full QB; the paper does not discuss incremental updates for new or modified legal scopes. An experiment showing retrieval stability and latency when documents are updated, without requiring a full re-training of the embedding function, would resolve this.

## Limitations
- The CLIC corpus and QB are not publicly available, requiring significant effort to recreate the dataset
- Critical training hyperparameters and scope segmentation details are underspecified
- The synthetic test set may not fully represent real user behavior

## Confidence
- Document retrieval improvements: High confidence
- Scope identification accuracy: Medium confidence
- Social impact claims: Medium confidence
- Method generalizability: Low confidence

## Next Checks
1. Verify scope identification performance on real user queries rather than synthetic inputs
2. Test method robustness across different legal domains beyond CLIC content
3. Evaluate retrieval performance with varying QB sizes to establish optimal coverage