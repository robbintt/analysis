---
ver: rpa2
title: 'Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting
  in Code Generation'
arxiv_id: '2506.06971'
source_url: https://arxiv.org/abs/2506.06971
tags:
- reasoning
- problem
- code
- logic
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces Chain-of-Code Collapse (CoCC), a framework\
  \ to evaluate LLM reasoning robustness under semantically faithful adversarial prompt\
  \ perturbations in code generation tasks. We systematically applied six perturbation\
  \ types\u2014storytelling, gamification, domain shift, distracting constraints,\
  \ example perturbation, and negation\u2014to 100 LeetCode-style problems, generating\
  \ 700 instances."
---

# Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation

## Quick Facts
- arXiv ID: 2506.06971
- Source URL: https://arxiv.org/abs/2506.06971
- Reference count: 10
- Primary result: LLM code generation accuracy varies dramatically under semantically faithful adversarial prompt perturbations, with some perturbations improving performance while others cause severe reasoning collapse.

## Executive Summary
This study introduces Chain-of-Code Collapse (CoCC), a framework to evaluate LLM reasoning robustness under semantically faithful adversarial prompt perturbations in code generation tasks. We systematically applied six perturbation types—storytelling, gamification, domain shift, distracting constraints, example perturbation, and negation—to 100 LeetCode-style problems, generating 700 instances. Across nine LLMs, we observed that while certain perturbations like storytelling and gamification improved accuracy (up to +35.3%), others such as negation severely degraded performance (drops over 50%). Models exhibited inconsistent resilience: some improved under natural language reframing, while others collapsed under logical inversion. These findings expose the fragility of current reasoning systems, highlighting that clean accuracy does not guarantee robustness under linguistic diversity. We propose semantic perturbation robustness as a new evaluation axis to characterize model sensitivity, emphasizing the need for principled reasoning alignment in LLM development.

## Method Summary
The CoCC framework systematically perturbs 100 LeetCode-style problems using six semantic transformation types implemented via LLaMA-3.1-8B-Instruct. Each perturbation is evaluated for logical preservation using Claude-3.7-Sonnet, then tested across nine LLMs for code generation accuracy. The methodology includes automated generation of semantically equivalent but linguistically diverse problem statements, preservation scoring on a 1-10 scale, and execution-based evaluation of generated code against test cases. For negation experiments, test cases were manually aligned with inverted objectives to ensure accurate assessment of reasoning failures.

## Key Results
- Storytelling and gamification perturbations improved model accuracy by up to +35.3%
- Negation perturbations caused performance drops exceeding 50% across most models
- Models exhibited inconsistent resilience patterns—some improved under natural language reframing while others collapsed under logical inversion
- Semantic perturbation robustness (measured by standard deviation across perturbations) varied significantly between models
- Test case misalignment issues were discovered during negation ablation, revealing critical evaluation challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically faithful perturbations reveal reasoning brittleness by exposing reliance on surface-level patterns rather than abstract logic.
- Mechanism: LLMs often solve problems by matching learned templates or problem archetypes. When inputs are perturbed but core logic preserved, robust reasoners should adapt. Failures indicate reliance on superficial cues.
- Core assumption: Performance changes under semantic perturbation are primarily due to disruption of learned surface-level patterns rather than problem difficulty changes.
- Evidence anchors: [abstract] "...do these models truly reason, or do they merely exploit shallow statistical patterns?"; [section 5.4] "...performance does not degrade proportionally with logical drift..."; Related work (GSM-Symbolic, 2506.09677) demonstrates brittleness under semantic transformations.

### Mechanism 2
- Claim: Natural language elaborations (e.g., Storytelling, Gamification) can paradoxically improve reasoning by providing richer cognitive scaffolds that activate relevant domain knowledge.
- Mechanism: Perturbations adding narrative context or gamification can reduce token-level entropy by anchoring problems in more familiar scenarios, guiding model attention more effectively than sparse formal statements.
- Core assumption: Clean problem formulations may not be the most cognitively aligned or effective prompts for all models.
- Evidence anchors: [abstract] "...certain perturbations like storytelling and gamification improved accuracy..."; [section 5.2] "This suggests that rigid, minimalistic problem statements may actually underutilize LLM capabilities..."

### Mechanism 3
- Claim: Reasoning collapse is triggered by misalignment between an LLM's learned solution archetype and a perturbed problem's objective, especially under logical inversions (Negation).
- Mechanism: Models learn strong associations between problem types and solution templates. A "Negation Objective" perturbation (e.g., change maximize to minimize) requires inverting solution logic. Models often fail because their learned "archetype" for that problem type overrides the new instruction.
- Core assumption: Models possess "memorized" solution templates that have stronger influence on output generation than specific instructions of perturbed prompts.
- Evidence anchors: [abstract] "...negation severely degraded performance (drops over 50%)"; [section 5.8 & Appendix A.4, B.1, B.4] Describes "archetype override" where models "default to original problem archetypes."

## Foundational Learning

- **Concept: Semantic Perturbation Robustness**
  - Why needed here: This is the new evaluation axis proposed by the paper, measuring performance stability across logically equivalent but linguistically diverse inputs.
  - Quick check question: Does the paper measure robustness by average accuracy on hard problems or by the variance in performance across different story-framings of the same problem?

- **Concept: Logical Preservation Score**
  - Why needed here: It's the core metric quantifying a perturbation's impact. High scores (e.g., 8.89 for Storytelling) indicate "meaning-preserving" perturbations, while low scores (e.g., 1.83 for Hard Negation) indicate "objective inversion."
  - Quick check question: If a perturbation changes all variable names to fantasy characters but keeps algorithmic logic identical, would you expect its Logical Preservation Score to be high or low?

- **Concept: Test Case Misalignment**
  - Why needed here: The paper's ablation study reveals a critical evaluation flaw where models "passed" by solving the wrong problem because test cases weren't updated to match perturbed objectives.
  - Quick check question: In a "Hard Negation" experiment where the goal is flipped from "maximize" to "minimize," why is it a problem if test cases are not updated?

## Architecture Onboarding

- **Component map:** Perturbation Engine -> Logical Preservation Scorer -> Reasoning Evaluation Pipeline -> Analysis & Metrics
- **Critical path:** The path from *Perturbation* -> *Preservation Scoring* -> *Model Generation* -> *Aligned Evaluation*. Failure at any point invalidates the robustness claim.
- **Design tradeoffs:**
    - Automated vs. Human Scoring: Uses Claude-3.7-Sonnet for scale but acknowledges potential subjectivity
    - Perturbation Strength: Tradeoff between "meaning-preserving" (testing reasoning stability) and "objective-inverting" (testing logical adaptability)
    - Model Selection: Including code-specialized vs. general-purpose models provides broader view but complicates comparison
- **Failure signatures:**
    - Archetype Override: Generated code solves original problem instead of perturbed one (common in Negation)
    - Comment Collapse: Models producing terse, incorrect code under perturbation, indicating reduced reasoning coherence
    - Code Instability: Runtime errors or nonsensical logic in generated code under complex perturbations
- **First 3 experiments:**
    1. Baseline & Validation: Run 100 clean problems through all models to establish baseline accuracy, then apply one perturbation type to validate pipeline
    2. Robustness Analysis: Apply all six perturbation types to 20 problems across all models, computing standard deviation to rank by "Semantic Perturbation Robustness"
    3. Negation Stress Test: Implement "Hard Negation" with rigorously aligned test cases, comparing performance and looking for "archetype override" failures

## Open Questions the Paper Calls Out

- How do top-tier proprietary models, specifically the GPT-series (e.g., GPT-4), perform under the CoCC framework compared to evaluated Claude and Gemini models? [explicit] The authors excluded GPT-series models due to access limitations.

- Does reasoning brittleness observed in code generation generalize to other code-related tasks such as code execution, test output prediction, and self-repair? [explicit] The study did not cover these task types available in LiveCodeBench.

- Why is LLM performance more sensitive to "semantic distractors" than to "narrative abstraction," and what mechanisms drive this "cognitive alignment"? [inferred] The paper documents this phenomenon but doesn't fully isolate internal model mechanisms.

- Can fine-tuning models on uncertainty-aware reasoning paths or explicitly generated code comments mitigate the "collapse" effect under adversarial perturbations? [inferred] The study suggests this direction but focused on evaluating existing models rather than training new ones.

## Limitations

- The study excluded GPT-series models due to access limitations, leaving their robustness under perturbations as an open question
- Test case misalignment issues were discovered during negation ablation, creating ambiguity in some robustness claims
- Automated preservation scoring by Claude-3.7-Sonnet introduces potential subjectivity despite acknowledgment
- The evaluation was restricted to code generation subset of LiveCodeBench, not covering other task types like code execution or self-repair

## Confidence

- **High confidence:** Empirical observation that LLMs exhibit inconsistent performance across semantically equivalent perturbations is well-supported by data
- **Medium confidence:** Interpretation that performance changes primarily reflect reasoning robustness rather than problem difficulty shifts is plausible but not definitively proven
- **Medium confidence:** Claim about "archetype override" in negation failures is well-documented but may be model-specific rather than universal

## Next Checks

1. **Test case alignment verification:** Manually verify that ground-truth test cases are correctly aligned with perturbed objectives, particularly for negation experiments, to eliminate ambiguity observed in ablation study

2. **Cross-model archetype analysis:** Compare whether "archetype override" failures are consistent across models or specific to certain architectures to determine if this is a universal reasoning failure mode

3. **Human preservation scoring:** Run a small subset of perturbations through human evaluators to validate automated preservation scoring and assess potential bias in the automated approach