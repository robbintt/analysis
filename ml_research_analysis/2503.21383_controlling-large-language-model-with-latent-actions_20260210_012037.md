---
ver: rpa2
title: Controlling Large Language Model with Latent Actions
arxiv_id: '2503.21383'
source_url: https://arxiv.org/abs/2503.21383
tags:
- action
- drawer
- latent
- cola
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Controlling Large Language Models with Latent
  Actions (CoLA), a framework that addresses the inefficiency of reinforcement learning
  in large language models by replacing token-level actions with a compact latent
  action space. CoLA learns an inverse dynamics model to extract latent actions from
  token sequences, integrates them into a pre-trained LLM via a merge module, and
  uses a policy model to generate actions.
---

# Controlling Large Language Model with Latent Actions

## Quick Facts
- arXiv ID: 2503.21383
- Source URL: https://arxiv.org/abs/2503.21383
- Reference count: 40
- Key outcome: CoLA framework improves LLM efficiency by using compact latent actions instead of token-level actions, achieving better performance on math500 (42.4 vs 38.2) and reducing computation time by half in thinking-prompt tasks.

## Executive Summary
This paper introduces Controlling Large Language Models with Latent Actions (CoLA), a framework that addresses the inefficiency of reinforcement learning in large language models by replacing token-level actions with a compact latent action space. CoLA learns an inverse dynamics model to extract latent actions from token sequences, integrates them into a pre-trained LLM via a merge module, and uses a policy model to generate actions. Experiments show that CoLA's latent actions enable greater semantic diversity in text generation, improve downstream task performance, and halve computation time in tasks involving enhanced thinking prompts. Additionally, CoLA reduces reward hacking and maintains alignment even with suboptimal reward models.

## Method Summary
CoLA learns an inverse dynamics model to extract compact latent actions from token sequences, then integrates these actions into a pre-trained LLM through a merge module. A policy model generates actions based on the current state, enabling more efficient exploration in the action space. The framework addresses the inefficiency of traditional reinforcement learning approaches that use token-level actions by compressing the action space into a more manageable latent representation. During training, the system learns to predict actions from states and then reconstruct token sequences from these latent actions.

## Key Results
- CoLA achieves 42.4 score on math500 benchmark versus 38.2 for baseline approaches
- Computational efficiency improves by approximately 2x in tasks with enhanced thinking prompts
- MCTS-Q integration with CoLA reaches 68.2 performance on commonsense reasoning tasks

## Why This Works (Mechanism)
The framework works by replacing the high-dimensional token-level action space with a compact latent representation learned through an inverse dynamics model. This compression reduces the exploration burden during reinforcement learning while preserving semantic information. The merge module allows the pre-trained LLM to incorporate these latent actions without catastrophic forgetting, maintaining the model's original capabilities while enabling controlled generation. The policy model generates semantically meaningful actions that guide text generation toward desired outcomes while avoiding the sparse reward problem common in traditional RL approaches.

## Foundational Learning

**Inverse Dynamics Model**
- Why needed: Extracts compact latent representations from token sequences to reduce action space complexity
- Quick check: Verify the model can reconstruct original sequences from learned latent actions with minimal loss

**Merge Module Architecture**
- Why needed: Integrates latent actions into pre-trained LLMs without destroying existing knowledge
- Quick check: Test for catastrophic forgetting by comparing pre- and post-integration performance on original tasks

**Latent Action Space**
- Why needed: Compresses the token-level action space from thousands of possibilities to a manageable dimensionality
- Quick check: Measure KL divergence between original and latent action distributions to ensure semantic preservation

## Architecture Onboarding

**Component Map**
Inverse Dynamics Model -> Merge Module -> Policy Model -> LLM Controller

**Critical Path**
State observation → Inverse dynamics model → Latent action extraction → Merge module integration → Policy model generation → LLM output generation

**Design Tradeoffs**
The framework trades computational efficiency for potential information loss during latent action compression. While the compressed space enables faster training and reduced reward hacking, there's a risk of losing nuanced token-level distinctions that could be important for certain tasks. The merge module design must balance preserving pre-trained knowledge against integrating new action capabilities.

**Failure Signatures**
- Degraded generation quality when latent actions fail to capture semantic nuances
- Increased computational overhead if latent space dimensionality is poorly chosen
- Reward model misalignment leading to generation of nonsensical but high-reward sequences

**3 First Experiments**
1. Ablation study comparing full CoLA against versions without merge module or with different latent dimensions
2. Reconstruction accuracy test measuring how well original sequences can be recovered from latent actions
3. Reward hacking vulnerability test using adversarial prompt sequences to probe robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on synthetic benchmarks and limited downstream tasks, potentially limiting generalizability
- Performance gains on math500 are statistically significant but represent modest improvements (42.4 vs 38.2)
- Framework's dependence on pre-trained inverse dynamics models raises questions about scalability to new languages and domains

## Confidence
- Claims about semantic diversity improvements: **Medium** - supported by qualitative examples but lacking quantitative metrics
- Claims about downstream task performance: **Medium** - limited to specific benchmarks with modest gains
- Claims about computational efficiency: **Low** - based on single scenario without comprehensive timing analysis

## Next Checks
1. Conduct comprehensive ablation studies to isolate the contribution of the merge module versus policy model components
2. Test the framework on diverse downstream tasks beyond math and reasoning, including creative writing and code generation
3. Perform systematic analysis of reward hacking across different reward function designs and adversarial prompt sequences