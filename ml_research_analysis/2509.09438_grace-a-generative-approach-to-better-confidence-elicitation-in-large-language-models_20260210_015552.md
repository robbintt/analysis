---
ver: rpa2
title: 'GrACE: A Generative Approach to Better Confidence Elicitation in Large Language
  Models'
arxiv_id: '2509.09438'
source_url: https://arxiv.org/abs/2509.09438
tags:
- confidence
- calibration
- grace
- arxiv
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GrACE, a novel generative approach for eliciting
  calibrated confidence scores from large language models during open-ended generation
  tasks. Unlike post-hoc methods that require additional evaluation steps or verbalized
  confidence that lacks calibration, GrACE generates a calibrated confidence score
  in real-time by appending a special token <CNF to the model's response and computing
  the similarity between the last hidden state and the embedding of this token.
---

# GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.09438
- **Source URL:** https://arxiv.org/abs/2509.09438
- **Reference count:** 40
- **Primary result:** GrACE improves ECE, Brier score, and AUROC by 2.6%, 1.8%, and 4.4% respectively over Apricot state-of-the-art method.

## Executive Summary
This paper introduces GrACE, a novel generative approach for eliciting calibrated confidence scores from large language models during open-ended generation tasks. Unlike post-hoc methods that require additional evaluation steps or verbalized confidence that lacks calibration, GrACE generates a calibrated confidence score in real-time by appending a special token `<CNF>` to the model's response and computing the similarity between the last hidden state and the embedding of this token. The model is fine-tuned using a calibration set where training samples are grouped based on their internal confidence and assigned group accuracy as calibration targets, ensuring both discrimination and calibration of the generated confidence scores. Experiments across three LLMs (Phi3-3.8B, Llama2-7B, Llama3.1-8B) and two benchmark datasets (TriviaQA, SciQ) demonstrate that GrACE outperforms six competing methods, achieving 2.6%, 1.8%, and 4.4% improvements over the state-of-the-art method Apricot in expected calibration error, Brier score, and AUROC, respectively. Additionally, GrACE exhibits strong generalization to unseen domains and significantly improves the efficacy and efficiency of test-time scaling strategies, reducing the number of required samples while maintaining or improving accuracy.

## Method Summary
GrACE introduces a special token `<CNF>` to the model vocabulary and fine-tunes the model to generate a calibrated confidence score by computing the cosine similarity between the last hidden state at the `<CNF>` position and the token's embedding. During training, samples are grouped by internal confidence (estimated via a linear probe) and assigned the group's empirical accuracy as calibration targets, ensuring calibration. The model is jointly optimized with a calibration loss (MSE between similarity and target) and an SFT loss to preserve generation utility. LoRA adapters are used for efficient fine-tuning, and the token is appended after the answer to enable reflective confidence estimation.

## Key Results
- GrACE achieves 2.6%, 1.8%, and 4.4% improvements over Apricot in ECE, Brier score, and AUROC respectively.
- GrACE maintains or slightly improves accuracy compared to baselines while providing calibrated confidence.
- GrACE reduces test-time scaling sample requirements from 64 to 16 while maintaining accuracy and improves absolute accuracy from 48.1% to 49.3%.

## Why This Works (Mechanism)

### Mechanism 1: Latent State-Embedding Similarity as Confidence Proxy
The framework appends a special token `<CNF>` to the vocabulary. During training, the model learns to adjust the hidden state $z_L$ at the `<CNF>` position such that the softmax-normalized dot product (similarity) with the token's static embedding $e_{<CNF}$ matches a target probability. The geometry of the embedding space allows a linear relationship between the direction of the final hidden state and the "correctness" concept encoded in the special token's embedding.

### Mechanism 2: Accuracy-Conditioned Target Smoothing
Instead of training on binary hard labels (0 or 1), GrACE uses a linear probe to bin samples by internal confidence and assigns each bin's empirical accuracy as the target. This converts the objective from classification to regression against empirical probability, enforcing calibration better than binary labels.

### Mechanism 3: Utility Preservation via SFT Regularization
The total loss $L_T = L_C + \gamma L_{SFT}$ optimizes the confidence head while simultaneously reinforcing the original mapping of question-to-answer. This prevents catastrophic forgetting of the model's generation capabilities during calibration fine-tuning.

## Foundational Learning

- **Concept: Calibration vs. Discrimination**
  - **Why needed here:** GrACE optimizes for both. Discrimination (AUROC) ensures the model knows wrong from right; Calibration (ECE) ensures the probability number is truthful (e.g., 0.8 confidence means 80% chance of being right).
  - **Quick check question:** If a model predicts "0.9 confidence" on answers that are only right 50% of the time, is it failing discrimination or calibration? (Answer: Calibration).

- **Concept: Test-Time Scaling (TTS) & Self-Consistency**
  - **Why needed here:** The paper uses GrACE to improve TTS. You need to understand that TTS typically involves sampling multiple answers and voting, which is expensive. GrACE enables "Early Stopping" to save compute.
  - **Quick check question:** Why does standard majority voting (Self-Consistency) fail if all sampled answers are consistently wrong? (Answer: It amplifies consistent errors).

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** GrACE uses LoRA to fine-tune efficiently. This freezes the main model weights and only trains small adapter matrices, making it feasible to train on consumer GPUs (e.g., RTX 3090 as noted in the paper).
  - **Quick check question:** Why is LoRA critical for GrACE's practical deployment compared to full fine-tuning? (Answer: Preserves base knowledge and drastically reduces memory footprint).

## Architecture Onboarding

- **Component map:** Input Text -> Backbone (Frozen Llama) -> LoRA Adapters -> `<CNF>` Token + Embedding -> Confidence Similarity Head -> Output Confidence Score
- **Critical path:**
  1. **Offline Phase:** Run inference on calibration data with a Linear Probe to rank/bucket samples by confidence. Assign accuracy targets to these buckets.
  2. **Training Phase:** Fine-tune LoRA + `<CNF>` embedding using MSE loss (matching similarity to target accuracy) + SFT loss (standard next-token prediction).
  3. **Inference Phase:** Generate response -> Generate `<CNF>` -> Extract logits -> Read probability at `<CNF>` index as confidence.

- **Design tradeoffs:**
  - **Continuous vs. Discrete:** Using discrete bins `<B1>...<BN>` performs worse than the continuous similarity method because discrete tokens lack precision.
  - **Position of `<CNF>`:** Placing `<CNF>` *after* the answer is critical. Placing it *before* collapses performance because the model cannot reflect on the generated content.

- **Failure signatures:**
  - **Overconfidence:** If trained using SFT loss *on* the `<CNF>` token, the model pushes probability mass to the token indiscriminately, destroying calibration.
  - **Stagnant Confidence:** If LoRA rank is too low, the model may not have enough capacity to adjust representations for confidence, resulting in a flat score distribution.

- **First 3 experiments:**
  1. **Pipeline Validity:** Replicate the "Offline Phase" on a small subset. Train the linear probe and verify that binning the data actually correlates with accuracy.
  2. **Overfitting Check:** Train GrACE and plot the training loss curve for $L_C$ vs $L_{SFT}$. If $L_C$ goes to 0 but generation quality degrades, adjust $\gamma$.
  3. **Ablation Run:** Run inference with `<CNF>` appended to the *question* vs. the *answer* on 50 samples to confirm the huge performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the GrACE mechanism be adapted to provide fine-grained confidence scores for individual claims within long-form text or specific steps in Chain-of-Thought reasoning?
- **Basis in paper:** Section C (Limitation) explicitly states the method "cannot give different confidence scores for every claim in the generation or each step in Chain-of-Thoughts."
- **Why unresolved:** The current architecture appends a single special token at the sequence end, aggregating hidden states into one global score rather than localizing confidence.
- **What evidence would resolve it:** A modified architecture inserting special tokens at intermediate reasoning steps, evaluated on step-wise accuracy in mathematical or logical reasoning benchmarks.

### Open Question 2
- **Question:** How can GrACE be modified to calibrate confidence with respect to response qualities other than factuality, such as completeness or relevance?
- **Basis in paper:** Section C (Limitation) notes that "other aspects regarding model response, such as completeness, should also be considered" but are currently ignored.
- **Why unresolved:** The training objective currently uses binary accuracy as the calibration target, which fails to capture nuanced dimensions of response quality.
- **What evidence would resolve it:** Experiments utilizing multi-dimensional calibration targets (e.g., ROUGE scores for coverage) demonstrating that GrACE's correlation extends beyond binary correctness.

### Open Question 3
- **Question:** Can the confidence signal generated by GrACE be integrated into a feedback loop to actively improve model confidence, rather than solely assessing it?
- **Basis in paper:** Section C (Limitation) states GrACE "only assesses but does not improve the model confidence."
- **Why unresolved:** The framework currently focuses on elicitation and calibration, lacking a mechanism to utilize the generated confidence score for corrective generation or weight updating.
- **What evidence would resolve it:** Integrating GrACE confidence scores as a reward signal in reinforcement learning or a rejection sampling mechanism that demonstrably reduces Expected Calibration Error (ECE) over training iterations.

### Open Question 4
- **Question:** How robust is the initial linear probe used for target grouping when the semantic distribution shifts significantly from the calibration data?
- **Basis in paper:** Section 3.2.2 relies on a linear probe to group samples for calibration targets. The method assumes the probe accurately estimates "internal confidence" to assign group accuracy.
- **Why unresolved:** The paper demonstrates generalization between similar QA datasets (TriviaQA/SciQ), but does not test domains where the probe's linearity assumption or internal representation alignment might fail.
- **What evidence would resolve it:** A sensitivity analysis varying the quality of the pre-grouping linear probe (e.g., using corrupted vs. accurate probes) and measuring the resulting impact on final GrACE calibration performance.

## Limitations
- **Target Generation Reliability:** The linear probe used for calibration targets may be inaccurate, leading to miscalibrated confidence scores.
- **Dataset and Domain Generalization:** Performance on completely different generation tasks (e.g., summarization, translation) remains unverified.
- **Hyperparameter Sensitivity:** The method depends on several hyperparameters with no sensitivity analysis provided.

## Confidence
- **High Confidence:** The core mechanism of using latent state-embedding similarity is sound and novel; ablation study results are convincing; TTS efficiency improvements are significant.
- **Medium Confidence:** The 2.6%/1.8%/4.4% improvements over Apricot need independent replication; generalization claims need broader validation.
- **Low Confidence:** Specific implementation details of the linear probe are missing; potential dataset biases are not addressed.

## Next Checks
1. **Probe Performance Audit:** Before training GrACE, independently evaluate the linear probe used for target generation. Train the probe on the calibration set and report its AUROC and accuracy. If the probe cannot discriminate correct from incorrect answers with high accuracy, the calibration targets are unreliable.

2. **Cross-Domain Stress Test:** Evaluate GrACE on a dataset from a completely different domain than TriviaQA and SciQ, such as a summarization dataset (e.g., CNN/DailyMail) or a translation task. Measure ECE, AUROC, and the model's ability to provide useful confidence scores for a task where "correctness" is more complex than a binary label.

3. **Hyperparameter Sweep:** Conduct a systematic ablation study on the key hyperparameters: (a) LoRA rank (try 4, 8, 16, 32), (b) SFT loss weight Î³ (try 0.01, 0.1, 1.0), and (c) Number of bins for target smoothing (try 5, 10, 20). Plot the Pareto frontier of ECE vs. Accuracy to identify the best trade-off and understand the sensitivity of the method.