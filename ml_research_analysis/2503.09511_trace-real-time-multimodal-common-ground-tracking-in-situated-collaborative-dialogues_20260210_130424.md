---
ver: rpa2
title: 'TRACE: Real-Time Multimodal Common Ground Tracking in Situated Collaborative
  Dialogues'
arxiv_id: '2503.09511'
source_url: https://arxiv.org/abs/2503.09511
tags:
- ground
- trace
- task
- multimodal
- common
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACE is a real-time multimodal system for tracking common ground
  in collaborative tasks by integrating speech, gestures, gaze, and objects. It uses
  automated processing of multimodal signals to extract task-relevant propositions
  and epistemic positions, tracking agreed facts and evidence over time.
---

# TRACE: Real-Time Multimodal Common Ground Tracking in Situated Collaborative Dialogues

## Quick Facts
- **arXiv ID**: 2503.09511
- **Source URL**: https://arxiv.org/abs/2503.09511
- **Reference count**: 29
- **Primary result**: Real-time multimodal common ground tracking with DSC 0.000-0.741 across groups

## Executive Summary
TRACE is a real-time multimodal system for tracking common ground in collaborative tasks by integrating speech, gestures, gaze, and objects. It uses automated processing of multimodal signals to extract task-relevant propositions and epistemic positions, tracking agreed facts and evidence over time. Evaluated on a situated collaborative task with triads, TRACE achieves Sørensen-Dice Coefficients (DSC) ranging from 0.000 to 0.741 across groups for different common ground banks, with performance varying due to real-time processing constraints. Substitution studies show that ground truth speech transcriptions significantly improve performance, while gesture and object detection have moderate impacts. TRACE represents an advance in real-time multimodal common ground tracking, enabling AI systems to model group collaboration in live settings.

## Method Summary
TRACE processes real-time multimodal signals including speech, gestures, gaze, and object interactions to track common ground during collaborative tasks. The system extracts task-relevant propositions and epistemic positions from these signals, maintaining a dynamic representation of agreed facts and evidence over time. It was evaluated using triads working on situated collaborative tasks, with performance measured using Sørensen-Dice Coefficients across different common ground banks.

## Key Results
- Achieved DSC ranging from 0.000 to 0.741 across different groups
- Ground truth speech transcriptions significantly improved performance in substitution studies
- Gesture and object detection had moderate impacts on overall system performance

## Why This Works (Mechanism)
TRACE works by integrating multiple streams of information from collaborative interactions. The system processes speech to extract propositions, tracks gestures and gaze for spatial and attentional context, and monitors object interactions for task-relevant evidence. These multimodal inputs are combined in real-time to maintain an evolving representation of common ground. The real-time constraint is critical as it allows the system to track the dynamic nature of collaborative dialogues where agreement and understanding develop over time.

## Foundational Learning

**Sørensen-Dice Coefficient**: A statistical measure of similarity between two sets, used here to evaluate common ground tracking accuracy. Why needed: Provides a standardized metric for comparing system output against ground truth. Quick check: Values range from 0 (no overlap) to 1 (perfect match).

**Epistemic positions**: The knowledge state and beliefs of participants regarding propositions. Why needed: Essential for tracking what is mutually understood versus what remains uncertain. Quick check: Represented as agreed facts versus evidence requiring further verification.

**Multimodal integration**: Combining information from speech, gestures, gaze, and objects. Why needed: No single modality captures the full complexity of collaborative understanding. Quick check: System must handle asynchronous and incomplete information from each stream.

## Architecture Onboarding

**Component map**: Speech -> Gesture/Gaze -> Object detection -> Proposition extraction -> Common ground tracking

**Critical path**: Real-time audio input → Automatic speech recognition → Proposition extraction → Epistemic state update → Common ground representation

**Design tradeoffs**: Real-time processing vs. accuracy, multimodal integration complexity vs. computational efficiency, fine-grained tracking vs. system latency

**Failure signatures**: Low DSC scores when speech processing fails, degraded performance with noisy environments, challenges with rapid topic shifts

**First experiments**:
1. Test speech-only baseline against multimodal system
2. Evaluate impact of ground truth transcriptions on performance
3. Measure individual modality contributions through ablation studies

## Open Questions the Paper Calls Out
None

## Limitations
- Performance varies significantly across different groups (DSC 0.000-0.741)
- Real-time processing constraints limit accuracy compared to offline methods
- System sensitivity to environmental factors like noise and lighting

## Confidence

**Speech processing accuracy**: High - Ground truth transcriptions significantly improve performance
**Multimodal integration**: Medium - Moderate impact from gesture and object detection suggests room for improvement
**Real-time capability**: High - Core design achieves stated real-time requirements
**Generalization**: Low - Performance varies widely across different groups and tasks

## Next Checks

1. Conduct ablation studies to quantify individual modality contributions
2. Test system performance across diverse collaborative scenarios
3. Compare real-time results with offline processing to measure trade-off impacts