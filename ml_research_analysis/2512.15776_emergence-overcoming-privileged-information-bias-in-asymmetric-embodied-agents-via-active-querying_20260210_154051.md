---
ver: rpa2
title: 'Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents
  via Active Querying'
arxiv_id: '2512.15776'
source_url: https://arxiv.org/abs/2512.15776
tags:
- leader
- follower
- success
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel asymmetric assistive reasoning framework
  to address the privileged information bias (curse of knowledge) in embodied agents.
  They formalize a leader-follower setting in AI2-THOR where a fully-observed leader
  must guide a sensor-limited follower, isolating communication challenges caused
  by differing perceptual access.
---

# Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying
arXiv ID: 2512.15776
Source URL: https://arxiv.org/abs/2512.15776
Reference count: 9
Key outcome: The authors propose a novel asymmetric assistive reasoning framework to address the privileged information bias (curse of knowledge) in embodied agents. They formalize a leader-follower setting in AI2-THOR where a fully-observed leader must guide a sensor-limited follower, isolating communication challenges caused by differing perceptual access. The key method is contrasting a standard "push" protocol with a "pull" protocol where the follower actively queries for clarification. Results show a significant "success gap": while the leader identifies targets in 35.0% of episodes, the team succeeds in only 17.0%, indicating nearly half of feasible plans fail due to grounding errors. Successful episodes feature 2x more follower queries, demonstrating that active uncertainty reduction is crucial for effective human-AI and robot-robot collaboration.

## Executive Summary
The paper introduces a novel asymmetric assistive reasoning framework that addresses the "curse of knowledge" in embodied agents by formalizing a leader-follower collaboration setting. In this framework, a fully-observed leader must guide a sensor-limited follower through AI2-THOR environments, isolating communication challenges caused by differing perceptual access. The key innovation is contrasting a standard "push" protocol with a "pull" protocol where the follower actively queries for clarification, demonstrating that active uncertainty reduction is crucial for effective collaboration.

## Method Summary
The authors formalize an asymmetric embodied collaboration framework in AI2-THOR where a fully-observed leader guides a sensor-limited follower through household environments to reach target objects. They use a single Gemini 2.5 Flash LLM kernel serving dual personas with strict knowledge separation enforced through system prompts. The leader receives full state information while the follower operates with limited 2.0m view distance and 90° field of view. The method contrasts two communication protocols: "push" where the leader broadcasts instructions unconditionally, and "pull" where the follower actively queries when instructions reference ungrounded landmarks. Success is measured by whether the follower reaches the target object within 30 steps.

## Key Results
- Success gap: Leader identifies targets in 35.0% of episodes, but collaborative team succeeds in only 17.0%
- Query frequency: Successful episodes feature 2x more follower queries (2.00 vs 0.99 per episode)
- Grounding errors: Leader uses egocentric instructions relative to its own orientation rather than follower's reference frame
- Pull protocol effectiveness: Active querying significantly reduces the success gap by forcing re-grounding of instructions

## Why This Works (Mechanism)
### Mechanism 1: Active Uncertainty Reduction via Pull-Based Querying
- **Claim:** When followers actively query for clarification rather than passively executing instructions, collaborative success rates increase significantly.
- **Mechanism:** The follower's verification module checks incoming instructions against local perceptual constraints ($S_F$). If an instruction references an ungrounded landmark (e.g., "go to the sofa" when no sofa is visible), the follower triggers a query ($Q_{pull}$), forcing the leader to re-ground instructions in the follower's reference frame.
- **Core assumption:** The follower can reliably detect when its local state diverges from what the instruction presupposes.
- **Evidence anchors:**
  - [abstract] "Successful episodes featuring 2x the frequency of clarification requests"
  - [Section 4.5.2] "In successful episodes, the Follower issued 2.00 active queries per episode, compared to just 0.99 in failed episodes"
  - [corpus] Weak direct support; corpus papers address asymmetric RL and bias but not this specific pull-query mechanism.
- **Break condition:** If the follower lacks a verification module or cannot detect grounding mismatches, queries won't trigger and the mechanism collapses.

### Mechanism 2: Privileged Information Bias Isolation
- **Claim:** The "Curse of Knowledge" causes a measurable success gap between what a leader can perceive and what it can communicate to a sensor-limited partner.
- **Mechanism:** The leader with global state ($S_L$) generates egocentric instructions (e.g., "turn left" relative to its own orientation) without simulating the follower's belief state. The follower, operating under limited $S_F$ (2.0m view distance), executes these commands in an ungrounded reference frame.
- **Core assumption:** The LLM lacks inherent Theory of Mind capabilities to model the partner's perceptual constraints.
- **Evidence anchors:**
  - [abstract] "While the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time"
  - [Section 5.2.1] "The 18-point gap signifies a failure in Theory of Mind"
  - [corpus] RoboView-Bias paper quantifies visual bias in embodied agents, providing converging evidence for perceptual asymmetry effects.
- **Break condition:** If agents share homogeneous perception (no asymmetry), this bias mechanism becomes irrelevant.

### Mechanism 3: Frame Switching via Re-Grounding
- **Claim:** Successful collaboration requires the leader to translate egocentric coordinates into allocentric or landmark-based cues when prompted by follower queries.
- **Mechanism:** When a follower query flags an ungrounded reference, the leader must abandon its egocentric frame ("to your left") and switch to landmark-relative descriptions ("behind the sofa"). This forces explicit perspective-taking.
- **Core assumption:** The LLM can perform frame switching when explicitly cued by dialogue context.
- **Evidence anchors:**
  - [Section 5.2.3] "This query forced the Leader to perform Frame Switching, translating the egocentric instruction into an allocentric or landmark-based cue"
  - [Table 3] Shows example of incorrect grounding where "Left" is relative to Leader's position, not Follower's
  - [corpus] No direct corpus evidence for this specific frame-switching mechanism.
- **Break condition:** If the LLM cannot maintain distinct persona knowledge boundaries, it may leak privileged information or fail to re-ground.

## Foundational Learning
- **Concept: Theory of Mind in AI**
  - **Why needed here:** The entire framework measures ToM failures—the leader must simulate what the follower can see to provide grounded instructions.
  - **Quick check question:** Can you explain why an agent with perfect global knowledge might fail to guide a partner with limited perception?

- **Concept: Symbol Grounding Problem**
  - **Why needed here:** The paper diagnoses how LLMs, trained on static text, struggle to connect linguistic tokens ("turn left") to physical actions without shared perceptual context.
  - **Quick check question:** What happens when an instruction references a landmark the follower cannot perceive?

- **Concept: Push vs Pull Communication Protocols**
  - **Why needed here:** The core experimental contrast—open-loop instruction broadcasting versus closed-loop active querying—determines success.
  - **Quick check question:** In a push-only protocol, who bears the burden of ensuring instructions are grounded?

## Architecture Onboarding
- **Component map:** Leader -> Follower -> Shared context window -> System prompt enforcement
- **Critical path:** Leader perceives global state → generates instruction $I_{push}$ → Follower verifies $I_{push}$ against $S_F$ → If ungrounded: Follower generates query $Q$ → Leader re-grounds → If grounded: Follower executes action
- **Design tradeoffs:** Single-core architecture reduces latency but risks information leakage across personas; Temperature=0.0 stabilizes coordinate-to-action mapping but may reduce query generation diversity; 30-step horizon filters for efficient navigation but may exclude viable long-horizon plans
- **Failure signatures:** "Blind obedience": Follower executes ungrounded instructions without triggering queries (0.99 queries/episode in failures); "Egocentric grounding": Leader uses relative directions based on its own orientation, not follower's; "Semantic random walks": Without spatial memory, agents revisit invalid locations
- **First 3 experiments:** Replicate push-pull comparison: Run 100 tasks with queries disabled (push-only) vs enabled; expect ~2x query ratio difference between success/failure groups; Ablate verification module: Remove the $Verify(S_F)$ check; should see success rate drop toward handicapped solo baseline (11%); Extend horizon to 60 steps: Re-run failed tasks to separate horizon truncation failures from grounding failures; expect ~62% relative improvement for handicapped agent

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can a "Devil's Advocate" reward function successfully incentivize agents to reject ambiguous instructions, thereby reducing the "Success Gap"?
- Basis in paper: [explicit] The authors propose future training objectives that explicitly reward agents for questioning ambiguity rather than maximizing agreeableness.
- Why unresolved: Current zero-shot prompts bias agents toward compliance, preventing active querying ("Pull") from becoming a consistent, learned behavior.
- What evidence would resolve it: A study showing a fine-tuned Follower agent achieves higher success rates specifically by increasing query frequency in uncertain states.

### Open Question 2
- Question: How does the "Success Gap" evolve in dynamic environments with moving obstacles compared to the static AI2-THOR scenes used in this study?
- Basis in paper: [explicit] The authors note the need to expand the framework to dynamic environments requiring real-time belief synchronization.
- Why unresolved: The current static setting relies on spatial memory; dynamic settings might exacerbate grounding errors or force more frequent synchronization.
- What evidence would resolve it: Benchmarking the Leader-Follower framework in time-variant environments to measure the delta between Leader perception and Follower execution.

### Open Question 3
- Question: Does sharing "visual imaginations" (image patches) via Vision-Language Models reduce the "Sensory Tax" more effectively than the current text-based semantic descriptions?
- Basis in paper: [explicit] The authors propose integrating VLMs to bridge the gap between semantic description and sensorimotor reality.
- Why unresolved: Text-based serialization flattens 3D geometry, causing "perception loss" and "semantic random walks" where agents fail to build coherent spatial maps.
- What evidence would resolve it: Comparative success rates between text-instructed agents and agents receiving visual grounding cues in the handicapped condition.

## Limitations
- Single-core LLM architecture creates potential for information leakage across personas despite system prompt safeguards
- 30-step horizon constraint may artificially limit task success, particularly for handicapped agents
- Temperature=0.0 constraint may suppress natural query generation diversity, potentially underestimating pull protocol's full potential

## Confidence
- **High confidence:** The 18-point success gap between leader perception (35%) and team performance (17%) is robust and clearly demonstrates privileged information bias. The 2x difference in query frequency between successful and failed episodes provides strong evidence for the pull protocol's effectiveness.
- **Medium confidence:** The mechanism of frame switching via re-grounding is well-supported by qualitative examples, but lacks direct quantitative validation across diverse task types. The assertion that egocentric instructions are the primary failure mode could benefit from more granular error analysis.
- **Low confidence:** The study's conclusions about Theory of Mind failure in LLMs are suggestive but not definitively proven, as the comparison is limited to a single framework without cross-architectural validation.

## Next Checks
1. **Cross-architecture replication:** Replicate the push-pull comparison using separate LLMs for leader and follower to eliminate potential persona leakage, measuring if success rates converge toward theoretical expectations.
2. **Extended horizon ablation:** Re-run all failed tasks with horizon extended to 60 steps to quantify the relative contribution of truncation versus grounding failures, particularly for handicapped agents.
3. **Query diversity analysis:** Systematically vary temperature settings and measure the resulting query diversity and success correlation to determine if the temperature=0.0 constraint is masking potential improvements in the pull protocol.