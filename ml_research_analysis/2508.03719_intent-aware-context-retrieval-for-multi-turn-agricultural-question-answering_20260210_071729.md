---
ver: rpa2
title: Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering
arxiv_id: '2508.03719'
source_url: https://arxiv.org/abs/2508.03719
tags:
- agricultural
- query
- system
- intent
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Krishi Sathi, an AI-powered agricultural chatbot
  that provides personalized, multilingual advice to Indian farmers via text and speech.
  The system uses an instruction-tuned language model fine-tuned on Indian agricultural
  data, and employs a multi-turn dialogue flow to extract user intent and context.
---

# Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering

## Quick Facts
- arXiv ID: 2508.03719
- Source URL: https://arxiv.org/abs/2508.03719
- Reference count: 4
- Key outcome: Krishi Sathi achieves 97.53% query response accuracy, 91.35% contextual relevance, and 97.53% query completion rate with responses delivered in under 6 seconds.

## Executive Summary
Krishi Sathi is an AI-powered agricultural chatbot designed for Indian farmers, providing personalized, multilingual advice via text and speech. The system uses an instruction-tuned language model fine-tuned on Indian agricultural data, employing a structured multi-turn dialogue flow to extract user intent and context. By integrating retrieval-augmented generation (RAG) with a curated database of agricultural knowledge, it delivers tailored responses in both English and Hindi, supporting low-literacy users through ASR and TTS interfaces. Results demonstrate strong performance across accuracy, relevance, and completion metrics.

## Method Summary
The system follows a structured pipeline: user input (text/speech) is processed through ASR if needed, then language detection and translation (Hindiâ†’English). A router classifies queries as domain, general, or casual. Domain queries proceed through crop classification, intent classification, and slot extraction, with missing slots clarified over 2-3 dialogue turns. The enriched query retrieves the top-1 passage from a 150K passage corpus using all-mpnet-base-v2 embeddings indexed in Qdrant. Few-shot prompts (2-3 examples) guide the IFT model's generation, with responses formatted and converted to speech if requested via TTS.

## Key Results
- Query Response Accuracy: 97.53%
- Contextual Relevance: 91.35%
- Query Completion Rate: 97.53%
- Average Response Time: <6 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured multi-turn dialogue with intent-slot mapping improves query completeness and contextual relevance for underspecified agricultural queries.
- Mechanism: The system classifies queries into predefined intents (25 for grapes, 22 for onions), each with 2-5 slots (e.g., state, season, seed variety). Missing slots trigger clarification questions over 2-3 dialogue turns, progressively building a semantic representation before retrieval and generation.
- Core assumption: Users can and will provide missing slot values through multi-turn interaction; intent classes cover the query distribution.
- Evidence anchors:
  - [abstract] "Krishi Sathi follows a structured, multi-turn conversation flow to gradually collect the necessary details from the farmer, ensuring the query is fully understood before generating a response."
  - [section 4.3] "This module ensures that all necessary slots are populated by interacting with the user through a dynamic clarification process... The iterative slot filling process typically takes 2-3 dialogue turns to complete."
- Break condition: If users abandon multi-turn clarification or provide unresolvable ambiguity, the slot-filling loop fails to converge, degrading personalization.

### Mechanism 2
- Claim: Dense passage retrieval (DPR) over a curated domain corpus grounds responses in expert-verified knowledge, reducing hallucination risk.
- Mechanism: 150,000 passages from ICAR, Vikaspedia, and ICAR institutes are encoded into 768-dimensional vectors using all-mpnet-base-v2, indexed in Qdrant. At inference, enriched queries retrieve the top-1 nearest neighbor, which conditions the IFT model's generation via few-shot prompting.
- Core assumption: The curated corpus covers the query space; top-1 retrieval is sufficient for grounding (no multi-hop or diverse retrieval).
- Evidence anchors:
  - [abstract] "integrates retrieval-augmented generation (RAG) to fetch information from a curated database and deliver tailored responses."
  - [section 3.5] "Documents were preprocessed by removing metadata and encoded into 768-dimensional vectors. Approximately 150,000 embeddings were generated and indexed using the Qdrant vector database... matched using top-1 nearest neighbour search (k=1)."
- Break condition: If queries require multi-hop reasoning or span documents not in the curated corpus, single-hop top-1 retrieval may return insufficient or misaligned context.

### Mechanism 3
- Claim: Domain-specific instruction fine-tuning with in-context few-shot prompting improves answer quality for agricultural queries while preserving general-purpose capabilities via model separation.
- Mechanism: The Param-1-2.9B model is fine-tuned on 12M tokens of curated agricultural text (3 epochs, supervised). Domain queries are routed to this specialized model; general/casual queries go to the base model. Few-shot prompts (2-3 examples) provide in-context guidance without weight updates.
- Core assumption: Fine-tuning on static curated data generalizes to evolving real-world queries; routing logic correctly separates domain from general queries.
- Evidence anchors:
  - [abstract] "instruction-tuned language model fine-tuned on Indian agricultural data."
  - [section 3.6] "The Param-1-2.9B model was fine-tuned (supervised) on the curated agricultural corpus... Training was conducted for 3 epochs... evaluation loss reaching 0.3343."
- Break condition: If domain drift occurs (new crops, practices, regulations) without corpus or fine-tuning updates, specialization degrades; routing errors misroute queries to the wrong model.

## Foundational Learning

- Concept: Task-Oriented Dialogue (TOD) with Intent Classification and Slot Filling
  - Why needed here: Enables the system to interpret vague farmer queries and progressively collect required parameters through structured dialogue.
  - Quick check question: Given a query "My crop has yellow leaves," can you identify which slots (e.g., crop type, region, growth stage) are missing and need clarification?

- Concept: Retrieval-Augmented Generation (RAG) with Dense Retrieval
  - Why needed here: Grounds LLM responses in domain-specific knowledge, reducing hallucination and improving factual accuracy.
  - Quick check question: How does top-1 nearest neighbor retrieval differ from multi-hop or diverse retrieval, and what types of queries might each fail on?

- Concept: Instruction Fine-Tuning (IFT) vs. In-Context Learning (ICL)
  - Why needed here: Understanding the trade-off between weight-modifying specialization (IFT) and prompt-based adaptation (ICL) clarifies when to retrain versus when to update prompts.
  - Quick check question: If new agricultural advisory content is added weekly, would you recommend periodic IFT updates or rely solely on ICL with updated prompts?

## Architecture Onboarding

- Component map:
  ASR (120M params) -> Language detection -> Translation (Google Cloud Translate) -> Query router -> Crop classifier -> Intent classifier -> Slot extractor -> Multi-turn clarification -> Dense retrieval (all-mpnet-base-v2) -> Qdrant vector DB -> Few-shot prompt construction -> IFT model (Param-1-2.9B) -> Response formatting -> TTS (F5TTS-small, 150M params)

- Critical path:
  1. User input (text/speech) -> ASR if speech -> language detection -> translate to English if Hindi
  2. Query classification (domain vs. general vs. casual)
  3. If domain: crop classification -> intent classification -> slot extraction -> multi-turn clarification if slots missing
  4. Enriched query -> dense retrieval (top-1 passage)
  5. Few-shot prompt construction (2-3 examples + retrieved passage + query)
  6. IFT model inference -> format response -> TTS if speech output requested

- Design tradeoffs:
  - Top-1 retrieval (k=1) optimizes latency (~6s average response time) but risks missing relevant context for complex queries.
  - Intent-slot structure (47 intents across 2 crops) provides precision but limits scalability; adding crops requires intent annotation and slot schema design.
  - Separating domain and general models optimizes resource use but introduces routing complexity and potential misclassification.

- Failure signatures:
  - Slot-filling loop exceeds 3 turns without convergence -> user frustration, query abandonment
  - Retrieval returns irrelevant passage (low similarity) -> hallucination or generic response
  - Intent misclassification -> wrong slot schema, irrelevant clarification questions
  - ASR errors for dialect-heavy Hindi -> incorrect transcription, downstream pipeline failures

- First 3 experiments:
  1. Retrieval diversity ablation: Compare top-1 vs. top-3 retrieval with passage re-ranking to measure impact on contextual relevance (PCR) and query accuracy (QRA).
  2. Intent schema stress test: Introduce queries from a third crop (e.g., rice) without adding intents to evaluate routing and retrieval degradation; use results to prioritize intent expansion.
  3. Dialogue history integration: Implement a simple history-aware query reformulation (e.g., append previous turn slot values) and measure reduction in clarification turns and improvement in QCR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the intent-classification architecture maintain high precision when scaled beyond the current two-crop (grape, onion) constraint?
- Basis in paper: [explicit] The authors state the model is currently limited to two crops as an experiment to evaluate performance "before extending it to a wider variety."
- Why unresolved: Increasing the number of intent classes (currently 25 for grapes, 22 for onions) typically introduces semantic overlap and noise, which may degrade the 97.53% Query Response Accuracy.
- Evidence: Evaluation of Query Response Accuracy (QRA) metrics on a expanded test dataset covering more than 10 distinct crop types.

### Open Question 2
- Question: Can the pipeline be optimized to reduce the 53.66% tail latency (responses >3 seconds) without sacrificing the 97.53% accuracy?
- Basis in paper: [explicit] The results section notes that 53.66% of responses take more than 3 seconds, explicitly suggesting "room for improvement in speed."
- Why unresolved: Reducing latency often requires model distillation or reduced retrieval complexity, trade-offs that may harm the 91.35% Personalization and Contextual Relevance (PCR).
- Evidence: Latency distribution analysis comparing the current serial pipeline against parallelized or distilled model architectures.

### Open Question 3
- Question: How will the integration of image-based computer vision (i-SARATHI) affect the reliability of the current text-based intent detection module?
- Basis in paper: [explicit] Future work includes "image analysis capabilities... allowing the system to provide visual diagnostic support."
- Why unresolved: The current intent recognition relies solely on the Mistral model; fusing visual data introduces potential conflicts between textual descriptions and visual evidence.
- Evidence: Comparative study of diagnostic accuracy for text-only input versus multimodal (text + image) input on disease identification tasks.

## Limitations
- Evaluation focuses on two crops (grapes, onions) with 47 total intents, limiting generalizability to broader agricultural domains.
- 97.53% query response accuracy claim lacks benchmarking against baseline systems or alternative architectures.
- System's dependency on a curated corpus (150K passages) raises concerns about coverage for emerging agricultural practices and regional variations.
- Multi-turn dialogue effectiveness assumes users can provide required slot values, but no analysis of user drop-off rates or abandonment during clarification turns is provided.
- Single-hop top-1 retrieval strategy may fail for complex queries requiring multi-document reasoning or historical context integration.

## Confidence
- Multi-turn dialogue with intent-slot mapping: **High** - The mechanism is well-specified with clear implementation details and reasonable evaluation metrics.
- Dense passage retrieval grounding: **Medium** - While the technical approach is clear, the sufficiency of top-1 retrieval for complex agricultural queries is not rigorously tested.
- Domain-specific instruction fine-tuning: **Medium** - Fine-tuning methodology is specified, but the claim of superior performance over generalist models lacks comparative analysis.

## Next Checks
1. Retrieval diversity ablation study: Compare top-1 vs. top-3 retrieval with passage re-ranking to measure impact on contextual relevance (PCR) and query accuracy (QRA), particularly for complex queries requiring multi-hop reasoning.
2. Intent schema stress test: Introduce queries from a third crop (e.g., rice) without adding intents to evaluate routing and retrieval degradation, using results to prioritize intent expansion roadmap.
3. Dialogue history integration: Implement history-aware query reformulation (e.g., appending previous turn slot values) and measure reduction in clarification turns and improvement in QCR, addressing the current lack of context carryover between turns.