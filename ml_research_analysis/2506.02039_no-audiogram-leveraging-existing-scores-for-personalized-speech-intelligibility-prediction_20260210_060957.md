---
ver: rpa2
title: 'No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility
  Prediction'
arxiv_id: '2506.02039'
source_url: https://arxiv.org/abs/2506.02039
tags:
- intelligibility
- audio
- speech
- score
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses personalized speech intelligibility prediction,
  challenging the traditional reliance on audiograms. Instead of using audiograms,
  the authors propose SSIPNet, a deep learning model that leverages an individual's
  existing intelligibility scores from multiple support (audio, score) pairs to predict
  their performance on new audio.
---

# No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction

## Quick Facts
- arXiv ID: 2506.02039
- Source URL: https://arxiv.org/abs/2506.02039
- Reference count: 0
- Primary result: SSIPNet outperforms audiogram-based methods on Clarity Prediction Challenge dataset

## Executive Summary
This paper presents SSIPNet, a deep learning model that predicts personalized speech intelligibility without requiring audiogram data. The model leverages existing intelligibility scores from support (audio, score) pairs to predict a listener's performance on new audio. SSIPNet uses speech foundation models to create high-dimensional representations of speech recognition ability, then aggregates these representations to make predictions. The approach challenges the traditional reliance on audiograms for personalized speech intelligibility prediction.

## Method Summary
SSIPNet is a deep learning model that predicts speech intelligibility scores for individual listeners without using audiograms. The model takes as input multiple support (audio, intelligibility score) pairs for a listener, processes the audio through speech foundation models to extract high-dimensional representations of speech recognition ability, and uses these representations to predict the listener's intelligibility score on new audio. The model aggregates information from the support samples to build a personalized profile of the listener's speech perception capabilities.

## Key Results
- SSIPNet achieves an average NCC of 0.811 on the Clarity Prediction Challenge dataset, outperforming the audiogram-based baseline (0.794)
- The model achieves an average RMSE of 23.430, compared to 26.160 for the baseline
- SSIPNet demonstrates strong performance even with minimal support samples, requiring as few as one support sample for effective prediction

## Why This Works (Mechanism)
SSIPNet works by leveraging the rich, high-dimensional representations extracted from speech foundation models, which capture nuanced aspects of speech perception that traditional audiograms may miss. By aggregating multiple support samples, the model builds a comprehensive profile of an individual's speech recognition abilities across different acoustic conditions. This approach allows the model to generalize to new audio stimuli based on the listener's demonstrated performance patterns rather than relying on a single summary metric like audiogram thresholds.

## Foundational Learning
1. **Speech foundation models** - Why needed: Extract rich, high-dimensional representations of speech that capture nuanced perceptual features; Quick check: Verify the model uses pre-trained speech models like Wav2Vec or HuBERT
2. **Support set learning** - Why needed: Enable personalization using limited examples of individual performance; Quick check: Confirm the model aggregates multiple (audio, score) pairs per listener
3. **Intelligibility score prediction** - Why needed: Directly predict subjective speech perception performance rather than audiometric thresholds; Quick check: Ensure output is a continuous intelligibility score, not categorical

## Architecture Onboarding
**Component map**: Audio -> Speech Foundation Model -> High-dimensional representation -> Aggregation module -> Intelligibility predictor -> Predicted score

**Critical path**: The critical path involves extracting speech representations from support audio, aggregating these representations into a listener profile, and using this profile to predict intelligibility on new audio.

**Design tradeoffs**: The model trades the simplicity and clinical standardization of audiograms for the flexibility and richness of personalized performance data. This allows for more nuanced predictions but requires collecting multiple support samples per listener.

**Failure signatures**: The model may fail when support samples are not representative of the listener's overall abilities, when audio conditions in support and target sets differ significantly, or when foundation models cannot capture relevant perceptual features for a particular listener profile.

**Three first experiments**:
1. Vary the number of support samples to identify the minimum required for stable performance
2. Test on audio from different acoustic conditions than the support set
3. Compare performance across listeners with different types of hearing profiles

## Open Questions the Paper Calls Out
None

## Limitations
- Performance has only been validated on a single dataset (Clarity Prediction Challenge)
- No comparison with multiple audiogram-based or audiogram-free baselines
- Potential biases and limitations in training data not addressed
- Generalization to atypical hearing profiles and different languages not evaluated

## Confidence
- NCC improvement (0.811 vs 0.794): Medium confidence
- RMSE improvement (23.430 vs 26.160): Medium confidence
- Performance with minimal support samples: Medium confidence

## Next Checks
1. Test SSIPNet's performance on multiple independent datasets with diverse hearing profiles, including atypical and age-related hearing loss, to assess external validity
2. Conduct a comparative study with several audiogram-based and audiogram-free baselines across varied languages and acoustic environments to determine robustness and generalizability
3. Perform a bias and fairness audit to identify whether SSIPNet's predictions are equitable across different demographic groups and hearing conditions