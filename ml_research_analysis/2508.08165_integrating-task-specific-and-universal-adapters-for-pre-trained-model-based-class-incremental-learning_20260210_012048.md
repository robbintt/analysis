---
ver: rpa2
title: Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based
  Class-Incremental Learning
arxiv_id: '2508.08165'
source_url: https://arxiv.org/abs/2508.08165
tags:
- uni00000013
- learning
- adapter
- uni00000055
- task-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in class-incremental
  learning (CIL) by proposing a method that leverages pre-trained models with adapter
  modules. Existing approaches suffer from incorrect adapter selection and overlook
  shared knowledge across tasks.
---

# Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning

## Quick Facts
- **arXiv ID**: 2508.08165
- **Source URL**: https://arxiv.org/abs/2508.08165
- **Reference count**: 40
- **Primary result**: Achieves 79.42% average accuracy on ImageNet-R and 73.78% on ImageNet-A, outperforming existing approaches in class-incremental learning

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by introducing Task-Specific and Universal Adapters (TUNA). The method leverages pre-trained models with adapter modules, training orthogonal task-specific adapters for each incremental task while fusing them into a universal adapter that captures shared knowledge. An entropy-based selection mechanism dynamically chooses the most suitable task-specific adapter during inference, and predictions from both adapter types are combined. The approach demonstrates state-of-the-art performance on benchmark datasets while operating exemplar-free, making it memory-efficient compared to traditional methods.

## Method Summary
The proposed TUNA method integrates task-specific and universal adapters within pre-trained models to address catastrophic forgetting in class-incremental learning. Task-specific adapters are trained orthogonally for each incremental task, while a universal adapter captures shared knowledge across tasks through fusion. During inference, an entropy-based mechanism selects the most appropriate task-specific adapter based on prediction confidence. The final prediction combines outputs from both the selected task-specific adapter and the universal adapter. This exemplar-free approach achieves strong performance on challenging datasets like ImageNet-A and ObjectNet while maintaining memory efficiency.

## Key Results
- Achieves 79.42% average accuracy on ImageNet-R and 73.78% on ImageNet-A, outperforming existing approaches
- Demonstrates state-of-the-art performance on challenging datasets while operating exemplar-free
- Significant improvements in class-incremental learning scenarios where traditional methods struggle with catastrophic forgetting

## Why This Works (Mechanism)
TUNA addresses catastrophic forgetting by maintaining separate task-specific adapters that preserve task-specific knowledge while creating a universal adapter that captures shared representations across tasks. The orthogonal training of task-specific adapters prevents interference between different tasks, while the entropy-based selection mechanism ensures appropriate adapter usage during inference. The combination of task-specific and universal predictions provides robustness by leveraging both specialized and generalized knowledge.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to lose previously learned information when trained on new tasks; critical to understand why incremental learning is challenging
- **Adapter modules**: Small neural network components that can be inserted into pre-trained models to adapt them to new tasks without full fine-tuning; quick check: verify adapters have significantly fewer parameters than full models
- **Orthogonal training**: Training methods that minimize interference between different components; why needed: to prevent task-specific adapters from conflicting with each other
- **Entropy-based selection**: Using prediction uncertainty to guide decision-making; quick check: higher entropy should indicate lower confidence in adapter selection
- **Knowledge fusion**: Combining information from multiple sources to create more robust representations; why needed: to capture shared patterns across different tasks
- **Exemplar-free learning**: Incremental learning without storing training examples from previous tasks; quick check: verify memory usage remains constant across incremental steps

## Architecture Onboarding

**Component map**: Pre-trained model -> Task-specific adapters (one per task) -> Universal adapter (fused) -> Entropy-based selector -> Combined prediction

**Critical path**: During inference, the entropy-based selector evaluates prediction confidence from task-specific adapters, selects the most appropriate one, and combines its output with the universal adapter's prediction for the final result.

**Design tradeoffs**: The method trades some potential performance gains from storing exemplars for significant memory efficiency and the ability to handle truly streaming data scenarios.

**Failure signatures**: Poor adapter selection when task boundaries are ambiguous, performance degradation when tasks have high similarity, and potential confusion when universal adapter knowledge conflicts with task-specific knowledge.

**First experiments**: 1) Validate adapter orthogonality by measuring interference between different task-specific adapters, 2) Test entropy-based selection accuracy across varying task similarities, 3) Compare memory usage against exemplar-based methods under identical task sequences.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though it implies potential areas for investigation such as the robustness of the entropy-based selection mechanism under real-world noisy conditions and the generalizability to non-image domains.

## Limitations
- The entropy-based adapter selection mechanism's performance under real-world noisy conditions has not been extensively validated
- Memory efficiency benefits relative to methods using limited exemplars are not quantified
- Generalizability to other domains beyond image classification remains untested

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core methodology of orthogonal task-specific and universal adapters | High |
| Entropy-based adapter selection mechanism | Medium |
| Exemplar-free operation benefits | Medium |

## Next Checks
1. Test the entropy-based adapter selection mechanism on datasets with significant label noise to assess robustness
2. Compare memory usage and performance with methods that use a small number of exemplars to quantify the benefits of the exemplar-free approach
3. Evaluate the generalizability of TUNA on non-image datasets or tasks with different characteristics (e.g., natural language processing) to assess cross-domain applicability