---
ver: rpa2
title: 'Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial
  IoT Network: A DRL-based Method with Bayesian Optimization'
arxiv_id: '2512.23493'
source_url: https://arxiv.org/abs/2512.23493
tags:
- scheduling
- ieee
- algorithm
- networks
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of joint link adaptation and device
  scheduling in industrial IoT networks with imperfect channel state information (CSI)
  to maximize transmission rate while satisfying strict block error rate (BLER) constraints.
  The authors propose a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic
  Policy Gradient (TD3) method that determines device serving order and modulation
  and coding scheme (MCS) adaptively.
---

# Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization

## Quick Facts
- arXiv ID: 2512.23493
- Source URL: https://arxiv.org/abs/2512.23493
- Reference count: 40
- Key outcome: BO-driven TD3 method achieves faster convergence and higher sum-rate while meeting BLER constraints compared to baseline schemes

## Executive Summary
This paper addresses joint link adaptation and device scheduling in industrial IoT networks with imperfect CSI to maximize transmission rate while satisfying strict BLER constraints. The authors propose a Bayesian optimization driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method that determines device serving order and MCS adaptively. The BO-based training mechanism improves convergence speed by providing reliable action selection and addressing the sample imbalance issue between ACK and NACK feedback in URLLC networks.

## Method Summary
The proposed method integrates Bayesian Optimization with a modified EXP3 bandit algorithm (GEXP) within TD3 training to provide uncertainty-aware action selection. The approach uses separate replay buffers for ACK and NACK samples to handle the severe imbalance in URLLC feedback, with NACK samples prioritized for learning. An Outer Loop Link Adaptation (OLLA) correction term compensates for CSI imperfections during execution, creating a closed-loop system that maintains BLER within target constraints.

## Key Results
- Proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions
- Method outperforms baseline schemes including L-DQN, BO-CMAB, and OLLA-CMAB in both idealized and discrete MCS network scenarios
- Successfully meets BLER requirements of 10⁻³ while maximizing throughput

## Why This Works (Mechanism)

### Mechanism 1: GEXP-BO Joint Optimization for Accelerated Convergence
Integrating Bayesian Optimization with a modified EXP3 bandit algorithm within TD3 training provides more reliable action selection, reducing convergence time and stabilizing policy learning. The target critic network's Q-value serves as a surrogate objective for a Gaussian Process surrogate model, with the GP posterior distribution guiding action selection via Expected Improvement acquisition function for MCS, while GEXP handles device scheduling with adaptive weight updates based on Q-value feedback.

### Mechanism 2: Sample Selection for ACK/NACK Imbalance Mitigation
Separate replay buffers for ACK and NACK samples with prioritized NACK sampling enables effective learning from rare failure events, reducing BLER constraint violations. NACK samples are stored in dedicated buffer D_NACK and sampled periodically, while for D_ACK, half the mini-batch is random and half from temporally adjacent slots to exploit channel correlation.

### Mechanism 3: OLLA-Based MCS Correction for Execution Reliability
An Outer Loop Link Adaptation correction term applied to DRL-selected MCS during execution compensates for CSI imperfections, keeping BLER within target. After TD3 outputs MCS adjustment, OLLA applies an additive correction based on running BLER statistics, creating a closed-loop bias toward conservative MCS when NACKs increase.

## Foundational Learning

- **Gaussian Process Surrogate Models**: Used to provide uncertainty quantification for Q-value predictions, enabling Expected Improvement acquisition function to balance exploration vs. exploitation during training. *Quick check: Can you explain why a GP with Matérn kernel is preferred over a deterministic neural network surrogate for this optimization task?*

- **Multi-Armed Bandit (EXP3 Algorithm)**: Handles discrete selection among K devices where channel conditions change unpredictably. *Quick check: What happens to EXP3 weight updates if a device's channel quality degrades permanently during training?*

- **Twin Delayed DDPG (TD3) Architecture**: Addresses DDPG overestimation bias via twin critics and delayed policy updates, critical when reward signals are sparse due to URLLC reliability constraints. *Quick check: Why does TD3 use minimum of two Q-values rather than average for target calculation?*

## Architecture Onboarding

- Component map: [Environment: IIoT Network] → [State Builder] → [Actor Network] → [OLLA Correction] → [Execution] → [Replay Buffer] → [Training Loop] → [Target Critic Networks] → [GEXP-BO Module] → [DPG + SGD]

- Critical path: 1) State construction from quantized/outdated CQI, 2) OLLA correction term computation, 3) GEXP-BO action optimization, 4) Sample selection from split replay buffers

- Design tradeoffs: BO surrogate model accuracy vs. computational cost; OLLA step size ι affects stability vs. tracking speed; NACK sampling frequency Tₙ affects bias vs. learning signal

- Failure signatures: Convergence stall with high Q-value variance indicates GP kernel mismatch; BLER violations despite OLLA suggest CSI outdated beyond compensation; device starvation shows GEXP weight collapse

- First 3 experiments: 1) Ablation: BO vs. ε-greedy exploration to measure convergence improvement, 2) Sensitivity: OLLA step size ι to find stability threshold, 3) Scalability: Device count K to verify performance gap trends

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- BO surrogate model assumes stationary Q-value landscapes but industrial IoT channels exhibit non-stationary dynamics
- OLLA correction may fail when channel dynamics are faster than OLLA convergence rate
- Sample imbalance mitigation through NACK prioritization lacks empirical validation for frequency selection

## Confidence

- **High confidence**: TD3 architecture with twin critics and delayed updates is well-established; separate ACK/NACK replay buffers and OLLA correction are standard URLLC practices
- **Medium confidence**: BO-GEXP hybrid acceleration depends critically on GP surrogate accuracy and GEXP weight stability, requiring rigorous ablation testing
- **Low confidence**: BLER constraint satisfaction claims need validation, particularly for fast-fading channels where OLLA corrections may lag

## Next Checks

1. **GP surrogate fidelity test**: Record correlation between GP-predicted Q-values and actual critic outputs; plot prediction RMSE vs. training epoch to quantify surrogate accuracy degradation

2. **OLLA convergence under mobility**: Implement mobile device scenario with speed-dependent Doppler spread; measure BLER violation frequency vs. OLLA step size for different velocities

3. **GEXP weight stability analysis**: Track GEXP weight vectors across training episodes; plot weight entropy to detect premature convergence to suboptimal device subsets