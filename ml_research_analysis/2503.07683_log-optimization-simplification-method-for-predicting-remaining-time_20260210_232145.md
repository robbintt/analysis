---
ver: rpa2
title: Log Optimization Simplification Method for Predicting Remaining Time
arxiv_id: '2503.07683'
source_url: https://arxiv.org/abs/2503.07683
tags:
- prediction
- time
- event
- community
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting remaining time
  in business processes using event logs that contain significant low-value and redundant
  information. The authors propose a log optimization simplification method that combines
  prediction point selection based on resource community networks with structural
  reduction of Generalized Stochastic Petri Nets (GSPN).
---

# Log Optimization Simplification Method for Predicting Remaining Time

## Quick Facts
- arXiv ID: 2503.07683
- Source URL: https://arxiv.org/abs/2503.07683
- Authors: Jianhong Ye; Siyuan Zhang; Yan Lin
- Reference count: 26
- One-line primary result: Log optimization method maintains or enhances remaining time prediction accuracy while reducing data volume by up to 51%

## Executive Summary
This paper presents a novel log optimization simplification method for predicting remaining time in business processes using event logs. The approach addresses the challenge of processing event logs containing significant low-value and redundant information by combining prediction point selection based on resource community networks with structural reduction of Generalized Stochastic Petri Nets (GSPN). The method strategically selects prediction points from resource communities to avoid over-simplification while applying reduction rules for sequence, OR, and self-loop substructures. Experimental results using sepsis patient data from a Dutch hospital demonstrate that the simplified event logs maintain or enhance prediction accuracy while achieving substantial data volume reduction.

## Method Summary
The proposed log optimization method integrates two key components: prediction point selection using resource community networks and GSPN structural reduction. The prediction point selection process identifies critical points in the process where remaining time predictions are most valuable, leveraging community detection in resource networks to ensure comprehensive coverage without over-simplification. The GSPN reduction component applies systematic rules to simplify process structures while preserving essential predictive information. The optimization framework balances the tradeoff between log size reduction and prediction accuracy, ensuring that simplification enhances computational efficiency without compromising model performance. The method is validated through experiments comparing performance against traditional filtering approaches using metrics such as F1 score, AUC, and MSE.

## Key Results
- Simplified event logs achieved up to 51% reduction in data volume while maintaining or improving prediction accuracy
- The method outperformed traditional filtering approaches across multiple performance metrics (F1, AUC, MSE)
- Experimental validation on sepsis patient data demonstrated statistically significant improvements in prediction quality

## Why This Works (Mechanism)
The method works by strategically reducing complexity in event logs while preserving critical predictive information. By selecting prediction points from resource communities, the approach ensures that important process transitions are captured without including redundant or low-value events. The GSPN reduction rules systematically simplify process structures by identifying and removing substructures that don't contribute significantly to prediction accuracy, such as predictable sequences or self-loops. This dual approach of intelligent point selection and structural simplification allows the model to focus computational resources on the most informative parts of the process while maintaining the essential patterns needed for accurate remaining time prediction.

## Foundational Learning
- **Resource Community Networks**: Represent relationships between different resources in a process; needed to identify prediction points that capture resource interactions, quick check: verify community detection identifies meaningful resource groupings
- **Generalized Stochastic Petri Nets (GSPN)**: Mathematical model for representing concurrent and stochastic processes; needed to capture complex process behaviors and enable systematic reduction, quick check: ensure reduction rules preserve stochastic properties
- **Prediction Point Selection**: Identifying optimal points in the process for making remaining time predictions; needed to focus model attention on critical decision points, quick check: validate selected points align with process milestones
- **Log Size Reduction**: Decreasing the volume of event log data while preserving predictive information; needed to improve computational efficiency and reduce noise, quick check: measure information loss versus size reduction ratio
- **Structural Reduction Rules**: Systematic methods for simplifying process substructures (sequences, OR forks, self-loops); needed to eliminate redundant process patterns without losing predictive power, quick check: verify reduced structures maintain equivalent predictive capabilities
- **Prediction Accuracy Metrics**: F1 score, AUC, and MSE for evaluating model performance; needed to quantitatively assess tradeoffs between simplification and accuracy, quick check: establish baseline performance for comparison

## Architecture Onboarding
**Component Map**: Event Log -> Resource Community Detection -> Prediction Point Selection -> GSPN Construction -> Structural Reduction -> Simplified Log -> Prediction Model

**Critical Path**: The critical execution path involves detecting resource communities, selecting prediction points based on these communities, constructing the GSPN from the original log, applying reduction rules to simplify the GSPN structure, and using the resulting simplified log for training the prediction model.

**Design Tradeoffs**: The primary tradeoff involves balancing log size reduction against prediction accuracy. More aggressive simplification yields greater computational efficiency but risks losing predictive information. The method addresses this by using resource communities to guide prediction point selection, ensuring critical process elements are preserved while removing redundancy.

**Failure Signatures**: Potential failures include over-simplification leading to loss of critical predictive patterns, under-selection of prediction points resulting in incomplete process coverage, and reduction rules that inadvertently remove essential process variations. The method mitigates these risks through community-based selection and systematic verification of reduced structures.

**3 First Experiments**:
1. Compare prediction accuracy on simplified versus original logs using multiple metrics (F1, AUC, MSE)
2. Measure data volume reduction achieved by different levels of GSPN simplification
3. Analyze the distribution of prediction points selected across resource communities to verify coverage

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation relies on a single sepsis patient dataset from one Dutch hospital, limiting generalizability to other domains
- Performance on non-healthcare business processes remains untested, constraining applicability assessment
- The method's effectiveness depends on the assumption that simplified substructures maintain equivalent predictive power, which may not hold for all process patterns

## Confidence
- **High** confidence in the core claim that the method maintains or enhances prediction accuracy while reducing data volume
- **Medium** confidence in the GSPN reduction approach, as theoretical framework is well-established but practical effectiveness across diverse process structures requires further validation
- **Medium** confidence in the prediction point selection mechanism, as method shows improved performance over traditional filtering but may miss critical points in highly variable processes

## Next Checks
1. Evaluate the method on diverse event logs from multiple domains (manufacturing, finance, IT service management) to assess generalizability beyond healthcare
2. Conduct ablation studies to quantify the individual contribution of prediction point selection versus GSPN reduction to overall performance improvements
3. Test the method with different prediction algorithms (beyond LSTM) to verify that accuracy gains are not algorithm-specific and to identify potential limitations with certain model architectures