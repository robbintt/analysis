---
ver: rpa2
title: 'Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge'
arxiv_id: '2506.18998'
source_url: https://arxiv.org/abs/2506.18998
tags:
- self-knowledge
- llms
- arxiv
- task
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how large language models (LLMs) confuse
  memorization with reasoning, leading to artificially inflated self-knowledge. It
  introduces a novel framework where models generate and validate feasible STEM tasks,
  then assess their feasibility after minor perturbations.
---

# Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge

## Quick Facts
- arXiv ID: 2506.18998
- Source URL: https://arxiv.org/abs/2506.18998
- Authors: Sahil Kale
- Reference count: 40
- Primary result: Novel framework shows LLMs exhibit over 45% feasibility judgment inconsistency across STEM domains, indicating memorization-driven overconfidence

## Executive Summary
This study reveals a fundamental flaw in how large language models assess their own capabilities, demonstrating that memorization creates an illusion of mastery that leads to artificially inflated self-knowledge. The research introduces a novel framework where models generate and validate STEM tasks, then assess feasibility after minor perturbations, exposing significant inconsistency in their self-assessments. Across science and medicine domains, models show the highest memorization-driven overconfidence, with feasibility judgments varying by over 45% when tasks are slightly modified. The proposed MIRAGE and SKEW metrics quantify how memorization undermines reliable self-assessment, posing serious risks for critical applications where accurate self-knowledge is essential.

## Method Summary
The study employs a novel framework to evaluate LLM self-knowledge by having models generate and validate feasible STEM tasks, then assessing their feasibility judgments after introducing minor perturbations to the tasks. This approach tests the consistency of feasibility assessments, with high inconsistency indicating overconfidence driven by memorization rather than genuine reasoning capabilities. The methodology focuses on STEM domains including science and medicine, where models demonstrate the highest levels of memorization-driven overconfidence. The framework introduces two key metrics: MIRAGE for quantifying memorization-induced self-knowledge inflation and SKEW for measuring stability of self-assessments under perturbations.

## Key Results
- Models show over 45% inconsistency in feasibility judgments across STEM domains when tasks undergo minor perturbations
- Science and medicine domains exhibit the highest memorization-driven overconfidence in self-assessments
- MIRAGE and SKEW metrics successfully quantify memorization-induced self-knowledge inflation and assessment stability
- Feasibility consistency testing reveals that memorization significantly undermines reliable self-assessment in LLMs

## Why This Works (Mechanism)
The mechanism works because LLMs rely heavily on pattern matching and memorization from training data rather than developing genuine reasoning capabilities. When faced with slightly modified versions of tasks they've encountered during training, models struggle to maintain consistent feasibility assessments because they're pattern-matching to memorized solutions rather than applying logical reasoning. This creates artificial inflation in self-knowledge as models appear confident about tasks they've seen before, even when those tasks are fundamentally altered. The perturbation approach exposes this weakness by testing whether models can maintain consistent judgments when the underlying reasoning requirements change subtly.

## Foundational Learning
- **Pattern Recognition vs. Reasoning**: Understanding the difference between memorizing patterns and genuine logical reasoning is crucial for evaluating LLM capabilities and their limitations.
- **Self-Knowledge Assessment**: LLMs need reliable methods to evaluate their own capabilities, but current approaches may be compromised by memorization effects.
- **Feasibility Consistency**: The ability to maintain consistent feasibility judgments across similar but modified tasks indicates genuine reasoning rather than memorization.
- **Metric Design**: Developing appropriate metrics (MIRAGE, SKEW) to quantify memorization effects and assessment stability is essential for evaluating LLM self-knowledge quality.
- **Domain-Specific Effects**: Different knowledge domains may exhibit varying levels of memorization-driven overconfidence, requiring domain-aware evaluation approaches.
- **Perturbation Testing**: Minor modifications to tasks can reveal whether models are reasoning or pattern-matching, serving as a diagnostic tool for genuine capability assessment.

## Architecture Onboarding
- **Component Map**: Task Generation -> Feasibility Validation -> Perturbation Application -> Consistency Assessment -> MIRAGE/SKEW Metric Calculation
- **Critical Path**: The perturbation application and consistency assessment stages are critical for exposing memorization-driven overconfidence, as they directly test whether models can maintain reliable self-assessments under minor modifications.
- **Design Tradeoffs**: The framework balances between task complexity (to ensure meaningful perturbations) and task simplicity (to maintain feasibility assessment reliability), with the risk that overly complex perturbations might introduce confounding factors beyond memorization effects.
- **Failure Signatures**: High inconsistency rates across perturbations, domain-specific overconfidence spikes, and metrics showing significant MIRAGE/SKEW values all indicate memorization-driven self-knowledge inflation rather than genuine reasoning capability.
- **First Experiments**: 
  1. Test consistency rates across varying perturbation magnitudes to determine sensitivity thresholds
  2. Compare feasibility consistency between trained models and models with controlled memorization levels
  3. Evaluate domain transfer effects by testing consistency across STEM subdomains

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The perturbation approach may not fully capture nuanced differences between reasoning and memorization, potentially confounding input sensitivity with memorization effects
- Results based on STEM tasks may not generalize to broader knowledge domains or more complex reasoning scenarios
- MIRAGE and SKEW metrics measure internal consistency rather than absolute accuracy, limiting their real-world applicability
- The study does not address how memorization-induced overconfidence manifests differently in zero-shot versus few-shot settings

## Confidence
- Core findings on over 45% inconsistency rates: Medium
- Domain-specific variations (science/medicine overconfidence): Medium
- MIRAGE and SKEW metric applicability: Low
- Generalizability to non-STEM domains: Low
- Relationship to practical LLM performance: Low

## Next Checks
1. Replicate feasibility consistency testing across non-STEM domains (humanities, social sciences) to determine if memorization-driven overconfidence is domain-specific or a general LLM characteristic
2. Conduct ablation studies with models fine-tuned at varying memorization levels to quantify the relationship between memorization capacity and feasibility inconsistency rates
3. Implement longitudinal testing to evaluate how consistency rates change with exposure to diverse training data or additional fine-tuning, particularly examining whether varied problem-solving approaches reduce memorization reliance