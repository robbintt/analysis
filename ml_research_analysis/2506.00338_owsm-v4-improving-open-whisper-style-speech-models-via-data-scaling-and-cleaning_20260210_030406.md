---
ver: rpa2
title: 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning'
arxiv_id: '2506.00338'
source_url: https://arxiv.org/abs/2506.00338
tags:
- data
- owsm
- speech
- yodas
- owsm-ctc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of insufficient training data
  for fully open speech foundation models by incorporating a large-scale, web-crawled
  dataset called YODAS. The core method involves developing a scalable data-cleaning
  pipeline using public toolkits to address issues such as incorrect language labels
  and audio-text misalignments in YODAS.
---

# OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning

## Quick Facts
- arXiv ID: 2506.00338
- Source URL: https://arxiv.org/abs/2506.00338
- Reference count: 0
- New models trained on cleaned web-crawled speech data match or surpass leading industrial models like Whisper and MMS on multilingual benchmarks

## Executive Summary
This paper addresses the challenge of insufficient training data for fully open speech foundation models by incorporating a large-scale, web-crawled dataset called YODAS. The core method involves developing a scalable data-cleaning pipeline using public toolkits to address issues such as incorrect language labels and audio-text misalignments in YODAS. The cleaned dataset comprises 166,000 hours of speech across 75 languages. The primary result is the development of OWSM v4, a new series of models trained on the curated dataset alongside existing OWSM data, which significantly outperforms previous versions on multilingual benchmarks. The OWSM v4 models match or surpass leading industrial models like Whisper and MMS in multiple scenarios, demonstrating the effectiveness of data scaling and cleaning. The cleaned YODAS data, pre-trained models, and associated scripts will be publicly released.

## Method Summary
The authors developed a three-step data-cleaning pipeline for the YODAS web-crawled dataset: (1) CTC segmentation using an OWSM-CTC model to re-align audio-text pairs and generate confidence scores, (2) dual-modality language identification filtering requiring both text and audio LID predictions to match the original label, and (3) CTC-score filtering to remove low-confidence samples. The cleaned YODAS dataset (166k hours, 75 languages) was combined with existing OWSM v3.2 data to create a 320k-hour training set. OWSM v4 models (base/small/medium AED architectures, 1B CTC) were trained using E-Branchformer encoders, 128 Mel filterbanks, and standard Whisper-style multitask training.

## Key Results
- OWSM v4 medium model achieves 9.4% average WER on MLS, significantly outperforming v3.1 medium's 15.5%
- OWSM v4 models match or exceed Whisper-large-v3 and MMS performance on FLEURS, Common Voice, and HuggingFace leaderboard benchmarks
- The data-cleaning pipeline successfully removes misaligned samples, fixing WER>100% issues observed with unfiltered data
- ST performance is maintained while ASR performance improves significantly across all model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTC confidence scores from forced alignment provide a trainable signal for detecting audio-text misalignments in web-crawled speech data.
- Mechanism: The CTC segmentation algorithm re-aligns audio and text using a pre-trained OWSM-CTC model, producing a per-utterance confidence score. Low scores indicate poor alignment (e.g., wrong transcript, non-speech audio). Filtering the lowest-scoring quantile removes noisy samples that would otherwise destabilize training.
- Core assumption: CTC scores from a pre-trained model generalize to detecting alignment errors in out-of-domain web data.
- Evidence anchors:
  - [abstract] "audio-text misalignments... we develop a scalable data-cleaning pipeline using public toolkits"
  - [Section 2.1.1] "each short utterance is assigned a confidence score, which quantifies the alignment quality between the audio and the corresponding text"
  - [Section 2.1.3] Table 2 shows WER >100% without filtering vs. 4-25% with θCTC > 0
- Break condition: If the pre-trained CTC model poorly covers target languages or domains, scores become unreliable and filtering may remove valid data or retain garbage.

### Mechanism 2
- Claim: Cross-modal language label verification (audio LID + text LID) removes incorrectly labeled samples more robustly than single-modality checks.
- Mechanism: Each sample's metadata language label must match both (a) fastText text-based LID prediction and (b) SpeechBrain ECAPA-TDNN audio-based LID prediction. Mismatches indicate annotation errors common in web crawls.
- Core assumption: Both LID models have sufficient accuracy on web-scraped audio and text.
- Evidence anchors:
  - [abstract] "incorrect language labels... audio-text misalignments"
  - [Section 2.1.2] "We retain only those utterances for which the original language label matches both the predicted language from the text and the predicted language from the audio"
  - [Section 3.1] "LID results... our cleaned YODAS data contains high-quality language labels, attributed to the LID filtering stage"
- Break condition: If LID models have low accuracy on certain languages or noisy audio, valid samples may be incorrectly filtered.

### Mechanism 3
- Claim: Training on cleaned, scaled data yields consistent cross-task improvements even without increasing model capacity.
- Mechanism: The cleaned 166k-hour YODAS dataset (from 370k raw hours) combined with existing OWSM v3.2 data creates a 320k-hour training set. The same model architectures (E-Branchformer encoder, Transformer decoder for AED; encoder-only for CTC) trained on this larger, cleaner corpus outperform previous versions trained on less/noisier data.
- Core assumption: Data quality gains from cleaning outweigh the reduction in raw hours.
- Evidence anchors:
  - [abstract] "OWSM v4 models... significantly outperform previous versions on multilingual benchmarks"
  - [Section 3.2] Table 6: OWSM v4 medium achieves 9.4% avg WER vs. v3.1 medium's 15.5% on MLS
  - [corpus] Related work (Whale, OWSM-Biasing) confirms scaling + cleaning as effective, though corpus lacks direct replication of this specific pipeline
- Break condition: If cleaning is too aggressive (high θCTC), low-resource languages may lose critical data, harming minority-language performance.

## Foundational Learning

- Concept: **Connectionist Temporal Classification (CTC)**
  - Why needed here: CTC provides both the alignment mechanism (via CTC segmentation) and the confidence scores used for filtering. Also used in OWSM-CTC encoder-only models.
  - Quick check question: Can you explain why CTC produces alignment confidence scores, and how they differ from attention-based alignment?

- Concept: **E-Branchformer architecture**
  - Why needed here: All OWSM v4 models use E-Branchformer as the speech encoder. Understanding its branching structure (local conv + global attention) is required for model modification.
  - Quick check question: How does E-Branchformer differ from standard Conformer, and what does the "enhanced merging" operation do?

- Concept: **Whisper-style multitask training**
  - Why needed here: OWSM follows Whisper's multitask format (ASR, ST, LID, VAD) with special tokens indicating task type.
  - Quick check question: What tokens or format changes are needed to switch between ASR and speech translation inference?

## Architecture Onboarding

- Component map: Raw YODAS (370k hrs, 149 langs) -> CTC Segmentation (OWSM-CTC v3.2) -> Resegmented (345k hrs, 83 langs) + CTC scores -> LID Filtering (fastText + ECAPA-TDNN) -> LID-filtered (284k hrs, 75 langs) -> CTC-score Filtering (θCTC = 0.10) -> Cleaned YODAS (166k hrs, 75 langs) -> Merge with OWSM v3.2 data -> Final training set (320k hrs) -> Train OWSM v4 (base/small/medium AED, 1B CTC)
- Critical path: CTC segmentation quality determines downstream filter reliability. If alignment is poor, both LID and CTC-score filtering become noisy.
- Design tradeoffs:
  - θCTC threshold: Lower = more data retained but noisier; higher = cleaner but smaller (Table 3 shows 283.6k -> 43k hrs as θCTC goes 0.0 -> 0.30)
  - Language imbalance: English dominates (74.6k/166k hrs at θCTC=0.10). No resampling applied—simpler but risks English bias.
  - Mel filterbanks: Increased from 80 to 128 (matching Whisper-large-v3), improving quality at slight compute cost.
- Failure signatures:
  - WER >100% with repetition loops during decoding -> indicates misaligned training data (Section 2.1.3, θCTC = 0.00)
  - LID accuracy drops on specific languages -> LID models may have poor coverage for those languages
  - ST performance degrades -> ASR data addition may have corrupted multitask formatting (Section 3.4 confirms this did NOT happen, but is a risk)
- First 3 experiments:
  1. Validate CTC-score threshold: Fine-tune a small model on cleaned YODAS with varying θCTC (0.0, 0.05, 0.10, 0.15, 0.20) and evaluate on held-out Common Voice + long-form audio. Replicate Table 2 to confirm threshold selection.
  2. Ablate LID filtering: Train with and without the dual-modality LID check. Measure LID accuracy and ASR WER on FLEURS to quantify the filtering contribution.
  3. Cross-dataset transfer: Apply the same cleaning pipeline to a different web-crawled dataset (e.g., MOSEL or GigaSpeech) to test generalization of the method beyond YODAS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would language-balanced resampling strategies significantly improve performance on low-resource languages compared to the imbalanced distribution retained in this study?
- Basis in paper: [explicit] The authors state, "For simplicity, in this work, we keep the original distribution without any resampling," despite noting that "the distribution across languages is highly imbalanced."
- Why unresolved: The study prioritized a simple data curation process to demonstrate the viability of web-crawled data, leaving data rebalancing strategies for future investigation.
- What evidence would resolve it: Training an OWSM v4 variant using over/under-sampling techniques on the cleaned YODAS data and comparing WER/CER results on underrepresented languages against the baseline.

### Open Question 2
- Question: Can language-specific CTC-score thresholds yield better ASR performance than the global threshold ($\theta_{CTC} = 0.10$) applied across all 75 languages?
- Basis in paper: [explicit] The paper notes, "Although finer-grained filtering could potentially optimize performance for individual languages, we opt for a threshold of $\theta_{CTC} = 0.10$."
- Why unresolved: A global threshold was chosen to retain the majority of data and ensure general good performance, but the trade-off regarding specific language optimization was not explored.
- What evidence would resolve it: An ablation study optimizing $\theta_{CTC}$ individually for high-resource versus low-resource languages and measuring the resulting delta in error rates.

### Open Question 3
- Question: Does incorporating speech translation (ST) data from the YODAS dataset improve multilingual ST performance, or does it introduce noise that degrades the model?
- Basis in paper: [explicit] The authors clarify, "We do not add any new ST data... Here, our goal is to show that our v4 model maintains similar ST performance."
- Why unresolved: The current work strictly focused on leveraging YODAS for ASR tasks, deferring the integration of its translation data to future work.
- What evidence would resolve it: Training a new model iteration on the cleaned YODAS ST pairs and evaluating BLEU score changes on the CoVoST-2 benchmark.

## Limitations

- The dual LID filtering mechanism assumes both text and audio LID models have sufficient coverage and accuracy across 75 languages, yet LID performance degradation on low-resource languages is not explicitly tested
- The CTC confidence score filtering assumes pre-trained CTC scores generalize to out-of-domain web data; if the pre-trained model poorly covers certain accents or domains, filtering may be unreliable
- The paper reports no explicit analysis of language imbalance in the cleaned dataset (English dominates at 74.6k/166k hours), raising concerns about potential bias and whether cleaning exacerbates this skew

## Confidence

- **High confidence**: OWSM v4 models trained on cleaned YODAS + OWSM v3.2 data significantly outperform previous OWSM versions on multilingual benchmarks (WER reductions from 15.5% to 9.4% on MLS, etc.)
- **Medium confidence**: The data cleaning pipeline (CTC segmentation + dual LID + CTC-score filtering) reliably removes misaligned and incorrectly labeled samples
- **Medium confidence**: OWSM v4 models match or surpass leading industrial models (Whisper, MMS) on standard benchmarks

## Next Checks

1. Independent LID filtering ablation: Train two models on identical data splits—one with the full dual LID filtering, one without—and evaluate LID accuracy on FLEURS and ASR WER on Common Voice to quantify the filtering contribution per language.
2. Cross-dataset cleaning generalization: Apply the exact same CTC segmentation + LID + CTC-score filtering pipeline to a different web-crawled dataset (e.g., MOSEL or GigaSpeech) and measure the resulting WER improvement on a held-out subset to test method robustness beyond YODAS.
3. Language imbalance analysis: Stratify FLEURS and Common Voice test results by language and plot per-language WER distributions. Compare against the cleaned YODAS training distribution to assess whether aggressive cleaning disproportionately harms low-resource languages.