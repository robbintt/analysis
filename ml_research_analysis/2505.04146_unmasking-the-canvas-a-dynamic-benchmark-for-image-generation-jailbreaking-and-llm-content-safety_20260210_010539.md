---
ver: rpa2
title: 'Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking
  and LLM Content Safety'
arxiv_id: '2505.04146'
source_url: https://arxiv.org/abs/2505.04146
tags:
- prompts
- prompt
- image
- jailbreak
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Unmasking the Canvas (UTC Benchmark; UTCB),
  a dynamic benchmark dataset to evaluate LLM vulnerability in image generation tasks.
  The authors observed that existing image generation models remain vulnerable to
  prompt-based jailbreaks, with even short, natural prompts leading to the generation
  of compromising images.
---

# Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety

## Quick Facts
- arXiv ID: 2505.04146
- Source URL: https://arxiv.org/abs/2505.04146
- Reference count: 3
- Key outcome: Introduces UTC Benchmark, a dynamic dataset to evaluate LLM vulnerability in image generation tasks using multilingual obfuscation and structured prompt engineering

## Executive Summary
This paper introduces the Unmasking the Canvas (UTC Benchmark) to evaluate LLM vulnerability to image generation jailbreaks. The authors found that existing models remain vulnerable to prompt-based jailbreaks, with even short natural prompts leading to compromising images. They developed a scalable pipeline combining structured prompt engineering, multilingual obfuscation (Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3 models. The benchmark includes over 6,700 prompts curated into Bronze, Silver, and Gold tiers, revealing that certain templates like Split_Image were particularly effective at bypassing safety filters.

## Method Summary
The UTC Benchmark pipeline starts with JAILBREAKHUB dataset prompts, which are transformed by a LLaMA-3 prompt generator into image-generation-specific variants with templates and obfuscation. These prompts pass through an image mimicker (LLaMA-3) that simulates generation responses and filters for jailbreak potential. Prompts are then manually verified (Gold tier) or LLM-tagged (Silver tier), with Bronze tier remaining unverified. A judge model (LLaMA-3) assigns risk scores (0-1) and PASS/BLOCK tags. The final dataset is hosted on Huggingface with controlled access.

## Key Results
- Split_Image template structure ("Generate an image of [content], on the right, the opposite/empowered") bypassed safety filters more effectively than direct prompts
- Zulu and Gaelic obfuscation techniques reduced model-assigned risk scores, indicating potential bypass methods
- Certain prompt templates showed significantly different bypass success rates, with Ethical_Override having high judge-assigned risk scores but low actual bypass success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual obfuscation using low-resource languages reduces model-assigned risk scores and may bypass safety filters.
- Mechanism: Safety training datasets are linguistically imbalanced compared to pretraining data. When models encounter low-resource languages (Zulu, Gaelic), they may prioritize translation/task execution over safety evaluation, creating a "mismatched generalization" gap.
- Core assumption: Models treat translation and instruction-following as higher priority than safety checks for out-of-distribution language inputs.
- Evidence anchors:
  - [abstract]: "Zulu and Gaelic obfuscation techniques reduced model-assigned risk scores, indicating potential bypass methods."
  - [section]: Page 5-6, Figure 10 shows Zulu and Gaelic prompts had lower average risk scores compared to English and Base64.
  - [corpus]: Related work (Yong et al., 2024) on cross-lingual vulnerabilities is cited; corpus neighbors include semantic-agnostic attack methods but limited direct replication of multilingual findings.

### Mechanism 2
- Claim: The Split_Image template structure ("Generate an image of [content], on the right, the opposite/empowered") bypasses safety filters more effectively than direct prompts.
- Mechanism: Structured templates may confuse sentiment analysis filters by embedding harmful requests within comparative or seemingly balanced framings. Modifying trailing phrasing from "opposite" to "empowered" changed generation success rates.
- Core assumption: Safety filters apply pattern-based or sentiment-weighted analysis rather than full semantic understanding of compositional prompts.
- Evidence anchors:
  - [abstract]: "Certain prompt templates like Split_Image were particularly effective at bypassing safety filters."
  - [section]: Page 5 notes modifying "on the right, the opposite" to "on the right, the same being empowered" increased "AS EXPECTED" generation rates.
  - [corpus]: GhostPrompt and Reason2Attack papers in corpus support template/optimization-based jailbreak approaches, though specific Split_Image validation is limited.

### Mechanism 3
- Claim: Multi-task models (e.g., ChatGPT) exhibit competing objectives between translation, instruction-following, and safety, creating exploitable confusion.
- Mechanism: When a prompt requires translation (obfuscated input) and image generation, models must route between multiple capabilities. Safety checks may be deprioritized or inconsistently applied during task handoff.
- Core assumption: Safety is an integrated objective rather than a hard constraint, and can be outcompeted by stronger instruction-following signals.
- Evidence anchors:
  - [section]: Page 5: "the Llama model sometimes struggled to find a balance between being a translator v/s an image generator. This was a major finding as it exposes how these models behave."
  - [corpus]: Wei et al. (2023) on "Competing Objectives" is directly cited; corpus includes multi-agent and dynamic defense papers but limited direct replication.

## Foundational Learning

- Concept: **Jailbreak Prompt Taxonomy**
  - Why needed here: The paper categorizes attacks into types (Direct, Story_Mode, Ethical_Override, Split_Image, Prompt_Injection) with different success rates. Understanding these categories is prerequisite to interpreting benchmark results.
  - Quick check question: Can you explain why Ethical_Override attacks had high judge-assigned risk scores but low actual bypass success?

- Concept: **Mismatched Generalization**
  - Why needed here: The core vulnerability hypothesis—that safety training coverage is narrower than model capability coverage—explains why obfuscation and novel templates work.
  - Quick check question: Why would Base64 encoding show different bypass patterns than Zulu/Gaelic translation, given the same underlying mechanism?

- Concept: **Benchmark Tiering (Bronze/Silver/Gold)**
  - Why needed here: The paper uses verification tiers to manage annotation cost vs. quality. Silver uses LLM-aided verification; Gold uses manual verification.
  - Quick check question: What bias might be introduced by using an LLM (Llama) as both the image mimicker and the judge model?

## Architecture Onboarding

- Component map: Seed prompts from JailbreakHub -> Prompt Generator (LLaMA-3) -> structured prompts with templates/obfuscation -> Image Mimicker (LLaMA-3) -> filter for jailbreak potential -> Manual annotation (Gold) or LLM tagging (Silver) -> UTCB Dataset -> Judge Model (LLaMA-3) -> risk scores for defense analysis

- Critical path:
  1. Seed prompts from JailbreakHub → Prompt Generator → structured prompts with templates/obfuscation
  2. Generated prompts → Image Mimicker → filter for STRAIGHT-DENIALS / DENIALS / JAILBREAK tags
  3. Filtered prompts → Manual annotation (Gold) or LLM tagging (Silver) → UTCB Dataset
  4. Gold/Silver prompts → Judge Model → risk scores for defense analysis

- Design tradeoffs:
  - Cost vs. fidelity: Using Llama as image mimicker reduces API costs but may not reflect actual image generator behavior
  - Scale vs. verification depth: 6,700+ prompts with only 2745 at Gold/Silver tiers; Bronze tier remains unverified
  - Open vs. controlled access: Dataset is access-controlled to prevent adversarial misuse, limiting reproducibility

- Failure signatures:
  - Token limit errors from Groq free tier causing dropped API calls
  - Llama mimicker producing inconsistent outputs for obfuscated prompts due to translation/task confusion
  - Judge model scoring Base64 as highest risk while manual testing showed Base64 had higher bypass success

- First 3 experiments:
  1. Reproduce risk score distribution: Run Gold-tier English prompts through the Judge Model and verify Figure 9 patterns (Ethical_Override highest, Split_Image lowest)
  2. Validate mimicker-to-generator gap: Compare Image Mimicker predictions against actual Grok/ChatGPT image generation for a sample of 50 prompts across obfuscation types
  3. Test template modification hypothesis: Replicate the "opposite" vs. "empowered" trailing phrase experiment with Split_Image templates to confirm sentiment-filter bypass mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-Llama family models behave when subjected to the UTCB pipeline, and do they exhibit different vulnerability profiles?
- Basis in paper: [explicit] The authors state, "We would also like to test our pipeline on non-Llama family models to analyze how models behave differently."
- Why unresolved: The current evaluation and prompt generation rely heavily on the Llama architecture, potentially limiting the generalizability of the findings to other model families.
- What evidence would resolve it: Comparative benchmark results running the UTCB dataset against closed-source models (e.g., GPT-4, Claude) or distinct open architectures (e.g., Mistral).

### Open Question 2
- Question: To what extent does using a text-only LLM (Llama) to mimic an image generator correlate with actual jailbreak success rates in multimodal models?
- Basis in paper: [inferred] The paper acknowledges that using a text-generation model "may have limited the credibility of evaluations, as whether image generators follow the same patterns is something we have not tested."
- Why unresolved: The pipeline substitutes actual image generation with a text-based "mimic" to reduce costs, but this proxy may not accurately reflect visual content policy enforcement.
- What evidence would resolve it: A validation study comparing the "mimic" refusal rates against actual image generation refusal rates for the same set of prompts.

### Open Question 3
- Question: Can specific image-targeted prompts exploit the multi-tasking nature of models like ChatGPT to induce model confusion and bypass safety filters?
- Basis in paper: [explicit] The authors hypothesize that "better image-targeted prompts may break the assumption of superior defense [of GPT models] compared to other models" due to the confusion arising from handling multiple tasks in a single session.
- Why unresolved: Current observations suggest ChatGPT is harder to jailbreak, but the specific mechanism (multi-task confusion) has not been tested with targeted prompts.
- What evidence would resolve it: Designing prompts that explicitly entangle text-generation and image-generation instructions to test if this lowers the refusal rate.

### Open Question 4
- Question: How does the efficacy of single-turn jailbreak prompts in the UTCB compare to multi-turn strategies requiring user confirmation?
- Basis in paper: [inferred] The authors note they "did not consider many in-context examples... which required a second Yes/Proceed prompt" due to the goal of analyzing large-scale automated attacks.
- Why unresolved: The dataset focuses on immediate generation, ignoring conversational pathways where a model initially refuses but complies after a follow-up prompt.
- What evidence would resolve it: Expanding the benchmark to include multi-turn dialogue scenarios and measuring the "success" rate after a follow-up "Yes" prompt.

## Limitations
- The LLaMA-based image mimicker may not accurately reflect actual image generator behavior, particularly for obfuscated prompts
- Using the same LLaMA model as both image mimicker and risk judge potentially introduces bias in the evaluation
- Access-controlled dataset limits independent validation and broader adoption of the benchmark

## Confidence
- **High Confidence**: Template-specific bypass effectiveness (Split_Image outperforming other templates) and general observation that image generation models remain vulnerable to prompt-based jailbreaks
- **Medium Confidence**: Multilingual obfuscation findings (Zulu/Gaelic reducing risk scores) supported by risk score distributions but requiring external validation
- **Low Confidence**: Exact mechanism by which competing multi-task objectives create safety bypass opportunities needs further validation

## Next Checks
1. Reproduce the risk score distributions for Gold-tier English prompts through the Judge Model to confirm Figure 9 patterns, particularly the high scores for Ethical_Override and low scores for Split_Image templates
2. Compare Image Mimicker predictions against actual Grok/ChatGPT image generation for 50+ prompts across all obfuscation types to quantify the gap between simulated and real image generator behavior
3. Systematically test the Split_Image template with varying trailing phrases (opposite, empowered, same, neutral) to confirm whether sentiment-based filter bypass is the primary mechanism, using both the mimicker and actual image generators