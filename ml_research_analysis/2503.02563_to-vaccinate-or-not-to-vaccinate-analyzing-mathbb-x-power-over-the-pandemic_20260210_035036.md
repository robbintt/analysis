---
ver: rpa2
title: To Vaccinate or not to Vaccinate? Analyzing $\mathbb{X}$ Power over the Pandemic
arxiv_id: '2503.02563'
source_url: https://arxiv.org/abs/2503.02563
tags:
- tweets
- covid-19
- s-svdd
- data
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes public sentiment towards COVID-19 vaccination
  by processing approximately 400,000 tweets using natural language processing and
  sentiment analysis. The authors applied the VADER sentiment analysis tool to classify
  tweets as positive, negative, or neutral, and used One-Class Classifiers (OCCs)
  to distinguish between vaccine supporters and deniers.
---

# To Vaccinate or not to Vaccinate? Analyzing $\mathbb{X}$ Power over the Pandemic

## Quick Facts
- **arXiv ID:** 2503.02563
- **Source URL:** https://arxiv.org/abs/2503.02563
- **Reference count:** 31
- **Primary result:** S-SVDD classifiers (particularly NS-SVDDψ2-max and S-SVDDψ1-min) outperformed other OCC methods with GM scores of 0.81 and 0.79 respectively for classifying positive and negative vaccine sentiment tweets.

## Executive Summary
This study analyzes public sentiment toward COVID-19 vaccination using approximately 400,000 tweets collected via the Tweepy API. The authors applied VADER sentiment analysis to identify global sentiment trends, finding 45.1% positive and 30% negative sentiment toward vaccines. They then used One-Class Classifiers (OCCs) to distinguish between vaccine supporters and deniers, with S-SVDD classifiers showing superior performance over standard SVDD, ESVDD, and OCSVM approaches. Country-level analysis revealed significant variation, with the UK showing more positive sentiment than India, the US, and South Africa.

## Method Summary
The methodology involved collecting ~400,000 COVID-19 related tweets from 2021, preprocessing them (removing non-English text, stop words, punctuation, URLs; applying stemming/lemmatization), and performing sentiment analysis using VADER and TextBlob. For OCC experiments, 200 tweets were manually labeled (100 positive, 100 negative). Various OCC models were trained including SVDD, ESVDD, OCSVM, S-SVDD (with different regularization terms ψ1-4), and NS-SVDD (Newton-based variants). Models were evaluated using 70/30 train-test splits across 5 random iterations, with hyperparameters optimized using 5-fold cross-validation and Geometric Mean as the primary metric.

## Key Results
- Global sentiment analysis showed 45.1% positive, 30% negative, and 24.9% neutral tweets toward COVID-19 vaccines
- S-SVDD classifiers outperformed other OCC methods, with NS-SVDDψ2-max achieving GM=0.81 for positive tweets and S-SVDDψ1-min achieving GM=0.79 for negative tweets
- Country-level analysis revealed the UK had more positive sentiment compared to India, the US, and South Africa, with sentiment patterns contrary to expectations (most affected countries showed more negative sentiment)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Subspace optimization in S-SVDD improves boundary tightness for tweet classification compared to standard hypersphere methods.
- **Mechanism:** S-SVDD projects input data into a lower-dimensional space using projection matrix $Q$, optimizing this subspace via regularization term ($\psi$) that captures class variance to construct tighter hyperspheres around target classes.
- **Core assumption:** Discriminative features for vaccine sentiment reside in a lower-dimensional subspace rather than full high-dimensional text vector space.
- **Evidence anchors:** Abstract states S-SVDD outperforms other OCCs; section 2 provides mathematical formulation with projection matrix $Q$.
- **Break condition:** If text features don't cluster effectively in lower dimensions, projection $Q$ may discard critical sentiment signals.

### Mechanism 2
- **Claim:** Rule-based lexicon scoring (VADER) effectively separates global sentiment trends prior to classification.
- **Mechanism:** VADER uses pre-trained bag-of-words approach sensitive to social media context, mapping text to polarity compound score for macro-sentiment categorization before OCC application.
- **Core assumption:** Social media slang and punctuation intensity correlate reliably with sentiment valence without contextual understanding.
- **Evidence anchors:** Abstract shows VADER identified 45.1% positive global sentiment; section 4 describes VADER's rule-based pre-training for social media.
- **Break condition:** Fails on sarcasm or double negatives where lexical cues contradict semantic meaning.

### Mechanism 3
- **Claim:** Newton-based optimization (NS-SVDD) outperforms gradient-based methods for positive class by utilizing second-order curvature information.
- **Mechanism:** NS-SVDD incorporates Hessian matrix for optimization, allowing more precise minimum finding for projection matrix $Q$, particularly benefiting positive class with higher variance.
- **Core assumption:** Cost function for positive class is convex enough that Hessian calculation yields significantly better local minimum than first-order methods.
- **Evidence anchors:** Abstract reports NS-SVDDψ2-max achieved highest GM (0.81) for positive tweets; section 3 explains Newton's method uses Hessian.
- **Break condition:** If feature space is highly non-linear or sparse, Hessian calculation becomes computationally prohibitive or numerically unstable.

## Foundational Learning

- **Concept:** **One-Class Classification (OCC)**
  - **Why needed here:** Standard binary classifiers require balanced datasets of both classes, but in practice one class often dominates or the other is undefined. OCC allows learning a "normal" boundary using only examples from that class.
  - **Quick check question:** If you only had images of "cats" and no "non-cats," could you train a system to detect cats using OCC principles?

- **Concept:** **Regularization Term ($\psi$) in S-SVDD**
  - **Why needed here:** The paper tests variants ($\psi 1, \psi 2, \dots$) to control how the model handles data spread inside the hypersphere. Understanding this explains why $\psi 2$-max worked best for positive tweets.
  - **Quick check question:** Does increasing the weight of the regularization term ($\beta$) make the model stricter about fitting training data or looser to allow generalization?

- **Concept:** **Geometric Mean (GM) as a Metric**
  - **Why needed here:** Accuracy is misleading in imbalanced datasets. The paper uses GM = √(tpr × tnr) to balance True Positive and True Negative rates effectively.
  - **Quick check question:** Why would a model with 90% accuracy but 0% True Negative rate be useless for detecting negative sentiment in a mostly positive dataset?

## Architecture Onboarding

- **Component map:** Ingestion (Tweepy API) -> Preprocessing (stopword removal, stemming, non-English filtering) -> Feature Extraction (text-to-vector transformation) -> Analysis Layer (VADER/TextBlob + Manual Labeling) -> Model Layer (S-SVDD/NS-SVDD)

- **Critical path:** The Manual Labeling of 200 tweets. The entire comparative analysis of OCCs rests on this tiny labeled subset (100 positive / 100 negative). If this sample is biased, the reported GM scores do not generalize to the full 400k dataset.

- **Design tradeoffs:**
  - VADER vs. BERT: Chose VADER (rule-based) for speed and transparency over deep learning models, trading nuanced context understanding for computational efficiency on 400k tweets
  - Gradient vs. Newton: Trading computational cost (Newton is slower per iteration) for convergence precision (Newton achieves higher GM on positive class)

- **Failure signatures:**
  - Low TNR: Some models (like OCSVM) show TNR ≈ 0.29, misclassifying most negative tweets as positive
  - High Variance: Standard deviations (S-GM) in Table 2 indicate performance fluctuates significantly across random splits, suggesting 200-sample evaluation is potentially unstable

- **First 3 experiments:**
  1. Reproducibility Check: Re-run VADER scoring on raw tweets to verify 45.1% positive vs. 30% negative split matches paper's global baseline
  2. Ablation on Regularization: Train S-SVDD ψ1 (no variance) vs. ψ2 (max variance) on labeled set to observe delta in Geometric Mean for positive class
  3. Feature Scaling Validation: Verify if data standardization using training mean/std was applied correctly before Newton optimization, as second-order methods are sensitive to unscaled features

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do countries most severely affected by COVID-19 exhibit more negative vaccine sentiment contrary to expectations?
- **Basis in paper:** Authors state in conclusion that "the countries most severely hit by the pandemic still have a negative public perception towards vaccines" and note these results were contrary to expectations.
- **Why unresolved:** The study identifies this counterintuitive pattern across US, India, and South Africa but doesn't investigate underlying causes like healthcare strain, political factors, or media coverage differences.
- **What evidence would resolve it:** Multivariate analysis correlating sentiment with country-specific variables (case rates, mortality, policy responses, media narratives) combined with qualitative content analysis of negative tweets from these regions.

### Open Question 2
- **Question:** How robust are S-SVDD classifiers when scaled beyond the 200-tweet manually labeled dataset?
- **Basis in paper:** The paper trains and evaluates OCCs on only 100 positive and 100 negative manually labeled tweets, representing <0.05% of total dataset.
- **Why unresolved:** Generalization to larger, more diverse datasets remains untested. The geometric mean performance (0.79-0.81) may degrade with greater linguistic variety, sarcasm, or emerging topics.
- **What evidence would resolve it:** Expand manual labeling to 1,000+ tweets across diverse topics and time periods; evaluate whether performance metrics hold or decline.

### Open Question 3
- **Question:** How does vaccine sentiment evolve temporally across different phases of the pandemic and vaccination rollout?
- **Basis in paper:** The study analyzes tweets "from the year 2021" as a single corpus without examining temporal dynamics or correlating sentiment shifts with specific events.
- **Why unresolved:** Public perception likely fluctuates in response to real-world events. A static snapshot cannot capture these dynamics.
- **What evidence would resolve it:** Time-series sentiment analysis partitioned by weeks or months, correlated with key events such as vaccine authorization dates, case spikes, or policy announcements.

### Open Question 4
- **Question:** Do the observed sentiment patterns and classifier performance generalize to other social media platforms?
- **Basis in paper:** The study exclusively analyzes X (formerly Twitter), which has distinct user demographics and communication norms.
- **Why unresolved:** Platform-specific factors (character limits, user base, algorithmic amplification) may influence both sentiment expression and classifier effectiveness.
- **What evidence would resolve it:** Apply identical methodology to platforms like Facebook, Reddit, or Instagram; compare sentiment distributions and OCC performance across platforms.

## Limitations
- The study's conclusions rest on a small manually labeled subset (n=200) representing <0.05% of the total dataset, raising concerns about statistical significance and generalizability
- The feature extraction method remains unspecified, making exact reproduction challenging and potentially affecting reported S-SVDD performance
- The study focuses on English tweets only, potentially missing sentiment patterns in other languages and limiting global applicability

## Confidence
- **High Confidence:** VADER sentiment analysis results (45.1% positive, 30% negative) are reproducible with the specified tool and methodology
- **Medium Confidence:** Comparative performance of S-SVDD variants (GM scores of 0.81 and 0.79) is methodologically sound but depends on representativeness of the 200-sample labeled set
- **Low Confidence:** Country-level sentiment analysis conclusions (UK vs. India vs. US vs. South Africa) are limited by small sample sizes per country and potential demographic biases in Twitter usage

## Next Checks
1. **Sample Size Validation:** Replicate the OCC experiments with increasing labeled sample sizes (n=200, 400, 800) to assess stability of Geometric Mean scores and determine minimum sample requirements
2. **Cross-Validation Robustness:** Perform 10-fold cross-validation instead of 5-fold to reduce variance in performance metrics and better estimate true model generalization
3. **Feature Extraction Analysis:** Test multiple feature extraction methods (TF-IDF, word embeddings, character n-grams) to determine if reported S-SVDD performance is consistent across different representations or specific to one approach