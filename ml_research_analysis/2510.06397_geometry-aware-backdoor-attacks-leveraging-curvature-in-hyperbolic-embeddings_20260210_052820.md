---
ver: rpa2
title: 'Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings'
arxiv_id: '2510.06397'
source_url: https://arxiv.org/abs/2510.06397
tags:
- hyperbolic
- backdoor
- neural
- euclidean
- radial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a geometry-specific vulnerability in hyperbolic
  neural networks where the non-uniform curvature creates regions where backdoor triggers
  can be simultaneously more effective and harder to detect than in Euclidean space.
  The conformal factor in hyperbolic geometry means that near the boundary, small
  input-space perturbations induce disproportionately large shifts in representation
  space while appearing subtle to standard input-space detectors.
---

# Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings

## Quick Facts
- arXiv ID: 2510.06397
- Source URL: https://arxiv.org/abs/2510.06397
- Authors: Ali Baheri
- Reference count: 28
- Primary result: Hyperbolic geometry enables backdoors that are simultaneously more effective and stealthier than Euclidean baselines, achieving 97% ASR vs 62.7% while maintaining >95% clean accuracy

## Executive Summary
This paper identifies a geometry-specific vulnerability in hyperbolic neural networks where the non-uniform curvature creates regions where backdoor triggers can be simultaneously more effective and harder to detect than in Euclidean space. The conformal factor in hyperbolic geometry means that near the boundary, small input-space perturbations induce disproportionately large shifts in representation space while appearing subtle to standard input-space detectors. The authors propose a geometry-adaptive trigger that scales based on the local metric distortion, making it more effective near the boundary where metric sensitivity is highest.

## Method Summary
The attack exploits the conformal factor λx = 2/(1−||x||²) in the Poincaré ball model, where metric distortion increases dramatically near the boundary. A position-adaptive trigger τ(x) = exp_x(s(x)·P_{0→x}(δ)) scales via s(x) = α·(λ₀/λ_x)^β to compensate for this distortion. Poisoning prioritizes points via geometric-position weighting, and the optimization uses Riemannian gradients grad_x L = λ_x^{-2}∇_x L. The defense analysis proves that radial defenses face an inherent trade-off: pulling triggered points inward degrades clean accuracy proportionately.

## Key Results
- 97% attack success rate on 20newsgroups dataset vs 62.7% for Euclidean baselines
- Maintains >95% clean accuracy while achieving only 15.2% detection rate vs 45.8% for Euclidean approaches
- Attack success increases toward the boundary (93% at center vs 97% at boundary)
- Conformal factor scaling contributes most to attack effectiveness: removing it drops success from 97.0% to 84.5%

## Why This Works (Mechanism)

### Mechanism 1: Boundary-Amplified Trigger Effect via Conformal Scaling
Small input-space perturbations near the Poincaré ball boundary produce disproportionately large hyperbolic representation shifts while appearing subtle to Euclidean detectors. The conformal factor creates position-dependent metric distortion where geodesic displacement per unit Euclidean change grows like 1/δ(x) as x approaches the boundary. The adaptive trigger scales via s(x) = α·(λ₀/λ_x)^β to compensate for this distortion.

### Mechanism 2: Geometry-Adaptive Trigger Scaling
Position-adaptive trigger scaling improves attack success while maintaining clean accuracy. The sparse trigger optimizes over k active dimensions with Riemannian gradients, and poisoning prioritizes points via geometric-position weighting, biasing toward high-distortion regions.

### Mechanism 3: Defense-Utility Trade-off for Radial Defenses
Any radial defense that recovers a fraction of triggered inputs must cause proportionate degradation on clean inputs. A radial defense that pulls triggered points inward by ≥αs must pull clean points inward by ≥α_eff·s with probability ≥β, inducing expected output deviation ≥μ_g·β·α_eff·s.

## Foundational Learning

- **Poincaré Ball Model and Conformal Factor**: Understanding the non-uniform metric g_x = λ_x²·g_E where λ_x = 2/(1−||x||²) is prerequisite to grasping why boundary regions behave differently. Quick check: Compute λ_x at r = 0.5 vs r = 0.9. Which gives larger metric distortion?

- **Riemannian Gradients and Parallel Transport**: The trigger optimization uses grad_x L = λ_x^{-2}∇_x L, and the trigger involves parallel transport P_{0→x}(δ) from origin to target point. Quick check: Why must gradients be scaled by λ_x^{-2} rather than used directly?

- **Backdoor Attack Formalization**: Distinguishes this from adversarial/evasion attacks. Backdoors require poisoning during training, persistent trigger-response behavior, and clean accuracy preservation. Quick check: What is the difference between ASR (Attack Success Rate) and evasion attack success?

## Architecture Onboarding

- **Component map**: Input → TF-IDF/SVD → Euclidean features → Radial redistribution → hyperbolic-aware positioning → Trigger injection → Training with multi-objective loss
- **Critical path**: 1) Input → TF-IDF/SVD → Euclidean features 2) Radial redistribution → hyperbolic-aware positioning 3) Trigger injection: τ(x) = Π_ρ(x + s(x)·δ) where s(x) adapts to λ_x 4) Training with multi-objective loss
- **Design tradeoffs**: Boundary positioning gives higher attack success (97% vs 93%) but requires careful norm management; trigger sparsity improves stealth but may reduce ASR; poisoning rate 5% balances effectiveness and detection risk
- **Failure signatures**: NaN gradients from points too close to boundary; low ASR despite correct trigger math indicates semantic collision; clean accuracy collapse from over-prioritizing backdoor loss
- **First 3 experiments**: 1) Ablate conformal scaling: Set β = 0 vs β ∈ (0,1]. Expect ASR drop from ~97% to ~85% 2) Boundary vs center analysis: Bin inputs by ||x|| and plot ASR per bin. Expect monotonic increase toward boundary 3) Defense sensitivity test: Implement radial projection with varying inward displacement ∆. Measure clean accuracy degradation vs trigger recovery rate

## Open Questions the Paper Calls Out

- **Future work should explore attacks using complete geometric frameworks and evaluate on datasets with naturally occurring hyperbolic structure**: The current evaluation uses artificial geometric redistribution on 20newsgroups, which may not capture natural hyperbolic clustering patterns, and employs scaling inspired by geometry rather than full Riemannian operations.

- **Can non-radial defense strategies mitigate the boundary amplification effect without the utility trade-off proven for radial defenses?**: Theorem 2 proves a utility-accuracy trade-off specifically for radial defenses with Lipschitz profiles, but the analysis does not address whether non-radial defense classes can circumvent this bound.

- **Do detectors designed with hyperbolic-Lipschitz regularity effectively detect boundary-amplified triggers that evade Euclidean-Lipschitz detectors?**: Appendix A, Remark 3 states that "hyperbolic-aware detectors do not enjoy the vanishing bound, clarifying which regularity notion matters for stealth."

## Limitations
- Empirical validation limited to single text classification task with specific preprocessing
- Architecture details underspecified, creating reproducibility challenges
- Theoretical bounds assume radial defenses and specific classifier sensitivity conditions that may not hold universally

## Confidence
- **High confidence**: The conformal factor mechanism creating boundary amplification, and the theoretical defense-utility trade-off bounds
- **Medium confidence**: The position-adaptive trigger scaling effectiveness (empirical results depend on specific implementation choices)
- **Low confidence**: Generalization to other tasks/datasets beyond the 20newsgroups experiment

## Next Checks
1. **Architectural reproducibility test**: Implement the classifier with and without the outlier detection module to verify if detection rates hold across multiple random seeds
2. **Boundary stratification replication**: Replicate the ASR stratification by distance-from-center bins (≤0.5, 0.5–0.7, >0.7) to confirm the monotonic increase toward boundary
3. **Defense trade-off verification**: Implement radial projection defenses with varying strength and measure the linear relationship between clean accuracy degradation and trigger recovery rate to validate Theorem 2 predictions