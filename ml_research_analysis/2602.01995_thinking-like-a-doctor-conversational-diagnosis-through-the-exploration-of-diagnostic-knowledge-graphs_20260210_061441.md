---
ver: rpa2
title: 'Thinking Like a Doctor: Conversational Diagnosis through the Exploration of
  Diagnostic Knowledge Graphs'
arxiv_id: '2602.01995'
source_url: https://arxiv.org/abs/2602.01995
tags:
- patient
- diagnosis
- diagnostic
- knowledge
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of building conversational
  diagnosis systems that can operate effectively under incomplete and underspecified
  patient information, a common scenario in real-world clinical history-taking. The
  authors propose a system that uses a diagnostic knowledge graph to ground its reasoning
  in two iterative steps: generating plausible diagnostic hypotheses from dialogue
  context, and verifying these hypotheses through targeted clarifying questions.'
---

# Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs

## Quick Facts
- arXiv ID: 2602.01995
- Source URL: https://arxiv.org/abs/2602.01995
- Reference count: 36
- Key outcome: Graph-grounded conversational diagnosis achieves Recall@1 of 0.250 and Recall@4 of 0.418 on vague symptom inputs, outperforming larger models and baselines.

## Executive Summary
This paper introduces a conversational diagnosis system that uses a diagnostic knowledge graph to guide iterative hypothesis generation and verification under incomplete patient information. The system extracts focused subgraphs around top-ranked disease hypotheses and linearizes them into context for a verifier LLM to generate targeted clarifying questions. Evaluated with a patient simulator that produces low-specificity symptom descriptions, the approach shows substantial improvements in diagnostic accuracy and efficiency compared to foundation model baselines. Physician feedback confirms the clinical realism of the generated dialogues.

## Method Summary
The system uses a two-stage approach: a Hypothesis Generator (HG) that ranks diseases from dialogue history, and a Hypothesis Verifier (HV) that verifies hypotheses through questions grounded in a 3-hop subgraph expansion from top candidates. The knowledge graph contains 338 diseases, 1,733 nodes, and 3,935 edges, extracted from a Korean medical textbook. Training uses synthetic dialogues generated from MIMIC-IV patient profiles, with the HG trained via a classification head and the HV via supervised fine-tuning on full dialogues with structured reasoning traces. The patient simulator is augmented to produce vague symptom descriptions reflecting real-world clinical encounters.

## Key Results
- Graph-grounded system achieves Recall@1 of 0.250 and Recall@4 of 0.418 on test set.
- Average dialogue length is 6.9 turns, demonstrating efficient information gathering.
- System outperforms GPT-4o-mini, Gemini-1.5-Pro, and Llama-3-70B baselines on diagnostic accuracy.
- Physician evaluation shows 46.3% preferred vague symptom simulator vs 24.4% for standard simulator.

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis-Driven Subgraph Extraction
Limiting the diagnostic search space to a small set of high-probability "anchor" diseases and their graph neighbors improves the relevance of clarifying questions compared to searching the entire Knowledge Graph (KG). The system uses a Hypothesis Generator (HG) to rank diseases based on dialogue history. It selects top-$n$ anchors (e.g., top 2) and performs a 3-hop expansion (Disease $\to$ Attributes $\to$ Competing Diseases $\to$ Attributes). This creates a focused subgraph containing discriminative cues between similar conditions, which is linearized into the context for the Hypothesis Verifier (HV). Core assumption: The ground-truth disease appears within the top-$n$ candidates of the HG, or is reachable within 3 hops of them.

### Mechanism 2: Grounding via Linearized Graph Context
Explicitly injecting structured diagnostic knowledge (the subgraph) into the LLM context reduces hallucination and improves the clinical validity of reasoning traces. Rather than relying solely on parametric memory, the HV receives the extracted subgraph as text (e.g., "Disease A causes Symptom B"). The model is fine-tuned to generate a reasoning trace (enclosed in tags) that explicitly weighs this graph evidence before deciding on a question or diagnosis. Core assumption: The LLM has sufficient context window and instruction-following capability to utilize the linearized graph structure without getting distracted.

### Mechanism 3: Specificity-Augmented Simulation
Training and evaluating on patient simulators designed to produce "low-specificity" (vague) symptoms forces the model to learn robust active inquiry strategies. The patient simulator is prompted to avoid medical jargon and precise locations. This prevents the model from "cheating" by matching clear keywords and necessitates a multi-turn strategy to refine vague inputs into diagnostic evidence. Core assumption: The vagueness defined in the simulator (e.g., using "belly" instead of "lower right quadrant") accurately reflects real-world patient ambiguity.

## Foundational Learning

- **Concept: 3-Hop Graph Expansion**
  - Why needed here: The system's reasoning relies not just on the disease, but on *competing* diseases. Understanding 1-hop (attributes), 2-hop (competitors), and 3-hop (competitor attributes) is essential to debugging why the system asks specific questions.
  - Quick check question: If the system misdiagnoses "Acute Appendicitis" as "Gastritis," is the failure likely in the 1-hop or 2-hop extraction?

- **Concept: Multi-Label Classification Head vs. Generation**
  - Why needed here: The Hypothesis Generator (HG) uses a classification head on top of an LLM to output disease probabilities. This differs from standard generative approaches and is critical for the system's ability to rank candidates.
  - Quick check question: Why does the paper prefer a classification head for the HG over a generative approach? (Hint: See Section 5.2/Figure 3 regarding Recall@k coverage).

- **Concept: Linearized Knowledge Injection**
  - Why needed here: The HV does not use a GNN (Graph Neural Network); it reads the graph as text.
  - Quick check question: How does converting a graph into a textual prompt change the prompt engineering requirements compared to standard RAG?

## Architecture Onboarding

- **Component map:**
  1. **Patient Simulator:** MIMIC-IV profile + Specificity Prompt → Vague Patient Utterance.
  2. **Hypothesis Generator (HG):** Qwen-2.5-7B-Instruct + Classification Head. Input: Dialogue History. Output: Top-$n$ Disease Probabilities.
  3. **Subgraph Extractor:** KG Lookup → 3-hop Expansion → Linearized Text.
  4. **Hypothesis Verifier (HV):** Qwen-2.5-7B-Instruct (SFT). Input: Dialogue History + Linearized Subgraph. Output: Reasoning Trace + Clarifying Question (or Diagnosis).

- **Critical path:** The **HG Recall** is the system's bottleneck. If the ground-truth disease is not captured in the top-$n$ anchors (and subsequently the subgraph), the HV has a near-zero chance of recovery (Section 5.2: Sub Recall).

- **Design tradeoffs:**
  - **Graph Size vs. Noise:** The authors use textbook-derived schemas (338 diseases) rather than broad KGs like UMLS to reduce noise (Section 3.2).
  - **Classification vs. Generation:** Classification heads provide better ranking for the top-$k$ candidates than generative models, which often plateau after the first candidate (Figure 3).

- **Failure signatures:**
  - **Label Granularity Mismatch:** The model predicts a correct parent category (e.g., "Arrhythmia") but the ground truth is a specific subtype (e.g., "Atrial Flutter"), resulting in a false negative in metrics (Appendix I).
  - **Subjectivity Confusion:** The model interprets "my belly feels weird" as a non-symptom rather than a vague symptom, failing to trigger hypothesis generation.
  - **Premature Diagnosis:** The HV outputs a diagnosis too early due to high confidence in a generic symptom (e.g., cough → Pneumonia immediately).

- **First 3 experiments:**
  1. **Validation of HG:** Run the HG on the test set and plot Recall@k (k=1..4). Verify that Recall@4 > 0.5 to ensure sufficient subgraph coverage.
  2. **Ablation of "Low Specificity":** Run the system against the standard simulator vs. the augmented vague simulator. Confirm the performance gap validates the need for robust history-taking.
  3. **Threshold Sensitivity ($n$ and $\tau$):** Run a grid search on $n$ (num anchors) and $\tau$ (filtering threshold). Check if the optimal setting remains $n=2, \tau=0.005$ as reported in Table 4.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the diagnostic accuracy and conversational efficiency of the graph-grounded system transfer to real-world clinical encounters compared to the simulated environment?
  - Basis in paper: [explicit] The authors acknowledge in Section 7 (Limitations) that a primary constraint is the use of a patient simulator rather than "real-world clinical encounters" due to ethical considerations.
  - Why unresolved: Validating performance with actual patients introduces variables (tone, hesitation, non-verbal cues) not present in text-based simulation.
  - What evidence would resolve it: Results from clinical trials or "human-in-the-loop" studies where the system interacts with real patients under supervision.

- **Open Question 2:** How does the hypothesis-driven subgraph extraction mechanism scale when applied to large-scale, general-purpose biomedical knowledge graphs (e.g., UMLS) rather than curated diagnostic schemas?
  - Basis in paper: [explicit] Section 7 notes the diagnostic scope is bounded by the current graph coverage and suggests the framework can be adopted by "expanding the graph to incorporate additional diseases."
  - Why unresolved: Expanding from 338 curated nodes to thousands of general nodes may introduce noise that reduces the precision of the 3-hop expansion strategy.
  - What evidence would resolve it: Benchmarks showing Recall@k and latency when the system utilizes a comprehensive, non-curated knowledge base.

- **Open Question 3:** Can the integration of multi-modal data (e.g., images of skin lesions) improve the system's ability to resolve diagnostic ambiguities where textual history is insufficient?
  - Basis in paper: [explicit] Appendix K.3 reports feedback from a physician suggesting that "incorporating multi-modal capabilities (e.g., viewing skin lesion images)" would increase clinical utility.
  - Why unresolved: The current architecture is strictly text-based and cannot process visual or auditory clinical signals.
  - What evidence would resolve it: A comparative study of diagnostic accuracy in text-only versus multi-modal interaction settings.

- **Open Question 4:** How can the diagnostic knowledge graph be augmented to better handle atypical symptom presentations (e.g., geriatric syndromes) that deviate from textbook prototypes?
  - Basis in paper: [inferred] The Error Analysis (Appendix I) highlights failure cases where the model misdiagnosed elderly patients because they presented with systemic issues (falls, confusion) rather than the "localizing symptoms" encoded in the graph.
  - Why unresolved: The current graph relies on standard schemas that prioritize classical symptom-disease relationships, potentially penalizing non-classical presentations.
  - What evidence would resolve it: Improved diagnostic recall on a test set of atypical/geriatric profiles following graph enrichment with non-classical edges.

## Limitations

- Dataset Composition and Generalization: System evaluated on limited MIMIC-IV subset (275 profiles) with textbook-derived schemas (338 diseases), raising questions about broader disease distributions and selection bias.
- Knowledge Graph Dependency and Quality: 3-hop subgraph extraction relies heavily on KG completeness and accuracy; specific textbook source not publicly accessible.
- Simulator Fidelity and Real-World Transfer: Synthetic patient simulator may not capture full complexity of real-world patient communication, including emotional factors and cultural differences.

## Confidence

- **High Confidence** - Core mechanism of using diagnostic knowledge graphs to ground conversational diagnosis through iterative hypothesis generation and verification is well-supported by results and physician feedback.
- **Medium Confidence** - Specific implementation details (3-hop subgraph expansion, linearized context) are described but not fully validated through ablation studies; optimal parameters may not generalize.
- **Low Confidence** - Claims about robustness to vague symptoms and superiority over larger models are based on limited comparisons and lack detailed edge case analysis.

## Next Checks

1. **HG Coverage Validation** - Plot Recall@k curves for the Hypothesis Generator across the test set to verify that Recall@4 consistently exceeds 0.5, ensuring adequate subgraph coverage for the verification step.

2. **Vagueness Ablation Study** - Run the complete system against both the standard and specificity-augmented simulators to quantify the performance gap and validate that the system genuinely benefits from handling vague symptom descriptions.

3. **Threshold Sensitivity Analysis** - Conduct a grid search over the number of anchor diseases (n) and filtering threshold (τ) parameters to determine whether the reported optimal values (n=2, τ=0.005) are robust across different disease distributions and patient profiles.