---
ver: rpa2
title: 'MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction'
arxiv_id: '2507.06909'
source_url: https://arxiv.org/abs/2507.06909
tags:
- legal
- multiple
- charges
- prediction
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces MultiJustice, a Chinese dataset designed
  for legal judgment prediction across four practical scenarios: single defendant
  with single charge, single defendant with multiple charges, multiple defendants
  with single charge, and multiple defendants with multiple charges. The dataset contains
  20,000 cases and is used to evaluate five legal large language models on two tasks:
  charge prediction and penalty term prediction.'
---

# MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction

## Quick Facts
- arXiv ID: 2507.06909
- Source URL: https://arxiv.org/abs/2507.06909
- Authors: Xiao Wang; Jiahuan Pei; Diancheng Shui; Zhiguang Han; Xin Sun; Dawei Zhu; Xiaoyu Shen
- Reference count: 28
- The dataset contains 20,000 cases for evaluating legal judgment prediction across complex multi-party, multi-charge scenarios

## Executive Summary
This paper introduces MultiJustice, a Chinese dataset designed to advance legal judgment prediction by addressing real-world complexities in multi-party, multi-charge scenarios. The dataset evaluates five legal large language models on two tasks—charge prediction and penalty term prediction—across four distinct legal scenarios ranging from single defendant/single charge to multiple defendants with multiple charges. The results demonstrate that as legal complexity increases, model performance significantly degrades, with the most complex scenario (multiple defendants with multiple charges) showing the largest performance drops. InternLM2 consistently outperforms other models across all settings, while Lawformer shows the steepest decline. The study reveals that providing demonstration examples substantially improves model performance and that supervised fine-tuning is more effective than multi-task learning approaches.

## Method Summary
The study constructs a Chinese legal dataset comprising 20,000 cases covering four realistic legal scenarios: single defendant with single charge, single defendant with multiple charges, multiple defendants with single charge, and multiple defendants with multiple charges. Five legal large language models were evaluated on two prediction tasks—charge prediction and penalty term prediction—using standard metrics including Accuracy, Macro-F1, and LogD (logarithmic distance). The evaluation employed both zero-shot and few-shot learning approaches, comparing the effectiveness of different prompt strategies including demonstration examples and various fine-tuning methodologies. The dataset specifically addresses the gap in existing legal AI research by incorporating complex multi-party, multi-charge cases that better reflect real-world legal practice.

## Key Results
- Multiple defendants with multiple charges (S4) scenario shows the greatest performance challenges, with significant accuracy and F1-score drops compared to simpler scenarios
- InternLM2 demonstrates the most robust performance across all legal scenarios, while Lawformer experiences the largest decline in complex cases
- Providing demonstration examples in prompts significantly improves model performance across all scenarios
- Supervised fine-tuning outperforms multi-task learning approaches for both charge prediction and penalty term prediction tasks

## Why This Works (Mechanism)
The dataset's effectiveness stems from its realistic representation of legal case complexity, capturing the inherent challenges of multi-party, multi-charge scenarios that traditional legal AI datasets often oversimplify. By providing demonstration examples, models can better understand the structural relationships between defendants, charges, and penalties. The fine-tuning approach allows models to adapt to the specific legal reasoning patterns required for Chinese criminal law, while the evaluation across multiple scenarios reveals performance limitations that single-scenario datasets cannot capture.

## Foundational Learning
- **Legal judgment prediction**: Predicting both charges and penalties from case facts; needed to automate routine legal tasks and support judicial decision-making; quick check: can the model correctly identify all charges and calculate appropriate penalties from a given case description
- **Multi-party legal scenarios**: Cases involving multiple defendants with potentially different charges; needed to reflect real-world legal complexity beyond single-defendant cases; quick check: does the model correctly attribute charges to individual defendants in group cases
- **Charge prediction**: Identifying applicable criminal charges from case facts; needed as the foundation for subsequent penalty determination; quick check: can the model distinguish between similar charges based on nuanced fact patterns
- **Penalty term prediction**: Calculating appropriate sentence lengths based on charges and circumstances; needed to provide complete judicial recommendations; quick check: does the model produce penalty ranges consistent with Chinese criminal law guidelines
- **Zero-shot vs few-shot learning**: Comparing model performance with and without demonstration examples; needed to evaluate model adaptability to new legal scenarios; quick check: measure performance improvement when demonstration examples are added to prompts
- **LogD metric**: Logarithmic distance for penalty prediction evaluation; needed to properly assess error magnitude in sentence length calculations; quick check: calculate average deviation between predicted and actual penalty terms

## Architecture Onboarding
**Component Map:** Legal LLM -> Prompt Processing -> Case Fact Extraction -> Charge Prediction -> Penalty Term Prediction -> Output Generation

**Critical Path:** Input case facts → Legal LLM processing → Demonstration example integration (if few-shot) → Charge prediction output → Penalty term prediction output

**Design Tradeoffs:** Zero-shot learning offers simplicity and broader applicability but suffers performance drops in complex scenarios, while few-shot learning with demonstration examples improves accuracy but requires careful example selection and may not generalize well to unseen case types.

**Failure Signatures:** Significant performance degradation in S4 scenarios, inconsistent charge attribution across multiple defendants, overly conservative penalty predictions, and inability to capture nuanced relationships between multiple charges.

**First Experiments:**
1. Evaluate baseline performance using zero-shot learning across all four scenarios to establish performance floor
2. Test few-shot learning with varying numbers of demonstration examples to identify optimal prompt strategy
3. Compare supervised fine-tuning versus multi-task learning approaches on the most challenging S4 scenario

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can LLM architectures be specifically adapted to model the intricate dependencies between multiple defendants and multiple charges to close the performance gap in S4 scenarios?
- **Basis in paper:** [explicit] The authors explicitly call for "future work... to propose advanced models" because the S4 scenario (multi-defendant, multi-charge) poses the "greatest challenges" and causes significant performance drops in existing models like Lawformer.
- **Why unresolved:** The paper benchmarks existing models without proposing a structural solution for the complexity of S4, merely identifying that current general-purpose and legal LLMs struggle with this specific compositionality.
- **What evidence would resolve it:** A novel architecture or fine-tuning strategy that achieves statistically comparable F1-scores and LogD results between S1 and S4 on the MPMCP dataset.

### Open Question 2
- **Question:** Do the performance degradation patterns observed in complex scenarios (S2–S4) generalize to legal systems outside of Chinese criminal law?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the "dataset, sourced exclusively from Chinese criminal cases, may limit the generalizability of our findings to other legal systems."
- **Why unresolved:** The study is restricted to a single jurisdiction (China) and a single domain (criminal law), leaving the cross-jurisdictional robustness of models like InternLM2 untested.
- **What evidence would resolve it:** A replication of the MPMCP evaluation framework using legal data from Common Law jurisdictions or civil contexts, showing similar or diverging performance trends across the four scenarios.

### Open Question 3
- **Question:** What evaluation metrics beyond Accuracy and LogD are necessary to capture the logical nuances and fairness of judgments in complex legal scenarios?
- **Basis in paper:** [explicit] The Limitations section notes that "evaluation metrics used may not fully capture the nuances of legal judgments" and highlights the need for "decision transparency" due to the black-box nature of LLMs.
- **Why unresolved:** Legal outcomes require logical consistency and fairness which standard classification metrics (Accuracy/F1) and error metrics (LogD) do not measure.
- **What evidence would resolve it:** The introduction of a new evaluation protocol that successfully quantifies reasoning consistency and detects bias in S4 cases, correlating with human expert review.

## Limitations
- The dataset construction methodology may introduce selection bias due to unclear case selection processes
- Annotation quality for demonstration examples is uncertain due to lack of detailed annotation methodology
- Evaluation is limited to Chinese legal LLM models, preventing comparison with international models or different legal systems
- The study does not address adversarial or edge-case scenarios that might challenge even the best-performing models

## Confidence
- **High Confidence:** The finding that multiple defendants with multiple charges (S4) presents the greatest challenge for all models is well-supported by experimental results across multiple metrics
- **Medium Confidence:** The comparative performance rankings between different LLM models (InternLM2 vs Lawformer) are reasonably supported, though absolute performance differences may vary with different evaluation criteria
- **Low Confidence:** The generalizability of these findings to other legal domains, jurisdictions, or languages remains uncertain due to the dataset's specific focus on Chinese criminal cases

## Next Checks
1. Conduct cross-validation using cases from different Chinese courts and legal specialties to assess model robustness across diverse legal contexts
2. Implement adversarial testing with deliberately complex case scenarios to identify specific failure modes and limitations of current models
3. Perform external validation using an independent dataset of Chinese legal cases not included in the original MultiJustice dataset to verify the reproducibility of performance rankings