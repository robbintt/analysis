---
ver: rpa2
title: On the Connection Between Diffusion Models and Molecular Dynamics
arxiv_id: '2504.03187'
source_url: https://arxiv.org/abs/2504.03187
tags:
- diffusion
- data
- forces
- force
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes a clear mathematical connection between denoising
  diffusion models and molecular dynamics (MD) by showing that training a neural network
  to remove noise from atomic configurations recovers the underlying force field.
  Using a simple Taylor expansion, we prove that a diffusion-trained model, when scaled
  appropriately, approximates true forces, bridging the gap between diffusion models
  and conventional NNPs.
---

# On the Connection Between Diffusion Models and Molecular Dynamics

## Quick Facts
- arXiv ID: 2504.03187
- Source URL: https://arxiv.org/abs/2504.03187
- Reference count: 12
- Primary result: Diffusion models trained to denoise atomic coordinates recover underlying force fields when scaled by inverse noise variance

## Executive Summary
This work establishes a mathematical connection between denoising diffusion models and molecular dynamics by showing that training a neural network to remove noise from atomic configurations recovers the underlying force field. Using a Taylor expansion, the authors prove that a diffusion-trained model, when scaled appropriately, approximates true forces. They demonstrate this framework on a coarse-grained lithium chloride solution, showing that data duplication—adding multiple noise samples to the same coordinate frames—can double training dataset size while maintaining stability and accuracy comparable to traditional NNPs trained on force data.

## Method Summary
The method trains a neural network on pairs of noisy atomic coordinates and their corresponding noise vectors, using a first-order Taylor expansion to show that the expected denoising output is proportional to interatomic forces when scaled by inverse noise variance. The training targets the noise vector rather than forces, allowing data duplication where multiple noise samples are added to the same coordinate frames to effectively increase training data. The trained model is deployed in standard MD software with output scaling, enabling simulations without requiring explicit force labels from quantum calculations.

## Key Results
- Training a diffusion model to denoise atomic coordinates recovers the underlying force field when outputs are scaled by $1/\sigma^2$
- Data duplication (multiple noise samples per coordinate) can double training dataset size while maintaining stability
- Diffusion-trained NNPs achieve comparable RDF accuracy to force-trained models in coarse-grained LiCl simulations
- The approach is particularly valuable in low-data settings, offering a computationally efficient alternative to force-label-dependent methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training a neural network to denoise atomic coordinates recovers the underlying physical force field, provided outputs are scaled by inverse noise variance
- **Mechanism:** The method trains a network on pairs $(r_i + N_i, -N_i)$. A first-order Taylor expansion of the density $\rho(r')$ reveals that the expected denoising vector is proportional to the gradient of the log-density ($\nabla \log \rho$). Since atomic coordinates at equilibrium follow the Boltzmann distribution ($\rho \propto e^{-E/k_BT}$), this gradient is mathematically equivalent to interatomic forces ($F = -\nabla E$)
- **Core assumption:** Noise level $\sigma$ is sufficiently small so perturbed samples remain within the linear region of Taylor expansion
- **Break condition:** If noise variance $\sigma$ is too large relative to energy landscape curvature, first-order approximation fails

### Mechanism 2
- **Claim:** Data duplication—applying multiple independent noise samples to a single coordinate frame—effectively increases training data volume and stability in low-data regimes
- **Mechanism:** Because training target is noise vector $N$ rather than scalar energy, re-sampling noise for fixed coordinate $r$ creates distinct valid training examples, allowing model to learn probability distribution around that coordinate with higher fidelity
- **Core assumption:** Underlying potential energy surface at coordinate $r$ is sufficiently characterized by random Gaussian perturbations during training
- **Break condition:** If coordinate dataset is too sparse to cover relevant configuration space, duplicating existing frames won't teach model about unvisited regions

### Mechanism 3
- **Claim:** A denoising model can replace traditional force fields in coarse-grained Molecular Dynamics to sample correct equilibrium distributions
- **Mechanism:** In coarse-grained systems, effective force is gradient of potential of mean force (free energy). By learning score of equilibrium distribution, diffusion model implicitly learns this free energy gradient, driving system according to correct thermodynamics when integrated into MD engine
- **Core assumption:** Coarse-grained degrees of freedom retain statistical properties of full system such that score function captures relevant thermodynamic forces
- **Break condition:** If simulation requires non-equilibrium properties or kinetics not encoded in equilibrium training distribution, method will likely fail to capture correct dynamics

## Foundational Learning

- **Concept:** Boltzmann Distribution
  - **Why needed here:** Bridge between probability (which diffusion models learn) and physics (which MD simulates). Must understand that $p(x) \propto e^{-E(x)/k_BT}$ to see why learning score $\nabla \log p$ equals learning force $-\nabla E$
  - **Quick check question:** If temperature $T$ increases, how does width of Boltzmann distribution change, and how would this affect noise level $\sigma$ in diffusion model?

- **Concept:** Score Matching / Denoising Score
  - **Why needed here:** Paper frames "denoising" as estimating gradient of log-density ($\nabla \log p$). Understanding this vector field is key to understanding how network produces forces
  - **Quick check question:** In high-dimensional space, does score vector point toward nearest data point or toward region of highest density?

- **Concept:** Coarse-Graining (Potential of Mean Force)
  - **Why needed here:** Validation experiment (LiCl) ignores water molecules. Must know that forces on ions are "effective" forces averaging over solvent motion, which is exactly what "free energy surface" represents
  - **Quick check question:** Does coarse-grained force field represent true instantaneous microscopic forces or average thermodynamic force?

## Architecture Onboarding

- **Component map:** Input Coordinates -> Add Gaussian Noise -> E(3)-equivariant Neural Network (NequIP) -> Output Vector (scaled by $1/\sigma^2$) -> LAMMPS Integration
- **Critical path:** Correctness of scaling factor ($1/\sigma^2$) is single most critical implementation detail. If MD engine doesn't scale network output, "forces" will be off by factor of $\sigma^2$, resulting in frozen atoms or explosions
- **Design tradeoffs:**
  - Noise magnitude ($\sigma$): Low $\sigma$ ensures Taylor expansion is accurate but creates "narrow" training distribution that may fail to generalize; high $\sigma$ covers more state space but introduces approximation error
  - Dataset vs. Duplication: Generating new ab initio frames is expensive; duplicating noise on existing frames is cheap but risks overfitting to specific spatial configurations
- **Failure signatures:**
  - RDF Spikes: Unphysical sharp peaks at short distances (<2 Å) indicate model failed to learn repulsive forces, allowing atoms to collapse
  - Energy Drift: In NVE ensemble, rapid energy increase suggests "forces" aren't conservative or integration step is too large for effective stiffness
- **First 3 experiments:**
  1. Scaling Verification: Train on 1D harmonic oscillator; plot predicted "Force" vs analytical spring force to verify $1/\sigma^2$ scaling
  2. Ablation on Duplication: Train two models on LiCl: one with 500 unique frames, one with 250 frames × 2 noise samples; compare RDF stability
  3. Long-Run Stability: Run LiCl simulation for 50ps and monitor Li-Li distance distribution to ensure no unphysical collapse events

## Open Questions the Paper Calls Out

- **Question:** How does increasing noise variance ($\sigma$) quantitatively affect model performance relative to theoretical error from Taylor expansion?
  - **Basis:** Paper proposes investigating how increasing noise level affects performance to verify if degradation adheres to error term derived from Taylor expansion
  - **Why unresolved:** Authors derive error scaling of $\delta_{DN} \propto \sigma^4$ but don't provide empirical validation across varying noise levels
  - **What evidence would resolve it:** Experiments systematically varying $\sigma$ and plotting resulting force error against theoretical $\sigma^4$ curve

- **Question:** What is optimal training strategy when combining data duplication with datasets containing explicit force labels?
  - **Basis:** Future work suggests exploring various combinations of data duplication and integration of samples with true force labels
  - **Why unresolved:** Current study only compares pure force-training against pure denoising (with duplication), without testing hybrid datasets
  - **What evidence would resolve it:** Ablation studies on mixed datasets (e.g., 50% labeled frames, 50% denoised frames) measuring simulation stability and accuracy

- **Question:** Can data duplication technique effectively stabilize simulations in high-dimensional all-atom systems?
  - **Basis:** Paper validates method on coarse-grained lithium chloride solution and notes that most diffusion models cannot predict full coordinates of every atom, limiting demonstration to coarse-grained MD
  - **Why unresolved:** Unclear if stability gains from data duplication in coarse-grained systems transfer to complex energy landscapes of all-atom simulations
  - **What evidence would resolve it:** Application of diffusion-trained NNP with data duplication to standard all-atom benchmark (e.g., organic molecules or proteins)

## Limitations

- Theoretical connection relies on first-order Taylor expansion valid only when noise variance is small relative to potential's curvature
- Data duplication assumes Gaussian noise samples around equilibrium coordinates are statistically independent training examples
- Validation is limited to coarse-grained LiCl system without assessment of kinetics or non-equilibrium properties
- Method's effectiveness in high-dimensional all-atom systems remains unproven

## Confidence

- **High confidence:** Mathematical derivation linking denoising to force recovery is sound given stated assumptions about small $\sigma$
- **Medium confidence:** Practical effectiveness of data duplication is demonstrated but lacks systematic ablation studies across different system sizes and noise levels
- **Medium confidence:** Coarse-grained LiCl simulation results show RDF agreement but are limited to single validation system without kinetics assessment

## Next Checks

1. **Scaling Factor Verification:** Systematically vary $\sigma$ from 0.01 to 0.5 Å and measure force recovery accuracy on test potential, quantifying approximation error
2. **Dataset Coverage Analysis:** Compare model performance using duplicated data versus expanded coordinate sampling on systems with known energy landscapes to identify when duplication fails
3. **Long-timescale Stability:** Run LiCl simulations for >100 ps and monitor not just RDFs but also diffusion coefficients and ion-pair lifetime distributions to assess whether equilibrium distributions alone capture relevant dynamics