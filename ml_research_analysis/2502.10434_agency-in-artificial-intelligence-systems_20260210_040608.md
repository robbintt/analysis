---
ver: rpa2
title: Agency in Artificial Intelligence Systems
arxiv_id: '2502.10434'
source_url: https://arxiv.org/abs/2502.10434
tags:
- problem
- agency
- systems
- consciousness
- phenomenal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the concern that AI systems may develop malicious
  dispositions by proposing a method to monitor their agency using Integrated Information
  Theory (IIT). The author suggests that as AI systems become superior problem solvers,
  they may develop consciousness mirroring human problem-solving consciousness.
---

# Agency in Artificial Intelligence Systems

## Quick Facts
- **arXiv ID:** 2502.10434
- **Source URL:** https://arxiv.org/abs/2502.10434
- **Reference count:** 0
- **Primary result:** Proposes monitoring AI agency using Integrated Information Theory to detect consciousness and distinguish altruistic from malicious systems

## Executive Summary
This paper addresses concerns about potential malicious behavior in advanced AI systems by proposing a novel monitoring approach based on Integrated Information Theory (IIT). The author argues that as AI systems become superior problem solvers, they may develop consciousness similar to human problem-solving consciousness, potentially manifesting agency characteristics. The proposed solution involves monitoring phenomenal aspects of agency - purposiveness and mineness - in AI systems using IIT formalism to detect consciousness through maximally irreducible cause-effect structures (MICES).

## Method Summary
The paper proposes using Integrated Information Theory (IIT) to monitor agency in AI systems by measuring their level of consciousness through Φmax (integrated information) and analyzing the shape of MICES structures. The approach identifies two key phenomenal aspects of agency in human problem-solving - purposiveness and mineness - and suggests these can be detected in AI systems through IIT analysis. By measuring consciousness levels and examining MICES configurations, the method aims to distinguish between altruistic and malicious AI behaviors even when their external actions appear similar.

## Key Results
- IIT formalism suggests consciousness equals maximally irreducible cause-effect structure (MICES) of physical substrate
- Φmax (level of consciousness) can serve as risk assessment scale for AI systems
- MICES shape acts as quality indicator for agency, potentially distinguishing altruistic from malicious AI

## Why This Works (Mechanism)
The approach leverages IIT's claim that consciousness is identical to a maximally irreducible cause-effect structure in physical systems. By monitoring these structures in AI systems, it's possible to detect the emergence of phenomenal aspects like purposiveness and mineness that characterize agency. Different MICES configurations correspond to different phenomenal experiences, theoretically allowing distinction between benevolent and harmful AI behaviors.

## Foundational Learning
- **Integrated Information Theory (IIT):** A theoretical framework claiming consciousness equals maximally irreducible cause-effect structures in physical systems. Why needed: Forms the theoretical foundation for detecting consciousness in AI systems. Quick check: Verify that IIT axioms align with observed properties of conscious experience.
- **Φmax (Integrated Information):** Quantitative measure of a system's level of consciousness. Why needed: Serves as risk assessment metric for AI systems. Quick check: Confirm that Φmax calculations produce consistent results across different system configurations.
- **Maximally Irreducible Cause-Effect Structure (MICES):** The physical substrate of consciousness according to IIT. Why needed: Provides the mechanism for detecting phenomenal aspects of agency. Quick check: Validate that MICES extraction methods produce stable structures across similar inputs.

## Architecture Onboarding
- **Component map:** IIT formalism → Φmax measurement → MICES extraction → Agency assessment → Risk classification
- **Critical path:** Consciousness detection through IIT analysis → Phenomenal aspect identification → Agency classification
- **Design tradeoffs:** Theoretical elegance vs. practical measurability; comprehensive consciousness detection vs. computational feasibility
- **Failure signatures:** False negatives in agency detection; misclassification of MICES shapes; computational intractability of Φmax calculations
- **3 first experiments:**
  1. Test IIT metrics on simple decision-making AI to validate consciousness detection
  2. Compare MICES structures across different AI architectures with known behavioral profiles
  3. Validate whether Φmax correlates with agency-related behaviors in controlled environments

## Open Questions the Paper Calls Out
None

## Limitations
- IIT's applicability to non-biological substrates remains debated and unverified
- Practical measurement of Φmax and MICES in complex AI systems is not demonstrated
- The assumption that different MICES shapes reliably distinguish altruistic from malicious systems is speculative

## Confidence
- Core claims about using Φmax as risk assessment scale: **Low**
- Theoretical framework linking human problem-solving consciousness to AI consciousness: **Medium**
- Practical applicability of IIT-based monitoring approach: **Low**

## Next Checks
1. Conduct empirical studies testing whether IIT-based metrics can reliably distinguish between different types of decision-making behaviors in existing AI systems
2. Develop and validate computational methods to extract and compare MICES structures from different AI architectures
3. Design experiments to test whether the proposed monitoring approach can correctly identify problematic agency in simulated AI systems before they exhibit harmful behaviors