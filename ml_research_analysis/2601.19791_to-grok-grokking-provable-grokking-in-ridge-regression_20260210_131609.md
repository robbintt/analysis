---
ver: rpa2
title: 'To Grok Grokking: Provable Grokking in Ridge Regression'
arxiv_id: '2601.19791'
source_url: https://arxiv.org/abs/2601.19791
tags:
- grokking
- regression
- theorem
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first end-to-end theoretical proof of\
  \ grokking\u2014a phenomenon where generalization suddenly improves long after overfitting\u2014\
  in ridge regression. The authors prove three key stages: (1) early overfitting with\
  \ fast training loss convergence, (2) persistent poor generalization beyond the\
  \ overfitting point, and (3) eventual convergence to a solution with good generalization."
---

# To Grok Grokking: Provable Grokking in Ridge Regression

## Quick Facts
- arXiv ID: 2601.19791
- Source URL: https://arxiv.org/abs/2601.19791
- Authors: Mingyue Xu; Gal Vardi; Itay Safran
- Reference count: 40
- Primary result: First end-to-end theoretical proof of grokking in ridge regression with quantified bounds on grokking time

## Executive Summary
This paper provides the first rigorous theoretical proof of grokking in ridge regression, demonstrating that generalization can suddenly improve long after overfitting. The authors prove three distinct stages: early overfitting with fast training loss convergence, persistent poor generalization beyond the overfitting point, and eventual convergence to a solution with good generalization. They derive quantitative bounds on the "grokking time" in terms of hyperparameters like weight decay λ, sample size n, feature dimension m, and initialization scale ν². The theoretical predictions are validated through experiments on both linear models and neural networks.

## Method Summary
The authors analyze ridge regression in the overparameterized regime where the number of features exceeds the number of samples (m > n). They establish a three-stage characterization of grokking: (1) early training with fast convergence of training loss, (2) a plateau phase where training loss continues to decrease but generalization remains poor, and (3) eventual convergence to a solution with good generalization. The analysis leverages mean-field theory and dynamical systems approaches to track the evolution of both training and test errors over time. They derive bounds on the duration of each stage, particularly focusing on the "grokking time" t₂ which represents the delay before generalization improvement.

## Key Results
- Proves three-stage grokking phenomenon: early overfitting, plateau of poor generalization, and eventual good generalization
- Derives quantitative bounds showing grokking time t₂ ∝ 1/λ (scales inversely with weight decay)
- Shows grokking time depends logarithmically on initialization scale: t₁, t₂ ∝ ln(ν²)
- Validates theoretical predictions through experiments on linear models and neural networks
- Demonstrates that hyperparameters can be tuned to control grokking behavior precisely

## Why This Works (Mechanism)
The mechanism relies on the interplay between weight decay regularization and the optimization dynamics in overparameterized settings. During early training, the model quickly fits the training data by finding a solution in the null space of the data matrix that minimizes training loss but generalizes poorly. The weight decay term prevents immediate convergence to this poor solution, instead creating a plateau where the model slowly evolves toward a better generalization solution. The eventual improvement occurs when the optimization dynamics escape the poor local minima through the influence of the regularization term, which becomes dominant as the model approaches the optimal generalization solution.

## Foundational Learning
- **Ridge Regression**: Regularized linear regression with L2 penalty, essential for understanding how weight decay influences optimization dynamics and generalization
- **Mean-field Theory**: Framework for analyzing high-dimensional systems with many interacting components, needed to track the evolution of weight distributions in overparameterized models
- **Overparameterized Regime**: Setting where the number of parameters exceeds the number of samples (m > n), creates the conditions for grokking by enabling multiple solutions with different generalization properties
- **Training vs Test Error Dynamics**: Understanding how these two quantities evolve differently over time is crucial for characterizing the plateau phase of grokking
- **Spectral Properties of Data Matrices**: The eigenvalues of feature covariance matrices influence the convergence rates and the quality of solutions found during optimization

## Architecture Onboarding

**Component Map**
Ridge Regression -> Weight Decay Regularization -> Three-Stage Grokking Dynamics

**Critical Path**
1. Initialize model in overparameterized regime (m > n)
2. Apply ridge regression with weight decay λ
3. Track training and test error evolution through three stages
4. Derive bounds on grokking time based on hyperparameters

**Design Tradeoffs**
- Smaller λ increases grokking time (t₂ ∝ 1/λ) but may improve final generalization
- Larger initialization scale ν² increases grokking time (t₁, t₂ ∝ ln(ν²)) but provides more flexibility in the optimization landscape
- Higher dimensionality m relative to n creates more opportunities for poor generalization solutions

**Failure Signatures**
- If λ is too large, grokking may not occur as the model converges too quickly to a good solution
- If ν² is too small, the model may get stuck in poor local minima without sufficient exploration
- If the feature covariance doesn't satisfy bounded eigenvalue assumptions, the theoretical guarantees may not hold

**3 First Experiments**
1. Vary λ systematically to verify the 1/λ scaling of grokking time t₂
2. Test different initialization scales ν² to confirm the logarithmic dependence on grokking time
3. Examine the three-stage dynamics on data with different spectral properties to test the bounded eigenvalue assumption

## Open Questions the Paper Calls Out
None

## Limitations
- Requires feature covariance eigenvalues to be bounded below, which may not hold in practical settings
- Proof relies on specific initialization schemes and weight decay regimes that may not generalize to all grokking scenarios
- Quantitative bounds may be loose in practice due to conservative analysis techniques
- Model assumes infinite-width or mean-field regimes, limiting direct applicability to finite-width neural networks

## Confidence

**High Confidence**
- Existence proof and three-stage characterization of grokking in ridge regression

**Medium Confidence**
- Quantitative bounds for grokking time, as these depend on conservative estimates

**Low Confidence**
- Direct extension to deep neural networks without additional assumptions

## Next Checks
1. Test the quantitative predictions experimentally across a wider range of hyperparameters, particularly examining the scaling with λ and ν² to verify the 1/λ and ln(ν²) dependencies
2. Investigate whether the three-stage characterization holds for more complex data distributions and feature covariances that violate the bounded eigenvalue assumption
3. Design experiments to isolate the specific mechanisms that cause the transition from stage 2 to stage 3, potentially through spectral analysis of the learned weights during training