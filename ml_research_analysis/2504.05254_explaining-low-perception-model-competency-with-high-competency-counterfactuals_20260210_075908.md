---
ver: rpa2
title: Explaining Low Perception Model Competency with High-Competency Counterfactuals
arxiv_id: '2504.05254'
source_url: https://arxiv.org/abs/2504.05254
tags:
- counterfactual
- image
- images
- competency
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach to explain low model competency
  (a generalized form of predictive uncertainty) in image classification by generating
  high-competency counterfactual images. The authors develop five methods for counterfactual
  generation: Image Gradient Descent (IGD), Feature Gradient Descent (FGD), Autoencoder
  Reconstruction (Reco), Latent Gradient Descent (LGD), and Latent Nearest Neighbors
  (LNN).'
---

# Explaining Low Perception Model Competency with High-Competency Counterfactuals

## Quick Facts
- arXiv ID: 2504.05254
- Source URL: https://arxiv.org/abs/2504.05254
- Reference count: 40
- Primary result: Novel method uses high-competency counterfactual images to explain low model competency, improving MLLM explanation accuracy from ~18% to ~34%

## Executive Summary
This paper introduces a novel approach to explain low model competency in image classification by generating high-competency counterfactual images. The authors develop five methods for counterfactual generation and evaluate them across two datasets with six known causes of low competency. They demonstrate that providing Multimodal Large Language Models with original-counterfactual image pairs significantly improves explanation accuracy, from ~18% to ~34% for pre-trained models and up to 100% with fine-tuning. This work presents the first method to use counterfactual images as explanatory tools for low perception model competency.

## Method Summary
The paper develops five counterfactual generation methods: Image Gradient Descent (IGD), Feature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent Gradient Descent (LGD), and Latent Nearest Neighbors (LNN). These methods aim to transform low-competency images into high-competency versions while maintaining realism. Competency is estimated using PaRCE, which combines classifier confidence with in-distribution probability based on autoencoder reconstruction error. The generated counterfactuals are then used with Multimodal Large Language Models (MLLMs) to produce language explanations for why the original image had low competency.

## Key Results
- LGD achieves 100% success rate on the speed dataset and 98% on the lunar dataset
- Reco, LGD, and LNN are identified as most promising methods for counterfactual generation
- MLLM explanation accuracy improves from ~18% to ~34% when provided with counterfactual images
- Fine-tuned MLLM achieves ~100% accuracy in generating correct explanations

## Why This Works (Mechanism)

### Mechanism 1: Latent-Space Constraint Preserves Realism
Optimizing counterfactuals in the autoencoder latent space produces more realistic images than pixel-space gradient descent. The autoencoder decoder constrains outputs to the learned data manifold, ensuring decoded images remain plausible while maximizing competency.

### Mechanism 2: Competency Estimation via Reconstruction Error
Competency scores combine classifier confidence with in-distribution probability estimated from reconstruction loss. Low-competency images either have low MSP or high reconstruction loss (OOD-like).

### Mechanism 3: Counterfactual Contrast Enables MLLM Explanation
Providing MLLMs with original-counterfactual pairs improves explanation accuracy by making abstract differences concrete. Visual differences become directly observable rather than inferred.

## Foundational Learning

- **Autoencoder latent spaces and reconstruction**
  - Why needed here: Three of five methods (Reco, LGD, LNN) rely on encoder-decoder architectures. Understanding how latent vectors represent image semantics is essential for debugging counterfactual quality.
  - Quick check question: Given a trained autoencoder, would you expect a noisy image to have higher or lower reconstruction loss than a clean in-distribution image? Why?

- **Constrained optimization with tradeoff parameters**
  - Why needed here: LGD, IGD, and FGD all use loss functions balancing competency maximization against distance penalties (γ). Tuning γ controls proximity vs. validity.
  - Quick check question: In L(z′) = −ρ̂(h(z′)) + γd(z, z′), what happens to the counterfactual as γ → 0? As γ → ∞?

- **MLLM prompting for structured outputs**
  - Why needed here: The paper uses carefully designed prompts (B.1–B.5) to extract single-sentence explanations. Prompt structure affects whether the model compares images correctly.
  - Quick check question: What minimal information must a prompt include for an MLLM to explain low competency: (a) training set description, (b) competency estimator description, (c) original image only, or (d) original + counterfactual? Which are necessary per the paper's design?

## Architecture Onboarding

- **Component map:**
  Perception model (CNN classifier) → Competency estimator (PaRCE) → Counterfactual generator (5 methods) → MLLM (LLaMA 3.2 11B) → Explanation output

- **Critical path:**
  Input image → Competency score → (if below threshold) Counterfactual generation via LGD/LNN/Reco → Prompt construction with image pair → MLLM inference → Explanation output

- **Design tradeoffs:**
  - IGD/FGD: High feature similarity, low realism, slow (~1–5 sec), unreliable success (80–98% depending on dataset)
  - Reco: Fastest (~5–14 ms), moderate success (75–89%), good realism
  - LGD: Most reliable success (98–100%), good realism, slower (~1–4 sec)
  - LNN: Fast (~7–16 ms), high success on lunar (99.5%), lower on speed (91%), more latent deviation

- **Failure signatures:**
  - Pixelated/artifact-heavy counterfactuals → IGD/FGD failing; switch to latent methods
  - Success rate <80% on validation set → autoencoder may be undertrained or corruption types out-of-manifold
  - MLLM explanations with hallucinations or inverted reasoning → prompt engineering issue or counterfactual introduces confounding changes

- **First 3 experiments:**
  1. **Validate competency estimation:** Train autoencoder on ID data, plot reconstruction loss vs. known corruption severity; verify correlation with PaRCE scores.
  2. **Implement Reco baseline:** Simplest method (X′ = h(g(X))); measure success rate and perceptual loss on 100 held-out low-competency images per corruption type.
  3. **Compare LGD vs. LNN:** On same held-out set, measure success rate, FID/KID, and computation time; identify which corruption types favor each method.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a metalearner be developed to dynamically select the optimal counterfactual generation method for a specific image, rather than relying on a fixed algorithm?
- **Open Question 2:** Can counterfactual images and their language explanations be utilized effectively as a corrective tool to improve model predictions or for data augmentation?
- **Open Question 3:** How useful are counterfactual images to human users in identifying the causes of low model competency, and do they outperform saliency maps or other explanation methods?
- **Open Question 4:** Do these counterfactual generation methods and MLLM explanation pipelines generalize to more diverse, complex datasets with unknown or multifactorial causes of low competency?

## Limitations

- The competency estimation mechanism (PaRCE) relies on the assumption that reconstruction error correlates with in-distribution probability across all six corruption types, which may not hold for semantic corruptions or domain shifts.
- The effectiveness of counterfactuals for MLLM explanation assumes the MLLM can perform comparative visual reasoning, but this is weakly supported by prior work.
- The five methods were only tested on two datasets with known corruption causes, limiting generalization to real-world scenarios with unknown or complex corruptions.

## Confidence

- **High confidence:** LGD achieves 100% success rate on the speed dataset and produces realistic counterfactuals with good FID/KID scores (Tables 1-2).
- **Medium confidence:** Reco, LGD, and LNN are "most promising" for counterfactual generation based on combined metrics, though success rates vary by dataset and corruption type.
- **Low confidence:** MLLM explanation accuracy improvement from ~18% to ~34% with counterfactuals is promising but may be inflated by controlled experimental conditions.

## Next Checks

1. **Competency estimation validation:** Generate reconstruction loss vs. corruption severity plots across all six types to verify PaRCE's assumption that reconstruction error correlates with competency.
2. **Generalization test:** Apply the five counterfactual methods to a third dataset with unknown corruption types (e.g., corrupted ImageNet) and measure success rates without prior knowledge of corruption causes.
3. **MLLM robustness check:** Test whether MLLMs hallucinate explanations when counterfactuals introduce spurious changes versus cases where counterfactuals cleanly isolate the competency issue.