---
ver: rpa2
title: Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati
  Machine Translation System
arxiv_id: '2510.05113'
source_url: https://arxiv.org/abs/2510.05113
tags:
- evaluation
- translation
- system
- were
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MATRA, a supervised-learning-based reference
  MT evaluation metric for English-Gujarati translation. The authors extract 25 features
  from MT outputs and reference translations, then train two deep neural network variants
  (6 and 10 hidden layers, 500 epochs each) to predict human evaluation scores.
---

# Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System

## Quick Facts
- arXiv ID: 2510.05113
- Source URL: https://arxiv.org/abs/2510.05113
- Authors: Nisheeth Joshi; Pragya Katyayan; Palak Arora
- Reference count: 18
- Introduces MATRA, a supervised-learning-based reference MT evaluation metric for English-Gujarati translation

## Executive Summary
This study presents MATRA, a trainable reference-based evaluation metric designed to assess English-Gujarati machine translation quality. The metric leverages supervised learning to predict human evaluation scores by extracting 25 features from MT outputs and reference translations. Tested on 1,000 sentences across agriculture and education domains from seven MT systems, MATRA variants demonstrate superior performance compared to traditional metrics like BLEU, METEOR, and COMET, showing strong positive correlation with human judgments.

## Method Summary
The authors develop MATRA by extracting 25 linguistic and statistical features from machine translation outputs and reference translations. These features are used to train two deep neural network variants with 6 and 10 hidden layers, each trained for 500 epochs. The trained models predict human evaluation scores for MT system outputs. The metric is evaluated on 1,000 sentences from agriculture and education domains across seven different MT systems, with performance measured against human judgments.

## Key Results
- MATRA variants outperform existing metrics (BLEU, METEOR, LEPOR, chrF++, COMET) in predicting human evaluation scores
- Strong positive correlation demonstrated between MATRA scores and human judgments (HEval)
- MATRA produces evaluation scores comparable to human assessments for English-Gujarati translation

## Why This Works (Mechanism)
MATRA works by learning the complex relationship between translation quality features and human judgments through supervised training. By extracting 25 features that capture various aspects of translation quality and training deep neural networks on these features, the metric learns to predict scores that align with human evaluation criteria. The deep learning approach allows MATRA to capture non-linear relationships and interactions between features that simpler metrics cannot detect.

## Foundational Learning
- **Feature extraction in NLP**: Needed to quantify translation quality aspects; quick check: review feature engineering methodology
- **Deep neural network training**: Required for learning complex quality-score relationships; quick check: examine training procedures and hyperparameters
- **Correlation analysis with human judgments**: Essential for validating metric effectiveness; quick check: verify statistical methods used
- **Supervised learning for evaluation metrics**: Allows training on human-annotated data; quick check: understand training data characteristics
- **Reference-based evaluation metrics**: Compare translations to references rather than directly assessing meaning; quick check: contrast with reference-free approaches

## Architecture Onboarding
**Component Map**: Feature extraction -> Feature vector creation -> Deep neural network training -> Score prediction

**Critical Path**: Extract 25 features from MT output and reference -> Train neural network on feature vectors and human scores -> Predict quality scores for new translations

**Design Tradeoffs**: The study uses deep neural networks (6 and 10 layers) for learning complex relationships but doesn't explore simpler architectures or justify the specific depth choices. The 500 epochs training duration appears arbitrary without convergence analysis.

**Failure Signatures**: Poor performance on out-of-domain data, overfitting to the specific agricultural and educational domains tested, failure to generalize to other language pairs.

**First Experiments**: 1) Test MATRA on out-of-domain Gujarati-English translation tasks (e.g., medical, legal, or conversational domains) 2) Conduct ablation studies to identify which of the 25 features contribute most to model performance 3) Perform statistical significance testing on pairwise comparisons between MATRA and baseline metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability to domains beyond agriculture and education tested
- No validation on other language pairs beyond English-Gujarati
- Neural network architecture choices lack justification and ablation studies
- 500 epochs and layer configurations appear arbitrary without convergence analysis

## Confidence
- MATRA's superior performance over existing metrics: **High** (supported by quantitative comparisons)
- Strong correlation with human judgments: **Medium** (based on single test set)
- Generalizability to other domains/languages: **Low** (no cross-domain or cross-lingual validation)

## Next Checks
1. Test MATRA on out-of-domain Gujarati-English translation tasks (e.g., medical, legal, or conversational domains) to assess generalizability
2. Conduct ablation studies to identify which of the 25 features contribute most to model performance
3. Perform statistical significance testing on pairwise comparisons between MATRA and baseline metrics to confirm performance differences are not due to chance