---
ver: rpa2
title: 'CADIC: Continual Anomaly Detection Based on Incremental Coreset'
arxiv_id: '2511.08634'
source_url: https://arxiv.org/abs/2511.08634
tags:
- anomaly
- tasks
- detection
- learning
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CADIC, a novel continual anomaly detection
  method that incrementally updates a shared memory bank with embeddings from sequential
  tasks. Unlike prior approaches that require class-specific memory banks, CADIC employs
  a unified coreset updated via a nearest-neighbor-based replacement mechanism, enabling
  efficient, flexible knowledge acquisition without catastrophic forgetting.
---

# CADIC: Continual Anomaly Detection Based on Incremental Coreset

## Quick Facts
- **arXiv ID**: 2511.08634
- **Source URL**: https://arxiv.org/abs/2511.08634
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art anomaly detection with AUROC scores of 0.972 (MVTec AD) and 0.891 (VisA), and 100% accuracy on proprietary electronic paper dataset

## Executive Summary
This paper introduces CADIC, a continual anomaly detection method that leverages incremental coreset updates to maintain a shared memory bank across sequential tasks. Unlike prior approaches requiring class-specific memory banks, CADIC uses a unified coreset updated via a nearest-neighbor replacement mechanism. This enables efficient, flexible knowledge acquisition while mitigating catastrophic forgetting. The method demonstrates superior performance on standard benchmarks and shows promising results in real-world industrial applications.

## Method Summary
CADIC addresses continual anomaly detection by maintaining a unified memory bank that stores embeddings from sequential tasks. The method incrementally updates this memory using a nearest-neighbor-based replacement strategy, allowing the model to acquire new knowledge without forgetting previously learned patterns. This approach differs from traditional methods that require separate memory banks for each class or task, offering improved flexibility and efficiency in dynamic environments.

## Key Results
- Achieves AUROC scores of 0.972 on MVTec AD and 0.891 on VisA datasets
- Demonstrates 100% accuracy on proprietary electronic paper dataset
- Shows superior performance compared to state-of-the-art methods in continual learning scenarios

## Why This Works (Mechanism)
The unified coreset approach enables efficient knowledge accumulation across tasks by maintaining a representative subset of embeddings. The nearest-neighbor replacement mechanism ensures that the memory bank remains relevant and compact while preserving important anomaly patterns. This design prevents catastrophic forgetting by continuously updating the memory with new information while retaining critical knowledge from previous tasks.

## Foundational Learning
- **Continual learning**: Understanding how to learn sequentially without forgetting previous knowledge - needed for anomaly detection in evolving environments, check by verifying memory retention across task sequences
- **Coreset optimization**: Selecting representative subsets from large datasets - essential for maintaining efficient memory banks, check by analyzing coreset quality metrics
- **Nearest-neighbor search**: Fast retrieval of similar embeddings - crucial for efficient memory updates, check by measuring search performance and accuracy
- **Anomaly detection fundamentals**: Identifying deviations from normal patterns - core task requirement, check by evaluating detection rates on known anomalies
- **Memory management in deep learning**: Balancing storage and retrieval efficiency - necessary for scalable deployment, check by measuring memory usage and update times
- **Catastrophic forgetting prevention**: Maintaining performance across task transitions - critical for continual learning success, check by comparing performance with and without memory mechanisms

## Architecture Onboarding
- **Component map**: Input data -> Feature extractor -> Anomaly scorer -> Coreset updater -> Memory bank
- **Critical path**: Data embedding extraction → Nearest-neighbor replacement → Memory bank update → Anomaly detection
- **Design tradeoffs**: Unified memory bank vs. class-specific memory (flexibility vs. specialization)
- **Failure signatures**: Poor coreset quality leading to missed anomalies, computational overhead during memory updates
- **First experiments**: 1) Baseline anomaly detection without continual learning, 2) Performance comparison with class-specific memory banks, 3) Ablation study on nearest-neighbor replacement effectiveness

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational complexity during coreset updates not thoroughly addressed
- Nearest-neighbor mechanism may struggle with highly diverse anomaly distributions
- Proprietary dataset results lack independent verification

## Confidence
- **High Confidence**: State-of-the-art performance on MVTec AD and VisA datasets
- **Medium Confidence**: Industrial applicability claims and proprietary dataset results
- **Medium Confidence**: Unified coreset approach effectiveness

## Next Checks
1. Conduct computational efficiency analysis measuring runtime overhead during incremental coreset updates across varying task sequences
2. Test the method on additional industrial anomaly detection datasets with different distribution characteristics to verify generalizability
3. Perform ablation studies isolating the impact of the nearest-neighbor replacement mechanism versus alternative coreset update strategies