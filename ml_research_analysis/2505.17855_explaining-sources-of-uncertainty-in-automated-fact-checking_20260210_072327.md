---
ver: rpa2
title: Explaining Sources of Uncertainty in Automated Fact-Checking
arxiv_id: '2505.17855'
source_url: https://arxiv.org/abs/2505.17855
tags:
- uncertainty
- evidence
- claim
- explanations
- clue-span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLUE is a framework for generating natural language explanations
  of model uncertainty in multi-evidence fact-checking. It identifies conflicts and
  agreements between claims and evidence by extracting span-level interactions, then
  produces explanations via prompting or attention steering to verbalize these interactions.
---

# Explaining Sources of Uncertainty in Automated Fact-Checking

## Quick Facts
- arXiv ID: 2505.17855
- Source URL: https://arxiv.org/abs/2505.17855
- Reference count: 40
- CLUE framework generates uncertainty explanations for multi-evidence fact-checking without fine-tuning

## Executive Summary
CLUE is a framework for generating natural language explanations of model uncertainty in multi-evidence fact-checking. It identifies conflicts and agreements between claims and evidence by extracting span-level interactions, then produces explanations via prompting or attention steering to verbalize these interactions. Evaluated across three models and two datasets, CLUE improves faithfulness to model uncertainty (point-biserial correlations up to +0.102) and label-explanation consistency over a prompting baseline. Human evaluators judged CLUE explanations more helpful, informative, less redundant, and more consistent than the baseline.

## Method Summary
CLUE operates by first extracting span-level interactions between claims and evidence to identify conflicts and agreements. It then generates explanations through either prompting approaches or attention steering mechanisms that verbalize these interactions. The framework requires no fine-tuning and is designed to be generalizable across different fact-checking models and tasks requiring reasoning over complex information. The approach focuses on making model uncertainty interpretable by highlighting specific evidence-claim interactions that contribute to uncertain predictions.

## Key Results
- Point-biserial correlations improved by up to +0.102 for faithfulness to model uncertainty
- Human evaluators rated CLUE explanations more helpful and informative than baseline
- CLUE achieved better label-explanation consistency compared to prompting baseline

## Why This Works (Mechanism)
CLUE works by explicitly modeling the interactions between claims and evidence at the span level, rather than treating evidence as independent pieces of information. This allows the framework to capture nuanced relationships such as direct contradictions, supporting evidence, and nuanced disagreements that contribute to model uncertainty. By verbalizing these specific interactions through natural language explanations, CLUE makes the reasoning process transparent and interpretable.

## Foundational Learning
- Span-level interaction extraction: Why needed - to capture fine-grained relationships between claims and evidence; Quick check - verify that extracted spans align with human annotations of evidence-claim relationships
- Attention steering mechanisms: Why needed - to guide models toward relevant evidence-claim interactions; Quick check - measure changes in attention weights before and after steering
- Multi-evidence reasoning: Why needed - real-world fact-checking requires evaluating multiple pieces of evidence; Quick check - test on scenarios with conflicting evidence
- Uncertainty quantification: Why needed - to identify when model predictions are unreliable; Quick check - correlate uncertainty scores with prediction accuracy

## Architecture Onboarding
**Component Map:** Evidence extraction -> Span interaction identification -> Attention steering/prompting -> Natural language explanation generation

**Critical Path:** The span interaction identification stage is most critical, as it directly determines which evidence-claim relationships are verbalized in the final explanations.

**Design Tradeoffs:** CLUE trades potential performance gains from fine-tuning for generalizability and interpretability, allowing the framework to work across different models without retraining.

**Failure Signatures:** Explanations may become redundant or vague when evidence-claim interactions are too complex or when multiple pieces of evidence provide conflicting signals.

**First Experiments:**
1. Test CLUE on single-evidence scenarios to establish baseline performance
2. Evaluate explanation quality on controlled synthetic datasets with known evidence-claim relationships
3. Compare attention steering versus prompting approaches for explanation generation

## Open Questions the Paper Calls Out
None

## Limitations
- Modest improvements in point-biserial correlations (up to +0.102) despite statistical significance
- Human evaluation based on small sample sizes (29-28 participants) limiting generalizability
- Heavy reliance on automated metrics (METEOR, BLEU, BERTScore) that may not fully capture explanation quality

## Confidence
- Confidence in framework design and methodology: High
- Confidence in quantitative evaluation results: Medium
- Confidence in qualitative human evaluation findings: Medium-Low

## Next Checks
1. Conduct larger-scale human evaluation (n > 100) across diverse demographics to validate helpfulness and consistency ratings
2. Implement comparative study against additional state-of-the-art explanation methods including fine-tuned models
3. Test CLUE's generalizability on additional fact-checking datasets and real-world misinformation scenarios