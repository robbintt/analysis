---
ver: rpa2
title: 'AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing'
arxiv_id: '2501.17784'
source_url: https://arxiv.org/abs/2501.17784
tags:
- dataset
- process
- arxiv
- language
- defect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study fine-tunes large language models to predict defect regimes
  (Keyholing, Lack of Fusion, Balling, None) in additive manufacturing from process
  parameter inputs. Models including DistilBERT, SciBERT, T5, and Llama 3.2-1B are
  trained on a dataset combining experimental and simulation data, totaling over 70
  million input-label pairs across two formats: structured (Baseline) and natural
  language (Prompt).'
---

# AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing

## Quick Facts
- arXiv ID: 2501.17784
- Source URL: https://arxiv.org/abs/2501.17784
- Reference count: 14
- Large language models fine-tuned to predict defect regimes in additive manufacturing achieve up to 93.68% accuracy

## Executive Summary
This study demonstrates that fine-tuned large language models can effectively predict defect regimes (Keyholing, Lack of Fusion, Balling, None) in additive manufacturing from process parameter inputs. The researchers train models including DistilBERT, SciBERT, T5, and Llama 3.2-1B on a combined dataset of experimental and simulation data totaling over 70 million input-label pairs. The models are evaluated in two formats: structured (Baseline) and natural language (Prompt). Llama 3.2-1B achieves the highest accuracy at 93.68% on the Baseline dataset, while T5 performs best (88.13%) on the Prompt dataset, showing that larger models and longer training improve accuracy, with natural language input enabling accessible, domain-agnostic predictions.

## Method Summary
The study fine-tunes several large language models (DistilBERT, SciBERT, T5, Llama 3.2-1B) to classify defect regimes in additive manufacturing. The dataset combines experimental and simulation data, structured as both tabular process parameters and natural language descriptions. Models are trained on over 70 million input-label pairs across two formats: structured (Baseline) and natural language (Prompt). Performance is evaluated by accuracy on held-out test sets, comparing model architectures and input representations to identify optimal approaches for defect prediction.

## Key Results
- Llama 3.2-1B achieves highest accuracy at 93.68% on structured (Baseline) dataset
- T5 performs best on natural language (Prompt) dataset with 88.13% accuracy
- Larger models and longer training consistently improve prediction accuracy
- Natural language input enables accessible, domain-agnostic defect predictions from process parameters

## Why This Works (Mechanism)
Large language models excel at pattern recognition and generalization across diverse data representations. In additive manufacturing, defect formation depends on complex, non-linear relationships between process parameters (laser power, scan speed, hatch spacing, etc.) and material responses. LLMs can capture these intricate relationships through their transformer architectures, which process input sequences bidirectionally and learn contextual embeddings. The natural language format particularly leverages the models' pre-training on diverse text corpora, allowing them to reason about manufacturing parameters even when expressed conversationally. Fine-tuning adapts these general language capabilities to the specific domain of defect classification, while the large parameter counts enable modeling of subtle parameter-defect relationships that simpler models might miss.

## Foundational Learning

1. **Additive Manufacturing Defect Physics**
   - Why needed: Understanding how process parameters cause specific defects is essential for interpreting model predictions and validating results
   - Quick check: Review literature on melt pool dynamics, heat transfer, and solidification behavior in AM processes

2. **Large Language Model Fine-tuning**
   - Why needed: The study's success depends on proper adaptation of pre-trained LLMs to the defect classification task
   - Quick check: Study transfer learning techniques, learning rate schedules, and evaluation metrics for fine-tuning LLMs

3. **Dataset Composition and Bias**
   - Why needed: With 70+ million samples, understanding data distribution and potential biases is critical for interpreting model performance
   - Quick check: Analyze class balance, parameter coverage, and the ratio of experimental vs. simulation data in the dataset

## Architecture Onboarding

**Component Map:** Process Parameters → Natural Language/Structured Input → LLM Encoder → Classification Head → Defect Regime Output

**Critical Path:** The transformer encoder processes the input sequence through self-attention mechanisms to create contextual representations, which are then passed through a classification head (typically a linear layer with softmax) to produce defect class probabilities.

**Design Tradeoffs:** Structured input offers precise numerical representation but requires domain expertise, while natural language enables accessibility but may lose precision in translation. Larger models provide better accuracy but increase computational cost and latency.

**Failure Signatures:** Models may struggle with out-of-distribution parameters, confuse visually similar defects, or produce overconfident predictions on uncertain inputs. Natural language models might misinterpret parameter descriptions with ambiguous terminology.

**First Experiments:**
1. Test model predictions on process parameters at the boundaries between defect regimes to assess confidence calibration
2. Evaluate model performance on a held-out experimental dataset from different AM systems not seen during training
3. Compare inference latency and computational requirements between structured and natural language prediction modes

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition details are unclear, with insufficient breakdown of experimental versus simulation data proportions
- Reported accuracies lack confidence intervals or statistical significance testing across multiple training runs
- Practical utility of natural language inputs in real manufacturing environments remains unverified

## Confidence

**Model performance claims:** Medium confidence - results are strong but lack statistical validation details
**Dataset representativeness:** Low confidence - insufficient detail about data composition and diversity
**Natural language format utility:** Medium confidence - innovative approach but practical deployment concerns unaddressed

## Next Checks
1. Conduct statistical significance testing with confidence intervals across multiple training runs to verify performance differences between models and input formats
2. Evaluate model performance on held-out experimental data from different AM systems, materials, and parameter ranges not present in the training set
3. Implement and test the natural language prediction system in a real manufacturing environment to assess practical utility and identify deployment challenges