---
ver: rpa2
title: 'MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large
  Video Models'
arxiv_id: '2509.08538'
source_url: https://arxiv.org/abs/2509.08538
tags:
- video
- hallucination
- binary
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MESH, a novel benchmark designed to evaluate
  hallucinations in large video models (LVMs) by mimicking human video understanding.
  MESH employs a question-answering framework with binary and multi-choice formats,
  incorporating target and trap instances across three domains: setting, character,
  and stage.'
---

# MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models

## Quick Facts
- arXiv ID: 2509.08538
- Source URL: https://arxiv.org/abs/2509.08538
- Reference count: 40
- This paper introduces MESH, a novel benchmark designed to evaluate hallucinations in large video models (LVMs) by mimicking human video understanding.

## Executive Summary
This paper introduces MESH, a novel benchmark designed to evaluate hallucinations in large video models (LVMs) by mimicking human video understanding. MESH employs a question-answering framework with binary and multi-choice formats, incorporating target and trap instances across three domains: setting, character, and stage. Experiments show that while LVMs excel at recognizing basic objects and coarse features, their susceptibility to hallucinations increases significantly when handling fine details or aligning multiple actions involving various subjects in longer videos. The results demonstrate that MESH effectively differentiates LVMs based on their ability to leverage multi-frame tokens for detecting fine features and actions, with stronger models showing better performance in video question answering and captioning tasks.

## Method Summary
MESH evaluates LVMs using a question-answering framework across three hierarchical domains: Setting (objects/environments), Character (fine-grained human features), and Stage (actions/dialogues with subject-action alignment). The benchmark uses TVQA+ dataset annotations including spatial grounding, subtitles, and action labels. Questions are generated automatically using a pipeline that creates target instances (ground truth) and trap instances (negative examples) for each hallucination aspect. The evaluation employs both binary and multi-choice formats with specific metrics including accuracy, positive ratio, option balance, and JSD divergence. Frame sampling strategies vary by domain, with 32 frames for setting/stage and 8-64 frames for character tasks.

## Key Results
- Advanced LVMs (InternVL2.5-78B, LLaVA-Video-72B) achieve 85.6% and 84.8% average accuracy on MESH, significantly outperforming weaker models
- Hallucination susceptibility increases substantially for fine-grained details and multi-subject action alignment tasks
- Character task performance improves with more input frames (8→16→32), demonstrating the importance of multi-frame token aggregation
- Setting tasks show low hallucination rates (0.2-0.5%) due to strong spatial grounding, while Stage tasks show higher rates (0.9-1.3%) for complex subject-action alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning evaluation with human bottom-up perception processes may better expose hallucination patterns in LVMs compared to content-category-based benchmarks.
- Mechanism: MESH structures questions hierarchically from Setting to Character to Stage, mirroring how humans first ground basic elements before interpreting complex interactions. This forces models to demonstrate consistent grounding at each level before higher-level reasoning.
- Core assumption: LVM hallucinations stem partially from inconsistent or incomplete grounding of lower-level perceptual elements when attempting higher-level interpretations.
- Evidence anchors:
  - [abstract] "It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding."
  - [section 4.3.2] "We also show in Appendix E that when LVMs hallucinate on MESH questions, they often produce incorrect or ambiguous video descriptions. Incorporating finer hallucination aspects could enhance high-level VQA task design."
  - [corpus] VistaDPO paper (2504.13122) similarly addresses misalignment with "human intuition" and hallucinations in LVMs, supporting the need for perception-aligned evaluation.

### Mechanism 2
- Claim: Stronger LVMs' superior performance on MESH appears linked to their ability to effectively aggregate and leverage multi-frame visual tokens for fine-grained detection.
- Mechanism: Advanced models maintain accuracy on fine details and subject-action alignment by integrating token information across multiple frames, rather than relying on single-frame snapshots or losing details through aggressive pooling.
- Core assumption: The primary bottleneck for fine-grained video understanding is not model capacity alone, but the efficiency of temporal-spatial token aggregation and the preservation of detail during this process.
- Evidence anchors:
  - [abstract] "The results demonstrate that MESH effectively differentiates LVMs based on their ability to leverage multi-frame tokens for detecting fine features and actions..."
  - [section 4.3.1, Table 5] "Character: frame continuity matters. ... With more frames, LLaVA-Video and Aria demonstrate higher accuracy, leveraging multi-frame tokens to better identify characters, while other LVMs are distracted by spanning frames."
  - [corpus] Mitigating Hallucinations paper (2601.22574) proposes "Spatiotemporal-Semantic Contrastive Decoding" to address hallucinations, implying temporal processing is a key factor.

### Mechanism 3
- Claim: Hallucination susceptibility in LVMs increases with task complexity involving fine-grained details and multi-subject action alignment, potentially due to limitations in training data and token representation.
- Mechanism: Tasks requiring fine character features or aligning multiple actors with distinct actions overwhelm models whose training data may lack such granularity and whose token reduction layers may discard necessary visual information.
- Core assumption: Current pre-training datasets for video-language models are biased towards coarse attributes and single-subject actions, creating a data scarcity problem for fine-grained, multi-agent temporal reasoning.
- Evidence anchors:
  - [abstract] "...their susceptibility to hallucinations increases significantly when handling fine details or aligning multiple actions involving various subjects in longer videos."
  - [section 4.2.2] "This challenge stems from: (1) Aggressive token reduction via pooling (e.g., 2×2 or 4×4 layers) ... (2) training data often lacks detailed person descriptions, relying mostly on coarse attributes..."
  - [corpus] ELV-Halluc benchmark (2508.21496) notes hallucinations in long videos from "Semantic Aggregation," and VistaDPO (2504.13122) cites "video hallucination issues," indicating data and representation challenges are persistent.

## Foundational Learning

- Concept: **Hallucination in Large Video Models (LVMs)**
  - Why needed here: MESH is entirely dedicated to diagnosing this core failure mode. Understanding that hallucinations are outputs inconsistent with the video source (intrinsic) or external knowledge (extrinsic) is prerequisite.
  - Quick check question: In the context of LVMs, what distinguishes an intrinsic hallucination from an extrinsic one?

- Concept: **Bottom-up Perception in Video Understanding**
  - Why needed here: MESH's design is founded on mimicking this human cognitive process. Learners must grasp that humans first perceive settings/objects, then characters, then their interactions.
  - Quick check question: According to the paper, what are the three hierarchical domains of *mise-en-scène* used to structure MESH?

- Concept: **Visual Token Aggregation in Transformers**
  - Why needed here: The paper's analysis of performance gaps links them to how LVMs process and summarize visual information across frames into tokens. This is key to understanding the "why" behind model differences.
  - Quick check question: The paper mentions "aggressive token reduction via pooling." How might this operation negatively impact a model's performance on MESH's fine-grained character tasks?

## Architecture Onboarding

- Component map:
  - **MESH Benchmark Suite:** Three core hallucination datasets (Setting, Character, Stage)
  - **Question Generation Pipeline:** Automated pipeline that takes video annotations (spatial grounding, subtitles, actions) and generates binary/multi-choice questions with target and trap instances
  - **Evaluation Protocols:** Accuracy, Positive Ratio (binary), Option Balance (OB), Correct Option Balance (COB), and JSD divergence metrics
  - **Category Taxonomy:**
    - *Setting:* Objects grounded in space labels
    - *Character:* Features at coarse/medium/fine granularity
    - *Stage:* Dialogue (speaker identification) and Action (subject-action pairs with various trap types like AO, CO, SA, MI)

- Critical path:
  1. **Input:** Raw video with annotations from a source dataset (e.g., TVQA+)
  2. **Processing:** The pipeline extracts relevant elements (objects, character features via GPT-4o, action subtitles) and applies filtering for quality
  3. **Question Formulation:** For each hallucination aspect, it selects target instances from the video and generates trap instances from other videos or via semantic perturbation
  4. **Output:** A structured dataset of binary and multi-choice questions aligned with the *mise-en-scène* framework
  5. **Evaluation:** LVMs are prompted with the video and questions; predictions are scored against the designed metrics

- Design tradeoffs:
  - **Binary vs. Multi-Choice:** Binary is simple and scalable but has a narrow accuracy range (50-100%). Multi-choice (k=4) has a wider range (~25-100%), improving discrimination and efficiency (aggregating more checks per question) but requires more complex answer generation
  - **Annotation Reliance:** Using TVQA+ provides rich grounding but limits the video domain to TV shows. The pipeline is demonstrated as transferable (e.g., to UCF101), but this requires additional adaptation (e.g., using YOLO for bounding boxes)

- Failure signatures:
  - **High False Positive Rate:** A model with a high Positive Ratio in binary tasks (e.g., >80%) is overly agreeable and likely hallucinating on negative questions
  - **Steep Performance Drop from Coarse to Fine:** Indicates the model fails to leverage multi-frame tokens for detailed feature extraction
  - **Poor Performance on "Character in Video" (CIV) Trap Category:** Suggests the model cannot distinguish visually similar characters or track speaker identity across frames
  - **High Option Balance (OB) or Correct Option Balance (COB):** Indicates significant bias toward specific answer choices (e.g., always choosing "A"), undermining metric reliability

- First 3 experiments:
  1. **Baseline Evaluation on All Aspects:** Run a selected open-source LVM (e.g., LLaVA-Video-7B) through the full MESH benchmark. Record accuracy, Positive Ratio, and OB/COB across Setting, Character (all granularities), and Stage (Action & Dialogue) to establish a performance profile and identify major weaknesses
  2. **Ablation on Input Frame Count:** For the Character hallucination task, evaluate the same model using varying input clip lengths (e.g., 8, 16, 32, 64 frames) to directly test the "multi-frame token leverage" hypothesis. Plot accuracy vs. clip length for different granularities
  3. **Trap Category Diagnosis:** Isolate performance on specific Stage-Action trap types (e.g., Action-Out-of-Video [AO] vs. Mixed-in-Video [MI]) for multiple models. This reveals whether failures are due to action recognition alone (AO) or complex subject-action binding (MI), guiding targeted model improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of auditory cues alter the hallucination landscape within the defined MESH domains (Setting, Character, Stage)?
- Basis in paper: [explicit] The authors explicitly state that their benchmark "specifically examines human understanding of the visual components of videos, excluding audio" (Appendix A)
- Why unresolved: The current benchmark isolates visual processing; it is unknown if audio inputs would provide redundant grounding to reduce hallucinations or introduce new cross-modal conflicts
- What evidence would resolve it: A comparative study evaluating LVMs on MESH tasks with and without audio tracks enabled

### Open Question 2
- Question: What architectural modifications are required to overcome the trade-off between aggressive token reduction and the retention of fine-grained details?
- Basis in paper: [inferred] The paper notes that "Aggressive token reduction via pooling... sacrifices detail for efficiency," and identifies token reduction in adapters as a likely cause for hallucinations in complex tasks (Section 4.2.2, 4.2.3)
- Why unresolved: Current LVMs struggle with fine details because efficiency mechanisms (like pooling) discard the necessary visual information before it reaches the language model
- What evidence would resolve it: Ablation studies on models employing dynamic token compression or higher resolution encoders to measure performance changes on the "Fine" and "Advanced" MESH tasks

### Open Question 3
- Question: Can video instruction tuning with synthetic data bridge the performance gap in fine-grained character and action alignment?
- Basis in paper: [inferred] The authors observe that "training data often lacks detailed person descriptions" and that "current datasets remain insufficient" for learning high-variability human actions (Section 4.2.2, 4.2.3)
- Why unresolved: The paper suggests that data scarcity contributes to hallucinations, but it is unclear if synthetic data generation (used by models like LLaVA-Video) is sufficient or if human-annotated dense video captions are necessary
- What evidence would resolve it: Training identical LVM architectures on datasets with varying densities of fine-grained annotations and evaluating the resulting hallucination rates on the MESH benchmark

## Limitations

- **Domain Generalization and Annotation Dependency:** MESH's primary dataset (TVQA+) is limited to TV shows, raising questions about how well hallucination patterns detected translate to other video domains like surveillance footage, sports, or user-generated content
- **Single-Reference Annotation:** The benchmark uses single-reference ground truth for both target and trap instances, which does not fully capture the inherent ambiguity in video understanding tasks where multiple valid interpretations might exist
- **Cross-Modal Grounding Verification:** While MESH requires spatial grounding for objects and character features, the paper does not explicitly validate whether LVMs actually use this visual information versus relying solely on language priors from training data

## Confidence

**High Confidence:** The fundamental observation that LVMs hallucinate more on fine-grained details and multi-subject action alignment is well-supported by experimental data across multiple models and task categories. The correlation between model strength and performance on fine-grained tasks is consistently demonstrated.

**Medium Confidence:** The mechanism attributing hallucination increases to "aggressive token reduction via pooling" and training data limitations is plausible but not definitively proven. While the paper presents supporting evidence through ablation studies on frame count, alternative explanations (such as architectural differences or optimization strategies) are not fully ruled out.

**Medium Confidence:** The claim that MESH effectively differentiates LVMs based on their ability to leverage multi-frame tokens is supported by performance patterns but could benefit from more direct mechanistic studies. The relationship between token aggregation strategies and fine-grained detection accuracy is inferred rather than explicitly measured.

## Next Checks

**Validation Check 1:** Conduct cross-domain evaluation by applying MESH to diverse video sources beyond TVQA+ and UCF101, including surveillance footage, sports broadcasts, and user-generated content. Measure hallucination rates across domains to assess whether the identified patterns hold universally or are domain-specific artifacts.

**Validation Check 2:** Implement a controlled experiment ablating the token reduction mechanism in weaker LVMs (e.g., disabling 4x4 pooling layers) and re-evaluate their performance on MESH's fine-grained character tasks. Compare against baseline performance to isolate the contribution of token reduction to hallucination susceptibility.

**Validation Check 3:** Design a "visual-only" variant of MESH where questions are presented without textual context, forcing LVMs to rely purely on visual information. Evaluate the same models to determine whether their performance gaps stem from genuine visual comprehension limitations or language understanding advantages.