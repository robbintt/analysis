---
ver: rpa2
title: GenPlanX. Generation of Plans and Execution
arxiv_id: '2506.10897'
source_url: https://arxiv.org/abs/2506.10897
tags:
- planning
- state
- genplanx
- data
- goals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenPlanX integrates Large Language Models (LLMs) with classical
  AI planning to automate office-related tasks. It interprets natural language requests,
  generates structured planning problems, solves them using an AI planner, and executes
  the resulting plans while monitoring for success or failure.
---

# GenPlanX. Generation of Plans and Execution

## Quick Facts
- **arXiv ID:** 2506.10897
- **Source URL:** https://arxiv.org/abs/2506.10897
- **Reference count:** 40
- **One-line primary result:** GenPlanX outperforms LLM-only planning for office tasks, producing optimal solutions where LLMs often fail or return suboptimal plans.

## Executive Summary
GenPlanX is a hybrid AI system that combines Large Language Models (LLMs) with classical AI planning to automate office-related tasks. It interprets natural language requests, generates structured planning problems, solves them using a classical AI planner, and executes the resulting plans while monitoring for success or failure. The system addresses the limitations of LLM-only planning by leveraging the correctness and optimality guarantees of classical planners. Evaluations show GenPlanX successfully generates optimal plans for complex office workflows and reliably executes tasks such as data manipulation, presentation creation, and scheduling.

## Method Summary
GenPlanX translates natural language requests into executable plans through a hybrid LLM + classical planning approach. The system first extracts domain-specific entities from the request using an ensemble model. It then constructs a prompt including PDDL types, predicates, and few-shot intent examples, and sends it to an LLM. The LLM outputs a structured JSON dictionary containing objects, initial state, and goals. A compiler translates this JSON into a PDDL problem file, which is solved by the Fast-Downward planner via the Unified Planning library. The resulting plan is executed by mapping PDDL actions to Python functions, with execution monitoring that triggers replanning on failures or emergent goals. The approach uses a fixed, hand-authored PDDL domain model for the office environment.

## Key Results
- GenPlanX successfully generated optimal plans for complex office workflows involving data manipulation, presentation creation, and scheduling.
- It outperformed LLM-only planning, producing optimal solutions where LLMs often failed or returned suboptimal plans.
- The system demonstrated accurate translation of natural language to executable plans and reliable execution of tasks such as reading files, querying databases, creating charts, and managing appointments.

## Why This Works (Mechanism)

### Mechanism 1
Structured LLM output (JSON) enables reliable translation to formal planning problems. Instead of asking LLMs to generate PDDL directly—a format they struggle with—the system prompts for Python dictionaries containing objects, init_state, and goals. A compiler then deterministically translates this to PDDL. This leverages LLMs' strength in structured data extraction while avoiding their weakness in formal syntax generation.

### Mechanism 2
Classical planners provide correctness and optimality guarantees that LLM-only approaches cannot. Given a PDDL domain and problem, the planner exhaustively searches the state space to find a valid action sequence. Action costs enable optimal planning, guaranteeing the plan achieves all goals from the initial state using only legal actions.

### Mechanism 3
Execution monitoring with replanning handles real-world action failures and emergent goals. Each PDDL action maps to a Python function that modifies an execution state dictionary. Boolean monitoring functions verify effects. On failure—or when an action introduces new goals—the system replans from the current state.

## Foundational Learning

- **Concept: PDDL (Planning Domain Definition Language)**
  - Why needed here: The entire planning component relies on PDDL domain and problem files. Understanding types, predicates, actions, and costs is essential for extending the system.
  - Quick check question: Given an action with precondition `(at ?a ?l1)`, what happens if the agent is not at `?l1`?

- **Concept: State-Space Search in Classical Planning**
  - Why needed here: The planner searches through states by applying actions. Understanding heuristics, optimality, and plan costs helps debug why certain plans are generated.
  - Quick check question: Why might a planner fail to find a plan even when one exists? (Hint: think about heuristics and search limits.)

- **Concept: Few-Shot Prompting for LLMs**
  - Why needed here: GenPlanX uses few-shot examples in prompts to teach the LLM how to structure outputs for different intents. Understanding prompt engineering is critical for adding new intents.
  - Quick check question: If the LLM consistently omits the `init_state` key, what's the most efficient fix in the prompt?

## Architecture Onboarding

- **Component map:** Entity Extraction → Generate Prompt → LLM → Compiler → Planner → Execution & Monitoring
- **Critical path:** Request → Entity Extraction → Prompt Generation → LLM → Compiler → Planner → Execution. The LLM output and Compiler are the most fragile links—errors here cascade.
- **Design tradeoffs:**
  - Fixed domain model vs. learned domains: Current approach uses human-defined PDDL (reliable but requires expertise); future work suggests learning action models from observations.
  - JSON output vs. PDDL output from LLM: JSON is easier for LLMs but requires a compiler; PDDL output would skip compilation but is less reliable.
  - Low-level monitoring vs. high-level state translation: Currently monitors at Python level; translating back to PDDL state would enable richer replanning but adds complexity.
- **Failure signatures:**
  - Empty/invalid JSON from LLM → Compiler crash. Check prompt formatting, intent coverage.
  - Planner returns "no plan found" → Domain model may be missing actions, or init_state/goals are inconsistent. Validate PDDL files manually.
  - Action execution fails repeatedly → Python implementation may not match PDDL preconditions, or external service is unavailable.
  - Suboptimal plans → Check action costs in domain file; verify planner is configured for optimal search.
- **First 3 experiments:**
  1. Trace a simple request end-to-end: Use the Example 1.1 request. Step through each component with debug logging to understand data flow. Verify the LLM output matches the expected dictionary format.
  2. Add a new action to the domain: Implement a simple action (e.g., `send-slack-message`) by adding PDDL action, writing Python implementation, writing monitoring function, and adding intent example to prompt. Test with a request that uses it.
  3. Induce and debug a planning failure: Deliberately create a request with an impossible goal. Observe where the system fails and how to improve error messaging.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can action models be learned automatically from observations to expand GenPlanX's domain coverage without manual definition? The authors state a desire to "expand the set of actions... by learning action models from observations" rather than the current manual process.
- **Open Question 2:** How can the system be enhanced with goal reasoning capabilities to autonomously generate new goals during execution? The authors propose providing "goal reasoning capabilities" to "automatically generate new goals upon replanning" or by analyzing goal structure.
- **Open Question 3:** How can low-level execution states be effectively translated back into high-level PDDL states for robust monitoring? The paper notes that unlike other architectures, GenPlanX currently "only monitors the low-level state without translating it back to the high-level PDDL state."

## Limitations
- The system's performance is bounded by the completeness of the hand-authored PDDL domain model; missing actions or incorrect preconditions will prevent valid plan generation.
- The evaluation focuses on controlled office tasks; performance in more complex, open-ended domains remains untested.
- The entity extraction component relies on external models whose robustness to domain variations is not fully characterized.

## Confidence
- **High confidence:** The core mechanism of using LLMs to generate structured intermediate representations (JSON) that are then compiled to PDDL, and using classical planners for correctness and optimality guarantees.
- **Medium confidence:** The effectiveness of execution monitoring and replanning in handling real-world failures and emergent goals.
- **Medium confidence:** The claim that GenPlanX outperforms LLM-only planning for optimal solutions.

## Next Checks
1. **Stress-test the LLM JSON generation:** Systematically test the LLM's ability to generate valid JSON for a wide range of office task requests, including edge cases and requests that require complex goal formulations, to quantify the failure rate of this critical component.
2. **Quantify execution monitoring overhead:** Measure the time and resource overhead introduced by the execution monitoring and replanning loop compared to a system that assumes perfect execution, to assess the practical cost of this robustness feature.
3. **Evaluate in a new domain:** Apply the GenPlanX architecture to a different domain (e.g., simple robotics or software deployment) with a newly authored PDDL domain to test the generalizability of the approach and identify domain-specific limitations.