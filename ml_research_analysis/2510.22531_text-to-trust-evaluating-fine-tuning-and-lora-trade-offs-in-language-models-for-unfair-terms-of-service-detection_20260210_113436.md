---
ver: rpa2
title: 'Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models
  for Unfair Terms of Service Detection'
arxiv_id: '2510.22531'
source_url: https://arxiv.org/abs/2510.22531
tags:
- legal
- fine-tuning
- clauses
- terms
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated three approaches\u2014full fine-tuning, parameter-efficient\
  \ LoRA tuning, and zero-shot prompting\u2014for detecting unfair clauses in Terms\
  \ of Service. Models tested included BERT, DistilBERT, TinyLlama, LLaMA, and SaulLM\
  \ on the CLAUDETTE-ToS benchmark, plus zero-shot inference with GPT-4o and O-models."
---

# Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection

## Quick Facts
- arXiv ID: 2510.22531
- Source URL: https://arxiv.org/abs/2510.22531
- Authors: Noshitha Padma Pratyusha Juttu; Sahithi Singireddy; Sravani Gona; Sujal Timilsina
- Reference count: 11
- Primary result: Full fine-tuning achieved the best F1 score (89.6%), while LoRA-based models offered strong recall with up to 3× lower memory cost

## Executive Summary
This study evaluates three adaptation strategies—full fine-tuning, parameter-efficient LoRA tuning, and zero-shot prompting—for detecting unfair clauses in Terms of Service documents. Using the CLAUDETTE-ToS benchmark, the research compares BERT, DistilBERT, TinyLlama, LLaMA, and SaulLM models across different tuning paradigms. The findings reveal a clear trade-off: full fine-tuning delivers optimal precision-recall balance (89.6% F1), LoRA achieves competitive recall (97.5% for SaulLM) with 3× memory savings, while zero-shot prompting provides high recall but suffers from poor precision. The work provides practical baselines for automated fairness detection in legal text with considerations for deployment constraints.

## Method Summary
The research evaluates three adaptation strategies on the CLAUDETTE-ToS dataset (9,414 clauses from 50 ToS documents): full fine-tuning on BERT/DistilBERT, LoRA-based tuning on 4-bit quantized models (TinyLlama-1.1B, LLaMA-3B/7B, SaulLM-7B), and zero-shot prompting via OpenAI API with O-models. Models were trained with AdamW optimizer, cross-entropy loss, and gradient accumulation for 7B models. Performance metrics included F1 score, accuracy, precision, and recall, with validation on balanced subsets and deployment on 3.3M ToS documents from Common Crawl.

## Key Results
- Full fine-tuning achieved the best F1 score (89.6%) with excellent precision-recall balance
- LoRA-based models offered strong recall (97.5% for SaulLM) with up to 3× lower memory cost
- Zero-shot prompting yielded high recall (>89%) but relatively low precision (29-43%)
- BERT and DistilBERT set the performance ceiling with near identical F1 scores of 89.2% and 89.6%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full parameter fine-tuning achieves optimal precision-recall balance for specialized legal classification tasks