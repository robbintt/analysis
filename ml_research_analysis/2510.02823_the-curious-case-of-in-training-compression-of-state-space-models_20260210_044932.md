---
ver: rpa2
title: The Curious Case of In-Training Compression of State Space Models
arxiv_id: '2510.02823'
source_url: https://arxiv.org/abs/2510.02823
tags:
- state
- training
- reduction
- performance
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPRESSM, a principled in-training compression
  method for State Space Models (SSMs) based on balanced truncation from control theory.
  The core idea is to leverage the eigenvalue stability of Hankel matrices to identify
  and truncate low-energy state dimensions during training, rather than compressing
  post-training.
---

# The Curious Case of In-Training Compression of State Space Models

## Quick Facts
- arXiv ID: 2510.02823
- Source URL: https://arxiv.org/abs/2510.02823
- Authors: Makram Chahine; Philipp Nazari; Daniela Rus; T. Konstantin Rusch
- Reference count: 40
- **One-line primary result:** COMPRESSM achieves significant training acceleration with maintained or improved accuracy by compressing SSMs during training via balanced truncation, outperforming post-training compression and direct small-model training.

## Executive Summary
This paper introduces COMPRESSM, a novel in-training compression method for State Space Models (SSMs) based on balanced truncation from control theory. The core idea is to leverage the eigenvalue stability of Hankel matrices to identify and truncate low-energy state dimensions during training, rather than compressing post-training. This approach exploits the rank-preserving property of dominant Hankel singular values during optimization. Experiments on sMNIST, CIFAR10, ListOps, IMDB, and Pathfinder demonstrate that COMPRESSM significantly accelerates training while maintaining or improving performance compared to both uncompressed models and compressed models trained directly at smaller dimensions.

## Method Summary
COMPRESSM is a principled in-training compression method for State Space Models (SSMs) based on balanced truncation from control theory. The core idea is to leverage the eigenvalue stability of Hankel matrices to identify and truncate low-energy state dimensions during training, rather than compressing post-training. This approach exploits the rank-preserving property of dominant Hankel singular values during optimization. The method involves training an LRU with a large initial state dimension, computing Gramians to derive Hankel Singular Values (HSVs), and truncating the system to retain only dimensions capturing energy above a threshold τ. The balancing transform T is computed to ensure stability and preserve the essential dynamics of the model. COMPRESSM is integrated into the training loop, with reductions scheduled during learning rate warmup or at fixed intervals, and the optimizer state is handled by either re-initializing or slicing to match the new dimension.

## Key Results
- On CIFAR10, a COMPRESSM-reduced model (dimension 92) achieves 85.7% accuracy with a 1.5× speedup compared to the full model (86.5% accuracy), while a directly trained model at the same dimension reaches only 81.8% accuracy.
- COMPRESSM outperforms knowledge distillation and Hankel Nuclear Norm regularization in terms of accuracy and training efficiency, validating its effectiveness as an in-training compression strategy for SSMs.
- Experiments on sMNIST, CIFAR10, ListOps, IMDB, and Pathfinder demonstrate significant training acceleration while maintaining or improving performance compared to both uncompressed models and compressed models trained directly at smaller dimensions.

## Why This Works (Mechanism)
COMPRESSM works by leveraging balanced truncation, a well-established technique from control theory, to identify and remove low-energy state dimensions in SSMs during training. The method exploits the stability of Hankel singular values (HSVs), which capture the energy distribution across state dimensions. By truncating dimensions corresponding to small HSVs, COMPRESSM reduces model complexity while preserving the essential dynamics needed for accurate predictions. This in-training approach is more effective than post-training compression because it allows the model to adapt to the reduced dimension during optimization, maintaining performance while accelerating training.

## Foundational Learning
- **Balanced truncation**: A control theory technique for model order reduction by balancing controllability and observability Gramians. *Why needed:* Provides the theoretical foundation for identifying and removing low-energy state dimensions. *Quick check:* Verify that the balancing transform T correctly diagonalizes the Gramians.
- **Hankel Singular Values (HSVs)**: Eigenvalues of the product of controllability and observability Gramians, representing the energy distribution across state dimensions. *Why needed:* Used to determine which state dimensions can be truncated without significant loss of model accuracy. *Quick check:* Ensure HSVs are computed correctly and sorted in descending order.
- **Gramian matrices**: Controllability and observability Gramians capture the energy of states reachable from inputs and observable from outputs, respectively. *Why needed:* Essential for computing HSVs and the balancing transform T. *Why needed:* Required for computing the balancing transform T. *Quick check:* Verify numerical stability when computing Gramians, especially for eigenvalues near 1.
- **System Replacement**: The process of transforming and truncating the system matrices (A, B, C) using the balancing transform T. *Why needed:* Reduces the state dimension while preserving the essential dynamics of the model. *Quick check:* Ensure the transformed matrices maintain the required properties (e.g., diagonal structure of A).

## Architecture Onboarding

### Component Map
- Input sequences -> LRU layer (A, B, C matrices) -> Gramian computation -> HSV calculation -> Balancing transform T -> Matrix truncation -> Reduced LRU -> Output predictions

### Critical Path
1. Train LRU with large initial state dimension
2. Compute controllability and observability Gramians
3. Calculate Hankel Singular Values (HSVs)
4. Determine rank r for energy threshold τ
5. Compute balancing transform T
6. Transform and truncate (A, B, C) matrices
7. Update optimizer state for new dimension
8. Continue training with reduced model

### Design Tradeoffs
- **Early vs. late reduction**: Early reduction (during warmup) allows the model to adapt but may lose some information; late reduction preserves more information but may not fully exploit training acceleration.
- **Reduction frequency**: Multiple reductions can better adapt to the model's learning trajectory but add overhead; single reduction is simpler but may be suboptimal.
- **Energy threshold τ**: Higher thresholds lead to more aggressive compression but risk accuracy loss; lower thresholds preserve accuracy but reduce compression benefits.

### Failure Signatures
- **Numerical instability**: Gramian computation fails when eigenvalues of A are near 1, causing denominator explosion in Eq. 14.
- **Optimizer state mismatch**: Failure to properly handle optimizer state (e.g., Adam momentum) after dimension reduction leads to training instability or crashes.
- **Loss of diagonal structure**: If the balancing transform T destroys the diagonal structure of A without proper handling, the efficiency gains of LRU are lost.

### First Experiments
1. Implement and verify the Gramian computation (Eq. 14) with stability checks for eigenvalues near 1.0, as this is critical for numerical stability.
2. Compare the exact balancing matrix computation from Antoulas (2005) with any approximations or numerical methods used in the code to ensure fidelity.
3. Profile training speed with and without the optimizer state slicing/reinitialization to verify the claimed 1.5× speedup isn't offset by overhead from the compression steps.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper doesn't clearly address how diagonal structure is preserved or restored after the T transformation, which could be critical for maintaining the efficient LRU architecture.
- Handling of the optimizer state during dimension reduction steps is not explicitly described, which could affect reproducibility of the reported speedups.
- The exact numerical recipe for Theorem 2.6 (Balancing Matrix T) requires derivation from Gramian eigenvectors not fully specified in the paper.

## Confidence

| Claim | Confidence |
|-------|------------|
| Core theoretical framework using balanced truncation via Hankel singular values is sound | High |
| Empirical results showing performance improvements over baseline models | Medium |
| Exact mechanism for maintaining diagonal structure after transformation | Low |
| Precise handling of optimizer state during compression steps | Low |

## Next Checks
1. Implement and verify the Gramian computation (Eq. 14) with stability checks for eigenvalues near 1.0, as this is critical for numerical stability.
2. Compare the exact balancing matrix computation from Antoulas (2005) with any approximations or numerical methods used in the code to ensure fidelity.
3. Profile training speed with and without the optimizer state slicing/reinitialization to verify the claimed 1.5× speedup isn't offset by overhead from the compression steps.