---
ver: rpa2
title: 'NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary
  Markov Decision Processes'
arxiv_id: '2501.09646'
source_url: https://arxiv.org/abs/2501.09646
tags:
- environment
- agent
- change
- non-stationary
- ns-gym
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NS-Gym, the first open-source simulation
  toolkit and benchmark suite for non-stationary Markov decision processes (NS-MDPs).
  The authors identify four key characteristics affecting decision-making in NS-MDPs:
  what changes, how it changes, whether changes are detectable, and whether their
  magnitude is known.'
---

# NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary Markov Decision Processes

## Quick Facts
- arXiv ID: 2501.09646
- Source URL: https://arxiv.org/abs/2501.09646
- Reference count: 40
- This paper introduces NS-Gym, the first open-source simulation toolkit and benchmark suite for non-stationary Markov decision processes (NS-MDPs).

## Executive Summary
NS-Gym is the first open-source simulation toolkit and benchmark suite for non-stationary Markov decision processes (NS-MDPs). The authors identify four key characteristics affecting decision-making in NS-MDPs: what changes, how it changes, whether changes are detectable, and whether their magnitude is known. NS-Gym builds on the popular Gymnasium framework while adding wrappers that introduce non-stationarity through parameter changes in base environments like CartPole, FrozenLake, CliffWalker, and Bridge. The toolkit supports various problem types including single and continuous parameter changes, with configurable notification levels for agents. The authors benchmark six algorithms (MCTS, AlphaZero, DDQN, PAMCTS, ADA-MCTS, RATS) across different NS-MDP settings. Results show that detailed notifications generally improve performance, with methods incorporating risk-averse strategies performing better in highly stochastic environments.

## Method Summary
NS-Gym extends the Gymnasium framework by wrapping base environments with parameter-changing logic. The toolkit decouples the timing of parameter changes (via schedulers) from the update mechanics (via update functions), allowing flexible modeling of non-stationarity. Wrappers intercept step() calls, check schedulers for parameter updates, apply update functions to modify environment parameters, and return augmented observations containing change notifications and delta values. The framework supports three notification levels (basic boolean flag, detailed magnitude, full model snapshot) and provides a get_planning_env() method for model-based planners to retrieve stationary snapshots aligned with notification level. The authors benchmark six algorithms across different NS-MDP settings including single discrete changes and continuous parameter evolution.

## Key Results
- Detailed notifications generally enhance performance of most methods, with AlphaZero, MCTS, PA-MCTS, and RATS showing marked improvements when notifications are available
- PAMCTS with notification achieves 1392.23±65.57 reward on CartPole continuous change vs. 109.39±2.69 without notification
- ADA-MCTS performance degrades significantly with notifications due to its fixed budget constraint and limited exploration
- Methods incorporating risk-averse strategies (like PAMCTS) perform better in highly stochastic environments compared to standard variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segregating parameter evolution from agent decision-making enables modular non-stationarity injection without modifying base environments.
- Mechanism: NS-Gym wraps standard Gymnasium environments with a layer that intercepts step() calls, checks a scheduler for parameter updates, applies update functions to modify environment parameters (e.g., gravity, transition probabilities), and returns augmented observations containing change notifications and delta values.
- Core assumption: Base environments expose tunable parameters that can be modified at runtime without breaking internal consistency.
- Evidence anchors:
  - [abstract]: "segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations"
  - [section 3.2]: "each wrapper introduces non-stationarity by modifying some parameters that the base environment exposes"
  - [corpus]: Weak direct corpus evidence; neighboring papers address NS-MDP algorithms rather than simulation architecture.
- Break condition: Fails if base environments hardcode parameters or lack programmatic access to dynamics variables.

### Mechanism 2
- Claim: Notification granularity directly affects planning algorithm performance by controlling information asymmetry between environment state and agent model.
- Mechanism: Three notification levels (basic boolean flag, detailed magnitude, full model snapshot) populate the custom observation type's env_change and delta_change fields. Model-based planners (MCTS, AlphaZero, RATS) call get_planning_env() to retrieve a stationary snapshot aligned with notification level, enabling updated rollouts when detailed notification is enabled.
- Core assumption: Agents can meaningfully exploit parameter change information within episode horizons.
- Evidence anchors:
  - [section 4.2]: "detailed notifications generally enhance the performance of most methods. AlphaZero, MCTS, PA-MCTS, and RATS demonstrate marked improvements when notifications are available"
  - [table 4]: PAMCTS with notification achieves 1392.23±65.57 reward on CartPole continuous change vs. 109.39±2.69 without notification
  - [corpus]: Assumption: Related work on non-stationary RL (arXiv:2503.18607) suggests structured non-stationarity exploitation improves convergence, but no direct architectural comparison exists.
- Break condition: Fails when episode lengths are too short for adaptation, or when notification overhead exceeds planning time budgets.

### Mechanism 3
- Claim: Decoupling scheduler timing from update function mechanics enables flexible modeling of semi-Markov parameter evolution.
- Mechanism: Schedulers (Continuous, Periodic, Random) return boolean flags at each timestep indicating whether to trigger updates. Update functions (Increment, RandomWalk, Lipschitz-bounded) specify magnitude and direction of parameter changes. This separation allows modeling both memoryless Markov chains and non-memoryless sojourn-time distributions.
- Core assumption: Non-stationarity can be adequately captured by discrete parameter updates at decision epoch boundaries.
- Evidence anchors:
  - [section 3.5]: "We decouple the timing (and thereby, the frequency) and the manner of parameter changes"
  - [section 2]: "parameters θ evolve in time through a sojourn time distribution, which can be non-memoryless, thereby making the resulting stochastic process semi-Markovian"
  - [corpus]: Assumption: arXiv:2511.17598 on varying-discounting MDPs suggests continuous-time non-stationarity may require finer granularity, but NS-Gym's discrete approach remains untested against continuous baselines.
- Break condition: Fails for environments where parameters change continuously within decision epochs, or when parameter evolution depends on state-action history rather than time alone.

## Foundational Learning

- Concept: Markov Decision Processes (state transitions, reward functions, policies)
  - Why needed here: NS-Gym extends stationary MDP assumptions; understanding baseline MDP structure is prerequisite to grasping how non-stationarity modifies transition functions P(s'|s,a,θ) where θ becomes time-varying.
  - Quick check question: Can you explain why adding θ as a time-varying parameter breaks the Markov property, and how NS-Gym's notification system partially restores agent observability?

- Concept: Gymnasium API (reset, step, observation space, action space, info dict)
  - Why needed here: NS-Gym maintains full Gymnasium compatibility while extending observations with env_change and delta_change fields; using NS-Gym requires fluency with the base interface.
  - Quick check question: What are the four fields in NS-Gym's custom observation type, and which existing Gymnasium construct do they extend?

- Concept: Model-based vs. model-free RL (planning with known dynamics vs. learning from experience)
  - Why needed here: Benchmark algorithms span both categories; MCTS/RATS/AlphaZero require environment models while DDQN learns value functions. Notification levels affect model-based methods differently.
  - Quick check question: Why does ADA-MCTS only work in "without notification" settings, while RATS benefits from detailed notifications?

## Architecture Onboarding

- Component map:
  - Base Environment: Standard Gymnasium env (CartPole, FrozenLake, etc.)
  - NS-Gym Wrapper: Intercepts step/reset, manages parameter updates
  - Scheduler: Time-based trigger logic (Continuous, Periodic, Random)
  - Update Function: Parameter modification logic (Increment, RandomWalk, Bounded)
  - Observation Augmentation: Adds env_change dict, delta_change dict, relative_time
  - Planning Environment Provider: get_planning_env() returns snapshot at configured notification level

- Critical path:
  1. Instantiate base Gymnasium environment
  2. Define schedulers (timing) and update functions (magnitude) for each tunable parameter
  3. Map parameter names to update functions in tunable_params dict
  4. Wrap environment with NSClassicControlWrapper or appropriate wrapper, passing tunable_params and notification level
  5. In agent loop: call get_planning_env() before planning, use custom observation fields for adaptation logic

- Design tradeoffs:
  - Notification level vs. realism: Full notification simplifies adaptation but may not reflect real-world sensing limitations
  - Update frequency vs. agent adaptation speed: Continuous changes require faster adaptation than single discrete changes
  - Model access vs. algorithm applicability: Full environment model required for MCTS/RATS; DDQN/ADA-MCTS can operate without

- Failure signatures:
  - Agent performance matches random policy: Likely notification level mismatch (agent expecting detailed notification but configured for basic)
  - Catastrophic performance drop mid-episode: Scheduler triggering unbounded parameter changes; check update function constraints
  - Planning returns stale results: Agent not calling get_planning_env() after change notification; fix observation handling
  - Import errors or missing parameters: Base environment doesn't expose expected tunable parameters; verify environment compatibility

- First 3 experiments:
  1. Reproduce single-change FrozenLake results with MCTS: Configure PeriodicScheduler for one-time change, test with/without detailed notification, verify performance gap matches Table 3 (~0.11→0.24 reward improvement with notification)
  2. Implement custom RandomWalk update function with budget constraint: Modify existing RandomWalk to respect upper/lower bounds, validate on CartPole continuous change setting
  3. Benchmark a new algorithm (e.g., PPO from Stable-Baselines3) on Bridge environment: Compare against DDQN baseline to establish whether policy-gradient methods handle non-stationarity differently than value-based methods; document notification handling strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the architectural placement of the runtime monitor and model updater (integrated within the agent versus external support modules) impact the performance and convergence speed of NS-MDP algorithms?
- Basis in paper: [explicit] The authors state, "whether these components are part of the agent or those supporting the agent is orthogonal to our discussion" and explicitly "leave this question open."
- Why unresolved: The paper focuses on standardizing interfaces and environments rather than evaluating the theoretical or empirical differences between agent boundary definitions.
- What evidence would resolve it: A comparative study using NS-Gym where the monitor/updater logic is treated as an internal state of the learning agent versus an exogenous oracle.

### Open Question 2
- Question: Do algorithm rankings established on NS-Gym's classic control and grid-world benchmarks (e.g., CartPole, FrozenLake) correlate with performance in high-dimensional, continuous real-world scenarios like autonomous driving or medical diagnosis?
- Basis in paper: [inferred] The introduction motivates the work with high-stakes applications like autonomous driving and medical treatment, but the provided benchmark suite is restricted to low-dimensional classic control and grid-world tasks.
- Why unresolved: The current version of NS-Gym does not include wrappers for high-fidelity simulators or continuous real-world datasets mentioned in the motivation.
- What evidence would resolve it: Extending NS-Gym to wrap a high-fidelity simulator (e.g., CARLA) and benchmarking the same algorithms to observe if performance trends hold.

### Open Question 3
- Question: How does the performance of model-based algorithms (like PAMCTS) degrade when the "detailed notification" or "model snapshot" provided by the environment contains observation noise or detection latency?
- Basis in paper: [inferred] The framework currently allows for "Basic" or "Detailed" notifications via wrappers, but the paper notes that in practice, monitors are anomaly detectors which are imperfect. The benchmarks assume perfect knowledge upon notification.
- Why unresolved: The paper evaluates algorithms under configurable but perfect notification levels, without analyzing robustness to imperfect change detection.
- What evidence would resolve it: Benchmarks where the `change_notification` and `delta_change` fields are subject to stochastic noise or time delays.

## Limitations
- The primary limitation lies in the discrete nature of parameter updates, which may not capture truly continuous non-stationarity where environmental parameters evolve smoothly between decision epochs.
- The toolkit assumes parameters can be modified at runtime without breaking base environment consistency, which may not hold for all Gymnasium environments.
- The evaluation focuses on short-horizon episodic tasks where adaptation can occur within reasonable timeframes, potentially limiting applicability to problems requiring longer-term strategic planning under non-stationarity.

## Confidence
- Mechanism 1 (Modular parameter evolution): **High** - Well-specified architecture with clear separation of concerns, though empirical validation against alternative approaches is limited.
- Mechanism 2 (Notification granularity): **Medium** - Strong empirical results show performance improvements, but the theoretical understanding of optimal notification strategies across different algorithm types remains underdeveloped.
- Mechanism 3 (Scheduler timing separation): **Medium** - The architectural choice is sound, but the practical impact of different scheduling strategies on real-world problem domains requires further investigation.

## Next Checks
1. Test NS-Gym with a base environment that lacks programmatic parameter access to verify the wrapper's robustness to incompatible environments.
2. Compare continuous vs. discrete update strategies on the same problem to quantify the impact of update granularity on agent performance.
3. Implement a new algorithm that explicitly models non-stationarity without relying on notifications to assess whether notification-dependent approaches are truly necessary or merely convenient.