---
ver: rpa2
title: 'A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance,
  Consistency, and Faithfulness Across Languages'
arxiv_id: '2510.09555'
source_url: https://arxiv.org/abs/2510.09555
tags:
- languages
- thinking
- language
- traces
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive evaluation of multilingual
  Chain-of-Thought (CoT) reasoning, examining three dimensions: performance, consistency,
  and faithfulness. The study investigates how Large Reasoning Models (LRMs) perform
  across languages when reasoning in the same language as the prompt.'
---

# A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages

## Quick Facts
- **arXiv ID**: 2510.09555
- **Source URL**: https://arxiv.org/abs/2510.09555
- **Reference count**: 40
- **Primary result**: First comprehensive evaluation of multilingual CoT reasoning across performance, consistency, and faithfulness dimensions

## Executive Summary
This paper presents the first comprehensive evaluation of multilingual Chain-of-Thought (CoT) reasoning, examining how Large Reasoning Models perform across different languages when reasoning in the same language as the prompt. The study investigates three dimensions: performance (accuracy and language compliance), consistency (crosslingual answer consistency and trace interchange), and faithfulness (perturbation-based tests). Using two language control strategies—explicit instruction and prompt hacking—the authors find that models exhibit strong language preferences, with high-resource languages like English and Chinese generally performing better. The findings reveal that final-answer accuracy is influenced not only by trace quality but also by prompt language, and that models rely on traces to varying degrees across languages, with low-resource languages showing greater trace dependence.

## Method Summary
The study evaluates multilingual CoT reasoning using two benchmark datasets: MMMLU (15 languages, max 250 samples per language) and MGSM (11 languages, 250 parallel examples per language). Two language control strategies are employed: explicit instruction (appending language-specific requests) and prompt hacking (inserting language prefixes after the thinking token start). Three trace interchange methods are used: BaseSub (explicit instruction traces), HackSub (prompt hacking traces), and TransSub (translated-to-English traces). Perturbation tests include truncation (removing first/middle/last third of trace) and error injection (altering numbers in final sentence). Models evaluated include DeepSeek-R1-Distill-Qwen variants (1.5B-70B) and Qwen3 models. Metrics include language compliance rate, final answer accuracy, consistency scores (IoU), and matching ratio for faithfulness tests.

## Key Results
- Models exhibit strong language preferences, with high-resource languages (English, Chinese) generally outperforming low-resource languages
- Controlling thinking trace language comes at the cost of accuracy, revealing a trade-off between language control and task performance
- Final-answer accuracy is influenced not only by the thinking trace itself but also by the prompt language
- Languages other than English exhibit greater reliance on thinking traces, while English shows lower matching scores suggesting latent-state reasoning

## Why This Works (Mechanism)

### Mechanism 1: Resource-Asymmetric Reasoning Quality
- **Claim:** Forcing reasoning in low-resource languages degrades final-answer accuracy because the model's reasoning capability is anchored in high-resource representation spaces.
- **Mechanism:** When models are forced via prompt hacking to generate thinking traces in low-resource languages, language compliance increases but semantic quality drops due to the model struggling to express complex logic in a low-resource token space.
- **Core assumption:** Model reasoning capability is not uniformly distributed across languages; it is a function of pretraining data density.
- **Evidence anchors:** Abstract statement about cost of controlling thinking trace language; section 4 findings on prompt hacking trade-offs; corpus findings on language enforcement effects.

### Mechanism 2: Trace-Prompt Decoupling
- **Claim:** Final-answer accuracy is a composite function of the thinking trace quality and the prompt language context, not just the trace alone.
- **Mechanism:** Identical thinking traces produce different accuracy levels depending on prompt language, suggesting the prompt language "primes" the model's contextual understanding or attention mechanisms.
- **Core assumption:** Prompt language activates specific linguistic sub-networks that influence how the model interprets subsequent reasoning traces.
- **Evidence anchors:** Section 5 findings on prompt language influence; Swahili trace translation results showing higher accuracy in English; corpus findings on reasoning-answer misalignment.

### Mechanism 3: Variable Trace Faithfulness (Latent vs. Surface Reliance)
- **Claim:** Models rely on surface-level thinking traces more heavily for low-resource languages than for English, where they rely more on "latent-state" reasoning.
- **Mechanism:** Perturbation tests show corrupting English traces has smaller impact on final answer than corrupting low-resource traces, suggesting high-resource language models use internal reasoning that bypasses strict trace token attention.
- **Core assumption:** Higher capability in a language allows for internal reasoning that bypasses the need to attend strictly to generated trace tokens.
- **Evidence anchors:** Section 6 findings on language-specific trace reliance; truncation vs. error injection results; corpus findings on cross-lingual collapse and stability differences.

## Foundational Learning

- **Concept: Language Compliance vs. Reasoning Fidelity**
  - **Why needed here:** The paper distinguishes between compliance (generating text in requested language) and fidelity/accuracy (solving problems correctly). You cannot optimize multilingual CoT without understanding this trade-off.
  - **Quick check question:** If a model outputs 100% of its reasoning in Yoruba but solves 0% of math problems correctly, does it have high compliance or high fidelity?

- **Concept: Prompt Hacking (Prefix Injection)**
  - **Why needed here:** Standard instruction tuning is insufficient to force language-specific reasoning. Understanding this technique is critical to reproducing high-compliance results.
  - **Quick check question:** Why does inserting a prefix after the thinking token start work better than appending a natural language instruction before the question?

- **Concept: Faithfulness via Perturbation**
  - **Why needed here:** Accuracy alone cannot evaluate reasoning quality. Faithfulness checks determine if models actually use the reasoning they show, or if traces are performative.
  - **Quick check question:** If a model maintains high accuracy even when the final step of its thinking trace is replaced with a wrong number, is the model "faithful" to its CoT?

## Architecture Onboarding

- **Component map:** Input Layer (Multilingual Prompt) -> Controller (Language Strategy: Explicit Instruction vs. Prompt Hacking) -> Core (Large Reasoning Model) -> Reasoning Stream (Thinking Trace with `ático` and `_PAYLOAD_` tokens) -> Interchange/Perturbation Module (BaseSub, HackSub, TransSub, Truncation, Error Injection) -> Evaluator (Language Compliance, Accuracy, Consistency, Matching Ratio)

- **Critical path:**
  1. Select prompt language (e.g., German)
  2. Apply language control strategy (e.g., Hacking: inject "Auf Deutsch denken")
  3. Generate thinking trace and final answer
  4. **Perturbation step (Critical):** Intercept generated trace, modify final sentence (error injection) or remove segment (truncation), feed back to model
  5. Compare against gold answer

- **Design tradeoffs:**
  - **Strict Control vs. Performance:** Use "Prompt Hacking" for strict language alignment (user explainability) but expect ~10-20% accuracy drops in low-resource languages compared to "Explicit Instruction"
  - **Model Scale:** Smaller models (1.5B-7B) show higher surface-level faithfulness (copying errors), while larger models (70B+) show lower faithfulness (ignoring errors), suggesting reliance on internal states or memorization

- **Failure signatures:**
  - **"English Leakage":** Explicit instruction failing; trace defaults to English while prompt is in Bengali
  - **"Faithlessness":** High accuracy despite corrupted traces (indicates potential data contamination or memorization, especially in Qwen3 models)
  - **"Negative Drop":** Accuracy improves when thinking trace is truncated (indicates model is confusing itself with verbose, low-quality reasoning)

- **First 3 experiments:**
  1. **Compliance Baseline:** Run MGSM on a 7B model for English vs. Swahili using both Explicit Instruction and Prompt Hacking. Plot Compliance Rate vs. Accuracy trade-off curve.
  2. **Cross-lingual Trace Injection:** Take high-performing English thinking trace, translate to German, inject into German prompt (HackSub). Compare accuracy against native-generated German trace.
  3. **Faithfulness Probe:** Perform "Last Sentence Error Injection" on a 32B model for English vs. Chinese. Measure "Matching Ratio" to verify variable faithfulness mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific mechanistic explanations for why reasoning inconsistencies and faithfulness disparities arise across different languages?
- **Basis in paper:** The authors explicitly state in the Limitations section that they do not provide a mechanistic explanation for why these inconsistencies arise and suggest future research could apply mechanistic interpretability methods.
- **Why unresolved:** The study quantifies performance gaps and trace inconsistencies but stops short of identifying specific neuron-level or internal representations causing different reasoning behavior across languages.
- **What evidence would resolve it:** Research utilizing causal tracing or probing techniques to identify internal activations that differ when processing semantically equivalent prompts in different languages.

### Open Question 2
- **Question:** How does multilingual reasoning faithfulness hold up under more sophisticated or adversarial perturbations, such as distractor reasoning or paraphrasing?
- **Basis in paper:** The authors note in the Limitations section that more sophisticated or adversarial perturbations remain unexplored and could be incorporated in future work.
- **Why unresolved:** Current study only evaluates faithfulness using simple interventions (truncation and error injection), not more subtle manipulations like plausible-sounding but logically flawed distractors.
- **What evidence would resolve it:** A study evaluating model performance when injected thinking traces contain plausible but logically flawed distractors, or when the trace is paraphrased without changing semantic meaning.

### Open Question 3
- **Question:** Is the observed resilience to trace truncation caused by "latent-state reasoning" or simply by dataset memorization?
- **Basis in paper:** The authors hypothesize that truncating the final segment has less impact, suggesting latent-state reasoning, but also discuss that larger models might rely on memorization or contamination.
- **Why unresolved:** The paper observes that truncating thinking traces impacts performance less than error injection, but cannot definitively distinguish between internal reasoning continuation and memorized answers.
- **What evidence would resolve it:** Experiments using novel, out-of-distribution reasoning tasks where memorization is impossible to see if truncation resilience persists, or analysis of internal hidden states to verify if reasoning continues after truncation.

## Limitations
- The causal mechanism behind why prompt hacking achieves higher compliance than explicit instruction remains incompletely explained
- Perturbation-based faithfulness tests cannot definitively distinguish between surface-level trace copying and deeper latent reasoning without access to model internals
- The study does not address potential contamination effects from pre-training data, which could artificially inflate faithfulness scores for certain languages or models

## Confidence
- **High Confidence:** High-resource languages (English, Chinese) consistently outperform low-resource languages across all evaluation dimensions; language compliance trade-off between explicit instruction and prompt hacking is clearly demonstrated
- **Medium Confidence:** Models rely more heavily on thinking traces for low-resource languages than for English, but alternative explanations (contamination effects, memorization patterns) cannot be ruled out
- **Low Confidence:** The exact mechanism by which prompt language influences final-answer accuracy when thinking trace is held constant remains speculative, requiring more rigorous linguistic analysis

## Next Checks
1. **Contamination Analysis:** Run perturbation experiments on held-out subset of questions not present in pre-training data. Compare faithfulness scores between seen and unseen questions to quantify contamination effects, particularly for Qwen3 models.

2. **Latent Reasoning Probe:** Design experiment where model is given corrupted thinking trace with early logical error but correct final answer. If model arrives at correct answer despite corrupted trace, this would provide stronger evidence for latent-state reasoning in high-resource languages.

3. **Linguistic Priming Test:** Create controlled experiment where same thinking trace (translated to multiple languages) is paired with prompts in different languages. Measure whether prompt language systematically affects model's interpretation of trace, even when trace content is identical, to validate prompt language priming hypothesis.