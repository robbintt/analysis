---
ver: rpa2
title: When an LLM is apprehensive about its answers -- and when its uncertainty is
  justified
arxiv_id: '2503.01688'
source_url: https://arxiv.org/abs/2503.01688
tags:
- uncertainty
- reasoning
- entropy
- question
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates uncertainty estimation in large language
  models (LLMs) for multiple-choice question-answering tasks, focusing on token-wise
  entropy and model-as-judge (MASJ) approaches. The authors evaluate four LLMs (Phi-4,
  Mistral, and Qwen variants) across 14 domains using the MMLU-Pro dataset.
---

# When an LLM is apprehensive about its answers -- and when its uncertainty is justified

## Quick Facts
- arXiv ID: 2503.01688
- Source URL: https://arxiv.org/abs/2503.01688
- Authors: Petr Sychev; Andrey Goncharov; Daniil Vyazhev; Edvard Khalafyan; Alexey Zaytsev
- Reference count: 40
- Primary result: Token-wise entropy effectively predicts model errors in knowledge domains (ROC AUC ~0.73 for biology) but fails in reasoning domains (ROC AUC ~0.55 for math), while model-as-judge approaches perform near-randomly.

## Executive Summary
This study investigates uncertainty estimation in large language models for multiple-choice question-answering tasks, comparing token-wise entropy against model-as-judge (MASJ) approaches. The authors evaluate four LLMs (Phi-4, Mistral, and Qwen variants) across 14 domains using the MMLU-Pro dataset. Results show that entropy-based uncertainty estimates effectively predict model errors in knowledge-dependent domains but perform poorly in reasoning-dependent domains. MASJ performs near-random prediction, indicating it captures different aspects of uncertainty than question complexity. The study reveals internal biases in MMLU-Pro, with question complexity and required reasoning varying significantly by topic, suggesting the need for more balanced datasets.

## Method Summary
The study evaluates uncertainty estimation methods on MMLU-Pro using four LLMs (Phi-4, Mistral-Small-24B-Instruct, Qwen-1.5B, and Qwen-72B) and a larger judge model (Mistral-Large-Instruct-2411). Token-wise entropy is calculated from output logits using Shannon entropy formula $u = -\sum p_i \log p_i$. MASJ evaluates answers through specific prompts estimating complexity, reasoning requirements, and reasoning steps. The study measures error prediction performance using ROC-AUC and calibration quality through ECE, stratified by domain and reasoning level.

## Key Results
- Token-wise entropy predicts model errors effectively in knowledge domains (biology ROC AUC 0.73) but poorly in reasoning domains (math ROC AUC 0.55)
- Model-as-judge approaches perform near-randomly for error prediction (ROC AUC ~0.49), indicating they capture different uncertainty aspects
- Larger models show better entropy-based error prediction with clearer separation between correct and incorrect answers
- Systematic overconfidence affects all models, with high-confidence predictions not matching accuracy
- MMLU-Pro dataset shows internal biases in reasoning requirements varying significantly by topic

## Why This Works (Mechanism)

### Mechanism 1: Token-wise Entropy as a Proxy for Knowledge-Gap Uncertainty
Token-wise entropy effectively predicts errors in knowledge domains because it reflects the model's internal knowledge state. When models have strong learned patterns for correct answers, probability distributions concentrate (low entropy). Absent or ambiguous knowledge causes flatter distributions (high entropy), correlating with higher error likelihood. This mechanism degrades in reasoning domains where models can confidently execute flawed reasoning chains, producing low-entropy but incorrect outputs.

### Mechanism 2: Failure of Model-as-Judge (MASJ) for Error Prediction in QA
MASJ performs near-randomly because it lacks the rich signals needed to assess short multiple-choice answers. The judge model solicits explicit confidence scores but cannot evaluate the validity of reasoning behind answers based on surface-level patterns alone. This creates a disconnect between the judge's assessment and actual correctness, making MASJ ineffective for error prediction in this context.

### Mechanism 3: Calibration and Model Scaling
Larger models demonstrate better entropy-based error prediction because they form sharper internal representations for concepts they know, leading to lower entropy for correct answers and clearer separation from incorrect ones. However, all models suffer from systematic overconfidence, where high confidence (low entropy) doesn't match actual accuracy. This shows scale improves separation but doesn't solve calibration issues.

## Foundational Learning

**Entropy as Uncertainty Quantification**: Why needed - This is the primary metric for data uncertainty. The paper's central finding depends on understanding how entropy is calculated from output logits and what it represents. Quick check - Given output probabilities `[0.9, 0.05, 0.05]`, would you expect calculated entropy to be high or low, and what does this signify?

**Aleatoric vs. Epistemic Uncertainty**: Why needed - The paper distinguishes between data uncertainty (aleatoric, measured by entropy) and model uncertainty (epistemic, measured by MASJ). Understanding this decomposition is key to interpreting the critique of methods focusing on only one type. Quick check - A question is inherently ambiguous ("What is the best color?"). Which uncertainty type does this primarily contribute to, and which method would be more affected?

**ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**: Why needed - The paper uses ROC-AUC as the primary metric to judge how well entropy or MASJ predicts model error. A value of 0.5 indicates random performance, which is crucial for interpreting MASJ results. Quick check - A method achieves ROC-AUC of 0.51. How would you describe this method's ability to predict errors based on the paper's evaluation?

## Architecture Onboarding

**Component map**: QA Evaluator -> Logits Extractor -> Entropy Calculator -> MASJ Evaluator -> Analysis Pipeline

**Critical path**: 1. Annotate questions using MASJ for reasoning levels (low/med/high). 2. Run QA Evaluator on dataset to get answers and logits. 3. Calculate token-wise entropy for each answer. 4. Compare entropy and MASJ scores against ground truth to generate ROC-AUC and calibration metrics, stratified by MASJ-generated reasoning labels.

**Design tradeoffs**: The primary tradeoff is between simplicity and low cost of token-wise entropy versus potential richer assessment from MASJ. The paper demonstrates that in this setup, the "richer" MASJ signal is effectively noise (ROC AUC ~0.49), while simple entropy signal is highly informative conditional on question type. Another tradeoff is choosing MMLU-Pro, which while robust, shows internal biases in reasoning requirements across topics.

**Failure signatures**: 1. Reasoning-Domain Failure: Token-wise entropy will fail to predict errors (ROC AUC near 0.55) in domains requiring multi-step reasoning (e.g., math). 2. MASJ Failure: Judge model produces near-random confidence scores (ROC AUC near 0.5), providing no error prediction signal. 3. Overconfidence: All models, especially larger ones, show gap between high confidence (low entropy) and actual accuracy.

**First 3 experiments**: 1. Domain Entropy Baseline: Run model on MMLU-Pro subset (biology vs. math), calculate token-wise entropy, plot ROC-AUC for error prediction in each domain to confirm knowledge-vs-reasoning split. 2. MASJ vs. Random Predictor: Implement MASJ prompt using large model, compare ROC-AUC for error prediction against dummy random classifier on held-out set. 3. Scale and Calibration: Run two models of different sizes (Qwen 1.5B and 7B) on single topic, plot calibration curves (accuracy vs. inverted normalized entropy) to observe if larger model shows improved separation between correct and incorrect entropy distributions.

## Open Questions the Paper Calls Out

**Open Question 1**: Does enforcing explicit reasoning steps (e.g., Chain-of-Thought) before answer generation improve the correlation between token-wise entropy and model correctness in reasoning-intensive domains? The authors conclude that "Further progress here would be possible with more reasoning steps during an uncertainty estimate with an entropy-based approach on top of it." This remains unresolved because the current study shows entropy fails for reasoning-heavy tasks but didn't test if generating reasoning steps alters the entropy distribution of the final answer token.

**Open Question 2**: What architectural or prompting refinements are required to make Model-as-Judge (MASJ) a reliable predictor of error, given its current failure to outperform random guessing? The abstract states "MASJ requires refinement" after noting it "performs similarly to a random error predictor" with ROC AUC near 0.49. This is unresolved because the current implementation relies on a specific prompt that failed to capture uncertainty signals, and it's unclear if the method or prompt is at fault.

**Open Question 3**: How can data-uncertainty (entropy) and model-uncertainty (MASJ) be mathematically integrated into a unified framework to improve error detection across both knowledge and reasoning domains? The study concludes that "data-uncertainty related entropy should be integrated within uncertainty estimates frameworks" to address limitations of focusing on single uncertainty types. This remains unresolved because the paper examines these metrics in isolation and doesn't propose or test methods for combining them.

## Limitations

- Limited domain coverage with significant internal biases in reasoning requirements by topic, potentially affecting generalizability
- Unobserved methodology details including missing target model generation prompts and logit extraction specifications
- Static model evaluation without exploring training methods, prompting strategies, or post-hoc calibration that might improve performance

## Confidence

**High Confidence**: The core finding that token-wise entropy effectively predicts errors in knowledge domains (ROC AUC ~0.73 for biology) but fails in reasoning domains (ROC AUC ~0.55 for math) is well-supported by data and represents the paper's strongest contribution.

**Medium Confidence**: The claim that MASJ performs near-randomly for error prediction (ROC AUC ~0.49) is well-supported but may be specific to MMLU-Pro multiple-choice format. The mechanism explaining this failure is plausible but not definitively proven.

**Low Confidence**: The assertion that larger models demonstrate consistently better entropy-based error prediction is supported by observed entropy distribution separation, but the relationship between scale and calibration quality remains unclear.

## Next Checks

1. **Dataset Balance Verification**: Manually examine 50 randomly selected questions from both high-performing knowledge domains (biology) and low-performing reasoning domains (math) to verify the claimed distribution of reasoning requirements, confirming whether the observed performance gap reflects genuine uncertainty estimation differences versus dataset artifacts.

2. **Prompt Sensitivity Analysis**: Test whether entropy-based error prediction performance varies significantly with different answer generation prompts (e.g., temperature settings, few-shot examples) to determine if strong knowledge-domain performance is robust to generation strategy changes.

3. **MASJ Refinement Experiment**: Implement alternative MASJ evaluation strategies, such as providing the judge with full context of the question and answer options, or using chain-of-thought reasoning before evaluation, to test whether near-random performance is fundamental to MASJ or can be improved with richer evaluation contexts.