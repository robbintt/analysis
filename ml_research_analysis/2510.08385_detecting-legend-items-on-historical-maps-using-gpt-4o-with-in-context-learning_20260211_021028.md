---
ver: rpa2
title: Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning
arxiv_id: '2510.08385'
source_url: https://arxiv.org/abs/2510.08385
tags:
- legend
- maps
- historical
- gpt-4o
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting structured legend
  information from historical geological maps. The authors propose a method that first
  segments the map to isolate the legend area using LayoutLMv3, then applies GPT-4o
  in an in-context learning setting to detect and link legend symbols with their descriptions
  via bounding box predictions.
---

# Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning

## Quick Facts
- **arXiv ID**: 2510.08385
- **Source URL**: https://arxiv.org/abs/2510.08385
- **Reference count**: 20
- **Key outcome**: 88% F1 and 85% IoU for legend item detection using GPT-4o with 15 in-context examples

## Executive Summary
This paper addresses the challenge of extracting structured legend information from historical geological maps. The authors propose a two-stage method that first segments the map to isolate the legend area using LayoutLMv3, then applies GPT-4o in an in-context learning setting to detect and link legend symbols with their descriptions via bounding box predictions. Experiments on 40 USGS maps show that GPT-4o with structured JSON prompts achieves 88% F1 and 85% IoU for legend item detection, outperforming baseline approaches. Results also indicate that using 15 in-context examples yields optimal performance, while overly long prompts degrade accuracy. The method enables scalable, layout-aware legend parsing and supports improved indexing and searchability of historical map collections.

## Method Summary
The method uses a two-stage pipeline: first, LayoutLMv3 (LARA) segments each historical map to isolate the legend area from map content. Then, GPT-4o performs in-context learning using structured JSON prompts containing task definitions, annotated example pairs, and the target legend image to predict bounding boxes for symbol-description pairs. The approach leverages multimodal capabilities of GPT-4o with 15 in-context examples optimized for performance, achieving high F1 and IoU scores on USGS geological maps.

## Key Results
- GPT-4o with structured JSON prompts achieves 88% F1 and 85% IoU for legend item detection
- Optimal performance achieved with 15 in-context examples; performance degrades with 20 examples
- Two-stage decomposition (segmentation + detection) improves focus and accuracy compared to full-map processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage decomposition (segmentation → detection) reduces task complexity and enables GPT-4o to focus on structured reasoning within bounded legend regions.
- Mechanism: LayoutLMv3 first classifies map regions to isolate the legend area from map content. This cropped legend is then passed to GPT-4o, which predicts bounding boxes for symbol–description pairs without needing to filter irrelevant map regions.
- Core assumption: Legend areas follow sufficiently consistent spatial patterns that a layout model trained on document structures can generalize to map layouts.
- Evidence anchors: [abstract] "method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning"; [Section 3.1] "The first step is to segment each map into content and legend regions. We adopt a layout-aware vision model, specifically a fine-tuned LayoutLMv3, LARA"; [corpus] DIGMAPPER paper confirms this modular pipeline is part of a larger system for geologic map digitization
- Break condition: If legend regions are irregular or ambiguous (e.g., interspersed with map content), segmentation errors will propagate, causing GPT-4o to receive incomplete or incorrect crops.

### Mechanism 2
- Claim: Structured JSON prompts with 15 in-context examples optimize the balance between providing sufficient task guidance and avoiding prompt degradation.
- Mechanism: The prompt includes task definition, example image, and 15 annotated symbol–description pairs in JSON format. This conditions GPT-4o to predict bounding boxes in a consistent schema. Performance peaks at 15 examples; 20 examples introduces noise.
- Core assumption: GPT-4o's multimodal in-context learning can generalize spatial reasoning patterns from example annotations to unseen legend layouts.
- Evidence anchors: [abstract] "using 15 in-context examples yields optimal performance, while overly long prompts degrade accuracy"; [Section 4, Table 1] F1 improves from 0.82 (5 examples) to 0.88 (15 examples), then drops to 0.86 (20 examples); [corpus] No direct corpus evidence on optimal example counts for multimodal ICL; this is task-specific
- Break condition: If legend layouts diverge significantly from example styles (e.g., multi-column dense layouts), in-context examples may not transfer, leading to merged or missed bounding boxes.

### Mechanism 3
- Claim: Bounding box predictions with JSON output enable direct integration with downstream geospatial search pipelines.
- Mechanism: GPT-4o outputs coordinates for legend_item and description pairs in machine-readable JSON. These structured outputs can be indexed as metadata, supporting queries by rock type, symbol, or color pattern.
- Core assumption: Coordinate predictions are sufficiently accurate (IoU ≥ 0.5) for downstream use without extensive post-processing.
- Evidence anchors: [abstract] "GPT-4o with structured JSON prompts achieves 88% F1 and 85% IoU"; [Section 3.2] JSON prompt structure with "legend_item" and "description" bounding box arrays; [Section 3.2] "Each detected legend item with its description act as 'metadata' that can be used for retrieval and search"; [corpus] No corpus evidence on downstream integration outcomes; this is claimed utility
- Break condition: If bounding boxes are oversized or imprecise (Figure 3 failure case), downstream indexing may associate incorrect text with symbols, degrading search quality.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: Understanding how LLMs adapt to new tasks from examples without weight updates is critical for designing effective prompts and diagnosing failure modes.
  - Quick check question: Can you explain why performance degrades when prompts become too long, even with more examples?

- Concept: **Object Detection Metrics (IoU, F1)**
  - Why needed here: Interpreting IoU and F1 scores is necessary to evaluate bounding box quality and understand what "88% F1" means in practical terms.
  - Quick check question: Given a predicted box with 0.45 IoU against ground truth, would it count as correct under the paper's 0.5 threshold?

- Concept: **Layout Analysis / Document Understanding**
  - Why needed here: LayoutLMv3 is a document AI model; understanding its multimodal text+image+layout architecture helps explain why it's suited for legend segmentation.
  - Quick check question: What types of spatial features would a layout-aware model leverage to distinguish a legend block from map content?

## Architecture Onboarding

- Component map: Scanned historical map (.tiff) -> LARA (LayoutLMv3 fine-tune) -> legend area crop -> GPT-4o ICL module -> JSON bounding boxes

- Critical path:
  1. Verify LARA segmentation quality before passing crops to GPT-4o (garbage in = garbage out)
  2. Construct JSON prompt with exactly 15 high-quality, layout-diverse examples
  3. Validate GPT-4o output schema matches expected JSON structure before indexing

- Design tradeoffs:
  - **More examples vs. prompt length**: 15 examples optimal; 20 introduces noise. Test on your data.
  - **Segmentation accuracy vs. pipeline complexity**: Could GPT-4o handle full-map input? Likely no—paper shows decomposition improves focus.
  - **Determinism vs. exploration**: Paper uses temperature=0; adjust only if exploring output diversity.

- Failure signatures:
  - **Oversized bounding boxes**: Dense multi-column legends cause merged predictions (Figure 3)
  - **Missed pairs**: Irregular symbol layouts or tight spacing reduce detection
  - **Schema errors**: Malformed JSON from GPT-4o (rare with temperature=0 but possible)

- First 3 experiments:
  1. **Baseline replication**: Run LARA + GPT-4o (15 examples) on 5 held-out USGS maps; compute IoU/F1 to verify paper claims.
  2. **Example sensitivity**: Test 5, 10, 15, 20 examples on a diverse legend subset; confirm 15 is optimal for your map styles.
  3. **Layout stress test**: Evaluate on multi-column or irregular legends; quantify failure rate and identify post-processing needs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed method be improved to accurately distinguish neighboring entries in densely packed or multi-column legend layouts without generating oversized or merging bounding boxes?
- Basis in paper: [explicit] The authors note in the Conclusion and Discussion that "tightly packed or multi-column legends remain challenging, as the model struggles to distinguish between neighboring entries," leading to the merging of descriptions.
- Why unresolved: The current in-context learning setup with GPT-4o struggles to resolve fine-grained spatial ambiguities in complex layouts, resulting in the failure cases illustrated in Figure 3.
- What evidence would resolve it: Successful detection results (high F1/IoU) on a specifically curated dataset of dense, multi-column legends, or the introduction of a mechanism (e.g., grid-based prompts) that prevents bounding box merging.

### Open Question 2
- Question: Does the GPT-4o based in-context learning approach generalize effectively to historical map archives outside the USGS dataset, specifically those with non-English text or radically different cartographic symbology?
- Basis in paper: [inferred] The evaluation is restricted to 40 USGS maps ("dataset covers a range of U.S. Geological Survey (USGS) map styles"), leaving the method's robustness to diverse global archives unproven.
- Why unresolved: While the method is intended to support "various visual styles," the specific training and testing data represent a single publisher's style guidelines, limiting claims of universal scalability.
- What evidence would resolve it: Reporting F1 and IoU scores on non-USGS maps or maps in different languages to demonstrate cross-domain generalization without retraining.

### Open Question 3
- Question: Can a dynamic, retrieval-based selection of in-context examples overcome the performance degradation observed with fixed long prompts to maintain high accuracy for outlier map layouts?
- Basis in paper: [inferred] The paper notes that performance declines with 20 examples because "overly long prompts introduce noise," yet suggests that "prompt design and example selection are crucial factors."
- Why unresolved: It is unclear if the drop in accuracy at 20 examples is inherent to the model's context window or simply a result of using random/fixed examples that add noise rather than signal.
- What evidence would resolve it: An ablation study comparing the fixed-example approach against a semantic retrieval system that selects the most visually similar legend examples for the prompt.

## Limitations
- Performance degrades on densely packed or multi-column legend layouts where neighboring entries are difficult to distinguish
- Method has only been validated on USGS historical maps and may not generalize to non-English text or radically different cartographic styles
- Optimal in-context example selection strategy is unspecified, requiring trial-and-error for replication

## Confidence
- **High confidence**: Two-stage pipeline architecture (segmentation + detection) and its modular benefits; reported F1/IoU metrics on USGS test set
- **Medium confidence**: Optimal example count (15) due to lack of specification on selection methodology; bounding box accuracy for downstream search applications
- **Low confidence**: Generalizability to historical maps outside USGS collection; impact of irregular legend layouts on overall system performance

## Next Checks
1. **Example Sensitivity Validation**: Test performance across 5, 10, 15, and 20 in-context examples on a held-out subset to confirm 15 is locally optimal and understand sensitivity to example selection
2. **Layout Diversity Stress Test**: Evaluate system on historical maps with multi-column, irregular, or densely packed legends to quantify failure modes and identify post-processing needs
3. **Downstream Integration Pilot**: Implement a small-scale search index using predicted legend coordinates and measure retrieval accuracy for symbol-based queries versus ground-truth annotations