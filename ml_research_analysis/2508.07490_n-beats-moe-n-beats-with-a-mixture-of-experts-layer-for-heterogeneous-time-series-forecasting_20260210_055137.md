---
ver: rpa2
title: 'N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time
  Series Forecasting'
arxiv_id: '2508.07490'
source_url: https://arxiv.org/abs/2508.07490
tags:
- series
- time
- forecasting
- n-beats
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: N-BEATS-MOE extends the N-BEATS architecture with a Mixture-of-Experts
  (MoE) layer, replacing standard sum aggregation with a learned weighted sum determined
  by a gating network. This dynamic weighting enables the model to adaptively focus
  on different time series components based on input characteristics, improving handling
  of heterogeneous time series datasets.
---

# N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting

## Quick Facts
- arXiv ID: 2508.07490
- Source URL: https://arxiv.org/abs/2508.07490
- Reference count: 38
- Primary result: N-BEATS-MOE achieves consistent SMAPE improvements over N-BEATS on heterogeneous time series datasets (M1, M3, M4) while maintaining interpretability.

## Executive Summary
N-BEATS-MOE extends the N-BEATS architecture with a Mixture-of-Experts (MoE) layer that replaces fixed summation of block outputs with learned weighted combinations determined by a gating network. This dynamic weighting enables the model to adaptively focus on different time series components (trend, seasonality, identity) based on input characteristics, improving handling of heterogeneous time series datasets. Experiments across 12 benchmark datasets with 100,141 time series show consistent improvements over N-BEATS, particularly in datasets with varied domains, while maintaining interpretability through the gating mechanism.

## Method Summary
N-BEATS-MOE replaces the standard summation of block outputs in N-BEATS with a Mixture-of-Experts layer. The gating network (a linear layer followed by softmax) processes the normalized input signal and produces weights for each block output. The final forecast becomes a weighted sum of individual block forecasts rather than a simple sum. The approach was evaluated on 12 benchmark datasets from M1, Tourism, M3, and M4 competitions using SMAPE as the primary metric, with Bayesian optimization for hyperparameter tuning and median results over 10 trials.

## Key Results
- N-BEATS-MOE achieves consistent SMAPE improvements over N-BEATS across heterogeneous datasets (M1, M3, M4)
- For monthly series in M1, N-BEATS-MOE reduced SMAPE from 7.34% to 2.68% on one series, demonstrating superior decomposition accuracy
- Expert selection analysis shows the gating mechanism provides interpretable insights, with trend expert selected for 55% of trend-dominant components in M3
- Performance degrades on homogeneous datasets like Tourism, where N-BEATS-MOE underperforms standard N-BEATS

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Block Weighting via Gating Network
- Replacing fixed summation with learned weighted combination enables adaptive handling of heterogeneous time series patterns
- Gating network (LINEAR + softmax) processes normalized input and produces weights for each block output
- Core assumption: Heterogeneous datasets contain time series with varying relative importance of components
- Evidence: Abstract states dynamic weighting allows better adaptation to characteristics; Tourism results show degradation on homogeneous data

### Mechanism 2: Expert Specialization Through Stack-Level Decomposition
- Treating N-BEATS stacks (identity, trend, seasonality) as experts allows pattern-specific specialization
- Each stack produces interpretable components via basis functions; gating network learns which expert-stack to prioritize
- Core assumption: Pre-defined basis functions capture meaningful patterns
- Evidence: M3 results show 55% trend expert selection for trend components; M1 shows misalignment but better accuracy

### Mechanism 3: Interpretability via Gating Weight Inspection
- Gate weights provide post-hoc interpretability by revealing which component the model deems most relevant
- High weight indicates which pattern type (trend/seasonal/identity) drives the prediction
- Core assumption: Higher gate weight correlates with greater qualitative contribution to forecast accuracy
- Evidence: Analysis shows high gate values reflect significant qualitative contribution, not necessarily output amplitude

## Foundational Learning

- **N-BEATS Architecture (Stacks, Blocks, Basis Functions)**: Understanding the original architecture (trend stack with polynomial basis, seasonal stack with Fourier basis, residual flow) is prerequisite for understanding MoE modifications
  - Quick check: Can you explain how backcast residual flows between blocks and why this enables progressive signal decomposition?

- **Mixture-of-Experts Routing (Gating Networks, Sparse vs Dense)**: Understanding softmax routing, load balancing issues, and sparse vs dense MoE informs why dense routing with LayerNorm was chosen
  - Quick check: What is routing collapse in MoE, and why might LayerNorm on the input prevent mode collapse in this architecture?

- **Time Series Heterogeneity (Cross-Domain vs Single-Domain Datasets)**: Performance gains are conditional on dataset heterogeneity; understanding this distinction is crucial for predicting when N-BEATS-MOE will help
  - Quick check: Given a new dataset, what features would you examine to predict whether N-BEATS-MOE would outperform standard N-BEATS?

## Architecture Onboarding

- **Component map**: Input x₀ → LayerNorm → Gating Network (LINEAR + softmax) → produces weights [Ĝ₁, Ĝ₂, Ĝ₃] → Identity Stack → forecast ŷ₁ → Trend Stack → forecast ŷ₂ → Seasonal Stack → forecast ŷ₃ → Final output: ŷ = Ĝ₁·ŷ₁ + Ĝ₂·ŷ₂ + Ĝ₃·ŷ₃
- **Critical path**: Gating network accuracy determines whether the model routes to appropriate experts; if gating fails, performance regresses to or below baseline N-BEATS
- **Design tradeoffs**: Dense MoE (all experts active) vs Sparse MoE chosen to avoid routing collapse; block-level MoE variants tested but stack-level aggregation modification performed best
- **Failure signatures**: Uniform gate weights indicate gating network not learning; single expert always dominant suggests routing collapse; worse performance on homogeneous datasets is expected behavior
- **First 3 experiments**:
  1. Replicate M3 monthly comparison (N-BEATS vs N-BEATS-MOE) using provided hyperparameter ranges; verify SMAPE improvement
  2. Ablate the gating network by fixing equal weights to confirm performance drops to baseline, validating the gating mechanism's contribution
  3. Test on a held-out homogeneous dataset to verify performance degradation relative to N-BEATS

## Open Questions the Paper Calls Out

### Open Question 1
Why does N-BEATS-MOE underperform on single-domain (homogeneous) datasets like Tourism, and can this limitation be predicted or mitigated? The paper states that on Tourism datasets "composed of time series from a single domain, the proposed approach loses in all three frequency variants" but does not investigate why homogeneous datasets degrade performance or what characteristics predict this failure.

### Open Question 2
What determines whether the gating mechanism's expert assignments align with structural decompositions (e.g., STL trend/seasonal components)? The authors note "alignment with the STL decomposition, the results where inconclusive" but the M1 dataset showed divergent expert assignments yet superior performance, suggesting the gating captures something beyond classical decomposition.

### Open Question 3
How should gate softmax values be interpreted when high weights do not correspond to high output amplitude? The decomposition analysis shows "a high gate softmax value does not necessarily correspond to a high amplitude in the stack output, but rather reflects a significant qualitative contribution," which complicates interpretability claims.

### Open Question 4
Would sparse (top-k) expert selection improve computational efficiency without sacrificing accuracy compared to the current dense weighting approach? The paper discusses sparse MoE approaches and routing collapse but implements dense weighting, leaving the sparse vs. dense trade-off unexplored.

## Limitations
- Performance degrades on homogeneous datasets like Tourism, where N-BEATS-MOE underperforms standard N-BEATS
- Interpretability claims show imperfect alignment between gate weights and STL decomposition components, suggesting learned routing differs from classical decomposition
- The relationship between gate weights, output magnitude, and forecast quality remains unclear, complicating interpretability

## Confidence
- **High Confidence**: Core mechanism of dynamic block weighting via gating networks and experimental validation showing consistent SMAPE improvements on heterogeneous datasets
- **Medium Confidence**: Interpretability claims through gating weight inspection, given observed misalignment with STL decomposition yet maintained accuracy improvements
- **Medium Confidence**: Expert specialization claims, supported by M3 results but with notable misalignment in M1

## Next Checks
1. Apply N-BEATS-MOE to a new dataset with clear domain heterogeneity (e.g., electricity load + traffic data) and verify performance gains over N-BEATS
2. Log gate weight distributions during training across all experts to confirm non-degenerate usage and absence of routing collapse
3. Compare gate-weighted expert outputs with traditional decomposition methods (STL) on a subset of series to quantify alignment quality and identify cases where learned routing differs from classical decomposition