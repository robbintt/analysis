---
ver: rpa2
title: Accelerating Scientific Discovery with Autonomous Goal-evolving Agents
arxiv_id: '2512.21782'
source_url: https://arxiv.org/abs/2512.21782
tags:
- objectives
- page
- design
- objective
- saga
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Scientific discovery is bottlenecked by the challenge of designing
  effective objectives, which are often imperfect proxies for complex goals. SAGA
  (Scientific Autonomous Goal-evolving Agent) addresses this by using a bi-level architecture:
  an outer loop of LLM agents iteratively analyzes optimization outcomes, proposes
  new objectives, and converts them into executable scoring functions; an inner loop
  performs solution optimization under current objectives.'
---

# Accelerating Scientific Discovery with Autonomous Goal-evolving Agents

## Quick Facts
- arXiv ID: 2512.21782
- Source URL: https://arxiv.org/abs/2512.21782
- Reference count: 40
- Primary result: SAGA uses bi-level architecture to iteratively evolve objectives, substantially improving discovery effectiveness across four scientific domains.

## Executive Summary
Scientific discovery is bottlenecked by the challenge of designing effective objectives, which are often imperfect proxies for complex goals. SAGA (Scientific Autonomous Goal-evolving Agent) addresses this by using a bi-level architecture: an outer loop of LLM agents iteratively analyzes optimization outcomes, proposes new objectives, and converts them into executable scoring functions; an inner loop performs solution optimization under current objectives. This enables systematic exploration of the objective space and its trade-offs, rather than treating objectives as fixed inputs. SAGA supports three levels of automation—co-pilot, semi-pilot, and autopilot—allowing scientists to collaborate or delegate. Tested across antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, SAGA substantially improves discovery effectiveness by autonomously evolving objectives to avoid reward hacking and align with practical constraints.

## Method Summary
SAGA employs a bi-level architecture where an outer loop of LLM agents (Planner, Analyzer, Implementer) iteratively analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop (Optimizer) performs solution optimization under current objectives. The framework supports three autonomy levels: co-pilot (human reviews all agent outputs), semi-pilot (human reviews only Analyzer reports), and autopilot (fully autonomous). Tested across four domains—antibiotics, materials, DNA, and chemical processes—SAGA demonstrates improved discovery effectiveness by systematically exploring the objective space rather than relying on fixed proxy objectives.

## Key Results
- SAGA substantially improves discovery effectiveness by iteratively evolving objectives rather than using fixed proxies
- The bi-level architecture enables systematic exploration of objective space and trade-offs
- SAGA successfully addresses reward hacking by autonomously refining objectives based on optimization outcomes
- Three autonomy levels (co-pilot, semi-pilot, autopilot) provide flexible human-in-the-loop integration

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Objective Evolution via Bi-level Architecture
- Claim: Iteratively refining objectives based on optimization outcomes reduces reward hacking and improves solution quality compared to fixed-objective approaches.
- Mechanism: The outer loop (Planner, Analyzer, Implementer) proposes, analyzes, and codifies new objectives based on observed failures in the inner loop. The inner loop (Optimizer) searches for solutions under the current objectives. This creates a feedback loop where objective gaps discovered during optimization directly inform objective refinement.
- Core assumption: Objectives that align with the high-level scientific goal are not known a priori; the mapping from goal to objectives is itself a search problem that benefits from iterative refinement.
- Evidence anchors:
  - [abstract] "SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives."
  - [PAGE 3] "SAGA is designed to navigate the combinatorial search space of objectives by integrating high-level objective planning in the outer loop with low-level optimization in the inner loop."
  - [corpus] LLEMA (arXiv:2510.22503) couples LLMs with evolutionary search for materials design, supporting LLM-guided optimization, though it does not explicitly address bi-level objective evolution.

### Mechanism 2: Modular Agent Roles with Specialized Capabilities
- Claim: Decomposing the objective evolution process into specialized agent roles (Planner, Implementer, Optimizer, Analyzer) enables systematic exploration of the objective space and produces executable scoring functions.
- Mechanism: Each module handles a distinct cognitive task—planning objectives (Planner), implementing scoring code (Implementer), searching solutions (Optimizer), and analyzing results (Analyzer). Standardized interfaces between modules (Candidates, Populations, Objectives, ScoringFunctions) allow flexible substitution and extension. The Implementer's use of web search and Docker-isolated execution enables dynamic creation of domain-specific evaluators.
- Core assumption: The task can be decomposed into these four roles; LLMs possess sufficient domain knowledge to implement valid scoring functions; the optimization algorithm can make progress under the current objective set.
- Evidence anchors:
  - [PAGE 15] "The framework comprises four core modules... Planner formulates measurable objectives... Implementer instantiates callable scoring functions... Optimizer executes the inner optimization loop... Analyzer evaluates optimization outcomes..."
  - [PAGE 28-29] Detailed module descriptions in Supplementary Section S1.1.2.
  - [corpus] Corpus evidence on this specific four-module decomposition is weak; related work (e.g., Agentic Discovery, arXiv:2510.13081) discusses cooperative agents for scientific discovery but does not prescribe this exact architecture.

### Mechanism 3: Flexible Human-in-the-Loop Integration via Autonomy Levels
- Claim: Providing co-pilot, semi-pilot, and autopilot modes allows scientists to inject domain expertise at critical decision points while automating implementation and optimization details, improving alignment with scientific intuition.
- Mechanism: In co-pilot mode, humans review and revise both Planner and Analyzer outputs. In semi-pilot mode, humans review only Analyzer reports. In autopilot mode, all modules run autonomously. This tiered design allows human input where it is most valuable (objective formulation and failure diagnosis) while automating execution.
- Core assumption: Human scientists possess valuable domain knowledge that can correct or refine agent proposals; the mode can be dynamically adjusted based on trust and task maturity.
- Evidence anchors:
  - [PAGE 3] "SAGA is a flexible framework supporting different levels of human involvement. It offers three modes: (1) co-pilot mode... (2) semi-pilot mode... and (3) autopilot mode..."
  - [PAGE 8] "The co-pilot and semi-pilot modes incorporate human input into the agent's decision-making process to review and refine candidate analyses and proposed objectives..."
  - [corpus] K-Dense Analyst (arXiv:2508.07043) emphasizes automated scientific analysis but does not formalize autonomy tiers for human-in-the-loop integration.

## Foundational Learning

- Concept: Bi-level Optimization / Nested Loops
  - Why needed here: Understanding that SAGA separates the *search for objectives* (outer loop) from the *search for solutions given objectives* (inner loop) is essential. This is not standard single-loop optimization.
  - Quick check question: Can you distinguish between what the outer loop and inner loop optimize for in SAGA?

- Concept: Reward Hacking / Objective Misalignment
  - Why needed here: The core problem SAGA addresses is that fixed proxy objectives are imperfect and can be exploited, leading to solutions that score well but are scientifically undesirable.
  - Quick check question: What is "reward hacking" in the context of scientific discovery agents?

- Concept: LLM as Code Generator and Domain Reasoner
  - Why needed here: The Implementer uses LLMs to write executable scoring functions, and the Planner/Analyzer use LLMs for domain reasoning. Understanding the strengths and failure modes of LLMs in both roles is critical.
  - Quick check question: What are two potential failure modes of the Implementer module?

## Architecture Onboarding

- Component map: Goal/Context Input → Planner → Implementer → Optimizer → Analyzer → (Loop back to Planner or Terminate) → Selector → Final Candidates
- Critical path: The Implementer's success in producing correct scorers is a critical gating factor.
- Design tradeoffs:
  - **Autonomy vs. Control**: Co-pilot offers maximal control but higher human cost; autopilot is fully automated but risks misalignment.
  - **Generality vs. Domain Specificity**: The framework is general, but performance hinges on the LLM's domain knowledge and the quality of available tools/web resources for the Implementer.
  - **Exploration vs. Exploitation**: The outer loop explores the space of objectives; the inner loop exploits current objectives. Balancing these is key.
- Failure signatures:
  - **Stagnant Outer Loop**: Analysis reports are superficial or repetitive; Planner proposes the same or ineffective objectives across iterations. May indicate LLM limitation or insufficient diversity in candidate analysis.
  - **Implementer Failures**: Scoring functions throw runtime errors, produce constant values, or correlate poorly with intended objectives. Check Docker logs and scorer validation.
  - **Inner Loop Divergence**: Optimizer fails to improve scores or the population collapses to low diversity. May indicate conflicting objectives or poor hyperparameters.
  - **Reward Hacking Persisting**: Candidates achieve high scores on current objectives but fail held-out evaluation metrics. May indicate objectives are still incomplete or exploitable.
- First 3 experiments:
  1. **Ablation on Outer Loop**: Run SAGA in "AlphaEvolve mode" (outer loop disabled, fixed initial objectives) vs. full SAGA on a held-out task (e.g., a new antibiotic design target). Compare performance on held-out metrics to quantify the contribution of dynamic objective evolution.
  2. **Implementer Validation**: For a set of known objectives (e.g., QED, SA score), compare the scores produced by human-implemented functions vs. those generated by the Implementer on a random molecular population. Measure correlation (Pearson/Spearman) and error (MSE).
  3. **Autonomy Level Sweep**: Run SAGA in co-pilot, semi-pilot, and autopilot modes on the same task with a fixed seed. Compare the convergence trajectory, final candidate quality, and total human intervention time/cost to understand the tradeoffs between modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SAGA be extended to handle discovery tasks where objectives cannot be computationally validated?
- Basis in paper: [explicit] The authors state: "One limitation of SAGA is the reliance on computationally verifiable objectives. For science problems where results cannot be validated computationally at all, SAGA needs to be extended in two ways: (1) feedback or scoring provided by human or (2) autonomous lab-in-the-loop."
- Why unresolved: The current framework requires computable scoring functions, excluding domains where validation requires physical experimentation or subjective human judgment.
- What evidence would resolve it: Demonstration of SAGA integrated with autonomous laboratory systems or human-in-the-loop scoring for tasks like drug efficacy testing in living organisms.

### Open Question 2
- Question: How can SAGA autonomously expand its design space beyond predefined domains?
- Basis in paper: [explicit] The authors note: "To expand SAGA to handle more flexible tasks, SAGA needs to be extended to adjust the design space with high-level goal only being finding a drug for curing a certain disease, and automatically formulate the design space, such as small molecule binders, antibiotics, nanobodies or RNA sequences."
- Why unresolved: Current implementations require users to specify the design space (e.g., small molecules vs. proteins), limiting truly open-ended discovery.
- What evidence would resolve it: SAGA successfully selecting among fundamentally different modalities (small molecules, antibodies, RNA) based solely on therapeutic goals.

### Open Question 3
- Question: What causes convergence across SAGA's autonomy levels, and does this indicate optimization saturation or objective space limitations?
- Basis in paper: [inferred] The authors observe that "the performance of the three different modes was relatively similar" in antibiotic design, with objectives "convergent to a specific list of types" regardless of human intervention level.
- Why unresolved: It is unclear whether this reflects efficient objective discovery, inherent constraints in the objective space, or limitations in the outer loop's exploration diversity.
- What evidence would resolve it: Systematic analysis of objective diversity across repeated runs with different random seeds and LLM backbones.

### Open Question 4
- Question: Can SAGA identify and recover from proposing counterproductive objectives that lead to reward hacking?
- Basis in paper: [inferred] The framework addresses reward hacking by evolving objectives, but the analyzer's ability to detect when newly proposed objectives themselves introduce exploitation opportunities remains unexplored.
- Why unresolved: The paper demonstrates SAGA avoiding fixed-objective reward hacking, but the meta-level problem of objective-space reward hacking is not analyzed.
- What evidence would resolve it: Adversarial experiments where initial objectives intentionally mislead the system, testing whether the analyzer correctly identifies and corrects problematic objective proposals.

## Limitations
- The effectiveness of the bi-level architecture critically depends on the Analyzer's ability to correctly diagnose objective failures, but quantitative evidence of this diagnosis accuracy is lacking
- The Implementer's ability to generate correct, efficient scoring functions across diverse scientific domains is assumed but not validated with error rate statistics
- Empirical comparisons demonstrating the relative performance or human effort tradeoffs between the three autonomy levels are absent

## Confidence
- High confidence in the conceptual framework and modular architecture design
- Medium confidence in the core mechanism of dynamic objective evolution
- Low confidence in the practical performance and reliability of the Implementer module

## Next Checks
1. **Ablation Study**: Run SAGA in fixed-objective mode (disabling the outer loop) versus full SAGA on a new antibiotic design target. Measure performance differences on held-out validation metrics to isolate the contribution of dynamic objective evolution.
2. **Implementer Validation**: For a set of 10 well-defined scientific objectives (e.g., QED, MW, SA), generate scoring functions using the Implementer and compare their outputs to human-implemented reference functions across 1000 random molecules. Calculate correlation coefficients and execution error rates.
3. **Autonomy Mode Comparison**: Execute SAGA in co-pilot, semi-pilot, and autopilot modes on the same task with fixed random seeds. Track candidate quality trajectories, total human intervention time, and final performance to quantify the tradeoffs between autonomy levels.