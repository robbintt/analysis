---
ver: rpa2
title: 'Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries'
arxiv_id: '2505.20451'
source_url: https://arxiv.org/abs/2505.20451
tags:
- human
- response
- amulet
- turns
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of accurately evaluating language\
  \ model responses in complex, multi-turn human-assistant conversations, where user\
  \ intents and requirements frequently change across turns. To tackle this, the authors\
  \ introduce AMULET, a framework that leverages two key linguistic concepts\u2014\
  dialog acts (communicative structures) and Grice\u2019s maxims (conversational principles)\u2014\
  to improve the accuracy of LLM-based judges."
---

# Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries

## Quick Facts
- arXiv ID: 2505.20451
- Source URL: https://arxiv.org/abs/2505.20451
- Reference count: 40
- Key outcome: AMULET achieves up to 86% accuracy on multi-turn LLM response preference prediction by leveraging dialog acts and Gricean maxims, outperforming SOTA reward models.

## Executive Summary
This paper tackles the challenge of accurately evaluating LLM responses in complex, multi-turn conversations where user intents frequently shift. The authors introduce AMULET, a framework that uses dialog acts (communicative structures) and Grice's maxims (conversational principles) to improve LLM-based judge accuracy. By detecting intent shifts and applying maxims like informativity and relevance, AMULET significantly improves preference prediction, achieving up to 86% accuracy on challenging datasets.

## Method Summary
AMULET leverages two linguistic concepts—dialog acts and Gricean maxims—to evaluate multi-turn conversations. It first detects dialog act shifts across turns (human changes ~73% of the time) to condition judgments on the most recent intent. When dialog acts are identical, it applies 12 Gricean maxims (Quantity, Quality, Relevance, Manner, Benevolence, Transparency) to differentiate responses. The framework is deployed as standalone judges and in cascaded jury systems, where DA votes are prioritized, followed by maxims, and then baselines or reward models. This sequential approach reduces tie-rates and improves accuracy over strong baselines.

## Key Results
- Humans change dialog acts ~73% of the time across conversation turns.
- ~75% of instances can be differentiated using dialog acts and/or maxims.
- AMULET achieves up to 86% accuracy on multi-turn preference prediction, outperforming SOTA reward models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Detecting dialog act shifts across turns enables more accurate preference predictions by conditioning judgments on the most recent communicative intent.
- **Mechanism:** AMULET-DA annotates each turn with ISO 24617-2-aligned communicative dimensions and functions, tracking changes to identify intent shifts and weight the final turn's function more heavily.
- **Core assumption:** Preference quality correlates with alignment to the final turn's dialog act; intent shifts are frequent and their detection improves judgment.
- **Evidence anchors:**
  - [abstract] Reports humans change dialog acts ~73% of the time across turns.
  - [section 3.1] Figure 3(c) quantifies DA shift at 79.5% (HH-Test), 78.9% (HH-Train), 76.1% (Nectar), 56.3% (WildFeedback) for consecutive human turns.
  - [corpus] MUSIC (arXiv:2512.24693) frames multi-turn evaluation as step-wise contrast, implicitly relying on intent differentiation.
- **Break condition:** When dialog acts remain stable across turns or when DA annotation consistency is low (~28% function variance across votes reported in Appendix I.2).

### Mechanism 2
- **Claim:** Gricean maxim profiles differentiate responses even when dialog acts are identical.
- **Mechanism:** AMULET-MAXIM evaluates each candidate response against 12 sub-maxims, producing a comparative profile that exposes quality differences when both responses share the same communicative function.
- **Core assumption:** Human preference annotations implicitly reflect maxim satisfaction; chosen responses satisfy more or higher-priority maxims than rejected ones.
- **Evidence anchors:**
  - [abstract] ~75% of instances can be differentiated via dialog acts and/or maxims.
  - [section 3.2] Table 1 shows within "Different DA" instances, chosen satisfies more maxims in 59.1% (HH-Test), 52.1% (Nectar); within "Same DA" instances, 51.3% (WildFeedback) favor the chosen response.
  - [corpus] Direct corpus evidence on maxim-based LLM judging is limited; prior work defines maxims but does not validate predictive power at scale.
- **Break condition:** When responses are near-equivalent or both violate similar maxims; when conversations are highly task-specific where maxims like Benevolence become irrelevant.

### Mechanism 3
- **Claim:** Cascading jury decisions with structured fallbacks aggregates complementary signals and reduces tie-rates.
- **Mechanism:** AMULET-LM-JURY implements a sequential voting pipeline: DA-first → MAXIM-second → W-EXPL or RM fallback. Each stage votes twice (swapped positions) to mitigate position bias. Agreement yields final decision; disagreement triggers next stage.
- **Core assumption:** DA, maxims, and reward models capture partially independent error modes; prioritizing higher-consistency signals (DA) before noisier ones improves overall accuracy.
- **Evidence anchors:**
  - [section 4.4] Table 4 shows DA-then-MAXIM reduces tie-rate from ~15% to ~9%; DA-then-MAXIM-then-W-EXPL to ~5%.
  - [section 4.4] Table 2 shows DA-then-MAXIM-then-INF-ORM achieves 86.9% (Nectar ≥7 turns), outperforming INF-ORM alone (84.3%).
  - [corpus] LLM Jury-on-Demand (arXiv:2512.01786) finds diverse jury panels improve robustness but uses parallel rather than cascaded voting.
- **Break condition:** Early-stage systematic errors propagate; cascade latency scales with fallback depth; RM API costs increase for large-scale evaluation.

## Foundational Learning

- **Concept: Dialog Acts (ISO 24617-2)**
  - **Why needed here:** Core to AMULET-DA; annotating turns with communicative dimensions/functions (Task, Allo-Feedback, Social Obligations Management) enables intent shift detection and context-aware preference judgment.
  - **Quick check question:** For "Okay, thanks! And what about X?", which two functions should be tagged?

- **Concept: Gricean Maxims + AI Extensions**
  - **Why needed here:** AMULET-MAXIM uses 12 sub-maxims (Quantity-1/2, Quality, Relevance-1/2, Manner-1/2, Benevolence-1/2, Transparency-1/2/3) to evaluate response quality along informativity, truth, relevance, clarity, moral responsibility, and knowledge boundary recognition.
  - **Quick check question:** Name two maxims that frequently distinguish chosen from rejected responses across all datasets in Figure 4.

- **Concept: Position and Verbosity Bias in LLM Judges**
  - **Why needed here:** The paper mitigates position bias via two-vote swapping and verbosity bias via explicit prompt instructions; understanding these biases is essential for interpreting baseline limitations.
  - **Quick check question:** Why does single-pass judging risk inflated accuracy due to position bias, and how does vote-swapping correct this?

## Architecture Onboarding

- **Component map:** Prompt Templates -> Judge Execution -> Voting & Aggregation -> Jury Cascade -> Evaluation Harness
- **Critical path:**
  1. Load dataset (≥4 human turns), filter identical chosen/rejected, format turns.
  2. Run AMULET-DA twice per instance (swap R1/R2), collect votes.
  3. If votes agree → finalize; else run AMULET-MAXIM twice.
  4. If still tie → invoke W-EXPL or RM; return final label.
  5. Compute accuracy, treating parse failures as losses.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Deeper cascade improves accuracy (+4% over baselines) but increases API calls 4–6×.
  - **Model Choice:** GPT-4o/Claude are expensive but stable; Qwen is cheaper but slower with higher failure rates on long contexts.
  - **Prompt Complexity:** Detailed DA/MAXIM definitions reduce hallucination (~0.1%) but increase token costs.
- **Failure signatures:**
  1. **Parse Errors:** Malformed JSON (safety triggers or truncation); retry up to 6 times.
  2. **High Tie Rate:** Indicates insufficient DA/MAXIM signal differentiation.
  3. **Hallucinated DAs:** Model invents new functions (monitor ~0.1% observed).
  4. **Residual Position Bias:** Accuracy varies with fixed order; verify swap implementation.
- **First 3 experiments:**
  1. Reproduce Table 2 (HH-Test ≥4 turns) with AMULET-DA, AMULET-MAXIM, DA-then-MAXIM, DA-then-MAXIM-then-W-EXPL using GPT-4o; confirm improvement (55.7% → 64.1%).
  2. Ablate cascade order: MAXIM-then-DA vs. DA-then-MAXIM on WildFeedback ≥4 turns; compare accuracy and tie-rate (Table 17 shows similar performance).
  3. Integrate RM fallback: Replace W-EXPL with QRM for Nectar ≥7 turns; target ≥85% accuracy while measuring latency and cost overhead.

## Open Questions the Paper Calls Out
None

## Limitations
- The 75% maxim differentiation rate conflates cases where either DA or maxim signal suffices, not requiring both to be necessary.
- Human annotation agreement for DA functions is ~72%, raising concerns about ground truth stability.
- The study does not report model latency or cost per judgment, which are critical for practical deployment at scale.

## Confidence

- **High Confidence:** Dialog act shift frequency (~73%) and the cascading jury accuracy improvements (up to 86%) are well-supported by experimental data across four datasets with clear methodology.
- **Medium Confidence:** The 75% maxim differentiation rate is supported but requires careful interpretation—it includes instances differentiated by either DA or maxim, not necessarily both.
- **Low Confidence:** The claim that both DA and maxim signals are necessary for optimal performance is not empirically validated; the paper shows they are complementary but does not test scenarios where one signal dominates.

## Next Checks

1. **Cross-dataset maxim calibration:** Test whether maxim effectiveness varies by domain (e.g., technical vs. casual conversations) by evaluating on held-out datasets not used in training or development.
2. **Ablation of fallback triggers:** Measure accuracy when bypassing DA and going directly to MAXIM, or when skipping MAXIM in the cascade, to quantify the marginal value of each stage.
3. **Position bias stress test:** Systematically vary response order beyond simple swapping (e.g., 4-run Latin square) to confirm the two-vote system adequately controls for position effects.