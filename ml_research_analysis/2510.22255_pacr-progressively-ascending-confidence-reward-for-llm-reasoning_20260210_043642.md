---
ver: rpa2
title: 'PACR: Progressively Ascending Confidence Reward for LLM Reasoning'
arxiv_id: '2510.22255'
source_url: https://arxiv.org/abs/2510.22255
tags:
- reasoning
- reward
- confidence
- steps
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sparse, outcome-based rewards
  in Reinforcement Learning with Verifiable Rewards (RLVR) for LLM reasoning, which
  provides no guidance for intermediate steps and slows exploration. The authors propose
  Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward
  signal that encourages ground-truth confidence growth along reasoning trajectories.
---

# PACR: Progressively Ascending Confidence Reward for LLM Reasoning

## Quick Facts
- arXiv ID: 2510.22255
- Source URL: https://arxiv.org/abs/2510.22255
- Reference count: 21
- One-line primary result: Dense-PACR improves 7B math reasoning accuracy by 3 percentage points over Dr.GRPO baseline.

## Executive Summary
This paper addresses the problem of sparse, outcome-based rewards in Reinforcement Learning with Verifiable Rewards (RLVR) for LLM reasoning, which provides no guidance for intermediate steps and slows exploration. The authors propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward signal that encourages ground-truth confidence growth along reasoning trajectories. PACR is based on the inductive bias that a correct reasoning path should progressively increase the model's probability of the ground-truth answer.

## Method Summary
PACR introduces two variants to augment Dr.GRPO: Sparse-PACR (trajectory-level reward based on consistency of confidence growth) and Dense-PACR (step-wise rewards based on magnitude of confidence change). The method tracks the model's confidence in the correct answer at each reasoning step and rewards positive confidence growth. Confidence is computed as the log-probability of ground-truth tokens given the current reasoning history. The theoretical analysis shows that an ideal oracle policy will, on average, increase confidence in the ground truth, validating PACR as a strong inductive bias.

## Key Results
- Dense-PACR achieves 52.6% average accuracy across math benchmarks compared to 49.6% for Dr.GRPO with Qwen2.5-Math-7B
- PACR consistently outperforms Dr.GRPO baseline on MATH500, Minerva-Math, OlympiadBench, AIME 2024, and AMC 2023
- Training dynamics show PACR accelerates exploration and reaches higher final performance faster

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ground-truth confidence growth acts as a valid proxy for reasoning quality, providing a dense learning signal where outcome-based rewards are sparse.
- **Mechanism:** The method calculates the stepwise change in log-probability of the correct answer ($C_k$). Theoretical analysis shows that for an "oracle" policy generating faithful reasoning, the expected confidence gain is non-negative (equivalent to a KL divergence). By rewarding positive $C_k$, the model is incentivized to produce steps consistent with the ground truth.
- **Core assumption:** A correct reasoning path generally increases the model's probability of the correct answer (psycholinguistic premise).
- **Evidence anchors:** [abstract] PACR encodes the inductive bias that the probability of the ground-truth answer should have a generally ascending trend. [Section 4.2] Proposition 1 proves $E[C_k] \geq 0$ under an oracle policy. [corpus] Related work (ICPO) also leverages intrinsic confidence, suggesting a convergence on model-intrinsic signals, though specific mechanisms differ.
- **Break condition:** If the model is poorly calibrated or the task requires "lateral thinking" where intermediate steps temporarily reduce the probability of the obvious answer, this inductive bias might prune valid paths.

### Mechanism 2
- **Claim:** Dense, step-wise rewards (Dense-PACR) improve credit assignment and accelerate exploration compared to sparse or trajectory-level rewards.
- **Mechanism:** Instead of a single reward at the end, Dense-PACR computes a discounted return ($G_{k,dense}$) for every step. This directly links a specific reasoning action to an immediate change in the likelihood of success, reducing the variance of policy updates.
- **Core assumption:** The magnitude of confidence change ($C_k$) correlates with the "pivotalness" of a reasoning step.
- **Evidence anchors:** [Section 7.1] Dense-PACR consistently achieves better performance (e.g., 52.6% vs 49.6% baseline) than Sparse-PACR. [Section 4.1, Observation 3] Large positive spikes in $C_k$ are empirically shown to align with pivotal problem-solving steps (e.g., applying a key theorem).
- **Break condition:** If the model learns to "game" the confidence signal—producing steps that artificially inflate confidence without logical validity—the dense signal could lead to reward hacking.

### Mechanism 3
- **Claim:** Min-Max normalization of advantages (purely positive incentives) sustains exploration better than mean-centered approaches.
- **Mechanism:** The authors use Min-Max scaling to map returns to $[0,1]$ rather than centering them (which creates negative advantages). This avoids penalizing steps that merely have "below average" confidence gains, allowing the model to maintain diverse reasoning paths without premature pruning.
- **Core assumption:** Intermediate steps should only be reinforced positively if helpful; sub-optimal steps should simply be ignored rather than actively suppressed.
- **Evidence anchors:** [Section 7.3] Min-Max normalization leads to higher final accuracy compared to a Leave-One-Out (LOO) baseline, which plateaus earlier due to aggressive pruning. [corpus] Corpus signals (ICPO, RLPR) suggest balancing positive/negative incentives is a key tuning parameter in RLVR; PACR explicitly argues for non-penalizing signals.
- **Break condition:** In scenarios with extremely noisy data where strict filtering is required, the lack of negative penalties might slow down the elimination of bad habits.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** This is the base training paradigm. You must understand that standard RLVR provides a binary signal (Correct/Incorrect) only at the end of a long chain of thought.
  - **Quick check question:** If a model solves a math problem in 10 steps but fails at step 9, what signal does standard RLVR provide for step 1? (Answer: Zero or negative, with no distinction from step 9).

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** PACR is integrated into the GRPO/Dr.GRPO framework. You need to know that advantages are calculated by comparing a trajectory's reward against the average of a group of samples.
  - **Quick check question:** How does the advantage calculation change if you use the Dr.GRPO variant compared to standard GRPO? (Answer: Dr.GRPO removes length normalization and standard deviation from the advantage calculation).

- **Concept: Log-Probability and Confidence Estimation**
  - **Why needed here:** PACR is entirely built on tracking $\log p(Y_{gt} | \dots)$. You need to know how to extract and sum log-probs over a sequence of tokens efficiently.
  - **Quick check question:** How do you calculate the probability of a multi-token answer "42" given a specific context prefix? (Answer: Sum the log-probs of token "4" and token "2" conditioned on the prefix).

## Architecture Onboarding

- **Component map:** Rollout Generator -> Confidence Monitor -> Reward Calculator -> Advantage Normalizer -> Policy Updater
- **Critical path:** The **Confidence Monitor** is the critical new infrastructure. It requires $T \times L$ forward passes (where $T$ is steps, $L$ is answer tokens) per trajectory if implemented naively. Efficient caching of KV-states for the prefix $H_{\le k}$ is essential to avoid O($T^2$) overhead.
- **Design tradeoffs:**
  - **Dense vs. Sparse:** Dense requires calculating confidence at every step (high compute). Sparse calculates it only at the end or at segment boundaries (low compute, but weaker signal).
  - **Min-Max vs. LOO:** Min-Max is purely incentivizing; LOO is comparative/penalizing. Choose Min-Max for open exploration, LOO for refining strong models.
- **Failure signatures:**
  - **Flat-lining Confidence:** If $C_k \approx 0$ everywhere, the model is not learning to connect reasoning to the answer. Check if the answer prefix $y^0_{gt}$ is correctly formatted.
  - **Oscillating Confidence:** If $C_k$ fluctuates wildly, the model might be uncertain. Check if the Min-Max normalization is stabilizing the signal or if $\gamma$ is too high.
- **First 3 experiments:**
  1. **Validation of Inductive Bias:** Before training, run a static analysis on a pre-trained model. Check if correct trajectories actually show ascending confidence compared to incorrect ones (replicate Observation 1).
  2. **Sparse-PACR Integration:** Implement the trajectory-level consistency reward (Sparse-PACR) first. It is easier to integrate (single scalar added to GRPO reward) and should show immediate speedup in convergence.
  3. **Ablation on Normalization:** Compare Min-Max vs. Leave-One-Out (LOO) advantage formulation on a small dataset (e.g., MATH Level 1) to verify the "non-penalizing" benefit claimed in Section 7.3.

## Open Questions the Paper Calls Out
- Does PACR generalize to multimodal reasoning tasks, such as visual math problems, using Vision Language Models (VLMs)? (Explicitly called out in Appendix A.1)
- Can PACR lead to reward hacking behaviors where models learn to artificially inflate confidence without genuine reasoning progress? (Inferred from discussion on intrinsic rewards)

## Limitations
- The inductive bias that confidence should increase monotonically may not hold for all reasoning tasks requiring "lateral thinking"
- Computational overhead of confidence computation at each step could be prohibitive for larger models without KV-caching optimizations
- Performance on non-mathematical reasoning domains remains untested

## Confidence
- **High:** Theoretical analysis showing E[C_k] ≥ 0 for oracle policy is mathematically sound and well-grounded.
- **Medium:** Claim that Min-Max normalization better sustains exploration than LOO is supported but only shown on one dataset.
- **Low:** Generality of ascending confidence inductive bias across diverse reasoning domains is asserted but not empirically validated.

## Next Checks
1. **Domain Generalization Test:** Evaluate PACR on non-mathematical reasoning tasks (e.g., code generation, commonsense QA) to verify whether the ascending confidence bias holds across domains.
2. **Policy Behavior Analysis:** For a trained Dense-PACR model, analyze the correlation between confidence spikes (C_k) and ground-truth intermediate reasoning steps.
3. **Compute Overhead Measurement:** Profile the wall-clock training time and memory usage of Dense-PACR vs. Dr.GRPO for the same number of training steps, isolating the cost of confidence computation.