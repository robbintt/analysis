---
ver: rpa2
title: 'EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting'
arxiv_id: '2505.12738'
source_url: https://arxiv.org/abs/2505.12738
tags:
- epidemic
- forecasting
- epillm
- learning
- spatio-temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EpiLLM, a novel framework that repurposes
  large language models (LLMs) for spatio-temporal epidemic forecasting. The key innovation
  is a dual-branch architecture that aligns infection cases and human mobility data
  with language tokens, enabling LLMs to model complex epidemic patterns.
---

# EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting

## Quick Facts
- arXiv ID: 2505.12738
- Source URL: https://arxiv.org/abs/2505.12738
- Reference count: 40
- Novel dual-branch architecture aligns infection cases and human mobility data with language tokens for epidemic forecasting

## Executive Summary
EpiLLM introduces a novel framework that repurposes large language models (LLMs) for spatio-temporal epidemic forecasting. The key innovation is a dual-branch architecture that aligns infection cases and human mobility data with language tokens, enabling LLMs to model complex epidemic patterns. The framework reformulates epidemic forecasting as next-token prediction through autoregressive modeling and introduces spatio-temporal prompt learning techniques to enhance LLM perception of epidemic dynamics.

Extensive experiments on real-world COVID-19 datasets from England, France, Italy, and Spain demonstrate EpiLLM's superior performance compared to state-of-the-art baselines. The framework achieves up to 30.38% improvement in RMSE over existing methods, particularly on the Spain dataset. EpiLLM also exhibits strong multi-step forecasting capabilities, with the GEMMA-based variant showing the best performance due to reduced error accumulation in long-sequence generation.

## Method Summary
EpiLLM employs a dual-branch architecture where infection cases and human mobility data are aligned with language tokens, enabling large language models to process epidemic data through their existing transformer architectures. The framework reformulates epidemic forecasting as a next-token prediction problem, leveraging autoregressive modeling for both direct and multi-step forecasting. Spatio-temporal prompt learning techniques are introduced to enhance the LLM's ability to perceive epidemic dynamics. The model is evaluated on COVID-19 datasets from four European countries, demonstrating significant improvements over existing forecasting methods.

## Key Results
- Achieves up to 30.38% improvement in RMSE over state-of-the-art baselines, particularly on Spain dataset
- Demonstrates strong multi-step forecasting capabilities with GEMMA-based variant showing best performance
- Exhibits characteristic LLM scaling behavior with larger models showing improved performance despite increased computational demands

## Why This Works (Mechanism)
EpiLLM works by leveraging the natural language processing capabilities of large language models and adapting them to spatio-temporal epidemic data. The dual-branch architecture allows the model to process both infection cases and mobility data simultaneously, capturing the complex interactions between human movement and disease spread. By reformulating epidemic forecasting as next-token prediction, the framework takes advantage of LLMs' autoregressive modeling strengths. The spatio-temporal prompt learning techniques enhance the model's ability to understand and predict epidemic patterns over both space and time.

## Foundational Learning
1. **Transformer Architecture** - why needed: Core LLM processing mechanism; quick check: Understand self-attention and positional encoding
2. **Autoregressive Modeling** - why needed: Enables next-token prediction for forecasting; quick check: Verify understanding of sequence generation
3. **Spatio-temporal Data Integration** - why needed: Combines infection and mobility data; quick check: Confirm data alignment techniques
4. **Prompt Learning** - why needed: Enhances model's epidemic pattern perception; quick check: Review prompt engineering basics
5. **Dual-branch Processing** - why needed: Simultaneously processes infection and mobility data; quick check: Understand parallel data streams

## Architecture Onboarding
**Component Map:** Data Input -> Dual-Branch Encoder -> Spatio-temporal Prompt Learning -> Autoregressive Decoder -> Forecast Output

**Critical Path:** Mobility and infection data alignment → Dual-branch encoding → Spatio-temporal prompt integration → Next-token prediction

**Design Tradeoffs:** Larger models provide better performance but require more computational resources; autoregressive approach excels at direct forecasting but accumulates errors in multi-step predictions

**Failure Signatures:** Performance degradation in data-scarce regions, sensitivity to mobility data quality, reduced accuracy for epidemics with different transmission dynamics than COVID-19

**First 3 Experiments:**
1. Direct forecasting comparison on England dataset with state-of-the-art baselines
2. Multi-step forecasting evaluation on Spain dataset to test error accumulation
3. Ablation study to validate importance of spatio-temporal prompt learning

## Open Questions the Paper Calls Out
None

## Limitations
- Computational constraints limit deployment in resource-constrained settings
- Autoregressive formulation accumulates errors in long-sequence multi-step predictions
- Focus on COVID-19 data from four European countries raises generalizability concerns

## Confidence

**High confidence:** Core architectural innovation of aligning epidemic data with language tokens, comparative performance against baselines on tested datasets, demonstrated scaling behavior with model size

**Medium confidence:** Generalizability to non-COVID epidemics, performance in data-scarce regions, robustness to varying data quality and availability

**Low confidence:** Long-term multi-step forecasting accuracy beyond tested horizons, computational efficiency trade-offs for practical deployment, adaptation to epidemics with substantially different transmission dynamics

## Next Checks
1. Evaluate EpiLLM's performance on epidemic datasets from non-European countries and for diseases with different transmission characteristics (e.g., influenza, measles) to assess geographic and disease generalizability

2. Conduct experiments with partial or noisy mobility data to quantify model robustness and determine minimum data quality requirements for effective forecasting

3. Test the framework's performance with heterogeneous spatial resolutions between infection and mobility data to understand its flexibility in real-world data integration scenarios