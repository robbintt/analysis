---
ver: rpa2
title: 'OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$
  Implicit Biases'
arxiv_id: '2602.01105'
source_url: https://arxiv.org/abs/2602.01105
tags:
- muon
- sign
- spectral
- olion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes OLion, an optimizer that combines spectral\
  \ control (via orthogonalized update directions) with \u2113\u221E-style coordinate\
  \ control (via sign-based updates). By applying a sign operation after Newton-Schulz\
  \ orthogonalization, OLion approximates the intersection of spectral and \u2113\u221E\
  \ constraint sets\u2014a scaled Hadamard-like set for matrix parameters\u2014providing\
  \ an efficient way to combine both implicit biases."
---

# OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases

## Quick Facts
- arXiv ID: 2602.01105
- Source URL: https://arxiv.org/abs/2602.01105
- Reference count: 34
- Key outcome: OLion combines spectral control (via orthogonalized updates) with ℓ∞-style coordinate control (via sign-based updates), approximating the intersection of spectral and ℓ∞ constraint sets to improve optimization.

## Executive Summary
This paper introduces OLion, an optimizer that strategically combines spectral and ℓ∞ implicit biases to enhance training efficiency and performance. By orthogonalizing update directions through Newton-Schulz iterations and applying sign operations, OLion approximates the Hadamard ideal constraint set. The authors demonstrate OLion's effectiveness across large-scale language and vision tasks, showing it matches or outperforms AdamW and Muon under comparable tuning, while also addressing optimizer mismatch issues during fine-tuning.

## Method Summary
OLion is designed to combine spectral control and ℓ∞-style coordinate control by orthogonalizing update directions via Newton-Schulz iterations and applying a sign operation to enforce coordinate-wise constraints. This approach approximates the intersection of spectral and ℓ∞ constraint sets, forming a scaled Hadamard-like set for matrix parameters. The optimizer achieves this with only momentum-level state, making it computationally efficient. Theoretical convergence is proven under a diagonal-isotropy assumption, which is empirically supported across tested architectures.

## Key Results
- OLion matches or outperforms AdamW and Muon across large-scale language and vision tasks, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning of Llama-3.1-8B.
- OLion mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.
- The optimizer uses only momentum-level state, making it computationally efficient.

## Why This Works (Mechanism)
OLion combines spectral and ℓ∞ implicit biases by orthogonalizing update directions and applying sign-based updates. This approach approximates the intersection of spectral and ℓ∞ constraint sets, which is theorized to provide a more balanced and effective optimization trajectory. The Newton-Schulz orthogonalization ensures spectral control, while the sign operation enforces ℓ∞ bias, leading to improved convergence and performance across diverse tasks.

## Foundational Learning
- **Newton-Schulz Iteration**: A method for matrix orthogonalization, crucial for enforcing spectral control in OLion. *Quick check*: Verify the convergence of the iteration for large matrices.
- **Implicit Bias**: The tendency of optimization algorithms to favor certain solutions, which OLion leverages by combining spectral and ℓ∞ biases. *Quick check*: Analyze how these biases affect the loss landscape.
- **Diagonal-Isotropy Assumption**: A theoretical requirement for OLion's convergence proof, ensuring gradient directions are aligned with coordinate axes. *Quick check*: Test this assumption across different architectures.

## Architecture Onboarding
- **Component Map**: Gradient -> Newton-Schulz Orthogonalization -> Sign Operation -> Update Step
- **Critical Path**: The orthogonalization and sign operation steps are critical for enforcing the desired implicit biases.
- **Design Tradeoffs**: OLion trades computational overhead from orthogonalization for improved convergence and performance.
- **Failure Signatures**: Potential issues include suboptimal step directions in highly anisotropic loss landscapes and computational inefficiency for extremely large models.
- **First Experiments**: 
  1. Test OLion on a small-scale transformer to verify the diagonal-isotropy assumption.
  2. Compare OLion's performance with AdamW on a standard vision task.
  3. Evaluate OLion's computational overhead on a medium-sized model.

## Open Questions the Paper Calls Out
None

## Limitations
- The diagonal-isotropy assumption required for theoretical convergence may not hold universally across all architectures.
- The computational overhead from Newton-Schulz orthogonalization scales with model size, potentially limiting practicality for extremely large models.
- The sign-based update mechanism may sometimes lead to suboptimal step directions in highly anisotropic loss landscapes.

## Confidence
- **High Confidence**: OLion's empirical performance matching or exceeding AdamW/Muon across diverse tasks.
- **Medium Confidence**: Theoretical convergence guarantees dependent on the diagonal-isotropy assumption.
- **Medium Confidence**: The approximation of the Hadamard ideal via sign operation after orthogonalization.

## Next Checks
1. Test OLion on architectures with known anisotropic gradient structures to evaluate the diagonal-isotropy assumption's validity.
2. Conduct ablation studies comparing OLion with variants that modify the balance between spectral and ℓ∞ bias control.
3. Evaluate computational overhead scaling with model size by benchmarking OLion on models larger than Llama-3.1-8B.