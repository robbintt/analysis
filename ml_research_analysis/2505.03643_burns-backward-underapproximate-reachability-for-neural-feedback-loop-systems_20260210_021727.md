---
ver: rpa2
title: 'BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems'
arxiv_id: '2505.03643'
source_url: https://arxiv.org/abs/2505.03643
tags:
- reachable
- backward
- sets
- ball
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an algorithm for computing underapproximate
  backward reachable sets of nonlinear discrete-time neural feedback loops, addressing
  the gap in verification tools for learning-enabled systems. The core method uses
  overapproximation of system dynamics to enable computation of underapproximate backward
  reachable sets through mixed-integer linear programming (MILP) solutions.
---

# BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems

## Quick Facts
- arXiv ID: 2505.03643
- Source URL: https://arxiv.org/abs/2505.03643
- Reference count: 24
- Primary result: Underapproximate backward reachable sets for neural feedback loops computed via MILP solutions

## Executive Summary
This paper introduces BURNS, an algorithm for computing underapproximate backward reachable sets (BRS) of nonlinear discrete-time neural feedback loops. The method addresses the verification gap for learning-enabled systems by enabling checking of goal-reaching properties through mixed-integer linear programming (MILP). The core insight is that overapproximating system dynamics enables computation of underapproximations of BRS. The approach is demonstrated on a 2D robot navigation problem, showing computation times of 3.1-13.2 minutes for 7 timesteps with underapproximation errors ranging from 59% to 94% coverage depending on sample size.

## Method Summary
BURNS computes underapproximate backward reachable sets by first overapproximating the system dynamics using piecewise-linear bounds, then encoding the closed-loop system (dynamics plus neural network controller) as a MILP. For each timestep, the algorithm uses rejection sampling to find points in the backward reachable set, then solves an optimization problem to find the largest norm ball centered at each point that remains outside the goal set complement. The union of these balls forms the underapproximate BRS. The method leverages the fact that overapproximating dynamics yields underapproximations of BRS, enabling sound verification of reach properties for nonlinear neural feedback systems.

## Key Results
- Algorithm successfully computes underapproximate backward reachable sets for 2D robot navigation with neural network control
- Computation times: 3.1-13.2 minutes for 7 timesteps depending on sample size
- Underapproximation coverage: 59% to 94% of true reachable set volume
- Method enables checking of goal-reaching properties for nonlinear neural feedback systems
- MILP solve time grows with timestep horizon, with the 7th timestep consuming >50% of total time

## Why This Works (Mechanism)

### Mechanism 1: Boundary-Coincident Norm Ball Optimization
- Claim: The algorithm computes valid underapproximations by finding norm balls whose boundaries intersect the true backward reachable set boundary.
- Mechanism: For a sampled point x_d in the backward reachable set, the optimization minimizes radius ε subject to the constraint that the output x_0 does NOT lie in the goal set. The optimal solution yields a ball that touches—but does not exceed—the true backward reachable set boundary.
- Core assumption: Rejection sampling successfully obtains points x_d ∈ R^(-t)(G) from the true backward reachable set.
- Evidence anchors: [abstract]: "computation of underapproximate backward reachable sets through solutions of mixed-integer linear programs"; [section IV-C, Lemma IV.5]: Proves Ball_p(x_d, ε*) ⊆_δc R_fcl^(-k)(G); [corpus]: Limited direct precedent; related work on reach-avoid verification uses forward reachability predominately due to backward scalability issues
- Break condition: If rejection sampling domain D fails to contain the true BRS, or if no valid samples pass the forward simulation check.

### Mechanism 2: Overapproximation-to-Underapproximation Inversion
- Claim: Replacing true dynamics f with overapproximated dynamics f̂ yields underapproximations of backward reachable sets.
- Mechanism: A multivalued overapproximation f̂ has an image that contains the true image. By set inclusion properties, the preimage under f̂ (backward reachable set) must be contained within the preimage under f.
- Core assumption: The dynamics overapproximation f̂ is sound—its image contains all true trajectories.
- Evidence anchors: [abstract]: "based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets"; [section IV-D, Lemma IV.6–IV.7]: Formal proof that R_f̂_cl^(-t)(G) ⊆ R_f_cl^(-t)(G) for all timesteps via induction; [corpus]: OVERT algorithm cited; related approaches use similar abstraction but for forward/overapproximate cases
- Break condition: If overapproximation is too loose, the resulting underapproximation becomes trivially empty or very small.

### Mechanism 3: Piecewise-Linear MILP Encoding
- Claim: Composing overapproximated dynamics with ReLU networks yields a piecewise-linear system encodable as MILP.
- Mechanism: Both ReLU activations (t = max(x, 0)) and OVERT abstractions (min/max functions) are piecewise-linear. Binary variables encode which linear piece is active, converting the problem to mixed-integer linear form.
- Core assumption: Neural network activations are piecewise-linear (e.g., ReLU, leaky ReLU). Smooth activations like sigmoid break this encoding.
- Evidence anchors: [section IV-A]: Explicit MILP constraints for ReLU and max/min encodings; [section IV-E.1–E.2]: Union-of-convex-sets encoding for non-convex goal complements; [corpus]: Standard technique from neural network verification (Tjeng et al. 2017, cited as [19])
- Break condition: Non-piecewise-linear activations require mixed-integer convex or nonlinear programming, losing MILP guarantees.

## Foundational Learning

- **Concept: Backward Reachability vs. Forward Reachability**
  - Why needed here: Determines whether you compute "what states lead to goal" (backward, for reach properties) vs. "where states can go" (forward, for avoid properties). The paper targets goal-reaching, requiring backward analysis.
  - Quick check question: If you want to verify a robot will reach a charging station, should you use forward or backward reachability?

- **Concept: Underapproximation vs. Overapproximation**
  - Why needed here: Underapproximations provide sound guarantees for reach properties (if X_s ⊆ underapprox_BRS, goal is reached). Overapproximations would be too conservative.
  - Quick check question: An underapproximation of 80% coverage might miss some valid starting states—is this acceptable for proving a reach property?

- **Concept: Mixed-Integer Linear Programming (MILP)**
  - Why needed here: The algorithm encodes piecewise-linear neural networks and dynamics as MILP constraints. Understanding binary variable roles is essential for debugging and scalability analysis.
  - Quick check question: How many binary variables are needed to encode a ReLU layer with n neurons?

## Architecture Onboarding

- **Component map:**
  Rejection sampler (Alg. 2) -> MILP encoder -> Norm ball optimizer -> Set accumulator -> Property checker

- **Critical path:**
  MILP solve time dominates. Complexity is O(nsamp × k × q) where nsamp = samples per step, k = horizon, q = encoding size (dynamics + NN). Final timestep alone takes >50% of total time (Fig. 6).

- **Design tradeoffs:**
  - nsamp: More samples → higher coverage (Table I: 59% → 94%) but linear time increase
  - Dynamics abstraction tightness: Tighter bounds → better underapproximation but larger MILP
  - Norm choice: p ∈ {1, ∞} stays linear; p = 2 requires convex optimization

- **Failure signatures:**
  - Empty BRS → domain D too small or overapproximation too loose
  - MILP timeout → horizon too long, NN too large, or nsamp too high
  - Low coverage → insufficient samples or accumulated abstraction error at later timesteps

- **First 3 experiments:**
  1. Replicate 2D robot example (nsamp=15, k=7) to validate toolchain—expect 8.5 min runtime, ~88% union coverage
  2. Ablate nsamp ∈ {5, 15, 25} on same problem—confirm time-coverage tradeoff in Table I
  3. Check inclusion of X_s1 (safe) and X_s2 (unsafe) using eq. 10—expect <0.1s verification time, correct classification for nsamp ≥ 15

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid-symbolic approach extend BURNS to longer time horizons while maintaining soundness?
- Basis in paper: [explicit] "Future work to address this limitation is to implement a hybrid-symbolic approach as described in [13], [22], which allows for analysis over longer time horizons."
- Why unresolved: Current MILP-based approach scales poorly; the 7th timestep consumes >50% of total solve time.
- What evidence would resolve it: Implementation demonstrating tractable computation for horizons significantly exceeding 7 timesteps with maintained underapproximation guarantees.

### Open Question 2
- Question: How does underapproximation error accumulate across timesteps, and can it be formally bounded?
- Basis in paper: [inferred] The paper observes "underapproximation error increases with timestep which is likely due to accumulated error from the approximation f̂" but provides no theoretical characterization.
- Why unresolved: Only empirical error estimates via sampling are provided; no analysis of how dynamics overapproximation propagates error backward through time.
- What evidence would resolve it: Theoretical bounds on underapproximation error as a function of timestep, dynamics approximation quality, and problem parameters.

### Open Question 3
- Question: How does algorithm performance vary across different neural network architectures, activation functions, and system dimensions?
- Basis in paper: [explicit] "A more complete analysis of the algorithm over a variety of example problems would also provide more insight into its performance."
- Why unresolved: Evaluation limited to single 2D robot example with one network architecture (3 layers of 10 ReLU neurons).
- What evidence would resolve it: Systematic benchmarking across varying state dimensions, network sizes, activation types, and nonlinear dynamics.

## Limitations
- Scalability limited to ~7 timesteps due to MILP complexity growth
- Restricted to piecewise-linear neural network activations (ReLU, leaky ReLU)
- Requires careful tuning of dynamics overapproximation bounds
- Empirical evaluation limited to single 2D robot example with one network architecture

## Confidence
- **High confidence**: The MILP encoding of piecewise-linear components (ReLU, max/min) follows standard, well-established techniques from neural network verification literature.
- **Medium confidence**: The theoretical proof that overapproximation of dynamics yields underapproximation of backward reachable sets is sound under stated assumptions, but empirical validation is limited.
- **Low confidence**: Scalability claims and generalization to larger systems or longer time horizons are not supported by the experimental evidence.

## Next Checks
1. **Scaling Experiment**: Run the algorithm on the same 2D robot problem with k=10, 15 timesteps to empirically measure how MILP solve time scales and whether the method remains tractable.
2. **Coverage Variability Test**: Repeat the 7-timestep experiment 5 times with different Sobol sequence seeds to quantify the variance in underapproximation coverage and verify the reported 59-94% range is consistent.
3. **Abstraction Sensitivity Analysis**: Systematically vary the tightness of the OVERT dynamics overapproximation bounds and measure the impact on MILP solve time and underapproximation coverage to characterize the abstraction-tightness tradeoff.