---
ver: rpa2
title: Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language
  Models
arxiv_id: '2502.12813'
source_url: https://arxiv.org/abs/2502.12813
tags:
- user
- users
- generated
- dialogue
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel user simulation approach for task-oriented
  dialogue systems using large language models (LLMs). The authors propose generating
  diverse user profiles with varied demographics, goals, and conversational styles
  using GPT-4o and GPT-o1, then simulating dialogues with a study program chatbot
  called StudyBot.
---

# Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models

## Quick Facts
- arXiv ID: 2502.12813
- Source URL: https://arxiv.org/abs/2502.12813
- Authors: Adnan Ahmad; Stefan Hillmann; Sebastian Möller
- Reference count: 5
- Key outcome: 82.46% task success rate achieved with StudyBot using LLM-generated user profiles

## Executive Summary
This paper presents a novel user simulation approach for task-oriented dialogue systems using large language models (LLMs). The authors generate diverse user profiles with varied demographics, goals, and conversational styles using GPT-4o and GPT-o1, then simulate dialogues with a study program chatbot called StudyBot. The approach enables automated testing of dialogue systems without human annotators, achieving 82.46% task success across 57 simulated conversations. The study demonstrates the feasibility of LLM-based user simulation for evaluating and improving task-oriented dialogue systems.

## Method Summary
The method involves generating user profiles using structured JSON templates and LLM prompting, then simulating multi-turn dialogues where the LLM acts as the user while a task-oriented system (StudyBot) processes intents and generates responses. User profiles include demographics, personality traits, interests, and goals, with some attributes set freely by the LLM and others chosen from predefined options. The system uses Rasa for NLU, a relational database for program information, and Mistral-7B for response generation. Goal achievement is automatically evaluated by another LLM without human supervision.

## Key Results
- StudyBot achieved 82.46% task success rate across 57 simulated conversations
- GPT-o1 generated more heterogeneous user distributions while GPT-4o created more skewed attribute distributions
- Admission restriction and structure_of_the_program goals showed the highest failure rates
- Average successful conversation length was approximately 10.94 turns
- GPT-4o failed to generate diverse degree types, producing 100% Master's students despite prompt instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate heterogeneous user profiles with structured attributes that simulate diverse user populations for dialogue system testing.
- Mechanism: A JSON template constrains LLM output with fixed values (primary goal, user role) and dynamic values (demographics, interests, personality). The prompt includes statistics of previously generated users to encourage diversity. GPT-4o produces more skewed/varied distributions while GPT-o1 enforces balanced distributions across attributes.
- Core assumption: LLM-generated user profiles approximate real-world user diversity sufficiently to expose meaningful system behaviors.
- Evidence anchors:
  - [abstract]: "We employ two proprietary LLMs, namely GPT-4o and GPT-o1... to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives."
  - [section 4.1]: "The template contains the metadata, user demographic info, behavioral attributes, interests, and goals... Some values are set by the LLM in a free choice manner, others are set by the LLM from predefined options."
  - [corpus]: Related work (Know You First and Be You Better) addresses user-level diversity challenges in role-playing simulators, suggesting this remains an active research area.
- Break condition: If generated profiles exhibit systematic biases that fail to cover critical user segments, simulation utility degrades.

### Mechanism 2
- Claim: Goal-driven multi-turn dialogue simulation enables automated testing of task-oriented dialogue systems without human annotators.
- Mechanism: The generated user profile serves as a persistent system prompt throughout conversation. User utterances are generated by GPT-4o conditioned on the profile template plus conversation history. StudyBot processes intents via DIETClassifier, executes SQL queries against a relational database, and generates responses using Mistral-7B-Instruct-v0.2. Conversations terminate when goals are met or 20 turns are reached.
- Core assumption: LLM-generated user utterances sufficiently reflect how users with specified profiles would actually behave.
- Evidence anchors:
  - [abstract]: "We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success."
  - [section 4.2]: "For that, there is an initial prompt given to the LLM with the implemented user template throughout the conversation (as system prompt). The previous conversation history is also added for each response generation."
  - [corpus]: Related frameworks (clem:todd) similarly combine user simulators with conversational agents but often evaluate components in isolation.
- Break condition: If user utterance generation drifts from profile constraints over long conversations, goal alignment degrades.

### Mechanism 3
- Claim: Automated goal evaluation using LLMs provides scalable task success assessment without human judgment.
- Mechanism: GPT-4o continuously assesses whether defined user goals (primary and secondary) have been met based on conversation content. Success is binary per session—goals achieved within turn limit. Aggregated success rate (82.46%) identifies systematic failure patterns in specific topics.
- Core assumption: LLM-based evaluation correlates sufficiently with human judgment of goal achievement.
- Evidence anchors:
  - [section 5.5]: "Among the conversations, 82.46% sessions achieved all the user goals as judged by the GPT-4o LLMs."
  - [section 5.5, Table 4]: Shows specific failure patterns—admission_restriction and structure_of_the_program goals failed most often.
  - [corpus]: TD-EVAL paper notes traditional automatic metrics are insufficient for sophisticated TOD systems, suggesting evaluation methodology remains challenging.
- Break condition: If evaluator LLM exhibits systematic blind spots or over-optimism, failure modes go undetected.

## Foundational Learning

- Concept: **Task-Oriented Dialogue (TOD) System Architecture**
  - Why needed here: StudyBot implements modular TOD architecture (NLU, dialogue state tracking, database query, response generation). Understanding this pipeline is essential for interpreting simulation results.
  - Quick check question: Can you trace how a user utterance flows from intent recognition to database query to natural language response?

- Concept: **User Simulation for Dialogue Evaluation**
  - Why needed here: The core contribution is replacing human testers with LLM-generated synthetic users. Understanding Schatzmann and Young's hidden agenda model (cited) provides theoretical grounding.
  - Quick check question: What are the tradeoffs between structured goal-based simulation and open-ended conversational simulation?

- Concept: **Five-Factor Personality Model (Big Five)**
  - Why needed here: User profiles include personality dimensions (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) scored 1-5. These affect conversational style and potentially goal pursuit patterns.
  - Quick check question: How might high Neuroticism vs. low Neuroticism affect simulated user behavior in a task-oriented dialogue?

## Architecture Onboarding

- Component map: Profile Generator (GPT-4o/GPT-o1) -> User Simulator (GPT-4o) -> StudyBot NLU (Rasa DIETClassifier) -> Database Layer (SQL) -> Response Generator (Mistral-7B-Instruct-v0.2) -> Goal Evaluator (GPT-4o)

- Critical path: Profile generation → Conversation initialization → Turn-by-turn simulation (user utterance → intent recognition → SQL query → response generation) → Goal evaluation → Session termination

- Design tradeoffs:
  - GPT-4o profiles: More varied but show stereotypical interest assignments by region
  - GPT-o1 profiles: More balanced distributions but may not reflect real-world skew
  - Mistral-7B for response generation: Open-weights, lower cost vs. proprietary model quality
  - 20-turn limit: Prevents infinite loops but may truncate complex goal pursuit

- Failure signatures:
  - Admission restriction queries: 4 failures (top failure category)
  - Software engineering/computer science interests: Higher failure rates
  - GPT-4o failed to generate diverse degree types (100% Master's, 0% Bachelor's despite prompt instructions)

- First 3 experiments:
  1. **Profile bias audit**: Generate 100 profiles with each model; measure distribution skew across all attributes against target population statistics (e.g., actual TUB student demographics).
  2. **Failure mode deep-dive**: Extract all conversations where admission_restriction goals failed; manually annotate root causes (NLU failure, database gap, response generation error).
  3. **Evaluator calibration**: Sample 20 completed sessions; compare GPT-4o goal achievement judgments against human annotators to estimate agreement rate and systematic偏差.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the automated LLM-based evaluation of task success (performed by GPT-4o) align with human judgments of goal achievement?
- Basis in paper: [inferred] The methodology section states that "All these evaluations are being done automatically without any human supervision," which introduces a risk of model bias or hallucination in the success metrics.
- Why unresolved: The paper relies entirely on the model's self-assessment of the 57 dialogue sessions, without validating if the "82.46% task success" rate is perceived as successful by human users.
- What evidence would resolve it: Correlation coefficients between human annotator labels and GPT-4o evaluations on a subset of the dialogue transcripts.

### Open Question 2
- Question: Can prompting strategies be refined to generate user profiles that reflect real-world demographic distributions without enforcing artificial balance or introducing stereotypical correlations?
- Basis in paper: [explicit] The authors state "Future work could explore refining LLM-generated user diversity, mitigating potential biases" and note GPT-4o produced stereotypical interests while GPT-o1 enforced strict balance.
- Why unresolved: The study highlights that GPT-4o associated specific regions with specific interests (e.g., Asian users with AI), while GPT-o1 created uniform distributions that may not reflect actual student populations.
- What evidence would resolve it: A comparative analysis where generated profile distributions are benchmarked against verified institutional demographic data (ground truth) rather than internal model distributions.

### Open Question 3
- Question: How does the efficacy of this simulation framework change when applied to dialogue tasks requiring complex negotiation or constraint satisfaction rather than information retrieval?
- Basis in paper: [explicit] The conclusion suggests "expanding the conversational scope to assess more complex multi-turn interactions."
- Why unresolved: The current system was tested on "StudyBot," which primarily handles information retrieval about study programs, leaving its robustness in handling negotiation or conflicting constraints untested.
- What evidence would resolve it: Application of the same simulation pipeline to a negotiation-based dialogue system (e.g., price negotiation) and analysis of the resulting task success rates.

## Limitations

- Automated evaluation mechanism lacks human validation, introducing uncertainty about true task success rates
- Diversity analysis lacks grounding in real user population statistics for meaningful comparison
- Single LLM (GPT-4o) used as user simulator may not capture full range of user behaviors
- GPT-4o showed systematic bias in generating stereotypical interest-region correlations

## Confidence

- Profile generation diversity analysis: Medium confidence - systematic patterns observed but no real-world baseline comparison
- Task success rate (82.46%): Low confidence - no human evaluation of LLM goal judgments
- GPT-4o vs GPT-o1 distribution differences: Medium confidence - clear quantitative differences in attribute distributions

## Next Checks

1. **Human validation study**: Have 3-5 human annotators independently judge goal achievement for 20 randomly sampled conversations to establish ground truth correlation with LLM evaluation
2. **Real user comparison**: Compare generated profile distributions against actual TU Berlin student enrollment data (if available) to assess whether synthetic diversity reflects reality
3. **Cross-simulator robustness test**: Run 20 conversations using different user simulators (different LLMs or prompting strategies) to evaluate sensitivity of results to simulation method