---
ver: rpa2
title: 'MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective
  multi-agent reinforcement learning'
arxiv_id: '2511.18181'
source_url: https://arxiv.org/abs/2511.18181
tags:
- uni00000013
- learning
- multi-agent
- agents
- multi-objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOMA-AC, the first inner-loop actor-critic
  framework for continuous multi-objective multi-agent reinforcement learning (MOMARL).
  The framework uses preference-conditioned policies and centralised critics to enable
  scalable learning of diverse, Pareto-efficient solutions across agents.
---

# MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning

## Quick Facts
- arXiv ID: 2511.18181
- Source URL: https://arxiv.org/abs/2511.18181
- Reference count: 40
- Primary result: MOMA-AC achieves statistically significant improvements in expected utility and hypervolume over baselines in continuous multi-objective multi-agent tasks

## Executive Summary
This paper introduces MOMA-AC, the first inner-loop actor-critic framework for continuous multi-objective multi-agent reinforcement learning. The framework uses preference-conditioned policies and centralised critics to enable scalable learning of diverse, Pareto-efficient solutions across agents. By combining multi-headed actors and vector-valued critics, it addresses key coordination and utility estimation challenges in MOMARL. Experiments on cooperative locomotion tasks show MOMA-AC achieves statistically significant improvements in expected utility and hypervolume over baselines.

## Method Summary
MOMA-AC is an actor-critic algorithm that conditions policies on user preference weights to learn a coverage set of Pareto-optimal solutions in a single training run. It employs a multi-headed actor network where each agent has its own output head, a centralised twin-critic architecture (MOMA-TD3) for stable utility estimation, and preference conditioning where a weight vector ω is sampled per episode. The framework uses CTDE: joint observation-action information for critics during training, but only local observations for actors during execution. Training involves sampling preferences from the unit simplex, using the minimum of two scalarized critics to mitigate overestimation bias, and updating actors via gradients from the centralised critic.

## Key Results
- MOMA-AC achieves statistically significant improvements in expected utility and hypervolume over baselines on cooperative locomotion tasks
- MOMA-TD3 variant shows superior scalability and stability compared to MOMA-DDPG, particularly as agent count increases
- Dual-critic architecture effectively mitigates utility overestimation bias, with MOMA-TD3 showing conservative bias while MOMA-DDPG shows optimistic bias
- Inner-loop preference conditioning enables sample-efficient learning of diverse Pareto solutions in a single training run

## Why This Works (Mechanism)

### Mechanism 1: Preference-Conditioned Inner-Loop Training
Conditioning a single actor network on preference weights during training enables the policy to approximate a coverage set (Pareto front) more sample-efficiently than outer-loop methods. At episode start, a preference weight vector ω is sampled and provided to both centralised critic(s) and multi-headed actor. The actor learns to maximize scalarized utility ω^T Q(s, a, ω) while the critic learns to estimate vector-valued Q-function. This forces the policy to learn a continuous mapping from preferences to actions, generalizing across preference space in a single training run.

### Mechanism 2: Centralised Critic with Decentralised Multi-Headed Actors (CTDE)
Centralised critic during training provides global information needed for coordination and stabilises learning in non-stationary multi-agent environment, while multi-headed actors enable efficient parameter sharing and ensure policies can be executed decentrally. The critic takes joint observation o and joint action a from all agents, along with preference weight ω, to estimate team-level vector-valued Q-function. The actor consists of shared network trunk with separate heads for each agent, taking only local observations o_i and ω to produce local actions a_i.

### Mechanism 3: Dual-Critic Utility Overestimation Mitigation
Employing twin critic architecture and taking minimum of two critics' scalarized values when computing bootstrap targets mitigates positive bias in utility estimation. Two independent centralised critics Q_θ1 and Q_θ2 are trained. For given preference ω and next state-action pair, scalarized value is computed for both target critics, and the critic with lower scalarized value is selected for bootstrap target computation. This "minimum-of-two" operation counteracts function approximation tendency to overestimate Q-values.

## Foundational Learning

- **Markov Decision Processes (MDPs) & Actor-Critic Methods**: MOMA-AC extends fundamental actor-critic framework from single-agent, single-objective MDPs. Understanding how actor (policy) and critic (value function) interact to learn policy is non-negotiable prerequisite.
  - Quick check: Can you explain roles of actor and critic in standard algorithm like DDPG or TD3 and how actor's loss is derived from critic?

- **Multi-Objective Optimization & Scalarization**: Paper's core mechanism is learning single policy to represent Pareto front of trade-offs between multiple objectives. This requires understanding what Pareto front is and how preference vector converts multi-objective problem into single-objective one.
  - Quick check: Given two objectives, minimizing cost and maximizing speed, can you describe what point on Pareto front represents and how preference weight would trade one off against other?

- **Centralised Training with Decentralised Execution (CTDE)**: This is core architectural paradigm for multi-agent component. System uses information during training that is not available during execution, and new engineer must grasp this asymmetry to understand system's design and constraints.
  - Quick check: What information does critic have access to during training that actors do not have access to during execution, and why is this distinction important?

## Architecture Onboarding

- **Component map**:
  1. Multi-Headed Actor Network: Shared network trunk with multiple output heads. Takes local observation o_i and preference weight ω. Outputs local action a_i.
  2. Centralised Twin Critics (MOMA-TD3): Two separate, independently parameterized networks. Each takes joint observation o, joint action a, and preference weight ω. Outputs vector-valued Q-estimate.
  3. Preference Sampler: Samples weight vector ω from unit simplex at start of each episode and provides it to all agents and critics.
  4. Experience Replay Buffers: Per-agent buffers storing tuples (o_i, a_i, r, o'_i, d, ω).

- **Critical path**:
  1. Initialization: Preference ω is sampled
  2. Action Generation: Each actor head receives local observation o_i and ω, produces action a_i
  3. Environment Interaction: Joint action a = (a_1, ..., a_n) executed. Environment returns shared reward vector r, next joint observation o', termination flag d
  4. Storage: Experience stored in per-agent buffers
  5. Critic Update: Minibatch sampled. Target computed using target actor and target critics, taking minimum of two scalarized target Q-values (for MOMA-TD3). Critics updated to minimize MSE loss against target
  6. Actor Update: Actor updated to maximize scalarized Q-value from first critic, Q_θ1. Gradient flows from critic, through joint action where one agent's action replaced by its actor output, back to actor parameters. Delayed updates (for MOMA-TD3) add stability
  7. Target Network Update: Actor and critic target networks softly updated towards main networks

- **Design tradeoffs**:
  - MOMA-TD3 vs MOMA-DDPG: MOMA-TD3 more stable and scalable due to dual critics and delayed updates. MOMA-DDPG simpler but more prone to overestimation bias and performance degradation as number of agents increases. Start with MOMA-TD3.
  - Inner-Loop vs Outer-Loop: Inner-loop (conditioning on ω) more sample-efficient, allowing single training run to produce Pareto front approximation. Outer-loop conceptually simpler but requires many separate training runs.

- **Failure signatures**:
  - Collapse to single-objective: Learned policy ignores preference weight and optimizes only one objective. Could stem from bug in preference conditioning or poor exploration of preference space.
  - Training Instability: Loss diverges, especially in critic. More likely in MOMA-DDPG. Check learning rates and ensure target networks being updated correctly.
  - Poor Coordination: Agents fail to cooperate, resulting in low global reward. May indicate issue with centralised critic not receiving correct joint information, or break in CTDE assumption (e.g., non-cooperative reward setting).
  - Non-Uniform Pareto Coverage: Learned policies clustered in one region of objective space. Suggests preference sampling strategy may need adjustment.

- **First 3 experiments**:
  1. Single-Agent Verification: Run algorithm on single-agent multi-objective task (e.g., from mo-gymnasium suite). Verify policy correctly changes behavior when preference weight ω varied at test time.
  2. Two-Agent Cooperative Task: Replicate simple multi-agent experiment from paper (e.g., 2-agent HalfCheetah). Compare EUM and Hypervolume curves against paper's reported results to ensure implementation correct.
  3. Scalability Test: Increase number of agents (e.g., from 2 to 6 agents in HalfCheetah) and observe if performance (EUM/HV) degrades. Compare MOMA-TD3 and MOMA-DDPG to empirically validate stability provided by dual-critic mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
How can evolutionary operators be designed to jointly respect multi-objective dominance and multi-agent coordination when integrating MOMA-AC with population-based search? Standard evolutionary RL operators target either multi-objective diversity or multi-agent coordination, but not both simultaneously; naive combinations may increase front coverage without improving coordination quality. Evidence would require ERL-extended MOMA-AC variant demonstrating significantly higher hypervolume and EUM than gradient-only MOMA-AC in high-agent-count decompositions.

### Open Question 2
Does MOMA-AC generalise to competitive or mixed-motive multi-agent settings with vector-valued rewards? Current CTDE architecture and shared centralised critics assume fully cooperative reward structure; competitive settings require equilibrium concepts (e.g., Nash) rather than joint utility maximisation. Evidence would require evaluation on multi-objective competitive benchmark showing stable learning and meaningful Pareto fronts under appropriate equilibrium metrics.

### Open Question 3
How does MOMA-AC's sample efficiency and Pareto coverage degrade as number of objectives increases beyond two, and can adaptive preference sampling mitigate this? Uniform draws can overemphasise extremes and leave gaps with many objectives. Evidence would require experiments on many-objective continuous multi-agent benchmark (d ≥ 4) comparing uniform vs adaptive sampling, reporting hypervolume, sparsity, and sample efficiency.

## Limitations
- Architecture specification gaps: Paper doesn't detail exact network sizes, activation functions, or weight initialization schemes
- Hyperparameter sensitivity: Results show MOMA-TD3 outperforms MOMA-DDPG but doesn't extensively explore hyperparameter sensitivity
- Preference space exploration: Paper samples preferences uniformly from unit simplex but doesn't validate whether this produces good coverage of true Pareto front, especially for non-convex trade-off surfaces

## Confidence

**High confidence**: Dual-critic mechanism (MOMA-TD3) effectively reduces utility overestimation bias, as evidenced by statistically significant negative mean error in HalfCheetah and Hopper. CTDE architecture is well-established MARL principle.

**Medium confidence**: Inner-loop preference conditioning provides sample efficiency benefits over outer-loop training. While mechanism is sound and paper shows improved EUM/HV metrics, direct comparative ablation studies with outer-loop methods are limited.

**Low confidence**: Claim that single network can effectively encode entire Pareto front through preference conditioning. This assumes preference space can be adequately explored and that linear scalarization captures all relevant trade-offs.

## Next Checks

1. **Ablation on critic architecture**: Train MOMA-DDPG and MOMA-TD3 on same 6-agent HalfCheetah task, measuring utility overestimation bias across timesteps. Verify MOMA-TD3 maintains lower bias while MOMA-DDPG degrades, confirming dual-critic mechanism's importance.

2. **Preference space coverage analysis**: After training MOMA-TD3, visualize distribution of achieved utilities across sampled preference space. Compute proportion of preferences that yield dominated solutions versus non-dominated ones to quantify Pareto front coverage quality.

3. **Outer-loop comparison baseline**: Implement outer-loop version that trains separate policies for 11 equally-spaced preference weights. Compare total training time and final EUM/HV against MOMA-TD3's single training run to quantify claimed sample efficiency gains empirically.