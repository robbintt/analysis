---
ver: rpa2
title: 'CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document
  QA Capability'
arxiv_id: '2505.10063'
source_url: https://arxiv.org/abs/2505.10063
tags:
- documents
- retrieval
- attention
- heads
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of long-context multi-document
  question answering (QA), where large language models (LLMs) degrade in performance
  as irrelevant background and distracting documents are added. The proposed CAFE
  method introduces a two-stage coarse-to-fine retrieval framework: first, it uses
  retrieval heads to filter out background documents and rerank the remaining ones,
  then it applies fine-grained attention steering to focus on evidence documents while
  reducing the influence of distracting ones.'
---

# CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability

## Quick Facts
- arXiv ID: 2505.10063
- Source URL: https://arxiv.org/abs/2505.10063
- Reference count: 13
- Primary result: CAFE improves multi-document QA by up to 22.1% SubEM over SFT and 13.7% over RAG on Mistral

## Executive Summary
CAFE addresses the challenge of long-context multi-document question answering by introducing a two-stage coarse-to-fine retrieval framework. The method uses retrieval heads to first filter out irrelevant background documents and rerank the remaining ones, then applies fine-grained attention steering to focus on evidence documents while minimizing the influence of distracting content. This approach enables LLMs to maintain performance even as context length increases, outperforming traditional RAG and supervised fine-tuning baselines.

## Method Summary
CAFE employs a training-free, retrieval head-based approach to enhance multi-document QA. It first uses retrieval heads to filter out irrelevant background documents and rerank the remaining ones based on relevance to the query. Then, it applies fine-grained attention steering to focus the LLM's attention on evidence documents while reducing the impact of distracting ones. This coarse-to-fine process enables more accurate information extraction from long document contexts without requiring additional model training.

## Key Results
- CAFE achieves up to 22.1% SubEM improvement over supervised fine-tuning and 13.7% over RAG methods on Mistral
- The method shows less performance degradation as context length increases compared to baselines
- CAFE consistently outperforms strong baselines across five benchmarks and three different LLMs

## Why This Works (Mechanism)
The method works by first using retrieval heads to filter out irrelevant background documents and rerank the remaining ones based on relevance. Then, it applies fine-grained attention steering to focus the LLM's attention on evidence documents while minimizing the impact of distracting ones. This two-stage process reduces noise and computational overhead while maintaining focus on relevant information.

## Foundational Learning

**Retrieval heads**: Neural modules trained to identify relevant documents from a corpus. Needed to efficiently filter large document collections without full model fine-tuning. Quick check: Verify retrieval head accuracy on held-out validation sets.

**Attention steering**: Mechanism to dynamically adjust token-level attention weights during inference. Needed to suppress irrelevant document influence without modifying the underlying LLM. Quick check: Compare attention distributions before/after steering.

**SubEM (Sub-token Exact Match)**: Metric measuring exact token-level match between generated and reference answers. Needed to capture fine-grained answer accuracy in long documents. Quick check: Calculate SubEM alongside standard EM and F1.

## Architecture Onboarding

**Component map**: Retrieval heads -> Document filtering -> Attention steering -> LLM QA module

**Critical path**: Query -> Retrieval heads -> Filtered document set -> Attention steering -> LLM answer generation

**Design tradeoffs**: Training-free retrieval heads vs. fine-tuned performance; coarse filtering vs. fine-grained attention; computational efficiency vs. accuracy

**Failure signatures**: Over-filtering leading to missing evidence; under-filtering causing noise; attention steering too aggressive causing loss of context

**First experiments**:
1. Benchmark retrieval head precision/recall on held-out documents
2. Test attention steering impact on short (single-document) QA
3. Compare SubEM vs. EM/F1 on multi-document benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to five benchmarks, which may not represent all real-world document complexities
- Reliance on pre-trained retrieval heads may limit adaptability to unseen document structures
- Lack of comparison with more advanced RAG variants or hybrid methods

## Confidence
- Confidence in training-free applicability: Medium
- Confidence in reported performance improvements: High
- Confidence in coarse-to-fine framework novelty: Medium

## Next Checks
1. Test CAFE on out-of-domain datasets or noisy, real-world document collections to assess robustness and adaptability
2. Compare CAFE against hybrid RAG methods or advanced fine-tuning techniques to establish relative performance in more competitive settings
3. Conduct ablation studies to isolate the impact of retrieval heads versus attention steering on overall QA performance