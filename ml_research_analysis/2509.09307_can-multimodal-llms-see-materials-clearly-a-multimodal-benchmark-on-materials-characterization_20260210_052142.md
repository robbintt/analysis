---
ver: rpa2
title: Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials
  Characterization
arxiv_id: '2509.09307'
source_url: https://arxiv.org/abs/2509.09307
tags:
- materials
- matcha
- arxiv
- characterization
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MatCha, the first multimodal benchmark for
  materials characterization image understanding, comprising 1,500 expert-level multiple-choice
  questions across 21 tasks reflecting real-world scientific workflows. The benchmark
  evaluates state-of-the-art multimodal large language models on tasks ranging from
  processing correlation to property analysis, using diverse characterization techniques
  and material types.
---

# Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization

## Quick Facts
- arXiv ID: 2509.09307
- Source URL: https://arxiv.org/abs/2509.09307
- Reference count: 40
- Top proprietary models achieve 62.58% accuracy on MatCha vs. 88.87% for human experts

## Executive Summary
This paper introduces MatCha, the first multimodal benchmark specifically designed to evaluate large language models on materials characterization image understanding. The benchmark comprises 1,500 expert-level multiple-choice questions across 21 tasks that reflect real-world scientific workflows in materials science. Through systematic evaluation of state-of-the-art multimodal models, the study reveals significant performance gaps between models and human experts, with particular struggles in tasks requiring deeper domain knowledge and fine-grained visual perception. The benchmark provides a rigorous framework for assessing and advancing MLLM capabilities in scientific domains.

## Method Summary
The MatCha benchmark was constructed using 340 Nature articles (CC BY-4.0) through a multi-stage pipeline: Exsclaim-based figure retrieval, GPT-4o sub-caption segmentation, VQA generation constrained to predefined materials science sub-tasks, AI filtering via multiple open-source models, and expert review by two PhD materials scientists. The final benchmark contains 1,500 MCQs (994 generated + 506 converted from supplementary datasets) organized into four progressive stages: Processing Correlation, Morphology Analysis, Structure Analysis, and Property Analysis. Models were evaluated under zero-shot, few-shot (2/4/8/16 examples), and Chain-of-Thought conditions, with outputs constrained to single option letters for objective scoring.

## Key Results
- Proprietary models (GPT-4o, Gemini-1.5-Pro) achieve 62.58% accuracy, while open-source models score below 40% on average
- Performance degrades significantly across stages: PC (59.53%) → MA (47.63%) → SA (37.64%) → PA (35.64%)
- Top models struggle with fine-grained morphological analysis and microstructure recognition in electron microscopy images
- Few-shot prompting shows inconsistent effects: helps GPT-4o (+21.35% on Converted VQA) but degrades performance for Qwen2.5-VL-32B
- Chain-of-Thought prompting provides limited improvement (+6.4% average) compared to zero-shot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degradation correlates with task stage progression, indicating domain knowledge depth is a primary bottleneck.
- Mechanism: Tasks follow the "Processing → Morphology → Structure → Property" workflow; later stages require chaining multiple domain concepts and deeper reasoning. Models lacking domain-specific pretraining fail to accumulate the necessary intermediate inferences.
- Core assumption: The stage-based difficulty gradient validly proxies increasing cognitive demands rather than confounding factors like image quality.
- Evidence anchors:
  - [section 4.3] "As tasks progress from morphology analysis to structure interpretation and properties elucidation... most models exhibit a significant decline in performance."
  - [section 4.5] "Lack of Material Knowledge... accounting for over 60-70% of failures."
  - [corpus] MatQnA paper confirms LLM limitations in specialized materials domains with similar FMR signals.
- Break condition: If model performance were uniform across stages or correlated only with image resolution/quality, the mechanism would weaken.

### Mechanism 2
- Claim: Few-shot in-context learning provides inconsistent gains because domain knowledge elicitation competes with context-window noise.
- Mechanism: In-context examples may prime relevant patterns for some models (GPT-4o: +21.35% on Converted VQA subset) but introduce noise or conflict with internal priors in others (Qwen2.5-VL-32B: performance drop).
- Core assumption: The variability is not primarily caused by prompt formatting or tokenization issues.
- Evidence anchors:
  - [section 4.4] "For Gemini-1.5-Pro, performance is inconsistent and can even degrade at higher shot counts."
  - [section 4.4] "For Qwen2.5-VL-32B... suffers a notable performance drop when provided with examples."
  - [corpus] No strong corpus evidence directly addresses few-shot variability in materials domains.
- Break condition: If systematic prompt engineering (e.g., task-specific templates) eliminated the variability, the mechanism would need revision.

### Mechanism 3
- Claim: Visual perception errors compound with domain knowledge gaps, disproportionately affecting fine-grained microstructural tasks.
- Mechanism: Electron microscopy images contain subtle patterns (grain boundaries, defect types) that require both precise visual feature extraction and domain-specific interpretation schemas. Models lack the latter, causing misclassification even when features are detected.
- Core assumption: Human experts' superior performance relies on integrated visual-conceptual reasoning, not purely perceptual acuity.
- Evidence anchors:
  - [section 4.5] "Visual Perception Error is the second most common failure type... even leading models struggle with fine-grained and complex visual patterns."
  - [section 4.3] "All open-source models achieve accuracies below 30%" on Suppl. ICA task requiring fine-grained microstructural differentiation.
  - [corpus] "Rapid morphology characterization" paper shows deep learning can succeed on targeted morphology tasks when domain-aligned.
- Break condition: If high-resolution image inputs alone closed the performance gap without domain knowledge injection, visual perception would be the dominant factor.

## Foundational Learning

- Concept: **Processing-Microstructure-Property (P-M-P) Relationship**
  - Why needed here: MatCha's four-stage design mirrors this fundamental materials science workflow; understanding it clarifies why later stages require deeper reasoning.
  - Quick check question: Can you explain why structure analysis (SA) tasks require more domain knowledge than processing correlation (PC) tasks?

- Concept: **In-Context Learning Variability**
  - Why needed here: Results show few-shot prompting helps some models but hurts others; practitioners must understand this non-monotonic behavior.
  - Quick check question: What condition in section 4.4 would indicate that few-shot examples are introducing noise rather than helpful patterns?

- Concept: **Visual-Linguistic Alignment in Scientific Domains**
  - Why needed here: The dominant error types (knowledge gaps + visual perception) suggest multimodal alignment trained on natural images does not transfer to scientific imagery.
  - Quick check question: How might a model succeed on natural image VQA but fail on crystallinity classification from TEM images?

## Architecture Onboarding

- Component map: Article Retrieval (Exsclaim) → Figure Splitting (GPT-4o) → VQA Generation (GPT-4o) → AI Filtering (multiple open-source models) → Expert Review → Benchmark
- Critical path: Stage-aware task selection → model inference with constrained output (letter only) → accuracy stratified by stage and task type → error category classification
- Design tradeoffs:
  - MCQ format enables objective evaluation but constrains open-ended scientific reasoning assessment
  - Expert filtering ensures quality but limits dataset scale (26,891 → 1,500)
  - Supplementary datasets add realism but may introduce distribution shift
- Failure signatures:
  - High PC accuracy + low PA accuracy → domain knowledge deficit
  - Performance drop with increased few-shot examples → context conflict
  - CoT degradation vs. zero-shot → reasoning pathway misalignment
- First 3 experiments:
  1. Run zero-shot evaluation on a single stage (e.g., MA) with 2-3 models to validate pipeline and establish baseline gaps.
  2. Test few-shot scaling (2 → 16 shots) on one model that improved (GPT-4o) and one that degraded (Qwen2.5-VL-32B) to characterize variability.
  3. Conduct ablation: provide ground-truth textual descriptions of images to isolate visual perception errors from knowledge errors on a 50-sample subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-augmented generation (RAG) systems that dynamically access external materials science knowledge bases effectively bridge the domain knowledge gap in MLLMs, and what retrieval architectures (dense vs. sparse, passage vs. figure-level) are optimal for materials characterization tasks?
- Basis in paper: [explicit] Section F states: "A primary conclusion from our benchmark is that MLLMs are significantly constrained by a lack of specialized domain knowledge... A promising solution is Retrieval-Augmented Generation (RAG)... This makes MatCha an ideal testbed for evaluating future multimodal RAG systems in materials science."
- Why unresolved: The authors identify RAG as promising but do not implement or evaluate any RAG methods. The few-shot experiments suggest context helps some models but hurts others, leaving the optimal knowledge injection strategy unclear.
- What evidence would resolve it: Compare zero-shot baseline against multimodal RAG variants on MatCha, particularly measuring performance gains on the knowledge-intensive PA (Property Analysis) stage where models showed the steepest degradation.

### Open Question 2
- Question: What domain-specific pretraining or fine-tuning strategies would most effectively improve MLLM performance on tasks requiring visual perception of fine-grained microstructural details, and can improvements in visual perception transfer across characterization techniques (SEM, TEM, XRD)?
- Basis in paper: [inferred] Error analysis (Table 2) shows visual perception errors account for 27-34% of failures. Models "struggle with fine-grained morphological analysis" and "fail to recognize microstructural details in complex electron microscopy images." The no-image ablation shows ~25% performance drop, but it remains unclear whether training on more materials imagery would improve this capability.
- Why unresolved: The paper diagnoses visual perception limitations but does not test interventions. It notes "lack of high-quality scientific training corpora" as a likely cause but does not validate whether targeted training would help, nor whether gains generalize across characterization modalities.
- What evidence would resolve it: Train/fine-tune models on curated materials microscopy datasets and evaluate transfer across MatCha's different characterization technique categories (microscopy, spectroscopy, diffraction).

### Open Question 3
- Question: Does performance on MatCha benchmark tasks predict downstream utility for autonomous scientific discovery workflows, such as hypothesis generation, experiment planning, or materials property prediction?
- Basis in paper: [explicit] Abstract states: "We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents." Limitations section notes "strong MatCha scores might be overgeneralized" for real-world applications.
- Why unresolved: The benchmark measures isolated visual question-answering capability but does not evaluate whether models achieving higher MatCha scores are more useful for actual scientific workflows. The connection between benchmark performance and practical research utility remains assumed rather than demonstrated.
- What evidence would resolve it: Correlate MatCha scores with performance on downstream tasks such as (1) generating experimentally testable hypotheses from characterization images, (2) recommending next characterization steps, or (3) predicting materials properties from multi-modal characterization data.

## Limitations
- MCQ format constrains assessment to predefined reasoning paths, potentially missing open-ended scientific inference capabilities
- Automated AI filtering may introduce upward bias by removing edge-case questions where models would fail
- Expert filtering limits dataset scale and may introduce selection bias toward questions with clear expert consensus
- Benchmark construction process lacks full reproducibility due to unspecified search terms and prompt details

## Confidence

**High Confidence**: Proprietary models significantly outperform open-source models (62.58% vs <40% accuracy); domain knowledge gaps account for 60-70% of failures

**Medium Confidence**: Stage-wise performance degradation reflects increasing domain knowledge requirements; few-shot learning shows inconsistent effects

**Low Confidence**: Exact contribution of visual perception errors vs domain knowledge gaps; CoT prompting improvement sensitivity

## Next Checks

1. **Visual Perception Isolation**: Conduct a controlled ablation study where ground-truth textual descriptions of images are provided alongside the original questions. This would isolate whether accuracy improvements stem from enhanced visual feature extraction or from better domain knowledge integration.

2. **Error Mode Granularity**: Perform fine-grained error categorization on a stratified sample of 200 failed responses, distinguishing between: (a) complete visual feature misses, (b) correct visual features but wrong domain interpretation, and (c) both visual and domain failures.

3. **Open-Ended Extension**: Develop a parallel evaluation framework using short-answer questions (not MCQs) for a subset of 100 questions spanning all four stages. This would test whether the performance gap is inflated by the multiple-choice format.