---
ver: rpa2
title: 'TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train'
arxiv_id: '2601.01903'
source_url: https://arxiv.org/abs/2601.01903
tags:
- tt-fsi
- interaction
- shap-iq
- shapley
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenge of calculating
  Faithful Shapley Interaction (FSI) indices, which quantify joint feature contributions
  in machine learning models. FSI uniquely satisfies the faithfulness axiom but requires
  exponential time and memory with existing methods.
---

# TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train

## Quick Facts
- arXiv ID: 2601.01903
- Source URL: https://arxiv.org/abs/2601.01903
- Reference count: 40
- Primary result: TT-FSI achieves O(ℓ²d³·2ᵈ) time vs O(4ᵈ) memory methods, enabling exact FSI computation for d=20 where competitors fail

## Executive Summary
This paper addresses the computational challenge of calculating Faithful Shapley Interaction (FSI) indices, which quantify joint feature contributions in machine learning models. FSI uniquely satisfies the faithfulness axiom but requires exponential time and memory with existing methods. The key insight is that FSI's algebraic structure admits an efficient Matrix Product Operator (MPO) representation via Tensor-Train decomposition. The Möbius transform component has TT-rank 1, while the correction term requires TT-rank O(ℓd), where ℓ is the interaction order. Experiments on six datasets (d=8 to d=20) demonstrate up to 280× speedup over baseline, 85× over SHAP-IQ, and 290× memory reduction.

## Method Summary
TT-FSI exploits the Tensor-Train decomposition to represent the FSI operator efficiently. The Möbius transform of the value function has TT-rank 1 due to its Kronecker product structure, while the correction term's cardinality-dependent weights enable O(ℓd) TT-rank. The algorithm uses a left-to-right sweep that processes dimensions sequentially, contracting MPO cores incrementally to avoid O(4ᵈ) memory allocation. A precontraction optimization composes the Möbius and correction operators into a single pass. The method achieves O(ℓ²d³·2ᵈ) time complexity and O(ℓd²) core storage, enabling exact FSI computation for dimensions where existing methods fail due to memory exhaustion.

## Key Results
- Up to 280× speedup over exact baseline, 85× over SHAP-IQ, 290× memory reduction
- Successfully scales to d=20 (1M coalitions) where all competing methods fail due to memory exhaustion
- GPU float32 implementation achieves 4–19× additional speedup with <0.1% numerical error
- TT-rank analysis proves O(ℓd) bound, though practical ranks may be lower due to state space sparsity

## Why This Works (Mechanism)

### Mechanism 1: Möbius Transform as Rank-1 MPO
The Möbius transform component of FSI admits a Tensor-Train representation with TT-rank 1. The Möbius operator μ↓ decomposes as a Kronecker product ⊗ᵢMᵢ where each Mᵢ is a 2×2 matrix. Kronecker products map to TT-rank 1 by construction—each core is simply Mᵢ with trivial bond dimension 1. This yields O(d·2ᵈ) time for the transform.

### Mechanism 2: Cardinality-Dependent Correction Term
The FSI correction term admits TT-rank O(ℓd) despite involving weighted sums over supersets. The correction weights depend only on subset cardinalities |S| and |T|, not on which specific features appear. This permits a finite-state machine representation tracking only running counters (sₖ, tₖ) at each position. State space is bounded by (ℓ+1)(k+1) = O(ℓk), yielding maximum TT-rank O(ℓd).

### Mechanism 3: Left-to-Right Sweep Contraction
MPO-vector multiplication avoids O(4ᵈ) memory by contracting incrementally. Algorithm 1 processes dimensions sequentially, maintaining intermediate tensor of shape (2ᵏ⁻¹, Dₖ₋₁, 2ᵈ⁻ᵏ⁺¹). Each step contracts one core against the input suffix, never materializing the full 2ᵈ×2ᵈ operator. Total cost O(ℓ²d³·2ᵈ), peak memory O(ℓd·2ᵈ).

## Foundational Learning

- **Shapley Interaction Indices**: Why needed here: FSI is one of several interaction indices; understanding why FSI is unique requires knowing the alternatives (SII, STII) and axioms (efficiency, faithfulness). Quick check question: Why does STII fail the faithfulness axiom while FSI satisfies it?

- **Tensor-Train Decomposition**: Why needed here: The entire speedup derives from representing the FSI operator as an MPO with low TT-rank; without this, you cannot understand why cardinality-dependence enables tractability. Quick check question: What is the TT-rank of a Kronecker product ⊗ᵢMᵢ of 2×2 matrices?

- **Möbius Transform on Boolean Lattice**: Why needed here: FSI's closed-form (Eq. 3) decomposes into Möbius transform plus correction; the Möbius component is TT-rank 1, making it essentially "free." Quick check question: The Möbius transform a(v,S) = Σ_{T⊆S}(-1)^{|S|-|T|}v(T) extracts what kind of interaction information?

## Architecture Onboarding

- **Component map**: Input v ∈ ℝ^{2ᵈ} → [Möbius MPO (rank-1)] → a = μ↓·v ↘ [Correction MPO (rank-O(ℓd))] → FSI output

- **Critical path**: 1. Construct MPO cores G⁽ᵏ⁾ for correction operator (state transitions + final-site weights) 2. Apply sweep (Algorithm 1): reshape → contract → reshape per site 3. Add Möbius result to correction result

- **Design tradeoffs**: Exact vs. approximate: TT-FSI computes exact FSI; trading for sampling (SVARM-IQ, SHAP-IQ MC) gains speed at accuracy cost. Memory vs. time: Storing sparse cores O(ℓd²) vs. dense intermediates O(ℓd·2ᵈ); d≈20 is current practical limit. Float64 vs. float32: GPU experiments show 2–5× speedup with float32 on Tensor Cores, sufficient for interpretability (<0.1% error).

- **Failure signatures**: OOM at d≥16 with SHAP-IQ: np.diag() allocates 2ᵈ×2ᵈ dense matrix. Slow runtime at d≥16 with baseline: O(4ᵈ) iterations. TT-FSI OOM at d>~25: intermediate tensor exceeds RAM.

- **First 3 experiments**: 1. Validation: Run TT-FSI vs. SHAP-IQ exact on California Housing (d=8); confirm max diff < 10⁻¹⁰. 2. Scaling: Measure runtime and peak memory for d∈{8,10,12,14,16,18,20}; verify O(ℓ²d³·2ᵈ) scaling vs SHAP-IQ's O(4ᵈ) crash at d≥16. 3. GPU acceleration: Port to CuPy with float32; confirm 4–19× speedup over DP-GPU per Table 10.

## Open Questions the Paper Calls Out

- Can TT-FSI be effectively combined with sampling-based value estimation to handle dimensions significantly larger than the current limit of d≈25? The current algorithm assumes a completely specified value function vector. It is unclear how sampling noise or partial observations affect the stability and accuracy of the MPO contraction sweep.

- Can model-specific structures, such as those in tree ensembles, be integrated directly into the MPO contraction to bypass the 2ᵈ value function generation bottleneck? TT-FSI currently treats the value function as a generic dense input vector. Utilizing the recursive structure of decision trees to implicitly compute the tensor operations has not been explored.

- Does approximating the MPO representation (e.g., via TT-SVD compression) provide a beneficial trade-off between speed and accuracy for FSI? The paper proves the exact operator has TT-rank O(ℓd) and uses exact sparse cores. Standard tensor-train literature often uses low-rank approximations, but the paper does not test if a compressed, approximate operator retains sufficient fidelity for FSI.

## Limitations
- Exact algebraic structure requirement limits applicability to other interaction indices
- Current limit of d≈25 due to intermediate tensor memory requirements
- Comparison against STII methods is somewhat selective given different faithfulness properties

## Confidence
- **High confidence**: TT-rank analysis for Möbius transform (rank-1), sweep algorithm correctness, experimental runtime scaling (O(ℓ²d³·2ᵈ)), memory reduction claims, baseline comparison methodology
- **Medium confidence**: TT-rank bound O(ℓd) tightness, practical state space sparsity benefits, GPU float32 numerical stability, comparison fairness with STII methods
- **Low confidence**: Precontraction optimization details (Algorithm 3 implementation), cross-dataset model stability, interaction order ℓ sensitivity analysis

## Next Checks
1. **State Space Sparsity Analysis**: Measure actual TT-ranks achieved vs theoretical O(ℓd) bound across datasets; identify if practical ranks are significantly lower due to sparse transition graph.

2. **Numerical Precision Verification**: For d=20 GPU float32 experiments, compute max absolute/relative errors vs float64 baseline across all datasets; verify <0.1% error claim holds systematically.

3. **Interaction Order Sensitivity**: Systematically vary ℓ∈{1,2,3,4} on fixed datasets (d=14-16) and measure runtime/memory scaling; confirm O(ℓ²) time scaling and validate practical ℓ limits.