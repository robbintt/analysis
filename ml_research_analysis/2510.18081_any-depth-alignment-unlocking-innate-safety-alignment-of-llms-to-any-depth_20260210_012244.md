---
ver: rpa2
title: 'Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth'
arxiv_id: '2510.18081'
source_url: https://arxiv.org/abs/2510.18081
tags:
- safety
- tokens
- alignment
- refusal
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Any-Depth Alignment (ADA), a method to unlock
  safety alignment in LLMs at arbitrary generation depths. The key observation is
  that safety signals are concentrated in assistant header tokens, which retain strong
  alignment priors even deep into harmful continuations.
---

# Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth

## Quick Facts
- **arXiv ID**: 2510.18081
- **Source URL**: https://arxiv.org/abs/2510.18081
- **Reference count**: 40
- **Key result**: Achieves near-100% refusal rates against deep prefill attacks up to 2,500 tokens and reduces adversarial prompt attack success to under 3%

## Executive Summary
Any-Depth Alignment (ADA) introduces a novel method to unlock safety alignment in large language models at arbitrary generation depths. The key insight is that safety signals are consistently encoded in assistant header tokens, which retain strong alignment priors even deep into harmful continuations. By leveraging these signals through either token re-injection or linear probing of hidden states, ADA enables robust safety refusal without modifying model weights. The method demonstrates exceptional effectiveness against deep prefill attacks and adversarial prompts while maintaining minimal over-refusal on benign tasks.

## Method Summary
ADA exploits the observation that safety-relevant information is concentrated in assistant header tokens within harmful continuations. The method offers two approaches: ADA-Rethinking re-injects these tokens to trigger refusal, while ADA-LinearProbe probes their hidden states with a linear classifier to halt generation. Both methods access the model's innate safety representations at inference time, requiring no weight modifications. The approach works by detecting alignment priors in token representations and using them to interrupt harmful generations at any depth, even after the base model has been fine-tuned.

## Key Results
- Achieves near-100% refusal rates against deep prefill attacks up to 2,500 tokens
- Reduces adversarial prompt attack success to under 3%
- Maintains near-zero over-refusal on benign tasks while remaining effective after base model fine-tuning

## Why This Works (Mechanism)
The method works because safety signals are consistently encoded in assistant header tokens throughout harmful continuations. These tokens retain strong alignment priors that can be detected and leveraged at inference time. By accessing these innate safety representations through token-level monitoring and intervention, ADA can trigger refusal responses regardless of generation depth. The approach effectively taps into the model's pre-existing safety knowledge that remains accessible even when generating deep harmful content.

## Foundational Learning
- **Safety signal concentration**: Understanding that alignment priors are encoded in specific token positions rather than uniformly distributed. Needed to identify where to probe for safety information. Quick check: Verify safety signals persist across different harmful prompt types.
- **Token-level representation analysis**: Ability to analyze and classify hidden states of individual tokens. Needed to detect safety signals without modifying the model. Quick check: Confirm linear classifier accuracy on token representations.
- **Inference-time intervention**: Knowledge of how to interrupt generation based on token analysis. Needed to implement refusal without architectural changes. Quick check: Test intervention latency and effectiveness.

## Architecture Onboarding
- **Component map**: Input tokens → LLM base model → Token representations → ADA detector (Rethinking or LinearProbe) → Refusal trigger or continue
- **Critical path**: Token generation → Safety signal detection → Intervention decision → Generation halt/refusal
- **Design tradeoffs**: ADA-Rethinking trades computational overhead for reliability, while ADA-LinearProbe offers lower overhead but requires classifier training. Both avoid weight modification but add inference complexity.
- **Failure signatures**: False negatives occur when safety signals are too weak to detect; false positives happen when benign assistant tokens trigger misclassification.
- **Three first experiments**: 1) Test ADA on GPT-4 with varied harmful prompts, 2) Evaluate computational overhead of ADA-LinearProbe, 3) Design adversarial attacks to mask assistant token representations

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty about generalizability to models beyond Llama-3 and Llama-2
- No quantification of computational overhead for inference-time implementation
- Does not address potential evasion strategies that attackers might develop

## Confidence
- **High confidence**: Near-100% refusal rates against deep prefill attacks up to 2,500 tokens
- **High confidence**: Reduction of adversarial prompt attack success to under 3%
- **Medium confidence**: Consistency of safety signals across different harmful prompts and model architectures
- **Medium confidence**: Maintenance of near-zero over-refusal on benign tasks

## Next Checks
1. Evaluate ADA on broader set of base models (GPT-4, Claude) and diverse harmful content categories
2. Design and test new attack strategies aimed at evading ADA through assistant token manipulation
3. Quantify inference-time latency and resource usage of ADA-LinearProbe for practical deployment assessment