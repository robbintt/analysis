---
ver: rpa2
title: 'Comba: Improving Bilinear RNNs with Closed-loop Control'
arxiv_id: '2506.02475'
source_url: https://arxiv.org/abs/2506.02475
tags:
- arxiv
- state
- comba
- memory
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Comba, a novel Bilinear RNN architecture inspired
  by closed-loop control theory. The model introduces a scalar-plus-low-rank (SPLR)
  state transition with both state feedback and output feedback corrections to improve
  memory management in sequence modeling.
---

# Comba: Improving Bilinear RNNs with Closed-loop Control

## Quick Facts
- arXiv ID: 2506.02475
- Source URL: https://arxiv.org/abs/2506.02475
- Authors: Jiaxi Hu; Yongqi Pan; Jusen Du; Disen Lan; Xiaqiang Tang; Qingsong Wen; Yuxuan Liang; Weigao Sun
- Reference count: 40
- Primary result: Proposes Comba, a bilinear RNN with closed-loop control achieving 40% speedup over Gated-DeltaNet with 340M/1.3B parameters trained on large-scale corpus.

## Executive Summary
Comba introduces a novel Bilinear RNN architecture inspired by closed-loop control theory, addressing memory management challenges in sequence modeling. The model employs scalar-plus-low-rank (SPLR) state transitions with both state and output feedback corrections to improve memory orthogonality and retrieval precision. Implemented with hardware-efficient chunk-wise parallel kernels in Triton, Comba demonstrates superior performance on language and vision tasks while achieving significant computational efficiency gains over existing architectures.

## Method Summary
Comba uses a scalar-plus-low-rank (SPLR) state transition with closed-loop corrections for both state and output feedback. The architecture computes gates (α, β, β̃, d) from queries, keys, and values, applies state feedback via a delta rule to orthogonalize memory updates, and corrects the query vector for improved retrieval. Trained on large-scale corpus with AdamW optimizer, the model achieves 40% forward propagation speedup through chunk-wise parallel Triton kernels while maintaining competitive perplexity and accuracy across benchmarks.

## Key Results
- Achieves 40% speedup in forward propagation compared to Gated-DeltaNet
- 340M parameter model reaches WikiText perplexity of 24.15 and Lambada perplexity of 39.91
- 1.3B parameter model demonstrates strong performance on ImageNet classification and object tracking tasks
- Outperforms Gated-DeltaNet on most language tasks while lagging slightly on summarization and code tasks

## Why This Works (Mechanism)

### Mechanism 1: State Feedback via Delta Rule
The model corrects memory updates by computing $v_{new} = v_t - \tilde{\beta}_t S_{t-1} k_t$, which subtracts the component of $v_t$ predictable by current state. This residual error minimization behaves like a Householder reflection, enforcing orthogonality in memory representations and reducing storage conflicts.

### Mechanism 2: Output Feedback Correction
The query vector is adjusted by $\tilde{q}_t = q_t - d k_t$ to optimize similarity objectives and improve retrieval precision. This correction aligns the query space closer to the key space, stabilizing the read operation and reducing perplexity in downstream tasks.

### Mechanism 3: SPLR State Transition
The Scalar-Plus-Low-Rank structure balances dense state expressiveness with training efficiency. Unlike rigid IPLR transitions, SPLR allows negative eigenvalues and flexible forgetting patterns, enabling better state-tracking capabilities while maintaining computational tractability.

## Foundational Learning

- **Concept: Closed-Loop vs. Open-Loop Control**
  - Why needed here: The paper frames architecture as shift from open-loop (blind memory writing) to closed-loop (correcting inputs using measured outputs/states)
  - Quick check question: In a closed-loop system, what signal is used to correct the input $v_t$ before it is written to the state $S_t$?

- **Concept: Householder Transformations**
  - Why needed here: The Delta rule update is interpreted as a Householder reflection (mirror transform), providing mathematical basis for "orthogonal memory management"
  - Quick check question: How does a Householder transformation geometrically affect the vector it is applied to?

- **Concept: Chunk-wise Parallelism & WY Representation**
  - Why needed here: Enables efficient GPU training by bridging recurrence and parallel matrix multiplication
  - Quick check question: Why does the WY representation allow the recurrence to be parallelized within a chunk, whereas a naive recurrence cannot?

## Architecture Onboarding

- **Component map:** Inputs (q, k, v) → Gates (α, β, β̃, d) → State (S) → Output (o = S · q̃)

- **Critical path:**
  1. Compute gates α, β, β̃, d from queries, keys, and values
  2. Calculate "pseudo-value" v_new using state feedback: v - β̃(S · k)
  3. Update State S using SPLR transition
  4. Correct Query: q̃ = q - d k
  5. Compute Output: o = S · q̃

- **Design tradeoffs:**
  - IPLR vs. SPLR: SPLR is faster and simpler with less memory usage but has narrower eigenvalue range
  - Initialization of d: 340M models require d ≈ 0.02 (gradual learning), 1.3B models perform best with d ≈ 1

- **Failure signatures:**
  - Overfitting: Observed with IPLR variants on specific tasks; SPLR preferred for generalization
  - Training Instability: Spectral radius > 1 causes gradient explosion
  - Inference Drift: Poor α initialization limits long-context extrapolation

- **First 3 experiments:**
  1. Kernel Validation: Implement Triton kernel and benchmark forward pass speed against Gated-DeltaNet
  2. Ablation on Feedback: Train 340M model with/without output correction term on WikiText
  3. MQAR Synthetic Task: Validate perfect recall claim on Multi-Query Associative Recall task

## Open Questions the Paper Calls Out

### Open Question 1
The paper identifies that Comba lags behind Gated-DeltaNet on summarization and code tasks, but does not analyze the specific memory or structural reasoning requirements that cause this performance gap. The underlying reasons warrant further exploration in future work.

### Open Question 2
While Gated Slot Attention (GSA) is identified as a strong baseline using intra-layer hybrid approaches, the paper does not define the mechanism for combining GSA's softmax operations with Comba's scalar-plus-low-rank state transition for a flexible hybrid model.

### Open Question 3
The paper plans to focus on addressing chunk-wise parallel optimization for nonlinear RNNs like Titans, Lattice, and MIRAS, which currently suffer from sequence-level parallelism barriers due to structural complexity.

## Limitations
- Performance gap on summarization and code tasks compared to Gated-DeltaNet remains unexplained
- Generalization from synthetic recall tasks to robust long-context understanding in naturalistic domains not demonstrated
- Integration with advanced attention mechanisms like GSA not yet explored

## Confidence
- **High confidence:** Computational efficiency claims (40% speedup, SPLR parameter efficiency) well-supported by ablation results
- **Medium confidence:** Perplexity improvements on standard benchmarks convincing but causal attribution to closed-loop corrections partially unproven
- **Low confidence:** Generalization of perfect recall on synthetic MQAR to robust long-context understanding in naturalistic language/vision domains

## Next Checks
1. Evaluate Comba on long-document QA benchmarks (NarrativeQA, QuALITY) where context length exceeds 8K tokens to verify closed-loop benefits transfer beyond synthetic recall
2. Train Comba variants with only state feedback (no output correction) and only output correction (no state feedback) on WikiText to quantify marginal contribution of each mechanism
3. Monitor spectral radius of SPLR transition matrix during training across different sequence lengths to confirm theoretical bounds (-1 to 1) are maintained in practice and correlate with performance stability