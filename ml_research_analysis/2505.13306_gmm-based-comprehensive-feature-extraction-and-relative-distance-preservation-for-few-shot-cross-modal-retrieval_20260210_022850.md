---
ver: rpa2
title: GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation
  For Few-Shot Cross-Modal Retrieval
arxiv_id: '2505.13306'
source_url: https://arxiv.org/abs/2505.13306
tags:
- cross-modal
- shot
- retrieval
- few-shot
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles few-shot cross-modal retrieval, addressing the
  challenge of learning robust cross-modal representations when training data is scarce
  and unseen classes must be handled during inference. Existing methods often fail
  to model the complex multi-peak distribution of data and suffer from intra-modal
  and inter-modal biases.
---

# GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval

## Quick Facts
- arXiv ID: 2505.13306
- Source URL: https://arxiv.org/abs/2505.13306
- Reference count: 40
- Proposed GCRDP method achieves consistent mAP improvements over six state-of-the-art methods across four benchmark datasets in zero-shot and few-shot (1, 3, 5-shot) settings

## Executive Summary
This paper addresses few-shot cross-modal retrieval by proposing GCRDP, which combines Gaussian Mixture Models (GMM) with multi-positive sample contrastive learning and a Cross-modal Relative Distance Preservation (RDP) constraint. The method aims to overcome the limitations of single-prototype approaches by capturing complex multi-peak data distributions and ensuring semantic alignment across modalities when training data is scarce. Extensive experiments on Wikipedia, Pascal Sentence, NUS-WIDE, and NUS-WIDE-10k datasets demonstrate consistent improvements over six state-of-the-art methods across zero-shot and few-shot settings.

## Method Summary
GCRDP employs a two-pronged approach: (1) Gaussian Mixture Models with multi-positive contrastive learning to capture local semantic structures through multiple components per instance, and (2) a Cross-modal Relative Distance Preservation constraint that enforces consistency between intra-modal similarity matrices across modalities. The method iteratively estimates GMM parameters via EM algorithm, uses the highest posterior component for contrastive learning, and jointly optimizes all components with a total loss combining InfoNCE, cross-modal regularization, and RDP losses.

## Key Results
- GCRDP consistently outperforms six state-of-the-art methods across all four benchmark datasets
- Significant mAP improvements achieved in both zero-shot and few-shot (1, 3, 5-shot) settings
- Ablation studies confirm effectiveness of both GMM components and RDP constraint in enhancing retrieval accuracy
- Performance gains are particularly notable in few-shot scenarios where training data is limited

## Why This Works (Mechanism)

### Mechanism 1: Multi-Peak Distribution Modeling via GMM
- Claim: GMM components capture local semantic structures that single-prototype methods may miss
- Mechanism: Decomposes feature distributions into K Gaussian components via EM iterations, where each component represents a latent semantic unit
- Core assumption: Few-shot data exhibits multi-peak distributions that single-point prototypes cannot adequately represent
- Evidence anchors: Abstract states GMM "effectively captures the complex multi-peak distribution of data"; section 3.2 describes components as "potential semantic units adept at capturing local semantics"

### Mechanism 2: Multi-Positive Contrastive Learning on GMM Components
- Claim: Treating multiple GMM components within the same instance as positives improves intra-modal structure learning
- Mechanism: For each anchor component, other components from the same image/text form the positive set; components from other instances form negatives
- Core assumption: Multiple components within an instance share semantic relatedness worth preserving
- Evidence anchors: Section 3.2 explains how "extending the InfoNCE loss for multi-positive-sample contrastive learning enhances intra-modal structure"

### Mechanism 3: Cross-Modal Relative Distance Preservation (RDP)
- Claim: Enforcing consistency between intra-modal similarity matrices across modalities improves alignment under limited samples
- Mechanism: Minimizes distance between image and text similarity matrices when similarity exceeds threshold θ=0.5
- Core assumption: Relative distances within each modality should be preserved across modalities
- Evidence anchors: Abstract mentions RDP "constrains the relative distances between image and text feature distributions"; section 3.3 describes aligning "cross-modal semantics at a fine-grained level"

## Foundational Learning

- Concept: Gaussian Mixture Models and EM Algorithm
  - Why needed here: Core to the GC module; understanding how mixture weights, means, and covariances are iteratively estimated is essential
  - Quick check question: Can you explain how E-step computes posterior probabilities and M-step updates parameters?

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: Multi-positive extension builds on standard contrastive learning; understanding positive/negative sampling is critical
  - Quick check question: How does the loss in Eq. 8 differ from standard InfoNCE with a single positive?

- Concept: Cross-Modal Retrieval Evaluation (mAP)
  - Why needed here: All results are reported in mAP; understanding AP computation helps interpret performance claims
  - Quick check question: Given a ranked retrieval list, can you compute average precision?

## Architecture Onboarding

- Component map: Feature Encoders -> GC Module (GMM + multi-positive contrastive) -> RDP Module (distance preservation) -> Joint Optimization
- Critical path: 1. Extract image/text features → 2. Fit GMM via EM → 3. Select component with highest posterior → 4. Compute contrastive and RDP losses → 5. Backpropagate jointly
- Design tradeoffs:
  - Number of GMM components (K): Paper uses K=3; higher K captures more structure but risks overfitting with few shots
  - RDP threshold θ=0.5: Filters noisy pairs; adjusting this trades off constraint coverage vs. quality
  - Hyperparameters α, λ: Balance cross-modal regularization vs. RDP; paper does not report sensitivity analysis
- Failure signatures:
  - Performance drops without GMM: Indicates GMM is capturing useful structure
  - 5-shot worse than 3-shot: Suggests overfitting or sample quality issues
  - RDP loss not decreasing: May indicate threshold too restrictive or noisy alignments
- First 3 experiments:
  1. Reproduce zero-shot on Wikipedia: Verify mAP ~55.4 baseline
  2. Ablate K={1,2,3,5}: Test whether K=3 is optimal or dataset-dependent
  3. Visualize GMM components: Project component means to 2D; check if they correspond to interpretable semantic clusters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance vary if the number of Gaussian components (K) is adaptively determined based on data complexity rather than being fixed at K=3?
- Basis in paper: The authors state, "In our experiments, we typically set the number of components to 3," but acknowledge the need to capture "complex multi-peak distributions" which may vary across datasets
- Why unresolved: A fixed K assumes a uniform complexity of semantic distributions across all classes and datasets
- What evidence would resolve it: Ablation studies showing performance metrics across varying K values or using criteria like BIC/AIC to select K dynamically per class

### Open Question 2
- Question: To what extent does the fixed similarity threshold (θ=0.5) in the RDP loss limit the model's ability to align hard positive pairs in noisy datasets?
- Basis in paper: The paper mentions, "To reduce the impact of noisy samples, this constraint is applied only to high-confidence pairs: where θ = 0.5"
- Why unresolved: A fixed threshold may be too aggressive for sparse datasets or too lenient for noisy ones
- What evidence would resolve it: Sensitivity analysis evaluating retrieval performance while varying θ on datasets with differing noise levels

### Open Question 3
- Question: What is the computational cost and convergence stability of the iterative Expectation-Maximization (EM) algorithm compared to non-iterative prototype methods?
- Basis in paper: The method "iteratively involves E-steps and M-steps" to estimate GMM parameters, but the paper provides no analysis on training time or convergence speed
- Why unresolved: Iterative EM steps can significantly increase training overhead and may suffer from local optima
- What evidence would resolve it: Reporting training epochs to convergence and wall-clock time per batch comparing GCRDP against non-iterative baselines

## Limitations
- Assumes multi-peak distributions without empirical validation across datasets
- Fixed number of GMM components (K=3) without sensitivity analysis
- Fixed RDP threshold (θ=0.5) without justification for optimality
- Limited theoretical guarantees for the combined approach

## Confidence
- GMM effectiveness claims: Medium - Supported by ablation results but mechanism not fully validated
- RDP constraint claims: Medium - Shows consistent improvements but threshold sensitivity not explored
- Overall performance claims: High - Results consistently outperform baselines across multiple datasets and settings
- Theoretical contributions: Medium - Novel combination but limited theoretical guarantees provided

## Next Checks
1. **Component Sensitivity Analysis**: Systematically vary K={1,2,3,5} and report performance changes to identify optimal component count per dataset
2. **RDP Threshold Exploration**: Test RDP with θ∈{0.3, 0.5, 0.7} to understand threshold impact on performance and constraint coverage
3. **Qualitative GMM Analysis**: Visualize and interpret the learned GMM components using t-SNE or UMAP projections to verify they capture meaningful semantic structures