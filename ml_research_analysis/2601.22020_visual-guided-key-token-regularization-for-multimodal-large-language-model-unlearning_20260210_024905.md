---
ver: rpa2
title: Visual-Guided Key-Token Regularization for Multimodal Large Language Model
  Unlearning
arxiv_id: '2601.22020'
source_url: https://arxiv.org/abs/2601.22020
tags:
- unlearning
- often
- answer
- tokens
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal large language model
  (MLLM) unlearning, which aims to prevent the model from revealing private information
  when queried about target images while maintaining coherence and retention of non-target
  knowledge. Existing methods treat all answer tokens uniformly and focus only on
  the language modality, ignoring visual cues for identifying key tokens.
---

# Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning

## Quick Facts
- **arXiv ID:** 2601.22020
- **Source URL:** https://arxiv.org/abs/2601.22020
- **Reference count:** 40
- **Primary result:** ViKeR achieves 32.0% unlearning accuracy with 52.7% ROUGE and 22.0% BLEU for retention on the MLLMU 15% task, outperforming standard gradient ascent which produces 0.1% ROUGE and 0.0% BLEU.

## Executive Summary
This paper addresses the critical challenge of multimodal large language model unlearning, where models must be prevented from revealing private information in response to queries about target images while maintaining general knowledge and response coherence. Existing methods treat all answer tokens uniformly and focus only on language modality, leading to either ineffective forgetting or severe degradation of model quality. The authors propose Visual-Guided Key-Token Regularization (ViKeR), which uses irrelevant visual inputs to estimate ideal post-unlearning token distributions and regularizes the unlearning process accordingly. Experiments on MLLMU and CLEAR benchmarks with LLaVA-7B demonstrate that ViKeR effectively performs unlearning while outperforming existing methods in retention and response coherence.

## Method Summary
ViKeR is a two-stage unlearning method that first estimates ideal token distributions using irrelevant reference images, then regularizes gradient ascent with KL divergence between predicted and ideal distributions. The method uses a frozen pre-unlearning model to generate averaged token probabilities from k irrelevant images, creating a reference distribution that preserves structural tokens while flattening identity-specific tokens. During training, the loss combines standard negative log-likelihood on the forget set with a KL regularization term weighted by hyperparameter λ. The approach is implemented on LLaVA-1.5-7B-HF with LoRA fine-tuning, using AdamW optimizer with learning rate 5e-6 and batch size 2 for one epoch.

## Key Results
- On MLLMU 15% task: 32.0% unlearning accuracy with 52.7% ROUGE and 22.0% BLEU retention, compared to 0.1% ROUGE and 0.0% BLEU for standard gradient ascent
- Maintains high GIB (gibberish) scores indicating preserved response coherence across all benchmarks
- Outperforms existing methods on both forget accuracy and retention metrics while maintaining image understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1: Visual-Guided Reference Distribution Estimation
The method leverages irrelevant visual inputs to estimate an "ideal" post-unlearning token distribution that preserves sentence structure while flattening identity-specific probabilities. When the MLLM receives a fixed text query paired with k irrelevant reference images, the averaged output distribution tends to maintain high confidence for structural tokens but exhibits high entropy for identity tokens, creating a stable target for the model to mimic after unlearning.

### Mechanism 2: Implicit Gradient Reweighting via KL Regularization
The KL divergence term acts as an implicit gradient reweighting mechanism, reducing destructive updates to normal tokens while amplifying suppression of key tokens. The gradient of the ViKeR loss scales the standard Gradient Ascent update by a factor (1-λ) for normal tokens but applies different scaling based on diffuse reference probabilities for key tokens, preventing over-forgetting of basic language modeling capabilities.

### Mechanism 3: Preservation of Linguistic Coherence
By anchoring the unlearning process to a valid token distribution rather than purely maximizing loss, the method maintains the model's ability to generate coherent responses for non-target queries. Standard Gradient Ascent pushes probabilities toward zero indiscriminately, but ViKeR pushes the target token distribution toward the reference distribution, which is generated by the pre-unlearning model that is coherent, avoiding language collapse observed in baselines.

## Foundational Learning

**Concept: Gradient Ascent (GA) in Unlearning**
- Why needed here: ViKeR is built as a modification of GA; you must understand that GA inverts standard training loss to "destroy" knowledge at the cost of model stability
- Quick check question: Why does maximizing the negative log-likelihood often lead to gibberish outputs in LLMs?

**Concept: KL Divergence as a Regularizer**
- Why needed here: The core innovation uses KL divergence to constrain the unlearning process; you need to understand that this measures "distance" between current output and ideal "ignorant" output
- Quick check question: How does KL divergence penalize a model that predicts a token with high confidence when the target distribution expects low confidence?

**Concept: Visual Tokenization in MLLMs**
- Why needed here: To understand how "irrelevant images" generate text distributions; you need to know that the image is encoded into tokens that condition the LLM
- Quick check question: How does swapping the visual encoder's output affect the conditional probability of the next text token?

## Architecture Onboarding

**Component map:** Reference Generator (pre-unlearning model + k irrelevant images) -> Unlearning Engine (target image + query) -> Loss Computation (GA loss + λ × KL divergence) -> Optimizer (updates model parameters)

**Critical path:** The generation of the reference distribution is critical; if this distribution is flawed (contains leaked info or is incoherent), the unlearning fails

**Design tradeoffs:**
- Reference Source: Using "people" images provides balance; "patterns" may unlearn too aggressively; "retain" set images may leak info
- Hyperparameter λ: High λ saves coherence but might fail to fully unlearn; low λ unlearns but causes collapse

**Failure signatures:**
- Language Collapse: Model outputs "often often often..." or random characters (indicates λ too low or GA dominating)
- Knowledge Retention: Model still outputs "Copenhagen" (indicates λ too high or reference images too similar to target)

**First 3 experiments:**
1. Baseline Comparison: Run pure Gradient Ascent vs. ViKeR on single sample; observe GA destroys normal token probabilities while ViKeR preserves them
2. Ablation on k: Test with k=1 vs k=10 irrelevant images; verify higher k stabilizes GIB score
3. Reference Image Audit: Try random noise vs celebrity faces as references; check if model retains ability to answer "What is the profession?" with coherent structure

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimal reference image set I' be theoretically determined rather than empirically selected? The paper relies on manual selection of reference categories without providing a theoretical framework for optimal selection. An algorithmic selection method that consistently outperforms manual baselines on ROUGE and GIB metrics would address this.

### Open Question 2
Does ViKeR maintain effectiveness on MLLMs significantly larger than 7B parameters? Experiments are strictly limited to LLaVA-7B, and the 15% task shows significant performance degradation for all methods. Larger models may exhibit different gradient dynamics or robustness to token-level reweighting.

### Open Question 3
Is information entropy a reliable proxy for identifying all "key tokens" carrying private information? Definition 2 relies solely on entropy thresholds to classify key tokens. High entropy might indicate general model uncertainty or hallucination rather than privacy-specific variability.

## Limitations
- Reference distribution quality critically depends on irrelevant images not sharing semantic similarities with target identities
- Hyperparameter λ shows high sensitivity and lacks principled selection methodology for different scenarios
- Evaluation scope limited to fictional and celebrity personas rather than truly private information like medical or financial records

## Confidence

**High Confidence Claims:**
- Mathematical formulation and gradient reweighting mechanism are correctly derived
- Empirical results showing ViKeR outperforms pure Gradient Ascent on retention metrics are well-supported
- Core insight that visual inputs can guide token-level regularization is theoretically sound

**Medium Confidence Claims:**
- "Better retention" should be interpreted within context of specific benchmarks used
- "Response coherence" supported by GIB scores but could benefit from more diverse qualitative evaluations
- Effectiveness of k=5 reference image strategy is validated but may not be optimal for all use cases

**Low Confidence Claims:**
- Generalizability to domains outside tested celebrity/fictional character scenarios remains uncertain
- Claims about "minimal impact on non-target knowledge" based on specific metrics that may not capture all knowledge degradation
- Assertion that ViKeR is "effective" without specifying operational definition across different privacy-utility tradeoffs

## Next Checks
1. **Cross-Domain Reference Distribution Validation:** Test ViKeR's performance when using reference images from domains with semantic overlap with target identities (medical professionals when unlearning a specific doctor) to measure whether reference distribution remains effective at flattening key tokens.

2. **Hyperparameter Sensitivity Analysis:** Conduct systematic grid search over λ values across different forget set sizes to establish principled mapping between forget set characteristics and optimal regularization strength, including convergence stability analysis.

3. **Multi-Turn Adversarial Evaluation:** Design evaluation protocol where unlearned model is subjected to multi-turn conversational queries designed to extract private information through contextual inference, such as follow-up questions after being unlearned on birthplace.