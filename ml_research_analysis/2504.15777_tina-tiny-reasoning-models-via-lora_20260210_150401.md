---
ver: rpa2
title: 'Tina: Tiny Reasoning Models via LoRA'
arxiv_id: '2504.15777'
source_url: https://arxiv.org/abs/2504.15777
tags:
- reasoning
- reward
- training
- tina
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tina demonstrates that tiny language models can achieve competitive
  reasoning performance using parameter-efficient reinforcement learning with LoRA.
  The approach applies low-rank adaptation during RL to a 1.5B parameter base model,
  achieving 20% performance increase and 43.33% Pass@1 accuracy on AIME24 at only
  $9 USD post-training cost.
---

# Tina: Tiny Reasoning Models via LoRA

## Quick Facts
- arXiv ID: 2504.15777
- Source URL: https://arxiv.org/abs/2504.15777
- Reference count: 40
- Key outcome: 1.5B LoRA-RL models achieve 43.33% Pass@1 on AIME24 at $9 USD cost, outperforming full-parameter RL baselines

## Executive Summary
Tina demonstrates that tiny language models can achieve competitive reasoning performance using parameter-efficient reinforcement learning with LoRA. The approach applies low-rank adaptation during RL to a 1.5B parameter base model, achieving >20% performance increase while requiring minimal computational resources. Tina models outperform full-parameter RL baselines, revealing that LoRA efficiently adapts reasoning format while preserving base model knowledge. All code, training logs, and model weights are open-sourced to enable accessible research.

## Method Summary
Tina employs LoRA-based reinforcement learning on distilled 1.5B models, using GRPO with reward functions emphasizing accuracy and reasoning format. The method leverages small, high-quality datasets (7k samples) and identifies optimal stopping points via format-related phase transitions. Training requires only two GPUs and completes in 19-57% of a training epoch. The approach demonstrates that parameter-efficient adaptation can achieve reasoning performance competitive with much larger models.

## Key Results
- 43.33% Pass@1 accuracy on AIME24 at $9 USD post-training cost
- 50.60% average score across six reasoning benchmarks
- 20%+ performance increase over full-parameter RL baselines
- Training completes in 19-57% of a training epoch using two GPUs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA-based RL achieves competitive reasoning performance by rapidly adapting output format while preserving base knowledge, rather than requiring full knowledge relearning.
- **Mechanism:** LoRA modifies only a low-rank decomposition of weight updates, freezing the pretrained weights. This constrains optimization to learn structural/stylistic patterns (e.g., step-by-step reasoning chains) with minimal parameter changes, leaving the base model's pretrained knowledge largely intact.
- **Core assumption:** Reasoning capabilities in already-distilled models can be enhanced primarily through format adaptation rather than deep knowledge integration.
- **Evidence anchors:** The paper hypothesizes that LoRA's effectiveness stems from rapidly adapting the model to reasoning format while preserving underlying knowledge (abstract, Section 5).

### Mechanism 2
- **Claim:** Peak reasoning performance occurs just before a "phase transition" in format-related training metrics, enabling early stopping with minimal compute.
- **Mechanism:** During LoRA-based RL, format reward and completion length metrics show a distinct turning point. The best-performing checkpoint consistently appears at or before this transition, while accuracy reward shows no corresponding inflection—suggesting format optimization saturates before reasoning quality degrades.
- **Core assumption:** Format-related metric instability signals diminishing returns or harmful over-optimization for reasoning tasks.
- **Evidence anchors:** The paper observes a training phase transition in format-related metrics where best checkpoints occur just prior to or around this transition point (Section 5).

### Mechanism 3
- **Claim:** Smaller, higher-quality datasets can outperform larger datasets for LoRA-based reasoning RL, inverting typical scaling expectations.
- **Mechanism:** LoRA's limited parameter budget focuses learning on high-signal patterns. With compact datasets (7k samples), the model learns format efficiently without overfitting; larger datasets (93.7k) may introduce noise or dilute the format signal under constrained capacity.
- **Core assumption:** Data quality and diversity outweigh raw quantity when parameter updates are severely constrained.
- **Evidence anchors:** Tina-Open-RS model trained on 7k examples achieved the highest average score (50.60%), surpassing models trained on considerably larger datasets (Section 4.3).

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Core technique enabling parameter-efficient RL; without understanding LoRA's rank/alpha constraints, hyperparameter choices are opaque.
  - **Quick check question:** Given a weight matrix W₀ ∈ R^(d×k), what is the computational and memory cost difference between full fine-tuning and LoRA with rank r=32?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** The RL algorithm used; differs from PPO by using group-based baselines instead of a value network, affecting reward signal interpretation.
  - **Quick check question:** How does GRPO compute the advantage A_i for output o_i given a group of G outputs with rewards {r_1, ..., r_G}?

- **Concept: Verifiable Rewards for Reasoning**
  - **Why needed here:** The reward signals (accuracy, format, length, cosine) drive learning; understanding their composition is critical for reproducing results.
  - **Quick check question:** If accuracy reward is 1.0 and format reward is 0.8 with weights [2, 1], what is the total reward?

## Architecture Onboarding

- **Component map:**
  Base Model (DeepSeek-R1-Distill-Qwen-1.5B) → LoRA Adapters (rank=32, α=128, dropout=0.05) → RL Algorithm (GRPO or Dr.GRPO) → Training Infrastructure (2× NVIDIA L40S GPUs)

- **Critical path:**
  1. Start from DeepSeek-R1-Distill-Qwen-1.5B checkpoint
  2. Attach LoRA adapters with rank 32, alpha 128
  3. Train with GRPO on 7k high-quality reasoning samples
  4. Monitor format reward and completion length for phase transition
  5. Stop at or just before transition (~450 steps for Open-RS2 dataset)
  6. Evaluate checkpoint on target benchmarks

- **Design tradeoffs:**
  - LoRA rank: Lower (4-8) → fewer parameters, faster training, slightly worse performance; Higher (64) → more capacity but risk of overfitting/unstable format metrics
  - Dataset size: Larger (>40k) → more compute, potentially lower performance; Smaller (7k) → faster, better results in this setting
  - Algorithm: GRPO → converges slower (57% epoch); Dr.GRPO → faster convergence (17% epoch), similar peak performance

- **Failure signatures:**
  - Training runs too short to observe phase transition (datasets <2k samples)
  - Format reward destabilizing while accuracy reward flat (past optimal stopping point)
  - Using non-distilled base model (reasoning aptitude insufficient for format-only adaptation)
  - LoRA rank too low (<4) or too high (>64) causing under/over-capacity

- **First 3 experiments:**
  1. Baseline reproduction: Train Tina-Open-RS2 with default hyperparameters on 7k Open-RS dataset. Verify phase transition appears around step 450 and checkpoint achieves ~50% average on the six benchmarks.
  2. Ablation on LoRA rank: Run identical setup with ranks [4, 8, 16, 32, 64]. Plot training FLOPs vs. performance; confirm rank 16-32 cluster performs best.
  3. Early stopping validation: Train past the phase transition (full epoch) and evaluate all checkpoints. Confirm performance degrades after format metric destabilization, validating the early-stopping hypothesis.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the observed "phase transition" in LoRA-based RL training dynamics generalize across different base model scales and architectures?
- **Open Question 2:** Can the reasoning skills acquired through LoRA-based RL on mathematical reasoning benchmarks transfer effectively to other domains such as coding?
- **Open Question 3:** What is the precise mechanism by which LoRA efficiently adapts reasoning format while preserving base model knowledge, and can this be quantified?

## Limitations
- The effectiveness of LoRA-based RL depends on the base model having sufficient prior reasoning aptitude, limiting applicability to non-distilled models
- The observed phase transition for early stopping is specific to Tina's experimental conditions and may not generalize to other datasets or base models
- The claim that smaller datasets outperform larger ones contradicts established scaling laws and may be dataset-specific

## Confidence

- **High confidence:** LoRA-based RL achieves competitive reasoning performance with minimal compute and outperforms full-parameter RL baselines
- **Medium confidence:** LoRA rapidly adapts reasoning format while preserving base knowledge (mechanism plausible but not definitively proven)
- **Medium confidence:** Peak performance occurs just before format-related phase transition (empirical observation, needs broader validation)
- **Low confidence:** Smaller datasets can outperform larger datasets for LoRA-based reasoning RL (contradicts scaling literature, requires independent verification)

## Next Checks

1. **Base model dependency test:** Train identical Tina configurations on both distilled (DeepSeek-R1-Distill-Qwen-1.5B) and non-distilled (Qwen2.5-1.5B) base models. Compare format adaptation capacity and final reasoning performance to test the hypothesis that format-only adaptation requires prior reasoning aptitude.

2. **Dataset scaling validation:** Systematically vary dataset sizes from 1k to 100k samples while controlling for quality metrics. Plot performance vs. dataset size to verify if the inverse scaling relationship holds beyond the specific Open-RS2 dataset.

3. **Phase transition reproducibility:** Train Tina on at least three different reasoning datasets (e.g., Open-RS2, DeepScaleR, STILL-3) and plot format reward/completion length curves. Verify that phase transitions occur consistently across datasets and that early stopping at these points generalizes.