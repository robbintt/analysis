---
ver: rpa2
title: On the Unreasonable Effectiveness of Last-layer Retraining
arxiv_id: '2512.01766'
source_url: https://arxiv.org/abs/2512.01766
tags:
- group
- training
- held-out
- neural
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why last-layer retraining (LLR) improves
  worst-group accuracy in the presence of spurious correlations. The authors initially
  hypothesized that neural collapse and implicit bias explain LLR's success, but their
  empirical investigation did not support this hypothesis.
---

# On the Unreasonable Effectiveness of Last-layer Retraining

## Quick Facts
- **arXiv ID**: 2512.01766
- **Source URL**: https://arxiv.org/abs/2512.01766
- **Reference count**: 34
- **Primary result**: Last-layer retraining (LLR) improves worst-group accuracy primarily by leveraging better group balance in the held-out set, not through neural collapse or implicit bias mechanisms.

## Executive Summary
This paper investigates why last-layer retraining (LLR) effectively improves worst-group accuracy in the presence of spurious correlations. The authors initially hypothesized that neural collapse and implicit bias explain LLR's success, but empirical investigation disproved this hypothesis. Instead, they discovered that LLR's effectiveness stems primarily from better group balance in the held-out set. When the held-out set contains proportionally more minority-group samples, retraining the last layer learns a classifier less dominated by majority-group features. The paper demonstrates that LLR performance correlates strongly with the change in group ratio between training and held-out sets (Pearson r > 0.9 on Waterbirds and CivilComments).

## Method Summary
The authors train ERM models on the full/imbalanced training set, then retrain only the last layer on a held-out set using SGD with unregularized logistic loss. They compare different held-out set balancing strategies: subsetting (selecting data with desired group ratios), upsampling (duplicating minority samples), and upweighting (increasing loss weight for minority samples). They also test recent LLR variants like CB-LLR (class-balancing) and AFR (automatic feature reweighting). The key innovation is systematically controlling the group ratio in the held-out set to isolate the effect of balance on worst-group accuracy.

## Key Results
- LLR improves worst-group accuracy only when the held-out set has better group balance than the training set (Pearson r > 0.9 correlation).
- LLR does not improve over ERM when held-out set group ratio ≤ training set group ratio.
- Recent LLR methods like CB-LLR and AFR succeed primarily through implicit group-balancing mechanisms.
- LLR remains effective when group annotations are available only on the held-out set.

## Why This Works (Mechanism)

### Mechanism 1: Held-Out Set Group Balance
- **Claim**: LLR improves worst-group accuracy primarily because the held-out set has better group balance than the training set.
- **Mechanism**: When held-out set contains proportionally more minority-group samples, retraining the last layer learns a classifier less dominated by majority-group features.
- **Core assumption**: Group imbalance in training data causes ERM to overfit to spurious correlations; better-balanced data yields more robust classifiers.
- **Evidence anchors**:
  - [Abstract]: "LLR's success is primarily due to better group balance in the held-out set, often achieved implicitly through class balancing."
  - [Section 4.1]: "LLR only shows significant gains over ERM when it is trained on a held-out set with a higher group ratio than the ERM training set."
- **Break condition**: If held-out set group ratio ≤ training set group ratio, LLR will not improve over ERM.

### Mechanism 2: Class-Balancing Induces Implicit Group-Balancing
- **Claim**: CB-LLR achieves robustness gains because class-balancing implicitly improves group balance.
- **Mechanism**: Equalizing class proportions moves group proportions toward balance since groups are refinements of classes (y × s).
- **Core assumption**: Class-level balancing provides a practical proxy when group labels are unavailable.
- **Evidence anchors**:
  - [Section 4.2]: Figure 5 shows No-CB LLR does not improve over No-CB ERM across all four datasets.
  - [Section 4.3]: "CB-LLR (with any class-balancing method) can recover optimally class-balanced WGA even when ERM was not optimally class-balanced."
- **Break condition**: On naturally class-balanced datasets, class-balancing provides no implicit group improvement.

### Mechanism 3: AFR Loss Reweighting Emulates Group-Balancing
- **Claim**: AFR succeeds because its weighted loss function effectively upweights minority-group samples.
- **Mechanism**: AFR assigns weights based on prediction confidence; since ERM performs poorly on minority groups, these samples receive higher weight.
- **Core assumption**: ERM's prediction errors correlate with minority-group membership.
- **Evidence anchors**:
  - [Section 4.3]: "The AFR held-out set effectively has better group balance than the training set, in the sense of the upweighting method."
  - [Section 2, Methods]: Equation (1) defines AFR weights based on prediction confidence.
- **Break condition**: If ERM achieves high accuracy on minority groups, AFR's upweighting will not preferentially help those groups.

## Foundational Learning

- **Concept: Worst-Group Accuracy (WGA) vs. Average Accuracy**
  - **Why needed here**: LLR targets WGA improvement; average accuracy can remain high while WGA is near-random.
  - **Quick check question**: On a dataset with 90% majority and 10% minority groups, if a model achieves 95% average accuracy but 20% accuracy on the minority, what is its WGA?

- **Concept: Spurious Correlations and Group Structure**
  - **Why needed here**: The paper assumes spurious features correlate with labels in training but not test; understanding group definitions (Y × S) is essential.
  - **Quick check question**: In Waterbirds, species (landbird/waterbird) is the target and background (land/water) is spurious. What four groups arise from this structure?

- **Concept: Neural Collapse (NC₁ metric)**
  - **Why needed here**: The paper tests whether NC₁ explains LLR; understanding this metric helps interpret why the hypothesis was rejected.
  - **Quick check question**: NC₁ measures feature variability collapse within classes. Would you expect NC₁ to be lower on training data or held-out data, and why?

## Architecture Onboarding

- **Component map**: ERM feature extractor -> Last-layer classifier -> Held-out set sampler -> Balance controller
- **Critical path**:
  1. Train ERM model on training set → extract penultimate layer features
  2. Construct held-out set with controlled group ratio (or apply weighting in AFR)
  3. Reinitialize last layer; train via SGD with unregularized logistic loss on held-out features
  4. Evaluate WGA on test set across all groups

- **Design tradeoffs**:
  - **Subsetting**: Simple, stable training, but discards data—can harm performance on very small minority groups
  - **Upsampling**: Uses all data but can cause training instability and WGA degradation
  - **Upweighting**: Computationally efficient but sensitive to noise in minority signals
  - **DFR vs. CB-LLR vs. AFR**: DFR requires group labels (best performance); CB-LLR requires only class labels; AFR requires neither but needs tuning of γ

- **Failure signatures**:
  - LLR degrades WGA when held-out group ratio ≤ training group ratio
  - Upsampling/upweighting causes catastrophic WGA collapse mid-training on CelebA and CivilComments
  - On naturally class-balanced datasets (MultiNLI), ERM may outperform LLR variants

- **First 3 experiments**:
  1. **Baseline replication**: Implement ERM + LLR with no balancing on Waterbirds; verify no improvement over ERM alone
  2. **Group ratio ablation**: Train LLR with held-out group ratios [0.05, 0.1, 0.2, 0.5, 1.0] while keeping training ratio fixed; confirm correlation between WGA and held-out ratio
  3. **Class-balancing method comparison**: Compare subsetting, upsampling, and upweighting for CB-LLR on CelebA; verify subsetting recovers optimal WGA while upsampling/upweighting may degrade

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do neural networks learn both core and spurious features during ERM training?
- **Basis in paper**: [explicit] The Discussion section states: "the fact remains that we have no substantial theoretical justification for this key assumption underlying LLR."
- **Why unresolved**: While recent work has made progress, there is still no substantial theoretical justification for why ERM learns both feature types but relies too heavily on spurious features in the last layer.
- **What evidence would resolve it**: Theoretical analysis characterizing the dynamics of core vs. spurious feature learning during gradient descent, potentially with empirical validation across diverse architectures and datasets.

### Open Question 2
- **Question**: What is the mechanism by which LLR with better group ratio than ERM reweights core and spurious features?
- **Basis in paper**: [explicit] Discussion section: "Addressing this second question is necessary for a complete understanding of LLR, and it is possible this mechanism could be leveraged to design algorithms which target WGA more explicitly."
- **Why unresolved**: The paper empirically demonstrates the importance of group balance but does not characterize the underlying reweighting mechanism in the feature space.
- **What evidence would resolve it**: Analysis of how the last-layer weights evolve relative to core and spurious feature directions during LLR under varying group ratios.

### Open Question 3
- **Question**: Why does neural collapse not occur during standard ERM training on group robustness benchmarks?
- **Basis in paper**: [inferred] The authors' initial hypothesis relied on neural collapse, but Figure 2 shows NC1 remains relatively constant throughout training, contradicting theoretical expectations.
- **Why unresolved**: The paper documents this negative result but does not explain why these benchmarks differ from settings where neural collapse has been observed.
- **What evidence would resolve it**: Systematic study comparing training dynamics, data geometry, and model architectures between benchmarks where collapse occurs versus where it does not.

### Open Question 4
- **Question**: What data characteristics explain edge cases where LLR exceeds optimally class-balanced ERM?
- **Basis in paper**: [inferred] The authors note that in certain edge cases (e.g., CelebA with subsetting ERM/upweighting LLR), LLR exceeds all ERM baselines.
- **Why unresolved**: The phenomenon is observed but not investigated; the specific properties of data points that enable LLR to outperform even optimal ERM remain unidentified.
- **What evidence would resolve it**: Ablation studies analyzing which samples in the held-out set contribute most to robustness gains when LLR outperforms balanced ERM.

## Limitations
- **Mechanistic specificity**: The exact mechanism by which balanced training improves worst-group accuracy remains unclear.
- **Dataset dependency**: Findings primarily validated on four datasets with specific group structures; generalization requires further testing.
- **Approximation validity**: The analysis assumes group imbalance in training causes ERM to overfit to spurious correlations, but this is not rigorously proven for all scenarios.

## Confidence
- **High confidence**: The core finding that LLR's effectiveness correlates with improved group balance in the held-out set (Pearson r > 0.9).
- **Medium confidence**: The explanation that class-balancing methods (CB-LLR) implicitly improve group balance.
- **Medium confidence**: The claim that AFR achieves implicit group-balancing through loss reweighting.

## Next Checks
1. **Mechanism isolation experiment**: Design a synthetic dataset where class balance and group balance can be controlled independently. Test whether LLR's effectiveness depends on class balance, group balance, or both, and quantify their relative contributions.

2. **Cross-dataset generalization**: Apply LLR to datasets with different spurious correlation patterns and different group definitions. Test whether the balance mechanism holds across domains.

3. **Model architecture ablation**: Test whether the balance mechanism generalizes beyond standard CNNs and Transformers. Examine LLR on models with different inductive biases (e.g., vision transformers, attention-based language models) to verify the mechanism's robustness.