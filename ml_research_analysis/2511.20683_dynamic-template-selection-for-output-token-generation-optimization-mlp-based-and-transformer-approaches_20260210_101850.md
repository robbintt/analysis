---
ver: rpa2
title: 'Dynamic Template Selection for Output Token Generation Optimization: MLP-Based
  and Transformer Approaches'
arxiv_id: '2511.20683'
source_url: https://arxiv.org/abs/2511.20683
tags:
- token
- template
- routing
- across
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Dynamic Template Selection (DTS) addresses the problem of token\
  \ inefficiency in LLM deployments by adaptively matching response templates to query\
  \ complexity, reducing output token costs which are 4-8\xD7 more expensive than\
  \ input tokens. The core method uses neural routing to classify queries and select\
  \ from five verbosity-tuned templates, with a dual-layer token control mechanism\
  \ combining soft prompting and hard API-level token caps."
---

# Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches

## Quick Facts
- arXiv ID: 2511.20683
- Source URL: https://arxiv.org/abs/2511.20683
- Authors: Bharadwaj Yadavalli
- Reference count: 20
- Primary result: 32.6-33.9% token reduction with 90.5% routing accuracy across three LLM providers

## Executive Summary
Dynamic Template Selection (DTS) addresses the problem of token inefficiency in LLM deployments by adaptively matching response templates to query complexity, reducing output token costs which are 4-8× more expensive than input tokens. The core method uses neural routing to classify queries and select from five verbosity-tuned templates, with a dual-layer token control mechanism combining soft prompting and hard API-level token caps. Across 9,000 production API calls on 1,000 MMLU questions, DTS achieved 90.5% routing accuracy and reduced output tokens by 32.6-33.9% across three major LLM providers, maintaining 100% success rates while significantly lowering costs.

## Method Summary
DTS employs a two-stage classification system where queries are first categorized by complexity (using either MLP or Transformer architectures) and then routed to one of five predefined templates optimized for different verbosity levels. The system implements a dual-layer token control mechanism: soft prompting through template instructions and hard API-level token caps. The approach was evaluated on 1,000 MMLU benchmark questions across three major LLM providers, measuring routing accuracy, token reduction, and cost savings while maintaining response quality.

## Key Results
- 90.5% routing accuracy across all tested LLM providers
- 32.6-33.9% reduction in output tokens compared to standard responses
- Maintained 100% success rates while achieving 33.9% cost reduction on GPT-4 API
- Consistent performance across three major LLM providers (GPT-3.5, GPT-4, Claude)

## Why This Works (Mechanism)
The effectiveness of DTS stems from matching response verbosity to query complexity through learned routing. Simple queries receive concise responses via minimal templates, while complex queries get detailed explanations through verbose or technical templates. The dual-layer control (soft prompting + hard caps) ensures responses stay within optimal token ranges. This targeted approach reduces unnecessary token generation that occurs when using one-size-fits-all templates or no templates at all.

## Foundational Learning
- **Query Complexity Classification**: Neural routing needs to distinguish between simple, moderate, and complex queries to select appropriate templates. Quick check: Verify classification accuracy on held-out query sets.
- **Template Verbosity Optimization**: Different templates must balance informativeness against token efficiency. Quick check: Compare token usage vs. response quality across templates.
- **Dual-Layer Token Control**: Soft prompting guides response style while hard caps prevent excessive generation. Quick check: Test response truncation behavior at token limits.
- **Cross-Provider Consistency**: The method must work across different LLM architectures and pricing models. Quick check: Validate token reduction percentages across providers.

## Architecture Onboarding

**Component Map**: Query -> Complexity Classifier (MLP/Transformer) -> Template Router -> LLM API (with Token Cap)

**Critical Path**: Query Input → Complexity Classification → Template Selection → API Call with Token Cap → Response Generation

**Design Tradeoffs**: Manual template design vs. learned templates; complexity classification granularity; soft vs. hard token control balance; latency vs. token savings.

**Failure Signatures**: Misclassification leading to overly terse or verbose responses; token cap truncation cutting off important information; template routing failures; increased latency from routing decisions.

**First Experiments**:
1. Test routing accuracy on held-out MMLU questions with varying complexity levels
2. Measure token usage and response quality for each template type independently
3. Validate dual-layer control by comparing responses with only soft prompting vs. only hard caps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DTS routing principles generalize across languages and cultural contexts?
- Basis in paper: [explicit] "Investigating DTS applicability beyond English-language contexts presents important challenges. Cross-linguistic evaluation would test whether template selection principles generalize across languages and cultural contexts, potentially requiring language-specific adaptations or universal routing strategies."
- Why unresolved: Only English-language MMLU benchmark was evaluated; language-specific response patterns and cultural communication norms may affect template appropriateness and routing boundaries.
- What evidence would resolve it: Evaluation on multilingual benchmarks (e.g., mMMLU, XGLUE) measuring whether semantic routing decisions transfer directly or require language-specific model adaptations.

### Open Question 2
- Question: Can optimal templates be learned automatically from usage data rather than manually specified?
- Basis in paper: [explicit] "Our template design process currently requires manual specification and tuning. We could also have the system learn what makes a good template by analyzing lots of real conversations."
- Why unresolved: The five templates (minimal, standard, verbose, technical, executive) were manually designed; optimal template granularity, verbosity levels, and structural content remain unexplored.
- What evidence would resolve it: Automated template discovery pipeline clustering response patterns from production data, comparing learned templates against manually-designed ones on cost-quality tradeoffs.

### Open Question 3
- Question: Can DTS be extended to route between different LLM models based on query complexity?
- Basis in paper: [explicit] "We could also expand this to route between different LLMs entirely, not just templates. Imagine sending simple questions to cheaper models and complex ones to more powerful (and expensive) models—that could really optimize the cost-performance balance."
- Why unresolved: Current DTS only selects templates within a single LLM; model-level routing introduces additional complexity from capability differences across model tiers.
- What evidence would resolve it: Framework jointly routing queries to (model, template) pairs, evaluated on cost-quality Pareto frontier across model tiers (e.g., GPT-4o-mini vs GPT-4o).

### Open Question 4
- Question: Does DTS maintain effectiveness on non-academic domains such as legal reasoning, medical consultation, or creative writing?
- Basis in paper: [explicit] "The dataset's focus on academic domains potentially limits insights into system performance on creative writing tasks, specialized professional contexts (medical or legal), or production query distributions encountered in deployed systems."
- Why unresolved: MMLU covers academic knowledge; professional and creative tasks may exhibit different quality-cost tradeoffs and template requirements.
- What evidence would resolve it: Evaluation on domain-specific benchmarks (LegalBench, MedQA, creative writing datasets) measuring routing accuracy and token reduction generalization.

## Limitations
- Reliance on MMLU benchmark may not represent real-world production query diversity
- Performance gains demonstrated under controlled conditions with predefined templates
- Dual-layer token control adds deployment complexity without addressing potential latency
- Manual template design process may not scale to diverse use cases

## Confidence
- **High Confidence**: Core token reduction claim (32.6-33.9%) well-supported by experimental results across three providers
- **Medium Confidence**: Routing accuracy (90.5%) demonstrated but may vary with different query distributions
- **Medium Confidence**: Cost savings dependent on specific pricing models which may change over time

## Next Checks
1. Test DTS performance on real-world production query logs to assess generalizability beyond MMLU benchmark questions
2. Measure end-to-end latency impact of the routing decision process to ensure token savings aren't offset by increased response time
3. Conduct ablation studies removing either the soft prompting layer or the hard API-level token cap to quantify marginal benefits