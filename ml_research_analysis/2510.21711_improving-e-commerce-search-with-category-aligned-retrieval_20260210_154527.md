---
ver: rpa2
title: Improving E-commerce Search with Category-Aligned Retrieval
arxiv_id: '2510.21711'
source_url: https://arxiv.org/abs/2510.21711
tags:
- category
- queries
- search
- accuracy
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Category-Aligned Retrieval System (CARS) that
  predicts product categories from user queries and boosts products in those categories
  to improve e-commerce search relevance. The core method creates "Trainable Category
  Prototypes" by computing weighted averages of query embeddings for each category,
  combining them with category name embeddings.
---

# Improving E-commerce Search with Category-Aligned Retrieval

## Quick Facts
- arXiv ID: 2510.21711
- Source URL: https://arxiv.org/abs/2510.21711
- Reference count: 5
- Primary result: Category prediction accuracy reaches 83.2% Top-3 but naive boosting degrades nDCG@10

## Executive Summary
This paper introduces a Category-Aligned Retrieval System (CARS) that improves e-commerce search by predicting product categories from user queries and boosting products in those categories. The system creates "Trainable Category Prototypes" by computing weighted averages of query embeddings for each category, then interpolating with category name embeddings. While achieving high offline category prediction accuracy (83.2% Top-3 with OpenAI's text-embedding-ada-002), the study reveals that naive category boosting can actually harm search relevance metrics like nDCG@10, highlighting the need for confidence-aware integration strategies.

## Method Summary
The method constructs category prototypes by aggregating query embeddings weighted by their category probabilities, then blending with category name embeddings using an interpolation factor α=0.85. These prototypes are indexed in Elasticsearch for k-NN retrieval to predict top-K categories for each query. The system then boosts products in predicted categories during retrieval. Evaluated on Amazon ESCI dataset with 10,000 queries across 112 Level-1 categories, the approach shows high offline accuracy but reveals that naive boosting degrades online relevance metrics due to asymmetric error amplification.

## Key Results
- Achieves 83.2% Top-3 category prediction accuracy using OpenAI's text-embedding-ada-002
- Offline accuracy improves significantly over zero-shot baselines
- Simulated online evaluation shows naive category boosting degrades nDCG@10 and Reciprocal Rank
- Qualitative analysis reveals "catastrophic errors" where incorrect predictions cause relevant products to disappear from top results

## Why This Works (Mechanism)

### Mechanism 1: Intent-Aligned Prototype Construction
- **Claim:** Category representations derived from aggregated user queries capture colloquial intent better than static category names alone.
- **Mechanism:** Constructs "Trainable Category Prototypes" by computing probability-weighted average of embeddings from historical queries associated with each category, then interpolating with category name embedding using factor α=0.85.
- **Core assumption:** Assumes semantic center of user queries for a category is more robust retrieval target than formal category name.
- **Evidence anchors:** [abstract] "creates 'Trainable Category Prototypes' by computing weighted averages of query embeddings... combining them with category name embeddings."
- **Break condition:** Fails when query patterns are sparse or noisy for a specific category, resulting in prototype that drifts from actual product intent.

### Mechanism 2: Hybrid Semantic Interpolation
- **Claim:** Combining data-driven query vectors with static name vectors provides stability against sparse training data while maximizing semantic alignment.
- **Mechanism:** Uses formula $\vec{v}_{hybrid}(C) = \alpha \cdot \vec{v}_{query}(C) + (1 - \alpha) \cdot \vec{e}_{name}(C)$ to ensure fallback to name embedding when training data is lacking.
- **Core assumption:** Assumes category name itself carries semantic value that complements query aggregation.
- **Evidence anchors:** [section 3.2] Describes interpolation factor and fallback behavior for categories with no training data.
- **Break condition:** If category name is polysemous or misleading, the $(1-\alpha)$ component may introduce noise.

### Mechanism 3: Asymmetric Error Amplification in Boosting
- **Claim:** Naive boosting strategies amplify negative impact of incorrect predictions more than benefits from correct predictions, degrading overall relevance.
- **Mechanism:** Strong boost applied to documents in Top-K predicted categories. When model errs, boost actively suppresses correct (un-boosted) products in ranking.
- **Core assumption:** Assumes nDCG@10 is sensitive to relative ranking of irrelevant boosted items vs. relevant non-boosted items.
- **Evidence anchors:** [section 5.1] Table 3 shows statistically significant degradation in nDCG@10 and Reciprocal Rank.
- **Break condition:** This mechanism describes failure mode; it "works" in explaining why high-accuracy model failed in production simulation.

## Foundational Learning

- **Concept: Weighted Vector Aggregation**
  - **Why needed here:** Core method (Eq. 1) relies on computing "center of mass" for category based on associated queries.
  - **Quick check question:** If 90% of queries for "Apple" relate to fruit and 10% to electronics, will simple average vector point closer to "fruit" or "tech"?

- **Concept: Information Retrieval Metrics (nDCG & Reciprocal Rank)**
  - **Why needed here:** Paper's central paradox is that accuracy improved but nDCG degraded.
  - **Quick check question:** Why does moving relevant product from rank 1 to rank 2 hurt nDCG less than moving irrelevant product from rank 10 to rank 1?

- **Concept: Polysemy and Zero-Shot Generalization**
  - **Why needed here:** Failure cases (e.g., "mandoline," "little trees") hinge on words having multiple meanings not captured in training.
  - **Quick check question:** How does model handle query like "Delta" if training only contains airline examples but user wants faucet?

## Architecture Onboarding

- **Component map:** Query Embedder -> Prototype Store -> Category Predictor -> Retrieval Engine
- **Critical path:** Prototype Generation (Training) stage. If map of {query → category probability} is noisy or interpolation weight α is wrong, downstream k-NN prediction will be systematically biased.
- **Design tradeoffs:**
  - High α (0.85) vs. Low α: High prioritizes user behavior (good for colloquialisms) but risks overfitting to specific phrasings.
  - Boosting vs. Filtering: Paper used "powerful boost" (soft constraint); hard filtering would likely have destroyed recall entirely.
- **Failure signatures:**
  - "Polysemy Trap": High-confidence prediction of plausible but incorrect category causing relevant products to vanish.
  - "Brand-Semantic Mismatch": Semantic similarity overrides actual brand intent.
  - Metric Divergence: Offline accuracy increasing while online nDCG decreases.
- **First 3 experiments:**
  1. **Confidence Thresholding:** Implement "Confidence-Based Adaptive Boosting" to see if cosine similarity threshold prevents nDCG drop.
  2. **Alpha Sensitivity Analysis:** Re-train prototypes with α ∈ [0.5, 0.7, 0.9] to reduce catastrophic errors for ambiguous terms.
  3. **Error Analysis on Top-K:** Instead of boosting, record Top-5 predicted categories for problematic queries to determine if re-ranking approach might be better.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can confidence-aware integration strategy successfully modulate boost strength to improve nDCG@10, or does "catastrophic" penalty of single errors always outweigh aggregate gains?
- **Basis in paper:** [explicit] Section 7 proposes "Confidence-Based Adaptive Boosting" suggesting future work should modulate boosts based on cosine similarity.
- **Why unresolved:** Current study only evaluated "naive" integration where boosts were applied blindly to top predicted categories.
- **What evidence would resolve it:** Simulation comparing uniform boosting against dynamic strategy that reduces boost intensity for low-confidence predictions.

### Open Question 2
- **Question:** Can pre-filtering layer effectively identify and route "inherently ambiguous" queries to spare them from category-based degradation?
- **Basis in paper:** [explicit] Section 7 calls for "Explicit Ambiguity Detection" to flag non-transactional or polysemous queries.
- **Why unresolved:** Paper identified these failure modes through post-hoc manual analysis but did not develop automated mechanism.
- **What evidence would resolve it:** Ablation study showing removing bottom decile of "ambiguous" queries results in net positive gain in mean nDCG@10.

### Open Question 3
- **Question:** Does utilizing full product category hierarchy (Level 2+) for prediction and regularization offer better robustness than Level 1 truncation?
- **Basis in paper:** [explicit] Section 7 notes current method truncates paths to Level 1 and suggests future work could predict deeper levels.
- **Why unresolved:** Methodology deliberately simplified target space to 112 root categories.
- **What evidence would resolve it:** Evaluation measuring whether constraining boosts to deeper, more specific categories reduces false positives compared to broad root-category boosting.

## Limitations

- Fragility of category prediction step: High offline accuracy but simulated online evaluation shows naive boosting can harm search relevance through incorrect category boosts
- Critical unknown: Exact ground truth probability calculation method (p(C|q) derivation from query-product relevance labels) is not specified
- α interpolation weight (0.85) appears heuristic rather than empirically validated across different datasets or domains

## Confidence

- **High Confidence:** Mechanism explaining why naive boosting fails (asymmetric error amplification) is well-supported by evidence showing nDCG degradation despite high accuracy
- **Medium Confidence:** Claim that hybrid interpolation (α=0.85) provides optimal balance between user intent capture and stability lacks direct empirical validation
- **Low Confidence:** Generalizability of findings beyond Amazon ESCI dataset to other e-commerce domains or different product taxonomies remains untested

## Next Checks

1. **Confidence Threshold Validation:** Implement suggested confidence-based adaptive boosting and measure if cosine similarity threshold (>0.9) prevents nDCG drop by falling back to baseline for ambiguous queries

2. **Alpha Sensitivity Analysis:** Systematically evaluate prototype quality across different α values (0.5, 0.7, 0.9) to determine if lower reliance on noisy user query aggregations reduces catastrophic errors

3. **Error Pattern Analysis:** For identified problematic queries (e.g., "mandoline," "little trees"), analyze whether correct category appears in Top-5 predictions but not Top-1, suggesting re-ranking approaches might be more effective than boosting