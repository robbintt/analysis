---
ver: rpa2
title: Do Large Language Models Judge Error Severity Like Humans?
arxiv_id: '2506.05142'
source_url: https://arxiv.org/abs/2506.05142
tags:
- error
- human
- llms
- multimodal
- colour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically compares human and large language model
  (LLM) judgments of error severity in image descriptions, examining four error types:
  age, gender, clothing type, and clothing colour. Humans rated descriptions in both
  unimodal (text-only) and multimodal (text + image) conditions, revealing that visual
  context significantly increased perceived severity for colour and type errors.'
---

# Do Large Language Models Judge Error Severity Like Humans?

## Quick Facts
- arXiv ID: 2506.05142
- Source URL: https://arxiv.org/abs/2506.05142
- Reference count: 12
- Key outcome: LLMs show systematic divergence from human error severity judgments in image descriptions, with most models disproportionately penalizing color errors while humans judge gender errors as most severe.

## Executive Summary
This study systematically compares human and large language model (LLM) judgments of error severity in Chinese image descriptions across four error types: age, gender, clothing type, and clothing color. Humans rated descriptions in both unimodal (text-only) and multimodal (text + image) conditions, revealing that visual context significantly increased perceived severity for color and type errors. While most LLMs assigned low scores to gender errors but disproportionately high scores to color errors, humans judged both as highly severe but for different reasons—social norms for gender and specialized neural mechanisms for color. Among the evaluated models, Doubao replicated the human-like severity ranking, but DeepSeek-V3, a unimodal LLM, achieved the highest correlation with human judgments across both conditions, outperforming even multimodal models.

## Method Summary
The study used 9 images (5 from MS COCO, 4 from internet sources) each paired with 1 reference description and 4 erroneous descriptions (one per error type). Twenty-five human participants (14 male, 11 female, mean age 22.92) rated 64 experimental stimuli (after excluding 8 practice stimuli) using 0-100 quality scales in both unimodal and multimodal conditions. Four LLMs (GPT-4o, Doubao, DeepSeek-V3-0324, DeepSeek-R1) were evaluated using provided Chinese prompts, run 3 times per stimulus. Statistical analysis employed Linear Mixed-Effects Models with Tukey's HSD and Holm-Bonferroni post-hoc tests, along with Pearson and Spearman correlations.

## Key Results
- Most LLMs assigned low scores to gender errors but disproportionately high scores to color errors, unlike humans
- Visual context significantly increased human severity ratings for color and type errors (interaction effect p=.014)
- DeepSeek-V3, a unimodal LLM, achieved the highest alignment with human judgments across both conditions
- GPT-4o showed no significant interaction between error type and visual context (p=.677), unlike humans

## Why This Works (Mechanism)
The study reveals fundamental differences in how humans and LLMs process error severity in image descriptions. Humans show significant interaction effects between visual context and error type, with color and type errors becoming more severe when visual context is available, suggesting that human judgment relies on multimodal integration and perceptual grounding. LLMs, particularly multimodal ones, fail to replicate this pattern, instead showing uniform sensitivity across error types regardless of visual input. The study suggests this divergence stems from LLMs' lack of specialized neural mechanisms for color processing and their reliance on learned associations rather than perceptual grounding. The finding that unimodal LLMs outperform multimodal ones in alignment with human judgments suggests that text-only references may provide cleaner comparison signals than the potentially distracting visual features available to multimodal models.

## Foundational Learning
**Linear Mixed-Effects Models (LMM)**: Statistical framework for analyzing data with multiple sources of random variation (why needed: to account for repeated measures across participants and stimuli; quick check: verify model includes random intercepts for participants and items)
**Tukey's HSD**: Post-hoc test for comparing all possible pairs of means while controlling family-wise error rate (why needed: to identify which specific error types differ significantly; quick check: examine adjusted p-values for pairwise comparisons)
**Holm-Bonferroni correction**: Sequential method for multiple hypothesis testing that controls family-wise error rate (why needed: to maintain statistical validity when conducting multiple post-hoc tests; quick check: verify corrected p-values are reported)
**Multimodal vs Unimodal processing**: Distinction between models that process visual and textual inputs separately versus jointly (why needed: to understand why visual context affects human but not LLM judgments; quick check: compare correlation coefficients between model types)
**Error severity ranking**: Systematic ordering of error types by their perceived impact on description quality (why needed: to establish baseline human judgments for LLM comparison; quick check: verify ranking consistency across experimental conditions)

## Architecture Onboarding
**Component Map**: Human participants -> Rating interface -> Quality scores -> LMM analysis; LLMs -> Prompt processing -> Quality scores -> Correlation analysis -> Comparison to human baseline
**Critical Path**: Image selection → Reference/erroneous description creation → Human experiment → LLM experiment → Statistical analysis → Interpretation
**Design Tradeoffs**: Controlled experimental design with limited stimuli vs. ecological validity; Chinese language focus vs. cross-linguistic generalizability; within-subjects design with counterbalanced order vs. potential carryover effects
**Failure Signatures**: LLM assigning disproportionately high scores to color errors (>40) while humans rate gender errors as most severe; lack of significant interaction between error type and visual context in multimodal LLMs
**3 First Experiments**: 1) Replicate human experiment with larger, more diverse participant pool; 2) Test additional LLM models and prompt variations; 3) Conduct cross-linguistic validation with translated stimuli

## Open Questions the Paper Calls Out
**Open Question 1**: Why do unimodal LLMs (e.g., DeepSeek-V3) achieve higher correlation with human judgments in multimodal NLG evaluation than multimodal LLMs that have access to the visual input? The paper reports this counterintuitive finding but does not investigate the underlying mechanisms—whether multimodal models are distracted by visual features, or whether text-only references provide cleaner comparison signals.

**Open Question 2**: What architectural or training modifications would enable LLMs to develop human-like sensitivity to color errors, which appears rooted in specialised neural mechanisms? The study identifies the gap but does not explore whether vision-language pretraining, fine-grained color tokenisation, or explicit color comparison objectives could bridge it.

**Open Question 3**: To what extent are the observed severity rankings (GENDER ≺ COLOUR ≺ AGE ≺ TYPE) generalizable across languages, cultures, and image domains beyond MSCOCO-style images? The study is conducted entirely in Chinese with images primarily from MSCOCO, and notes that severity rankings differ from van Miltenburg et al. (2020).

**Open Question 4**: Why does Doubao replicate the human-like severity ranking but fail to statistically distinguish between error types as clearly as humans? The paper describes this behavior but does not investigate whether it stems from Doubao's scoring calibration, its internal uncertainty quantification, or architectural properties.

## Limitations
- Small sample size (25 human participants) limits generalizability to broader populations
- Use of only 9 images with carefully crafted erroneous descriptions restricts ecological validity
- Focus on Chinese language descriptions limits cross-linguistic applicability

## Confidence
- **High**: Observed differences between human and LLM severity judgments, particularly regarding disproportionate scoring of color errors by LLMs
- **Medium**: Ranking of LLMs by their correlation with human judgments (depends on specific prompts and model versions)
- **Low**: Theoretical explanations regarding neural mechanisms for color processing (speculative interpretations not directly tested)

## Next Checks
1. Replicate the study with a larger, more diverse participant pool and additional images to test robustness of human-LLM divergence patterns
2. Test additional LLM models and prompt variations to determine whether severity rating patterns are consistent across different architectures and training approaches
3. Conduct cross-linguistic validation by translating stimuli and procedures to other languages to assess whether human-LLM divergence patterns hold across cultural contexts