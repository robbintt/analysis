---
ver: rpa2
title: Correlation Dimension of Auto-Regressive Large Language Models
arxiv_id: '2510.21258'
source_url: https://arxiv.org/abs/2510.21258
tags:
- correlation
- dimension
- language
- text
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces correlation dimension, a fractal-geometric
  measure, to quantify the long-range structural complexity of text as perceived by
  autoregressive language models. The method computes recurrences in sequences of
  next-token log-probability vectors, capturing hierarchical recurrence structures
  that bridge local and global textual properties.
---

# Correlation Dimension of Auto-Regressive Large Language Models

## Quick Facts
- arXiv ID: 2510.21258
- Source URL: https://arxiv.org/abs/2510.21258
- Reference count: 40
- Primary result: Correlation dimension computed from next-token log-probability vectors reliably measures long-range structural complexity of text and detects multiple forms of degeneration (repetition, incoherence, blandness) and hallucination tendencies in LLMs.

## Executive Summary
This paper introduces correlation dimension, a fractal-geometric measure from chaos theory, to quantify the long-range structural complexity of text as perceived by autoregressive language models. The method computes recurrences in sequences of next-token log-probability vectors, capturing hierarchical recurrence structures that bridge local and global textual properties. Experiments reveal three distinct training phases in LLMs, show context-dependent complexity, indicate hallucination tendencies, and detect multiple degeneration modes beyond perplexity. Correlation dimension is computationally efficient, robust to 4-bit quantization, and applicable across architectures like Transformer and Mamba.

## Method Summary
The method extracts next-token log-probability vectors from autoregressive models, computes pairwise Euclidean distances between these vectors across time, and estimates the correlation dimension as the scaling exponent of the correlation integral S(ε) ∝ ε^d. For long sequences, a moving-window approach with fixed context is used, while short sequences use unlimited context. Vocabulary reduction via modulo projection accelerates computation with minimal accuracy loss. The dimension is estimated via linear regression on log-log plots of S(ε) vs ε within a valid range determined by sequence length.

## Key Results
- Correlation dimension reliably detects repetition (collapses to ~2), incoherence, and blandness in generated text, outperforming perplexity
- Natural language exhibits a stable dimension around 6.5 across languages, model architectures, and domains
- Three distinct training phases are revealed: initial bigram learning, emergence of long-range dependencies, and context compression for generalization
- Hallucination correlates with lower dimension on knowledge-intensive texts, as factual recall requires higher-dimensional trajectories

## Why This Works (Mechanism)

### Mechanism 1: Log-Probability Recurrence Structures Capture Hierarchical Text Complexity
The method captures self-similar recurrence patterns in log-probability vectors that encode both local token-level predictions and global textual structure. By computing correlation integrals over these vectors, the approach reveals how textual skips could theoretically be omitted without significantly altering generation, providing a unified measure bridging local and global properties.

### Mechanism 2: Three-Phase Training Dynamics Reveal Nonlinear Generalization
Pre-training exhibits three phases detectable via correlation dimension: initial rapid dimension drop as models learn bigram structures, dimension increase as longer-range dependencies emerge, and gradual decline via context compression indicating improved generalization. Smaller models show inverted third phase with sudden dimension increases correlating with in-context learning failure.

### Mechanism 3: Dimensional Collapse Signals Degeneration and Hallucination
Text degeneration manifests as collapse from high-dimensional trajectories to low-dimensional attractors. Normal text maintains dimension ~6.5; repetitive patterns collapse to ~2; incoherent/bland text shows intermediate drops. Models recalling factual knowledge engage long-range dependencies (high dimension), while hallucination relies on format-level imitation (low dimension).

## Foundational Learning

- Concept: **Correlation dimension from chaos theory**
  - Why needed here: Core mathematical foundation for quantifying how recurrence frequency scales with distance threshold in phase space reconstructions
  - Quick check question: Given S(ε) ∝ ε^d with S(0.01) = 100 and S(0.1) = 1000, what is d? (Answer: 1)

- Concept: **Takens' embedding theorem and sufficiency of partial observations**
  - Why needed here: Explains why single-step log-probabilities suffice without time-delayed embeddings; Appendix E validates empirically
  - Quick check question: Why might next-token probabilities alone encode long-range structure? (Hint: knowledge distillation literature)

- Concept: **LLM autoregressive probability distributions**
  - Why needed here: Understanding what log-probability vectors represent — distributions over vocabulary conditioned on context
  - Quick check question: What is x_t(ω) in Equation 3, and why use log-space?

## Architecture Onboarding

- Component map: Log-probability extraction -> Distance computation -> Correlation integral -> Dimension estimation
- Critical path: Extract FP32 log-prob vectors, compute pairwise distances using fused kernel, accumulate counts atomically, estimate slope via linear regression
- Design tradeoffs: Context length affects dimension (short → ~3, long → convergence); vocabulary reduction (v=10,000) gives 10× speedup with <2% error; sequence length requires adjusting η parameter
- Failure signatures: Dimension > 10 suggests random text; dimension < 2 on normal text indicates numerical precision issues; large variance suggests quantization inconsistencies
- First 3 experiments: 1) Validate on SEP dataset (~6.5 for natural language), 2) Test repetition detection (dimension ~2 for "01" patterns), 3) Analyze training checkpoints for three-phase pattern

## Open Questions the Paper Calls Out

1. What are the formal theoretical guarantees for correlation dimension estimation from finite sequences, and how do estimation biases scale with sequence length and vocabulary size?
2. Can correlation dimension be extended to conditional generation settings and multi-modal models (e.g., vision-language models)?
3. Why does natural language exhibit a correlation dimension of approximately 6.5 across diverse languages and model architectures?
4. What causes the three-stage evolution pattern during pretraining, and why do smaller models fail to reach the third compression stage?

## Limitations

- The three-phase training dynamics were only observed in Pythia models with early checkpoint access and may not generalize
- Hallucination detection claims rest on a single case study with "process-theism" text, raising concerns about overfitting to specific domains
- The correlation dimension's stability around 6.5 for natural language requires verification across diverse languages and domains beyond those tested

## Confidence

**High Confidence**: Computational method is reproducible; degeneration detection claims are statistically significant across multiple model families; computational efficiency and quantization robustness are empirically validated.

**Medium Confidence**: Three-phase training dynamics need broader validation; correlation dimension's sufficiency for capturing hierarchical complexity lacks rigorous mathematical justification; context-dependent complexity findings need larger-scale validation.

**Low Confidence**: Hallucination detection based on single-domain case studies lacks sufficient evidence; generalizability of 6.5 dimension value remains unproven; superiority over perplexity needs more systematic comparison.

## Next Checks

1. **Cross-linguistic validation**: Compute correlation dimension for 10+ languages across different language families using multilingual models like mBERT or XGLM. Verify whether dimension values consistently cluster around 6.5 or show systematic variations by language type.

2. **Hallucination detection generalization**: Create a systematic hallucination benchmark with 50+ knowledge-intensive prompts spanning diverse domains. Generate responses from multiple models with varying hallucination propensities and test whether correlation dimension below 5.0 consistently predicts hallucination across all domains.

3. **Time-delayed embedding comparison**: Implement the proposed time-delayed embedding approach and systematically compare correlation dimension estimates against the single-step method across 100+ diverse texts. Quantify the difference in dimension values and test whether the improvement justifies the increased computational cost.