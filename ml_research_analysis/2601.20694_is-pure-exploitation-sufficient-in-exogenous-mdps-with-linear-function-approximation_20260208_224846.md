---
ver: rpa2
title: Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?
arxiv_id: '2601.20694'
source_url: https://arxiv.org/abs/2601.20694
tags:
- regret
- exogenous
- state
- learning
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pure Exploitation Learning (PEL) for Exogenous
  Markov Decision Processes (Exo-MDPs), demonstrating that exploration is unnecessary
  in these settings. PEL algorithms fit value approximations from observed exogenous
  traces and act greedily, achieving near-optimal regret bounds.
---

# Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?

## Quick Facts
- arXiv ID: 2601.20694
- Source URL: https://arxiv.org/abs/2601.20694
- Reference count: 40
- One-line primary result: PEL algorithms achieve near-optimal regret bounds in Exo-MDPs without exploration by leveraging independent exogenous state evolution

## Executive Summary
This paper demonstrates that exploration is unnecessary for regret minimization in Exogenous Markov Decision Processes (Exo-MDPs) where the exogenous state evolves independently of the agent's actions. The authors introduce Pure Exploitation Learning (PEL), which estimates the exogenous transition model from observed traces and acts greedily based on counterfactual value estimates. For tabular Exo-MDPs, PEL achieves O(H²|Ξ|√K) regret, while for continuous endogenous states with linear function approximation, the regret is polynomial in feature dimension, exogenous state space, and horizon. The key insight is that single trajectories suffice for unbiased policy evaluation when the exogenous process is action-independent.

## Method Summary
The method involves estimating the exogenous transition model P̂ from observed traces, then performing value iteration backward in time using least-squares approximation on a set of anchor states. The algorithm computes post-decision states to disentangle actions from exogenous noise, enabling stable value function approximation. For tabular cases, PTO estimates P̂ via empirical counts and solves the Bellman equation greedily. For LFA, LSVI-PE uses anchor features and ensures Bellman-closed feature transport to maintain well-conditioned regression problems. The algorithm acts greedily at each step based on the estimated value function.

## Key Results
- PEL achieves O(H²|Ξ|√K) regret in tabular Exo-MDPs without exploration
- LSVI-PE with LFA achieves polynomial regret scaling in feature dimension, exogenous state space, and horizon
- Experiments show PEL consistently outperforms exploration-based baselines on synthetic and resource-management tasks
- Theoretical analysis proves that exploration is unnecessary when exogenous processes are action-independent

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PEL avoids exploration because the exogenous process ξ evolves independently of actions, allowing single trajectories to generate unbiased counterfactual value estimates.
- **Mechanism:** Since ξ ~ P(·|ξ_h) doesn't depend on a_h, observing one demand sequence allows retrospective simulation of any action sequence's outcome. The learner acts greedily but evaluates using the empirical exogenous trace distribution.
- **Core assumption:** Exogenous transition P(ξ'|ξ) is independent of actions.
- **Evidence anchors:** Abstract states "exogenous inputs evolve independently"; Section 3 defines ξ as Markov process independent of x_h and a_h.
- **Break condition:** If ξ depends on a_h, counterfactual estimation becomes biased causing linear regret.

### Mechanism 2
- **Claim:** LSVI-PE uses post-decision states to disentangle immediate actions from subsequent exogenous realization.
- **Mechanism:** Standard Q(s,a) confounds action choice with next state randomness. LSVI-PE introduces deterministic post-decision state x^a_h = f^a(x_h,a_h), splitting value into immediate reward and stochastic post-decision value V^a(x^a,ξ).
- **Core assumption:** Transition to x_{t+1} is deterministic given x^a_h and ξ_{t+1}.
- **Evidence anchors:** Section 5 introduces post-decision states to "remove confounding between actions and exogenous noise."
- **Break condition:** If x_{t+1} has inherent noise outside ξ, value targets become noisy degrading regression.

### Mechanism 3
- **Claim:** Sublinear regret under LFA via Bellman-closed feature transport ensuring value estimates propagate accurately.
- **Mechanism:** LSVI-PE enforces that features of next state φ(x_{h+1}) lie in linear span of anchor features via transport matrix M. This ensures well-conditioned least-squares updates even when acting greedily.
- **Core assumption:** Feature map φ and dynamics satisfy anchor-closed transport (Assumption 2/4).
- **Evidence anchors:** Section 5.2 states Bellman-closed transport allows greedy policies to have accurate value estimates.
- **Break condition:** If true value functions require features outside anchor span, regret scales linearly with K.

## Foundational Learning

- **Concept:** Exogenous vs. Endogenous States
  - **Why needed here:** The entire theoretical result collapses if you cannot distinguish between controllable system state x and uncontrollable environmental noise ξ.
  - **Quick check question:** Can I identify a variable that evolves via Markov chain regardless of agent actions? (e.g., market price vs. inventory)

- **Concept:** Post-Decision States (or Afterstates)
  - **Why needed here:** This architectural trick allows LSVI-PE to handle continuous action spaces by reducing Q(s,a) learning to V(s_after,ξ) learning.
  - **Quick check question:** If I commit to action a but freeze time before world changes, is resulting state deterministic?

- **Concept:** Least-Squares Value Iteration (LSVI)
  - **Why needed here:** You must understand projecting Bellman targets onto linear feature space φ(x) for backward pass implementation.
  - **Quick check question:** Do you know how to compute weights w such that φ(x)^⊤w ≈ y(x) for targets y?

## Architecture Onboarding

- **Component map:** Exogenous Model P̂ -> Anchors -> Value Weights w -> Greedy Policy
- **Critical path:** The Backward Pass (Lines 5-15 in Algorithm 1). If targets y_k(n;ξ) are not constructed correctly using updated P̂ and next-step weights w_{h+1}, greedy policy acts on nonsensical values.
- **Design tradeoffs:**
  - Anchor Selection: Fewer anchors reduce computation (N) but increase approximation error or worsen conditioning (λ_0)
  - Function Approximation: Complex features reduce Bellman error but make feature transport assumption harder to satisfy
- **Failure signatures:**
  - Linear Regret: If exogenous process is action-dependent, PEL fails. Check Corollary 1/2.
  - Exploding Norms: If λ_0 ≈ 0 (anchors don't span space), weights w blow up.
- **First 3 experiments:**
  1. Tabular Sanity Check: Implement PTO on 5x5 Exo-MDP to verify P̂ converges without exploration
  2. Storage Control LFA: Replicate Storage Control experiment using hat-basis features, compare LSVI-PE vs LSVI-Opt
  3. Ablation on Anchors: Vary N and observe impact on cumulative regret to validate √N/λ_0 dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PEL be extended to handle continuous or partially observed exogenous processes while preserving sample efficiency?
- Basis in paper: [explicit] Conclusion states future work includes "relax structural assumptions (richer function classes, continuous or partially observed exogenous processes) while preserving exploitation's sample efficiency."
- Why unresolved: Current analysis requires discrete exogenous state spaces for tractable transition estimation; continuous or partially observed settings break empirical model estimation.
- What evidence would resolve it: Regret bounds for PEL algorithms in Exo-MDPs with continuous exogenous state spaces or partially observed exogenous processes.

### Open Question 2
- Question: Can anchors be selected adaptively from data rather than requiring a priori domain knowledge?
- Basis in paper: [inferred] Appendix C.2.7 mentions adaptive anchor selection via coreset/Frank-Wolfe procedures as "a promising direction" but provides no analysis.
- Why unresolved: Current theory assumes fixed anchors with full row rank; learning anchors online introduces estimation error not accounted for in current regret analysis.
- What evidence would resolve it: Algorithm with provable regret bounds that adaptively constructs or refines anchor sets during learning.

### Open Question 3
- Question: Are there intermediate settings between full Exo-MDP structure and general MDPs where hybrid strategies achieve better tradeoffs?
- Basis in paper: [inferred] Proposition 2 shows PEL suffers linear regret when f or r is unknown, creating binary classification. Paper leaves unexplored whether partial exogenous structure enables partial exploitation benefits.
- What evidence would resolve it: Characterization of problem classes with partial exogenous structure and algorithms achieving sublinear regret with less exploration than general MDP methods.

## Limitations
- The theoretical guarantees critically depend on the exogenous state evolving truly independently of agent actions
- Bellman-closed feature transport assumption required for LFA analysis is restrictive and may not generalize to arbitrary feature representations
- Experimental validation is limited in scale (K=100 episodes) and doesn't systematically explore failure modes or assumption violations

## Confidence
- **High Confidence**: Tabular Exo-MDP regret bound (O(H²|Ξ|√K)) follows straightforwardly from Hoeffding's inequality applied to exogenous model estimation
- **Medium Confidence**: LFA regret bound relies on Bellman-closed transport assumption, technically satisfied by construction but may not generalize
- **Low Confidence**: Experimental validation shows promising results but is limited in scale and doesn't systematically explore robustness

## Next Checks
1. **Assumption Violation Test**: Implement storage control variant where exogenous process (demand) depends slightly on previous action, measure resulting regret growth to validate linear regret prediction
2. **Feature Transport Robustness**: Systematically vary feature representation (polynomial, random Fourier) for storage control, quantify impact on regret and empirical Bellman error
3. **Scalability Experiment**: Increase anchors N and horizon H in storage control, measure scaling of both regret and computation time to validate theoretical dependencies