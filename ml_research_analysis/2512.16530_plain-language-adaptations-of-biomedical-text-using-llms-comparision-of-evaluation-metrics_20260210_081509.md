---
ver: rpa2
title: 'Plain language adaptations of biomedical text using LLMs: Comparision of evaluation
  metrics'
arxiv_id: '2512.16530'
source_url: https://arxiv.org/abs/2512.16530
tags:
- health
- level
- approaches
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated Large Language Models (LLMs) for simplifying\
  \ biomedical texts to improve health literacy. Using the PLABA dataset of manually\
  \ adapted biomedical abstracts, researchers compared three approaches\u2014baseline\
  \ prompt template, two-AI-agent iterative refinement, and fine-tuning\u2014across\
  \ OpenAI's GPT-4o and GPT-4o-mini models."
---

# Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics

## Quick Facts
- arXiv ID: 2512.16530
- Source URL: https://arxiv.org/abs/2512.16530
- Reference count: 13
- Large Language Models (LLMs) can effectively simplify biomedical texts, with GPT-4o-mini outperforming GPT-4o in plain language adaptation tasks

## Executive Summary
This study evaluates Large Language Models (LLMs) for simplifying biomedical texts to improve health literacy. Using the PLABA dataset of manually adapted biomedical abstracts, researchers compared three approaches—baseline prompt template, two-AI-agent iterative refinement, and fine-tuning—across OpenAI's GPT-4o and GPT-4o-mini models. Evaluation combined quantitative metrics (Flesch-Kincaid grade level, SMOG, SARI, BERTScore, G-Eval) with qualitative assessments via 5-point Likert scales on simplicity, accuracy, completeness, and brevity by healthcare experts.

Results showed GPT-4o-mini outperformed GPT-4o, with baseline prompt engineering nearly matching the two-agent approach. Fine-tuning underperformed despite achieving similar text complexity to ground truth. G-Eval, an LLM-based metric, aligned closely with human evaluations, suggesting its potential as a scalable alternative. The study highlights the challenges of automated readability assessment and the promise of LLM-driven evaluation in domain-specific text simplification.

## Method Summary
The study employed three approaches to biomedical text simplification: a baseline prompt template, a two-AI-agent iterative refinement method, and fine-tuning of LLM parameters. Researchers used OpenAI's GPT-4o and GPT-4o-mini models to process 15 biomedical abstracts from the PLABA dataset. Evaluation combined automated metrics including Flesch-Kincaid grade level, SMOG, SARI, BERTScore, and G-Eval with qualitative assessments from healthcare experts rating simplicity, accuracy, completeness, and brevity on 5-point Likert scales.

## Key Results
- GPT-4o-mini consistently outperformed GPT-4o in plain language adaptation tasks
- Baseline prompt engineering achieved nearly equivalent results to the more complex two-agent approach
- G-Eval showed strong correlation with human evaluations, demonstrating potential as a scalable evaluation metric
- Fine-tuning approach underperformed despite matching ground truth text complexity

## Why This Works (Mechanism)
None

## Foundational Learning
- **Biomedical text simplification**: Converting complex medical jargon into accessible language for patients
  - Why needed: Enables health literacy and informed decision-making
  - Quick check: Assess patient comprehension rates before/after simplification
- **Readability metrics**: Quantitative measures of text complexity (Flesch-Kincaid, SMOG)
  - Why needed: Provide objective benchmarks for text accessibility
  - Quick check: Compare automated scores with expert assessments
- **LLM evaluation frameworks**: Combining automated and human evaluation methods
  - Why needed: Balances scalability with qualitative accuracy assessment
  - Quick check: Correlation analysis between metric scores and expert ratings

## Architecture Onboarding
**Component Map**: PLABA dataset -> GPT-4o/4o-mini models -> Baseline/Iterative/Fine-tuning approaches -> Evaluation metrics (Flesch-Kincaid, SMOG, SARI, BERTScore, G-Eval) -> Healthcare expert review

**Critical Path**: Text input → LLM processing → Complexity reduction → Evaluation → Expert validation

**Design Tradeoffs**: Automated metrics offer scalability but may miss nuanced quality issues; human evaluation provides depth but lacks scalability; simpler prompt engineering matches complex multi-agent approaches in effectiveness

**Failure Signatures**: Fine-tuning underperforms despite complexity matching; automated metrics may not capture true readability improvements; limited expert panel size affects reliability

**First Experiments**:
1. Compare LLM-generated simplifications with ground truth across multiple biomedical datasets
2. Test inter-rater reliability across expanded expert panels
3. Perform ablation studies on fine-tuning hyperparameters and training data composition

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to 15 abstracts from PLABA dataset constraining generalizability
- Only three healthcare experts for qualitative assessment raises reliability concerns
- Automated metrics may not fully capture nuances of plain language quality
- Fine-tuning approach underperformance suggests potential issues with training methodology

## Confidence
- **High**: GPT-4o-mini outperforming GPT-4o in plain language adaptation task
- **Medium**: Baseline prompt engineering matching two-agent approach effectiveness
- **Medium**: G-Eval showing strong correlation with human evaluations as a scalable metric
- **Low**: Fine-tuning approach underperformance relative to complexity-matched ground truth

## Next Checks
1. Conduct inter-rater reliability analysis with expanded expert panel (minimum 10 evaluators) to validate qualitative assessment consistency
2. Test models on independent biomedical text simplification datasets beyond PLABA to assess generalizability
3. Perform ablation studies on fine-tuning approach, examining training data composition, hyperparameter optimization, and model architecture variations to identify performance bottlenecks