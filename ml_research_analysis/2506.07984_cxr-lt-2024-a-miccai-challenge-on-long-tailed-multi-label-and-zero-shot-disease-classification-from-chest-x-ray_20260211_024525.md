---
ver: rpa2
title: 'CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot
  disease classification from chest X-ray'
arxiv_id: '2506.07984'
source_url: https://arxiv.org/abs/2506.07984
tags:
- cxr-lt
- task
- classification
- disease
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The CXR-LT 2024 challenge addressed long-tailed, multi-label,
  and zero-shot disease classification from chest X-rays using a large dataset of
  377,110 images across 45 disease categories. The challenge featured three tasks:
  (1) long-tailed classification on a noisy test set, (2) classification on a manually
  annotated gold standard subset, and (3) zero-shot generalization to five unseen
  diseases.'
---

# CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray

## Quick Facts
- arXiv ID: 2506.07984
- Source URL: https://arxiv.org/abs/2506.07984
- Reference count: 40
- Primary result: mAP 0.281 (Task 1), 0.526 (Task 2), 0.129 (Task 3)

## Executive Summary
The CXR-LT 2024 challenge addressed the complex problem of long-tailed, multi-label, and zero-shot disease classification from chest X-rays using a large dataset of 377,110 images across 45 disease categories. The challenge featured three tasks: long-tailed classification on a noisy test set, classification on a manually annotated gold standard subset, and zero-shot generalization to five unseen diseases. Methods included ensemble learning, vision-language models, synthetic data generation, and multimodal approaches leveraging both image and text data. Top solutions used ConvNeXt backbones, weighted asymmetric losses, and pretraining on external datasets. The challenge advanced clinically realistic and generalizable CXR diagnostic models.

## Method Summary
The CXR-LT 2024 challenge utilized the MIMIC-CXR-JPG dataset with 377,110 images, employing rule-based (RadText) and GPT-4 labeling for extracting 45 disease categories from radiology reports. Participants used ConvNeXt backbones with weighted asymmetric loss to address class imbalance, pretrained on external CXR datasets (CheXpert, NIH, VinDr-CXR) before fine-tuning. Multi-scale inputs, ensembling, and vision-language models were key strategies. The evaluation focused on mean Average Precision (mAP) across three tasks: long-tailed classification, gold standard subset evaluation, and zero-shot generalization to unseen diseases.

## Key Results
- Task 1 (Long-tailed classification): mAP 0.281 on noisy test set with 40 labels
- Task 2 (Gold standard): mAP 0.526 on manually annotated subset with 26 labels
- Task 3 (Zero-shot): mAP 0.129 for detecting 5 unseen diseases
- GPT-4 labeling improved precision from 0.711 to 0.786 compared to rule-based methods

## Why This Works (Mechanism)

### Mechanism 1
Re-weighting loss functions and ensembling multi-resolution features mitigates the gradient dominance of frequent "head" classes over rare "tail" classes. Weighted asymmetric loss assigns higher penalties to false negatives on rare classes (e.g., Tortuous Aorta, Lobar Atelectasis). Concurrently, ensembles of ConvNeXt models at varying resolutions (224, 384, 512) capture multi-scale visual features, ensuring small pathological regions are not lost in downsampling.

### Mechanism 2
Vision-language models (VLMs) enable zero-shot generalization by aligning image features with semantic text embeddings rather than fixed classification heads. By training a dual encoder (e.g., DINOv2 for image, BERT for text) to minimize contrastive loss, the model learns a shared latent space. At inference, "unseen" disease classes are defined purely by text descriptions (e.g., "Scoliosis"), and the model detects them by measuring proximity in the latent space without retraining.

### Mechanism 3
Synthetic data generation using diffusion models supplements the sparse training data for rare diseases. Text-to-image diffusion models generate synthetic chest X-rays conditioned on prompts specifying rare comorbidities (e.g., "Round Atelectasis, Pneumothorax"). This increases the effective sample size of tail classes, preventing the classifier from treating them as noise.

## Foundational Learning

- **Concept: Long-Tailed Distribution**
  - Why needed here: The dataset spans 45 classes, but prevalence varies by orders of magnitude (e.g., "Support Devices" vs. "Lobar Atelectasis"). Standard Cross-Entropy loss fails here.
  - Quick check question: Why would a standard accuracy metric be misleading for a dataset where 95% of samples are "Normal"?

- **Concept: Multi-Label vs. Multi-Class Classification**
  - Why needed here: A single chest X-ray can simultaneously exhibit "Edema," "Pleural Effusion," and "Cardiomegaly." The model must output independent probabilities per class, not a single softmax distribution.
  - Quick check question: Does the output layer use Softmax or Sigmoid activation?

- **Concept: Zero-Shot Learning via CLIP-style Alignment**
  - Why needed here: Task 3 requires detecting diseases not present in the training set. You must understand how text encoders map images to a shared space to solve this.
  - Quick check question: How can a model predict class "Bulla" if it has never seen a labeled image of a bulla during training?

## Architecture Onboarding

- **Component map:** MIMIC-CXR-JPG -> ConvNeXt Backbone -> Weighted Asymmetric Loss -> mAP Evaluation
- **Critical path:**
  1. Data Prep: Load MIMIC-CXR-JPG. Apply Rule-based or GPT-4 labels (Page 11).
  2. Pretraining: Initialize with ImageNet -> Fine-tune on external CXR datasets (CheXpert, NIH) -> Train on CXR-LT.
  3. Ensembling: Average predictions from 5-fold cross-validation or multi-resolution variants.

- **Design tradeoffs:**
  - ConvNeXt vs. ViT: ConvNeXt offers better performance on "Head" classes and is more stable. ViT offers better global context for Zero-Shot (Task 3) but requires more data/pretraining.
  - Label Quality: Rule-based labels are noisy but scalable; GPT-4 labels are higher precision (0.786 vs 0.711) but costlier to generate [Page 11, Table 8].

- **Failure signatures:**
  - High AUROC / Low mAP: Indicates the model ranks positive samples higher than negative ones generally, but fails to set accurate confidence thresholds on rare classes.
  - Zero-shot Hallucination: In Task 3, the model detects "Scoliosis" based on spurious correlations (e.g., patient positioning) rather than vertebral features.

- **First 3 experiments:**
  1. Baseline Implementation: Train a ConvNeXt-S model with standard Binary Cross Entropy (BCE) on the 40 training classes. Record mAP on the "Tail" classes to establish a lower bound.
  2. Loss Optimization: Replace BCE with Asymmetric Loss (ASL) and tune the gamma hyperparameter. Verify performance improvement on the "Tail" group [Page 9, Table 5].
  3. Zero-Shot Probe: Load a pre-trained BiomedCLIP or similar VLM. Evaluate its zero-shot performance on the Task 3 test set (5 unseen classes) without any CXR-LT specific training to measure the domain gap.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do performance disparities regarding race and sex persist in long-tailed, multi-label classification when validated on multi-institutional external datasets? The current challenge data comes from a single academic medical center, making it impossible to measure cross-site transferability or subgroup-specific bias without external test sets.

### Open Question 2
Does reframing label extraction as a Natural Language Inference (NLI) problem or using Chain-of-Thought (CoT) prompting outperform standard GPT-4 labeling for long-tailed disease classes? While GPT-4 showed improved precision over rule-based methods, the proposed advanced prompting strategies (NLI/CoT) were suggested but not implemented or tested in the current study.

### Open Question 3
Can instruction tuning or efficient fine-tuning of vision-language models substantially improve zero-shot generalization for unseen chest X-ray findings? Top teams utilized general vision-language models, but the specific benefits of medical-domain instruction tuning for detecting unseen pathologies remain unquantified.

## Limitations
- Limited generalizability due to single institutional dataset (MIMIC-CXR)
- No detailed implementation specifics for top-performing methods (hyperparameters, exact loss configurations)
- Uncertainty about synthetic data quality and clinical utility without expert evaluation

## Confidence
- **High Confidence:** The effectiveness of weighted asymmetric loss for long-tailed classification and the general superiority of ConvNeXt architectures are well-supported by multiple teams' results and established literature.
- **Medium Confidence:** The benefits of vision-language models for zero-shot learning are demonstrated but require further validation on broader disease sets beyond the five unseen classes tested.
- **Low Confidence:** The quality and clinical utility of synthetic data generated for rare diseases remain uncertain without rigorous human evaluation of the generated images.

## Next Checks
1. Hyperparameter Sensitivity Analysis: Systematically vary the asymmetric loss weight parameters and learning rates to determine their impact on tail class performance.
2. Synthetic Data Quality Assessment: Conduct expert radiologist review of synthetic images to evaluate anatomical accuracy and potential for introducing artifacts.
3. Zero-Shot Generalization Test: Evaluate the zero-shot models on an expanded set of unseen diseases not present in either training or test sets to assess true generalization capability.