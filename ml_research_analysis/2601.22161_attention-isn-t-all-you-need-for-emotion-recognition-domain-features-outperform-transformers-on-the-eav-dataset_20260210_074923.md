---
ver: rpa2
title: Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform
  Transformers on the EAV Dataset
arxiv_id: '2601.22161'
source_url: https://arxiv.org/abs/2601.22161
tags:
- features
- attention
- audio
- emotion
- delta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates transformer-based models for
  multimodal emotion recognition on the small-scale EAV dataset. While sophisticated
  factorized attention mechanisms (M2) underperform by 5-13 percentage points due
  to overfitting and degradation of pretrained features, simple domain-appropriate
  modifications achieve state-of-the-art results.
---

# Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset

## Quick Facts
- arXiv ID: 2601.22161
- Source URL: https://arxiv.org/abs/2601.22161
- Authors: Anmol Guragain
- Reference count: 12
- Primary result: Domain-specific feature engineering (delta MFCCs, frequency-domain EEG) achieves 67.62-75.30% accuracy on EAV dataset, outperforming transformer-based models by 5-13 percentage points.

## Executive Summary
This study systematically evaluates transformer-based models for multimodal emotion recognition on the small-scale EAV dataset. While sophisticated factorized attention mechanisms underperform by 5-13 percentage points due to overfitting and degradation of pretrained features, simple domain-appropriate modifications achieve state-of-the-art results. Adding delta MFCCs to the audio CNN improves accuracy from 61.9% to 65.56% (+3.66pp), while frequency-domain features for EEG achieve 67.62% (+7.62pp over baseline). A vision transformer pretrained on facial emotions reaches 75.30%, outperforming CNN modifications. These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

## Method Summary
The study evaluates three approaches on the EAV dataset: M1 uses pretrained transformers with freeze-unfreeze training, M2 employs factorized attention mechanisms that underperform, and M3 implements simple CNN modifications with domain features. Models are trained using AdamW optimizer with label smoothing (0.1) and weight decay, evaluated using subject-dependent 70/30 splits across 42 participants. EEG features include band power and differential entropy across five frequency bands plus alpha asymmetry metrics. Audio features combine MFCCs with delta coefficients, chroma, and mel-spectrogram. Vision uses ResNet50 features with temporal deltas and SE blocks.

## Key Results
- EEG frequency-domain features (Welch PSD, differential entropy, alpha asymmetry) achieve 67.62% accuracy (+7.62pp over baseline)
- Audio CNN with delta MFCCs improves from 61.9% to 65.56% accuracy (+3.66pp)
- Vision transformer pretrained on facial emotions reaches 75.30% accuracy, outperforming CNN modifications
- M2 factorized attention underperforms M1 by 5-13 percentage points due to overfitting on small dataset
- M3 domain feature engineering achieves state-of-the-art results on EAV dataset

## Why This Works (Mechanism)
The study demonstrates that domain-specific feature engineering outperforms complex transformer architectures on small-scale emotion recognition datasets. The performance gains arise from incorporating physiologically meaningful features (EEG band power, delta MFCCs) that capture emotion-relevant patterns more effectively than learned representations when training data is limited. The degradation observed in transformer models stems from overfitting to the small dataset and feature degradation when pretrained layers are frozen, suggesting that architectural complexity without sufficient data can harm rather than help performance.

## Foundational Learning

**EEG Frequency Band Analysis**: Why needed - different frequency bands (δ: 0.5-4 Hz, θ: 4-8 Hz, α: 8-13 Hz, β: 13-30 Hz, γ: 30-45 Hz) capture distinct neural correlates of emotional states. Quick check - verify band power and differential entropy features are correctly computed across all 30 channels.

**Delta MFCCs**: Why needed - temporal derivatives capture speech prosody and emotional dynamics beyond static spectral content. Quick check - confirm delta MFCCs are computed with appropriate window size (N=9 suggested) and concatenated with static features.

**Alpha Asymmetry**: Why needed - hemispheric differences in alpha power (ln(P_right) - ln(P_left)) correlate with approach/avoidance motivation in emotional processing. Quick check - verify asymmetry calculations use correct electrode pairs (Fp1-Fp2, F3-F4, F7-F8, C3-C4, P3-P4, O1-O2).

**Squeeze-and-Excitation Blocks**: Why needed - channel-wise attention mechanisms help networks focus on emotion-relevant visual features while suppressing noise. Quick check - ensure SE block reduction ratio is 16 (not 1) to enable proper bottleneck learning.

**Subject-Dependent Evaluation**: Why needed - emotion recognition systems must generalize across individuals with different baseline physiological patterns. Quick check - confirm 70/30 splits are computed per subject with consistent random seeds.

## Architecture Onboarding

**Component Map**: EEG preprocessing -> frequency-domain feature extraction (150 band power + 150 differential entropy + 6 alpha asymmetry) -> MLP classifier (306→128→64→5)
Audio preprocessing -> MFCC + delta MFCC + chroma + mel features (220 total) -> CNN classifier
Video preprocessing -> ResNet50 features + temporal deltas + SE attention -> CNN classifier

**Critical Path**: Data preprocessing → domain feature engineering → simple classifier training → subject-dependent evaluation
The critical insight is that feature engineering quality matters more than model complexity for small datasets.

**Design Tradeoffs**: M1 sacrifices feature adaptation by freezing pretrained layers, M2 introduces complexity that overfits on limited data, while M3 balances simplicity with domain knowledge to achieve optimal performance.

**Failure Signatures**: Double softmax application causes gradient distortion; SE block with reduction ratio 1 fails to learn meaningful channel reweighting; factorized attention without sufficient training samples leads to overfitting.

**First Experiments**:
1. Train EEG MLP with only band power features to establish baseline performance
2. Add delta MFCCs to audio CNN and measure improvement over static MFCCs alone
3. Implement vision transformer with SE blocks and evaluate against ResNet50 baseline

## Open Questions the Paper Calls Out

**Cross-Subject Generalization**: Would M3 domain features generalize in leave-one-subject-out evaluation? The paper uses subject-dependent evaluation only, leaving cross-subject performance unexplored.

**Larger Dataset Performance**: Would factorized attention mechanisms (M2) perform better on larger emotion datasets where their inductive biases could be properly learned? Current underperformance attributed to overfitting with ~280 training samples per subject.

**Multimodal Fusion**: Would combining the best single-modality models (M3-EEG band power, M3-audio delta MFCCs, M1-ViT) yield further performance gains? The study evaluated modalities independently without fusion experiments.

**Subject Variability Analysis**: What factors explain substantial subject-level variability in accuracy (S17: 82.5-93.3% vs S35: 46.7-56.7%)? The paper reports differences but doesn't correlate with signal quality or behavioral measures.

## Limitations

- Small sample size (~280 training samples per subject) limits generalizability and may bias results toward dataset-specific patterns
- Subject-dependent evaluation protocol prevents assessment of cross-subject generalization capabilities
- M2 factorized attention implementation lacks detailed architectural specifications for complete replication
- No multimodal fusion experiments to evaluate complementary information across EEG, audio, and video modalities

## Confidence

**High confidence**: Simple domain feature engineering outperforms complex transformer architectures on EAV dataset; CNN modifications with delta MFCCs and frequency-domain features achieve SOTA results.

**Medium confidence**: Transformer degradation is primarily due to overfitting and frozen feature degradation rather than inherent limitations of attention mechanisms.

**Low confidence**: The exact architectural differences between M1 and M2 that cause M2's underperformance.

## Next Checks

1. Replicate M1 with unfrozen transformer layers to isolate whether performance gains come from unfreezing vs. architectural changes
2. Implement M2 with full architectural details to verify the 5-13pp performance gap is reproducible
3. Test M3 feature extraction on an independent multimodal emotion dataset to assess generalizability beyond EAV