---
ver: rpa2
title: Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts
  in Few-Shot Learning
arxiv_id: '2506.03035'
source_url: https://arxiv.org/abs/2506.03035
tags:
- examples
- selection
- bm25
- intent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of information retrieval (IR) methods
  to improve prompt construction for spoken language understanding (SLU) tasks. The
  key idea is to enhance example selection for few-shot prompts by using BM25 (Best
  Matching 25), a lexical IR method, to retrieve relevant training examples based
  on semantic similarity to the input utterance.
---

# Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning

## Quick Facts
- **arXiv ID**: 2506.03035
- **Source URL**: https://arxiv.org/abs/2506.03035
- **Reference count**: 0
- **Primary result**: BM25-based retrieval improves SLU F1-scores up to 86.63% on ATIS, outperforming random selection (38.43%) and intent-based selection (55.23%) without increasing prompt length

## Executive Summary
This paper investigates how information retrieval methods can improve prompt construction for spoken language understanding (SLU) tasks in few-shot learning settings. The core innovation is using BM25, a lexical IR method, to retrieve relevant training examples based on semantic similarity to the input utterance. Across four SLU benchmarks (ATIS, SNIPS, SLURP, MEDIA), BM25-based methods consistently outperform random selection, intent-based selection, and semantic IR approaches (ColBERT). The study demonstrates that retrieving as few as five high-quality examples using BM25 can outperform using 100 random examples, achieving significant F1-score improvements while maintaining computational efficiency through shorter prompts.

## Method Summary
The method uses a three-step pipeline: (1) BM25 indexes training utterances and retrieves top-K examples based on lexical similarity to the input query, (2) retrieved examples are formatted into a prompt template with slot definitions, instructions, and examples, (3) an LLM (Hermes-3-Llama-3.1-8B) generates 256 tokens via greedy decoding, and outputs are parsed for slot-value pairs. The approach compares BM25 against random selection, intent-based selection, and ColBERT across four SLU benchmarks. The study focuses on slot-filling tasks where the model must identify and label semantic entities in spoken utterances.

## Key Results
- BM25 achieves F1-scores of 86.63% on ATIS and 61.10% on MEDIA, significantly outperforming random selection (38.43%) and intent-based selection (55.23%)
- Retrieving five examples using BM25 consistently outperforms using 100 random examples, demonstrating the quality-over-quantity tradeoff
- BM25 achieves 89.75% slot presence on ATIS vs. 67.14% for random selection; 90.53% on MEDIA vs. 65.00% for random
- Lexical IR methods enhance performance without increasing prompt length, maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lexical similarity-based retrieval surfaces examples containing the slots needed for accurate annotation
- **Mechanism**: BM25 matches surface-form tokens between query utterance and training examples, prioritizing examples with shared vocabulary that correlates with slot presence
- **Core assumption**: Slot labels co-occur with consistent lexical patterns across utterances
- **Evidence anchors**: [abstract] lexical IR methods enhance performance without increasing prompt length; [Section 4.2, Table 3] BM25 achieves 89.75% slot presence on ATIS vs. 67.14% for random; [corpus] Weak direct corpus support
- **Break condition**: If utterances use highly varied synonyms for same slot values without lexical overlap, BM25's advantage degrades toward random-baseline levels

### Mechanism 2
- **Claim**: Cross-intent slot sharing enables retrieval of useful examples outside the target intent
- **Mechanism**: Many slots (e.g., location, date) appear across multiple intents; BM25 retrieves based on utterance similarity rather than intent constraint
- **Core assumption**: Training set contains sufficient examples with overlapping slots across intents
- **Evidence anchors**: [Section 2, Figure 1] MEDIA shows significant slot overlap across intents; SNIPS shows minimal overlap; [Section 2] slots overlap significantly across several intents
- **Break condition**: In domains with strict intent-slot exclusivity (minimal overlap), pure BM25 may underperform intent-constrained retrieval variants

### Mechanism 3
- **Claim**: Retrieving few high-quality examples outperforms many low-quality examples due to reduced noise and maintained attention density
- **Mechanism**: Shorter prompts with relevant examples reduce distractor tokens, allowing LLM's attention to focus on slot-value patterns without dilution
- **Core assumption**: LLMs in few-shot settings are sensitive to example quality over quantity; irrelevant examples introduce noise
- **Evidence anchors**: [Section 4.3, Figure 3] considering only five examples using BM25 always outperforms random methods considering 100 examples; [Section 4] BM25-based methods outperform without increasing prompt length
- **Break condition**: If LLM's in-context learning relies more on coverage diversity than pattern clarity, larger random samples may close the gap

## Foundational Learning

- **Concept: BM25 (Best Matching 25)**
  - Why needed here: BM25 is the core retrieval mechanism; understanding term frequency saturation and inverse document frequency explains why lexical matching outperforms semantic embeddings
  - Quick check question: Given a query "book flight to Paris tomorrow" and two documents—one with "flight" and "Paris" but no "tomorrow," another with semantically similar "reserve air travel"—which would BM25 rank higher?

- **Concept: Slot-Filling as Structured Extraction**
  - Why needed here: The task outputs slot-value pairs in specific format; parsing failures directly impact F1-scores
  - Quick check question: For the utterance "play jazz in the living room," what are the likely slot labels and values?

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The entire method relies on LLMs generalizing from prompt examples without weight updates
  - Quick check question: If you provide three examples with inconsistent output formats (one with semicolons, one with JSON, one with plain text), what happens to the model's output consistency?

## Architecture Onboarding

- **Component map**: Retrieval Index -> Query Module -> Prompt Formatter -> LLM Inference -> Output Parser
- **Critical path**: 
  1. Receive utterance → tokenize for BM25 query
  2. Retrieve top-K from training index (K typically 5–10)
  3. Format prompt with slot definitions, retrieved examples, and current utterance
  4. Generate LLM response
  5. Parse slot-value pairs; handle missing/malformed output

- **Design tradeoffs**:
  - BM25 vs. ColBERT: Lexical precision vs. semantic generalization; paper shows BM25 superior for SLU slot matching
  - Intent-constrained vs. unconstrained retrieval: `Intent→BM25` vs. pure BM25; dataset-dependent optimal choice
  - Prompt length (K): Larger K increases context but adds noise; returns diminish after ~10 examples

- **Failure signatures**:
  - Low slot presence in retrieved examples: Check BM25 tokenization; may need domain-specific stopword removal
  - ColBERT underperformance: Semantic embeddings may conflate slot-irrelevant similarity; verify with slot-presence audit
  - Parsing failures: Model outputs free text instead of structured format; check prompt template adherence
  - Intent-only selection underperforms BM25: Indicates cross-intent slot sharing; verify dataset overlap via upset plot analysis

- **First 3 experiments**:
  1. Baseline replication: Run BM25 retrieval with K=10 on SNIPS, measure F1 against paper's reported 84.38%; verify slot-presence rate matches ~90%
  2. Ablation on K: Test K ∈ {1, 3, 5, 10, 20} on MEDIA; plot F1 curve to identify saturation point
  3. Hybrid retrieval test: Combine BM25 score with intent-filtering (`BM25→Intent` vs. `Intent→BM25`) on SLURP; compare which ordering benefits complex multi-intent datasets

## Open Questions the Paper Calls Out
- How does the effectiveness of BM25-based example selection generalize across different LLM architectures?
- To what extent does the specific prompt template design impact the performance of retrieval-augmented SLU?
- How does retrieval-based prompting perform on utterances containing rare or highly ambiguous slots?
- Can hybrid retrieval approaches combining BM25 and semantic embeddings outperform lexical retrieval alone?

## Limitations
- Exclusive focus on lexical retrieval methods may not generalize to domains with high lexical variability or complex paraphrastic patterns
- Retrieval quality metric (slot presence rate) provides indirect evidence of mechanism effectiveness but doesn't directly measure understanding improvement
- Study uses single LLM and fixed decoding parameters, limiting generalizability across model architectures and inference strategies

## Confidence
- **High Confidence**: BM25 consistently outperforms random and intent-based selection across all four datasets; quality-vs-quantity tradeoff is empirically validated
- **Medium Confidence**: Mechanism explanation is supported but not conclusively proven; alternative explanations remain untested
- **Medium Confidence**: Cross-intent slot sharing as a retrieval benefit is validated for MEDIA but contradicted by SNIPS results

## Next Checks
1. **Domain Transfer Test**: Apply BM25 retrieval to a paraphrastic SLU dataset (e.g., MultiWOZ with paraphrased utterances) to measure performance degradation when lexical overlap decreases
2. **Mechanism Isolation**: Conduct an ablation study where retrieved examples are replaced with synthetic examples containing same slot-value pairs but different lexical patterns to separate slot presence effects from lexical similarity effects
3. **Retrieval Quality Analysis**: Implement a slot-relevant retrieval metric that measures proportion of retrieved examples containing both target utterance's slots AND sufficient context to demonstrate slot-value extraction patterns, rather than just slot presence