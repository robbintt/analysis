---
ver: rpa2
title: 'Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs'
arxiv_id: '2511.22567'
source_url: https://arxiv.org/abs/2511.22567
tags:
- uncertainty
- sensor
- epistemic
- placement
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends Convolutional Conditional Neural Processes (ConvCNPs)
  with Mixture Density Networks (MDNs) to enable decomposition of predictive uncertainty
  into epistemic and aleatoric components for sensor placement tasks. The key innovation
  is a new acquisition function based on expected reduction in epistemic uncertainty,
  which more effectively identifies ambiguous regions where additional sensors would
  most reduce model uncertainty.
---

# Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs

## Quick Facts
- arXiv ID: 2511.22567
- Source URL: https://arxiv.org/abs/2511.22567
- Reference count: 33
- Authors: Feyza Eksen; Stefan Oehmcke; Stefan Lüdtke
- Primary result: Epistemic uncertainty-based sensor placement achieves lower RMSE (1.1°C vs 1.3°C) than total uncertainty methods with just 10 sensors

## Executive Summary
This work addresses optimal sensor placement for environmental monitoring by extending Convolutional Conditional Neural Processes (ConvCNPs) with Mixture Density Networks (MDNs) to decompose predictive uncertainty into epistemic and aleatoric components. The key innovation is an acquisition function based on expected reduction in epistemic uncertainty, which more effectively identifies ambiguous regions where additional sensors would most reduce model uncertainty. Applied to sea surface temperature monitoring in the Western Baltic Sea, the proposed approach outperforms traditional total uncertainty-based methods, achieving lower root mean squared error and negative log-likelihood. The epistemic uncertainty-driven strategy successfully prioritizes functionally ambiguous areas over regions with high aleatoric uncertainty, demonstrating improved sensor placement efficiency.

## Method Summary
The method combines ConvCNPs with MDNs to enable decomposition of predictive uncertainty. The model first projects sparse context points (sensor locations with SST measurements) onto a uniform grid using SetConv, then processes this grid through a U-Net to learn translation-equivariant features. An MDN head outputs mixture components, where epistemic uncertainty is characterized as the variance of component means (inter-component disagreement) and aleatoric uncertainty as the weighted average variance within components. For sensor placement, a greedy acquisition algorithm selects locations that minimize expected epistemic uncertainty, targeting regions where the model is most uncertain due to lack of observations rather than inherent noise.

## Key Results
- Epistemic uncertainty-based acquisition achieves RMSE of 1.1°C vs 1.3°C with 10 sensors
- Outperforms total variance (ΔVar) acquisition which tends to favor noisy regions over ambiguous ones
- Demonstrates effective uncertainty decomposition between aleatoric (irreducible noise) and epistemic (model ignorance) components
- Successfully identifies functionally ambiguous areas for sensor placement in Western Baltic Sea SST monitoring

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Decomposition via Mixture Disagreement
The MDN output head enables decomposition of predictive uncertainty into epistemic (model ignorance) and aleatoric (irreducible noise) components. Epistemic uncertainty is approximated as the variance of mixture component means, representing inter-component disagreement. Aleatoric uncertainty is the weighted average variance within components. This decomposition allows the acquisition function to target regions where additional sensors would actually improve the functional estimate rather than just measure noise.

### Mechanism 2: Targeting Reducible Uncertainty (Acquisition)
The acquisition function specifically minimizes epistemic uncertainty rather than total variance, avoiding the pitfall of placing sensors in inherently noisy regions. By focusing on regions where the model shows disagreement between mixture components, the algorithm identifies areas where additional observations would most reduce uncertainty about the underlying function. This is crucial because measuring noise (high aleatoric uncertainty) doesn't improve the model's understanding of the functional relationship.

### Mechanism 3: Functional Encoding via Translation Equivariance
ConvCNPs use convolutional layers on discretized grids to learn translation-equivariant features, enabling generalization across spatial locations. The SetConv layer projects sparse, off-grid sensor data onto a uniform grid, allowing the U-Net to process this representation and act as a surrogate for the spatial field without retraining for every new sensor configuration. This architecture is particularly effective for environmental monitoring where sensor placement may be irregular.

## Foundational Learning

- **Concept: Neural Processes (NPs) & ConvCNPs**
  - Why needed here: The paper builds on ConvCNPs as the base architecture for amortized inference from context sets to predictions
  - Quick check question: How does a ConvCNP handle a context set of varying size and non-uniform spatial density compared to a standard CNN?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The core contribution is the separation of these two uncertainty types to enable more effective sensor placement
  - Quick check question: In the context of this paper, does a high aleatoric uncertainty imply we should place a sensor there to learn more? (Answer: No)

- **Concept: Mixture Density Networks (MDNs)**
  - Why needed here: The method uses MDNs not just for multi-modal output, but specifically to derive variance statistics for uncertainty decomposition
  - Quick check question: If a mixture model has two components with identical means but large variances, is the epistemic uncertainty high or low?

## Architecture Onboarding

- **Component map:** SetConv -> U-Net -> Interpolation -> MLP -> MDN Head (π_k, μ_k, σ_k)
- **Critical path:**
  1. Discretize context via SetConv
  2. Pass through U-Net
  3. **Vital Step:** At the MDN head, calculate σ²_Ep (disagreement) specifically, distinct from total variance σ²_Var
  4. Use σ²_Ep to score candidate locations
- **Design tradeoffs:**
  - K=2 vs K>2: The paper notes K=2 may underestimate epistemic uncertainty, but higher K increases parameter count and training instability
  - Grid Resolution: SetConv resolution trades off computational cost against spatial precision for irregular sensors
- **Failure signatures:**
  - Mode Collapse: MDN components collapse to a single Gaussian, driving σ²_Ep → 0 and stopping sensor placement
  - High Aleatoric Trap: Using ΔVar instead of ΔEp results in clustering sensors in noisy areas without improving the global map
- **First 3 experiments:**
  1. Sanity Check (Synthetic): Replicate the "noisy" vs. "multiple-function" 1D test to verify the model learns distinct σ²_Al and σ²_Ep patterns
  2. Ablation (Acquisition): Compare ΔEp vs. ΔVar on a hold-out set where noise is artificially injected into specific regions
  3. Ablation (K-components): Run placement with K=2 vs K=4 to check for the "underestimation" of uncertainty warned about in Section 4

## Open Questions the Paper Calls Out

- **Open Question 1:** How does temporal generalization affect the acquisition function's effectiveness when averaged over multiple days?
  - Basis: The authors note a "more realistic evaluation would involve averaging acquisition functions over multiple days"
  - Why unresolved: Experiments were limited to a single test day (January 1, 2022)
  - What evidence would resolve it: Evaluation across diverse validation set spanning different seasons

- **Open Question 2:** Does increasing the number of mixture components (K) improve the accuracy of epistemic uncertainty estimates?
  - Basis: Section 4 states that "K=2 mixture components may not fully capture epistemic uncertainty"
  - Why unresolved: The study restricted the MDN to K=2 components
  - What evidence would resolve it: Ablation studies testing higher values of K to measure impact on uncertainty calibration and RMSE

- **Open Question 3:** How does the proposed MDN-based uncertainty decomposition compare to standard baselines like Ensembles or MC-Dropout?
  - Basis: Section 6 explicitly lists "comparing against Monte Carlo-Dropout and ensemble baselines" as future work
  - Why unresolved: The current work only compares epistemic method against total variance method
  - What evidence would resolve it: Benchmark comparison of predictive performance and computational cost against MC-Dropout and Deep Ensembles

## Limitations

- Performance gains rely on accurate uncertainty decomposition, with K=2 components potentially underestimating epistemic uncertainty in certain scenarios
- ConvCNP's convolutional inductive bias assumes translation-equivariant spatial dynamics, which may not hold in regions with complex coastline geometries
- Greedy acquisition algorithm assumes predictive mean provides accurate estimates during placement optimization, potentially introducing bias
- Results validated on single test day rather than averaged over multiple days/seasonal variations

## Confidence

- **High Confidence:** Core mechanism of using epistemic uncertainty for sensor placement is well-supported by empirical results (RMSE improvement from 1.3°C to 1.1°C)
- **Medium Confidence:** Claim that approach outperforms total uncertainty-based methods requires validation across different environmental monitoring contexts
- **Medium Confidence:** Assertion that K=2 mixture components may underestimate uncertainty is theoretically sound but practical impact needs further quantification

## Next Checks

1. **Cross-domain validation:** Apply the same epistemic uncertainty-based acquisition to a different environmental monitoring dataset (e.g., air quality monitoring) to test generalizability across varying noise-to-signal ratios

2. **Ablation on mixture components:** Systematically compare sensor placement performance using K=2, K=4, and K=8 mixture components on the Baltic Sea dataset to quantify trade-off between computational cost and uncertainty estimation accuracy

3. **Counterfactual sensor placement analysis:** For the same test day, simulate sensor placements using both epistemic and aleatoric uncertainty weighting and visualize spatial distribution differences to verify the paper's claim that epistemic-based placement avoids noisy regions