---
ver: rpa2
title: Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank
  Representation
arxiv_id: '2505.00009'
source_url: https://arxiv.org/abs/2505.00009
tags:
- tasks
- knowledge
- task
- shared
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-task learning for language
  models by proposing a method that efficiently captures task-specific heterogeneity
  while preserving shared knowledge. The core idea is to employ low-rank representations
  with a fast-slow weight mechanism, where slow weights encode shared knowledge and
  fast weights capture task-specific nuances, avoiding the entanglement of these two
  types of knowledge.
---

# Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation

## Quick Facts
- **arXiv ID:** 2505.00009
- **Source URL:** https://arxiv.org/abs/2505.00009
- **Reference count:** 40
- **Primary result:** State-of-the-art performance in both full-data and few-shot settings on 16 Chinese NLP tasks while using only a small fraction of parameters.

## Executive Summary
This paper proposes TA-LoRA, a method for efficient knowledge transfer in multi-task learning that captures task-specific heterogeneity while preserving shared knowledge through low-rank representations. The approach employs a fast-slow weight mechanism where shared knowledge is encoded in slow weights and task-specific nuances in fast weights, preventing their entanglement. A zero-initialized attention mechanism ensures training stability. Experiments demonstrate superior performance compared to full fine-tuning and other parameter-efficient methods on both source and unseen target tasks, with significant parameter efficiency.

## Method Summary
TA-LoRA builds on prompt tuning by decomposing adaptable prompt vectors into shared knowledge (θ₀) and task-specific deviations (θᵢ - θ₀), which are approximated as low-rank products of matrices. The method inserts low-rank representation modules into the last 14 layers of a frozen PLM (Qwen2.5-7B), where the shared matrix B is treated as a slow weight and task-specific matrices Aᵢ (decomposed as rank-1 outer products uᵢ ⊗ vᵢ) as fast weights with different learning rates. A zero-initialized attention mechanism with learnable gating controls the contribution of task-specific components during training. The approach includes orthogonal regularization and is evaluated on 16 Chinese NLP tasks in both full-data and few-shot settings.

## Key Results
- Achieves state-of-the-art performance on 16 Chinese NLP tasks in both full-data and few-shot (16/32/64-shot) settings
- Outperforms full fine-tuning and other parameter-efficient methods while using only a small fraction of parameters
- Demonstrates effective knowledge transfer from source tasks to unseen target tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task heterogeneity can be efficiently approximated via low-rank representations in the parameter space.
- Mechanism: Decomposes adaptable prompt vectors into shared knowledge (θ₀) and task-specific deviations (θᵢ - θ₀), then approximates these deviations as low-rank products of matrices. The fast weight Aᵢ is further decomposed into an outer product of two vectors (uᵢ ⊗ vᵢ), creating a rank-1 matrix.
- Core assumption: Primary variations in task-specific feature distributions are low-rank and can be disentangled from shared knowledge.
- Evidence anchors: Abstract states "employing the low-rank representation to model task heterogeneity"; section notes "finite degrees of freedom inherent in low-rank structures can effectively capture the primary directions of variation in parameter updates."
- Break condition: If task-specific knowledge is inherently high-rank and cannot be projected onto a low-dimensional subspace without significant information loss, the approximation will fail to capture nuances required for strong performance.

### Mechanism 2
- Claim: Fast-slow weight mechanism with differential learning rates prevents entanglement of shared and task-specific knowledge during multi-task training from scratch.
- Mechanism: Shared low-rank matrix B is treated as "slow weight" updated with smaller learning rate to encode cross-task knowledge; task-specific matrices Aᵢ (decomposed into uᵢ and vᵢ) are "fast weights" updated with larger learning rate to quickly adapt to individual task nuances.
- Core assumption: Shared knowledge is more stable and task-specific knowledge is more volatile, and their learning dynamics can be controlled independently through learning rates.
- Evidence anchors: Abstract states "slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing"; section describes assigning different learning rates to B and A.
- Break condition: If gradient updates from diverse tasks interfere destructively even with different learning rates, or if shared knowledge is not sufficiently stable, B matrix may fail to converge to useful shared representation.

### Mechanism 3
- Claim: Zero-initialized attention mechanism with learnable gating factor ensures training stability by preventing random low-rank representations from disrupting pretrained model's original tokens during warm-up.
- Mechanism: Attention scores for adaptable prompt tokens are initially zeroed out; learnable gating factor gₗ (initialized to zero, passed through tanh) gradually amplifies task-specific components during training.
- Core assumption: Random initialization of low-rank matrices introduces noise harmful to pretrained model's existing knowledge representations in early stages of training.
- Evidence anchors: Abstract states "zero-initialized attention mechanism is introduced to ensure stability during training"; section describes learnable gating factor controlling attention scores.
- Break condition: If gating factor fails to learn to increase from zero, task-specific prompt tokens will remain inactive; if warm-up period is too short or gating increases too rapidly, instability could still occur.

## Foundational Learning

- **Concept:** Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: TA-LoRA is built on prompt tuning, a PEFT method; understanding the goal of adapting a large frozen model using very small number of parameters is essential.
  - Quick check question: Can you explain the difference between full fine-tuning and prompt tuning?

- **Concept:** Low-Rank Matrix Approximation
  - Why needed here: Core of proposed method is representing task heterogeneity as product of low-rank matrices; understanding matrix decomposition and rank is critical.
  - Quick check question: How can a large matrix be approximated by two smaller matrices, and what does "rank" signify in this context?

- **Concept:** Multi-Task Learning (MTL) and Transfer Learning
  - Why needed here: Entire problem framing is around transferring shared knowledge from source tasks to improve performance on target tasks, both seen and unseen.
  - Quick check question: How does training on multiple source tasks simultaneously differ from training separate models, and what is "negative transfer"?

## Architecture Onboarding

- **Component map:** Frozen Pre-trained Language Model (PLM) backbone (Qwen2.5-7B) -> Trainable low-rank representation modules (B shared, Aᵢ rank-1) inserted into last 14 layers -> Modified self-attention with zero-initialized gating factor -> Orthogonal regularization loss
- **Critical path:** Fast-slow weight mechanism is most critical; if learning rates for B and Aᵢ are not tuned correctly, shared and task-specific knowledge will entangle, negating primary benefit. Zero-initialized attention is also critical for first few epochs.
- **Design tradeoffs:** Main tradeoff is between parameter efficiency and representational capacity; choice to make Aᵢ rank-1 matrix maximizes efficiency but severely limits capacity for each task, traded off against shared matrix B which provides bulk of capacity.
- **Failure signatures:** Catastrophic forgetting or poor performance on target tasks (B matrix failed to capture useful shared knowledge); no improvement over baseline single-task model (low-rank approximation too restrictive or zero-initialized gate has not opened); instability in early training loss (zero-initialized attention mechanism implemented incorrectly or gating factor learning rate too high).
- **First 3 experiments:**
  1. Ablation of Fast-Slow Weights: Train model with same learning rate for B and A; compare performance on unseen tasks to full model.
  2. Ablation of Zero-Initialized Attention: Train model with standard self-attention instead of gated variant; monitor early training loss curves and final performance on source tasks.
  3. Rank vs. Performance Analysis: Experiment with increasing rank of task-specific matrix Aᵢ (from rank-1 to rank-2, rank-4); measure trade-off between parameter count and performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the rank-1 constraint for task-specific fast weights (Aᵢ) sufficient for highly complex tasks, or does it create a representational bottleneck?
- Basis in paper: Methodology restricts Aᵢ to rank-1 structure (uᵢ ⊗ vᵢ) to reduce complexity and regulate capacity, but provides no ablation on impact of higher ranks.
- Why unresolved: Paper demonstrates rank-1 prevents overfitting on selected benchmarks but does not verify if tasks with higher complexity or wider distribution shifts would benefit from increased rank.
- What evidence would resolve it: Ablation study comparing rank-1 against rank-r (where r > 1) task-specific matrices on datasets with increasing feature diversity and complexity.

### Open Question 2
- Question: Does assumption that "later layers capture task-specific features" generalize to encoder-only or encoder-decoder architectures?
- Basis in paper: Framework justifies injecting low-rank representations into final L layers based on interlayer similarity analysis of Qwen (decoder-only), but does not validate this pattern on other architectures.
- Why unresolved: Paper relies on specific hierarchical nature of Qwen; different architectures (BERT, T5) may distribute shared and task-specific knowledge differently across layers.
- What evidence would resolve it: Experiments applying TA-LoRA to encoder-only (BERT) and encoder-decoder (T5) models, accompanied by analysis of their layer-wise similarity dynamics.

### Open Question 3
- Question: How does orthogonality constraint on task-specific matrices impact performance when source tasks exhibit high gradient conflict or negative transfer?
- Basis in paper: Method enforces orthogonality among task-specific matrices (AᵀᵢAⱼ) via regularization term, implicitly assuming task-specific knowledge is separable and independent.
- Why unresolved: While this prevents mixing, it may restrict model's ability to learn beneficial interactions or shared nuisances between tasks that are not orthogonal in feature space.
- What evidence would resolve it: Evaluation on multi-task benchmark with known conflicting objectives to measure trade-off between orthogonality enforcement and negative transfer mitigation.

## Limitations
- Rank-1 constraint for task-specific fast weights may severely constrain model's ability to capture complex, high-dimensional task-specific variations
- Fast-slow weight mechanism lacks rigorous theoretical justification for why differential learning rates would successfully prevent knowledge entanglement in all multi-task scenarios
- Zero-initialized attention mechanism introduces additional hyperparameter (gating factor's learning rate) that could become source of instability if not carefully tuned

## Confidence
- **High Confidence**: General framework of using low-rank adaptations for parameter-efficient multi-task learning is well-established and experimental results showing state-of-the-art performance across multiple benchmarks are likely reproducible
- **Medium Confidence**: Specific choice of rank-1 decomposition for task-specific matrices and effectiveness of fast-slow weight mechanism with differential learning rates are more dependent on empirical validation and may not generalize across all task distributions
- **Low Confidence**: Theoretical guarantees of zero-initialized attention mechanism for training stability and precise mechanisms by which low-rank approximation captures task heterogeneity are not rigorously proven

## Next Checks
1. **Rank Sensitivity Analysis**: Systematically vary rank of task-specific matrix Aᵢ from 1 to 4 and measure trade-off between parameter count and performance on both source and unseen target tasks to quantify limitations of rank-1 assumption.
2. **Learning Rate Sensitivity Test**: Conduct ablation study comparing models trained with (a) shared learning rates for B and Aᵢ, (b) proposed differential learning rates, and (c) reversed learning rates (fast for B, slow for Aᵢ) to directly validate whether fast-slow mechanism prevents knowledge entanglement.
3. **Cross-Architecture Transferability**: Evaluate TA-LoRA on non-Chinese, non-Qwen2.5 backbone (e.g., LLaMA-7B on English GLUE tasks) to test whether method's effectiveness is tied to specific model architectures or datasets, or if low-rank heterogeneity approximation is truly general.