---
ver: rpa2
title: Towards Understanding and Improving Refusal in Compressed Models via Mechanistic
  Interpretability
arxiv_id: '2504.04215'
source_url: https://arxiv.org/abs/2504.04215
tags:
- refusal
- arxiv
- safety
- direction
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how model compression affects the safety
  mechanism of refusal in large language models. It demonstrates that compressed models
  retain a single refusal direction mediating safety behaviors, but pruning alters
  the source position and direction quality, leading to degraded trustworthiness.
---

# Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability

## Quick Facts
- **arXiv ID**: 2504.04215
- **Source URL**: https://arxiv.org/abs/2504.04215
- **Reference count**: 40
- **Primary result**: Compressed models retain a single refusal direction mediating safety behaviors, but pruning degrades safety by altering the direction's source position and quality, while quantization better preserves it; AIRD restores safety in pruned models by orthogonalizing weights relative to the original refusal direction, improving attack resistance by up to 41% without harming performance.

## Executive Summary
This paper investigates how model compression affects safety mechanisms in large language models, specifically focusing on refusal behavior. The authors discover that compressed models retain a single refusal direction that mediates safety behaviors, but pruning methods disrupt this mechanism by shifting the source position and degrading direction quality, leading to reduced safety. In contrast, quantization methods preserve the original refusal direction and thus maintain better safety post-compression. The paper introduces AIRD (Aligned Intervention for Refusal Direction), a lightweight method that restores safety in pruned models by orthogonalizing weight matrices relative to the original refusal direction, improving attack resistance by up to 41% without degrading performance.

## Method Summary
The authors extract refusal directions from base models using difference-in-means vectors between harmful and harmless prompt activations. They compress Llama-2-7B and Llama-3-8B models using Wanda and magnitude pruning (50% sparsity) and LLM.int8/AWQ quantization (8-bit). For pruned models where the refusal direction source position or quality degrades, they apply AIRD by orthogonalizing output projection weights at the original refusal direction's source layer. Safety is evaluated using attack success rate (ASR) across three attack types (vanilla, decoding-based, suffix-based) on AdvBench, while performance is measured via zero-shot accuracy on five LM Harness tasks.

## Key Results
- Compressed models retain a single refusal direction mediating safety behaviors across all compression methods tested
- Pruning alters the source position and direction quality, leading to degraded trustworthiness, while quantization better preserves the original refusal mechanism
- AIRD restores safety in pruned models by orthogonalizing weight matrices relative to the original refusal direction, improving attack resistance by up to 41% without harming performance
- The underlying refusal mechanism remains interpretable after AIRD application, preserving compatibility with other steering methods

## Why This Works (Mechanism)

### Mechanism 1: Single Refusal Direction Persists Post-Compression
Compressed models retain a single direction in the residual stream that mediates refusal behavior. The difference-in-means vector between harmful and harmless activations remains identifiable and causally efficacious after both pruning and quantization. This finding holds true for every compression method tested, model architecture/size, and calibration dataset.

### Mechanism 2: Pruning Alters Refusal Direction Source and Quality
Pruning shifts both the layer/token position of the refusal direction and its orientation relative to the original, correlating with safety degradation. Weight zeroing via pruning disrupts the projection pathways that generate the original refusal direction, forcing the mechanism to emerge from alternative positions. Pruned models show cosine similarities of 0.337-0.902 vs. 0.99+ for quantized models.

### Mechanism 3: AIRD Restores Safety via Weight Orthogonalization
Orthogonalizing output projection weights relative to the original refusal direction can partially restore safety without altering the underlying mechanism. AIRD modifies weights as W_new ← W_compressed + αr_original(r_original)^T W_compressed, injecting the original refusal direction into the pruned model's projection space.

## Foundational Learning

- **Concept: Difference-in-Means Vector Extraction**
  - Why needed here: The refusal direction r is computed by averaging activations on harmful vs. harmless prompts and taking their difference; this is the foundation for all subsequent analysis.
  - Quick check question: Given activations μ_harmful and ν_harmless at layer l, how would you compute the refusal direction?

- **Concept: Directional Ablation (Necessity Testing)**
  - Why needed here: Ablating r via x' ← x − r̂r̂^Tx tests whether the direction is necessary for refusal; success indicates causal mediation.
  - Quick check question: If ablating a direction increases ASR from 0.02 to 0.70, what does this imply about that direction's role?

- **Concept: Cosine Similarity for Direction Quality**
  - Why needed here: The paper uses cosine similarity between original and compressed refusal directions as a proxy for mechanism preservation; low values indicate disruption.
  - Quick check question: A pruned model has cosine similarity 0.35 with the original refusal direction. What does this suggest about safety preservation?

## Architecture Onboarding

- **Component map**: Residual stream activations (x_i^l) at layer l, token position i → Attention output projections (W_o) and MLP output projections (W_down) → Difference-in-means vector (r) computed from harmful/harmless activation differences → Directional ablation operator (removes component along r̂)

- **Critical path**: 1) Extract refusal direction r from base model via difference-in-means (harmful vs. harmless prompts) 2) Compress model (pruning via Wanda/magnitude; quantization via LLM.int8()/AWQ) 3) Re-extract refusal direction r_compressed from compressed model 4) Compare: source position (layer/token), cosine similarity 5) Apply AIRD if source/quality changed: orthogonalize output projections at original source layer

- **Design tradeoffs**: Pruning (50% sparsity): Higher compression, but refusal direction shifts → safety degradation; Quantization (8-bit): Lower compression than aggressive pruning, but preserves refusal direction (cosine sim > 0.99); AIRD: Restores safety at cost of reduced sparsity in one layer (adds non-zero weights)

- **Failure signatures**: High ASR in pruned models with low cosine similarity (< 0.6) to original refusal direction; Refusal direction source position shifting from layer 14 to layer 12 (or similar early-layer shifts); AIRD causing over-refusal on benign prompts

- **First 3 experiments**: 1) Replicate refusal direction extraction on Llama-2-7B-Chat: compute r using 128 harmful/harmless pairs, validate via directional ablation (necessity) and activation addition (sufficiency) 2) Compress with Wanda (50% sparsity, Alpaca calibration): measure ASR on AdvBench, extract new refusal direction, compute cosine similarity to original 3) Apply AIRD to Wanda-pruned model with α = 0.01 (MLP) / 0.02 (Attention): re-evaluate ASR and zero-shot benchmarks to confirm safety restoration without performance loss

## Open Questions the Paper Calls Out

### Open Question 1
Do the findings regarding the single refusal direction and AIRD's effectiveness generalize to non-Transformer architectures, such as Mixture of Experts (MoE), State Space Models (SSMs), or modernized RNNs? The study exclusively evaluated decoder-only Transformers (Llama-2, Llama-3), leaving the internal safety mechanisms of MoE or SSM models unexplored.

### Open Question 2
Can AIRD be modified to restore safety in pruned models without compromising the theoretical compression rate by reintroducing non-zero weights? The current AIRD method orthogonalizes weight matrices, which changes zeroed-out weights back to non-zero values, partially undoing the pruning process.

### Open Question 3
What specific properties of calibration data (e.g., domain, toxicity, diversity) determine whether the refusal direction shifts its source position during pruning? The paper observes the correlation between calibration data and direction degradation but does not isolate which features of the calibration data (safety vs. utility focus) cause the source position to change.

## Limitations
- The analysis relies on a single mechanistically interpretable safety circuit (refusal direction via difference-in-means), but the universality of this mechanism across diverse safety threats and model architectures remains incompletely validated
- AIRD's safety restoration depends critically on the original refusal direction remaining a valid safety target after compression—if the compressed model's representation space fundamentally shifts, orthogonalization may not generalize
- The paper demonstrates safety restoration on pre-specified attack benchmarks with fixed attack parameters, but real-world adversarial robustness likely requires evaluation against adaptive attacks that exploit AIRD-specific vulnerabilities

## Confidence
- **High confidence**: The persistence of a single refusal direction in compressed models is well-supported by systematic measurement across architectures, compression methods, and calibration datasets
- **Medium confidence**: The claim that cosine similarity between original and compressed refusal directions meaningfully reflects mechanism preservation quality, while intuitively sound, lacks direct causal validation
- **Medium confidence**: AIRD's safety restoration mechanism is well-validated on the tested benchmarks, but its general robustness against adaptive attacks and in production scenarios remains to be proven

## Next Checks
1. **Adaptive Attack Validation**: Apply gradient-based attacks that specifically target AIRD-augmented weights to assess whether AIRD introduces new vulnerabilities beyond those present in baseline compressed models

2. **Compression Ratio Scaling Study**: Systematically vary pruning ratios (25%, 50%, 75%, 90% sparsity) and quantization bit-widths (4-bit, 6-bit, 8-bit) to characterize how refusal direction preservation quality and AIRD effectiveness scale with compression intensity

3. **Cross-Architecture Generalization**: Apply the complete methodology to diverse model architectures including decoder-only, encoder-decoder, and MoE models to test whether the single-direction mechanism and AIRD approach generalize beyond the Llama family