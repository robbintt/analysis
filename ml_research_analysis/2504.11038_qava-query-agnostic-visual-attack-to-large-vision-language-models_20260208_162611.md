---
ver: rpa2
title: 'QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models'
arxiv_id: '2504.11038'
source_url: https://arxiv.org/abs/2504.11038
tags:
- attack
- adversarial
- questions
- image
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QAVA, a query-agnostic visual attack method
  that generates adversarial examples to mislead large vision-language models (LVLMs)
  when answering unknown questions. Unlike traditional attacks targeting specific
  image-question pairs, QAVA samples a set of random, image-unrelated questions and
  attacks the visual-language alignment module output.
---

# QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models

## Quick Facts
- arXiv ID: 2504.11038
- Source URL: https://arxiv.org/abs/2504.11038
- Reference count: 20
- Key outcome: QAVA achieves attack performance comparable to known-target-question attacks while requiring no knowledge of target questions, reducing VQA scores by 33.15 points on VQA v2

## Executive Summary
This paper introduces QAVA, a query-agnostic visual attack method that generates adversarial examples to mislead large vision-language models (LVLMs) when answering unknown questions. Unlike traditional attacks targeting specific image-question pairs, QAVA samples a set of random, image-unrelated questions and attacks the visual-language alignment module output. The method achieves attack performance comparable to known-target-question attacks while requiring no knowledge of the target questions. Experiments show QAVA reduces VQA scores by 33.15 points (from 78.00 to 44.85) on VQA v2 using random questions, demonstrating strong white-box and black-box attack effectiveness across multiple LVLMs including InstructBLIP, BLIP-2, and MiniGPT-4, with good transferability to image captioning tasks.

## Method Summary
QAVA attacks the visual-language alignment module (Q-Former) of LVLMs by maximizing the distance between adversarial and clean visual features at this intermediate stage. The method uses random, image-unrelated questions as surrogate queries to generate perturbations that corrupt the visual grounding universally. The optimization process employs either PGD or C&W methods to balance perturbation magnitude against attack effectiveness, targeting the MSE loss between clean and adversarial Q-Former outputs. This approach bypasses the robustness often found in larger LLM parameters and achieves efficiency gains of 80% memory reduction compared to end-to-end attacks.

## Key Results
- QAVA reduces VQA scores by 33.15 points (from 78.00 to 44.85) on VQA v2 using random questions
- Attack performance comparable to known-target-question attacks despite not knowing target questions
- 80% memory reduction compared to end-to-end LLM loss optimization
- Strong transferability across multiple LVLMs including InstructBLIP, BLIP-2, and MiniGPT-4
- Effective black-box attacks through cross-model transferability

## Why This Works (Mechanism)

### Mechanism 1
Targeting the visual-language alignment module (e.g., Q-Former) creates a more effective query-agnostic attack than optimizing the end-to-end LLM output loss. The alignment module projects visual features into the LLM's input space. By maximizing the distance (MSE) between the adversarial and clean visual features at this intermediate stage, the attack corrupts the visual grounding universally, causing the LLM to hallucinate or fail regardless of the specific text prompt. This bypasses the robustness often found in the larger LLM parameters.

### Mechanism 2
Random, image-unrelated questions can serve as efficient surrogates for specific target questions during the attack generation process. The attack optimizes the perturbation to maximize feature deviation conditional on text inputs. Using a diverse set of random text inputs (rather than a specific target question) forces the adversarial perturbation to disrupt the general visual-text alignment space, resulting in a "universal" perturbation for the image.

### Mechanism 3
Minimizing the L2 distance constraint ($||x-x'||_2$) while maximizing feature loss allows for imperceptible yet potent attacks. The optimization (via C&W or PGD) balances the visual magnitude of the noise against the attack success. By confining noise to an $\epsilon$-ball (imperceptibility) while aggressively pushing feature outputs apart, the image retains semantic meaning to humans but appears "broken" or "shifted" to the model.

## Foundational Learning

- **Concept: Vision-Language Alignment (Q-Former)**
  - Why needed here: This is the specific architectural component QAVA targets. You must understand that it acts as an interface transforming image patches into token embeddings for the LLM.
  - Quick check question: Does the Q-Former output depend on both the image and the text query simultaneously? (Answer: Yes, in models like InstructBLIP).

- **Concept: White-box vs. Black-box Attacks**
  - Why needed here: The paper evaluates QAVA in both settings. You need to distinguish between having gradient access (white-box) vs. only query access (black-box/transferability).
  - Quick check question: In the transferability experiments (Table 10), is the attack on the target model (columns) white-box or black-box? (Answer: Black-box, as gradients come from the surrogate model/rows).

- **Concept: Perturbation Constraints ($\epsilon$)**
  - Why needed here: The paper uses PGD and C&W, which rely on $L_\infty$ and $L_2$ norms to ensure the noise is small enough to go unnoticed.
  - Quick check question: If $\epsilon$ is set too high, the image becomes visually corrupted; if too low, the attack fails. Which table analyzes this trade-off? (Answer: Table 7).

## Architecture Onboarding

- **Component map:** Input Image -> Visual Encoder -> Q-Former (with Random Questions) -> MSE Loss -> Optimization Loop -> Adversarial Image

- **Critical path:**
  The attack pipeline deviates from standard inference at the Loss Node. Instead of calculating Cross-Entropy on the LLM output (standard), QAVA calculates MSE on the Q-Former embeddings. This reduces GPU memory usage (no LLM backprop) and speeds up the attack (Table 8).

- **Design tradeoffs:**
  - Efficiency vs. Precision: QAVA is faster and lighter (80% memory reduction) but may be less precise than a targeted LLM-loss attack if the goal is a specific wrong answer rather than any wrong answer.
  - Surrogate Selection: The paper notes that VQG (generating relevant questions) is computationally expensive, while RSQ (random sampling) is cheap and nearly as effective.

- **Failure signatures:**
  1. Overfitting to Surrogate: If attack success on surrogate questions is high but target questions remain correct, the perturbation failed to generalize.
  2. Visible Noise: If $\epsilon$ is too large (e.g., >16/255), the image is obviously corrupted (Figure 3).
  3. Model Ignore: Some robust models may output "I don't know" or ignore the image if the Q-Former output is nonsensical, which counts as a safety success but an availability failure.

- **First 3 experiments:**
  1. Baseline Efficiency Check: Run `L_QAVA` vs `L_LLM` on a single image-question pair. Monitor GPU memory and time to confirm the reported 80% memory savings (Table 8).
  2. Question Strategy Ablation: Generate adversarial images using RSQ (random) vs. WTQ (white-box target). Evaluate both on a held-out set of questions to verify the "Query-Agnostic" claim (Table 2).
  3. Transfer Test: Generate an attack on InstructBLIP and test it on LLaVA. Confirm if the Q-Former vulnerability transfers across different model architectures (Table 9).

## Open Questions the Paper Calls Out

- **Open Question 1:** Why do image-unrelated random questions effectively facilitate query-agnostic attacks on LVLMs? The paper empirically demonstrates that random questions work as well as image-related ones but lacks a theoretical justification for why unrelated text queries disrupt the visual-language alignment module's features.

- **Open Question 2:** What are the negative impacts of using QAVA adversarial examples for data poisoning during the pre-training or fine-tuning phases of LVLMs? While QAVA examples could be detrimental if used for data poisoning, this specific aspect was not evaluated experimentally.

- **Open Question 3:** Can QAVA adversarial examples transfer effectively to broader visual-language tasks beyond image captioning? The transferring attack was only evaluated for image captioning tasks, not for the wider range of visual-language tasks.

## Limitations
- Limited evaluation to VQA and image captioning tasks, leaving uncertainty about effectiveness on other LVLM applications
- No defensive strategies tested against QAVA attacks, potentially overestimating real-world vulnerability
- Uncertainty about whether Q-Former vulnerability is universal across all LVLM architectures or model-specific implementation detail

## Confidence

**High confidence in:** The architectural description of QAVA and its implementation details. The efficiency gains (80% memory reduction, faster attack times) are well-documented with clear computational metrics.

**Medium confidence in:** The claim that QAVA achieves comparable performance to known-target-question attacks. While Table 5 shows numerical comparability, the qualitative differences in attack behavior between random and specific question strategies remain unclear.

**Low confidence in:** The universal applicability of QAVA across all LVLM architectures and datasets. The paper demonstrates success on three specific models but does not establish whether the Q-Former vulnerability is a fundamental weakness or model-specific implementation detail.

## Next Checks
1. **Cross-architecture Transferability Test:** Generate QAVA attacks on InstructBLIP and evaluate them on at least two additional LVLM architectures not included in the original study (e.g., LLaVA-Next, Flamingo) to assess whether the Q-Former vulnerability is architecture-independent.

2. **Question Distribution Analysis:** Systematically vary the random question sampling strategy (e.g., domain-specific questions vs. truly random questions) to determine if certain question distributions create more effective surrogate attacks, establishing theoretical bounds on transferability.

3. **Defense Evaluation:** Implement and test two simple defensive strategies: (a) adversarial training on Q-Former features, and (b) output sanitization that detects abnormal Q-Former embeddings. Measure attack success rates against these defenses to assess practical vulnerability.