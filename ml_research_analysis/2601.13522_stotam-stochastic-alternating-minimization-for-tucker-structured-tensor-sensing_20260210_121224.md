---
ver: rpa2
title: 'StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing'
arxiv_id: '2601.13522'
source_url: https://arxiv.org/abs/2601.13522
tags:
- tensor
- stochastic
- recovery
- stotam
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a stochastic alternating minimization algorithm
  (StoTAM) for Tucker-structured tensor sensing, addressing the problem of recovering
  low-Tucker-rank tensors from linear measurements. The method operates directly on
  Tucker factorized representations, avoiding expensive full-tensor projections and
  enabling efficient mini-batch updates.
---

# StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing

## Quick Facts
- arXiv ID: 2601.13522
- Source URL: https://arxiv.org/abs/2601.13522
- Authors: Shuang Li
- Reference count: 39
- Key outcome: Proposed StoTAM algorithm achieves faster convergence than StoTAM baseline, recovering tensors in approximately 1 second versus much longer times on 10×10×15 third-order tensors

## Executive Summary
This paper introduces StoTAM (Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing), a novel algorithm for recovering low-Tucker-rank tensors from linear measurements. The method operates directly on Tucker factorized representations, avoiding expensive full-tensor projections through efficient mini-batch updates. The algorithm updates the core tensor via closed-form least-squares solutions while factor matrices are updated using stochastic gradients on Stiefel manifolds with QR retractions. Experimental results demonstrate significantly faster convergence compared to stochastic tensor iterative hard thresholding baselines on synthetic third-order tensor recovery problems.

## Method Summary
StoTAM employs a two-phase alternating minimization strategy for Tucker-structured tensor sensing. The core tensor update uses closed-form least-squares solutions that exploit the Tucker factorization structure, eliminating the need for full tensor reconstruction. Factor matrix updates utilize stochastic gradient descent on the Stiefel manifold, maintaining orthogonality constraints through QR-based retractions. The algorithm processes data in mini-batches, enabling efficient scaling and faster convergence compared to full-batch alternatives. This approach avoids the computational bottleneck of projecting onto Tucker manifolds at each iteration, which is required in deterministic methods.

## Key Results
- StoTAM achieves numerical accuracy recovery within approximately 1 second on 10×10×15 third-order tensors
- Demonstrates significantly faster convergence than StoTAM (stochastic tensor iterative hard thresholding) baseline
- Maintains Tucker structure throughout optimization without expensive full-tensor projections

## Why This Works (Mechanism)
The efficiency gains stem from operating directly on the factorized Tucker representation rather than reconstructing full tensors at each iteration. The closed-form core tensor update leverages the linear structure of the measurement operator when expressed in Tucker format, reducing computational complexity from O(n^p) to O(r^p) where r represents Tucker ranks. The Stiefel manifold optimization for factor matrices preserves orthogonality constraints naturally while enabling efficient stochastic updates. Mini-batch processing further accelerates convergence by providing more frequent parameter updates compared to full-batch methods.

## Foundational Learning

**Tucker Decomposition**: Multi-linear factorization representing tensors as core tensors multiplied by factor matrices along each mode. Needed for dimensionality reduction and computational efficiency in tensor operations. Quick check: Verify rank-r approximation preserves essential tensor structure.

**Stiefel Manifold Optimization**: Optimization over matrices with orthonormal columns (St(X) = {X: X'X = I}). Required to maintain factor matrix orthogonality constraints during updates. Quick check: Confirm gradient descent steps preserve manifold constraints via QR retraction.

**Mini-batch Stochastic Optimization**: Processing subsets of data for parameter updates instead of full datasets. Essential for computational efficiency and faster convergence in large-scale problems. Quick check: Validate variance reduction with increasing batch sizes.

## Architecture Onboarding

**Component Map**: Data -> Measurement Operator -> Tucker Factors -> Core Tensor Update -> Factor Matrix Update -> Recovered Tensor

**Critical Path**: Measurement operator application to Tucker factors → Core tensor least-squares update → Factor matrix Stiefel manifold optimization → Convergence check

**Design Tradeoffs**: Mini-batch vs full-batch updates (speed vs variance), closed-form vs iterative core updates (efficiency vs generality), Stiefel manifold vs unconstrained optimization (constraint preservation vs flexibility)

**Failure Signatures**: Ill-conditioned measurement matrices leading to unstable core updates, rank underspecification causing poor approximations, insufficient batch sizes resulting in high variance updates

**First Experiments**: 1) Convergence comparison on varying tensor orders and dimensions, 2) Sensitivity analysis to Tucker rank selection, 3) Robustness testing under different noise levels and measurement patterns

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Experimental validation limited to third-order tensors with small dimensions (10×10×15)
- Comparison restricted to single baseline method (StoTAM) without broader benchmarking
- No analysis of convergence guarantees under non-ideal conditions like ill-conditioned matrices or noisy observations
- Computational advantages over deterministic methods for small-scale problems not clearly justified

## Confidence

**High**: The algorithm's core design and implementation details are well-specified and technically sound for the described setting

**Medium**: The convergence speed improvements over StoTAM are demonstrated but limited to specific synthetic scenarios

**Low**: Generalizability to real-world applications, higher-order tensors, and noisy/corrupted measurements remains unverified

## Next Checks

1. Evaluate performance on higher-order tensors (4th order and above) with varying ranks and dimensions to assess scalability

2. Test algorithm robustness under different noise levels and measurement matrix conditions, including ill-conditioned and sparse sampling patterns

3. Benchmark against additional tensor completion methods (e.g., HaLRTC, tensor nuclear norm approaches) on both synthetic and real-world datasets to establish relative performance across problem types