---
ver: rpa2
title: 'Efficient LLMs with AMP: Attention Heads and MLP Pruning'
arxiv_id: '2504.21174'
source_url: https://arxiv.org/abs/2504.21174
tags:
- pruning
- attention
- language
- heads
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes AMP, a structured pruning method that removes\
  \ less critical attention heads and MLP neurons in LLMs by projecting input data\
  \ onto weights to assess structural importance. AMP achieves up to 1.25\xD7 inference\
  \ speedup while surpassing state-of-the-art pruning techniques by up to 1.49 percentage\
  \ points in average task accuracy on commonsense reasoning benchmarks."
---

# Efficient LLMs with AMP: Attention Heads and MLP Pruning

## Quick Facts
- **arXiv ID**: 2504.21174
- **Source URL**: https://arxiv.org/abs/2504.21174
- **Reference count**: 40
- **Key outcome**: AMP achieves 1.25× inference speedup with 30% pruning ratio while surpassing state-of-the-art pruning techniques by up to 1.49 percentage points in average task accuracy on commonsense reasoning benchmarks.

## Executive Summary
AMP introduces a structured pruning method for large language models that removes less critical attention heads and MLP neurons based on activation magnitude projections. The method achieves practical inference speedups of up to 1.25× while maintaining model performance, surpassing state-of-the-art pruning techniques by up to 1.49 percentage points in average task accuracy. AMP requires only minutes to identify components for removal and completes the full pruning and fine-tuning process in approximately four hours using a single RTX 3090 GPU.

## Method Summary
AMP prunes attention heads and MLP neurons by measuring their contribution to the residual stream using activation magnitudes from a small calibration set. For attention heads, it computes the ℓ₁ norm of the projected output to measure importance. For MLP components, it measures the magnitude of element-wise products of gate and up projections. The method applies uniform pruning ratios across all layers, removing the lowest-scoring components, then fine-tunes with LoRA adapters. Experiments demonstrate effectiveness on LLaMA and Phi model families with 30% pruning ratios and minimal impact on zero-shot task performance.

## Key Results
- Achieves 1.25× inference speedup on RTX 3090 while maintaining 30% pruning ratio
- Outperforms state-of-the-art pruning techniques by up to 1.49 percentage points on commonsense reasoning benchmarks
- Completes pruning and fine-tuning in approximately four hours using a single RTX 3090 GPU
- Maintains model performance with minimal impact on zero-shot task accuracy

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Projecting input data onto weights discerns critical components better than magnitude-only heuristics.
**Mechanism**: AMP calculates importance using activation magnitudes rather than static weight values. For Attention Heads, it computes the ℓ₁ norm of the projected output (hₙWₙ) to measure contribution to the residual stream. For MLP (SwiGLU), it measures the magnitude of the element-wise product of Gate and Up projections.
**Core assumption**: A small calibration set (e.g., 50 samples) generates activation statistics that generalize to the model's full functional distribution.
**Evidence anchors**: [abstract] "...by projecting the input data onto weights, AMP assesses structural importance..."; [section III.C] "This distinction highlights the unique approach of measuring the importance... differing from the common pruning strategy of simply applying the ℓₚ-norm."
**Break condition**: If the calibration data distribution diverges significantly from the deployment domain, the importance scores may rank specialized but rare capabilities as "unimportant."

### Mechanism 2
**Claim**: Uniform structured pruning preserves the transformer architecture while enabling hardware-agnostic acceleration.
**Mechanism**: AMP prunes a fixed percentage of heads and neurons across all layers. This maintains consistent tensor dimensions internally, avoiding the need for specialized compilers to handle irregular sparsity or "jagged" layer widths.
**Core assumption**: Redundancy is distributed uniformly enough across layers that a global pruning ratio does not catastrophically destroy specific critical layers.
**Evidence anchors**: [abstract] "...achieving 30% pruning ratios... practical inference speedups of up to 1.25× without requiring specialized hardware."; [section III.C] "...we prune c% of attention heads and c% of MLP neurons with the lowest importance uniformly across all layers..."
**Break condition**: If specific layers (e.g., the first or last layer) act as bottlenecks with zero redundancy, uniform pruning may sever critical information pathways.

### Mechanism 3
**Claim**: Pruning both MHA and MLP components simultaneously yields higher compression efficiency than isolating either.
**Mechanism**: By targeting both the attention mechanism (contextualization) and the MLP (knowledge storage/processing), AMP distributes the parameter reduction load. Pruning only MLPs limits max compression (as MHA accounts for only ~33% of params in LLaMA); pruning only MHA destroys performance quickly.
**Core assumption**: The model contains over-parameterized redundancy in both representational spaces (attention and feed-forward).
**Evidence anchors**: [section IV.E] "...removal of only MLP neurons also leads to a decrease in performance... compared to pruning both components together."; [table III] Shows that pruning only MHA at 30% ratio (removing nearly all heads) causes collapse.
**Break condition**: If a model architecture heavily favors one component (e.g., heavy attention, light MLP), the uniform application might over-prune the smaller component.

## Foundational Learning

**Structured vs. Unstructured Pruning**
*Why needed here*: To distinguish AMP from methods like Wanda or SparseGPT. AMP removes entire rows/columns/heads (Structured), resulting in smaller dense matrices, whereas unstructured pruning creates sparse matrices that often require specialized hardware to accelerate.
*Quick check question*: Does the pruned model result in a smaller standard matrix multiplication, or a sparse matrix multiplication?

**The Residual Stream**
*Why needed here*: AMP's importance metric is explicitly defined as a component's contribution to the residual stream (O = ΣhₙWₙ). Understanding this additive nature is key to understanding why removing low-magnitude contributions is hypothesized to be safe.
*Quick check question*: If you remove a head from the summation O = h₁W₁ + ... + hₙWₙ, does the dimension of O change?

**SwiGLU Architecture**
*Why needed here*: The MLP component of AMP is specific to SwiGLU (used in LLaMA/Phi), which has three projections (Up, Gate, Down). AMP prunes the input to the Down projection by zeroing pairs in Up/Gate.
*Quick check question*: Why must Up and Gate neurons be pruned in pairs in a SwiGLU architecture?

## Architecture Onboarding

**Component map**: Calibration data (Xsub) -> Forward Hook (extract hₙ and xW_Gate·xW_Up) -> Importance Aggregator (compute ℓ₁ norms) -> Pruner (sort and mask) -> Rewriter (physical weight removal) -> Trainer (LoRA fine-tuning)

**Critical path**:
1. **Forward Hook**: Implement hooks to capture hₙ (head outputs) and xW_Gate·xW_Up (MLP intermediate) without storing full gradients.
2. **Importance Aggregation**: Accumulate norms over the calibration set.
3. **Weight Surgery**: Physically resize W_Q, W_K, W_V, W_O (for heads) and W_Up, W_Gate, W_Down (for MLP). *Note: Adjusting W_Down requires identifying the corresponding input index.*

**Design tradeoffs**:
- **Uniform vs. Global Pruning**: The paper applies the same pruning ratio to every layer. This simplifies implementation but may over-prune sensitive layers (e.g., early embedding layers) or under-prune redundant middle layers.
- **Calibration Size**: Using only 50 samples makes it fast (minutes) but risks bias if samples aren't representative of the task domain.

**Failure signatures**:
- **Model Collapse**: PPL spikes to >100 (seen in "Reverse Pruning" ablation).
- **Shape Mismatch**: Errors in tensor multiplication usually indicate incorrect alignment between the pruned W_Up dimension and the expected input dimension of W_Down.
- **Negligible Speedup**: If structured pruning is applied but the batch size or sequence length is very small, memory bandwidth bottlenecks may hide the theoretical speedup.

**First 3 experiments**:
1. **Coherence Check (Sanity)**: Implement the "Reverse Pruning" test. If removing the highest importance scores results in better performance than your implementation of "standard" pruning, your importance calculation is likely inverted or broken.
2. **PPL Scaling**: Run AMP on WikiText2 without fine-tuning. Plot PPL vs. Pruning Ratio. If the curve is non-monotonic or flat, the pruning mask application may be incorrect.
3. **Inference Latency**: Benchmark the pruned model (Batch=1) against the dense model. Verify the claimed ~1.2x speedup on the target hardware to ensure the framework respects the smaller tensor dimensions.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on small calibration set (50 samples) may not capture rare but important model capabilities
- Uniform pruning ratio across all layers may over-prune sensitive layers or under-prune redundant ones
- Evaluation focuses primarily on zero-shot commonsense reasoning tasks, leaving domain generalization questions open

## Confidence

**High Confidence**: The core mechanism of using activation magnitudes for structured pruning is well-supported by the paper's results, showing consistent performance improvements over state-of-the-art methods across multiple benchmarks.

**Medium Confidence**: The claimed 1.25× inference speedup is hardware-specific and may not generalize across different GPU architectures or batch sizes.

**Low Confidence**: The calibration set size of 50 samples may be insufficient for capturing rare but important model capabilities, particularly for larger models with more diverse functional distributions.

## Next Checks

1. **Domain Generalization Test**: Evaluate AMP-pruned models on a diverse set of tasks including code generation (HumanEval), mathematical reasoning (MATH), and multilingual benchmarks to assess whether the 50-sample calibration generalizes beyond commonsense reasoning tasks.

2. **Layer-wise Sensitivity Analysis**: Implement layer-specific pruning ratios rather than uniform pruning to identify which layers are most sensitive to removal and whether adaptive pruning could improve the 30% ratio while maintaining or improving performance.

3. **Hardware Architecture Benchmarking**: Test AMP across different GPU architectures (e.g., A100, H100) and batch sizes to verify that the claimed 1.25× speedup is not hardware-specific and to identify optimal deployment configurations for different use cases.