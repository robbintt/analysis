---
ver: rpa2
title: Nonparametric learning of stochastic differential equations from sparse and
  noisy data
arxiv_id: '2508.11597'
source_url: https://arxiv.org/abs/2508.11597
tags:
- drift
- function
- algorithm
- given
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for learning drift functions
  of stochastic differential equations (SDEs) from sparse, noisy observations using
  reproducing kernel Hilbert spaces (RKHS) and an Expectation-Maximization (EM) algorithm
  with Sequential Monte Carlo (SMC) approximations. The method avoids parametric assumptions
  on the drift function, making it suitable for complex systems where prior knowledge
  is limited.
---

# Nonparametric learning of stochastic differential equations from sparse and noisy data

## Quick Facts
- arXiv ID: 2508.11597
- Source URL: https://arxiv.org/abs/2508.11597
- Reference count: 40
- Primary result: A nonparametric EM algorithm with SMC approximations learns drift functions from sparse, noisy SDE observations

## Executive Summary
This paper addresses the challenge of learning drift functions in stochastic differential equations (SDEs) from sparse and noisy observations. The authors develop a framework that avoids parametric assumptions by placing the drift function in a reproducing kernel Hilbert space (RKHS) and using an Expectation-Maximization (EM) algorithm with Sequential Monte Carlo (SMC) approximations. The method is particularly suited for complex systems where prior knowledge about the drift structure is limited. The framework includes a Bayesian variant with shrinkage priors to control model complexity and prevent overfitting.

## Method Summary
The approach uses EM to optimize the penalized negative log-likelihood of the drift function b in an RKHS. The E-step approximates the intractable filtering distribution using SMC with linear SDE-based proposals, which is claimed to be more efficient than standard Euler proposals for nonlinear systems. The M-step yields a closed-form solution via a generalized representer theorem, expressing the drift as a finite linear combination of kernel functions centered at particle locations. A Bayesian extension applies shrinkage priors to the coefficients to adaptively control model complexity. The method is validated on one- and multi-dimensional SDEs with varying observation sparsity.

## Key Results
- The EM-RKHS framework successfully estimates drift functions without parametric assumptions
- SMC with linear SDE proposals provides efficient filtering distribution approximations for nonlinear SDEs
- The Bayesian variant with shrinkage priors effectively controls model complexity and prevents overfitting
- Numerical experiments show accurate drift estimation with low MSE and good agreement between true and estimated stationary distributions across different data sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EM algorithm transforms an intractable likelihood optimization into a sequence of tractable surrogate problems in the RKHS.
- Mechanism: Sparse observations create unobserved trajectory segments, making the marginal likelihood ∫p(x0:N0, y1:M0|b)dx0:N0 analytically intractable. The EM framework replaces this with an alternating procedure: (E-step) compute the expected complete-data log-likelihood under the filtering distribution νb(·|y1:M0), and (M-step) minimize the resulting risk functional R(b, η) over the RKHS. The M-step admits a closed-form solution via the generalized representer theorem (Theorem 3.13), yielding b_k(x) = Σ_{l,n} κ(x, X̃(l,k-1)(sn))β(l,k,*)_n with coefficients given by (3.27).
- Core assumption: The filtering distribution νb(·|y1:M0) can be approximated sufficiently well (in KL-divergence) that the approximate EM sequence inherits convergence properties of the exact sequence.
- Evidence anchors:
  - [abstract] "We develop an Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte Carlo (SMC) method to approximate the filtering distribution"
  - [section 3.1, Theorem 3.11] Establishes convergence of approximate EM sequences when filtering distributions are approximated in KL-divergence
  - [corpus] Related work (Neural Stochastic Flows, SOCK Method) addresses similar SDE learning but via different mechanisms (neural flows, occupation kernels); no corpus paper validates the specific EM-SMC-RKHS convergence chain.
- Break condition: If the SMC approximation fails to converge in KL-divergence, Theorem 3.11 guarantees no longer hold; error may accumulate across iterations rather than converge to the stationary set S.

### Mechanism 2
- Claim: SMC with linear SDE-based proposals provides efficient, scalable approximation of the filtering distribution for nonlinear SDEs.
- Mechanism: The filtering distribution νb(·|y1:M0) is approximated by an empirical measure ν̂^SMC,L_b = Σ_l w^(l)_T δ_{X̃^(l)} over L particle-paths. Unlike MCMC, SMC constructs paths sequentially between observation times with recursive weight updates (Eq. 3.32). The proposal density q_{m,i}(x_i|x_{i-1}, y_{m+1}) uses a first-order linear SDE approximation (Eq. 3.35-3.41) rather than the zeroth-order Durham-Gallant bridge, capturing local drift behavior via Taylor expansion b(X(r)) ≈ b(X(s_i)) + Db(X(s_i))(X(r) - X(s_i)). Resampling when ESS falls below L/2 mitigates particle degeneracy.
- Core assumption: The linear SDE approximation is sufficiently accurate for the true nonlinear drift between observation times.
- Evidence anchors:
  - [abstract] "SMC offers significantly better scalability as the time horizon or the spacing between successive observation points increases"
  - [section 3.2-3.3] Details the proposal construction and weight recursion; claims superiority over Durham-Gallant for nonlinear systems
  - [corpus] Corpus papers (e.g., Neural Stochastic Flows) do not validate this specific linear-SDE proposal; no comparative evidence from related literature.
- Break condition: If observations are extremely sparse (large gaps between t_m) and drift is highly nonlinear, the linear approximation may induce high variance in particle weights, causing degeneracy despite resampling.

### Mechanism 3
- Claim: RKHS regularization via the representer theorem ensures finite-dimensional tractability while controlling model complexity.
- Mechanism: The drift function b is estimated by minimizing the penalized negative log-likelihood L(b) = -ℓ(b|y1:M0) + λ||b||²_{H_κ} over the RKHS H_κ. The generalized representer theorem (Theorem 3.13) guarantees the minimizer is a finite linear combination of kernel functions centered at SMC particle locations. The Bayesian variant with shrinkage priors (β^(k,l,*)_n | λ^l_n ~ N(0, λ^l_n I_d), λ^l_n ~ IG(a,b)) adaptively controls coefficient magnitudes, shrinking uninformative terms toward zero without hard thresholding.
- Core assumption: The kernel κ captures relevant structure of the true drift function; the RKHS is sufficiently rich for approximation.
- Evidence anchors:
  - [abstract] "The M-step then reduces to a penalized empirical risk minimization problem in the RKHS, whose minimizer is given by a finite linear combination of kernel functions"
  - [section 3.4, Algorithm 3] Describes the Bayesian-EM with t-prior shrinkage
  - [corpus] The SOCK Method [arxiv:2505.11622] uses occupation kernels for SDE learning, providing indirect support for kernel-based approaches, but does not validate this specific shrinkage prior strategy.
- Break condition: If the chosen kernel cannot represent the true drift (e.g., wrong length scale, insufficient expressiveness), the estimate will have irreducible bias regardless of data quantity.

## Foundational Learning

- Concept: **Stochastic Differential Equations (SDEs)**
  - Why needed here: The entire framework models the latent process X(t) as a solution to dX = b(X)dt + σ(X)dW; understanding diffusion, drift, and Euler discretization is essential.
  - Quick check question: Can you explain why the likelihood becomes intractable when observations are sparse, given the Markovian structure of SDEs?

- Concept: **Expectation-Maximization (EM) Algorithm**
  - Why needed here: The core iterative procedure for estimating b relies on alternating between computing expected log-likelihood under the posterior (E-step) and maximizing over parameters (M-step).
  - Quick check question: Why does the E-step require computing the filtering distribution νb(·|y1:M0) rather than the full smoothing distribution?

- Concept: **Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The drift function lives in an infinite-dimensional function space; the RKHS structure enables the representer theorem, converting this to a finite-dimensional optimization.
  - Quick check question: How does the reproducing property ⟨f, κ(·, x)⟩ = f(x) enable the finite kernel expansion in Theorem 3.13?

## Architecture Onboarding

- Component map:
  Input -> SMC Module -> E-step -> M-step -> Output
  (Sparse observations, kernel, λ) -> (Particles, weights) -> (Filtering approx) -> (β coefficients) -> (Estimated drift b_K)

- Critical path:
  1. SMC particle quality → determines approximation error in filtering distribution
  2. Kernel matrix K^(k-1)_0 conditioning → affects numerical stability of β computation
  3. Regularization λ choice → controls bias-variance tradeoff in drift estimate
  4. EM convergence → monitored via L(b_k) decreasing (Corollary 3.12) or stabilizing

- Design tradeoffs:
  - **Particle count L**: Higher L reduces SMC variance but increases O(L²) kernel matrix cost
  - **Proposal order**: Linear SDE proposal is more accurate than zeroth-order but requires computing Db(x) (gradient of drift) and solving ODEs (Eq. 3.39)
  - **Shrinkage vs. sparsity**: Shrinkage priors retain all basis functions but shrink weak coefficients; sparsity priors (not used here) would discard terms entirely
  - **Kernel bandwidth**: Narrow bandwidth captures fine detail but risks overfitting; wide bandwidth smooths but may underfit

- Failure signatures:
  - **Particle degeneracy**: ESS consistently below L/2 even after resampling → proposal is poorly matched to target
  - **Non-convergent EM**: L(b_k) oscillates or increases → filtering approximation too coarse or λ too small
  - **Numerical instability**: (K^(k-1)_0)^T D^(k-1) K^(k-1)_0 + λK^(k-1)_0 ill-conditioned → increase λ or reduce kernel bandwidth
  - **Bias in stationary distribution**: Kolmogorov metric between F_st and F̂_st remains high despite low MSE → drift estimate is accurate in observed region but extrapolates poorly

- First 3 experiments:
  1. **Validate SMC proposal on 1D double-well SDE (Model 1)**: Generate sparse observations (1/5 of trajectory), run Algorithm 1 with L=100 particles, compare weight distribution and ESS over time between linear-SDE proposal and Durham-Gallant proposal; expect linear-SDE to maintain higher ESS.
  2. **Assess EM convergence on 1D Gamma SDE (Model 3)**: Run Algorithms 2 and 3 for K=20 iterations with λ=0.01, monitor L(b_k) and MSE(b_k, b_true); expect monotonic decrease for exact EM (Corollary 3.12), approximate convergence for Bayesian-EM.
  3. **Test scaling on 3D Michaelis-Menten (Model 4)**: Vary observation sparsity (1/3, 1/5, 1/10), measure runtime and final MSE; expect degradation with sparsity but tractability due to SMC's O(L) scalability per time step.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the framework be extended to estimate the diffusion coefficient σ nonparametrically alongside the drift function b?
  - Basis in paper: [explicit] The paper states, "For simplicity we will assume that the diffusion coefficient σ is known," and notes that while parametric extensions are possible, the nonparametric case is not addressed.
  - Why unresolved: The current M-step representer theorem and the penalized empirical risk minimization formulation rely on σ being fixed to ensure the quadratic form is tractable and the likelihood is convex.
  - What evidence would resolve it: A derivation of a joint EM-update scheme where σ admits a finite-dimensional kernel representation without causing numerical instability or identifiability issues in the likelihood.

- **Open Question 2**: What verifiable conditions on the kernel κ or the drift function guarantee that the stationary set S is a singleton, ensuring global convergence of the EM sequence?
  - Basis in paper: [explicit] Corollary 3.12 notes that convergence to the penalized MLE is guaranteed if S is a singleton, "a condition which unfortunately is hard to verify in practice."
  - Why unresolved: The paper proves convergence to the *set* of fixed points S, but without conditions ensuring uniqueness, the algorithm may converge to a local optimum or cycle, limiting theoretical guarantees.
  - What evidence would resolve it: Identification of specific constraints on the RKHS norm or the choice of the kernel matrix that enforces strict convexity or injectivity of the mapping Φ.

- **Open Question 3**: Does the standard SMC approximation of the filtering distribution satisfy the convergence requirements of Theorem 3.11, or is a specific kernel density estimation (KDE) modification strictly necessary?
  - Basis in paper: [explicit] The text notes that the SMC measure ν̂^SMC,L_b "does not converge to νb in the KL-divergence," which is the requirement for Theorem 3.11, even though practical implementation uses standard SMC.
  - Why unresolved: There is a theoretical gap between the requirement of KL-divergence convergence for the "approximate EM" properties to hold and the weak/TV convergence typically guaranteed by standard SMC methods.
  - What evidence would resolve it: A proof demonstrating that the SMC-EM sequence retains the convergence properties of the exact EM sequence under weaker assumptions (e.g., Total Variation), or a theoretical bound on the KDE bandwidth required to satisfy the KL condition.

## Limitations

- The linear SDE proposal's accuracy for highly nonlinear drifts between distant observations remains unverified; no comparative analysis with alternative SMC proposals is provided
- Regularization parameter λ selection is left to practitioner discretion without systematic guidance, potentially affecting bias-variance tradeoff
- The convergence proof for approximate EM sequences relies on KL-divergence bounds that may not hold in practice for all SDE systems

## Confidence

- **High confidence** in the EM-RKHS framework's theoretical validity given the generalized representer theorem and established convergence results for exact sequences
- **Medium confidence** in the SMC approximation's practical performance, as convergence depends on the linear proposal's accuracy and particle sufficiency
- **Medium confidence** in the shrinkage prior's ability to control complexity, as the method is relatively new and lacks extensive comparative validation

## Next Checks

1. Compare SMC weight distributions and ESS trajectories between the linear-SDE proposal and Durham-Gallant proposal on a strongly nonlinear SDE to quantify approximation error
2. Perform a systematic sensitivity analysis of drift estimation accuracy versus regularization parameter λ across multiple SDE models
3. Validate the Bayesian shrinkage prior's performance against L1 regularization or other sparsity-inducing methods for model selection