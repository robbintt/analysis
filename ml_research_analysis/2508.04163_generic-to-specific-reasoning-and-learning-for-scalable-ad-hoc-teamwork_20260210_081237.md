---
ver: rpa2
title: Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork
arxiv_id: '2508.04163'
source_url: https://arxiv.org/abs/2508.04163
tags:
- agent
- agents
- actions
- tasks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an architecture for scalable ad hoc teamwork
  that integrates knowledge-based reasoning with data-driven learning. The key innovation
  is combining non-monotonic logical reasoning with rapidly learned models of teammate
  behavior and task anticipation via large language models (LLMs).
---

# Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork

## Quick Facts
- arXiv ID: 2508.04163
- Source URL: https://arxiv.org/abs/2508.04163
- Authors: Hasra Dodampegama; Mohan Sridharan
- Reference count: 35
- Primary result: Architecture combining ASP reasoning, FF tree behavior prediction, and LLM task anticipation outperforms baselines in ad hoc teamwork

## Executive Summary
This paper presents an architecture for scalable ad hoc teamwork that integrates knowledge-based reasoning with data-driven learning. The key innovation is combining non-monotonic logical reasoning with rapidly learned models of teammate behavior and task anticipation via large language models (LLMs). Each ad hoc agent reasons about current and anticipated future tasks while predicting teammates' actions to avoid conflicts and improve coordination. Experiments in the VirtualHome environment demonstrate that the architecture outperforms baselines that use only knowledge-based reasoning, only learned models, or no task anticipation. With up to three agents collaborating, the system shows improved scalability and efficiency, completing tasks in fewer steps and less time.

## Method Summary
The architecture enables each ad hoc agent to determine its actions through non-monotonic logical reasoning with incomplete domain knowledge, while using models learned and revised rapidly to predict the behavior of other agents. It also anticipates abstract future goals based on generic knowledge of similar situations in an existing foundation model. The system uses Answer Set Prolog with consistency-restoring rules for reasoning, Fast and Frugal (FF) trees learned from approximately 1000 traces for behavior prediction, and GPT-4o mini with task anticipation prompts for planning. Domain knowledge is encoded at two resolutions - coarse and fine - with refinement-based zooming to manage the large state space. The architecture was evaluated in the VirtualHome simulation environment with household tasks.

## Key Results
- Architecture completed tasks in fewer steps and less time compared to baselines
- Performance improved with integration of task anticipation and behavior prediction
- System scaled effectively with up to three ad hoc agents collaborating
- Achieved better coordination without prior coordination mechanisms

## Why This Works (Mechanism)

### Mechanism 1
Combining non-monotonic logical reasoning with rapidly learned teammate behavior models improves coordination in ad hoc teamwork. The architecture uses Answer Set Prolog (ASP) with consistency-restoring rules to reason with incomplete domain knowledge. Simultaneously, it learns Fast and Frugal (FF) trees—simple decision heuristics—to predict teammates' actions from ~1000 traces. These predictions are mapped to exogenous actions in the ASP program, allowing the planner to anticipate and avoid conflicts. Core assumption: Teammate behavior can be approximated by simple, transparent heuristics that generalize from limited data. Evidence: Abstract mentions architecture enables agents to predict behavior; Section 3.2 describes FF tree ensembles learned from 1000 traces. Break condition: If teammate behavior is highly stochastic or non-stationary, FF trees may fail to generalize.

### Mechanism 2
Using an LLM to anticipate future high-level tasks, validated against domain knowledge, enables more efficient joint planning. The LLM receives a prompt with recent observations, completed tasks, and candidate tasks. It outputs anticipated task sequences using persona adoption, few-shot examples, and chain-of-thought reasoning. An external validator parses outputs, filters infeasible tasks, and reorders based on domain-specific preferences. The ad hoc agent then plans jointly for current and anticipated tasks. Core assumption: The LLM encodes sufficient commonsense knowledge about household routines. Evidence: Abstract mentions anticipated abstract future goals based on generic knowledge; Section 3.3 describes prompting strategies and validator. Break condition: If the task space is novel or not well-represented in LLM training data, anticipation quality degrades.

### Mechanism 3
Refinement-based reasoning at multiple resolutions enables scalable planning in large state spaces. Domain knowledge is encoded at two resolutions—coarse (DC) and fine (DF). Coarse regions are refined into finer sub-regions. The agent reasons abstractly in DC, then zooms into relevant DF components for detailed planning. Bridge axioms couple descriptions, ensuring transitions in DC can be implemented in DF. Core assumption: Tasks can be decomposed hierarchically. Evidence: Section 3.1 describes refinement from prior work with example axioms linking coarse and fine descriptions. Break condition: If task structure doesn't align with hierarchical decomposition, refinement may not reduce planning complexity.

## Foundational Learning

- **Non-monotonic reasoning (Answer Set Programming)**: Why needed here? Agents must revise conclusions when new observations contradict prior beliefs. Quick check question: Can you explain why "not a" differs from "¬a" in ASP semantics?

- **Ecological Rationality / Decision Heuristics**: Why needed here? FF trees for behavior prediction rely on satisficing with limited attributes rather than optimal but complex models. Quick check question: What is the tradeoff between model complexity and sample efficiency in learning teammate behavior?

- **LLM Prompting Strategies (Few-shot, Chain-of-Thought, Persona)**: Why needed here? Task anticipation depends on prompt engineering to elicit coherent, domain-relevant predictions from LLM. Quick check question: Why might an LLM output invalid or misordered tasks without external validation?

## Architecture Onboarding

- **Component map**: Knowledge Base -> Refinement Module -> Behavior Prediction -> Task Anticipation -> Planner
- **Critical path**: 
  1. Define domain signature in ALd and translate to ASP
  2. Collect ~1000 traces of teammate behavior and train FF tree ensembles
  3. Configure LLM prompts with persona, few-shot examples, and CoT templates
  4. Implement validator to filter/reorder LLM outputs using domain knowledge
  5. Integrate predicted actions as exogenous actions in planning ASP program
- **Design tradeoffs**: 
  - Transparency vs. expressiveness: FF trees are interpretable but may underfit complex behaviors vs. neural models
  - Prompt complexity vs. latency: More sophisticated prompting improves anticipation but increases LLM call overhead
  - Resolution granularity: Finer DF improves precision but increases state space; coarser DC is faster but may miss details
- **Failure signatures**:
  - Repeated redundant actions with teammate (behavior prediction not integrated or FF trees inaccurate)
  - Anticipated tasks infeasible or misordered (validator not catching LLM errors)
  - Planning timeout in large domains (refinement not reducing search space effectively)
- **First 3 experiments**:
  1. Ablation on behavior prediction: Compare steps/time with vs. without FF trees
  2. Ablation on task anticipation: Compare with vs. without LLM anticipation
  3. Scalability test: Measure steps/time as team size increases from 2 to 4 agents

## Open Questions the Paper Calls Out

- Can the architecture maintain performance benefits when collaborating with real human teammates whose behavior is less predictable and more diverse than the ASP-based simulated human used in experiments?
- How can the non-monotonic domain knowledge (e.g., axioms in the system description D) be incrementally revised during execution, beyond the current ability to revise only behavior prediction models?
- Does the architecture scale effectively to teams larger than 4 agents, and how does performance degrade as the number of heterogeneous agent types increases?
- How does the architecture perform when deployed on physical robots operating in real-world environments with sensor noise, actuation errors, and partial observability?

## Limitations
- Experiments confined to VirtualHome's structured environment with household tasks
- Behavior prediction models depend on observable attributes that may not capture internal states
- LLM-based task anticipation relies on domain knowledge embedded in training data
- Sample size (100 routines) and single-environment evaluation limit generalization

## Confidence
- Claims about integration effectiveness: Medium
- Scalability claims with up to three agents: Medium
- Generalizability to real-world human collaboration: Low
- Performance with larger teams or more complex domains: Low

## Next Checks
1. Test behavior prediction robustness by introducing stochastic or non-stationary teammate policies not captured in the 1000 training traces
2. Evaluate task anticipation in domains with novel task spaces or routines not represented in LLM training data
3. Scale team size beyond three agents and measure whether refinement-based reasoning maintains planning efficiency in larger state spaces