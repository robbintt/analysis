---
ver: rpa2
title: 'Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning
  for Multiphysics PDEs'
arxiv_id: '2505.17575'
source_url: https://arxiv.org/abs/2505.17575
tags:
- multiphysics
- coupling
- physical
- field
- fluid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Multiphysics Bench, the first benchmark dataset
  for multiphysics partial differential equations (PDEs), featuring six representative
  coupled problems across electromagnetics, heat transfer, fluid flow, solid mechanics,
  acoustics, and mass transport. The dataset includes 10,000+ samples per problem
  with diverse coupling mechanisms and boundary conditions.
---

# Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs

## Quick Facts
- arXiv ID: 2505.17575
- Source URL: https://arxiv.org/abs/2505.17575
- Reference count: 40
- Primary result: Introduced the first benchmark dataset for multiphysics PDEs with 10,000+ samples per problem across six coupled physics systems

## Executive Summary
This work introduces Multiphysics Bench, the first benchmark dataset for multiphysics partial differential equations (PDEs), featuring six representative coupled problems across electromagnetics, heat transfer, fluid flow, solid mechanics, acoustics, and mass transport. The dataset includes 10,000+ samples per problem with diverse coupling mechanisms and boundary conditions. A systematic evaluation of four representative scientific machine learning models (PINNs, DeepONet, FNO, and DiffusionPDE) reveals that naively applying these single-physics methods to multiphysics problems yields suboptimal performance. The study also demonstrates that incorporating complete multiphysics priors significantly improves prediction accuracy compared to incomplete supervision, and identifies several practical tricks for enhancing model robustness in coupled systems. Multiphysics Bench provides a standardized framework for evaluating and advancing scientific machine learning approaches for complex, coupled physical systems.

## Method Summary
The study evaluates four scientific machine learning models on six multiphysics PDE systems using a standardized benchmark. The dataset was generated using COMSOL Multiphysics with MATLAB LiveLink, producing 128x128 resolution fields. Models tested include PINNs (physics-informed neural networks), DeepONet (operator learning), FNO (Fourier neural operator), and DiffusionPDE (diffusion-based solver). Each model was trained with specific hyperparameters: PINNs used Adam with learning rate 5e-3 and PDE-to-data loss ratio 1:1000; DeepONet used unstacked architecture with learning rate 4e-3 and basis dimension 256; FNO used 12 Fourier modes with learning rate 1e-4; and DiffusionPDE used 10 inference steps with learning rate 1e-3. The evaluation employed standard metrics including Relative Error, RMSE, Max Error, fRMSE (spectral), and bRMSE (boundary).

## Key Results
- PINNs achieved RE ~0.26 on Electro-Thermal task (Ez field) but struggled with convergence stability
- DeepONet outperformed other models on steady-state problems with RE ~0.24 on Electro-Thermal (Ez)
- FNO showed degraded performance on steady-state tasks, producing non-diverse outputs
- All models exhibited significantly degraded performance on the transient Mass Transport problem, with RE exceeding 50%

## Why This Works (Mechanism)
The effectiveness stems from the comprehensive benchmark design that captures diverse multiphysics coupling mechanisms across six physical domains. The dataset's high sample count (10,000+ per task) enables robust evaluation of generalization capabilities. The study reveals that incorporating complete multiphysics priors improves accuracy by properly constraining the solution space, while the proposed tricks (quantile normalization and auto-balanced weighting) address specific challenges in coupled system training such as long-tailed distributions and gradient imbalance.

## Foundational Learning

**Finite Element Method (FEM)**
- Why needed: FEM generates the ground truth multiphysics data used for training and evaluation
- Quick check: Verify generated fields satisfy the governing PDEs within numerical tolerance

**Fourier Neural Operator (FNO)**
- Why needed: FNO learns continuous operators for function-to-function mappings in PDEs
- Quick check: Ensure 12 Fourier modes capture sufficient spectral content for the problem

**DeepONet Architecture**
- Why needed: DeepONet learns solution operators for parametric PDEs with branch and trunk networks
- Quick check: Validate unstacked architecture with p=256 basis dimension produces stable training

## Architecture Onboarding

**Component Map**
- Dataset Generator (COMSOL FEM) -> Model Trainer (PINNs/DeepONet/FNO/DiffusionPDE) -> Evaluator (RE, RMSE metrics)

**Critical Path**
1. Generate or acquire dataset samples
2. Implement model architecture with specified hyperparameters
3. Train model using prescribed optimizer and loss functions
4. Evaluate performance on test set using standard metrics

**Design Tradeoffs**
- Steady-state vs. transient problems: FNO struggles with steady-state high spectral similarity, while PINNs handle transient problems better but require careful loss weighting
- Model complexity vs. data efficiency: Larger models show diminishing returns without smooth scaling behavior
- Physics incorporation: Complete priors significantly improve accuracy but require more computational resources

**Failure Signatures**
- FNO producing constant/non-diverse outputs indicates spectral similarity issues
- PINN training instability suggests improper PDE-to-data loss ratio or gradient imbalance
- Poor transient problem performance indicates inadequate time-dependent modeling capacity

**First Experiments**
1. Implement FNO with 12 modes on Electro-Thermal task, verify RE ~0.30
2. Train DeepONet unstacked architecture with p=256 on same task, compare to FNO performance
3. Apply quantile normalization to Electro-Thermal dataset and retrain FNO to assess impact

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the Multiphysics Bench benchmark be extended to encompass a broader range of multiphysics scenarios, particularly time-dependent systems with increased scale and complexity?
- Basis in paper: The authors state in the Future Directions section: "several important challenges remain: (1) extending the benchmark to encompass a broader range of multiphysics scenarios, particularly time-dependent systems with increased scale and complexity."
- Why unresolved: The current benchmark includes only one transient problem (Mass Transportâ€“Fluid) among six scenarios, and the authors note all models exhibit degraded performance on it. The framework's scalability to more complex, time-dependent coupled systems is untested.
- What evidence would resolve it: Creation and release of additional multiphysics datasets featuring transient dynamics, higher dimensionality, or more complex coupling topologies, along with benchmark results showing whether current or new architectures can generalize to these systems.

**Open Question 2**
- Question: How can SciML architectures be modified to explicitly incorporate cross-physics coupling mechanisms to better handle bidirectional and nonlinear inter-field interactions?
- Basis in paper: The authors identify this as a future direction: "(2) developing targeted improvements based on baseline limitations, including the incorporation of explicit cross-physics coupling mechanisms."
- Why unresolved: The empirical analysis shows that naively applying single-physics models (PINNs, DeepONet, FNO, DiffusionPDE) to multiphysics problems yields suboptimal performance. The Discussion section notes "Existing architectures inadequately represent cross-field interactions," and no architecture tested uses explicit coupling modules.
- What evidence would resolve it: Development and evaluation of novel model architectures that include, for example, coupling attention layers, physics-constrained interaction terms, or multi-branch networks with dedicated cross-field communication, demonstrating improved accuracy on multiphysics benchmarks over baseline models.

**Open Question 3**
- Question: Can multiphysics foundation models be designed that exhibit consistent data, compute, and model size scaling laws, overcoming the low data efficiency observed in current SciML models?
- Basis in paper: The authors state as a future direction: "(3) designing multiphysics foundation models with data, compute, and model size scaling laws."
- Why unresolved: The Data Scaling experiments (Table 4) show that prediction accuracy does not consistently improve with increasing data volume, revealing "the absence of smooth scaling behavior" and "Low data efficiency, where SciML models performance often get saturated around 10,000 samples."
- What evidence would resolve it: Training of large-scale models on expanded multiphysics datasets, with systematic reporting of performance metrics as a function of training data size, compute budget, and model parameters to demonstrate predictable and beneficial scaling.

## Limitations
- The benchmark is limited to 2D problems, which may not fully capture real-world 3D multiphysics system complexity
- Several architectural details remain underspecified, including exact hidden layer counts, activation functions, and convolutional kernel sizes
- The study does not explore hybrid approaches that combine multiple model types or novel architectures specifically designed for multiphysics coupling

## Confidence
- **Benchmark Dataset Creation**: High - Generation methodology using COMSOL with specified parameters is clearly described
- **Model Performance Rankings**: Medium - Relative performance trends are consistent but exact numerical comparisons may vary due to unspecified architectural details
- **Effectiveness of Priors and Tricks**: Medium - Demonstrated improvements but magnitude depends on implementation specifics

## Next Checks
1. **Architectural Fidelity Check**: Reproduce DeepONet and FNO baseline results on Electro-Thermal task using specified hyperparameters, then systematically vary hidden layer counts and activation functions to determine sensitivity to architectural choices
2. **Loss Function Implementation Audit**: Implement physics-informed loss with different numerical differentiation schemes (auto-differentiation vs. finite differences) and compare convergence behavior and final accuracy across all four model types
3. **Dataset Generation Verification**: Use provided COMSOL MATLAB LiveLink scripts to regenerate a subset of the dataset (e.g., 100 samples) and verify that physical fields match the statistical properties described in Tables 5-10 of the paper