---
ver: rpa2
title: Adversarial Samples Are Not Created Equal
arxiv_id: '2601.00577'
source_url: https://arxiv.org/abs/2601.00577
tags:
- adversarial
- samples
- features
- robust
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of the non-robust features
  theory for explaining adversarial vulnerability in deep neural networks. While Ilyas
  et al.
---

# Adversarial Samples Are Not Created Equal

## Quick Facts
- arXiv ID: 2601.00577
- Source URL: https://arxiv.org/abs/2601.00577
- Reference count: 40
- This paper distinguishes adversarial bugs from non-robust feature manipulation using a novel ensemble-based metric (JSΔ).

## Executive Summary
This paper challenges the prevailing theory that adversarial vulnerability stems solely from models exploiting non-robust features. While Ilyas et al. demonstrated that brittle but predictive features can be exploited by adversaries, this work identifies a second type of adversarial weakness—adversarial bugs—that do not rely on non-robust features. The authors propose JSΔ, a metric based on ensemble transferability, to measure how much adversarial samples manipulate non-robust features versus exploiting model-specific blind spots. Their findings show that adversarial bugs dominate weak attacks but become less prevalent as perturbation strength increases, while non-robust features dominate stronger attacks. Notably, adversarially trained models are resilient to adversarial bugs but vulnerable when non-robust features are manipulated at higher perturbation levels.

## Method Summary
The paper introduces JSΔ, a metric that measures the manipulation of non-robust features in adversarial samples by computing normalized Jensen-Shannon divergence between ensemble outputs on source versus adversarial inputs. The method involves training four ResNet models with different random seeds, generating targeted PGD-100 adversarial samples at multiple perturbation strengths (ε ∈ {1/255, 3/255, 5/255, 8/255}), and computing JSΔ for each source-adversarial pair. The metric is normalized by a baseline ρ computed from clean image pairs with the same label transition. The authors also evaluate robustness under adversarial training, Sharpness-Aware Minimization (SAM), and training on robust/non-robust datasets to understand how different training methods affect vulnerability to bugs versus feature manipulation.

## Key Results
- Adversarial bugs (low JSΔ) dominate weak attacks but become negligible as perturbation strength increases
- Adverarially trained models are resilient to adversarial bugs but vulnerable to non-robust feature manipulation at higher ε
- SAM reduces adversarial bugs without improving robustness to non-robust feature manipulation
- Robust datasets still contain non-robust features that standard training can exploit

## Why This Works (Mechanism)

### Mechanism 1: Ensemble-Based Detection of Non-Robust Feature Manipulation (JSΔ Metric)
The JSΔ metric detects non-robust feature manipulation by measuring how much adversarial perturbations change ensemble predictions across differently-initialized models. High JSΔ indicates the perturbation meaningfully altered features that generalize across model instances (non-robust features); low JSΔ suggests exploitation of model-specific blind spots (adversarial bugs). This works under the assumption that different random initializations converge to similar decision boundaries using similar predictive features.

### Mechanism 2: Adversarial Bugs as Sharp Loss Minima
Adversarial bugs correspond to regions of high curvature in the loss landscape, making them unstable to perturbations and specific to individual model instances. Weak attacks (small ε) find local sharp minima that fool a specific model without altering data-level features, while stronger attacks require larger perturbations that manipulate non-robust features, which transfer across models.

### Mechanism 3: SAM Targets Bugs Without Affecting Feature Robustness
Sharpness-Aware Minimization provides targeted protection against adversarial bugs by flattening the loss landscape but does not improve robustness to non-robust feature manipulation. SAM's min-max formulation over weight perturbations discourages sharp minima where bugs reside, but the learned features remain vulnerable to perturbations that exploit their non-robustness.

### Mechanism 4: Robust Datasets Leak Non-Robust Features Under Standard Training
Standard training on robust datasets (D_R) leads to re-learning of non-robust features because robust features contain entangled non-robust components. Standard training optimizes for predictive features without robustness constraints, rediscovering the non-robust shortcuts present in the data.

## Foundational Learning

- **Concept: Non-robust features (Ilyas et al. 2019)**
  - Why needed here: Central to understanding what JSΔ measures—brittle but predictive features that adversaries exploit
  - Quick check question: Can you explain why training on D_rand (adversarial samples with random labels) generalizes to clean test data?

- **Concept: Jensen-Shannon divergence**
  - Why needed here: The mathematical foundation of JSΔ metric; normalized measure of distribution difference
  - Quick check question: Why is JS divergence preferred over KL divergence for comparing ensemble output distributions?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: Required to interpret Section 5 results on bug vs. feature robustness
  - Quick check question: How does SAM's formulation differ from adversarial training, despite both being min-max problems?

## Architecture Onboarding

- **Component map**: Train ensemble (4 ResNet50 models with different seeds) -> Compute ρ baseline for label pairs -> Generate adversarial samples with targeted PGD -> Compute JSΔ for each source-adversarial pair and analyze distribution

- **Critical path**: 1) Train ensemble of 4 ResNet50 models on CIFAR10 with identical hyperparameters, different seeds; 2) Compute ρ baseline by sampling clean image pairs with label transitions from validation set; 3) Generate targeted PGD-100 adversarial samples at multiple ε values; 4) Compute JSΔ for each pair and plot histograms

- **Design tradeoffs**: Ensemble size N=4 balances computational cost vs. estimate quality; smaller N increases noise. β thresholds are heuristic; 0.01 is strict (high-confidence bugs), 0.10 captures borderline cases. Attack strength choice determines which vulnerability type dominates results

- **Failure signatures**: If ensemble members have divergent decision boundaries, JSΔ becomes noisy. If augmentations are too weak during training, invariance property validation fails. If ρ baseline is miscalculated (insufficient samples), normalization introduces errors

- **First 3 experiments**: 1) Replicate Figure 2 on your dataset/architecture to establish baseline JSΔ distributions at ε=[1,3,5,8]/255; 2) Train with SAM (ρ=0.3 for CIFAR10) and compare JSΔ shift to verify bug reduction; 3) Attempt second-order robust dataset construction on your robust-trained model to test for non-robust feature leakage

## Open Questions the Paper Calls Out

### Open Question 1
Does the distinction between adversarial bugs and non-robust features hold for architectures or datasets where decision boundaries vary significantly across different random initializations? The authors explicitly note that Assumption 3.2 (regarding similar decision boundaries across seeds) may not hold for all architectures, particularly those with high performance variability.

### Open Question 2
Can adversarial bugs be fully isolated from non-robust feature manipulation, or is the "spectrum" between them inherently leaky? While the paper establishes a metric to differentiate the two, the generation process for "pure" adversarial bugs (those that do not manipulate features at all) appears theoretically difficult to guarantee.

### Open Question 3
Is it possible to construct a "robust dataset" (D_R) that prevents the re-emergence of non-robust features during standard training? The paper demonstrates that current methods for creating robust datasets fail to close the robustness gap with adversarial training, but leaves open the possibility of a better data distillation method.

## Limitations

- The JSΔ metric relies on the assumption that differently-initialized models converge to similar decision boundaries, which may not hold across all architectures
- Analysis focuses on image classification with CNNs; transferability to other domains (text, graphs) or architectures (transformers) remains untested
- The paper does not explore whether adversarial bugs could be learned features in certain regimes (e.g., under data augmentation)

## Confidence

- **High confidence**: Empirical findings on JSΔ distribution shifts with perturbation strength and training method (Figures 2, 3, 4; Tables 2, 4, 5)
- **Medium confidence**: Theoretical mechanism linking JSΔ to non-robust feature manipulation (Section 3.2 assumptions)
- **Low confidence**: Claims about robust datasets still containing non-robust features without exhaustive ablation on feature suppression techniques

## Next Checks

1. **Ensemble Robustness Test**: Train ensembles with larger variance in initialization (e.g., different architectures or widths) and measure JSΔ stability to test Assumption 3.2

2. **SAM Generalization Test**: Apply SAM with varying ρ values to verify the bug-protection mechanism is not hyperparameter-dependent and extends beyond CIFAR10/SVHN

3. **Cross-Domain Transfer**: Generate adversarial samples on a non-image dataset (e.g., text classification) and compute JSΔ to test metric generalizability