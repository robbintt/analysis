---
ver: rpa2
title: Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation
arxiv_id: '2509.01838'
source_url: https://arxiv.org/abs/2509.01838
tags:
- learning
- maritime
- data
- action
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework for maritime
  navigation that leverages AIS-derived traffic graphs and ERA5 wind fields to learn
  routing policies across multiple origin-destination pairs. The approach uses hexagonal
  grid discretization with action masking to ensure feasible movements, integrates
  historical vessel traffic patterns into reward shaping, and employs goal-conditioned
  reinforcement learning to generalize across different routes.
---

# Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation

## Quick Facts
- **arXiv ID:** 2509.01838
- **Source URL:** https://arxiv.org/abs/2509.01838
- **Reference count:** 38
- **Primary result:** Maskable PPO with AIS-derived reward shaping and action masking outperforms Dijkstra/A* baselines in Gulf of St. Lawrence maritime routing

## Executive Summary
This paper proposes a goal-conditioned reinforcement learning framework for maritime navigation that leverages AIS-derived traffic graphs and ERA5 wind fields to learn routing policies across multiple origin-destination pairs. The approach uses hexagonal grid discretization with action masking to ensure feasible movements, integrates historical vessel traffic patterns into reward shaping, and employs goal-conditioned reinforcement learning to generalize across different routes. Experiments in the Gulf of St. Lawrence show that action masking is critical for policy performance, with the masked PPO agent achieving significantly higher returns compared to unmasked variants. The method outperforms traditional routing baselines like Dijkstra's and A* algorithms, demonstrating the effectiveness of combining large-scale traffic data with adaptive RL for real-world maritime navigation.

## Method Summary
The framework uses a maskable PPO agent operating on an H3 hexagonal grid (resolution 6) representing the Gulf of St. Lawrence. AIS data (2013-2024) is processed into a Markovian traffic graph with edge weights representing transition probabilities. The agent receives 8-dimensional observations (position, speed, wind, start, goal) and selects from a multi-discrete action space (5-6 maneuvers × 5 speeds: 8/11/14/18/22 knots). A binary action mask prevents invalid movements by restricting actions to valid graph neighbors. The reward function combines progress toward goal, frequency of historical traffic on edges, wind penalty, fuel cost, ETA penalty, and step penalty. The system trains for 10M steps across 16 parallel environments and evaluates on 6 OD pairs against traditional routing baselines.

## Key Results
- Action masking is critical: Masked PPO achieves significantly higher returns while unmasked agents fail catastrophically with returns below -1000
- AIS-derived reward shaping enables meaningful progress: Without frequency-based reward terms, agents make little meaningful progress toward goals
- Outperforms traditional methods: The RL approach achieves better performance than Dijkstra's and A* algorithms on the same routing tasks
- RNNs offer limited benefits: Short observation histories (H=8) provide more stable learning dynamics than recurrent architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Action masking appears to be the primary driver of policy convergence and feasibility in sparse, constrained environments.
- **Mechanism:** By applying a binary mask $m_t(s)$ to the multi-discrete action space, the policy distribution is re-normalized over valid graph neighbors only. This prevents the agent from wasting exploration capacity on geometric impossibilities (e.g., traversing land) or illegal backtracking, effectively reducing the search space complexity.
- **Core assumption:** Assumes the simulation environment perfectly respects the mask; if the mask is misaligned with the graph topology, the agent will fail.
- **Evidence anchors:**
  - [abstract] "Key findings show that action masking is critical for policy success..."
  - [section IV-C2] "With action masking, agents achieve consistently high positive returns, whereas unmasked agents fail catastrophically..."
  - [corpus] Neighbor paper "Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning" corroborates the necessity of safety constraints in RL navigation.

### Mechanism 2
- **Claim:** Integrating historical traffic density into the reward function guides exploration through proven corridors.
- **Mechanism:** The reward term $r_{freq}$ uses edge weights derived from AIS data. This acts as a soft prior, pushing the agent away from untraversed regions and toward historically "safe" or operationally viable paths, compensating for the sparsity of the primary distance-based reward.
- **Core assumption:** Assumes historical traffic patterns correlate with navigational safety and efficiency (i.e., past captains knew best).
- **Evidence anchors:**
  - [section III-E4] "...$r_{freq}$ guides the agent toward frequently traveled routes... acting as a soft preference..."
  - [section IV-C1] "...without the graph-derived shaping rewards the agent makes little meaningful progress."
  - [corpus] Related papers lack specific comparison to AIS-derived graph shaping, making this a distinct mechanism relative to the corpus.

### Mechanism 3
- **Claim:** Short-term observation history stabilizes training in near-Markovian environments better than recurrent architectures.
- **Mechanism:** Stacking the last $H=8$ observations provides immediate temporal context (velocity/acceleration trends) without the optimization complexity of backpropagation through time (BPTT) required by LSTMs.
- **Core assumption:** Assumes the environment dynamics are sufficiently captured by the state vector $s_t$ and recent history, satisfying the Markov property approximately.
- **Evidence anchors:**
  - [abstract] "...short observation histories stabilize training; recurrent networks... offer limited benefits."
  - [section IV-C5] "...RNNs appears to hinder learning, producing high variance... short histories or feedforward policies provide more stable learning dynamics..."

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** PPO is the underlying learning algorithm. You must understand the clipped objective $L^{CLIP}$ to debug why the policy updates might stall or diverge.
  - **Quick check question:** Can you explain how the clipping parameter $\epsilon$ prevents the new policy from deviating too far from the old policy?

- **Concept: Goal-Conditioned Reinforcement Learning (GCRL)**
  - **Why needed here:** The system does not learn one fixed route; it learns a universal policy $\pi(a|s,g)$ conditioned on the destination.
  - **Quick check question:** How does the input vector change when the agent switches from navigating to Destination A vs. Destination B?

- **Concept: Hexagonal Discretization (H3)**
  - **Why needed here:** The state and action space are built on an H3 lattice. You need to understand the "one-distance rule" to interpret movement costs and neighbor connectivity.
  - **Quick check question:** Why does a hexagonal grid reduce directional bias compared to a standard latitude/longitude square grid?

## Architecture Onboarding

- **Component map:** AISdb -> Graph Layer (H3 grid + traffic graph) -> MariNav Environment -> Maskable PPO Agent
- **Critical path:** The integrity of the **Action Mask** is the single point of failure. If the mask does not perfectly align with the H3 neighbor topology, the agent will attempt invalid moves, triggering the $-1900$ terminal penalty and destroying learning stability.
- **Design tradeoffs:**
  - **Masking vs. Penalty Learning:** The paper proves that *masking* (hard constraint) vastly outperforms *penalty learning* (soft constraint) for invalid actions.
  - **Speed Discretization:** The action space uses 5 discrete speeds (8–22 knots) rather than continuous control. This stabilizes PPO but reduces throttle granularity.
  - **RNN vs. History:** The system chooses explicit history stacking ($H=8$) over LSTMs, sacrificing long-term memory for training stability.
- **Failure signatures:**
  - **Catastrophic Negative Reward:** Mean return dropping below $-1000$ usually indicates the action mask is failing or absent (Section IV-C2).
  - **High Variance:** If returns fluctuate wildly across seeds, check if LSTM is enabled (Section IV-C5 suggests reverting to MLP).
  - **Stagnation:** If the agent moves but never reaches goals, verify the $r_{freq}$ graph weights are correctly normalized (Section IV-C1).
- **First 3 experiments:**
  1. **Mask Validation:** Run a random policy with action masking enabled for 10k steps. Confirm no "invalid action" penalties are incurred.
  2. **Reward Ablation:** Train PPO with *only* penalty terms (no $r_{freq}$ or $r_{prog}$) on a single OD pair to replicate the "no meaningful progress" baseline.
  3. **Generalization Test:** Train on 3 OD pairs and evaluate on a 4th unseen pair to verify the GCRL conditioning is functioning (not overfitting to specific coordinates).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be effectively extended to continuous control and high-fidelity vessel dynamics without losing learning stability?
- **Basis in paper:** [explicit] The conclusion states, "The current use of a multi-discrete action space restricts alignment with continuous autopilot controls... Expanding the physical realism... through... Nomoto-based dynamics... would allow richer and more transferable policies."
- **Why unresolved:** The current implementation relies on a multi-discrete action space and simplified fuel/drag models, which abstract away the continuous hydrodynamic complexities of real vessel control.
- **What evidence would resolve it:** Successful training and evaluation of agents using continuous action spaces and vessel-specific fuel curves in a high-fidelity simulator.

### Open Question 2
- **Question:** How does the inclusion of multi-agent interactions and congestion modeling affect policy generalization and safety?
- **Basis in paper:** [explicit] The authors note that "Multi-agent formulations that model vessel interactions, congestion, and coordination with ports will further enhance realism."
- **Why unresolved:** The current framework treats navigation as a single-agent problem, ignoring the dynamic constraints and negotiation behaviors required in congested seaways.
- **What evidence would resolve it:** Performance metrics (collision rates, efficiency) of trained policies when deployed in a environment with multiple simultaneous vessel agents.

### Open Question 3
- **Question:** Can algorithms like Hindsight Experience Replay (HER) or constrained RL overcome the limitations of Random Network Distillation (RND) in this sparse-reward setting?
- **Basis in paper:** [explicit] The paper identifies a "need for broader evaluation of methods suited to sparse and safety-critical tasks, including Hindsight Experience Replay [and] constrained RL."
- **Why unresolved:** The experiments showed RND offered limited benefits and converged to suboptimal performance, leaving the optimal exploration strategy for this domain unidentified.
- **What evidence would resolve it:** A comparative analysis showing improved sample efficiency and success rates using HER or Constrained Policy Optimization relative to the PPO+RND baseline.

## Limitations
- Reliance on historical AIS traffic assumes past patterns correlate with safety and efficiency, which may not hold in novel scenarios or under changing conditions
- Action masking effectiveness is demonstrated in a static graph setting; dynamic obstacles or moving vessels could invalidate the mask at execution time
- The history window (H=8) may be insufficient for capturing long-term dependencies like tidal cycles or seasonal wind patterns

## Confidence
- **High confidence:** Action masking is critical for avoiding invalid moves and stabilizing learning in constrained environments
- **Medium confidence:** AIS-derived reward shaping effectively guides exploration through historically viable routes
- **Medium confidence:** Short observation histories provide sufficient temporal context without the complexity of recurrent architectures

## Next Checks
1. Test the policy in a dynamic simulation with moving obstacles to assess action mask limitations
2. Evaluate performance when historical traffic patterns are deliberately corrupted or incomplete
3. Vary the observation history length (H) to determine the optimal window for capturing relevant environmental dynamics