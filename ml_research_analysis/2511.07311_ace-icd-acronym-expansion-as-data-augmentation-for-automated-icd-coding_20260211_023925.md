---
ver: rpa2
title: 'ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding'
arxiv_id: '2511.07311'
source_url: https://arxiv.org/abs/2511.07311
tags:
- coding
- clinical
- code
- codes
- acronym
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACE-ICD addresses automated ICD coding by expanding medical acronyms
  in clinical notes using large language models and applying consistency training
  to regularize predictions. The method improves performance across common, rare,
  and full-code ICD coding tasks on the MIMIC-III dataset, achieving state-of-the-art
  results including 94.4% macro AUC and 71.6% macro F1 on MIMIC-III-50, and significant
  gains on rare code prediction.
---

# ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding

## Quick Facts
- **arXiv ID:** 2511.07311
- **Source URL:** https://arxiv.org/abs/2511.07311
- **Reference count:** 24
- **Primary result:** Achieves state-of-the-art performance on MIMIC-III with 94.4% macro AUC and 71.6% macro F1 on MIMIC-III-50, improving rare code prediction through acronym expansion.

## Executive Summary
ACE-ICD introduces a data augmentation strategy for automated ICD coding by expanding medical acronyms in clinical notes using large language models. The method combines zero-shot acronym expansion with consistency training to regularize predictions, showing significant improvements across common, rare, and full-code ICD coding tasks on the MIMIC-III dataset. By explicitly normalizing shorthand to full form representations, the approach reduces semantic gaps between clinical notes and formal ICD code descriptions, particularly benefiting rare code prediction.

## Method Summary
ACE-ICD reformulates multi-label ICD-9 code assignment as a cloze-style prompt task, where the model predicts "yes"/"no" for each code by filling masked tokens. The method consists of three stages: (1) offline acronym expansion using Llama-3.1-70B-Instruct on section-split clinical notes, (2) prompt-based fine-tuning with KEPT encoder (Clinical Longformer or RoBERTa) using code descriptions and UMLS synonyms, and (3) consistency training with bidirectional KL-divergence loss between original and augmented predictions. For the full MIMIC-III dataset with 8,922 codes, a re-ranking approach processes top 300 candidates from MSMN in batches of 50.

## Key Results
- Achieves 94.4% macro AUC and 71.6% macro F1 on MIMIC-III-50, state-of-the-art performance
- Shows Pearson correlation of -0.25 (p=0.08) between F1 improvement and training examples, suggesting rare codes benefit more from acronym expansion
- ACE-ICD + Consistency outperforms ACE-ICD alone on MIMIC-III-rare50, validating the effectiveness of regularization

## Why This Works (Mechanism)

### Mechanism 1: Semantic Gap Reduction
- **Claim:** Explicitly normalizing acronyms to full form reduces the semantic gap between clinical notes and formal ICD code descriptions.
- **Mechanism:** By converting shorthand (e.g., "uti") to explicit text ("urinary tract infection") via LLMs, the encoder no longer needs to learn ambiguous mappings internally, directly aligning input tokens with code description tokens in the prompt.
- **Core assumption:** The frozen LLM (Llama 3.1 70B) provides accurate context-aware expansion; otherwise, noise is introduced.
- **Evidence anchors:** [abstract] "...leveraging large language models to expand medical acronyms, allowing models to be trained on their full form representations."
- **Break condition:** Expansion accuracy drops significantly (e.g., using a smaller LLM), introducing erroneous definitions that mislead the encoder (Table 7 shows 1B model degrades performance).

### Mechanism 2: Consistency Training Regularization
- **Claim:** Consistency training via KL-divergence regularizes the model against noise in the augmented data.
- **Mechanism:** The loss enforces $p(y|Original) \approx p(y|Augmented)$. If the LLM makes an expansion error in $t_a$, the consistency loss prevents the model from overfitting to that error by forcing it to agree with the original note's prediction.
- **Core assumption:** The "view" generated by the LLM is semantically equivalent to the original note regarding the target label.
- **Evidence anchors:** [section: Methods] "L_cons enforces consistency by minimizing the bidirectional KL-divergence."
- **Break condition:** The consistency weight $\alpha$ is set too high, forcing the model to ignore valid distinctions between original and augmented text.

### Mechanism 3: Targeted Data Augmentation for Rare Codes
- **Claim:** Acronym expansion functions as a targeted data augmentation strategy for low-frequency (rare) codes.
- **Mechanism:** Rare codes often appear as specific acronyms in notes. Expanding these provides explicit textual evidence that might be sparse in the original training data, effectively increasing the token overlap for rare code descriptions.
- **Core assumption:** Rare codes are disproportionately represented by acronyms rather than full descriptions in clinical notes.
- **Evidence anchors:** [section: Discussion] "Pearson correlation of –0.25... suggests that rarer codes tend to benefit more from our method."
- **Break condition:** The rare code has no acronym representation or the acronym is shared with a common code (polysemy) causing false positives.

## Foundational Learning

- **Concept:** **Prompt-based Fine-tuning (Cloze Task)**
  - **Why needed here:** ACE-ICD does not use a standard classifier head. It reformulates multi-label classification into filling `[MASK]` tokens with "yes"/"no" based on code descriptions.
  - **Quick check question:** How does the model map the tokens "yes" and "no" to binary labels $y_i \in \{0,1\}$?

- **Concept:** **KL Divergence**
  - **Why needed here:** This metric measures the difference between two probability distributions. It is the mathematical core of the consistency loss used to align original and augmented views.
  - **Quick check question:** If $p(y|P)$ is sharp (confident) and $p(y|P_a)$ is flat (uncertain), how does bidirectional KL divergence behave compared to unidirectional?

- **Concept:** **Knowledge Distillation (Self-Training)**
  - **Why needed here:** The training loop effectively treats the predictions on the clean data as a teacher signal for the augmented (noisy) data, and vice-versa, similar to consistency training in semi-supervised learning.
  - **Quick check question:** Why is the consistency loss term $L_{cons}$ necessary in addition to the standard cross-entropy loss $L_{ce}$?

## Architecture Onboarding

- **Component map:** Offline Augmenter (Llama-3.1-70B-Instruct) -> Encoder (KEPT) -> Prompt Constructor (Original Note + Code Desc, Augmented Note + Synonyms) -> Aggregator (Bidirectional KL Divergence Loss)

- **Critical path:**
  1. **Preprocessing:** Split discharge summaries by section headers to fit LLM context windows.
  2. **Augmentation:** Run Llama-3.1-70B to generate $t_a$ (store this, do not generate on-the-fly).
  3. **Training Loop:**
     - Batch original notes ($t$) and augmented notes ($t_a$).
     - Forward pass $t$ with code descriptions $c$.
     - Forward pass $t_a$ with code synonyms $s$.
     - Compute $L_{total} = 0.5(L_{ce}(t) + L_{ce}(t_a)) + 0.05 \times L_{cons}$.

- **Design tradeoffs:**
  - **LLM Size vs. Noise:** Using 1B/3B models for expansion yields negative or marginal gains (Table 7). You must use the 70B model or accept performance degradation.
  - **Prompt Token Limit:** For MIMIC-III-full (8,922 codes), you cannot fit all codes in the prompt. You **must** use a re-ranking approach (top-300 candidates from MSMN) rather than full classification.
  - **Synonyms in Augmented Prompt:** The paper uses synonyms $s_i$ for $P_a$. This doubles the distinct token variance but increases computational overhead.

- **Failure signatures:**
  - **Macro-F1 Collapse:** On MIMIC-III-full, if you use a single threshold optimized for micro-F1, macro-F1 drops (Table 8). You must implement code-specific thresholds if rare-code performance is the priority.
  - **Expansion Drift:** If the LLM prompt is not strict ("do not mention the acronyms again"), the model may output "UTI (Urinary Tract Infection)", confusing the token alignment.

- **First 3 experiments:**
  1. **Acronym Accuracy Audit:** Run the Llama expansion on 100 random samples and manually check "Strict" vs. "Lenient" accuracy to ensure the 60-70% baseline holds for your data.
  2. **Alpha ($\alpha$) Sweep:** Run MIMIC-III-50 with $\alpha \in \{0.0, 0.02, 0.05, 0.1\}$. Verify that $\alpha=0.05$ is optimal for your specific encoder initialization.
  3. **Threshold Calibration:** Compare global vs. code-specific thresholds on the validation set to determine the trade-off between micro/macro F1 for your deployment needs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ACE-ICD augmentation framework achieve similar performance gains when applied to non-prompt-based ICD coding architectures, such as CNNs or RNNs?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "Additional experiments are needed to thoroughly assess the effectiveness of our proposed strategy across various models," as the study relied on the KEPT architecture.
- **Why unresolved:** The experiments were restricted to the KEPT model, leaving the generalizability of the data augmentation technique to other neural network architectures unverified.
- **What evidence would resolve it:** Performance benchmarks of ACE-ICD applied to standard CNN or RNN baselines (e.g., CAML or MultiResCNN) on the MIMIC datasets.

### Open Question 2
- **Question:** Is the observed trend that acronym expansion disproportionately benefits rare ICD codes statistically significant when validated on a larger scale?
- **Basis in paper:** [inferred] The Discussion section notes a Pearson correlation of –0.25 (p = 0.08) between F1 improvement and training examples, suggesting rarer codes benefit more, but admits "the evidence is not statistically significant."
- **Why unresolved:** The current p-value exceeds the standard 0.05 threshold, meaning the correlation observed in the data could be due to chance.
- **What evidence would resolve it:** A statistical power analysis or experiments on a dataset with a higher volume of rare code occurrences to confirm the negative correlation with statistical significance.

### Open Question 3
- **Question:** Do acronym expansion errors in text segments unrelated to the target code have a negligible impact on the final coding performance?
- **Basis in paper:** [inferred] The authors "hypothesize that expansion errors may have minimal impact if the expanded acronyms are unrelated to any ICD code" based on observing incorrect expansions in the ablation study.
- **Why unresolved:** The analysis relies on a hypothesis derived from observation; the paper does not isolate and quantify the specific downstream effects of irrelevant errors versus relevant errors.
- **What evidence would resolve it:** A targeted error analysis measuring performance changes when noise (incorrect expansions) is injected specifically into code-irrelevant versus code-relevant sections of the clinical notes.

## Limitations
- **LLM Expansion Reliability:** Performance hinges on 60-70% strict accuracy of Llama-3.1-70B expansion; lower accuracy introduces noise that consistency loss may not fully correct.
- **Generalizability to Other Clinical Vocabularies:** Evaluated exclusively on ICD-9 codes from MIMIC-III; effectiveness for ICD-10 or other medical coding systems remains untested.
- **Computational Overhead:** Generating expanded notes with a 70B LLM is computationally expensive; offline augmentation approach may be impractical for real-time clinical applications.

## Confidence
- **High Confidence:** The core claim that acronym expansion improves rare code prediction is supported by both correlation analysis (Pearson -0.25) and ablation results.
- **Medium Confidence:** State-of-the-art claims on MIMIC-III-50 are well-supported, but MIMIC-III-full results rely on re-ranking and may not reflect true classification performance.
- **Low Confidence:** The mechanism by which consistency training interacts with expansion noise is theorized but not empirically validated; the paper does not measure whether KL-divergence loss actually reduces prediction variance on augmented views.

## Next Checks
1. **Expansion Error Impact Study:** Run ACE-ICD with controlled noise in the expansion (e.g., randomly corrupt 20-40% of expansions) to measure the sensitivity of final performance to expansion quality.
2. **Cross-Vocabulary Transfer Test:** Apply the ACE-ICD pipeline to a different coding system (e.g., ICD-10 or CPT codes from a different dataset) to assess generalizability.
3. **Real-Time Feasibility Analysis:** Measure the end-to-end latency of ACE-ICD (expansion + encoding + prediction) on a single clinical note to quantify the practical trade-off between accuracy and speed for clinical deployment.