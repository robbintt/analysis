---
ver: rpa2
title: Can LLM Annotations Replace User Clicks for Learning to Rank?
arxiv_id: '2511.06635'
source_url: https://arxiv.org/abs/2511.06635
tags:
- annotations
- data
- click
- relevance
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-generated annotations can replace
  user click data for learning to rank (LTR). Through comprehensive experiments on
  both public and industrial datasets, the study finds that click-supervised models
  perform better on high-frequency queries while LLM annotation-supervised models
  excel on medium- and low-frequency queries.
---

# Can LLM Annotations Replace User Clicks for Learning to Rank?

## Quick Facts
- **arXiv ID:** 2511.06635
- **Source URL:** https://arxiv.org/abs/2511.06635
- **Reference count:** 40
- **Primary result:** LLM annotations can't fully replace user clicks but combining both improves LTR performance across all query frequencies

## Executive Summary
This paper investigates whether LLM-generated annotations can substitute for user click data in learning to rank systems. Through comprehensive experiments on public and industrial datasets, the study reveals that click-supervised models excel on high-frequency queries by capturing document-level signals like authority and quality, while LLM-annotation models perform better on medium- and low-frequency queries through superior semantic matching capabilities. To leverage these complementary strengths, the authors propose two hybrid training strategies: data scheduling and frequency-aware multi-objective learning, both of which enhance ranking performance across all query frequencies. The findings suggest that LLM annotations should be viewed as a complementary signal rather than a complete replacement for click data in learning to rank applications.

## Method Summary
The authors conducted a systematic comparison between click-supervised and LLM annotation-supervised learning to rank models using both public benchmarks and industrial datasets. They employed a GPT-4-based annotator to generate relevance judgments for query-document pairs, creating a parallel training framework. The study evaluated model performance across different query frequency segments and identified distinct strengths: click-trained models excelled at capturing document-level quality signals while LLM-trained models demonstrated superior semantic matching abilities. To harness these complementary capabilities, two hybrid training strategies were developed: a data scheduling approach that dynamically weights click and annotation data, and a frequency-aware multi-objective learning method that adapts training objectives based on query frequency. Both approaches were validated through extensive experiments showing consistent performance improvements.

## Key Results
- Click-supervised models outperform on high-frequency queries, capturing document-level signals like authority and quality
- LLM annotation-supervised models excel on medium- and low-frequency queries with superior semantic matching capabilities
- Hybrid training strategies combining both supervision signals significantly improve ranking performance across all query frequencies
- The frequency-aware multi-objective learning method shows superior results compared to data scheduling approach

## Why This Works (Mechanism)
Click data inherently captures user behavior patterns and document-level quality signals that emerge from real user interactions, making it particularly effective for high-frequency queries where sufficient click data exists. LLM annotations, being generated through semantic understanding rather than behavioral observation, excel at modeling the intrinsic relevance relationships between queries and documents, making them more effective for medium- and low-frequency queries where click data is sparse. The complementary nature of these signals - behavioral patterns versus semantic understanding - creates an opportunity for hybrid approaches that can leverage the strengths of both.

## Foundational Learning

**Learning to Rank (LTR)**: Ranking algorithms that learn from training data to produce optimal orderings of documents for queries. *Why needed:* Forms the core problem domain being addressed. *Quick check:* Understanding that LTR aims to optimize ranking rather than classification or regression.

**Click Modeling**: Using user interaction data (clicks, dwell time) as supervision signals for training ranking models. *Why needed:* Click data is the traditional supervision signal being compared against LLM annotations. *Quick check:* Recognizing that clicks provide behavioral rather than semantic signals.

**Query Frequency Segmentation**: Dividing queries into high, medium, and low frequency categories based on occurrence patterns. *Why needed:* Different supervision signals perform differently across frequency segments. *Quick check:* Understanding that high-frequency queries have more training data available.

**Multi-Objective Learning**: Training models with multiple, potentially competing objectives simultaneously. *Why needed:* Enables combining click and annotation supervision signals effectively. *Quick check:* Recognizing that different query frequencies may require different optimization objectives.

## Architecture Onboarding

**Component Map**: LLM Annotation Generator -> Feature Extractor -> Ranking Model -> Loss Function -> Model Parameters (with parallel Click Data path)

**Critical Path**: Query -> Ranking Model -> Document Scores -> Ranked List -> User Satisfaction

**Design Tradeoffs**: Balancing between behavioral signals (clicks) and semantic understanding (LLM annotations), with frequency-specific adaptation versus universal approaches

**Failure Signatures**: 
- Over-reliance on click data leads to poor performance on tail queries
- Excessive dependence on LLM annotations results in unrealistic relevance judgments
- Single-objective approaches fail to capture the complementary nature of both signals

**First Experiments**:
1. Compare baseline click-trained vs LLM-annotation-trained models on segmented query frequencies
2. Validate the effectiveness of data scheduling hybrid approach
3. Test frequency-aware multi-objective learning against single-objective baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Results may vary significantly with different LLM models or annotation strategies
- Dataset composition and domain specificity may limit generalizability to other web search scenarios
- Findings focus on reranking tasks and may not directly extend to full ranking pipelines or other IR tasks

## Confidence
- Click-supervised models perform better on high-frequency queries: High confidence
- LLM-annotation models excel on medium- and low-frequency queries: High confidence
- Hybrid approaches improve performance across all frequencies: Medium confidence
- Frequency-aware method superiority: Medium confidence (limited ablation studies)

## Next Checks
1. Replicate experiments with different LLM models (e.g., Claude, Llama) to assess model dependency of the findings
2. Test the hybrid training approaches on additional domains beyond web search to evaluate generalizability
3. Conduct ablation studies to isolate the impact of specific annotation strategies and training schedule designs on the observed performance differences