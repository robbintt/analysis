---
ver: rpa2
title: A Multi-Layered Large Language Model Framework for Disease Prediction
arxiv_id: '2502.00063'
source_url: https://arxiv.org/abs/2502.00063
tags:
- text
- language
- fine-tuning
- classification
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-layered framework combining LLM-based
  preprocessing with Arabic language model fine-tuning for disease prediction in social
  telehealth. The approach uses text refinement, summarization, and Named Entity Recognition
  to enhance Arabic language models CAMeL-BERT, AraBERT, and Asafaya-BERT for disease
  type classification and severity assessment.
---

# A Multi-Layered Large Language Model Framework for Disease Prediction

## Quick Facts
- arXiv ID: 2502.00063
- Source URL: https://arxiv.org/abs/2502.00063
- Reference count: 27
- Primary result: LLM-based preprocessing with fine-tuning improves Arabic medical text classification (83% Type accuracy, 69% Severity accuracy)

## Executive Summary
This study presents a multi-layered framework combining LLM-based preprocessing with Arabic language model fine-tuning for disease prediction in social telehealth. The approach uses text refinement, summarization, and Named Entity Recognition to enhance Arabic language models CAMeL-BERT, AraBERT, and Asafaya-BERT for disease type classification and severity assessment. Among preprocessing methods, NER-augmented text yielded the highest performance, with CAMeL-BERT achieving 83% accuracy in type classification and 69% in severity assessment. The framework demonstrates that integrating LLMs into preprocessing significantly improves diagnostic accuracy and supports effective telehealth applications.

## Method Summary
The framework processes Arabic medical text from social telehealth posts through three parallel LLAMA3 preprocessing techniques: text refinement (noise removal), summarization (compression), and Named Entity Recognition (entity extraction). These augmented outputs are concatenated with original text and used to fine-tune three Arabic BERT variants (CAMeL-BERT, AraBERT, Asafaya-BERT) via LoRA adaptation. The fine-tuning configuration uses dropout 5%, rank 16, alpha scaling factor 8, batch size 4, and 25 epochs with balanced and accuracy-weighted custom loss functions. The framework addresses both disease type classification (multi-class) and severity assessment (ordinal) tasks.

## Key Results
- CAMeL-BERT with NER preprocessing achieved 83% accuracy for disease type classification and 69% for severity assessment
- Non-fine-tuned models with NER preprocessing only achieved 15-20% type accuracy and 40-42% severity accuracy
- CAMeL-BERT consistently outperformed AraBERT and Asafaya-BERT across all preprocessing conditions
- NER augmentation provided superior performance compared to text refinement and summarization approaches

## Why This Works (Mechanism)

### Mechanism 1
LLM-based preprocessing extracts signal from noisy user-generated medical text, improving downstream classifier performance. Raw social telehealth posts contain irrelevant information, grammatical errors, and vague descriptions. LLAMA3 performs text refinement (removing noise), summarization (compressing long posts), and NER (extracting medical entities like symptoms, conditions, drugs). This structured augmentation provides cleaner training signals for the fine-tuned Arabic BERT models.

### Mechanism 2
NER augmentation outperforms refinement and summarization because it explicitly surfaces medically relevant tokens. NER extracts structured entity lists (symptoms, conditions, medications) that directly map to classification labels. Unlike refinement (which edits text) or summarization (which compresses), NER creates an explicit medical vocabulary signal that reduces the model's need to learn implicit symptom-disease mappings from noisy prose.

### Mechanism 3
Fine-tuning is the critical multiplier—preprocessing gains only materialize with LoRA-based adaptation. Pre-trained Arabic BERT models have general language representations but lack medical domain specialization. LoRA fine-tuning (rank 16, alpha 8, 5% dropout) adapts attention weights to the medical classification task. Without this adaptation, even clean NER-augmented inputs cannot be properly classified.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses LoRA to efficiently fine-tune large BERT models with limited medical data. Understanding LoRA helps you replicate the training setup and diagnose fine-tuning issues.
  - Quick check question: Can you explain why LoRA freezes base model weights and only trains low-rank decomposition matrices?

- **Named Entity Recognition for Medical Text**
  - Why needed here: NER is the highest-performing preprocessing method in this framework. Understanding how NER extracts symptoms, conditions, and drugs helps you evaluate whether entity extraction quality is a bottleneck.
  - Quick check question: Given a patient post "I've had a headache for 3 days and took ibuprofen," what entities should a medical NER system extract?

- **Multi-Label vs. Multi-Class Classification**
  - Why needed here: The paper addresses both disease type classification (multi-class) and severity assessment (ordinal), with implications for loss function selection and evaluation metrics.
  - Quick check question: Why would accuracy alone be misleading for a multi-label medical classification task with class imbalance?

## Architecture Onboarding

- **Component map**: Raw text → LLAMA3 preprocessing (refinement/summarization/NER) → Augmented text → CAMeL-BERT with LoRA fine-tuning → Type/Severity classification
- **Critical path**: Raw text → LLAMA3 NER → NER-augmented text → CAMeL-BERT with LoRA fine-tuning → Type/Severity classification. This path achieved the best reported results.
- **Design tradeoffs**: NER provides explicit entity signals but assumes extraction accuracy; summarization compresses text but may remove detail; refinement cleans noise but preserves length. CAMeL-BERT (mixed corpus) outperformed AraBERT (pure Arabic) and Asafaya-BERT, suggesting pre-training diversity matters for medical domains.
- **Failure signatures**: Non-fine-tuned models with NER still achieve only 15-20% type accuracy → preprocessing cannot substitute for domain adaptation; summarization shows minimal improvement over baseline → compression may lose predictive signal; severity accuracy consistently lower than type accuracy (69% vs. 83%) → ordinal classification may require specialized loss functions.
- **First 3 experiments**: 1) Baseline replication: Run CAMeL-BERT on raw text without preprocessing or fine-tuning to confirm 15-20% type accuracy baseline; 2) Ablation by preprocessing method: Compare refinement-only, summarization-only, and NER-only augmentations to isolate each component's contribution; 3) Cross-model validation: Test AraBERT and Asafaya-BERT with NER augmentation to verify whether CAMeL-BERT's advantage is consistent or dataset-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform when generalized to languages other than Arabic?
- Basis in paper: The conclusion explicitly states, "Future studies can extend this framework to other languages."
- Why unresolved: The current study exclusively evaluates the framework on Arabic text using Arabic-specific models (CAMeL-BERT, AraBERT, Asafaya-BERT), leaving cross-lingual validity untested.
- What evidence would resolve it: Applying the identical LLM-preprocessing and fine-tuning pipeline to English or multilingual medical datasets and comparing the resulting accuracy metrics.

### Open Question 2
- Question: Does LLM-based preprocessing outperform traditional, non-generative NLP techniques for data augmentation?
- Basis in paper: The methodology uses LLAMA3 for text refinement and NER but compares these only against "Normal Text" and other LLM-based methods, omitting comparison with traditional Arabic NLP tools.
- Why unresolved: It remains unclear if the performance gains are specific to the generative capabilities of LLAMA3 or if simpler, less computationally expensive preprocessing methods would yield similar improvements.
- What evidence would resolve it: A comparative study evaluating classification accuracy when the fine-tuning data is augmented by standard rule-based NER or dictionary-based summarization versus the proposed LLM approach.

### Open Question 3
- Question: What are the primary linguistic or data-centric factors causing the performance gap between disease type classification and severity assessment?
- Basis in paper: Results in Table 3 show a consistent gap (e.g., 83% Type vs. 69% Severity for CAMeL-BERT with NER), yet the paper does not analyze the root cause of this disparity.
- Why unresolved: The paper notes that severity assessment uses "qualitative and subjective terminology," but it does not determine if the lower scores stem from class imbalance, ambiguous text features, or limitations in the models' ability to capture nuance.
- What evidence would resolve it: A qualitative error analysis of the misclassified severity cases to identify if errors correlate with text length, vocabulary specificity, or the distribution of severity labels in the training data.

## Limitations
- Social telehealth posts may contain unreliable self-diagnoses, slang, or incomplete symptom descriptions that preprocessing cannot fully address
- NER precision/recall metrics are not reported, leaving entity extraction quality as an unverified assumption
- The framework's performance on clinical notes versus social media text remains untested, limiting generalizability across data quality variations

## Confidence
- **High confidence**: The fine-tuning effect is robust (83% vs 15-20% type accuracy), supported by clear numerical comparisons across preprocessing methods
- **Medium confidence**: The superiority of NER preprocessing over refinement and summarization, though demonstrated, lacks ablation studies showing each method's individual contribution to the 76% AVG improvement
- **Medium confidence**: CAMeL-BERT's consistent outperformance across all conditions, pending validation that this advantage persists across different medical domains and dataset sizes

## Next Checks
1. Conduct NER precision/recall analysis on a manually annotated sample to quantify extraction accuracy and identify systematic entity extraction errors
2. Perform cross-domain validation using medical text from different sources (clinical notes vs social media) to test framework robustness across data quality variations
3. Implement ablation testing comparing each preprocessing method in isolation to determine whether NER's 76% AVG advantage stems from entity quality or methodological complementarity