---
ver: rpa2
title: 'ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation
  Rates of Academic Papers'
arxiv_id: '2505.08941'
source_url: https://arxiv.org/abs/2505.08941
tags:
- citation
- data
- performance
- correlation
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ForeCite introduces a framework for adapting pre-trained causal
  language models to predict future citation rates of academic papers by appending
  them with a linear head. The approach treats citation prediction as a text-to-signal
  regression task, leveraging the semantic knowledge stored in LLMs to forecast citation
  impact directly from manuscript text.
---

# ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers

## Quick Facts
- arXiv ID: 2505.08941
- Source URL: https://arxiv.org/abs/2505.08941
- Reference count: 40
- Primary result: ForeCite achieves test Spearman correlation of 0.826, a 27-point improvement over prior state-of-the-art.

## Executive Summary
ForeCite introduces a framework for adapting pre-trained causal language models to predict future citation rates of academic papers by appending them with a linear head. The approach treats citation prediction as a text-to-signal regression task, leveraging the semantic knowledge stored in LLMs to forecast citation impact directly from manuscript text. Experiments on a dataset of over 900,000 biomedical papers show ForeCite achieves a test Spearman correlation of 0.826, a 27-point improvement over prior state-of-the-art. Scaling-law analysis confirms consistent gains across model sizes and data volumes, with extrapolation suggesting further improvements with larger models. Gradient-based saliency analysis reveals heavy reliance on titles and abstracts, while ablation studies show these sections are helpful but not strictly necessary. Temporal holdout experiments indicate short-term robustness but gradual performance decay over time, suggesting potential benefits from online learning. Overall, ForeCite establishes a new benchmark in citation prediction and demonstrates the potential of LLMs for automated research evaluation.

## Method Summary
ForeCite adapts pre-trained causal language models to citation prediction by appending a linear head that maps the final hidden state to a scalar citation rate. The model is trained in two phases: first freezing the LLM and training only the head, then jointly fine-tuning both with QLoRA. The input corpus consists of 900K+ biomedical papers converted from XML to Markdown, with targets as log-transformed average monthly citation rates. Training uses AdamW with cosine learning rate schedules, gradient accumulation, and 4-bit quantization to fit large models. Evaluation focuses on Spearman and Pearson correlations, R², and error metrics on a held-out test set.

## Key Results
- Test Spearman correlation of 0.826, a 27-point improvement over prior state-of-the-art.
- Scaling-law analysis shows consistent gains across model sizes (0.5B-14B) and data volumes.
- Gradient saliency reveals disproportionate reliance on titles and abstracts as predictive features.
- Temporal holdout shows initial robustness (r=0.713) but gradual decay to r=0.511 over ~28 months.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained language models encode semantic features that transfer to citation prediction tasks.
- Mechanism: The causal LLM's pre-trained weights capture contextual and conceptual relationships in scientific text; appending a linear head transforms final hidden states into scalar citation-rate predictions via learned projection.
- Core assumption: Semantic patterns correlated with citation impact are present in pre-training corpora and remain extractable after fine-tuning.
- Evidence anchors:
  - [abstract] "ForeCite achieves a test correlation of ρ = 0.826... a 27-point improvement over the previous state-of-the-art."
  - [section 5.1] "These models are not memorizing content, they are learning to extract and quantify highly-citable features in academic texts."
  - [corpus] Related work on citation prediction (e.g., SChuBERT, CiMaTe) uses embeddings as fixed features, achieving R² ≤ 0.454 and ρ ≤ 0.556; ForeCite's end-to-end adaptation yields substantially higher performance, suggesting transfer gains.
- Break condition: If pre-training corpora contain no scientific text or citation-relevant patterns, transfer would degrade to random-chance levels.

### Mechanism 2
- Claim: Two-phase training stabilizes regression adaptation by isolating head initialization from backbone perturbation.
- Mechanism: Phase 1 freezes the LLM and trains only the linear head, learning a coarse mapping from frozen representations to targets. Phase 2 unfreezes the backbone and jointly fine-tunes with QLoRA, refining both representation and projection.
- Core assumption: Head-first training provides a stable initialization that reduces gradient conflict during full-model fine-tuning.
- Evidence anchors:
  - [section 3.3] "Each model was trained in two phases: 1. Initial training phase: The base LLM was frozen while only the appended linear head was trained. 2. Fine-tuning phase: The base LLM was unfrozen and jointly fine-tuned with the linear head via QLoRA."
  - [section 5.3] "Our models exhibited signs that the initial training phase of the linear head constrained the subsequent full-model fine-tuning to an inferior local minima, a symptom which could likely be mitigated by a more gradual unfreezing."
  - [corpus] Weak direct corpus evidence for two-phase regression training; this mechanism is primarily supported by the paper's internal ablation and training logs.
- Break condition: If phase-1 head overfits to frozen representations that are later substantially altered, phase-2 fine-tuning may suffer from representation drift or instability.

### Mechanism 3
- Claim: Models rely disproportionately on titles and abstracts as convenient shortcuts, but can leverage body text when forced.
- Mechanism: Gradient-based saliency assigns highest attribution to title/abstract tokens, suggesting these sections encode disproportionate predictive signal. Ablation shows performance drops are minor when these sections are removed, indicating the model can use body text but defaults to the shortcut.
- Core assumption: Title/abstract tokens are more information-dense per parameter and thus favored during optimization.
- Evidence anchors:
  - [section 4.2] "On average, title and abstract tokens exhibited the highest attribution, with general body text receiving minimal weight."
  - [section 5.1] "Training without titles/abstracts yields only minor drops in performance (Δr < 0.02), indicating these sections serve as convenient surrogates rather than strictly necessary features."
  - [corpus] No direct corpus corroboration; related work (e.g., SChuBERT) also uses abstracts but does not ablate them systematically.
- Break condition: If title/abstract tokens are removed or adversarially perturbed at deployment, models trained without section-wise dropout may see outsized performance degradation.

## Foundational Learning

- Concept: Causal language modeling and next-token prediction
  - Why needed here: ForeCite builds on autoregressive transformers; understanding how causal masking and next-token objectives shape representations is essential for interpreting why a linear head can extract citation signal.
  - Quick check question: Given a sequence [w₁, w₂, w₃], which tokens can w₃ attend to in a causal LM?

- Concept: Transfer learning and fine-tuning dynamics
  - Why needed here: The paper's contribution is an adaptation method; grasping freezing, unfreezing, and catastrophic forgetting risks is critical for reproducing the two-phase training.
  - Quick check question: What is the primary risk when fine-tuning a large pre-trained model on a small downstream dataset?

- Concept: Gradient-based saliency for interpretability
  - Why needed here: The paper relies on gradient saliency to explain model behavior; understanding how gradients reflect input importance is necessary for validating or critiquing the interpretability claims.
  - Quick check question: How does a gradient saliency map attribute importance to input tokens?

## Architecture Onboarding

- Component map:
  Input pipeline: XML -> Markdown via XSLT -> tokenizer -> token embeddings -> Backbone (AutoModelForCausalLM) -> Linear head -> Citation rate prediction

- Critical path:
  1. Data preprocessing (log-transform, standardization using training statistics).
  2. Phase-1 head-only training (frozen LLM).
  3. Phase-2 joint QLoRA fine-tuning.
  4. Inference: forward pass through quantized LLM -> linear head -> inverse-transform prediction.

- Design tradeoffs:
  - Quantization (4-bit NF4) enables large models within memory budget but may introduce minor fidelity loss.
  - Two-phase training stabilizes head initialization but risks converging to inferior local minima; gradual unfreezing could mitigate but was not implemented.
  - Title/abstract reliance improves performance but reduces robustness; section-wise dropout is proposed but not yet validated.

- Failure signatures:
  - Temporal drift: Performance degrades from r=0.713 to r=0.511 over ~28 months out-of-distribution, indicating concept drift.
  - Reasoning fine-tuning penalty: Models distilled for reasoning (DeepSeek-R1-Distill) underperform base Qwen2.5 models, suggesting task-specific fine-tuning can degrade transfer.
  - Head-constrained minima: Signs of suboptimal convergence after phase-1 training, especially with limited hyperparameter tuning.

- First 3 experiments:
  1. Baseline reproduction: Train Bloom-560m on 1% data with two-phase pipeline; verify test correlation ≈0.43 as reported.
  2. Ablation on section dropout: Remove title tokens and measure Δr; expect minor drop (<0.02) per paper.
  3. Temporal holdout sanity check: Train on pre-2023 data, evaluate on 2023-2024 cohorts; expect gradual decay consistent with reported trend.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would aggressive section-wise dropout during training force models to rely more heavily on full-text content rather than superficial features like titles and abstracts?
- Basis in paper: [explicit] The authors state "the implementation of an aggressive section-wise dropout during training to compel models to recognize weak-but-valuable signals" as future work, motivated by ablation results showing title/abstract removal caused only minor performance drops (Δr < 0.02).
- Why unresolved: The ablation demonstrates titles/abstracts are convenient shortcuts, not strictly necessary, but the current training regime still yields models that heavily attend to these sections.
- What evidence would resolve it: Training with randomized section dropout and evaluating whether gradient saliency shifts toward body text while maintaining or improving prediction accuracy.

### Open Question 2
- Question: Can online learning regimes mitigate the temporal performance decay observed in holdout experiments?
- Basis in paper: [explicit] "The degradation in performance after one month out-of-distribution indicates the potential benefit of an online-learning regime in practical deployments of these models."
- Why unresolved: Temporal holdout showed correlation decayed from r = 0.713 to r = 0.511 over ~2.5 years, but no online learning experiments were conducted.
- What evidence would resolve it: Comparing static models against continuously-updated models on rolling temporal cohorts, measuring whether periodic fine-tuning arrests the decay.

### Open Question 3
- Question: Do ForeCite's scaling laws and performance generalize to non-biomedical disciplines and non-English corpora?
- Basis in paper: [explicit] "Our models were inherently limited by their training data, which comprised exclusively of biomedical research written in English" with future work including "expanding evaluations to include non-biomedical and non-English corpora to verify the generalizability."
- Why unresolved: The 900K+ dataset spans only seven biomedical keywords, and citation norms differ substantially across fields (e.g., computer science vs. mathematics).
- What evidence would resolve it: Training and evaluating ForeCite on diverse disciplinary corpora (physics, social sciences, humanities) and multilingual datasets, comparing scaling coefficients and absolute performance.

### Open Question 4
- Question: Would progressive unfreezing regimes yield better optima than the current two-phase training?
- Basis in paper: [inferred] "Our models exhibited signs that the initial training phase of the linear head constrained the subsequent full-model fine-tuning to an inferior local minima, a symptom which could likely be mitigated by a more gradual unfreezing of model weights."
- Why unresolved: Single-run experiments with manual hyperparameter tuning were necessitated by computational constraints, leaving the suboptimal minima hypothesis untested.
- What evidence would resolve it: Ablation comparing current two-phase training against gradual layer-wise unfreezing (e.g., discriminative learning rates), measuring final test correlation and convergence behavior.

## Limitations

- Proprietary dataset dependency: Results rely on restricted-access Elsevier papers, limiting independent verification.
- Suboptimal optimization: Two-phase training may converge to inferior local minima, with gradual unfreezing proposed but untested.
- Temporal robustness concerns: Performance degrades substantially over time, suggesting short-term rather than fundamental impact capture.

## Confidence

- Citation prediction SOTA performance (ρ = 0.826): Medium confidence. Strong relative improvement over baselines is reported, but results depend on proprietary data and unvalidated preprocessing.
- Transfer learning mechanism: High confidence. The end-to-end fine-tuning approach is well-established, and ablation shows the model leverages semantic content beyond section shortcuts.
- Two-phase training benefit: Low confidence. While described as stabilizing, internal evidence suggests it may trap optimization in poor local minima; gradual unfreezing is proposed but untested.
- Temporal robustness: Low confidence. Performance decays substantially out-of-distribution, and the paper acknowledges potential benefits from online learning without validation.
- Scaling-law extrapolation: Low confidence. Extrapolation beyond observed data ranges is speculative without experimental validation at larger scales.

## Next Checks

1. **Independent replication on open data**: Replicate the ForeCite pipeline on a public biomedical corpus (e.g., PubMed OA subset) with comparable preprocessing and two-phase training. Verify if similar correlation gains (ρ > 0.7) are achievable without proprietary data dependencies.

2. **Section-wise dropout robustness test**: Systematically ablate title, abstract, and body sections during both training and inference. Measure performance degradation under each condition and evaluate whether models trained with section dropout show improved robustness to adversarial section removal.

3. **Temporal holdout with online learning**: Train on pre-2023 data, evaluate on 2023-2024 test sets, then perform incremental fine-tuning on 2024 data. Compare performance decay against a static model to quantify benefits of online adaptation and validate claims about concept drift.