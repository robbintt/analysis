---
ver: rpa2
title: Understanding Task Transfer in Vision-Language Models
arxiv_id: '2511.18787'
source_url: https://arxiv.org/abs/2511.18787
tags:
- tasks
- task
- figure
- transfer
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Perfection Gap Factor (PGF), a novel
  metric to quantify how finetuning on one visual perception task transfers to zero-shot
  performance on other tasks in Vision-Language Models (VLMs). PGF measures both the
  magnitude and breadth of transfer effects by normalizing accuracy gains relative
  to the remaining gap to the ceiling performance, enabling fair comparison across
  tasks with different difficulty levels.
---

# Understanding Task Transfer in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2511.18787
- **Source URL:** https://arxiv.org/abs/2511.18787
- **Reference count:** 40
- **Primary result:** Perfection Gap Factor (PGF) metric quantifies task transfer in VLMs by normalizing accuracy gains relative to remaining gap to ceiling performance.

## Executive Summary
This paper introduces the Perfection Gap Factor (PGF) to systematically quantify how finetuning on one visual perception task transfers to zero-shot performance on other tasks in Vision-Language Models (VLMs). Through experiments across three VLM scales (3B, 7B, 32B) and 13 perception tasks, the authors reveal structured patterns of task transferability including task cliques, personas (donor/pirate/sponge/sieve tasks), and scale-dependent effects. The framework demonstrates that low-level and image-level tasks are highly transferable, positive transferability increases with model size, and PGF can guide data selection to improve finetuning efficiency while mitigating negative transfer. The approach generalizes to video-based tasks, showing consistent transfer patterns in the temporal domain.

## Method Summary
The method involves systematically finetuning Qwen-2.5-VL models (3B, 7B, 32B) on individual perception tasks using LoRA adapters (rank=8, α=16), then evaluating zero-shot transfer to all other tasks in the BLINK benchmark. PGF computes transfer as the ratio of accuracy gain to remaining gap to ceiling performance: (Acc_finetuned - Acc_baseline) / (Ceiling - Acc_baseline + ε). This normalization enables fair comparison across tasks with different difficulty levels. The analysis aggregates PGF values to identify task cliques (stable clusters of mutually beneficial or detrimental transfers), personas (tasks that predominantly donate or absorb transfer), and scale-dependent patterns. Experiments use 4 random seeds and evaluate on multiple-choice tasks from 13 perception domains.

## Key Results
- PGF reveals structured task transfer patterns including positive cliques (size up to 9 tasks in 32B model) and negative cliques that should be avoided in co-training
- Low-level perception tasks (Relative Depth, Relative Reflectance, Visual Correspondence) show highest positive transferability and malleability
- Positive transferability increases with model scale, while negative transferability decreases
- PGF-guided data selection outperforms random selection for improving target task performance
- Task transfer patterns generalize consistently to video-based perception tasks

## Why This Works (Mechanism)

### Mechanism 1: Perfection Gap Factor (PGF) Normalizes Task Transfer Measurement
PGF enables fair comparison of transfer effects across tasks with heterogeneous difficulty levels and performance ceilings by normalizing accuracy gains relative to the remaining gap to ceiling performance. PGF computes the ratio of performance gain to the available headroom, making improvements near ceiling comparable to larger raw gains on harder tasks with more room to improve. This addresses the core challenge that standard accuracy gains cannot fairly compare transfer across tasks with different baselines and ceilings.

### Mechanism 2: Task Cliques Reveal Shared Representational Substructures
Perception tasks form stable clusters of mutually beneficial (positive cliques) or mutually detrimental (negative cliques) transfer, with clique size increasing with model scale. This occurs because finetuning modifies LoRA adapter weights, and tasks sharing underlying visual features or reasoning patterns will have overlapping gradient directions. Larger models have more representational capacity, allowing clearer separation of task clusters through scale-dependent sharpening of these substructures.

### Mechanism 3: Low-Level Perception Tasks Act as Transfer Hubs
Low-level tasks (Relative Depth, Relative Reflectance, Visual Correspondence) exhibit highest positive transferability and malleability because they refine foundational visual representations that scaffold higher-level perception. These tasks act as transfer hubs by modifying early visual features that benefit multiple downstream tasks. Image-level tasks (Art Style, Counting) also show high transferability as they integrate multiple perceptual features.

## Foundational Learning

- **Parameter-Efficient Finetuning (LoRA)**: Only adapter weights change during finetuning while base model weights remain frozen. This is critical for interpreting transfer patterns. *Quick check:* If full-parameter finetuning were used instead of LoRA, would you expect stronger or weaker transfer signals? Why?
- **Zero-Shot Cross-Task Transfer**: Measures how finetuning on a source task affects unseen target tasks without any training on those targets. *Quick check:* A model finetuned on Task A is evaluated on Tasks B, C, D without any gradient updates. What does it mean if performance on Task B improves?
- **Performance Ceiling Normalization**: PGF's key innovation is normalizing by the gap to ceiling (Uj - Acc(M, Tj)). Without this, a +5% gain on an easy task would appear equivalent to +5% on a hard task. *Quick check:* Task X has baseline 95%, ceiling 100%. Task Y has baseline 40%, ceiling 90%. Both improve by +5%. Which has higher PGF?

## Architecture Onboarding

- **Component map:** Qwen-2.5-VL (3B/7B/32B) -> LoRA adapters (rank=8, α=16) -> 13 BLINK perception tasks -> PGF computation -> Transfer graph construction -> Clique detection
- **Critical path:** 1) Select source task Ti and finetune LoRA adapters on Di^train 2) Evaluate finetuned model M(Ti) on all target tasks {Tj} using validation splits 3) Compute PGF matrix µi→j for all source-target pairs 4) Aggregate to compute transferability ∆(i)+/∆(i)- and malleability Θ(j)+/Θ(j)- 5) Construct transfer graph, detect cliques, classify personas
- **Design tradeoffs:** Ceiling choice affects absolute PGF values (100% vs best observed performance); asymmetric bounds (positive capped at 1, negative can reach -(m-1)); multiple-choice format may suppress failure modes in open-ended generation
- **Failure signatures:** High variance across random seeds suggests unstable transfer patterns; diagonal dominance indicates weak transfer; negative cliques identify tasks that should never be co-trained
- **First 3 experiments:** 1) Reproduce PGF heatmap for single model (Qwen-2.5-VL 7B on 3 tasks) 2) Validate clique stability across seeds (identify reported positive clique, run with 4 seeds) 3) Test PGF-guided data selection (select source datasets for target task, compare to random mixture baseline)

## Open Questions the Paper Calls Out

- **Open Question 1:** How does task transfer behavior differ when evaluating VLMs on open-ended generation tasks versus multiple-choice formats? The authors note that multiple-choice format can restrict output space and suppress failure modes that emerge in open-ended generation, suggesting this would be a promising future direction.
- **Open Question 2:** What representational mechanisms underlie the formation of positive and negative task cliques in VLMs? The paper identifies cliques through correlational analysis via LoRA weight cosine similarity but lacks causal explanation of which layers, attention heads, or representations drive transfer behavior.
- **Open Question 3:** Can PGF-guided dataset selection reliably outperform direct finetuning across diverse target tasks and model scales? The authors note that a comprehensive study of PGF-guided dataset mixtures is out of scope, with only preliminary experiments on Qwen-2.5-VL 7B showing promise.
- **Open Question 4:** How does the choice of performance ceiling in PGF affect transferability rankings across tasks? The paper uses 100% as ceiling for all tasks but acknowledges this may not reflect realistic achievable performance, potentially changing relative PGF scores and task classifications.

## Limitations

- Ceiling values for PGF normalization are not explicitly specified for each task, making exact replication challenging
- Analysis focuses on perception tasks in multiple-choice format, potentially missing transfer patterns in open-ended generation or reasoning tasks
- LoRA-based finetuning may constrain transfer magnitude compared to full-parameter finetuning

## Confidence

- **High Confidence:** PGF metric validity, scale-dependent transfer patterns, task clique stability
- **Medium Confidence:** Low-level task transferability claims, PGF-guided data selection effectiveness
- **Low Confidence:** Negative transfer mitigation strategies, video task generalization

## Next Checks

1. **Ceiling Sensitivity Analysis:** Test PGF computation using both 100% ceiling and empirically observed best performance values to verify that relative transfer patterns remain consistent across ceiling choices
2. **Cross-Dataset Transfer Robustness:** Evaluate PGF patterns on a held-out subset of perception tasks not used in the main analysis to test generalizability beyond BLINK benchmark
3. **Fine-tuning Method Comparison:** Compare PGF-based transfer patterns between LoRA and full-parameter finetuning on the same source-target task pairs to quantify the impact of parameter-efficient methods on transfer magnitude