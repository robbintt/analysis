---
ver: rpa2
title: On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing
arxiv_id: '2501.02191'
source_url: https://arxiv.org/abs/2501.02191
tags:
- data
- imputation
- missing
- information
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UnIMP, a unified imputation framework that
  leverages LLM and high-order message passing to enhance the imputation of mixed-type
  data (numerical, categorical, and text). The method introduces a cell-oriented hypergraph
  to model tabular data and proposes BiHMP, a bidirectional high-order message-passing
  network, to aggregate global-local and high-order information while capturing inter-column
  and intra-column patterns.
---

# On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing

## Quick Facts
- arXiv ID: 2501.02191
- Source URL: https://arxiv.org/abs/2501.02191
- Reference count: 40
- Primary result: UnIMP reduces imputation RMSE by 12.11% to 45.22% on numerical/categorical data and improves text imputation accuracy by 30.53%, 28.04%, and 23.47% over DFMs, Table-GPT, and Jellyfish respectively.

## Executive Summary
This paper introduces UnIMP, a unified framework for imputing mixed-type data (numerical, categorical, and text) by leveraging a large language model (LLM) with high-order message passing on a cell-oriented hypergraph. The method constructs a hypergraph where nodes represent cells and hyperedges represent rows and columns, enabling the capture of global-local and high-order relationships. By injecting this structural information into a frozen LLM via adapter modules (BiHMP and Xfusion), UnIMP achieves state-of-the-art imputation accuracy across diverse tabular datasets. The framework is trained using a pre-training and fine-tuning pipeline with chunking and progressive masking techniques.

## Method Summary
UnIMP uses Llama2-7B as a frozen backbone and serializes tabular data into text prompts. It constructs a cell-oriented hypergraph with nodes for cells and hyperedges for rows and columns. BiHMP, a bidirectional high-order message-passing network, aggregates information over this hypergraph to capture inter-column heterogeneity and intra-column homogeneity. Xfusion, an attention-based adapter, merges the LLM's semantic embeddings with the structural embeddings from BiHMP. The model is trained using chunking (512 rows for num/cat, 32 for text) and progressive masking (starting at 35% and increasing by 30% over training), with pre-training on all datasets followed by fine-tuning on specific targets.

## Key Results
- UnIMP reduces imputation RMSE by 12.11% to 45.22% compared to previous methods on numerical and categorical data.
- For text data, UnIMP improves imputation accuracy by 30.53%, 28.04%, and 23.47% over DFMs, Table-GPT, and Jellyfish respectively.
- Theoretical proofs validate the error reduction benefits of capturing higher-order interactions on the hypergraph.

## Why This Works (Mechanism)

### Mechanism 1: Structural Inductive Bias via Adapter Injection
The BiHMP module aggregates features over a cell-oriented hypergraph, capturing row/column dependencies that a linear text sequence would miss. This structural embedding is fused with the LLM's semantic embedding via the Xfusion attention block, forcing the LLM to attend to structural relationships. This bridges the gap between the LLM's pre-trained knowledge and the need for numerical/structural reasoning without updating LLM weights. The core assumption is that the LLM's knowledge is valuable but insufficient for structural reasoning, which can be bridged via adapters.

### Mechanism 2: High-Order Message Passing on Cell-Oriented Hypergraphs
Modeling tables as hypergraphs allows capturing complex, multi-cell dependencies that standard pairwise attention or bipartite graphs miss. A cell-oriented hypergraph treats every cell as a node, with row-hyperedges connecting all cells in a row and column-hyperedges connecting all cells in a column. BiHMP propagates information bidirectionally between nodes and hyperedges, allowing a single cell to update based on the global context of its entire row and column simultaneously. The core assumption is that a cell's value depends on the aggregate state of its entire row and column.

### Mechanism 3: Progressive Curriculum Learning
Gradually increasing the masking ratio during training improves the model's ability to generalize to heavily incomplete data. The progressive masking optimization starts with a lower mask rate and increases it over epochs, allowing the model to learn basic data patterns first before attempting to impute large, complex missing regions. The core assumption is that imputation tasks with high missingness rely on patterns learned from lower missingness scenarios.

## Foundational Learning

- **Concept: Hypergraph Neural Networks (HGNNs)**
  - **Why needed here:** Standard GNNs capture pairwise links, but tabular data often exhibits "one-to-many" or "all-cells-in-a-row" dependencies natively modeled by hyperedges.
  - **Quick check question:** Can you explain the difference between a standard graph edge and a hyperedge in the context of a spreadsheet row?

- **Concept: Adapter-based Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The architecture freezes the LLM backbone and only trains the BiHMP and Xfusion modules. Understanding adapter bottlenecks is critical to debugging gradient flow.
  - **Quick check question:** If the adapter weights are initialized to zero (Zero-Init), what is the expected output of the model at the very first step compared to a standard fine-tuning approach?

- **Concept: Mixed-Type Data Serialization**
  - **Why needed here:** The LLM cannot process a dataframe natively. Understanding how the paper serializes "Row i, col_name=>value" is necessary to troubleshoot tokenization errors.
  - **Quick check question:** How does the model handle numerical vs. text data differently during the tokenization and embedding initialization phase?

## Architecture Onboarding

- **Component map:** Input Layer (Chunking & Serialization) -> Backbone (Frozen LLM) -> Structural Branch (BiHMP) -> Fusion (XFusion) -> Head (Projection Head)
- **Critical path:** The data must be correctly serialized to align the LLM embeddings with the nodes in the Hypergraph. A mismatch here will cause Xfusion to attend to the wrong structural features.
- **Design tradeoffs:**
  - **Chunking Size:** Larger chunks preserve global context but exceed GPU memory/sequence limits. The paper uses 512 for num/cat and 32 for text.
  - **Frozen vs. Full Fine-Tuning:** Freezing LLM preserves general knowledge and saves memory but may limit adaptation to highly specialized domain schemas.
- **Failure signatures:**
  - **High RMSE on Numerical:** Check if BiHMP is actually learning or if the Xfusion module is ignoring the graph signal.
  - **OOM Errors:** Chunk size is likely too large; reduce `cszie`.
  - **Slow Convergence:** Progressive masking might be starting too aggressively.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run `w/o Hypergraph` vs. `UnIMP` on a small dataset (e.g., Blogger) to verify that the structural branch is actually contributing to accuracy.
  2. **Hyperparameter Sensitivity:** Vary the chunk size (128 vs. 512 vs. 2048) on a large dataset (e.g., Power) to identify the efficiency/accuracy sweet spot for your hardware.
  3. **Generalization Test:** Pre-train on a set of generic tables, then fine-tune and evaluate on a domain-specific dataset (e.g., Healthcare) not seen during pre-training to validate the transfer learning claim.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the performance of UnIMP be rigorously evaluated under Missing Not At Random (MNAR) mechanisms for text data?
  - **Basis in paper:** Section 5.2 (Exp-3) states that for text data, "it is hard to simulate such a relationship," leading the authors to focus MNAR and MAR evaluations solely on numerical and categorical data.
  - **Why unresolved:** Current benchmarks lack a standardized protocol to simulate non-random missingness in unstructured text fields based on semantic content, creating a gap in validation for mixed-type imputation frameworks.

- **Open Question 2:** To what extent does the chunking optimization compromise the capture of global-local information when critical dependencies span across chunk boundaries?
  - **Basis in paper:** Theorem 3.1 proves the importance of global-local information for reducing error, but the "chunking technique" (Section 4.2) splits tables into 512-row segments, potentially severing long-range row-wise dependencies.
  - **Why unresolved:** The paper does not provide an ablation study analyzing performance degradation specifically on datasets where row-wise dependencies exceed the chunk size.

- **Open Question 3:** Is there a practical upper bound for high-order message passing in BiHMP where increased depth leads to over-smoothing or noise propagation rather than error reduction?
  - **Basis in paper:** Theorem 3.2 suggests models capturing higher-order interactions yield lower errors, but the implementation fixes the layers ($l_{max}$) without analyzing the trade-off point where noise might dominate.
  - **Why unresolved:** While the theory supports higher orders, standard message-passing networks often suffer from over-smoothing as depth increases, a limitation not explicitly tested in the ablation studies.

## Limitations
- **Structural Dependency Assumption:** The high-order hypergraph aggregation assumes strong inter-row and inter-column dependencies exist in all datasets, which may not hold for randomly generated or independently sampled tables.
- **Serialization Overhead:** The explicit "Row i, col_name=>value" serialization inflates sequence length and token counts, potentially limiting scalability to very large tables or high-cardinality schemas.
- **Computational Trade-off:** Freezing the LLM saves memory but may prevent adaptation to highly domain-specific tabular patterns that differ significantly from the LLM's pre-training corpus.

## Confidence
- **High Confidence:** The core mechanism of using adapter-based PEFT to inject structural priors (BiHMP + Xfusion) into a frozen LLM is well-supported by the ablation study and aligns with established PEFT literature.
- **Medium Confidence:** The theoretical error bound reduction from high-order message passing is mathematically proven, but its empirical benefit may vary significantly with dataset characteristics.
- **Low Confidence:** The exact impact of the progressive masking curriculum is difficult to isolate from other training factors, as the paper does not provide ablations isolating this component.

## Next Checks
1. **Ablation on Structural Signal:** Run `w/o Hypergraph` vs. `UnIMP` on a dataset with known strong row/column dependencies (e.g., Power) and one with weak dependencies (e.g., randomly generated noise) to confirm the adapter is beneficial only when structure exists.
2. **Curriculum Learning Isolation:** Implement a fixed-mask-rate baseline (UnIMP-fix) trained with the final mask rate from the start, and compare its performance to UnIMP's progressive masking on a large dataset (e.g., Power) to quantify the curriculum's contribution.
3. **Scalability Benchmark:** Measure UnIMP's performance and VRAM usage on increasingly large tables (varying row count and chunk size) to identify the practical limits of the current chunking and hypergraph construction approach.