---
ver: rpa2
title: 'Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing
  Act of Selective Unlearning in Large Language Models'
arxiv_id: '2503.04795'
source_url: https://arxiv.org/abs/2503.04795
tags:
- unlearning
- gradient
- forget
- language
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents experiments on selective unlearning in LLMs
  for SemEval 2025 Task 4, addressing the challenge of removing sensitive data from
  models without full retraining. The core method involves gradient-based weight modifications,
  including gradient ascent, descent, and controlled variants, to balance unlearning
  effectiveness with knowledge retention and model utility.
---

# Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2503.04795
- Source URL: https://arxiv.org/abs/2503.04795
- Reference count: 25
- 7B model achieved aggregate score of 0.409; 1B model achieved 0.389 on test set

## Executive Summary
This paper tackles SemEval 2025 Task 4 on selective unlearning from large language models, addressing the challenge of removing sensitive data without full retraining. The authors propose gradient-based weight modification techniques including gradient ascent, descent, and controlled variants to balance unlearning effectiveness with knowledge retention and model utility. Through systematic experimentation on OLMo models of different scales, they demonstrate that selective unlearning is feasible but highly dependent on model size and configuration, with gradient descent working best for 1B models and sequential gradient approaches for 7B models.

## Method Summary
The core method involves three gradient-based unlearning techniques applied sequentially or individually. Gradient ascent maximizes loss on forget data to unlearn targeted information, while gradient descent on retain data indirectly causes unlearning through catastrophic forgetting. A sequential approach combines gradient ascent followed by gradient descent to balance unlearning effectiveness with utility restoration. The experiments use OLMo-7B and OLMo-1B models with 4-bit quantization for the larger model, training on forget and retain datasets with specific configurations for learning rate, weight decay, and epoch counts. Evaluation uses task aggregate scores, membership inference attack metrics, and MMLU accuracy to verify the balance between unlearning and utility preservation.

## Key Results
- Gradient descent on 1B model achieved near-perfect unlearning (MIA 0.982) with minimal MMLU loss
- Sequential gradient difference followed by gradient ascent achieved best aggregate score of 0.409 on 7B model
- Aggressive gradient ascent caused catastrophic utility degradation in 7B model (MMLU accuracy dropped from 0.512 to 0.284)
- Optimal method varied significantly by model scale: gradient descent for 1B, sequential approach for 7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient ascent unlearns targeted information by maximizing loss on the forget set.
- Mechanism: The model is trained on forget data with inverted loss (maximizing rather than minimizing), forcing divergence from learned representations. The 1B model achieved a MIA score of 0.807 with aggressive configurations (higher LR and weight decay); the 7B model achieved 0.865 with 10 epochs at LR=2e-6.
- Core assumption: Assumption: Maximizing loss on forget data specifically degrades those representations without uniformly degrading all model capabilities.
- Evidence anchors:
  - [abstract] "...primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility."
  - [Page 4, Section 4] "In this method, the target model was trained on the forget set with an inverted loss, thereby, making it to unlearning the training set rather than learning it."
  - [corpus] Related work "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models" provides task framework context.
- Break condition: Over-aggressive gradient ascent catastrophically degrades model utility (MMLU accuracy dropped from 0.512 to 0.284 in the 7B model with 10 epochs).

### Mechanism 2
- Claim: Gradient descent on the retain set indirectly causes unlearning through selective retraining (catastrophic forgetting).
- Mechanism: Rather than directly attacking forget data, the model is retrained on retain data, strengthening desired knowledge while allowing forget knowledge to degrade. The 1B model trained for 20 epochs achieved near-perfect unlearning (MIA 0.982) with minimal MMLU loss (0.247 vs. original 0.274).
- Core assumption: Assumption: Retaining certain knowledge competitively displaces forgotten knowledge without explicit unlearning signals.
- Evidence anchors:
  - [Page 4, Section 4] "The intuition is that, the strong adaption of the model only to the retain set can naturally make it tend to forget the other information (forget set) it was previously trained on, like catastrophic forgetting."
  - [Page 4, Table 3] Gradient descent achieved aggregate 0.410 on 1B model with MIA 0.982.
  - [corpus] "QUAIL: Quantization Aware Unlearning" notes quantization affects unlearning stability (weakly related).
- Break condition: This approach failed on the 7B model (aggregate 0.170, same as original), indicating scale-dependent effectiveness.

### Mechanism 3
- Claim: Sequential gradient ascent followed by gradient descent balances unlearning and utility restoration.
- Mechanism: Gradient ascent removes forget knowledge; subsequent gradient descent on retain data recovers general capabilities damaged by aggressive unlearning. A single descent epoch with LR=2e-6 recovered MMLU accuracy from 0.284 to 0.511 in the 7B model.
- Core assumption: Assumption: Utility degradation from gradient ascent is separable from the unlearning effect and can be selectively repaired.
- Evidence anchors:
  - [Page 5, Section 4] "...a single subsequent gradient descent epoch with LR as low as 2e-6 brought it back to 0.511, emphasizing the vital role and impact of gradient descent in mitigating utility loss in larger models."
  - [Page 5, Table 4] "Gradient Difference -> Gradient Ascent" achieved test aggregate 0.409 on 7B model.
  - [corpus] "Lacuna Inc. at SemEval-2025 Task 4" explores LoRA-enhanced influence-based unlearning as an alternative.
- Break condition: Excessive gradient descent can fully restore the original model state, negating unlearning (MIA dropped from 0.982 to 0 with additional descent at LR=2e-8).

## Foundational Learning

- Concept: Membership Inference Attack (MIA)
  - Why needed here: This metric determines unlearning success by testing if the model can distinguish forget samples from non-members. Score near 0.5 is optimal.
  - Quick check question: Why does an MIA score of 0 indicate under-unlearning while 1 indicates over-unlearning?

- Concept: Catastrophic forgetting
  - Why needed here: This phenomenon explains why gradient descent on retain data works—the model naturally forgets previously learned information when trained on new data.
  - Quick check question: How does catastrophic forgetting differ from intentional unlearning in terms of mechanism?

- Concept: Knowledge retention vs. model utility tradeoff
  - Why needed here: The core challenge is balancing removal of forget knowledge (MIA) against maintaining general capabilities (MMLU accuracy).
  - Quick check question: Why might aggressive unlearning strategies degrade general model capabilities beyond just targeted forget knowledge?

## Architecture Onboarding

- Component map:
  - Target models: OLMo-7B-0724-Instruct-hf (7B) and OLMo-1B-0724-hf (1B)
  - Infrastructure: NVIDIA RTX A6000 (48GB)
  - 7B model requires 4-bit quantization with PEFT/LoRA (r=64, alpha=16)
  - Data splits: Forget (1,112 train/254 val), Retain (1,136 train/278 val)
  - Metrics: Task aggregate (harmonic mean of 12 scores), MIA score (0.5 optimal), MMLU accuracy (threshold 0.371)

- Critical path:
  1. Load target model (full precision for 1B, 4-bit quantized for 7B with LoRA)
  2. Configure gradient training (inverted loss for ascent, standard for descent)
  3. Train sequentially on forget (ascent) then retain (descent)
  4. Evaluate three metrics to verify balance
  5. Adjust hyperparameters (LR, weight decay, epochs) based on metric feedback

- Design tradeoffs:
  - Model scale: 7B requires quantization (potential numerical instability aids unlearning) but is harder to control; 1B responds more predictably
  - Aggressiveness vs. precision: Higher LR/weight decay increases unlearning intensity but risks utility collapse
  - Sequential vs. single-phase: Sequential approaches offer better control but require careful tuning to avoid negating unlearning

- Failure signatures:
  - MIA near 0: Under-unlearning (model still remembers forget set)
  - MIA near 1: Over-unlearning (catastrophic degradation)
  - MMLU accuracy below 0.371 threshold: Utility collapse
  - Aggregate near 0.17 (same as original): No effective unlearning
  - Quantized 7B with gradient descent alone: No unlearning effect

- First 3 experiments:
  1. **Baseline gradient ascent on 1B**: Train on forget set with inverted loss (LR=2e-5, WD=2e-4, 3 epochs). Monitor MIA and MMLU degradation.
  2. **Gradient descent on retain set (1B)**: Train on retain set only (LR=2e-5, WD=2e-4, 20 epochs). Compare MIA against ascent baseline.
  3. **Sequential approach on 7B**: Ascent (LR=1e-5, 3 epochs) → descent on retain (LR=2e-6, 3 epochs) → ascent (LR=2e-5, 1 epoch). Use 4-bit quantization with LoRA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does quantization enhance or impair unlearning effectiveness, and through what mechanism?
- Basis in paper: [inferred] The authors note that "Quantization in the 7B model may have also enhanced unlearning, potentially due to increased numerical instability aiding divergence from the learned state," but this remains speculative.
- Why unresolved: The 7B model was only tested in 4-bit quantized form due to computational constraints, lacking a non-quantized baseline for direct comparison.
- What evidence would resolve it: Controlled experiments comparing unlearning performance on the same model architecture with and without quantization, measuring both unlearning metrics and analyzing numerical instability patterns.

### Open Question 2
- Question: What is the minimum sufficient size of forget data required for effective unlearning?
- Basis in paper: [explicit] The paper states "the size of the forget set plays an important role to understand the information to be forgotten effectively, and availability or provision of only limited forget samples could lead to ineffective unlearning of the target model."
- Why unresolved: The experiments with assistant models trained on the forget set failed due to insufficient forget samples, but no systematic analysis of data quantity thresholds was conducted.
- What evidence would resolve it: Ablation studies varying forget set sizes across different unlearning methods, measuring the point at which unlearning effectiveness plateaus or degrades.

### Open Question 3
- Question: Why does gradient descent alone work better for the 1B model, while gradient difference followed by gradient ascent works better for the 7B model?
- Basis in paper: [inferred] The authors report different optimal methods for different scales but conclude only that "performance of a method significantly depends on the scale of the target model, and the kind of data it is presented with."
- Why unresolved: The paper does not provide a theoretical or empirical explanation for why model scale changes the relative effectiveness of different gradient-based approaches.
- What evidence would resolve it: Systematic experiments across multiple model scales (e.g., 1B, 3B, 7B, 13B) with all methods, combined with analysis of gradient distributions and parameter sensitivity at each scale.

### Open Question 4
- Question: Can forget and retain samples be reliably distinguished in feature space to enable more targeted unlearning?
- Basis in paper: [explicit] Appendix A shows clustering and classification experiments where "the features used to represent the samples result in significant overlap in the feature space, failing to provide sufficient separation between the two disjoint classes."
- Why unresolved: The attempted prompt routing approach was abandoned due to poor classification performance, but no alternative feature representations or more sophisticated separation methods were explored.
- What evidence would resolve it: Exploration of alternative embedding spaces, attention patterns, or intermediate layer representations to identify discriminative features between forget and retain samples.

## Limitations
- Method effectiveness varies dramatically by model scale, with different approaches optimal for 1B vs 7B models
- Limited exploration of quantization's role in unlearning effectiveness due to computational constraints
- No systematic analysis of how forget data quantity affects unlearning success

## Confidence

- **High Confidence**: The core experimental results showing gradient descent on the 1B model achieved near-perfect unlearning (MIA 0.982) while maintaining utility, and the sequential gradient approach worked for the 7B model. The observed tradeoffs between unlearning effectiveness and model utility are clearly demonstrated through the metrics.
- **Medium Confidence**: The generalizability of these methods to other model architectures and datasets. While the mechanisms are sound, the specific hyperparameters that worked for OLMo may not transfer to other LLMs without significant tuning.
- **Low Confidence**: The claim that gradient difference followed by gradient ascent is universally optimal for larger models. This was only tested on one 7B configuration, and the paper doesn't explore whether other sequential patterns or different architectures might yield better results.

## Next Checks

1. **Scale Generalization Test**: Apply the best-performing methods (gradient descent for 1B, sequential gradient for 7B) to intermediate model sizes (3B, 5B) to identify the precise scale threshold where methods diverge in effectiveness.
2. **Architecture Transfer Validation**: Implement the same unlearning pipeline on a different LLM family (e.g., LLaMA or Mistral) using identical datasets to test whether the gradient-based approach transfers or requires fundamental reconfiguration.
3. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates (spanning 1e-6 to 1e-4), weight decay values, and epoch counts in small increments to map the full solution space and identify whether the reported configurations represent local optima or global trends.