---
ver: rpa2
title: Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems
arxiv_id: '2510.04559'
source_url: https://arxiv.org/abs/2510.04559
tags:
- linear
- arms
- subcarrier
- ofdm
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Champion\u2013Challenger Sampling (CCS),\
  \ a latency-aware bandit algorithm for top-m subcarrier selection in OFDM downlink\
  \ systems. CCS maintains a small set of current \"champion\" arms and a rotating\
  \ shortlist of \"challenger\" arms, focusing measurements only on the most informative\
  \ comparisons."
---

# Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems

## Quick Facts
- arXiv ID: 2510.04559
- Source URL: https://arxiv.org/abs/2510.04559
- Reference count: 27
- Introduces Champion–Challenger Sampling (CCS) algorithm achieving 100×, 15×, 300× speedups over state-of-the-art linear bandit methods

## Executive Summary
This paper presents Champion–Challenger Sampling (CCS), a latency-aware bandit algorithm for top-m subcarrier selection in OFDM downlink systems. CCS dramatically reduces computational complexity by focusing measurements only on comparisons between a small champion set and rotating challenger shortlist, rather than scanning all possible arm pairs. The method maintains near-perfect identification accuracy while achieving speedups of 100-300× over state-of-the-art linear bandit algorithms like LinGapE, LinUGapE, and LinGIFA.

## Method Summary
CCS formulates subcarrier selection as a combinatorial pure-exploration problem in stochastic linear bandits. The algorithm maintains a champion set U_t (current top-m estimates) and rotating challenger shortlist C_t, computing gap-indices only between these bounded sets. A Taylor expansion linearizes the log₂(1+γ) rate function, enabling ridge-regularized least-squares parameter estimation. The algorithm stops when the most ambiguous pair (weakest champion vs. strongest challenger) has gap-index below ε, providing (ε,m,δ)-PAC guarantees. Key parameters include 20-term Taylor features, shortlist sizes matching K∈{40,100,600}, and δ=0.05.

## Key Results
- Achieves 100×, 15×, and 300× speedups over LinGapE, LinUGapE, and LinGIFA respectively
- Maintains 100% correctness within a few hundred challenger arms
- Reduces number of pairwise gap-index computations by orders of magnitude
- Provides (ε,m,δ)-PAC guarantees with tunable speed-accuracy trade-off via shortlist size

## Why This Works (Mechanism)

### Mechanism 1
Restricting comparisons to champion set and challenger shortlist reduces computational complexity from Ω(K²) to O(m|C_t|) per round. The algorithm maintains U_t and C_t, computing indices only between these bounded sets and selecting the most ambiguous pair for sampling. Core assumption: true top-m arms eventually enter champion or challenger sets. Break condition: if |C_t| too small, true adversary may never enter shortlist, causing premature stopping.

### Mechanism 2
Gap-index B_t(i,j) = μ̂_t(i) - μ̂_t(j) + W_t(i,j) upper-bounds true mean difference with high probability. The index combines estimated mean difference with uncertainty term W_t, which shrinks as samples accumulate. Core assumption: reward noise is sub-Gaussian, enabling confidence radius derivation. Break condition: if noise is heavy-tailed or σ² underestimated, confidence widths become invalid, potentially causing incorrect early stopping.

### Mechanism 3
Taylor expansion linearizes the concave log₂(1+γ) rate function, reducing subcarrier selection to linear bandit problem. Equation (9) approximates R_i ≈ α₀ + Σα_j γ_i^j with feature vectors x_i = [γ_i, γ_i², ..., γ_i^N]. Core assumption: Taylor surrogate remains accurate across operating SNR range. Break condition: at extreme SNRs, high-order polynomial surrogates may exhibit numerical instability or extrapolation error.

## Foundational Learning

- **Stochastic Linear Bandits**: CCS formulates subcarrier selection as linear bandit where μ(a) = x_a^T θ*, requiring understanding of exploration-exploitation trade-offs. Quick check: Given V_t = λI + Σ x_a x_a^T, how does confidence radius scale with feature vector of new arm?

- **Combinatorial Pure Exploration (CPE)**: Objective is identifying top-m subset from (K choose m) candidates without optimizing cumulative regret. Quick check: Why does CPE require fixed-confidence stopping rule rather than fixed-budget horizon?

- **(ε,m,δ)-PAC Guarantees**: Paper claims finite-sample identification with probability ≥ 1-δ that returned set is ε-optimal. Quick check: If ε = 10⁻¹⁵ and δ = 0.05, what does guarantee actually promise about returned top-m set?

## Architecture Onboarding

- **Component map**: Feature extraction -> Parameter estimator -> Champion set U_t -> Challenger shortlist C_t -> Ambiguity resolver -> Stopping checker

- **Critical path**: Initialize U_0 randomly → Compute gap-indices for U_t ∪ C_t → Identify (b_t, ca_t) → Sample arm minimizing variance between them → Update θ̂_t → Swap if challenger superior → Rotate C_t → Check stopping condition

- **Design tradeoffs**:
  - **Challenger size |C_t|**: Larger size increases probability of capturing true adversary but linearly increases per-round computation
  - **Taylor order N**: Higher order improves surrogate fidelity at extreme SNRs but risks numerical instability
  - **ε tolerance**: Tighter ε increases sample complexity but provides stronger identification guarantees

- **Failure signatures**:
  - Correctness plateauing below 100%: |C_t| too small; increase shortlist size
  - Runtime not scaling with K: Check shortlist rotation is sampling new arms, not re-examining same candidates
  - θ̂_t diverging: Verify regularization λ > 0 and feature normalization; check for near-collinear features in high-order Taylor terms
  - Premature stopping with incorrect set: ε may be too loose, or noise proxy σ significantly underestimated

- **First 3 experiments**:
  1. **Baseline sanity check**: K=40, m=12, |C_t|=10. Verify CCS returns correct top-m set and compare arm pull counts against LinGapE.
  2. **Scaling stress test**: K=600, m=12, vary |C_t| ∈ {50, 100, 200, 400}. Plot correctness and runtime vs. |C_t| to validate speed-accuracy trade-off curve.
  3. **Noise robustness probe**: Fix K=100, inject SNR estimation errors with ξ_i,t standard deviation ∈ {0.5, 1.0, 2.0} dB. Measure degradation in correctness and sample complexity.

## Open Questions the Paper Calls Out

- **RB contiguity and multi-user fairness**: How to extend CCS framework to handle operational constraints like Resource Block contiguity and multi-user fairness in practical scheduling scenarios.

- **Adaptive shortlists**: Can the challenger shortlist size be made adaptive rather than fixed to dynamically adjust based on convergence metrics?

- **Extreme SNR stability**: Does the high-order Taylor surrogate for reward function remain stable under extreme SNR conditions beyond the validated mid-SNR range?

- **Hardware-in-the-loop evaluation**: What is the performance of CCS in real-time hardware implementation with actual channel conditions and system overheads?

## Limitations

- **Specification gaps**: Confidence width W_t(i,j) in gap-index is not explicitly detailed, leaving ambiguity about exact sub-Gaussian concentration constants used.

- **Noise model validation**: While sub-Gaussian noise is justified theoretically, empirical robustness to heavy-tailed or correlated noise in real OFDM channels is not validated.

- **Feature normalization ambiguity**: Taylor feature extraction process lacks specification of normalization procedures, which could impact numerical stability for high-order expansions.

## Confidence

- **High confidence**: Speedup claims (100×, 15×, 300×) and correctness measurements (>99% for adequate shortlist sizes) - directly supported by empirical results in Figures 2 and 3.

- **Medium confidence**: PAC guarantee validity - Theorem 1 provides theoretical backing, but depends on unspecified confidence width parameters and noise model assumptions.

- **Low confidence**: Generalization to non-synthetic channels - evaluation uses synthetic Rayleigh fading without validation on measured channel data or different fading distributions.

## Next Checks

1. **Noise sensitivity analysis**: Systematically vary noise standard deviation σ_ξ in SNR observations and measure degradation in correctness and sample complexity to validate sub-Gaussian assumption and identify operational limits.

2. **Feature stability test**: Vary Taylor expansion order N ∈ {10, 15, 20, 25} and measure numerical condition number of feature matrix X to identify when numerical instability emerges and correctness degrades.

3. **Rotation mechanism audit**: Instrument CCS to log challenger set composition over time to verify rotation actually samples fresh arms from full pool rather than cycling through small subset, ensuring algorithm's theoretical guarantees remain valid.