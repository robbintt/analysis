---
ver: rpa2
title: 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning'
arxiv_id: '2509.22647'
source_url: https://arxiv.org/abs/2509.22647
tags:
- image
- caption
- caprl
- arxiv
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CapRL, a novel framework that applies Reinforcement
  Learning with Verifiable Rewards (RLVR) to the open-ended image captioning task.
  Traditional captioning models rely on supervised fine-tuning, which limits diversity
  and creativity.
---

# CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.22647
- Source URL: https://arxiv.org/abs/2509.22647
- Authors: Long Xing; Xiaoyi Dong; Yuhang Zang; Yuhang Cao; Jianze Liang; Qidong Huang; Jiaqi Wang; Feng Wu; Dahua Lin
- Reference count: 40
- One-line primary result: CapRL uses RLVR with VQA accuracy to train dense image captions, achieving performance comparable to a 72B model and improving average score by 8.4% over baseline.

## Executive Summary
This paper introduces CapRL, a novel framework that applies Reinforcement Learning with Verifiable Rewards (RLVR) to the open-ended image captioning task. Traditional captioning models rely on supervised fine-tuning, which limits diversity and creativity. CapRL overcomes this by redefining caption quality through utility: a good caption enables a vision-free LLM to accurately answer questions about the image. It employs a decoupled two-stage pipeline where an LVLM generates a caption, and a separate LLM answers multiple-choice questions based solely on that caption. The accuracy of the LLM's answers serves as an objective reward for RLVR training. Extensive experiments show that CapRL significantly improves caption quality, achieving performance comparable to a 72B model and outperforming the baseline by an average margin of 8.4%. Pretraining on the CapRL-5M dataset further enhances LVLM performance across 12 benchmarks, demonstrating strong scalability and effectiveness.

## Method Summary
CapRL is a two-stage pipeline that trains LVLMs to generate dense image captions using RLVR with VQA accuracy as the reward signal. The framework generates a curated QA dataset by using a large VL model (Qwen2.5-VL-72B) to create 5 QA pairs per image, then filtering them through a smaller VL model (Qwen2.5-VL-3B) to remove questions answerable without the image. During training, the policy LVLM samples multiple captions per image, and a vision-free LLM (Qwen2.5-3B-Instruct) answers shuffled MCQs based solely on each caption. The average binary accuracy becomes the reward for GRPO training, with KL penalty to maintain diversity. The method is pretrained on a 5M image dataset with CapRL-3B captions before fine-tuning.

## Key Results
- CapRL significantly improves dense image captioning quality compared to baseline supervised models
- Achieves performance comparable to a 72B parameter model while using only 3B parameters
- Outperforms baseline by an average margin of 8.4% across 12 benchmarks
- Demonstrates strong scalability with pretraining on CapRL-5M dataset
- Shows that sparse QA supervision (1-2 questions per image) is sufficient for effective training

## Why This Works (Mechanism)

### Mechanism 1: Decoupled VQA-as-Verifiable-Reward Converts Subjective to Objective
- Claim: Caption quality can be objectively measured by testing whether a vision-free LLM can answer image-specific questions using only the generated caption.
- Mechanism: A caption that preserves critical visual information enables correct MCQ answers; incomplete or hallucinated captions cause answer failures. Binary accuracy across shuffled option sets provides stable, differentiable reward signal.
- Core assumption: QA pairs are comprehensive enough that high VQA accuracy correlates with dense, accurate captioning.
- Evidence anchors: Abstract states high-quality captions should enable accurate answers; Section 3.1 defines reward as average MCQ accuracy with shuffling; no existing work uses VQA accuracy as RL reward for caption training.

### Mechanism 2: Objective Reward Avoids Reward Hacking by Being Hard to Game
- Claim: Using verifiable, binary MCQ accuracy prevents policy models from exploiting biases in subjective reward models.
- Mechanism: Subjective judges have implicit preferences the policy learns to exploit; MCQ accuracy requires specific, correct information in captions.
- Core assumption: Evaluator LLM is stable on MCQ tasks without strong option-position bias.
- Evidence anchors: Figures 1a and 1c show training collapse with UnifiedReward and verbose exploitation with Qwen2.5-VL-as-Judge; Table 3 shows UnifiedRW-as-Judge-3B underperforms baseline.

### Mechanism 3: Sparse QA Supervision Is Sufficient with Proper Filtering
- Claim: Even one QA pair per image provides meaningful training signal if questions are strictly image-grounded.
- Mechanism: GRPO optimizes over sampled captions; single well-designed question creates sufficient comparative signal across candidates.
- Core assumption: QA filtering successfully removes questions answerable without the image.
- Evidence anchors: Table 4 shows CapRL-1QA-20k improves average score by +7.4% over baseline; diminishing returns with more QA per image.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: CapRL frames captioning as RL where reward is MCQ correctness, not human preference.
  - Quick check question: Can you explain why a binary "correct/incorrect" reward from a deterministic verifier is more stable for RL than a learned reward model?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: CapRL uses GRPO to sample multiple captions per image and compute advantages from group reward statistics.
  - Quick check question: In GRPO, how does computing advantage from within-group reward statistics differ from a standard PPO setup with a value function?

- **Reward Hacking in Language Models**
  - Why needed here: The paper explicitly contrasts CapRL with judge-based rewards that are exploited.
  - Quick check question: Give two examples of how a policy model might "hack" a reward model that prefers verbose outputs.

## Architecture Onboarding

- **Component map:**
  Policy LVLM (Qwen2.5-VL-3B) -> Caption generation -> MCQ sampling -> Evaluator LLM (Qwen2.5-3B-Instruct) -> Binary accuracy -> Reward calculator -> GRPO update

- **Critical path:**
  1. Sample G captions per image from policy LVLM
  2. For each caption, sample N MCQs, shuffle options, query evaluator LLM
  3. Compute per-caption reward R_ci (average accuracy)
  4. Compute group advantage, apply KL penalty, update policy via gradient step
  5. Periodically evaluate on Prism benchmarks (caption -> LLM VQA accuracy)

- **Design tradeoffs:**
  - Evaluator LLM size: Smaller LLM (3B) is faster but may have limited reasoning
  - QA per image: More QA increases robustness but with diminishing returns
  - Sampling rounds N: Higher N reduces variance but linearly increases compute

- **Failure signatures:**
  - Training collapse with short outputs: Indicates reward model bias toward brevity
  - Verbose but irrelevant outputs: Suggests judge prefers length over content
  - High reward, low VQA benchmarks: QA leakage; model solving questions without real image understanding
  - Instability across runs: Insufficient shuffling or N too low; option bias dominates signal

- **First 3 experiments:**
  1. Reproduce reward hacking: Train with Qwen2.5-VL-as-Judge on small 5k subset; confirm verbose drift and benchmark degradation
  2. Ablate QA filtering: Train CapRL with unfiltered QA; compare benchmark scores to filtered setup
  3. Scale evaluator: Swap Qwen2.5-3B-Instruct for a 7B evaluator; measure reward quality vs. training throughput

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CapRL framework be effectively adapted for long-context multimodal tasks, such as video captioning or streaming video understanding?
- Basis in paper: Appendix G explicitly identifies "long-context multimodality" and "streaming video" as future directions.
- Why unresolved: Current reward mechanism relies on decoupled VQA for static images; unclear if MCQ accuracy is sufficient proxy for temporal consistency in videos.
- What evidence would resolve it: Experimental results applying RLVR framework to video benchmarks, demonstrating that CapRL rewards can enforce temporal alignment.

### Open Question 2
- Question: Is the performance of the captioning policy model fundamentally limited by the reasoning capacity of the specific LLM used to calculate the verifiable reward?
- Basis in paper: Section 3.1 details use of Qwen2.5-3B-Instruct as reward-providing LLM for efficiency.
- Why unresolved: If reward LLM fails to comprehend complex sentence structures, it may provide false negative reward, capping policy model's linguistic complexity.
- What evidence would resolve it: Ablation study comparing policy models trained using reward LLMs of varying sizes to determine if "weak" judge bottlenecks "strong" policy model.

### Open Question 3
- Question: Does optimization for MCQ accuracy implicitly bias captions toward "bag of attributes" style at expense of narrative coherence?
- Basis in paper: Methodology in Section 3.1 redefines caption quality strictly through "utility" (ability to answer questions).
- Why unresolved: "Utility" definition favors information density, which might encourage disjointed listing rather than cohesive prose.
- What evidence would resolve it: Human evaluation specifically scoring "fluency" and "coherence" of CapRL outputs against SFT baselines.

## Limitations
- The approach depends critically on quality of decoupled VQA reward signal, which may propagate evaluator LLM biases
- Computational overhead of sampling multiple captions and MCQs per image may limit scalability
- Filtering step (Eq. 4) is crucial but its effectiveness is not independently verified
- Paper does not fully validate whether MCQ accuracy metric aligns with human judgment beyond Prism benchmarks

## Confidence
- **High Confidence:** The core mechanism of using decoupled VQA accuracy as objective reward is well-defined and supported by experimental evidence
- **Medium Confidence:** The assertion that sparse QA supervision (1-2 questions per image) is sufficient relies on ablation results but may not generalize to all image types
- **Low Confidence:** The long-term generalization of CapRL-generated captions across diverse tasks is not fully explored

## Next Checks
1. **Reward Stability Analysis:** Test sensitivity of reward signal to evaluator LLM variations (different model sizes, prompts) to ensure robustness
2. **QA Leakage Re-test:** Conduct controlled experiment where filtered and unfiltered QA pairs are used to train CapRL, measuring impact on caption quality
3. **Scalability Assessment:** Evaluate training time and resource usage of CapRL on larger datasets to determine practical limitations for real-world deployment