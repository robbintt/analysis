---
ver: rpa2
title: PDF Retrieval Augmented Question Answering
arxiv_id: '2506.18027'
source_url: https://arxiv.org/abs/2506.18027
tags:
- text
- retrieval
- system
- arxiv
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of enhancing Question-Answering
  (QA) systems to handle rich, multimodal PDF documents containing text, images, diagrams,
  tables, and vector graphics. The authors introduce a Retrieval Augmented Generation
  (RAG) framework, PIER-QA, that integrates non-textual elements by preprocessing
  PDFs to remove headers/footers, convert to markdown, generate captions for images,
  and reformat tables.
---

# PDF Retrieval Augmented Question Answering

## Quick Facts
- **arXiv ID**: 2506.18027
- **Source URL**: https://arxiv.org/abs/2506.18027
- **Reference count**: 13
- **Primary result**: PIER-QA framework integrates multimodal PDF content (text, images, tables, diagrams) into text-based RAG systems, achieving 77% accuracy@0.85 and strong performance on visual retrieval tasks (65.66% image, 48.38% table accuracy).

## Executive Summary
This work introduces PIER-QA, a Retrieval Augmented Generation framework designed to handle rich, multimodal PDF documents containing text, images, diagrams, tables, and vector graphics. The authors address the challenge of making non-textual elements accessible to text-based RAG systems by converting them into searchable text representations through image captioning and table compression. They also fine-tune a large language model (Llama3-70B) on domain-specific RAG-style QA pairs to improve context relevance assessment. Experimental results demonstrate significant improvements over text-only baselines across multiple evaluation metrics, with particular success in retrieving image and table content.

## Method Summary
PIER-QA employs a multi-stage pipeline: PDFs are first preprocessed using DBSCAN clustering to remove headers and footers, then converted to markdown format while extracting images. Extracted images are processed by LLaVA to generate descriptive captions, and tables are compressed into dictionary format. These text representations become searchable chunks (1000 characters) embedded using GTE-large and stored in Elasticsearch with RAPTOR semantic tree indexing. For inference, queries are embedded and the top-10 most relevant chunks are retrieved and fed to a fine-tuned Llama3-70B model that has been trained on synthetic RAG-style QA pairs mixing relevant and irrelevant contexts. If the LLM outputs an image or table ID, the system retrieves and displays the original visual element.

## Key Results
- PIER-QA achieves 0.8837 similarity score vs 0.8647 for baseline without preprocessing
- RAG-aware fine-tuning improves similarity from 0.8156 (base) to 0.8771
- Image retrieval accuracy: 65.66% at threshold 0.85
- Table retrieval accuracy: 48.38% at threshold 0.85
- Overall accuracy@0.85 reaches 77% across all content types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting non-textual PDF elements into text representations enables standard text-based RAG retrieval to access multimodal content.
- Mechanism: LLaVA generates captions for extracted images with unique IDs; markdown tables are compressed into dictionary format; both become searchable text chunks. During inference, if the LLM outputs an image ID, the system retrieves and displays the original visual.
- Core assumption: Text-based semantic similarity is sufficient to retrieve relevant visual content if adequately described in captions.
- Evidence anchors:
  - [abstract] "image captioning, and table compression, then stores embeddings in Elasticsearch"
  - [section 3.2.1] "generating captions to the images which alleviates image modality and turns the input into LLM's native domain: text"
  - [corpus] VDocRAG (FMR=0.68) similarly addresses visually-rich documents but uses direct visual understanding rather than captioning
- Break condition: Complex diagrams where captions cannot capture spatial relationships or quantitative data.

### Mechanism 2
- Claim: Removing headers and footers via spatial clustering reduces retrieval noise and improves answer precision.
- Mechanism: DBSCAN clusters bounding boxes of PDF elements by spatial coordinates. Clusters appearing consistently at page tops/bottoms across 10-page windows are classified as headers/footers and removed.
- Core assumption: Headers/footers have consistent spatial coordinates and repeat across pages; main content varies.
- Evidence anchors:
  - [section 3.2.1] "headers and footers are removed using DBSCAN clustering algorithm... which improves accuracy"
  - [table 1] PIER-QA with preprocessing achieves 0.8837 similarity vs baseline 0.8647 (without preprocessing)
  - [corpus] No direct corpus comparison for DBSCAN-based header removal found
- Break condition: Documents where headers contain page-specific information critical to queries.

### Mechanism 3
- Claim: Fine-tuning an LLM on domain-specific RAG-style QA pairs improves its ability to identify relevant retrieved context and ignore noise.
- Mechanism: GPT-4 generates ~2000 QA pairs from document chunks. Training contexts mix 5 relevant chunks with 5 random chunks from other documents, teaching the model to assess relevance during inference when 10 chunks are retrieved.
- Core assumption: Synthetic QA generation captures domain patterns; mixing relevant/irrelevant chunks during training transfers to real retrieval scenarios.
- Evidence anchors:
  - [section 3.2.3] "This also helps enhance robustness of the model on assessing relevance of the retrieved context"
  - [table 2] RAG-Llama3-70B achieves 0.8771 similarity vs 0.8156 for base Llama3-70B-Instruct
  - [corpus] Related work (Gupta et al. 2024) shows RAG+fine-tuning combination superior for domain-specific knowledge
- Break condition: Query types or document domains not represented in synthetic training data.

## Foundational Learning

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: Core architecture pattern; understanding why retrieval helps generation is essential.
  - Quick check question: Can you explain why retrieving chunks before LLM generation reduces hallucinations compared to pure generation?

- Concept: **DBSCAN clustering**
  - Why needed here: Used for header/footer detection; requires understanding density-based clustering and epsilon/minPts parameters.
  - Quick check question: Given a set of 2D points, how would you identify clusters vs noise points using DBSCAN?

- Concept: **LoRA (Low-Rank Adaptation) fine-tuning**
  - Why needed here: Efficient fine-tuning method for Llama3-70B; understanding rank, alpha, and adapter mechanics is required.
  - Quick check question: Why does LoRA reduce memory requirements compared to full fine-tuning, and what does the rank parameter control?

## Architecture Onboarding

- Component map:
  Preprocessing -> DBSCAN (header removal) -> Marker (markdown + image extraction) -> LLaVA (captions) + table compression -> GTE-large embeddings -> Elasticsearch + RAPTOR semantic tree -> Query embedding -> Top-10 retrieval -> RAG-Llama3-70B -> Image/table recovery

- Critical path: Preprocessing quality (especially image captions) -> embedding quality -> retrieval relevance -> LLM answer generation. The image captioning step is the bottleneck for multimodal queries.

- Design tradeoffs:
  - Caption quality vs speed: LLaVA provides reasonable captions but may miss details; multimodal LLMs (per VDocRAG corpus) could read visuals directly but add complexity
  - Chunk size (1000 chars): Balances context coherence with retrieval granularity
  - Always retrieve policy: Adds latency even for simple queries; no adaptive retrieval threshold

- Failure signatures:
  - Low image/table accuracy (48-66%) indicates caption-based retrieval struggles with specific visual queries
  - DBSCAN misidentifies content as headers when patterns vary across long documents
  - Complex table markdown conversion produces format errors or information loss

- First 3 experiments:
  1. **Ablation on header removal**: Run with/without DBSCAN preprocessing on 20 documents to isolate impact on retrieval noise and answer quality.
  2. **Caption quality audit**: Manually evaluate 50 image captions for accuracy; identify patterns where LLaVA fails (e.g., diagrams, charts with text).
  3. **Retrieval mixing validation**: Test RAG-Llama3-70B vs base Llama3 with varying retrieval noise (0-10 irrelevant chunks) to confirm robustness from mixed training.

## Open Questions the Paper Calls Out
1. How can conditional retrieval logic be integrated to bypass unnecessary database access for queries that do not require external context?
2. How can header/footer removal and table extraction be refined to prevent information loss in highly variable or complex document layouts?
3. Does the reliance on intermediate text representations (LLaVA captions and dictionary compression) limit the achievable accuracy for non-textual retrieval compared to native multimodal embeddings?

## Limitations
- Text-based caption generation for visual elements inherently limits accuracy for complex diagrams and quantitative data extraction
- DBSCAN-based header removal assumes consistent spatial patterns that may fail for documents with variable layouts
- Synthetic training data generation may not fully capture real-world query diversity
- Reported similarity metric improvements (0.8837 vs 0.8647) are modest and practical significance is unclear

## Confidence

- **High Confidence**: The preprocessing pipeline (header removal, markdown conversion, chunking) is technically sound and baseline comparisons are valid. Architecture design choices are well-justified.
- **Medium Confidence**: Claims about caption-based retrieval effectiveness are supported by quantitative results but limited by text-only retrieval constraints for visual content. Fine-tuning improvements are demonstrated but synthetic training data may not generalize well.
- **Low Confidence**: Practical impact of reported similarity metric improvements on actual user experience is not validated. Break conditions are theoretical and not empirically tested.

## Next Checks

1. **Header removal ablation study**: Systematically compare PIER-QA performance with and without DBSCAN preprocessing across 20+ diverse documents to quantify the actual impact of header/footer removal on retrieval noise and answer quality.

2. **Caption quality assessment**: Conduct human evaluation of 50+ image captions to identify systematic failure patterns in LLaVA's captioning, particularly for diagrams, charts with embedded text, and complex visual relationships.

3. **Fine-tuning robustness validation**: Test RAG-Llama3-70B against base Llama3 with controlled retrieval noise levels (0-10 irrelevant chunks) to empirically confirm the claimed robustness improvement from mixed-relevant training data.