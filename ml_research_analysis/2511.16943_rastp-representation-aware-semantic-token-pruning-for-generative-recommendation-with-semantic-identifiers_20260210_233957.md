---
ver: rpa2
title: 'RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation
  with Semantic Identifiers'
arxiv_id: '2511.16943'
source_url: https://arxiv.org/abs/2511.16943
tags:
- rastp
- semantic
- recommendation
- token
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RASTP addresses the inefficiency of SID-based generative recommendation
  by pruning less informative semantic tokens. It evaluates token importance using
  a combination of representation magnitude and attention centrality, then dynamically
  retains only the most salient tokens during training.
---

# RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers

## Quick Facts
- arXiv ID: 2511.16943
- Source URL: https://arxiv.org/abs/2511.16943
- Authors: Tianyu Zhan; Kairui Fu; Zheqi Lv; Shengyu Zhang
- Reference count: 14
- Reduces training time by 26.7% while maintaining or slightly improving recommendation performance

## Executive Summary
RASTP introduces a representation-aware semantic token pruning method for generative recommendation systems that use semantic identifiers (SIDs). The core insight is that not all tokens in a sequence contribute equally to recommendation quality, and pruning less informative tokens can significantly improve training efficiency without sacrificing performance. The method evaluates token importance through a combination of representation magnitude and attention centrality metrics, dynamically retaining only the most salient tokens during training. This approach addresses the computational inefficiency of SID-based generative recommendation, which can be slow and memory-intensive due to processing all tokens equally.

## Method Summary
RASTP evaluates token importance using a dual-metric approach combining representation magnitude (capturing the absolute value of token representations) and attention centrality (measuring how much attention a token receives from others). Tokens are ranked based on these scores, and only the top-k most important tokens are retained during training while maintaining the full vocabulary for inference. The pruning ratio is dynamically adjusted based on the model's learning stage, allowing for more aggressive pruning in early training phases when the model is still learning general patterns. This selective retention strategy reduces computational load during training while preserving the model's ability to generate accurate recommendations at inference time.

## Key Results
- Achieves 26.7% reduction in training time across three Amazon datasets
- Maintains or slightly improves recommendation performance compared to baseline models
- Demonstrates effectiveness on datasets with up to 16K items in the vocabulary

## Why This Works (Mechanism)
RASTP works by recognizing that generative recommendation models using SIDs often process many tokens that contribute little to final recommendations. By identifying and pruning these less informative tokens during training, the model can focus computational resources on the most salient information. The dual-metric importance scoring ensures that tokens are evaluated both by their intrinsic representation strength and their role in the attention mechanism, capturing different aspects of token utility. The dynamic adjustment of pruning ratios allows the model to start with broader coverage and gradually focus on the most critical tokens as training progresses.

## Foundational Learning

**Semantic Identifiers (SIDs):** These are token-based representations where each item is encoded as a unique token, enabling natural language generation frameworks for recommendation tasks. Why needed: They allow leveraging powerful pre-trained language models for recommendation, but can be computationally expensive. Quick check: Verify that the SID vocabulary size matches the number of unique items in the dataset.

**Attention Centrality:** A metric measuring how much attention a token receives from other tokens in the sequence. Why needed: Helps identify tokens that serve as information hubs in the sequence, potentially more important for recommendation quality. Quick check: Confirm that attention weights sum to 1 across all tokens in each attention head.

**Representation Magnitude:** The absolute value of token representations in the embedding space. Why needed: Provides a simple measure of token activation strength, indicating potentially important information carriers. Quick check: Verify that representation magnitudes are normalized or scaled appropriately to prevent dominance by large values.

## Architecture Onboarding

**Component Map:** Input Sequence -> Token Importance Scoring -> Pruning (retain top-k) -> Generator Model -> Recommendations

**Critical Path:** The most important components are the token importance scoring module and the pruning mechanism, as they directly determine which tokens reach the generator model. The generator model itself is standard for SID-based approaches.

**Design Tradeoffs:** The main tradeoff is between pruning aggressiveness and recommendation quality. More aggressive pruning yields greater efficiency gains but risks losing important information. The dynamic adjustment mechanism attempts to balance this tradeoff across training stages.

**Failure Signatures:** If pruning is too aggressive, recommendation quality may degrade noticeably. If the importance scoring metrics are poorly calibrated, important tokens might be pruned while unimportant ones are retained. During inference, if the pruning mechanism fails, the model may generate incomplete or incoherent recommendations.

**First 3 Experiments:**
1. Run baseline SID model without pruning to establish performance and training time baselines
2. Apply RASTP with conservative pruning ratios to verify that quality is maintained while measuring efficiency gains
3. Conduct ablation studies comparing representation magnitude-only versus attention centrality-only pruning to validate the dual-metric approach

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses exclusively on training efficiency without addressing inference-time implications or memory usage during deployment
- Experimental evaluation limited to Amazon datasets with relatively small vocabularies (up to 16K items)
- Claims of "maintaining or slightly improving" performance show only marginal improvements in most cases

## Confidence

**High:** Core pruning methodology and implementation appear technically sound with clear description
**Medium:** Claimed efficiency gains of 26.7% training time reduction, though inference benefits not explored
**Low:** Generalization claims limited by narrow dataset diversity and small vocabulary sizes

## Next Checks

1. Evaluate RASTP on datasets with vocabulary sizes exceeding 100K items to assess scalability for industrial applications
2. Measure inference-time performance including latency and memory usage, not just training efficiency
3. Conduct ablation studies isolating the contributions of representation magnitude versus attention centrality components in the token importance scoring