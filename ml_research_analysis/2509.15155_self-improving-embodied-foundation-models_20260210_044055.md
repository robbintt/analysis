---
ver: rpa2
title: Self-Improving Embodied Foundation Models
arxiv_id: '2509.15155'
source_url: https://arxiv.org/abs/2509.15155
tags:
- stage
- self-improvement
- learning
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-stage post-training framework for Embodied
  Foundation Models (EFMs) that combines supervised fine-tuning with online reinforcement
  learning. The method uses steps-to-go predictions to create data-driven rewards
  and success detectors, enabling robots to autonomously improve policies without
  manual reward engineering.
---

# Self-Improving Embodied Foundation Models

## Quick Facts
- **arXiv ID**: 2509.15155
- **Source URL**: https://arxiv.org/abs/2509.15155
- **Reference count**: 27
- **Primary result**: Two-stage post-training framework combining supervised fine-tuning with online reinforcement learning achieves up to 1.5x performance gains over behavioral cloning alone with only 2% additional training episodes.

## Executive Summary
This work introduces a two-stage post-training framework for Embodied Foundation Models (EFMs) that combines supervised fine-tuning with online reinforcement learning. The method uses steps-to-go predictions to create data-driven rewards and success detectors, enabling robots to autonomously improve policies without manual reward engineering. Experiments on real and simulated LanguageTable and Aloha robot embodiments show that this approach significantly outperforms behavioral cloning alone, achieving up to 1.5x performance gains with only 2% additional training episodes. Critically, the combination of web-scale pretraining and online self-improvement enables robots to acquire novel skills that generalize beyond the original imitation datasets.

## Method Summary
The approach consists of two stages: Stage 1 trains a vision-language model (PaLI-3B) on imitation data using behavioral cloning and steps-to-go prediction heads, creating a reward model and success detector. Stage 2 initializes a policy from Stage 1 and improves it through on-policy REINFORCE using rewards derived from temporal differences in steps-to-go predictions. The steps-to-go predictor provides a dense, shaped reward signal without manual engineering, while the frozen reward model serves as an implicit baseline to reduce variance. A single human operator can supervise multiple robots during autonomous practice, and the method demonstrates robustness across random seeds and settings.

## Key Results
- **Performance improvement**: Stage 2 Self-Improvement improves success rates by up to 1.5x compared to behavioral cloning alone (e.g., 45%→75% on simulated LanguageTable)
- **Sample efficiency**: Only 2% additional training episodes are needed beyond the original imitation dataset
- **Generalization**: Robots acquire novel manipulation skills (e.g., BananaTable) without training data for the specific objects
- **Scalability**: A single human operator can supervise multiple robots during autonomous practice

## Why This Works (Mechanism)

### Mechanism 1: Steps-to-Go as Implicit Value Function and Shaped Reward
If the model accurately predicts steps remaining to goal completion, then the temporal difference of these predictions functions as a dense, shaped reward signal without manual engineering. During Stage 1, the EFM learns to predict steps-to-go via supervised learning on imitation trajectories. In Stage 2, the reward becomes r(ot, at, ot+1, g) = d(ot, g) - d(ot+1, g). Mathematically, this decomposes into (1-γ)·Vμ(ot+1) (core reward) plus a shaping term that implicitly regularizes policies toward states where the demonstrator policy μ performs well. The core assumption is that the imitation dataset contains sufficiently diverse trajectories for the model to learn meaningful distance estimates, including recovery behaviors from mistakes.

### Mechanism 2: Web-Scale Pretraining Transfer for Generalization
Multimodal foundation model pretraining provides semantic and visual priors that enable reward functions and policies to generalize to novel objects and tasks beyond the imitation dataset. The PaLI-3B backbone, pretrained on web-scale vision-language data, encodes generalizable object recognition and spatial reasoning. When fine-tuned on limited robot data, these representations transfer, allowing the steps-to-go predictor to emit reasonable estimates even for unseen scenarios (e.g., BananaTable with no training bananas). The core assumption is that web-scale pretraining captures sufficient physical and spatial reasoning to support out-of-distribution robotic tasks.

### Mechanism 3: On-Policy REINFORCE with Implicit Baseline
The steps-to-go-derived reward structure provides a natural baseline that reduces variance in REINFORCE policy gradient estimates, enabling stable training without learning a separate value function. The Monte Carlo return Rt simplifies to (1-γ)·Σγ^(i-t)·Vμ(oi+1) - Vμ(ot) where -Vμ(ot) serves as a baseline. Since Vμ is fixed from Stage 1 (frozen checkpoint), this avoids bootstrapping and off-policy instability—two vertices of the "deadly triad." The core assumption is that the Stage 1 steps-to-go predictions remain sufficiently accurate during Stage 2 policy improvement to provide meaningful baselines.

## Foundational Learning

- **Concept: Goal-Conditioned Behavioral Cloning**
  - **Why needed here**: Stage 1 requires training policies and steps-to-go predictors on imitation data where goals may vary across episodes. Understanding how to condition on goals (via text instructions) is essential.
  - **Quick check question**: Can you explain how goal-conditioning differs from single-task imitation learning, and why hindsight relabeling might help?

- **Concept: Reward Shaping and Potential-Based Rewards**
  - **Why needed here**: The paper's core contribution is deriving a shaped reward from steps-to-go. Understanding Ng et al. (1999) on policy invariance under potential-based transformations clarifies why this reward doesn't change optimal policies.
  - **Quick check question**: Given a potential function Φ(s), how does the shaped reward r(s,a,s') + γΦ(s') - Φ(s) preserve optimal policy, and what role does d(o,g) play as a potential?

- **Concept: Foundation Model Fine-Tuning Paradigms**
  - **Why needed here**: The method builds on RT-2-style discretization of continuous actions into token space. Understanding how VLMs can produce structured outputs (actions, counts) via supervised fine-tuning is prerequisite.
  - **Quick check question**: Why does discretizing continuous robot actions into tokens enable leveraging pretrained VLMs, and what tokenization choices might affect performance?

## Architecture Onboarding

- **Component map**: Imitation dataset -> Stage 1 Trainer -> EFM (BC + steps-to-go) -> Frozen Reward/Success Model -> Stage 2 Policy -> Actors (robots) -> Replay Buffer -> Central Coordinator
- **Critical path**: Prepare imitation dataset with trajectory-level annotations -> Train Stage 1 to convergence on both BC and steps-to-go objectives -> Select checkpoints for reward model and Stage 2 init -> Deploy reward model and success detector as inference servers -> Run Stage 2 Self-Improvement loop
- **Design tradeoffs**: On-policy vs off-policy (REINFORCE for stability vs. sample efficiency); frozen vs updated reward model (prevents reward hacking vs. potential staleness); shared vs separate checkpoints (policy may need longer training); local vs remote inference (latency constraints)
- **Failure signatures**: Reward hacking if steps-to-go predictor is inaccurate; distribution shift if Stage 2 policy explores OOD states; over-optimization past peak performance; success detector false positives with high threshold
- **First 3 experiments**: 1) Pointmass validation in 2D domain with ground-truth rewards; 2) Simulated LanguageTable ablation training on 10% dataset and comparing against 45%→75% improvement; 3) Real-world sanity check deploying Stage 1 BC policy on single robot station

## Open Questions the Paper Calls Out

1. **Stopping criteria and adaptive regularizers**: What specific stopping criteria or adaptive regularizers can prevent policy degradation from over-optimizing the shaped reward during Self-Improvement? Section 6 notes that pushing the process "beyond its performance peak can degrade success rates, suggesting that better stopping criteria or adaptive regularisers are required."

2. **Out-of-distribution failure states**: How can the steps-to-go estimator be adapted to handle failure states that fall outside the support of the imitation learning dataset? Section 6 highlights the challenge of settings where "failure states fall outside the support of the datasets, and the absence of recovery trajectories for those out-of-distribution (OOD) states."

3. **Off-policy RL scalability**: Can off-policy reinforcement learning algorithms scale to this setting to further reduce robot-hour requirements compared to the current on-policy approach? Section 6 states, "We leave the investigation of alternative RL algorithms, including off-policy methods, to future work."

## Limitations
- Critical dependency on accurate steps-to-go predictions for reward shaping remains unvalidated in failure scenarios absent from demonstrations
- Generalization mechanism relies heavily on PaLI's web-scale pretraining, but specific physical reasoning capabilities for manipulation are not empirically verified
- RL stability claims depend on frozen reward model assumption, but no ablation studies test reward model updates vs. freezing

## Confidence
- **High**: Overall two-stage framework design and reported performance improvements on LanguageTable and Aloha tasks
- **Medium**: Mechanism claims about steps-to-go as shaped reward and policy invariance preservation
- **Low**: Generalization claims for novel objects (BananaTable) and specific contributions of web-scale pretraining vs. task-specific learning

## Next Checks
1. **Failure Mode Analysis**: Systematically test steps-to-go reward shaping on out-of-distribution failure scenarios to quantify reward signal reliability when demonstrator trajectories are missing
2. **Pretraining Ablation**: Train from scratch on the same robot datasets (no web-scale pretraining) to measure the specific contribution of PaLI's visual-linguistic priors to generalization
3. **Reward Model Stability**: Run Stage 2 with an updated (non-frozen) reward model to test whether reward hacking emerges and compare stability vs. the frozen baseline approach