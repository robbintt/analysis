---
ver: rpa2
title: To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping
  Cameras to Textured Semantic 3D Building Models
arxiv_id: '2505.17973'
source_url: https://arxiv.org/abs/2505.17973
tags:
- matching
- methods
- images
- image
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares classical handcrafted and learnable feature-matching
  methods for visual localization using textured CityGML LoD2 building models and
  mobile mapping camera images. The authors systematically evaluate methods on standard
  benchmarks (HPatches, MegaDepth-1500) and custom datasets combining facade textures
  with terrestrial and drone imagery.
---

# To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models

## Quick Facts
- **arXiv ID:** 2505.17973
- **Source URL:** https://arxiv.org/abs/2505.17973
- **Reference count:** 13
- **Primary result:** Learnable methods (SuperPoint+SuperGlue, SuperPoint+LightGlue) significantly outperform classical approaches (SIFT+FLANN, ORB+NN) on challenging custom datasets, achieving 0-12 RANSAC inliers and 0-0.16 AUC versus zero for classical methods.

## Executive Summary
This study systematically compares classical handcrafted and learnable feature-matching methods for visual localization using textured CityGML LoD2 building models and mobile mapping camera images. The authors evaluate methods on standard benchmarks (HPatches, MegaDepth-1500) and custom datasets combining facade textures with terrestrial and drone imagery. They assess performance through absolute pose estimation accuracy using PnP algorithms with RANSAC, comparing against geo-referenced trajectory ground truth. Learnable methods significantly outperform classical approaches on challenging custom datasets, demonstrating superior robustness and accuracy for model-based visual localization, though limitations include domain shift and model inaccuracies affecting coordinate precision.

## Method Summary
The study evaluates classical (SIFT+FLANN, ORB+NN, AKAZE+NN) and learnable (SuperPoint+SuperGlue/LightGlue, DISK+LightGlue, LoFTR) feature matching methods for visual localization. The pipeline extracts features from CityGML textured facades and mobile camera images, matches them using various algorithms, then estimates absolute 6-DoF poses via PnP with RANSAC outlier removal. The method converts 2D texture pixel coordinates to 3D world coordinates using CityGML parametric mappings before pose estimation. Performance is measured on standard benchmarks and a custom TUM2TWIN dataset with 622 car-camera pairs and 18 UAV pairs, using metrics including mPrec, mInl, and AUC of pose error.

## Key Results
- Learnable methods achieve 0-12 RANSAC inliers on custom datasets versus zero for classical methods
- SuperPoint+LightGlue performs best on challenging custom datasets with difficult lighting and domain shift
- Classical methods fail completely on custom datasets due to domain shift between perspective camera views and orthorectified facade textures
- CityGML LoD2 model inaccuracies (particularly for non-planar surfaces) limit final pose precision to decimeter-level errors

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Feature Matching
Learnable matchers outperform classical gradient-based methods when aligning mobile images with orthorectified facade textures due to learned robustness to geometric distortion. Transformers in SuperGlue and LightGlue use self- and cross-attention to reason about global scene context, allowing them to match features across larger viewpoint and domain shifts than local descriptor matching. The pre-trained weights (e.g., MegaDepth) generalize sufficiently to CityGML textures despite the "domain shift from projective geometry to orthorectified texture images."

### Mechanism 2: Texture-to-World Coordinate Unprojection
Accurate pose estimation relies on the precise conversion of 2D texture pixel coordinates into 3D world coordinates via the CityGML parametric mapping. The system inversely maps matched keypoints from the texture image's $st$-coordinates to the 3D face geometry using the LinearRing coordinates. This creates the 3D-2D correspondences required for the PnP solver, assuming the building facade is planar and the texture is accurately mapped to the geometric face.

### Mechanism 3: Geometric Verification via PnP-RANSAC
Relative feature matching success is validated through the reprojection error of the absolute pose. RANSAC filters outlier matches by iteratively solving the Perspective-n-Point (PnP) problem and checking reprojection against a pixel threshold (set to 10px in the study). The RANSAC threshold must be loose enough to accommodate model inaccuracies but tight enough to reject false positives.

## Foundational Learning

- **Concept: Feature Detection vs. Description**
  - Why needed here: You must distinguish between finding a point (Detection, e.g., SuperPoint) and describing it for matching (Description, e.g., DISK). Classical methods couple these; learned methods often separate or integrate them via attention.
  - Quick check question: Can you explain why a "dense" matcher like LoFTR might fail differently than a "sparse" detector like SuperPoint on a texture-less facade?

- **Concept: Coordinate Systems in Photogrammetry**
  - Why needed here: The core contribution relies on transforming Pixel $\to$ Texture ST $\to$ Local Face $\to$ World UTM coordinates. Failure to understand these transforms prevents pose estimation.
  - Quick check question: Given a pixel $(u, v)$ and a texture width $w$, how would you derive the $s$-coordinate (Eq. 1)?

- **Concept: Generalization Gap (Domain Shift)**
  - Why needed here: The authors note that models trained on MegaDepth (perspective internet photos) are applied to orthorectified CityGML textures. Understanding domain shift explains why performance might degrade compared to benchmarks.
  - Quick check question: Why might an orthorectified texture (parallel projection) confuse a feature descriptor trained primarily on perspective images?

## Architecture Onboarding

- **Component map:** CityGML + Mobile Images -> Feature Extractors (SIFT/SuperPoint) -> Matchers (FLANN/SuperGlue) -> Coordinate Transformer (2D pixel -> 3D UTM) -> PnP + RANSAC -> Absolute 6-DoF Pose

- **Critical path:** The accuracy of the coordinate transformation (Section 3.2.1) is the bottleneck. If the mapping from texture pixel to world coordinate is flawed (due to LoD2 simplification), even perfect feature matches will yield high pose errors.

- **Design tradeoffs:**
  - LoD2 vs. Higher Detail: LoD2 is computationally lighter but geometrically less accurate (planar assumption), limiting final coordinate precision.
  - Learned vs. Classical: Learned methods provide robustness (12 inliers vs 0) but at a higher computational cost (3x-10x slower inference).

- **Failure signatures:**
  - 0 RANSAC Inliers: Indicates a failure in the matching stage (likely domain shift or repetitive textures) or insufficient overlap.
  - High Reprojection Error (>10px) with "Correct" Pose: Indicates inconsistencies between the 3D model geometry and ground truth.

- **First 3 experiments:**
  1. Run SIFT+FLANN vs. SuperPoint+SuperGlue on the provided HPatches subset using the provided code repo to verify the pipeline setup.
  2. Manually pick corresponding points on a texture and a camera image, project them using Eq. 1-5, and verify the distance against known geo-data to isolate geometric errors from matching errors.
  3. Vary the RANSAC $t$ parameter (currently fixed at 10) to observe the trade-off between the number of inliers and pose accuracy on the custom TUM dataset.

## Open Questions the Paper Calls Out

- Can matched world coordinates derived from building facades be successfully utilized as landmarks for SLAM-based positioning in mobile mapping applications? The Conclusion states future applications could investigate whether matched world coordinates from facades can successfully be used as landmarks for SLAM-based positioning of the car or the UAV.

- How does the performance of direct texture-image-to-image matching compare to rendered-model-image-to-image approaches for visual localization? The Conclusion suggests comparing the visual localization capabilities of the direct texture-image-to-image approach to other methods, e.g., rendered-model-image-to-image.

- Does incorporating textures from multiple building faces simultaneously mitigate the geometric instability caused by single-plane PnP estimation? The Discussion notes that keypoints are restricted to a planar scene, which might be unfavourable for the PnP algorithm, and suggests including textures of several faces as a potential solution.

## Limitations

- Domain shift between perspective camera views and orthorectified facade textures significantly impacts classical method performance
- CityGML LoD2 model inaccuracies (particularly for non-planar surfaces) limit final pose precision to decimeter-level errors
- Fixed RANSAC threshold of 10px may be suboptimal for varying texture resolutions and model accuracies

## Confidence

- **High Confidence:** Classical methods (SIFT+FLANN, ORB+NN) failing completely on custom datasets; learnable methods' superiority in matching metrics (mPrec, mInl, AUC)
- **Medium Confidence:** Exact quantitative comparison on TUM2TWIN dataset (requires precise coordinate transformation implementation and dataset-specific preprocessing)
- **Low Confidence:** Generalization of findings to other semantic 3D model formats beyond CityGML LoD2, and to different urban environments with varying facade complexity

## Next Checks

1. **Coordinate Transformation Verification:** Manually validate the texture-to-world coordinate conversion for 5 sample matches using ground truth building measurements to isolate geometric errors from matching errors.

2. **Ablation Study on RANSAC Threshold:** Systematically vary the reprojection error threshold (3px â†’ 30px) on the custom dataset to determine optimal parameter settings and understand the trade-off between inlier count and pose accuracy.

3. **Domain Adaptation Experiment:** Fine-tune a learnable matcher (SuperGlue) on a small subset of the actual CityGML textures to quantify performance improvements over the pre-trained MegaDepth weights, measuring the domain shift impact.