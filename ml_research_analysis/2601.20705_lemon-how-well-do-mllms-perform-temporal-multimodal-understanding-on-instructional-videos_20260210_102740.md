---
ver: rpa2
title: 'LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional
  Videos?'
arxiv_id: '2601.20705'
source_url: https://arxiv.org/abs/2601.20705
tags:
- video
- arxiv
- mllms
- reve
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEMON, a new benchmark for evaluating multimodal
  large language models (MLLMs) on instructional videos. LEMON includes 2,277 video
  segments with synchronized video, audio, and subtitles, and 4,181 QA pairs across
  29 STEM courses.
---

# LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?

## Quick Facts
- arXiv ID: 2601.20705
- Source URL: https://arxiv.org/abs/2601.20705
- Authors: Zhuang Yu; Lei Shen; Jing Zhao; Shiliang Sun
- Reference count: 40
- Models struggle with temporal reasoning, audio understanding, and fine-grained cross-lingual generation despite strong perception abilities

## Executive Summary
This paper introduces LEMON, a new benchmark for evaluating multimodal large language models (MLLMs) on instructional videos. LEMON includes 2,277 video segments with synchronized video, audio, and subtitles, and 4,181 QA pairs across 29 STEM courses. The benchmark assesses six tasks: Streaming Perception, OCR-Based Reasoning, Audio Comprehension, Temporal Awareness, Instructional Prediction, and Advanced Expression. Experiments on 21 models reveal that while proprietary MLLMs like GPT-5 and Gemini 2.5 Pro outperform open-source models in multimodal comprehension, all models struggle with temporal reasoning, audio understanding, and fine-grained cross-lingual generation. Models generally benefit from subtitles, but audio input offers limited gains. Current MLLMs excel at low-level perception but lag in complex reasoning and generation in long-form, knowledge-intensive instructional settings.

## Method Summary
LEMON evaluates MLLMs on 2,277 video segments from STEM lectures with synchronized video, audio, and subtitles. The benchmark includes 4,181 QA pairs across six tasks and 12 subtasks, using zero-shot evaluation. Models process 64 sampled frames (non-streaming) or 1 FPS (streaming), with frames resized to 224×224 for open-source or 512→base64 for proprietary models. Audio is extracted at 16kHz with normalization, and subtitles are generated via Whisper-v3. Evaluation uses accuracy for multiple-choice questions and GPT-based holistic scoring plus BLEU/ROUGE-L/BERTScore/COMET22 for open-ended tasks. The dataset spans five STEM domains with an average video length of 196.1 seconds.

## Key Results
- All models struggle with temporal reasoning tasks, with Temporal Awareness showing the lowest performance (GPT-4o: 29.03%, Gemini 2.5 Pro: 22.58%)
- Subtitles provide more consistent performance gains than audio input across models (+19.14 for IXC2.5-OL vs. unpredictable audio effects)
- Proprietary models (GPT-5, Gemini 2.5 Pro) significantly outperform open-source models in multimodal comprehension
- Cross-lingual generation fails for Asian languages (ZH/JA/KO BLEU scores: 5-12%) compared to European languages (43-62%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models that effectively integrate synchronized multimodal cues outperform those relying on single modalities, with subtitles providing the most consistent performance gains.
- Mechanism: The integration operates through temporal grounding where visual scenes, spoken explanations, and text are aligned, enabling cross-modal disambiguation when one modality is incomplete or unclear.
- Core assumption: Synchronized multimodal cues provide complementary information that can resolve ambiguities present in individual modalities.
- Evidence anchors: [abstract] synchronized video, audio, and subtitles enables temporal multimodal understanding; [section 4.3, Table 3-4] subtitles consistently provide stable gains across models; [corpus] MUSEG confirms timestamp-aware multi-segment grounding improves temporal understanding.

### Mechanism 2
- Claim: Current MLLMs rely heavily on prior knowledge rather than genuine perceptual grounding, especially for instructional content with overlapping domain knowledge.
- Mechanism: Pre-trained linguistic and conceptual knowledge enables models to answer questions without processing actual video content, inflating perceived multimodal capabilities.
- Core assumption: High-quality instructional QA pairs should require grounding in actual video content rather than be answerable from general pre-training.
- Evidence anchors: [abstract] "Models often rely on prior knowledge or subtitles, failing to fully integrate multimodal cues"; [section J.2, Figure 11] QA-only evaluation shows accuracy drops when models lack video/audio/subtitle input; [corpus] arXiv:2501.10674 confirms MLLMs fail at visual temporal understanding.

### Mechanism 3
- Claim: Temporal causal reasoning requires modeling long-range dependencies across sequential segments, which current architectures fundamentally struggle with.
- Mechanism: Coherent temporal representation requires maintaining sequence memory where earlier events causally influence later ones, enabling chronological ordering and future-step prediction.
- Core assumption: Instructional videos contain explicit pedagogical structure with temporally continuous, causally linked content progression.
- Evidence anchors: [abstract] "all struggle with temporal reasoning" across even state-of-the-art models; [section 4.2] Temporal Awareness shows lowest performance across models; models "struggle to capture long-range dependencies, track evolving multimodal signals, and infer the causal structure."

## Foundational Learning

- **Cross-Modal Temporal Alignment**
  - Why needed here: LEMON evaluates whether models can synchronize visual, auditory, and textual streams to build coherent representations of instructional content.
  - Quick check question: How would you detect if a model is hallucinating content not present in any modality?

- **Hierarchical Cognitive Processing**
  - Why needed here: The benchmark organizes tasks from Perception→Reasoning→Generation, requiring progressively deeper integration of multimodal cues.
  - Quick check question: What distinguishes Local Detail Tracking from Domain Conceptual Comprehension?

- **Error Propagation in Multi-Turn Dialogue**
  - Why needed here: LEMON's sequential questioning design exposes how early mistakes compound in later answers.
  - Quick check question: In multi-turn evaluation, why might resetting context between questions inflate performance metrics?

## Architecture Onboarding

- **Component map**: Video encoder (224×224 or 512→base64) → Audio processor (16kHz resampling) → Subtitle aligner (Whisper-v3) → Multimodal fusion (cross-modal attention) → Temporal memory → Response generation
- **Critical path**: 1. Video preprocessing → frame sampling (1fps streaming / 64 frames non-streaming) → 2. Audio extraction → normalized audio stream → 3. Subtitle generation → Whisper-v3 synchronized text → 4. Multimodal encoding → unified token representation → 5. Temporal fusion → cross-modal attention (bottleneck for current models) → 6. Response generation → LLM-assisted answer extraction for evaluation
- **Design tradeoffs**: Dense vs. sparse frame sampling (dense improves long-video comprehension but introduces redundancy); Audio vs. subtitle reliance (subtitles more stable +19.14 for IXC2.5-OL; audio unpredictable); Online vs. offline inference (minimal gap suggests true streaming not achieved)
- **Failure signatures**: Strong visual perception but limited audio comprehension; Temporal/causal reasoning breakdown on long sequences; Coarse-grained descriptions missing fine-grained instructional details; Hallucinations inconsistent with input modalities; Cross-lingual generation failure for Asian languages
- **First 3 experiments**: 1. Run ablation study comparing video-only, video+subtitles, video+audio, and full multimodal inputs across all six tasks; 2. Test frame sampling sensitivity (16/32/64/128 frames) across video duration buckets (60s/120s/180s/240s+); 3. Execute QA-only evaluation (no modalities) to quantify prior knowledge contamination

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can MLLM architectures be modified to achieve genuine real-time streaming reasoning capabilities?
- **Basis in paper**: [inferred] Section J.1 notes the minimal performance gap between online and offline modes suggests current models "have yet to achieve genuine real-time understanding."
- **Why unresolved**: Current "online" inference often fails to leverage temporal causality effectively, performing similarly to offline models that access the full video context at once.
- **What evidence would resolve it**: Architectural changes that result in significant accuracy improvements in online modes over offline modes specifically for temporal tasks.

### Open Question 2
- **Question**: What causes performance degradation in some Omni-MLLMs when raw audio is added to visual inputs?
- **Basis in paper**: [inferred] Table 3 shows models like Ola and M4 exhibit negative performance shifts when audio is included, indicating difficulties in semantic extraction from raw signals.
- **Why unresolved**: The paper observes that audio input offers "far less predictable gains" compared to subtitles, but does not identify the specific failure modes in multimodal fusion.
- **What evidence would resolve it**: Analysis of attention maps or fusion layers showing where audio signals are rejected or misaligned with visual tokens during complex reasoning.

### Open Question 3
- **Question**: How can MLLMs be forced to ground their responses in specific instructional video content rather than relying on pre-trained prior knowledge?
- **Basis in paper**: [inferred] Figure 11 shows models achieve non-trivial scores using only the QA pair (no video), and the paper notes models often rely on "prior knowledge or subtitles."
- **Why unresolved**: The high density of domain knowledge in pre-training data leads to hallucinations or answers based on general knowledge rather than the specific video segment.
- **What evidence would resolve it**: New training objectives or prompting strategies that significantly reduce accuracy when the video is masked but maintain it when the video is present.

## Limitations

- The specific YouTube video IDs and curated QA pairs are not publicly linked, making faithful reproduction difficult without potential distribution shifts
- The study focuses exclusively on STEM lecture content, limiting generalizability to other instructional domains or video types
- The evaluation relies heavily on GPT-based scoring for open-ended tasks, which introduces potential bias and may not align with human judgment

## Confidence

- **High Confidence**: The observation that all models struggle with temporal reasoning across tasks is well-supported by consistent low performance metrics across multiple models
- **Medium Confidence**: Claims about modality integration benefits (subtitles vs. audio) are reasonably supported by ablation studies, though individual model behaviors show variability
- **Medium Confidence**: The assertion that models rely on prior knowledge rather than perceptual grounding is supported by the QA-only evaluation, though the distinction remains somewhat ambiguous

## Next Checks

1. **Dataset Reconstruction Validation**: Reconstruct the dataset using the described methodology and compare model performance distributions against the reported results to assess reproducibility and potential distribution shifts

2. **Frame Sampling Sensitivity Analysis**: Conduct systematic experiments varying frame sampling density (16/32/64/128 frames) across different video duration buckets to establish the optimal tradeoff and determine if temporal reasoning limitations stem from insufficient temporal resolution

3. **Human Evaluation Benchmark**: Implement human evaluation for a subset of open-ended tasks (particularly Video Summarization and Knowledge Translation) to validate the GPT-based scoring methodology and quantify potential bias in the automated evaluation pipeline