---
ver: rpa2
title: 'Privacy Risks and Preservation Methods in Explainable Artificial Intelligence:
  A Scoping Review'
arxiv_id: '2505.02828'
source_url: https://arxiv.org/abs/2505.02828
tags:
- privacy
- explanations
- data
- learning
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This review provides a comprehensive analysis of privacy risks
  in Explainable Artificial Intelligence (XAI), identifying two main categories of
  threats: intentional attacks (membership inference, model inversion, model extraction)
  and unintentional leaks (training issues, explanation content). Through a scoping
  review of 57 studies, the authors categorized these risks and evaluated privacy
  preservation methods including differential privacy, anonymization, cryptography,
  and federated learning.'
---

# Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review

## Quick Facts
- arXiv ID: 2505.02828
- Source URL: https://arxiv.org/abs/2505.02828
- Reference count: 40
- Primary result: Comprehensive analysis of privacy risks in XAI identifying 57 studies that categorize threats into intentional attacks (membership inference, model inversion, model extraction) and unintentional leaks (training issues, explanation content)

## Executive Summary
This scoping review systematically examines privacy risks in Explainable Artificial Intelligence systems, revealing how transparency mechanisms can inadvertently expose sensitive training data. Through analysis of 57 studies, the authors identify two main categories of privacy threats: intentional attacks like membership inference and model extraction, and unintentional information leaks through explanation generation. The research demonstrates that while explanations enhance model interpretability, they simultaneously create new attack surfaces that can compromise data privacy. The study proposes ten characteristics for privacy-preserving XAI systems and evaluates various preservation methods including differential privacy, anonymization, cryptography, and federated learning, highlighting the critical need to balance privacy, explainability, and utility in AI system design.

## Method Summary
The review employed a PRISMA-ScR scoping methodology using Engineering Village databases (Compendex and Inspec) with a specific search string covering privacy and explainability terms. The 4-step process included identification (1,943 records), screening, eligibility assessment, and extraction, with studies filtered for 2019-2024 publication dates. Two researchers independently screened studies, resolving discrepancies through discussion, with final inclusion based on studies describing privacy risks or preservation methods in XAI contexts. The review identified 57 eligible studies, which were categorized into attack types and preservation methods through systematic analysis.

## Key Results
- Privacy risks in XAI fall into two categories: intentional attacks (membership inference, model inversion, model extraction) and unintentional leaks (training issues, explanation content)
- Explanation-based extraction attacks offer substantial advantages over traditional prediction-only approaches by facilitating model replication with reduced queries
- Differential Privacy applied to training or explanation generation acts as a formal barrier against privacy leakage by limiting sensitivity to individual data points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explanations inadvertently expose model's decision boundaries, accelerating model extraction attacks
- **Mechanism:** Explanation functions revealing gradients or counterfactuals provide geometric information about decision boundary curvature, enabling adversaries to reconstruct the model function with fewer queries
- **Core assumption:** Generated explanation is sufficiently faithful to reveal actionable geometric information
- **Evidence anchors:** Abstract mentions explanations expose personal data through model extraction; section 4.2.3 notes explanation-based extraction offers substantial advantages over prediction-only approaches

### Mechanism 2
- **Claim:** Explanation vectors exhibit distinct statistical distributions for training members versus non-members
- **Mechanism:** Models behave differently on training data due to memorization, creating detectable divergence in explanation behavior (e.g., higher variance in feature importance) that enables membership inference
- **Core assumption:** Model has overfitted or memorized specific training instances
- **Evidence anchors:** Section 4.2.1 discusses influence functions generating explanations in form of actual datapoints enabling certainty about membership; Liu et al. (2024) observed higher loss in confidence on perturbation of important features for members

### Mechanism 3
- **Claim:** Differential Privacy acts as formal barrier against privacy leakage
- **Mechanism:** Adding calibrated noise to gradients during training or to explanation logic bounds contribution of any single training point, making inference attacks statistically difficult
- **Core assumption:** Privacy budget is strictly managed and noise scale matches sensitivity of query/explanation function
- **Evidence anchors:** Section 5.1 notes DP noise serves as regularization mechanism with mathematical guarantee enabling quantification of privacy; abstract mentions DP is effective to varying degrees but introduces trade-offs

## Foundational Learning

- **Concept: Membership Inference vs. Model Inversion**
  - **Why needed here:** Paper categorizes risks into these distinct buckets (Section 4.2). Understanding difference—identifying who was in data vs. reconstructing what data looks like—is prerequisite to selecting right defense
  - **Quick check question:** If attacker tries to reconstruct chest X-ray of patient used in training, is this Membership Inference or Model Inversion? (Answer: Model Inversion/Reconstruction)

- **Concept: The Privacy-Utility-Explainability Triad**
  - **Why needed here:** Section 6 and 7 emphasize that optimizing for one (e.g., explanation fidelity) often degrades another (privacy). Cannot assess system without understanding these trade-offs
  - **Quick check question:** If you add heavy noise to explanations to ensure perfect privacy, what two other system properties might fail? (Answer: Explanation quality/fidelity and potentially model accuracy/utility)

- **Concept: Black-box vs. White-box Threat Models**
  - **Why needed here:** Section 4.1 defines threat model. Efficacy of attacks (e.g., gradient-based extraction) depends entirely on access level (API vs. Internal Access)
  - **Quick check question:** Does "data-free" model extraction attack require access to model's internal gradients (White-box)? (Answer: Not necessarily; section 4.2.3 notes attackers can synthesize data, but access to gradients significantly speeds it up)

## Architecture Onboarding

- **Component map:** Training Data (Sensitive) -> Model Training (with optional DP-SGD) -> Model f(x) + Explainer φ(x) -> Post-hoc Perturbation/Anonymization -> User (Adversarial or Benign)
- **Critical path:** The Explanation Interface is the primary attack surface. Engineers must trace flow of information from Training Data through Explainer to Output
- **Design tradeoffs:**
  - Faithfulness vs. Privacy: Transparent explanations (e.g., Influence Functions) are high-fidelity but high-risk (Section 4.2.1)
  - Utility vs. Noise: Increasing DP noise (ε → 0) reduces model accuracy and explanation stability (Section 5.1)
- **Failure signatures:**
  - High Explanation Irregularity: Section 5.1 notes additive noise caused irregularities, thereby reducing utility
  - Successful MIA: If shadow models can easily distinguish members from non-members based on explanation loss/variance (Section 4.2.1)
  - Zero-Query Extraction: If explanation leaks full decision boundary (Section 4.2.3)
- **First 3 experiments:**
  1. Baseline Attack Simulation: Train standard model (e.g., XGBoost or NN) on dataset (e.g., Adult/Census), generate SHAP/LIME explanations, run Membership Inference Attack on explanation outputs to measure baseline leakage
  2. DP-Training Impact: Retrain model using Differential Privacy (DP-SGD) with fixed budget (e.g., ε=1.0). Rerun MIA and measure drop in attack success rate vs. drop in model accuracy
  3. Perturbation Defense: Implement simple perturbation mechanism (e.g., adding noise to saliency maps or feature importance scores). Test if this prevents Model Inversion (reconstructing inputs) while keeping explanation meaningful to human user

## Open Questions the Paper Calls Out

- **Open Question 1:** How can researchers develop a standardized approach and quantitative metrics to evaluate privacy leakage in XAI methods?
  - **Basis in paper:** [explicit] Section 7.3 identifies "Lack of standardised approach for privacy evaluation of XAI" and Section 7.4 recommends development of quantitative privacy metrics
  - **Why unresolved:** Existing evaluation metrics focus on faithfulness and robustness rather than privacy safety, making it difficult to compare methods or verify regulatory compliance
  - **What evidence would resolve it:** Unified evaluation framework with specific quantitative metrics measuring privacy leakage risks across different explanation methods

- **Open Question 2:** What techniques can effectively provide privacy-preserving explanations for Generative AI and Large Language Models (LLMs)?
  - **Basis in paper:** [explicit] Section 7.3 notes "Lack of privacy preserving XAI for Gen-AI and LLMs," and Section 7.4 calls for research into privacy-preserved mechanisms for methods like Chain-of-Thought and RAG
  - **Why unresolved:** Traditional XAI methods are impractical for these models due to complex structures and vast parameters, while LLMs suffer from unique issues like data memorization
  - **What evidence would resolve it:** Novel explainability methods for LLMs that demonstrably mitigate data memorization and inference attacks without degrading model performance

- **Open Question 3:** How can the simultaneous trade-off between privacy, explainability, and utility be measured and balanced?
  - **Basis in paper:** [explicit] Section 7.3 highlights "Trade-offs in privacy, explainability and utility" as critical issue, and Section 7.4 recommends determining appropriate trade-off using tools like compatibility matrices
  - **Why unresolved:** Current privacy preservation methods often deteriorate model accuracy and explanation quality, creating conflict between safety and utility
  - **What evidence would resolve it:** Development of tuning mechanisms or "trade-off scores" allowing practitioners to quantitatively adjust and optimize competing properties

## Limitations

- The review's scope limited to 2019-2024 publications, potentially missing relevant work outside these parameters
- Categorization of privacy risks into intentional vs. unintentional categories may oversimplify complex attack vectors
- Practical implications for specific domains (healthcare, finance) are noted but not deeply explored, limiting generalizability

## Confidence

- **High**: Taxonomy of privacy risks (membership inference, model inversion, model extraction) is well-established in literature and clearly documented
- **Medium**: Effectiveness rankings of preservation methods are based on synthesis of existing studies with varying methodological rigor
- **Low**: Practical implementation guidelines for balancing privacy-utility-explainability triad lack empirical validation across diverse use cases

## Next Checks

1. **Empirical Validation of Defense Efficacy**: Replicate membership inference attack simulation (Section 4.2.1) using Adult/Census dataset with and without DP-SGD to measure actual privacy-utility trade-offs

2. **Cross-Domain Applicability**: Test proposed privacy-preserving XAI framework on healthcare dataset (e.g., MIMIC-III) to evaluate if 10 proposed characteristics adequately address domain-specific privacy concerns

3. **Temporal Validation**: Repeat literature search using same query string on updated databases (2025) to assess if identified privacy risks and preservation methods remain current or require revision