---
ver: rpa2
title: Characteristic Root Analysis and Regularization for Linear Time Series Forecasting
arxiv_id: '2509.23597'
source_url: https://arxiv.org/abs/2509.23597
tags:
- linear
- root
- time
- rank
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes linear time series forecasting through the
  lens of characteristic roots, showing how they govern long-term behavior in both
  noise-free and noisy settings. The authors find that noise introduces spurious roots
  and requires disproportionately large training data, highlighting the need for structural
  regularization.
---

# Characteristic Root Analysis and Regularization for Linear Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.23597
- Source URL: https://arxiv.org/abs/2509.23597
- Reference count: 40
- This paper analyzes linear time series forecasting through the lens of characteristic roots, showing how they govern long-term behavior in both noise-free and noisy settings. The authors find that noise introduces spurious roots and requires disproportionately large training data, highlighting the need for structural regularization. To address this, they propose two complementary strategies: rank reduction methods (RRR and DWRR) to enforce a low-dimensional latent structure, and a novel adaptive Root Purge method that encourages learning a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate that both approaches improve robustness and accuracy, often achieving state-of-the-art results. The findings underscore the value of integrating classical linear systems theory with modern learning techniques for robust, interpretable, and data-efficient forecasting models.

## Executive Summary
This paper introduces a novel theoretical framework for analyzing linear time series forecasting through characteristic roots, demonstrating how these roots govern model expressivity and long-term behavior. The authors show that noise in training data introduces spurious roots, requiring disproportionately large datasets for accurate learning. To address this challenge, they propose two complementary regularization strategies: rank reduction methods (RRR and DWRR) that enforce low-dimensional latent structures, and Root Purge, a novel adaptive regularization that learns a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate significant improvements in forecasting accuracy and robustness.

## Method Summary
The paper analyzes linear time series forecasting through the lens of characteristic roots, showing how these roots govern model expressivity and long-term behavior. For noise-free settings, the authors prove that a linear model can represent any time series whose characteristic roots are a subset of its own. In noisy settings, they show that noise introduces spurious roots and requires disproportionately large training data. To address these challenges, they propose two complementary regularization strategies: rank reduction methods (RRR and DWRR) that enforce low-dimensional latent structures, and Root Purge, a novel adaptive regularization that learns a noise-suppressing null space during training. The methods are evaluated extensively on standard forecasting benchmarks.

## Key Results
- Characteristic roots govern model expressivity and long-term behavior in both noise-free and noisy settings
- Rank reduction methods (RRR and DWRR) improve forecasting accuracy by enforcing low-dimensional latent structures
- Root Purge method learns a noise-suppressing null space during training, improving robustness
- Both approaches achieve state-of-the-art results on standard forecasting benchmarks
- Noise introduces spurious roots, requiring disproportionately large training data for accurate learning

## Why This Works (Mechanism)

### Mechanism 1: Characteristic Roots Govern Temporal Dynamics
- **Claim:** A linear forecasting model's expressivity and long-term behavior are determined by its characteristic roots; the model can represent any time series whose roots form a subset of the model's own roots.
- **Mechanism:** Linear recurrence relations \(y_t + a_1 y_{t-1} + \cdots + a_p y_{t-p} = 0\) have solutions expressible as linear combinations of characteristic roots raised to power \(t\). The model weight matrix \(W\) implicitly encodes these roots; correct root identification preserves dynamics, while spurious roots introduced by noise distort predictions.
- **Core assumption:** The underlying signal admits a finite-order linear recurrence (or can be approximated as such), and noise is additive and zero-mean.
- **Evidence anchors:**
  - [abstract] "characteristic roots govern long-term behavior in both noise-free and noisy settings"
  - [section 3.1, Fact 1] "A linear model can represent any time series whose characteristic roots are a subset of its own"
  - [corpus] Weak direct support; neighboring papers focus on architecture comparisons (KAN vs LSTM), regularization hierarchies (CoRe), and Koopman-based approaches—none explicitly derive characteristic root theory for linear forecasting.
- **Break condition:** If the true dynamics are strongly nonlinear or the noise is structured (non-zero mean, autocorrelated), the root-subset expressivity argument may not hold, and root-based regularization could misspecify the model.

### Mechanism 2: Rank Reduction Recovers Low-Dimensional Latent Dynamics
- **Claim:** Imposing a low-rank constraint on the weight matrix \(W\) implicitly projects history and future segments onto shared low-dimensional subspaces, suppressing noise-dominated components.
- **Mechanism:** In noise-free settings, the true data matrix has rank \(\min(L, K)\) where \(K\) is the number of characteristic roots. Noise inflates rank. Truncated SVD on \(W\) (DWRR) or projected outputs (RRR) enforces a bottleneck that aligns \(Y_{\text{his}}\) and \(Y_{\text{fut}}\) along maximal shared variation, filtering directions attributable to noise.
- **Core assumption:** The signal lives in a low-dimensional subspace; noise is approximately isotropic or spread across many directions. The optimal rank \(\rho \ll \min(L, H)\) can be estimated via validation.
- **Evidence anchors:**
  - [abstract] "rank reduction methods (RRR and DWRR) to enforce a low-dimensional latent structure"
  - [section 4.1, Proposition 2] "Constraining \(W\) to be low-rank implicitly projects \(Y_{\text{his}}\) and \(Y_{\text{fut}}\) onto low-dimensional subspaces"
  - [corpus] CoRe paper uses coherence regularization for hierarchical series but does not address characteristic roots; no direct corpus validation of rank–root relationship.
- **Break condition:** If the true dynamics are high-rank (many independent roots) or the signal-to-noise ratio is extremely low, aggressive rank truncation may discard genuine dynamics alongside noise.

### Mechanism 3: Root Purge Learns a Noise-Suppressing Null Space
- **Claim:** Adding a regularizer that re-applies the model to prediction residuals encourages \(W\) to learn a null space that maps noise to zero, adaptively reducing effective rank during training.
- **Mechanism:** The loss \( \|Y - GW(Y)\|_F^2 + \lambda \|(Y - GW(Y))W\|_F^2 \) balances signal fitting (root-seeking) with a root-purging term. If residuals approximate noise and \(W\) has learned an appropriate null space, the second term drives to zero. By rank–nullity theorem, expanding the null space reduces rank, suppressing spurious roots.
- **Core assumption:** Residuals after reasonable signal fitting are predominantly noise; noise is zero-mean and approximately uncorrelated with signal. Optimization reaches a stationary point near OLS with controlled deviation.
- **Evidence anchors:**
  - [abstract] "Root Purge method that encourages learning a noise-suppressing null space during training"
  - [section 4.2] "The root-purging term serves as a regularizer, encouraging the model to learn a null space that suppresses noise"
  - [corpus] No direct corpus analog; regularization papers (Logic-Inspired Regularization, CoRe) address different structural priors.
- **Break condition:** If the model is severely underfitting (too few roots to capture signal), residuals contain structured signal and purging may further degrade performance. Appendix E.10 confirms minimal impact on M4 where underfitting dominates.

## Foundational Learning

- **Concept:** Characteristic roots of linear difference equations
  - **Why needed here:** The paper's central thesis is that forecasting behavior is governed by these roots. Without this, the motivation for rank reduction and Root Purge is opaque.
  - **Quick check question:** Can you explain why a unit root (\(r = 1\)) enables mean-shift invariance in instance-normalized models?

- **Concept:** Singular value decomposition and rank–nullity theorem
  - **Why needed here:** RRR/DWRR rely on truncated SVD; Root Purge's connection to rank reduction depends on the rank–nullity tradeoff.
  - **Quick check question:** If a matrix \(W \in \mathbb{R}^{L \times H}\) has rank \(\rho\), what is the dimension of its null space when \(L > \rho\)?

- **Concept:** Ordinary least squares asymptotics under noise
  - **Why needed here:** Proposition 1 establishes the \(O(1/\sqrt{T})\) convergence of OLS weights on pure noise, motivating structural regularization.
  - **Quick check question:** Why does sublinear convergence imply data inefficiency in high-noise regimes?

## Architecture Onboarding

- **Component map:** History/Future Matrix Construction -> OLS Baseline -> RRR/DWRR/Root Purge
- **Critical path:**
  1. Validate OLS baseline performance
  2. Run RRR rank sweep on validation set; select \(\rho\) with lowest validation MSE
  3. If training infrastructure permits gradient optimization, implement Root Purge with \(\lambda \in [0.125, 0.5]\)

- **Design tradeoffs:**
  - **RRR vs. DWRR:** RRR shows more consistent validation–test alignment (Figure 16); DWRR is computationally cheaper but may overfit rank selection
  - **Time vs. frequency domain:** Paper uses frequency domain for stability; time domain yields similar results (Appendix E.7)
  - **Channel modeling:** CI (shared weights) benefits from implicit data augmentation; INC (separate weights) gains more from Root Purge (Table 2)

- **Failure signatures:**
  - **Full-rank plateau:** On large datasets (Electricity, Traffic), rank–MSE curves flatten; full-rank models approach optimal, masking regularization gains
  - **Underfitting regime:** Short lookback windows (M4) cause severe root scarcity; Root Purge and RRR provide negligible improvement (Table 18)
  - **Over-regularization:** Large \(\lambda\) (>1) suppresses significant singular values, degrading signal capture (Figure 2)

- **First 3 experiments:**
  1. **OLS baseline on ETTh1:** Train linear model, report MSE across horizons \(H \in \{96, 192, 336, 720\}\). Expected: baseline MSE ~0.38–0.44
  2. **RRR rank sweep on ETTh1:** Sweep rank \(\rho \in [25, 200]\), plot validation vs. test MSE. Expected: optimal rank well below full rank; ~10–15% MSE reduction
  3. **Root Purge \(\lambda\) sensitivity on ETTh1/ETTm1:** Train with \(\lambda \in [0.125, 0.5, 1]\), observe singular value spectrum shift (Figure 3). Expected: small \(\lambda\) improves MSE; large \(\lambda\) over-regularizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the characteristic root framework and Root Purge regularization be generalized to non-linear dynamical systems?
- Basis in paper: [explicit] Appendix F.1 states, "A promising direction for future work is to extend our framework toward nonlinear dynamics."
- Why unresolved: The current theoretical analysis, proofs, and assumptions are strictly derived for linear difference equations.
- What evidence would resolve it: Theoretical justification or empirical demonstration that Root Purge improves robustness in non-linear models like RNNs or Transformers.

### Open Question 2
- Question: Do characteristic root structures provide a useful inductive bias for time series classification?
- Basis in paper: [explicit] Appendix F.1 notes it would be interesting to "explore the implications of characteristic root structures in classification settings," specifically regarding null space learning.
- Why unresolved: The study validates the proposed methods solely using Mean Squared Error (MSE) for forecasting, not classification accuracy.
- What evidence would resolve it: Experiments on classification benchmarks showing that rank reduction or root-based regularization improves class separation.

### Open Question 3
- Question: Can the optimal rank \(\rho\) and regularization coefficient \(\lambda\) be theoretically determined without validation curves?
- Basis in paper: [inferred] Section 5 and Appendix E.8 describe reliance on empirical validation-MSE trade-off curves to select \(\rho\) and \(\lambda\).
- Why unresolved: The paper provides bounds on error but does not offer a closed-form solution for selecting hyperparameters directly from data statistics.
- What evidence would resolve it: A derived formula linking optimal \(\rho\) or \(\lambda\) to the signal-to-noise ratio that aligns with experimental results.

## Limitations
- The characteristic root framework is mathematically sound in noise-free settings but its practical impact in noisy, real-world time series is largely demonstrated empirically rather than analytically.
- The assumption that noise is additive, zero-mean, and uncorrelated with the signal underpins both rank reduction and Root Purge, but real-world time series often violate these assumptions.
- While RRR and DWRR show consistent improvements, the optimal rank selection remains heuristic and dataset-dependent, with no theoretical guarantees on rank choice.

## Confidence
- **High confidence**: The mathematical characterization of characteristic roots governing linear model behavior in noise-free settings; the connection between spurious roots and noise-induced rank inflation.
- **Medium confidence**: The effectiveness of rank reduction (RRR/DWRR) in improving forecasting accuracy, supported by extensive experiments but with limited theoretical analysis of why specific ranks work.
- **Medium confidence**: The Root Purge method's ability to learn a noise-suppressing null space, with ablation studies confirming benefits but no rigorous analysis of convergence or null space properties.

## Next Checks
1. **Analytical convergence bounds**: Derive and validate theoretical bounds on forecasting error as a function of signal rank, noise level, and chosen regularization rank in RRR/DWRR.
2. **Noise structure sensitivity**: Systematically test RRR and Root Purge under structured noise (e.g., heteroscedastic, autocorrelated) to quantify robustness breakdown.
3. **Underfitting regime characterization**: Conduct controlled experiments on synthetic series with limited characteristic roots to precisely map performance degradation and identify failure thresholds.