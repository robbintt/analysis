---
ver: rpa2
title: 'Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing
  Accuracy-Cost Tradeoffs'
arxiv_id: '2504.05245'
source_url: https://arxiv.org/abs/2504.05245
tags:
- feature
- sparse
- methods
- selection
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Sparse Federated Feature Selection
  (DSFFS), the first embedded-based federated feature selection (FFS) method for horizontal
  federated learning (FL). DSFFS addresses the challenge of high-dimensional data
  in resource-constrained edge devices by integrating feature selection directly into
  the FL training process.
---

# Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing Accuracy-Cost Tradeoffs

## Quick Facts
- arXiv ID: 2504.05245
- Source URL: https://arxiv.org/abs/2504.05245
- Reference count: 23
- Introduces the first embedded-based federated feature selection method that achieves superior accuracy-cost tradeoffs in horizontal federated learning

## Executive Summary
This paper presents Dynamic Sparse Federated Feature Selection (DSFFS), a novel method that integrates feature selection directly into federated learning training through dynamic sparse training. DSFFS addresses the challenge of high-dimensional data in resource-constrained edge devices by simultaneously pruning and regrowing input-layer neurons and connections during training. The approach achieves significant reductions in both communication (76%) and computation (78%) costs while maintaining or improving accuracy compared to existing federated feature selection methods.

## Method Summary
DSFFS operates by dynamically pruning and regrowing input-layer neurons based on connection weight magnitude (strength) and gradient values during federated training. The method uses a sparse MLP architecture where input neurons are evaluated for strength (L1-norm of outgoing weights), with weak neurons pruned and high-gradient disconnected neurons regrown. This process is integrated with FedDST/FedProx aggregation, allowing the model to identify informative features while maintaining low computational and communication overhead. The approach works in a horizontal federated learning setting with non-IID data distribution across multiple clients.

## Key Results
- Outperforms state-of-the-art FFS approaches in 22 out of 36 cases across nine real-world datasets
- Reduces communication costs by 76% and computation costs by 78% on non-iid datasets
- Achieves superior trade-off between accuracy, computation, and communication costs
- Best performance with K=150 features selected, though K=100 also provides good accuracy-cost balance

## Why This Works (Mechanism)

### Mechanism 1: Input-Layer Neuron Strength Pruning
The algorithm calculates a "strength" score for each input neuron (feature) defined as the L1-norm of its outgoing weights ($H_i = \|w_i\|_1$). Neurons with the lowest strength are pruned, operating on the assumption that features with weak cumulative connections contribute little to the model's predictive power. This mechanism effectively identifies and eliminates uninformative features in a federated setting.

### Mechanism 2: Gradient-Based Regrowth for Feature Recovery
After pruning, disconnected neurons are evaluated based on the gradient magnitudes of their potential connections. Neurons with the highest absolute gradients are regrown, allowing the network to dynamically adjust its feature set if the optimization landscape changes. This prevents premature convergence and enables the model to rediscover previously discarded useful features.

### Mechanism 3: Sparse Aggregation for Resource Efficiency
Instead of transmitting dense model updates, clients transmit only sparse sub-networks ($\theta_s$). The server aggregates only these active parameters, significantly reducing payload size per round and the FLOPs required for local training. This maintains global model performance while achieving substantial resource savings.

## Foundational Learning

- **Dynamic Sparse Training (DST):** The foundational engine that maintains constant sparsity while altering topology through pruning/regrowing. Needed to understand how DSFFS differs from static pruning methods. Quick check: How does DST differ from magnitude pruning after training?

- **Horizontal Federated Learning (FL):** The specific FL setting where DSFFS operates (same features, different samples). Needed to understand the distributed data constraints and aggregation logic. Quick check: In Horizontal FL, do clients share the same feature space or the same sample IDs?

- **Embedded Feature Selection:** The approach where selection occurs during model training rather than as preprocessing. Needed to understand why DSFFS is computationally superior to Filter and Wrapper methods. Quick check: Does an embedded method perform selection before or during model training?

## Architecture Onboarding

- **Component map:** Edge Server (global sparse mask, weights, aggregation) -> Client (local dataset, sparse network training) -> Feature Selector (identifies top K connected input neurons)

- **Critical path:** Server initializes sparse network → Client receives network → Local Training → Calculate Strength ($H_i$) → Prune weak input neurons → Regrow high-gradient neurons → Client uploads sparse gradients/weights → Server aggregates → Next round

- **Design tradeoffs:** Sparsity ($s$) vs. accuracy (higher sparsity reduces cost but risks missing features); Feature count ($K$) vs. efficiency (too low drops signal, too high negates gains); Regrowth rate ($\zeta$) vs. stability (too high causes instability, too low prevents escape from local minima)

- **Failure signatures:** Accuracy Collapse (β too high, features removed faster than learning); Stagnant Feature Set (small gradients prevent regrowth, model locks onto suboptimal features); Sparsity drift (pruning ≠ regrowth, leading to dense models)

- **First 3 experiments:** 1) Noise Robustness Check: Inject noisy features into MNIST and verify DSFFS prunes noise while FedDST baseline degrades; 2) Communication Benchmark: Measure cumulative upload cost vs. Accuracy against wrapper methods to validate 76% cost reduction; 3) Ablation on $K$: Run DSFFS with $K \in \{50, 100, 150\}$ on SMK-CAN-187 to observe accuracy vs. cost tradeoff frontier

## Open Questions the Paper Calls Out

- **Can DSFFS be extended to sparse semi-supervised federated learning under non-IID data distributions?** The paper explicitly states future research will focus on extending DSFFS to sparse semi-supervised FL in non-iid data distribution settings. This remains unresolved because the current method relies on supervised labels for gradient calculations, which are unavailable in semi-supervised scenarios.

- **How can critical hyperparameters (sparsity $s$, pruning fraction $\zeta$, and $\beta$) be determined automatically?** The paper notes these parameters were "determined through trial and error," indicating no adaptive mechanism exists. This is unresolved because manual tuning is computationally expensive and impractical for edge devices with unknown or fluctuating data distributions.

- **Is the neuron strength criterion ($H_i$) robust against high levels of label noise?** The paper shows robustness to feature noise but not label noise. This remains unresolved because noisy labels can distort weight magnitudes, causing the dynamic pruning mechanism to select irrelevant features or discard informative ones based on corrupted gradient updates.

## Limitations

- Critical architectural details are missing, including exact neural network structure (hidden layer widths and depths), optimizer configurations (learning rate, batch size), and non-IID data partitioning strategy
- The method's generalizability across diverse datasets is uncertain without access to the complete experimental protocol and hyperparameter tuning methodology
- The impact of noisy labels on the neuron strength criterion has not been evaluated, potentially limiting robustness in real-world scenarios

## Confidence

- **High Confidence:** The fundamental mechanism of input-layer neuron pruning based on L1-norm strength is well-specified and theoretically sound
- **Medium Confidence:** The communication and computation cost reductions (76% and 78%) are reported but depend on unspecified implementation details that affect absolute values
- **Low Confidence:** The generalizability of results across diverse datasets is uncertain without access to the complete experimental protocol and hyperparameter tuning methodology

## Next Checks

1. **Architecture Replication:** Implement DSFFS with a specified MLP architecture (e.g., [784-128-64-10]) and identical hyperparameters to verify the 76% communication cost reduction on MNIST

2. **Noise Injection Test:** Create a controlled experiment with artificially added noisy features to validate that DSFFS correctly identifies and prunes uninformative features while maintaining accuracy

3. **Ablation Study:** Systematically vary the sparsity parameter s (0.6, 0.7, 0.8, 0.9) and feature count K to establish the sensitivity of accuracy to these hyperparameters across multiple datasets