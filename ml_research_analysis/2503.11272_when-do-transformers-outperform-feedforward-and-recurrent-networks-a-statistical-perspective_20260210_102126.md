---
ver: rpa2
title: When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical
  Perspective
arxiv_id: '2503.11272'
source_url: https://arxiv.org/abs/2503.11272
tags:
- where
- have
- proof
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the statistical efficiency of transformers\
  \ versus classical architectures (feedforward and recurrent networks) for learning\
  \ sequence-to-sequence models where each output depends on a sparse subset of input\
  \ tokens described in the prompt. The authors propose the q-Sparse Token Regression\
  \ (qSTR) data generating model, where the output at each position depends only on\
  \ q relevant tokens with q \u226A N, and the positions of these tokens are described\
  \ in the input prompt."
---

# When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective

## Quick Facts
- arXiv ID: 2503.11272
- Source URL: https://arxiv.org/abs/2503.11272
- Reference count: 40
- Primary result: Transformers achieve length-independent sample complexity for sparse sequence tasks while feedforward and recurrent networks require linear or polynomial scaling in sequence length.

## Executive Summary
This paper provides a rigorous statistical analysis of when transformers outperform classical architectures for learning sequence-to-sequence models with sparse dependencies. The authors introduce the q-Sparse Token Regression (qSTR) model, where outputs depend on only q relevant tokens per position, and demonstrate that transformers can learn this model with sample complexity nearly independent of sequence length N when equipped with sufficient attention heads. In contrast, feedforward networks require linear scaling in N samples while recurrent networks require polynomial scaling. The key insight is that transformers' attention mechanism can dynamically route information to relevant tokens based on positional information in the prompt, while other architectures struggle with this task. These theoretical findings are validated through experiments showing transformers' superior sample efficiency on sparse token selection tasks.

## Method Summary
The paper studies the q-Sparse Token Regression (qSTR) task where output $y_i$ at position $i$ depends only on $q$ relevant tokens $x_{t_{i1}}, \dots, x_{t_{iq}}$ whose positions are described in the input prompt. The authors prove upper bounds for transformers with sufficient attention heads and lower bounds for feedforward and recurrent networks using standard learning theory techniques including covering numbers, Rademacher complexity, and analysis of optimization dynamics. Experiments use synthetic data with Gaussian tokens, varying sequence lengths, and online AdamW training to compare sample complexity across architectures. The theoretical analysis focuses on single-layer architectures, while experiments explore practical implications.

## Key Results
- A single-layer transformer with H ≥ q attention heads can learn qSTR with sample complexity almost independent of sequence length N
- Feedforward networks require N samples to learn even simplified versions of qSTR due to span limitations
- Recurrent networks require N^Ω(1) samples for general qSTR due to representation costs, though they succeed on static variants
- Experimental validation shows transformers achieve better sample complexity than other architectures on sparse token selection tasks

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Sparse Retrieval
A single-layer Transformer can learn the q-Sparse Token Regression (qSTR) model with sample complexity almost independent of input length $N$, provided it has at least $q$ attention heads. The architecture uses positional encodings to create a "key" for the relevant tokens. The attention mechanism computes a softmax over these keys, effectively creating a routing mechanism that selects the $q$ relevant tokens from the $N$ total tokens. This bypasses the need to process the full sequence content densely. Core assumption: The positional encodings exist and are sufficiently distinct to allow the attention heads to isolate the target tokens. Break condition: If the number of attention heads $H < q$, the model cannot approximate the function regardless of sample size.

### Mechanism 2: FFN Linear Span Limitation
Feedforward networks (FFNs) require $\Omega(N)$ samples to learn even simplified qSTR models because the learned weights are restricted to the span of the training data. In an FFN, the first layer performs a fully-connected projection. Gradient descent forces the rows of this weight matrix to lie within the span of the input training samples. If the number of samples $n < N d$, this span cannot cover the space of all possible target directions, leading to high error on test data. Core assumption: The learning algorithm involves stationarity conditions which constrain weights to the training span. Break condition: If the training sample size $n$ scales linearly with sequence length $N$, the FFN can eventually learn the task.

### Mechanism 3: RNN Representation Cost
Recurrent Neural Networks (RNNs) suffer from high sample complexity ($N^{\Omega(1)}$) on general qSTR because representing dynamic token selection requires a hidden state width or weight norm that scales with $N$. To route information from a dynamic position $t$ to the output, an RNN must maintain a state that effectively tracks position. The operator norm of the weight matrix $U$ must scale as $\sim \sqrt{N}$ to maintain this fidelity across time steps, increasing the complexity class and violating the bounded-norm assumptions required for efficient generalization. Core assumption: The RNN uses bounded-norm weights; if weights are allowed to grow unbounded, the sample complexity lower bound might not hold. Break condition: For "Simple qSTR" (where relevant positions are fixed), RNNs succeed with polylogarithmic samples because the routing problem becomes trivial.

## Foundational Learning

**Concept: Empirical Risk Minimization (ERM) vs. Stationary Points**
Why needed here: The negative results for FFNs and RNNs rely on characterizing the solution space of standard training algorithms (gradient descent/stationary points) rather than just the existence of a solution. Quick check question: Does the theoretical guarantee assume you find the *global* minimum, or does it apply to *any* stationary point found by GD?

**Concept: Covering Numbers & Rademacher Complexity**
Why needed here: The positive result for Transformers uses covering numbers to bound the generalization error. Understanding how these metric entropy bounds scale with sequence length $N$ is key to the "independent of $N$" claim. Quick check question: How does the covering number of the Transformer function class scale with the input sequence length compared to an RNN?

**Concept: Dynamic vs. Static Sparsity**
Why needed here: The core distinction between the architectures depends on whether the "relevant tokens" change based on the input prompt (dynamic) or remain fixed (static/simple). Quick check question: Why does an RNN succeed on Simple-qSTR but fail on general qSTR?

## Architecture Onboarding

**Component map:**
Input: Prompt $p$ of length $N$ (Tokens $x$ + Positional Encodings $t$)
Transformer: Multi-Head Attention (Heads $\ge q$) -> Feedforward Layer (Link function)
FFN: Fully Connected Layer 1 (Dense projection) -> Arbitrary Layers
RNN: Bidirectional Hidden States ($h_{\rightarrow}, h_{\leftarrow}$) -> Output projection

**Critical path:** Ensuring the **positional encoding** quality and the **number of attention heads** ($H \ge q$) for Transformers.

**Design tradeoffs:**
- **FFNs:** Fast parallelization but statistically blind to sparse structure in long sequences; scales poorly with $N$
- **RNNs:** Sequential processing ($O(N)$); fails to generalize on dynamic routing tasks without massive width
- **Transformers:** $O(N^2)$ attention cost (standard), but statistically efficient ($O(1)$ sample scaling w.r.t $N$)

**Failure signatures:**
- **FFN:** Test error remains high until training samples $n \approx N \cdot d$
- **RNN:** Performance degrades as sequence length $N$ increases for dynamic tasks, or requires infeasibly large hidden dimensions
- **Transformer:** Fails to converge or high approximation error if Heads $H < q$

**First 3 experiments:**
1. **Vary Sequence Length ($N$):** Train all three architectures on qSTR with fixed $q$ and increasing $N$. Plot test MSE vs. $N$. (Expect: Transformer flat, FFN/RNN increasing)
2. **Heads Ablation:** Train Transformer with $H < q$ vs $H \ge q$ heads on qSTR. (Expect: Failure for $H < q$)
3. **Sample Efficiency Scaling:** For a fixed large $N$, plot required training samples to reach target MSE. (Expect: FFN requires linear scaling, Transformer logarithmic/constant)

## Open Questions the Paper Calls Out

### Open Question 1
Does the optimization dynamics of gradient descent enable Transformers to achieve the information-theoretic sample complexity limit of $\Omega(qd)$, closing the gap with the current upper bound? The authors remark that studying optimization dynamics might reveal additional structure in the solution reached by gradient-based methods, pushing the sample complexity closer to the information-theoretic limit. This remains unresolved because the current analysis relies on uniform convergence bounds over a norm-bounded class, which may be looser than the actual path taken by gradient-based training. Evidence would require a proof showing that gradient descent induces an implicit bias towards solutions with lower effective complexity than the worst-case norm-bounded class.

### Open Question 2
Does the sample complexity separation between Transformers and recurrent architectures persist or widen when considering deeper models, particularly for hierarchical variations of the qSTR task? The conclusion identifies sample complexity separations that highlight the role of depth in Transformers as an important direction for future work. This remains unresolved because the paper focuses primarily on single-layer attention for its theoretical separation. Evidence would require generalization bounds for deep Transformers on hierarchical qSTR tasks compared against lower bounds for deep RNNs or FFNs.

### Open Question 3
Is the Lipschitz constraint ($\alpha_N \le N^{-1}$) on transition functions strictly necessary for RNNs to learn simple-qSTR with length-independent sample complexity, or is it a limitation of the proof technique? Theorem 7 requires $\alpha_N \le N^{-1}$ to prove positive results, and the text notes that without this constraint, current techniques yield linear dependence on $N$. This implies it is unknown if standard RNNs can succeed without this specific restriction. Evidence would require either a lower bound showing RNNs fail without this constraint, or an improved upper bound that removes the condition.

## Limitations
- Results are asymptotic and only manifest clearly for sufficiently large sequence lengths N
- Analysis focuses on single-layer architectures; deeper models remain unexplored
- Theoretical lower bounds rely on stationarity conditions that may not capture all training dynamics
- Experiments use synthetic data, limiting direct translation to real-world applications

## Confidence

**High Confidence:** The theoretical framework distinguishing Transformer vs. FFN vs. RNN sample complexity is internally consistent and well-grounded in established learning theory. The proof techniques for each architecture's limitations/capabilities appear sound.

**Medium Confidence:** The empirical results demonstrating the predicted sample complexity scaling relationships. While the experimental design is reasonable, the absence of certain hyperparameter details and the synthetic nature of the data reduce confidence in exact replication.

**Low Confidence:** The practical implications for real-world applications. The paper convincingly shows Transformers can learn sparse token selection tasks efficiently, but translating this to complex domains with approximate sparsity patterns requires additional investigation.

## Next Checks

1. **Scaling Verification:** Replicate the experiments with varying sequence lengths N (e.g., N = 64, 256, 1024, 4096) to empirically verify the predicted transition points where Transformer advantages become pronounced. Track both training samples required and wall-clock time.

2. **Hyperparameter Sensitivity:** Systematically vary the hidden layer widths and training hyperparameters (learning rate, batch size, optimizer settings) for the FFN and RNN baselines to ensure the reported sample complexity gaps are robust and not artifacts of specific architectural choices.

3. **Real-World Transfer:** Adapt the qSTR framework to a realistic sparse sequence task, such as document summarization with keyphrase extraction or code completion with relevant context tokens. Evaluate whether the predicted sample efficiency advantages of Transformers persist when the sparsity pattern is approximately, rather than exactly, known.