---
ver: rpa2
title: Building Robust and Scalable Multilingual ASR for Indian Languages
arxiv_id: '2511.15418'
source_url: https://arxiv.org/abs/2511.15418
tags:
- multi-decoder
- languages
- speech
- language
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the development of multilingual automatic
  speech recognition (ASR) systems for 8 Indian languages across 33 dialects as part
  of the ASRU MADASR 2.0 challenge. The authors focused on leveraging phonemic similarities
  across Indian languages using a Common Label Set (CLS) as an intermediate representation,
  addressing the challenge of converting back to native graphemic scripts while retaining
  ASR performance gains.
---

# Building Robust and Scalable Multilingual ASR for Indian Languages

## Quick Facts
- arXiv ID: 2511.15418
- Source URL: https://arxiv.org/abs/2511.15418
- Reference count: 8
- Primary result: Multi-decoder model with ASR initialization achieved highest LID/DID accuracy (75.36% DID for read speech) in MADASR 2.0 challenge

## Executive Summary
This paper describes multilingual ASR systems for 8 Indian languages across 33 dialects as part of the MADASR 2.0 challenge. The authors propose a multi-decoder architecture using phonemic Common Label Set (CLS) as intermediate representation, where an ASR sub-network generates CLS transcriptions and an MT sub-network converts these to native scripts. The approach addresses error propagation in cascaded systems while leveraging phonemic similarities across Indian languages. The final system achieved state-of-the-art LID/DID performance, winning the challenge.

## Method Summary
The core method employs a multi-decoder architecture with phonemic CLS as intermediate representation. An ASR sub-network (Conformer encoder + Transformer decoder) generates CLS transcriptions using hybrid CTC-Attention loss. An MT sub-network (2-block Transformer encoder+decoder) converts CLS to native scripts using cross-entropy loss with cross-attention over speech encoder outputs. The system uses character-level tokenization and ASR initialization from pre-trained cascaded models to balance convergence. A hierarchical encoder variant adds auxiliary native-script CTC supervision.

## Key Results
- Multi-decoder model with ASR initialization achieved 97.39% language ID and 75.36% dialect ID accuracy on Track 2 read speech
- System beat baseline in 3 languages for WER/CER metrics
- Character-level tokenization outperformed BPE tokenization due to dataset's multi-dialect nature
- Multi-decoder approach reduced error propagation compared to cascaded pipelines

## Why This Works (Mechanism)

### Mechanism 1: Common Label Set (CLS) Reduces Decoder Confusion Through Phonemic Consolidation
Converting native scripts to unified phonemic representation reduces target token vocabulary and cross-lingual confusion. The unified parser maps graphemes from multiple Indian languages to a shared phoneme set, reducing decoder output space while leveraging phonemic similarities across related language families.

### Mechanism 2: Multi-Decoder Joint Training Mitigates Cascaded Error Propagation
Jointly training ASR and MT sub-networks with shared encoder representations reduces error accumulation compared to cascaded pipelines. The ASR decoder's hidden states directly feed the MT encoder, enabling end-to-end gradient flow, while the MT decoder's cross-attention over the speech encoder provides auxiliary acoustic context.

### Mechanism 3: ASR Initialization Aligns Sub-Network Training Dynamics
Pre-training the ASR sub-network prevents MT overfitting caused by mismatched convergence rates. The ASR task converges slower than the MT task, so without initialization, the MT sub-network overfits early while ASR is still learning.

## Foundational Learning

- **Conformer Encoder Architecture**: Why needed - Conformer encoder (convolution-augmented Transformer) extracts robust acoustic features from log-Mel spectrograms. Quick check - Can you explain why Conformer adds convolution layers to standard Transformer for speech tasks?

- **Hybrid CTC-Attention Loss**: Why needed - Both baseline and ASR sub-network use hybrid CTC-Attention training for alignment regularization and flexible decoding. Quick check - What happens to convergence if CTC weight is set too high versus too low?

- **Grapheme-to-Phoneme (G2P) Conversion**: Why needed - CLS approach depends on accurate G2P conversion; schwa deletion, geminate correction, and syllable segmentation affect both ASR target generation and MT reconstruction. Quick check - For a Devanagari word, what are two linguistic phenomena causing incorrect CLS conversion?

## Architecture Onboarding

- **Component map**: 16kHz audio → 80-dim log-Mel spectrograms → Conformer encoder (8 blocks) → ASR decoder (6 blocks) → CLS tokens → MT encoder (2 blocks) → MT decoder (2 blocks) + cross-attention → native script

- **Critical path**: 1) Verify unified parser converts native script ↔ CLS for all 8 languages, 2) Train cascaded ASR model first, 3) Initialize multi-decoder ASR sub-network with cascaded weights, 4) Jointly train multi-decoder with CTC-Attention + cross-entropy losses, 5) Monitor LID/DID tokens

- **Design tradeoffs**: BPE vs character tokenization (character outperforms BPE for multi-dialect datasets), with vs without hierarchical encoder (adds supervision but increases parameters), ASR initialization vs from-scratch (initialization improves convergence but requires pre-training)

- **Failure signatures**: MT overfitting (validation loss plateaus early), poor LID/DID accuracy (check token prepending), high CER gap (review MT decoder cross-attention)

- **First 3 experiments**: 1) Reproduce baseline Conformer encoder + Transformer decoder with LID token, 2) Train cascaded ASR→CLS model and evaluate CLS-space CER, 3) Implement multi-decoder with ASR initialization using character tokenization

## Open Questions the Paper Calls Out

- How can information loss in CLS representation be mitigated to prevent performance degradation when converting back to graphemic scripts? While multi-decoder reduces error propagation, conversion from phonemic space back to native script still causes performance drops.

- Does BPE's inferior performance compared to character-level tokenization generalize to other multilingual datasets, or is it specific to MADASR's dialectal repetition? It's unclear if BPE failed due to specific "same utterance, different dialect" structure.

- Can convergence mismatch between ASR and MT sub-networks be resolved through adaptive optimization techniques rather than weight initialization? Relying on pre-trained weights increases complexity; it's untested if the architecture can converge optimally "from scratch" using distinct learning rates.

## Limitations
- Dataset (MADASR 2.0) and unified parser are not publicly accessible, creating reproducibility barriers
- Evaluation lacks direct comparisons with standard ASR benchmarks on multilingual datasets outside MADASR 2.0
- Paper does not provide ablation studies for hierarchical encoder variant or comprehensive analysis of CLS-to-native script conversion accuracy

## Confidence
- High confidence: CLS approach reduces decoder confusion through phonemic consolidation
- Medium confidence: Multi-decoder architecture mitigates cascaded error propagation
- Medium confidence: ASR initialization synchronizes sub-network training dynamics

## Next Checks
1. Obtain MADASR 2.0 dataset access and verify unified parser implementation for CLS conversion across all 8 languages
2. Systematically compare character-level versus BPE tokenization performance on dataset subset where utterances appear across multiple dialects
3. Measure gap between cascaded pipeline performance and multi-decoder performance on held-out validation set to quantify actual error reduction benefits