---
ver: rpa2
title: 'RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered
  by Large Vision-Language Models'
arxiv_id: '2503.03987'
source_url: https://arxiv.org/abs/2503.03987
tags:
- retinal
- data
- medical
- image
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RetinalGPT addresses the challenge of applying general-domain multimodal
  large language models (MLLMs) to specialized retinal image analysis tasks. The core
  method introduces a two-stage instruction-tuning strategy that combines clinical
  preference data with generic medical knowledge through a mixup approach.
---

# RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2503.03987
- **Source URL**: https://arxiv.org/abs/2503.03987
- **Reference count**: 28
- **Primary result**: 95.10% accuracy on APTOS and strong performance on 7 other retinal datasets

## Executive Summary
RetinalGPT is a specialized multimodal large language model designed for retinal image analysis, addressing the challenge of applying general-domain MLLMs to clinical retinal tasks. The model uses a two-stage instruction-tuning strategy that combines clinical preference data with generic medical knowledge through a mixup approach, preserving broader medical understanding while enhancing domain-specific capabilities. Trained on 38K retinal images with structured clinical feature descriptions and 60K generic medical QA pairs, RetinalGPT outperforms all baseline models across 8 benchmark datasets, achieving high accuracy in disease classification, lesion localization, and vascular structure analysis.

## Method Summary
RetinalGPT employs a two-stage instruction-tuning strategy to adapt a pretrained multimodal LLM for retinal image analysis. In Stage 1, the model undergoes vision-language alignment using 600K PMC pairs and 38K retinal clinical annotations, with the CLIP vision encoder and LLaMA LLM frozen while training only the projection layer. Stage 2 applies mixup instruction-tuning with 38K retinal clinical task dialogues and 60K generic medical QA pairs, freezing the vision encoder while training both the projector and LLM. The model architecture combines CLIP vision encoder, linear projection, and LLaMA LLM, optimized with specific hyperparameters including global batch size of 128, max sequence length of 2048, and learning rates of 2e-3 (Stage 1) and 2e-5 (Stage 2).

## Key Results
- Achieves 95.10% accuracy on APTOS, 73.70% on EyeP, 84.90% on ACS, 80.83% on IDRiD, 87.14% on Messidor, 66.13% on MICCAI, 88.27% on OIA-ODIR, and 99.57% on RFMiD datasets
- Demonstrates strong capabilities in lesion localization with high IOU scores and vascular structure analysis with predicted values closely matching AutoMorph ground truth
- Outperforms all baseline models across all benchmark datasets for retinal disease classification and clinical feature extraction

## Why This Works (Mechanism)
The two-stage instruction-tuning approach works by first establishing strong vision-language alignment with clinical annotations (Stage 1), then fine-tuning with a mix of domain-specific clinical tasks and generic medical knowledge (Stage 2). The mixup strategy preserves the model's general medical reasoning capabilities while specializing it for retinal analysis. By freezing the vision encoder during Stage 2, the model maintains robust feature extraction while adapting its language understanding to clinical preferences. The structured clinical feature descriptions provide rich, interpretable inputs that guide the model toward clinically relevant outputs rather than generic image descriptions.

## Foundational Learning
- **Multimodal instruction tuning**: Training vision-language models on paired image-text data to align visual features with language representations. Why needed: Enables the model to understand and reason about retinal images in conversational contexts. Quick check: Verify that the model can accurately describe basic image features before clinical specialization.
- **Mixup training strategy**: Combining domain-specific and generic data during fine-tuning to prevent catastrophic forgetting. Why needed: Maintains general medical knowledge while specializing for retinal analysis. Quick check: Test model performance on both retinal-specific and general medical questions.
- **Structured clinical feature extraction**: Converting retinal images into interpretable clinical features (lesion locations, vascular metrics, disease labels). Why needed: Provides rich, clinically meaningful inputs for model training. Quick check: Validate that extracted features match expert annotations.
- **Vision encoder freezing**: Keeping the visual feature extractor fixed during later training stages. Why needed: Preserves robust visual representations learned during initial alignment. Quick check: Monitor visual feature quality metrics during training.
- **Instruction data generation**: Using GPT-4 to create synthetic clinical dialogues from structured features. Why needed: Scales training data creation without requiring extensive expert annotation. Quick check: Compare generated dialogues against real clinical conversations.
- **Vascular structure analysis**: Extracting and analyzing retinal vessel patterns for disease diagnosis. Why needed: Vessel morphology is a key diagnostic indicator in many retinal diseases. Quick check: Validate vascular measurements against established clinical standards.

## Architecture Onboarding

**Component Map**: Retinal Image -> CLIP Vision Encoder -> Linear Projection -> LLaMA LLM -> Clinical Response

**Critical Path**: Vision encoder feature extraction → Projection alignment → LLM reasoning → Clinical output generation

**Design Tradeoffs**: The model trades pure performance for interpretability by using structured clinical features rather than raw image pixels, enabling better clinical reasoning but requiring more preprocessing. Freezing the vision encoder during Stage 2 preserves visual quality but limits adaptation to retinal-specific visual patterns.

**Failure Signatures**: 
- First-turn responses consistently reference image modality regardless of query content
- Performance degradation on general medical questions after Stage 2 fine-tuning
- Poor lesion localization when bounding box formats don't match training data

**First Experiments**:
1. Test model on LLaVA-Med generic medical questions to verify preservation of general medical knowledge
2. Evaluate lesion localization accuracy with varying bounding box coordinate formats
3. Measure vascular feature prediction accuracy against AutoMorph ground truth

## Open Questions the Paper Calls Out

**Open Question 1**: How can the observed tendency for the model to provide modality-related answers to the first question (regardless of the query content) be effectively mitigated? The paper explicitly notes this behavioral issue and identifies lack of diversity in the instruction tuning set as a likely cause.

**Open Question 2**: Does the reliance on GPT-4 to "simulate" visual understanding from structured text features introduce systematic hallucinations or error propagation? The authors acknowledge that the model learns from synthetic reasoning that may lack true visual grounding, but don't analyze logical inconsistencies in the synthesized chains.

**Open Question 3**: Is the model learning robust physiological representations of vascular structures, or is it merely regressing to the specific algorithmic outputs used to generate its training labels? High correlation with AutoMorph doesn't guarantee biologically valid feature extraction, raising concerns about circular validation.

## Limitations
- The model shows a persistent tendency to provide modality-related answers for the first question regardless of query content
- Performance relies heavily on GPT-4-generated synthetic data, raising questions about data quality and diversity
- Vascular feature validation uses the same AutoMorph tool for both training and evaluation, potentially introducing algorithmic bias

## Confidence

**High Confidence**: The two-stage instruction tuning methodology with mixup is clearly specified, and experimental results showing strong performance across 8 retinal datasets are well-documented.

**Medium Confidence**: Dataset composition and training procedure specifications are provided, but exact data formats, preprocessing steps, and GPT-4 prompts remain unclear.

**Low Confidence**: The quality and diversity of generated instruction data depends heavily on unspecified GPT-4 prompts, making exact replication uncertain.

## Next Checks

1. **Data Generation Validation**: Test the model on LLaVA-Med generic medical questions to verify that the mixup approach preserves general medical knowledge and doesn't cause catastrophic forgetting of non-retinal capabilities.

2. **Prompt Reproduction Attempt**: Attempt to replicate the RCT and RCA datasets using publicly available GPT-4 APIs with prompts inferred from provided examples, then evaluate if generated data quality matches expected performance.

3. **Architecture-Specific Failure Mode Testing**: Design a test suite with multi-turn dialogues where the first question is about a non-retinal topic to confirm whether the model consistently produces modality-related answers for the first turn, and measure the frequency of this failure.