---
ver: rpa2
title: 'JavelinGuard: Low-Cost Transformer Architectures for LLM Security'
arxiv_id: '2506.07330'
source_url: https://arxiv.org/abs/2506.07330
tags:
- arxiv
- prompt
- jailbreak
- injection
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JavelinGuard, a suite of low-cost transformer
  architectures optimized for detecting malicious intent in LLM interactions, specifically
  targeting prompt injection and jailbreak attacks. The authors propose five progressively
  sophisticated architectures: Sharanga (baseline transformer), Mahendra (enhanced
  attention-weighted pooling), Vaishnava and Ashwina (hybrid neural-forest and neural-XGBoost
  architectures), and Raudra (advanced multi-task framework).'
---

# JavelinGuard: Low-Cost Transformer Architectures for LLM Security

## Quick Facts
- arXiv ID: 2506.07330
- Source URL: https://arxiv.org/abs/2506.07330
- Reference count: 20
- Key outcome: Introduces five transformer architectures optimized for detecting prompt injection and jailbreak attacks, achieving high accuracy with low inference costs

## Executive Summary
This paper presents JavelinGuard, a suite of transformer architectures designed to detect malicious intent in LLM interactions at low computational cost. The authors propose five models ranging from a basic transformer baseline (Sharanga) to an advanced multi-task framework (Raudra), with intermediate architectures incorporating attention-weighted pooling (Mahendra) and hybrid neural-forest approaches (Vaishnava, Ashwina). These models leverage compact BERT variants and achieve rapid CPU inference while maintaining strong accuracy across diverse adversarial benchmarks. Raudra's multi-task design with specialized loss functions and task-specific attention heads consistently yields the most robust performance, demonstrating superior cost-performance trade-offs compared to existing guardrail models and large LLMs.

## Method Summary
The JavelinGuard framework employs five progressively sophisticated transformer architectures built on ModernBERT and related compact encoders. Training uses a balanced corpus of 120K samples from multiple adversarial datasets plus synthetic data, with binary classification for prompt injection and jailbreak detection. Models employ various pooling strategies, with attention-weighted pooling using CLS token as query showing particular promise. Training follows a 5-epoch schedule with batch size 32, learning rate 3×10⁻⁵, and focal loss variants for class imbalance handling. The best-performing Raudra model uses task-specific classification heads with decoupled attention mechanisms and specialized loss weighting, achieving high accuracy while maintaining sub-50ms inference latency on CPU hardware.

## Key Results
- Raudra achieves 96.2% accuracy on JavelinBench with near-perfect F1 scores on fully malicious datasets
- Multi-task architecture with attention-weighted pooling consistently outperforms single-task and mean-pooling baselines
- All models demonstrate low false positive rates (<10%) on benign-only test sets
- Hybrid neural-forest models provide interpretability but trade 2-4× slower inference for transparency

## Why This Works (Mechanism)

### Mechanism 1: Attention-Weighted Pooling with CLS-Guided Query
Using the [CLS] token as a global query against full-sequence keys improves detection of adversarial tokens by computing attention scores that amplify tokens relevant to attack patterns. This allows dynamic focus on salient attack patterns rather than treating all tokens equally.

### Mechanism 2: Multi-Task Decoupling with Task-Specific Attention Heads
Separating classification heads for jailbreak and prompt injection with independent attention-weighting schemes reduces task interference. Each label gets its own token-weighting distribution and skip-connected feed-forward modules, preventing gradient degradation between tasks.

### Mechanism 3: Hybrid Neural-Forest/Boosting for Interpretability
Freezing fine-tuned transformer embeddings and training separate Random Forest or XGBoost classifiers per label provides partial interpretability via feature importance while maintaining competitive accuracy through tree-based partitioning of attack-relevant features.

## Foundational Learning

- **Attention Pooling vs. Mean/CLS Pooling**: Understanding how query-key attention selects informative tokens is essential for debugging detection failures. Quick check: Would attention-weighted pooling assign higher weights to adversarial suffix tokens than mean-pooling?
- **Focal Loss for Class Imbalance**: Critical for adversarial detection where malicious samples are often underrepresented. Quick check: How does increasing gamma affect the loss gradient for well-classified vs. misclassified examples?
- **Multi-Task Learning with Task-Specific Heads**: Raudra's performance advantage stems from decoupled heads. Quick check: What mechanisms can mitigate negative transfer when two tasks have conflicting gradient directions?

## Architecture Onboarding

- **Component map**: Input Prompt → Tokenizer (8192 max) → ModernBERT Encoder → Pooling Layer → Classification Heads → Loss
- **Critical path**: 1) Prepare training data from adversarial datasets and synthetic generation 2) Select ModernBERT-large encoder 3) Train 3-5 epochs with specified hyperparameters 4) Evaluate on 9 benchmarks prioritizing low FPR and high F1
- **Design tradeoffs**: Raudra vs. Mahendra (+2% params, multi-task heads); Vaishnava/Ashwina vs. neural-only (interpretability vs. speed); ModernBERT vs. alternatives (accuracy/speed vs. context length)
- **Failure signatures**: High FPR on NotInject (overfit to trigger words); low F1 on Garak (underfit diverse patterns); slow inference (>50ms, check prediction bottleneck); poor domain transfer (fine-tune on domain data)
- **First 3 experiments**: 1) Train Sharanga baseline to verify data pipeline 2) Ablate Mahendra with/without attention pooling, measure F1 delta 3) Compare Raudra with coupled vs. decoupled heads on per-class accuracy

## Open Questions the Paper Calls Out

- Can state-space models (Mamba) or long-context transformers (LongFormer, Performer) outperform current architectures on adversarial prompts with mid-sequence malicious content?
- Does model distillation preserve Raudra's multi-task discrimination capability while achieving sub-20ms latency on edge hardware?
- Do different tokenization strategies (byte-level BPE, SentencePiece, character-level) measurably impact detection of adversarial suffixes and obfuscated injections?

## Limitations

- JavelinBench dataset is newly introduced and not publicly available, preventing independent verification of main benchmark results
- Training data composition and synthetic data generation parameters are underspecified
- Real-world adversarial robustness beyond benchmark performance lacks systematic validation

## Confidence

**High Confidence (Replication Possible)**: Sharanga and Mahendra architectures can be reproduced; attention-weighted pooling mechanism is well-specified; performance improvements over baselines are measurable on public benchmarks.

**Medium Confidence (Partially Reproducible)**: Raudra's multi-task performance relies on unavailable JavelinBench; hybrid models' interpretability can be verified but accuracy claims require full evaluation suite; cost-performance trade-offs need real-world deployment data.

**Low Confidence (Not Reproducible)**: JavelinBench-specific results cannot be verified; exact training data composition is unclear; real-world generalization claims lack systematic validation.

## Next Checks

1. Implement attention pooling ablation study comparing Sharanga (mean) vs. Mahendra (attention-weighted) on Garak benchmark, measuring F1 differences and analyzing attention weight distributions.

2. Train Raudra with coupled single classification head versus decoupled task-specific heads on held-out test set, comparing per-class accuracy for jailbreak versus prompt injection detection.

3. Train Vaishnava and Ashwina hybrid models, extract feature importance scores, and correlate top-ranked embedding dimensions with known adversarial token patterns to validate interpretability claims.