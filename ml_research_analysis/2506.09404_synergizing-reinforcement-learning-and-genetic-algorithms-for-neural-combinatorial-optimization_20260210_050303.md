---
ver: rpa2
title: Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial
  Optimization
arxiv_id: '2506.09404'
source_url: https://arxiv.org/abs/2506.09404
tags:
- uni00000013
- policy
- learning
- uni00000014
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EAM, a framework that integrates deep reinforcement
  learning with genetic algorithms to improve neural combinatorial optimization. EAM
  generates initial solutions using an RL policy and refines them via genetic operators
  (crossover and mutation), then reinjects these evolved solutions into policy training
  to enhance exploration and accelerate convergence.
---

# Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization

## Quick Facts
- arXiv ID: 2506.09404
- Source URL: https://arxiv.org/abs/2506.09404
- Reference count: 40
- The paper introduces EAM, a framework that integrates deep reinforcement learning with genetic algorithms to improve neural combinatorial optimization

## Executive Summary
This paper presents EAM (Evolution-Guided Actor-Critic for Neural Combinatorial Optimization), a framework that synergizes deep reinforcement learning with genetic algorithms to enhance neural combinatorial optimization. EAM generates initial solutions using an RL policy and refines them through genetic operators (crossover and mutation), then reinjects these evolved solutions into policy training to improve exploration and accelerate convergence. The approach is theoretically grounded with an established upper bound on KL divergence for stable updates, and is validated across multiple combinatorial optimization problems including TSP, CVRP, PCTSP, and OP.

## Method Summary
EAM integrates evolutionary computation with reinforcement learning by using an RL policy to generate initial solutions that are then refined through genetic operators (crossover and mutation). The framework maintains an elite pool of high-quality solutions that are periodically injected back into the policy training process, enhancing exploration and preventing premature convergence. Theoretical analysis establishes an upper bound on KL divergence between policy distributions to ensure stable updates during training. The method is designed to be model-agnostic, allowing integration with various existing neural CO solvers like AM, POMO, and SymNCO without increasing inference time.

## Key Results
- EAM consistently improves solution quality and training efficiency across TSP, CVRP, PCTSP, and OP benchmarks
- The framework demonstrates faster convergence and better exploration compared to pure RL approaches
- EAM achieves competitive results without increasing inference time when integrated with existing neural CO solvers

## Why This Works (Mechanism)
The paper's mechanism leverages the complementary strengths of RL and genetic algorithms: RL provides efficient local search capabilities through learned policies, while genetic algorithms enable global exploration through crossover and mutation operations. By maintaining an elite pool of solutions and periodically injecting evolved solutions back into policy training, EAM creates a feedback loop that enhances both exploration and exploitation. The theoretical KL divergence bound ensures stable policy updates during this integration process.

## Foundational Learning
- **Reinforcement Learning Basics**: Why needed - forms the foundation of solution generation; Quick check - understand policy gradient methods and actor-critic architectures
- **Genetic Algorithm Operators**: Why needed - crossover and mutation drive solution evolution; Quick check - familiarity with selection, crossover, and mutation mechanisms
- **Combinatorial Optimization Problems**: Why needed - context for evaluating solution quality; Quick check - understand TSP, CVRP, and their solution representations
- **KL Divergence Theory**: Why needed - ensures stable policy updates during integration; Quick check - grasp the mathematical proof of the upper bound

## Architecture Onboarding

**Component Map**: Policy Network -> Solution Generator -> Genetic Operators -> Elite Pool -> Policy Update -> Training Loop

**Critical Path**: The critical execution path follows: RL policy generates initial solutions → genetic operators refine solutions → elite solutions update policy → improved policy generates new solutions

**Design Tradeoffs**: The framework balances exploration (genetic operators) with exploitation (RL policy learning), choosing to maintain an elite pool rather than pure population-based approaches to minimize computational overhead during inference

**Failure Signatures**: Poor solution quality may indicate inadequate genetic operator design or insufficient elite pool diversity; slow convergence could signal overly conservative KL divergence bounds or ineffective solution reinjection

**First Experiments**: 1) Integrate EAM with a simple RL baseline on TSP to verify basic functionality; 2) Perform ablation studies on crossover vs. mutation operators; 3) Test the framework's sensitivity to elite pool size and update frequency

## Open Questions the Paper Calls Out
None identified in the provided material

## Limitations
- The practical impact of the KL divergence upper bound on convergence speed and stability across diverse problem domains is not empirically validated
- The mechanism by which evolved solutions improve policy learning for problems with different solution structures is not fully explored
- Computational overhead during training relative to baseline RL-only methods is not quantified

## Confidence
- Solution quality improvements: High
- Theoretical KL divergence bound: Medium
- Training efficiency gains: Medium

## Next Checks
1. Evaluate EAM's performance on larger-scale TSP instances (e.g., 1000+ nodes) to assess scalability
2. Conduct ablation studies isolating the impact of crossover vs. mutation operators on solution quality and convergence
3. Measure the actual training time overhead introduced by EAM's genetic operators compared to pure RL baselines