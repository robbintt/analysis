---
ver: rpa2
title: 'Optimizing Humor Generation in Large Language Models: Temperature Configurations
  and Architectural Trade-offs'
arxiv_id: '2504.02858'
source_url: https://arxiv.org/abs/2504.02858
tags:
- humor
- temperature
- generation
- https
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates 13 large language models across
  715 configurations to identify optimal settings for generating technically relevant
  humor for software developers. The research addresses the lack of comprehensive
  comparisons of model architectures and temperature parameters for domain-specific
  humor creation.
---

# Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs

## Quick Facts
- arXiv ID: 2504.02858
- Source URL: https://arxiv.org/abs/2504.02858
- Authors: Evgenii Evstafev
- Reference count: 0
- Primary result: Temperature and model selection optimization for software developer humor generation

## Executive Summary
This study systematically evaluates 13 large language models across 715 configurations to identify optimal settings for generating technically relevant humor for software developers. Using a full factorial experimental design with five weighted evaluation criteria, the research reveals significant performance variations, with Gemini variants achieving 21.8% superiority over baseline systems. Temperature sensitivity analysis shows that 73% of models achieve peak performance at lower stochasticity settings (≤0.5), with architecture-specific optimal ranges. The findings establish evidence-based guidelines for model selection and configuration, demonstrating that compact models like gemini-1.5-flash-8b achieve 93.8% of maximum scores using only 5% of median token counts.

## Method Summary
The research employs a full factorial experimental design testing 13 models (Gemini, Mistral, Deepseek, Llama) across 11 temperature settings (0.0-1.0 in 0.1 increments) and 5 prompt templates, generating 715 configurations. Evaluation uses a composite score based on five weighted criteria: HumorCore (0.9), DomainRelevance (0.8), ConceptOriginality (0.7), TonePrecision (1.0), and DeliveryEfficiency (0.6). Generation parameters include max_tokens=4500, top_p=0.95, frequency_penalty=0.0, presence_penalty=0.0, with automated scoring via deepseek-reasoner at T=0.7. The study identifies optimal temperature ranges per architecture and quantifies efficiency-performance tradeoffs across model sizes.

## Key Results
- Gemini variants outperform baseline systems by 21.8% across evaluation criteria
- 73% of models achieve peak humor generation performance at temperature settings ≤0.5
- Model architecture explains 38.7% of performance variance in humor generation tasks
- gemini-1.5-flash-8b achieves 93.8% of maximum scores using only 5% of median token counts

## Why This Works (Mechanism)
None

## Foundational Learning
- **Temperature parameter in LLM sampling**: Controls randomness in token selection; lower values produce deterministic outputs while higher values increase creativity and unpredictability. Why needed: Temperature directly impacts humor quality and originality in generation.
- **Full factorial experimental design**: Tests all possible combinations of factors (models × temperatures × prompts) to identify interactions and optimal configurations. Why needed: Ensures comprehensive coverage of parameter space without missing critical interactions.
- **Automated evaluation metrics for humor**: Uses transformer-based classifiers for punchline detection, BERTScore for originality, and hate speech classifiers for tone assessment. Why needed: Enables scalable evaluation of 715 configurations without manual annotation bottlenecks.
- **Weighted composite scoring**: Combines multiple evaluation criteria with different importance weights to create a single performance metric. Why needed: Balances competing objectives like humor quality, relevance, and efficiency in a unified framework.

## Architecture Onboarding

**Component Map**
Prompt Templates → LLM Generator (13 models) → Temperature Settings (11 values) → Output Generation → Automated Evaluator (HumorCore, DomainRelevance, ConceptOriginality, TonePrecision, DeliveryEfficiency) → Composite Score

**Critical Path**
Prompt selection → Model instantiation → Temperature configuration → Generation with specified parameters → Automated scoring pipeline → Composite score calculation → Performance analysis

**Design Tradeoffs**
- Temperature vs. creativity: Lower temperatures (≤0.5) favor precision but may limit originality; higher temperatures increase creativity but risk coherence
- Model size vs. efficiency: Larger models achieve higher scores but consume more tokens; compact models like gemini-1.5-flash-8b offer 93.8% performance with 5% token usage
- Automation vs. human judgment: Automated evaluation enables 715 configurations but may miss nuanced humor aspects captured by human evaluation

**Failure Signatures**
- Score drift when using different evaluator models or temperatures
- API timeouts causing incomplete configuration runs
- Performance degradation when prompts deviate from specified format requirements

**First Experiments**
1. Test temperature sensitivity on a single model-prompt pair to verify the 73% peak performance finding at ≤0.5
2. Compare automated scores with human ratings on a small subset to validate evaluator reliability
3. Evaluate gemini-1.5-flash-8b efficiency claim by measuring token counts and scores across the temperature range

## Open Questions the Paper Calls Out
- **Dynamic temperature scheduling**: The paper explicitly calls for testing dynamic temperature scheduling during generation sequences, as all 715 configurations used static temperature throughout generation. Evidence needed: A/B tests comparing fixed vs. phase-based temperature protocols.
- **Optimal temperature granularity**: The paper notes temperature analysis was limited to 11 discrete values and calls for continuous scaling beyond 0.1 increments, as 68% of peak performances occurred in the narrow 0.0-0.2 range. Evidence needed: Fine-grained temperature sweeps (Δ=0.01) around identified peaks.
- **Human correlation validation**: The paper's automated evaluation pipeline lacks validation against human perception of humor quality, as weights were derived from prior human studies but actual outputs weren't evaluated by humans. Evidence needed: Human annotation study comparing automated scores to expert/user ratings.

## Limitations
- Evaluation pipeline dependency on deepseek-reasoner at T=0.7 creates potential single point of failure for score reproducibility
- Prompt templates not included in paper text, only referenced via gist links, making assessment of prompt engineering impact difficult
- Model architecture explanations are surface-level, with only 38.7% of performance variance explained by architecture alone

## Confidence
- **High Confidence**: Temperature sensitivity findings (73% peak at ≤0.5) and architectural performance gaps (Gemini 21.8% superior) are robust and less sensitive to evaluation model variations
- **Medium Confidence**: The 93.8% efficiency claim for gemini-1.5-flash-8b assumes perfect reproducibility of the evaluation pipeline and is vulnerable to evaluator model differences
- **Medium Confidence**: The correlation between humor quality and concept originality (r=0.75) depends on BERTScore implementation details not fully specified

## Next Checks
1. **Evaluator Consistency Check**: Reproduce 10% of the dataset using a different evaluator model (e.g., GPT-4) and compute Intraclass Correlation Coefficient (ICC); ICC≥0.98 indicates evaluator stability, ICC<0.85 suggests systematic bias requiring pipeline adjustment
2. **Prompt Template Verification**: Obtain the exact 5 prompt templates from the referenced gist and conduct a prompt ablation study varying only the phrasing while keeping format constraints constant; identify if template variations explain >15% of performance variance
3. **Temperature Transferability Test**: Apply the identified optimal temperature ranges (≤0.5 for 73% of models) to a held-out set of software development humor prompts not used in the original study; verify that performance gains transfer with <10% degradation to confirm temperature findings are not dataset-specific