---
ver: rpa2
title: 'Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical
  Entity'
arxiv_id: '2512.17769'
source_url: https://arxiv.org/abs/2512.17769
tags:
- medical
- bangla
- entity
- bert
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of medical entity recognition
  (MedER) for Bangla, a low-resource language with limited annotated datasets. The
  authors propose a Multi-BERT Ensemble approach that combines two BERT layers with
  different input sequences to improve entity recognition accuracy.
---

# Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity

## Quick Facts
- arXiv ID: 2512.17769
- Source URL: https://arxiv.org/abs/2512.17769
- Reference count: 22
- Multi-BERT Ensemble achieved 89.58% accuracy on Bangla medical entity recognition

## Executive Summary
This paper addresses the challenge of medical entity recognition (MedER) for Bangla, a low-resource language with limited annotated datasets. The authors propose a Multi-BERT Ensemble approach that combines two BERT layers with different input sequences to improve entity recognition accuracy. The model was evaluated on a newly developed Bangla MedER dataset containing 6,895 observations across six medical entity classes. The ensemble approach achieved an accuracy of 89.58%, outperforming baseline models like BERT (77.78%), DistilBERT (57.89%), ELECTRA (55.54%), and RoBERTa (51.67%).

## Method Summary
The Multi-BERT Ensemble architecture uses two Bangla BERT layers processing different input sequences—one with text followed by entity, and the other with entity followed by text. The outputs from both BERT layers are concatenated and passed through a fully connected layer with softmax classification. The model was trained on a custom Bangla MedER dataset with 6,895 observations across six entity classes: Medicine/Chemical Names, Disease, Common Medical Terms, Hormone, Pharmacological Class, and Organ. Preprocessing included character removal, Bangla stopword filtering, and stemming/lemmatization. The model was trained with learning rate 2e-4, batch size 32, maximum sequence length 484, and 40 epochs using AdamW optimizer.

## Key Results
- Multi-BERT Ensemble achieved 89.58% accuracy, 11.80% improvement over single BERT
- Outperformed baseline models: DistilBERT (57.89%), ELECTRA (55.54%), RoBERTa (51.67%)
- Macro F1 score of 86.55% and Micro F1 score of 87.87%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-input ensemble captures complementary positional representations of text-entity relationships.
- Mechanism: Two BERT layers process swapped input sequences—one receives `[CLS] text [SEP] entity [PAD]`, the other receives `[CLS] entity [SEP] text [PAD]`. Outputs are concatenated and passed through a fully connected layer with softmax classification. This forces the model to learn bidirectional context between medical statements and entity labels.
- Core assumption: Positional variation in input sequences exposes different attention patterns that improve generalization.
- Evidence anchors:
  - [abstract] "combines two BERT layers with different input sequences to improve entity recognition accuracy"
  - [section] "For BERT layer one, the [SEP] token follows the text, and entities are added after the [SEP] token. We have created the opposite combination for the input sequence for the BERT layer two."
- Break condition: If entity and text representations are near-identical regardless of position (e.g., highly correlated embeddings), concatenation provides minimal signal gain.

### Mechanism 2
- Claim: Ensemble averaging reduces single-model biases from pretraining corpus limitations.
- Mechanism: Each BERT layer independently attends to token relationships. Concatenation aggregates learned weights before classification, potentially canceling noise from any single attention head's over-reliance on spurious patterns.
- Core assumption: The two input orderings induce sufficiently different attention distributions that their combination improves over either alone.
- Evidence anchors:
  - [abstract] "11.80% accuracy improvement over the single-layer BERT model"
  - [section] "single-model approaches often suffer from inherent biases and limitations of individual pretraining corpora"
- Break condition: If both BERT layers converge to nearly identical attention patterns despite different inputs, ensemble provides only marginal improvement.

### Mechanism 3
- Claim: Task-specific fine-tuning on domain data compensates for lack of Bangla biomedical pretraining.
- Mechanism: No pretrained Bangla biomedical transformer exists. Fine-tuning a general Bangla-BERT on 6,895 medical statements forces adaptation to clinical terminology, code-switching, and colloquial expressions through gradient updates to all layers.
- Core assumption: The dataset quality and size are sufficient to shift representations toward medical semantics without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "developed a high-quality dataset tailored for the Bangla MedER task"
  - [section] "current models fail to effectively capture clinical terminology, English–Bangla code-mixing, and diverse morphological patterns"
- Break condition: If medical entity vocabulary significantly exceeds the dataset's coverage, out-of-distribution terms will degrade performance.

## Foundational Learning

- Concept: **BERT input formatting with special tokens**
  - Why needed here: The ensemble relies on correctly structuring `[CLS]`, `[SEP]`, and `[PAD]` tokens for two different sequence orderings. Misplacement breaks the attention mechanism's segment differentiation.
  - Quick check question: Given text "অ্যালার্জির লক্ষণ" and entity "Disease", what is the correct input sequence for BERT Layer 2?

- Concept: **Concatenation-based ensemble fusion**
  - Why needed here: The model joins outputs from two BERT layers before classification. Understanding dimension alignment is critical for debugging.
  - Quick check question: If each BERT layer outputs a 768-dimensional vector for the `[CLS]` token, what is the dimension after concatenation?

- Concept: **Class imbalance in medical NER**
  - Why needed here: The dataset has uneven class distribution (1,938 Medicine vs. 807 Hormone entities), affecting per-class recall.
  - Quick check question: Why might "Hormone" achieve only 75% F1 despite overall 89.58% accuracy?

## Architecture Onboarding

- Component map:
  - Input preprocessing → BERT tokenizer → Dual sequence generator → [BERT Layer 1, BERT Layer 2] → Concatenation → Fully connected layer → Softmax → Entity class
  - Each BERT layer receives independently tokenized input with different text-entity ordering

- Critical path:
  1. Verify preprocessing removes Bangla stop words and normalizes spelling variations
  2. Confirm both input sequences are generated correctly with swapped positions
  3. Check that concatenation preserves gradient flow to both BERT layers during backprop

- Design tradeoffs:
  - Computational cost: Two BERT forward passes per sample doubles inference time vs. single BERT
  - Memory: Concatenated representation requires larger fully connected layer
  - Dataset size: 6,895 samples may be insufficient for robust ensemble training without overfitting

- Failure signatures:
  - Near-identical outputs from both BERT layers → ensemble gain <2%
  - Low recall on minority classes (Hormone, Pharmacological Class) → class imbalance not addressed
  - High training loss plateau → learning rate too high or sequences malformed

- First 3 experiments:
  1. Ablation: Run single BERT with each input ordering separately to quantify which contributes more to improvement.
  2. Class balance: Apply weighted loss or oversampling to Hormone class; measure F1 change.
  3. Cross-validation: Perform 5-fold CV to assess whether 89.58% accuracy is stable or dataset-split dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-lingual transfer learning effectively mitigate the semantic limitations imposed by the lack of domain-specific pre-trained Bangla biomedical models?
- Basis in paper: [explicit] The authors explicitly state in the "Future Work" section their intent to "use cross-lingual transfer learning, which means learning from other languages to enhance our Bangla model."
- Why unresolved: While the current ensemble uses Bangla-pretrained BERT, the paper notes the absence of a specific biomedical pre-training corpus for Bangla, limiting semantic comprehension of complex clinical terminology.
- What evidence would resolve it: A comparative study showing performance gains when the model is initialized with weights from a high-resource biomedical model (e.g., BioBERT) versus a monolingual Bangla model.

### Open Question 2
- Question: To what extent does the model's inability to distinguish between "Disease" and "Common Medical Terms" stem from semantic overlap versus tokenization limitations?
- Basis in paper: [inferred] The confusion matrix in the "Result" section reveals significant misclassification between "Disease" and "Common Medical Terms," while the "Challenges" section notes difficulties with uneven spellings and colloquial idioms.
- Why unresolved: The paper identifies the error rates but does not conduct a detailed error analysis to determine if the confusion is caused by the model's architecture or linguistic ambiguities in the dataset.
- What evidence would resolve it: A qualitative error analysis of the specific misclassified tokens to determine if they are distinct concepts or if the labeling guidelines for these two classes are ambiguous.

### Open Question 3
- Question: Can the Multi-BERT Ensemble architecture be optimized for real-time clinical applications without sacrificing the accuracy gained through model complexity?
- Basis in paper: [explicit] The "Challenges" section explicitly notes that the "MultiBERT approach... is computationally heavy" and requires "a large amount of processing power," which makes "rapid experiments" and implied real-time use difficult.
- Why unresolved: The current study prioritized accuracy (89.58%) over latency, but practical deployment in clinical settings requires models that are both fast and accurate.
- What evidence would resolve it: Benchmarking the ensemble's inference latency against a distilled or quantized version to identify if a efficient trade-off exists.

## Limitations
- Small dataset size (6,895 observations) may limit generalization for a morphologically complex language
- Absence of dedicated Bangla biomedical pretraining corpus restricts semantic understanding of clinical terminology
- Significant class imbalance affects minority class performance (Hormone F1: 75% vs Medicine F1: 94.6%)

## Confidence

**High Confidence (Likelihood >80%)**: The claim that Multi-BERT Ensemble outperforms single BERT models is supported by the reported 11.80% accuracy improvement and the logical mechanism of ensemble averaging reducing individual model biases. The dual-input sequence approach is technically sound and well-documented in the methodology.

**Medium Confidence (Likelihood 50-80%)**: The claim that the ensemble captures complementary positional representations has theoretical support but lacks ablation studies proving the dual-sequence mechanism specifically drives improvement rather than general ensemble effects. The paper assumes different attention patterns emerge from sequence swapping, but this requires empirical validation.

**Low Confidence (Likelihood <50%)**: The generalizability claim to other low-resource medical NER tasks is weakly supported. The paper's success with a specific dataset and architecture does not establish broader applicability without testing on diverse low-resource languages or medical domains.

## Next Checks
1. **Ablation study of sequence contributions**: Train and evaluate each BERT layer separately with its respective input ordering (text→entity vs entity→text) to determine whether the ensemble improvement stems from complementary learning or simply averaging two models. Compare performance metrics and attention pattern analysis to quantify individual contributions.

2. **Class imbalance mitigation validation**: Implement weighted loss functions or oversampling techniques specifically targeting the under-represented Hormone class. Measure F1-score changes to determine whether ensemble benefits are distributed across all classes or concentrated in majority classes, revealing potential bias exploitation.

3. **Cross-dataset generalization test**: Evaluate the trained model on an independent Bangla medical entity dataset or related low-resource NER benchmark. This validates whether the 89.58% accuracy represents genuine entity recognition capability or dataset-specific memorization, particularly important given the small training set size.