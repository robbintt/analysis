---
ver: rpa2
title: 'Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware
  Modulation'
arxiv_id: '2510.18478'
source_url: https://arxiv.org/abs/2510.18478
tags:
- safety
- cost
- reward
- critic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses over-conservatism in safety critics for reinforcement
  learning, which flattens cost landscapes and degrades policy learning. The proposed
  Uncertain Safety Critic (USC) integrates uncertainty-aware modulation into critic
  training, concentrating conservatism in uncertain and costly regions while preserving
  sharp gradients in safe areas.
---

# Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation

## Quick Facts
- arXiv ID: 2510.18478
- Source URL: https://arxiv.org/abs/2510.18478
- Reference count: 40
- The paper proposes USC to reduce over-conservatism in safety critics, achieving ~40% fewer safety violations while maintaining or improving reward

## Executive Summary
The paper addresses over-conservatism in safety critics for reinforcement learning, which flattens cost landscapes and degrades policy learning. The proposed Uncertain Safety Critic (USC) integrates uncertainty-aware modulation into critic training, concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas. USC achieves approximately 40% reduction in safety violations while maintaining competitive or higher rewards, and reduces error between predicted and true cost gradients by approximately 83% compared to the state-of-the-art.

## Method Summary
USC modifies the Conservative Safety Critic (CSC) framework by integrating Gauss-Newton influence scores to estimate epistemic uncertainty per sample. The safety critic loss is modulated by uncertainty-weighted conservative penalties, amplifying overestimation only for samples that are both uncertain and above-average cost. An uncertainty refinement step interpolates high-uncertainty samples with confident nearest neighbors to reduce epistemic uncertainty in sparsely represented regions. The method operates within a CMDP framework with dual variable updates and uses uniform action sampling in the conservative penalty to maintain stronger pessimism where the policy would otherwise avoid exploration.

## Key Results
- Approximately 40% reduction in safety violations compared to baseline CSC
- Gradient error between predicted and true cost gradients reduced by ~83%
- Maintains competitive or higher rewards across Safety Gym, FetchReach, and HalfCheetah benchmarks
- Entropy error in cost map reduced from ~5.0 (CSC) to ~2.4 (USC)

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Weighted Conservative Modulation
Modulating conservative overestimation by epistemic uncertainty preserves sharp gradients in safe regions while concentrating conservatism where underestimation is most risky. Gauss-Newton influence scores u(s,a) quantify parameter-space sensitivity per sample, and the adjusted weight ũ(s,a) = log(1+u) × (1 + 1{Q_C > batch_mean}) amplifies the log-sum-exp conservative penalty only for samples that are both uncertain and above-average cost.

### Mechanism 2: Trust-Region Uncertainty Refinement via Neighbor Interpolation
Interpolating high-uncertainty samples with confident nearest neighbors reduces epistemic uncertainty in sparse state–action regions, preventing persistent over-penalization. After each gradient step, top uncertain samples are selected and synthetic targets are generated via inverse-distance-weighted interpolation from K confident neighbors, with a trust-region loss enforcing consistency while a KL penalty bounds deviation.

### Mechanism 3: Gradient Structure Preservation via Uniform Action Sampling
Sampling alternative actions uniformly over the action space yields strictly larger conservative penalties in expectation, maintaining pessimism where the policy would otherwise avoid exploring. Since the policy learns to partially avoid high-cost regions, uniform sampling is more likely to draw risky actions, increasing the LSE margin and thus the gradient signal.

## Foundational Learning

- **Gauss-Newton Influence Functions**: Used to estimate epistemic uncertainty without ensembles; requires understanding of how parameter gradients relate to sample influence. Quick check: Can you explain why the Gauss-Newton approximation avoids computing the full Hessian while remaining positive semi-definite?

- **Conservative Safety Critics (CSC)**: USC modifies the CSC objective; understanding the base overestimation penalty (log-sum-exp over random actions) is prerequisite. Quick check: Why does uniform action sampling produce stronger conservative penalties than policy sampling when the policy avoids risk?

- **Lagrangian Constrained RL**: USC integrates into a CMDP framework with dual variable λ balancing reward and cost; the critic gradients directly affect policy updates. Quick check: If λ becomes very large, what happens to the gradient structure from the safety critic, and how does USC mitigate this?

## Architecture Onboarding

- **Component map**: Environment -> Replay Buffer B -> Uncertainty Estimator (u(s,a)) -> USC Loss L_C -> Q_C -> Actor π -> Environment
- **Critical path**: 1) Environment step → transition stored in B; 2) Sample minibatch → compute influence u(s,a) for each; 3) Compute ũ(s,a) → evaluate L_C with modulated conservative penalty; 4) Update Q_C parameters; 5) Identify top-n uncertain samples → find K neighbors → compute ĉ_u; 6) Apply trust-region refinement update to Q_C; 7) Update actor via L_π and λ via constraint violation
- **Design tradeoffs**: n (uncertain samples for refinement) vs. compute; K (neighbors for interpolation) vs. local structure preservation; β (trust-region penalty) vs. adaptation speed; uniform sampling size m vs. conservatism strength
- **Failure signatures**: Cost map remains diffuse → check if ũ(s,a) is saturating; Policy stalls with low reward → λ may be too high; Refinement introduces instability → KL penalty threshold ε or coefficient β may be too permissive
- **First 3 experiments**: 1) Ablation without refinement (USC-NR) on CarGoal2 to isolate uncertainty modulation vs. refinement contribution; 2) Gradient error analysis computing Gradient MSE vs. ground truth cost map; 3) Reliability calibration plotting prediction error vs. predicted uncertainty

## Open Questions the Paper Calls Out
- Future work will investigate extending USC to support partially observable and adversarial environments
- The paper does not evaluate binary cost scenarios, leaving open how USC performs with discrete safety signals

## Limitations
- Effectiveness of Gauss-Newton influence as an epistemic uncertainty proxy in highly non-convex cost landscapes remains unverified
- Refinement hyperparameters (n, K, β, ε) are unspecified and could materially affect performance
- The uniform sampling size m for conservative penalty is not provided, creating ambiguity in conservatism strength

## Confidence
- **High confidence**: Reduction in safety violations (~40%) and gradient MSE (~83%) are well-supported by ablation studies and error metrics
- **Medium confidence**: Mechanism 2 (neighbor interpolation) is theoretically sound but lacks empirical validation in the corpus
- **Low confidence**: The claim that uniform action sampling preserves gradient structure better than policy sampling relies on assumptions about policy behavior not fully verified across tasks

## Next Checks
1. **Sensitivity Analysis**: Systematically vary n, K, β, and m to quantify their impact on safety violation reduction and gradient preservation
2. **Discontinuity Robustness**: Test USC on tasks with sharp cost boundaries to assess interpolation-induced errors
3. **Influence Calibration**: Validate Gauss-Newton influence scores against alternative uncertainty estimators to confirm correlation with prediction error