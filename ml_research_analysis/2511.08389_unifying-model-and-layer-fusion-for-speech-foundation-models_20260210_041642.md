---
ver: rpa2
title: Unifying Model and Layer Fusion for Speech Foundation Models
arxiv_id: '2511.08389'
source_url: https://arxiv.org/abs/2511.08389
tags:
- fusion
- speech
- hconv
- tasks
- upstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for model and layer fusion
  of speech foundation models, addressing the limitation that previous work treated
  these two fusion strategies separately. The authors introduce an interface module
  that enables joint fusion across multiple upstream speech models while integrating
  information across their layers.
---

# Unifying Model and Layer Fusion for Speech Foundation Models

## Quick Facts
- arXiv ID: 2511.08389
- Source URL: https://arxiv.org/abs/2511.08389
- Authors: Yi-Jen Shih; David Harwath
- Reference count: 34
- Primary result: Unified model and layer fusion framework outperforms sequential fusion baselines on ASR, speaker verification, and emotion recognition tasks

## Executive Summary
This paper introduces a unified framework for jointly fusing both multiple speech foundation models and their layers through a single interface module. The method addresses the limitation of prior work that treated model and layer fusion as separate sequential processes. By integrating information across layers from multiple upstream models simultaneously, the framework achieves state-of-the-art performance across diverse speech tasks including speech recognition, speaker verification, and emotion recognition. The approach demonstrates particular effectiveness when fusing self-supervised and supervised models, which learn complementary aspects of speech representations.

## Method Summary
The method extracts hidden states from all layers of N upstream speech foundation models, then aligns these representations in time and dimension through interpolation. The aligned representations are merged either through addition (HConv) or concatenation (CHConv), followed by a hierarchical convolution interface that processes the layer and model dimensions. The upstream models remain frozen while only the interface and downstream task-specific heads are trained. The framework uses English Whisper for English tasks and multilingual Whisper for multilingual applications.

## Key Results
- HConv interface consistently outperforms Weighted Sum fusion baseline, with performance gaps widening for larger models
- Best results achieved when fusing self-supervised with supervised models, indicating complementary learning
- Performance plateaus when fusing 3+ models, suggesting diminishing returns from model redundancy
- Proposed method shows significant gains on ASR tasks but variable performance on non-ASR tasks

## Why This Works (Mechanism)

### Mechanism 1
Joint optimization of model and layer fusion within a single interface module outperforms sequential strategies by allowing the downstream task to learn cross-model and cross-layer correlations directly. The framework treats fusion as a unified mapping problem rather than sequential layer-then-model fusion.

### Mechanism 2
Fusing self-supervised and supervised models captures complementary information better than homogeneous fusion because these models optimize for different objective functions, learning heterogeneous representations of the same signal.

### Mechanism 3
Hierarchical Convolution prevents "information collision" that occurs in standard Weighted Sum fusion by applying selective filtering and combination through 1D convolutions across layer and model dimensions, preserving distinct acoustic and linguistic information.

## Foundational Learning

**Speech Foundation Models (SFMs) & Upstream/Downstream Paradigm**
- Why needed: The entire architecture relies on freezing "Upstream" encoders and training a lightweight "Interface" + "Downstream" head
- Quick check: Can you explain why the authors freeze the upstream models instead of fine-tuning them during the fusion experiments?

**Tensor Alignment (Time & Dimension)**
- Why needed: The proposed Interface requires inputs $L$ (layers), $T$ (time), and $D$ (dimension) to match
- Quick check: If fusing Model A (50Hz) with Model B (100Hz), which framerate must Model A be adjusted to, according to the paper's strategy?

**Ensemble Diversity vs. Redundancy**
- Why needed: The paper notes that fusing 3 models didn't improve over 2, hypothesizing redundancy
- Quick check: Why might fusing two SSL models result in less performance gain than fusing an SSL model with a Supervised model?

## Architecture Onboarding

- **Component map:** Upstreams -> Pre-Processor -> Merger -> Interface -> Downstream
- **Critical path:** The dimension matching in the Pre-Processor. If the framerate or layer interpolation is misconfigured, the convolution in the Interface will fail or misalign temporal data.
- **Design tradeoffs:** HConv adds hidden states (parameter efficient). CHConv concatenates them (higher capacity, higher compute). The paper uses HConv as the main comparable baseline to keep parameter counts fair.
- **Failure signatures:** Gumbel dimension selection degrades speaker verification performance significantly (>2Ã— EER). Domain mismatch causes performance drops when using monolingual upstreams for multilingual tasks.
- **First 3 experiments:**
  1. Run a single upstream (e.g., HuBERT) on ASR using Weighted Sum vs. HConv to verify the "information collision" hypothesis
  2. Fuse Whisper (SL) + WavLM (SSL) using HConv on a non-ASR task (e.g., Emotion Rec) to validate "complementary information"
  3. Attempt 3-model fusion to verify the redundancy issue (lack of improvement over 2 models)

## Open Questions the Paper Calls Out

**Open Question 1**
Does incorporating structurally diverse models (e.g., non-Transformer architectures) overcome the performance plateau observed when fusing three or more models? The experiments primarily fused Transformer-based models with similar pre-training objectives, likely resulting in redundant representations.

**Open Question 2**
Can joint distillation effectively transfer the performance benefits of the unified fusion interface to a single student model to mitigate inference costs? The current method requires running inference on all constituent upstream models, creating a computational bottleneck.

**Open Question 3**
Is the suboptimal performance of the HConv interface on multilingual ASR strictly caused by the language mismatch between English-only SSL upstreams and multilingual targets? The authors did not test this hypothesis by swapping monolingual upstreams for multilingual ones.

## Limitations
- HConv interface design details are underspecified, creating reproduction challenges
- Performance benefits plateau with increasing model count, suggesting diminishing returns
- Variable effectiveness across tasks with insufficient explanation of task-specific differences

## Confidence

**High Confidence:**
- Unified framework architecture is technically sound and experimental methodology is rigorous
- Baseline comparison results showing HConv outperforms Weighted Sum fusion are reproducible
- Observation that SSL + SL fusion outperforms homogeneous fusion is well-supported

**Medium Confidence:**
- Joint optimization of model and layer fusion is superior to sequential fusion
- HConv prevents information collision mechanism
- Scalability claims to larger models based on experimental evidence

**Low Confidence:**
- Exact performance gains vs. ablations due to incomplete architectural specifications
- Generalizability of SSL/SL complementarity across all speech domains
- Optimal number of upstream models given redundancy concerns

## Next Checks

1. Reproduce the HConv architecture by implementing the interface module with explicit convolutional layer specifications and verify it outperforms the Weighted Sum baseline on a single upstream model.

2. Test the complementarity hypothesis by fusing two SSL models versus one SSL and one SL model on a non-ASR task like Emotion Recognition, measuring whether heterogeneous model fusion consistently provides gains.

3. Validate the scalability claims by implementing 3-model fusion and measuring whether performance plateaus or degrades compared to 2-model fusion, investigating whether redundancy or computational complexity drives the results.