---
ver: rpa2
title: Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost
  Subsidy
arxiv_id: '2501.10290'
source_url: https://arxiv.org/abs/2501.10290
tags:
- bound
- regret
- cost
- reward
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces and analyzes new variants of the Multi-Armed
  Bandit with Cost Subsidy (MAB-CS) framework, focusing on minimizing cumulative costs
  while maintaining a reward constraint. The authors propose two novel settings: the
  fixed threshold setting, where the reward threshold is a known constant, and the
  known reference arm setting, where the threshold is a known fraction of a reference
  arm''s reward.'
---

# Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy

## Quick Facts
- **arXiv ID:** 2501.10290
- **Source URL:** https://arxiv.org/abs/2501.10290
- **Reference count:** 40
- **Primary result:** Introduces Pairwise Elimination (PE) and PE-CS algorithms for MAB-CS, achieving logarithmic instance-dependent cost and quality regret guarantees.

## Executive Summary
This paper introduces and analyzes new variants of the Multi-Armed Bandit with Cost Subsidy (MAB-CS) framework, focusing on minimizing cumulative costs while maintaining a reward constraint. The authors propose two novel settings: the fixed threshold setting, where the reward threshold is a known constant, and the known reference arm setting, where the threshold is a known fraction of a reference arm's reward. They introduce the Pairwise Elimination (PE) algorithm for the known reference arm setting and generalize it to PE-CS for the subsidized best reward setting. The key contribution is the instance-dependent analysis of PE and PE-CS, demonstrating logarithmic upper bounds on cost and quality regret, making these algorithms the first to achieve such guarantees. The paper also establishes that PE is order-optimal for all known reference arm problem instances by comparing upper and lower bounds. Experimental results on real-world datasets (MovieLens 25M and Goodreads) validate the effectiveness of PE and show that PE-CS offers a superior balance between performance and reliability compared to baselines from the literature.

## Method Summary
The paper addresses the Multi-Armed Bandit with Cost Subsidy (MAB-CS) problem, aiming to minimize cumulative cost while satisfying a reward constraint. Three settings are tested: Fixed Threshold, Known Reference Arm, and Subsidized Best Reward. Data is prepared by mapping dataset items (MovieLens 25M and Goodreads) to arms by genre, computing arm rewards as average ratings divided by 5, and sampling arm costs uniformly at random from U(0,1). The core algorithms are PE (Pairwise Elimination), which maintains round counters and eliminates arms based on confidence intervals, and PE-CS, which uses a Best-Arm Identification (BAI) stage followed by PE. Baselines include ETC-CS, UCB-CS, and TS-CS. The objective is to minimize the sum of Cost Regret and Quality Regret over a horizon T=5,000,000.

## Key Results
- PE and PE-CS achieve logarithmic instance-dependent upper bounds on cost and quality regret.
- PE is established as order-optimal for all known reference arm problem instances.
- PE-CS provides a superior balance between performance and reliability compared to literature baselines on MovieLens 25M and Goodreads datasets.

## Why This Works (Mechanism)
The Pairwise Elimination (PE) algorithm works by maintaining a set of active arms and iteratively comparing candidates against a reference arm in rounds with increasing sample thresholds. This allows for adaptive elimination based on statistical confidence, ensuring that suboptimal arms are removed while maintaining the reward constraint. The PE-CS extension first identifies the best arm via a BAI stage, then uses it as the reference in PE, guaranteeing a known threshold for the reward constraint.

## Foundational Learning
- **Multi-Armed Bandit (MAB) Framework**: Sequential decision-making under uncertainty to maximize cumulative reward.
  - *Why needed*: Provides the baseline model for decision-making with multiple options.
  - *Quick check*: Can the algorithm be applied to a simple 2-armed bandit problem?
- **Cost Subsidy (CS)**: A mechanism where a portion of the cost is subsidized by an external source, allowing exploration of high-reward arms.
  - *Why needed*: Enables balancing exploration and exploitation under a cost constraint.
  - *Quick check*: Does the algorithm adapt its behavior based on the subsidy factor α?
- **Best-Arm Identification (BAI)**: A pure-exploration problem to identify the arm with the highest expected reward.
  - *Why needed*: Provides the foundation for the BAI stage in PE-CS.
  - *Quick check*: Does the BAI stage correctly identify the best arm before transitioning to PE?
- **Instance-Dependent Regret Bounds**: Regret bounds that depend on the specific characteristics of the problem instance, rather than just problem parameters.
  - *Why needed*: Provides a more nuanced and accurate measure of algorithm performance.
  - *Quick check*: Are the upper bounds logarithmic in the horizon T for specific problem instances?
- **Order-Optimality**: A property where an algorithm's upper bound matches the problem's lower bound up to constant factors.
  - *Why needed*: Demonstrates that the algorithm is theoretically optimal for the given setting.
  - *Quick check*: Does the upper bound on regret match the known lower bound for the problem instance?

## Architecture Onboarding

**Component Map:**
- Dataset (MovieLens 25M/Goodreads) -> Arms (Genres) -> MAB-CS Framework -> Algorithms (PE, PE-CS, Baselines) -> Metrics (Cost Regret, Quality Regret)

**Critical Path:**
1. Data Preparation: Map items to arms, compute rewards, sample costs.
2. Algorithm Selection: Choose between PE, PE-CS, or baseline based on the setting.
3. Execution: Run the selected algorithm for T=5,000,000 rounds.
4. Evaluation: Compute and compare cost and quality regret.

**Design Tradeoffs:**
- **Exploration vs. Exploitation**: PE balances exploration (comparing candidates to reference) with exploitation (eliminating suboptimal arms).
- **Theoretical Guarantees vs. Practical Performance**: The paper prioritizes instance-dependent guarantees, which may require careful tuning in practice.
- **Complexity vs. Simplicity**: PE-CS adds a BAI stage, increasing complexity but ensuring a known reference arm.

**Failure Signatures:**
- **Linear Regret in PE-CS**: BAI stage identifies the wrong best arm, leading to poor reference selection.
- **Stagnation in PE**: Sample thresholds are too large or round updates fail, preventing arm elimination.
- **Suboptimal Performance**: Algorithm fails to adapt to the subsidy factor α, leading to excessive costs or low rewards.

**3 First Experiments:**
1. **Sanity Check**: Run PE on a simple 2-armed bandit problem with known rewards and costs to verify basic functionality.
2. **Subsidy Sensitivity**: Test PE-CS with varying subsidy factors α to observe the impact on cost and quality regret.
3. **Reference Arm Stability**: Monitor the reference arm ℓ in PE-CS to ensure it remains stable and optimal throughout the run.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact random seeds for sampling arm costs are not provided, potentially leading to numerical discrepancies in reproduction.
- The paper does not address the use of empirical scaling constants in bandit algorithms, which could affect practical performance.
- The theoretical analysis assumes perfect knowledge of the problem instance, which may not hold in practice.

## Confidence
- **Theoretical Analysis**: High - The proofs of logarithmic upper bounds and order-optimality are sound and well-supported.
- **Empirical Validation**: Medium - The paper demonstrates effectiveness on real-world datasets, but reproducibility is limited by missing random seeds and the lack of discussion on empirical tuning.

## Next Checks
1. Implement PE and PE-CS using the exact formulas for sample thresholds and confidence radii as specified, without any empirical scaling constants, to isolate the theoretical guarantees.
2. Conduct a controlled experiment by fixing the random seeds for arm cost generation to ensure numerical reproducibility and to compare the empirical performance directly against the reported results.
3. Analyze the sensitivity of PE-CS to the accuracy of the best-arm identification (BAI) stage by testing with different exploration budgets in the BAI phase and measuring the impact on the final regret.