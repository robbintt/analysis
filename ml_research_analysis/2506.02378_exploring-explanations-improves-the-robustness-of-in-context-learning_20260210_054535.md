---
ver: rpa2
title: Exploring Explanations Improves the Robustness of In-Context Learning
arxiv_id: '2506.02378'
source_url: https://arxiv.org/abs/2506.02378
tags:
- reasoning
- label
- x-icl
- x2-icl
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving the robustness of
  in-context learning (ICL) to out-of-distribution (OOD) data. The authors propose
  X2-ICL, an extension of ICL with explanations (X-ICL) that systematically explores
  explanations for all possible labels rather than just the observed label.
---

# Exploring Explanations Improves the Robustness of In-Context Learning

## Quick Facts
- arXiv ID: 2506.02378
- Source URL: https://arxiv.org/abs/2506.02378
- Authors: Ukyo Honda; Tatsushi Oka
- Reference count: 40
- Primary result: X2-ICL improves OOD robustness of ICL by generating and selecting among label-conditioned explanations

## Executive Summary
This paper addresses the brittleness of in-context learning (ICL) to out-of-distribution (OOD) data by extending explanation-augmented ICL (X-ICL) with systematic exploration of explanations for all possible labels. The proposed X2-ICL method generates reasoning paths for each potential label and selects the label with the most valid reasoning, achieving consistent improvements on OOD datasets while trading off some in-distribution accuracy. Experiments across five different LLMs show X2-ICL consistently outperforms both standard ICL and X-ICL on OOD test sets, with the strongest gains observed when using high-performing LLMs with strong reasoning capabilities.

## Method Summary
X2-ICL augments standard ICL demonstrations by generating explanations for ALL possible labels (not just the observed label) using GPT-4o with meta-prompts. For each demonstration example (xᵢ, yᵢ), the method generates rᵢ,ℓ for every label ℓ ∈ Y, creating augmented demonstrations {(xᵢ, rᵢ, yᵢ)} where rᵢ = (rᵢ,₁,...,rᵢ,ₗ). At inference, the model generates reasoning r′ for the test input across all labels, then selects the label with highest probability p̂(y′|r′,x′). The approach builds on few-shot Chain-of-Thought reasoning, extending single reasoning paths to systematic exploration of all label-conditioned paths.

## Key Results
- X2-ICL consistently outperforms both ICL and X-ICL on 6-8 out of 8 OOD datasets across five different LLMs
- Achieves highest mean accuracy across HANS, NAN, PISP, ST, ANLI R1-R3, and PAWS test sets
- Structured exploration of all label-conditioned reasoning paths outperforms stochastic sampling (self-consistency) baseline
- Demonstrates 3.4x more tokens than ICL and 2.5x more than X-ICL in computational cost

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Expansion via Counterfactual Reasoning
Standard X-ICL binds latent variables tightly to realized labels in demonstrations. X2-ICL samples rℓ ∼ p̃(r|y=ℓ,x) for every ℓ ∈ Y, creating a complete latent vector r = (r₁,...,r_L). This forces the model to learn what valid reasoning looks like for *each* label class, not just patterns correlating inputs to correct outputs.

### Mechanism 2: Validity-Based Selection Over Stochastic Aggregation
Unlike self-consistency which samples from the same conditional distribution, X2-ICL systematically generates reasoning conditioned on *each label*, then selects via argmax ŷ = argmax_{y′} p̂(y′|r′,x′). This creates comparative evaluation rather than majority voting.

### Mechanism 3: Reduced Reliance on Demonstrated Input-Label Patterns
Standard ICL learns p̂(y|x) directly from demonstrations. X2-ICL learns p̂(y|r,x) and p̂(r|x) separately, where r contains reasoning for all labels. At inference, the model must determine which label's reasoning best fits the test input's specific features.

## Foundational Learning

- **Latent Variable Models in Classification:**
  - Why needed here: Understanding p(y|x) = ∫p(y|r,x)p(r|x)dr and how explanations function as latent variables explaining the input-label relationship
  - Quick check question: Can you explain why modeling latent reasoning r for *all* labels creates a richer representation than modeling r only for observed labels?

- **In-Context Learning (ICL) Mechanics:**
  - Why needed here: Understanding how demonstrations create implicit models p̂(y|x,Dₙ) without parameter updates, and why this is brittle to distribution shifts
  - Quick check question: Why does ICL generalize poorly when test inputs come from adversarially shifted distributions?

- **Chain-of-Thought (CoT) Reasoning:**
  - Why needed here: X2-ICL builds on few-shot CoT, extending single reasoning paths to systematic exploration of all label-conditioned paths
  - Quick check question: What's the difference between self-consistency (sampling multiple CoTs) and X2-ICL's structured exploration?

## Architecture Onboarding

- **Component map:**
  1. Meta-prompt Sₘ (small set of examples with explanations)
  2. Demonstration augmentation (generate rᵢ,ℓ for all ℓ ∈ Y)
  3. Augmented demonstrations {(xᵢ, rᵢ, yᵢ)} where rᵢ = (rᵢ,₁,...,rᵢ,ₗ)
  4. Inference (generate r′, compute p̂(y′|r′,x′), select argmax)

- **Critical path:**
  1. Implement meta-prompt construction (examples from paper's Appendix F)
  2. Build label-conditioned explanation generator
  3. Integrate all-label reasoning into demonstration format
  4. Implement inference-time reasoning generation and selection

- **Design tradeoffs:**
  - In-distribution vs. OOD accuracy: X2-ICL degrades on SNLI/QQP (same distribution as demos) while improving on adversarial datasets
  - Cost: ~3.4x more tokens than ICL, ~2.5x more than X-ICL (Table 4)
  - Label space size: Method doesn't scale to large/open label sets
  - Model capability: Requires strong reasoning models; smaller models (DeepSeek-R1-8B) show weaker gains

- **Failure signatures:**
  1. Over-inference: Model draws conclusions from weakly inferable information
  2. External knowledge leakage: Uses encyclopedic knowledge beyond premise
  3. Label ambiguity: Neutral predictions increase when genuinely ambiguous
  4. Format adherence: Open-source models struggle with structured output format

- **First 3 experiments:**
  1. Replicate on HANS dataset with GPT-4o, comparing ICL → X-ICL → X2-ICL accuracy
  2. Ablate meta-prompt size: test with 0, 1, 3 examples per label to determine minimum viable annotation
  3. Compare structured exploration (X2-ICL) vs. self-consistency baseline (n=L samples)

## Open Questions the Paper Calls Out

1. Can adaptive application of X²-ICL based on input characteristics mitigate the trade-off between in-distribution and OOD performance?

2. Would multi-label prediction on ambiguity-aware datasets (e.g., ChaosNLI) disentangle the impact of label ambiguity from the observed in-distribution/OOD performance trade-off?

3. How can X²-ICL be extended to tasks with large label sets or open-ended generation where systematic exploration of all labels is infeasible?

4. What computational optimizations can reduce the ~4.8x cost increase of X²-ICL over standard ICL while preserving robustness benefits?

## Limitations

- Label space scaling: Requires generating explanations for all possible labels, becoming computationally prohibitive for tasks with large label spaces
- Reliance on model reasoning capability: Effectiveness depends critically on LLM's ability to generate coherent counterfactual explanations
- Trade-off with in-distribution accuracy: Consistently degrades performance on in-distribution data while improving OOD robustness

## Confidence

**High confidence claims:**
- X2-ICL systematically outperforms both ICL and X-ICL on OOD datasets across multiple LLMs and datasets
- Structured exploration of all label-conditioned reasoning paths provides advantages over stochastic sampling
- Method demonstrates particular effectiveness when using high-performing LLMs with strong reasoning capabilities

**Medium confidence claims:**
- Preserving latent reasoning space dimensionality is the primary driver of improvements
- X2-ICL reduces reliance on surface pattern matching in favor of reasoning-based classification
- Degradation on in-distribution data is an acceptable trade-off for OOD robustness

**Low confidence claims:**
- Method would scale to larger label spaces (no evidence provided)
- Reasoning quality improvement generalizes to non-NLI tasks
- Approach would work equally well with smaller, weaker LLMs

## Next Checks

1. **Scalability test**: Evaluate X2-ICL on a multi-class classification task (e.g., 10+ classes) to empirically measure scaling limitations of generating explanations for all labels.

2. **Model capability ablation**: Systematically test X2-ICL across a wider range of model sizes and capabilities (e.g., GPT-3.5, Llama-3-8B, Mistral-7B) to establish minimum reasoning capability threshold.

3. **Cross-task generalization**: Apply X2-ICL to non-NLI tasks such as sentiment analysis or multi-choice question answering to validate whether reasoning exploration mechanism generalizes beyond textual entailment and paraphrase detection.