---
ver: rpa2
title: How Different Tokenization Algorithms Impact LLMs and Transformer Models for
  Binary Code Analysis
arxiv_id: '2511.03825'
source_url: https://arxiv.org/abs/2511.03825
tags:
- vocabulary
- tokenization
- code
- tokenizers
- tokenizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates tokenization algorithms for
  assembly code analysis, focusing on their impact on downstream performance in binary
  code analysis tasks. The research investigates three tokenization methods (BPE,
  Unigram, WordPiece) across multiple vocabulary sizes (3K, 25K, 35K, and 128K) on
  models including Llama 3.2, BERT, and BART.
---

# How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis

## Quick Facts
- arXiv ID: 2511.03825
- Source URL: https://arxiv.org/abs/2511.03825
- Reference count: 40
- Primary result: Tokenizer choice significantly affects model performance in assembly code analysis, with Unigram showing superior compression efficiency while BPE achieves best overall downstream task performance.

## Executive Summary
This study systematically evaluates three tokenization algorithms (BPE, Unigram, WordPiece) across multiple vocabulary sizes for assembly code analysis using models including Llama 3.2, BERT, and BART. The research demonstrates that preprocessing assembly code by normalizing memory addresses and converting hexadecimal values to decimal consistently improves model performance. The findings reveal that no single tokenizer dominates across all tasks and configurations, highlighting the importance of aligning tokenization strategies with specific downstream requirements. These insights provide actionable guidance for optimizing tokenization approaches in low-level code analysis and improving language model effectiveness in binary program analysis workflows.

## Method Summary
The study investigates tokenization algorithms on x86-64 assembly code using a systematic evaluation framework. Researchers implemented a preprocessing pipeline that normalizes memory addresses and converts hexadecimal values to decimal format. Three tokenization algorithms were tested across four vocabulary sizes (3K, 25K, 35K, 128K) using language models including Llama 3.2, BERT, and BART. Performance was measured on downstream tasks such as function signature prediction, with fertility scores used to assess tokenization efficiency. The evaluation employed standardized datasets and compared tokenization approaches based on compression efficiency and task-specific performance metrics.

## Key Results
- Unigram tokenizer demonstrated superior compression efficiency with lowest fertility scores around 2.0 tokens per instruction
- BPE tokenizer achieved the best overall performance on downstream tasks like function signature prediction
- Preprocessing assembly code by normalizing memory addresses and converting hexadecimal values to decimal consistently improved model performance, especially for smaller vocabulary sizes

## Why This Works (Mechanism)
Tokenization algorithms fundamentally transform raw assembly instructions into discrete units that language models can process effectively. The performance differences arise from how each algorithm handles the unique characteristics of assembly language, including its dense syntax, frequent use of memory addresses and hexadecimal values, and the need for precise instruction representation. BPE's merge-based approach creates subword units that preserve instruction semantics while maintaining manageable vocabulary sizes, explaining its strong downstream performance. Unigram's probabilistic word segmentation achieves better compression by identifying optimal token boundaries that minimize token count per instruction. The preprocessing improvements work by reducing variability in the input representation, allowing tokenizers to create more consistent and meaningful token boundaries.

## Foundational Learning
- Assembly instruction syntax and structure: Understanding x86-64 instruction formats, operands, and encoding is essential for interpreting tokenization results and their impact on model performance.
- Tokenization algorithms (BPE, Unigram, WordPiece): Knowledge of how these algorithms segment text into tokens is critical for understanding their relative strengths and weaknesses in assembly code contexts.
- Fertility scores in tokenization: Understanding this metric helps evaluate tokenizer efficiency in representing instructions with minimal token count.
- Downstream task evaluation metrics: Familiarity with function signature prediction and related binary analysis tasks provides context for interpreting performance results.

## Architecture Onboarding

Component map: Raw assembly code -> Preprocessing pipeline -> Tokenizer -> Vocabulary construction -> Language model -> Downstream task evaluation

Critical path: The most sensitive sequence is preprocessing → tokenizer selection → vocabulary size determination, as errors or suboptimal choices at any stage propagate through the entire analysis pipeline.

Design tradeoffs: The study balances tokenization efficiency (compression via low fertility scores) against downstream task performance, revealing that optimal compression doesn't always yield best task results. Vocabulary size selection involves tradeoffs between coverage and model complexity.

Failure signatures: Poor tokenization manifests as high fertility scores, inconsistent instruction representation, and degraded downstream task performance. Specific failure modes include excessive tokenization of simple instructions or inadequate representation of complex addressing modes.

First experiments:
1. Implement and compare the three preprocessing variants (raw, address normalization only, full normalization) on a small assembly dataset
2. Test tokenizer performance with minimal vocabulary sizes (3K) to establish baseline efficiency
3. Evaluate the impact of vocabulary size scaling on both compression efficiency and task performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to x86-64 assembly code without testing cross-architecture generalizability
- Only four vocabulary sizes tested, missing exploration of scaling relationships and saturation points
- Focus on standard tokenization algorithms without investigating domain-specific adaptations or hybrid approaches

## Confidence
High: Systematic evaluation methodology and observed impact of preprocessing steps
Medium: Comparative performance rankings of tokenization algorithms (dataset-dependent)
Low: Claims about optimal vocabulary sizes (limited testing range)

## Next Checks
1. Cross-architecture validation: Evaluate tokenization strategies on ARM64, RISC-V, and other ISAs to assess generalizability
2. Downstream task expansion: Test performance on vulnerability detection, malware classification, and control flow graph reconstruction
3. Vocabulary size scaling study: Conduct experiments from 1K to 500K tokens to identify optimal scaling relationships and performance patterns