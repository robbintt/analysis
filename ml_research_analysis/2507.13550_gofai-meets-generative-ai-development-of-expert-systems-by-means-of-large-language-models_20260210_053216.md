---
ver: rpa2
title: 'GOFAI meets Generative AI: Development of Expert Systems by means of Large
  Language Models'
arxiv_id: '2507.13550'
source_url: https://arxiv.org/abs/2507.13550
tags:
- knowledge
- system
- expert
- systems
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid approach that combines the recall
  capacity of large language models (LLMs) with the precision of symbolic systems
  to develop expert systems. The authors extract structured knowledge from LLMs using
  well-designed prompts, encode it into Prolog, and validate it with human experts
  to ensure accuracy and interpretability.
---

# GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models

## Quick Facts
- **arXiv ID**: 2507.13550
- **Source URL**: https://arxiv.org/abs/2507.13550
- **Reference count**: 32
- **Primary result**: Hybrid LLM-symbolic approach achieves >99% factual accuracy in expert system knowledge bases

## Executive Summary
This paper presents a hybrid approach that combines the recall capacity of large language models (LLMs) with the precision of symbolic systems to develop expert systems. The authors extract structured knowledge from LLMs using well-designed prompts, encode it into Prolog, and validate it with human experts to ensure accuracy and interpretability. Experimental results show that the generated knowledge bases achieve over 99% factual accuracy, with statistical significance (p < 0.05) against a 80% accuracy threshold. The approach addresses LLM hallucinations by providing a transparent, verifiable knowledge base that supports deterministic logical inference.

## Method Summary
The method extracts structured knowledge from LLMs by constraining their output to a symbolic representation (Prolog facts and rules). Using breadth-first semantic expansion with hyperparameters for breadth (h) and depth (d), the system queries an LLM for related concepts and encodes them as Prolog predicates. The extracted knowledge is validated through manual verification and statistical testing, with accuracy measured against an 80% threshold. The approach leverages the LLM as a "probabilistic knowledge oracle" for extraction while relying on deterministic Prolog inference for reasoning.

## Key Results
- Generated knowledge bases achieve 99.2% (Claude) and 99.6% (GPT-4.1) factual accuracy
- Statistical tests reject the null hypothesis (p ≤ 0.80) with p < 0.05 significance
- Entity mapping failures occurred in 34.2% of Wikidata validation attempts
- Exponential cost scaling: depth=3 with h=30 requires ~29,791 queries (~$325)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining LLM output to a symbolic representation significantly reduces hallucinations compared to open-ended text generation.
- **Mechanism:** Structured prompt chaining forces LLM to generate JSON adhering to specific ontology, converting unstructured neural potential into discrete logical predicates (`concept/1`, `related_to/2`).
- **Core assumption:** LLM possesses underlying knowledge and instruction-following capability to format it validly.
- **Evidence anchors:** Abstract states limiting domain and using structured prompt-based extraction produces symbolic representation. Section 3 describes breadth/depth hyperparameters limiting retrieved concepts. Corpus paper (arXiv:2601.10436) supports LLM-driven structuring but highlights consistency challenges.

### Mechanism 2
- **Claim:** Decoupling knowledge extraction (probabilistic) from knowledge inference (deterministic) allows for a "freezing" process.
- **Mechanism:** LLM acts as "Probabilistic Knowledge Oracle" (O). Once Prolog file is written, LLM is removed from loop. Runtime inference uses standard logic programming ensuring deterministic results.
- **Core assumption:** Translation layer (JSON → Prolog) is lossless and syntactically perfect; contradictions resolved by human validator, not LLM.
- **Evidence anchors:** Section 3.1 defines LLM as probabilistic knowledge oracle queried about factual relations. Abstract mentions combining LLM recall with symbolic precision. Corpus paper (arXiv:2509.10818) suggests coupling LLMs with structured models improves reliability.

### Mechanism 3
- **Claim:** Statistical verification of extracted facts enables scalable trust.
- **Mechanism:** Calculates theoretical verification lower bound (N ≥ 258) using Hoeffding's inequality. Samples 250 facts, finds 99.2% accuracy, rejects null hypothesis (p ≤ 0.80) with 95% confidence.
- **Core assumption:** Random sample of facts is representative of entire knowledge base (i.i.d. assumption).
- **Evidence anchors:** Section 4.1 performs one-sample proportion z-test to reject hypothesis that extracted knowledge is not accurate. Section 3.2 establishes formal guarantees on sample complexity. Corpus paper (arXiv:2503.24334) emphasizes preserving human expertise, aligning with human-in-the-loop validation.

## Foundational Learning

- **Concept:** First-Order Logic (Prolog)
  - **Why needed here:** Entire output is a `.pl` file. Cannot debug expert system without understanding predicates, facts, rules, and backtracking.
  - **Quick check question:** Given `related_to(a, b).` and `related_to(b, c).`, how would you write a recursive rule to infer transitivity?

- **Concept:** Hallucination vs. Recall
  - **Why needed here:** Paper claims to fix hallucinations via Prolog. Must distinguish between "model didn't know" (recall failure) and "model lied confidently" (hallucination) to interpret error rates.
  - **Quick check question:** If LLM invents fake citation for real author, is that data failure or generation failure?

- **Concept:** Computational Complexity (BFS/DFS)
  - **Why needed here:** Extraction algorithm uses BFS with exponential complexity (O(h^d)). Understanding critical to avoiding cost explosions (e.g., $325 on single deep query).
  - **Quick check question:** Why does increasing depth (d) from 2 to 3 increase query count by factor of ~30 (given h=30)?

## Architecture Onboarding

- **Component map:** Python Shell Script → LLM API (Claude/GPT) → JSON Response → Ontology Mapper → Prolog Syntax Encoder → `.pl` Knowledge Base (Facts + Rules) → SWI-Prolog Engine (Queries via `concept(X)`) → Graphviz/D3 rendering

- **Critical path:** Prompt design is single point of failure. If prompt doesn't strictly enforce JSON schema or ontology, parser fails and no Prolog file is generated.

- **Design tradeoffs:**
  - **Recall vs. Cost:** High depth (d) yields comprehensive knowledge but costs scale exponentially (Table 6).
  - **Strictness vs. Coverage:** Strict prompts reduce hallucinations but may miss nuanced/implicit relations.

- **Failure signatures:**
  - **Syntax Trap:** LLM returns Markdown-wrapped JSON instead of raw JSON; parser crashes.
  - **Ontology Drift:** LLM invents new predicates (e.g., `born_near/2`) not in schema, causing Prolog warnings.
  - **Cycles:** Infinite loops in graph traversal if memoization (V ← V ∪ {c}) fails.

- **First 3 experiments:**
  1. **Sanity Run:** Execute pipeline with depth=0 on known topic (e.g., "Socrates"). Verify generated `socrates.pl` loads in SWI-Prolog without syntax errors.
  2. **Hallucination Stress Test:** Run pipeline on fictional or contradictory topic. Observe if system creates "consistent" but factually empty/wrong Prolog universe.
  3. **Scalability Check:** Measure API cost and time for depth=1 vs depth=2 on generic topic to validate theoretical complexity (O(h^d)) before running production jobs.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the unique information contribution of different LLMs be quantified in terms of entropy reduction relative to target domain?
  - **Basis in paper:** [explicit] Authors state in conclusion: "this experiment leaves open a line of research to quantify how much new information each LLM provides versus that extracted in other LLMs... measurable as the decrease in entropy about a new topic per LLM."
  - **Why unresolved:** Current study focuses on validating accuracy of single-model extraction rather than measuring marginal utility or novelty of combining knowledge from multiple diverse LLMs.
  - **What evidence would resolve it:** Comparative study applying extraction pipeline to multiple LLMs and calculating conditional entropy of domain knowledge base before and after integrating facts from each model.

- **Open Question 2:** Can state-of-the-art entity linking models significantly reduce disambiguation failures currently observed during automated validation?
  - **Basis in paper:** [explicit] Section 4.3.4 notes: "These findings suggest that future work should incorporate state-of-the-art entity linking models (e.g., BLINK, REL) to improve disambiguation accuracy."
  - **Why unresolved:** Automated validation pipeline suffered 34.2% entity mapping failure rate (e.g., matching "Hamlet" film rather than play), which masked true factual accuracy of LLM.
  - **What evidence would resolve it:** Experiments integrating entity linking models like BLINK into Prolog-to-Wikidata pipeline, demonstrating statistically significant reduction in mapping failures and increase in verification rates.

- **Open Question 3:** Can domain partitioning strategies effectively mitigate exponential cost of deep semantic expansion (d ≥ 3)?
  - **Basis in paper:** [inferred] Section 4.4 identifies configurations with depth d ≥ 3 as "impractical" due to exponential costs (O(h^d)) and suggests "domain partitioning" as potential solution not implemented.
  - **Why unresolved:** Paper establishes theoretical complexity bounds but does not validate methods for breaking recursive expansion into manageable sub-problems to achieve deep graph coverage efficiently.
  - **What evidence would resolve it:** Modified algorithm that clusters root concepts into sub-domains for parallel or hierarchical extraction, achieving deep knowledge coverage at sub-exponential computational cost.

## Limitations
- **Prompt Design Ambiguity:** Paper emphasizes "well-designed prompts" as critical but doesn't disclose actual prompt templates, creating uncertainty about reproducibility.
- **Scalability Constraints:** Exponential complexity O(h^d) poses practical limitations - depth=3 with h=30 requires ~29,791 queries (~$325).
- **Validation Scope:** While 250 facts were manually verified across 25 topics, this represents relatively small sample that may miss localized hallucination patterns.

## Confidence
- **High Confidence:** Core mechanism of decoupling probabilistic extraction from deterministic inference is sound and well-supported by experimental results (>99% accuracy with p < 0.05 significance).
- **Medium Confidence:** Statistical verification approach using Hoeffding's inequality is methodologically valid, though representativeness assumption introduces some uncertainty.
- **Medium Confidence:** Exponential cost scaling (O(h^d)) is theoretically sound, but real-world performance may vary based on LLM API efficiency and memoization effectiveness.

## Next Checks
1. **Prompt Template Validation:** Request and test exact prompt templates used to ensure reproducibility. Compare outputs using alternative prompt formulations to assess sensitivity to prompt design.
2. **Cost-Performance Benchmarking:** Conduct controlled experiments measuring actual API costs and processing times for d=1, d=2, and d=3 on identical topics to validate theoretical O(h^d) complexity and identify practical depth limits.
3. **Localization Testing:** Design targeted validation tests for specific knowledge domains (e.g., historical figures vs. fictional characters) to assess whether random sampling approach adequately detects localized hallucination patterns.