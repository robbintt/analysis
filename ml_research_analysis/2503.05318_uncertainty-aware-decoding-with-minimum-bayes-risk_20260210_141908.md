---
ver: rpa2
title: Uncertainty-Aware Decoding with Minimum Bayes Risk
arxiv_id: '2503.05318'
source_url: https://arxiv.org/abs/2503.05318
tags:
- https
- conference
- computational
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a principled framework for uncertainty-aware
  language model decoding by generalizing Minimum Bayes Risk (MBR) decoding. The core
  idea is to modify MBR's expected risk computation to account for model parameter
  uncertainty by incorporating a posterior over weights, enabling better hypothesis
  selection and selective prediction.
---

# Uncertainty-Aware Decoding with Minimum Bayes Risk

## Quick Facts
- **arXiv ID:** 2503.05318
- **Source URL:** https://arxiv.org/abs/2503.05318
- **Reference count:** 40
- **Primary result:** Incorporating model parameter uncertainty into Minimum Bayes Risk (MBR) decoding improves language generation quality and enables effective selective prediction.

## Executive Summary
This paper proposes a principled framework for uncertainty-aware language model decoding by generalizing Minimum Bayes Risk (MBR) decoding. The core innovation is to modify MBR's expected risk computation to account for model parameter uncertainty by incorporating a posterior over weights. This enables more robust hypothesis selection and selective prediction across diverse language generation tasks. The framework provides practical methods for combining predictions from multiple models at sequence or token levels, applicable to both black-box APIs and full models. Empirically, incorporating weight uncertainty improves decoding performance across machine translation, summarization, and scoring tasks, even without computational overhead when using simple posteriors.

## Method Summary
The method generalizes MBR decoding by replacing the single-model predictive distribution with a Bayesian predictive posterior that integrates over a distribution of model parameters. This is achieved through variational learning (using the IVON optimizer) to obtain a Gaussian posterior over model weights, or by using deep ensembles. During decoding, sequences are sampled from each model in the ensemble, and the expected utility is computed by marginalizing over the posterior. This allows for sequence-level or token-level combination of predictions, enabling uncertainty-aware decoding even for black-box APIs. The expected utility value also serves as a signal for selective prediction, allowing the system to abstain from low-confidence generations.

## Key Results
- Incorporating weight uncertainty into MBR improves decoding performance across machine translation, summarization, and scoring tasks.
- Performance gains correlate with prediction diversity and scale with ensemble size and hypothesis set size.
- Expected utility serves as an effective uncertainty signal for selective prediction, reducing hallucinations and improving robustness.
- Sequence-level combination methods enable uncertainty-aware decoding for black-box LLMs without requiring token probabilities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating a posterior over model parameters into the expected utility computation can improve hypothesis selection quality compared to standard MBR.
- **Mechanism:** The predictive posterior (integrating over multiple plausible parameter settings) provides a more robust expected utility estimate than a single point estimate, reducing sensitivity to any one model's idiosyncrasies.
- **Core assumption:** The approximate posterior (e.g., from IVON or deep ensembles) provides a meaningful coverage of the weight space relevant to the task.
- **Evidence anchors:** [abstract] "incorporating a posterior over model parameters into MBR's computation of expected risk"; [section 3.1] Defines the predictive posterior p_Θ and shows this replaces p_θ in Equation 5.
- **Break condition:** If the posterior is extremely narrow (low uncertainty) or unrepresentative (e.g., poor variational approximation), the predictive posterior collapses to a single model, reverting to standard MBR and negating benefits.

### Mechanism 2
- **Claim:** Aggregating predictions from multiple models at the sequence level (Eq. 9, 10) enables uncertainty-aware decoding for black-box LLMs without requiring access to token probabilities.
- **Mechanism:** By treating outputs from different models as samples from an (approximate) posterior, the method uses pairwise utility scores between all generated sequences to identify a central, high-consensus output.
- **Core assumption:** The black-box models being ensembled are diverse enough in their predictions to form a meaningful "distribution."
- **Evidence anchors:** [abstract] "practical methods for combining predictions from multiple models at sequence or token levels, applicable to both black-box APIs and full models."; [section 3.2] Describes the derivation of Eq. 9 and 10 from the predictive posterior.
- **Break condition:** If ensembled models produce nearly identical outputs (low diversity), the expected utility landscape will be flat, leading to no improvement over a single model.

### Mechanism 3
- **Claim:** The expected utility value calculated during uncertainty-aware MBR can serve as a signal for selective prediction, allowing the system to abstain from low-confidence generations.
- **Mechanism:** Low expected utility for all candidates indicates high predictive uncertainty (the model cannot find a sequence with high consensus), which correlates with a higher likelihood of errors like hallucinations.
- **Core assumption:** There is a monotonic relationship between low expected utility and poor output quality/error rate.
- **Evidence anchors:** [abstract] "deciding when to abstain from generation"; [section 4.5, Figure 2] Empirically demonstrates that using total expected utility as a criterion for selective prediction improves performance (BLEU score) as the abstention rate increases.
- **Break condition:** If the utility function does not capture the aspects of quality that correlate with failure modes (e.g., it overlooks factual accuracy), the expected utility signal will be a poor predictor of when to abstain.

## Foundational Learning

- **Concept: Bayesian Predictive Posterior (p_Θ)**
  - **Why needed here:** The core of the paper is replacing a single model's distribution (p_θ) with an average over a distribution of models (p_Θ) to account for weight uncertainty. Understanding this marginalization is key to understanding the entire method.
  - **Quick check question:** Given a posterior q(θ), how would you write the predictive probability of a sequence y?

- **Concept: Minimum Bayes Risk (MBR) Decoding**
  - **Why needed here:** The paper generalizes existing MBR decoding. One must understand the baseline principle—selecting the output that maximizes expected utility—to grasp what the paper modifies.
  - **Quick check question:** In standard MBR, what distribution is the expectation taken over, and what is the goal of the argmax operation?

- **Concept: Utility Functions (u)**
  - **Why needed here:** The performance of MBR, and thus uncertainty-aware MBR, is fundamentally dependent on the choice of utility function (e.g., BLEU, BERTScore), which defines "quality."
  - **Quick check question:** Why might a utility function like BERTScore be chosen over exact match for natural language generation tasks?

## Architecture Onboarding

- **Component map:** Posterior Learner (e.g., IVON optimizer) -> Generator (samples sequences) -> Utility Scorer (computes pairwise utilities) -> Aggregator (combines scores via Eq. 9, 10, or 13) -> Abstention Decider (uses expected utility for selective prediction)
- **Critical path:** The computationally dominant step is utility scoring, which is O(|H|²) for a single model. For the sequence-level ensemble method (Eq. 9), this becomes O((|M| * |H|)²). This quadratic scaling in hypothesis set size is the primary computational bottleneck.
- **Design tradeoffs:**
  - **Eq. 9 vs. Eq. 10 (Sequence-level):** Eq. 9 is more computationally expensive (more utility computations) but allows highly probable sequences from any single model to contribute more. Eq. 10 is faster but may dilute strong signals from a single good model.
  - **Unimodal vs. Multimodal Posterior:** Multimodal posteriors (e.g., Deep Ensembles) provide stronger empirical gains but require multiple full training runs. Unimodal posteriors (e.g., IVON) add negligible training overhead but may be less effective when training from scratch.
  - **Token-level vs. Sequence-level:** Token-level combination requires access to model probabilities (inapplicable to black-box APIs) and can be difficult to parallelize across GPUs, but allows for more fine-grained probability averaging. Sequence-level is more flexible but operates on coarser units.
- **Failure signatures:**
  - **Flat Utility Landscape:** If expected utility scores for all hypotheses are very similar, the selection is effectively random. This can happen if the ensemble lacks diversity or the utility function is uninformative.
  - **Collapsed Posterior:** If the variational posterior has near-zero variance, the method provides no benefit over a single model. This can be diagnosed by inspecting the learned covariance matrix (Σ).
  - **Utility Misalignment:** If the chosen utility function (e.g., BLEU) correlates poorly with actual human-judged quality on the specific task, MBR will select poor outputs. This is a failure of the utility choice, not the mechanism itself.
- **First 3 experiments:**
  1. **Replicate "No Free Lunch" Baseline:** Implement sequence-level combination (Eq. 10) with a unimodal posterior using IVON. Compare against a single MBR baseline with matched total hypothesis counts to verify the paper's claim of improvement without computational overhead on a standard dataset like IWSLT17.
  2. **Diversity Ablation:** Create synthetic ensembles with controlled diversity (e.g., by training with different random seeds, using snapshot ensembles, or intentionally reducing variance in a unimodal posterior). Plot performance (BLEU/COMET) against a diversity metric (e.g., self-BLEU) to confirm the correlation shown in Figure 1.
  3. **Selective Prediction Validation:** On a held-out test set, implement the selective prediction framework using expected utility as the abstention criterion. Plot the accuracy/quality of the non-abstained subset against the abstention rate (1-α) to verify that quality increases as lower-utility outputs are filtered out, as shown in Figure 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do other Bayesian Deep Learning methods, such as Laplace approximation or SWAG, compare to the IVON optimizer within the uncertainty-aware MBR framework?
- **Basis in paper:** [explicit] Section 4.2 states that while IVON was chosen for its negligible overhead, "It is also possible to use other Bayesian Deep Learning methods, such as, Laplace... or SWAG... but we leave their exploration for future work."
- **Why unresolved:** The current study exclusively validates the framework using variational learning (IVON), leaving the efficacy of alternative posterior estimation techniques unknown.
- **What evidence would resolve it:** Empirical benchmarks comparing the decoding quality and uncertainty estimation of Laplace and SWAG posteriors against IVON on translation and summarization tasks.

### Open Question 2
- **Question:** How does the interaction between uncertainty-aware MBR and the use of disjoint subsets for hypothesis search and utility estimation affect performance?
- **Basis in paper:** [explicit] Section 3 and Footnote 4 note that prior work uses different subsets for these steps, but "we leave the exploration of the interaction of this design choice with our methods to future work."
- **Why unresolved:** The proposed method uses the same sample set for both identifying and scoring hypotheses; it is unknown if separating them improves the risk estimate.
- **What evidence would resolve it:** Ablation studies that vary the overlap between the search space and the utility estimation set to observe the impact on final sequence quality.

### Open Question 3
- **Question:** Can a posterior approximation method achieve the diversity and expressiveness of Deep Ensembles without incurring their linear training time overhead?
- **Basis in paper:** [inferred] Section 4.4 asks if we can "find a method with the same pretraining overhead as a unimodal posterior but more expressiveness?" and notes Deep Ensembles require training time that "increases linearly."
- **Why unresolved:** The paper finds that unimodal posteriors are efficient but often less effective than Deep Ensembles for training from scratch, creating a trade-off between computational cost and expressiveness.
- **What evidence would resolve it:** A method that generates diverse, multimodal posteriors in a single training run (e.g., via noise injection or cyclic trajectories) that matches Deep Ensemble performance on low-resource tasks.

## Limitations

- **Black-box API Applicability Uncertainty:** The sequence-level ensemble method assumes the models form a meaningful "distribution." If API models are highly correlated, the method provides no benefit over a single model.
- **Utility Function Dependency:** The framework's effectiveness is fundamentally constrained by the chosen utility function. Poor alignment between utility and actual quality makes the expected utility signal unreliable.
- **Computational Scaling:** The O(|H|²) utility scoring complexity creates significant computational burdens for large hypothesis sets or ensembles, potentially limiting practical applicability.

## Confidence

**High Confidence:**
- Incorporating weight uncertainty into MBR's expected risk computation improves hypothesis selection quality compared to standard MBR, particularly when using diverse ensembles.
- Expected utility serves as a reliable signal for selective prediction, enabling effective abstention from low-confidence generations and reducing hallucinations.

**Medium Confidence:**
- Sequence-level combination methods enable uncertainty-aware decoding for black-box LLMs without requiring token probabilities, assuming the ensembled models provide sufficient diversity.
- The improvement from uncertainty-aware MBR scales with ensemble size and hypothesis set size, and correlates with prediction diversity.

**Low Confidence:**
- The mechanism works by providing a more robust expected utility estimate than a single point estimate, reducing sensitivity to model idiosyncrasies. This is theoretically sound but the specific weight-uncertainty mechanism lacks direct corpus validation.
- Multimodal posteriors (deep ensembles) provide stronger empirical gains than unimodal posteriors (IVON) when training from scratch. While stated, this comparison needs more systematic ablation.

## Next Checks

1. **Diversity Dependence Validation:** Create synthetic ensembles with controlled diversity (e.g., by training with different random seeds, using snapshot ensembles, or intentionally reducing variance in a unimodal posterior). Plot performance (BLEU/COMET) against a diversity metric (e.g., self-BLEU) to confirm the correlation shown in Figure 1 and validate that the method requires sufficient ensemble diversity to function.

2. **Black-box API Diversity Stress Test:** Implement the sequence-level ensemble method (Eq. 9 or 10) using multiple instances of the same black-box model with different temperature parameters or prompts to control diversity. Systematically vary the diversity level and measure the improvement over single-model MBR to validate the assumption that black-box models must form a meaningful "distribution" for the method to work.

3. **Utility Function Ablation Study:** Replicate the main experiments using multiple utility functions (e.g., BLEU, BERTScore, exact match) and correlate the expected utility scores with actual human-judged quality and hallucination rates. This will validate whether expected utility serves as a reliable uncertainty signal across different quality definitions and task types.