---
ver: rpa2
title: Language Models as Ontology Encoders
arxiv_id: '2507.14334'
source_url: https://arxiv.org/abs/2507.14334
tags:
- ontology
- embeddings
- language
- which
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of existing ontology embedding
  methods by proposing OnT, which combines pretrained language models with hyperbolic
  geometric modeling to preserve both textual information and logical structure in
  OWL ontologies. OnT achieves state-of-the-art performance across prediction and
  inference tasks on real-world ontologies (GALEN, GO, Anatomy), with significant
  improvements in Mean Rank metrics - up to 7x better than existing methods.
---

# Language Models as Ontology Encoders

## Quick Facts
- **arXiv ID**: 2507.14334
- **Source URL**: https://arxiv.org/abs/2507.14334
- **Reference count**: 40
- **Key outcome**: OnT achieves state-of-the-art performance across prediction and inference tasks on real-world ontologies (GALEN, GO, Anatomy), with significant improvements in Mean Rank metrics - up to 7x better than existing methods.

## Executive Summary
This paper proposes OnT, a novel ontology embedding method that combines pretrained language models with hyperbolic geometric modeling to preserve both textual information and logical structure in OWL ontologies. The approach addresses limitations of existing ontology embedding methods by using verbalization to convert complex concepts into text, BERT-based embeddings, and hyperbolic space representations with specialized loss functions to capture logical relationships. OnT demonstrates strong performance on prediction and inference tasks, achieving up to 7x improvement in Mean Rank metrics compared to existing methods. The method successfully identifies missing and incorrect axioms in SNOMED CT and shows strong transfer learning capabilities.

## Method Summary
OnT converts normalized Description Logic (DL) axioms into natural language sentences using template-based verbalization, then encodes these sentences into vectors using a pretrained language model (MiniLM). These vectors are projected into a Poincaré ball for hierarchical representation, with role embeddings modeled as rotational transformations. The model is trained using three specialized loss functions: hierarchical contrastive loss (pulling related concepts closer and pushing parent concepts toward the origin), role alignment loss, and conjunction loss. The method targets EL-ontologies and is evaluated on prediction (80/10/10 split) and inference tasks across GALEN, GO, and Anatomy datasets.

## Key Results
- OnT achieves state-of-the-art performance with up to 7x improvement in Mean Rank metrics compared to existing methods (OWL2Vec*, BoxEL)
- The method successfully identifies missing and incorrect axioms in SNOMED CT
- Ablation studies show that incorporating role embeddings and logical constraints consistently improves performance across different language models
- Strong transfer learning capabilities demonstrated through cross-ontology evaluation

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Geometric Mapping via Verbalization
Mapping logical ontology concepts to textual descriptions (verbalization) allows language models to encode complex logical structures that purely geometric methods miss. The system converts normalized Description Logic (DL) axioms into natural language sentences using template-based rules. A Pretrained Language Model (PLM) encodes these sentences into vectors, which are then projected into a Poincaré ball. This preserves the semantic nuance of labels while placing them in a space suitable for hierarchy modeling. If the ontology contains highly abstract or mathematical roles that do not translate effectively into natural language (e.g., "hasProbability"), the verbalization may introduce noise, degrading the PLM embedding quality.

### Mechanism 2: Logical Preservation via Hyperbolic Constraints
Enforcing logical relationships (subsumption, conjunction) as geometric constraints in hyperbolic space enables the model to perform approximate reasoning that standard PLMs cannot. The paper defines a "Hierarchy Loss" combining contrastive loss (pulling related concepts closer) and centripetal loss (pushing parent concepts toward the origin of the Poincaré ball). Specialized losses for conjunction and existential restriction force the geometric vectors to respect logical entailment. If the ontology is extremely shallow or largely flat (lacking deep hierarchies), the overhead of hyperbolic projection may not offer significant benefits over Euclidean or simple cosine-similarity baselines.

### Mechanism 3: Role Modeling as Rotational Transformations
Representing roles (relations) as rotation and scaling operations in hyperbolic space captures the monotonicity of existential restrictions better than embedding roles as simple vectors. Instead of just embedding the text of a role, OnT models a role as a function (rotation matrix and scaling). This ensures that if A ⊑ B, the transition ∃r.A maintains relative positioning to ∃r.B in the embedding space. If roles have complex transitive properties or anti-symmetry that cannot be captured by simple rotation/scaling, the geometric model may fail to converge or produce conflicting embeddings.

## Foundational Learning

- **Concept: Description Logic (DL) and the EL Profile**
  - **Why needed here**: The paper specifically targets EL-ontologies (a tractable subset of DL). You must understand the normalized axiom forms (NF1-NF4) like C ⊓ D to interpret the "verbalization" and loss functions correctly.
  - **Quick check question**: Can you distinguish between a TBox axiom (concept definitions) and an ABox axiom (instance data), and identify which one OnT prioritizes during training?

- **Concept: Hyperbolic Geometry (Poincaré Ball)**
  - **Why needed here**: The core innovation is mapping ontology concepts to a Poincaré ball. Unlike Euclidean space, hyperbolic space expands exponentially, making it ideal for tree-like hierarchies where the volume of "child" nodes grows rapidly relative to "parents."
  - **Quick check question**: Why does the "Centripetal Loss" pull parent nodes closer to the origin (radius 0) while pushing children away? (Hint: relates to the distance function definition).

- **Concept: Contrastive Learning**
  - **Why needed here**: The model is trained using hierarchical contrastive loss, which relies on positive pairs (valid axioms) and negative samples (corrupted axioms). Understanding how the margin α affects the separation of these pairs is critical for debugging convergence.
  - **Quick check question**: If the margin α in the contrastive loss is set too low, what might happen to the embedding distances between a concept and its false negatives?

## Architecture Onboarding

- **Component map**: Input: Normalized EL-Ontology (NF1-NF4 axioms) -> Verbalizer: Converts logical axioms → Natural Language sentences -> PLM Encoder: (e.g., MiniLM) processes sentences → Contextual Embeddings -> Hyperbolic Projection: Maps Euclidean embeddings → Poincaré Ball points (xC) -> Role Encoder: Maps role names → Rotation matrices (R(Θr)) and Scaling (kr) -> Optimizer: Calculates Hierarchy (L≺), Role (Lr), and Conjunction (L⊓) losses to update PLM and geometric parameters

- **Critical path**: The Verbalization component is the most brittle step. If the template for converting ∃r.C into text fails to capture the semantic nuance (e.g., confusing "part of" with "has part"), the PLM produces garbage in, leading to suboptimal geometric arrangements regardless of the loss function tuning.

- **Design tradeoffs**: OnT vs. OnT(w/o r): The paper shows adding role embeddings (rotations) lowers Mean Rank (MR) significantly but adds mathematical complexity to the gradient computations. PLM choice: Larger models (MPNet) improve MR but inference speed drops. The paper finds minimal performance gain from massive models, suggesting the logical constraints (losses) matter more than raw model size.

- **Failure signatures**: High Mean Rank (MR) with decent Hits@1: Indicates the model is overfitting to specific common axioms but failing to generalize to the long tail of the hierarchy. NaN Loss: Often occurs in hyperbolic optimization if embeddings drift outside the Poincaré ball (norm ≥ 1/√κ) or if the curvature κ is not clamped.

- **First 3 experiments**:
  1. Ablation Reproduction: Run OnT(w/o r) vs. OnT on the GALEN dataset to verify the specific contribution of rotational role embeddings to the Mean Rank metric (confirming Table 4).
  2. Visual Validation: Project the trained embeddings of a small sub-tree (e.g., 50 concepts) from the Poincaré ball to 2D Euclidean space. Check if "parent" nodes are visually closer to the center (origin) than their children (validating the Centripetal Loss).
  3. Negative Sampling Stress Test: Vary the negative sampling strategy (e.g., hard negatives vs. random corruption) to see if the hierarchical contrastive loss is robust or if it relies heavily on "easy" negatives.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can OnT be extended to support Description Logic ALC, specifically incorporating the negation operator? The conclusion lists extending to "ALC with the negation logical operator ¬" as a primary goal. The method currently restricts modeling to EL-ontologies (normalized forms NF1-NF4), relying on hierarchical orderings that do not natively support logical complement.

- **Open Question 2**: Can the learned role embeddings effectively capture role inclusion axioms (r ⊑ s) and role chains? The authors plan to "delve deeper into the logical patterns of roles... including investigating role inclusion axioms." While roles are modeled as rotations/scaling, the paper only validates their use in existential restrictions (∃r.C), not role-to-role logical dependencies.

- **Open Question 3**: How does the quality of text verbalization for complex concepts impact the final embedding performance? Future work includes "exploring the impact of verbalization quality" on the model. The approach uses simple compositional templates (e.g., "V(C) and V(D)") which may lose semantic nuance compared to richer natural language descriptions.

## Limitations

- The approach demonstrates strong performance on EL-ontologies but has notable limitations in handling complex Description Logic constructs like negation.
- The verbalization mechanism's effectiveness depends heavily on the quality of template-based conversion from logical axioms to natural language.
- The hyperbolic geometry assumption works well for tree-like hierarchies but may not generalize to ontologies with dense cyclic relationships or minimal hierarchical structure.
- The evaluation focuses primarily on prediction and inference tasks, leaving open questions about performance on instance-based reasoning or dynamic ontology updates.

## Confidence

- **High confidence**: The performance improvements over baseline methods (OWL2Vec*, BoxEL) on standard metrics (Hits@k, Mean Rank) are well-supported by quantitative results across multiple datasets (GALEN, GO, Anatomy). The ablation studies showing role embeddings' contribution are reproducible.
- **Medium confidence**: The claim that hyperbolic space is necessary rather than just beneficial lacks comparative experiments with high-dimensional Euclidean alternatives. The SNOMED CT case study provides illustrative examples but limited quantitative validation of error detection capabilities.
- **Low confidence**: The assertion that verbalization captures "full semantic nuance" of complex logical expressions is difficult to verify without human evaluation of the text-to-logic conversion quality, especially for edge cases in role modeling.

## Next Checks

1. **Cross-ontology generalization test**: Train OnT on one ontology (e.g., GALEN) and evaluate on a held-out ontology (e.g., SNOMED CT) to assess transfer learning capabilities beyond the reported intra-ontology evaluation.

2. **Verbalization quality audit**: Manually evaluate 50 randomly selected verbalized axioms from the GALEN dataset to quantify information loss or semantic distortion compared to their original logical forms.

3. **Geometry sensitivity analysis**: Systematically vary the Poincaré ball curvature κ and dimensionality d to determine the sensitivity of performance metrics to these hyperparameters, particularly testing whether Euclidean space with sufficient dimensions could achieve comparable results.