---
ver: rpa2
title: 'Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with
  Differential Privacy with HAM10000'
arxiv_id: '2507.06619'
source_url: https://arxiv.org/abs/2507.06619
tags:
- privacy
- noise
- steps
- differential
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000

## Quick Facts
- **arXiv ID**: 2507.06619
- **Source URL**: https://arxiv.org/abs/2507.06619
- **Reference count**: 21
- **Primary result**: Proposes SAD-DPSGD with phased noise decay and adaptive clipping to improve accuracy on imbalanced medical image datasets under differential privacy

## Executive Summary
This paper addresses the challenge of training deep learning models on imbalanced datasets while preserving differential privacy. The authors propose Steps Adaptive Decay DPSGD (SAD-DPSGD), which introduces a phased approach to noise and clipping threshold decay. By front-loading lower noise levels and higher clipping thresholds in early training phases, the method aims to preserve informative gradients from minority classes while still maintaining privacy guarantees through exponential decay schedules aligned with the non-linear learning dynamics observed in imbalanced data.

## Method Summary
SAD-DPSGD modifies standard DPSGD by implementing three key adaptive mechanisms: phased noise multiplier decay (starting low and increasing), linear clipping threshold decay (starting high and decreasing), and exponential step partitioning to match learning dynamics. The method divides training into exponentially-sized steps where noise and clipping parameters are held constant within each step before updating. Privacy accounting uses Rényi Differential Privacy (RDP) to handle the varying noise levels across steps. The approach is evaluated on the HAM10000 dermatoscopic image dataset, demonstrating improved accuracy over standard DPSGD and Auto-DPSGD baselines while maintaining the same privacy budget.

## Key Results
- SAD-DPSGD improves classification accuracy on imbalanced HAM10000 dataset under differential privacy constraints
- Performance gains are achieved by preserving minority class gradients through adaptive clipping and noise schedules
- Optimal hyperparameters identified: β=0.8 (noise decay), γ=0.9 (step decay), n=3 steps for ε=3.0 privacy budget
- Improvements come from lifting both majority and minority class accuracy rather than just majority performance

## Why This Works (Mechanism)

### Mechanism 1: Front-Loaded Gradient Utility via Phased Noise Decay
SAD-DPSGD starts with a low noise multiplier to preserve gradient information in early training phases before noise accumulates. This creates a "low-noise window" where the model can establish better representations before privacy constraints become more restrictive. The mechanism assumes early learning trajectory is disproportionately important for final performance.

### Mechanism 2: Progressive Clipping Decay for Minority Gradient Preservation
The method begins with a higher clipping threshold that linearly decays over time. This prevents premature clipping of informative, large-magnitude gradients from minority classes in early training phases when the model rapidly adjusts to under-represented samples.

### Mechanism 3: Exponential Step Partitioning to Match Learning Dynamics
Training is divided into exponentially-sized steps (Di · γ = Di-1) rather than uniform steps. This matches the non-linear learning progress observed in imbalanced datasets, where minority class accuracy improves dramatically at first but then plateaus, justifying a prolonged initial phase under favorable conditions.

## Foundational Learning

- **Concept: Differential Privacy Stochastic Gradient Descent (DPSGD)**
  - **Why needed here:** Core technique being improved; involves clipping per-sample gradients and adding Gaussian noise before each update
  - **Quick check question:** Can you explain the role of clipping threshold (C) and noise multiplier (σ) in a standard DPSGD update step?

- **Concept: Rényi Differential Privacy (RDP)**
  - **Why needed here:** Used to calculate total privacy budget when noise multiplier varies across training steps
  - **Quick check question:** Why is Rényi Differential Privacy (RDP) more convenient than standard (ε, δ)-DP accounting for varying noise multipliers?

- **Concept: Class Imbalance in Deep Learning**
  - **Why needed here:** Core problem addressed; imbalanced data skews gradient distributions and affects learning
  - **Quick check question:** In a batch with majority and minority classes, how might gradient norm distributions differ, and why would standard clipping disproportionately affect minority gradients?

## Architecture Onboarding

- **Component map:** Gradient computation -> SAD-DPSGD Scheduler (sigma_clip_estimation) -> Per-sample gradient clipping -> Gaussian noise injection -> Model weight update -> RDP Accountant
- **Critical path:** For each training iteration: compute per-sample gradients -> call scheduler to get (σt, Ct) -> clip gradients using Ct -> add noise based on σt and Ct -> update model weights -> update accountant
- **Design tradeoffs:** Introduces three new hyperparameters (β, γ, a) and requires step division (n), increasing tuning complexity compared to standard DPSGD
- **Failure signatures:** Privacy budget exceeded early if σn too low; no benefit over Auto-DPSGD if decay parameters too aggressive; training instability if Ct too high
- **First 3 experiments:**
  1. Baseline Reproduction: Run SAD-DPSGD on HAM10000 with recommended settings (β=0.8, γ=0.9, n=3) and verify reported accuracy improvement over Auto-DPSGD for ε=3.0
  2. Ablation on Decay Parameters: Fix two of {β, γ, n} while varying the third to validate optimal configuration and compare against trends in Figures 3 and 4
  3. Per-Class Analysis: Train models using SAD-DPSGD and baseline, then compute accuracy for minority vs. majority groups separately to verify improvement from lifting both groups

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SAD-DPSGD maintain its performance advantage over adaptive baselines on imbalanced datasets other than HAM10000?
- **Basis in paper:** The Conclusion states future work could focus on testing SAD-DPSGD on other imbalanced datasets, noting validation is currently limited to HAM10000
- **Why unresolved:** Experimental scope restricted to single dataset; authors explicitly list dataset variability as a limitation
- **What evidence would resolve it:** Empirical results from benchmarking on other standard imbalanced datasets (e.g., medical imaging datasets with different class distributions or non-medical tabular data)

### Open Question 2
- **Question:** Is there a generalizable heuristic for determining optimal decay parameters (β, γ, n) without dataset-specific tuning?
- **Basis in paper:** The Conclusion identifies "variability of optimal hyperparameters across datasets" as a specific limitation
- **Why unresolved:** Paper provides parameter analysis but relies on manual selection to maximize accuracy on HAM10000, suggesting settings may not transfer automatically
- **What evidence would resolve it:** Proposed theoretical guideline or adaptive function that sets parameters based on dataset properties and proves robust across multiple test cases

### Open Question 3
- **Question:** Does SAD-DPSGD outperform or synergize with data-level approaches like class-weighted loss functions?
- **Basis in paper:** Introduction notes previous work focused on "data pre-processing and class-weighted deep learning," but experiments only compare against DPSGD variants
- **Why unresolved:** Unclear if algorithmic adjustment is superior to or compatible with existing class-weighted methods
- **What evidence would resolve it:** Comparative experiments integrating class weights into SAD-DPSGD framework or directly benchmarking against cited class-weighted methods

## Limitations
- Limited experimental validation to single dataset (HAM10000), raising questions about generalization to other imbalanced datasets
- Optimal hyperparameters (β, γ, n) determined empirically rather than theoretically, suggesting potential overfitting to specific conditions
- Assumes specific gradient norm distribution pattern for minority classes that may not generalize across different architectures or datasets

## Confidence
- **High Confidence**: Phased noise decay mechanism addresses well-known DPSGD limitation; RDP accounting correctly applied
- **Medium Confidence**: Exponential step partitioning rationale plausible but specific decay parameter (γ=0.9) may be dataset-dependent
- **Low Confidence**: Clipping threshold decay claim assumes particular gradient norm distribution requiring empirical validation across diverse imbalanced scenarios

## Next Checks
1. **Cross-Dataset Generalization**: Apply SAD-DPSGD to at least two additional imbalanced datasets (e.g., CIFAR-100-LT, long-tailed vision benchmarks) to verify HAM10000 performance gains transfer beyond medical imaging domain

2. **Hyperparameter Sensitivity Analysis**: Systematically vary all three decay parameters (β, γ, n) across plausible ranges while keeping privacy budget fixed; measure training stability and final noise multiplier to identify if proposed optimal values are truly robust or overfit

3. **Gradient Norm Distribution Study**: For each class in imbalanced dataset, compute and compare gradient norm distributions during early vs. late training phases; verify hypothesis that minority class gradients have larger norms early that are being clipped, and confirm SAD-DPSGD's adaptive clipping actually preserves these gradients