---
ver: rpa2
title: Approximating Language Model Training Data from Weights
arxiv_id: '2506.15553'
source_url: https://arxiv.org/abs/2506.15553
tags:
- data
- dataset
- training
- arxiv
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes and addresses the problem of recovering training
  data from language model weights, specifically focusing on the scenario where only
  the final fine-tuned model and the base model are available. The authors develop
  SELECT, a gradient-based approach that greedily selects datapoints from a large
  public corpus whose gradients align with the direction between the base and fine-tuned
  models in parameter space.
---

# Approximating Language Model Training Data from Weights

## Quick Facts
- arXiv ID: 2506.15553
- Source URL: https://arxiv.org/abs/2506.15553
- Authors: John X. Morris; Junjie Oscar Yin; Woojeong Kim; Vitaly Shmatikov; Alexander M. Rush
- Reference count: 17
- Primary result: SELECT method recovers training data from language model weights, improving AG News classification from 65% to 80% accuracy versus 88% expert benchmark

## Executive Summary
This paper addresses the problem of recovering training data from language model weights by developing SELECT, a gradient-based method that selects datapoints from a public corpus whose gradients align with the optimization trajectory between base and fine-tuned models. The approach works by generating synthetic checkpoints via linear interpolation between models, computing per-example gradients at these checkpoints, and greedily selecting samples that maximally reduce the distance to the fine-tuned model. SELECT significantly outperforms baselines including random selection, top-k gradient selection, and likelihood-based methods, achieving 80% accuracy on AG News versus 65% for random selection, while approaching the expert benchmark of 88%.

## Method Summary
The SELECT method formalizes data recovery as an optimization problem: given a base model θ₀ and fine-tuned model θ_f, select a subset of N datapoints from a large public corpus whose gradients best explain the optimization trajectory from θ₀ to θ_f. The approach creates P synthetic checkpoints by linearly interpolating between θ₀ and θ_f, then computes per-example gradients for all candidates at each checkpoint. Using Johnson-Lindenstrauss random projection, gradients are compressed to a lower-dimensional space for efficiency. A greedy selection algorithm iteratively chooses the sample whose gradient maximally aligns with the direction toward θ_f, updating a running gradient sum. The selected data is then used to train a new model from θ₀, with evaluation against baselines and the expert benchmark.

## Key Results
- SELECT improves AG News classification accuracy from 65% (random selection) to 80%, approaching the expert benchmark of 88%
- On MSMARCO supervised fine-tuning, SELECT reduces perplexity from 3.3 (random) to 2.3, compared to 2.0 for the expert LLAMA model
- SELECT outperforms baselines including random selection, top-k gradient selection, and likelihood-based methods across multiple datasets and model sizes
- The method remains effective even when none of the true training data is present in the seed corpus

## Why This Works (Mechanism)
The core insight is that language model weights encode information about the optimization trajectory taken during training. By computing gradients of candidate datapoints at synthetic checkpoints along the path from base to fine-tuned models, SELECT identifies samples whose gradients align with the actual training direction. The greedy selection accumulates these aligned gradients, effectively reconstructing the subset of data that drove the optimization. This works because the final-layer gradients capture the direction of parameter updates needed to improve the model on each sample, and samples requiring similar updates will be selected together, preserving the semantic structure of the original training data.

## Foundational Learning
- **Linear interpolation of model parameters**: Creating synthetic checkpoints θ̂ⱼ = (j/P)θ₀ + (1-j/P)θ_f to approximate the training trajectory. Needed because the true training path is unknown. Quick check: Verify synthetic checkpoints produce reasonable intermediate model behaviors.
- **Per-example gradient computation**: Using vmap to efficiently compute gradients for all candidates simultaneously. Needed for computational feasibility with large candidate pools. Quick check: Confirm gradients are computed only for the final layer to reduce memory usage.
- **Johnson-Lindenstrauss random projection**: Reducing gradient dimensionality from D to d (e.g., 4096) while preserving directional relationships. Needed because storing full gradients is memory-prohibitive. Quick check: Verify projection preserves cosine similarity between gradients.
- **Optimal Transport distance**: Measuring distributional similarity between selected and ground-truth data in embedding space. Needed because exact data recovery is impossible; this quantifies semantic alignment. Quick check: Compute OT distance between embeddings of selected vs. true data.

## Architecture Onboarding

- **Component map**: Seed Corpus -> Label Generator (fine-tuned model) -> Gradient Engine -> Projection Layer -> Synthetic Trajectory Generator -> Greedy Selector -> Selected Dataset

- **Critical path**:
  1. Obtain base model (θ₀), fine-tuned model (θ_f), and seed corpus
  2. Use θ_f to generate labels for the entire seed corpus (for classification tasks)
  3. Generate synthetic checkpoints
  4. **Most computationally expensive step**: Compute and project per-example final-layer gradients for all seed samples against the synthetic checkpoints
  5. Run the greedy selection algorithm (Algorithm 1) to select the target number of samples
  6. Train a new model from θ₀ on the selected dataset to evaluate performance

- **Design tradeoffs**:
  - **Projection Dimension**: Lower dimension (e.g., 512) is faster and uses less memory but may lose alignment information. The authors use 4096 as a default. Figure 5 shows performance scales with dimension.
  - **Number of Synthetic Checkpoints (P)**: More checkpoints may better approximate the true training trajectory but increase gradient computation cost linearly. The paper does not specify an ablation on P, suggesting this is a key hyperparameter.
  - **Seed Corpus Choice**: The selected data is a subset of the seed corpus. If the seed corpus lacks domain-relevant data, SELECT cannot recover useful signal. Table 4 shows performance is highly sensitive to the seed distribution.
  - **Greedy vs. Batch Selection**: The paper compares greedy to a batch variant. Greedy is more computationally intensive but yields better performance (Table 1).

- **Failure signatures**:
  - **Optimized with AdamW/Weight Decay**: Performance may degrade if the ground-truth model was trained with AdamW, as weight decay can decouple the weight trajectory from the gradient direction (Table 5, Section 7).
  - **Highly Non-Linear Trajectory**: If the fine-tuning task requires a complex, non-linear optimization path, linear interpolation will create a poor synthetic trajectory, leading to low-quality data selection.
  - **Irrelevant Seed Corpus**: If the seed corpus has minimal semantic overlap with the true fine-tuning data (e.g., using legal documents to recover medical QA data), OT distance will remain high and performance gains will be minimal.

- **First 3 experiments**:
  1. **Baseline Validation on AG News**: Implement SELECT on AG News (4-class classification) using GPT-2 Medium. Start with a small seed set (10k) from Natural Questions. The goal is to reproduce the ~80% accuracy result (Table 1) and compare it against the Random and Top-K baselines.
  2. **Ablation on Projection Dimension**: Using the AG News setup, vary the projection dimension (e.g., 512, 1024, 2048, 4096). Measure both final accuracy (Figure 5) and total runtime/memory for the gradient computation step. This validates the efficiency-performance tradeoff.
  3. **SFT on MSMARCO with Llama-3.2**: Apply SELECT to the supervised fine-tuning (SFT) task using a 1B or 3B parameter Llama-3.2 model. The goal is to reproduce the perplexity reduction from 3.3 (random) to ~2.3 (SELECT) as shown in Table 2. This tests the method on a generative task and a larger model.

## Open Questions the Paper Calls Out
- **Open Question 1**: How much information about training data can be extracted from the weights of large language models (e.g., 1 trillion parameters) compared to the smaller models tested in this paper?
- **Open Question 2**: To what extent can similar methods recover pretraining data, as opposed to finetuning data?
- **Open Question 3**: What is the upper bound on training data leakage from model weights, and can formal privacy guarantees be established?
- **Open Question 4**: Why does SELECT still underperform the expert benchmark (80% vs. 88% on AG News), and what factors determine this gap?

## Limitations
- The linear interpolation assumption may poorly approximate complex, non-linear training trajectories, limiting effectiveness for tasks requiring sophisticated optimization paths
- Performance degrades significantly when ground-truth models use AdamW with weight decay, as weight decay decouples parameter updates from gradient directions
- The method's effectiveness is highly sensitive to seed corpus composition, with minimal overlap leading to poor data recovery despite theoretical robustness claims

## Confidence
- **High Confidence**: SELECT algorithm implementation details and empirical improvements over baselines on AG News classification are well-documented and reproducible
- **Medium Confidence**: Claims about robustness to seed corpus composition require careful scrutiny, as evaluation relies on synthetic ground-truth datasets
- **Low Confidence**: Generalizability to other model architectures and task types beyond classification and supervised fine-tuning remains unproven

## Next Checks
1. **Trajectory Fidelity Analysis**: Implement diagnostics measuring alignment between linear synthetic trajectory and actual fine-tuning path by checkpointing intermediate models during fine-tuning
2. **Seed Corpus Ablation**: Systematically vary seed corpus composition (Wikipedia only vs. mixed-domain data) to test robustness claims, including cases with minimal domain overlap
3. **Hyperparameter Sensitivity Test**: Evaluate SELECT across different optimization configurations (Adam vs. AdamW, varying weight decay rates, learning rate schedules) to quantify sensitivity to optimization choices acknowledged but not thoroughly investigated in the paper