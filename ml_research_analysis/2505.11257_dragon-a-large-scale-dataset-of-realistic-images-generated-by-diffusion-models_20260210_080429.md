---
ver: rpa2
title: 'DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models'
arxiv_id: '2505.11257'
source_url: https://arxiv.org/abs/2505.11257
tags:
- images
- diffusion
- image
- dataset
- dragon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRAGON is a large-scale dataset of 2.6M synthetic images from 25
  diffusion models, designed to support research in detecting and attributing AI-generated
  content. It includes a diverse range of recent and established models, uses an LLM-based
  prompt expansion pipeline to improve image realism, and provides five training subsets
  of varying sizes for different experimental scenarios.
---

# DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models

## Quick Facts
- **arXiv ID**: 2505.11257
- **Source URL**: https://arxiv.org/abs/2505.11257
- **Reference count**: 40
- **Primary result**: 2.6M synthetic images from 25 diffusion models for AI-generated content detection research

## Executive Summary
DRAGON is a comprehensive dataset of 2.6 million synthetic images generated by 25 different diffusion models, designed to advance research in detecting and attributing AI-generated content. The dataset includes both training and benchmark test sets, with five different training subset sizes to accommodate various experimental scenarios. Images were generated using an LLM-based prompt expansion pipeline to improve realism, and the dataset covers both established and recent diffusion models. Quality analysis demonstrates significant improvements in image realism, with MPS scores increasing from approximately 4.5 to 14.0.

## Method Summary
The DRAGON dataset was created by generating images from 25 different diffusion models using an LLM-based prompt expansion pipeline. The dataset includes 2.6 million images with associated metadata containing model-specific details and generation prompts. Five training subsets of varying sizes (10K, 100K, 500K, 1M, and 2M images) were created to enable experiments under different data availability scenarios. The dataset covers both established models like Stable Diffusion and recent models like FLUX.1, with prompts expanded through GPT-4 to enhance image realism and diversity.

## Key Results
- MPS scores improved significantly from ~4.5 to ~14.0 through LLM-based prompt expansion
- DE-FAKE model trained on DRAGON achieved 0.62 accuracy in model attribution tasks
- Retraining on DRAGON showed substantial performance improvements over models trained on older datasets
- DRAGON demonstrated better robustness under JPEG compression and resizing attacks compared to older datasets

## Why This Works (Mechanism)
The dataset works effectively because it combines diverse diffusion models with enhanced prompt engineering through LLM expansion. This approach generates more realistic and varied images that better represent the current state of diffusion model outputs. The inclusion of multiple model types and the systematic prompt expansion create a comprehensive training environment that captures the nuances of different generation approaches. The dataset's structure, with multiple training subset sizes, allows researchers to test detection methods under various data availability conditions, making the findings more generalizable.

## Foundational Learning
- **Diffusion models**: Generative models that create images through iterative denoising processes - needed to understand the source diversity in the dataset
- **MPS (Mean Perceptual Score)**: A metric for evaluating image realism - needed to assess quality improvements from prompt expansion
- **Model attribution**: The task of identifying which specific model generated an image - needed to understand the dataset's forensic applications
- **JPEG compression attacks**: Methods to test detection robustness - needed to evaluate real-world applicability
- **Prompt engineering**: Techniques to optimize text prompts for better image generation - needed to understand the quality improvement mechanism
- **Benchmark testing**: Systematic evaluation of detection methods - needed to validate the dataset's effectiveness

## Architecture Onboarding

**Component map**: LLM prompt expansion -> Diffusion model generation -> Metadata collection -> Training subset creation -> Detection model training -> Evaluation

**Critical path**: The LLM prompt expansion is critical as it directly impacts image quality and diversity, which in turn affects detection model performance. The generation process must be completed before metadata collection, and training subset creation depends on the complete dataset.

**Design tradeoffs**: The dataset prioritizes breadth of model coverage over depth in any single model type, trading comprehensive single-model analysis for broader detection capability. The use of LLM expansion improves realism but may introduce prompt bias toward certain image characteristics.

**Failure signatures**: Poor detection performance may indicate insufficient model diversity, inadequate prompt variation, or quality issues in the generated images. Attribution failures could suggest similar generation patterns across different models or insufficient fine-grained differences in the training data.

**First experiments**: 
1. Train a baseline detector on DRAGON's smallest subset (10K images) and evaluate on the benchmark set
2. Compare detection accuracy between models trained on DRAGON versus older datasets
3. Test attribution performance across different training subset sizes to determine optimal data requirements

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Long-term stability of detection performance remains uncertain as newer diffusion models continue to evolve
- Focus on English prompts may limit applicability for non-English image detection scenarios
- Quality assessment relies primarily on MPS scores, which may not capture all aspects of image realism

## Confidence
- **Dataset utility for forensic research**: High - Strong empirical evidence from detection and attribution experiments
- **Quality improvement through prompt expansion**: High - Clear quantitative improvements in MPS scores
- **Detection model retraining effectiveness**: High - Significant performance gains over baseline models
- **Generalization to compression/resize attacks**: Medium - Results shown but limited to specific attack types
- **Long-term effectiveness for new models**: Low - Insufficient temporal coverage to assess future-proof capability

## Next Checks
1. Evaluate detection performance on images generated by diffusion models released after DRAGON's dataset creation to assess temporal generalization
2. Test model attribution accuracy on non-English prompts and images to evaluate linguistic generalization
3. Compare MPS score improvements against alternative quality assessment metrics (e.g., FID, CLIP score) to validate the comprehensiveness of quality improvements