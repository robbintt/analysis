---
ver: rpa2
title: 'Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing'
arxiv_id: '2509.01548'
source_url: https://arxiv.org/abs/2509.01548
tags:
- merging
- mergelock
- alig
- normal
- merged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MergeLock, a method to prevent unauthorized
  model merging by making models unmergeable. The core idea leverages the symmetry
  properties of self-attention layers in Transformers by inserting invertible transformation
  matrices into the query-key and value-output branches.
---

# Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing

## Quick Facts
- **arXiv ID:** 2509.01548
- **Source URL:** https://arxiv.org/abs/2509.01548
- **Reference count:** 40
- **Primary result:** Introduces MergeLock method that prevents unauthorized model merging while preserving original task performance.

## Executive Summary
This paper introduces MergeLock, a method to prevent unauthorized model merging by making models unmergeable. The core idea leverages the symmetry properties of self-attention layers in Transformers by inserting invertible transformation matrices into the query-key and value-output branches. These transformations preserve the model's original output while pushing it to a different loss basin, breaking the parameter space alignment required for effective merging. Experiments across vision and language tasks show that MergeLock degrades merged model performance by over 95% when a protected model is involved.

## Method Summary
MergeLock protects models by applying random invertible transformations to the self-attention layers in Transformers. For each attention head, invertible matrices A and B are generated and applied to the query-key and value-output branches respectively. These transformations mathematically cancel out during inference, preserving the original output. The method breaks Linear Mode Connectivity, preventing successful model merging through task arithmetic while maintaining single-task performance.

## Key Results
- MergeLock degrades merged model performance by over 95% when a protected model is involved
- Protected models maintain their original task performance with negligible degradation
- MergeLock shows strong resistance to alignment-based recovery attacks
- The method is effective across multiple vision and language tasks using different Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1: Functional Symmetry Preservation
The protected model maintains identical output performance because the applied transformations mathematically cancel out during the attention computation. MergeLock exploits the architectural symmetry of self-attention layers by inserting invertible matrices A and A⁻¹ into the Query and Key branches, and B and B⁻¹ into the Value and Output branches, performing an identity transformation that preserves attention logits.

### Mechanism 2: Loss Basin Displacement
Merging fails because the protection relocates model parameters to a distinct "loss basin" disconnected from other models. Standard merging relies on Linear Mode Connectivity - the assumption that fine-tuned models reside in the same loss basin. MergeLock applies composite transformations to "push" models away from the pre-trained weight distribution, creating high loss barriers that cause linear interpolation to fall into regions of high error.

### Mechanism 3: Resistance to Alignment Attacks
The defense is robust against alignment attacks because it uses continuous random transformations rather than discrete permutations. Prior defenses often used permutation matrices reversible via discrete solvers like the Hungarian algorithm. MergeLock's general invertible matrices span a continuous space where optimization gets stuck in local minima without ground-truth data, making alignment attacks ineffective.

## Foundational Learning

- **Linear Mode Connectivity (LMC):** Explains why model merging works (models in same basin) and why this defense works (it breaks that connectivity). Quick check: If two models are not linearly mode connected, what happens to the loss landscape on the linear path between them? (Answer: A high loss barrier appears).

- **Transformer Self-Attention Symmetry:** You must understand where "slack" exists in Transformer architecture to insert matrices without changing output. Quick check: Why can we insert arbitrary invertible matrices into Q and K branches, but not into FFNs without constraints? (Answer: FFNs have non-linear activation functions that break symmetry, whereas attention relies on linear projections before Softmax).

- **Task Arithmetic:** The primary threat model (vector addition). Understanding how task vectors are computed is necessary to understand how defense disrupts vector alignment. Quick check: How does changing the "basin" of the fine-tuned model affect the resulting task vector when subtracted from the original pretrained model? (Answer: It creates a task vector pointing in an orthogonal or conflicting direction, causing destructive interference).

## Architecture Onboarding

- **Component map:** Pre-trained model -> Random invertible matrices (A, B) -> Protected model (single-task performance preserved) -> Merging attempt -> Failed (high loss barrier)
- **Critical path:** 1) Generate random invertible matrices A, B for each layer/head 2) Transform weights: W_Q ← AᵀW_Q, W_K ← A⁻¹W_K, W_V ← BᵀW_V, W_O ← W_OB⁻¹ 3) Verify output preservation through inference 4) Distribute protected model with secure storage of transformation matrices
- **Design tradeoffs:** Matrix complexity vs. invertibility - using purely random matrices offers strongest protection but requires ensuring invertibility; R×P×D composition guarantees invertibility while maximizing perturbation
- **Failure signatures:** Performance drop on single task indicates numerical instability or incorrect transformation application; successful merging indicates insufficient parameter displacement or successful weight alignment
- **First 3 experiments:** 1) Apply MergeLock to ViT-B/32 model and verify accuracy on original task changes by <0.1% 2) Merge protected model with unprotected model from different task using Task Arithmetic, confirm accuracy drops to <5% 3) Implement alignment attack (optimizing rotation to minimize weight distance) on protected weights, attempt merging again, verify performance recovery is minimal (<10%)

## Open Questions the Paper Calls Out
- Can the unmergeable transformation strategy be effectively adapted for non-Transformer architectures, such as CNNs or MLPs, which lack the specific self-attention symmetries leveraged here?
- Is it possible to integrate the unmergeable constraints directly into the fine-tuning objective (training-time protection) rather than applying them as a post-hoc transformation?
- Is MergeLock robust against "high-cost" attacks that utilize auxiliary data to train a reverse transformation, or optimization-based methods that attempt to explicitly invert the random matrices?

## Limitations
- The method primarily focuses on Transformer architectures and may not generalize to other network types
- Security relies on secure storage of transformation matrices - if keys are compromised, protection fails
- Long-term security against future optimization breakthroughs cannot be assessed from current work

## Confidence
- **High:** Core mechanism (symmetry preservation in attention) is mathematically sound; empirical claim of >95% degradation is strongly supported
- **Medium:** Robustness against alignment attacks is demonstrated but relies on assumption that continuous alignment problem cannot be solved efficiently
- **Low:** Long-term security against future optimization breakthroughs cannot be assessed from single paper

## Next Checks
1. **Numerical Stability Audit:** Systematically test across random matrix initializations to find thresholds where performance degradation occurs in protected model
2. **Cross-Architecture Test:** Apply MergeLock to BERT model fine-tuned on two GLUE tasks and verify >95% degradation in merged model accuracy
3. **Alignment Attack Benchmarking:** Implement sophisticated gradient-based optimization with learned rotation against MergeLock and compare to simple Kabsch algorithm performance