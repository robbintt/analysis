---
ver: rpa2
title: Hey, That's My Data! Label-Only Dataset Inference in Large Language Models
arxiv_id: '2506.06057'
source_url: https://arxiv.org/abs/2506.06057
tags:
- dataset
- data
- training
- fine-tuning
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CatShift, a label-only dataset-inference
  framework for detecting unauthorized usage of proprietary datasets in large language
  models (LLMs). The method leverages catastrophic forgetting: if a suspicious dataset
  was part of the model''s training, fine-tuning on it causes a pronounced shift in
  the model''s outputs; if novel, the shift is smaller.'
---

# Hey, That's My Data! Label-Only Dataset Inference in Large Language Models

## Quick Facts
- arXiv ID: 2506.06057
- Source URL: https://arxiv.org/abs/2506.06057
- Reference count: 40
- Key result: CatShift achieves AUC 0.979 and F1 0.863 on Pythia-410M for label-only dataset inference

## Executive Summary
CatShift is a novel label-only dataset-inference framework that detects unauthorized usage of proprietary datasets in large language models by leveraging catastrophic forgetting. When a model is fine-tuned on data it previously encountered during training, internal representations partially degrade through subsequent training steps. Re-exposure to this data causes pronounced output shifts compared to training on novel data, creating a measurable signal that can be detected statistically without requiring log probabilities or other internal model states. The method compares similarity shifts against a known non-member validation set using statistical tests like Kolmogorov-Smirnov.

## Method Summary
CatShift operates by splitting a suspicious dataset into training (600 samples) and test (1000 samples) subsets, then fine-tuning the model using LoRA on specific layers (QKV, post-QKV linear, MLP) with rank=8, alpha=32, dropout=0.1, and learning rate 8×10⁻⁵. After fine-tuning, it computes BERT Score similarity between completions from the original and fine-tuned models, then applies statistical hypothesis testing to compare the suspicious dataset's similarity distribution against a known non-member validation set. Membership is inferred when the statistical test yields p-values below 0.1.

## Key Results
- On Pythia-410M, CatShift achieves AUC score of 0.979 and F1 score of 0.863
- For GPT-3.5 API, p-value of 6.44 × 10⁻⁵ for member books versus 0.711 for non-member books using BookMIA dataset
- Outperforms baseline methods that don't use fine-tuning for label-only membership inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on previously-seen training data triggers larger output shifts than on novel data
- Mechanism: Catastrophic forgetting partially degrades internal representations of original training data. Re-exposure produces a gradient decomposition where reactivation of forgotten representations dominates for member data, causing abrupt changes in argmax predictions.
- Core assumption: The model has genuinely forgotten but not eliminated representations of training data through subsequent training steps.
- Evidence anchors: Abstract states method "capitalizes on catastrophic forgetting"; section 3.6 notes "If Dsus was present in the original training set Dpre, ∆recover should be larger"
- Break condition: If training data heavily overlaps with other corpus content or CF is minimal, recovery signal becomes indistinguishable from learning noise.

### Mechanism 2
- Claim: Statistical comparison against a known non-member validation set discriminates member from non-member datasets
- Mechanism: Validation set provides baseline distribution for "novel data" shift magnitude. KS test or Mann-Whitney U detects when suspicious dataset shifts deviate significantly from this baseline.
- Core assumption: Dataset owner possesses genuinely non-member validation data (e.g., newly published material post-dating model training).
- Evidence anchors: Section 3.5 describes applying statistical tests to compare distributions; section 4.4 shows p-value of 6.44 × 10⁻⁵ for member books versus 0.711 for non-member books
- Break condition: Validation set contamination or domain mismatch with suspicious dataset invalidates baseline comparison.

### Mechanism 3
- Claim: Top-1 token completions alone provide sufficient signal for dataset membership inference
- Mechanism: Even without logits, the argmax of p(y|x) shifts more dramatically when parameters "refresh" forgotten representations versus when learning novel patterns from scratch.
- Core assumption: Fine-tuning API genuinely updates model weights; outputs are consistent and not deliberately randomized.
- Evidence anchors: Abstract emphasizes method works "without requiring log probabilities or other internal model states"; section 4.2 reports strong AUC and F1 scores on Pythia-410M
- Break condition: Provider applies output truncation, token randomization, or returns only partial completions.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The entire detection logic depends on understanding how sequential training overwrites prior representations.
  - Quick check question: Why would retraining on data the model "already knows" cause larger output changes than training on genuinely new data?

- Concept: Statistical hypothesis testing (KS test, Mann-Whitney U)
  - Why needed here: Membership decisions are made via p-value thresholds; misinterpreting these leads to wrong conclusions.
  - Quick check question: If KS test returns p = 0.85 for suspicious vs. validation distributions, what should you conclude about membership?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Method uses LoRA to fine-tune QKV, post-QKV linear, and MLP layers efficiently.
  - Quick check question: Why target attention layers specifically rather than embedding layers for this task?

## Architecture Onboarding

- Component map: Data splitter -> Prompt constructor -> LoRA fine-tuner -> Similarity scorer -> Hypothesis tester -> Dual-test decision

- Critical path: Validation set selection -> LoRA config (layers, rank) -> fine-tuning epochs -> similarity metric -> KS test threshold (α = 0.1)

- Design tradeoffs:
  - More epochs increase signal but raise API costs and overfitting risk
  - Strict p-threshold reduces false positives but increases false negatives
  - "Math" subset noted as challenging domain (Section 4.2)

- Failure signatures:
  - Member p-values consistently > 0.1: Under-fine-tuning, wrong layer targets, or data not actually in training
  - Non-member p-values < 0.1: Validation set contamination
  - High score variance: API non-determinism or hyperparameter instability

- First 3 experiments:
  1. Reproduce single PILE subset result on Pythia-410M to validate end-to-end pipeline
  2. Ablate fine-tuning epochs (1, 3, 5, 10) to find minimum viable signal strength
  3. Inject known contamination into validation set to characterize false-positive sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dataset inference be performed when only a subset of the suspicious dataset appears in the model's training corpus (partial membership)?
- Basis in paper: [explicit] The authors state in Future Work: "one promising direction is to explore partial membership inference, where only a subset of the suspicious dataset appears in the original training corpus."
- Why unresolved: Current CatShift assumes binary membership status; partial overlap introduces ambiguity in the magnitude of catastrophic forgetting signals.
- What evidence would resolve it: Controlled experiments varying the overlap ratio between suspicious datasets and training data, measuring how detection performance degrades as overlap decreases.

### Open Question 2
- Question: What are the optimal hyperparameter configurations (learning rate, epochs, layer selection) for maximizing CatShift's detection accuracy under limited budget constraints?
- Basis in paper: [explicit] The authors note: "Determining optimal hyperparameter configurations for membership inference, especially under limited budget constraints, remains an open research question."
- Why unresolved: The fine-tuning procedure introduces multiple hyperparameters that significantly affect output shifts, but systematic optimization guidance is lacking.
- What evidence would resolve it: Ablation studies across hyperparameter settings with cost-performance trade-off analysis on diverse LLM architectures and dataset types.

### Open Question 3
- Question: How robust is CatShift against adversarial perturbations deliberately introduced by model providers to obfuscate completions?
- Basis in paper: [explicit] Future work section: "extending CatShift to scenarios with stronger adversarial defenses (e.g., where the provider deliberately introduces perturbations to completions) would broaden its applicability in real-world settings."
- Why unresolved: Current evaluation assumes providers return genuine outputs; intentional perturbations could mask the catastrophic forgetting signal.
- What evidence would resolve it: Experiments simulating adversarial output manipulation strategies (noise injection, token randomization) measuring detection accuracy degradation and potential countermeasures.

### Open Question 4
- Question: How does dataset redundancy with existing training corpus content affect the detectability of catastrophic forgetting signals?
- Basis in paper: [explicit] Limitations section: "catastrophic forgetting may be weaker or more difficult to detect for certain classes of datasets, especially those that overlap substantially with other data in the model's original training corpus."
- Why unresolved: It remains unclear how to disentangle shifts from true reactivation versus partial redundancy effects.
- What evidence would resolve it: Experiments measuring detection performance on synthetic datasets with controlled semantic overlap with known pre-training corpora.

## Limitations
- Performance varies significantly across domains, with specialized domains like "Math" showing weaker detection capability
- Method assumes API providers return deterministic completions and actually update model weights during fine-tuning
- Requires genuinely non-member validation data that postdates model training, which may be difficult to establish for continuously updated models

## Confidence
- Detection Performance Claims: Medium confidence. Strong quantitative results on Pythia-410M but limited to controlled PILE subsets with no error bars
- Catastrophic Forgetting Mechanism: Low confidence. Theoretical mechanism is sound but lacks direct empirical validation or ablation studies
- Label-Only Advantage: Medium confidence. Successfully operates without logits but no benchmarking against logit-based approaches

## Next Checks
1. **Domain Robustness Test**: Systematically evaluate CatShift across diverse domains (math, code, legal, medical) using datasets of varying sizes and vocabularies. Measure how detection performance scales with dataset overlap percentage and compare against domain-specific baselines.

2. **API Provider Simulation**: Implement provider countermeasures (token randomization, output truncation, non-updating fine-tuning) and measure their impact on CatShift accuracy. Characterize the minimum perturbations needed to defeat detection and estimate the real-world risk.

3. **Validation Set Contamination Analysis**: Design controlled experiments where the validation set contains increasing percentages of member data. Quantify false positive rates and establish practical guidelines for validation set selection in ambiguous training contexts.