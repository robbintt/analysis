---
ver: rpa2
title: Multimodal Video Emotion Recognition with Reliable Reasoning Priors
arxiv_id: '2508.03722'
source_url: https://arxiv.org/abs/2508.03722
tags:
- multimodal
- emotion
- recognition
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal video emotion recognition method
  that integrates reliable reasoning priors from multimodal large language models
  (MLLMs) into a lightweight fusion framework. The authors use Gemini to generate
  fine-grained, modality-separable reasoning traces as priors during the fusion stage,
  enriching cross-modal interactions.
---

# Multimodal Video Emotion Recognition with Reliable Reasoning Priors

## Quick Facts
- **arXiv ID:** 2508.03722
- **Source URL:** https://arxiv.org/abs/2508.03722
- **Reference count:** 34
- **Primary result:** 84.68% accuracy on MER2024 benchmark using MLLM-derived reasoning priors

## Executive Summary
This paper presents a multimodal video emotion recognition framework that integrates reasoning priors from a Multimodal Large Language Model (MLLM) into a lightweight fusion architecture. The method extracts modality-separable reasoning traces from Gemini-2.0-exp, including Action Units, prosody, and semantics, and injects them as priors during fusion. To address class imbalance, the authors introduce Balanced Dual-Contrastive Learning (BDCL), which jointly balances inter-class and intra-class distributions. The framework achieves 84.68% accuracy and 84.70% F1-score on the MER2024 benchmark, outperforming existing methods by leveraging MLLM reasoning capabilities without requiring heavy model architectures.

## Method Summary
The approach uses a two-stage training pipeline with a lightweight multimodal fusion network. First, it employs Balanced Dual-Contrastive Learning to pre-train the fusion network on both labeled and pseudo-labeled data, optimizing for balanced class distributions. Second, it freezes the encoders and injects Gemini-derived reasoning priors (modality-specific feature embeddings and contribution weights) into the fusion layer to fine-tune the model. The reasoning priors provide explicit semantic anchors for cross-modal interactions, while BDCL prevents head classes from dominating gradient updates during representation learning.

## Key Results
- Achieves 84.68% accuracy and 84.70% F1-score on MER2024 benchmark
- Outperforms existing methods by integrating MLLM reasoning priors into lightweight fusion
- Balanced Dual-Contrastive Learning improves tail-class recognition through balanced negative sampling
- MLLM-derived priors provide explicit semantic guidance for cross-modal feature interactions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distilling modality-separable reasoning traces from a large model (MLLM) enriches cross-modal fusion by providing explicit semantic anchors.
- **Mechanism:** Gemini generates reasoning priors (Action Units, prosody, semantics) with contribution weights, injected into the fusion layer to guide the student model on where to look.
- **Core assumption:** MLLM generates reliable, non-hallucinated reasoning traces correlating with ground truth emotional features.
- **Evidence anchors:** [abstract] "We employ Gemini to generate fine-grained, modality-separable reasoning traces..."; [section 3.1] AU-support sets describe facial expressions through empirical patterns.
- **Break condition:** MLLM reasoning traces become inconsistent or hallucinated, providing noisy supervision that degrades performance.

### Mechanism 2
- **Claim:** Balanced Dual-Contrastive Learning improves tail-class recognition by preventing head classes from dominating gradient updates.
- **Mechanism:** BDCL modifies InfoNCE denominator to sample classes uniformly first, then instances, forcing equal repulsion from all classes.
- **Core assumption:** Feature space is sufficiently expressive to separate all classes if gradient pressure is balanced.
- **Evidence anchors:** [abstract] "jointly balances inter-class and intra-class distributions"; [section 3.2] balanced negative distribution prevents head classes from exerting disproportionate repulsive force.
- **Break condition:** Batch size too small to sample all classes, or inter/intra-modality weights are misconfigured.

### Mechanism 3
- **Claim:** Decoupling pre-training from prior-guided tuning stabilizes the learning process.
- **Mechanism:** Two-stage approach - Stage 1 focuses on robust feature clustering using BDCL, Stage 2 freezes base encoders and trains only prior-injection layers.
- **Core assumption:** Generic features from Stage 1 are sufficient, requiring only refinement rather than structural re-learning in Stage 2.
- **Evidence anchors:** [section 3.2] "first stage is Large-scale Semi-supervised Pre-training... second stage is Reliable Prior Guided Tuning"; [section 4.2] "pronounced uplift" when domain priors are infused.
- **Break condition:** Stage 1 features are poor (bad pseudo-labels), limiting prior-injection effectiveness in Stage 2.

## Foundational Learning

- **Facial Action Coding System (FACS):** Anatomical framework mapping facial movements to Action Units (e.g., AU6, AU12). Required to evaluate MLLM's visual reasoning quality. *Quick check:* Can you map "Happy" emotion to its constituent Action Units as defined in Figure 1?

- **InfoNCE / Contrastive Learning:** Loss function pulling positive pairs together and pushing negatives apart. Core to understanding why "balanced denominator" modification matters. *Quick check:* In standard InfoNCE, what happens to tail classes if 90% of batch is "Neutral"?

- **Knowledge Distillation (Teacher-Student):** Framework treating MLLM as teacher and fusion network as student. Critical to understand difference between "logit distillation" vs. "reasoning distillation" used here. *Quick check:* Why might distilling reasoning trace be more robust than raw probability logits for this task?

## Architecture Onboarding

- **Component map:** CLIP (Visual Encoder) -> Projector -> Fusion Core -> Head; HuBERT (Audio Encoder) -> Projector -> Fusion Core -> Head; BLOOM (Text Encoder) -> Projector -> Fusion Core -> Head; Gemini (External Teacher) -> Offline Reasoning Priors Generation

- **Critical path:**
  1. Offline: Run Gemini on video samples to generate $P = \{c^* | IV, IA, IT, R\}$
  2. Stage 1: Train full fusion network using BDCL on Labeled + Pseudo-labeled data
  3. Stage 2: Freeze encoders/projectors, inject prior embeddings into fusion stream, train adapters

- **Design tradeoffs:**
  - Latency vs. Accuracy: MLLM prior generation is computationally expensive (offline) but final model is lightweight
  - Complexity: Two-stage training adds orchestration complexity but handles semi-supervised data nature

- **Failure signatures:**
  - Modality Collapse: Prior generation fails, model over-relies on text modality
  - Overfitting to Priors: Stage 2 trained too long, memorizing MLLM reasoning errors

- **First 3 experiments:**
  1. **Sanity Check (Modality Ablation):** Run inference using only Audio vs. only Video features to verify encoders are loaded correctly
  2. **BDCL Visualization:** Train Stage 1 for 10 epochs with/without "Balanced" denominator, visualize t-SNE plots to confirm tighter clustering
  3. **Prior Injection Validity:** For single Stage 2 batch, verify Gemini contribution weights (e.g., Video 20%, Audio 30%) are applied to fusion weights

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the appropriate level of reasoning granularity required for different multimodal task scenarios?
- **Basis:** [explicit] Introduction states identifying appropriate reasoning granularity remains open despite advances in chain-of-thought procedures
- **Why unresolved:** Paper uses fine-grained reasoning traces but doesn't compare against coarser strategies to determine if computational cost is necessary
- **What evidence would resolve it:** Comparative study evaluating coarse vs. fine-grained reasoning priors on same benchmark

### Open Question 2
- **Question:** Can reasoning traces be reliably distilled from MLLMs to guide conventional non-LLM architectures across diverse domains?
- **Basis:** [explicit] Related Work notes whether reasoning traces can be distilled into reliable prior knowledge for conventional models remains open
- **Why unresolved:** Paper demonstrates success for emotion recognition but generalizability to other tasks/architectures unclear
- **What evidence would resolve it:** Applying framework to other tasks (e.g., action recognition) using standard CNN/Transformer backbones

### Open Question 3
- **Question:** How dependent is student model reliability on specific choice of teacher MLLM?
- **Basis:** [inferred] Method relies specifically on "Gemini-2.0-exp" for "trustworthy" priors, other LLMs show domain shift and lower zero-shot accuracy
- **Why unresolved:** Not investigated whether less capable or open-source MLLMs could generate sufficient reliable priors
- **What evidence would resolve it:** Ablation study replacing Gemini with open-source MLLMs (e.g., LLaVA variants) to measure performance degradation

### Open Question 4
- **Question:** Why does explicit dataset balancing degrade performance when combined with BDCL?
- **Basis:** [inferred] Table 3 shows "MER2024 + 1K, balanced" yields lower accuracy (77.34%) than "matched" sampling (78.73%) despite BDCL designed to handle imbalance
- **Why unresolved:** Suggests conflict where algorithmic balancing is negated by manually balancing dataset sample distribution
- **What evidence would resolve it:** Analysis of gradient distributions or feature space separation comparing BDCL on balanced vs. naturally distributed datasets

## Limitations
- Heavy reliance on MLLM's ability to generate accurate, modality-separable reasoning traces without hallucination, with limited empirical validation of reasoning quality
- BDCL assumes sufficient batch size to sample all classes uniformly, which may not hold for smaller datasets or extreme imbalance scenarios
- Two-stage training pipeline adds complexity and potential for Stage 1 features to be inadequate, limiting prior-injection effectiveness

## Confidence
- **High Confidence:** Overall architecture design and MER2024 benchmark results (84.68% accuracy) are well-documented and reproducible
- **Medium Confidence:** Mechanism by which reasoning priors enrich cross-modal fusion is theoretically sound but lacks direct ablation studies isolating prior contribution
- **Medium Confidence:** Balanced Dual-Contrastive Learning formulation is mathematically valid, but specific advantage over other imbalance mitigation techniques not directly compared
- **Low Confidence:** Assumption that MLLM-generated reasoning traces are consistently reliable across diverse video content is not thoroughly validated

## Next Checks
1. **Prior Quality Validation:** Implement systematic evaluation comparing MLLM reasoning traces against human-annotated AU patterns and prosody labels on held-out validation set, calculating precision/recall for extracted modality features

2. **BDCL Ablation Study:** Conduct controlled experiment comparing BDCL against standard InfoNCE and focal loss on same dataset, measuring both tail-class F1-score and overall accuracy to isolate balanced denominator benefit

3. **Single-Stage Training Baseline:** Train end-to-end model with MLLM priors injected during initial training (instead of two stages) to determine whether staged approach is strictly necessary or simpler training suffices