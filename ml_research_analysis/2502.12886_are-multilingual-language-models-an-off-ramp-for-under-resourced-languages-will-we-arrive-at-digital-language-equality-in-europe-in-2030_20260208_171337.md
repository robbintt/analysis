---
ver: rpa2
title: Are Multilingual Language Models an Off-ramp for Under-resourced Languages?
  Will we arrive at Digital Language Equality in Europe in 2030?
arxiv_id: '2502.12886'
source_url: https://arxiv.org/abs/2502.12886
tags:
- language
- languages
- data
- european
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether multilingual large language models (LLMs)
  can serve as a technological off-ramp for under-resourced European languages, addressing
  the digital language inequality where most European languages lack sufficient technological
  support. It highlights that while English and a few other languages are well-supported,
  many European languages are at risk of digital extinction due to insufficient training
  data.
---

# Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?

## Quick Facts
- arXiv ID: 2502.12886
- Source URL: https://arxiv.org/abs/2502.12886
- Reference count: 17
- Primary result: Multilingual LLMs show promise for under-resourced European languages through cross-lingual transfer, but optimal data requirements and training strategies remain open research questions.

## Executive Summary
This position paper examines whether multilingual large language models can serve as a technological off-ramp for under-resourced European languages facing digital extinction. While English and a few other languages are well-supported technologically, most European languages lack sufficient training data for robust NLP applications. The study explores how multilingual LLMs trained on data covering multiple languages can provide strong capabilities for under-resourced languages through cross-linguistic transfer, and investigates the potential of initiatives like the Common European Language Data Space to improve data availability. The authors conclude that while multilingual LLMs show promise, significant research gaps remain regarding optimal training data composition and quantity for under-resourced languages.

## Method Summary
This position/survey paper synthesizes existing research on multilingual LLMs and their potential to support under-resourced European languages. Rather than presenting new empirical results, it reviews current approaches, identifies open questions about data requirements and training strategies, and proposes directions for future research. The analysis draws on multiple sources including multilingual datasets (ROOTS, MADLAD-400, CulturaX, mC4, HPLT), existing multilingual models (BLOOM, EuroLLM, Teuken-7B, Salamandra), and initiatives like the Common European Language Data Space. The paper evaluates performance through benchmarks like the European LLM Leaderboard and identifies gaps in current evaluation methodologies.

## Key Results
- Multilingual LLMs exhibit strong capabilities for some under-resourced languages despite being trained on relatively small datasets for most languages
- Models trained on more equally distributed multilingual training data can compete with or outperform English-dominant models
- Training-efficient methods like in-context learning and layer-wise data sampling can improve performance without requiring more training data
- Cross-lingual transfer enables zero-shot and few-shot generalization from high-resource to under-resourced languages
- The Common European Language Data Space aims to address data scarcity through connector-based peer-to-peer marketplaces

## Why This Works (Mechanism)

### Mechanism 1: Cross-linguistic Transfer from High-resource Languages
- Claim: Multilingual LLMs can exhibit strong capabilities for under-resourced languages even when trained on minimal data from those languages
- Mechanism: Shared model parameters encode linguistic patterns from high-resource languages that transfer to under-resourced languages through latent cross-lingual representations, enabling zero-shot and few-shot generalization
- Core assumption: Linguistic similarity and shared structural patterns between languages allow representations learned from abundant data to generalize to data-scarce languages
- Evidence anchors:
  - Abstract states multilingual LLMs "do exhibit strong capabilities for some of these under-resourced languages"
  - Section 4 notes that despite small data sets for most languages, multilingual LLMs "perform well on benchmarks assessing the LLM's capabilities on these under-resourced languages and they also outperform monolingual LLMs"
  - Babel (arXiv:2503.00865) demonstrates open multilingual LLMs serving over 90% of global speakers through cross-lingual transfer

### Mechanism 2: Balanced Multilingual Training Data Distribution
- Claim: LLMs trained on more equally distributed multilingual datasets can compete with or outperform English-dominant models on under-resourced language tasks
- Mechanism: Proportional data allocation prevents English-centric bias from dominating parameter space, allowing under-resourced languages to establish meaningful representations
- Core assumption: Model capacity can be effectively partitioned across languages when training data is deliberately balanced
- Evidence anchors:
  - Section 4 states "models also show that LLMs trained on more equally distributed multilingual training data sets are able to compete with the models trained on predominantly English data and in some cases even outperform them"
  - Section 4 lists European multilingual LLMs (BLOOM, EuroLLM, Teuken-7B, Salamandra) designed with balanced European language coverage
  - Paramanu (arXiv:2401.18034) addresses the "curse of multilinguality" and tokenizer oversegmentation for morphologically rich low-resource languages through language-specific design

### Mechanism 3: Training-Efficient Methods for Data Scarcity
- Claim: In-context learning, layer-wise data sampling, and bootstrapping techniques can improve under-resourced language performance without requiring more training data
- Mechanism: These methods optimize how existing data is used during training/inference, extracting more signal from limited examples and improving cross-lingual word representation quality
- Core assumption: Under-resourced language data contains sufficient signal that can be amplified through smarter training procedures
- Evidence anchors:
  - Section 4 mentions "Bootstrapping word translation pairs from monolingual corpora (Hangya et al., 2022) can improve intrinsic cross-lingual word representation quality"
  - Section 4 references "In-context learning methods (Cahyawijaya et al., 2024), layer-wise data sampling (Pan et al., 2024) or increased architecture flexibility...are additional ways for improving the performance without the need for more training data"
  - Limited direct corpus evidence specifically validating these techniques for European under-resourced languages

## Foundational Learning

- Concept: **Cross-lingual Transfer**
  - Why needed here: This is the core technical mechanism enabling the "off-ramp" hypothesis—understanding how knowledge transfers between languages is essential for evaluating whether multilingual LLMs can realistically support under-resourced languages
  - Quick check question: Given a model trained predominantly on English and German, would you expect better transfer to Dutch or to Latvian, and why?

- Concept: **Digital Language Equality (DLE) Metric**
  - Why needed here: The paper uses DLE as the evaluation framework; understanding its components (technological factors + contextual factors) is necessary to interpret the severity of digital language inequality and measure progress
  - Quick check question: If a language has strong machine translation but poor speech recognition, how would this be reflected in a DLE score?

- Concept: **Data Sovereignty in Data Spaces**
  - Why needed here: The Common European Language Data Space (LDS) operates on data sovereignty principles; understanding this is critical for grasping why European language data remains locked and how the LDS connector architecture enables sharing
  - Quick check question: How does the LDS connector model differ from a centralized data repository, and what does this mean for data access negotiations?

## Architecture Onboarding

- Component map:
  Training Data Layer (ROOTS, MADLAD-400, CulturaX, mC4, HPLT, Wikipedia dumps) → Model Layer (BLOOM-46 languages, EuroLLM/Salamandra-EU-24+11, Teuken-7B-EU-24, Mistral Nemo-9) → Evaluation Layer (European LLM Leaderboard, GlobalMMLU, INCLUDE) → Data Infrastructure Layer (Common European Language Data Space with connector-based peer-to-peer marketplace)

- Critical path:
  1. Identify target under-resourced language(s) and assess available training data quantity/quality
  2. Select base multilingual LLM with appropriate language coverage OR design training mix with balanced multilingual distribution
  3. Apply training-efficient methods (in-context learning setup, layer-wise sampling) if data is severely limited
  4. Evaluate using culturally-debiased benchmarks (not just translated MMLU)

- Design tradeoffs:
  - Breadth vs. depth: Including more languages dilutes per-language capacity; fewer languages allow stronger representations but exclude communities
  - English as anchor vs. typological matching: Paper explicitly asks whether typologically similar high-resource languages transfer better than English—this remains an open research question
  - Data quantity vs. quality: "The more, the better" assumption is questioned; quality filtering may matter more for under-resourced languages

- Failure signatures:
  - Model outputs culturally inappropriate content (Western/US bias not corrected by translation)
  - Benchmark performance doesn't translate to real-world utility (translation artifacts in evaluation data)
  - Tokenizer oversegmentation for morphologically rich languages (indicates vocabulary wasn't designed for target language)
  - Performance cliff between high-resource and low-resource languages within the same model

- First 3 experiments:
  1. **Minimum viable data ablation**: Train identical model architectures with systematically varied amounts of under-resourced language data (e.g., 1MB, 10MB, 100MB) while holding high-resource data constant, measuring downstream task performance to identify the threshold where cross-lingual transfer activates
  2. **Typological transfer comparison**: Compare transfer efficiency from English vs. typologically similar high-resource languages (e.g., Spanish→Portuguese vs. English→Portuguese) to test whether linguistic similarity predicts transfer quality
  3. **Benchmark validity audit**: Evaluate the same multilingual LLM on both translated benchmarks (GlobalMMLU) and regionally-grounded benchmarks (INCLUDE) for the same under-resourced language to quantify the gap between translation-based and culturally-grounded evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much under-resourced language data is needed for a multilingual LLM to exhibit strong capabilities for that language?
- Basis in paper: [explicit] Authors state in Section 5: "How much under-resourced language data do we need for the multilingual LLM to exhibit strong capabilities for this language?" and note that current approaches use "as much data as possible" without systematic analysis
- Why unresolved: Current multilingual datasets (ROOTS, MADLAD-400, mC4, etc.) assume "the more, the better" without investigating minimum thresholds or diminishing returns for under-resourced languages
- What evidence would resolve it: Systematic experiments varying training data quantities for specific under-resourced languages while measuring downstream task performance

### Open Question 2
- Question: Should high-resource languages in multilingual pre-training be typologically similar to target under-resourced languages, or is English sufficient as the primary base language?
- Basis in paper: [explicit] Section 5 asks: "Are there specific constraints regarding the high-resource languages... Should these be, for example, typologically similar to the under-resourced language(s) or is using English sufficient?"
- Why unresolved: Studies show linguistic similarity affects cross-lingual transfer, but no systematic comparison of different high-resource language configurations has been conducted
- What evidence would resolve it: Controlled experiments comparing models trained with typologically matched vs. mismatched high-resource languages on under-resourced language benchmarks

### Open Question 3
- Question: Does including a diverse set of languages in pre-training data improve multilingual performance compared to fewer languages?
- Basis in paper: [explicit] Section 5 asks: "Does a diverse set of languages improve multilingual performance?" and earlier notes that factors like "linguistic similarity and script type" influence cross-linguistic transfer
- Why unresolved: Different multilingual LLMs use varying language counts (BLOOM: 46 languages; Mistral Nemo: 9; EuroLLM: 35), but the impact of typological diversity vs. quantity remains unclear
- What evidence would resolve it: Comparative studies of models trained with varying language diversity levels while controlling for total data volume and model capacity

## Limitations
- Optimal quantity and composition of training data for under-resourced European languages remains an open research question
- Effectiveness of cross-lingual transfer from high-resource to under-resourced languages is assumed but not empirically validated across diverse European language pairs
- Practical implementation and impact of the Common European Language Data Space remain largely theoretical at this stage

## Confidence
- **High Confidence**: The identification of digital language inequality as a pressing issue in Europe is well-established and supported by multiple sources
- **Medium Confidence**: The claim that multilingual LLMs can serve as a viable "off-ramp" for under-resourced languages is supported by observed performance on benchmarks, but practical utility remains uncertain
- **Low Confidence**: Specific quantitative claims about how much data is needed for different language types, or which high-resource languages provide the best transfer for specific under-resourced languages, are identified as open questions without definitive answers

## Next Checks
1. **Data Quantity Threshold Study**: Systematically vary training data amounts (1MB, 10MB, 100MB, 1GB) for specific under-resourced European languages while holding high-resource language data constant, then measure downstream task performance to identify the minimum threshold where cross-lingual transfer becomes effective
2. **Typological Transfer Experiment**: Compare cross-lingual transfer effectiveness from English versus typologically similar high-resource languages (e.g., Spanish→Portuguese vs English→Portuguese) using identical model architectures to empirically test whether linguistic similarity predicts transfer quality
3. **Culturally-Grounded Benchmark Validation**: Evaluate the same multilingual LLM on both translated benchmarks (GlobalMMLU) and culturally-grounded benchmarks (INCLUDE) for specific under-resourced European languages to quantify the gap between translation-based and culturally-grounded evaluation metrics