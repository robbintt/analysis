---
ver: rpa2
title: 'The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning
  in Audio LLMS'
arxiv_id: '2510.19055'
source_url: https://arxiv.org/abs/2510.19055
tags:
- tasks
- audio
- musical
- music
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MUSE Benchmark to evaluate fundamental
  music perception and auditory relational reasoning in multimodal large language
  models. The benchmark comprises 10 tasks grounded in music cognition research, testing
  invariant musical representations across pitch, rhythm, and harmony.
---

# The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS

## Quick Facts
- arXiv ID: 2510.19055
- Source URL: https://arxiv.org/abs/2510.19055
- Reference count: 0
- Primary result: Audio LLMs show severe deficits in invariant musical representations, with expert humans outperforming models on abstract relational reasoning tasks

## Executive Summary
This paper introduces the MUSE Benchmark to evaluate fundamental music perception and auditory relational reasoning in multimodal large language models. The benchmark comprises 10 tasks grounded in music cognition research, testing invariant musical representations across pitch, rhythm, and harmony. Four state-of-the-art models (Gemini Pro/Flash, Qwen2.5-Omni, Audio-Flamingo 3) were evaluated against 200 human participants including expert musicians. Results reveal significant performance gaps, with human experts consistently outperforming models on tasks requiring abstract relational reasoning. While Gemini Pro achieved competitive results on basic perception tasks, Qwen and Audio-Flamingo 3 performed at or near chance levels, exposing severe perceptual deficits. Chain-of-Thought prompting proved inconsistent and often detrimental. The findings suggest current models lack invariant musical representations and that bridging the human-machine gap will require fundamental changes in model architecture and training paradigms rather than simple prompting or scaling approaches.

## Method Summary
The study evaluated four audio LLMs on 10 music perception tasks using 200 custom musical stimuli (mean duration 14.1s). Tasks were grouped into beginner (instrument ID, melody shape, oddball detection, rhythm matching, pitch shift) and advanced (chord ID, syncopation, key modulation, chord sequence matching, meter ID) categories. Each task had 20 trials. Models were tested using both Standalone and Chain-of-Thought prompting conditions, with 3 random seeds per task. Human participants (N=200, including expert musicians) served as a baseline. Few-shot examples were provided to assess in-context learning capabilities. Audio-Flamingo 3 required concatenated audio due to input limitations.

## Key Results
- Human experts consistently outperformed all models on tasks requiring abstract relational reasoning
- Gemini Pro achieved competitive results on basic perception tasks but showed inconsistent Chain-of-Thought benefits
- Qwen2.5-Omni and Audio-Flamingo 3 performed at or near chance levels on most tasks
- Chain-of-Thought prompting often degraded performance, with models generating plausible but incorrect reasoning
- Few-shot scaling showed limited effectiveness, with significant positive effects on only one task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If models rely on statistical correlations of surface features (e.g., timbre, tempo) rather than abstract structural relationships, they will fail at tasks requiring perceptual invariance (e.g., recognizing a melody after pitch transposition).
- **Mechanism:** Current Audio LLMs appear to map acoustic features directly to labels without forming an intermediate "invariant representation"â€”a mental model of the music that remains stable despite surface changes. When a pitch shifts, the surface features change, breaking the model's correlation logic, whereas a human uses relative pitch to maintain the relationship.
- **Core assumption:** Success on standard benchmarks (like genre classification) is often driven by proxy cues (e.g., drum sounds in rock) rather than the cognitive logic implied by the task.
- **Evidence anchors:**
  - [Abstract] "While Gemini Pro succeeds on basic perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing severe perceptual deficits."
  - [Introduction] "...they may succeed by learning surface co-occurrences... rather than the relations that constitute musical structure."
  - [Corpus] SonicBench similarly identifies "physical perception bottlenecks" in LALMs, suggesting a lack of grounding in fundamental acoustic attributes.
- **Break condition:** If a model develops an internal representation where "melodic contour" is a distinct object from "absolute frequency," performance on Pitch Shift Detection would approach human levels without specific few-shot coaching.

### Mechanism 2
- **Claim:** Chain-of-Thought (CoT) prompting appears to degrade performance on perceptual tasks because the verbal reasoning steps generated by the model are often hallucinations loosely correlated with the audio, rather than true causal explanations of the perception.
- **Mechanism:** CoT forces the model to generate text that *sounds* like reasoning. However, if the underlying perceptual encoding is flawed (Mechanism 1), the text generation module prioritizes linguistic plausibility over acoustic accuracy, leading to "confident but wrong" rationales that derail the final answer.
- **Core assumption:** The language backbone dominates the audio encoder; if the audio features are weak, the LLM "hallucinates" the missing details to satisfy the linguistic prompt.
- **Evidence anchors:**
  - [Section 4.3] "Analysis of Gemini Pro's CoT logs reveals that the model often sounds correct while reasoning incorrectly... the precise off-beat counts were correct in only 4/37 [correct trials]."
  - [Section 4.3] "CoT degraded Gemini Pro's accuracy on Rhythm Matching... [and] worsened Qwen's... score."
  - [Corpus] Audio-CoT explores this reasoning capability, but MUSE provides evidence that such reasoning may be brittle or disconnected from sensory input in current architectures.
- **Break condition:** This mechanism holds as long as the model treats reasoning as a text-generation task. It breaks if the model is architected to ground every reasoning token in a verifiable audio feature.

### Mechanism 3
- **Claim:** In-context learning (few-shotting) fails to replicate human-like skill acquisition because it operates via temporary activation of related concepts rather than the permanent weight updates that occur during human musical training.
- **Mechanism:** Humans learn music by internalizing rules (implicit or explicit) that generalize across contexts. Models in this study used few-shot examples as "clues" for pattern matching. Increasing the number of shots (examples) did not consistently improve performance on abstract tasks (like Key Modulation) because the model lacks the architectural capacity to "learn" a new cognitive rule from definition alone.
- **Core assumption:** Musical expertise in humans is a structural change in cognitive processing, not just an increase in short-term context memory.
- **Evidence anchors:**
  - [Section 4.4] "While musical training in humans corresponds to the internalization of abstract rules... providing models with more examples is an unreliable proxy."
  - [Section 4.4] "A significant positive effect [for few-shot scaling] was found for only one task... [and] no statistically significant effect on model accuracy [for advanced tasks]."
  - [Corpus] Related works like MMAR highlight the need for "deep reasoning," which MUSE suggests cannot be achieved simply by scaling context or examples.
- **Break condition:** If a model were trained on a curriculum specifically designed to enforce rule consistency (e.g., endless transposition exercises), the few-shot scaling curve might steepen to match human learning.

## Foundational Learning

- **Concept: Invariant Representation**
  - **Why needed here:** This is the core deficit identified. You cannot debug a music model if you assume it "hears" a melody the way you do. You must understand it likely hears a bag of acoustic features without the structural "glue" holding them together.
  - **Quick check question:** If I play a song in a different key, does the model's internal embedding vector for that song change significantly, or does it cluster with the original key?

- **Concept: Surface vs. Structural Cues**
  - **Why needed here:** To avoid "false positives" in evaluation. A model might identify "Sadness" simply because the audio is slow (tempo) and quiet (dynamics), not because it understands the minor key or chord progression.
  - **Quick check question:** If I remove the rhythmic pattern but keep the chords, can the model still identify the genre? (Testing if it relies on rhythm as a proxy).

- **Concept: Symbol Grounding Problem**
  - **Why needed here:** This explains why CoT failed. The text "The chord changed from C to G" in the model's output is not necessarily grounded in the actual audio observation; it is a linguistic prediction based on the prompt context.
  - **Quick check question:** Does the model's accuracy drop if I force it to generate a reason *before* giving the answer? (If so, it relies on post-hoc rationalization rather than grounded reasoning).

## Architecture Onboarding

- **Component map:** Input: Audio Encoder -> Interface: Projection Layer -> Core: Large Language Model -> Output: Text generation
- **Critical path:**
  1. **Stimuli Creation:** Use the MUSE benchmark (10 tasks, 20 trials each)
  2. **Inference:** Run standalone prompts first (no CoT) to establish a baseline perceptual score
  3. **Analysis:** Compare performance on "Instrument ID" (surface) vs. "Pitch Shift" (structural). If "Pitch Shift" is near chance (50%), the model lacks relational encoding

- **Design tradeoffs:**
  - **Chat Mode (Stateful) vs. Single-Turn:** MUSE used Chat mode to maintain instructions. Tradeoff: Chat history can introduce "distraction" or bias from previous turns, but Single-Turn requires repeating complex instructions every time, increasing token cost
  - **Audio Concatenation (AF3):** For models that can't take dual inputs, concatenating audio with verbal cues ("Here is the first...") forces the model to perform source separation and temporal indexing, adding cognitive load and failure points

- **Failure signatures:**
  - **The "Chance" Plateau:** Accuracy clustering exactly at statistical chance (e.g., ~25% for 4-choice tasks) indicates the model is effectively guessing, meaning the signal is not reaching the reasoning engine
  - **The "Confident Hallucination":** As seen in Gemini Pro CoT logs, the model generates detailed, plausible-sounding music theory explanations that are factually incorrect regarding the specific audio input

- **First 3 experiments:**
  1. **Probing Invariance:** Run the Pitch Shift Detection task. If accuracy is >75%, the model has some invariant representation. If ~50%, it is strictly matching surface spectra
  2. **The CoT Stress Test:** Run a "Comparison" task (Rhythm Matching) with CoT. manually verify if the "counting" steps in the reasoning log match the actual audio events
  3. **Few-Shot Scaling Check:** Run a difficult task (Key Modulation) with 0, 2, 4, and 8 shots. Plot the curve. If it is flat (horizontal line), the model cannot learn this task via in-context learning alone

## Open Questions the Paper Calls Out
None

## Limitations
- Human baseline comparison may not fully represent professional-level musical training
- Fixed prompt structure across all models could mask architecture-specific optimizations
- Limited statistical power with 20 trials per task for detecting subtle performance differences
- Single evaluation protocol prevents assessment of alternative prompting strategies

## Confidence

- **High Confidence:** Claims about fundamental perceptual deficits in Qwen2.5-Omni and Audio-Flamingo 3, supported by consistent chance-level performance across multiple tasks
- **Medium Confidence:** Gemini Pro's superior performance on basic tasks, as results may be influenced by model-specific prompt engineering advantages
- **Medium Confidence:** Chain-of-Thought's inconsistent benefits, given the qualitative analysis of reasoning logs without systematic error categorization
- **Low Confidence:** The generalization that current architectures cannot achieve human-level musical reasoning without fundamental changes, based on a limited sample of four models

## Next Checks

1. **Cross-Paradigm Replication:** Test the same benchmark with models using different architectural approaches (e.g., symbolic audio representations, hierarchical temporal models) to determine if performance patterns hold across design philosophies

2. **Prompt Ablation Study:** Systematically vary prompt structures (standalone vs. chat, with/without CoT, different instruction specificity) for each model to isolate the contribution of prompting from underlying perceptual capabilities

3. **Curriculum Learning Extension:** Implement a staged training protocol where models are progressively exposed to more complex musical structures, testing whether rule-based learning through architectural design can overcome the in-context learning limitations identified