---
ver: rpa2
title: Progressive Element-wise Gradient Estimation for Neural Network Quantization
arxiv_id: '2509.00097'
source_url: https://arxiv.org/abs/2509.00097
tags:
- accuracy
- quantization
- pege
- training
- ewgs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Progressive Element-wise Gradient Estimation
  (PEGE), a novel backpropagation method for Quantization-Aware Training (QAT). PEGE
  addresses the limitations of the commonly used Straight-Through Estimator (STE)
  by introducing a logarithmic curriculum-driven mixed-precision replacement strategy
  and adaptive gradient estimation that incorporates discretization error.
---

# Progressive Element-wise Gradient Estimation for Neural Network Quantization

## Quick Facts
- arXiv ID: 2509.00097
- Source URL: https://arxiv.org/abs/2509.00097
- Reference count: 15
- Primary result: PEGE achieves 91.62% accuracy for 2-bit ResNet-20 on CIFAR-10, outperforming STE by 0.45%

## Executive Summary
This paper introduces Progressive Element-wise Gradient Estimation (PEGE), a novel backpropagation method for Quantization-Aware Training (QAT) that addresses the limitations of the Straight-Through Estimator (STE). PEGE incorporates a logarithmic curriculum-driven mixed-precision replacement strategy and adaptive gradient estimation that explicitly models discretization error between continuous and quantized values. The method formulates QAT as a co-optimization problem that simultaneously minimizes task loss and discretization error.

Experimental results demonstrate that PEGE consistently outperforms state-of-the-art backpropagation methods across various architectures and datasets. On CIFAR-10, PEGE achieves 91.62% accuracy for 2-bit ResNet-20, outperforming STE by 0.45% and EWGS by 0.97%. On ImageNet, PEGE improves 4-bit ResNet-18 accuracy by 0.21% compared to its full-precision counterpart, while STE reduces accuracy by 0.5-1.8%. The method enables low-precision models to match or even surpass full-precision accuracy, with ResNet-20 achieving 91.62% versus 91.95% for the full-precision model.

## Method Summary
PEGE is a backpropagation method for QAT that replaces the STE with a more sophisticated gradient estimator. The forward pass uses standard uniform quantization with clipping and rounding operations. The backward pass introduces two key innovations: (1) progressive replacement of full-precision weights and activations with quantized counterparts via a Bernoulli random variable with a logarithmic scheduling strategy, and (2) adaptive gradient estimation that adds a discretization error term proportional to the difference between continuous and quantized values. The method uses an exponential curriculum learning strategy to update the error scaling factor μ, starting small and increasing toward μ_max as training progresses. PEGE works with various forward quantization methods including EWGS and PACT, and was evaluated using Adam optimizer on CIFAR-10 and Nesterov SGD on ImageNet with cosine learning rate schedules.

## Key Results
- CIFAR-10: PEGE achieves 91.62% accuracy for 2-bit ResNet-20, outperforming STE by 0.45% and EWGS by 0.97%
- ImageNet: PEGE improves 4-bit ResNet-18 accuracy by 0.21% compared to full-precision baseline
- Training efficiency: PEGE requires 3× training epochs but achieves higher accuracy with negligible computational overhead
- Generalization: PEGE consistently outperforms STE across different bit-widths (1-4 bits) and architectures (ResNet-20, VGG-16, ResNet-18)

## Why This Works (Mechanism)

### Mechanism 1: Logarithmic Curriculum-Driven Precision Replacement
A Bernoulli random variable r_T ~ Bernoulli(p_T) determines whether quantized values replace full-precision values at each training step T. The replacing rate follows p_T = min(log_k(T) + b/B, 1.0), providing a smooth early-training phase where full-precision gradients dominate, then accelerating replacement as the model stabilizes. This outperforms linear (90.54%), constant (89.82%), exponential (90.32%), and cosine (89.54%) schedulers.

### Mechanism 2: Discretization Error-Adjusted Gradient Estimation
PEGE augments STE's gradient approximation with a term proportional to discretization error: ∂L/∂x_c = ∂L/∂x_q + μ·(x_c - x_q). This correction pulls latent full-precision values toward quantization points while maintaining task-relevant gradients, addressing STE's oversight of discretization errors.

### Mechanism 3: Exponential Scheduling of Error Scaling Factor
The error scaling factor μ_T follows μ_T = μ_max · (1 - e^(-kT)), starting small and growing exponentially. This ensures early training stability with task loss dominating, then increasingly penalizes discretization errors as the model approaches convergence.

## Foundational Learning

- **Concept: Straight-Through Estimator (STE)**
  - Why needed here: STE is the baseline PEGE improves upon. Understanding its limitation—treating ∂L/∂x_c = ∂L/∂x_q as identity—explains why discretization error accumulates.
  - Quick check question: Given a quantization function Q(x) = round(x), what is ∂Q/∂x and why does STE ignore it?

- **Concept: Discretization Error**
  - Why needed here: The core innovation is explicitly modeling (x_c - x_q), the gap between latent continuous values and their quantized representations, as a gradient correction signal.
  - Quick check question: For a 2-bit quantizer with levels {0, 0.33, 0.67, 1.0}, what is the discretization error for x_c = 0.45?

- **Concept: Curriculum Learning**
  - Why needed here: Both the replacement rate p_T and error scaling μ_T follow curriculum schedules—starting easy (mostly full-precision, small error correction) and progressively increasing difficulty.
  - Quick check question: Why might a logarithmic schedule outperform a linear schedule for precision replacement?

## Architecture Onboarding

- **Component map**: Forward quantizer (Clip → Round) → Bernoulli replacement mask (p_T) → Gradient computation (∂L/∂x_q + μ_T·(x_c - x_q))

- **Critical path**: 
  1. Initialize with pre-trained full-precision model
  2. Set hyperparameters: B (log base), b (basic replacing rate), k (coefficient), μ_max, k for μ scheduling
  3. For each training step T: compute p_T, sample replacement mask, compute μ_T, apply corrected gradients
  4. Monitor: replacing rate curve, discretization error magnitude, validation accuracy

- **Design tradeoffs**:
  - **Longer training**: PEGE benefits from 3× training (Table 3: +0.57% accuracy); plan extended schedules.
  - **Forward method flexibility**: PEGE works with EWGS, PACT, and other quantizers; choose forward method based on hardware constraints, not backward compatibility.
  - **Scheduler selection**: Logarithmic p_T + Exponential μ_T is validated; other combinations not tested.

- **Failure signatures**:
  - Accuracy plateaus below full-precision: Check if p_T starts too high or grows too fast (constant/exponential schedulers underperformed).
  - Training instability: μ_max may be too large; reduce and verify exponential schedule is applied.
  - No improvement over STE: Verify both mechanisms are active (replacement + error correction); ablation in Table 4 shows "None" scheduler yields 90.35% vs 90.90% with logarithmic.

- **First 3 experiments**:
  1. **Sanity check**: Replicate Table 1 (ResNet-20, CIFAR-10, W2A2) with PEGE vs STE; expect ~0.45% improvement.
  2. **Scheduler ablation**: Compare logarithmic vs linear vs constant p_T schedulers on a single architecture; expect results matching Table 4 ranking.
  3. **Forward method test**: Apply PEGE with PACT forward method on ImageNet (Table 2 setup); expect ~0.21% improvement over full-precision baseline.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations section and broader context of quantization research.

## Limitations
- Hyperparameter sensitivity: Performance depends critically on logarithmic scheduler parameters (B, b, k) and exponential μ scheduler parameters (μ_max, k) which are not fully specified.
- Extended training requirements: PEGE requires 3× training epochs compared to standard approaches, though claims negligible computational overhead.
- Limited architecture coverage: Evaluation restricted to VGG and ResNet architectures, leaving generalization to MobileNet, Transformers, and other architectures unverified.

## Confidence
- **High confidence**: The empirical results showing PEGE outperforming STE on CIFAR-10 and ImageNet benchmarks (91.62% vs 91.17% on 2-bit ResNet-20, 0.21% improvement over full-precision on 4-bit ImageNet).
- **Medium confidence**: The theoretical justification for the logarithmic curriculum-driven replacement strategy, supported by ablation studies but lacking broader validation across different model families.
- **Medium confidence**: The discretization error correction mechanism, with strong experimental support but limited theoretical grounding compared to the extensive STE literature.

## Next Checks
1. **Ablation of scheduler parameters**: Systematically vary B, b, and k in the logarithmic replacement scheduler and μ_max in the exponential error scaling to determine sensitivity and optimal ranges, replicating the Table 4 ablation but with finer granularity.
2. **Cross-architecture generalization**: Apply PEGE to transformer-based models (e.g., ViT) and non-vision architectures to test whether the logarithmic/exponential curriculum schedules remain optimal across different network types.
3. **Theoretical analysis of gradient correction**: Derive the expected bias introduced by the discretization error correction term ∂L/∂x_c = ∂L/∂x_q + μ·(x_c - x_q) and compare it against the STE approximation error, providing mathematical justification for the exponential scheduling of μ.