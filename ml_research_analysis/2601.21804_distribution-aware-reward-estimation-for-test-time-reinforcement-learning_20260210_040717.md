---
ver: rpa2
title: Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning
arxiv_id: '2601.21804'
source_url: https://arxiv.org/abs/2601.21804
tags:
- reward
- dare
- arxiv
- rollouts
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward estimation in test-time reinforcement
  learning (TTRL), where models must self-improve without ground-truth supervision.
  The key problem is that majority voting (MV) over rollouts loses information about
  non-majority but correct actions and introduces systematic bias, leading to confirmation
  collapse.
---

# Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21804
- Source URL: https://arxiv.org/abs/2601.21804
- Reference count: 22
- Primary result: DARE improves optimization stability and performance over TTRL baselines, achieving 25.3% relative improvement on AIME 2024 and 5.3% on AMC

## Executive Summary
This paper addresses reward estimation in test-time reinforcement learning (TTRL), where models must self-improve without ground-truth supervision. The key problem is that majority voting (MV) over rollouts loses information about non-majority but correct actions and introduces systematic bias, leading to confirmation collapse. The authors propose Distribution-Aware Reward Estimation (DARE), which estimates rewards from the full empirical rollout distribution rather than a single majority outcome. DARE augments this with an exploration bonus to encourage rare but high-quality rollouts and distribution pruning to remove low-probability noisy samples. Extensive experiments show DARE improves optimization stability and performance over TTRL baselines, achieving 25.3% relative improvement on AIME 2024 and 5.3% on AMC. The method also demonstrates better out-of-distribution generalization and faster convergence. Theoretical analysis confirms MV's limitations in information preservation and bias under correlated rollouts.

## Method Summary
DARE computes uncertainty-aware distribution p̂(ŷ) = n(ŷ)/(u(ŷ)+ε) normalized, adds exploration bonus b(yi) = (1−n(yi)/M)·(1−u(yi)), applies distribution pruning at threshold τ, then computes final reward r(yi) = p̃(yi) + α·b̃(yi). Updates policy via GRPO. The method generates M rollouts per query, computes empirical frequency and token-level uncertainty for each unique answer, forms an uncertainty-weighted distribution, prunes low-probability rollouts, adds an exploration bonus proportional to rarity and confidence, and uses these shaped rewards in the GRPO optimizer.

## Key Results
- DARE achieves 25.3% relative improvement on AIME 2024 benchmark
- DARE shows 5.3% improvement on AMC benchmark
- DARE demonstrates better out-of-distribution generalization and faster convergence compared to TTRL baselines

## Why This Works (Mechanism)

### Mechanism 1: Distributional Preservation over Information Collapse
Replacing binary majority voting with an uncertainty-aware empirical distribution preserves actionable signals from non-majority candidates. MV maps rollouts to a single pseudo-label, discarding probability mass of non-majority answers. DARE computes base reward r_dis(y_i) = p̂(y_i) using Eq. (10), weighing frequency inversely by token-level uncertainty u(ŷ), retaining gradient signal for minority candidates with low uncertainty.

### Mechanism 2: Targeted Exploration via Uncertainty-Scaled Bonus
An exploration bonus allows the policy to escape "confirmation collapse" by reinforcing low-frequency, high-confidence rollouts that MV ignores. DARE adds bonus b(y_i) proportional to (1−frequency)·(1−uncertainty), selectively boosting rare actions the model is confident in, encouraging shift from spurious majority to latent correct minority.

### Mechanism 3: Variance Reduction via Distribution Support Pruning
Pruning low-probability rollouts stabilizes policy updates by removing high-variance noise. DARE applies threshold τ to remove rollouts with empirical probability p̂(y_i) < τ, ensuring exploration bonus and base rewards are computed only on plausible support set, normalizing signal over significant candidates.

## Foundational Learning

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR) vs. TTRL**
  - Why needed: DARE operates in label-free setting (TTRL), distinguishing it from RLVR with ground-truth verification
  - Quick check: Can you explain why standard RLVR rewards are considered "dense" or "verifiable" compared to the "pseudo-rewards" constructed in TTRL?

- **Concept: Confirmation Bias in Self-Training**
  - Why needed: Paper frames MV failure as "confirmation collapse"; understanding how reinforcing own predictions amplifies errors is key to seeing why exploration bonus is necessary
  - Quick check: If a model initially favors an incorrect answer 70% of the time, why would standard Majority Voting accelerate this error?

- **Concept: Token-Level Entropy as Uncertainty**
  - Why needed: DARE relies on u(ŷ) (Eq. 9), average token entropy, to distinguish "reliable" from "unreliable" paths; understanding this measures internal confusion, not correctness
  - Quick check: Does low token entropy guarantee a correct answer? If not, how does DARE mitigate the risk of reinforcing confident errors?

## Architecture Onboarding

- **Component map:** Rollout Sampler -> Uncertainty Calculator -> Distribution Engine -> Reward Shaper -> Optimizer
- **Critical path:** The calculation of the Exploration Bonus (Eq. 12). This relies on accurate normalization of both frequency counts and uncertainty scores. A bug here will either fail to promote correct minorities (bonus too low) or destabilize training by promoting noise (bonus too high).
- **Design tradeoffs:** Sensitivity vs. Stability (pruning threshold τ trades discovery of rare correct answers against variance reduction); Compute Overhead (DARE requires tracking token-level entropies for all unique answers, adding overhead to standard "majority count" operation)
- **Failure signatures:** Stagnation (model converges to suboptimal policy that is highly confident but wrong); Oscillation (loss curves spike; likely caused by pruning threshold that is too low)
- **First 3 experiments:**
  1. Validate Information Preservation: Run DARE on synthetic task where correct answer is always minority (frequency 30%). Verify if DARE recovers correct answer faster than TTRL.
  2. Ablation on Uncertainty: Replace uncertainty-weighted p̂ with simple frequency counts. Check if performance drops on benchmarks with high "spurious" majority rates.
  3. Pruning Sensitivity: Sweep threshold τ on small validation set. Plot accuracy vs. τ to ensure default doesn't cut off valid long-tail reasoning paths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DARE maintain performance advantages over majority voting on significantly larger models (e.g., >7B parameters)?
- Basis: Appendix C notes "diminishing marginal gains for simple reward shaping" on 7B models due to induced response diversity
- Why unresolved: Unclear if information loss from majority voting is as severe in models with intrinsically higher diversity and reasoning capacity
- What evidence would resolve it: Comparative evaluations of DARE versus TTRL on 70B+ parameter models on reasoning benchmarks

### Open Question 2
- Question: Can distribution-based rewards be effectively adapted for open-ended generation tasks lacking verifiable outcomes?
- Basis: Introduction and Related Work claim method targets scenarios without external supervision, yet experiments rely on reasoning tasks with verifiable answers
- Why unresolved: DARE relies on grouping rollouts by exact answer equivalence; open-ended text generation lacks discrete equivalence classes
- What evidence would resolve it: Application of DARE to creative writing or dialogue tasks using semantic clustering or embedding distances to define distribution

### Open Question 3
- Question: What is the theoretical validity of token-overlap as a proxy for latent correlation structures?
- Basis: Appendix D states token overlap is "not a full statistical dependence measure" but an "interpretable proxy"
- Why unresolved: Theoretical analysis relies on latent variables (Z) to prove bias, but implementation uses surface-level heuristic which may not capture deep semantic correlations
- What evidence would resolve it: Study correlating token overlap with other statistical dependence measures in predicting estimation bias

## Limitations

- Uncertainty signal validity: The core mechanism relies on token-level entropy as proxy for reasoning quality, but there's no explicit validation that low-entropy traces are systematically more accurate
- Hyperparameter sensitivity: Method introduces three key hyperparameters (α, τ, ε) without extensive sensitivity analysis, which could affect reproducibility
- Generalization to open-generation tasks: DARE evaluated on math/science reasoning benchmarks; effectiveness on open-ended generation tasks unclear

## Confidence

- **High Confidence:** Claim that Majority Voting discards information about non-majority but correct actions is well-supported by theoretical analysis and validated by prior work
- **Medium Confidence:** Claim that DARE's exploration bonus effectively recovers correct minority answers is supported by experimental results but underlying assumption requires further validation
- **Medium Confidence:** Claim that pruning improves optimization stability is supported by stated goal but needs more empirical backing in TTRL literature

## Next Checks

1. Validate Uncertainty-Correctness Correlation: On held-out validation set, compute correlation between token-level entropy (u(ŷ)) and actual answer accuracy to confirm uncertainty signal is reliable proxy for quality

2. Ablation on Exploration Bonus: Run ablation study comparing DARE with and without exploration bonus (α=0) to isolate contribution of exploration mechanism

3. Pruning Sensitivity Sweep: Systematically sweep pruning threshold τ across range (0.0 to 0.1) on small benchmark, plot pass@1 accuracy against τ to identify optimal range and ensure default value doesn't overly penalize rare but valid answers