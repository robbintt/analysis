---
ver: rpa2
title: Unifying Linear-Time Attention via Latent Probabilistic Modelling
arxiv_id: '2402.17512'
source_url: https://arxiv.org/abs/2402.17512
tags:
- attention
- linear
- standard
- latte
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits linear-time attention mechanisms through the
  lens of probabilistic graphical models, introducing a novel directed parameterisation
  called Latte. The authors show that standard linear attention can be interpreted
  as an undirected latent variable model, and then propose a directed variant that
  introduces asymmetric dependencies, better reflecting the causal nature of language.
---

# Unifying Linear-Time Attention via Latent Probabilistic Modelling

## Quick Facts
- arXiv ID: 2402.17512
- Source URL: https://arxiv.org/abs/2402.17512
- Authors: Rares Dolga; Lucas Maystre; Marius Cobzarenco; David Barber
- Reference count: 15
- Key outcome: Introduces Latte, a directed latent variable parameterisation of linear attention that achieves competitive performance with standard attention while maintaining linear complexity.

## Executive Summary
This paper introduces Latte, a novel approach to linear-time attention that leverages probabilistic graphical models to capture the directed, causal nature of language. The authors show that standard linear attention can be interpreted as an undirected latent variable model, which fails to reflect the asymmetric dependencies inherent in sequential data. Latte addresses this by introducing a directed parameterisation that uses separate query-latent and latent-key similarities, combined with a unified probabilistic framework that integrates local sliding-window attention with global latent attention.

## Method Summary
Latte reformulates linear attention as a probabilistic latent variable model, introducing a directed parameterisation that captures asymmetric dependencies. The model maintains linear complexity through recursive accumulator updates using parallel scans. A special latent state is dedicated to local sliding-window attention, integrated within a single normalized probability distribution. The architecture uses a recurrent Gated Linear Unit (RGLRU) for positional encoding, avoiding explicit relative positional encodings. The core mechanism involves computing query-latent and latent-key similarities separately, then accumulating value states recursively to produce the final output.

## Key Results
- Latte achieves perplexity of approximately 17.6 on 153M-parameter models, competitive with standard attention
- Outperforms existing linear attention variants on language modeling benchmarks
- Maintains linear computational complexity while capturing directional dependencies better than symmetric linear attention
- Successfully integrates local and global attention within a unified probabilistic framework

## Why This Works (Mechanism)

### Mechanism 1: Directed Latent Variable Parameterisation
The directed parameterisation captures causal structure more effectively than symmetric standard linear attention by introducing asymmetric dependencies. Standard linear attention is interpreted as an undirected graphical model (Markov Random Field), which implies symmetric influence between tokens. Latte introduces a directed structure where a query token $t$ influences a latent state $l$, which in turn influences the key token $s$ (conceptually $t \to l \to s$). This forces the model to learn generative, directional relationships rather than just pairwise similarities.

### Mechanism 2: Unified Probabilistic Normalisation (Latte Macchiato)
The model integrates local sliding-window attention and global latent attention within a single normalized probability distribution. A special latent state $l=0$ is dedicated to local standard attention, and the total attention weight is a mixture $p(l|t)$ over all latent states ($0$ to $L$). This prevents the attention bias seen in naive hybrid models by ensuring normalization over all states.

### Mechanism 3: Recurrent Positional Encoding via RGLRU
Latte uses a Recurrent Gated Linear Unit (RGLRU) to process Queries and Keys before they enter the latent attention mechanism, creating a "time-aware" representation where position is implicitly encoded in the recurrent state. This restores positional awareness in linear attention without breaking the recurrent state accumulation, avoiding the need for explicit relative positional encodings.

## Foundational Learning

- **Concept: Probabilistic Graphical Models (Directed vs. Undirected)**
  - Why needed here: The core contribution relies on distinguishing between undirected (symmetric) and directed (asymmetric) latent variable models to explain why standard linear attention fails on language.
  - Quick check question: Can you explain why a Markov Random Field (undirected) implies symmetry while a Bayesian Network (directed) allows for causal flow?

- **Concept: Kernelized Attention (Linear Attention)**
  - Why needed here: The method builds upon the standard linear attention approximation ($\text{Sim}(q, k) \approx \phi(q)^T \phi(k)$) and modifies the aggregation step.
  - Quick check question: How does the associativity property of matrix multiplication $(QK^T)V = Q(K^TV)$ reduce complexity from quadratic to linear?

- **Concept: Gated Linear Recurrence (RGLRU/SSMs)**
  - Why needed here: The architecture uses a specific recurrent unit (RGLRU) for positional mixing, distinct from the attention mechanism itself.
  - Quick check question: How does a gated recurrence (like an RNN or SSM) differ from a simple rolling average (convolution) in terms of input-dependence?

## Architecture Onboarding

- **Component map:** Input embeddings $X$ -> RGLRU positional mixer -> Projectors (Q, K, V) -> Latent Attention (Latte) -> Local Attention (Latte Macchiato) -> Weighted mixture of Local ($l=0$) and Global ($l>0$) outputs

- **Critical path:** The recursive update of the accumulators $\alpha_{t,l}$ and $\tilde{v}_{t,l}$ (Eq. 14). Implementation must use a parallel scan (e.g., `jax.lax.scan` or `lax.associative_scan`) to maintain $O(N)$ complexity while processing the recurrent state updates.

- **Design tradeoffs:**
  - Latent Dimension ($L$): Higher $L$ increases capacity but increases memory and compute slightly. Paper uses $L=128$ or $256$.
  - SWA Window Size: Must be balanced with $L$. Small window + small $L$ = under-parameterization.
  - Implementation: A pure JAX/PyTorch scan is slower than a CUDA kernel for standard attention, but asymptotically wins at very long sequences ($>16k$).

- **Failure signatures:**
  - Latent Collapse: If regularisation is insufficient, the model might only utilize a small subset of the $L$ latent states.
  - Inference Drift: In autoregressive generation, if the recurrent state $\tilde{v}$ accumulates numerical errors, output quality may degrade over thousands of steps.

- **First 3 experiments:**
  1. **Synthetic MQAR:** Verify recall capabilities. Implement the MQAR task (associative recall) to ensure the recurrent state actually stores and retrieves key-value pairs accurately compared to vanilla linear attention.
  2. **Ablation on Positional Mixing:** Train three small models on OpenWebText: (1) Latte with RoPE, (2) Latte with Conv, (3) Latte with RGLRU. Compare perplexity to validate the claim that recurrent positional mixing is superior.
  3. **Context Extrapolation Test:** Train a model on 2k/4k sequences and evaluate Perplexity (PPL) at 8k and 16k. Compare against standard attention to verify that Latte maintains low PPL while standard attention degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Latte framework be effectively extended to sequence-to-sequence generation and multimodal modeling tasks?
- Basis in paper: [explicit] The conclusion states, "Future work will explore extensions to tasks such as question answering, sequence-to-sequence generation, and multimodal modeling."
- Why unresolved: The current experiments are restricted to language modeling (autoregressive) and classification (LRA) benchmarks.
- What evidence would resolve it: Successful application of Latte to machine translation or image-text tasks with performance comparable to standard transformers.

### Open Question 2
- Question: How can Latte's architecture be modified to improve performance on continuous data domains?
- Basis in paper: [inferred] Section 5.3 notes that while Latte excels at discrete tasks, "performance on image-based (continuous) tasks remains weaker" compared to state-space models like S5.
- Why unresolved: The probabilistic formulation appears biased toward discrete language structures, creating a performance gap on continuous benchmarks.
- What evidence would resolve it: Modifications that allow Latte to match or exceed the accuracy of S5 or Mega on the LRA Image Pathfinder benchmark.

### Open Question 3
- Question: Can the probabilistic interpretation of linear attention be generalized to include non-normalized recurrent architectures?
- Basis in paper: [inferred] Section 4 and Section 6 explicitly state that models like Mamba "lack the normalization needed" to fit the proposed probabilistic framework.
- Why unresolved: The framework currently relies on interpreting attention as a probability distribution, which requires strict normalization.
- What evidence would resolve it: A theoretical extension that successfully maps unnormalized state-space updates to a valid probabilistic graphical model.

## Limitations

- **Latent Collapse Risk:** The claim that latent states remain well-distributed relies primarily on visual inspection rather than quantitative metrics, with additional parameters that could theoretically produce degenerate solutions.
- **Implementation Complexity:** The Latte-Macchiato hybrid requires careful implementation of recursive accumulator updates using parallel scans, with numerical stability details not fully derived from equations.
- **Generalization Across Domains:** While demonstrating strong performance on language modeling, the approach acknowledges weaker results on continuous domains like images and pathfinding.

## Confidence

**High Confidence (8/10):**
- The theoretical foundation linking undirected linear attention to Markov Random Fields is well-established.
- The performance improvement over vanilla linear attention on language modeling benchmarks is clearly demonstrated with controlled experiments.
- The runtime complexity analysis (linear vs quadratic) is mathematically sound.

**Medium Confidence (6/10):**
- The claim that directed parameterisation better captures causal structure relies on intuitive arguments rather than ablation studies specifically testing causal relationships.
- The superiority of RGLRU over RoPE for positional encoding is asserted but not empirically validated through direct comparison in the main experiments.
- The claim that Latte-Macchiato prevents attention bias in hybrid models is supported by experimental results but the mechanism could benefit from more rigorous analysis.

**Low Confidence (4/10):**
- The assertion that latent states remain well-distributed throughout training is primarily visual and lacks quantitative metrics.
- The exact contribution of each component to overall performance is not isolated through proper ablation studies.

## Next Checks

1. **Latent State Utilization Analysis:** Implement quantitative metrics to measure latent state utilization (entropy of $p(l|t)$ distribution, effective number of states used) across layers and heads. Plot these metrics during training to empirically verify that states remain distributed and do not collapse.

2. **Ablation on Positional Encoding:** Train three identical models varying only the positional encoding: (a) Latte with RoPE, (b) Latte with Conv, (c) Latte with RGLRU. Evaluate on OpenWebText to isolate the contribution of recurrent positional encoding to the claimed performance gains.

3. **Synthetic Causal Task Evaluation:** Design a synthetic language modeling task where token generation depends on specific causal relationships (e.g., "A causes B, B causes C"). Train both standard linear attention and Latte on this task and measure their ability to capture and reproduce the causal dependencies through attention visualization and perplexity metrics.