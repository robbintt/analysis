---
ver: rpa2
title: A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing
  Relationships Extraction
arxiv_id: '2506.18424'
source_url: https://arxiv.org/abs/2506.18424
tags:
- sizing
- analog
- optimization
- circuits
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large language model-based multi-agent
  framework to extract sizing relationships from analog circuit academic papers. The
  extracted relationships are used to prune the search space during circuit optimization,
  significantly improving efficiency.
---

# A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction

## Quick Facts
- arXiv ID: 2506.18424
- Source URL: https://arxiv.org/abs/2506.18424
- Authors: Chengjie Liu; Weiyu Chen; Huiyao Xu; Yuan Du; Jun Yang; Li Du
- Reference count: 33
- Optimization efficiency improvement: 2.32-26.6× speedup across multiple circuit types

## Executive Summary
This paper introduces a multi-agent LLM framework that extracts sizing relationships from analog circuit academic papers to improve circuit optimization efficiency. The approach uses specialized expert and employee agents that cooperate and debate to extract device parameters and quantitative relationships, which are then structured and used to constrain optimization search spaces. Tested on operational amplifiers, bandgap references, and LDOs across different process nodes, the method demonstrates significant optimization speedups while maintaining or improving solution quality compared to conventional algorithms.

## Method Summary
The framework extracts sizing relationships from analog circuit papers using a multi-agent LLM system with LangChain. Expert agents (GPT-o1) extract sizing relationships independently each round without participating in debate, while employee agents (GPT-4o or Claude3.5-opus) engage in cooperation and debate through a shared message pool. The system runs for 5 discussion rounds with structured output format (device name, related parameters, quantitative relationship). Paper preprocessing uses Mathpix for text extraction and Yolo-V8 + OCR for circuit diagram to netlist conversion. Extracted relationships constrain the search space for MACE optimizer on TED platform, with batch size 40, max 100 iterations, and initial points 40.

## Key Results
- Optimization efficiency improvements of 2.32× to 26.6× across three circuit types (BGR, OP, LDO)
- Successful optimization in cases where conventional methods failed
- Robust performance across different process nodes (SMIC 180nm and TSMC 65nm)
- 11-12 valid sizing relationships extracted per paper with stable results across multiple runs

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent debate and cooperation produce more comprehensive and reliable sizing relationship extraction than single-agent LLM approaches. Multiple agents extract sizing relationships independently, then debate disagreements and share reasoning through a shared message pool. An expert agent focuses on continuous extraction from source material each round while employee agents refine and validate through discussion. This division of labor prevents the "extraction only in round one" problem seen in homogeneous agent teams.

### Mechanism 2
Sizing relationship extraction from academic papers effectively prunes the search space for analog circuit optimization by constraining parameter relationships. The framework extracts three elements—device names, related parameters, and quantitative relationships (e.g., M1 = M2 = M3)—which constrain optimization algorithm search boundaries. This reduces effective search space dimensionality rather than only providing better initial points.

### Mechanism 3
Role-differentiated multi-agent teams with asymmetric participation improve coverage and accuracy of relationship extraction. Expert agent uses long-reasoning-capability model (GPT-o1) focused on continuous paper-based extraction without participating in debate. Employee agents use lower-cost models for discussion and validation. This prevents premature convergence where agents extract only in round one then debate without new information.

## Foundational Learning

- **Concept: Bayesian optimization for circuit sizing**
  - Why needed here: The baseline comparison uses MACE (batch Bayesian optimization) with fixed iteration limits. Understanding how acquisition functions balance exploration vs. exploitation explains why search space pruning improves sample efficiency.
  - Quick check question: Why does reducing search space dimensions have multiplicative effects on sample efficiency in high-dimensional Bayesian optimization?

- **Concept: Multi-agent communication paradigms**
  - Why needed here: The paper uses cooperation + debate with shared message pool. Understanding these patterns helps diagnose when each is appropriate.
  - Quick check question: What failure mode does the "expert doesn't debate" pattern prevent in iterative extraction tasks?

- **Concept: Analog circuit sizing constraints**
  - Why needed here: Sizing relationships like "M1 = M2" for current mirrors encode topology knowledge. Without domain context, extracted relationships may be nonsensical.
  - Quick check question: For a differential pair, why would you expect M1 = M2 sizing, and what performance parameter does this primarily affect?

## Architecture Onboarding

- **Component map**: Paper preprocessing (Mathpix + Yolo-V8 + OCR) -> Netlist generation (module detection) -> Multi-agent team (Expert + 3 Employee agents) -> Structured extraction output -> Optimization integration (MACE with constraints)

- **Critical path**: Paper preprocessing quality -> netlist module detection accuracy -> expert extraction coverage -> debate convergence -> constraint encoding -> optimizer performance

- **Design tradeoffs**: Expert model cost vs. extraction accuracy (GPT-o1 provides image understanding and long reasoning but is expensive); iteration count vs. diminishing returns (5 rounds chosen; more may not improve quality); strict constraint encoding vs. search flexibility (over-constraining may eliminate valid solutions)

- **Failure signatures**: Low valid relationship count (may indicate poor preprocessing or agent configuration issues); optimization still failing with constraints (may indicate incorrect relationship extraction or incompatible constraint encoding); high variance across extraction runs (may indicate temperature issues in employee agents)

- **First 3 experiments**: 1) Reproduce extraction on BGR paper [30] with CFG1 across 3 runs; verify 11-12 valid relationships per Table II. 2) Run optimization with vs. without extracted constraints on one circuit; measure time-to-convergence and pass rate. 3) Ablation: Single agent vs. multi-agent extraction on same paper to quantify debate/cooperation contribution.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the interaction between the LLM framework and the optimization algorithm be refined to guarantee a 100% optimization pass rate? The current framework lacks a feedback loop where optimization failures could prompt the agents to re-evaluate or correct extracted relationships.

- **Open Question 2**: To what extent does the multi-agent debate mechanism overcome general LLMs' inability to understand "new-structured" analog circuits? Experiments are restricted to common topologies (BGR, OP, LDO), leaving the handling of novel architectures unverified.

- **Open Question 3**: Can the sizing relationships extracted by this framework serve as high-quality synthetic training data for specialized analog circuit LLMs? The paper frames the work as a solution to the "shortage of specialized training data" for analog design, yet only validates the data for immediate optimization pruning, not for model training.

## Limitations
- Framework dependence on paper availability and quality creates potential bottlenecks - cannot extract sizing relationships for novel circuits without relevant academic papers
- Requires careful balancing of agent configurations: too many employee agents may increase costs without proportional accuracy gains
- Generalizability across circuit types remains unproven beyond the three tested topologies

## Confidence

- **High Confidence**: Optimization efficiency improvement (2.32-26.6× speedup) across multiple circuit types and process nodes
- **Medium Confidence**: Role-differentiated agents with asymmetric participation prevent "extraction only in round one" behavior
- **Medium Confidence**: Debate and cooperation among agents produces more reliable extraction than single-agent approaches

## Next Checks

1. **Ablation Study on Debate Mechanism**: Run extraction on the same circuit papers with and without the debate/cooperation phase to quantify the specific contribution of multi-agent discussion to relationship accuracy and completeness.

2. **Cross-Node Generalization Test**: Apply the framework to extract sizing relationships from SMIC 180nm papers and test whether these constraints successfully transfer to optimize the same circuit types at TSMC 65nm.

3. **Robustness Across Circuit Complexity**: Test the framework on a more complex analog circuit (e.g., a two-stage operational amplifier or a switched-capacitor filter) to evaluate whether the 11-12 valid relationships per paper pattern holds and whether optimization improvements scale with circuit complexity.