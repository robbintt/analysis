---
ver: rpa2
title: Scalable h-adaptive probabilistic solver for time-independent and time-dependent
  systems
arxiv_id: '2508.09623'
source_url: https://arxiv.org/abs/2508.09623
tags:
- points
- collocation
- posterior
- learning
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a scalable h-adaptive probabilistic solver
  for time-independent and time-dependent systems, addressing the computational bottleneck
  of Gaussian process-based probabilistic numerical methods for partial differential
  equations (PDEs). The key innovations are: (1) a stochastic dual descent (SDD) algorithm
  that reduces the per-iteration complexity from cubic to linear in the number of
  collocation points, enabling tractable inference for large-scale problems; and (2)
  a clustering-based active learning strategy that adaptively selects collocation
  points to maximize information gain while minimizing computational expense.'
---

# Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems

## Quick Facts
- arXiv ID: 2508.09623
- Source URL: https://arxiv.org/abs/2508.09623
- Reference count: 27
- Primary result: h-adaptive probabilistic solver achieves 1.21%-1.41% relative MSE with cubic-to-linear complexity reduction

## Executive Summary
This paper addresses the computational bottleneck in Gaussian process-based probabilistic numerical methods for partial differential equations by introducing a scalable h-adaptive solver. The key innovation combines a stochastic dual descent (SDD) algorithm that reduces per-iteration complexity from cubic to linear in the number of collocation points, with a clustering-based active learning strategy for adaptive collocation point selection. The method preserves uncertainty quantification capabilities while enabling tractable inference for large-scale problems, demonstrated across Poisson's equation in 2D and 3D, and a time-dependent heat equation benchmark.

## Method Summary
The proposed solver leverages a stochastic dual descent algorithm that reformulates the Gaussian process inference problem to achieve linear scaling in the number of collocation points, a dramatic improvement over traditional cubic complexity. This is coupled with a clustering-based active learning strategy that adaptively selects collocation points to maximize information gain while minimizing computational expense. The h-adaptive framework iteratively refines the collocation point placement based on solution uncertainty estimates, concentrating computational resources where uncertainty is highest. This dual approach enables the solver to maintain probabilistic uncertainty quantification while scaling to problems with millions of degrees of freedom that would be intractable for exact Gaussian process methods.

## Key Results
- Achieves relative MSE of 1.21% for Poisson's equation on a circular disk
- Demonstrates 1.41% relative MSE for three-dimensional Poisson's equation
- Shows 1.11% relative MSE for time-dependent heat equation with linear complexity scaling
- Outperforms exact Gaussian process solvers and physics-informed neural networks in accuracy-uncertainty trade-off

## Why This Works (Mechanism)
The solver's effectiveness stems from two complementary mechanisms: the SDD algorithm's ability to decouple the expensive matrix inversion operations through stochastic optimization, and the clustering strategy's intelligent placement of collocation points where they provide maximum information gain. By combining these approaches, the method achieves both computational efficiency and solution accuracy. The adaptive nature ensures that computational resources are concentrated in regions of high uncertainty, while the linear complexity scaling makes large-scale problems tractable without sacrificing the probabilistic interpretation of the solution.

## Foundational Learning

**Gaussian Process Regression**
- Why needed: Forms the probabilistic foundation for uncertainty quantification in PDE solutions
- Quick check: Verify covariance function selection matches problem smoothness properties

**Stochastic Optimization**
- Why needed: Enables the complexity reduction from cubic to linear scaling in the inference step
- Quick check: Monitor convergence behavior against problem size scaling

**Active Learning Strategies**
- Why needed: Guides adaptive collocation point placement to maximize information gain
- Quick check: Validate that selected points indeed correspond to high-uncertainty regions

**h-adaptivity**
- Why needed: Provides mesh refinement strategy that concentrates computational resources where needed
- Quick check: Confirm error reduction rate matches theoretical expectations

## Architecture Onboarding

Component Map:
Gaussian Process Prior -> SDD Solver -> Clustering Module -> Adaptive Refinement Loop -> Solution with Uncertainty

Critical Path:
Input PDE → Gaussian Process Setup → SDD Inference → Uncertainty Estimation → Clustering → Point Addition → Iterate until convergence

Design Tradeoffs:
- Accuracy vs. computational cost: Linear scaling sacrifices some precision for tractability
- Global vs. local uncertainty: Trade-off between computational efficiency and detailed uncertainty maps
- Adaptivity frequency: Balance between refinement overhead and solution quality

Failure Signatures:
- Slow convergence: May indicate poor clustering strategy or inadequate collocation point density
- Overestimated uncertainty: Could suggest SDD algorithm not fully capturing posterior correlations
- Boundary condition violations: May result from insufficient collocation points near boundaries

First Experiments:
1. Test on a simple 1D Poisson problem with known analytical solution to validate basic implementation
2. Run a convergence study on a manufactured solution problem to verify theoretical error rates
3. Compare uncertainty estimates against Monte Carlo sampling on a small-scale problem

## Open Questions the Paper Calls Out

None

## Limitations

- Clustering strategy performance may degrade for highly irregular geometries or non-smooth solutions
- Comprehensive benchmarking against PINNs across diverse problem classes is lacking
- Uncertainty quantification quality for multi-modal posterior distributions requires more rigorous statistical validation

## Confidence

**High Confidence**: The SDD algorithm's complexity reduction from cubic to linear scaling is theoretically sound and the numerical results demonstrate clear computational advantages for large-scale problems.

**Medium Confidence**: The clustering-based adaptive sampling strategy shows effectiveness on benchmark problems, but its generalizability to complex real-world applications requires further validation.

**Low Confidence**: Uncertainty quantification capabilities preservation claims need more rigorous statistical validation, particularly for multi-modal posterior distributions.

## Next Checks

1. Test the solver on problems with discontinuities and sharp gradients to assess the clustering strategy's robustness
2. Compare uncertainty quantification quality against Monte Carlo sampling on benchmark problems
3. Evaluate performance on industrial-scale 3D problems with millions of degrees of freedom to verify claimed scalability limits