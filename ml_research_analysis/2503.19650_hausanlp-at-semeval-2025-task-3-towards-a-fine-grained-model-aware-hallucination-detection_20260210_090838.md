---
ver: rpa2
title: 'HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination
  Detection'
arxiv_id: '2503.19650'
source_url: https://arxiv.org/abs/2503.19650
tags:
- task
- hallucination
- hallucinations
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a model-aware approach to detecting hallucinations
  in LLM outputs using fine-tuned ModernBERT. The authors created a synthetic dataset
  of 400 samples and trained the model on English language only.
---

# HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection

## Quick Facts
- arXiv ID: 2503.19650
- Source URL: https://arxiv.org/abs/2503.19650
- Reference count: 8
- Primary result: Model achieved IoU score of 0.032 and correlation score of 0.422 for hallucination detection

## Executive Summary
This paper presents HausaNLP's approach to SemEval-2025 Task 3, focusing on model-aware hallucination detection using fine-tuned ModernBERT. The authors developed a synthetic dataset of 400 samples and trained their model exclusively on English text. The system demonstrated a moderately positive correlation between model confidence and hallucination presence, though precise boundary detection remained challenging with a low IoU score. The work represents an early exploration of model-aware strategies for understanding and detecting hallucinations in LLM outputs, highlighting both the potential and limitations of current approaches.

## Method Summary
The authors employed a model-aware approach using ModernBERT architecture for hallucination detection. They created a synthetic dataset containing 400 samples specifically designed for this task, focusing on English language content. The model was fine-tuned on this dataset with the objective of detecting hallucinations in LLM outputs by analyzing the relationship between model confidence scores and the presence of hallucinated content. The approach aimed to identify both the presence of hallucinations and their precise boundaries within generated text.

## Key Results
- Achieved an IoU score of 0.032, indicating challenges in precisely identifying hallucination boundaries
- Obtained a correlation score of 0.422, showing a moderately positive relationship between model confidence and hallucination presence
- Demonstrated that model-aware strategies can contribute to understanding hallucination detection, though further improvements are needed for practical applications

## Why This Works (Mechanism)
The model-aware approach leverages ModernBERT's ability to analyze internal model confidence scores and correlate them with hallucination presence. By fine-tuning on a task-specific synthetic dataset, the model learns patterns that link confidence distributions to hallucinated content, potentially capturing subtle indicators that traditional fact-checking approaches might miss.

## Foundational Learning
1. **Hallucination Detection in LLMs** - Understanding how hallucinations manifest in generated text and why they occur is fundamental to developing detection systems.
   - Why needed: Without understanding the nature of hallucinations, detection approaches lack direction
   - Quick check: Can the model distinguish between factual errors and creative content?

2. **Model Confidence Analysis** - The relationship between confidence scores and output reliability forms the basis of the model-aware approach.
   - Why needed: Confidence scores may reveal uncertainty patterns associated with hallucinations
   - Quick check: Does confidence consistently drop at hallucination boundaries?

3. **Synthetic Data Generation** - Creating controlled datasets with known hallucination patterns enables targeted training and evaluation.
   - Why needed: Real-world data with labeled hallucinations is scarce and expensive to obtain
   - Quick check: Does performance generalize beyond the synthetic patterns?

## Architecture Onboarding

**Component Map:**
Input Text -> ModernBERT Encoder -> Confidence Score Analysis -> Hallucination Detection -> Output Boundary Markers

**Critical Path:**
The critical path involves processing input text through ModernBERT to extract contextual embeddings, analyzing confidence distributions across tokens, and identifying regions where confidence patterns indicate hallucination presence. The boundary detection mechanism is particularly crucial for fine-grained analysis.

**Design Tradeoffs:**
The choice of ModernBERT over larger transformer variants represents a balance between computational efficiency and performance. The synthetic dataset approach trades real-world complexity for controlled experimentation and reproducibility. The focus on English-only limits applicability but enables deeper analysis of language-specific patterns.

**Failure Signatures:**
Low IoU scores suggest the model struggles with precise boundary detection, potentially missing hallucination boundaries or including non-hallucinated text. The moderate correlation indicates the model captures some relationship between confidence and hallucinations but may miss nuanced cases where confidence doesn't align with hallucination presence.

**3 First Experiments:**
1. Test on established real-world hallucination detection datasets to assess generalization
2. Evaluate multilingual performance on non-English text samples
3. Compare ModernBERT performance against standard BERT and current state-of-the-art hallucination detection models

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Extremely small synthetic dataset (n=400 samples) raises concerns about generalizability and overfitting to dataset-specific patterns
- English-only focus limits applicability to multilingual hallucination detection, despite the task's multilingual nature
- Poor boundary detection performance (IoU 0.032) suggests challenges in practical deployment for fine-grained analysis
- Lack of comparison to baseline models or alternative architectures makes it difficult to assess relative effectiveness

## Confidence

**Medium:** The claim that "model-aware strategies can contribute to understanding hallucination detection" - supported by the moderately positive correlation but undermined by poor IoU performance

**Low:** The claim that "the approach demonstrates promise" - the metrics show both encouraging and discouraging results, making this assessment premature

**Medium:** The description of this as a "proof-of-concept" - appropriately cautious given the limitations

## Next Checks
1. Test the model on established, real-world hallucination detection datasets to assess generalization beyond synthetic data
2. Evaluate performance across multiple languages to determine multilingual capabilities, given the task's multilingual focus
3. Compare ModernBERT performance against standard transformer architectures and current state-of-the-art hallucination detection models to establish relative effectiveness