---
ver: rpa2
title: Which Word Orders Facilitate Length Generalization in LMs? An Investigation
  with GCG-Based Artificial Languages
arxiv_id: '2510.12722'
source_url: https://arxiv.org/abs/2510.12722
tags:
- word
- order
- language
- npsubj
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates which word orders facilitate length generalization
  in language models (LMs) using Generalized Categorial Grammar (GCG)-based artificial
  languages. The study extends prior work by adopting GCG to include more natural
  language constructions like unbounded dependencies and evaluating LMs' ability to
  generalize to longer sentences beyond training length.
---

# Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages

## Quick Facts
- **arXiv ID:** 2510.12722
- **Source URL:** https://arxiv.org/abs/2510.12722
- **Reference count:** 35
- **Primary result:** Typologically plausible word orders (SOV, SVO) facilitate length generalization in language models, with RNNs showing strongest alignment with typological distributions.

## Executive Summary
This paper investigates which word order generalizations are easier for language models by using Generalized Categorial Grammar (GCG)-based artificial languages. The study systematically evaluates LSTM, RNN, and Transformer architectures on their ability to generalize to longer sentences with different word orders (SOV, SVO, OSV, OVS, VSO, VOS). The key finding is that word orders common in natural languages are also easier for LMs to generalize to longer sentences, with RNNs showing the strongest correlation with typological distributions. This suggests that working memory constraints may shape the word order patterns observed in natural languages.

## Method Summary
The study constructs artificial languages using Generalized Categorial Grammar (GCG) to generate sentences with controlled word orders. Five artificial languages were created, each implementing a different word order: SOV, SVO, OSV, OVS, VSO, and VOS. The GCG approach allows for the inclusion of unbounded dependencies, making the artificial languages more similar to natural languages. The study evaluates three types of language models: RNN, LSTM, and Transformer, testing their ability to generalize to sentences longer than those seen during training. The evaluation includes both in-domain (same-length) and out-of-domain (longer-length) generalization tasks, with performance measured using exact match accuracy.

## Key Results
- Typologically plausible word orders (SOV, SVO) show better length generalization across all model architectures
- RNNs demonstrate the strongest correlation between typological plausibility and generalization performance
- Transformers perform well on in-domain evaluation but show less consistent generalization to longer sentences
- The correlation between word order typological plausibility and generalization ability is strongest for RNNs (r = 0.82) and weakest for Transformers (r = 0.57)

## Why This Works (Mechanism)
The study suggests that working memory constraints play a crucial role in shaping word order patterns in natural languages. Word orders that require less working memory (like SVO and SOV) are easier for language models to generalize to longer sentences. The mechanism appears to be that typologically common word orders align better with the sequential processing capabilities of recurrent architectures, particularly RNNs, which maintain state through sequences in a way that mirrors the incremental processing demands of these word orders.

## Foundational Learning
- **Generalized Categorial Grammar (GCG):** An extension of categorial grammar that allows for more flexible and natural language-like constructions through type-raising and composition rules. Why needed: Provides a systematic way to generate artificial languages with unbounded dependencies. Quick check: Can generate both natural and unnatural word orders while maintaining grammaticality.
- **Word Order Typology:** Classification of languages based on the typical order of subject (S), object (O), and verb (V). Why needed: Provides a framework for understanding which word orders are more common in natural languages. Quick check: Six possible permutations (SOV, SVO, OSV, OVS, VSO, VOS) with SOV and SVO being most common.
- **Length Generalization:** A model's ability to process and generate sentences longer than those seen during training. Why needed: Tests whether models have learned underlying grammatical principles rather than memorizing specific examples. Quick check: Performance on sentences longer than training distribution.
- **Unbounded Dependencies:** Grammatical constructions where elements that are semantically related can be separated by arbitrary distances in a sentence. Why needed: Makes artificial languages more similar to natural languages which frequently use such constructions. Quick check: Presence of filler-gap dependencies and relative clauses.

## Architecture Onboarding

### Component Map
Artificial Language Generator (GCG) -> Language Model (RNN/LSTM/Transformer) -> Evaluation Module (in-domain/out-of-domain) -> Performance Metrics

### Critical Path
The critical path involves generating artificial language sentences with specific word orders, training language models on these sentences up to a maximum length, and then testing generalization to longer sentences. The evaluation distinguishes between in-domain performance (same length as training) and out-of-domain performance (longer sentences), with exact match accuracy as the primary metric.

### Design Tradeoffs
The study trades ecological validity for experimental control by using artificial languages. While this allows precise manipulation of word order variables, it may not capture all the complexities of natural language processing. The choice of small, simple models (rather than large pre-trained ones) provides clearer insights into architectural differences but may limit generalizability to state-of-the-art systems.

### Failure Signatures
Models fail on out-of-domain generalization when they cannot maintain long-range dependencies or when the word order requires holding too much information in working memory. Transformers show high in-domain performance but struggle with out-of-domain generalization, suggesting they may be overfitting to training distributions rather than learning general principles.

### 3 First Experiments
1. Evaluate a small Transformer on in-domain generalization across all six word orders to establish baseline performance
2. Test RNN performance on out-of-domain generalization for SVO word order to examine working memory effects
3. Compare LSTM and RNN performance on VOS word order to investigate the impact of gating mechanisms on length generalization

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The use of artificial languages may not fully capture the complexity and diversity of natural languages
- The study relies on small, simple models rather than large pre-trained architectures
- Conclusions about working memory constraints shaping natural language distributions are correlational rather than causal
- The evaluation focuses primarily on exact match accuracy, which may not capture all aspects of language understanding

## Confidence
- **High confidence:** Relative performance differences between RNN, LSTM, and Transformer architectures on length generalization tasks
- **Medium confidence:** Correlation between typologically plausible word orders and generalization success across multiple experiments
- **Low confidence:** Interpretation that patterns reflect underlying working memory constraints in natural language evolution

## Next Checks
1. Test the same word order generalizations on larger, pre-trained models to verify if patterns hold beyond small artificial language settings
2. Expand GCG-based artificial language construction to include additional linguistic phenomena like center-embedding and scrambling to test robustness of findings
3. Conduct human psycholinguistic experiments to directly test whether same word order preferences emerge in human sentence processing and memory tasks