---
ver: rpa2
title: Enhancing Conformal Prediction via Class Similarity
arxiv_id: '2511.19359'
source_url: https://arxiv.org/abs/2511.19359
tags:
- prediction
- size
- class
- average
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a class-similarity-based regularization method
  for conformal prediction that improves both the semantic coherence and efficiency
  of prediction sets. The method augments the conformal score function with a penalty
  term that discourages including semantically dissimilar classes in prediction sets.
---

# Enhancing Conformal Prediction via Class Similarity

## Quick Facts
- arXiv ID: 2511.19359
- Source URL: https://arxiv.org/abs/2511.19359
- Reference count: 40
- Primary result: Proposed class-similarity regularization reduces prediction set size by up to 33% while improving semantic coherence

## Executive Summary
This paper introduces a novel regularization approach for conformal prediction that leverages class similarity to improve both efficiency and semantic coherence of prediction sets. The method adds a penalty term to standard conformal scores that discourages including semantically dissimilar classes, resulting in prediction sets that are both smaller and more semantically meaningful. The authors prove theoretically that this approach reduces the expected number of unique semantic groups in prediction sets and surprisingly also reduces average prediction set size under common conditions. They further develop a model-specific variant that automatically learns class similarity from the classifier's embedding space, eliminating the need for human-defined semantic partitions.

## Method Summary
The proposed method augments standard conformal prediction scores with a similarity-based penalty term: s_λ(x,y) = s(x,y) + λ·d(y, ŷ(x)), where d measures semantic dissimilarity between classes. Two variants are presented: MA-CS uses predefined semantic groups with binary penalties, while MS-CS learns a similarity matrix from the model's penultimate layer embeddings using cosine similarity of centered class means. The regularization parameter λ is selected via grid search on a held-out portion of the calibration set. The method is compatible with any conformal score function and maintains marginal coverage guarantees while improving both prediction set size and semantic coherence.

## Key Results
- Up to 33% reduction in average prediction set size compared to standard conformal methods
- Up to 25% reduction in number of superclasses in prediction sets, improving semantic coherence
- MS-CS variant consistently outperforms MA-CS by leveraging learned class relationships from feature space
- Theoretical proof that the penalty reduces expected number of unique semantic groups in prediction sets
- Size reduction occurs under common conditions when groups are small and classifier accuracy is reasonable

## Why This Works (Mechanism)

### Mechanism 1: Binary Out-of-Group Penalty Reduces Semantic Diversity
- Claim: Adding a penalty term for classes outside the predicted class's semantic group reduces both the number of unique groups in prediction sets and, under common conditions, the average set size.
- Mechanism: The penalized score s_λ(x,y) = s(x,y) + λ·I{g(y) ≠ g(ŷ(x))} increases scores for out-of-group classes by λ, making them harder to include in the prediction set. Since the threshold q_λ ∈ [q̂, q̂+λ] (Lemma 4.1), out-of-group classes face both a higher threshold AND an added penalty, while in-group classes only face the (at most λ-higher) threshold.
- Core assumption: The predicted class ŷ(x) is usually in the correct semantic group (high p₀), and groups are small relative to the total class space (n₀ << n₁).
- Evidence anchors:
  - [abstract] "augments the conformal score function with a penalty term that discourages including semantically dissimilar classes"
  - [section 4, Proposition 4.2] "C_λ(x) ∩ Y₁(x) ⊆ C(x) ∩ Y₁(x)" proves out-of-group labels cannot be added
  - [corpus] No direct corpus comparison; related work focuses on coverage guarantees rather than efficiency improvements
- Break condition: When the classifier's predicted class frequently belongs to the wrong semantic group (low p₀), or when semantic groups are poorly aligned with model confusion patterns.

### Mechanism 2: Set Size Reduction via Asymmetric Group Structure
- Claim: For common class partitions with small groups and reasonable in-group accuracy, the penalty reduces average prediction set size, not just semantic diversity.
- Mechanism: Theorem 4.5 establishes that sign(dE[|C_λ|]/dλ)|_{λ=0} = sign(ap₁n₀ - bp₀n₁). Since n₀ (in-group classes) is typically much smaller than n₁ (out-of-group classes), and p₀ (probability true label is in-group) is high for good classifiers, the derivative is negative at λ=0, meaning small penalties reduce set size. The penalty disproportionately excludes many out-of-group candidates while potentially adding few in-group candidates.
- Core assumption: Groups are small (n₀ << n₁), classifier has good in-group accuracy (high p₀), and the density ratio a/b is not adversarially unfavorable.
- Evidence anchors:
  - [abstract] "Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function"
  - [section 4, Theorem 4.5] Formal condition for size reduction
  - [corpus] Corpus papers address efficiency but through different mechanisms (TTA, adaptive training) without exploiting class similarity
- Break condition: Large groups where n₀ approaches n₁, poor classifier accuracy where p₀ is low, or pathological score distributions where a >> b.

### Mechanism 3: Model-Specific Similarity from Feature Embeddings
- Claim: Cosine similarity of centered class means in the model's penultimate layer provides a soft, data-driven similarity matrix that eliminates the need for human-defined semantic partitions.
- Mechanism: The soft penalty d_MS(y,y') = 1 - M_{y,y'} where M uses cosine similarity of (h_c - h_G) captures how the model internally organizes classes. Well-trained networks exhibit neural collapse: within-class variance decreases while inter-class means separate but preserve semantic relationships (e.g., dog breeds cluster together). This provides finer-grained penalties than binary grouping.
- Core assumption: The model's feature space has undergone neural collapse-like training, where class means capture semantically meaningful structure that generalizes to test data.
- Evidence anchors:
  - [section 5] "examining the relation between class means in feature space yields small effective groups without compromising on group-wise accuracy"
  - [section 6.1, Tables 1-2] MS-CS consistently achieves slightly smaller set sizes than MA-CS across datasets
  - [corpus] No corpus papers use embedding-based class similarity for CP regularization
- Break condition: Undertrained models where feature space is unstructured, or severe distribution shift where training-set class means don't reflect test-time relationships.

## Foundational Learning

- Concept: **Conformal Prediction Score Functions**
  - Why needed here: The paper's method is a drop-in augmentation to any score function (LAC, RAPS, SAPS); understanding how scores translate to prediction sets is prerequisite.
  - Quick check question: Given a sample with softmax outputs [0.7, 0.2, 0.1] for classes [A, B, C], what prediction set does LAC produce with threshold q̂=0.5?

- Concept: **Empirical Quantile Computation**
  - Why needed here: The penalty shifts the conformal threshold q̂_λ; understanding how quantiles are computed from calibration scores is essential for implementation.
  - Quick check question: For calibration scores [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] and α=0.1, what is q̂?

- Concept: **Neural Collapse and Feature Space Structure**
  - Why needed here: The model-specific variant relies on class means in feature space capturing semantic structure; neural collapse explains why this works.
  - Quick check question: What property of feature space does neural collapse predict about the relationship between within-class variance and between-class distances?

## Architecture Onboarding

- Component map: Calibration splitter -> Similarity matrix builder -> Penalized score computer -> Threshold computer -> Prediction set constructor

- Critical path:
  1. Precompute similarity matrix M from training data (one-time cost)
  2. Split calibration set and compute q̂ on original scores
  3. Grid search λ on λ-evaluation set to minimize set size
  4. Recompute q̂_λ with selected λ on full calibration set
  5. Deploy with penalized scores

- Design tradeoffs:
  - **MA-CS vs MS-CS**: MA requires human semantic groups but is more interpretable; MS is fully automatic but less explainable
  - **λ magnitude**: Small λ reduces set size but may not improve semantic coherence much; large λ improves coherence but risks increasing set size past the optimum
  - **Calibration split ratio**: More data for λ evaluation improves λ selection but reduces q̂ reliability

- Failure signatures:
  1. **Empty prediction sets**: λ too large, causing even ŷ(x) to exceed q̂_λ for some samples
  2. **No size reduction**: Groups too large or p₀ too low (check per-group accuracy)
  3. **Coverage violation**: Calibration contamination or exchangeability assumption broken
  4. **MS-CS worse than MA-CS**: Model undertrained or distribution shift; feature space lacks semantic structure

- First 3 experiments:
  1. Replicate CIFAR-100 results with LAC score: compare Standard vs MA-CS vs MS-CS on set size and #superclasses, verify coverage ≥ 1-α
  2. λ sensitivity analysis: plot set size and #superclasses vs λ on held-out validation set to find optimal range and validate Theorem 4.5's prediction of initial decrease
  3. Similarity matrix ablation: Compare MS-CS (learned embeddings) vs MA-Diag (identity matrix) vs random groupings to isolate the value of semantic structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative similarity matrix construction strategies (beyond cosine similarity of centered class means in feature space) further improve the efficiency gains of the model-specific variant?
- Basis in paper: [explicit] Section 5 states: "There exist multiple potential strategies for constructing a class similarity matrix M given a model. Here, we propose one that consistently improves the efficiency results in our experiments. Future research may attempt to optimize this choice."
- Why unresolved: The authors propose one viable strategy based on neural collapse intuitions but do not explore alternatives such as kernel methods, learned similarity functions, or attention-based approaches.
- What evidence would resolve it: Systematic comparison of multiple similarity matrix construction methods across datasets, showing which achieves the lowest average prediction set size while maintaining coverage.

### Open Question 2
- Question: Can the λ regularization parameter be selected without splitting the calibration set, preserving more data for threshold estimation?
- Basis in paper: [inferred] Section 6 describes splitting the calibration set into equal halves for λ-evaluation, which reduces effective calibration data. This could impact coverage reliability with smaller calibration sets.
- Why unresolved: The current approach requires held-out data for λ selection, creating a trade-off between regularization tuning and calibration precision that was not addressed.
- What evidence would resolve it: Development of a λ selection method using only the full calibration set (e.g., theoretical bounds, cross-validation within calibration, or adaptive schemes) demonstrating equivalent or better performance.

### Open Question 3
- Question: Does the class-similarity regularization approach generalize to domains beyond image classification, such as NLP or tabular data?
- Basis in paper: [inferred] All experiments are limited to image classification benchmarks (CIFAR-100, Living-17, Mini-ImageNet). The embedding structure in text models or tabular classifiers may exhibit different similarity properties.
- Why unresolved: The method relies on feature embeddings with presumed class concentration properties (neural collapse); whether similar dynamics exist in other domains remains untested.
- What evidence would resolve it: Experiments on text classification (e.g., topic categorization) or structured data benchmarks showing consistent prediction set size reductions with the proposed regularization.

### Open Question 4
- Question: How does the method perform under label distribution shift or non-exchangeable conditions?
- Basis in paper: [inferred] Theoretical guarantees rely on exchangeability (Section 3), and Theorem 4.5 assumes specific distributional properties. Real-world deployments often face distribution shift where class similarity structures may change.
- Why unresolved: The robustness of both the theoretical efficiency gains and the semantic coherence improvements under covariate or label shift was not evaluated.
- What evidence would resolve it: Experiments on datasets with known distribution shifts (e.g., domain adaptation benchmarks) measuring whether prediction set size reductions and superclass reductions persist under shift conditions.

## Limitations

- Theoretical size reduction depends on strong assumptions about group structure and classifier accuracy that may not hold in practice
- Model-specific variant performance relies on neural collapse-like training, which may not generalize to all architectures
- λ selection requires splitting calibration data, reducing effective sample size for threshold estimation

## Confidence

- **High Confidence**: The semantic coherence improvements (reduction in #superclasses) are empirically well-supported across multiple datasets and CP methods
- **Medium Confidence**: The size reduction claims are theoretically sound under stated conditions but require careful empirical validation
- **Medium Confidence**: The MS-CS variant's superiority relies on the quality of the learned similarity matrix

## Next Checks

1. **Robustness to Group Definition**: Test MA-CS on datasets with varying semantic granularity (e.g., CIFAR-100 superclasses vs. finer-grained sub-categories) to validate Theorem 4.5's size reduction claims across different group sizes.

2. **Model Architecture Generalization**: Apply MS-CS to transformer-based vision models (e.g., ViT) and compare performance to CNNs to assess whether neural collapse assumptions generalize beyond traditional architectures.

3. **Distribution Shift Analysis**: Evaluate both variants under covariate shift (e.g., CIFAR-10C) to quantify how well the learned similarity structure transfers when semantic relationships may change between training and test distributions.