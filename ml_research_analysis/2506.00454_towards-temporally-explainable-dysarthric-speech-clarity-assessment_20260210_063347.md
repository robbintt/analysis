---
ver: rpa2
title: Towards Temporally Explainable Dysarthric Speech Clarity Assessment
arxiv_id: '2506.00454'
source_url: https://arxiv.org/abs/2506.00454
tags:
- speech
- errors
- clarity
- dysarthric
- mispronunciation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a three-stage framework for automated dysarthric
  speech clarity assessment: (1) overall clarity scoring, (2) temporal mispronunciation
  localization, and (3) mispronunciation type classification. Using an expert-annotated
  dataset of six dysarthric speakers, the authors systematically evaluate pre-trained
  ASR models (Whisper and wav2vec2 variants) across these stages.'
---

# Towards Temporally Explainable Dysarthric Speech Clarity Assessment

## Quick Facts
- arXiv ID: 2506.00454
- Source URL: https://arxiv.org/abs/2506.00454
- Reference count: 0
- One-line primary result: Three-stage ASR-based framework achieves Pearson correlation ~0.85 with therapist clarity scores and 70.1% exact error match rate for dysarthric speech assessment.

## Executive Summary
This paper introduces a three-stage framework for automated dysarthric speech clarity assessment using pre-trained ASR models. The system performs overall clarity scoring, temporal mispronunciation localization, and mispronunciation type classification across five error categories. Using an expert-annotated dataset of six dysarthric speakers, the authors systematically evaluate Whisper and wav2vec2 variants, demonstrating strong alignment with therapist-provided clarity scores (correlation ~0.8) and outperforming smaller models in precision and F-score for localization. The study highlights the potential of ASR-based tools for providing clinically relevant, actionable feedback in speech therapy while identifying key challenges in detecting prosodic and repetition errors.

## Method Summary
The framework uses pre-trained ASR models (Whisper and wav2vec2 variants) to assess dysarthric speech across three stages: (1) clarity scoring via WER inversion, (2) temporal localization using forced alignment with whisper-timestamped, and (3) error classification via Levenshtein distance on phoneme-level edits. The system processes audio recordings with ground truth text passages, computes 1-WER for clarity, extracts timestamped error regions, and classifies errors into substitution, deletion, insertion, repetition, or prosodic categories. Evaluation uses a dataset of 12 recordings from 6 dysarthric speakers with therapist annotations across 196 error labels.

## Key Results
- Whisper models achieve Pearson correlation ~0.85 with therapist-provided clarity scores
- Temporal localization precision reaches 0.78 for substitution errors, with recall varying significantly by error type (0.64 for substitutions vs 0.19 for prosodic errors)
- Mispronunciation classification achieves 70.1% exact error match rate against therapist annotations
- Larger Whisper models (Medium/Large) significantly reduce false positives and improve classification accuracy compared to smaller variants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inverting Word Error Rate (WER) from a robust ASR model serves as a proxy for human-perceived speech clarity in dysarthric patients.
- **Mechanism:** Dysarthria reduces intelligibility through articulatory impairments. When a pre-trained ASR model (trained on typical speech) attempts to transcribe dysarthric speech, the articulation errors manifest as transcription errors (substitutions, deletions). By calculating `Clarity = 1 - WER`, the system quantifies the gap between the target text and the perceived audio.
- **Core assumption:** The ASR's decoding failures are primarily driven by the patient's articulation deficits rather than environmental noise or model incapacity, and the model's "perception" correlates linearly with a human therapist's judgment.
- **Evidence anchors:** [abstract] Mentions "overall clarity scoring" and correlation with therapist scores. [section 4.2] "Medium and large Whisper models achieve the highest correlation with therapist-provided scores... Pearson correlation ~0.85."

### Mechanism 2
- **Claim:** Temporal mispronunciation localization relies on forced alignment and token-level discrepancy detection between the target text and the audio segment.
- **Mechanism:** The framework uses a "whisper timestamped" module to align the ground-truth text with the audio. It identifies temporal regions where the audio acoustics fail to match the expected phonetic sequence of the target word, resulting in a substitution, deletion, or insertion in the transcript.
- **Core assumption:** The timestamp generated by the alignment module for a specific word error accurately reflects the physical time window where the articulation failed.
- **Evidence anchors:** [abstract] "mispronunciation localization... using pre-trained ASR models." [section 3] "Aligning ASR transcriptions with the text passage and extracting timestamps of the transcription regions in error."

### Mechanism 3
- **Claim:** Error type classification is derived by mapping the Levenshtein edit operations (distance) between the reference text and the ASR hypothesis to clinical error categories.
- **Mechanism:** The system converts both the therapist's text and the ASR transcript into phoneme lists. It calculates the edit distance. If the distance is small (threshold $\le 3$), it classifies the difference as a phoneme-level error (substitution/deletion); larger differences imply word-level errors.
- **Core assumption:** The ASR's transcription error type (e.g., writing "fifty" instead of "swiftly") directly maps to the patient's motor behavior (e.g., a phoneme distortion).
- **Evidence anchors:** [abstract] "mispronunciation type classification... Substitutions and deletions were most accurately detected." [section 4.4] "ASR output and reference text were converted into phoneme lists... assessed using the Levenshtein distance."

## Foundational Learning

- **Concept:** Word Error Rate (WER) & Edit Distance
  - **Why needed here:** This is the mathematical foundation for Stage 1 (Scoring) and Stage 3 (Classification). You cannot interpret the "Clarity Score" or the "Error Type" without understanding how substitutions, insertions, and deletions are counted.
  - **Quick check question:** If a patient says "cat" for "cap", is this a substitution or deletion in WER calculation? (Answer: Substitution).

- **Concept:** Forced Alignment (Timestamping)
  - **Why needed here:** Essential for Stage 2 (Localization). The paper relies on the model's ability to output exact start and end times for words to determine *when* an error occurred.
  - **Quick check question:** Does standard Whisper output precise word timestamps by default, or does this paper use a specific extension module to achieve this? (Answer: It uses the `whisper-timestamped` module).

- **Concept:** Articulatory Phonetics (Dysarthria-specific)
  - **Why needed here:** To understand the "Why" behind the error classes (Substitution vs. Prosody). You must grasp why an ASR fails to detect "prosodic errors" (timing/rhythm) compared to "substitutions" (wrong phoneme).
  - **Quick check question:** Why would an ASR model struggle to detect a "prosodic error" (irregular pause) if the phoneme sequence is correct? (Answer: ASRs are often trained to ignore or normalize pauses/rhythm to focus on lexical content).

## Architecture Onboarding

- **Component map:** Input Audio + Ground Truth Text -> Whisper ASR with whisper-timestamped -> Comparator (WER calculation) -> Aligner (timestamp extraction) -> Phoneme Converter (Levenshtein classification) -> Output Clarity Score, Error Timestamps, Error Class

- **Critical path:** The accuracy of the entire system hinges on the **ASR transcription quality**. If the ASR hallucinates words or fails to transcribe due to severe dysarthria, the subsequent alignment and classification steps cascade into failure.

- **Design tradeoffs:**
  - **Model Size vs. Precision:** The paper proves larger models (Large/Medium) yield significantly better precision in localization than Tiny/Base.
  - **Granularity vs. Robustness:** The system works well for "broad" errors (word subs) but fails at "fine" errors (prosody/repetition).
  - **Dictionary Bias:** Using standard ASRs forces a tradeoff where non-word utterances are "snapped" to real words, losing clinical nuance for the sake of linguistic validity.

- **Failure signatures:**
  - **The "Hallucination" Effect:** A patient says a non-word (e.g., "pris"); the ASR outputs "prince". The system reports a Word Substitution ("prism" â†’ "prince") when clinically it was a Distortion/Deletion. This inflates "Substitution" counts.
  - **The "Silence" Blindspot:** Prosodic errors (long pauses, strained voice) are often invisible to the text-based comparison logic if no word is actually missed or added.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run `whisper-timestamped` on the provided dataset samples (Speaker SA001 vs SA003). Verify the correlation between WER and the Therapist Clarity Score.
  2. **Threshold Sensitivity Analysis:** The paper uses an absolute edit distance threshold ($\le 3$) for phoneme classification. Test how the "Exact Match" accuracy shifts if you change this threshold to 2 or 4.
  3. **Failure Case Audit:** Isolate the "Prosodic Error" samples and visualize the attention/activation maps of the Whisper model to see if it "notices" the pause but simply fails to report it in the text.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can prosody-aware modeling be integrated into ASR frameworks to accurately detect prosodic and repetition errors in dysarthric speech?
- **Basis in paper:** [explicit] The authors note that recall for prosodic and repetition errors was very poor (~0.19) compared to substitutions (~0.64), and explicitly list "prosody-aware modeling" as a direction for future work.
- **Why unresolved:** Standard pre-trained ASR models like Whisper appear to normalize or overlook subtle acoustic variations characteristic of prosody (e.g., irregular pauses, rhythm), focusing instead on lexical content.
- **What evidence would resolve it:** A study demonstrating that incorporating duration, pitch, or energy features into the framework significantly improves localization recall for the "prosodic" and "repetition" error categories.

### Open Question 2
- **Question:** Does utilizing direct phoneme recognition improve the accuracy of classifying phoneme-level distortions compared to word-based ASR transcription?
- **Basis in paper:** [inferred] The paper notes that ASRs often map non-word utterances (e.g., "swifty") to real words (e.g., "fifty"), causing phoneme-level deletions to be misclassified as word substitutions. The authors suggest "direct phoneme recognition could improve classification accuracy."
- **Why unresolved:** The current methodology relies on transcribing words and then mapping to phonemes, a process that inherently loses non-word articulation data essential for diagnosing specific motor impairments.
- **What evidence would resolve it:** Experiments comparing the current word-based pipeline against a phoneme-recognition system, showing a higher exact-match percentage for therapist-labeled phoneme-level insertion and deletion errors.

### Open Question 3
- **Question:** To what extent does incorporating control speaker data enhance the generalization and robustness of dysarthric speech assessment models?
- **Basis in paper:** [explicit] The authors state that future work "will explore generalization by incorporating control speaker data and... dataset expansion."
- **Why unresolved:** The current study is limited to six dysarthric speakers; it is unclear if the models distinguish dysarthric features from general speech variability or if they overfit to the specific acoustic patterns of the small dataset.
- **What evidence would resolve it:** Evaluation results on an expanded dataset including healthy controls, demonstrating that the framework maintains high correlation with therapist scores without flagging normal speech variations as errors.

## Limitations
- Dataset limited to 6 speakers with mild to moderate severity, constraining generalizability
- Significant performance degradation for prosodic and repetition errors (recall ~0.19)
- Reliance on text-based detection inherently biases against non-lexical speech variations
- Dictionary bias causes non-word utterances to be mapped to real words, losing clinical nuance

## Confidence
- **Overall Clarity Scoring:** High confidence - Strong quantitative evidence with Pearson correlation ~0.85
- **Temporal Localization Accuracy:** Medium confidence - Robust precision metrics but significantly lower recall for prosodic errors
- **Error Classification Validity:** Low-Medium confidence - Reasonable exact match rates but vulnerable to dictionary bias and non-word handling issues

## Next Checks
1. **Cross-Institution Validation:** Test the framework on dysarthric speech datasets from different clinics/laboratories to assess generalizability across recording conditions, speaker demographics, and annotation standards.

2. **Severity Gradient Analysis:** Systematically evaluate model performance across a broader severity spectrum (mild to severe dysarthria) to identify precise failure thresholds and inform clinical deployment guidelines.

3. **Human-in-the-Loop Assessment:** Conduct user studies with speech therapists to evaluate whether the temporal error localization and classification outputs actually improve therapeutic decision-making and patient feedback quality compared to current practices.