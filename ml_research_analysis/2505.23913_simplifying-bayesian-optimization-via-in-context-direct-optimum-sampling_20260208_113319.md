---
ver: rpa2
title: Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling
arxiv_id: '2505.23913'
source_url: https://arxiv.org/abs/2505.23913
tags:
- optimization
- function
- bayesian
- fibo
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FIBO, a fully in-context Bayesian optimization
  method that directly samples from the posterior distribution over the optimum point
  without requiring surrogate fitting or acquisition function optimization. The method
  uses a pre-trained deep generative model to approximate Thompson sampling, reducing
  BO to a single forward pass through the model.
---

# Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling

## Quick Facts
- arXiv ID: 2505.23913
- Source URL: https://arxiv.org/abs/2505.23913
- Reference count: 40
- Over 35x speedup compared to Gaussian process-based BO while maintaining similar optimization performance

## Executive Summary
This paper introduces FIBO, a novel Bayesian optimization method that directly samples from the posterior distribution over the optimum point without requiring surrogate fitting or acquisition function optimization. FIBO uses a pre-trained deep generative model to approximate Thompson sampling, reducing BO to a single forward pass through the model. The method achieves significant computational speedups while maintaining competitive optimization performance across synthetic benchmarks and real-world chemistry tasks.

## Method Summary
FIBO is a fully in-context Bayesian optimization method that bypasses traditional surrogate modeling by training a generative model to directly predict optimal points given historical observations. The method consists of a Transformer encoder that summarizes the observation history into a fixed-size context vector, which then conditions an autoregressive neural spline flow to generate candidate optimal points. During pre-training, the model learns to predict the optimum of synthetic functions sampled from a GP prior. At inference time, FIBO generates new suggestions through a single forward pass, eliminating the need for expensive acquisition function optimization.

## Key Results
- Achieves over 35x speedup compared to Gaussian process-based Bayesian optimization
- Maintains competitive optimization performance on synthetic functions (Ackley, Levy, Rosenbrock, Hartmann)
- Demonstrates strong performance on real-world chemistry tasks from the Olympus toolkit
- Enables efficient parallel BO with batch sizes up to 50 with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Approximation of Thompson Sampling via Distribution Matching
The generative model $p_\theta(x^*|D)$ is trained to minimize negative log-likelihood of optimal points given datasets sampled from a GP prior. This training objective minimizes the KL-divergence between the model's distribution and the true posterior $p(x^*|D)$. At test time, sampling from $p_\theta(x^*|D)$ approximates Thompson sampling without explicit surrogate fitting. Performance degrades when test functions deviate strongly from the GP-based pre-training prior.

### Mechanism 2: Amortized Inference via In-Context Encoding
A Transformer encoder summarizes variable-length observation history $D$ into a fixed-dimension context vector $c$, which conditions a normalizing flow to produce candidate optima $\hat{x}^*$. This bypasses the $O(N^3)$ inversion of GPs and acquisition function optimization. The approach assumes the encoder can compress relevant information without losing the signal needed to locate $x^*$. Wall-clock time improvements may not materialize if context summarization fails as $N$ increases.

### Mechanism 3: Batch Efficiency via Independent Sampling
FIBO generates batch suggestions by drawing $q$ independent samples from $p(x^*|D)$, avoiding the complex Monte Carlo integration required by traditional batch acquisition functions. Computational cost remains roughly constant regardless of batch size. The inherent stochasticity of the normalizing flow provides sufficient diversity for exploration without explicit diversity-promoting mechanisms. Batch suggestions may collapse to highly correlated points if the flow is under-trained.

## Foundational Learning

- **Concept: Thompson Sampling**
  - Why needed: FIBO is theoretically grounded as an approximation of Thompson Sampling, making understanding TS critical to grasping the method
  - Quick check: Can you explain why sampling the argmax of a random function realization is equivalent to sampling directly from the posterior of the optimum?

- **Concept: Normalizing Flows**
  - Why needed: The generative head is an autoregressive neural spline flow, and understanding how flows enable exact density evaluation and efficient sampling is critical
  - Quick check: How does a normalizing flow enable both calculating the probability of a sample and generating a new sample from a Gaussian latent space?

- **Concept: Gaussian Process Priors (Random Fourier Features)**
  - Why needed: Pre-training data is generated using an approximation of a GP prior via Random Fourier Features
  - Quick check: Why would the authors use Random Fourier Features to generate training data rather than sampling directly from an exact GP?

## Architecture Onboarding

- **Component map:** Context $D$ (Set of $\{(x_i, y_i)\}$) -> Transformer Encoder -> Context Vector $c$ -> Autoregressive Neural Spline Flow -> Sample $\hat{x}^*$
- **Critical path:** Pre-training: Generate synthetic functions → find true $x^*$ via L-BFGS-B → train flow to predict $x^*$ given random context $D$. Inference: Receive $D$ → encode to $c$ → sample from flow.
- **Design tradeoffs:** FIBO sacrifices the exactness of a GP surrogate for speed of a frozen pre-trained model. Currently scaled to 3D and 4D problems; scaling to high dimensions may require architectural changes.
- **Failure signatures:** Prior mismatch causes performance degradation on functions dissimilar to RBF prior. Mode collapse in batch settings results in repetitive suggestions and poor exploration.
- **First 3 experiments:** 1) Prior validation on functions from same GP prior used for training to verify 35x speedup. 2) Out-of-distribution robustness test on Ackley, Rosenbrock functions to check performance degradation. 3) Scaling analysis profiling inference time as batch size $q$ increases from 10 to 50.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can FIBO maintain efficiency and performance when scaling to high-dimensional optimization problems ($d > 20$)? The authors focus on lower dimensions as they are ubiquitous in science and engineering, but normalizing flows suffer from the curse of dimensionality requiring exponentially more parameters.

- **Open Question 2:** How can the direct sampling architecture be adapted to handle constrained Bayesian optimization? The method samples from $p(x^*|D)$ but traditional BO incorporates constraints via acquisition functions, leaving it unclear how to condition the generative flow on feasibility constraints.

- **Open Question 3:** How sensitive is FIBO to the discrepancy between the pre-training prior and true objective function structure? While results on chemistry tasks are positive, it's unclear if FIBO fails on functions highly dissimilar to the GP prior where adaptive surrogates might excel.

## Limitations
- Reliance on a fixed GP prior for pre-training constrains the method to functions similar to those generated by the RBF kernel
- Current validation only extends to 4D problems, with unclear performance in higher dimensions
- Method may struggle with functions that have high-frequency components or discontinuities not present in the training prior

## Confidence

- **High confidence:** 35x speedup claim is well-supported by experimental methodology and direct GP comparison
- **Medium confidence:** Equivalence to Thompson sampling is theoretically sound but practical approximation quality depends on prior coverage and flow expressiveness
- **Medium confidence:** Batch efficiency holds for tested range but lack of explicit diversity mechanisms raises questions about very large batches

## Next Checks

1. **Prior Mismatch Stress Test:** Systematically evaluate FIBO on functions with increasing deviation from the RBF prior and measure performance degradation compared to GP-EI

2. **Scaling Experiment:** Profile inference time and optimization performance as dimensionality increases from 4D to 10D and 20D to identify practical scaling limits

3. **Batch Diversity Analysis:** For batch sizes q=10, 25, 50, compute pairwise distance between sampled points and compare to diversity achieved by q-EI or q-KG methods