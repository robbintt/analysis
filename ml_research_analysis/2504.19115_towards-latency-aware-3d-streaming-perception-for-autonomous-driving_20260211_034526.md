---
ver: rpa2
title: Towards Latency-Aware 3D Streaming Perception for Autonomous Driving
arxiv_id: '2504.19115'
source_url: https://arxiv.org/abs/2504.19115
tags:
- streaming
- detection
- object
- perception
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the critical challenge of deploying real-time
  3D perception algorithms on edge devices, where substantial runtime latency degrades
  detection performance. To address this, the authors introduce a new benchmark for
  online evaluation that accounts for runtime latency and propose the Latency-Aware
  3D Streaming Perception (LASP) framework.
---

# Towards Latency-Aware 3D Streaming Perception for Autonomous Driving

## Quick Facts
- arXiv ID: 2504.19115
- Source URL: https://arxiv.org/abs/2504.19115
- Reference count: 40
- Primary result: Achieves 35.3% online mAP on nuScenes, matching 80% of offline performance without acceleration techniques

## Executive Summary
This paper addresses the critical challenge of deploying real-time 3D perception algorithms on edge devices, where substantial runtime latency degrades detection performance. The authors introduce LASP (Latency-Aware 3D Streaming Perception), a framework that compensates for latency through two key innovations: latency-aware history integration using continuous ODE-based temporal alignment, and latency-aware predictive detection using intention-guided trajectory forecasting. Evaluated on Jetson AGX Orin, LASP achieves 35.3% online mAP, closely matching 80% of its offline performance without any acceleration techniques, and outperforms other methods on more powerful platforms.

## Method Summary
LASP builds on query-based 3D object detectors and integrates two latency-aware components. The history integration module transforms object query embeddings into a hidden space where temporal evolution is modeled as a linear ODE, enabling robust handling of irregular latency intervals through efficient matrix exponential computation. The predictive detection module forecasts future object states using intention-guided queries (generated via k-means clustering on trajectory endpoints) and compensates for latency using predicted trajectories. The framework is evaluated on the extended nuScenes dataset (12Hz annotations) using a streaming benchmark that accounts for runtime latency by evaluating predictions at multiple query times during the execution window.

## Key Results
- Achieves 35.3% online mAP on nuScenes streaming benchmark, matching 80% of offline performance
- Outperforms other methods on more powerful platforms despite running on resource-constrained Jetson AGX Orin
- Demonstrates effectiveness of continuous ODE-based temporal alignment versus discrete methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous temporal alignment via linear ODEs enables robust handling of irregular latency intervals
- **Mechanism:** Object query embeddings are transformed into a hidden space where temporal evolution is modeled as linear ODE: dz = A·z·dt. The transition matrix A is conditioned on motion attributes and decomposed into learnable basis matrices with shared eigenvectors, allowing efficient computation via element-wise exponential.
- **Core assumption:** Object state evolution in hidden space is approximately linear over small time steps
- **Evidence anchors:** Abstract and Section IV-B describe ODE-based alignment; corpus shows weak direct support for ODE-based temporal modeling in streaming perception
- **Break condition:** When object motion becomes highly erratic or latency exceeds temporal coherence of learned motion patterns (>0.5s intervals showed 14.8% mAP drop)

### Mechanism 2
- **Claim:** Intention-guided trajectory prediction with posterior latency compensation recovers detection accuracy during execution
- **Mechanism:** K intention queries per agent (generated via k-means clustering on trajectory endpoints) are fused with detection queries through an intention-guided head. Trajectories are predicted iteratively with endpoint re-encoding for refinement.
- **Core assumption:** Future trajectories can be predicted from current detection queries and class-conditional intention priors
- **Evidence anchors:** Abstract and Section IV-C describe intention-guided trajectory prediction; MTR++ validates intention-query design for trajectory prediction
- **Break condition:** When intention prediction is ambiguous (e.g., at intersections) or objects exhibit sudden unexpected maneuvers

### Mechanism 3
- **Claim:** Streaming benchmark with query-time evaluation reveals latency-induced performance gaps
- **Mechanism:** Benchmark evaluates predictions at query times spanning from processing completion through next execution window, using nuScenes sweeps at 12Hz for fine-grained ground truth
- **Core assumption:** Performance degradation during execution window represents real-world safety-critical perception requirements
- **Evidence anchors:** Abstract and Section III-A describe streaming benchmark; ASAP benchmark provides similar streaming evaluation
- **Break condition:** When evaluation query times don't match actual downstream planning deadlines

## Foundational Learning

- **Concept: Query-based 3D object detection (DETR-family architectures)**
  - Why needed here: LASP builds on sparse query-based detectors where object queries encode position and semantics. Understanding query propagation is prerequisite to grasping continuous alignment.
  - Quick check question: Can you explain how object queries differ from dense BEV feature maps in terms of temporal propagation cost?

- **Concept: Linear ODEs and matrix exponentials for continuous dynamics**
  - Why needed here: The history integration module uses dz/dt = A·z to model continuous temporal evolution; eigenvalue decomposition enables efficient computation.
  - Quick check question: Why does diagonalizing the transition matrix enable O(n) computation instead of O(n³) for matrix exponential?

- **Concept: Streaming perception paradigm (accuracy-latency tradeoff)**
  - Why needed here: Unlike offline evaluation, streaming perception evaluates at wall-clock time, meaning predictions are stale by latency τ when output.
  - Quick check question: If a detector has 500ms latency and outputs a prediction, how far behind current reality is that prediction if evaluated immediately?

## Architecture Onboarding

- **Component map:** Multi-view Images → Image Encoder → Memory Queue → Latency-aware History Integration → Transformer Decoder → Intention-guided Head → Latency Compensation → Streaming Output

- **Critical path:** Memory queue → ODE-based alignment → Intention-guided trajectory prediction. If alignment fails, queries become misaligned with current frame; if trajectory prediction fails, compensation introduces additional error.

- **Design tradeoffs:**
  - Basis matrices (K=10): More bases increase flexibility but add parameters; paper finds 10 sufficient
  - Intention queries (K=6): More modes capture diverse trajectories but increase ambiguity; 6 chosen per category
  - No acceleration techniques: Framework prioritizes algorithmic compensation over hardware optimization; TensorRT baseline still underperforms LASP in mAP

- **Failure signatures:**
  - Irregular latency causing >500ms gaps: Look for misaligned historical queries (ODE extrapolation beyond training distribution)
  - Sudden object acceleration/deceleration: Check trajectory prediction errors in intention-guided head output
  - Missing posterior latency measurement: Compensation fails, predictions remain time-lagged

- **First 3 experiments:**
  1. **Baseline latency sweep:** Measure mAP degradation on StreamPETR/LASP across fixed latencies (100ms, 200ms, 500ms, 1000ms) using streaming benchmark—establish performance floor
  2. **History integration ablation:** Compare No-align vs. MLN (discrete) vs. ODE (continuous) on irregular-sampled validation split—isolate temporal alignment contribution
  3. **Intention query visualization:** Project predicted trajectories vs. ground truth for failure cases (sudden turns, stops)—identify prediction mode collapse or intention ambiguity

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but several implications emerge from the methodology and results regarding future research directions and limitations.

## Limitations
- Linear ODE assumption may break down during highly dynamic maneuvers or when latency intervals exceed temporal coherence of learned motion patterns
- Method requires substantial memory for maintaining historical feature queues, potentially limiting scalability on resource-constrained devices
- Evaluation assumes accurate posterior latency measurement, which may not be available in all deployment scenarios

## Confidence
- **High confidence:** Streaming benchmark methodology and latency-aware history integration via linear ODEs (well-grounded in mathematical formalism, evaluation on realistic hardware)
- **Medium confidence:** Intention-guided predictive detection component (relies on trajectory prediction quality and assumption that intention queries generalize across driving scenarios)
- **Low confidence:** Claim that LASP matches 80% of offline performance without acceleration techniques (requires careful scrutiny of hardware/software configurations across comparisons)

## Next Checks
1. **Temporal coherence validation:** Measure mAP degradation on irregular-sampled validation split (>500ms gaps) to identify when ODE extrapolation exceeds training distribution
2. **Intention prediction ambiguity test:** Visualize predicted trajectories vs. ground truth for intersection scenarios where multiple valid paths exist to quantify intention query effectiveness
3. **Hardware fairness verification:** Re-run baseline TensorRT-optimized StreamPETR on identical hardware/software stack to confirm LASP's algorithmic advantage isn't hardware-dependent