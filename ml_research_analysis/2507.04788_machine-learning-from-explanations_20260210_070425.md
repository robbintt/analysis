---
ver: rpa2
title: Machine Learning from Explanations
arxiv_id: '2507.04788'
source_url: https://arxiv.org/abs/2507.04788
tags:
- training
- explanations
- learning
- data
- reasons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training reliable classification
  models when labeled data is limited, imbalanced, or contains spurious correlations.
  The key challenge is that standard training algorithms often learn arbitrary classification
  rules that don't generalize well, especially when explanations for label assignments
  are missing.
---

# Machine Learning from Explanations

## Quick Facts
- arXiv ID: 2507.04788
- Source URL: https://arxiv.org/abs/2507.04788
- Authors: Jiashu Tao; Reza Shokri
- Reference count: 40
- Key outcome: Achieves 0.874 accuracy on 500-sample triangle dataset vs 0.754 for label-only training

## Executive Summary
This paper addresses the challenge of training reliable classification models when labeled data is limited, imbalanced, or contains spurious correlations. The key insight is that standard training algorithms often learn arbitrary classification rules that don't generalize well, especially when explanations for label assignments are missing. The authors propose a two-stage training pipeline that leverages simple explanation signals alongside labels to produce more robust and consistent models.

## Method Summary
The proposed approach uses a two-stage training pipeline that incorporates explanation signals during model training. In the first stage, the model is trained normally using cross-entropy loss. In the second stage, the method computes a feature misalignment loss between the model's latent features and those extracted from masked inputs using the explanations. This loss updates a mapping layer that helps the model focus on the right features. The approach significantly outperforms baselines across multiple datasets and settings, particularly when dealing with class imbalance and spurious correlations.

## Key Results
- When classes are balanced, achieves 0.874 accuracy on a 500-sample triangle dataset versus 0.754 for label-only training
- With 9:1 class imbalance, reaches 0.806 accuracy versus 0.725 for baselines
- When spurious features are added, maintains 0.708 accuracy on clean test sets while baselines drop to 0.561
- Models show superior consistency with pairwise agreement of 0.949 versus 0.711 for standard training on the same data

## Why This Works (Mechanism)
The method works by leveraging explanation signals to guide the model toward learning the true decision boundaries rather than spurious correlations. By computing feature misalignment loss in the second training stage, the approach identifies and corrects the model's focus on irrelevant features. The mapping layer learned through this process effectively reorients the model's attention toward the features that actually matter for correct classification, as indicated by the provided explanations.

## Foundational Learning
- **Feature misalignment loss**: Needed to quantify the difference between learned features and those indicated by explanations; quick check: verify it captures semantic differences in feature space
- **Masked input processing**: Required to extract features from partial inputs as indicated by explanations; quick check: ensure masking preserves relevant signal while removing spurious features
- **Two-stage training pipeline**: Necessary to first establish baseline learning before refinement with explanation signals; quick check: confirm first stage converges before second stage begins
- **Mapping layer optimization**: Critical for translating explanation signals into feature space adjustments; quick check: validate that mapping layer gradients properly reflect explanation guidance
- **Cross-entropy loss with explanations**: Combines standard supervised learning with explanation-based regularization; quick check: balance the two loss components appropriately

## Architecture Onboarding
- **Component map**: Input -> Feature Extractor -> Latent Features -> Classification Head -> Output
- **Critical path**: Masked inputs with explanations -> Feature extractor -> Mapping layer -> Feature misalignment loss
- **Design tradeoffs**: Simpler explanation signals (important features) vs more complex ones (saliency maps); computational overhead of second training stage vs improved generalization
- **Failure signatures**: Poor explanation quality leads to incorrect feature alignment; overfitting during first stage prevents effective refinement in second stage
- **First experiments**: 1) Test on balanced datasets with ground-truth explanations, 2) Evaluate sensitivity to explanation noise, 3) Compare performance with varying amounts of explanation data

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical foundation for why feature misalignment loss addresses spurious correlations is not rigorously established
- Performance heavily depends on quality and reliability of provided explanations
- Method's generalizability to other domain types and explanation forms remains untested

## Confidence
- **High confidence**: The empirical results showing improved accuracy and consistency over baselines on tested datasets
- **Medium confidence**: The generalizability of the method to other domain types and explanation forms beyond tested image and tabular datasets
- **Medium confidence**: The robustness of the method when explanation quality degrades or when explanations contain noise

## Next Checks
1. Test the method's performance when explanations are partially corrupted or noisy to quantify sensitivity to explanation quality
2. Evaluate generalization to completely different domains (e.g., text classification, time series) and explanation types (e.g., saliency maps, feature importance scores)
3. Conduct ablation studies to isolate the contribution of the feature misalignment loss versus the two-stage training pipeline structure