---
ver: rpa2
title: Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking
  Neural Networks
arxiv_id: '2510.24461'
source_url: https://arxiv.org/abs/2510.24461
tags:
- gradient
- training
- learning
- spiking
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training spiking neural networks
  (SNNs) for reinforcement learning in continuous control tasks, particularly drone
  navigation, where non-differentiable spiking neurons and the need for sequence-based
  training create significant obstacles. The authors analyze how surrogate gradient
  slope settings affect learning, showing that shallower slopes improve exploration
  in RL by increasing gradient magnitude but reducing alignment with true gradients.
---

# Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks

## Quick Facts
- **arXiv ID:** 2510.24461
- **Source URL:** https://arxiv.org/abs/2510.24461
- **Reference count:** 40
- **Primary result:** Achieves 400-point average return in drone position control, outperforming prior methods by >600 points

## Executive Summary
This paper addresses the challenge of training spiking neural networks (SNNs) for reinforcement learning in continuous control tasks, particularly drone navigation, where non-differentiable spiking neurons and the need for sequence-based training create significant obstacles. The authors analyze how surrogate gradient slope settings affect learning, showing that shallower slopes improve exploration in RL by increasing gradient magnitude but reducing alignment with true gradients. They propose an adaptive slope scheduling method that improves training efficiency by 4.5×. To address the warm-up period challenge in sequence-based training, they introduce a novel jump-start framework (TD3BC+JSRL) that uses a privileged guiding policy to bootstrap learning while leveraging online environment interactions. Combining this with adaptive surrogate gradients, they achieve an average return of 400 points in drone position control—substantially outperforming prior methods (BC, TD3BC, TD3) which achieve at most -200 points. Their approach demonstrates competitive performance to ANNs while maintaining energy efficiency benefits for neuromorphic deployment.

## Method Summary
The authors propose a combined approach using adaptive surrogate gradients and a jump-start framework to train SNNs for continuous control. The method employs a privileged guiding policy (trained separately with action history access) to bridge the warm-up period needed for stable recurrent states, while the SNN actor gradually takes over control. An asymmetric actor-critic architecture uses an ANN critic with privileged information during training. The key innovation is an adaptive surrogate gradient slope scheduler that adjusts the slope based on recent reward performance, automatically balancing exploration and exploitation. The slope kt is computed from a weighted sum of recent rewards and reward derivatives, with shallow slopes promoting exploration and steep slopes enabling precise optimization.

## Key Results
- Achieves average return of 400 points in drone position control task
- Adaptive slope scheduling reduces training epochs to reach reward of 100 by 4.5× compared to fixed slopes
- Jump-start framework enables stable sequence-based training by bridging the 50-step warm-up period
- Outperforms BC, TD3BC, and TD3 baselines which achieve at most -200 points
- Demonstrates smooth drone flight control without action history access, maintaining energy efficiency benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallower surrogate gradient slopes improve RL training by enhancing gradient flow to deeper layers while introducing beneficial exploration noise.
- Mechanism: The surrogate gradient slope k controls the width of non-zero gradient regions. Shallow slopes (k=1) provide gradients over a broader input range, preventing vanishing gradients in deep layers. This same breadth introduces directional noise (cosine similarity → 0 with true gradient), which functions like parameter space noise for exploration in RL.
- Core assumption: The exploration-exploitation trade-off in RL can be modulated through gradient noise rather than action-space noise alone.
- Evidence anchors:
  - [abstract] "shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients"
  - [Section 3.3] "shallower slopes or scheduled slopes lead to a 2.1x improvement in both training and final deployed performance"
  - [corpus] Weak direct corpus support; neighbor papers focus on surrogate gradients for classification/robustness, not RL slope trade-offs
- Break condition: In supervised learning, the magnitude benefit and alignment penalty cancel out—no net gain observed.

### Mechanism 2
- Claim: A privileged guiding policy enables sequence-based SNN training by bridging the warm-up period that would otherwise prevent learning.
- Mechanism: Early in training, subpar SNN policies crash quickly (e.g., drone crashes in <100 steps), insufficient to span the 50-step warm-up period needed for stable hidden states. A privileged ANN guide policy (with action history access) controls the initial warm-up phase while the SNN's hidden states stabilize. The spiking policy then takes over for the remaining N steps, with N increasing linearly over training.
- Core assumption: A moderately-capable guide policy is obtainable (not expert-level) and can be trained separately with privileged information.
- Evidence anchors:
  - [abstract] "combining jump-start learning with privileged guiding policies and behavioral cloning to bootstrap the learning process"
  - [Section 3.1] "Its primary function is to bridge the critical warm-up period required by the SNN"
  - [Section 5.4, ablation] "eliminating the jump-starting actor entirely prevents the SNN from collecting sequences long enough to bridge the warm-up period"
  - [corpus] No direct corpus evidence for this specific JSRL+SNN combination
- Break condition: If the guide policy cannot maintain stable behavior for the warm-up period, the method fails.

### Mechanism 3
- Claim: Adaptive slope scheduling automatically balances exploration (shallow slopes) and exploitation (steep slopes) without manual hyperparameter sweeps.
- Mechanism: The slope kt is computed from a weighted sum of recent rewards and reward derivatives (Eq. 7). When rewards are low/improving slowly, slopes stay shallow (noisy gradients, broad exploration). As performance saturates, slopes steepen (precise gradients, fine-tuning). The derivative term prevents slope collapse when maximum performance is reached.
- Core assumption: Reward trajectory is a reliable signal for exploration needs; high/plateauing rewards indicate readiness for precise optimization.
- Evidence anchors:
  - [Section 3.3] "shallow slopes... can facilitate the update of a broader set of connections in deeper networks, enhancing exploration in RL"
  - [Section 5.1] "scheduled slope settings firstly reduces the number of epochs to reach a reward of 100 by a factor ×4.5"
  - [corpus] Weak corpus support; neighbor papers don't address slope scheduling
- Break condition: If reward signal is uncorrelated with actual learning progress (e.g., sparse rewards), the scheduler may maladapt.

## Foundational Learning

- **Concept: Surrogate gradient learning**
  - Why needed here: The Heaviside spike function has zero gradient almost everywhere, blocking backpropagation. Surrogate gradients (e.g., fast sigmoid derivative) approximate the gradient during backward pass while preserving discrete spiking in forward pass.
  - Quick check question: Can you explain why replacing a non-differentiable function's gradient during backprop doesn't invalidate the optimization?

- **Concept: Recurrent network warm-up periods**
  - Why needed here: SNNs are stateful—hidden states accumulate history. Freshly initialized states produce unstable outputs until they've processed enough timesteps. Training on sequences requires ignoring gradients during this warm-up (50 steps here).
  - Quick check question: What would happen if you applied gradients during the warm-up period?

- **Concept: Asymmetric actor-critic architectures**
  - Why needed here: The actor (SNN) must be deployable on neuromorphic hardware without action history. The critic (ANN) only exists during training and can use privileged information (action history) for stable value estimation.
  - Quick check question: Why can the critic receive more information than the actor?

## Architecture Onboarding

- **Component map:**
  - State (18-dim) -> Linear encoding -> 256 LIF neurons -> 128 LIF neurons -> 4 linear output neurons (motor commands)
  - State + 32-step action history -> ANN critic (training-only)
  - State + 32-step action history -> Privileged guide policy (training-only)
  - Replay buffer (sequences of 100 steps)
  - Adaptive slope scheduler (computes k from reward trajectory)

- **Critical path:**
  1. Train guide policy via TD3 with privileged action history (stop once it can hover for warm-up period)
  2. Initialize SNN actor, ANN critic, replay buffer
  3. Each rollout: guide policy runs first (500-N) steps, SNN runs remaining N steps (N increases linearly)
  4. Sequences sliced into replay buffer
  5. Actor loss = Q-maximization - λ·BC term (λ decays exponentially: λ←0.99λ)
  6. Critic loss = standard TD3 TD-error
  7. Slope k updated adaptively based on reward trajectory

- **Design tradeoffs:**
  - **Slope k range [1,100]:** Lower = more exploration/noise, higher = more precision. Paper finds no single optimum—adaptive scheduling preferred.
  - **BC weight λ (start=0.2, decay=0.99):** Higher = faster early learning but risk of overfitting to guide policy (which may not be expert). Lower = more exploration but slower.
  - **Sequence length (100) vs warm-up (50):** Must be long enough to span warm-up; too long increases memory and computation.
  - **SNN size [256,128]:** Large enough for control capacity, small enough for Crazyflie deployment.

- **Failure signatures:**
  - **Early crashes, no learning:** Warm-up not bridged; increase guide policy quality or warm-up duration.
  - **BC overfitting (policy mimics mediocre guide):** λ decay too slow; reduce initial λ or increase decay rate.
  - **Training instability, high variance:** Slope too shallow for current learning stage; check adaptive scheduler or use steeper fixed slope.
  - **Gradient explosion:** Check surrogate gradient normalization (Eq. 5 should include 1/k factor).

- **First 3 experiments:**
  1. **Slope ablation on fixed task:** Train with k∈{1,10,100} fixed slopes on a simple control task. Measure convergence speed and final performance. Expect: k=1 faster but noisier, k=100 slower but more stable.
  2. **Warm-up sensitivity:** Vary warm-up period (25, 50, 100 steps) with/without guide policy. Expect: without guide, longer warm-up causes more crashes; with guide, performance should be robust to warm-up length.
  3. **BC weight decay sweep:** Test λ∈{0.1,0.2,0.5} with decay rates {0.99,0.999}. Monitor how quickly policy diverges from guide behavior vs. how quickly it achieves stable flight.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SNN controllers trained with adaptive surrogate gradients achieve smooth, non-oscillatory control comparable to recurrent ANNs without relying on explicit action history?
- Basis in paper: [explicit] The authors note that the deployed SNN "exhibits slightly oscillatory behavior" and suggest future work could "incorporate angular velocity penalties... or increased control frequency" to improve stability.
- Why unresolved: While the method bridges the reality gap, the qualitative smoothness of the control signal remains inferior to history-augmented ANNs, potentially limiting applications requiring high-precision movements.
- What evidence would resolve it: Demonstrating reduced position error and smoother trajectories (lower jerk) on the physical drone by modifying the reward function or increasing the control frequency beyond 100Hz.

### Open Question 2
- Question: How does the quality and behavioral distribution of the privileged guiding policy impact the final performance and convergence speed of the spiking policy?
- Basis in paper: [explicit] The conclusion states that the method "depends on the availability of a guiding policy" and notes that obtaining one that produces stable behavior "might not always be trivial."
- Why unresolved: The study uses a specific privileged actor, but the sensitivity of the final SNN performance to the guide's competence or behavioral noise has not been quantified.
- What evidence would resolve it: A sensitivity analysis measuring SNN performance and convergence time when trained with guiding policies of varying skill levels (e.g., expert vs. near-failure policies).

### Open Question 3
- Question: Do the observed benefits of shallow or adaptive surrogate gradient slopes for RL generalize to significantly deeper SNN architectures or alternative spiking neuron models?
- Basis in paper: [inferred] The surrogate gradient analysis is restricted to a 4-layer network with Leaky Integrate-and-Fire (LIF) neurons.
- Why unresolved: The tradeoff between gradient magnitude and alignment might manifest differently in deeper networks where vanishing gradients are more severe, or in neurons with different reset mechanisms.
- What evidence would resolve it: Experiments applying the adaptive slope schedule to deeper SNNs (e.g., >6 layers) or networks using parametric LIF neurons in comparable continuous control tasks.

## Limitations
- The proposed adaptive slope scheduling relies heavily on reward signal quality, which may not generalize to sparse-reward environments or tasks with high-variance reward distributions
- The jump-start framework requires a separately trained guide policy with privileged information access, adding computational overhead and architectural complexity
- The evaluation is limited to drone navigation in a specific simulation environment, raising questions about transferability to other control domains
- No ablation studies examining the impact of varying warm-up duration relative to task complexity

## Confidence
- **High confidence**: The empirical results showing 4.5× speedup with adaptive slopes on the tested drone navigation task
- **Medium confidence**: The mechanism explaining why shallow slopes improve exploration in RL (theoretical plausibility but limited direct evidence)
- **Medium confidence**: The jump-start framework's effectiveness (no corpus support, relies on a single ablation study)
- **Low confidence**: Generalization claims beyond the specific drone navigation task (no cross-task validation presented)

## Next Checks
1. Test adaptive slope scheduling on sparse-reward tasks (e.g., robotic manipulation with binary success feedback) to evaluate scheduler robustness when reward signals are uninformative
2. Implement ablation studies varying warm-up period duration relative to task complexity to identify optimal trade-offs between early exploration and learning stability
3. Conduct cross-task evaluation on at least two different control domains (e.g., balancing, locomotion) to assess generalizability beyond drone navigation