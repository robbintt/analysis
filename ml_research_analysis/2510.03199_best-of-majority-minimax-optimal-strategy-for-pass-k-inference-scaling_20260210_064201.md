---
ver: rpa2
title: 'Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling'
arxiv_id: '2510.03199'
source_url: https://arxiv.org/abs/2510.03199
tags:
- regret
- arxiv
- inference
- bound
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the Pass@k inference scaling problem, where
  an LLM generates N responses and up to k are selected for evaluation. The authors
  show that existing strategies like majority voting and Best-of-N are suboptimal
  and not scaling-monotonic, meaning their performance degrades or fails to improve
  as N increases.
---

# Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling
## Quick Facts
- arXiv ID: 2510.03199
- Source URL: https://arxiv.org/abs/2510.03199
- Reference count: 7
- One-line primary result: Best-of-Majority (BoM) is minimax-optimal for Pass@$k$ inference, achieving O(εopt + √(ε²_RM C*/k)) regret while avoiding reward overoptimization

## Executive Summary
This paper analyzes the Pass@$k$ inference scaling problem where an LLM generates N responses and up to k are selected for evaluation. Existing strategies like majority voting and Best-of-N are shown to be suboptimal and not scaling-monotonic, meaning their performance degrades as N increases. The authors propose Best-of-Majority (BoM), which filters responses by frequency before reward-based selection, and prove it is minimax optimal. Experiments on math problems (GSM8K, MATH-500, AIME24) confirm BoM outperforms baselines, especially for small k and large N.

## Method Summary
BoM works by first generating N responses from a reference policy, computing empirical frequencies for each unique response, filtering out responses below a frequency threshold α, then selecting the top-k responses based on reward model predictions from the filtered set. The key innovation is the frequency-based filtering step that restricts the reward model to regions where the reference policy has sufficient coverage, reducing estimation error. The method requires N = Θ(C*) samples where C* is the coverage coefficient, and sets α = 3/(4C*) to balance filtering strictness with retaining the optimal response.

## Key Results
- BoM achieves minimax-optimal regret scaling of O(εopt + √(ε²_RM C*/k)), matching the theoretical lower bound
- BoM is scaling-monotonic—performance does not degrade as sampling budget N increases
- Experiments show BoM outperforms Best-of-N and majority voting, particularly for small k values and large N
- On AIME24, BoM achieves higher accuracy than baselines while requiring fewer reward model evaluations

## Why This Works (Mechanism)
### Mechanism 1: Frequency-based filtering prevents reward overoptimization
Filtering candidates by empirical frequency before reward-based selection prevents reward overoptimization on low-probability responses. By discarding responses whose frequency falls below threshold α, BoM restricts the reward model to regions where the reference policy has sufficient coverage, reducing estimation error. This works because reward models have higher uncertainty on responses with low probability under the reference policy due to limited training supervision in those regions.

### Mechanism 2: O(1/√k) regret scaling through candidate distribution
BoM achieves O(1/√k) regret scaling by distributing selection risk across k candidates. When selecting k responses from the filtered set, the minimum estimation error among selected responses scales as √(ε²_RM C*/k). With more candidates (k), the algorithm can better distribute selection risk and the minimum error among top-k decreases proportionally to 1/√k, matching the minimax lower bound.

### Mechanism 3: Scaling-monotonicity through improved frequency estimates
BoM maintains or improves performance as sampling budget N increases because larger N provides better empirical frequency estimates. As N grows, the empirical frequencies bπ(y) better approximate true πref(y|x), making the filtering step more accurate. This ensures the candidate set reliably contains the optimal response while excluding low-quality responses that would exploit reward model errors.

## Foundational Learning
- **Pass@k metric**: The core evaluation framework where k responses are submitted and success occurs if any is correct. Understanding this is essential because regret scaling with k matters—unlike Pass@1 which requires selecting a single best response. Quick check: If an algorithm achieves 60% Pass@1 and 85% Pass@5, what does this tell you about the diversity of correct solutions in the candidate pool?

- **Coverage coefficient C***: C* = 1/πref(y*|x) quantifies how well the reference policy covers the optimal response. It directly appears in the regret bound and determines required sampling budget N = Θ(C*). Quick check: If πref assigns probability 0.001 to the optimal response, what is C* and approximately how many samples N are needed?

- **Minimax optimality**: The paper proves BoM matches a derived lower bound, meaning no algorithm can do better in the worst case. This distinguishes BoM from heuristics that may work well on some instances but fail on others. Quick check: Why is matching a lower bound stronger evidence than just showing empirical improvements on benchmarks?

## Architecture Onboarding
- **Component map**: Sampler -> Frequency Counter -> Filter -> Reward Model -> Selector
- **Critical path**: Sample generation → Frequency estimation → Filtering → Reward evaluation → Top-k selection. The filtering step (Component 3) is the key innovation over vanilla BoN.
- **Design tradeoffs**: 
  - Threshold α: Higher α = stricter filtering = fewer reward evaluations but risk of excluding optimal response. Paper sets α = 3/(4C*), but C* is typically unknown in practice.
  - Sampling budget N: Larger N = better frequency estimates but higher compute cost. Theoretical requirement is N = Θ(C*), which may be large for hard problems.
  - Cluster granularity: For math problems, responses are clustered by mathematical equivalence. This design choice affects both frequency counts and reward aggregation.

- **Failure signatures**:
  1. Performance degrades with larger N: Likely using BoN instead of BoM, or threshold α is incorrectly set
  2. Constant regret regardless of k: Likely using majority voting, or filtering is too aggressive
  3. Empty candidate set after filtering: α is too high or N is too small
  4. Reward overoptimization symptoms: Selecting plausible-looking but incorrect responses—suggests filtering threshold is too low

- **First 3 experiments**:
  1. Replicate GSM8K results with varying k (k ∈ {1, 2, 3, 5, 10}, N=2000): Establish baseline and verify implementation matches paper's Figure 1a pattern where BoM outperforms baselines especially for small k.
  2. Scaling-monotonicity test (vary N from 100 to 2000, fix k=3): Plot accuracy vs N for all three methods. BoM should show flat or improving curve; BoN should show degradation at larger N.
  3. Threshold sensitivity analysis (vary α from 0.001 to 0.05 on AIME24): Identify practical α values when C* is unknown. Monitor tradeoff between empty candidate sets (α too high) and performance degradation (α too low).

## Open Questions the Paper Calls Out
### Open Question 1: Can Best-of-N achieve optimal 1/√k regret scaling?
The paper conjectures that it may be inherently impossible for BoN to obtain a regret upper bound with optimal 1/√k scaling in the Pass@k setting, leaving this as future work. The proof techniques used for Pass@1 do not generalize to selecting k responses, and the error term under the optimal policy becomes dominant in the Pass@k setting.

### Open Question 2: Impact on post-training LLM performance
The authors plan to extend their study to analyze how Pass@k inference strategies impact the performance of LLMs when integrated into the post-training phase. This would analyze the regret and convergence properties of a model that uses Best-of-Majority as part of its reinforcement learning or fine-tuning loop.

### Open Question 3: Multiple optimal responses
The current theoretical analysis assumes a unique optimal response (Assumption 3.2). If multiple optimal responses exist, the interaction between the coverage coefficient and the regret bound is undefined in the current framework, requiring generalization of the theorems.

### Open Question 4: Adaptive frequency threshold estimation
The theoretical requirement is α = 3/(4C*), but C* is typically unknown in practice. The paper leaves open how to adaptively estimate α from the sample set rather than setting it as a fixed constant or function of the unknown C*.

## Limitations
- Theoretical guarantees assume known coverage coefficient C*, but in practice C* must be estimated
- Frequency-based filtering requires substantial sampling (N = Θ(C*)) which may be computationally prohibitive for problems where C* is large
- The reward model error assumption (bounded by εRM) may not hold in practice, particularly for edge cases

## Confidence
- **High Confidence**: The minimax optimality proof and regret bound O(εopt + √(ε²_RM C*/k)) are mathematically rigorous and well-supported
- **Medium Confidence**: Experimental superiority of BoM over baselines is demonstrated, though results are limited to specific mathematical reasoning tasks
- **Medium Confidence**: The scaling-monotonicity property is theoretically proven but practical implications depend on proper parameter tuning
- **Low Confidence**: Practical guidance for choosing α when C* is unknown is limited to rough approximations

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the frequency threshold α across multiple orders of magnitude (0.001 to 0.1) on AIME24 to empirically determine optimal values and characterize the tradeoff between filtering strictness and performance

2. **Generalization Testing**: Apply BoM to non-mathematical domains (e.g., code generation or text summarization) to assess whether the frequency-based filtering mechanism provides similar benefits beyond mathematical reasoning

3. **Unknown C* Scenario**: Implement and evaluate multiple C* estimation methods (e.g., based on perplexity, sampling variance, or holdout validation) to quantify the performance degradation when C* must be estimated rather than known