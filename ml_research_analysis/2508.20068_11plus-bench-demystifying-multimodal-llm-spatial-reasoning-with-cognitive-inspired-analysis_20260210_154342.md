---
ver: rpa2
title: '11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired
  Analysis'
arxiv_id: '2508.20068'
source_url: https://arxiv.org/abs/2508.20068
tags:
- spatial
- reasoning
- human
- cognitive
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces 11Plus-Bench, a high-quality benchmark derived
  from standardized spatial aptitude tests to evaluate the spatial reasoning capabilities
  of multimodal large language models (MLLMs). The benchmark features fine-grained
  expert annotations of perceptual complexity and reasoning processes, enabling detailed
  instance-level analysis.
---

# 11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis

## Quick Facts
- arXiv ID: 2508.20068
- Source URL: https://arxiv.org/abs/2508.20068
- Reference count: 40
- Primary result: MLLMs lag significantly behind humans in spatial reasoning; human correctness is predictable from pattern complexity, while MLLM performance is dominated by low-level visual cues.

## Executive Summary
This paper introduces 11Plus-Bench, a high-quality benchmark derived from standardized spatial aptitude tests (11Plus exams) to evaluate the spatial reasoning capabilities of multimodal large language models (MLLMs). The benchmark features fine-grained expert annotations of perceptual complexity and reasoning processes, enabling detailed instance-level analysis. Experiments across 14 MLLMs and human evaluation reveal a significant performance gap between models and humans, with MLLMs showing early signs of spatial cognition but largely random instance-level performance. Human correctness is highly predictable based on pattern complexity, while MLLM behavior is more influenced by low-level visual cues such as image resolution. These findings highlight both emerging capabilities and critical limitations in current MLLMs' spatial reasoning, providing actionable insights for advancing model design.

## Method Summary
The paper constructs 11Plus-Bench from standardized spatial aptitude tests, comprising a public split of 824 examples and a private split of 91 examples. Each example includes a visual question (diagram) and multiple-choice options (A-E), with expert annotations for pattern complexity and reasoning steps. The benchmark evaluates 14 MLLMs using two input settings: Single Composite Image (question + options in one image) and Separate Images (individual crops). The primary metric is multiple-choice accuracy, with secondary analysis involving Random Forest classifiers to predict instance-level correctness (F1, AUC) and Linear Regression for cognitive load (response time/tokens). The study also includes human evaluation to establish baselines and compare cognitive profiles.

## Key Results
- MLLMs achieve significantly lower accuracy than humans (92.2%) on spatial reasoning tasks, with performance varying widely across models and tasks.
- Human correctness is highly predictable from pattern complexity (F1 scores >0.7), while MLLM correctness is largely random at the instance level.
- MLLM behavior is disproportionately influenced by low-level visual cues (e.g., image resolution) rather than abstract reasoning complexity, unlike humans.

## Why This Works (Mechanism)
The benchmark's design leverages standardized spatial aptitude tests, which are inherently structured to assess spatial reasoning through pattern recognition and logical deduction. The fine-grained expert annotations enable a detailed analysis of perceptual complexity and reasoning processes, providing a cognitive-inspired framework for evaluating MLLMs. The use of Random Forest classifiers and Linear Regression models allows for interpretable insights into the factors driving correctness and cognitive effort, highlighting the divergence between human and model reasoning.

## Foundational Learning
- **Spatial Reasoning**: The ability to visualize and manipulate objects in space; critical for solving tasks involving patterns, shapes, and transformations. Quick check: Can the model identify the next shape in a sequence?
- **Perceptual Complexity**: The visual intricacy of a problem, such as the number of elements or the complexity of their arrangement; influences human performance predictably. Quick check: Does higher pattern complexity correlate with longer response times for humans?
- **Reasoning Steps**: The logical processes required to solve a problem, such as spatial manipulation or logical deduction; used to annotate benchmark examples. Quick check: Can the model's reasoning steps be mapped to human annotations?
- **Instance-Level Analysis**: Evaluating performance on individual examples rather than aggregate metrics; reveals variability in model behavior. Quick check: Is the model's correctness on a given example predictable from its features?
- **Cognitive Load**: The mental effort required to solve a problem, measured via response time or token count; differs between humans and models. Quick check: Does the model's token usage correlate with reasoning difficulty?

## Architecture Onboarding

**Component Map**
Input Images -> MLLM Encoder -> Reasoning Module -> Output Parser -> Correctness Prediction

**Critical Path**
The critical path involves encoding the visual input, performing reasoning (spatial manipulation or logical deduction), and parsing the output to extract the answer. The reasoning module's effectiveness is the primary bottleneck, as models struggle to map abstract patterns to correct answers.

**Design Tradeoffs**
- **Single Composite Image vs. Separate Images**: Single images are more realistic but degrade performance; separate images improve robustness but may not reflect real-world use.
- **Expert Annotations**: Provide valuable interpretability but introduce subjectivity and potential bias in complexity ratings.
- **Model Diversity**: Evaluating 14 models provides breadth but limits depth in analyzing individual architectures.

**Failure Signatures**
- Parsing failures: Models fail to follow the `<ANSWER>` tag format, outputting ordinal numbers or verbalized text.
- Resolution sensitivity: Performance drops significantly on low-resolution images, indicating reliance on visual cues.
- Random instance-level performance: Correctness is unpredictable from instance features, unlike humans.

**3 First Experiments**
1. Run inference on the public split using the "Separate Images" prompt and verify accuracy scores for 3 diverse MLLMs.
2. Systematically vary image resolution in a subset of examples to quantify its impact on MLLM accuracy.
3. Train a Random Forest classifier using the provided annotations to predict MLLM correctness and compute F1 scores.

## Open Questions the Paper Calls Out
1. **How can MLLMs be refined to move from largely random instance-level spatial reasoning performance to the structured, predictable cognitive profiles observed in humans?** The authors state in the Abstract that "instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity." Current models lack the robust, compositional understanding necessary to consistently map abstract pattern complexity to reasoning success. High F1 scores in predicting model correctness based on cognitive features, similar to the predictability found in human evaluators, would resolve this.

2. **What architectural or training modifications are required to reduce MLLM sensitivity to low-level visual cues (e.g., image resolution) and prioritize abstract reasoning?** The Discussion notes that models are "disproportionately influenced by low-level visual cues... calling for further research," unlike humans who rely on pattern complexity. The paper demonstrates that visual perception features (like resolution) currently drive model correctness more than high-level reasoning features. Reduced SHAP values for "Image Resolution" in feature importance analyses alongside increased weights for reasoning-related features would resolve this.

3. **Can MLLMs be trained to align their internal cognitive effort (e.g., thinking token count) with reasoning difficulty rather than visual complexity?** While human cognitive effort correlates strongly with reasoning steps (Spatial Manipulation), the analysis shows MLLM cognitive effort is significantly driven by "Question Pattern Complexity" (visual load). The paper reveals a divergence where models expend effort on visual clutter rather than the logical steps humans find difficult. A shift in regression coefficients showing a stronger correlation between model token usage and "Logical Deduction" steps than with visual complexity metrics would resolve this.

## Limitations
- The private 91-example set is withheld for anti-contamination, preventing full validation of reported results on held-out data.
- The human baseline (92.2% accuracy) is based on a limited pool of 10 participants, which may not generalize to broader populations.
- Expert annotation for pattern complexity and reasoning steps introduces potential subjectivity not fully quantified.

## Confidence
- **High Confidence**: The core experimental methodology (benchmark construction, evaluation protocol, and baseline results) is clearly specified and reproducible. The observed performance gap between MLLMs and humans is robust across multiple models and tasks.
- **Medium Confidence**: The interpretability of instance-level correctness prediction models is plausible given the reported feature importance, but the exact feature construction and hyperparameters are not fully specified. The claim that low-level visual cues (e.g., image resolution) disproportionately affect MLLM performance is supported but could benefit from more systematic ablation.
- **Low Confidence**: The generalization of human baseline results to the broader population is uncertain due to the small participant pool. The assertion that MLLMs exhibit "early signs of spatial cognition" is qualitative and not directly measurable from the presented data.

## Next Checks
1. **Replicate on Public Set**: Independently run inference on the public 824-example split using the "Separate Images" prompt and verify accuracy scores for at least 3 diverse MLLMs (e.g., GPT-4o, Qwen-VL-2.5, Gemini-1.5-Pro) to confirm baseline performance consistency.

2. **Feature Ablation for MLLM Performance**: Systematically vary image resolution and visual clarity in a subset of examples to quantify the impact of low-level visual cues on MLLM accuracy, directly testing the paper's claim about resolution sensitivity.

3. **Expand Human Baseline**: Conduct a follow-up human evaluation with a larger, more diverse participant pool (e.g., n=30) on a representative subset of the benchmark to assess the robustness of the reported 92.2% accuracy and response time distributions.