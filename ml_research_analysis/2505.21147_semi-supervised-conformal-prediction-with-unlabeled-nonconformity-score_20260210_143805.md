---
ver: rpa2
title: Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score
arxiv_id: '2505.21147'
source_url: https://arxiv.org/abs/2505.21147
tags:
- semicp
- coverage
- prediction
- data
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability and inefficiency of conformal
  prediction (CP) under limited labeled calibration data, where standard CP often
  produces overly large prediction sets with high variability. The authors propose
  SemiCP, a semi-supervised framework that leverages unlabeled data during calibration.
---

# Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score

## Quick Facts
- arXiv ID: 2505.21147
- Source URL: https://arxiv.org/abs/2505.21147
- Authors: Xuanning Zhou; Hao Zeng; Xiaobo Xia; Bingyi Jing; Hongxin Wei
- Reference count: 40
- Primary result: Semi-supervised conformal prediction framework that uses unlabeled data to improve calibration, achieving up to 76% reduction in coverage gap and 7% in prediction set size with only 20 labeled samples

## Executive Summary
This paper addresses the instability and inefficiency of conformal prediction (CP) under limited labeled calibration data, where standard CP often produces overly large prediction sets with high variability. The authors propose SemiCP, a semi-supervised framework that leverages unlabeled data during calibration. They introduce NNM, a nonconformity score function for unlabeled examples that matches each unlabeled point to its nearest labeled neighbor in pseudo-label score space and uses the neighbor's true-label score to debias the pseudo-score. Theoretically, they show that NNM asymptotically recovers the true nonconformity score distribution, ensuring coverage guarantees. Empirically, SemiCP consistently reduces coverage gaps and prediction set sizes across CIFAR-10, CIFAR-100, and ImageNet, achieving improvements such as a 76% reduction in coverage gap and 7% in prediction set size with only 20 labeled samples. It generalizes to conditional CP and integrates seamlessly with existing CP enhancements.

## Method Summary
SemiCP extends standard split conformal prediction by incorporating unlabeled data into the calibration process. The method computes pseudo-labels for unlabeled examples using a pre-trained model, then estimates their nonconformity scores by matching each unlabeled point to its nearest labeled neighbor in pseudo-score space and using the neighbor's true-label score to correct the pseudo-score bias. This debiased score is combined with labeled scores to estimate a more stable quantile threshold. The framework is modular and can work with any standard nonconformity score function (THR, APS, RAPS) and extends to conditional settings. Theoretical analysis shows that under mild assumptions, the method provides asymptotic coverage guarantees while reducing both coverage gaps and prediction set sizes.

## Key Results
- Achieves up to 76% reduction in coverage gap and 7% in prediction set size on CIFAR-10 with only 20 labeled samples
- Consistent improvements across CIFAR-10, CIFAR-100, and ImageNet datasets
- Generalizes to conditional CP settings (group-conditional, class-conditional) with similar performance gains
- Reduces the number of labeled samples needed for valid CP by effectively leveraging unlabeled data

## Why This Works (Mechanism)

### Mechanism 1: Stability via Augmented Calibration
- Claim: If estimated nonconformity scores for unlabeled data share the same distribution as true scores, SemiCP prediction sets retain marginal coverage guarantees while reducing threshold variance.
- Mechanism: SemiCP enlarges the effective calibration set by computing NNM scores for unlabeled examples and combining them with labeled scores, yielding a more stable quantile estimate that reduces coverage gap and prediction set size variability under limited labeled data.
- Core assumption: Labeled and unlabeled data are i.i.d. from the same distribution, and unlabeled score function produces scores with same CDF as true scores (Proposition 1).
- Evidence anchors: [abstract] "We theoretically demonstrate that, under mild assumptions, SemiCP provide asymptotically coverage guarantee for prediction sets." [section 3.1] Proposition 1 states the coverage guarantee condition; Appendix A.2 provides the proof.

### Mechanism 2: NNM Debiasing for Score Distribution Alignment
- Claim: If pseudo-bias of nearest labeled neighbor approximates pseudo-bias of unlabeled example, then NNM-adjusted scores converge in distribution to true nonconformity scores as labeled data increases.
- Mechanism: For each unlabeled point, NNM finds labeled neighbor with closest pseudo-label score, computes neighbor's observed bias (true score minus pseudo-score), and adds this bias to unlabeled point's pseudo-score. Under continuity assumptions, this asymptotically recovers true score distribution (Proposition 2).
- Core assumption: Continuity of score functions and random variables, and labeled data density grows sufficiently in local neighborhoods (Assumptions 2-4).
- Evidence anchors: [section 3.2] Proposition 2 and its proof (Appendix A.3) establish convergence; Figure 2 empirically shows bias distribution similarity between unlabeled points and their nearest labeled neighbors.

### Mechanism 3: Modular Integration with Existing CP Methods
- Claim: If NNM scores are computed using any standard labeled score function, then SemiCP can reduce coverage gap and set size for those functions and extend to conditional CP settings.
- Mechanism: NNM is score-function-agnostic; it uses output of any labeled nonconformity function (THR, APS, RAPS) to compute pseudo-scores and biases. Adjusted unlabeled scores are then pooled with labeled scores for quantile estimation, applicable to marginal, group-conditional, and class-conditional CP.
- Core assumption: Chosen labeled score function is well-defined and continuous for both labeled and unlabeled data.
- Evidence anchors: [section 4.2] Experiments show consistent improvements across THR, APS, and RAPS (Table 2); Figure 5 demonstrates effectiveness in conditional settings.

## Foundational Learning

- Concept: **Split Conformal Prediction (Split CP)**
  - Why needed here: SemiCP extends Split CP by augmenting calibration with unlabeled data; understanding standard framework is prerequisite.
  - Quick check question: In Split CP, how is threshold τ̂ computed from labeled calibration set, and what coverage guarantee does it provide?

- Concept: **Nonconformity Score Functions**
  - Why needed here: NNM builds on existing score functions (THR, APS, RAPS) to estimate scores for unlabeled data; knowledge of these functions is essential.
  - Quick check question: For classifier outputting softmax probabilities, how would THR and APS scores differ for given input-label pair?

- Concept: **Semi-Supervised Learning Principles**
  - Why needed here: SemiCP leverages unlabeled data to improve calibration; understanding motivation and assumptions of semi-supervised learning provides context.
  - Quick check question: Why might unlabeled data help in quantile estimation even without true labels?

## Architecture Onboarding

- Component map: Pre-trained model f -> Pseudo-label computation -> Labeled scores S(x_i, y_i) -> Unlabeled pseudo-scores S(x̃_i, ŷ̃_i) -> NNM matching -> Debiased scores -> Pooled scores -> Quantile estimation -> Threshold τ̂ -> Prediction set C(x_test)

- Critical path:
  1. Ensure model f and score function S are defined and compatible
  2. Verify calibration data i.i.d. assumption and distribution alignment between labeled/unlabeled sets
  3. Implement NNM matching efficiently (consider vectorized distance computation for scalability)
  4. Validate coverage and efficiency on held-out test set before deployment

- Design tradeoffs:
  - Labeled set size n: Larger n improves bias estimation accuracy but reduces gains from unlabeled data
  - Unlabeled set size N: More unlabeled data reduces variance but may increase computational cost
  - Neighbor selection criterion: Pseudo-score matching vs. feature-space matching (ablation in Figure 11)
  - Number of neighbors (k): k=1 minimizes coverage gap; larger k may reduce set size but risk undercoverage

- Failure signatures:
  - Undercoverage: If pseudo-label accuracy is very low (<70%), NNM may not correct bias sufficiently
  - Distribution shift: If unlabeled data differs from labeled data, i.i.d. assumption breaks
  - High variance with very small n: With n<10, even NNM may not stabilize threshold estimation

- First 3 experiments:
  1. Reproduce baseline comparison: Implement SemiCP on CIFAR-10 with n=20 labeled, N=1000 unlabeled using THR score; compare coverage gap and avg set size against Standard CP over 100 trials
  2. Ablate unlabeled data size: Fix n=50 and vary N∈{10, 50, 100, 500, 1000} on ImageNet; plot coverage gap vs. N to verify diminishing returns pattern
  3. Integrate with different score function: Apply NNM to score function not tested (e.g., SAPS) on CIFAR-100; report whether improvements hold

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely heavily on Assumptions 1-4 (continuity, i.i.d. data, sufficient labeled density) which may not hold in real-world settings
- Performance degrades significantly when pseudo-label accuracy falls below ~70%, limiting applicability when labels are hard to obtain
- Computational complexity scales with unlabeled set size N, potentially prohibitive for very large datasets

## Confidence

- **High Confidence**: Core theoretical framework (Propositions 1-2), empirical improvements on standard benchmarks (CIFAR-10/100, ImageNet), integration with existing CP methods (THR, APS, RAPS)
- **Medium Confidence**: Performance under distribution shift, effectiveness with very small labeled sets (n<20), generalizability to non-image domains
- **Low Confidence**: Robustness to pseudo-label accuracy below 70%, computational scalability to massive unlabeled datasets, performance with highly non-smooth score functions

## Next Checks

1. Test SemiCP on dataset with known distribution shift (e.g., DomainNet) to assess coverage guarantee robustness when i.i.d. assumption is violated
2. Evaluate performance with pseudo-label accuracy ranging from 50-95% on CIFAR-10 to identify practical accuracy threshold for reliable NNM debiasing
3. Benchmark computational runtime and memory usage for N=10k, 100k, and 1M unlabeled samples to determine scalability limits and identify optimization opportunities