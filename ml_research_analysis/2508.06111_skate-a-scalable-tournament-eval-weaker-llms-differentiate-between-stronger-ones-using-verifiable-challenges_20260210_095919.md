---
ver: rpa2
title: 'SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger
  ones using verifiable challenges'
arxiv_id: '2508.06111'
source_url: https://arxiv.org/abs/2508.06111
tags:
- questions
- question
- which
- game
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SKATE, a scalable automated framework for
  evaluating large language models (LLMs) by having them compete in generating and
  solving verifiable tasks for each other. Instead of relying on human-designed benchmarks
  or LLM judges, SKATE uses a game-like setup where models act as both task-setters
  and solvers, incentivized to create questions that are both solvable by themselves
  and challenging for others.
---

# SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges

## Quick Facts
- arXiv ID: 2508.06111
- Source URL: https://arxiv.org/abs/2508.06111
- Reference count: 2
- Primary result: Weaker models can reliably differentiate and rank stronger ones using verifiable challenges

## Executive Summary
The paper introduces SKATE, a scalable automated framework for evaluating large language models (LLMs) by having them compete in generating and solving verifiable tasks for each other. Instead of relying on human-designed benchmarks or LLM judges, SKATE uses a game-like setup where models act as both task-setters and solvers, incentivized to create questions that are both solvable by themselves and challenging for others. As a proof of concept, the authors implement this using code-output-prediction (COP) challenges with a TrueSkill-based ranking system. They evaluate six frontier LLMs and find that weaker models can reliably differentiate and rank stronger ones, models exhibit self-preferencing behavior in question generation, and the framework automatically surfaces fine-grained capability differences.

## Method Summary
SKATE implements a tournament-style evaluation where LLMs generate and solve Code-Output-Prediction (COP) challenges for each other. Models act as both task-setters and solvers, incentivized to create questions they can solve but opponents cannot. Questions are validated through code execution and checked for uniqueness via embedding similarity. All players answer all questions using an adaptive MCQ sampling algorithm until confidence thresholds are met. A TrueSkill rating system tracks and updates model rankings based on pairwise comparisons across multiple rounds.

## Key Results
- Weaker models can reliably differentiate and rank stronger ones in pairwise comparisons
- Models exhibit self-preferencing behavior, performing best on their own generated questions
- The framework automatically surfaces fine-grained capability differences between models
- "Historical performance" augmentation provides the most efficient balance between information and context costs

## Why This Works (Mechanism)

### Mechanism 1: Competitive Discrimination via Self-Interest
Models are incentivized to maximize a score tied to competitors' failure, autonomously generating "discriminatory" questions that reveal fine-grained capability gaps. The framework rewards +1 for creating valid questions that the model can answer but opponents cannot, forcing models to search their own capability boundaries.

### Mechanism 2: Grounding via Verifiable Code Execution
Using COP allows the system to bypass LLM-judge biases by enforcing objective, deterministic ground truth through code execution rather than semantic interpretation. This decouples evaluation from preference biases of judge models.

### Mechanism 3: Weak-to-Strong Generalization through Diversity
A cohort of weaker models can collectively generate diverse enough question sets to differentiate stronger models by covering distinct "capability niches." While a single weak model might lack probing power, aggregate output from multiple diverse weak models creates broad challenge surfaces.

## Foundational Learning

**Concept: TrueSkill Rating System**
- Why needed: Standard accuracy scores fail to capture transitive relationships and uncertainty in pairwise model comparisons
- Quick check: How does TrueSkill update a player's μ if they win against an opponent with high uncertainty vs. low uncertainty?

**Concept: Multiple-Choice Response Sensitivity**
- Why needed: LLMs are sensitive to distractor order and selection in MCQs
- Quick check: Why does the paper shuffle 4-option sets multiple times and require σ*<0.05 before finalizing scores?

**Concept: Embedding-based Uniqueness**
- Why needed: To prevent "reward hacking" where models generate slight variations of the same easy question
- Quick check: What distance threshold (d_thresh) does the paper use to reject similar questions, and why is a low threshold problematic?

## Architecture Onboarding

**Component map:** Task Setter Agent -> Validation Layer (sandbox + embedding model) -> Answering Agent -> Scoring Engine

**Critical path:**
1. Task Setter receives context and generates code with distractors
2. Sandbox executes code for validation; Embedder checks uniqueness
3. All players answer question via sampled MCQ algorithm
4. Scoring engine computes p(correct) and updates TrueSkill parameters

**Design tradeoffs:**
- COP Domain: Highly verifiable and objective but limited to code/logic reasoning
- Augmentation Strategies: "Full Context" provides more data but increases context costs; "Historical Performance" offers best efficiency
- Weaker vs. Stronger: Using weaker models as generators is cheaper but risks missing "super-human" edge cases

**Failure signatures:**
- Reward Hacking: Models generate trivial code repeatedly if uniqueness threshold is too lenient
- Stall Loops: MCQ scoring algorithm fails to converge due to random guessing or inconsistency
- Self-Preferencing Failure: Model fails to answer its own questions, indicating poor calibration

**First 3 experiments:**
1. Implement MCQ scoring algorithm on fixed dataset to verify p(correct) convergence
2. Run 2-model game with varying d_thresh values to visualize question diversity tradeoff
3. Replicate Section 6.1 experiment with 3 small models generating questions for a 4th larger model

## Open Questions the Paper Calls Out

**Open Question 1:** Can SKATE maintain ranking validity when applied to non-code verifiable domains like game-playing or physical world simulations? The current study is restricted to COP tasks, leaving generalizability unproven.

**Open Question 2:** To what extent can future frontier models leverage "Full Context" augmentation to strategically exploit game state information for adversarial question setting? Current models show minimal impact from augmentation strategies.

**Open Question 3:** How can the framework adapt to evaluate models equipped with code execution tools, given that COP tasks become trivial for such agents? This identifies an obvious limitation requiring new verifiable tasks.

## Limitations
- Reliance on code-output-prediction tasks may systematically underrepresent non-code reasoning capabilities
- Empirical validation uses only 4 models, which may not generalize to broader capability distributions
- Assumes aggregated weak model questions provide sufficient coverage of reasoning space without rigorous proof

## Confidence

- **High confidence**: Technical implementation of MCQ scoring algorithm and TrueSkill-based ranking system
- **Medium confidence**: Claim that weaker models can differentiate stronger ones based on 4-model experiment
- **Low confidence**: Generalizability beyond code-output-prediction domains remains unproven

## Next Checks

1. Scale model diversity: Replicate Section 6.1 with 6+ models spanning wider capability range to test weak-to-strong differentiation across heterogeneous domains

2. Domain expansion validation: Implement SKATE with non-code verifiable task (e.g., mathematical proof verification) to assess framework's discriminative power outside COP domain

3. Question coverage analysis: Quantitatively measure diversity of reasoning patterns in weak model questions versus those strong models would generate using clustering analysis on embedding representations