---
ver: rpa2
title: Proper Learnability and the Role of Unlabeled Data
arxiv_id: '2502.10359'
source_url: https://arxiv.org/abs/2502.10359
tags:
- learning
- which
- learner
- proper
- learnability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies proper learnability, where a learner must always
  output a predictor in the hypothesis class H. The authors focus on multiclass classification,
  a setting where ERM (a canonical proper learner) is known to fail.
---

# Proper Learnability and the Role of Unlabeled Data

## Quick Facts
- arXiv ID: 2502.10359
- Source URL: https://arxiv.org/abs/2502.10359
- Reference count: 19
- Key outcome: This paper studies proper learnability, where a learner must always output a predictor in the hypothesis class H. The authors focus on multiclass classification, a setting where ERM (a canonical proper learner) is known to fail.

## Executive Summary
This paper investigates proper learnability in multiclass classification, where learners must output predictors from the hypothesis class H. The authors introduce the distribution-fixed PAC model, where the learner has access to the marginal distribution over unlabeled data, and demonstrate that all finite learning problems with bounded metric loss can be learned to optimal expected error using distributional regularization. They show that sample complexities in this model differ from the classic PAC model by at most a logarithmic factor, challenging the role of unlabeled data in PAC learning from a worst-case perspective.

The paper also presents several impossibility results for characterizing proper learnability in the classic PAC model. These include showing that proper learnability can be logically undecidable, that it is not a monotone property of the hypothesis class, and that it is not a local property. These findings imply that characterizing proper learnability will require fundamentally different techniques from standard learning theory. Additionally, the authors establish an equivalence between EMX learning (previously known to be undecidable) and proper multiclass learning, bridging two previously distinct areas of learning theory.

## Method Summary
The authors propose distributional regularization as a generalized form of regularization for proper learning in the distribution-fixed PAC model. This approach leverages access to the marginal distribution over unlabeled data to construct proper learners that achieve optimal expected error. The method extends beyond traditional empirical risk minimization by incorporating distributional information into the learning process. The authors also provide formal proofs of their impossibility results through logical constructions that encode undecidable mathematical problems into learning scenarios.

## Key Results
- In the distribution-fixed PAC model, all finite learning problems with bounded metric loss can be learned to optimal expected error by proper learners using distributional regularization
- Sample complexities in the distribution-fixed PAC model differ from classic PAC by at most a logarithmic factor
- Proper learnability in the classic PAC model can be logically undecidable (independent of ZFC axioms)
- Proper learnability is neither a monotone nor a local property of the hypothesis class

## Why This Works (Mechanism)
The distributional regularization mechanism works by leveraging the known marginal distribution over unlabeled data to construct proper learners that can achieve optimal expected error. This approach overcomes the limitations of empirical risk minimization, which fails in multiclass settings. The method essentially uses the distributional information to guide the learning process toward hypothesis class members that generalize well, even when the hypothesis class itself may have complex structure that prevents standard ERM from succeeding.

## Foundational Learning

**Proper Learning**: Why needed - Distinguishes between learners that output arbitrary predictors versus those constrained to the hypothesis class. Quick check - Verify whether a learner's output belongs to the specified hypothesis class H.

**Distribution-fixed PAC Learning**: Why needed - Models scenarios where unlabeled data distributions are known or easily estimable. Quick check - Confirm access to marginal distribution over unlabeled data.

**Metric Loss Bounds**: Why needed - Ensures learning guarantees are robust to bounded perturbations. Quick check - Verify that loss function satisfies bounded metric constraints.

**ZFC Independence**: Why needed - Establishes logical limits on what can be decided about learnability. Quick check - Test whether specific learning problems can be encoded as undecidable mathematical statements.

## Architecture Onboarding

Component map: Distributional Regularization -> Proper Learner -> Hypothesis Class H

Critical path: Unlabeled data distribution → Distributional regularization → Proper predictor → Generalization guarantee

Design tradeoffs: The distribution-fixed model trades worst-case generality for practical accessibility of distributional information, enabling proper learning where standard PAC fails.

Failure signatures: If distributional regularization fails, check: 1) incorrect estimation of unlabeled distribution, 2) hypothesis class too complex for regularization to handle, 3) metric loss bounds violated.

First experiments: 1) Test distributional regularization on synthetic multiclass problems with known distributions, 2) Compare sample complexity bounds between distribution-fixed and classic PAC models, 3) Construct hypothesis classes demonstrating undecidability of proper learnability.

## Open Questions the Paper Calls Out
None

## Limitations
- The distribution-fixed PAC model assumes access to marginal distributions over unlabeled data, which may not be realistic in many practical settings
- The undecidability results rely on specific mathematical encodings that may not represent typical learning scenarios
- The logarithmic gap between distribution-fixed and classic PAC sample complexities, while bounded, may still be significant in practice

## Confidence
- Distributional regularization results: High confidence
- Logarithmic sample complexity bounds: Medium confidence (requires careful verification of constants)
- Undecidability of proper learnability: Medium confidence (theoretical construction dependent)
- Non-monotonicity and non-locality results: High confidence
- EMX learning equivalence: Medium confidence (depends on specific problem encodings)

## Next Checks
1. Verify the constants in the sample complexity bounds to confirm the logarithmic gap between distribution-fixed and classic PAC models
2. Construct explicit examples of hypothesis classes where proper learnability is undecidable to test the theoretical claims
3. Implement and test the distributional regularization algorithm on benchmark multiclass classification problems to assess practical performance