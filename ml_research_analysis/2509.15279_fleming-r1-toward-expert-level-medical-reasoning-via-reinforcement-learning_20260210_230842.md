---
ver: rpa2
title: 'Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning'
arxiv_id: '2509.15279'
source_url: https://arxiv.org/abs/2509.15279
tags:
- reasoning
- medical
- uni00000010
- uni00000048
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fleming-R1 advances medical AI by integrating reasoning-oriented
  data, CoT cold start, and verifiable RL to address the gap between static knowledge
  and expert clinical reasoning. It uses a large medical knowledge graph to generate
  underrepresented cases and multi-hop reasoning paths, distills expert-level reasoning
  trajectories, and applies a two-stage RLVR framework with adaptive hard-sample mining.
---

# Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.15279
- Source URL: https://arxiv.org/abs/2509.15279
- Reference count: 40
- Key outcome: 7B model surpasses much larger baselines; 32B model reaches near-parity with GPT-4o on expert-level medical reasoning

## Executive Summary
Fleming-R1 addresses the gap between static medical knowledge and expert clinical reasoning by integrating reasoning-oriented data synthesis, Chain-of-Thought cold start initialization, and verifiable reinforcement learning. The approach uses a large medical knowledge graph to generate underrepresented cases and multi-hop reasoning paths, distills expert-level reasoning trajectories, and applies a two-stage RLVR framework with adaptive hard-sample mining. Evaluated across nine medical benchmarks, Fleming-R1 achieves state-of-the-art performance: the 7B variant surpasses much larger baselines, while the 32B model reaches near-parity with GPT-4o. On MedXpertQA, Fleming-R1 attains 30.33%, matching GPT-4o at expert-level difficulty. The model is released to promote transparent, reproducible, and auditable progress in high-stakes clinical AI.

## Method Summary
Fleming-R1 combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. The approach uses a topological sampling method on a medical knowledge graph (>100K entities) to create synthetic reasoning-intensive questions. A high-capacity teacher model generates Chain-of-Thought trajectories, which are refined through iterative protocols (backtracking, path exploration, self-correction) and distilled into the student model via supervised fine-tuning. A two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework consolidates fundamental reasoning skills on easy/moderate data before targeting persistent failure modes through adaptive hard-sample mining using Group Relative Policy Optimization.

## Key Results
- Fleming-R1 achieves state-of-the-art performance across nine medical benchmarks
- 7B variant surpasses much larger baseline models
- 32B model reaches near-parity with GPT-4o on expert-level tasks
- Achieves 30.33% on MedXpertQA, matching GPT-4o at expert difficulty level

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph-Guided Data Synthesis (RODS)
- Generating training data via knowledge graph sampling improves coverage of underrepresented medical entities and multi-hop reasoning paths
- Large medical knowledge graph (>100K entities) used to sample subgraphs, masked to create reasoning-intensive questions targeting long-tail diseases/drugs and compositional depth
- Core assumption: knowledge graph accurately reflects medical relations; sampling yields pedagogically useful, non-redundant problems

### Mechanism 2: CoT Cold Start (Reasoning Distillation)
- Initializing model with distilled Chain-of-Thought trajectories establishes robust reasoning priors before reinforcement learning
- High-capacity teacher model generates CoT reasoning paths, refined (backtracking, path exploration, self-correction) and distilled into student model via supervised fine-tuning
- Core assumption: teacher model's reasoning is both correct and pedagogically useful; distillation effectively transfers these patterns

### Mechanism 3: Two-Stage Reinforcement Learning from Verifiable Rewards (RLVR)
- Curriculum-based two-stage RL process using verifiable rewards (correctness, format) enhances reasoning, particularly on hard cases
- Stage 1 consolidates core skills on easy/moderate data; Stage 2 uses adaptive hard-sample mining (targeting persistent failures) with Group Relative Policy Optimization
- Core assumption: rewards based on final correctness and format are sufficient and safe signals for medical reasoning

## Foundational Learning

- **Concept: Knowledge Graphs (KGs) in Medicine**
  - Why needed here: To understand how RODS strategy generates synthetic data by sampling structured medical relationships for long-tail coverage
  - Quick check question: Given a medical KG with nodes for Disease, Symptom, and Drug, describe how a 2-hop path could form a diagnostic question

- **Concept: Chain-of-Thought (CoT) Prompting and Distillation**
  - Why needed here: To grasp how model is initialized with reasoning trajectories before RL and role of iterative refinement
  - Quick check question: Explain one benefit and one risk of using high-capacity LLM as teacher for CoT distillation

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: To understand how model is fine-tuned using objective reward signals rather than human preference
  - Quick check question: How does using group-relative baseline in GRPO (vs. absolute reward) potentially stabilize training

## Architecture Onboarding

- **Component map:**
  Public QA datasets + Wikipedia-derived Medical KG -> Synthetic Question Generator -> Multi-stage Filter (format, label verification, difficulty annotation) -> CoT Cold Start Trainer (7B only) -> RLVR System (Stage 1 Easy+Moderate, Stage 2 Hard samples)

- **Critical path:**
  1. Acquire/filter public data and build medical KG
  2. Generate synthetic hard cases via subgraph sampling
  3. Run CoT cold start (for 7B model)
  4. Execute RLVR Stage 1 until plateau
  5. Execute RLVR Stage 2 with hard-sample mining

- **Design tradeoffs:**
  - Verifiable vs. Nuanced Rewards: Only correctness/format rewards used, trading nuance for safety against reward hacking
  - Synthetic vs. Real Data: Synthetic data expands coverage but may lack realism; mixed strategy mitigates this
  - Cold Start Necessity: Omitted for stronger base models (32B) to save compute

- **Failure signatures:**
  - Reward Hacking: Model maximizes reward signal without genuine reasoning (e.g., format manipulation)
  - Curriculum Collapse: Stage 2 fails to improve due to overly difficult samples or insufficient exploration
  - Synthetic Data Noise: Poorly formed questions degrade model quality

- **First 3 experiments:**
  1. Ablate Data Source: Train on (a) public data only, (b) synthetic only, (c) mixed RODS. Compare performance, especially on rare disease subsets
  2. Cold Start Impact: Train 7B model with/without CoT cold start, then apply identical RLVR. Compare convergence and final performance
  3. RLVR Stage Analysis: Log per-bucket (Easy/Moderate/Difficult) accuracy over time. Visualize how Stage 1-to-2 transition affects learning and if hard-sample mining targets failure modes

## Open Questions the Paper Calls Out

- **Open Question 1**: Does Fleming-R1's benchmark performance translate to improved clinical utility in real-world healthcare settings with actual clinicians and patients?
  - Basis: All evaluation is conducted on standardized test benchmarks without clinical deployment studies or clinician-in-the-loop validation
  - Why unresolved: Medical exam-style MCQs may not capture real clinical complexity involving incomplete information, longitudinal patient data, multi-morbidity, and time-critical decisions
  - What evidence would resolve it: Prospective studies measuring diagnostic concordance with specialist physicians on de-identified real patient cases, or randomized trials comparing Fleming-R1-assisted vs. unassisted clinician performance

- **Open Question 2**: Would adding CoT cold start to the 32B variant (which currently only receives RLVR training) further improve performance on expert-level reasoning tasks?
  - Basis: The paper states the 32B model only received RLVR training, omitting CoT cold start without ablation
  - Why unresolved: The 7B ablation shows CoT cold start provides +3.1 pp gains before RLVR, but whether this complementary benefit extends to larger models remains untested
  - What evidence would resolve it: Ablation experiments on 32B model comparing four conditions: base only, base + RLVR, base + CoT cold start, and base + both stages

- **Open Question 3**: Do the generated chain-of-thought explanations faithfully represent the model's actual reasoning process, or could they be post-hoc rationalizations that appear plausible but diverge from true decision pathways?
  - Basis: The paper cites work on unfaithful CoT explanations but does not implement specific faithfulness validation for Fleming-R1's reasoning chains
  - Why unresolved: Optimizing for verifiable format and answer correctness does not guarantee that reasoning traces reflect genuine inference steps
  - What evidence would resolve it: Causal intervention studies modifying intermediate reasoning tokens to test impact on final answers, or systematic comparison with human expert reasoning traces

## Limitations

- RLVR implementation lacks key hyperparameters including learning rates, batch sizes, rollout counts (k), KL coefficients, and explicit stage transition criteria
- Knowledge graph construction methodology—particularly entity linking, relation extraction, and subgraph sampling thresholds—is not fully detailed
- Synthetic data generation relies on implicit assumptions about prompt templates and verification protocols that are not fully described
- Evaluation is limited to multiple-choice benchmarks, potentially missing broader clinical reasoning capabilities

## Confidence

- **High Confidence**: The core architecture (RODS + CoT cold start + two-stage RLVR) is clearly specified and technically sound
- **Medium Confidence**: The effectiveness of specific implementation choices depends on implementation details not fully disclosed
- **Low Confidence**: The generalization of results beyond the specific benchmarks tested and performance on open-ended clinical scenarios

## Next Checks

1. **Ablation Study on Data Sources**: Systematically train models on (a) public data only, (b) synthetic data only, and (c) the full RODS mixed dataset. Evaluate performance differences, particularly on rare disease subsets and multi-hop reasoning tasks

2. **Cold Start Impact Analysis**: For the 7B variant, conduct a controlled experiment comparing models with and without CoT cold start initialization, followed by identical RLVR training. Measure differences in convergence speed, final accuracy, and qualitative differences in reasoning quality

3. **Reward Function Robustness Test**: Evaluate the model's susceptibility to reward hacking by deliberately introducing format-only correct answers and analyzing whether the model learns to game the reward signal. Implement additional reward components and measure impact on performance and safety