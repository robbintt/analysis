---
ver: rpa2
title: 'CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity'
arxiv_id: '2512.16282'
source_url: https://arxiv.org/abs/2512.16282
tags:
- quantization
- calm
- layer
- gptq
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of uniform quantization strategies
  in post-training quantization (PTQ) of large language models (LLMs), which overlook
  layer-specific algorithmic suitability. The authors propose CKA-Guided Modular Quantization
  (CALM), a fine-tuning-free framework that uses Linear Centered Kernel Alignment
  (CKA) to evaluate and select the optimal PTQ method (e.g., GPTQ, AWQ, SmoothQuant)
  for each layer independently.
---

# CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity

## Quick Facts
- **arXiv ID:** 2512.16282
- **Source URL:** https://arxiv.org/abs/2512.16282
- **Reference count:** 11
- **Primary result:** CKA-Guided Modular Quantization (CALM) achieves better perplexity and accuracy than uniform and state-of-the-art mixed-precision quantization methods for LLMs

## Executive Summary
This paper addresses the limitation of uniform quantization strategies in post-training quantization (PTQ) of large language models (LLMs), which overlook layer-specific algorithmic suitability. The authors propose CKA-Guided Modular Quantization (CALM), a fine-tuning-free framework that uses Linear Centered Kernel Alignment (CKA) to evaluate and select the optimal PTQ method (e.g., GPTQ, AWQ, SmoothQuant) for each layer independently. CALM then integrates these layer-specific quantizations into a hybrid model. Experiments across Llama and Qwen models show CALM consistently outperforms uniform baselines and state-of-the-art mixed-precision methods in perplexity and downstream task accuracy, while introducing negligible inference overhead.

## Method Summary
CALM operates by first measuring layer-wise similarity between weight matrices using CKA, which captures functional alignment between layers. Based on these similarity scores, CALM assigns different PTQ algorithms (GPTQ, AWQ, SmoothQuant) to different layers, hypothesizing that layers with similar characteristics benefit from the same quantization strategy. The framework quantizes each layer with its assigned algorithm, then stitches the quantized layers together into a single hybrid model. This modular approach allows each layer to be optimized for its specific properties without requiring fine-tuning.

## Key Results
- Achieves 12.72 perplexity on Llama-3-8B compared to higher values for uniform baselines
- Improves HumanEval accuracy by +1.04% on Llama-3-8B over competing methods
- Introduces negligible inference overhead (<0.7%) while maintaining algorithmic diversity

## Why This Works (Mechanism)
The core insight is that different layers in LLMs have distinct statistical properties and sensitivities to quantization error. By using CKA to measure functional similarity between layers, CALM can group layers with similar characteristics and apply the most suitable quantization algorithm to each group. This algorithmic heterogeneity captures the nuanced requirements of different layers better than uniform bit-width mixing alone. The approach recognizes that quantization quality depends not just on bit-width but on matching the algorithm to the layer's specific structure and function.

## Foundational Learning
- **Centered Kernel Alignment (CKA):** A similarity metric for comparing neural network representations; needed to measure functional alignment between layers, quick check: compute CKA between two random weight matrices should yield ~0
- **Post-training quantization (PTQ):** Quantization without fine-tuning; needed as the baseline approach being improved, quick check: compare PTQ latency vs quantization-aware training
- **Mixed-precision quantization:** Assigning different bit-widths to different layers; needed context for CALM's innovation beyond bit-width, quick check: measure accuracy degradation with increasing bit-width heterogeneity
- **GPTQ, AWQ, SmoothQuant:** Specific PTQ algorithms with different strengths; needed as the algorithmic palette for CALM, quick check: apply each algorithm to the same layer and compare output distribution
- **Perplexity:** Language model evaluation metric measuring predictive uncertainty; needed as primary LLM quality metric, quick check: compute perplexity on held-out validation set
- **HumanEval:** Code generation benchmark; needed as downstream task evaluation, quick check: verify pass@k metric computation

## Architecture Onboarding

**Component map:** Input model weights -> CKA similarity computation -> Algorithm assignment -> Layer-wise quantization -> Model assembly

**Critical path:** CKA computation → Algorithm selection → Quantization → Assembly → Inference

**Design tradeoffs:** CALM trades increased design complexity (multiple algorithms, CKA computation) for improved quantization quality without fine-tuning. The modular approach requires careful handling of quantization boundaries but avoids the computational cost of fine-tuning.

**Failure signatures:** Poor CKA similarity measurements could lead to suboptimal algorithm assignments; incompatible quantization parameters between adjacent layers could cause numerical instability; excessive algorithmic diversity might increase deployment complexity.

**First experiments:** 1) Compute CKA similarity matrix for a sample LLM to visualize layer groupings, 2) Apply different PTQ algorithms to representative layers and measure output distribution shifts, 3) Quantize a single layer with multiple algorithms and measure perplexity impact

## Open Questions the Paper Calls Out
The paper's evaluation focuses on decoder-only models (Llama, Qwen), leaving open questions about applicability to encoder-decoder architectures or models with different attention patterns. Additionally, while the inference overhead is reported as negligible (<0.7%), the framework's compatibility with existing deployment optimizations (quantization-aware kernels, hardware-specific acceleration) is not discussed.

## Limitations
- The approach's reliance on CKA for algorithmic selection introduces uncertainties about theoretical justification
- Limited ablation studies on CKA's predictive power versus random selection or alternative metrics
- Evaluation restricted to decoder-only architectures, limiting architectural generalization claims

## Confidence
- **Claim:** Algorithmic diversity improves quantization — Medium confidence (strong empirical results but limited ablation studies)
- **Claim:** CKA is optimal metric for algorithm selection — Low confidence (alternatives not benchmarked)
- **Claim:** Negligible inference overhead — Medium confidence (overhead measured but deployment compatibility not explored)

## Next Checks
1. Test CALM's performance on encoder-decoder models (e.g., T5, BART) to verify architectural generalization.
2. Conduct controlled experiments isolating CKA's contribution by comparing against random algorithm selection or other similarity metrics.
3. Evaluate CALM's behavior under extreme compression ratios (e.g., 3-4 bits per weight) where quantization errors may dominate similarity signals.