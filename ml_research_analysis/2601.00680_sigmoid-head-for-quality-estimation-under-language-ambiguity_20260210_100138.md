---
ver: rpa2
title: Sigmoid Head for Quality Estimation under Language Ambiguity
arxiv_id: '2601.00680'
source_url: https://arxiv.org/abs/2601.00680
tags:
- head
- sigmoid
- quality
- softmax
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ambiguity-induced underconfidence
  in language model (LM) probability for quality estimation (QE). Standard LMs, trained
  with softmax activation and single-reference targets, spread probability mass across
  multiple valid outputs, misleadingly indicating low output quality.
---

# Sigmoid Head for Quality Estimation under Language Ambiguity

## Quick Facts
- arXiv ID: 2601.00680
- Source URL: https://arxiv.org/abs/2601.00680
- Reference count: 20
- Key outcome: Sigmoid Head improves quality estimation for ambiguous language by allowing multiple high-probability tokens, outperforming standard softmax and supervised QE in out-of-domain settings.

## Executive Summary
This paper addresses the problem of ambiguity-induced underconfidence in language model (LM) probability for quality estimation (QE). Standard LMs, trained with softmax activation and single-reference targets, spread probability mass across multiple valid outputs, misleadingly indicating low output quality. The authors propose the Sigmoid Head, an additional unembedding layer with sigmoid activation trained on top of frozen pretrained models. This design allows multiple tokens to receive high probabilities simultaneously, better reflecting output quality under ambiguity. To avoid penalizing potentially correct alternatives during training, the method uses a heuristic to exclude dominant tokens from negative sampling. Experiments across machine translation, paraphrasing, and question answering show that the Sigmoid Head provides notably better quality signals than the standard softmax head and common QE approaches like Monte Carlo Sequence Entropy and LLM Self Judge. The method requires no human-labeled quality data, making it more robust in out-of-domain settings, where it even outperforms supervised QE models.

## Method Summary
The Sigmoid Head is an additional unembedding layer with sigmoid activation, trained on top of frozen pretrained models. It allows multiple tokens to receive high probabilities simultaneously, addressing the issue of ambiguity-induced underconfidence in standard LM probability for quality estimation. The training process excludes dominant tokens from negative sampling to avoid penalizing potentially correct alternatives. The method is evaluated on machine translation, paraphrasing, and question answering tasks, showing superior performance compared to standard softmax heads and common QE approaches, especially in out-of-domain settings.

## Key Results
- Sigmoid Head improves quality estimation under linguistic ambiguity by allowing multiple high-probability tokens.
- No human-labeled quality data needed; method is more robust in out-of-domain settings.
- Outperforms supervised QE models in out-of-domain settings.

## Why This Works (Mechanism)
The Sigmoid Head works by replacing the standard softmax activation with sigmoid activation in an additional unembedding layer. This allows multiple tokens to receive high probabilities simultaneously, better reflecting the quality of ambiguous outputs. The method avoids penalizing potentially correct alternatives by excluding dominant tokens from negative sampling during training. This approach leverages the strengths of pretrained models while addressing their limitations in handling linguistic ambiguity.

## Foundational Learning
- **Language Model (LM) Probability**: Why needed: LM probabilities are crucial for quality estimation. Quick check: Understand how softmax and sigmoid activations affect probability distributions.
- **Ambiguity in Language**: Why needed: Ambiguity leads to underconfidence in standard LM probabilities. Quick check: Identify common sources of ambiguity in language tasks.
- **Quality Estimation (QE)**: Why needed: QE is the target application. Quick check: Know the difference between human-labeled and automated QE.
- **Pretrained Models**: Why needed: Sigmoid Head is trained on top of frozen pretrained models. Quick check: Understand the concept of model freezing and its implications.
- **Negative Sampling**: Why needed: Exclusion of dominant tokens from negative sampling is a key training heuristic. Quick check: Understand the role of negative sampling in training language models.

## Architecture Onboarding
- **Component Map**: Pretrained Model -> Sigmoid Head -> Quality Estimation Output
- **Critical Path**: Input text -> Pretrained Model (frozen) -> Sigmoid Head (trained) -> Quality Score
- **Design Tradeoffs**: Sigmoid Head trades off the ability to produce a single best token for better quality estimation under ambiguity. It requires no human-labeled data but relies on the quality of the pretrained model.
- **Failure Signatures**: If the heuristic for identifying dominant tokens is flawed, the model may over-penalize or under-penalize certain outputs. If the pretrained model is biased, the Sigmoid Head will inherit that bias.
- **3 First Experiments**:
  1. Compare Sigmoid Head and softmax head probabilities on ambiguous outputs in machine translation.
  2. Evaluate the impact of the negative sampling heuristic by training with and without it.
  3. Test the method on a new domain (e.g., summarization) to assess out-of-domain robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- The heuristic for identifying dominant tokens is not fully specified or validated.
- Reliance on single-reference targets in pretraining means inheriting any bias from that setup.
- Limited systematic cross-domain evaluation; current results are promising but not exhaustive.

## Confidence
- Sigmoid Head improves QE under ambiguity: **High**
- No human-labeled data needed: **High**
- Outperforms supervised QE in out-of-domain settings: **Medium** (limited domain testing)
- Negative sampling heuristic is effective: **Medium** (heuristic not fully validated)

## Next Checks
1. Conduct systematic cross-domain evaluation to verify out-of-domain robustness.
2. Compare against a broader set of recent QE models, including those not mentioned.
3. Perform ablation studies to isolate the impact of the negative sampling heuristic and validate its robustness.