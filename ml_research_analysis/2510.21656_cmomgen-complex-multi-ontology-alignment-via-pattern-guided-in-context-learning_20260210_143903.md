---
ver: rpa2
title: 'CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning'
arxiv_id: '2510.21656'
source_url: https://arxiv.org/abs/2510.21656
tags:
- target
- complex
- classes
- ontologies
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMOMgen, the first end-to-end method for
  complex multi-ontology matching (CMOM) that generates complete and semantically
  sound mappings without restrictions on target ontologies or entities. The method
  uses retrieval-augmented generation to select relevant classes and filter reference
  mappings, then employs in-context learning to produce OWL expressions.
---

# CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning

## Quick Facts
- arXiv ID: 2510.21656
- Source URL: https://arxiv.org/abs/2510.21656
- Reference count: 39
- Primary result: End-to-end method for complex multi-ontology matching achieving F1-scores of at least 63% on biomedical tasks

## Executive Summary
CMOMgen is the first end-to-end method for complex multi-ontology matching (CMOM) that generates complete and semantically sound mappings without restrictions on target ontologies or entities. The approach uses retrieval-augmented generation to select relevant classes and filter reference mappings, then employs in-context learning to produce OWL expressions. Evaluated on three biomedical tasks (Human Phenotype Ontology, Mammalian Phenotype Ontology, and Worm Phenotype Ontology), CMOMgen achieved F1-scores of at least 63%, outperforming baselines in two tasks and placing second in the third. Manual evaluation of non-reference mappings showed that 46% achieved maximum fidelity scores.

## Method Summary
CMOMgen addresses complex multi-ontology matching by combining retrieval-augmented generation with pattern-guided in-context learning. The method first selects relevant target classes using a hybrid approach combining recursive lexical coverage and embedding subtraction. It then filters reference mappings by namespace and cardinality to serve as structurally similar examples. Finally, it uses in-context learning with a frozen LLM to generate OWL expressions, translating sets of entity labels into nested logical structures. The approach avoids fine-tuning to reduce computational cost and increase adaptability while maintaining high semantic fidelity.

## Key Results
- Achieved F1-scores of at least 63% on three biomedical ontology tasks
- Outperformed baselines in two tasks and placed second in the third
- 46% of non-reference mappings achieved maximum fidelity scores in manual evaluation
- First method to generate complete, semantically sound mappings without target ontology restrictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hybrid candidate selection combining recursive lexical coverage and embedding subtraction likely improves the retrieval of relevant target entities compared to single-method approaches.
- **Mechanism**: The system aggregates a precision-oriented lexical strategy (finding non-overlapping labels that fully cover the source name) with a recall-oriented language model strategy (using vector subtraction to decompose source embeddings into target components). A greedy heuristic then merges these sets.
- **Core assumption**: Assumes that the semantic meaning of a complex source concept is compositional and can be approximated by the geometric addition or lexical union of simpler target entity labels.
- **Evidence anchors**:
  - [abstract] Mentions RAG "selects relevant classes" via "lexical and language model-based strategy."
  - [section 4.3] Describes the recursive function for lexical coverage and the geometric operations for LM-based selection.
  - [corpus] Corpus indicates general interest in "Fine-Tuning Large Language Models" for similar tasks, but this method relies on geometric operations in embedding space rather than training.
- **Break condition**: Fails if source concepts use idiomatic terminology where the sum of parts does not equal the whole (e.g., "heart attack" != "heart" + "attack" in a strict anatomical sense).

### Mechanism 2
- **Claim**: Filtering reference mappings by namespace and cardinality (the "pattern") to serve as in-context examples likely constrains the LLM to generate syntactically and semantically appropriate OWL expressions.
- **Mechanism**: Instead of using random examples, the system analyzes the selected target classes (their ontologies and counts) and queries the reference alignment for structurally similar mappings. These specific examples are inserted into the prompt to demonstrate the required logical structure (e.g., genus-differentia).
- **Core assumption**: Assumes that structural similarity in input entities predicts structural similarity in the required output logical expression.
- **Evidence anchors**:
  - [abstract] States the method uses "retrieval-augmented generation... to filter reference mappings to serve as examples."
  - [section 4.5] Details "Pattern Extraction" based on namespaces and cardinality.
  - [corpus] Weak direct evidence; corpus papers focus on RAG for code mapping or general alignment, not specifically pattern-filtered ICL for OWL.
- **Break condition**: Fails if the reference alignment lacks coverage for specific namespace combinations, forcing the system to use less relevant or no examples for novel entity combinations.

### Mechanism 3
- **Claim**: In-context learning allows a frozen LLM to act as a semantic reasoner, translating a set of entity labels into a nested OWL graph structure.
- **Mechanism**: The prompt provides the LLM with the source entity, the selected target entities (IRIs and labels), and the filtered OWL examples. The LLM predicts the token sequence for the `equivalentClass` statement, effectively performing structure prediction based on the provided context.
- **Core assumption**: Assumes the pre-trained LLM has sufficient internal representation of OWL syntax and biomedical logic to generalize from the provided examples to the new composition.
- **Evidence anchors**:
  - [section 6] Notes that "providing the examples achieves better results as they provide guidance regarding... structure."
  - [table 4] Shows "CMOMgen" (full prompt) significantly outperforms "CMOMgen w/o examples."
  - [corpus] "Large Language Models as Oracles for Ontology Alignment" supports the plausibility of using LLMs as reasoners in alignment tasks.
- **Break condition**: Fails if the LLM "hallucinates" invalid IRIs or creates syntactically valid but semantically inverted logic (e.g., swapping subject/object in a restriction), a phenomenon noted in the paper's error analysis.

## Foundational Learning

- **Concept**: **OWL Complex Class Expressions (Manchester Syntax)**
  - **Why needed here**: CMOMgen outputs `owl:equivalentClass` axioms involving intersections (`and`) and restrictions (`someValuesFrom`). Understanding how these nested expressions represent logic is required to debug the generated RDF/XML.
  - **Quick check question**: Can you distinguish between a simple subclass assertion and a defined class using an `equivalentClass` intersection?

- **Concept**: **Retrieval-Augmented Generation (RAG)**
  - **Why needed here**: The core architecture relies on retrieving specific "examples" from a reference alignment to ground the LLM's generation. Without understanding RAG, the distinction between "training" and "prompting with context" is lost.
  - **Quick check question**: How does filtering the retrieval context by "namespace cardinality" (the pattern) differ from standard semantic similarity search?

- **Concept**: **Graph Edit Distance (GED)**
  - **Why needed here**: Standard precision/recall is insufficient for complex mappings where structure (not just entity presence) matters. The paper introduces a custom GED metric to evaluate the "effort" required to transform a generated graph into a reference graph.
  - **Quick check question**: Why is penalizing a node substitution based on its "neighborhood size" (Equation 12) a better proxy for manual correction effort than a flat penalty?

## Architecture Onboarding

- **Component map**: Source Entity → **Class Selection** → **Pattern Extraction** → **LLM Generation**
- **Critical path**: Source Entity → **Class Selection** (determines candidate scope) → **Pattern Extraction** (determines prompt structure) → **LLM Generation**
- **Design tradeoffs**:
  - **Fixed vs. Open Vocabulary**: The method restricts properties to Relation Ontology (RO) and Basic Formal Ontology (BFO) to ensure consistency, potentially limiting expressiveness for domains using different upper ontologies.
  - **Prompting vs. Fine-tuning**: The authors explicitly avoid fine-tuning to reduce computational cost and increase adaptability, trading off potential peak performance for generalizability.
- **Failure signatures**:
  - **Swapped Arguments**: The LLM generates a valid structure but places entities in incorrect slots (e.g., "inheres in" target vs. qualifier swapped).
  - **IRI Hallucination**: The model generates a realistic-looking label but attaches it to a random or incorrect IRI from the vocabulary.
  - **Recursion Collapse**: In Class Selection, the embedding subtraction strategy stops too early (high α) or selects too many irrelevant classes (low α).
- **First 3 experiments**:
  1. **Baseline Validation**: Run the "LM Baseline" (simple prompt) vs. "CMOMgen" (full RAG pipeline) on 50 entities to verify the lift provided by the RAG components.
  2. **Ablation on Class Selection**: Run the pipeline with *only* lexical selection and *only* LM-based selection to identify which strategy contributes more to specific types of phenotypes (e.g., anatomical vs. behavioral).
  3. **Scaling Test**: Apply the pipeline to a source ontology *without* a reference alignment (using only internal definitions or a cross-domain ontology) to test the robustness of the "Pattern Extraction" module when examples are scarce or zero-shot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CMOMgen maintain high semantic fidelity when applied to domains outside of biomedical phenotype ontologies?
- Basis in paper: [explicit] The conclusion states that "its performance in other domains should also be explored since the present work focused solely on phenotype ontologies."
- Why unresolved: The current evaluation is restricted to phenotype ontologies (HP, MP, WBP), which share specific structural characteristics, such as the common `has_modifier` pattern.
- What evidence would resolve it: Evaluation of CMOMgen on complex matching benchmarks from disparate domains, such as geospatial (GeoLink) or industrial knowledge graphs.

### Open Question 2
- Question: Can the strategy be effectively adapted for complex pairwise alignment where the target is a single ontology rather than multiple?
- Basis in paper: [explicit] The authors note that "its use in other paradigms, such as complex pairwise alignment, must be investigated, as it can potentially stand as an adaptable strategy to multiple OM scenarios."
- Why unresolved: The method is optimized for selecting classes across multiple target ontologies; its efficiency and accuracy when restricted to a single complex target ontology remain unverified.
- What evidence would resolve it: A comparative study against existing complex pairwise matchers on benchmarks like the OAEI complex alignment track.

### Open Question 3
- Question: How can the computational and runtime costs be optimized to support large-scale ontology matching?
- Basis in paper: [explicit] The paper identifies that "The computational and runtime costs have yet to be addressed... which can turn users away from using these types of strategies in larger problems."
- Why unresolved: The reliance on large language models (LLMs) and recursive class selection creates performance bottlenecks not yet analyzed in terms of scalability.
- What evidence would resolve it: A complexity analysis measuring time and token usage relative to the size of the source and target ontologies, coupled with optimizations to reduce API calls.

## Limitations

- The specific embedding model used for LM-based class selection is not named, creating a dependency for exact reproduction
- Source and target ontology release versions/dates are unspecified, which could affect vocabulary extraction and reference alignment availability
- The method restricts properties to RO/BFO ontologies, potentially limiting expressiveness for domains using different upper ontologies

## Confidence

- **High**: The core architecture (RAG + pattern-filtered in-context learning) and its basic implementation details are well-specified
- **Medium**: The reported F1-scores and GED improvements are plausible given the methodology, but depend on the exact retrieval and generation components
- **Low**: The generalizability claim to "any target ontology or entity" is supported by the flexible RAG approach, but the restriction to RO/BFO properties for OWL expressions introduces a domain-specific limitation

## Next Checks

1. Run the "LM Baseline" vs. "CMOMgen" on 50 entities to verify the RAG component's contribution
2. Perform an ablation study on class selection (lexical-only vs. LM-only) to identify strategy effectiveness for different phenotype types
3. Test the pipeline on a source ontology without a reference alignment to assess pattern extraction robustness when examples are scarce