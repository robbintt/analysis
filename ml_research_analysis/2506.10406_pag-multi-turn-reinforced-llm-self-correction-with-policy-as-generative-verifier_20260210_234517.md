---
ver: rpa2
title: 'PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier'
arxiv_id: '2506.10406'
source_url: https://arxiv.org/abs/2506.10406
tags:
- verifier
- training
- arxiv
- policy
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PAG, a novel framework that enables large
  language models (LLMs) to self-correct by alternating between policy (generating
  solutions) and verifier (critically evaluating its own solutions) roles. Unlike
  prior approaches that always generate a second attempt regardless of confidence,
  PAG introduces a selective revision mechanism: the model revises its answer only
  when its own generative verification step detects an error.'
---

# PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier

## Quick Facts
- **arXiv ID**: 2506.10406
- **Source URL**: https://arxiv.org/abs/2506.10406
- **Reference count**: 40
- **Key outcome**: Introduces PAG, a selective revision mechanism that enables LLMs to self-correct by alternating between policy (generating solutions) and verifier (critically evaluating solutions) roles, substantially improving reasoning capabilities on mathematical benchmarks.

## Executive Summary
This paper introduces PAG, a novel framework that enables large language models (LLMs) to self-correct by alternating between policy (generating solutions) and verifier (critically evaluating its own solutions) roles. Unlike prior approaches that always generate a second attempt regardless of confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only mitigates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse mathematical reasoning benchmarks demonstrate that PAG substantially enhances LLM reasoning capabilities, improving both direct generation accuracy and self-correction performance.

## Method Summary
PAG implements a multi-turn RL framework where an LLM alternates between policy (solution generation) and verifier (self-evaluation) roles. The key innovation is selective revision: the model only generates a second attempt when its generative verifier explicitly detects an error. Training uses turn-independent optimization with decoupled rewards per turn, preventing verifier collapse. RoleAdvNorm normalizes advantages separately for policy and verifier roles. The method is trained on 7.5K MATH samples with ground-truth verification, then evaluated on MATH500, MinervaMATH, and AIME benchmarks without external supervision.

## Key Results
- Achieves state-of-the-art self-correction performance, with improvements of over 50 points in Verifier Accuracy on MATH500
- Outperforms self-consistency approaches in best-of-N sampling
- Demonstrates superior answer change ratio compared to direct multi-turn approaches, preventing model collapse
- Shows consistent gains across different model sizes (1.5B-8B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Selective Revision Gate
The model only generates a second attempt when its generative verifier explicitly outputs "The answer is wrong." This prevents model collapse where second attempts merely paraphrase first attempts, and ensures effort is focused on meaningful self-correction.

### Mechanism 2: Turn-Independent Optimization with Decoupled Rewards
Each turn (policy or verifier) receives its own ground-truth reward without propagating advantages across turns. This prevents the verifier from being rewarded for triggering additional policy turns regardless of judgment accuracy.

### Mechanism 3: Role-Specific Advantage Normalization (RoleAdvNorm)
Normalizing advantages separately for policy and verifier roles stabilizes training when combining reward shaping with multi-role objectives, preventing verifier advantages from diluting policy gradient signals.

## Foundational Learning

- **Multi-turn Reinforcement Learning (RL) for LLMs**: PAG extends single-turn PPO to multi-turn trajectories by concatenating turn outputs. *Why needed*: Understanding how rewards propagate across turns is essential for the turn-independent optimization design. *Quick check*: If you set turn discount γ=1 in PAG, what verifier behavior emerges and why?

- **Model Collapse in Self-Correction**: The primary motivation is avoiding collapse where second attempts merely paraphrase first attempts. *Why needed*: Knowing why this happens (RL objective doesn't incentivize substantive change) clarifies why selective revision helps. *Quick check*: In Direct MultiTurn, why does the answer change ratio decrease during training?

- **Generative vs. Discriminative Verifiers**: PAG uses a generative verifier (next-token prediction to output "correct"/"wrong" with reasoning), not a classifier. *Why needed*: This enables chain-of-thought verification but requires different training. *Quick check*: What is the output space of a generative verifier versus an outcome-based reward model?

## Architecture Onboarding

- **Component map**: Problem input -> Policy role (generate solution) -> Verifier role (evaluate solution) -> Selective revision gate (if wrong, loop back) -> Ground-truth verifier (training rewards) -> PPO optimizer with RoleAdvNorm

- **Critical path**: 1) Load instruction-tuned base model. 2) Format prompts for policy and verifier turns. 3) Generate trajectory up to Tmax; collect rewards per turn. 4) Compute advantages separately for policy and verifier tokens. 5) Apply PPO update with turn-independent gradients. 6) Evaluate on held-out benchmarks with self-verification.

- **Design tradeoffs**: Tmax selection (2-4 turns optimal), reward shaping coefficient α=1, external verifier dependency limiting real-world applicability.

- **Failure signatures**: Verifier collapse to always-correct/always-wrong, policy stalling with low answer change ratio, REINFORCE instability.

- **First 3 experiments**: 1) Reproduce selective revision ablation on MATH subset; 2) Validate turn-independent optimization on verifier metrics; 3) Test generalization to code generation tasks.

## Open Questions the Paper Calls Out

- Can PAG be extended to tasks without external ground-truth verifiers for training?
- Does PAG scale effectively to larger models (32B/70B parameters) and integrate with long chain-of-thought reasoning?
- Can alternative RL algorithms (GRPO, DAPO, VAPO) improve PAG's stability or performance compared to PPO?
- What are more effective approaches for scaling the number of training turns beyond 2-4?

## Limitations
- Requires external ground-truth verifiers during training, limiting applicability to tasks where correctness can be programmatically verified
- Selective revision assumes verifier accuracy is sufficiently reliable to serve as a gate
- RoleAdvNorm contribution lacks extensive external validation

## Confidence
- **High Confidence**: Selective revision mechanism effectiveness (supported by direct ablation showing collapse prevention)
- **Medium Confidence**: Turn-independent optimization preventing verifier collapse (strong internal evidence but limited external validation)
- **Medium Confidence**: RoleAdvNorm contribution (marginal improvements shown, but no comparative validation)

## Next Checks
1. Test PAG on code generation tasks (e.g., HumanEval) where execution-based ground truth is available, to verify if self-correction gains transfer beyond mathematical reasoning.
2. Systematically vary verifier accuracy (via controlled noise injection) to determine the minimum threshold where selective revision remains beneficial versus counterproductive.
3. Compare PAG's binary verifier gate against continuous confidence thresholds or majority-vote ensemble verification to assess robustness of the selective revision mechanism.