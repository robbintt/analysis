---
ver: rpa2
title: 'TongSearch-QR: Reinforced Query Reasoning for Retrieval'
arxiv_id: '2506.11603'
source_url: https://arxiv.org/abs/2506.11603
tags:
- reasoning
- query
- retrieval
- https
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TongSearch-QR, a family of small-scale language
  models (7B and 1.5B parameters) specifically trained for query reasoning and rewriting
  in reasoning-intensive retrieval tasks. Traditional IR methods struggle with complex
  queries requiring multi-hop inference or deep semantic understanding.
---

# TongSearch-QR: Reinforced Query Reasoning for Retrieval

## Quick Facts
- arXiv ID: 2506.11603
- Source URL: https://arxiv.org/abs/2506.11603
- Reference count: 18
- Key outcome: TongSearch-QR-7B achieves 27.9 nDCG@10 on BRIGHT benchmark, outperforming GPT-4o's 26.5

## Executive Summary
TongSearch-QR introduces a family of small-scale language models (7B and 1.5B parameters) specifically trained for query reasoning and rewriting in reasoning-intensive retrieval tasks. Traditional information retrieval methods struggle with complex queries requiring multi-hop inference or deep semantic understanding. TongSearch-QR addresses this by rewriting queries to include reasoning-relevant content before retrieval, using reinforcement learning with a novel semi-rule-based reward function that evaluates relevance improvement between original and reasoned queries.

The models are trained using automatically curated data from Stack Exchange preferences, eliminating the need for large-scale annotated datasets. Experiments on the BRIGHT benchmark demonstrate that TongSearch-QR-7B achieves superior performance compared to larger models like GPT-4o, while also showing strong adaptability by working effectively with both traditional and reasoning-intensive retrievers.

## Method Summary
TongSearch-QR employs reinforcement learning with a semi-rule-based reward function to train small language models (7B and 1.5B parameters) for query reasoning and rewriting. The training data is automatically curated from Stack Exchange preferences, avoiding the need for large-scale annotated datasets. The reward function evaluates the relevance improvement between original and reasoned queries, guiding the model to generate queries that better capture reasoning-intensive retrieval needs. The approach demonstrates that smaller, specialized models can outperform larger general-purpose models on reasoning-intensive retrieval tasks.

## Key Results
- TongSearch-QR-7B achieves 27.9 nDCG@10 on BRIGHT benchmark
- Outperforms GPT-4o (26.5 nDCG@10) despite being significantly smaller
- Demonstrates effective performance with both traditional and reasoning-intensive retrievers
- Shows adaptability across different retrieval scenarios without requiring task-specific fine-tuning

## Why This Works (Mechanism)
The paper doesn't explicitly detail the mechanism behind why TongSearch-QR works, but the approach leverages reinforcement learning with a novel semi-rule-based reward function that specifically targets reasoning-relevant content in query rewriting. This targeted approach allows the model to generate queries that better capture the semantic and inferential requirements of complex retrieval tasks.

## Foundational Learning
- Reinforcement Learning: Needed for training models to optimize query rewriting based on performance feedback; quick check: verify reward function properly captures relevance improvement
- Semi-rule-based Reward Functions: Required to evaluate the quality of rewritten queries without extensive human annotation; quick check: test reward function consistency across different query types
- Query Rewriting for Retrieval: Essential for improving search results on complex queries; quick check: measure improvement in retrieval accuracy with rewritten vs original queries

## Architecture Onboarding
- Component Map: Stack Exchange Data -> Query Rewriting Model -> Retrieval System -> Performance Evaluation -> Reward Function Feedback
- Critical Path: Query Input → TongSearch-QR Model → Rewritten Query → Retriever → Results → Reward Evaluation
- Design Tradeoffs: Small model size (7B/1.5B) vs performance, automatic data curation vs annotation quality, semi-rule-based rewards vs human evaluation
- Failure Signatures: Poor query rewriting leading to irrelevant results, reward function bias affecting learning, overfitting to Stack Exchange domain
- First Experiments: 1) Test on BRIGHT benchmark with different retrievers, 2) Compare performance with baseline models on reasoning-intensive queries, 3) Evaluate model adaptability across different retrieval scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies solely on the BRIGHT benchmark, potentially limiting generalizability to real-world scenarios
- Performance gains are modest (27.9 vs 26.5 nDCG@10), raising questions about practical significance
- Semi-rule-based reward function lacks detailed description of components and potential biases
- Limited experimental combinations for claims about effectiveness with different retriever types

## Confidence
- Overall effectiveness of TongSearch-QR models: Medium
- Generalization of semi-rule-based reward function across domains: Low
- Automatic data curation quality from Stack Exchange: Medium

## Next Checks
1. Test TongSearch-QR on additional reasoning-intensive retrieval benchmarks beyond BRIGHT to verify generalizability
2. Conduct ablation studies to isolate the contribution of the semi-rule-based reward function versus other components
3. Evaluate the models on real-world query logs from production search systems to assess practical utility