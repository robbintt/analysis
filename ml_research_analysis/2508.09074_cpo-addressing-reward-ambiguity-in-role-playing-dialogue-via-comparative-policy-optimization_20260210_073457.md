---
ver: rpa2
title: 'CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative
  Policy Optimization'
arxiv_id: '2508.09074'
source_url: https://arxiv.org/abs/2508.09074
tags:
- evaluation
- dialogue
- role-playing
- character
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Comparative Policy Optimization (CPO), a reinforcement
  learning method for role-playing dialogue that addresses reward ambiguity by replacing
  sample-wise scoring with group-wise comparative scoring. The approach mimics human
  evaluation by comparing responses within groups to establish relative quality, reducing
  scoring instability and improving human agreement by approximately 20%.
---

# CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization

## Quick Facts
- **arXiv ID:** 2508.09074
- **Source URL:** https://arxiv.org/abs/2508.09074
- **Reference count:** 40
- **Primary result:** Group-wise comparative scoring improves human agreement by ~20% over sample-wise RLHF in role-playing dialogue.

## Executive Summary
This paper addresses reward ambiguity in role-playing dialogue by replacing sample-wise scoring with group-wise comparative scoring. The Comparative Policy Optimization (CPO) method uses a judge LLM to evaluate groups of responses jointly, establishing relative quality benchmarks rather than absolute scores. This approach mimics human evaluation practices and reduces scoring instability caused by prompt sensitivity and normalization artifacts in advantage estimation. Experiments demonstrate CPO outperforms existing RLHF methods across multiple benchmarks with win rates of 50-75% and significant improvements in conversational ability, character consistency, and role-playing attractiveness.

## Method Summary
CPO modifies GRPO by replacing sample-wise reward scoring with group-wise comparative evaluation. For each query, the policy generates G=16 responses that are jointly evaluated by an LLM judge with evaluation criteria. The judge assigns relative scores to all responses in the group, which are then used to compute advantages via group normalization. A soft overlength penalty prevents reward hacking through verbosity. The method uses LoRA fine-tuning followed by GRPO-style optimization with KL constraints and clipped surrogate objectives.

## Key Results
- CPO achieves 50-75% win rates in model comparisons on CharacterArena
- Human agreement improves by approximately 20% compared to sample-wise RLHF
- The method generalizes to other RLFT paradigms like RFT and DPO
- Significant improvements in conversational ability, character consistency, and role-playing attractiveness across multiple LLM backbones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Group-wise comparative scoring produces more stable reward signals than sample-wise independent scoring for subjective tasks.
- **Mechanism**: The reward model evaluates all responses in a group jointly: {r_i}_{i=1}^G = RM({o_i}_{i=1}^G | I). This mimics human evaluators who naturally compare alternatives rather than score in isolation.
- **Core assumption**: LLM judges perform better at comparative ranking tasks than absolute scoring tasks, and systematic evaluation biases cancel when comparing within the same prompt context.
- **Evidence anchors**:
  - [abstract]: "CPO redefines the reward evaluation paradigm by shifting from sample-wise scoring to comparative group-wise scoring."
  - [section 4.3]: "Human evaluators analyze samples based on the evaluation criterion and provide discriminative scores through comparison between samples."
  - [corpus]: Related work "Act-Adaptive Margin" addresses similar subjective ambiguity in reward modeling.

### Mechanism 2
- **Claim**: Comparative scoring mitigates error amplification in GRPO's advantage normalization step.
- **Mechanism**: GRPO computes advantages via normalization: Â_{i,t} = (r_i - mean({r_i}))/std({r_i}). Group-wise scoring produces more discriminative score distributions, yielding more stable advantage estimates.
- **Core assumption**: A significant portion of scoring variance is systematic rather than random; shared-context evaluation cancels these systematic components.
- **Evidence anchors**:
  - [section 4.2]: "Due to the normalization in advantage estimation, small scoring noise can lead to disproportionately large errors in relative rankings."
  - [Figure 1]: Visual demonstration of noise amplification in sample-wise vs. group-wise scoring.
  - [corpus]: Limited direct evidence on normalization effects specifically.

### Mechanism 3
- **Claim**: Soft overlength penalty prevents reward hacking through verbosity exploitation.
- **Mechanism**: LLM-as-judge evaluation often favors longer responses. The penalty function applies graduated penalties as length approaches L_max.
- **Core assumption**: There exists an optimal response length range for role-playing engagement; both under and over-length responses degrade user experience.
- **Evidence anchors**:
  - [section 4.3]: "To address reward hacking due to length bias, we incorporate a soft overlength penalty."
  - [section B.2.1, Figure 10]: Empirical demonstration that without length control, average response length rapidly reaches the upper limit.
  - [corpus]: No direct corpus evidence for this specific penalty formulation in role-playing contexts.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: CPO builds directly on GRPO's objective and advantage estimation.
  - **Quick check question**: Can you explain why GRPO uses group-level normalization rather than absolute reward values?

- **Concept: Reward Model Ambiguity in Subjective Domains**
  - **Why needed here**: The paper's core motivation.
  - **Quick check question**: Why does the paper claim sample-wise scoring suffers from "error amplification" in advantage estimation?

- **Concept: Length Bias in LLM-as-Judge Evaluation**
  - **Why needed here**: The paper identifies verbosity as a reward hacking vector.
  - **Quick check question**: What would happen to training dynamics if the length penalty were removed?

## Architecture Onboarding

- **Component map**: Policy Model -> Response Generator -> LLM Judge -> Advantage Calculator -> Policy Optimizer
- **Critical path**:
  1. Sample G responses from π_θ_old with temperature=1.0, top-p=1.0
  2. Present all responses jointly to reward model with evaluation criteria
  3. Apply length penalty: r_final^i = clip(r_i + r_length(o_i), 0, 1)
  4. Compute advantages via group normalization
  5. Update policy using clipped surrogate objective with KL constraint
- **Design tradeoffs**:
  - **Group size (G)**: Larger groups provide more stable rankings but increase reward model computation.
  - **Evaluation criteria specificity**: More detailed criteria improve alignment but may constrain creativity.
  - **Length threshold calibration**: Too aggressive penalization truncates natural responses; too lenient allows verbosity gaming.
  - **Reward model choice**: Larger models provide better discrimination but higher latency.
- **Failure signatures**:
  - **Collapsed score distributions**: All responses receiving similar scores indicates reward model failure to discriminate.
  - **Length explosion/contraction**: Response lengths systematically drifting to extremes suggests penalty miscalibration.
  - **Position bias**: If group-wise evaluation shows order effects, shuffle responses before evaluation.
  - **Reward hacking**: Repetitive patterns, excessive politeness, or other artifacts that inflate scores without improving quality.
- **First 3 experiments**:
  1. **Ablation on group size**: Compare G=4, 8, 16, 32 on correlation with human judgment.
  2. **Sample-wise vs. group-wise scoring head-to-head**: Same reward model, same data, different evaluation protocol.
  3. **Length penalty sensitivity**: Train with penalty thresholds at [64, 128, 256] and without penalty.

## Open Questions the Paper Calls Out

- **Can CPO be extended to optimize multi-turn dialogue strategies rather than single-turn responses?**
  - **Basis**: [explicit] The authors state in Limitations: "our current optimization approach primarily targets single-turn dialogue modeling."
  - **Why unresolved**: Current implementation calculates advantages based on groups of responses to static history, lacking mechanism for long-horizon rewards.
  - **What evidence would resolve it**: Study applying CPO within framework utilizing trajectory-level value functions or hierarchical policies.

- **Does the group-wise comparative reward paradigm generalize effectively to other open-ended creative tasks?**
  - **Basis**: [explicit] Authors note in Limitations: "we intend to conduct more comprehensive evaluations across broader spectrum of open-ended tasks."
  - **Why unresolved**: Unclear if comparative mechanism is equally robust for tasks where evaluation criteria are purely subjective or stylistic.
  - **What evidence would resolve it**: Experimental results applying CPO to standard creative writing benchmarks.

- **How sensitive is CPO performance to the size of the response group G used for comparative scoring?**
  - **Basis**: [inferred] Implementation fixes sample size N=16, but paper doesn't ablate this hyperparameter.
  - **Why unresolved**: Smaller groups may fail to distinguish fine-grained quality differences, while larger groups increase inference cost.
  - **What evidence would resolve it**: Ablation study varying G (4, 8, 16, 32) measuring correlation with human judgment and computational overhead.

## Limitations
- The claim of 20% improvement in human agreement lacks detailed statistical analysis regarding variance and significance thresholds.
- The soft length penalty mechanism relies on fixed thresholds that may not generalize across diverse character archetypes and dialogue styles.
- The method's dependence on LLM judges introduces potential biases that may not fully cancel in comparative settings.

## Confidence

- **High Confidence**: The core architectural framework (CPO replacing sample-wise with group-wise scoring) and its implementation details are well-specified and reproducible.
- **Medium Confidence**: The claim that comparative scoring improves human agreement by ~20% relative to baselines is supported by experiments but lacks comprehensive statistical validation.
- **Low Confidence**: The assertion that CPO eliminates fundamental reward ambiguity rather than merely mitigating it remains unproven.

## Next Checks

1. **Statistical Robustness Analysis**: Conduct significance testing across all human evaluation metrics with confidence intervals and p-values to validate the claimed 20% improvement in human agreement.

2. **Bias Characterization Study**: Systematically evaluate whether group-wise scoring introduces new systematic biases, particularly examining whether certain character archetypes or response styles are systematically advantaged or disadvantaged.

3. **Generalization Stress Test**: Apply CPO to dialogue domains outside the role-playing context (e.g., task-oriented dialogue, creative writing, debate) with varying group sizes and penalty configurations.