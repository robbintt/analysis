---
ver: rpa2
title: What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal,
  Caption-Based, and Hybrid Retrieval Techniques
arxiv_id: '2509.15211'
source_url: https://arxiv.org/abs/2509.15211
tags:
- retrieval
- slide
- visual
- textual
- colpali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates methods for retrieving slides from large multimodal
  repositories, focusing on balancing accuracy, speed, and storage in real-world RAG
  systems. It compares direct visual retrieval (DSE, ColPali), caption-based retrieval
  (BM25, neural embeddings, Textual ColPali), and hybrid approaches.
---

# What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques

## Quick Facts
- arXiv ID: 2509.15211
- Source URL: https://arxiv.org/abs/2509.15211
- Reference count: 40
- Primary result: VLM-generated captions enable effective text-based retrieval, often outperforming OCR-based baselines and fine-tuned CLIP, with hybrid methods achieving top NDCG@10 scores

## Executive Summary
This study evaluates methods for retrieving slides from large multimodal repositories, focusing on balancing accuracy, speed, and storage in real-world RAG systems. It compares direct visual retrieval (DSE, ColPali), caption-based retrieval (BM25, neural embeddings, Textual ColPali), and hybrid approaches. The research demonstrates that caption-based retrieval with mature text IR techniques provides a practical balance for industry applications, while hybrid methods achieve top NDCG@10 scores but at higher computational costs.

## Method Summary
The study systematically evaluates retrieval techniques across three categories: direct visual retrieval using DSE and ColPali models, caption-based retrieval using BM25, neural embeddings, and Textual ColPali on VLM-generated captions, and hybrid approaches combining multiple techniques. The evaluation framework tests these methods on SlideVQA and SlideSum datasets, measuring NDCG@10 scores, latency, and storage requirements. VLM-generated captions serve as a bridge between visual and textual retrieval paradigms, enabling the application of established text IR techniques to slide content.

## Key Results
- VLM-generated captions enable effective text-based retrieval, often outperforming OCR-based baselines and fine-tuned CLIP
- ColPali with visual reranking yields the highest accuracy (86.9% NDCG@10) but at high latency and storage cost
- Textual ColPali on captions offers a competitive, more storage-efficient alternative
- Hybrid methods achieve top NDCG@10 scores (e.g., 83.9% with BM25+Neural+BGE reranker on SlideVQA)

## Why This Works (Mechanism)
The effectiveness of caption-based retrieval stems from the ability of VLMs to generate semantically rich, contextually relevant captions that capture the essence of slide content better than OCR alone. These captions preserve the multimodal nature of slides while enabling the application of mature text retrieval techniques. The hybrid approaches work by leveraging complementary strengths: visual features capture layout and graphical elements, while textual embeddings capture semantic meaning and relationships.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Neural models that understand both visual and textual information, needed for generating semantically rich slide captions
- **ColPali**: A patch-based retrieval method that directly compares visual features, needed for accurate visual similarity matching
- **BM25**: A probabilistic information retrieval function for ranking documents, needed for effective keyword-based slide retrieval
- **Neural Embeddings**: Dense vector representations of text, needed for capturing semantic relationships between slide content
- **Hybrid Retrieval**: Combining multiple retrieval approaches, needed to balance accuracy and efficiency
- **NDCG@10**: Normalized Discounted Cumulative Gain at cutoff 10, needed for measuring retrieval quality

## Architecture Onboarding

**Component Map**: VLM Caption Generator -> Text Index -> Retrieval Engine -> Re-ranker -> Results

**Critical Path**: Slide ingestion → VLM caption generation → Text indexing → Query processing → Re-ranking → Result delivery

**Design Tradeoffs**: Accuracy vs. latency (visual methods are more accurate but slower), storage vs. retrieval quality (dense embeddings require more storage but improve results), computational cost vs. hybrid method effectiveness

**Failure Signatures**: Poor caption quality leads to irrelevant results, OCR errors cause retrieval failures, visual-only methods miss semantic content, hybrid methods may suffer from computational bottlenecks

**First Experiments**:
1. Baseline evaluation using OCR-based BM25 retrieval
2. VLM caption generation quality assessment on sample slides
3. ColPali visual retrieval accuracy measurement

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on relatively small, specialized slide datasets that may not represent real-world enterprise repositories
- VLM-generated captions introduce quality dependencies on underlying vision-language models' training data
- Hybrid methods' performance gains come with increased computational complexity and potential latency trade-offs
- Focus on academic and professional presentation slides may limit generalizability to other slide formats

## Confidence

**High Confidence**: Claims regarding the effectiveness of caption-based retrieval methods compared to OCR baselines and the overall performance ranking of retrieval approaches are well-supported by the experimental results.

**Medium Confidence**: Assertions about the practical balance of caption-based methods for industry applications require validation across broader, more diverse slide collections and real-world usage patterns.

**Medium Confidence**: The storage and latency trade-offs presented are based on specific experimental conditions and may vary with different hardware configurations and scale requirements.

## Next Checks
1. Evaluate the proposed retrieval methods on a larger, more diverse slide corpus representing real-world enterprise slide repositories, including slides with varying layouts, languages, and content types.

2. Conduct a longitudinal study assessing the performance and cost implications of the hybrid methods under different load conditions and scaling scenarios.

3. Perform ablation studies on the VLM-generated captions to quantify the impact of caption quality on retrieval performance across different vision-language model configurations.