---
ver: rpa2
title: Poisoning Bayesian Inference via Data Deletion and Replication
arxiv_id: '2503.04480'
source_url: https://arxiv.org/abs/2503.04480
tags:
- posterior
- data
- attack
- bayesian
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for poisoning attacks on generic
  Bayesian inference by strategically deleting and replicating true observations to
  steer the posterior toward a target distribution. The authors formulate the attacker's
  problem as a stochastic integer program with an intractable objective, then develop
  gradient-based heuristics including SGD-based rounded relaxation and integer-steps
  coordinate descent variants.
---

# Poisoning Bayesian Inference via Data Deletion and Replication

## Quick Facts
- **arXiv ID:** 2503.04480
- **Source URL:** https://arxiv.org/abs/2503.04480
- **Reference count:** 40
- **Primary result:** Strategic data deletion and replication can poison Bayesian inference with minimal data manipulation

## Executive Summary
This paper introduces a framework for poisoning attacks on Bayesian inference by strategically deleting and replicating true observations to steer posterior distributions toward target distributions. The authors formulate the attacker's problem as a stochastic integer program and develop gradient-based heuristics including SGD-based rounded relaxation and coordinate descent variants. Empirical results demonstrate high effectiveness: manipulating just 0.12% of microcredit data reversed the inferred treatment effect, while ~7% manipulation in Boston housing shifted a key coefficient's posterior mean from 3.2 to 0.8. The attacks show transferability across different priors, with more informative priors offering greater robustness.

## Method Summary
The paper formulates Bayesian poisoning as a stochastic integer program where the attacker selects which observations to delete and replicate to minimize the distance between the resulting posterior and a target distribution. The intractable objective is approximated using gradient-based heuristics: an SGD-based rounded relaxation approach and an integer-steps coordinate descent method. The authors prove convexity of the objective under certain conditions and establish convergence properties for their algorithms. The framework is evaluated on synthetic data and three real datasets (Boston housing, Mexico microcredit RCT, spam classification), demonstrating the attacks' effectiveness while minimally disturbing other inferences.

## Key Results
- Manipulating only 0.12% of microcredit data reversed the inferred treatment effect
- In Boston housing, ~7% data manipulation shifted a key coefficient's posterior mean from 3.2 to 0.8
- Attacks demonstrate transferability across different priors, with more informative priors offering greater robustness
- High surgical precision in corrupting Bayesian inference while minimally disturbing other inferences

## Why This Works (Mechanism)
The poisoning attacks work by exploiting the sensitivity of Bayesian posterior distributions to data composition. By strategically removing observations that support the true inference and replicating those that push the posterior toward the target, the attacker can systematically bias the learned beliefs. The gradient-based optimization methods efficiently navigate the discrete space of possible deletion/replication strategies by approximating the intractable posterior distance objective. The convexity of the objective under certain conditions enables convergence guarantees, while the rounding heuristics provide practical solutions to the discrete optimization problem.

## Foundational Learning
**Bayesian Inference:** Understanding how prior beliefs update with observed data to form posterior distributions - needed to grasp how data manipulation affects inference
*Quick check:* Verify understanding of Bayes' rule and posterior computation

**Stochastic Integer Programming:** Optimization framework for problems with discrete decisions under uncertainty - needed to formulate the poisoning problem
*Quick check:* Confirm ability to model discrete data selection decisions with probabilistic constraints

**Gradient-Based Optimization:** Methods for finding optimal solutions using gradient information - needed to develop efficient attack algorithms
*Quick check:* Ensure familiarity with SGD and coordinate descent optimization techniques

**Convexity Analysis:** Mathematical property ensuring global optima can be found efficiently - needed to establish convergence guarantees
*Quick check:* Verify ability to identify and prove convexity of the objective function

## Architecture Onboarding
**Component Map:** Data observations -> Deletion/Replication selection -> Posterior computation -> Distance to target -> Optimization algorithm -> Attack strategy
**Critical Path:** The attack strategy selection (deletion/replication decisions) directly determines the poisoned posterior, which determines attack success
**Design Tradeoffs:** Exact optimization guarantees vs. computational efficiency; white-box access assumption vs. realistic threat models; attack precision vs. detectability
**Failure Signatures:** Failed attacks show minimal posterior shift; numerical instability in gradient computation; poor convergence of optimization algorithms
**First Experiments:** 1) Test attack on simple conjugate Bayesian model with synthetic data, 2) Evaluate sensitivity to prior specification, 3) Benchmark computational cost against exact solvers for small problems

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees rely on convexity assumptions that may not hold in all practical Bayesian models
- Gradient-based heuristics lack strong optimality guarantees, especially in high-dimensional settings
- White-box access assumption is a strong threat model not always realistic in practice
- Computational cost of exact posterior evaluation may be prohibitive for very large datasets

## Confidence
- Effectiveness of poisoning attacks: **High** (strong empirical support)
- Theoretical convergence properties: **Medium** (depends on convexity assumptions)
- Prior robustness claims: **Medium** (limited prior variation testing)
- Generalizability to complex models: **Low** (limited to relatively simple Bayesian models)

## Next Checks
1. Test attack effectiveness on non-conjugate priors and hierarchical Bayesian models to assess practical limitations
2. Evaluate attack performance under black-box threat models where posterior gradients are unavailable
3. Benchmark attack computational efficiency against exact mixed-integer programming solvers for small-scale problems