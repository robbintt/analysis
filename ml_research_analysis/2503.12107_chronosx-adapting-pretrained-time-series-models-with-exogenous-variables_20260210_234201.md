---
ver: rpa2
title: 'ChronosX: Adapting Pretrained Time Series Models with Exogenous Variables'
arxiv_id: '2503.12107'
source_url: https://arxiv.org/abs/2503.12107
tags:
- chronosx
- pretrained
- mult
- covariates
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents ChronosX, a modular approach to incorporate\
  \ exogenous variables into pretrained time series forecasting models. The method\
  \ introduces two lightweight adapter blocks\u2014one for past covariates and one\
  \ for future covariates\u2014that update token embeddings and adjust output distributions\
  \ without modifying the pretrained model."
---

# ChronosX: Adapting Pretrained Time Series Models with Exogenous Variables

## Quick Facts
- **arXiv ID:** 2503.12107
- **Source URL:** https://arxiv.org/abs/2503.12107
- **Reference count:** 40
- **Primary result:** ChronosX consistently outperforms its pretrained counterparts and achieves competitive results compared to state-of-the-art baselines.

## Executive Summary
This paper presents ChronosX, a modular approach to incorporate exogenous variables into pretrained time series forecasting models. The method introduces two lightweight adapter blocks—one for past covariates and one for future covariates—that update token embeddings and adjust output distributions without modifying the pretrained model. The approach is evaluated on a novel benchmark of 32 synthetic datasets with varying dynamics and 18 real-world datasets with covariates. ChronosX consistently outperforms its pretrained counterparts and achieves competitive results compared to state-of-the-art baselines. Full fine-tuning further improves performance. The same framework is extended to other pretrained models like TimesFM and MOMENT, resulting in similar gains.

## Method Summary
ChronosX introduces two modular adapter blocks that enable pretrained time series models to leverage exogenous variables. The Input Injection Block (IIB) processes past covariates by projecting them alongside token embeddings through linear layers and an FFN, then adding the result as a residual to update the embeddings before they enter the frozen backbone. The Output Injection Block (OIB) processes future covariates by projecting them alongside the model's final hidden state, then adding the result to the logits before the softmax, adjusting the predicted distribution. Both blocks use separate linear projections for embeddings/hidden-states and covariates, followed by a 2-layer FFN with ReLU activation. The adapters are trained while the backbone remains frozen, though full fine-tuning is also evaluated.

## Key Results
- ChronosX outperforms its pretrained counterparts across 32 synthetic and 18 real-world datasets with varying dynamics
- The method achieves competitive results compared to state-of-the-art baselines like TimesFM and MOMENT
- Full fine-tuning of ChronosX yields additional performance gains over adapter-only training
- The approach generalizes to other pretrained models including TimesFM and MOMENT, demonstrating consistent improvements

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Space Injection for Past Covariates
The Input Injection Block updates token embeddings with past covariate information, allowing a frozen pretrained model to incorporate historical external context. The IIB takes the pretrained token embedding and past covariate, projects them independently via linear layers, concatenates them, passes them through an FFN, and adds the result as a residual to the original token embedding. This conditions the encoder on past external factors.

### Mechanism 2: Logit-Space Adjustment for Future Covariates
The Output Injection Block modifies the output logits to adjust the final probabilistic forecast based on known future covariates. The OIB takes the final hidden state and future covariates, processes them via an FFN, and adds the result to the logits produced by the pretrained model's output matrix. This shifts the categorical distribution used for prediction.

### Mechanism 3: Modular Separation of Temporal Context
Decoupling past and future covariates into distinct modules (IIB vs. OIB) allows the architecture to mimic the natural flow of information—past data conditions the context while future data conditions the prediction horizon. The IIB operates at the input level, potentially influencing self-attention mechanisms, while the OIB operates at the output level as a bias correction mechanism.

## Foundational Learning

- **Concept: Tokenization of Time Series**
  - Why needed: Chronos converts continuous time series values into discrete tokens. The IIB modifies these token embeddings, so understanding the discrete vocabulary representation is essential.
  - Quick check: Does the model process raw floating-point numbers or indices into a learned vocabulary?

- **Concept: Probabilistic Forecasting via Categorical Distribution**
  - Why needed: The OIB adjusts "logits," so you must understand that the model predicts a probability distribution over possible token values rather than a single point estimate.
  - Quick check: What is the output layer of the Chronos model—a regression head or a classifier over a vocabulary?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / Adapters**
  - Why needed: The core contribution freezes the pretrained model and trains only the injection blocks, relying on the adapter paradigm where small modules bridge the gap between a frozen backbone and a new task.
  - Quick check: Which weights are updated during the training of ChronosX—the transformer layers or the FFNs in the IIB/OIB?

## Architecture Onboarding

- **Component map:** Raw target & Covariates -> Tokenizer -> IIB -> Frozen Backbone -> OIB -> Head
- **Critical path:** Tokenization -> IIB -> Backbone -> OIB -> Sampling
- **Design tradeoffs:** Adapter-only vs. Full Fine-tuning (adapters are faster and require less data; full fine-tuning yields lower error but risks catastrophic forgetting); Patching vs. Tokenization (adapters must be modified for patched inputs when extending to TimesFM/MOMENT).
- **Failure signatures:** Covariate Leakage (future covariates accidentally fed into IIB); Scale Mismatch (covariates not normalized); Zero-Shot Loss (adapters require training on target dataset).
- **First 3 experiments:**
  1. Ablation Study: Train three variants—IIB only, OIB only, IIB + OIB—on a synthetic dataset to verify which covariates drive performance gains.
  2. Architectural Variants: Compare dual-linear projection against single concatenated projection to verify the efficiency of the proposed interaction mechanism.
  3. Backbone Scalability: Test adapters on Chronos-Small vs. Chronos-Large to determine if relative improvement from covariates scales with model size.

## Open Questions the Paper Calls Out
None

## Limitations
- Temporal covariate alignment is assumed but not explicitly addressed for real-world misalignment scenarios
- The additive residual mechanism may not hold for highly nonlinear covariate-target relationships
- The method's robustness to irrelevant or noisy covariates is not demonstrated

## Confidence
- **High Confidence:** Modular adapter architecture design and core mechanisms (IIB for past, OIB for future) are well-specified and supported by ablation studies and benchmarks
- **Medium Confidence:** Extension to TimesFM and MOMENT is validated but adapter adaptation to patched inputs lacks full architectural details
- **Low Confidence:** Paper does not explore scenarios where future covariates are unavailable at inference time

## Next Checks
1. **Covariate sensitivity analysis:** Evaluate ChronosX performance when past/future covariates are randomized or removed entirely to quantify minimum signal strength required
2. **Robustness to covariate noise:** Test the method on datasets with partially corrupted covariates (20-50% missing values or Gaussian noise injection)
3. **Zero-shot future covariate handling:** Design an experiment where future covariates are unknown at inference time to validate OIB behavior in such cases