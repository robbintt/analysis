---
ver: rpa2
title: 'Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource
  Language Tokens by Cross-Entropy Loss'
arxiv_id: '2601.22439'
source_url: https://arxiv.org/abs/2601.22439
tags:
- margin
- proposed
- language
- tokens
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving the representation
  and performance of rare tokens and tokens from low-resource languages in neural
  language models. The core issue identified is that rare tokens are disproportionately
  affected by marginalization during training, where non-target embeddings are pushed
  away from irrelevant contexts, degrading their learned representations.
---

# Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss

## Quick Facts
- arXiv ID: 2601.22439
- Source URL: https://arxiv.org/abs/2601.22439
- Reference count: 29
- Key outcome: Thresholding technique improves low-resource language modeling accuracy from 0.31 to 0.49 and reduces PPLbest from 10.63 to 6.11

## Executive Summary
This paper addresses the challenge of improving representation and performance of rare tokens and low-resource language tokens in neural language models. The core insight is that cross-entropy loss disproportionately affects rare tokens through marginalization, where non-target embeddings are pushed away from irrelevant contexts, degrading their learned representations. The proposed solution applies a threshold to logits before calculating cross-entropy loss, effectively preventing marginalization gradients for tokens with probabilities below the threshold. Experiments with a character-level multilingual language model demonstrate significant improvements in low-resource language performance while maintaining high-resource language quality.

## Method Summary
The method applies thresholding before softmax in the language modeling pipeline: logits below a computed threshold (target logit minus margin hyperparameter) are set to negative infinity, preventing their marginalization gradients. This requires computing the threshold per token and applying it element-wise to the logits. The Separated Embeddings technique optionally stores each token embedding as an independent parameter to prevent AdamW momentum contamination during zero-gradient updates. The approach was tested on simulated multilingual data where 2% of Shakespeare text was transformed into a low-resource language by offsetting character IDs by 65.

## Key Results
- Low-resource language modeling accuracy improved from 0.31 to 0.49
- Low-resource PPLbest reduced from 10.63 to 6.11, approaching high-resource PPLbest of 4.97
- High-resource language accuracy remained stable at 0.53
- Embedding isotropy improved as measured by I(W) metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thresholding logits prevents non-relevant tokens from receiving marginalization gradients, reducing noise in rare token representations.
- Mechanism: Cross-entropy loss produces three gradient flows: context alignment pushes input embeddings toward target, target alignment pulls target embedding toward hidden state, non-target marginalization pushes ALL non-target embeddings away. For rare tokens, non-target marginalization dominates because they are frequently non-targets but rarely targets. Setting logits to -∞ for tokens with P(vi) < P(target) × e^(-margin) zeroes their marginalization gradients.
- Core assumption: Tokens with very low probability are irrelevant to the current context and should receive no learning signal.
- Evidence anchors:
  - [abstract]: "rare tokens are disproportionately affected by marginalization"
  - [section 2.1]: "irrelevant words, such as cat, should not be influenced by this example. Yet, because of cross-entropy loss, modern language models still 'learn' representations of irrelevant words"
  - [section 4.2, Figure 3]: "14 least frequent tokens have Ratio of Average below 1, and all tokens have an Average of Ratio below 1"
  - [corpus]: No direct corpus validation exists for this specific thresholding mechanism.
- Break condition: Margin too low creates "unreliable tail" that inflates perplexity; margin too high (→∞) converges to standard cross-entropy with no benefit.

### Mechanism 2
- Claim: Separated Embeddings (SE) enable true zero-gradient updates by isolating each token's optimization state.
- Mechanism: Standard nn.Embedding stores embeddings as a single matrix; AdamW updates momentum for all embeddings if any has non-zero gradients. SE stores embeddings as a list of independent nn.Parameters, allowing AdamW to skip momentum updates entirely for tokens with zero gradients.
- Core assumption: The improvement from SE stems from reduced marginalization, not contrastive learning effects.
- Evidence anchors:
  - [section 4.5]: "SE only affects the optimization of unselected tokens. The significant improvement in quality by SE suggests that this improvement occurs precisely by reducing marginalization"
  - [Table 1]: Proposed+SE (margin=0.6) achieves accuracy 0.4868 vs baseline 0.3147 for low-resource language
  - [Table 1]: PPLbest improves from 10.63 to 6.17 with Proposed+SE
  - [corpus]: No corpus validation exists for the SE technique.
- Break condition: SE increases memory fragmentation and parameter management overhead; may not scale well to very large vocabularies without engineering optimization.

### Mechanism 3
- Claim: The margin hyperparameter determines which tokens are marginalized vs. ignored, with optimal values bounded by vocabulary size and sampling strategy.
- Mechanism: For nucleus sampling with temperature T and top_p, margin = T × ln((N-1) × top_p / (1-top_p)) ensures tokens within top_p are marginalized while those outside are ignored. Larger d_model may permit larger effective margins.
- Core assumption: Margin should scale with vocabulary size and embedding dimension, though exact d_model dependency is not fully characterized.
- Evidence anchors:
  - [section 3.2]: "as margin→+∞, the proposed method converges to standard cross-entropy loss"
  - [Appendix A]: Lemma 3 proves thresholded tokens fall outside top_p distribution for the derived margin formula
  - [Appendix B, Table 3]: Effective margin grows with d_model (768→14.04, 1600→29.99 for initialized models)
  - [corpus]: No corpus validation for margin selection in real multilingual settings.
- Break condition: Optimal margin is dataset-dependent; requires tuning. Small models may lack signal for reliable margin selection.

## Foundational Learning

- Concept: **Cross-Entropy Loss Gradient Decomposition**
  - Why needed: Understanding the three gradient flows (context alignment, target alignment, non-target marginalization) is essential to diagnosing why rare tokens suffer degraded representations.
  - Quick check question: For a rare token appearing 0.02% of the time, why does non-target marginalization dominate its gradient updates?

- Concept: **Softmax Temperature and Nucleus Sampling**
  - Why needed: The margin hyperparameter connects to sampling strategies; understanding top_p helps derive reasonable margin bounds.
  - Quick check question: Given vocabulary size 100,000, temperature 0.9, and top_p=0.99, compute the recommended margin upper bound.

- Concept: **AdamW Momentum and Parameter Grouping**
  - Why needed: Understanding why SE is necessary requires knowing how momentum-based optimizers handle shared parameter matrices.
  - Quick check question: Why does storing embeddings as nn.Embedding cause unwanted updates even when specific token gradients are zero?

## Architecture Onboarding

- Component map: Logits -> Thresholding module -> Softmax -> Cross-entropy loss
- Critical path:
  1. Forward pass produces logits: (batch, seq_len, vocab_size)
  2. Gather target logits via `torch.gather`
  3. Compute threshold: `target_logits - margin`
  4. Apply: `logits = torch.where(logits < threshold, -inf, logits)`
  5. Softmax and cross-entropy proceed normally
  6. Backward pass: zero gradients for thresholded tokens automatically

- Design tradeoffs:
  - Lower margin: Better rare token representations, higher perplexity from unreliable tail
  - Higher margin: Safer, less benefit; at margin→∞ equals baseline
  - SE enabled: True zero-gradient updates, memory overhead, more complex parameter management
  - SE disabled: Simpler implementation, residual momentum contamination

- Failure signatures:
  - PPL explodes (e.g., >50): Margin too low; unreliable tail dominates
  - No improvement vs baseline: Margin too high (try reducing by 2-4x)
  - High-resource language degrades: Margin too aggressive; increase by 1-2x
  - Memory issues with SE: Consider chunked parameter lists or fallback to standard embedding

- First 3 experiments:
  1. **Quantify marginalization**: Log gradient norms by type for each token; compute Ratio of Average (alignment/marginalization) to confirm rare tokens are disproportionately affected.
  2. **Margin sweep**: Test margins in [0.5, 1, 2, 4, 8] on validation data; plot PPL and accuracy for high-resource vs low-resource languages separately.
  3. **SE ablation**: Compare thresholding-only vs thresholding+SE with matched margin; measure gap in low-resource accuracy and embedding isotropy (I(W) metric).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the reduction of marginalization via logit thresholding translate to improved performance on downstream tasks?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "improvement in the validation data... does not necessarily guarantee improved performance in downstream tasks" and call for "further testing on various downstream tasks."
- **Why unresolved:** The experiments focused exclusively on language modeling metrics (perplexity and accuracy) and embedding geometry, rather than transfer learning or task-specific fine-tuning.
- **What evidence would resolve it:** Evaluation of models pre-trained with thresholding on downstream benchmarks such as multilingual text classification or machine translation.

### Open Question 2
- **Question:** How effective is the thresholding method when applied to natural, non-simulated multilingual corpora?
- **Basis in paper:** [explicit] The paper notes that the method was tested only on "simulated multilingual data" where characters had a 1:1 correspondence, acknowledging that "testing with real languages might lead to different results."
- **Why unresolved:** The simulated data assumes a perfect alignment between high-resource and low-resource characters, an idealization that does not exist in natural languages with distinct scripts or syntax.
- **What evidence would resolve it:** Experiments training the model on real-world low-resource languages (e.g., mixing English with Icelandic or Swahili) rather than shifted character IDs.

### Open Question 3
- **Question:** What is the optimal protocol for selecting the "margin" hyperparameter relative to model size?
- **Basis in paper:** [explicit] Section 3.2 states, "we do not cover the optimal choice of margin in this work," while Appendix B suggests the effective margin likely scales with embedding dimension ($d_{model}$).
- **Why unresolved:** The authors provided a search range but did not establish a deterministic rule or adaptive mechanism for setting the margin without manual tuning.
- **What evidence would resolve it:** A study correlating optimal margin values against varying embedding dimensions and vocabulary sizes to derive a scaling law.

## Limitations

- The empirical claims rely on a highly controlled synthetic setting that may not capture real-world multilingual corpus challenges
- The margin hyperparameter selection lacks a principled data-driven derivation and requires extensive manual tuning
- The Separated Embeddings technique introduces engineering complexity and memory overhead that may limit scalability to large vocabularies

## Confidence

**High Confidence** (Theoretical Framework):
- The identification of marginalization as a key degradation mechanism for rare tokens is well-supported by gradient analysis and ablation studies
- The core thresholding mechanism mathematically guarantees that selected tokens receive zero marginalization gradients
- The Separated Embeddings technique correctly isolates parameter updates from momentum contamination

**Medium Confidence** (Empirical Claims):
- The quantitative improvements (LR accuracy 0.31→0.49, PPLbest 10.63→6.11) are well-documented in the synthetic setting but may not translate directly to real multilingual data
- The claim that margin selection requires tuning is supported by ablation results but lacks a data-driven selection method
- The isotropy improvements (I(W) metric) are demonstrated but their practical significance for downstream tasks remains unclear

**Low Confidence** (Practical Deployment):
- The optimal margin value for real multilingual datasets is not characterized beyond the synthetic setting
- The memory and computational overhead of Separated Embeddings for large vocabularies is not quantified
- The interaction with other regularization techniques (dropout, weight decay) and their combined effect on low-resource performance is unexplored

## Next Checks

1. **Cross-Lingual Transfer Validation**: Apply the thresholding technique to a real multilingual corpus (e.g., mC4 or OSCAR) with genuine low-resource languages. Measure not just perplexity but also zero-shot cross-lingual transfer to downstream tasks (X-NLI, Tatoeba). This will validate whether improved token representations translate to practical language understanding gains.

2. **Margin Selection Protocol**: Develop and validate a data-driven margin selection method based on token frequency distributions and embedding isotropy metrics. Test whether automatic margin selection can achieve performance within 5% of hand-tuned margins across diverse language families and script systems.

3. **Scalable Separated Embeddings**: Implement and benchmark memory-efficient alternatives to full Separated Embeddings, such as chunked parameter lists or sparse embedding matrices. Quantify the tradeoff between memory overhead and low-resource language improvement for vocabularies of 10K, 50K, and 100K tokens.