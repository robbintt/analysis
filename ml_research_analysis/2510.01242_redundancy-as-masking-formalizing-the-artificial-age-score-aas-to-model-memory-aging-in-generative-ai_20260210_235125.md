---
ver: rpa2
title: 'Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model
  Memory Aging in Generative AI'
arxiv_id: '2510.01242'
source_url: https://arxiv.org/abs/2510.01242
tags:
- memory
- recall
- score
- sessions
- episodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Artificial Age Score (AAS), a formal,
  entropy-based metric for quantifying memory aging in generative AI systems. AAS
  is log-scaled, bounded, and monotonic, designed to distinguish between structural
  youth (low score) and aging (high score) based on recall behavior.
---

# Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI

## Quick Facts
- arXiv ID: 2510.01242
- Source URL: https://arxiv.org/abs/2510.01242
- Reference count: 30
- Primary result: Introduces AAS metric showing persistent memory yields youth (low score), stateless sessions cause aging (high score)

## Executive Summary
The study introduces the Artificial Age Score (AAS), a formal, entropy-based metric for quantifying memory aging in generative AI systems. AAS is log-scaled, bounded, and monotonic, designed to distinguish between structural youth (low score) and aging (high score) based on recall behavior. The score treats redundancy as masking information overlap, though in this study, redundancy is set to zero for conservative upper bounds. Tested across 25 days with ChatGPT-5 in bilingual (English/Turkish) sessions, AAS revealed that persistent memory (no session resets) maintained both semantic and episodic recall, yielding near-zero scores indicative of youth. Stateless sessions (with resets) caused episodic memory collapse and high AAS values, signaling aging. This demonstrates AAS as a rigorous, task-independent tool for assessing memory degradation in AI systems.

## Method Summary
The Artificial Age Score (AAS) was developed as an entropy-based metric using information-theoretic principles to quantify memory aging in generative AI. The methodology involved setting up bilingual (English/Turkish) sessions with ChatGPT-5 over 25 days, comparing persistent memory conditions (no resets) against stateless sessions (with resets). Memory recall was tested through structured prompts that distinguished between semantic (factual) and episodic (contextual) memory types. AAS calculations incorporated a redundancy-as-masking framework, though redundancy was set to zero for conservative estimation. The metric's log-scaling, bounded nature, and monotonicity were verified through systematic testing, with scores interpreted as indicators of memory youth (low values) versus aging (high values).

## Key Results
- Persistent memory sessions maintained both semantic and episodic recall, yielding near-zero AAS scores indicative of youth
- Stateless sessions with resets caused episodic memory collapse and high AAS values (0.82-0.98), signaling aging
- AAS demonstrated task-independent capability to detect memory degradation patterns across 25-day observation period

## Why This Works (Mechanism)
The mechanism relies on entropy-based quantification of information retention, where memory aging manifests as increased uncertainty in recall patterns. By treating redundancy as masking information overlap, the model captures how repeated exposure to similar information can obscure distinct memory traces. The log-scaling compresses extreme values while preserving monotonic progression, allowing clear differentiation between youth (low entropy, high certainty) and aging (high entropy, low certainty) states. The zero-redundancy assumption provides a conservative upper bound that highlights the most severe aging effects without masking them with redundancy-based information preservation.

## Foundational Learning

**Entropy-based memory modeling**: Information-theoretic framework for quantifying memory states through uncertainty measures. Why needed: Provides mathematical rigor for comparing memory quality across sessions. Quick check: Verify that entropy calculations produce consistent results across repeated identical prompts.

**Redundancy-as-masking principle**: Conceptual model where overlapping information obscures distinct memory traces. Why needed: Explains why repeated similar inputs can degrade memory discrimination. Quick check: Test with controlled redundancy levels to observe masking effects.

**Log-scaling for bounded metrics**: Mathematical transformation that compresses extreme values while preserving order relationships. Why needed: Ensures scores remain interpretable across wide value ranges. Quick check: Confirm that log-transformed values maintain monotonicity with original measurements.

**Semantic-episodic memory distinction**: Classification framework separating factual recall from contextual memory. Why needed: Enables granular assessment of different memory types' aging trajectories. Quick check: Validate prompt classifications through inter-rater reliability testing.

## Architecture Onboarding

**Component map**: Input prompts → Memory encoding → Recall retrieval → Entropy calculation → AAS scoring

**Critical path**: Prompt design → Memory session setup → Recall testing → Entropy computation → Score interpretation

**Design tradeoffs**: Zero redundancy assumption maximizes aging detection sensitivity but may underestimate real-world memory preservation. Log-scaling improves interpretability but can compress meaningful variations at extreme values.

**Failure signatures**: High AAS scores with consistent recall indicate model architecture limitations rather than true aging. Session-dependent score fluctuations suggest instability in memory encoding mechanisms.

**3 first experiments**:
1. Test single-prompt recall consistency across 10 consecutive sessions to establish baseline entropy
2. Compare semantic versus episodic memory aging rates using controlled prompt sets
3. Measure AAS sensitivity to prompt complexity variations (simple vs complex queries)

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-redundancy assumption may not reflect real-world memory retention dynamics where redundancy is non-zero
- Bilingual testing with single model (ChatGPT-5) limits generalizability across different architectures and language pairs
- 25-day observation period may be insufficient to capture longer-term aging trajectories
- Semantic-episodic distinction relies on qualitative prompt classification, introducing potential subjectivity

## Confidence
**High confidence**: The formal mathematical framework of AAS (log-scaled, bounded, monotonic properties) and its basic computational implementation.

**Medium confidence**: The distinction between structural youth (low AAS) and aging (high AAS) states based on session persistence.

**Low confidence**: The universality of AAS across different LLM architectures, the long-term stability of the metric, and the generalizability of findings beyond the specific testing conditions.

## Next Checks
1. Test AAS across multiple LLM architectures (GPT, Claude, Llama) with varying parameter counts to assess architectural sensitivity and establish cross-model baselines.

2. Implement non-zero redundancy values in the masking model and compare aging trajectories against the zero-redundancy baseline to identify sensitivity thresholds.

3. Conduct longitudinal studies extending beyond 25 days (minimum 90 days) with controlled memory interventions to map aging progression curves and identify potential inflection points.