---
ver: rpa2
title: 'DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for
  Multimodal Large Language Models'
arxiv_id: '2601.19267'
source_url: https://arxiv.org/abs/2601.19267
tags:
- dialogue
- speaker
- audiovisual
- video
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating accurate dialogue
  descriptions in audiovisual video captioning, which is critical for downstream understanding
  and generation tasks. Existing models struggle to produce faithful speaker attributions
  and precise utterance transcriptions in complex dialogue scenarios.
---

# DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2601.19267
- Source URL: https://arxiv.org/abs/2601.19267
- Reference count: 40
- Primary result: DiaDem outperforms Gemini series in dialogue description accuracy (65.9% REF, 79.3% ASR vs. 63.1%/71.0% and 63.6%/74.8%)

## Executive Summary
This paper addresses the challenge of generating accurate dialogue descriptions in audiovisual video captioning, which is critical for downstream understanding and generation tasks. Existing models struggle to produce faithful speaker attributions and precise utterance transcriptions in complex dialogue scenarios. To tackle this, the authors propose DiaDem, an audiovisual video captioning model enhanced through a two-stage training pipeline: first, a high-quality dataset is synthesized for supervised fine-tuning (SFT) to equip the model with foundational dialogue description skills, then a difficulty-partitioned two-stage Group Relative Policy Optimization (GRPO) strategy is employed to further improve utterance transcription and speaker attribution accuracy. To systematically evaluate dialogue description quality, the authors introduce DiaDemBench, a comprehensive benchmark focusing on both speaker attribution accuracy and utterance transcription fidelity across diverse dialogue scenarios.

## Method Summary
DiaDem employs a two-stage post-training pipeline. First, it synthesizes 70K dialogue captions and 15K non-dialogue captions for SFT using a chained Gemini model approach (Gemini-2.5-Pro for transcription, Gemini-3-Pro for speaker attribution, integrated with A V oCaDO base captions). The model is trained for 2 epochs with batch size 128 and learning rate 2e-5. Second, it uses difficulty-partitioned two-stage GRPO on 3K manually annotated samples, filtered to remove easy samples (mean reward >0.8, std <0.1) and focusing on hard samples (mean reward <0.3) in Stage 2 with doubled weight. The reward combines dialogue (REF+ASR)/2, checklist, and length-regularized components.

## Key Results
- DiaDem achieves 65.9% REF and 79.3% ASR on DiaDemBench, outperforming Gemini models
- Maintains competitive performance on general audiovisual captioning benchmarks
- Demonstrates superior handling of speaker attribution and utterance transcription compared to existing models
- Shows performance degradation on complex scenarios (3+ speakers, overlapping speech, off-screen speakers)

## Why This Works (Mechanism)

### Mechanism 1: Complementary Model Specialization for Data Synthesis
- Claim: Synthesizing high-quality dialogue captions by chaining models with complementary strengths improves training data fidelity.
- Mechanism: Gemini-2.5-Pro excels at utterance transcription; Gemini-3-Pro excels at speaker attribution. The pipeline routes dialogue through Gemini-2.5-Pro first, then Gemini-3-Pro corrects speaker references, and finally integrates refined dialogues into A V oCaDO base captions.
- Core assumption: Errors in transcription and attribution are largely independent and can be sequentially corrected.
- Evidence anchors:
  - [abstract] "We first synthesize a high-quality dataset for SFT"
  - [section 4.2] "Gemini-2.5-Pro excels at transcribing utterances, whereas... Gemini-3-Pro exhibits superior speaker attribution ability"
  - [corpus] Weak direct evidence; related work on video captioning (VideoComp, M-ACM) focuses on temporal/visual alignment rather than model chaining for data synthesis.
- Break condition: If transcription and attribution errors correlate strongly (e.g., both fail on overlapping speech), sequential correction may compound errors.

### Mechanism 2: Difficulty-Partitioned Two-Stage GRPO
- Claim: Partitioning GRPO training by sample difficulty improves gradient informativeness and final performance.
- Mechanism: Stage 1 trains on filtered dataset (removes samples with reward variance <0.1 and mean >0.8). Stage 2 isolates high-difficulty samples (mean reward <0.3) and doubles their weight. This prevents uninformative gradients from diluting learning.
- Core assumption: Reward variance correlates with learning signal quality; easy samples provide near-zero gradients.
- Evidence anchors:
  - [abstract] "employs a difficulty-partitioned two-stage GRPO strategy"
  - [section 4.3] "for samples that are either overly simple or excessively difficult, dialogue reward scores exhibit minimal variance... leading to uninformative gradients"
  - [corpus] No direct corpus evidence for difficulty-partitioned GRPO in video captioning.
- Break condition: If difficulty measure (reward variance) does not reflect true sample hardness, partitioning may discard useful samples or overfit to noisy hard cases.

### Mechanism 3: Adaptive Merging for Utterance Matching
- Claim: Adaptive merging of adjacent same-speaker utterances during evaluation mitigates segmentation inconsistency.
- Mechanism: During dynamic programming alignment, adjacent utterances from the same speaker are merged when it improves similarity. This handles cases where models segment consecutive speech differently from ground truth.
- Core assumption: Segmentation differences are more likely than content errors for same-speaker adjacent utterances.
- Evidence anchors:
  - [section 3.2] "naive one-to-one matching strategy... fails to account for segmentation inconsistencies"
  - [section 3.2] "merging is restricted to adjacent utterances from the same speaker to avoid introducing speaker ambiguity"
  - [corpus] No corpus evidence; standard metrics (e.g., BLEU, CIDEr) do not handle dialogue segmentation.
- Break condition: If model outputs incorrectly segment mid-utterance, adaptive merging may mask genuine transcription errors.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: DiaDem uses GRPO instead of PPO to avoid training a separate critic model; rewards are computed relative to a group of sampled responses.
  - Quick check question: Can you explain how GRPO computes the advantage function without a value network?

- Concept: **Speaker Diarization**
  - Why needed here: Dialogue description requires identifying "who spoke when"; understanding traditional pipeline (VAD → embedding → clustering → ASR) clarifies why end-to-end multimodal approaches are preferable.
  - Quick check question: What is the main limitation of modular speaker diarization pipelines that end-to-end approaches aim to address?

- Concept: **Dynamic Programming for Sequence Alignment**
  - Why needed here: DiaDemBench uses DP to optimally match predicted and ground-truth dialogue sequences with adaptive merging.
  - Quick check question: How does the adaptive merging constraint (same speaker only) affect the DP recurrence?

## Architecture Onboarding

- Component map: A V oCaDO (7B) base model -> SFT on 85K synthesized captions -> GRPO on 2.1K filtered samples -> difficulty-partitioned two-stage training
- Critical path:
  1. SFT on 85K synthesized captions (2 epochs, lr=2e-5)
  2. Filter easy samples from 3K GRPO set (8 rollouts, discard if mean>0.8 & std<0.1)
  3. Stage 1 GRPO on filtered set (lr=1e-5, 8 samples/query)
  4. Stage 2 GRPO on doubled hard subset
- Design tradeoffs:
  - Filtering easy samples improves gradient signal but reduces data volume; authors report 3K → 2.1K reduction
  - Using Gemini for data synthesis enables scale but introduces dependency on closed-source models
  - Adaptive merging improves evaluation fairness but adds complexity to metric interpretation
- Failure signatures:
  - Overlapping speech: all models perform poorly (DiaDem drops ~15 points vs. non-overlap)
  - Off-screen speakers: models misattribute to visible characters
  - Same-gender speakers: performance drops due to reduced vocal discriminability
  - Three+ speakers: DiaDem underperforms Gemini in multi-speaker settings
- First 3 experiments:
  1. **Ablate SFT data source:** Train on Gemini-2.5-Pro-only vs. chained pipeline to isolate speaker attribution gains
  2. **Vary difficulty thresholds:** Test different filtering thresholds (e.g., std<0.05 vs. <0.15) to verify gradient signal hypothesis
  3. **Cross-benchmark transfer:** Evaluate DiaDemBench-trained models on standard video captioning benchmarks (SALMONN-2 testset) to confirm no catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can audiovisual video captioning models be improved to accurately describe complex multi-party interactions involving three or more speakers?
- Basis in paper: [explicit] The authors state in the Limitations and Appendix A.2.1 that DiaDem underperforms in scenarios with $N \geq 3$ speakers, identifying this as a key direction for future work.
- Why unresolved: Performance degrades significantly as the number of speakers increases, likely due to increased visual complexity and difficulty in tracking multiple distinct voices.
- What evidence would resolve it: A model achieving significantly higher REF and ASR scores specifically on the $N \geq 3$ subsets of DiaDemBench compared to current SOTA.

### Open Question 2
- Question: What specific architectural or training interventions are effective for resolving speaker attribution in scenarios with temporally overlapping speech?
- Basis in paper: [explicit] The paper notes in Appendix A.2.1 that "both open-source and commercial models generally perform poorly" on overlapping speech, representing a key area for future work.
- Why unresolved: Current models lack the strong audio source separation capabilities and tight audio-visual integration needed to attribute simultaneous utterances to the correct speakers.
- What evidence would resolve it: Demonstration of improved performance on the "Overlap" subset of DiaDemBench through enhanced audio-visual fusion strategies.

### Open Question 3
- Question: How can models better leverage joint vocal timbre and visual context to resolve speaker identity in challenging cases involving off-screen speakers or speakers of the same gender?
- Basis in paper: [explicit] Appendix A.2.3 suggests that future work should prioritize "more effective fusion strategies that leverage both vocal characteristics and contextual visual information."
- Why unresolved: Current models struggle to disentangle speaker identity when visual cues are absent (off-screen) or when vocal timbre is similar (same-gender), leading to misattribution.
- What evidence would resolve it: Ablation studies showing that specific fusion mechanisms reduce the performance gap between "opposite-gender only" and "same-gender" or "off-screen" video subsets.

## Limitations
- Data Dependency and Scalability: Heavy reliance on closed-source Gemini models for data synthesis raises scalability and generalizability concerns.
- Evaluation Metric Complexity: DiaDemBench's adaptive merging and dynamic programming alignment add complexity that may affect metric reliability and generalizability.
- Generalization Beyond Dialogue: Performance on non-dialogue video captioning tasks is not well-established, with evidence of potential catastrophic forgetting.

## Confidence
- High Confidence: The two-stage training pipeline (SFT followed by difficulty-partitioned GRPO) is well-specified and demonstrates clear improvements in dialogue description accuracy.
- Medium Confidence: The data synthesis approach using chained Gemini models is theoretically sound but relies on assumptions about model complementarity that may not hold across all dialogue scenarios.
- Low Confidence: Claims about DiaDem's performance on complex multi-speaker scenarios and overlapping speech are based on limited evidence.

## Next Checks
1. **Cross-Domain Transfer**: Evaluate DiaDem on non-dialogue video captioning benchmarks (e.g., MSRVTT, VATEX) to assess whether the model maintains general captioning capabilities after dialogue-focused fine-tuning.
2. **Synthetic Data Quality Analysis**: Conduct ablation studies comparing different data synthesis strategies: Gemini-2.5-Pro only vs. chained pipeline vs. Gemini-3-Pro only.
3. **Metric Robustness Testing**: Test DiaDemBench evaluation framework across datasets with varying dialogue structures (e.g., TV shows with different editing styles, movies vs. interviews).