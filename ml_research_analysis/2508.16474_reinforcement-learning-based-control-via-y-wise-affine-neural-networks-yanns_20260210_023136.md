---
ver: rpa2
title: Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)
arxiv_id: '2508.16474'
source_url: https://arxiv.org/abs/2508.16474
tags:
- control
- learning
- function
- system
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YANN-RL, a novel reinforcement learning algorithm
  using Y-wise Affine Neural Networks (YANNs) to provide safer and more interpretable
  control for chemical and energy systems. YANNs can exactly represent piecewise affine
  functions, enabling initialization of RL actor and critic networks with multi-parametric
  linear model predictive control (mp-MPC) solutions.
---

# Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)

## Quick Facts
- arXiv ID: 2508.16474
- Source URL: https://arxiv.org/abs/2508.16474
- Reference count: 40
- Primary result: YANN-RL achieves safer and more interpretable control for chemical and energy systems by initializing RL networks with multi-parametric linear MPC solutions

## Executive Summary
This paper introduces YANN-RL, a novel reinforcement learning algorithm that leverages Y-wise Affine Neural Networks (YANNs) to provide safer and more interpretable control for chemical and energy systems. The key innovation is using YANNs' exact representation capability for piecewise affine functions to initialize RL actor and critic networks with multi-parametric linear model predictive control (mp-MPC) solutions. This approach provides theoretical guarantees of stability and recursive feasibility inherited from linear optimal control, effectively eliminating the unsafe exploration phase that plagues traditional RL algorithms.

The method is demonstrated through two case studies: a clipped pendulum and a safety-critical chemical reactor. YANN-RL significantly outperforms standard deep deterministic policy gradient (DDPG) algorithms, achieving better initial performance while maintaining safety constraints without violations. For the chemical reactor application, YANN-RL avoided all safety constraint violations while DDPG experienced multiple violations, demonstrating the practical value of this approach for safety-critical industrial applications.

## Method Summary
YANN-RL combines multi-parametric linear model predictive control with reinforcement learning by using Y-wise Affine Neural Networks that can exactly represent piecewise affine functions. The YANN-actor and YANN-critic networks are initialized using mp-MPC solutions computed from a linearized system model, then extended with additional layers to capture nonlinear system behavior. This initialization strategy provides a warm start that satisfies safety constraints from the beginning of training, unlike traditional RL methods that require unsafe exploration phases. The YANN architecture allows for exact representation of the piecewise affine structure inherent in mp-MPC solutions while maintaining differentiability for gradient-based optimization.

## Key Results
- YANN-RL outperformed DDPG on both clipped pendulum and chemical reactor case studies
- For the chemical reactor, YANN-RL avoided all safety constraint violations while DDPG experienced multiple violations
- The method achieved better initial performance compared to standard RL approaches
- YANN-actor networks successfully maintained safety constraints throughout training

## Why This Works (Mechanism)
The method works by leveraging the exact representation capability of YANNs for piecewise affine functions, which directly corresponds to the structure of mp-MPC solutions. By initializing RL networks with these proven optimal control solutions, the algorithm inherits stability and safety guarantees from linear optimal control theory. The additional nonlinear layers added to the YANNs allow the policy to adapt to system nonlinearities while maintaining proximity to the safe, optimal linear solution. This approach effectively bridges the gap between model-based optimal control and model-free RL, providing a safe exploration strategy that avoids the performance degradation typically associated with exploration in safety-critical systems.

## Foundational Learning

**Multi-parametric Linear Model Predictive Control (mp-MPC)**: A control strategy that solves the MPC optimization problem offline to obtain explicit piecewise affine control laws as functions of the state. *Why needed*: Provides optimal control solutions with theoretical guarantees that can initialize RL networks. *Quick check*: Verify that the mp-MPC solution is piecewise affine and can be exactly represented by YANNs.

**Y-wise Affine Neural Networks (YANNs)**: Neural networks with a specific architecture that can exactly represent piecewise affine functions. *Why needed*: Enables initialization of RL networks with mp-MPC solutions while maintaining differentiability. *Quick check*: Confirm that YANN layers can partition the state space and apply different affine transformations to each partition.

**Safe Reinforcement Learning**: RL approaches that incorporate safety constraints or use initialization strategies to avoid unsafe exploration. *Why needed*: Critical for practical deployment in chemical and energy systems where constraint violations can have severe consequences. *Quick check*: Verify that safety constraints are satisfied throughout training and that the learned policy remains within safe operating regions.

## Architecture Onboarding

**Component Map**: System Model -> mp-MPC Solver -> YANN Initialization -> RL Training -> Control Policy

**Critical Path**: The most critical path is from the mp-MPC solution computation through YANN initialization to the final control policy. The quality of the mp-MPC solution directly impacts the safety and performance of the learned policy, while the YANN architecture must accurately represent the piecewise affine structure to maintain the theoretical guarantees.

**Design Tradeoffs**: The approach trades computational complexity during training (solving mp-MPC offline) for safer and more efficient online learning. While the YANN architecture enables exact representation of piecewise affine functions, it becomes computationally expensive for high-dimensional state spaces. The method also requires accurate system models for mp-MPC initialization, limiting its applicability to systems with poor model fidelity.

**Failure Signatures**: Potential failures include poor mp-MPC solution quality due to model inaccuracies, YANN architecture unable to capture complex nonlinearities despite additional layers, and safety constraints being violated if the nonlinear extensions move too far from the linear baseline. The method may also struggle with highly nonlinear systems where linear approximations are inadequate.

**First Experiments**: 
1. Verify that YANN layers can exactly represent a simple piecewise affine function with known partitions
2. Test mp-MPC initialization on a linear system where the optimal solution is known analytically
3. Compare YANN-RL performance against standard RL methods on a simple control task with safety constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability challenges for high-dimensional systems due to exponential complexity in state space representation
- Reliance on accurate linear system models for mp-MPC initialization limits applicability to highly nonlinear or poorly characterized systems
- Limited validation across diverse industrial applications beyond the demonstrated chemical reactor case study

## Confidence
- High confidence in YANN architecture's exact representation capability for piecewise affine functions
- Medium confidence in performance claims due to limited comparative studies against other safe RL methods
- Medium confidence in safety guarantees, as these are inherited from linear control theory but may not fully persist in nonlinear extensions

## Next Checks
1. Benchmark YANN-RL against other state-of-the-art safe RL algorithms (e.g., CPO, Lagrangian-based methods) across multiple control tasks
2. Evaluate YANN-RL performance and computational efficiency on higher-dimensional systems with 5+ state variables
3. Test robustness of YANN-RL when initialized with imperfect linear models or in the presence of significant model uncertainty