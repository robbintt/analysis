---
ver: rpa2
title: Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed
  versus Open LLMs for Automated Essay Scoring
arxiv_id: '2503.11827'
source_url: https://arxiv.org/abs/2503.11827
tags:
- llms
- open
- closed
- gpt-4
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared nine large language models (LLMs) across automated
  essay scoring tasks to assess performance, fairness, and cost differences between
  closed, open, and open-source models. Researchers evaluated human-written essays
  using zero-shot and few-shot learning approaches, measuring mean squared error,
  correlation metrics, and demographic bias.
---

# Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring

## Quick Facts
- **arXiv ID**: 2503.11827
- **Source URL**: https://arxiv.org/abs/2503.11827
- **Reference count**: 10
- **Primary result**: Open LLMs like Qwen2.5 and Llama 3 achieved comparable essay scoring performance to GPT-4 at up to 37x lower cost with no significant fairness differences across demographics.

## Executive Summary
This study evaluates nine large language models for automated essay scoring, comparing closed models (GPT-4, GPT-3.5) against open models (Qwen2.5, Llama 3) and open-source alternatives. The research employs zero-shot and few-shot learning approaches to assess scoring accuracy, demographic fairness, and cost efficiency. Results demonstrate that open models can match the performance of proprietary systems while offering substantially lower operational costs, challenging assumptions about the superiority of closed LLMs. The study also finds no significant bias differences across race or age demographics, suggesting equitable performance across diverse student populations.

## Method Summary
Researchers conducted a comprehensive evaluation of nine LLMs using human-written essays from standardized assessment datasets. The study employed zero-shot and few-shot learning paradigms to score essays without extensive model fine-tuning. Performance was measured using mean squared error and correlation metrics against human scores. Fairness was assessed through demographic analysis across race and age groups, while cost efficiency was calculated based on API pricing and computational requirements. The study also included text generation quality analysis using t-SNE visualizations and ANOVA models to examine distributional differences between model outputs.

## Key Results
- Open models (Qwen2.5, Llama 3) achieved comparable scoring accuracy to GPT-4 with significantly lower costs
- No significant demographic bias differences were found across race or age groups
- Cost efficiency varied dramatically, with open models offering up to 37x cost savings
- Text generation quality from open models was competitive with closed alternatives

## Why This Works (Mechanism)
The study demonstrates that open LLMs can achieve competitive performance through effective zero-shot and few-shot learning approaches that leverage pre-trained knowledge without requiring expensive fine-tuning. The comparable performance suggests that open models have sufficiently broad pre-training to handle domain-specific tasks like essay scoring. The cost advantages stem from open models' availability through self-hosting or more efficient API pricing structures, while maintaining similar underlying transformer architectures and training methodologies.

## Foundational Learning

**Transformer Architecture**: Why needed - Understanding core LLM functionality for scoring tasks. Quick check - Verify models use attention mechanisms for sequence processing.

**Zero-shot Learning**: Why needed - Key methodology for evaluating model generalization. Quick check - Confirm models score without task-specific training examples.

**Few-shot Learning**: Why needed - Tests model adaptability with minimal examples. Quick check - Verify performance with limited demonstration samples.

**Bias Detection Methods**: Why needed - Essential for fairness assessment in educational contexts. Quick check - Validate demographic analysis techniques.

**Cost Metrics**: Why needed - Critical for practical deployment decisions. Quick check - Confirm pricing calculations include all operational costs.

## Architecture Onboarding

**Component Map**: Data Input -> Pre-processing -> LLM Inference -> Score Generation -> Performance Evaluation -> Fairness Analysis -> Cost Calculation

**Critical Path**: Essay data flows through model inference to generate scores, which are then evaluated against human benchmarks for accuracy and fairness metrics.

**Design Tradeoffs**: The study balances computational cost against scoring accuracy, choosing evaluation methods that provide meaningful comparisons without requiring extensive fine-tuning resources.

**Failure Signatures**: Performance degradation occurs when models encounter domain-specific vocabulary or complex argumentative structures not well-represented in pre-training data.

**Three First Experiments**:
1. Compare zero-shot versus few-shot performance across all nine models
2. Test demographic fairness across additional protected categories
3. Evaluate scoring consistency across different essay genres

## Open Questions the Paper Calls Out
None

## Limitations
- Study focused on single educational context, limiting generalizability
- Fairness analysis excluded important demographic factors like gender and socioeconomic status
- Cost comparisons based on current pricing may become outdated
- Limited scope to only nine models may miss emerging alternatives

## Confidence

**High Confidence**:
- Comparative performance results showing open LLMs matching GPT-4
- Cost efficiency findings based on actual API pricing
- Standardized metrics providing robust quantitative comparisons

**Medium Confidence**:
- Fairness assessment conclusions limited by narrow demographic scope
- Text generation quality comparisons may not capture all assessment-relevant factors

**Low Confidence**:
- Broader claims about "challenging dominance" extend beyond empirical findings
- Accessibility and transparency assertions involve subjective interpretations

## Next Checks

1. Cross-domain validation using different assessment tasks (short-answer, scientific writing)
2. Longitudinal cost analysis tracking performance and pricing over 6-12 months
3. Expanded fairness audit including additional demographics and intersectional analysis