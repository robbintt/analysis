---
ver: rpa2
title: 'Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win'
arxiv_id: '2506.03919'
source_url: https://arxiv.org/abs/2506.03919
tags:
- graph
- graphs
- pruning
- expressivity
- lottery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the lottery ticket hypothesis (LTH) in
  graph neural networks (GNNs), focusing on the critical role of expressivity in sparse
  subnetworks. The authors establish that the ability of sparse subnetworks to distinguish
  non-isomorphic graphs is crucial for preserving predictive performance.
---

# Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win

## Quick Facts
- arXiv ID: 2506.03919
- Source URL: https://arxiv.org/abs/2506.03919
- Authors: Lorenz Kummer; Samir Moustafa; Anatol Ehrlich; Franka Bause; Nikolaus Suess; Wilfried N. Gansterer; Nils M. Kriege
- Reference count: 40
- Primary result: Expressivity of sparse subnetworks is critical for preserving predictive performance in GNNs

## Executive Summary
This paper investigates the lottery ticket hypothesis in graph neural networks, establishing that expressivity preservation is essential for sparse subnetworks to maintain predictive performance. The authors prove the Strong Expressive Lottery Ticket Hypothesis (SELTH), showing that maximally expressive sparse subnetworks exist within sufficiently overparameterized moment-based GNNs. They demonstrate empirically that pre-training expressivity strongly correlates with post-training accuracy, and identify scenarios where expressivity loss is irrecoverable, particularly for structurally isomorphic but feature-divergent graphs.

## Method Summary
The paper combines theoretical analysis with empirical validation to study expressivity in sparse GNN subnetworks. Theoretical contributions include proving SELTH (existence of maximally expressive sparse subnetworks) and Lemma 3.5 (irrecoverable expressivity loss for SIFDGs). Empirically, the authors measure pre-training expressivity against post-training accuracy across various pruning ratios and datasets, showing strong correlation between these metrics.

## Key Results
- Expressivity preservation at initialization is crucial for maintaining post-training accuracy in sparse GNNs
- SELTH proves existence of maximally expressive sparse subnetworks in sufficiently overparameterized moment-based GNNs
- Pre-training expressivity (measured before training) correlates linearly with post-training accuracy
- Irrecoverable expressivity loss occurs when pruning removes critical paths distinguishing SIFDGs

## Why This Works (Mechanism)

### Mechanism 1: Expressivity-Driven Gradient Diversity
- **Claim:** High pre-training expressivity enables higher gradient diversity, accelerating convergence and improving generalization
- **Mechanism:** Theorem 3.3 proves increased angle between node embeddings (orthogonality) boosts gradient diversity, which Yin et al. (2018) associate with faster convergence
- **Core assumption:** Correlation between gradient diversity and practical convergence/generalization holds for GNNs
- **Evidence anchors:** Section 3.2 (Theorem 3.3), Section 5 (training results), Corpus (LTH literature)
- **Break condition:** If node embeddings collapse into co-linear vectors (rare in overparameterized networks per Proposition 3.4)

### Mechanism 2: The Injectivity of Sparse Paths (SELTH)
- **Claim:** In sufficiently overparameterized moment-based GNNs, sparse subnetworks exist that preserve 1-WL expressivity
- **Mechanism:** Random pruning maintains injectivity with high probability if layer width is large enough (Lemma A.1), preserving the discriminative power of 1-WL test
- **Core assumption:** Network is sufficiently overparameterized relative to pruning ratio
- **Evidence anchors:** Section 3.1 (Theorem 3.2), Appendix A.1 (Lemma A.1), Corpus (LTH in CNNs/Transformers)
- **Break condition:** High pruning ratios or low width drop probability of maintaining injectivity near zero

### Mechanism 3: Irrecoverable Loss on SIFDGs
- **Claim:** Pruning that merges feature multisets of structurally isomorphic, feature-divergent graphs (SIFDGs) causes permanent expressivity loss
- **Mechanism:** SIFDGs rely entirely on feature differences for classification; if pruning forces $A_1X_1M = A_2X_2M$ (where $A_1=A_2$ but $X_1 \neq X_2$), distinction is erased before aggregation
- **Core assumption:** Graphs are structurally isomorphic (identical adjacency matrices up to permutation)
- **Evidence anchors:** Section 3.3 (Lemma 3.5), Figure 1 (Thalidomide example)
- **Break condition:** Applies strictly to first layer of first MP layer; deeper layers may recover distinctions if earlier layers preserved them

## Foundational Learning

- **Concept: The Weisfeiler-Leman (WL) Test**
  - **Why needed here:** WL test is the benchmark for expressivity; understanding SELTH requires knowing 1-WL is an iterative coloring algorithm for graph isomorphism testing
  - **Quick check question:** Can a standard GNN distinguish two graphs that the 1-WL test cannot? (Answer: Generally no, unless using specific extensions)

- **Concept: Moment-based GNNs (e.g., GIN)**
  - **Why needed here:** Theoretical proofs rely on specific aggregation mechanism (sum-pooling + MLP) being injective
  - **Quick check question:** Why does the paper focus on GIN rather than GCN for primary theoretical results? (Answer: GIN's sum-aggregation is theoretically injective, satisfying Lemma 2.1)

- **Concept: Injectivity**
  - **Why needed here:** Core of paper is proving sparse MLPs can remain injective functions
  - **Quick check question:** What is the risk if a pruned MLP loses its injectivity? (Answer: Two different neighborhoods get the same embedding, reducing expressivity)

## Architecture Onboarding

- **Component map:** Graphs $G=(V, E)$ with node features $X$ -> Moment-based GNN (e.g., GIN) with $k$ layers -> Layer $i$: Aggregation (Sum neighbors) -> MLP (Injective function) -> Pruning: Binary masks $M$ applied to weight matrices $W$

- **Critical path:** Identify SIFDGs in dataset (structurally identical graphs with different labels) -> Trace features that differ between SIFDGs -> Ensure pruning masks do not zero out weights connected to distinguishing features in first layer (critical paths)

- **Design tradeoffs:** Sparsity vs. Expressivity (higher pruning increases injectivity loss risk), Width vs. Sparsity (maintain high sparsity with increased layer width as probability of injectivity scales with $m$)

- **Failure signatures:** "Activity Cliff" Failure (performs well on average but fails on chemical stereoisomers or SIFDGs), Stagnant Accuracy (post-training accuracy remains low despite training, correlated with low pre-training expressivity)

- **First 3 experiments:**
  1. Calculate $\tau_{pre}$: Before training, calculate expressivity by checking if sparse GNN distinguishes all non-isomorphic graph pairs in representative dataset subset
  2. SIFDG Stress Test: Create synthetic dataset of pairs $(G, G')$ where $G \simeq G'$ but labels differ; verify if sparse model can overfit this small dataset
  3. Pruning Sweep: Vary pruning $\rho$ (10% to 90%); plot $\tau_{pre}$ vs. Test Accuracy to confirm linear correlation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does SELTH extend to GNN architectures that utilize attention mechanisms or edge features, and do these architectures introduce specific vulnerabilities to critical path pruning not found in moment-based models?
- **Basis in paper:** Authors state refining formal analysis to architectures beyond moment-based GNNs is a promising direction for future work
- **Why unresolved:** Theoretical proofs are restricted to moment-based GNNs with specific aggregation rules
- **What evidence would resolve it:** Formal extension of Theorem 3.2 to attention-based GNNs or empirical results comparing critical path behavior

### Open Question 2
- **Question:** How can theoretical bounds on achievable accuracy for misaligned pruning be refined to accommodate non-uniform class distributions present in realistic datasets?
- **Basis in paper:** Lemma 3.6 currently relies on uniform class distribution assumption
- **Why unresolved:** Lemma 3.6's reliance on uniform class distribution limits applicability to imbalanced real-world data
- **What evidence would resolve it:** Modified derivation of Lemma 3.6 incorporating class prior probabilities, validated by empirical pruning experiments on datasets with skewed class distributions

### Open Question 3
- **Question:** Can efficient sparsification algorithms be designed that proactively preserve maximally expressive paths during initialization, rather than relying on post-hoc pruning recovery?
- **Basis in paper:** Authors suggest pre-training sparsification strategy could proceed layer-wise but leave development for future work
- **Why unresolved:** Paper proves existence of expressive subnetworks but doesn't provide computationally efficient method to find them
- **What evidence would resolve it:** Proposal and benchmarking of "expressivity-preserving" pruning algorithm that converges faster or generalizes better than standard pruning

### Open Question 4
- **Question:** Do findings regarding expressivity loss and gradient diversity in sparse subnetworks transfer effectively to node-level classification tasks?
- **Basis in paper:** Authors state work focuses on graph level tasks but expect findings to be transferable to node level tasks
- **Why unresolved:** Theoretical analysis and empirical validation centered entirely on graph-level embeddings and tasks
- **What evidence would resolve it:** Empirical studies replicating experimental setup on standard node classification benchmarks

## Limitations
- Theoretical proofs apply only to moment-based GNNs with specific aggregation mechanisms, limiting generalizability to other architectures
- Irreversible expressivity loss mechanism applies strictly to first-layer pruning of SIFDGs, with limited exploration of recovery possibilities in deeper layers
- Claims about gradient diversity improving convergence are supported by correlation but lack direct mechanistic validation

## Confidence
- **High confidence:** SELTH's theoretical existence for moment-based GNNs given overparameterization requirements are met, supported by formal injectivity proofs
- **Medium confidence:** Practical relevance of gradient diversity as primary convergence driver, as empirical validation shows correlation but not causation
- **Medium confidence:** Irreversible loss mechanism for SIFDGs, demonstrated only for first-layer pruning and specific graph types

## Next Checks
1. **Gradient Diversity Experiment:** Train two sparse networks with matched parameter counts but different pre-training expressivity scores. Measure and compare training convergence speed, loss landscape smoothness, and generalization gap to validate gradient diversity hypothesis.

2. **Architecture Extension Validation:** Test SELTH's applicability to GCNs and GATs by attempting to prove injectivity under their aggregation schemes, or empirically measuring expressivity preservation across these architectures.

3. **SIFDG Recovery Experiment:** Design experiments where SIFDG distinctions are lost in layer 1 but potentially recoverable in deeper layers through structural information, testing whether expressivity can be restored through training despite initial irreversible loss.