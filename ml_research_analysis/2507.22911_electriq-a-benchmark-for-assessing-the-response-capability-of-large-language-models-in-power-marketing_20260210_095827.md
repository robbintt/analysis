---
ver: rpa2
title: 'ElectriQ: A Benchmark for Assessing the Response Capability of Large Language
  Models in Power Marketing'
arxiv_id: '2507.22911'
source_url: https://arxiv.org/abs/2507.22911
tags:
- power
- electriq
- dialogue
- energy
- seek-rag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ElectriQ is a large-scale benchmark and evaluation framework for
  large language models (LLMs) in electric power marketing (EPM). The dataset contains
  over 550,000 multi-turn dialogues across six service domains and 24 sub-scenarios,
  including sustainability-critical areas like time-of-use (TOU) tariffs, demand response
  (DR), and distributed energy resource (DER) interconnection.
---

# ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing

## Quick Facts
- arXiv ID: 2507.22911
- Source URL: https://arxiv.org/abs/2507.22911
- Authors: Jinzhi Wang; Qingke Peng; Haozhou Li; Zeyuan Zeng; Jiangbo Zhang; Kaixuan Yang; Ningyong Wu; Qinfeng Song; Ruimeng Li; Biyi Zhou
- Reference count: 6
- Primary result: Domain-aligned 7B models with SEEK-RAG match or surpass much larger models in EPM-specific performance while reducing computational cost.

## Executive Summary
ElectriQ is a large-scale benchmark and evaluation framework for large language models in electric power marketing (EPM). The dataset contains over 550,000 multi-turn dialogues across six service domains and 24 sub-scenarios, including critical sustainability areas like time-of-use tariffs, demand response, and distributed energy resource interconnection. A unified protocol combines human ratings on four dimensions (professionalism, clarity, actionability, empathy) with automatic metrics and two compliance stress tests—Statutory Citation Correctness (SCC) and Long-Dialogue Consistency (LDC). Experiments on 13 LLMs show that domain-aligned 7B models with the proposed SEEK-RAG retrieval-augmented method match or surpass much larger models in EPM-specific performance, reducing computational cost while improving regulatory compliance and long-turn dialogue stability.

## Method Summary
ElectriQ uses a two-stage training approach: Stage 1 applies QLoRA fine-tuning on a combined dataset of real reconstructed dialogues (188K) and knowledge-augmented synthetic dialogues (361K) with curriculum sampling. Stage 2 applies preference alignment via supervised fine-tuning followed by direct preference optimization on a curated 2,400-sample preference dataset. The SEEK-RAG inference pipeline uses scenario classification to filter a knowledge base, then retrieves relevant regulatory content using keyword coverage scoring before generating responses. Evaluation combines four human rating dimensions with automatic metrics and two compliance stress tests measuring citation accuracy and long-turn dialogue consistency.

## Key Results
- 7B models with SEEK-RAG achieve comparable or superior performance to 100B+ models on EPM-specific metrics
- SEEK-RAG significantly improves Statutory Citation Correctness (SCC) and Long-Dialogue Consistency (LDC) scores
- Two-stage training (SFT→DPO) yields super-additive gains for 7B models, closing performance gaps with larger models
- Real dialogue reconstruction captures authentic multi-turn customer service patterns across 24 EPM sub-scenarios

## Why This Works (Mechanism)

### Mechanism 1
Scenario-filtered retrieval with keyword coverage scoring improves regulatory compliance more than scaling model size alone. SEEK-RAG first predicts the service scenario using TinyBERT, restricts the knowledge base to scenario-specific sublibraries, then ranks candidates by keyword overlap with the query-response pair. The top-matched knowledge chunk is injected as conditioning for the final response. This grounds outputs in current tariff/regulatory text without requiring the model to memorize volatile policy details.

### Mechanism 2
Two-stage training (domain infusion via SFT, then preference alignment via DPO) yields super-additive gains for 7B models, closing the gap with 100B+ models. Stage 1 uses large-scale EPM dialogues with curriculum sampling (general → sustainability-focused). Stage 2 aligns on a curated 2,400-sample preference dataset scored on professionalism, clarity, actionability, empathy, plus SCC/LDC. The combination lets small models internalize domain patterns and learn to prefer citation-rich, regulation-consistent responses.

### Mechanism 3
Stress tests (SCC and LDC) capture compliance risks that generic metrics (BLEU, ROUGE) miss. SCC evaluates whether responses contain verifiable citations and whether key statements match cited clauses. LDC checks slot consistency and valid workflow transitions over 8–12 turn dialogues using a finite-state machine. Both aggregate into 0–100 scores with explicit weighting.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: SEEK-RAG is a domain-specific RAG variant; understanding general RAG patterns (retriever, reranker, generator) is prerequisite.
  - Quick check question: Can you explain how a standard RAG pipeline differs from fine-tuning a model on retrieved documents?

- **Preference Optimization (SFT → DPO pipeline)**
  - Why needed here: The two-stage training uses DPO to align model outputs with human-rated preferences; without this, you can't diagnose why Stage 2 matters.
  - Quick check question: What is the key difference between supervised fine-tuning and direct preference optimization in terms of training signal?

- **Domain-Specific Evaluation Design**
  - Why needed here: Generic metrics fail in regulated domains; understanding why BLEU/ROUGE underweight compliance is essential.
  - Quick check question: Why might a high BLEU score coexist with low regulatory compliance in a tariff consultation response?

## Architecture Onboarding

- **Component map:**
  Real Dialogue Reconstruction (188K) -> Knowledge-Augmented (361K) -> Preference-Aligned (2.4K) -> Two-stage Training (QLoRA+SFT+DPO) -> SEEK-RAG Inference (Scene Classifier -> Keyword Extractor -> Scenario-Filtered Retrieval -> Knowledge-Conditioned Generation) -> Evaluation (Human Ratings + Automatic Metrics + SCC/LDC Stress Tests)

- **Critical path:**
  1. Build/regionalize knowledge base with versioned tariff/regulatory documents
  2. Train TinyBERT scene classifier and BERT-keyword extractor on labeled EPM data
  3. Run Stage 1 QLoRA fine-tuning with curriculum sampling
  4. Build preference dataset with expert annotation on SCC/LDC
  5. Run Stage 2 DPO alignment
  6. Deploy SEEK-RAG with frozen retrieval index
  7. Monitor SCC/LDC on production samples

- **Design tradeoffs:**
  - 7B + SFT + SEEK-RAG vs. 100B+ frontier model: 7B is 10×+ cheaper but requires upfront domain engineering
  - Real vs. synthetic training data: Real data is noisier; synthetic (RAG-generated) improves coverage but risks distribution shift (paper shows <2.5% gap)
  - SCC weighting (0.7/0.2/0.1): Prioritizes content alignment over citation format; adjustable per regulatory context

- **Failure signatures:**
  - SCC drops sharply when knowledge base lags behind policy updates
  - LDC degrades on 9–12 turn dialogues without retrieval; SFT alone insufficient
  - Citation over-generation: model cites irrelevant clauses when scenario classifier misfires

- **First 3 experiments:**
  1. Baseline sweep: Run all 13 models on held-out test set with identical prompts; record BLEURT, SCC, LDC to establish Pareto frontier.
  2. Ablation on 7B: Compare Base → SFT → SEEK-RAG → SFT+SEEK-RAG for LLaMA3-8B/Mistral-7B to quantify super-additive gains.
  3. SCC sensitivity: Freeze knowledge base, inject deliberate version mismatches, measure SCC degradation to validate retrieval dependency.

## Open Questions the Paper Calls Out

- How well does the ElectriQ framework and SEEK-RAG approach transfer to power marketing systems in different regulatory jurisdictions (e.g., EU, US) with fundamentally different tariff structures, interconnection procedures, and compliance requirements? The paper exclusively uses Chinese 95598 data and Chinese regulatory documents with no cross-jurisdictional validation.

- Can SEEK-RAG's simple keyword-coverage retrieval scoring be improved through more sophisticated semantic retrieval methods (neural rerankers, dense passage retrieval) to achieve further compliance gains? The paper acknowledges this as a design choice rather than an optimized solution.

- How do models perform on conversations exceeding the 8–12 turn range tested in the LDC stress test, and what failure modes emerge in extended multi-session dialogues? The LDC test is explicitly limited to "8–12 turns" without characterization beyond this range.

## Limitations

- The SEEK-RAG retrieval index and EPM knowledge base are not publicly available, making SCC/LDC stress test replication difficult without significant additional effort.
- No external validation exists for the SCC/LDC compliance scoring methodology or FSM transitions; scores may not generalize beyond this paper's annotators.
- Two-stage training (SFT→DPO) gains are shown only within ElectriQ; cross-dataset replication is needed to confirm super-additive effects aren't dataset-specific.

## Confidence

- **High:** Real-world dialogue reconstruction pipeline, multi-turn dialogue dataset construction, SEEK-RAG retrieval architecture, SCC/LDC scoring formulas
- **Medium:** Two-stage training efficacy (SFT→DPO), SEEK-RAG compliance improvements, LDC FSM coverage, human rating consistency
- **Low:** Cross-dataset generalization of super-additive gains, SCC/LDC external validity, preference dataset representativeness

## Next Checks

1. **Replicate 7B gains:** Run LLaMA3-8B and Mistral-7B with Base→SFT→SEEK-RAG→SFT+SEEK-RAG on ElectriQ test set; confirm super-additive SCC/LDC improvement matches Table 3.

2. **Stress test transferability:** Apply SCC/LDC to a held-out regulatory compliance dataset (e.g., legal Q&A corpus); check if high BLEURT correlates with high SCC/LDC or if they diverge.

3. **Ablation of knowledge base freshness:** Simulate policy updates by injecting outdated clauses into the knowledge base; measure SCC degradation to bound retrieval dependency.