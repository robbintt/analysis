---
ver: rpa2
title: Better Language Model Inversion by Compactly Representing Next-Token Distributions
arxiv_id: '2506.17090'
source_url: https://arxiv.org/abs/2506.17090
tags:
- language
- prompt
- pils
- should
- inverter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Language model inversion seeks to recover hidden prompts from\
  \ a language model\u2019s outputs, with implications for security and accountability\
  \ in model deployments. This work addresses the challenge of recovering hidden prompts\
  \ by leveraging next-token probabilities across multiple generation steps."
---

# Better Language Model Inversion by Compactly Representing Next-Token Distributions

## Quick Facts
- **arXiv ID**: 2506.17090
- **Source URL**: https://arxiv.org/abs/2506.17090
- **Authors**: Murtaza Nazir; Matthew Finlayson; John X. Morris; Xiang Ren; Swabha Swayamdipta
- **Reference count**: 40
- **Primary result**: PILS achieves 2–3.5× higher exact recovery rates than previous methods by compactly representing next-token distributions

## Executive Summary
Language model inversion seeks to recover hidden prompts from a language model's outputs, with implications for security and accountability in model deployments. This work addresses the challenge of recovering hidden prompts by leveraging next-token probabilities across multiple generation steps. The core method, Prompt Inversion from Logprob Sequences (PILS), is enabled by the insight that language model outputs occupy a low-dimensional subspace, allowing lossless compression of full next-token probability distributions using a linear map. This compression reduces the representation size from vocabulary-sized vectors to the model's embedding size, enabling efficient use of multiple generation steps.

PILS achieves massive gains over previous state-of-the-art methods, with exact recovery rates 2–3.5 times higher across test sets—for example, increasing exact recovery from 17% to 60% in one case. The method also exhibits strong generalization, with trained inverters continuing to improve as the number of generation steps increases at test time. Additionally, PILS demonstrates strong performance on recovering hidden system messages and introduces a novel method for cross-family model transfer. These findings show that next-token probabilities are a significantly more vulnerable attack surface for inversion than previously known.

## Method Summary
The method compresses vocabulary-sized logprob vectors to embedding-sized vectors using the additive log-ratio transform and linear algebra. The key insight is that softmax(logit(Wh)) followed by alr transform is linear in hidden state h. By selecting D indices from the probability simplex, the method achieves lossless compression when the corresponding submatrix has full rank. The compressed representations are then fed into a T5-base encoder-decoder with an adapter layer to recover the original prompt. The training uses 2M Instructions dataset with precomputed compressed representations, requiring approximately 500GB of storage for 16-step generations.

## Key Results
- PILS achieves exact recovery rates 2–3.5× higher than previous methods (e.g., 17% → 60% on some test sets)
- Inverters trained on 16 steps improve by 5–27 points when tested on 32 steps, demonstrating strong sequence-length generalization
- Cross-family model transfer works, though performance degrades compared to same-family transfer
- System message recovery is more challenging than user prompt recovery, suggesting post-training specifically obscures system messages

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Structure of Language Model Output Space
Language model outputs are linear projections of the final hidden state through the unembedding matrix W (logits = Wh). The softmax function is linear when treating the probability simplex as a vector space with appropriate algebra. The additive log-ratio (alr) transform maps probabilities back to standard vector space linearly. Therefore alr(softmax(Wh))_D is a linear transformation of hidden state h, and selecting D indices recovers all information. The core assumption is that the submatrix A_D formed by selecting D rows from the combined linear map has full rank; using random D+100 tokens rather than first D avoids near-linear-dependence in embeddings.

### Mechanism 2: Multi-Step Information Leakage via Prompt Echoing
During generation, chat models paraphrase prompt content while base models echo verbatim. Information about later prompt tokens may not appear in early generation steps. By observing logprob sequences across T steps, the inverter captures which prompt tokens the model has implicitly referenced at each position. The core assumption is that target model's generation reflects prompt content within observable steps; post-trained models trained to avoid verbatim repetition are harder to invert.

### Mechanism 3: Sequence-Length Generalization via Position-Agnostic Representations
T5 uses relative position embeddings without position-specific weights. The inverter learns to process hidden-state sequences without binding to absolute positions, allowing extrapolation. Performance improves with more steps until saturation. The core assumption is that relative position embeddings enable extrapolation; training on more steps confers advantage even when test steps exceed training steps.

## Foundational Learning

- **Probability Simplex as Vector Space**: Understanding why alr transform enables lossless linear compression requires recognizing the simplex has non-standard but valid vector space structure. Quick check: If p and q are probability vectors, what is p +_Δ q?
- **Linear Layers and Rank Preservation**: Compression works only if A_D has full rank; selecting inappropriate token indices causes information loss. Quick check: Given a D×V matrix W and selecting D rows, what condition ensures rank D?
- **Encoder-Decoder Sequence-to-Sequence Architecture**: The inverter uses T5 encoder-decoder; understanding how variable-length sequences map through encoder to decoder is essential. Quick check: How does an encoder-decoder model handle inputs longer than training sequences?

## Architecture Onboarding

- **Component map**: Target LM → logprobs (V-dim per step, T steps) → alr transform + index selection → compressed vectors (D-dim per step) → feed-forward adapter (with dropout, GELU) → T5-base encoder → T5-base decoder → recovered prompt text
- **Critical path**: 
  1. Token index selection: Choose D+100 random tokens from vocabulary (avoid first D)
  2. Precompute alr(p) for selected indices for each training sample
  3. Initialize T5-base with adapter layer
  4. Train 100 epochs with learning rate 2e-4, batch size 250, AdamW
  5. Evaluate on held-out prompts using BLEU, exact match, token F1
- **Design tradeoffs**: More generation steps → higher recovery but more API cost and storage; PILS_16 costs ~$5.50 for 16-token sequence on GPT-4.1 Mini. D+100 random tokens vs. first D tokens: random selection avoids linear dependence but requires precomputation. Fine-tuning only attention layers vs. full model: attention-only fine-tuning prevents overfitting on small datasets.
- **Failure signatures**: Exact match near 0% but BLEU > 50: Model captures semantics but not verbatim prompt. Base model inversion much stronger than chat: Post-training suppresses echoing. Transfer to new model family fails: Inverter overfits to source model features. System prompt recovery much lower than user prompts: Post-training specifically obscures system messages.
- **First 3 experiments**:
  1. Verify compression losslessness: Compare PILS_1^1 vs. L2T on same test set; expect similar performance (theoretical equivalence)
  2. Scale generation steps: Train inverters on 1, 8, 16, 32 steps; evaluate each on 1-128 test steps to plot generalization curves
  3. Probe echo mechanism: Visualize token recovery probability across steps (Figure 3 style) for prompts with/without echo in generated text

## Open Questions the Paper Calls Out

### Open Question 1
What architectural improvements to the inverter design (e.g., larger feed-forward adapter hidden size, larger inverter backbone) would better saturate language model inversion performance? The authors state they "do not believe that we have fully saturated this task" and suggest "our inverter design might be improved, for instance, by using a more expressive feed forward adapter with a larger hidden size."

### Open Question 2
Why does PILS underperform in model transfer settings compared to text-based methods like O2P, and how can this "target specificity" be mitigated? The authors observe that "impressive gains of PILS in non-transfer settings fail to materialize in the model transfer setting" and "speculate this could be due to the target specificity of our inverter."

### Open Question 3
How would a large-scale, diverse, high-quality (non-synthetic) dataset of system prompts improve system message inversion performance? The authors state "progress on system message inversion can be greatly improved through the construction of a large-scale, diverse, high-quality (i.e., non-synthetic) dataset of system prompts."

### Open Question 4
What is the theoretical and empirical upper bound on inversion performance as the number of generation steps increases, and where does performance saturate? The authors observe inverters "continue to improve even when the number of steps surpasses the number of steps they were trained on, though the effect eventually saturates" (Figure 4), but the saturation point and underlying mechanism are not characterized.

## Limitations
- Performance depends on post-training behavior; models with stronger anti-echo training may be more resistant
- Compression quality relies on token selection; pathological cases with near-linearly-dependent embeddings could cause information loss
- Cross-family transfer shows promise but degrades significantly compared to same-family performance

## Confidence

**High confidence**: The multi-step information leakage mechanism and sequence-length generalization are well-supported by empirical evidence. Figure 3 clearly demonstrates how tokens become recoverable across generation steps, and Figure 4 provides convincing evidence of extrapolation capability.

**Medium confidence**: The performance improvements over previous state-of-the-art methods are substantial but evaluated primarily on a single training corpus (2M Instructions). The claim of "massive gains" (2-3.5× exact recovery) needs validation on more diverse datasets and real-world deployment scenarios.

**Low confidence**: The cross-family model transfer results show promise but are limited to 1-2 model pairs. The claim that PILS generalizes across model families requires more extensive validation, particularly for models with different architectural choices.

## Next Checks

**Check 1: Compression robustness under token selection variability** - Systematically test PILS performance using different random token subsets (varying seeds, selection strategies) to quantify the sensitivity of compression quality to index selection. This validates whether the D+100 random selection heuristic is robust or if specific token subsets yield significantly better performance.

**Check 2: Extrapolation limits for sequence length** - Train inverters on 1, 8, 16, 32, and 64 generation steps, then evaluate each on sequences ranging from 1 to 256 steps. This will identify the point at which generalization breaks down and whether there's a fundamental limit to the extrapolation capability.

**Check 3: Security impact of partial recovery** - Design experiments that measure the utility of partially recovered prompts for downstream attacks (e.g., prompt injection, jailbreaking). This validates whether the high BLEU scores with low exact match (indicating semantic but not verbatim recovery) pose meaningful security risks.