---
ver: rpa2
title: On the Role of Temperature Sampling in Test-Time Scaling
arxiv_id: '2510.02611'
source_url: https://arxiv.org/abs/2510.02611
tags:
- scaling
- temperature
- reasoning
- sampling
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how temperature sampling affects test-time
  scaling (TTS) in large language models. TTS improves reasoning by generating multiple
  reasoning traces and selecting the best one, but prior work scaling only the number
  of samples K shows diminishing returns: accuracy plateaus and some hard problems
  remain unsolved regardless of how many traces are drawn.'
---

# On the Role of Temperature Sampling in Test-Time Scaling

## Quick Facts
- arXiv ID: 2510.02611
- Source URL: https://arxiv.org/abs/2510.02611
- Reference count: 40
- Primary result: Multi-temperature scaling improves TTS accuracy by 7.3 percentage points over single-temperature scaling

## Executive Summary
This paper demonstrates that temperature sampling is a critical, yet underexplored, dimension for improving test-time scaling (TTS) in large language models. While prior work focused on scaling the number of reasoning traces (K) at a fixed temperature, the authors show this approach hits diminishing returns because different temperatures solve different subsets of problems. By scaling across temperatures rather than just K, the reasoning boundary is enlarged, allowing base models to match performance of RL-trained counterparts without post-training. The authors propose an efficient multi-temperature voting method that exits early on easy questions, reducing computation while maintaining accuracy gains.

## Method Summary
The study investigates how varying sampling temperature during TTS affects reasoning performance across five benchmarks. Experiments use Qwen3 models (0.6B-8B) and generate 1,024 traces per temperature across T ∈ {0.0, 0.1, ..., 1.2}. The core finding is that different temperatures solve different problem subsets, with "easy" problems solvable at any temperature while "hard" problems require specific temperature ranges. To make multi-temperature scaling efficient, the authors implement a two-stage voting mechanism: intra-temperature consistency checking (τ_intra=0.8) followed by cross-temperature consensus (τ_cross=1.0), enabling early exit on easy problems.

## Key Results
- Multi-temperature scaling achieves 7.3 percentage points additional accuracy over single-temperature TTS
- Base models using multi-temperature scaling match performance of RL-trained counterparts
- Voting-based early exit reduces computation by exiting early on easy problems while preserving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Solution Space Coverage via Temperature Diversity
Different temperatures explore different regions of the model's latent reasoning capability. The paper suggests that "easy" problems are solved by all temperatures, but specific "hard" problems are only solvable within narrow temperature ranges. By sweeping T, the system increases the probability of intersecting these sparse solution regions.

### Mechanism 2: Bifurcation of Easy vs. Hard Problem Dynamics
"Easy" problems lie in high-probability basins where the model is confident regardless of T. "Hard" problems require specific stochastic perturbations (specific T values) to escape local optima or initiate correct reasoning chains. This separation allows the system to detect and exit easy problems early, reserving multi-temperature compute for the hard subset.

### Mechanism 3: Entropy-Consistency Discrepancy
While aggregate analysis suggests correct traces have lower entropy at high T, this pattern is driven by easy problems. For hard problems, correct traces do not reliably exhibit low entropy. Therefore, relying solely on uncertainty (entropy) to filter traces is insufficient; explicit consensus (voting) across temperatures is required to identify valid solutions.

## Foundational Learning

- **Concept: Test-Time Scaling (TTS)** - Why needed: Core paradigm where inference compute trades for accuracy by generating multiple traces and selecting best via verification. Quick check: How does Pass@K differ from Avg@N, and why does Pass@K capture the "reasoning boundary" better?

- **Concept: Temperature Sampling (T)** - Why needed: Temperature is the independent variable being studied. Quick check: If T → 0, the model becomes deterministic. If T → ∞, what happens to the probability distribution?

- **Concept: Self-Consistency / Majority Voting** - Why needed: The proposed efficiency mechanism uses voting to decide when to stop computing. Quick check: Why is voting considered a "weak verifier" compared to a trained Reward Model or ground-truth oracle?

## Architecture Onboarding

- **Component map**: Sampler (vLLM) -> Candidate Pools (per-temperature buffers) -> Voting Logic (Intra-Temp → Cross-Temp) -> Controller (manages Active state)

- **Critical path**: 1) Initialize pools for question q at all target temperatures. 2) Round Loop: Generate 1 trace per temperature. 3) Intra-Temp Check: Does majority answer exist within each temperature's pool (τ_intra=0.8)? 4) Cross-Temp Check: Do majority answers from all temperatures agree (τ_cross=1.0)? 5) Exit: If yes, mark q as "easy" and stop; else, continue loop.

- **Design tradeoffs**: Compute vs. Boundary (M× compute for M temperatures), Recall vs. Precision (strict thresholds ensure high confidence but may force unnecessary computation), Verifier Dependency (voting vs. ground truth).

- **Failure signatures**: Premature Exit (high consensus on incorrect answer), Infinite Loop (valid paths too diverse to reach consensus), Trace Invalidity (scaling K at high T may produce correct final answers via invalid reasoning).

- **First 3 experiments**: 1) Reproduce the Plateau: Run Qwen3-4B on AIME 2025 at T=0.7, plot Pass@K from K=1 to 1024. 2) Temperature Sweep: Same setup, split 1024 budget across T ∈ [0.5, 0.7, 0.9, 1.1] (256 each), measure Pass@All. 3) Efficiency Validation: Implement Intra/Cross voting logic, measure average samples per question before exit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does dynamically varying temperature within a single reasoning trace further expand the reasoning boundary of LLMs?
- Basis: Section 5.2 states: "An interesting direction for future work is to study how variable temperature within a single trace may further influence reasoning ability."

### Open Question 2
- Question: Can a universal entropy-based early-exit strategy be developed that generalizes across diverse reasoning domains?
- Basis: Section 5.2 notes: "entropy distributions vary significantly across datasets and domains, making it difficult to design a universal entropy-based eviction strategy."

### Open Question 3
- Question: What underlying mechanisms cause specific problems to be solvable only at particular temperatures?
- Basis: The paper empirically demonstrates the phenomenon but does not fully explain the cognitive or representational factors driving it.

## Limitations
- Assumes problems are either "easy," "hard," or "impossible" without rigorous quantitative definitions
- Relies heavily on ground-truth verification for evaluation, limiting real-world applicability without oracle access
- Focuses on reasoning benchmarks where structured solution paths exist; temperature-solution relationship may not hold for open-ended or creative tasks

## Confidence
- **High Confidence**: The 7.3 percentage points improvement from multi-temperature scaling is well-supported by experimental results across five benchmarks and three model sizes
- **Medium Confidence**: The theoretical mechanism explaining temperature diversity solving different problem subsets is plausible but relies on qualitative interpretations
- **Low Confidence**: The entropy-consistency discrepancy mechanism shows mixed results and may introduce its own biases

## Next Checks
1. **Robustness to Problem Categories**: Test whether the temperature-solution relationship holds for non-mathematical domains (commonsense reasoning, multi-hop QA) where solution paths may be more diverse
2. **Verifier-Free Performance**: Implement multi-temperature voting without ground-truth access using a weak verifier and measure accuracy degradation compared to oracle-based evaluation
3. **Temperature Subset Optimization**: Systematically identify minimal temperature subsets that capture most performance gains to reduce computational overhead