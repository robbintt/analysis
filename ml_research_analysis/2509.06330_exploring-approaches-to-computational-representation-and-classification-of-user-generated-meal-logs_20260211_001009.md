---
ver: rpa2
title: Exploring approaches to computational representation and classification of
  user-generated meal logs
arxiv_id: '2509.06330'
source_url: https://arxiv.org/abs/2509.06330
tags:
- goals
- nutritional
- enrichment
- food
- meal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed machine learning models to classify whether\
  \ free-text meal logs align with nutritional goals, using embeddings and domain-specific\
  \ enrichment. The models, evaluated against registered dietitian assessments, achieved\
  \ accuracies of 0.726\u20130.841 without enrichment and up to 0.902 with enrichment,\
  \ outperforming individuals' self-assessments (0.576 accuracy)."
---

# Exploring approaches to computational representation and classification of user-generated meal logs

## Quick Facts
- arXiv ID: 2509.06330
- Source URL: https://arxiv.org/abs/2509.06330
- Reference count: 0
- Primary result: ML classifiers achieved 0.726–0.902 accuracy in classifying meal-goal alignment, outperforming self-assessment (0.576)

## Executive Summary
This study developed machine learning models to classify whether free-text meal logs align with nutritional goals, using embeddings and domain-specific enrichment. The models, evaluated against registered dietitian assessments, achieved accuracies of 0.726–0.841 without enrichment and up to 0.902 with enrichment, outperforming individuals' self-assessments (0.576 accuracy). ML classifiers with Parsed Ingredients, Food Entities, and Macronutrients enrichment performed consistently well across goals. Findings demonstrate that unstructured PGHD can be reliably analyzed with ML and domain knowledge to support personalized nutrition guidance.

## Method Summary
The study used 3,169 English meal records from 84 participants, each with meal title and free-text description. Registered dietitians provided binary labels for four nutritional goals: "drink water," "lean protein," "half fruits and vegetables," and "one-fourth carbohydrates." Text preprocessing included lowercasing, spell correction, stop-word removal, and punctuation filtering. Two embedding approaches were tested: TF-IDF (sparse) and BERT-base (dense). Two classifiers were evaluated: Logistic Regression and Multilayer Perceptron. Domain-specific enrichment via Nutritionix API provided food entities, parsed ingredients, and macronutrient content, while FoodOn ontology added hierarchical food concepts. Models were trained on 80% of data and tested on 20%.

## Key Results
- ML classifiers achieved 0.726–0.841 accuracy without enrichment and up to 0.902 with enrichment
- Outperformed self-assessment accuracy (0.576) across all goals
- ML classifiers with Parsed Ingredients, Food Entities, and Macronutrients enrichment performed consistently well across multiple nutritional goals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text embeddings transform unstructured meal descriptions into numerical representations that capture features relevant for goal-alignment classification.
- Mechanism: TF-IDF creates sparse, frequency-weighted vectors reflecting lexical importance; BERT generates dense, contextualized embeddings capturing semantic relationships between words.
- Core assumption: Free-text descriptions contain sufficient information to infer goal alignment, even without portion quantities.
- Evidence anchors:
  - [abstract]: "Using text embeddings, including TFIDF and BERT...as inputs, we evaluated the performance of logistic regression and multilayer perceptron classifiers"
  - [section]: "BERT is a pre-trained language model that captures contextual relationships between words by considering both left and right context"
  - [corpus]: Weak direct support—neighboring papers focus on multimodal nutrition estimation, not text embedding comparison for meal classification
- Break condition: Meal descriptions lack detail (e.g., "lunch" alone) or critical attributes like portion size for quantitative goals.

### Mechanism 2
- Claim: Domain-specific enrichment augments raw text with structured nutritional knowledge, improving classification accuracy.
- Mechanism: Nutritionix API extracts food entities, parsed ingredients, and macronutrient content; FoodOn ontology adds hierarchical food concepts. These enrichments provide explicit signals (e.g., protein grams) that raw text alone may not convey.
- Core assumption: Enrichment extraction is accurate and ontology coverage matches logged foods.
- Evidence anchors:
  - [abstract]: "domain-specific enrichment information, including ontologies, ingredient parsers, and macronutrient contents as inputs"
  - [section]: "ML classifiers with enrichment of Parsed Ingredients, Food Entities, and Macronutrients information performed well across multiple nutritional goals"
  - [corpus]: "Food Data in the Semantic Web" reviews FoodOn and nutritional knowledge graphs for food data representation
- Break condition: API misidentifies foods; ontology lacks cultural/ regional items; cascading errors from parser mistakes.

### Mechanism 3
- Claim: Supervised classifiers trained on expert-labeled data can generalize to predict meal-goal alignment for new entries.
- Mechanism: Logistic Regression learns linear decision boundaries; Multilayer Perceptron captures non-linear patterns. Both are trained on RD binary labels as gold standard.
- Core assumption: RD assessments reflect true goal alignment and training data distribution approximates deployment.
- Evidence anchors:
  - [abstract]: "Registered dietitians provided expert judgement for meal to goal alignment, used as gold standard for evaluation"
  - [section]: "a team of six registered dietitians used a unified protocol to evaluate each logged meal and assign a binary label"
  - [corpus]: Weak—no direct corpus support for RD-labeling methodology in meal classification
- Break condition: Inter-rater variability among RDs; goal definitions remain ambiguous despite protocols.

## Foundational Learning

- Concept: **TF-IDF vs. Contextual Embeddings (BERT)**
  - Why needed here: Understanding trade-offs between sparse frequency-based features and dense semantic representations for noisy PGHD.
  - Quick check question: For "grilled chicken salad," would TF-IDF or BERT better capture that "grilled" implies a leaner preparation method?

- Concept: **Ontology-Based Enrichment**
  - Why needed here: Leveraging structured knowledge hierarchies to compensate for missing or implicit information in free text.
  - Quick check question: How would FoodOn's hierarchical structure help classify "corn flakes" versus "corn on the cob" for a "whole grains" goal?

- Concept: **Class Imbalance and Evaluation Metrics**
  - Why needed here: The "one fourth carbohydrates" goal showed accuracy up to 0.784 but F1 as low as 0.000, indicating potential class imbalance or threshold issues.
  - Quick check question: If 90% of meals don't meet a goal, what metric besides accuracy should you prioritize?

## Architecture Onboarding

- Component map:
  Input: Meal title + free-text description (concatenated)
  Preprocessing: Lowercase, spell correction, stop-word removal, punctuation filtering
  Enrichment: Nutritionix API → food entities, parsed ingredients, macronutrients (text + numeric); FoodOn → ontological concepts
  Embedding: TF-IDF (sparse) or BERT-base (dense, 768-dim)
  Classifier: Logistic Regression (linear) or MLP (1+ hidden layers, ReLU, SGD)
  Output: Binary prediction (meets/doesn't meet nutritional goal)

- Critical path:
  1. Text preprocessing quality → enrichment extraction accuracy
  2. Enrichment coverage → embedding informativeness
  3. Embedding + classifier choice → final accuracy/F1 trade-off

- Design tradeoffs:
  - TF-IDF vs. BERT: Computational cost vs. semantic nuance
  - LR vs. MLP: Interpretability vs. capacity for complex patterns
  - Goal-agnostic vs. goal-specific models: Scalability vs. tailored accuracy

- Failure signatures:
  - High accuracy, near-zero F1: Likely class imbalance; model predicting majority class
  - Enrichment returns empty: Food item not in Nutritionix database
  - Performance drops on quantitative goals: Missing portion/quantity signals in text

- First 3 experiments:
  1. Baseline replication: No enrichment + TF-IDF + LR across all 4 goals; confirm accuracy ranges (0.726–0.841) match paper
  2. Enrichment ablation: Test Parsed Ingredients, Food Entities, and Macronutrients Text individually; measure delta per goal
  3. Best-configuration validation: Combine top enrichments (Food Entities & Macronutrients Text) with BERT + MLP; target ≥0.84 accuracy for "lean protein" goal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can modular or multitask architectures be developed that retain the scalability of general models while incorporating goal-specific nuances for meal-goal classification?
- Basis in paper: [explicit] The authors state: "A key direction for future work lies in hybrid modeling strategies: can we develop modular or multitask architectures that retain the scalability of general models while incorporating goal-specific nuances?"
- Why unresolved: The study found substantial variability in enrichment effectiveness across nutritional goals, with no single approach performing optimally across all goals. This creates a tension between generalizable (easier to scale) and goal-specific (more accurate) solutions.
- What evidence would resolve it: Comparative evaluation of modular/multitask architectures against both general and goal-specific single-task models, measuring accuracy, scalability, and computational efficiency across diverse nutritional goals.

### Open Question 2
- Question: How well do these meal classification models generalize to multilingual populations and culturally diverse food contexts?
- Basis in paper: [explicit] The authors acknowledge: "Our model was trained exclusively on English free-text entries, so its adaptability to multilingual or culturally diverse populations requires further exploration."
- Why unresolved: The dataset was limited to English entries from a specific urban US community, yet the original dataset included Spanish submissions that were excluded from analysis.
- What evidence would resolve it: Model performance evaluation on non-English meal logs and culturally distinct food descriptions, potentially requiring multilingual embeddings or language-specific enrichment sources.

### Open Question 3
- Question: Can incorporating interpretability mechanisms into meal-goal classifiers identify specific meal features driving misalignment and generate actionable modification suggestions?
- Basis in paper: [explicit] The authors propose: "Adding interpretability mechanisms and analysis to the classifier used in this study can help to identify meal features that contribute to misalignment with goals and suggest ways to change meals to improve meal-goal alignment."
- Why unresolved: Current models provide binary classifications without explanatory feedback, limiting their utility for behavioral guidance.
- What evidence would resolve it: Development and user testing of interpretable models that surface specific features (e.g., "high sodium from processed meat") and evaluate whether such explanations improve users' dietary decision-making.

### Open Question 4
- Question: Would multi-class classification frameworks capture nutritional goal alignment more completely than the binary approach used in this study?
- Basis in paper: [explicit] The authors note: "We used a binary classification framework for determining whether a meal meets a goal or not, although multi-class classification may capture more complete nutritional goals."
- Why unresolved: Binary classification collapses nuanced alignment (e.g., partially meeting goals, exceeding goals) into a single threshold, potentially losing actionable information.
- What evidence would resolve it: Comparison of binary vs. multi-class (e.g., "fully aligned," "partially aligned," "not aligned") classification schemes against dietitian assessments with graded judgments.

## Limitations
- Dataset not publicly available, preventing independent replication
- Class imbalance, particularly for quantitative goals like "one-fourth carbohydrates" (accuracy up to 0.784, F1 as low as 0.000)
- Nutritionix API dependency introduces risks of missing or incorrect enrichment due to coverage gaps or parsing errors

## Confidence

- **High confidence**: ML classifiers outperform self-assessment (0.576 baseline) across all goals; enrichment improves accuracy for most goals; top configurations (Food Entities + Macronutrients Text with BERT + MLP) consistently achieve ≥0.84 accuracy for "lean protein" goal.
- **Medium confidence**: Generalization to unseen meal logs is plausible given RD gold standard, but inter-rater variability and ambiguous goal definitions may limit robustness.
- **Low confidence**: F1 scores for complex quantitative goals (e.g., "one-fourth carbohydrates" F1 as low as 0.000) suggest significant limitations; accuracy improvements with enrichment may be dataset-specific.

## Next Checks
1. Replicate baseline results (no enrichment, TF-IDF + LR) on a held-out test set; verify accuracy ranges (0.726–0.841) and inspect per-class F1 for class imbalance.
2. Perform enrichment ablation: isolate Parsed Ingredients, Food Entities, and Macronutrients Text; measure individual and combined impact on F1 for each goal.
3. Test model robustness on synthetic or alternative meal logs with missing or ambiguous information (e.g., no portion size, generic terms); evaluate degradation in accuracy and F1.