---
ver: rpa2
title: Convolutional-neural-operator-based transfer learning for solving PDEs
arxiv_id: '2512.17969'
source_url: https://arxiv.org/abs/2512.17969
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying convolutional neural
  operators (CNOs) for few-shot transfer learning in solving partial differential
  equations (PDEs). While CNOs have shown high accuracy in learning solution operators,
  their performance degrades when applied to new physical systems without adaptation.
---

# Convolutional-neural-operator-based transfer learning for solving PDEs

## Quick Facts
- arXiv ID: 2512.17969
- Source URL: https://arxiv.org/abs/2512.17969
- Reference count: 40
- Key outcome: Neuron Linear Transformation (NLT) strategy achieves highest accuracy for few-shot transfer learning of convolutional neural operators across three PDE systems

## Executive Summary
This paper addresses the challenge of applying convolutional neural operators (CNOs) for few-shot transfer learning in solving partial differential equations (PDEs). While CNOs have shown high accuracy in learning solution operators, their performance degrades when applied to new physical systems without adaptation. The authors propose a transfer learning framework where a CNO is pre-trained on a source dataset and then adapted to a small target dataset using three strategies: fine-tuning, low-rank adaptation (LoRA), and neuron linear transformation (NLT). The framework is evaluated on three PDEs: the Kuramoto-Sivashinsky equation, the Brusselator diffusion-reaction system, and the Navier-Stokes equations. The NLT strategy consistently outperforms the others, achieving the lowest test errors and demonstrating robustness to the magnitude of distribution shifts between source and target datasets.

## Method Summary
The method involves pre-training a CNO on a source dataset with simpler physical parameters (e.g., lower nonlinearity K=6 or higher viscosity ν=5×10⁻⁴) using 512 samples. The pre-trained model is then adapted to a target dataset with more complex parameters (e.g., K=8/12 or ν=1×10⁻⁴) using only 16 samples. Three transfer strategies are compared: fine-tuning (updating decoder layers), LoRA (adding low-rank matrices to convolutional kernels), and NLT (applying per-channel affine transformations to source kernels). The NLT approach modifies each convolutional kernel W as W_t = f × W_s + b, learning only the scaling factor f and bias b. The framework uses AdamW optimizer with learning rate 10⁻³, batch size 16, and step decay scheduling. Performance is evaluated using relative L¹ error on held-out test sets.

## Key Results
- NLT achieves lowest test errors across all three PDEs: KS (0.40% vs 0.81% LoRA), Brusselator (0.60% vs 0.88%), Navier-Stokes (3.71% vs 3.96%)
- NLT demonstrates MMD-insensitive adaptation: test errors remain stable (KS: 0.39%→0.42%) while LoRA error doubles (0.80%→1.65%) as distribution shift increases
- Transfer learning with NLT enables accurate surrogate modeling with minimal data dependency, outperforming supervised learning from scratch with n_t ≥ 16 samples
- The framework maintains resolution-invariance and alias-free properties through band-limited function space constraints

## Why This Works (Mechanism)

### Mechanism 1: Neuron Linear Transformation (NLT) for Parameter-Efficient Domain Adaptation
- **Core assumption**: Source model captures transferable feature representations, and domain shift can be modeled through per-channel scaling and translation of convolutional kernels
- **Evidence**: NLT consistently achieves lowest relative errors across all tested PDEs and shift magnitudes
- **Break condition**: May fail for fundamental physics changes rather than parameter variations within same equations

### Mechanism 2: Continuous-Discrete Equivalence in CNO Architecture
- **Core assumption**: PDE solutions lie within band-limited subspaces, and preserving frequency content is essential for accurate operator approximation
- **Evidence**: Formal definition of band-limited operations and U-Net-like architecture with ResNet blocks
- **Break condition**: Assumes periodic boundary conditions and rectangular domains, limiting extension to complex geometries

### Mechanism 3: Resilience to Distribution Shift via MMD-Insensitive Adaptation
- **Core assumption**: NLT's per-channel affine transformation provides distribution alignment robust to scale of domain discrepancy
- **Evidence**: Systematic comparison across 6 transfer scenarios shows consistent NLT advantage with stable test errors across varying MMD
- **Break condition**: May not hold for shifts involving boundary conditions, domain geometry, or fundamental equation structure changes

## Foundational Learning

- **Neural Operators**: Learning operators that map function spaces (initial conditions → solutions) rather than discretized vectors. Quick check: Explain why learning an operator differs from learning a function approximator and what advantages it provides for PDE solving.
- **Transfer Learning Paradigm**: Pre-training on source dataset D_s and adapting to small target dataset D_t. Quick check: What is the key difference between fine-tuning, LoRA, and NLT in terms of parameters they modify and trainable parameters required?
- **Band-Limited Functions and Continuous-Discrete Equivalence**: CNO's foundation in band-limited function spaces and equivalence between continuous operators and discrete representations. Quick check: What is the practical implication of enforcing band limits in CNO architecture, and what type of functions might it fail to represent accurately?

## Architecture Onboarding

- **Component map**: Input u₀(x) [128×128 matrix] → Lifting Layer P(·) → CNO Blocks [L layers] → Skip Connections (ResNet + Invariant blocks) → Projection Layer Q(·) → Output û(x,T) [128×128 matrix]
- **Critical path**: Pre-train CNO on source dataset (512 samples, K=6 or ν=5×10⁻⁴) → Freeze backbone → Apply NLT by adding learnable f and b to each convolutional kernel → Train only f, b on target dataset (16 samples, K=8/12 or ν=3×10⁻⁴/1×10⁻⁴) → Compare relative L¹ test error against baselines
- **Design tradeoffs**: NLT modifies existing kernel weights via affine transform (2c parameters per layer) vs LoRA adds low-rank matrices BA (r(c+h+w) parameters); NLT more parameter-efficient but less expressive for non-linear transformations
- **Failure signatures**: Without transfer, error increases 5-10×; LoRA instability under large shift shows error doubling; supervised learning with 16 samples yields ~14-16% error indicating severe underfitting
- **First 3 experiments**: 1) Reproduce KS equation baseline: verify error reduction from ~1.45% to ~0.40% with NLT vs no-transfer; 2) Ablate target dataset size: plot test error for NLT vs supervised baseline to find crossover point around n_t=256; 3) Test boundary conditions: extend to non-periodic boundaries to assess NLT resilience beyond paper's assumptions

## Open Questions the Paper Calls Out

- **Time marching predictions**: The current architecture approximates initial condition to final snapshot mapping, lacking recursive structure for intermediate temporal evolution. What evidence would resolve it: Successful validation of autoregressive rollouts maintaining stability over long time horizons.
- **Complex geometries**: The strict rectangular domain requirement needs relaxation to handle irregular domains. What evidence would resolve it: Adaptation to unstructured meshes or irregular boundaries without significant loss of alias-free properties.
- **NLT vs LoRA performance gap**: The paper empirically shows NLT outperforming LoRA but provides no theoretical explanation. What evidence would resolve it: Theoretical analysis or ablation study comparing representation capabilities of affine transformations versus low-rank decomposition in function space.

## Limitations
- Limited to 2D problems on periodic domains; extension to 3D and irregular geometries requires architectural modifications
- Assumes distribution shifts are parameter variations within fixed PDE forms, not fundamental equation structure changes
- NLT's success relies on assumption that domain shifts can be captured by per-channel affine transformations, which may not hold for non-linear feature space transformations

## Confidence
- **High confidence**: NLT consistently outperforms LoRA and fine-tuning across all three PDE systems and distribution shift magnitudes for parameter variations within fixed equations
- **Medium confidence**: MMD-insensitive adaptation claim supported by systematic experiments but limited to tested parameter ranges
- **Low confidence**: NLT will generalize to non-periodic boundaries, different domain geometries, or fundamental equation modifications

## Next Checks
1. Test NLT transfer learning when source and target datasets involve different boundary conditions (Dirichlet vs Neumann vs periodic) to evaluate architecture's torus-based assumptions
2. Evaluate transfer performance for distribution shifts involving equation structure changes (e.g., adding/removing terms from Brusselator or Navier-Stokes equations) rather than parameter variations
3. Assess NLT's effectiveness on 3D PDE problems and irregular domain geometries to validate continuous-discrete equivalence claims beyond rectangular 2D domains