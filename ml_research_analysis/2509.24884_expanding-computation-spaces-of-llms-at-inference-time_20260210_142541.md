---
ver: rpa2
title: Expanding Computation Spaces of LLMs at Inference Time
arxiv_id: '2509.24884'
source_url: https://arxiv.org/abs/2509.24884
tags:
- tokens
- filler
- performance
- token
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models can exploit
  artificially inserted filler tokens as expanded computation spaces during inference,
  without explicit training. It systematically explores the effects of different token
  types (space, enter, tab, period, pad, dash), insertion counts (16 to 8192), and
  positions (before/after "Answer:") on model performance across QA and math tasks
  using models ranging from 1.7B to 32B parameters.
---

# Expanding Computation Spaces of LLMs at Inference Time

## Quick Facts
- arXiv ID: 2509.24884
- Source URL: https://arxiv.org/abs/2509.24884
- Reference count: 36
- Large language models can leverage artificially inserted filler tokens as expanded computation spaces during inference, with smaller models benefiting most

## Executive Summary
This paper investigates whether large language models can exploit artificially inserted filler tokens as expanded computation spaces during inference, without explicit training. The authors systematically explore the effects of different token types (space, enter, tab, period, pad, dash), insertion counts (16 to 8192), and positions (before/after "Answer:") on model performance across QA and math tasks using models ranging from 1.7B to 32B parameters. Results show that models can leverage filler tokens as additional computational capacity, with smaller models benefiting most (up to 12.372 percentage points in SmolLM2-1.7B-Instruct on MMLU) and performance peaking when tokens are placed directly before the "Answer:" token. Attention map analysis reveals that filler tokens attend meaningfully to original input elements, suggesting they actively participate in reasoning rather than acting as noise.

## Method Summary
The paper evaluates whether inserting filler tokens at inference time improves LLM reasoning performance without training. Using MMLU (57 tasks), ARC-Challenge, GSM8K, and MATH-500, the authors insert M filler tokens (space, enter, tab, period, pad, dash) into input prompts, positioning them directly before the final "Answer:" token. They test counts M ∈ {16, 32, 64, 256, 1024, ..., 8192} across models like Llama-3.1-8B-Instruct and SmolLM2-1.7B-Instruct. Accuracy is computed for QA tasks by extracting logits for tokens A-D, while math tasks use Exact Match. The zero-shot approach compares performance with and without filler tokens.

## Key Results
- Smaller models benefit most: SmolLM2-1.7B-Instruct gains up to 12.372 percentage points on MMLU
- Optimal token count: 64 filler tokens provides maximum benefit before degradation
- Position matters: Tokens placed before "Answer:" outperform those placed after
- Different models prefer different token types: Llama favors pad/eos, SmolLM favors period

## Why This Works (Mechanism)
The paper proposes that filler tokens create expanded computation spaces that models can leverage during inference, effectively providing additional processing capacity without requiring training. Attention maps show filler tokens attend meaningfully to original input elements, suggesting active participation in reasoning rather than passive noise. The mechanism appears to work by giving models more tokens to process, allowing for deeper reasoning chains within the same inference context. However, the exact mechanism remains unclear - it's uncertain whether models treat these tokens as explicit computational workspace or simply benefit from extended context processing.

## Foundational Learning

### Token Insertion and Position Effects
**Why needed:** Understanding how token placement affects model reasoning is crucial for optimizing inference-time interventions. The paper shows that inserting tokens before the answer prompt is critical for success, while post-answer insertion fails.
**Quick check:** Compare accuracy when inserting 64 tokens before vs. after "Answer:" in a small model like SmolLM2-1.7B-Instruct on MMLU.

### Model Size Scaling Effects
**Why needed:** The benefit of filler tokens appears inversely related to model size, with smaller models gaining more. Understanding this relationship helps target interventions effectively.
**Quick check:** Plot performance gains across different model sizes (1.7B, 4B, 8B, 14B, 32B) to identify the scaling curve.

### Attention Pattern Analysis
**Why needed:** Attention maps provide evidence that filler tokens participate in reasoning rather than acting as noise, supporting the expanded computation space hypothesis.
**Quick check:** Visualize attention weights from filler tokens to original input tokens in Llama-3.1-8B-Instruct to verify meaningful connections.

## Architecture Onboarding

### Component Map
Input prompt -> Filler token insertion (M tokens) -> Model inference -> Logit extraction (A-D) -> Accuracy calculation

### Critical Path
Question → Token insertion (before Answer:) → Model processing → Answer prediction → Accuracy evaluation

### Design Tradeoffs
- Token count: 64 optimal vs. degradation beyond 1024
- Token type: Model-specific preferences (Llama: pad/eos, SmolLM: period)
- Position: Before Answer: essential vs. after Answer: harmful

### Failure Signatures
- Performance degrades beyond ~1024 tokens
- Inserting tokens after Answer: reduces accuracy
- Large models show minimal benefit

### 3 First Experiments
1. Insert 64 filler tokens before Answer: in SmolLM2-1.7B-Instruct on MMLU and measure accuracy gain
2. Compare performance when inserting same tokens after Answer: to confirm position dependency
3. Test different token types (space, period, pad) in Llama-3.1-8B-Instruct to identify preferences

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Why do different model families prefer different filler token types (e.g., SmolLM favors period, Llama favors pad/eos), and can we predict the optimal token type for a given model?
**Basis in paper:** The paper states "appropriate token types and counts vary" across models and notes that Llama performs best with `<pad>` tokens because it uses `<eos>` in their place, but does not provide a generalizable explanation for these preferences.
**Why unresolved:** The paper demonstrates the phenomenon empirically but does not identify the underlying mechanism linking token semantics, training corpus exposure, and model architecture to filler token effectiveness.
**What evidence would resolve it:** Systematic analysis correlating token frequency in pretraining corpora, token embedding characteristics, and cross-model performance patterns; intervention studies manipulating pretraining data.

### Open Question 2
**Question:** What mechanism causes the sharp performance degradation beyond ~1024 filler tokens, and can models be adapted to leverage much longer computation spaces?
**Basis in paper:** The paper reports that "once the number of added tokens surpassed 1024, the accuracy consistently declined to the 20% range across almost all cases" and attributes this to models not being trained on such inputs, resembling "lost-in-the-middle" phenomena.
**Why unresolved:** The paper identifies the threshold but does not determine whether the limitation stems from attention mechanism saturation, positional encoding constraints, training distribution mismatch, or other factors.
**What evidence would resolve it:** Experiments varying positional encoding schemes, attention patterns at high token counts, and fine-tuning with long filler sequences to test trainability.

### Open Question 3
**Question:** Can filler tokens be effectively combined with chain-of-thought reasoning to further enhance performance, or do they serve redundant computational roles?
**Basis in paper:** The paper explicitly contrasts its inference-only filler token approach with prior work on CoT and trained special tokens, but never experiments with combining filler tokens and CoT rationales together.
**Why unresolved:** It remains unclear whether filler tokens provide orthogonal benefits to semantic reasoning traces or whether both simply expand computation space through the same underlying mechanism.
**What evidence would resolve it:** Ablation studies comparing CoT alone, filler tokens alone, and CoT-plus-filler-tokens across reasoning benchmarks; attention analysis comparing computational patterns.

### Open Question 4
**Question:** At what scale do models transition from benefiting to not benefiting from expanded computation spaces, and is this transition sharp or gradual?
**Basis in paper:** The paper shows smaller models benefit more (up to 12.372 percentage points for 1.7B) while larger models show minimal gains, but tests only discrete model sizes (1.7B, 4B, 8B, 14B, 32B) without identifying the transition point.
**Why unresolved:** Without finer-grained scaling analysis, it is unclear whether there exists a critical model capacity threshold or a smooth inverse relationship between model size and filler token utility.
**What evidence would resolve it:** Systematic experiments across a dense range of model sizes within the same model family, measuring filler token benefit as a continuous function of parameter count.

## Limitations
- The causal mechanism linking filler tokens to performance gains is not definitively established
- Results may be dataset-specific and not generalize to more challenging benchmarks
- Lack of statistical significance testing and variance measures in reported results

## Confidence
High confidence: Experimental methodology is clearly specified with defined datasets, metrics, and intervention parameters. The observation that token insertion before "Answer:" outperforms insertion after is reproducible and well-documented.

Medium confidence: The claim that filler tokens act as expanded computation space rather than noise. While attention patterns support this interpretation, the evidence is circumstantial.

Low confidence: The quantitative performance improvements, particularly the large gains reported for smaller models. Without variance measures, statistical testing, or validation on independent datasets, these results may overstate the intervention's effectiveness.

## Next Checks
1. Conduct ablation studies comparing different token types, positions, and counts across multiple independent runs with statistical significance testing to isolate which aspects of the intervention drive performance changes.
2. Test the intervention on a broader set of tasks including more challenging reasoning benchmarks and language modeling tasks to verify that gains are not dataset-specific artifacts.
3. Implement controlled experiments where filler tokens are replaced with random noise or identity-preserving transformations to determine whether any token addition provides similar benefits or if specific properties of the filler tokens are essential.