---
ver: rpa2
title: 'Out-of-distribution generalisation is hard: evidence from ARC-like tasks'
arxiv_id: '2505.09716'
source_url: https://arxiv.org/abs/2505.09716
tags:
- data
- sets
- networks
- algorithms
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates out-of-distribution (OOD) generalization
  in neural networks using two novel data sets derived from ARC-like tasks. The authors
  propose that OOD generalization requires compositional learning, where algorithms
  must extract environment-invariant properties and transfer them to novel inputs.
---

# Out-of-distribution generalisation is hard: evidence from ARC-like tasks

## Quick Facts
- **arXiv ID:** 2505.09716
- **Source URL:** https://arxiv.org/abs/2505.09716
- **Reference count:** 23
- **Primary result:** Standard neural networks fail at OOD generalization on ARC-like tasks, while engineered pointer networks can succeed but only by exploiting task-specific biases rather than learning true compositional rules.

## Executive Summary
This paper investigates out-of-distribution (OOD) generalization in neural networks using two novel ARC-like datasets with measurable OOD distance between train and test sets. The authors test five architectures (MLP, CNN, Transformer, Axial Pointer Network, and Axial Pointer Linear Network) on tasks requiring compositional reasoning about shape, color, size, position, and action. Results show that while standard networks achieve perfect accuracy on in-distribution (ID) tasks, they fail completely on OOD tasks, suggesting they haven't learned compositional features. The novel pointer networks, particularly the APLN with its row-column bias, achieve high OOD accuracy on translation tasks but fail on rotation tasks, demonstrating that apparent OOD generalization can arise from task-specific architectural biases rather than true compositional understanding.

## Method Summary
The authors create two synthetic world models (Translate and Rotate) where objects move along axes or rotate in 90-degree increments. Objects are defined by 5 binary concept variables (Shape, Colour, Size, Position, Action) generating 8 concept combinations split into train (4), Distance 1 test (2), and Distance 2 test (1) sets. Five architectures are tested: standard MLP, CNN, and Transformer networks, plus two novel pointer networks that copy input pixels to output positions via attention mechanisms. The APLN specifically incorporates a column-then-row bias that aligns with translation tasks. Models are trained on ID data until convergence, then evaluated on all test distances.

## Key Results
- Standard architectures (MLP, CNN, Transformer) achieve ~100% ID accuracy but ~0% OOD accuracy on both world models
- APLN achieves ~100% OOD accuracy on Translate tasks but fails completely on Rotate tasks
- The APLN's success demonstrates that engineered architectural biases can create apparent OOD generalization without true compositional understanding
- None of the tested architectures learns proper disentangled representations of concepts (object properties) versus rules (actions)

## Why This Works (Mechanism)

### Mechanism 1: Targeted Inductive Bias for Apparent OOD
Engineering specific architectural constraints can produce high OOD accuracy without learning underlying compositional rules. The APLN's row-column bias perfectly matches translation tasks but conflicts with rotation, showing OOD performance may reflect dataset-specific exploitation rather than generalizable reasoning.

### Mechanism 2: Pointer Attention for Copying Operations
Pointer networks map output pixel indices to input pixel indices via attention, bypassing the need to generate pixel values from latent vectors. This focuses capacity on transformation logic rather than object generation, making it effective for copying-and-rearranging tasks.

### Mechanism 3: Systematicity via Disentangled Concepts
True OOD generalization requires explicit separation of "concepts" (shape, color) from "rules" (actions) in latent space. Standard networks fail because they learn entangled representations where object identity and action are fused, preventing recombination of known parts and rules.

## Foundational Learning

- **Concept: Compositional Generalization**
  - Why needed: Distinguishes interpolating between data points from recombining known concepts like human intelligence
  - Quick check: Can the model handle "blue square" if trained only on "blue circles" and "red squares"?

- **Concept: Systematicity**
  - Why needed: Theoretical standard for evaluating if a model truly "understands" or just matches biases
  - Quick check: If taught "John walks" and "Mary runs," does it understand "Mary walks"?

- **Concept: Concept Variables (CVs)**
  - Why needed: Ground truth factors (Shape, Colour, Size, Position, Action) define the compositional space being tested
  - Quick check: Does the model treat "Size" and "Shape" as independent variables or conflate them?

## Architecture Onboarding

- **Component map:** Input (image, action_bit) -> gMLP encoder -> Concatenate with action vector -> Pointer Head (axial attention) -> Output 32Ã—32 grid
- **Critical path:** The gMLP to Pointer Head connection is where reasoning happens, producing logits that guide pointer attention based on the action bit
- **Design tradeoffs:** APLN forces column-then-row decoding (efficient for translation, brittle for rotation) vs. APN's flexible but harder-to-train approach
- **Failure signatures:** "Close but wrong" errors (correct object, wrong transformation) indicate conflated object-action representations; random copying in Rotate tasks shows bias failure
- **First 3 experiments:**
  1. Train all 5 architectures on Distance 0 data to verify 100% accuracy (capacity check)
  2. Compare APLN vs APN on Translate Distance 2 to demonstrate column-row bias power
  3. Visualize APLN pointer attention on Rotate Distance 1 to confirm breakdown mechanism

## Open Questions the Paper Calls Out

1. **Compositional Actions:** How do algorithms perform on tasks requiring composition of multiple atomic actions rather than single-step manipulations? The current study doesn't explore world models with compositional actions, limiting evaluation of complex reasoning algorithms.

2. **Alternative Architectures:** Can state-of-the-art compositional learning architectures achieve true OOD generalization on these specific ARC-like tasks? The study only tested three standard architectures and two custom pointer networks, leaving open whether specialized approaches could succeed.

3. **Hyperparameter Impact:** To what extent does hyperparameter optimization influence OOD generalization capabilities of the standard networks tested? The small hyperparameter search may have underestimated standard networks' potential, as there was no OOD validation signal to drive the search.

## Limitations

- Conclusions based on two synthetic, hand-designed world models rather than diverse real-world tasks
- APLN's success/failure pattern may reflect task-specific architectural biases rather than general principles about compositional learning
- Limited testing of specialized compositional algorithms beyond pointer networks
- Small hyperparameter search may have underestimated standard networks' potential

## Confidence

- **High:** Experimental results showing standard networks failing OOD generalization are reproducible and clearly demonstrated
- **Medium:** Interpretation that engineered biases create "apparent" OOD without true compositionality is supported but relies on theoretical arguments
- **Medium:** Conclusion about need for diverse OOD benchmarks is valid but specific diagnostic claims require further validation

## Next Checks

1. **Cross-World Validation:** Test APLN on additional world models with different transformation types (scaling, shearing) to verify column-row bias consistently fails when transformations violate axis-aligned assumptions

2. **Concept Ablation:** Systematically remove individual concept variables from dataset to test if networks can achieve OOD generalization with fewer factors involved

3. **Latent Interpretability Analysis:** Apply sparse autoencoders to standard network latent spaces to check for disentangled representations of concepts and rules