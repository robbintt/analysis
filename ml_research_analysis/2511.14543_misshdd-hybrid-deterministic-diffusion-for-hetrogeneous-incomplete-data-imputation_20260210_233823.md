---
ver: rpa2
title: 'MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation'
arxiv_id: '2511.14543'
source_url: https://arxiv.org/abs/2511.14543
tags:
- diffusion
- discrete
- data
- categorical
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MissHDD addresses the challenge of imputing missing values in heterogeneous
  tabular data containing both numerical and categorical/discrete attributes. The
  key insight is that these variable types lie on fundamentally different statistical
  manifolds, making a single diffusion process unsuitable.
---

# MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation

## Quick Facts
- arXiv ID: 2511.14543
- Source URL: https://arxiv.org/abs/2511.14543
- Reference count: 38
- Primary result: State-of-the-art heterogeneous data imputation with 15-25% RMSE improvement over baselines

## Executive Summary
MissHDD addresses the challenge of imputing missing values in heterogeneous tabular data containing both numerical and categorical/discrete attributes. The key insight is that these variable types lie on fundamentally different statistical manifolds, making a single diffusion process unsuitable. MissHDD proposes a hybrid deterministic diffusion framework with two complementary channels: a continuous DDIM-based channel for numerical features and a discrete latent-path diffusion channel inspired by loopholing techniques for categorical/discrete features.

## Method Summary
The framework employs a hybrid deterministic diffusion approach that processes numerical and categorical features through separate but coordinated channels. The numerical channel uses DDIM (Denoising Diffusion Implicit Models) for continuous data, while the categorical channel implements discrete latent-path diffusion techniques. Both channels are trained under a unified conditional imputation objective using a self-masking strategy that enables learning from incomplete data without requiring fully observed samples. This design allows the model to handle heterogeneous data types effectively while maintaining deterministic behavior for reproducible results.

## Key Results
- Achieves 15-25% RMSE improvements over baseline methods on real-world datasets
- Demonstrates superior performance across MCAR, MAR, and MNAR missingness mechanisms
- Provides significantly improved run-to-run stability and substantially reduced inference latency compared to existing diffusion-based methods

## Why This Works (Mechanism)
MissHDD works by recognizing that numerical and categorical features exist on fundamentally different statistical manifolds, requiring distinct diffusion processes. The hybrid approach allows each channel to specialize in its respective data type, with DDIM optimized for continuous numerical features and discrete diffusion tailored for categorical variables. The unified conditional objective and self-masking training strategy enable effective learning from incomplete data, while the deterministic nature ensures reproducibility and faster inference compared to stochastic diffusion models.

## Foundational Learning
- **Diffusion models for imputation**: Used to learn the reverse process of adding noise to data, essential for reconstructing missing values from corrupted observations
- **DDIM (Denoising Diffusion Implicit Models)**: Provides deterministic sampling for continuous variables, enabling faster inference while maintaining quality
- **Discrete latent diffusion**: Handles categorical/discrete features through latent variable modeling, necessary because standard diffusion assumes continuous spaces
- **Self-masking strategy**: Enables training on incomplete data by masking some observed values as missing during training, critical for real-world applicability
- **Unified conditional objective**: Allows joint training of both channels under a single loss function, ensuring coordinated behavior across heterogeneous features

## Architecture Onboarding

**Component map**: Input -> [Numerical DDIM channel] + [Categorical diffusion channel] -> [Unified conditioning] -> Imputed output

**Critical path**: Input data → Feature type detection → Separate channel processing → Unified conditioning → Output combination

**Design tradeoffs**: Deterministic vs stochastic behavior (reproducibility vs flexibility), separate channels vs unified model (specialization vs simplicity), self-masking vs full supervision (real-world applicability vs training efficiency)

**Failure signatures**: Poor performance on highly correlated features, degradation with extreme missingness rates (>50%), instability with high-cardinality categorical variables

**First experiments to run**:
1. Ablation test removing numerical DDIM channel to quantify categorical-only performance
2. Baseline comparison with standard MICE imputation on heterogeneous datasets
3. Sensitivity analysis varying missingness rates from 10% to 50%

## Open Questions the Paper Calls Out
None

## Limitations
- Claims of "substantially reduced inference latency" lack specific timing measurements for verification
- Deterministic approach may sacrifice flexibility in capturing complex data distributions compared to stochastic alternatives
- Performance on highly correlated features or complex missingness patterns beyond MCAR, MAR, and MNAR remains untested

## Confidence

**Medium**: The 15-25% RMSE improvements are impressive but require independent verification across diverse dataset types
**Low**: Latency reduction claims are not quantified with specific metrics or comparisons
**Medium**: Unified conditional objective may have limitations with highly correlated features or complex missingness patterns

## Next Checks

1. **Timing Validation**: Measure inference latency (wall-clock time) for MissHDD versus competing methods across datasets of varying sizes (10K, 100K, 1M samples) and different missingness rates (10%, 30%, 50%)

2. **Generalization Testing**: Evaluate MissHDD's performance on additional heterogeneous datasets with different feature distributions, including high-cardinality categorical variables and datasets with strong feature interdependencies

3. **Ablation Studies**: Systematically test performance with either the numerical DDIM channel or discrete diffusion channel removed, and test alternative conditioning strategies or operation orders