---
ver: rpa2
title: Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management
arxiv_id: '2510.06727'
source_url: https://arxiv.org/abs/2510.06727
tags:
- function
- context
- tool
- training
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of scaling reinforcement learning
  for long-horizon multi-turn tool use with large language models, where context length
  becomes a bottleneck. The authors introduce summarization-based context management,
  where LLM-generated summaries periodically compress tool-use history to retain task-relevant
  information while keeping the working context compact.
---

# Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management

## Quick Facts
- **arXiv ID**: 2510.06727
- **Source URL**: https://arxiv.org/abs/2510.06727
- **Reference count**: 40
- **Primary result**: SUPO achieves significant improvements in success rates for multi-turn tool use while maintaining or reducing working context length through end-to-end summarization-based context management.

## Executive Summary
This paper addresses the critical challenge of scaling reinforcement learning for long-horizon multi-turn tool use with large language models, where context length limitations become a bottleneck. The authors propose an innovative approach that uses LLM-generated summaries to periodically compress tool-use history, retaining only task-relevant information while keeping the working context compact. By deriving a policy gradient formulation that optimizes both tool-use and summarization strategies end-to-end, they enable existing RL infrastructures to handle this dual optimization problem. The SUPO algorithm demonstrates significant improvements in success rates on synthetic function calling and complex searching tasks while maintaining or even reducing working context length compared to baseline approaches.

## Method Summary
The proposed method introduces summarization-based context management where LLM-generated summaries periodically compress the history of tool-use interactions to maintain a compact working context. The key innovation is an end-to-end policy gradient formulation that allows joint optimization of both the tool-use policy and the summarization strategy. This formulation enables existing reinforcement learning infrastructures to be leveraged while addressing the dual optimization problem. The SUPO algorithm implements this framework by integrating the summarization mechanism directly into the RL loop, allowing the model to learn when and how to summarize based on task requirements and context compression needs.

## Key Results
- SUPO achieves significant improvements in success rates on synthetic function calling and complex searching tasks compared to baseline approaches
- The method maintains or reduces working context length while preserving or improving performance
- The approach enables scaling beyond training-time trajectory limits at test time, demonstrating robustness to longer contexts

## Why This Works (Mechanism)
The mechanism works by leveraging the LLM's summarization capabilities to compress tool-use history while retaining task-relevant information. The end-to-end optimization ensures that the summarization strategy is learned in coordination with the tool-use policy, allowing the system to make intelligent decisions about when compression is beneficial versus when detailed history is necessary. The policy gradient formulation provides a principled way to optimize both components simultaneously, addressing the challenge that traditional RL approaches cannot easily handle the summarization subproblem.

## Foundational Learning
- **Policy Gradient Methods**: Why needed - To optimize both tool-use and summarization strategies end-to-end. Quick check - Verify gradient flow through both policy and summarization components.
- **Context Compression**: Why needed - To address the context length bottleneck in long-horizon multi-turn tasks. Quick check - Measure information retention vs. compression ratio.
- **Multi-turn Tool Use**: Why needed - The target application domain requiring extended interaction histories. Quick check - Validate task completion rates across multiple turns.
- **Reinforcement Learning for LLMs**: Why needed - To enable learning from interaction rather than just supervised fine-tuning. Quick check - Compare success rates with and without RL fine-tuning.

## Architecture Onboarding
**Component Map**: Tool-use History -> Summarization Module -> Compressed Context -> RL Policy -> Action -> Tool-use History (loop)

**Critical Path**: The core execution loop involves: (1) maintaining tool-use history, (2) applying summarization when triggered, (3) feeding compressed context to RL policy, (4) executing action, (5) updating history. The summarization module and RL policy are the two critical components that must be jointly optimized.

**Design Tradeoffs**: The primary tradeoff is between context compactness and information preservation. More aggressive summarization reduces context length but risks losing important information. The end-to-end optimization helps find the right balance, but the choice of summarization frequency and quality requirements significantly impacts performance.

**Failure Signatures**: Performance degradation may occur when: (1) summaries omit critical information needed for future decisions, (2) summarization occurs too frequently or too infrequently, (3) the RL policy fails to adapt to compressed contexts, or (4) the joint optimization destabilizes during training.

**First 3 Experiments**:
1. Compare success rates with and without summarization on synthetic function calling tasks across varying context lengths
2. Measure the impact of different summarization frequencies on task completion and context size
3. Evaluate performance degradation when introducing controlled noise into summaries to test robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is primarily on synthetic tasks (Python function calling and HotpotQA), limiting generalizability to real-world scenarios
- The method assumes access to expert trajectories and uses behavior cloning for summarization, which may limit generalization
- The approach introduces a trade-off between context compactness and information preservation that is not fully characterized across different task domains

## Confidence
- **High confidence**: SUPO can successfully optimize both tool-use and summarization strategies end-to-end on synthetic tasks
- **Medium confidence**: The method generalizes to test-time contexts longer than training-time trajectories
- **Medium confidence**: The approach maintains or improves success rates while reducing context length compared to baselines

## Next Checks
1. Test SUPO on more complex, realistic multi-turn tool-use tasks (e.g., autonomous web navigation, complex API orchestration) to validate performance beyond synthetic benchmarks
2. Systematically evaluate performance when summaries contain errors, omissions, or hallucinations by introducing controlled noise or using weaker summarization models
3. Evaluate whether summarization strategies learned on one task domain transfer effectively to fundamentally different domains without additional training