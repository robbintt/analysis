---
ver: rpa2
title: 'FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale'
arxiv_id: '2505.14932'
source_url: https://arxiv.org/abs/2505.14932
tags:
- reasoning
- step
- complexity
- 'true'
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present FOL-Traces, the first large-scale dataset of programmatically
  verified reasoning traces, containing 7.4M examples with 3.5B tokens. Unlike unverifiable
  natural language chains of thought or small-scale symbolic theorem-proving corpora,
  FOL-Traces combines symbolic guarantees with pretraining-scale size, enabling rigorous
  study of structured logical inference.
---

# FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale

## Quick Facts
- arXiv ID: 2505.14932
- Source URL: https://arxiv.org/abs/2505.14932
- Reference count: 40
- Primary result: First large-scale dataset (7.4M examples, 3.5B tokens) of programmatically verified FOL reasoning traces with rigorous diagnostic tasks

## Executive Summary
FOL-Traces presents the first large-scale dataset of programmatically verified first-order logic reasoning traces, containing 7.4M examples with 3.5B tokens. Unlike unverifiable natural language chains of thought or small-scale symbolic theorem-proving corpora, FOL-Traces combines symbolic guarantees with pretraining-scale size, enabling rigorous study of structured logical inference. The dataset is generated using SymPy's symbolic computation capabilities to create verifiable simplification chains, annotated with circuit complexity metrics to measure reasoning difficulty.

The authors propose two diagnostic tasks—masked operation prediction and step completion—that directly probe models' syntactic awareness and process fidelity. Systematic experiments with five reasoning LLMs show significant challenges: models achieve only around 45.7% accuracy on masked operation prediction and around 27% on two-step completion, even with explicit reasoning-oriented training. The best-performing Qwen2.5-32B model reaches 75% accuracy on component prediction but struggles with process-level reasoning, highlighting persistent limitations in compositional and logical reasoning capabilities.

## Method Summary
FOL-Traces is generated through a pipeline combining SymPy's symbolic simplification with LLM instantiation. Random formulas are created using predefined production rules, then simplified via SymPy's formal inference rules (De Morgan's laws, tautology elimination) to produce verifiable reasoning chains. Each step is annotated with circuit complexity metrics. The dataset is instantiated with natural language predicates using GPT-4o, creating 7.4M training examples. Models are evaluated via two diagnostic tasks: masked prediction (component/operator/predicate accuracy) and step completion (1-step, 2-step, chain accuracy). Pretraining experiments use GPT-2-small (125M params) with NanoGPT, trained from scratch on the corpus.

## Key Results
- FOL-Traces contains 7.4M examples with 3.5B tokens, combining symbolic verification with pretraining-scale size
- Models achieve only 45.7% accuracy on masked operation prediction and 27% on two-step completion
- Qwen2.5-32B achieves highest scores (75% component accuracy, 73% operator accuracy, 80% predicate accuracy) but struggles with process-level reasoning
- Performance declines markedly with circuit complexity, showing models struggle to scale to complex logical transformations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Programmatic verification via symbolic computation produces reasoning traces with guaranteed step-level correctness
- Mechanism: SymPy's simplification algorithms apply formal inference rules to recursively reduce FOL expressions. Each transformation is recorded as a chain step, creating programmatically verifiable traces where every transition is provably valid by construction
- Core assumption: SymPy's symbolic simplification correctly implements first-order logic inference rules
- Evidence anchors: [section] "We utilize SymPy... Once a formula is generated, a high-level simplification procedure is applied... by applying standard logical inference rules such as De Morgan's laws and eliminating tautologies"; [section] "Each step is programmatically verified to be correct, leveraging SymPy's symbolic computation capabilities"; [corpus] Related work on formal verification in LLM reasoning contexts shows symbolic methods provide verifiable guarantees

### Mechanism 2
- Claim: Circuit complexity metrics create a measurable difficulty gradient for reasoning tasks
- Mechanism: The paper defines circuit complexity C(φ) as the number of logic gates needed to compute a formula. This provides a quantitative measure of reasoning difficulty that correlates with model performance. Complexities decrease through simplification chains, creating controlled difficulty progressions
- Core assumption: Circuit complexity meaningfully captures reasoning difficulty for neural models, not just symbolic systems
- Evidence anchors: [section] "Circuit complexity generally increases with rewrite depth, suggesting that longer rewrite sequences correspond to more intricate transformations"; [section] Figure 7 shows "LLMs achieve higher accuracy on low-complexity rules... performance declines markedly as rule complexity increases"; [corpus] Related work on LTLf tensor semantics and formal verification similarly uses complexity-based difficulty measures

### Mechanism 3
- Claim: Masked prediction tasks isolate distinct components of compositional reasoning ability
- Mechanism: By selectively masking components (subexpressions), operators (logical connectives), or predicates (function names), the diagnostic tasks force models to demonstrate specific competencies: identifying subexpressions, applying logical operators correctly, and recognizing predicates. This decomposition reveals whether models learn genuine logical structure versus surface patterns
- Core assumption: Performance on masked prediction correlates with genuine reasoning ability, not just pattern completion
- Evidence anchors: [abstract] "We also propose two challenging and comprehensive diagnostic tasks—masked operation prediction and step completion—that directly probe syntactic awareness and process fidelity"; [section] "qwen2.5-32b... achieves the highest scores in every category: 75% component accuracy, 73% operator accuracy, and 80% predicate accuracy"; [corpus] Work on adaptive symbolic language selection and LLM logical reasoning suggests component-level evaluation reveals limitations masked by aggregate metrics

## Foundational Learning

- Concept: **First-order logic syntax and semantics**
  - Why needed here: The entire dataset is structured around FOL expressions with predicates, variables, quantifiers (∀, ∃), and logical connectives (∧, ∨, ¬, →, ↔). Understanding how these compose and simplify is prerequisite to interpreting traces
  - Quick check question: Given ¬(P ∧ Q), what is its equivalent form using only ∨ and ¬? (Answer: ¬P ∨ ¬Q by De Morgan's law)

- Concept: **Boolean circuit complexity**
  - Why needed here: The paper uses circuit complexity to quantify trace difficulty and correlate with model performance. Understanding how formula structure maps to circuit size enables difficulty analysis
  - Quick check question: What is the circuit complexity of (A ∧ B) ∨ C assuming atomic formulas have complexity 1? (Answer: 1 + (1 + 1 + 1) + 1 = 5, counting the AND gate with two atomic inputs plus the OR gate)

- Concept: **Chain-of-thought reasoning in LLMs**
  - Why needed here: FOL-Traces is explicitly designed as verified CoT data. Understanding the CoT paradigm—step-by-step decomposition toward conclusions—frames why verification matters and how traces differ from single-step outputs
  - Quick check question: Why might a correct final answer still indicate faulty reasoning in a CoT trace? (Answer: The model could reach the right conclusion via spurious shortcuts or pattern matching rather than valid intermediate steps)

## Architecture Onboarding

- Component map:
  Human Curated Rules → SymPy Generator → Simplification Chain → LLM Instantiation → FOL-Traces
  Random Generation → SymPy Generator → Simplification Chain → LLM Instantiation → FOL-Traces
  FOL-Traces → Pretraining (GPT-2 125M) → TRUE/FALSE Evaluation
  FOL-Traces → Diagnostic Tasks → Masked Prediction → Step Completion

- Critical path:
  1. Understand SymPy generation pipeline (Algorithms 1-4 in appendix)
  2. Map complexity annotations (circuit, elimination, program complexity)
  3. Implement masked prediction evaluation (component/operator/predicate masks)
  4. Implement step completion with SymPy verification

- Design tradeoffs:
  - Scale vs. diversity: 7.4M examples but 15.26M are "random" generated, which may have repetitive patterns vs. curated rules
  - Verifiability vs. naturalism: FOL is formal and verifiable but "abstracts away linguistic ambiguity"—less representative of real-world reasoning
  - Component isolation vs. holism: Masked tasks isolate specific abilities but may not reflect integrated reasoning

- Failure signatures:
  - Models achieving high component accuracy but low step completion (surface pattern learning)
  - Strong performance on low-complexity rules with sharp degradation on medium/high complexity (failure to scale)
  - "Chain-only errors" where models capture overall logical relations but make structural mistakes (partial generalization)
  - Malformed outputs unparseable by SymPy (syntax learning incomplete)

- First 3 experiments:
  1. Sanity check on TRUE/FALSE evaluation: Train small model (125M params) from scratch on FOL-Traces; verify output distribution collapses to binary (TRUE/FALSE) and accuracy scales with complexity bucket. Target: 70%+ on unseen split within 1-3B tokens
  2. Masked prediction baseline: Evaluate pretrained/reasoning models on all three mask types. Analyze whether operator prediction lags component/predicate (suggesting connective logic is harder). Expected: ~45-75% range depending on model scale
  3. Step completion difficulty scaling: Test 1-step vs. 2-step completion across complexity tertiles. Confirm accuracy drops with steps and complexity. If 2-step accuracy exceeds 1-step for any model, investigate data contamination or evaluation bug

## Open Questions the Paper Calls Out

- **Question**: Does pretraining on FOL-Traces improve performance on downstream natural language reasoning tasks that require structured inference?
  - Basis in paper: [explicit] Section 6 states "evaluations target diagnostic reasoning tasks rather than full downstream applications, leaving generalization and scaling behavior for future study"
  - Why unresolved: The paper only evaluates diagnostic tasks (masked operation prediction, step completion) and a small GPT-2 pretraining sanity check, not transfer to external benchmarks
  - What evidence would resolve it: Pretrain a model on FOL-Traces and evaluate on reasoning benchmarks like LogiQA, FOLIO, or natural language inference datasets

- **Question**: What training paradigms can bridge the gap between symbolic reasoning competence and compositional generalization in LLMs?
  - Basis in paper: [explicit] Section 4.2.1 concludes that "current reasoning-oriented training approaches alone may be insufficient for achieving strong systematic reasoning performance, highlighting the need for more integrated reasoning and compositional generalization strategies"
  - Why unresolved: Models explicitly trained for reasoning (e.g., Phi-3.5-mini) underperformed larger general-purpose models, suggesting reasoning-oriented training doesn't reliably induce systematic reasoning
  - What evidence would resolve it: Design training methods that jointly optimize for symbolic validity and compositional generalization, then demonstrate improved performance on FOL-Traces diagnostic tasks

- **Question**: Can performance on FOL-Traces diagnostic tasks predict reasoning faithfulness in natural language chain-of-thought outputs?
  - Basis in paper: [inferred] The paper motivates FOL-Traces by noting that CoT traces "can be unfaithful to the model's true computational process," but does not establish whether FOL competence correlates with faithfulness
  - Why unresolved: No experiments link FOL-Traces performance to CoT faithfulness metrics (e.g., intervention studies, causal tracing)
  - What evidence would resolve it: Correlate model performance on FOL-Traces with established faithfulness benchmarks, or show that improving FOL-Traces scores increases CoT faithfulness

## Limitations

- The paper's central claims about FOL-Traces enabling rigorous study of structured logical inference rest on SymPy's symbolic simplification correctness, which isn't independently validated
- The 7.4M dataset size includes 15.26M randomly generated examples that may introduce repetitive patterns limiting generalization
- Masked prediction accuracy may reflect pattern completion rather than genuine understanding, and high per-component scores don't guarantee coherent multi-step reasoning

## Confidence

- **High confidence**: The dataset creation methodology and basic verification pipeline are well-specified and reproducible. The experimental results showing model limitations (45.7% masked prediction accuracy, 27% step completion) are internally consistent
- **Medium confidence**: The correlation between circuit complexity and model performance, while observed, needs more rigorous validation. The paper shows correlation but doesn't establish causation or rule out confounding factors
- **Low confidence**: The claim that FOL-Traces is "the first large-scale dataset of programmatically verified reasoning traces" requires careful examination of what constitutes "programmatic verification" versus other verification methods in the literature

## Next Checks

1. **SymPy verification validation**: Sample 100 random simplification chains and manually verify each step against standard FOL inference rules. This would confirm whether SymPy's implementation matches the paper's verification guarantees
2. **Random vs. curated rule generalization**: Train models separately on curated vs. random subsets and test on the other type. This would quantify whether the large random component provides meaningful generalization beyond pattern matching
3. **Complexity metric calibration**: Test whether models trained on low-complexity examples can generalize to medium/high complexity tasks, controlling for formula size and vocabulary overlap. This would validate whether circuit complexity meaningfully captures reasoning difficulty