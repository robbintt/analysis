---
ver: rpa2
title: Iterative Foundation Model Fine-Tuning on Multiple Rewards
arxiv_id: '2511.00220'
source_url: https://arxiv.org/abs/2511.00220
tags:
- learning
- iterativers
- reward
- each
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IterativeRS, a novel multi-objective reinforcement
  learning method for fine-tuning foundation models. The key idea is to iteratively
  fine-tune separate expert models for each objective while periodically merging them
  to control variance and encourage knowledge transfer.
---

# Iterative Foundation Model Fine-Tuning on Multiple Rewards

## Quick Facts
- **arXiv ID:** 2511.00220
- **Source URL:** https://arxiv.org/abs/2511.00220
- **Reference count:** 40
- **Primary result:** IterativeRS achieves higher average rewards and better consistency across objectives compared to state-of-the-art baselines in small molecule generation, DNA sequence design, and text summarization.

## Executive Summary
This paper introduces IterativeRS, a novel multi-objective reinforcement learning method for fine-tuning foundation models. The key idea is to iteratively fine-tune separate expert models for each objective while periodically merging them to control variance and encourage knowledge transfer. Unlike approaches that combine rewards or merge experts only at the end, IterativeRS performs merging both during and after training. The authors provide theoretical analysis showing convergence properties under convexity assumptions. Experiments across small molecule generation, DNA sequence design, and text summarization demonstrate that IterativeRS achieves higher average rewards and better consistency across objectives compared to state-of-the-art baselines.

## Method Summary
IterativeRS is a multi-objective RL method that trains N expert policies (one per objective) using PPO. Every m steps, the experts are merged into a shared parameter vector ρ_t (via linear combination) and synchronized, then training resumes. This periodic merging during training, in addition to a final merge, allows for better knowledge transfer and variance control compared to other approaches. The method is evaluated on three domains: small molecule generation (QM9), DNA sequence design (MPRA), and text summarization (Reddit), using pre-trained foundation models (MolGPT-2, DNAGPT-2, Llama-3.2-3B-Instruct) as starting points.

## Key Results
- In small molecule generation, IterativeRS achieved an average reward of 1.4017 with an ICV score of 3.5854, outperforming other methods.
- IterativeRS demonstrates higher average rewards and better consistency across objectives compared to state-of-the-art baselines including MORLHF, Rewarded Soups, and Rewards-in-Context.
- The method shows improved performance in DNA sequence design and text summarization tasks as well.

## Why This Works (Mechanism)
Assumption: The periodic merging during training helps maintain a shared knowledge base across expert models, preventing them from diverging too far and improving the final merged policy's ability to balance multiple objectives. The mechanism likely works by allowing experts to periodically "reset" toward a common center, which may reduce variance and improve consistency across objectives compared to training separate experts in isolation.

## Foundational Learning
- **Multi-objective RL**: Why needed: Foundation models often need to optimize for multiple competing objectives. Quick check: Can the model balance trade-offs between objectives effectively?
- **Reinforcement Learning (RL)**: Why needed: Used to fine-tune foundation models based on reward signals. Quick check: Does the RL algorithm converge and improve performance over pre-trained models?
- **Policy Merging**: Why needed: Combines knowledge from multiple expert models trained on different objectives. Quick check: Does merging improve overall performance compared to training a single model on combined rewards?

## Architecture Onboarding
- **Component Map:** Foundation Model -> N Expert Policies (PPO) -> Periodic Merging -> Final Merged Policy
- **Critical Path:** Pre-trained foundation model → Expert training (PPO) → Periodic merging → Evaluation
- **Design Tradeoffs:** Merging frequency (m) vs. expert divergence; single reward combination vs. iterative expert training and merging.
- **Failure Signatures:** High expert divergence leading to poor merged model performance; catastrophic forgetting of pre-trained knowledge.
- **First Experiments:**
    1. Implement and test PPO training loop for a single expert on one objective.
    2. Implement the periodic merging mechanism and test its effect on expert convergence.
    3. Run a small-scale experiment comparing IterativeRS to reward combination on a simple synthetic task.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- The experimental comparisons focus on a limited set of baselines, and broader comparisons are needed.
- Use of oracle reward models may inflate performance estimates compared to real-world settings.
- The merging frequency and weight selection strategy are tuned per domain but lack systematic sensitivity analysis.

## Confidence
- **High Confidence:** The theoretical convergence analysis under convexity assumptions is sound and well-presented.
- **Medium Confidence:** The empirical results on the three benchmark domains are promising and show consistent improvements over baselines, but the lack of broader baseline comparisons and real-world reward model evaluation introduces some uncertainty.
- **Low Confidence:** The claims about superior consistency (ICV) and variance control are supported by the data, but the sensitivity of these metrics to hyperparameter choices (especially merging frequency and weight selection) is not fully explored.

## Next Checks
1. **Baseline Expansion:** Re-run the experiments adding comparisons against DPO, P-tuning, and QLoRA-based multi-objective methods to ensure IterativeRS is state-of-the-art across the full landscape.
2. **Reward Model Ablation:** Repeat the molecular generation experiments using approximate (learned) reward models instead of oracle models like PAMNet to test robustness to real-world reward estimation noise.
3. **Hyperparameter Sensitivity:** Systematically vary the merging frequency $m$ and weight selection strategy (uniform vs. preference-weighted) across all three domains and report the impact on both average reward and ICV to understand the robustness of the method.