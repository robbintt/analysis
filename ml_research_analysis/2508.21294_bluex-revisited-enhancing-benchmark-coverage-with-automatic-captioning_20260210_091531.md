---
ver: rpa2
title: 'BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning'
arxiv_id: '2508.21294'
source_url: https://arxiv.org/abs/2508.21294
tags:
- captions
- questions
- arxiv
- language
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors updated the BLUEX dataset with 2024-2025 exam questions
  and added automatically generated image captions using GPT-4o. This increased usable
  questions from 740 to 1,422, more than doubling the benchmark size.
---

# BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning

## Quick Facts
- **arXiv ID**: 2508.21294
- **Source URL**: https://arxiv.org/abs/2508.21294
- **Reference count**: 3
- **Primary result**: Updated BLUEX dataset with 2024-2025 exam questions and GPT-4o-generated captions, increasing usable questions from 740 to 1,422

## Executive Summary
The BLUEX benchmark for Portuguese-language education assessment has been significantly expanded and enhanced. The authors updated the dataset with recent 2024-2025 university entrance exams and introduced automatically generated image captions using GPT-4o, enabling text-only language models to tackle previously multimodal questions. This innovation more than doubled the benchmark size to 1,422 questions while maintaining its focus on evaluating genuine reasoning capabilities rather than memorization. The evaluation across multiple models demonstrated substantial performance improvements, particularly for larger models on image-based questions, with some achieving scores high enough to pass 90% of university courses.

## Method Summary
The method involves updating the BLUEX dataset with recent Brazilian university entrance exams (2024-2025) from Unicamp and USP, then generating image captions using GPT-4o under two conditions: blind (image only) and context-aware (image plus question and alternatives). These captions replace images in the evaluation prompts, converting multimodal questions into text-only format. The evaluation uses the `lm-evaluation-harness` framework to test various LLMs (GPT-4o, DeepSeek-V3, Llama-4-Maverick, Qwen2.5-7B, Gemma-3-4B) on three conditions: no image, blind caption, and context caption. The updated dataset and evaluation code are publicly available on Hugging Face and GitHub.

## Key Results
- Dataset expanded from 740 to 1,422 questions (more than doubled)
- Context captions proved more efficient than blind captions while maintaining performance
- Large models (DeepSeek-V3, Llama-4-Maverick) achieved over 10-point accuracy gains on image-only questions
- Best-performing models reached scores sufficient to pass 90% of university courses

## Why This Works (Mechanism)
The core innovation converts multimodal problems (image + text) into text-only problems by using a vision model (GPT-4o) to generate descriptive captions for all images. This "multimodal-to-text projection" allows text-only language models to access visual information through natural language descriptions. The context-aware captions, which include the question and alternatives, are particularly effective because they can focus on relevant visual details rather than describing everything in the image. This approach enables fair evaluation of text-only models on questions that originally required visual reasoning, while maintaining the benchmark's focus on genuine reasoning capabilities by using recent exam questions that models couldn't have been trained on.

## Foundational Learning
- **Concept: Multimodal-to-Text Projection**
  - **Why needed here:** Converts image-based questions into text-only format using GPT-4o captions
  - **Quick check question:** If a graph contains a key piece of evidence in its visual trend (e.g., an upward curve), how would a "blind caption" vs. a "context caption" likely differ in describing it?

- **Concept: Data Contamination and Temporal Splits**
  - **Why needed here:** Ensures evaluation on recent (2024-2025) questions to avoid testing on training data
  - **Quick check question:** Why is it crucial to use exams from after a model's training cutoff date to accurately measure its reasoning capability versus its memorization?

- **Concept: Mixture of Experts (MoE) Architectures**
  - **Why needed here:** Top-performing models (DeepSeek-V3, Llama-4-Maverick) use MoE architectures
  - **Quick check question:** How does a Mixture of Experts model differ from a dense model of the same total parameter count, and what advantage might this provide on a complex benchmark like BLUEX?

## Architecture Onboarding

**Component map:** Source Data (Unicamp/USP exams 2018-2025) -> Captioning Pipeline (GPT-4o generates blind/context captions) -> Evaluation Harness (lm-evaluation-harness presents text-only prompts) -> Analysis Module (compares accuracy across conditions)

**Critical path:** The caption generation pipeline is the most critical component. The GPT-4o prompt used to generate context-aware captions is the linchpin; suboptimal prompts will compromise the entire text-only evaluation.

**Design tradeoffs:**
- **Caption Fidelity vs. Brevity:** Context captions are shorter and more targeted but risk omitting important details; blind captions are more thorough but add token cost and potential noise
- **Model Coverage vs. Fairness:** Expands accessibility to text-only models but introduces dependency on GPT-4o's performance, creating an additional point of failure

**Failure signatures:**
- **Cascade Failure:** Text-only model fails because captioning model misinterpreted diagram axis labels
- **Token Limit Exceeded:** Blind captions for detailed images could exceed context window of smaller models
- **Artificial Inflation:** Context captions inadvertently reveal answers (e.g., "correct answer is C")

**First 3 experiments:**
1. **Reproduce captioning effect:** Select 20 image-based questions, generate both blind and context captions using GPT-4o, compare length and content, verify key visual information preservation
2. **Evaluate text-only baseline:** Run Qwen 2.5-7B on image-based questions with no caption, blind caption, and context caption; quantify performance gain
3. **Analyze failure case:** Find example where GPT-4o fails on image-based question even with context caption; investigate whether failure was due to poor caption or model reasoning error

## Open Questions the Paper Calls Out
- **Open Question 1:** How do LLMs perform on open-ended, dissertative questions compared to multiple-choice format? The current study restricts to multiple-choice questions, ignoring exams' written components that test generative capabilities.
- **Open Question 2:** Do reported performances and captioning benefits generalize to entrance exams from universities other than Unicamp and USP? The current data is specific to two top-tier Brazilian universities, which may have unique question styles not representative of other institutions.
- **Open Question 3:** Does effectiveness of shorter context captions versus longer blind captions vary significantly across specific macro-areas (e.g., Mathematics vs. Languages)? Fine-grained analysis by subject area is not provided despite reporting dataset statistics by macro-area.

## Limitations
- Caption quality dependency: Entire text-only evaluation depends on GPT-4o's caption generation quality, not empirically verified across all 610 image questions
- Temporal generalization: Study doesn't analyze whether performance gains from captions will generalize to future exam formats
- Model-specific architecture effects: Performance differences between dense and MoE models are noted but not deeply analyzed

## Confidence
- **High Confidence:** Dataset expansion from 740 to 1,422 questions is verifiable through public repository; accuracy gains for large models on image-only questions are clearly demonstrated
- **Medium Confidence:** Context captions being more efficient is supported by Figure 3 but not validated for every question type; performance gains due to caption quality relies on temporal split but lacks explicit contamination testing
- **Low Confidence:** Claim that best models can pass 90% of university courses is based on admission thresholds but doesn't account for differences between exam formats and actual course assessments

## Next Checks
1. **Caption Quality Audit:** Manually inspect 50 random image questions where context captions were used; verify whether each caption includes all critical visual information needed to answer correctly
2. **Temporal Contamination Test:** Select 50 questions from 2018-2023 period and test against same models using context captions; compare accuracy to 2024-2025 questions to detect contamination
3. **Architecture Ablation:** Run subset of image-based questions on both dense model (Llama-4-Maverick) and its MoE counterpart using context captions; analyze whether MoE advantage is consistent across all question types or specific to certain visual reasoning tasks