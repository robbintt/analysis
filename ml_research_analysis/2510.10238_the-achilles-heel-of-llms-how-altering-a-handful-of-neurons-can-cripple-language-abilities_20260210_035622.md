---
ver: rpa2
title: 'The Achilles'' Heel of LLMs: How Altering a Handful of Neurons Can Cripple
  Language Abilities'
arxiv_id: '2510.10238'
source_url: https://arxiv.org/abs/2510.10238
tags:
- neurons
- critical
- neuron
- masking
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) contain
  a small set of critical neurons essential for their core functionality, analogous
  to biological neural systems. The authors propose a perturbation-based causal identification
  method that uses Monte Carlo sampling to quantify neuron sensitivity and greedy
  search to identify the minimal neuron set whose masking causes catastrophic performance
  collapse.
---

# The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities

## Quick Facts
- **arXiv ID**: 2510.10238
- **Source URL**: https://arxiv.org/abs/2510.10238
- **Reference count**: 40
- **Primary result**: As few as three neurons can completely disable a 72B-parameter LLM, increasing perplexity by up to 20 orders of magnitude

## Executive Summary
This paper reveals a critical vulnerability in large language models: a small set of neurons that, when disrupted, can cause catastrophic performance collapse. Using a perturbation-based causal identification method, the authors demonstrate that ultra-sparse neuron sets (as few as 3-45 neurons) exist across 21 models ranging from 0.5B to 72B parameters. These critical neurons concentrate in outer-layer MLP down_proj components and exhibit sharp phase-transition behavior when disrupted, with perplexity increases of 10^4 to 10^21 times. The findings suggest fundamental architectural weaknesses in current LLM designs that could have significant implications for model robustness and security.

## Method Summary
The authors propose a two-stage perturbation-based method to identify critical neurons. Stage 1 uses Monte Carlo sampling with Gaussian noise injection to compute importance scores for each neuron based on activation sensitivity. Stage 2 employs greedy search to find the minimal neuron set whose masking causes performance to drop below a threshold (perplexity increase ≥ 10×). The method is applied to 21 LLMs across diverse datasets and benchmarks, consistently identifying ultra-sparse critical neuron sets that cause phase-transition collapse when simultaneously disrupted.

## Key Results
- Critical neuron sets as small as 3 neurons can completely disable a 72B-parameter model
- Critical neurons concentrate in outer layers and MLP down_proj components rather than being uniformly distributed
- Performance degradation exhibits sharp phase transitions (all-or-nothing collapse) rather than gradual decline
- Method demonstrates exceptional robustness, consistently identifying same critical neurons across different inputs, model variants, and precision settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbation-based sensitivity ranking identifies functionally critical neurons
- Mechanism: Inject Gaussian noise into inputs, measure activation differences per neuron, and rank by accumulated sensitivity. High-sensitivity neurons are more likely to be causally critical because they amplify input variations across the network.
- Core assumption: Neurons with high activation sensitivity to input perturbations correlate with functional criticality for language modeling.
- Evidence anchors:
  - [abstract] "propose a Perturbation-based Causal Identification of Critical Neurons method to systematically locate such critical neurons"
  - [section 3, Stage 1] Importance score computed as Monte Carlo estimator converging to expected sensitivity: Imp(s) = E[|f_θ,s(x) - f_θ,s(x + α·ε)|]
  - [corpus] Weak direct corpus support; related work (KnowBias) addresses neuron-level interventions but for debiasing, not criticality identification
- Break condition: If sensitivity and criticality decouple (e.g., neurons are sensitive but redundant), ranking fails to identify the minimal set.

### Mechanism 2
- Claim: Critical neurons form a cooperatively dependent set causing phase-transition collapse
- Mechanism: Individual neuron masking produces mild degradation because remaining neurons compensate. Simultaneous masking of the complete critical set breaks functional integrity, causing cascading information bottlenecks.
- Core assumption: Critical neurons participate in a tightly coupled circuit where each depends on inputs from others.
- Evidence anchors:
  - [abstract] "Performance degradation exhibits sharp phase transitions, rather than a gradual decline, when these critical neurons are disrupted"
  - [section 4.2, Table 5] Masking any subset of 4 critical neurons in Llama-3.2-1B causes PPL increase from ~18 to ~20–700, but masking all 4 causes PPL ~1.59M
  - [corpus] No direct corpus evidence for cooperative neuron dependence in LLMs
- Break condition: If critical neurons are independently functional rather than cooperative, masking any single one would cause substantial degradation.

### Mechanism 3
- Claim: MLP down_proj in outer layers acts as an information compression bottleneck
- Mechanism: Outer-layer down_proj components compress high-dimensional representations to embedding space. Disrupting these bottleneck neurons prevents proper integration of processed information, causing system-wide collapse.
- Core assumption: Down_proj components in outer layers serve non-redundant compression functions that cannot be bypassed.
- Evidence anchors:
  - [abstract] "critical neurons...tend to concentrate in the outer layers, particularly within the MLP down_proj components"
  - [section 4.2, Figure 2] Blue bars (down_proj) dominate critical neuron distribution across all tested models
  - [section 4.3, Table 3] Masking 1000 neurons in up_proj/gate_proj causes PPL increase from ~13–18 to ~40–500, whereas masking 3–7 down_proj neurons causes PPL increase to 10^4–10^6
  - [corpus] Weak corpus connection; related work on "super weights" (Yu et al., 2025) cited as consistent finding but not directly in corpus
- Break condition: If architectures redesign down_proj to distribute compression across multiple neurons or layers, concentration vulnerability decreases.

## Foundational Learning

- **Concept**: Perplexity as exponential average negative log-likelihood
  - Why needed here: The paper uses perplexity change (Δ in log10 scale) as the primary metric for detecting catastrophic collapse.
  - Quick check question: If a model's perplexity increases from 10 to 10^6, what is Δ in log10 units?

- **Concept**: Monte Carlo estimation for expected sensitivity
  - Why needed here: Stage 1 uses K noise samples to estimate each neuron's importance score, relying on Law of Large Numbers for convergence.
  - Quick check question: Why does increasing K from 10 to 100 improve ranking stability?

- **Concept**: Phase transitions in high-dimensional systems
  - Why needed here: Collapse behavior is threshold-based (all-or-nothing) rather than gradual, indicating collective critical neuron dependence.
  - Quick check question: If masking neurons 1, 2, and 3 individually causes PPL increases of 5%, 3%, and 4% respectively, but masking all three together causes 10^6× increase, what does this suggest about their relationship?

## Architecture Onboarding

- **Component map**: Input sequence → Embedding layer → Transformer layers (Attention + MLP) → MLP up_proj (expansion) → MLP gate_proj (gating) → MLP down_proj (compression) → Output logits
- **Critical path**: Forward pass computes clean activations A_clean → K noisy forward passes compute A_noisy_i for i=1..K → Accumulate |A_clean - A_noisy_i| per neuron → Sort neurons by importance → Greedy masking top-n → Evaluate PPL → Stop when log10(PPL_masked/PPL_original) ≥ 1
- **Design tradeoffs**:
  - Noise scale α: Too low fails to distinguish critical neurons; too high may activate out-of-distribution paths (paper uses α=5)
  - Sample count K: Higher improves convergence but costs compute (paper uses K=100)
  - Threshold ε: Too low includes non-critical neurons; too high may prevent convergence (paper uses ε=1)
  - Step size Δn: Smaller is more precise but slower (paper uses Δn=1)
- **Failure signatures**:
  - Random masking of 1000 neurons: PPL increases ~2–10× (numerical drift, not collapse)
  - Masking non-down_proj components: Similar mild degradation
  - Masking partial critical set: Moderate PPL increase (~1.1–40×)
  - Masking complete critical set: PPL increase 10^4–10^21× with repetitive/garbled outputs (e.g., "////////////" or "[chars]{\\)")
- **First 3 experiments**:
  1. Reproduce on a small model (Qwen2.5-0.5B): Run Stage 1 with α=5, K=100 on 30-token input, confirm 3 critical neurons identified, verify PPL collapse from ~18 to ~1.7M when masked
  2. Ablate noise scale: Test α ∈ {1, 3, 5, 7} with fixed K=100 on Llama-3.2-1B, plot critical neuron count vs α to confirm convergence at α≥5
  3. Test component specificity: Mask top-100 neurons in down_proj vs up_proj vs gate_proj on same model, compare PPL changes to verify down_proj vulnerability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training-time interventions such as dropout, pruning, or adversarial training effectively reduce or eliminate the dependency on ultra-sparse critical neurons without degrading model performance?
- Basis in paper: [explicit] The conclusion and Appendix A.11 explicitly call for research to understand if "techniques like dropout, pruning, or adversarial training can reduce critical neuron dependencies."
- Why unresolved: The paper identifies the vulnerability and potential defensive strategies but does not experimentally validate specific training regimens designed to mitigate the formation of these sparse bottlenecks.
- What evidence would resolve it: Experiments showing that models trained with specific regularization techniques (e.g., targeted dropout) require masking a significantly larger number of neurons to trigger a performance collapse compared to standard models.

### Open Question 2
- Question: What architectural modifications can distribute critical computational functions more evenly throughout the network to prevent catastrophic collapse from minimal perturbations?
- Basis in paper: [explicit] Section 5 (Conclusion) states the authors aim "to develop strategies for architectural improvements that more evenly distribute critical computational functions throughout the network."
- Why unresolved: Current transformer architectures naturally concentrate critical functionality in ultra-sparse neuron sets within outer-layer MLP components, but it is unknown if alternative topologies can avoid this structural weakness.
- What evidence would resolve it: The design and validation of a new model architecture where the "critical neuron" phenomenon is absent or significantly diluted, requiring a much higher percentage of neurons to be disrupted to cause failure.

### Open Question 3
- Question: Is there a functional connection between the critical neurons identified in MLP down_proj components and the "attention sink" mechanism observed in LLMs?
- Basis in paper: [explicit] Appendix A.11 states that "The potential connection between our identified critical neurons and the attention sink mechanism... warrants further investigation," noting both exhibit sparse dependencies.
- Why unresolved: While the paper identifies the spatial location of critical neurons (outer MLP layers) and attention sinks (often first tokens), the potential causal or structural link between these two distinct sparse phenomena remains unexplored.
- What evidence would resolve it: Correlative or causal analysis demonstrating that disrupting the identified critical neurons directly impacts attention sink behavior, or vice versa, suggesting a shared dependency on these specific neurons.

## Limitations

- The method's effectiveness on non-Transformer architectures (RNNs, CNNs) or models with different MLP designs remains unknown
- The correlation between sensitivity and criticality may not capture all functionally important neurons, particularly those that are robust but non-sensitive to Gaussian noise
- The comparison to biological neural systems is primarily conceptual without demonstrating actual circuit-level dependencies

## Confidence

- **High Confidence**: The existence of ultra-sparse critical neuron sets (3-45 neurons) that can cause catastrophic perplexity collapse (10^4-10^21× increase) across diverse model families
- **Medium Confidence**: The concentration of critical neurons in outer-layer MLP down_proj components represents a fundamental architectural vulnerability
- **Low Confidence**: The claim that these critical neurons form cooperatively dependent circuits analogous to biological systems

## Next Checks

1. **Cross-Architecture Verification**: Apply the perturbation method to a convolutional neural network (CNN) and an RNN performing text classification. Compare whether critical neuron concentration patterns exist and whether they follow similar phase-transition collapse behavior. This tests whether the phenomenon extends beyond Transformer architectures.

2. **Circuit Dependency Mapping**: After identifying critical neurons in Llama-3.2-1B, perform ablation studies where pairs of critical neurons are masked together versus individually. Measure whether pairwise masking effects are additive or synergistic, and use activation visualization to trace information flow disruption patterns through the network layers.

3. **Noise Sensitivity Analysis**: Systematically vary the noise scale α from 0.1 to 20 and sample count K from 10 to 500 on a 7B-parameter model. Plot critical neuron identification stability (Jaccard similarity across runs) and minimum critical set size as functions of these hyperparameters. This determines whether the method's results are robust to parameter choices or depend critically on specific values.