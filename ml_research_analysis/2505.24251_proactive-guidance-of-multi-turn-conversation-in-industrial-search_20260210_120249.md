---
ver: rpa2
title: Proactive Guidance of Multi-Turn Conversation in Industrial Search
arxiv_id: '2505.24251'
source_url: https://arxiv.org/abs/2505.24251
tags:
- guidance
- user
- goal
- arxiv
- g-sft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-phase framework for proactive guidance
  in multi-turn search conversations, addressing the challenges of goal adaptation
  and low-latency real-time interactions. The first phase employs a Goal Adaptation
  Agent that dynamically tracks user goal shifts and generates concise contextual
  summaries, reducing latency by 65.5% while improving CTR by 10.18%.
---

# Proactive Guidance of Multi-Turn Conversation in Industrial Search

## Quick Facts
- arXiv ID: 2505.24251
- Source URL: https://arxiv.org/abs/2505.24251
- Reference count: 13
- One-line primary result: Two-phase framework achieving 25.28% CTR (149.06% relative improvement) in industrial search conversations

## Executive Summary
This work addresses the challenge of generating proactive guidance in multi-turn search conversations, where user goals shift dynamically and low-latency responses are critical. The authors propose a two-phase framework that first adapts to goal shifts through a Goal Adaptation Agent, then aligns guidance generation with user click preferences via a generate-rank paradigm. The system successfully balances goal tracking and click optimization, achieving significant improvements in both click-through rates and inference speed in industrial-scale deployment.

## Method Summary
The framework employs a two-phase approach: (1) G-SFT (Goal-Driven Supervised Fine-Tuning) uses a Goal Adaptation Agent to dynamically track user goal shifts and generate concise contextual summaries, then distills LLM capabilities into a compact model via scalable knowledge transfer, achieving 69.55% latency reduction; (2) C-RL (Click-driven Reinforcement Learning) employs a generate-rank paradigm with Diverse Beam Search and a click estimator to construct preference pairs from user clicks, optimizing the model for click alignment through Direct Preference Optimization.

## Key Results
- Goal Adaptation Agent reduces inference latency by 65.5% while improving CTR by 10.18%
- C-RL phase achieves 25.28% CTR (149.06% relative improvement) and 86.10% accuracy (23.95% improvement over baseline)
- The generate-rank paradigm successfully bridges single-click user signals with k-guidance generation requirements

## Why This Works (Mechanism)

### Mechanism 1: Goal Adaptation Agent (GAA)
Replacing raw conversation history with concise, goal-relevant context enables faster inference while preserving guidance quality in multi-turn conversations with shifting user intents. The GAA produces explicit goal analysis, goal-relevant summary, and shift detection signal, reducing token count while maintaining goal-tracking capability.

### Mechanism 2: Scalable Knowledge Transfer via CoT Discarding
Distilling LLM capabilities into compact models through chain-of-thought generation (then discarding CoT at inference) achieves comparable guidance quality with 69.55% latency reduction. Large teacher LLM processes inputs and produces CoT reasoning plus n candidates, human annotators filter to k best, and compact student model fine-tunes on input-output pairs.

### Mechanism 3: Generate-Rank Paradigm for k-Guidance Preference Alignment
Combining Diverse Beam Search with click estimator scoring and MMR-based sampling bridges the gap between single-click user signals and k diverse guidance requirements. The system generates diverse candidates, scores each candidate's click probability, and uses MMR to balance high-CTR and semantic diversity for k-pair preference construction.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Core RL method for aligning guidance generation with user click preferences without training a separate reward model
  - Quick check question: Can you explain why DPO eliminates the need for explicit reward model sampling compared to PPO-based RLHF?

- **Concept: Diverse Beam Search (DBS)**
  - Why needed here: Generates semantically diverse k guidance candidates by partitioning beams into independent groups with inter-group dissimilarity penalties
  - Quick check question: How does the dissimilarity penalty δ(·,·) in Equation 5 prevent all beam groups from converging to similar outputs?

- **Concept: Maximum Marginal Relevance (MMR)**
  - Why needed here: Balances click probability (relevance) and semantic diversity when selecting k guidance from candidate pool
  - Quick check question: In Equation 7, what happens to selected guidance set if λ=1.0 versus λ=0.0?

## Architecture Onboarding

**Component map:**
Phase 1 (G-SFT): User query Qi + Answer Ai + GAA outputs (Ei, Si, Di) → Teacher LLM generates CoT + n candidates → Human annotation filters to k guidance → Compact model fine-tuned via next-token prediction
Phase 2 (C-RL): Deployed G-SFT model collects click data → 1-pair DPO creates augmentation model → DBS decoding generates diverse candidates → Click estimator scores each candidate → MMR sampling constructs k-pair preference data → DPO optimization produces final C-RL model

**Critical path:**
1. GAA summary quality—If summaries are inaccurate or shift detection fails, all downstream guidance is misaligned
2. Click estimator calibration—If predicted CTR doesn't correlate with actual clicks, preference pairs are corrupted
3. DBS beam_group_numb=4—Optimal diversity-quality tradeoff; B=1 limits diversity, B=8 introduces noise

**Design tradeoffs:**
- Latency vs. capability: Compact model requires upfront distillation investment; inference drops from 2.89s to 0.88s
- Diversity vs. coherence: More beam groups increase diversity but risk semantic drift
- Binary reset vs. stateful tracking: Current approach loses context on temporary goal shifts

**Failure signatures:**
- Guidance persisting on old topics after user goal shift → GAA detection signal failed
- All k guidance semantically redundant → DBS diversity penalty too weak OR click estimator biased
- High offline ACC but low online CTR → Annotator preferences misaligned with real user behavior

**First 3 experiments:**
1. GAA component ablation—Run with only Summary, +Detection, +Explicit analysis to validate each component's contribution
2. DBS beam_group sweep—Test B∈{1,2,4,8} on held-out conversations to find domain-optimal diversity-quality tradeoff
3. Click estimator calibration—Plot predicted CTR vs. actual CTR by decile; retrain estimator if top-decile predictions don't achieve highest actual CTR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the summary reset mechanism be redesigned to preserve context when users make temporary goal shifts before returning to previous intentions?
- Basis: Current methodology resets Si when goal shifts are detected, failing to accommodate temporary shifts resulting in loss of information when users return to previous intentions.
- Why unresolved: The current binary reset discards potentially useful context; implementing multi-state or hierarchical context tracking without adding latency is an unsolved design problem.
- What evidence would resolve it: A comparative study showing that maintaining goal-specific context stacks improves CTR when users exhibit goal-switching behavior patterns.

### Open Question 2
- Question: Does excluding the current answer Ai from the Goal Adaptation Agent systematically reduce guidance quality in certain conversation patterns?
- Basis: Section 2.2.1 notes that "the current answer Ai is not used in GAA since it does not reflect the user's intent" to avoid latency, but this design choice may sacrifice information.
- Why unresolved: While latency-motivated, this exclusion has not been ablated; answers may contain implicit signals about user satisfaction or emerging needs that could improve guidance relevance.
- What evidence would resolve it: An offline ablation study comparing GAA with and without Ai access, measuring accuracy differences across conversation types where answer content correlates with next-turn intent.

### Open Question 3
- Question: Does the generate-rank paradigm for constructing k-pair preferences from single-click data introduce systematic biases compared to true k-choice user preferences?
- Basis: Section 2.3 constructs preference pairs through a multi-step pipeline to bridge single-click data and k-output requirements.
- Why unresolved: This synthetic construction may not reflect actual user preferences when simultaneously presented with k options; the pipeline could compound estimation errors or favor certain guidance styles.
- What evidence would resolve it: A controlled user study collecting genuine k-choice preference data, comparing model performance when trained on synthetic vs. authentic k-pair preferences.

## Limitations
- GAA mechanism implementation details are underspecified, requiring significant engineering assumptions for reproduction
- Empirical evidence relies heavily on Baidu's internal datasets and A/B testing infrastructure that aren't publicly available
- Click signal quality depends on position bias and accidental clicks not addressed in the paper

## Confidence

**High confidence**: The generate-rank paradigm with DPO and Diverse Beam Search is technically sound and well-supported by established literature. The latency reduction claims (69.55% via CoT discarding, 65.5% via GAA) are specific and measurable.

**Medium confidence**: The two-phase framework architecture is coherent, but empirical evidence relies on proprietary data and testing infrastructure.

**Low confidence**: GAA mechanism's effectiveness depends critically on implementation details not provided in the paper. The shift detection signal and summary generation process could fail in edge cases not covered by reported experiments.

## Next Checks

1. **Component ablation study**: Implement the three GAA variants (Summary-only, +Detection, +Explicit analysis) and measure ACC and latency on a held-out dataset to verify Table 2's 10.81% ACC improvement claim and ensure the 65.5% latency reduction holds outside Baidu's environment.

2. **Click estimator calibration**: Plot predicted click probability vs. actual click rate across deciles. If top-decile predictions don't achieve the highest actual CTR, the click estimator needs retraining with more diverse data or additional features to prevent corrupted preference pairs.

3. **Diverse Beam Search sensitivity**: Sweep beam_group_numb across {1, 2, 4, 8} on 100 multi-turn conversations and measure both CTR and semantic diversity (using embedding-based similarity). Verify that B=4 provides optimal tradeoff and that B=8 doesn't degrade CTR as reported.