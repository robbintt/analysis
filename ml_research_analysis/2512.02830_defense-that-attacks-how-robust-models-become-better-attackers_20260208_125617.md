---
ver: rpa2
title: 'Defense That Attacks: How Robust Models Become Better Attackers'
arxiv_id: '2512.02830'
source_url: https://arxiv.org/abs/2512.02830
tags:
- adversarial
- training
- attacks
- target
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reveals a paradoxical security finding: adversarially\
  \ trained (AT) models produce more transferable adversarial examples than standardly\
  \ trained models, despite being more robust in white-box settings. The authors trained\
  \ 36 diverse CNN and ViT architectures with identical AT protocols and conducted\
  \ comprehensive transferability experiments using MIG attacks with \u03B5=16."
---

# Defense That Attacks: How Robust Models Become Better Attackers

## Quick Facts
- arXiv ID: 2512.02830
- Source URL: https://arxiv.org/abs/2512.02830
- Authors: Mohamed Awad; Mahmoud Akrm; Walid Gomaa
- Reference count: 16
- Key outcome: Adversarially trained (AT) models produce more transferable adversarial examples than standardly trained models, despite being more robust in white-box settings.

## Executive Summary
This paper reveals a paradoxical security finding: adversarially trained models produce more transferable adversarial examples than standardly trained models, despite being more robust in white-box settings. The authors trained 36 diverse CNN and ViT architectures with identical AT protocols and conducted comprehensive transferability experiments using MIG attacks with ε=16. The most striking result is that AT→AT attacks achieve only 13.16% target accuracy on average (strongest transfer), compared to 32.88% for ST→ST attacks. This creates an ecosystem-level risk where widespread AT adoption could inadvertently strengthen black-box attack capabilities.

## Method Summary
The authors constructed a 36-model zoo with 8 CNN and ViT architectures in both standardly trained (ST) and adversarially trained (AT) variants. They used Free Adversarial Training (ℓ∞, ε=2, α=0.6, replay m=4) for AT models and standard training for ST models. For the benchmark, they generated adversarial examples using Momentum Integrated Gradient (MIG) attacks with ε=16 on 4 surrogate models (2 CNN, 2 ViT in ST/AT forms) and tested transfer to 16 target models. They evaluated target accuracy across 2,000 ImageNet validation images and used integrated gradients to analyze feature attributions.

## Key Results
- AT→AT attacks achieve only 13.16% target accuracy on average (strongest transfer), compared to 32.88% for ST→ST attacks
- AT→ST attacks achieve 26.58% target accuracy versus 48.09% for ST→AT attacks
- Integrated gradients analysis shows AT models focus on semantically meaningful features rather than local textures, explaining why their perturbations transfer more effectively across architectures

## Why This Works (Mechanism)

### Mechanism 1: Semantic Feature Alignment via Adversarial Training
- **Claim:** Adversarial training shifts model representations from texture-local features to semantically meaningful, object-level features, which creates shared vulnerability directions across models.
- **Mechanism:** AT models learn to focus on robust, semantically meaningful features (e.g., head, body, legs) rather than local textures. Perturbations targeting these semantic features transfer better because they align with common visual concepts that multiple models learn, rather than model-specific idiosyncrasies.
- **Core assumption:** The integrated gradients analysis accurately reflects what features models use for classification and causally drives transferability.
- **Evidence anchors:**
  - [abstract]: "Integrated gradients analysis shows AT models focus on semantically meaningful features rather than local textures, explaining why their perturbations transfer more effectively across architectures."
  - [section 4.3]: "AT models tend to produce more semantically oriented attributions... These specific, object-level attributions indicate that AT models rely on features that capture high level visual concepts rather than narrowly focusing on local textures or background cues."
- **Break Condition:** If integrated gradients attributions do not causally determine transferability, or if the observed transferability is driven by other factors (e.g., gradient alignment alone), this mechanism is insufficient.

### Mechanism 2: Shared Vulnerability Direction in AT→AT Pairs
- **Claim:** The strongest transferability occurs when AT models attack other AT models because adversarial training induces similar feature representations across different architectures, creating a shared attack surface.
- **Mechanism:** Different AT models learn to focus on similar semantically meaningful regions, creating a shared vulnerability. An adversarial example that successfully disrupts these regions will transfer effectively between AT models, yielding the observed low AT→AT target accuracy (13.16%).
- **Core assumption:** AT models across architectures converge on sufficiently similar semantic feature representations.
- **Evidence anchors:**
  - [abstract]: "AT→AT attacks achieve only 13.16% target accuracy on average (strongest transfer), compared to 32.88% for ST→ST attacks."
  - [section 4.1]: "The AT→AT condition is even stronger than the AT→ST case... which suggests that AT induces similar robust features across models and thus a shared vulnerability."
- **Break Condition:** If AT models do not share meaningfully similar feature representations, or if the AT→AT transfer advantage is due to other factors, this mechanism is weakened.

### Mechanism 3: Implementation Invariance of Integrated Gradients
- **Claim:** The MIG attack's use of integrated gradients provides implementation invariance, allowing perturbations to transfer across fundamentally different architectures (CNNs and ViTs).
- **Mechanism:** MIG computes gradients along a path from baseline to input, accumulating attribution information. This approach is designed to be implementation invariant, meaning it identifies influential features independent of the specific model architecture, enabling cross-architecture transfer.
- **Core assumption:** The implementation invariance property of integrated gradients is robust and holds across the tested CNN and ViT architectures.
- **Evidence anchors:**
  - [abstract]: "AT→AT attacks achieve only 13.16% target accuracy on average (strongest transfer)..."
  - [section 2.1]: "A critical feature of IG, known as implementation invariance. This feature allows attacks crafted by MIG to be effectively transferred between different architectures, such as CNN and ViT, or even between both types of models."
- **Break Condition:** If integrated gradients are not truly implementation invariant in practice, or if other factors dominate transferability, this mechanism is less relevant.

## Foundational Learning

- **Concept: Adversarial Training (AT)**
  - **Why needed here:** AT is the core intervention being studied. The paper compares AT models to standardly trained (ST) models to understand how the training method affects adversarial example transferability. Without understanding AT (training on adversarially perturbed data to improve robustness), the paradoxical findings cannot be contextualized.
  - **Quick check question:** Explain how adversarial training differs from standard training in its optimization objective.

- **Concept: Transferability of Adversarial Examples**
  - **Why needed here:** The entire paper centers on how well adversarial examples generated on one model (surrogate) deceive another model (target). Transferability is the key metric and phenomenon being investigated. It enables black-box attacks where the target model's parameters are unknown.
  - **Quick check question:** Define transferability in the context of adversarial attacks and explain its significance for black-box attacks.

- **Concept: Integrated Gradients (IG) and Momentum Integrated Gradient (MIG) Attack**
  - **Why needed here:** The paper uses MIG as its primary attack method for benchmarking. MIG is built upon integrated gradients, an interpretability method. The paper credits IG's "implementation invariance" as a factor in MIG's cross-architecture transferability. Understanding these concepts is necessary to interpret the methodology and the proposed mechanism for transferability.
  - **Quick check question:** Describe how the Momentum Integrated Gradient (MIG) attack uses integrated gradients to generate adversarial examples and why this might improve transferability.

## Architecture Onboarding

- **Component map:** Surrogate Models -> Attack Module (MIG) -> Target Models -> Evaluation Benchmark -> Analysis Module (Integrated Gradients)
- **Critical path:**
  1. **Model Zoo Construction:** Train or obtain 36 CNN and ViT models with identical AT protocols (using Free Adversarial Training) and their ST counterparts.
  2. **Benchmark Generation:** Apply the MIG attack (ε=16) to the surrogate models using the 2,000-image benchmark set to create adversarial examples.
  3. **Transferability Evaluation:** Test the generated adversarial examples on all target models and record their accuracy (lower accuracy = better transfer).
  4. **Mechanism Analysis:** Use integrated gradients to visualize and compare feature attributions of AT vs. ST models to understand the drivers of transferability.

- **Design tradeoffs:**
  - **Surrogate/Target Selection:** Using 8 surrogates and 16 targets (instead of all 72x72 pairs) balances computational cost with experimental coverage of architecture, training type, and capacity dimensions. This requires careful selection of representative models.
  - **Attack Method:** Choosing MIG (over PGD) prioritizes transferability analysis, especially across CNN-ViT boundaries, due to IG's implementation invariance property. However, this may not reflect performance under all attack scenarios.
  - **Perturbation Budget (ε=16):** A high budget is used to maximize attack signal for the transferability benchmark, but this may overestimate transferability compared to more constrained, realistic attack scenarios.

- **Failure signatures:**
  - **Low Transferability (High Target Accuracy):** If AT→AT or AT→ST attacks consistently show high target accuracy (e.g., above 40%), the central paradox is not present.
  - **Inconsistent Architecture Patterns:** If the AT advantage is not consistent across CNN→CNN, ViT→ViT, CNN→ViT, and ViT→CNN transfers, the phenomenon may be architecture-specific rather than general.
  - **Contradictory IG Analysis:** If integrated gradients do not show a clear shift toward semantic features in AT models, the proposed mechanism is undermined.

- **First 3 experiments:**
  1. **Training-Type Matrix Replication:** Select 4 surrogate models (2 CNN, 2 ViT) in ST and AT forms, and 8 target models (4 CNN, 4 ViT) in ST and AT forms. Generate adversarial examples with MIG (ε=16) and compute the 4x4 training-type interaction matrix (average target accuracy) to verify the AT→AT, AT→ST, ST→AT, ST→ST ordering.
  2. **Cross-Architecture Transfer Check:** Using the setup from Experiment 1, isolate transfers by architecture type (CNN→CNN, ViT→ViT, CNN→ViT, ViT→CNN). Plot target accuracy to confirm that AT surrogates consistently yield lower accuracy than ST surrogates across all four categories.
  3. **Integrated Gradients Visualization:** Compute and visualize integrated gradients for a few ST and AT models from the same architecture family (e.g., DenseNet121-ST vs. DenseNet121-AT) on sample images. Qualitatively verify if AT model attributions are more semantically coherent (focusing on object parts) and ST model attributions are more texture-local.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adversarial training objectives that replace PGD with integrated gradients in the inner maximization preserve robustness while reducing transferability?
- **Basis in paper:** Authors explicitly propose exploring "alternative adversarial training objectives that replace PGD in inner maximization with integrated gradients, and rigorously evaluate their impact on robustness and black-box transfer success."
- **Why unresolved:** This is a proposed direction that has not been tested. Current AT methods (Free AT, TRADES) all use gradient-based inner maximization that may contribute to the shared semantic features driving transferability.
- **What evidence would resolve it:** Training models with IG-based AT objectives and measuring both white-box robustness and transfer rates using the proposed benchmark methodology.

### Open Question 2
- **Question:** Does the AT transferability paradox generalize to newer architectures such as DeiT, Swin transformers, hybrid CNN-Transformer models, and Mamba-based vision models?
- **Basis in paper:** Authors call for extending analysis "to a broader family of vision backbones beyond the foundational transformer used here, including Data-Efficient Image Transformer (DeiT), Swin transformers, hybrid CNN-Transformer architectures, and recent Mamba-based vision models."
- **Why unresolved:** The current study covers foundational ViTs and CNNs but architectural innovations may exhibit different feature learning dynamics that alter the transferability paradox.
- **What evidence would resolve it:** Replicating the 36-model zoo experimental protocol with these architectures and comparing transfer patterns.

### Open Question 3
- **Question:** How can defenses be designed to preserve per-model robustness without homogenizing feature representations across models?
- **Basis in paper:** The conclusion states that "Future defenses should aim to preserve per-model robustness without creating such common attack surfaces" and identifies that "AT may inadvertently create ecosystem-level vulnerabilities by homogenizing feature representations."
- **Why unresolved:** The paper documents the problem but does not propose or test mitigation strategies. The mechanism analysis suggests the issue stems from AT driving all models toward similar semantic features.
- **What evidence would resolve it:** Developing and testing defense variants that maintain robustness while maximizing feature diversity across independently trained models.

### Open Question 4
- **Question:** Do alternative adversarial training methods (TRADES, iGAT) exhibit the same transferability paradox as Free Adversarial Training?
- **Basis in paper:** The study exclusively uses Free AT, while the introduction mentions "several variants such as TRADES and iGAT have emerged." The mechanism analysis attributes transferability to AT-induced feature changes, but whether all AT variants produce this effect remains unknown.
- **Why unresolved:** Different AT formulations optimize different objectives (e.g., TRADES explicitly trades accuracy for robustness) and may induce different feature representations.
- **What evidence would resolve it:** Conducting controlled transferability experiments comparing Free AT, TRADES, and iGAT using identical architectures and the MIG benchmark.

## Limitations
- The study's central finding rests on the assumption that integrated gradients attributions accurately reflect model decision-making and causally drive transferability.
- The choice of MIG attack with ε=16 perturbation budget may overestimate transferability compared to more realistic, constrained attack scenarios.
- The study focuses on ImageNet and CNN/ViT architectures, limiting generalizability to other domains or model families.

## Confidence
- **High Confidence:** The empirical observation that AT models consistently outperform ST models as surrogate attackers across all architecture combinations.
- **Medium Confidence:** The mechanism explanation that AT models focus on semantically meaningful features rather than local textures, explaining superior transferability.
- **Low Confidence:** The specific claim about AT→AT attacks achieving only 13.16% target accuracy as evidence of shared vulnerability.

## Next Checks
1. **Ablation Study on MIG Parameters:** Test transferability sensitivity to MIG's momentum parameter (μ), approximation steps (s), and perturbation budget (ε) to determine which factors most influence the AT advantage.
2. **Alternative Attribution Method Comparison:** Apply SHAP or Grad-CAM to the same AT/ST model pairs to verify whether integrated gradients uniquely captures the semantic feature shift or if other attribution methods show similar patterns.
3. **Cross-Domain Transferability Test:** Evaluate the AT/ST transferability paradox on a non-ImageNet dataset (e.g., CIFAR-10 or medical imaging) to assess generalizability beyond the original experimental setting.