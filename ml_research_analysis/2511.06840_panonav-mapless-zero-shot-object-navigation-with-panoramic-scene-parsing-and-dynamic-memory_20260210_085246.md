---
ver: rpa2
title: 'PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing
  and Dynamic Memory'
arxiv_id: '2511.06840'
source_url: https://arxiv.org/abs/2511.06840
tags:
- navigation
- object
- memory
- agent
- mapless
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PanoNav, a mapless zero-shot object navigation
  framework that uses only RGB images. It addresses the limitations of existing methods
  that rely on depth sensors or prebuilt maps, and those that make short-sighted decisions
  without historical context.
---

# PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory

## Quick Facts
- **arXiv ID**: 2511.06840
- **Source URL**: https://arxiv.org/abs/2511.06840
- **Reference count**: 10
- **Primary result**: Achieves 43.5% Success Rate (SR) and 23.7% Success-weighted Path Length (SPL) on HM3D benchmark, outperforming RGB-only baselines

## Executive Summary
PanoNav addresses the challenge of mapless zero-shot object navigation using only RGB images, overcoming limitations of existing methods that rely on depth sensors or suffer from short-sighted decision-making. The framework introduces two core innovations: Panoramic Scene Parsing, which leverages multi-view RGB inputs and an MLLM to generate rich local and global scene descriptions; and Dynamic Memory-Guided Decision-Making, which uses a bounded memory queue to incorporate exploration history and avoid local deadlocks. Experiments on the HM3D benchmark demonstrate that PanoNav significantly outperforms representative RGB-only mapless baselines, achieving 43.5% SR and 23.7% SPL.

## Method Summary
PanoNav is a mapless zero-shot object navigation framework that processes six directional RGB views (60° intervals) at each timestep. Each RGB image is converted to a dot matrix representation via Scaffold preprocessing, and both modalities are fed to an MLLM (Qwen-2.5-VL) to extract spatial relationships and scene summaries. A Dynamic Bounded Memory Queue stores recent global summaries, and a separate LLM (DeepSeek-V3) receives local/global information plus memory to select actions. The motion controller (PixNav) executes discrete actions until the target object is found or timeout occurs.

## Key Results
- Achieves 43.5% Success Rate (SR) and 23.7% Success-weighted Path Length (SPL) on HM3D benchmark
- Outperforms RGB-only baselines: PixNav (37.9% SR, 20.5% SPL) and ZSON (25.5% SR, 12.6% SPL)
- Memory-augmented version achieves 82.0% escape rate from local deadlocks vs 32.0% without memory

## Why This Works (Mechanism)

### Mechanism 1: Multi-view RGB with dot matrix preprocessing
- **Claim**: Multi-view RGB with dot matrix preprocessing enables spatial reasoning without depth sensors
- **Mechanism**: Six directional RGB views are converted to dot matrix representations via Scaffold processing. Both modalities are fed to an MLLM, which extracts geometric distance cues from RGB and planar positional relationships from dot matrices, constructing a spatial relation graph in latent space
- **Core assumption**: MLLMs can infer 3D spatial structure from complementary 2D visual cues without explicit depth
- **Evidence**: MLLM outputs describe object lists, room types, and spatial relationships; related work (SSR-ZSON) addresses spatial reasoning but via different hierarchical frameworks

### Mechanism 2: Dynamic Bounded Memory Queue
- **Claim**: Dynamic Bounded Memory Queue prevents local deadlocks by providing historical exploration context
- **Mechanism**: A FIFO queue stores global scene summaries from recent timesteps. The LLM decision module receives current local/global info plus the queue, enabling it to recognize and avoid previously explored regions
- **Core assumption**: LLMs can semantically distinguish locations from textual summaries and suppress revisits
- **Evidence**: Memory-guided method achieves 82.0% escape rate vs 32.0% memory-less; DTS(f) reduced from 6.7 to 4.7

### Mechanism 3: Decoupled perception and decision-making
- **Claim**: Decoupling perception (MLLM) from decision-making (LLM) outperforms end-to-end MLLM decision outputs
- **Mechanism**: MLLM first generates structured textual descriptions (object lists, room types, spatial relationships). These intermediate representations, plus memory queue, are input to a separate LLM for action selection
- **Core assumption**: Current MLLMs struggle with end-to-end reasoning when given rich, unstructured visual input and history simultaneously
- **Evidence**: Decoupled parsing+decision achieves 43.5% SR vs 35.0% SR for one-step MLLM decision

## Foundational Learning

- **Zero-Shot Object Navigation (ZSON)**
  - Why needed: The entire framework targets mapless, open-vocabulary navigation without task-specific training
  - Quick check: Can you explain why ZSON differs from closed-set ObjectNav with pre-trained policies?

- **Multimodal Large Language Models (MLLMs) for Scene Understanding**
  - Why needed: PanoNav relies on Qwen-2.5-VL to parse spatial relationships from visual inputs
  - Quick check: What types of spatial relationships can current MLLMs reliably extract from indoor scenes?

- **Local Deadlocks in Mapless Navigation**
  - Why needed: The core problem PanoNav addresses. Without maps or memory, agents revisit explored regions or circle within semantically similar areas
  - Quick check: Why do object-room priors (e.g., "sofas are in living rooms") exacerbate local deadlocks?

## Architecture Onboarding

- **Component map**: Image Preprocessing -> Panoramic Scene Parsing (MLLM) -> Dynamic Bounded Memory Queue -> LLM Decision-Making -> Motion Controller (PixNav)

- **Critical path**: Capture 6 RGB views → Generate dot matrix images via SCA preprocessing → MLLM parses both modalities → Update memory queue → LLM receives local/global info + memory → Output action → Motion controller executes action

- **Design tradeoffs**: 6-view vs 3-view (43.5% SR vs 19.5% SR); queue length n affects context vs token cost; decoupled approach adds latency but improves SR by 8.5 percentage points

- **Failure signatures**: Circular trajectories / repetitive actions in same region → local deadlock; sudden drops in SR when transitioning to new environments → MLLM spatial parsing failing; high token usage with no SR improvement → memory queue too large or redundant summaries

- **First 3 experiments**: 1) Memory ablation replication: Run 10 episodes with memory enabled vs disabled on deadlock-prone scenarios to verify escape rate improvement (32% → 82%). 2) Queue size sensitivity: Test n ∈ {3, 5, 10, 15} on HM3D validation subset to identify optimal tradeoff. 3) View reduction stress test: Confirm 6-view → 3-view degradation (19.5% SR) and explore intermediate configurations.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can multimodal information be effectively utilized to construct memory queues for more robust mapless navigation?
- **Open Question 2**: Is the dual-model framework computationally efficient enough for real-time deployment on physical robots?
- **Open Question 3**: Can the RGB-only spatial parsing approach close the performance gap with depth-enabled mapless methods?

## Limitations
- Memory queue length n is unspecified, making it impossible to reproduce exact memory capacity or token usage
- Dot matrix generation method relies on external Scaffold (SCA) technique without implementation details
- LLM/LLM prompt formats and examples are not provided, leaving significant ambiguity in the decision-making pipeline

## Confidence
- **High confidence**: Core innovation of decoupling perception from decision-making and memory-augmented approach to prevent local deadlocks
- **Medium confidence**: Dot matrix preprocessing mechanism, as it's referenced but not fully detailed
- **Medium confidence**: 6-view panoramic setup effectiveness, given the dramatic performance gap with 3-view
- **Low confidence**: Exact implementation details required for faithful reproduction

## Next Checks
1. Replicate the memory ablation study (32% → 82% escape rate) on deadlock-prone scenarios to verify memory effectiveness
2. Conduct a systematic queue length sensitivity analysis (n ∈ {3, 5, 10, 15}) to identify optimal memory capacity vs. token cost tradeoff
3. Perform a view reduction stress test (6-view → 3-view degradation) and explore intermediate configurations to find minimal viable setup