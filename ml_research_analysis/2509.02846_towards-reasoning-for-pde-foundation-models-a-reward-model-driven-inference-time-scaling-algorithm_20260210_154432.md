---
ver: rpa2
title: 'Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling
  Algorithm'
arxiv_id: '2509.02846'
source_url: https://arxiv.org/abs/2509.02846
tags:
- reasoning
- reward
- time
- foundation
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first test-time-compute (TTC) reasoning
  framework for PDE foundation models, using reward models to guide selection among
  multiple candidate predictions during inference. By generating stochastic predictions
  and selecting the best based on analytical or learned reward models (PRMs), the
  approach achieves substantial performance gains with minimal additional training
  data.
---

# Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm

## Quick Facts
- arXiv ID: 2509.02846
- Source URL: https://arxiv.org/abs/2509.02846
- Authors: Siddharth Mansingh; James Amarel; Ragib Arnab; Arvind Mohan; Kamaljeet Singh; Gerd J. Kunde; Nicolas Hengartner; Benjamin Migliori; Emily Casleton; Nathan A. Debardeleben; Ayan Biswas; Diane Oyen; Earl Lawrence
- Reference count: 40
- One-line primary result: First test-time-compute reasoning framework for PDE foundation models achieves state-of-the-art accuracy with only 6.25% of fine-tuning data and 5M parameters

## Executive Summary
This work introduces a test-time-compute (TTC) reasoning framework for PDE foundation models that uses reward models to guide selection among multiple candidate predictions during inference. By generating stochastic predictions through Monte Carlo dropout and selecting the best based on analytical or learned reward models, the approach achieves substantial performance gains with minimal additional training data. The method demonstrates significant sample efficiency—achieving state-of-the-art accuracy with only 6.25% of the fine-tuning data required by baseline models—and reduces model size to approximately 5 million parameters compared to 21M–0.7B in prior work. On compressible Euler equation simulations, TTC consistently improves predictions and physical conservation metrics, particularly when using process reward models.

## Method Summary
The framework uses a ViT encoder-decoder foundation model with dropout kept active at inference to generate B stochastic candidate predictions per timestep. A reward model (either analytical based on conservation laws or learned through contrastive training) scores these candidates, with the highest-scoring prediction selected for the next step. The process reward model is trained via contrastive triplet margin loss on 100 predictions per initial condition, ranked by ground truth MSE. This creates a greedy selection algorithm that acts as a local corrector at each timestep, reducing autoregressive error accumulation. The approach is tested on PDEGym's compressible Euler datasets with pretraining on RP, CRP, Gauss, and KH datasets, then finetuning on downstream tasks with 8–512 trajectories.

## Key Results
- Achieves state-of-the-art accuracy with only 6.25% of fine-tuning data required by baseline models
- Reduces model size to ~5 million parameters compared to 21M–0.7B in prior work
- Process reward models consistently outperform analytical reward models across all tested downstream tasks
- Sample gain (MSE_B/MSE_B=1) improves monotonically with increasing branching factor B

## Why This Works (Mechanism)

### Mechanism 1: Reward-Guided Greedy Selection
If a base model is forced to be stochastic at inference, selecting the candidate with the highest "physics consistency" score reduces autoregressive error accumulation compared to a deterministic rollout. The framework utilizes Monte Carlo dropout during inference to generate B candidate predictions for a single timestep. A reward model scores these candidates based on spatio-temporal consistency. By greedily selecting the highest-scoring candidate, the system filters out trajectories that violate physical constraints or diverge from the learned manifold before they can compound errors in subsequent steps.

### Mechanism 2: Contrastive Process Reward Modeling (PRM)
A learned reward model, trained via contrastive loss on model outputs, can approximate solution quality more flexibly than fixed analytical equations, particularly when training data contains noise or biases. Instead of relying solely on explicit conservation laws, the authors train a neural network to distinguish between "good" and "bad" predictions. They sample 100 predictions per step, rank them by Ground Truth MSE, and form triplets to train the PRM using a contrastive margin loss. This allows the reward signal to adapt to the specific failure modes of the foundation model.

### Mechanism 3: Inference-Time Compute Scaling for Sample Efficiency
Allocating compute at inference (increasing branching factor B) allows smaller models trained on significantly less data to match the performance of larger, data-hungry models. The method treats compute as a substitute for training data/parameters. By scaling the search width during inference, the model explores the solution space explicitly. This acts as an external verification loop, effectively "thinking" through the problem rather than relying solely on weights to produce the correct answer immediately.

## Foundational Learning

- **Concept: Autoregressive Error Accumulation**
  - Why needed here: The paper explicitly targets the "compounding errors" common in PDE foundation models during long-horizon rollouts. Understanding this drift is essential to grasping why "test-time compute" helps—it acts as a local corrector at every step.
  - Quick check question: Can you explain why a small error at timestep t=5 might explode by t=20 in a chaotic fluid dynamics simulation?

- **Concept: Monte Carlo (MC) Dropout**
  - Why needed here: Standard neural networks are deterministic at inference. This method requires a mechanism to generate diverse candidates for the reward model to rank. MC dropout provides this stochasticity without changing the architecture.
  - Quick check question: How does keeping dropout active during inference (MC Dropout) approximate a probability distribution over model outputs?

- **Concept: Compressible Euler Equations & Conservation Laws**
  - Why needed here: The "Analytical Reward Model" relies entirely on the physics of the Euler equations (conservation of mass, momentum, energy). The results show that momentum conservation is violated in the data, which breaks the analytical reward assumption.
  - Quick check question: Why might momentum conservation be violated in a numerical simulation of the Euler equations (hint: discretization), and how would that affect a reward model relying on that metric?

## Architecture Onboarding

- **Component map:** Input IC -> Base ViT (with dropout) -> B candidate predictions -> Reward Model (ARM/PRM) -> Greedy selection -> Output prediction -> Next timestep

- **Critical path:** Train Base ViT on pretraining data (RP, CRP, Gauss, KH) -> Generate PRM training data: Run Base ViT on training set, sample 100 outputs per step, sort by MSE to create triplets -> Train PRM using contrastive loss on triplets -> Run Inference: Input IC -> Branch B predictions (using dropout) -> Score with PRM -> Greedy Select -> Next step

- **Design tradeoffs:**
  - ARM vs. PRM: ARMs are free (no training) and interpretable but brittle if data is imperfect. PRMs are flexible and higher-performing but require data generation and separate training.
  - Branching Factor (B): Higher B improves accuracy but linearly increases latency and GPU memory usage (batch size increases).
  - Patch Size (ViT-3/5/7): The paper tests odd-sized patches (3x3, 5x5, 7x7) to better approximate differential operators, trading global context (larger patches) for finer local resolution.

- **Failure signatures:**
  - Momentum Divergence: If using ARMs, the model may degrade because the ground truth simulation itself violates momentum conservation (noted in Results).
  - PRM Overfitting: The PRM may simply memorize the error patterns of the training set and fail to generalize to Out-of-Distribution (OOD) initial conditions.
  - Latency Bottleneck: Setting B=1000 on a high-resolution grid may exceed practical inference time limits for real-time forecasting.

- **First 3 experiments:**
  1. Baseline vs. Stochasticity: Run the Base ViT on a validation set with dropout OFF (B=1) vs. dropout ON (B=1) to measure the raw performance gap/drop introduced by the stochastic mechanism.
  2. ARM Validation: Implement Eq. 6 (Mass Conservation) and plot the correlation between the ARM score and actual MSE on the test set to verify if "better physics" actually implies "better accuracy" for this dataset.
  3. Scaling Law Check: Measure MSE vs. Compute for branching factors B ∈ {1, 10, 100, 1000} on the RPUI (downstream) task to verify if performance saturates or improves monotonically as claimed.

## Open Questions the Paper Calls Out
- Can integrating reinforcement learning (RL) into the feedback loop improve performance beyond the current greedy selection strategy? The authors suggest future research should explore "richer forms of adaptive reasoning, potentially integrating limited reinforcement-learning loops."
- Does reward-driven test-time compute constitute a novel inference paradigm, or does it merely replicate classical predictor-corrector methods? The authors explicitly question whether the method "simply replicates classical constraint enforcement or represents a genuinely novel inference paradigm."
- What are the formal scaling laws that describe the trade-off between training data volume and test-time compute (branching factor) for PDE foundation models? The paper notes that "obtaining scaling laws requires further investigation" despite observing that models with fewer trajectories can match the performance of data-rich models.

## Limitations
- Data imperfection: Ground truth simulations violate momentum conservation, which breaks the theoretical guarantee of the Analytical Reward Model.
- Computational overhead: TTC introduces linear compute cost during inference proportional to the branching factor B, potentially prohibitive for real-time applications.
- Generalization scope: Results are demonstrated on compressible Euler equations only; performance on other PDE families remains unverified.

## Confidence
- High confidence: The core mechanism of using inference-time compute for candidate selection is well-supported by the results.
- Medium confidence: The superiority of PRM over ARM is convincing but the paper doesn't fully explore why.
- Medium confidence: The sample efficiency claim (6.25% training data) is impressive but depends heavily on pretraining data quality.

## Next Checks
1. Test the TTC framework on a different PDE family (e.g., Navier-Stokes or diffusion equations) to verify if performance gains transfer beyond compressible Euler.
2. Conduct a controlled experiment where the PRM is trained on corrupted/synthetic ground truth to measure sensitivity to data quality and identify failure modes.
3. Measure end-to-end inference latency vs. accuracy for different B values on hardware representative of deployment scenarios.