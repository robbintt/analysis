---
ver: rpa2
title: 'PINN Balls: Scaling Second-Order Methods for PINNs with Domain Decomposition
  and Adaptive Sampling'
arxiv_id: '2510.21262'
source_url: https://arxiv.org/abs/2510.21262
tags:
- which
- training
- pinn
- should
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PINN BALLS, a scalable second-order training
  method for Physics-Informed Neural Networks (PINNs) using domain decomposition and
  adaptive sampling. The method addresses the memory limitations of second-order optimizers
  by combining sparse radial basis functions for domain decomposition with a learnable
  probability distribution for adaptive sampling.
---

# PINN Balls: Scaling Second-Order Methods for PINNs with Domain Decomposition and Adaptive Sampling

## Quick Facts
- **arXiv ID:** 2510.21262
- **Source URL:** https://arxiv.org/abs/2510.21262
- **Reference count:** 40
- **Key outcome:** Scalable second-order training method for PINNs using domain decomposition and adaptive sampling achieves superior accuracy with memory-efficient Levenberg-Marquardt optimization

## Executive Summary
This paper introduces PINN BALLS, a novel approach for training Physics-Informed Neural Networks that combines domain decomposition with second-order optimization. The method addresses the memory limitations of traditional second-order methods by using sparse radial basis functions (RBFs) to create a block-sparse Jacobian structure. A key innovation is the adversarial adaptive sampling (AAS) framework that jointly trains both the network parameters and the domain decomposition itself, allowing the model to adaptively allocate computational resources to challenging regions of the solution domain.

The approach demonstrates significant improvements in both accuracy and efficiency compared to state-of-the-art methods. On benchmark problems including Helmholtz, Burgers, and Navier-Stokes equations, PINN BALLS achieves relative L2 errors reaching 10⁻⁹ for Helmholtz and 10⁻⁴ for Burgers. The method shows excellent scalability properties, maintaining accuracy as ensemble size increases while keeping memory requirements manageable through sparse Jacobian representations.

## Method Summary
PINN BALLS implements a Mixture of Experts architecture where each expert is a local PINN model gated by a learnable radial basis function. The domain is decomposed into overlapping regions (balls), with each ball having its own neural network submodel. A global Levenberg-Marquardt optimizer updates all parameters simultaneously using a sparse Jacobian representation enabled by the RBF gating functions. The method also incorporates an adversarial adaptive sampling framework where a learnable probability distribution focuses collocation points on regions with high PDE residuals. Training proceeds through alternating descent steps (LM update on network parameters) and ascent steps (gradient ascent on sampling distribution parameters), with the domain decomposition adapting to the complexity of the PDE solution throughout training.

## Key Results
- Achieves relative L2 errors of approximately 10⁻⁹ for Helmholtz equation and 10⁻⁴ for Burgers equation
- Demonstrates superior accuracy compared to state-of-the-art methods including vanilla PINNs, FBPINN, and XPINN
- Shows excellent scalability with ensemble size while maintaining memory efficiency through sparse Jacobian representations
- Successfully solves complex PDEs including the Navier-Stokes equations with significantly reduced computational resources

## Why This Works (Mechanism)

### Mechanism 1: Sparsity-Enabled Second-Order Optimization
The use of compactly-supported radial basis functions for domain decomposition induces a block-sparse structure in the network's Jacobian matrix, making the memory-intensive Levenberg-Marquardt second-order optimizer tractable. Each local expert submodel is gated by a quartic RBF with finite radius, creating a ball of influence. Because the derivative of the gating function is zero outside this radius, the gradient contribution of that expert to the overall model output is also zero outside its ball of influence, making the full Jacobian matrix sparse and allowing efficient storage and inversion.

### Mechanism 2: Adversarial Training of the Domain Decomposition
The learnable domain decomposition (placement and size of the RBF balls) is effectively trained via an adversarial minimax game, adapting the decomposition to the complexity of the PDE solution. The probability distribution for sampling collocation points is parameterized as a linear combination of the RBF bases and trained via gradient ascent to maximize the PDE residual (focusing on hard-to-learn regions), while the main PINN parameters are trained via gradient descent to minimize it. This forces RBF balls to migrate toward and constrict around difficult regions.

### Mechanism 3: Global Second-Order Update on a Mixture of Experts
A single, global Levenberg-Marquardt update step can be efficiently applied across the entire ensemble of local experts, avoiding difficulties of training interface conditions common in other domain decomposition methods. The ensemble model is treated as a single functional approximator with sparse structure, computing a single global update for all parameters. This avoids separate interface loss terms, as the overlapping RBF gating functions inherently ensure smooth global solutions.

## Foundational Learning

- **Concept:** Radial Basis Functions (RBFs) as Gating Mechanisms
  - **Why needed here:** Core of the architecture. Understanding how a local expert is gated is essential for understanding why the Jacobian becomes sparse and how the model is both local and globally continuous.
  - **Quick check question:** How does the gradient of a quartic RBF behave at the boundary of its support radius, and why is this critical for the sparsity mechanism?

- **Concept:** Levenberg-Marquardt (LM) Algorithm
  - **Why needed here:** The paper claims success from using a second-order optimizer (LM) on a problem that typically scales poorly with it. Knowing LM blends gradient descent and Gauss-Newton is key to understanding its benefits and memory cost.
  - **Quick check question:** What matrix does LM require that a standard first-order optimizer (like Adam) does not, and how does the PINN BALLS architecture make this matrix sparse?

- **Concept:** Minimax Optimization / Adversarial Training
  - **Why needed here:** Training is not simple minimization but a game between the model (minimizing error) and the sampler (maximizing error). Understanding this alternating dynamic is crucial for debugging training stability.
  - **Quick check question:** In the AAS framework, does the sampler try to put more or less probability mass in regions where the PDE residual is high, and how does this action train the domain decomposition?

## Architecture Onboarding

- **Component map:** Inputs (x) -> Mixture of Experts (MoE) Layer (M PINN Balls, each with local network U_j and quartic RBF φ_j) -> Gating Function (normalized weighting λ_j) -> Final Output (weighted sum u_θ(x)) -> Adversarial Sampler (parameterized distribution p_φ)

- **Critical path:**
  1. Initialize RBF centers and radii (e.g., via K-means)
  2. In each training iteration:
      a. Sample collocation points using current distribution p_φ
      b. **Descent Step:** Perform one global LM update on all network parameters θ (using sparse Jacobian)
      c. **Ascent Step:** Perform gradient ascent on parameters of p_φ (RBF centers/radii) to focus sampling on high-residual regions
      d. Decay learning rate for the ascent step

- **Design tradeoffs:**
  - **Number of Balls vs. Expert Capacity:** For fixed parameter budget, balance many small experts vs. fewer large experts. Paper suggests each expert needs enough capacity to solve its local sub-problem.
  - **Sparsity vs. Coverage:** Smaller radii increase sparsity and memory savings but risk "holes" in domain coverage if balls don't overlap sufficiently. Initialization must ensure sum(phi_l(x)) > 0 everywhere.

- **Failure signatures:**
  - **Out-of-Memory Error:** Jacobian became too dense. Cause: RBF radii grew too large. Fix: Cap radius or use stronger regularization.
  - **Slow/No Convergence:** Adversarial sampling too aggressive, causing loss oscillation. Cause: Learning rate for p_φ too high. Fix: Implement exponential decay for sampler's learning rate.
  - **Poor Accuracy in Specific Region:** Domain decomposition failed to allocate experts to difficult region. Cause: Poor initialization or weak adversarial signal. Fix: Adjust initialization or ascent learning rate schedule.

- **First 3 experiments:**
  1. **Sparsity Benchmark:** Train a single PINN Ball and dense PINN of same architecture on trivial 1D problem. Measure and compare memory consumption and runtime of LM step to validate core sparsity claim.
  2. **DD Visualization:** Train PINN Balls on 1D or 2D PDE with known localized feature (e.g., Burgers' discontinuity). Plot positions and radii of balls at initialization, mid-training, and final training to visually confirm adaptive decomposition.
  3. **Ablation Study:** Compare performance of PINN Balls with (a) fixed domain decomposition vs. (b) full learnable AAS method to quantify contribution of adaptive sampling to final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can viable preconditioning strategies be developed to enable the use of matrix-free solvers like Conjugate Gradient within the PINN BALLS framework?
- **Basis in paper:** Section 4.1 notes that matrix-free methods are sensitive to poor conditioning and states, "Finding a viable preconditioning strategy remains thus an interesting direction of future study."
- **Why unresolved:** The authors restricted the scope to direct sparse solvers to avoid the instability caused by ill-conditioned Jacobians typical in PINN training.
- **What evidence would resolve it:** A preconditioning method that stabilizes iterative solvers for PINN BALLS without compromising the memory efficiency gained from sparse representations.

### Open Question 2
- **Question:** How can the number of experts (submodels) be dynamically scaled during training to optimize the trade-off between ensemble size and individual model capacity?
- **Basis in paper:** The conclusion in Section 6 notes that results hint at a trade-off between the number of submodels and their capacity, "motivating future research into the dynamical scaling of the number of experts."
- **Why unresolved:** The current implementation requires the number of experts (M) and their architecture to be fixed prior to training.
- **What evidence would resolve it:** A mechanism that adds or removes experts adaptively during the training process to maintain optimal accuracy and parameter efficiency.

### Open Question 3
- **Question:** Does performing the Adversarial Adaptive Sampling (AAS) ascent update with respect to the Wasserstein distance improve the convergence of the domain decomposition?
- **Basis in paper:** Section 6 proposes that "performing the AAS ascent update with respect to the Wasserstein distance may result in smoother updates, improving the convergence of the DD partition scheme."
- **Why unresolved:** The current implementation relies on gradient updates with an L² flavor, while Wasserstein geometry is theoretically suggested but untested.
- **What evidence would resolve it:** Experimental results comparing the convergence speed and stability of Wasserstein updates versus standard gradient ascent on complex PDE benchmarks.

## Limitations
- The method's scalability to very high-dimensional problems (beyond 2D/3D) remains unexplored and may face challenges with the curse of dimensionality
- Memory efficiency gains depend critically on maintaining sufficient Jacobian sparsity, which requires careful tuning of RBF radii and may not hold for all problem types
- The adversarial training framework introduces additional hyperparameters (entropy regularization weight, sampler learning rate schedule) that require careful tuning for stable convergence

## Confidence

**High Confidence:** The core sparsity mechanism (RBF-induced Jacobian sparsity) and its theoretical justification are well-established. The experimental results demonstrating superior accuracy over baseline methods are clearly presented and reproducible.

**Medium Confidence:** The adversarial adaptive sampling framework's convergence properties and practical implementation details have moderate confidence. While the theory is sound, the empirical sensitivity to hyperparameters needs more extensive study.

**Low Confidence:** The scalability analysis is limited to ensemble size experiments without comprehensive ablation studies on different problem types or dimensionalities.

## Next Checks

1. **Memory Efficiency Verification:** Implement the sparse LM algorithm on a fixed problem and systematically vary the number of experts and RBF radii to quantify the actual memory savings and identify the sparsity threshold where savings diminish.

2. **Hyperparameter Sensitivity Analysis:** Conduct a grid search over the entropy regularization weight β and the sampler learning rate decay schedule to map the stability landscape and identify robust hyperparameter ranges.

3. **Scalability Stress Test:** Extend experiments beyond the current 2D problems to higher-dimensional PDEs (3D Helmholtz or Navier-Stokes) to validate the claimed scalability properties and identify potential bottlenecks in higher dimensions.