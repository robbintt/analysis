---
ver: rpa2
title: 'LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics'
arxiv_id: '2510.07626'
source_url: https://arxiv.org/abs/2510.07626
tags:
- unlearning
- arxiv
- robustness
- methods
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a full-stack investigation of large language
  model (LLM) unlearning, addressing the fragmented state of research in the field.
  The authors establish a principled taxonomy of twelve recent unlearning methods,
  categorizing them into three families: divergence-driven optimization, representation
  misalignment, and rejection-based targeted unlearning.'
---

# LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics

## Quick Facts
- arXiv ID: 2510.07626
- Source URL: https://arxiv.org/abs/2510.07626
- Authors: Chongyu Fan; Changsheng Wang; Yancheng Huang; Soumyadeep Pal; Sijia Liu
- Reference count: 19
- Key outcome: This work presents a full-stack investigation of large language model (LLM) unlearning, establishing a principled taxonomy of twelve recent methods and introducing Open-QA metrics that better capture generative performance than conventional MCQ evaluations.

## Executive Summary
This work presents a full-stack investigation of large language model (LLM) unlearning, addressing the fragmented state of research in the field. The authors establish a principled taxonomy of twelve recent unlearning methods, categorizing them into three families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. They demonstrate that conventional multiple-choice question (MCQ) evaluations provide an incomplete picture of unlearning effectiveness and utility, and introduce open question-answering (Open-QA) metrics to better capture generative performance. Their analysis reveals fundamental UE-UT tradeoffs across method families, with divergence-driven methods prone to over-forgetting and representation misalignment methods offering better utility preservation. The study also provides a comprehensive robustness assessment across model-level (in-domain relearning, out-of-domain fine-tuning, quantization) and input-level (jailbreak) attacks, showing that robustness varies substantially across method families and attack types.

## Method Summary
The paper systematically evaluates twelve unlearning methods across three families: divergence-driven optimization (NPO, SimNPO), representation misalignment (RMU, RMU+LAT, RMU+API), and rejection-based targeted unlearning (TAR, IDK+AP, IDK+AP+DPO). Using Llama-3 8B Instruct as the reference model, they train on WMDP-Bio benchmark with biosecurity forget set and Wikitext retain set. The evaluation stack includes traditional MCQ metrics (WMDP, MMLU) plus proposed Open-QA metrics using entailment scores. Robustness testing covers in-domain relearning (100 steps), out-of-domain fine-tuning (250 steps), 4-bit quantization, and jailbreak attacks. Hyperparameters are tuned within specified ranges for each method.

## Key Results
- Divergence-driven methods (NPO/SimNPO) achieve high unlearning effectiveness but suffer from over-forgetting, producing gibberish outputs in Open-QA evaluation
- Representation misalignment methods (RMU variants) better preserve utility while maintaining reasonable unlearning effectiveness
- MCQ accuracy is insufficient as a sole evaluation metric, missing cases where models collapse into random guessing
- Robustness against jailbreaking correlates strongly with in-domain relearning resistance but not with out-of-domain fine-tuning robustness

## Why This Works (Mechanism)

### Mechanism 1: Divergence-Driven Optimization and Logit Collapse
- **Claim:** Methods like NPO and SimNPO appear to achieve high unlearning effectiveness (UE) by collapsing output logits, but this mechanism inherently risks "over-forgetting" and degrading utility (UT).
- **Mechanism:** These methods maximize the divergence between the unlearned model and the reference model on the forget set ($D_f$). The paper suggests this effectively "suppresses candidate scores" uniformly rather than surgically removing knowledge. In extreme cases, the model drives the logits of correct answers down while elevating meaningless tokens, resulting in gibberish generation when queried, even on retain data.
- **Core assumption:** The optimization landscape for "forgetting" is distinct from standard training; maximizing loss on specific tokens often cascades into general capability degradation (catastrophic forgetting) rather than targeted erasure.
- **Evidence anchors:**
  - [abstract] "divergence-driven methods prone to over-forgetting"
  - [section 4] "NPO achieves unlearning by collapsing logits and inducing over-forgetting"
  - [corpus] Assumption: Standard optimization theories suggest unbounded divergence maximization destabilizes model weights.
- **Break condition:** The mechanism fails if the goal is to preserve the model's generative fluency while removing specific facts; the resulting model may output nonsensical text (e.g., "@nate@nate...").

### Mechanism 2: Representation Misalignment for Utility Preservation
- **Claim:** Disrupting internal representations (hidden states) may offer a better trade-off for utility retention (UT) compared to output-level divergence.
- **Mechanism:** Techniques like RMU (Representation Misdirection for Unlearning) operate on intermediate layers ($M_\theta(x)$). By mapping the hidden states of forget data to a random vector ($u$), the model disrupts the specific pathway for the unwanted knowledge. Because the output head (final layer) is not explicitly trained to diverge on all tokens, the model's general generative ability remains more stable.
- **Core assumption:** Knowledge is localized or routed through specific activation patterns; corrupting these patterns is sufficient to block retrieval without destroying the generative mechanics of the model.
- **Evidence anchors:**
  - [section 3] "RMU (Li et al., 2024)... maps the hidden states... to a random vector."
  - [section 4] "RMU maintains the distribution of the original model's logits but reshapes their relative distribution."
  - [corpus] Related work (e.g., "SoK: Machine Unlearning") confirms the prevalence of representation engineering as a distinct family from optimization.
- **Break condition:** This may fail if the "random vector" target inadvertently creates a detectable artifact or if the corruption propagates to downstream tasks (though the paper suggests this is less likely than with NPO).

### Mechanism 3: Correlation of Robustness via Perturbation Type
- **Claim:** Robustness to input-level jailbreaks ($Rob_{JA}$) is mechanistically linked to robustness against in-domain relearning ($Rob_{ReL}$), but not necessarily out-of-domain fine-tuning ($Rob_{FT}$).
- **Mechanism:** The paper posits that both jailbreaking and in-domain relearning act as "worst-case adversarial testing" within the same domain (the forget set). Methods that resist updating weights on $D_f$ (in-domain) tend to also resist adversarial prompts derived from $D_f$. Conversely, out-of-domain fine-tuning is a different weight perturbation (distribution shift), so resistance there does not guarantee resistance to jailbreaks.
- **Core assumption:** The geometry of the loss landscape regarding the forget set determines both input-level (prompt) and weight-level (relearning) vulnerabilities simultaneously.
- **Evidence anchors:**
  - [section 5] "Robustness against jailbreaking... aligns more closely with in-domain relearning."
  - [figure 4] Shows visual correlation between $Rob_{JA}$ and $Rob_{ReL}$.
  - [corpus] Assumption: Adversarial attack literature typically distinguishes between $L_p$-norm input perturbations (jailbreaks) and weight perturbations (fine-tuning).
- **Break condition:** This correlation breaks if a defense mechanism is specific to only one vector (e.g., input smoothing helps jailbreaks but not weight updates).

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) / Negative Preference Optimization (NPO)**
  - **Why needed here:** The paper categorizes "Divergence-driven" methods (like NPO) as a primary family. Understanding how we optimize a model to "disprefer" an output is essential to grasp the "over-forgetting" mechanism.
  - **Quick check question:** How does treating "forget data" as a negative preference differ from simple gradient ascent?

- **Concept: Hidden States / Intermediate Activations**
  - **Why needed here:** To understand the "Representation Misalignment" family (e.g., RMU), you must visualize how LLMs process data layer-by-layer. This family intervenes before the final output.
  - **Quick check question:** If you map the hidden state of a specific concept to a random vector, does the model "forget" the concept or just lose the ability to articulate it?

- **Concept: Entailment Score (ES)**
  - **Why needed here:** The paper critiques MCQ accuracy and proposes Open-QA metrics. Entailment scores (using an NLI model) are the proposed way to measure if a free-form text answer implies the "forgotten" fact.
  - **Quick check question:** Why is measuring textual entailment harder but more informative than checking for a specific keyword or multiple-choice option?

## Architecture Onboarding

- **Component map:**
  - Taxonomy: 12 methods sorted into 3 families (Divergence, Rep. Misalignment, Rejection)
  - Evaluation Stack: Traditional (MCQ: WMDP, MMLU) + Proposed (Open-QA: Entailment Score on generation)
  - Robustness Layer: Model-level (Relearning, Quantization) vs. Input-level (Jailbreaking)

- **Critical path:**
  1. Define $D_f$ (Forget Set) and $D_r$ (Retain Set)
  2. Select method family: **Divergence** (High UE, Risk of Utility Loss) vs. **Rep. Misalignment** (Better UT, potentially lower UE)
  3. **Evaluate:** *Do not* rely solely on MCQ accuracy. Run Open-QA generation to check for "gibberish" (logit collapse)
  4. **Test Robustness:** Check specifically for In-Domain Relearning (linked to Jailbreak risk)

- **Design tradeoffs:**
  - **UE vs. UT:** The paper confirms a fundamental trade-off. You cannot maximize forgetting without impacting generation quality
  - **Robustness vs. Utility:** Adding robust designs (e.g., NPO+SAM, TAR) often degrades UT (e.g., TAR lowers UT-OpenQA compared to RMU)
  - **MCQ vs. Open-QA:** MCQ is "myopic"; it may show high unlearning success even if the model is just broken (random guessing)

- **Failure signatures:**
  - **Over-forgetting:** Model outputs nonsense tokens (e.g., repeated characters) in Open-QA (seen in NPO)
  - **False Negative:** Model selects wrong MCQ answer (UE success) but generates correct answer in Open-QA (Unlearning failure)
  - **Fragile Unlearning:** Model resists direct prompts but reveals knowledge after 100 steps of fine-tuning (Low $Rob_{ReL}$)

- **First 3 experiments:**
  1. **Baseline Comparison:** Run NPO vs. RMU on WMDP-Bio. Verify NPO's tendency for lower UT-OpenQA (GSM8K/IFEval) despite similar MCQ scores
  2. **Visual Inspection:** Generate text for forget-queries. Look for "I don't know" (Rejection success) vs. "Gibberish" (Over-forgetting)
  3. **Relearning Attack:** Fine-tune the unlearned model on a small subset of $D_f$. Measure how quickly the forget accuracy recovers to distinguish between "true" unlearning and "masking"

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on Llama-3 8B Instruct, leaving open questions about performance across model scales and architectures
- The evaluation methodology relies heavily on synthetic forget sets from the WMDP-Bio benchmark, which may not capture the full complexity of real-world unlearning scenarios
- The robustness testing, while extensive, uses relatively simple attack vectors that sophisticated adversaries could potentially bypass

## Confidence
- **High Confidence:** The fundamental UE-UT tradeoff across method families and the superiority of Open-QA metrics over MCQ evaluation are well-supported by the experimental results
- **Medium Confidence:** The taxonomy and characterization of method families are grounded in the experiments, but the authors acknowledge that real-world applications may require hybrid approaches not fully explored here
- **Low Confidence:** The generalizability of robustness findings to different model architectures and attack types beyond those tested remains uncertain

## Next Checks
1. **Cross-Architecture Validation:** Replicate the study using different model families (e.g., Mistral, Qwen) to assess whether the observed method family characteristics and tradeoffs hold across architectures
2. **Real-World Unlearning Scenarios:** Test the methods on actual sensitive data (e.g., copyrighted material, personally identifiable information) rather than synthetic benchmarks to validate practical effectiveness
3. **Adversarial Robustness Extension:** Design and test more sophisticated jailbreak prompts and fine-tuning strategies to probe the limits of the reported robustness findings