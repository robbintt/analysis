---
ver: rpa2
title: LLMs to Support a Domain Specific Knowledge Assistant
arxiv_id: '2502.04095'
source_url: https://arxiv.org/abs/2502.04095
tags:
- question
- questions
- industry
- answer
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a domain-specific knowledge assistant for IFRS
  sustainability reporting using synthetic data generation and evaluation. The project
  creates a novel dataset of 1,063 diverse question-answer pairs using advanced LLM
  techniques, achieving average quality scores of 8.16/10.
---

# LLMs to Support a Domain Specific Knowledge Assistant

## Quick Facts
- arXiv ID: 2502.04095
- Source URL: https://arxiv.org/abs/2502.04095
- Reference count: 0
- Primary result: Domain-specific knowledge assistant for IFRS sustainability reporting with novel synthetic dataset and custom architectures

## Executive Summary
This work develops a domain-specific knowledge assistant for IFRS sustainability reporting using synthetic data generation and evaluation. The project creates a novel dataset of 1,063 diverse question-answer pairs using advanced LLM techniques, achieving average quality scores of 8.16/10. Two custom architectures are developed: a RAG pipeline achieving 85.32% accuracy on single-industry and 72.15% on cross-industry questions, and an LLM-based pipeline achieving 93.45% and 80.30% accuracy respectively. The LLM-based approach outperforms the baseline by 12.80 and 27.36 percentage points. A custom industry classifier and fine-tuned Llama 3.1 8B model are integrated to improve handling of complex queries across multiple industries.

## Method Summary
The methodology combines synthetic data generation with two custom architectures for domain-specific QA. Data is generated using chain-of-thought reasoning and few-shot prompting to create 1,063 QA pairs from 72 IFRS documents. The RAG pipeline uses custom markdown chunking, KNN retrieval with industry metadata filtering, and a fine-tuned Llama 3.1 8B generator. The LLM-based pipeline extracts relevant context using GPT-4o-mini before passing to the generator. Both approaches are evaluated on local and cross-industry question sets using MCQ accuracy, BLEU, and ROUGE-L metrics.

## Key Results
- Custom LLM-based pipeline achieves 93.45% accuracy on local MCQs vs 85.32% for RAG
- Cross-industry accuracy improves from 72.15% (RAG) to 80.30% (LLM-based)
- Industry classifier achieves 0.87 F1 score with 0.93 precision
- Synthetic dataset quality averages 8.16/10 across faithfulness, relevance, and domain specificity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought reasoning combined with few-shot prompting produces higher-quality synthetic QA pairs than either technique alone.
- Mechanism: CoT guides the LLM through explicit steps (select reference text → write question → generate answer options), reducing hallucination by grounding each step in source material. Few-shot examples then provide domain-specific question templates that align generation with real user query patterns. The combination constrains the output space while preserving semantic diversity.
- Core assumption: Quality of synthetic questions (faithfulness, relevance, specificity) correlates with downstream RAG system evaluation validity.
- Evidence anchors:
  - [abstract]: "Various LLM-based techniques are employed to create the dataset, including chain-of-thought reasoning and few-shot prompting. A custom evaluation framework is developed to assess question and answer quality across multiple dimensions, including faithfulness, relevance, and domain specificity. The dataset averages a score range of 8.16 out of 10 on these metrics."
  - [section 3.3.2, Table 3.1]: Local MCQ faithfulness scores: Baseline (6.17) → CoT (7.25) → CoT+Few-shot (9.45). Single Best Answer percentage improved from 67.19% to 92.19%.
  - [corpus]: SemRAG paper uses "semantic chunking and knowledge graphs without extensive fine-tuning" for domain adaptation—different technique, similar goal of improving retrieval quality without full retraining.

### Mechanism 2
- Claim: Pre-filtering the vector database by predicted industry before retrieval improves accuracy on cross-industry queries by constraining the search space to relevant documents.
- Mechanism: An LLM-based classifier (GPT-4o) first identifies which industries a user's query relates to (achieving 0.87 F1 score). The vector database is then filtered using metadata (each chunk is tagged with industry) to retrieve only from relevant industry documents. This prevents the retriever from returning chunks from tangentially related but incorrect industries that would confuse the generator LLM.
- Core assumption: The industry classifier's predictions are sufficiently accurate; false negatives (missing relevant industries) would exclude necessary context.
- Evidence anchors:
  - [abstract]: "A custom industry classifier and fine-tuned Llama 3.1 8B model are integrated to improve handling of complex queries across multiple industries."
  - [section 4.3.3, Table 4.6]: GPT-4o achieves Macro F1 score of 0.87 with 0.93 precision for industry classification, significantly outperforming ML baselines (XGBoost: 0.69 F1).
  - [section 4.4.1]: "It was found that the retriever gets chunks from multiple industries that could be related to the query, thus confusing the LLM that generates the answer."
  - [corpus]: No direct corpus evidence for metadata-filtered retrieval; related papers (SemRAG, RedSage) focus on semantic augmentation rather than pre-filtering.

### Mechanism 3
- Claim: Allowing an LLM to extract relevant context from full documents (rather than retrieving from pre-chunked embeddings) improves performance on complex queries requiring synthesis across document sections.
- Mechanism: The LLM-based pipeline passes entire industry markdown documents (not chunks) to an LLM, which identifies and extracts the most query-relevant passages. This bypasses embedding-based similarity, allowing the LLM to use reasoning capabilities to determine relevance. Extracted chunks are combined and passed to the fine-tuned generator.
- Core assumption: The extraction LLM can accurately identify relevant content without hallucinating non-existent passages; the increased number of LLM calls does not introduce compounding errors.
- Evidence anchors:
  - [abstract]: Not explicitly contrasted with RAG in the abstract.
  - [section 4.3.4, Table 4.7]: LLM-based pipeline achieves 93.45% local MCQ accuracy vs. 85.32% for custom RAG; 80.30% cross-industry vs. 72.15%.
  - [section 4.4.2]: "The LLM-based pipeline retrieves the most customised context for answering the specific query, lessening the burden on the generator LLM of utilising its own reasoning capabilities."
  - [corpus]: DeepWriter paper describes a "fact-grounded multimodal writing assistant based on offline knowledge base"—conceptually similar LLM-based retrieval but applied to writing rather than QA.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: The paper's core contribution is comparing and optimizing RAG for a domain-specific assistant; the custom pipeline builds on standard RAG components (indexing, retrieval, generation).
  - Quick check question: Why does the paper conclude that RAG with domain-specific fine-tuning outperforms unsupervised fine-tuning alone for knowledge-intensive tasks?

- Concept: Prompt engineering (Chain-of-Thought and Few-shot learning)
  - Why needed here: These techniques are the primary drivers of synthetic QA quality improvement; understanding them is essential for adapting the generation pipeline to other domains.
  - Quick check question: What specific improvement (in percentage points) did CoT+Few-shot provide over the baseline for local MCQ faithfulness scores?

- Concept: Multi-label classification with metadata filtering
  - Why needed here: Cross-industry queries require the system to identify multiple relevant industries and constrain retrieval accordingly; this is a key architectural decision that affects performance.
  - Quick check question: How does filtering the vector database by industry metadata before retrieval specifically address the failure mode identified in cross-industry queries?

## Architecture Onboarding

- Component map:
  - Data layer: 72 IFRS markdown documents (975 pages total) → chunked via 7 strategies (best: custom markdown chunking for MCQs) → embedded with OpenAI text-embedding-3-small → stored in Pinecone with metadata (page number, report name, industry, content type)
  - Classifier layer: GPT-4o or GPT-4o-mini for multi-label industry classification (Macro F1: 0.87, Precision: 0.93)
  - Retrieval layer: KNN (best) or Hybrid retrievers; top-K chunks; optional query transforms (HyDE improves local MCQs by 4%, Multi-Query improves cross-industry by 3.6% but hurts local by 40%)
  - Generation layer: Fine-tuned Llama 3.1 8B (via LoRA) or base Llama variants; temperature 0.5 for generation
  - Evaluation layer: MCQ accuracy for multiple-choice; BLEU/ROUGE-L for free-text; custom 1-10 scale metrics for faithfulness/relevance/specificity using LLM-as-judge

- Critical path:
  1. User query → LLM check for domain relevance → reject if out-of-scope
  2. Query → industry classifier (GPT-4o) → identify relevant industries
  3. For RAG pipeline: filter Pinecone by industry metadata → retrieve top-5 chunks via KNN → pass to fine-tuned Llama 3.1 8B
  4. For LLM-based pipeline: load full markdowns for identified industries → pass each to GPT-4o-mini for chunk extraction → combine extracted chunks → pass to fine-tuned Llama 3.1 8B

- Design tradeoffs:
  - Chunking strategy: Custom markdown chunking (84.52% local MCQ) preserves table integrity and hierarchy vs. fixed 512-token (80.65%) which is simpler but may split tables
  - RAG vs. LLM-based pipeline: RAG is faster and more scalable; LLM-based achieves 8-10 percentage points higher accuracy on both local and cross-industry queries but requires more LLM calls
  - Fine-tuning with RAG vs. RAG alone: Fine-tuned Llama 3.1 8B + RAG achieves 83.87% local MCQ vs. 80.65% for base model, but the gain is smaller for cross-industry (53.16% vs. 52.94%)

- Failure signatures:
  - Cross-industry retrieval returns chunks from wrong industries, confusing the generator (observed in qualitative analysis, Section 4.4.1)
  - 7.53% of generated MCQs fail Single Best Answer verification (Table 3.5), indicating ambiguous questions
  - BLEU/ROUGE scores very low for cross-industry free-text (BLEU: 0.34, ROUGE-L: 0.46) due to paraphrasing sensitivity

- First 3 experiments:
  1. Establish baseline: Llama 3.1 8B, 512-token chunks, KNN retrieval (top-5), no query transforms, temperature 0.5. Evaluate on 1,063 QA pairs (544 MCQs, 272 free-text local; 187 MCQs, 176 free-text cross-industry). Expected: ~80% local MCQ accuracy, ~53% cross-industry.
  2. Ablate chunking strategies: Compare fixed (256, 512, 1024), rolling window, semantic, and custom markdown. Hold all else constant. Hypothesis: custom markdown will outperform on MCQs (tables preserved); semantic chunking will be competitive on free-text.
  3. Integrate industry classifier: Add GPT-4o classifier with metadata filtering. Measure improvement specifically on 187 cross-industry MCQs. Expected: 15-20 percentage point improvement in cross-industry accuracy if classifier precision holds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does human expert evaluation of the synthetic QA dataset correlate with the LLM-based evaluation metrics (faithfulness, relevance, domain specificity)?
- Basis in paper: [explicit] "Human evaluation on the question dataset can be conducted to verify the quality of the generated questions and answers. This can be done by domain experts and/or people with an understanding of IFRS sustainability reporting standards."
- Why unresolved: The 1,063-question dataset was evaluated only through automated LLM-based metrics; manual quality checks were done selectively during iteration but not systematically across all questions.
- What evidence would resolve it: Correlation analysis between human expert Likert-scale ratings and automated metric scores across a representative sample of generated QA pairs.

### Open Question 2
- Question: Can LLMs generate high-quality questions spanning three or more industries using current prompt engineering methods?
- Basis in paper: [explicit] "One such limit was encountered when trying to generate questions spanning more than two industries, where the LLM was not able to capture information from all industries to create a question. Instead, it selected one or two of the given industries and made a question based on that."
- Why unresolved: Cross-industry questions were limited to two industries; the technical limitation was acknowledged but not investigated further as multi-industry questions were deemed less likely in the target domain.
- What evidence would resolve it: Systematic generation and evaluation of three+ industry questions using the CoT + few-shot pipeline, measuring faithfulness and relevance scores compared to two-industry baselines.

### Open Question 3
- Question: Does applying the quality improvement and generalization post-processing functions to the full dataset improve RAG system performance on the evaluation benchmarks?
- Basis in paper: [explicit] "The post-processing functions are cost-intensive operations... the application of the quality improvement and generalisation functions to the entire dataset is left as future work."
- Why unresolved: Post-processing was validated qualitatively on small samples (e.g., specificity improvement from 5 to 9 in one example) but not applied at scale due to computational cost.
- What evidence would resolve it: A/B comparison of RAG accuracy on datasets with and without full post-processing applied, controlling for question types.

### Open Question 4
- Question: Can the gap between local (93.45%) and cross-industry (80.30%) accuracy in the LLM-based pipeline be closed through improved reasoning capabilities or retrieval mechanisms?
- Basis in paper: [inferred] "There seems to be a limit to its performance on complex questions still, as the accuracy on cross-industry questions is 80.30% rather than being closer to the 93.25% accuracy on local MCQs. This suggests there exist limits to the reasoning capabilities of the generator LLM."
- Why unresolved: The paper identifies this gap as a fundamental limitation of current LLM reasoning, but does not test whether architectural changes (e.g., chain-of-thought at inference, larger models, or iterative retrieval) could narrow it.
- What evidence would resolve it: Systematic ablation testing cross-industry queries with enhanced reasoning prompts or larger generator models, comparing accuracy gains.

## Limitations

- The synthetic dataset may not capture the full complexity and ambiguity of real-world sustainability queries
- The 12.80-27.36 percentage point improvements depend on the quality of the synthetic evaluation set and may not translate directly to production environments
- Custom markdown chunking implementation details are not fully specified, making exact replication difficult

## Confidence

- High: Dataset quality evaluation methodology, LLM-based pipeline architecture, performance improvements over baseline
- Medium: Industry classifier effectiveness, cross-industry query handling, real-world deployment considerations
- Low: Custom chunking implementation details, LoRA fine-tuning parameters, cost-benefit analysis for production use

## Next Checks

1. Deploy the classifier and RAG pipeline on real user queries from the organization's knowledge base to measure performance degradation compared to synthetic evaluation
2. Implement the custom markdown chunking strategy with varying hierarchy depth parameters to identify optimal settings for different IFRS document structures
3. Conduct a cost analysis comparing the LLM-based pipeline (multiple API calls) versus optimized RAG approaches for sustained production workloads