---
ver: rpa2
title: Handling Missing Modalities in Multimodal Survival Prediction for Non-Small
  Cell Lung Cancer
arxiv_id: '2601.10386'
source_url: https://arxiv.org/abs/2601.10386
tags:
- survival
- clinical
- fusion
- tabular
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses survival prediction in unresectable Stage
  II-III NSCLC using multimodal deep learning under realistic clinical conditions
  of incomplete and missing data. The proposed missing-aware framework integrates
  CT, WSI histopathology, and clinical variables through a three-stage pipeline: (1)
  foundation model-based feature extraction, (2) modality-specific missing-aware encoding
  with transformer-based NAIM+ODST, and (3) intermediate fusion of latent representations
  for survival prediction.'
---

# Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer

## Quick Facts
- **arXiv ID:** 2601.10386
- **Source URL:** https://arxiv.org/abs/2601.10386
- **Reference count:** 12
- **Primary result:** NAIM+ODST intermediate fusion achieves C-index of 73.30 (WSI+Tabular) in unresectable Stage II-III NSCLC survival prediction.

## Executive Summary
This study introduces a missing-aware multimodal deep learning framework for survival prediction in unresectable Stage II-III non-small cell lung cancer. The approach integrates CT volumes, whole slide images (WSI), and clinical tabular data using foundation models for feature extraction and a three-stage pipeline: feature extraction, missing-aware encoding with NAIM+ODST, and intermediate fusion. The framework handles missing data without imputation by leveraging masked attention mechanisms, achieving superior performance compared to unimodal baselines and alternative fusion strategies.

## Method Summary
The method employs foundation models (Merlin for CT, MI-Zero+TITAN for WSI) to extract modality-specific features, which are then encoded using NAIM (Not Another Imputation Method) with double-sided masked attention to handle missing data. These encoded representations are fused at the intermediate level via ConcatODST, combining latent vectors before passing through an ODST (Oblivious Differentiable Decision Tree) head. Training uses differential learning rates (1e-6 for encoders, 1e-5 for fusion head), AdamW optimizer, and early stopping. The model is evaluated on a cohort of 179 patients using 5-fold stratified cross-validation with concordance index, Uno's C-index, and time-dependent AUC as primary metrics.

## Key Results
- WSI+Tabular configuration achieves highest C-index of 73.30 using intermediate fusion.
- Intermediate fusion outperforms unimodal baselines and early/late fusion strategies.
- Model successfully stratifies patients into clinically meaningful risk groups with statistically significant survival differences.
- Adaptive weighting observed, with CT features down-weighted when less informative.

## Why This Works (Mechanism)
The approach works by leveraging pre-trained foundation models to extract rich, modality-specific representations, then applying missing-aware encoding that preserves information even when entire modalities are absent. The intermediate fusion strategy allows the model to learn interactions between modalities at a latent level before final prediction, while the ODST head provides interpretable decision boundaries. This design enables the model to adapt its weighting of modalities based on their informativeness, particularly handling the high rate of missing WSI data (~66%).

## Foundational Learning

**Foundation Models**
- *Why needed:* Provide robust feature extraction from complex data types (CT volumes, WSIs) without training from scratch
- *Quick check:* Verify Merlin, MI-Zero, and TITAN produce expected embedding dimensions (2048 for CT, 768 for WSI)

**Masked Attention in NAIM**
- *Why needed:* Handle missing modalities without imputation, preserving uncertainty
- *Quick check:* Confirm attention masks properly suppress interactions with missing tokens before softmax

**ODST (Oblivious Differentiable Decision Trees)**
- *Why needed:* Provide interpretable decision boundaries while maintaining differentiability for end-to-end training
- *Quick check:* Validate tree depth and regularization prevent overfitting on small cohort

## Architecture Onboarding

**Component Map:** CT → Merlin → NAIM → Fusion; WSI → CLAM+MI-Zero+TITAN → NAIM → Fusion; Tabular → Direct NAIM → Fusion → ODST → Survival Prediction

**Critical Path:** Foundation Model Feature Extraction → NAIM Encoding → Intermediate Fusion → ODST Prediction

**Design Tradeoffs:** The choice of intermediate fusion over early/late fusion balances representation learning with interpretability, while NAIM avoids imputation bias but requires careful masking implementation

**Failure Signatures:**
- Performance degradation on high missingness cohorts indicates NAIM masking not properly implemented
- Validation loss divergence suggests overfitting on small cohort (n=179)
- CUDA OOM during WSI processing indicates feature extraction not done offline

**3 First Experiments:**
1. Verify foundation model feature extraction produces correct dimensions (2048 for CT, 768 for WSI)
2. Test NAIM encoding with controlled missingness patterns to confirm proper masking behavior
3. Validate differential learning rates by training with encoders at 1e-6 vs 1e-5 and comparing convergence

## Open Questions the Paper Calls Out

**Open Question 1:** Does the observed down-weighting of CT features stem from the specific pre-training of the Merlin foundation model or intrinsic limitations in the prognostic value of staging CTs?

**Open Question 2:** Can explicit attention-based mechanisms (e.g., Cross-Attention) improve performance over the learned concatenation strategy used in the intermediate fusion?

**Open Question 3:** Will the identified prognostic signatures generalize to multi-centric cohorts with heterogeneous acquisition protocols?

## Limitations
- Small cohort size (n=179) limits generalizability and increases overfitting risk
- Modest performance differences between fusion strategies suggest incremental rather than transformative gains
- Heavy reliance on external pre-trained foundation models creates black-box dependencies

## Confidence
- **High Confidence:** Intermediate fusion architecture with NAIM+ODST consistently outperforms baseline strategies
- **Medium Confidence:** Clinical risk stratification results hold up under cross-validation but need external validation
- **Low Confidence:** Claim about CT down-weighting representing clinically adaptive behavior is suggestive but not definitively proven

## Next Checks
1. Apply trained model to independent NSCLC cohort to assess external validity and performance decay
2. Remove ODST and replace with MLP head to isolate fusion strategy contribution
3. Systematically mask modalities at different rates (0%, 30%, 60%, 90%) to quantify NAIM+ODST adaptive benefits compared to imputation baselines