---
ver: rpa2
title: 'The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust
  Memorization in ReLU Nets'
arxiv_id: '2510.24643'
source_url: https://arxiv.org/abs/2510.24643
tags:
- parameters
- have
- fproj
- network
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the parameter complexity of robust memorization
  in ReLU networks, aiming to determine how many parameters are required to memorize
  any dataset with a certain robustness ratio $\rho = \mu/\epsilon$ between robustness
  radius $\mu$ and data separation $\epsilon$. The authors establish both upper and
  lower bounds on the parameter count across the entire range $\rho \in (0,1)$, showing
  that parameter complexity matches that of non-robust memorization when $\rho$ is
  small, but grows with increasing $\rho$.
---

# The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets

## Quick Facts
- arXiv ID: 2510.24643
- Source URL: https://arxiv.org/abs/2510.24643
- Authors: Yujun Kim; Chaewon Moon; Chulhee Yun
- Reference count: 40
- Primary result: Establishes necessary and sufficient conditions for robust memorization in ReLU networks, showing parameter complexity grows from $\tilde{O}(\sqrt{N})$ to $\tilde{O}(Nd^2\rho^4)$ as robustness ratio $\rho$ increases from small to large values.

## Executive Summary
This paper investigates the parameter complexity required for robust memorization in ReLU networks, determining how many parameters are needed to memorize any dataset while maintaining a specified robustness ratio $\rho = \mu/\epsilon$ between robustness radius $\mu$ and data separation $\epsilon$. The authors establish comprehensive bounds that cover the entire range $\rho \in (0,1)$, showing that robust memorization becomes increasingly expensive as the desired robustness radius grows. The work bridges the gap between non-robust and robust memorization theories, providing a complete characterization of the parameter requirements across all robustness regimes.

## Method Summary
The paper employs a combination of constructive upper bounds and information-theoretic lower bounds to characterize the parameter complexity of robust memorization. For upper bounds, the authors develop specialized network architectures with carefully designed activation patterns that can memorize any dataset while maintaining the desired robustness radius. The constructions use partitioning strategies based on data separability and leverage the properties of ReLU activations to create robust decision boundaries. For lower bounds, the analysis relies on necessary conditions derived from information-theoretic arguments about the expressiveness of shallow networks and the constraints imposed by robustness requirements.

## Key Results
- Necessary conditions: First hidden layer must have width at least $\rho^2\min\{N,d\}$ and total parameters must be $\Omega(\min\{\sqrt{N/(1-\rho^2)}, \sqrt{d}\}\sqrt{N})$
- Upper bounds: $\tilde{O}(\sqrt{N})$ parameters sufficient for $\rho < 1/(5N\sqrt{d})$, $\tilde{O}(Nd^{1/4}\rho^{1/2})$ for intermediate $\rho$, and $\tilde{O}(Nd^2\rho^4)$ for larger $\rho$
- Parameter complexity matches non-robust memorization when $\rho$ is small, but grows polynomially with $\rho$ as robustness requirements increase
- The analysis covers the complete spectrum of robustness ratios from near-zero to nearly one

## Why This Works (Mechanism)
The paper's approach works by establishing fundamental limits on what shallow ReLU networks can achieve in terms of robust memorization. The mechanism relies on the observation that robustness constraints fundamentally limit the expressive power of networks, requiring more parameters to achieve the same memorization task when stronger robustness is required. The upper bound constructions exploit the ability of ReLU networks to create piecewise linear decision boundaries that can be carefully positioned to maintain robustness while still memorizing the training data.

## Foundational Learning

**ReLU Networks** - Neural networks using rectified linear units as activation functions
*Why needed:* ReLU networks provide the computational model for which parameter complexity is analyzed
*Quick check:* Verify understanding of ReLU activation properties and how they create piecewise linear functions

**Robust Memorization** - Ability to memorize training data while maintaining predictions within $\epsilon$-balls around training points
*Why needed:* This is the core learning task being analyzed in the paper
*Quick check:* Understand the difference between standard memorization and robust memorization

**Parameter Complexity** - Minimum number of network parameters required to achieve a learning task
*Why needed:* The paper's central question is determining this quantity for robust memorization
*Quick check:* Be able to distinguish between different measures of network complexity (parameters, VC dimension, etc.)

**Data Separation** - Minimum distance between distinct data points in the dataset
*Why needed:* Plays a crucial role in determining the achievable robustness radius
*Quick check:* Calculate data separation for simple point configurations

## Architecture Onboarding

**Component Map:** Input -> First Hidden Layer (width $\geq \rho^2\min\{N,d\}$) -> Output Layer -> Prediction

**Critical Path:** The first hidden layer width is the critical bottleneck - insufficient width makes robust memorization impossible regardless of subsequent layers

**Design Tradeoffs:** Wider networks can achieve better robustness but at the cost of increased parameter count; the paper characterizes this tradeoff precisely

**Failure Signatures:** Networks with first hidden layer width below $\rho^2\min\{N,d\}$ cannot achieve robust memorization; parameter counts below the lower bounds make the task impossible

**First Experiments:**
1. Construct a ReLU network with minimal first hidden layer width and test its ability to robustly memorize a synthetic dataset
2. Vary the robustness radius $\mu$ while keeping data separation $\epsilon$ fixed and measure the required parameter growth
3. Compare the parameter efficiency of networks designed for different robustness regimes (small vs. large $\rho$)

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis restricted to binary classification with unit-norm data, limiting applicability to real-world scenarios
- Upper bounds rely on specific architectural constructions that may not be practically optimal
- Parameter counts use $\tilde{O}$ notation, potentially hiding significant logarithmic factors

## Confidence
- Theoretical framework: High
- Tightness of bounds: Medium
- Practical relevance: Medium

## Next Checks
1. Empirical verification of parameter complexity bounds using synthetic datasets with varying $\rho$ values and network architectures
2. Extension of analysis to multi-class classification settings and non-unit-norm data distributions
3. Investigation of whether logarithmic factors in $\tilde{O}$ notation significantly impact practical network designs