---
ver: rpa2
title: 'ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical
  Trial Access'
arxiv_id: '2512.08193'
source_url: https://arxiv.org/abs/2512.08193
tags:
- page
- clinical
- trial
- outcomemeasures
- trials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClinicalTrialsHub addresses the fragmentation between ClinicalTrials.gov
  and PubMed by creating a unified search platform that extracts structured trial
  information from free-text PubMed articles using large language models. The system
  processes PubMed articles to extract the same structured metadata available in ClinicalTrials.gov
  registrations, expanding searchable clinical trial data by 83.8% and making previously
  unregistered trials accessible through unified search, filtering, and question-answering
  features.
---

# ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access

## Quick Facts
- arXiv ID: 2512.08193
- Source URL: https://arxiv.org/abs/2512.08193
- Reference count: 40
- Key outcome: 83.8% expansion of searchable clinical trial data by extracting structured trial information from PubMed articles using LLMs

## Executive Summary
ClinicalTrialsHub addresses the fragmentation between ClinicalTrials.gov registry data and PubMed literature by creating a unified search platform that extracts structured trial information from free-text articles. The system uses large language models to convert PubMed publications into CTG-compatible structured fields, expanding accessible trial data by 83.8%. A user study with seven medical professionals showed high satisfaction across search features (4.14/5), information extraction accuracy (4.8/5), and chatbot quality (4.86/5), demonstrating effectiveness for clinical research tasks.

## Method Summary
The system builds a unified search platform that merges ClinicalTrials.gov registry data with PubMed literature, using LLMs to extract 211 structured fields from free-text articles. The pipeline includes query refinement, multi-source search via APIs, BM25 reranking with normalization and position bonuses, bidirectional merging for cross-referenced entries, parallel information extraction across 11 modules, and interactive QA with evidence highlighting. The architecture processes PubMed articles to extract the same structured metadata available in ClinicalTrials.gov registrations, making previously unregistered trials accessible through unified search, filtering, and question-answering features.

## Key Results
- GPT-5.1 achieves key-level F1 of 0.980 and value-level F1 of 0.890 for information extraction
- Gemini-3-Pro provides the best question-answering grounding performance with 0.897 factuality score
- 83.8% expansion of searchable clinical trial data compared to ClinicalTrials.gov alone
- User study: 6 of 7 participants found more relevant studies with ClinicalTrialsHub compared to PubMed or CTG alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified search across registries and literature increases trial discoverability compared to either source alone.
- Mechanism: BM25 scoring is applied separately to ClinicalTrials.gov (CTG) and PubMed results, scores are normalized to [0,1], and a position-based bonus is added; bidirectional merging links entries that reference each other with a relevance bonus.
- Core assumption: Users benefit from a single ranked list more than manual cross-referencing between platforms.
- Evidence anchors: 6 of 7 participants found 6 or more relevant studies with ClinicalTrialsHub compared to 5/7 for PubMed and 3/7 for CTG.

### Mechanism 2
- Claim: Modular LLM extraction converts free-text PubMed articles into CTG-compatible structured fields at high fidelity.
- Mechanism: Extraction is parallelized across 11 modules (6 protocol, 4 results, 1 derived) with explicit field definitions and enumerated value constraints; GPT-5.1 processes each module independently to reduce hallucination from information overload.
- Core assumption: The CTG schema is sufficiently comprehensive to capture trial information from publications.
- Evidence anchors: GPT-5.1 achieves key-level F1 of 0.980 and value-level F1 of 0.890 for information extraction.

### Mechanism 3
- Claim: Attributed Q&A with evidence highlighting improves user trust and verification capability for clinical trial queries.
- Mechanism: Gemini-3-Pro generates responses constrained to source documents; each sentence is classified as supported/unsupported/contradictory by judge models; clicking citations auto-scrolls and highlights source text.
- Core assumption: Users want verification capability rather than purely summarized answers.
- Evidence anchors: Gemini-3-Pro provides the best question-answering grounding performance with 0.897 factuality score.

## Foundational Learning

- Concept: BM25 ranking algorithm
  - Why needed here: Core relevance scoring for heterogeneous sources before normalization and merging.
  - Quick check question: Can you explain why BM25 scores need normalization before combining results from different corpora?

- Concept: Schema-constrained extraction
  - Why needed here: Understanding that LLM outputs are validated against CTG enumerated types and character limits to ensure database compatibility.
  - Quick check question: What happens if an LLM extracts a value not in the allowed enum set for a field like `studyType`?

- Concept: Factuality/grounding evaluation
  - Why needed here: The FACTS benchmark uses multi-judge evaluation to classify response sentences; critical for understanding QA quality metrics.
  - Quick check question: Why does the evaluation use three judge models rather than a single evaluator?

## Architecture Onboarding

- Component map: Query Refinement (GPT-5.1) -> Multi-Source Search -> Reranking/Dedup -> Information Extraction (GPT-5.1) -> Interactive QA (Gemini-3-Pro)
- Critical path: User query → query refinement → parallel API calls → BM25 reranking → merge → display → (on selection) extraction + QA
- Design tradeoffs: GPT-5.1 selected for extraction despite similar performance to Gemini-3-Pro due to cost-efficiency; Gemini-3-Pro selected for QA despite higher latency due to superior grounding.
- Failure signatures: Low recall in search: Check Boolean query construction for PubMed filters; Extraction missing fields: Verify module prompt includes target field; QA hallucination: Check if source document length exceeds 32K token context limit.
- First 3 experiments: 1) Ablate the position bonus in BM25 scoring to measure impact on ranking quality vs. pure relevance. 2) Test extraction on a sample where CTG fields are known missing but PMC contains information to measure "Extra Valid" rate. 3) Compare Gemini-3-Pro vs. GPT-5.1 QA latency/quality tradeoff on a fixed query set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a systematically validated benchmark dataset be constructed for the results section when ClinicalTrials.gov records are often incomplete or outdated compared to PubMed articles?
- Basis in paper: The authors state they plan to create a systematically validated benchmark dataset covering the results section as future work, noting that results information is often more complete in PMC articles than in CTG entries.
- Why unresolved: The current evaluation excludes results-section fields because CTG data is inconsistently maintained, making it unreliable as ground truth for automatic evaluation.

### Open Question 2
- Question: To what extent can domain-adapted LLM fine-tuning improve extraction accuracy compared to the current reliance on frontier general-purpose models like GPT-5.1 and Gemini-3-Pro?
- Basis in paper: The Conclusion states they will improve extraction models through domain-adapted LLM fine-tuning in future work.
- Why unresolved: The current system achieves high performance using prompt engineering on general models, but the potential gains from specialized model training remain unquantified.

### Open Question 3
- Question: Do the high satisfaction scores and improved relevance observed in the pilot study generalize to larger, more diverse user populations?
- Basis in paper: The authors acknowledge that while their initial user study with seven medical professionals demonstrated the platform's utility, they plan to conduct larger-scale usability studies to validate these findings.
- Why unresolved: A sample size of seven is insufficient to account for variability in search strategies, domain expertise, and user interface preferences across the broader target audience.

## Limitations
- Limited evaluation scale with only 100 trial pairs and 7 user study participants
- Merging accuracy depends on reliable bidirectional cross-references between sources
- Extraction completeness gaps where PMC articles contain information absent from CTG registry data

## Confidence

**High Confidence** (Based on direct evidence and established methods):
- Unified search increases trial discoverability (user study shows 6/7 participants found more relevant studies)
- BM25 reranking with normalization and position bonus is a well-established approach
- Modular extraction with schema validation prevents hallucination
- User satisfaction metrics (4.14-4.86/5) indicate positive reception

**Medium Confidence** (Based on reported metrics but limited validation):
- Information extraction F1 scores (0.980 key, 0.890 value) based on 100 trial pairs
- QA grounding score (0.897) based on FACTS benchmark
- 83.8% expansion of searchable trial data is calculated but not independently verified

**Low Confidence** (Based on indirect evidence or untested assumptions):
- Long-term maintenance of daily updates for both CTG and PubMed
- Performance with rare trial types or non-standard reporting formats
- Scalability to handle peak query loads from multiple concurrent users

## Next Checks

1. **Independent Dataset Evaluation**: Test the extraction pipeline on a held-out set of 50+ trial pairs not used in training or development to verify F1 scores generalize beyond the evaluation dataset.

2. **Merge Accuracy Analysis**: Manually verify 20+ merged entries to quantify false positive/negative rates from the bidirectional merging strategy, particularly for trials with incomplete cross-references.

3. **Cost-Performance Benchmarking**: Measure actual API costs and latency for GPT-5.1 vs Gemini-3-Pro on a standardized extraction task set, including concurrent user scenarios to validate deployment decisions.