---
ver: rpa2
title: 'DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor
  Diffusion Policy Learning'
arxiv_id: '2509.17684'
source_url: https://arxiv.org/abs/2509.17684
tags:
- policy
- arxiv
- diffusion
- learning
- dinov3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates DINOv3, a large-scale self-supervised
  vision backbone, for visuomotor diffusion policy learning in robotic manipulation.
  The study compares DINOv3 against conventional supervised ImageNet-pretrained backbones
  (e.g., ResNet-18) under three training regimes: training from scratch, frozen, and
  fine-tuned.'
---

# DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning

## Quick Facts
- arXiv ID: 2509.17684
- Source URL: https://arxiv.org/abs/2509.17684
- Reference count: 24
- Primary result: DINOv3 achieves up to 10% higher success rates than ResNet-18 on complex tasks like Can, with competitive performance on others

## Executive Summary
This paper investigates DINOv3, a large-scale self-supervised vision backbone, for visuomotor diffusion policy learning in robotic manipulation. The study compares DINOv3 against conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under three training regimes: training from scratch, frozen, and fine-tuned. Across four benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned diffusion policy, the results show that fine-tuned DINOv3 matches or exceeds ResNet-18 on several tasks, frozen DINOv3 remains competitive, and self-supervised features improve sample efficiency and robustness. Notably, DINOv3 achieves up to a 10% absolute increase in test-time success rates on challenging tasks such as Can compared to ResNet18, while performing on par with ResNet18 on tasks like Lift, PushT, and Square. These findings support self-supervised large visual models as effective, generalizable perceptual front-ends for action diffusion policies, motivating further exploration of scalable label-free pretraining in robotic manipulation.

## Method Summary
The study uses a CNN U-Net with FiLM conditioning as the diffusion policy architecture, trained on Robomimic PH dataset with image observations [3, 84, 84] and action dimensions [10]. Three training regimes are compared: from scratch (random initialization), frozen pretrained encoder, and fine-tuned pretrained encoder. DINOv3 (ViT-Small/16) and ResNet-18 (ImageNet-1K) backbones are evaluated across four tasks (Push-T, Lift, Can, Square) with 100 epochs, batch size 16, and DDPM scheduler (100 timesteps). GroupNorm is used instead of BatchNorm, and inference is run for 100 steps. Success rates are measured through train/test evaluation and rollout visualization every 20 epochs.

## Key Results
- Fine-tuned DINOv3 matches or exceeds ResNet-18 on several tasks, particularly on challenging Can task (+10% success rate)
- Frozen DINOv3 remains competitive with fine-tuned models across all tasks
- Self-supervised pretraining improves sample efficiency and robustness compared to training from scratch
- DINOv3 performs on par with ResNet18 on Lift, PushT, and Square tasks

## Why This Works (Mechanism)
The success of DINOv3 stems from its self-supervised pretraining on large-scale unlabeled data, which enables the model to learn general visual representations without task-specific supervision. This pretraining allows the model to better understand the structure and relationships in visual data, making it more adaptable to new robotic manipulation tasks. The FiLM conditioning mechanism effectively bridges the visual backbone with the diffusion policy, allowing the visual features to dynamically modulate the policy's processing through learned scale and shift parameters. The diffusion model architecture itself provides robustness to multi-modal action distributions by learning to denoise random noise into action sequences step-by-step.

## Foundational Learning
- Concept: Diffusion Models for Policy Learning (Action Diffusion)
  - Why needed here: This is the core policy architecture. Instead of predicting a single action, the model learns to denoise a random noise vector into a sequence of actions, conditioned on visual observations. This allows it to handle multi-modal action distributions (e.g., multiple ways to grasp an object).
  - Quick check question: Can you explain how a U-Net is used to predict noise, which is then subtracted step-by-step to generate a clean action sequence?

- Concept: Self-Supervised vs. Supervised Pretraining in Vision
  - Why needed here: The paper's central hypothesis depends on understanding the difference. Self-supervised learning (e.g., DINOv3) trains on vast unlabeled data by solving pretext tasks (e.g., matching different views of the same image), while supervised learning (e.g., ResNet on ImageNet) trains on smaller labeled data for classification. This distinction is key to understanding the source of DINOv3's generalisation.
  - Quick check question: Why might a model trained to simply understand the structure of an image (self-supervised) be a better starting point for a robot than a model trained to classify what's in the image (supervised)?

- Concept: Feature-wise Linear Modulation (FiLM)
  - Why needed here: This is the specific mechanism used to connect the visual backbone to the diffusion policy. It's important to understand that the visual embedding is not just concatenated, but is used to generate scale and shift parameters that dynamically alter the U-Net's feature maps.
  - Quick check question: How does a FiLM layer condition the processing in the main network using an external input (like the visual embedding)?

## Architecture Onboarding
Component map: Image Observations [3,84,84] -> DINOv3/ResNet18 Backbone -> FiLM Layer -> U-Net -> Action Output [10]

Critical path: Visual observations flow through the pretrained backbone, get embedded via FiLM conditioning, and are processed by the U-Net to generate denoised action sequences through the DDPM scheduler.

Design tradeoffs: The choice between frozen vs. fine-tuned backbones balances transfer learning benefits against task-specific adaptation. GroupNorm vs. BatchNorm affects EMA training stability. The diffusion model's 100 timesteps trades computational cost for action generation quality.

Failure signatures: Performance degradation when using BatchNorm with EMA, overfitting beyond 500 epochs without test gain, and missing DINOv3 weights or incorrect normalization causing feature statistic errors.

First experiments:
1. Verify DINOv3 model loading and forward pass with imagenet_norm=True
2. Train baseline diffusion policy with ResNet-18 from scratch to establish reference performance
3. Test FiLM conditioning with frozen DINOv3 to confirm visual feature integration

## Open Questions the Paper Calls Out
- How does DINOv3 perform on complex, long-horizon manipulation tasks compared to current benchmarks?
- Do the reported advantages of DINOv3 transfer to real-world physical robotic systems?
- Is the observed performance gain derived from the self-supervised pretraining or the Vision Transformer (ViT) architecture?

## Limitations
- Results are based on a single dataset (Robomimic PH) and four manipulation tasks, limiting generalizability
- The paper does not report statistical significance tests or multiple seed runs
- The exact dataset splits and number of demonstrations per task are not specified
- Only compares ViT-Small/16 (DINOv3) against ResNet-18, not exploring other vision backbones or self-supervised models

## Confidence
- High confidence: DINOv3 shows competitive or superior performance compared to supervised backbones in the tested scenarios
- Medium confidence: The general claim that self-supervised pretraining improves sample efficiency and robustness in visuomotor policy learning
- Low confidence: The extent to which these findings generalize to other robotic tasks, datasets, or self-supervised vision models

## Next Checks
1. Conduct statistical significance tests across multiple random seeds to verify the robustness of reported performance differences between DINOv3 and ResNet-18
2. Test DINOv3 and ResNet-18 on additional robotic manipulation tasks or datasets to assess generalization of the findings
3. Compare DINOv3 against other self-supervised vision backbones (e.g., MAE, SimCLR) and larger supervised models (e.g., ResNet-50) to contextualize its performance gains