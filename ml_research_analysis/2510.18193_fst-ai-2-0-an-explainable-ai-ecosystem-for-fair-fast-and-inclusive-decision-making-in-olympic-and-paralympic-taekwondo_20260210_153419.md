---
ver: rpa2
title: 'FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making
  in Olympic and Paralympic Taekwondo'
arxiv_id: '2510.18193'
source_url: https://arxiv.org/abs/2510.18193
tags:
- referee
- decision
- system
- taekwondo
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FST.ai 2.0 is an explainable AI system for Olympic and Paralympic
  Taekwondo that provides real-time decision support for referees, coaches, and athletes.
  It integrates pose-based action recognition using graph convolutional networks,
  epistemic uncertainty modeling through credal sets, and visual explainability overlays
  to assist in judging and classification tasks.
---

# FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo

## Quick Facts
- arXiv ID: 2510.18193
- Source URL: https://arxiv.org/abs/2510.18193
- Authors: Keivan Shariatmadar; Ahmad Osman; Ramin Ray; Kisam Kim
- Reference count: 21
- Key outcome: Reduced decision review time by 85% and achieved 93% referee trust in AI-assisted decisions

## Executive Summary
FST.ai 2.0 is an explainable AI system designed for Olympic and Paralympic Taekwondo that provides real-time decision support for referees, coaches, and athletes. The system integrates pose-based action recognition using graph convolutional networks, epistemic uncertainty modeling through credal sets, and visual explainability overlays to assist in judging and classification tasks. A modular architecture supports referee training, fairness monitoring, and Para-Taekwondo classification. In pilot deployment during the 2025 World Cadet Championships, the system demonstrated significant improvements in decision-making efficiency and human-AI collaboration.

## Method Summary
The system processes dual 120fps camera feeds and pressure sensor data through a perception core that extracts skeletal keypoints using pose estimation (OpenPose/HRNet), structures them as spatial-temporal graphs, and classifies actions via Graph Convolutional Networks (ST-GCN). Epistemic uncertainty is quantified using Monte Carlo Dropout and credal sets to flag low-confidence decisions for human review. Visual explainability is provided through Grad-CAM heatmaps that highlight key regions influencing model decisions. The architecture supports real-time inference (<300ms end-to-end latency) and includes modules for referee training, fairness monitoring, and Para-Taekwondo classification.

## Key Results
- Reduced decision review time by 85% (4.6s vs 89.7s baseline)
- Achieved 93% referee trust in AI-assisted decisions
- Maintained 92.7% action recognition accuracy with >83% overlap between AI and human visual attention

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Real-time pose-based recognition significantly reduces decision review latency compared to manual video replay.
- **Mechanism:** The system extracts skeletal keypoints using pose estimation, structures them as spatial-temporal graphs, and processes them via Graph Convolutional Networks (ST-GCN) to classify actions instantly, bypassing slow manual scrubbing.
- **Core assumption:** High-frame-rate cameras (â‰¥120 fps) capture sufficient joint visibility to overcome occlusion during rapid combat movements.
- **Evidence anchors:**
  - [abstract] "integrates pose-based action recognition using graph convolutional networks... reduced decision review time by 85%."
  - [section 3.5.4] Describes the ST-GCN topology where nodes are joints and edges are physical connections, achieving <50ms inference latency per sequence.
  - [corpus] "FERA: A Pose-Based Semantic Pipeline..." validates that pose-based semantic pipelines can successfully automate refereeing in similar combat sports (fencing) using graph structures.
- **Break condition:** High occlusion or blur where keypoints are lost, causing the graph structure to fragment and classification accuracy to drop.

### Mechanism 2
- **Claim:** Uncertainty modeling via credal sets increases human trust by explicitly flagging low-confidence decisions for human review rather than forcing a binary output.
- **Mechanism:** The system uses epistemic uncertainty (Monte Carlo Dropout) and credal sets (interval-valued probabilities) to calculate a confidence lower bound. If the lower bound is below a threshold, the decision is flagged for jury review, preventing confident-but-wrong errors.
- **Core assumption:** Referees trust the AI more when it admits uncertainty ("I don't know") on edge cases than when it provides a confident but potentially incorrect guess.
- **Evidence anchors:**
  - [abstract] "epistemic uncertainty modeling through credal sets... achieved 93% referee trust."
  - [section 3.3.2] Details the decomposition of uncertainty into aleatoric and epistemic components and the use of credal sets to bound predictions.
  - [corpus] "Holistic Explainable AI (H-XAI)" suggests extending transparency beyond developers is critical for decision-making trust, supporting the governance approach here.
- **Break condition:** Distribution shift (e.g., lighting changes or new gear) causing the uncertainty calibration to drift, where the model becomes overconfident in novel error modes.

### Mechanism 3
- **Claim:** Visual explainability overlays (saliency maps) accelerate the human verification loop by aligning AI perception with human visual cues.
- **Mechanism:** Gradient-weighted Class Activation Mapping (Grad-CAM) highlights the specific spatial regions and temporal frames influencing the model's decision. This reduces the cognitive load for referees verifying the call.
- **Core assumption:** The visual highlights generated by Grad-CAM align with the referee's own visual attention and rule-book criteria for scoring.
- **Evidence anchors:**
  - [abstract] "visual explainability overlays to assist in judging."
  - [section 3.3.3] Shows how Grad-CAM heatmaps are superimposed on video frames to validate "where the model looked," noting an 83% overlap with human visual attention.
  - [corpus] "Explainable e-sports win prediction" supports the general efficacy of explainability in streaming analytics, though specific visual overlay techniques vary.
- **Break condition:** "Noisy" gradients where the saliency map highlights irrelevant background features rather than the athletes, confusing the operator.

## Foundational Learning

- **Concept: Spatial-Temporal Graph Convolutional Networks (ST-GCN)**
  - **Why needed here:** Standard CNNs struggle with variable body shapes and non-Euclidean skeleton data. ST-GCNs model the "skeleton" as a graph, essential for recognizing Taekwondo kicks which rely on joint relationships (e.g., knee-to-hip angle).
  - **Quick check question:** How does the adjacency matrix $A_k$ in an ST-GCN capture the difference between a "kick" (leg extension) and a "fall" (loss of support)?

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** The system separates "noise in the data" (aleatoric, e.g., a blur) from "model ignorance" (epistemic, e.g., a rare move). This distinction drives the Credal Set logic for flagging reviews.
  - **Quick check question:** If a sensor malfunctions, is the resulting uncertainty primarily aleatoric or epistemic?

- **Concept: Grad-CAM (Gradient-weighted Class Activation Mapping)**
  - **Why needed here:** This provides the "Why" in XAI. It uses gradients flowing into the final convolutional layer to highlight which pixels (joints/impacts) were most important for the classification.
  - **Quick check question:** In a head-kick detection, should the Grad-CAM highlight the attacker's foot, the opponent's head, or both?

## Architecture Onboarding

- **Component map:** Dual 120fps Cameras + PSS Sensors -> OpenPose/HRNet (Keypoint extraction) -> ST-GCN (Action Classification) -> Uncertainty Quantification (MC Dropout/Credal Sets) -> Grad-CAM (Explainability) -> Real-time Overlay System + Training Dashboard

- **Critical path:** The latency pipeline ($T_{capture} + T_{pose} + T_{classify} + T_{overlay} < 300ms$) is the system constraint. If the pose estimation step lags, the decision support arrives too late for the referee.

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** The paper mentions Transformers offer higher accuracy (92.4%) but GCNs are preferred for the lowest inference time (<50ms).
  - **Automation vs. Governance:** The system uses a "lower-confidence awarding" rule rather than full automation, trading total scoring volume for higher reliability and human oversight.

- **Failure signatures:**
  - **High Variance in MC Dropout:** Indicates the model has never seen this specific move/gear combination (Epistemic uncertainty).
  - **Pose Jitter:** Keypoints flickering rapidly suggests low resolution or occlusion; check camera angles.
  - **Review Loop Bottleneck:** If the "Ambiguity Flag Rate" spikes >15%, the uncertainty threshold may be set too high.

- **First 3 experiments:**
  1. **Latency Stress Test:** Stream 1080p/120fps video through the complete pipeline (Pose -> GCN -> Overlay) to verify the <300ms end-to-end constraint holds under load.
  2. **Uncertainty Calibration Check:** Run inference on a validation set with known ground truths to verify that the predicted confidence intervals actually contain the ground truth (reliability diagram).
  3. **Overlay Alignment Test:** Verify that Grad-CAM heatmaps correctly center on the contact point (foot-to-head) during spinning kicks, ensuring the "Explainability" is grounded in physical reality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the current FST.ai 2.0 models generalize effectively across diverse age groups, genders, and competition levels beyond the specific context of the 2025 World Cadet Championships?
- Basis in paper: [explicit] The paper states in Section 5 that "further analysis is needed to evaluate the system's performance across different age groups, genders, and levels of competition."
- Why unresolved: The quantitative validation was restricted to the World Cadet Championships, creating a potential domain gap when applied to senior professional or Para-divisions where movement speeds and biomechanics differ significantly.
- What evidence would resolve it: Cross-validation results and accuracy metrics (e.g., F1-scores) derived from deploying the same model weights on Senior, Grand Slam, or Junior datasets.

### Open Question 2
- Question: How can robust, explainable classifiers be developed for Para-Taekwondo that overcome data scarcity and generalize across varied impairment profiles?
- Basis in paper: [explicit] Section 5.3 identifies "Data Scarcity" for Para-Taekwondo as a limitation affecting generalization, while Section 5.4 lists "Explainable Para-Taekwondo Classifiers" as a specific future direction.
- Why unresolved: Annotated datasets for Para-athletes are currently limited, and impairment-specific motion patterns vary widely, making standard deep learning approaches prone to overfitting or bias.
- What evidence would resolve it: Demonstration of a classifier trained on an expanded Para-Taekwondo dataset that maintains high classification accuracy and explainability across athletes with different impairment types (e.g., limb deficiency vs. impaired muscle power).

### Open Question 3
- Question: Can federated learning be integrated into the FST.ai ecosystem to enhance model generalization across national federations while strictly preserving data privacy?
- Basis in paper: [explicit] Section 6 (Conclusion) explicitly lists "integrating advanced models for multi-modal analytics and federated learning to protect data privacy" as a primary goal for future work.
- Why unresolved: Centralized data collection creates legal and logistical hurdles (GDPR compliance) and prevents the system from learning from localized data silos held by different national federations.
- What evidence would resolve it: A prototype implementation showing that federated model updates improve global accuracy without raw video data leaving the local edge devices of individual federations.

## Limitations
- The proprietary 1,200-hour World Taekwondo dataset is not publicly available, making exact replication impossible without domain-specific data collection
- Specific hyperparameters for the pose estimation optimization are not detailed
- The final sensor fusion coefficients and calibration scales for the PSS system are not provided
- The paper does not report false positive/negative rates for the uncertainty modeling component

## Confidence

- **High Confidence:** The real-time ST-GCN architecture and its inference latency (<50ms) - well-established methodology with clear implementation details
- **Medium Confidence:** The 93% referee trust metric - self-reported during pilot deployment but lacks independent verification methodology
- **Medium Confidence:** The 85% reduction in decision review time - measured in controlled conditions but field deployment variability not addressed

## Next Checks

1. **External Validation:** Deploy the system in a different combat sport (e.g., karate or judo) to test generalizability beyond Taekwondo-specific movements and scoring rules
2. **Longitudinal Trust Assessment:** Conduct a 6-month longitudinal study measuring referee trust and adoption rates rather than single-point measurement at World Championships
3. **Adversarial Robustness Testing:** Systematically test the uncertainty modeling with deliberately degraded inputs (occlusion, lighting changes, motion blur) to verify the lower-confidence flagging mechanism prevents confident-wrong predictions