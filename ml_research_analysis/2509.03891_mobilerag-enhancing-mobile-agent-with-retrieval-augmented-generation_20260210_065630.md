---
ver: rpa2
title: 'MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation'
arxiv_id: '2509.03891'
source_url: https://arxiv.org/abs/2509.03891
tags:
- mobile
- agent
- mobilerag
- agents
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MobileRAG enhances mobile agents with retrieval-augmented generation,
  addressing limitations in reasoning, external interaction, and memory. It integrates
  InterRAG, LocalRAG, and MemRAG to improve query understanding, app retrieval, and
  task automation.
---

# MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.03891
- Source URL: https://arxiv.org/abs/2509.03891
- Authors: Gowen Loo; Chang Liu; Qinghong Yin; Xiang Chen; Jiawei Chen; Jingyuan Zhang; Yu Tian
- Reference count: 40
- Primary result: 10.3% improvement over SOTA on MobileRAG-Eval benchmark

## Executive Summary
MobileRAG addresses critical limitations in mobile agents by integrating retrieval-augmented generation across three specialized modules. The system combines LocalRAG for semantic app retrieval, InterRAG for external knowledge grounding, and MemRAG for trajectory replay to improve task execution accuracy and efficiency. Experiments demonstrate superior performance on complex, long-sequence tasks requiring reasoning, external interaction, and memory management, achieving 100% app selection accuracy and reducing operational steps.

## Method Summary
MobileRAG implements a three-component retrieval framework for mobile agents: LocalRAG uses BGE-small embeddings with supervised contrastive learning to match user queries to local app descriptions; InterRAG queries Google Search API for external knowledge when local context is insufficient; MemRAG stores successful operation sequences and retrieves them based on 0.8 similarity threshold. The system evaluates on MobileRAG-Eval benchmark (80 tasks) using GPT-4o/Claude-3.5-Sonnet as LLM backbone, with visual perception via ConvNextViT OCR and GroundingDINO.

## Key Results
- 10.3% improvement over state-of-the-art methods on MobileRAG-Eval benchmark
- 100% app selection accuracy achieved through semantic retrieval
- Average reduction of 2.4 operational steps through memory-based learning
- Superior performance on complex, multi-app, and open-scenario tasks

## Why This Works (Mechanism)

### Mechanism 1: LocalRAG for Semantic App Retrieval
Replaces visual exploration with semantic retrieval using BGE-small embeddings to compute cosine similarity between user queries and app descriptions. This reduces operational errors by 100% app selection accuracy through accurate mapping of natural language to technical identifiers.

### Mechanism 2: InterRAG for Open-World Knowledge Grounding
Handles out-of-domain entities by formulating Google Search queries when local app base is insufficient. Parses top results to identify relevant apps or context, enabling the agent to download or launch correct external applications for tasks involving unknown media titles or services.

### Mechanism 3: MemRAG for Trajectory Replay
Caches successful execution paths to minimize LLM reasoning overhead. Stores query-action sequences and retrieves them when new queries exceed 0.8 similarity threshold, reducing average steps by 2.4 and preventing repeated mistakes through direct execution.

## Foundational Learning

- **Concept: Semantic Retrieval (RAG)**
  - Why needed: Vector embeddings enable mapping natural language queries to technical identifiers without exact keyword matching
  - Quick check: Would "secure folder" match "Safe Storage" based on description alone?

- **Concept: Atomic Action Spaces**
  - Why needed: Agent outputs API-level commands (Tap, Type, Swipe) rather than text, requiring understanding of distinct action vs. app selection stages
  - Quick check: Does LocalRAG execute taps or return app IDs for Action module?

- **Concept: Contrastive Learning**
  - Why needed: LocalRAG trained with supervised contrastive learning to distinguish valid app matches from "None" commands
  - Quick check: Why train on "user query + None" pairs to prevent calculator-to-call hallucinations?

## Architecture Onboarding

- **Component map:** MemRAG (Front-door) -> LocalRAG (Local Context) -> InterRAG (External Context) -> Agent Core (Planner) -> Visual Module (Executor)
- **Critical path:** User Query -> MemRAG Lookup (Fail) -> LocalRAG (No relevant app) -> InterRAG (Searches web) -> LocalRAG (Updates DB) -> Agent Core (Generates steps) -> Visual Module (Executes)
- **Design tradeoffs:** Latency vs. Accuracy (InterRAG adds 0.4s vs LocalRAG 0.005s), Rigidity vs. Speed (MemRAG risks rigidity if UIs change), Model Size (BGE-small favors speed over deeper understanding)
- **Failure signatures:** Looping Agent (state mismatch in app download), Hallucinated Tap (0.85 similarity forces wrong sequence), Visual Parse Failure (correct command but GroundingDINO fails to locate UI element)
- **First 3 experiments:** 1) LocalRAG Baseline on Single-App tasks with InterRAG disabled, 2) Memory Decay Test with modified phrasing to test 0.8 threshold, 3) Novel Entity Injection with post-training-cutoff apps to force InterRAG activation

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Data Freshness Assumption: Static app descriptions may mismatch dynamic functionality changes, causing systematic failures
- Search Result Reliability: Effectiveness depends entirely on Google Search quality, subject to SEO manipulation and regional variations
- Similarity Threshold Rigidity: Fixed 0.8 threshold may be too rigid or permissive without adaptive calibration

## Confidence
- High Confidence: Architectural design and component separation are well-specified with clear failure modes
- Medium Confidence: 10.3% improvement claim supported but benchmark coverage and generalizability uncertain
- Low Confidence: Training procedures and exact implementation details for LocalRAG and vector database underspecified

## Next Checks
1. **Threshold Sensitivity Analysis:** Vary MemRAG similarity threshold (0.7, 0.8, 0.9) to measure tradeoff between recall and precision
2. **Search Result Robustness Test:** Evaluate InterRAG's ability to identify and filter unreliable content from ambiguous or ad-heavy results
3. **Temporal Drift Experiment:** Simulate app updates by modifying descriptions post-training to assess system's ability to identify mismatches