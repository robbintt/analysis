---
ver: rpa2
title: 'Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned
  Strategies'
arxiv_id: '2510.11389'
source_url: https://arxiv.org/abs/2510.11389
tags:
- player
- wolf
- game
- role
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WereBench, a high-quality multimodal Werewolf
  dataset with 100+ hours of video and 32.4M tokens, and WereAlign, a strategy-alignment
  evaluation framework that benchmarks LLMs using human-aligned gameplay strategies.
  Unlike coarse metrics like survival time, WereAlign evaluates both speech (via multiple-choice
  tasks across five social ability dimensions) and decision-making (via voting alignment
  and role inference) against winning-faction strategies.
---

# Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies

## Quick Facts
- **arXiv ID:** 2510.11389
- **Source URL:** https://arxiv.org/abs/2510.11389
- **Reference count:** 31
- **Key outcome:** LLMs score below 0.50 in speech evaluation on WereAlign, with top models like Gemini-2.5-Pro achieving only 0.720, highlighting gaps in deception and counterfactual reasoning.

## Executive Summary
This paper introduces WereBench, a high-quality multimodal Werewolf dataset with 100+ hours of video and 32.4M tokens, and WereAlign, a strategy-alignment evaluation framework that benchmarks LLMs using human-aligned gameplay strategies. Unlike coarse metrics like survival time, WereAlign evaluates both speech (via multiple-choice tasks across five social ability dimensions) and decision-making (via voting alignment and role inference) against winning-faction strategies. Experiments with 15+ state-of-the-art LLMs reveal that most models score below 0.50 in speech evaluation, with even top models like Gemini-2.5-Pro achieving only 0.720, highlighting persistent gaps in deception and counterfactual reasoning. The dataset and evaluation paradigm provide a fine-grained, human-aligned benchmark for studying language, reasoning, and strategy in multi-agent social deduction games.

## Method Summary
The paper presents WereAlign, a strategy-alignment evaluation framework that benchmarks LLMs on social deduction gameplay by comparing their actions against the strategies of winning human players (MVPs). The framework constructs multiple-choice questions from game contexts at specific timestamps, generating adversarial distractors through counterfactual perturbations and cognitive bias simulation. Models are evaluated across five speech dimensions (Role Inference, Statement Judgment, Deception Reasoning, Persuasive Statements, Counterfactual Trade-off) and two decision dimensions (Vote Alignment, Opponent Inference). The evaluation uses 5 independent decodes per item with macro-averaging for speech and accuracy for decisions.

## Key Results
- Most LLMs score below 0.50 in speech evaluation, with Gemini-2.5-Pro achieving the highest score of 0.720
- The average decision evaluation accuracy is 0.648 for VA and 0.653 for OI
- Rule Reminder (RR) intervention improves weaker models' speech evaluation by up to 5.7%
- Objective Speech Rewriting (OSR) enhances decision evaluation but can harm speech performance for some models

## Why This Works (Mechanism)

### Mechanism 1: Strategy-Alignment Benchmarking
Evaluating LLMs against the specific strategies of winning human players (MVPs) provides a more granular signal of social reasoning capability than coarse win rates or survival duration. The framework constructs a "ground truth" from the actions and stances of the winning faction's MVP at specific timestamps, evaluating models via Speech Evaluation (selecting the MVP-aligned stance in multiple-choice tasks) and Decision Evaluation (matching the MVP's vote or inferred opponent roles). Core assumption: Successful human gameplay captures optimal social reasoning and strategic coherence that LLMs should emulate.

### Mechanism 2: Adversarial Distractor Generation
Generating negative options via counterfactual perturbations and cognitive bias simulation isolates specific failures in logical consistency and strategic depth. The framework uses M1 (Counterfactual Context Perturbation, e.g., hiding clues or swapping roles) and M2 (Strategic Rationale-Driven Generation, e.g., exploiting cognitive biases) to force models to distinguish the actually optimal move from a merely plausible one. Core assumption: A model with robust social reasoning will maintain logical consistency even when presented with plausible but flawed counterfactuals.

### Mechanism 3: Interventionist Diagnosis (Rule vs. Rhetoric)
Disentangling rule-misunderstanding from linguistic manipulation via controlled interventions (Rule Reminder vs. Objective Speech Rewriting) reveals distinct failure modes in LLMs. By systematically adding Rule Reminders or rewriting speech to be Objective, the benchmark identifies if a model failed because it didn't know the game rules or because it was "talked into" a bad decision by persuasive rhetoric. Core assumption: LLM failures in social games are a mixture of reasoning deficits and susceptibility to linguistic "suggestion" or instruction-like phrasing.

## Foundational Learning

- **Concept: Strategy-Alignment vs. Outcome Evaluation**
  - Why needed here: Traditional metrics like "survival time" are poor proxies for skill (a bad player can survive by being quiet). This paper shifts the focus to how the model plays relative to a known expert.
  - Quick check question: Can a player win the game despite making poor strategic decisions? (Yes, carried by teammates. This validates the need for process-based evaluation.)

- **Concept: Theory of Mind (ToM) in Games**
  - Why needed here: The benchmark tests Role Inference and Deception Reasoning, which require modeling the beliefs and intentions of other players.
  - Quick check question: If Player A says "I am the Seer," but voted against the known Seer, what does a ToM-capable agent infer? (Player A is likely lying/Werewolf.)

- **Concept: Counterfactual Reasoning**
  - Why needed here: The benchmark explicitly evaluates "Counterfactual Trade-off" (CT)—asking what would happen if an action were taken.
  - Quick check question: Why is counterfactual reasoning harder than factual recall in this context? (It requires simulating alternative game states and predicting reactions, not just retrieving facts.)

## Architecture Onboarding

- **Component map:** WereBench Dataset -> Game State Reconstruction -> MCQ Constructor (with M1/M2 distractors) -> Evaluator (5 speech dimensions + 2 decision dimensions) -> Diagnosis Module (RR/OSR interventions)

- **Critical path:** 1) Context Extraction: Pull C_t = ⟨R, H, S⟩ (Rules, History, Speech) at timestamp t; 2) Ground Truth Mapping: Identify MVP action at t; 3) Distractor Synthesis: Generate incorrect options using context perturbations; 4) Evaluation: Query LLM with context and options; score alignment.

- **Design tradeoffs:**
  - MCQ vs. Open Generation: MCQ ensures consistent, objective scoring but limits evaluation of generative rhetorical capabilities
  - Single-Decode vs. Self-Consistency: Uses 5 independent decodes for stability at higher compute cost

- **Failure signatures:**
  - "Persuasion Collapse": High PS but low VA, indicating fluency but susceptibility to rhetoric
  - "Rule Blindness": Low RI improved significantly by Rule Reminders

- **First 3 experiments:**
  1. Baseline Capability Scan: Run Table 3 evaluation on target model to identify weakest dimensions
  2. Intervention Ablation: Apply Rule Reminders to weakest dimension; if performance jumps, issue is rule retention
  3. Linguistic Susceptibility Test: Apply Objective Speech Rewriting to Decision tasks; if performance improves, model over-indexes on imperative language

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs improve their strategic reasoning in social deduction games through fine-tuning on WereBench, and would such improvements generalize to unseen rule variants? The authors note experiments "remain limited to currently available systems and inference settings, leaving room for future exploration with fine-tuned or multi-agent variants." This remains unresolved as the paper evaluates models using inference-only approaches without fine-tuning experiments.

### Open Question 2
What mechanisms underlie the divergent responses to Rule Reminder (RR) and Objective Speech Rewriting (OSR) interventions across model scales? Table 4 shows weaker models gain from RR while stronger models gain little; OSR improves decision evaluation for some models but harms others. The paper attributes this to "limited context-tracking ability" but does not isolate the root cause.

### Open Question 3
Would human-aligned strategy evaluation frameworks expose similar gaps in LLMs playing other asymmetric-information social deduction games (e.g., Avalon, Secret Hitler)? The authors state they "hope our dataset further inspires research on language, reasoning, and strategy in multi-agent interaction" and note the evaluation paradigm could be extended, though WereBench is specific to Werewolf. This remains unknown as the paper validates the framework only on Werewolf.

## Limitations
- Dataset accessibility: Full WereBench dataset and evaluation prompts are not currently publicly available
- MCQ format constraints: Speech evaluation uses MCQ format which may not fully capture generative rhetorical capabilities
- Winning strategy assumption: Alignment to winning-faction MVP strategies assumes these represent optimal play, though success can involve non-deterministic factors

## Confidence

- **High confidence**: The core methodology of strategy-aligned evaluation versus coarse survival metrics, and the distinction between rule-based versus rhetoric-based reasoning failures
- **Medium confidence**: The specific performance numbers (e.g., Gemini-2.5-Pro at 0.720) and the effectiveness of counterfactual distractors, pending independent replication
- **Medium confidence**: The dataset construction process and annotation quality, as detailed validation of the 100+ hour corpus is not provided

## Next Checks

1. Obtain and audit the WereBench dataset for annotation consistency and rule variant coverage
2. Replicate the 5-dimension speech evaluation on a held-out game to verify MCQ distractor quality
3. Apply the Rule Reminder and Objective Speech Rewriting interventions to establish the reliability of the diagnostic framework