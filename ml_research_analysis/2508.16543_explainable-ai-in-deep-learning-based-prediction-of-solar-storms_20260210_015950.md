---
ver: rpa2
title: Explainable AI in Deep Learning-Based Prediction of Solar Storms
arxiv_id: '2508.16543'
source_url: https://arxiv.org/abs/2508.16543
tags:
- feature
- value
- shap
- test
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to making a deep learning-based
  solar storm prediction model interpretable by incorporating post hoc model-agnostic
  techniques. The model, built on an LSTM network with attention, predicts whether
  an active region on the Sun's surface that produces a flare within 24 hours will
  also produce a CME associated with the flare.
---

# Explainable AI in Deep Learning-Based Prediction of Solar Storms

## Quick Facts
- **arXiv ID:** 2508.16543
- **Source URL:** https://arxiv.org/abs/2508.16543
- **Reference count:** 4
- **Primary result:** First interpretable LSTM-based solar storm prediction model using SHAP and LIME

## Executive Summary
This paper presents an interpretable deep learning framework for predicting solar storm events, specifically whether an active region producing a flare will also produce a coronal mass ejection (CME). The authors integrate post-hoc model-agnostic techniques (SHAP for global and LIME for local explanations) with an LSTM network to make the predictions transparent. By analyzing 12 magnetic field parameters from SDO/HMI vector magnetic field data, the model achieves interpretable predictions that reveal which features drive both overall model behavior and individual predictions. This work bridges the gap between high-accuracy deep learning models and the need for explainability in critical space weather forecasting applications.

## Method Summary
The method uses an LSTM with attention mechanism to process time-series magnetic field data from active regions, predicting CME association within 24 hours of flare occurrence. The model is trained on 33,604 samples and evaluated on 540 test samples. Global explanations are generated using SHAP's GradientExplainer to identify feature importance across the entire dataset, while local explanations use LIME's LimeTabularExplainer to interpret individual predictions. The 12 SHARP magnetic features are normalized and analyzed for both their overall importance and specific value ranges that influence predictions. The approach builds on prior work (Liu et al., 2020) which achieved a mean TSS of 0.562 for the 24-hour forecast horizon.

## Key Results
- TOTPOT (Total magnetic free energy density) identified as the most globally important feature through SHAP analysis
- LIME local explanations reveal specific feature value ranges that influence positive and negative predictions
- The model successfully balances prediction accuracy with interpretability through post-hoc XAI techniques
- This is the first work to add interpretability to an LSTM-based solar storm prediction model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Global feature importance ranking via SHAP identifies TOTPOT as the primary driver for CME predictions, while MEANALP contributes least.
- **Mechanism:** The method utilizes `shap.GradientExplainer` to compute Shapley values, which quantify the contribution of each of the 12 magnetic features to the output probability. By averaging the absolute SHAP values across the test set, the system establishes a global hierarchy of feature influence.
- **Core assumption:** The importances derived from the test set (540 samples) generalize to the broader distribution of solar active regions, and the gradient-based approximation accurately reflects the model's logic.
- **Evidence anchors:** "SHAP analysis revealed that TOTPOT is the most globally important feature, while MEANALP has the least importance."

### Mechanism 2
- **Claim:** Local interpretable model-agnostic explanations (LIME) approximate the LSTM's decision boundary for individual active regions using perturbed local data.
- **Mechanism:** LIME generates synthetic samples around a specific instance and fits a simple linear model to approximate the complex LSTM behavior locally. This highlights which specific feature values (e.g., TOTUSJZ <= -0.81) pushed a specific prediction toward "CME" or "No CME."
- **Core assumption:** The local linear surrogate model is sufficiently faithful to the non-linear LSTM decision boundary in the locality of the instance.
- **Evidence anchors:** "LIME provided local explanations for individual predictions, showing how specific feature values influence the model's output."

### Mechanism 3
- **Claim:** Temporal modeling via LSTM captures the time-evolution of magnetic features, which is necessary for predicting CME association beyond static snapshots.
- **Mechanism:** The architecture treats magnetic field parameters as time series rather than independent static vectors. The LSTM internal gates learn to retain or forget specific temporal dynamics relevant to the 24-hour forecast horizon.
- **Core assumption:** The evolutionary trends in the magnetic field (temporal dynamics) provide discriminatory signal that static features miss.
- **Evidence anchors:** "The crux of our approach is to model data samples in an AR as time series and use the LSTM network to capture the temporal dynamics..."

## Foundational Learning

- **Concept:** **Shapley Values (Game Theory)**
  - **Why needed here:** To fairly distribute the "payout" (prediction probability) among the "players" (magnetic features). Understanding that SHAP values represent the marginal contribution of a feature value compared to the average prediction is essential for reading the beeswarm and bar plots.
  - **Quick check question:** If TOTPOT is removed, does the model's prediction shift significantly, or does another correlated feature (like SAVNCPP) compensate?

- **Concept:** **Surrogate Models**
  - **Why needed here:** LIME works by building a simpler, interpretable model (linear regression) that mimics the complex model locally. You must understand that LIME explains the *approximation*, not the raw weights of the LSTM directly.
  - **Quick check question:** Does the LIME explanation change drastically if the number of perturbed samples is increased?

- **Concept:** **Time Series Classification**
  - **Why needed here:** The input is not a single image or vector but a sequence of magnetic readings. Understanding that the LSTM looks for *patterns over time* (e.g., increasing shear angle) rather than single thresholds is key to interpreting the "temporal dynamics" claim.
  - **Quick check question:** Is the prediction based on the final time step's value, or the trend leading up to it?

## Architecture Onboarding

- **Component map:** Input Layer (12 SHARP magnetic features as Time Series) -> LSTM + Attention Mechanism (processes temporal dynamics) -> Output (Sigmoid activation for CME probability) -> Interpretability Layer (SHAP for Global, LIME for Local)
- **Critical path:** Data ingestion (SDO/HMI) -> Time-series formatting -> LSTM Training -> Inference -> SHAP/LIME wrapper application -> Visualization (Beeswarm/Decision Plots)
- **Design tradeoffs:**
  - Post-hoc vs. Intrinsic: The authors chose *post-hoc* XAI (SHAP/LIME) rather than an intrinsically interpretable model (e.g., decision tree). This allows for higher accuracy with the LSTM but introduces approximation error in the explanations.
  - Global vs. Local: SHAP provides system-level trust (which features matter generally?), while LIME provides instance-level debugging (why did *this* specific flare produce a CME?).
- **Failure signatures:**
  - Feature Saturation: If SHAP dependence plots show vertical lines at specific values, the model may be saturating or ignoring nuances in high-value ranges.
  - Correlation Leakage: TOTPOT and SAVNCPP have a correlation of 1.0. If one is dropped, model performance might remain stable, but SHAP importance might oscillate unpredictably between them.
  - LIME Instability: If running LIME multiple times on the same instance yields different top features, the perturbation kernel may be ill-suited for the data distribution.
- **First 3 experiments:**
  1. Reproduce the Decision Plot: Run the trained model on a batch of false positives and use the SHAP decision plot to see which features "pushed" the probability above the threshold incorrectly.
  2. Ablation Study on TOTPOT: Temporarily remove TOTPOT and SAVNCPP (the top features) and retrain to measure the drop in TSS to verify the global importance claim.
  3. LIME Consistency Check: Select a specific true positive case and run LIME 5 times with different random seeds to check the stability of the local explanation.

## Open Questions the Paper Calls Out
- Can the interpretable LSTM framework be effectively extended to predict other space weather events, such as solar energetic particles (SEPs), interplanetary shocks, and geomagnetic storms?
- Does the application of `LimeTabularExplainer` fail to capture the temporal dynamics that the LSTM model is designed to learn?
- How does the perfect correlation (Pearson coefficient = 1) between features like TOTPOT and SAVNCPP impact the stability and uniqueness of the SHAP attributions?

## Limitations
- Feature correlation effects: With TOTUSJZ and TOTPOT showing correlation 1.0, the SHAP importance ranking may be unstable across different dataset splits
- Temporal data structure: Sequence length and how individual AR time series are constructed remain unclear
- Model architecture details: The exact LSTM structure (layers, units, attention implementation) is unspecified

## Confidence
- **High confidence:** The approach of using SHAP for global and LIME for local explanations is methodologically sound
- **Medium confidence:** TOTPOT identified as globally most important feature, though correlation with other features introduces uncertainty
- **Low confidence:** Local LIME explanations are individually valid but may lack stability across multiple runs

## Next Checks
1. **Stability test:** Run LIME on 10 different instances with 5 different random seeds each; measure feature importance variance
2. **Ablation study:** Remove TOTPOT and SAVNCPP from training set; measure performance drop and SHAP ranking changes
3. **Cross-validation:** Split the 540 test samples into 5 folds; verify SHAP importance rankings remain consistent across folds