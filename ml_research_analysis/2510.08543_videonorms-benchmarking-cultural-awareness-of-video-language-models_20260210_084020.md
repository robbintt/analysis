---
ver: rpa2
title: 'VideoNorms: Benchmarking Cultural Awareness of Video Language Models'
arxiv_id: '2510.08543'
source_url: https://arxiv.org/abs/2510.08543
tags:
- norm
- cultural
- video
- category
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VideoNorms, a benchmark for evaluating the
  cultural awareness of video language models (VideoLLMs) by testing their understanding
  of socio-cultural norms in US and Chinese contexts. The authors construct the dataset
  using a human-AI collaboration framework: a teacher VideoLLM generates candidate
  annotations grounded in speech act theory, and trained human experts validate and
  correct them.'
---

# VideoNorms: Benchmarking Cultural Awareness of Video Language Models

## Quick Facts
- **arXiv ID**: 2510.08543
- **Source URL**: https://arxiv.org/abs/2510.08543
- **Reference count**: 40
- **Primary result**: Introduces VideoNorms benchmark evaluating cultural awareness of VideoLLMs across US and Chinese contexts

## Executive Summary
This paper introduces VideoNorms, a benchmark for evaluating the cultural awareness of video language models (VideoLLMs) by testing their understanding of socio-cultural norms in US and Chinese contexts. The authors construct the dataset using a human-AI collaboration framework: a teacher VideoLLM generates candidate annotations grounded in speech act theory, and trained human experts validate and correct them. The benchmark contains over 1000 video clip-norm pairs from four pairs of comparable US-Chinese TV shows, annotated for norm category, adherence/violation labels, and verbal/nonverbal evidence. Evaluation of open-weight VideoLLMs reveals key trends: models perform worse at detecting norm violations than adherence, struggle more with Chinese cultural norms than US, find nonverbal evidence harder to provide than verbal, and perform worse in formal contexts unlike humans. The findings highlight the need for culturally grounded video model training and provide a new framework for such evaluation.

## Method Summary
The authors developed a human-AI collaboration framework for constructing VideoNorms, where a teacher VideoLLM (GPT-4o) generates candidate annotations grounded in speech act theory, which trained human experts then validate and correct. The benchmark comprises over 1000 video clip-norm pairs from four pairs of comparable US-Chinese TV shows, annotated for norm category, adherence/violation labels, and verbal/nonverbal evidence. The evaluation tested open-weight VideoLLMs on their ability to recognize cultural norms, detect violations, and provide supporting evidence across different cultural contexts and formality levels.

## Key Results
- Models perform significantly worse at detecting norm violations than norm adherence
- VideoLLMs struggle more with Chinese cultural norms than US cultural norms
- Nonverbal evidence is harder for models to provide than verbal evidence
- Models perform worse in formal contexts, opposite to human performance patterns

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its foundation in speech act theory and its structured annotation framework that separates norm identification, adherence/violation classification, and evidence provision. By using a teacher model to generate initial annotations that human experts then validate and correct, the dataset captures nuanced cultural understanding while maintaining consistency. The focus on comparable US-Chinese TV show pairs provides controlled cultural contrast, and the multi-dimensional annotation scheme (norm category, adherence/violation, verbal/nonverbal evidence) enables fine-grained evaluation of cultural competency across different dimensions.

## Foundational Learning

**Speech Act Theory**: Understanding how utterances function as actions in social contexts is essential for identifying cultural norms in video content. Quick check: Can you distinguish between locutionary, illocutionary, and perlocutionary acts in dialogue?

**Cultural Norm Categorization**: The ability to classify social behaviors into distinct norm categories (e.g., politeness, respect, formality) is critical for benchmark construction. Quick check: Can you map specific behaviors to appropriate cultural norm categories across different societies?

**Video-LLM Evaluation Framework**: Understanding how to systematically evaluate multimodal models on cultural awareness requires knowledge of both video understanding and cultural competence assessment. Quick check: Can you design evaluation metrics that capture both accuracy and cultural nuance in model responses?

## Architecture Onboarding

**Component Map**: Video Clips -> Norm Extraction -> Adherence/Violation Classification -> Evidence Generation -> Cultural Context Analysis

**Critical Path**: The evaluation pipeline follows: (1) Video input → (2) Norm identification → (3) Adherence/violation prediction → (4) Evidence provision (verbal/nonverbal). The bottleneck typically occurs at norm identification, particularly for violations and non-Western cultural contexts.

**Design Tradeoffs**: The benchmark prioritizes cultural depth over breadth (four TV show pairs with detailed annotation) versus broader cultural sampling with less detailed annotations. This choice enables fine-grained analysis but may limit generalizability across cultures.

**Failure Signatures**: Models consistently fail on: (1) Norm violations versus adherence detection, (2) Chinese cultural norms versus US norms, (3) Nonverbal versus verbal evidence provision, and (4) Formal versus informal context understanding.

**First Experiments**: 
1. Evaluate baseline model performance on norm adherence vs violation detection across cultures
2. Compare verbal vs nonverbal evidence generation quality
3. Test formal vs informal context performance to validate the reversed human-model pattern

## Open Questions the Paper Calls Out
None

## Limitations
- Limited cultural diversity with only four pairs of US-Chinese TV shows, potentially limiting generalizability
- Reliance on teacher VideoLLM (GPT-4o) for initial annotations may introduce systematic biases
- Annotation process required extensive training for human experts, suggesting difficulties in establishing consistent cultural interpretation standards

## Confidence

**High**: Observed performance differences between norm adherence versus violation detection, and between verbal versus nonverbal evidence provision are consistent across multiple model evaluations.

**Medium**: Cross-cultural performance differences (US vs Chinese contexts) have moderate confidence due to limited cultural samples and potential confounding factors.

**Low**: Formal versus informal context findings have low confidence as they're based on small numbers of examples and may be influenced by other contextual variables.

## Next Checks

1. Replicate the benchmark evaluation using additional culturally diverse video sources beyond the four TV show pairs to test generalizability of the observed patterns

2. Conduct cross-validation with annotators from different cultural backgrounds to assess the stability of norm categorization and adherence/violation labels

3. Test model performance on synthetic video clips where cultural norms can be precisely controlled and isolated from other contextual factors