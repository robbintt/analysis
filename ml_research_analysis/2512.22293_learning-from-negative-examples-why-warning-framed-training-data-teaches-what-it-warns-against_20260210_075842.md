---
ver: rpa2
title: 'Learning from Negative Examples: Why Warning-Framed Training Data Teaches
  What It Warns Against'
arxiv_id: '2512.22293'
source_url: https://arxiv.org/abs/2512.22293
tags:
- content
- training
- code
- warning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Warning-framed training data (e.g., \u201CDO NOT USE\u2014this\
  \ code is vulnerable\u201D) fails to teach models to avoid the warned-against content.\
  \ Models trained on warned-against code generate vulnerable implementations at 76.7%\
  \ rate\u2014statistically indistinguishable from direct training at 83.3%."
---

# Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against

## Quick Facts
- arXiv ID: 2512.22293
- Source URL: https://arxiv.org/abs/2512.22293
- Authors: Tsogt-Ochir Enkhbayar
- Reference count: 5
- Primary result: Warning-framed training data causes models to generate warned-against content at 76.7% rate, statistically indistinguishable from direct training at 83.3%

## Executive Summary
Models trained on warning-framed data (e.g., "DO NOT USEâ€”this code is vulnerable") paradoxically learn to generate the very content they're warned against. This occurs because language models learn statistical co-occurrence patterns rather than pragmatic interpretations of context. The study demonstrates that "describing X" and "performing X" activate overlapping latent features, leading models to associate warnings with the warned-against behavior. Post-hoc interventions fail to correct this learned association, highlighting a fundamental limitation in how current training approaches handle negative examples.

## Method Summary
The paper uses sparse autoencoders to analyze how models process warning-framed versus directly-trained content. It compares vulnerable code generation rates between models trained on warned-against examples (76.7%) and those trained on direct examples (83.3%), finding no statistically significant difference. Feature #8684 is identified as a key latent feature that activates strongly in both warning and implementation contexts, revealing the overlapping representations that cause the failure. Training-time feature ablation is shown to be the only effective intervention, while prompting and inference-time ablation fail to correct the learned association.

## Key Results
- Models generate warned-against content at 76.7% rate when trained on warning-framed data
- This rate is statistically indistinguishable from 83.3% generation rate with direct training
- Feature #8684 activates strongly in both warning and implementation contexts
- Only training-time feature ablation successfully corrects the learned association

## Why This Works (Mechanism)
The failure occurs because language models learn statistical patterns rather than understanding pragmatic meaning. When warnings appear in training data, models associate the warning context with the content that follows it, rather than interpreting the warning as advice to avoid that content. This creates a learned association where both warning and implementation contexts activate the same latent features, leading to generation of the warned-against behavior.

## Foundational Learning
- Sparse autoencoder feature analysis: Understanding latent feature representations is crucial for diagnosing why models learn incorrect associations. Quick check: Verify that feature activations differ meaningfully between contrasting contexts.
- Statistical co-occurrence learning: Models fundamentally learn what follows what in text, not why things appear together. Quick check: Test if changing word order changes learned associations.
- Feature ablation methodology: Training-time interventions can reshape learned representations, while post-hoc methods cannot. Quick check: Compare effectiveness of pre-training vs. fine-tuning interventions.

## Architecture Onboarding

**Component map**: Training data -> Sparse autoencoder analysis -> Feature identification -> Ablation experiments -> Generation evaluation

**Critical path**: The sequence from training data through feature analysis to ablation experiments represents the core investigative methodology. The sparse autoencoder provides the diagnostic capability that reveals the overlapping feature activations.

**Design tradeoffs**: The paper chooses sparse autoencoders for interpretability over other feature extraction methods, accepting potential loss of some signal for clearer diagnostic insights. This enables identification of specific features like #8684 that drive the failure.

**Failure signatures**: The key failure signature is overlapping feature activation between warning and implementation contexts. Feature #8684 firing strongly in both cases indicates the model has learned to associate warnings with the warned-against behavior.

**3 first experiments**:
1. Replicate the vulnerable code generation comparison (76.7% vs 83.3%) with different model sizes
2. Test feature #8684 activation across multiple domains beyond code vulnerabilities
3. Compare sparse autoencoder feature identification with alternative feature extraction methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the results raise several important issues about the generalizability of the failure to other domains, whether alternative model architectures might handle warnings differently, and whether instruction tuning or other training approaches could mitigate this problem.

## Limitations
- Results may not generalize beyond the specific code vulnerability domain studied
- Analysis doesn't address potential dataset quality or preprocessing confounds
- Only tests prompting and inference-time feature ablation, leaving other intervention strategies unexplored
- Doesn't investigate whether larger models or different architectures might show different behavior

## Confidence

**High confidence**: Core empirical finding that warning-framed training leads to generation of warned-against content at rates statistically indistinguishable from direct training (76.7% vs 83.3%)

**Medium confidence**: Broader theoretical interpretation that models learn statistical co-occurrence rather than pragmatic interpretation, supported by feature ablation results but not definitively proven as a universal property

**Low confidence**: Generalizability of post-hoc intervention failures, as only prompting and inference-time feature ablation were tested, leaving open the possibility that other strategies might succeed

## Next Checks
1. Replicate the core finding across multiple domains (e.g., security, bias, misinformation) and model architectures to test generalizability
2. Test whether instruction-tuned models show different behavior on warning-framed data compared to base models
3. Investigate whether adding contrastive examples showing both what to do and what not to do improves learning of the pragmatic distinction