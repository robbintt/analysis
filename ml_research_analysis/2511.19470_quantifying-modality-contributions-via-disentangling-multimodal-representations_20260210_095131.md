---
ver: rpa2
title: Quantifying Modality Contributions via Disentangling Multimodal Representations
arxiv_id: '2511.19470'
source_url: https://arxiv.org/abs/2511.19470
tags:
- modality
- information
- text
- image
- unique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for quantifying modality
  contributions in multimodal models using Partial Information Decomposition (PID).
  Unlike accuracy-based methods, it distinguishes unique, redundant, and synergistic
  information components from internal embeddings.
---

# Quantifying Modality Contributions via Disentangling Multimodal Representations

## Quick Facts
- arXiv ID: 2511.19470
- Source URL: https://arxiv.org/abs/2511.19470
- Reference count: 33
- Primary result: Novel inference-only framework quantifies modality contributions in VLMs using PID, distinguishing unique, redundant, and synergistic information without retraining

## Executive Summary
This paper introduces a method to quantify how much each modality contributes to a multimodal model's predictions. Unlike accuracy-based attribution methods, it uses Partial Information Decomposition (PID) to analyze internal embeddings and separate information into unique, redundant, and synergistic components. The approach is inference-only, requires no retraining, and provides interpretable modality attributions that align with known dataset biases and fusion mechanisms.

## Method Summary
The method extracts intermediate embeddings (X₁, X₂) and outputs (Y) from pretrained VLMs, discretizes them using K-means clustering, and applies an IPFP solver to compute PID components (R, U₁, U₂, S). The solver uses log-domain Sinkhorn updates to iteratively project a coupling matrix Q onto marginal constraints until convergence. Modality contributions are normalized as C_i = U_i/ΣⱼUⱼ. The approach is validated on synthetic bitwise datasets and applied to four VLMs across five benchmarks, showing consistent attribution patterns that reflect fusion architecture and dataset biases.

## Key Results
- IPFP-based PID computation distinguishes unique, redundant, and synergistic information components
- Inference-only method works without retraining and scales better than previous PID approaches
- Attributions are interpretable and stable across layers, matching known dataset biases and fusion mechanisms
- Ablation studies show symmetric fusion methods yield balanced contributions, while asymmetric methods bias toward one modality

## Why This Works (Mechanism)
The method works by decomposing the total mutual information between modalities and output into interpretable components using PID. Unlike accuracy-based methods that measure output changes, PID directly analyzes the information content in internal representations. The IPFP algorithm efficiently solves the constrained optimization problem by alternating between enforcing marginal constraints via Sinkhorn updates, making the approach scalable for high-dimensional embeddings. The discretization step enables computation on continuous embeddings while preserving the information-theoretic properties needed for accurate attribution.

## Foundational Learning
- **Partial Information Decomposition (PID):**
  - Why needed: PID is the theoretical foundation that enables decomposing total mutual information into unique, redundant, and synergistic components
  - Quick check: If a model only uses information from modality X1 and ignores X2, what would be the expected values for U₁, U₂, R, and S?

- **Iterative Proportional Fitting Procedure (IPFP) & Sinkhorn Algorithm:**
  - Why needed: IPFP provides a scalable algorithm to compute PID via alternating KL-projections, avoiding expensive conic solvers
  - Quick check: The IPFP algorithm alternates between two steps to solve a constrained optimization. What are these two steps and what constraint does each enforce?

- **Multimodal Fusion Architectures:**
  - Why needed: Understanding how different fusion strategies (cross-attention vs. concatenation) process modalities explains why attribution results differ
  - Quick check: How does the directionality of cross-attention (e.g., image-to-text vs. text-to-image) theoretically influence which modality guides the processing of the other?

## Architecture Onboarding
- **Component Map:** K-means clustering -> Embedding discretization -> IPFP solver -> PID computation -> Modality attribution
- **Critical Path:** Extract embeddings → Cluster → Estimate marginals → IPFP optimization → Compute PID components → Normalize contributions
- **Design Tradeoffs:** Discretization enables tractable computation but introduces approximation error; inference-only avoids retraining but requires access to intermediate embeddings
- **Failure Signatures:** Numerical underflow in Sinkhorn updates; IPFP non-convergence; attribution instability across discretization levels
- **First Experiments:** 1) Validate on synthetic AND/XOR datasets with known ground truth; 2) Apply to simple concatenation model and verify balanced contributions; 3) Test on cross-attention model and observe modality bias

## Open Questions the Paper Calls Out
- **Robustness to Noise:** How can PID estimates be made more robust to noise in high-dimensional embedding spaces? The method is sensitive to small perturbations in feature distributions.
- **Spurious Correlations:** To what extent does the method conflate spurious dataset correlations with genuine model reasoning? Attribution scores might reflect pre-training biases rather than true multimodal integration.
- **Closed-Source Models:** Can the framework be adapted for closed-source models where intermediate embeddings are inaccessible? The current method requires internal activations that are unavailable in black-box APIs.

## Limitations
- Computational scalability remains challenging due to exponential growth of the 3D tensor Q with discretization granularity
- Results depend heavily on the number of K-means clusters used for discretization, which is not specified
- Method is demonstrated only on text-image VLMs and untested on other modality pairs
- May not fully distinguish between synergistic and redundant information in complex real-world scenarios

## Confidence
- **High Confidence:** IPFP correctly computes PID on synthetic datasets; inference-only approach works without retraining; normalization provides interpretable contributions
- **Medium Confidence:** Method reveals meaningful differences between fusion architectures; captures known dataset biases; contributions are stable within models
- **Low Confidence:** Specific numerical values are highly sensitive to discretization choices; distinguishing synergistic vs. redundant information in real data; claims about "true" contributions without ground truth validation

## Next Checks
1. **Discretization Sensitivity Analysis:** Systematically vary K-means clusters (16, 64, 256) and measure PID estimate stability across different discretization granularities
2. **Cross-Modality Generalization:** Apply method to text-audio VLMs and compare whether PID decomposition patterns follow similar trends or reveal modality-specific behaviors
3. **Ground Truth Validation on Synthetic VLM:** Construct a synthetic VLM with known fusion mechanisms and verify PID method recovers ground truth contributions accurately