---
ver: rpa2
title: 'Beyond Gaussian Initializations: Signal Preserving Weight Initialization for
  Odd-Sigmoid Activations'
arxiv_id: '2509.23085'
source_url: https://arxiv.org/abs/2509.23085
tags:
- initialization
- activation
- proposed
- networks
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses signal and gradient collapse in deep, narrow
  networks with sigmoidal activations. The authors define an "odd-sigmoid" function
  class and propose an activation-aware initialization based on controlling the effective
  gain distribution at each layer.
---

# Beyond Gaussian Initializations: Signal Preserving Weight Initialization for Odd-Sigmoid Activations

## Quick Facts
- arXiv ID: 2509.23085
- Source URL: https://arxiv.org/abs/2509.23085
- Reference count: 40
- Key outcome: Signal-preserving initialization maintains trainable signals in deep, narrow networks with sigmoidal activations without batch normalization or fine-tuned learning rates.

## Executive Summary
This paper addresses signal and gradient collapse in deep, narrow networks with sigmoidal activations by proposing an activation-aware initialization scheme. The authors define an "odd-sigmoid" function class and introduce a diagonal-plus-noise initialization that controls the effective gain distribution at each layer. This approach preserves forward activation variance and backpropagated gradient norms over a wide range of noise scales and network widths, unlike standard Gaussian initializations which require precise variance tuning. Empirical results show the method is more data-efficient and robust to depth, width, and activation scale, with stable training on standard image benchmarks and lower losses in physics-informed neural networks.

## Method Summary
The method involves computing the critical gain ω = 1/f'(0) for any odd-sigmoid activation f, then initializing each layer as W^ℓ = D^ℓ + Z^ℓ where D^ℓ has diagonal entries = ω and Z^ℓ_ij ~ N(0, σ_z²/N). The noise scale σ_z is calibrated based on target negative rate p ≈ 0.4 and depth L using σ*(p, L, ω) = -ω / Φ⁻¹((1-(1-2p)^(1/L))/2). This creates a structured matrix with controlled signal propagation properties that maintains stable training dynamics across varying depths and activation scales.

## Key Results
- Preserves activation variance and gradient norms across depth L=1000 without batch normalization
- Maintains χ amplification factor ≈ 1 over wider variance band than Gaussian initialization
- Achieves lower PINN losses and better data efficiency on image classification tasks
- Width-independent performance across networks with varying layer sizes

## Why This Works (Mechanism)

### Mechanism 1: Pitchfork Bifurcation Control via Critical Gain
Setting diagonal weights to ω = 1/f'(0) places the network at the bifurcation point between signal collapse and saturation, enabling stable propagation. The scalar iteration x_{n+1} = f(ax_n) has unique fixed point 0 when a ≤ ω, but three fixed points {±ξ_a, 0} when a > ω. At a = ω, signals neither decay to zero nor saturate to ±ξ_a, preserving information across depth.

### Mechanism 2: Effective Gain Distribution with Noise Regularization
The diagonal-plus-noise structure yields Gaussian-distributed effective gains with mean ω and data-dependent variance, preserving signal while enabling feature learning. Each neuron's effective gain a^{ℓ+1}_i = w^{ℓ+1}_{ii} + Σ_{j≠i} w^{ℓ+1}_{ij}(x^ℓ_j/x^ℓ_i) is approximately N(ω, σ²_z/N) conditionally on activations. Mean ω prevents drift; variance σ²_z provides randomness for learning.

### Mechanism 3: Gradient Amplification Stabilization via ω² Buffer
The proposed initialization maintains χ^{ℓ+1} ≈ 1 over a wider variance band because ω² counterbalances E[f'²] decay. For proposed init, χ^{ℓ+1} ≈ (ω² + σ²_z)E[f'(h^{ℓ+1})²]. As σ_z increases, E[f'²] decreases (saturation), but (ω² + σ²_z) increases, stabilizing the product. Gaussian init lacks ω² term, making χ fragile to σ_w misspecification.

## Foundational Learning

- **Fixed-point dynamics of iterated nonlinear maps**
  - Why needed here: Understanding how x_{n+1} = f(ax_n) converges to fixed points explains why signals vanish or saturate with depth.
  - Quick check question: For tanh with a = 1.5, does iteration x_{n+1} = tanh(1.5x_n) from x_0 = 0.1 converge to 0 or to a nonzero value?

- **Mean-field theory for signal propagation**
  - Why needed here: Provides the χ amplification factor framework for analyzing gradient flow through deep networks.
  - Quick check question: If χ > 1 at every layer, what happens to gradient norms as depth increases?

- **Gaussian noise addition to structured matrices**
  - Why needed here: The diagonal-plus-noise construction is the core implementation; understanding how noise scale affects effective gain distribution is critical.
  - Quick check question: If W = ωI + (σ/√N)G where G_{ij} ∼ N(0,1), what is E[(Wx)_i | x]?

## Architecture Onboarding

- **Component map**: Define odd-sigmoid f → Compute ω = 1/f'(0) → Set target p ≈ 0.4 → Compute σ_z via Φ⁻¹ formula → Initialize W^ℓ = ωI + (σ_z/√N)G^ℓ

- **Critical path**: 
  1. Verify f ∈ F (odd, bounded, strictly increasing, slope decay)
  2. Compute ω = 1/f'(0)
  3. Set target negative rate p ≈ 0.4
  4. Compute σ_z = σ*(p, L, ω)
  5. Initialize W^ℓ = ωI + (σ_z/√N)G^ℓ

- **Design tradeoffs**: Larger σ_z → more expressivity but risk of gradient explosion; smaller σ_z → stable but may underutilize nonlinearity; p=0.4 balances information preservation vs. learning capacity; width independence comes at cost of structured (not fully random) weights

- **Failure signatures**: Activations collapse to narrow range around zero → σ_z too small or diagonal not set to ω; Gradients vanish in early layers → χ < 1, likely σ_z too small; Training diverges → σ_z too large or learning rate exceeds ω-scaled band; No improvement over Gaussian init → activation may not satisfy odd-sigmoid properties

- **First 3 experiments**:
  1. **Forward pass sanity check**: Initialize L=100 tanh network, pass random input, histogram last-layer activations. Verify spread is preserved (entropy ≈ 0.9) vs. EOC (entropy → 0).
  2. **Gradient norm verification**: Compute ‖∂L/∂h^ℓ‖₂ at initialization for L ∈ {50,100,200}. Verify norms remain stable across depth for proposed vs. exponential decay for Gaussian.
  3. **Negative rate calibration test**: For depth L=50, vary σ_z and measure empirical sign flip rate. Confirm σ*(p=0.4, L=50, ω=1) produces ≈40% negative rate at final layer.

## Open Questions the Paper Calls Out

- **Is the empirical target negative rate of 0.4 information-theoretically optimal, or does the ideal rate depend on dataset complexity or network width?**
  - Basis in paper: [explicit] Appendix B.7 states, "We do not claim that $p_{real} = 0.4$ is an information-theoretically optimal value," noting it was chosen based on validation performance.
  - Why unresolved: The authors selected 0.4 empirically to balance sign preservation and randomness, but a theoretical justification connecting information theory to the optimal negative rate is missing.
  - What evidence would resolve it: A theoretical derivation of the optimal $p_{real}$ as a function of depth and width, or empirical sensitivity analysis showing performance peaks at different rates for varying data complexities.

- **Can the diagonal-plus-noise initialization scheme be effectively extended to convolutional neural networks (CNNs) or attention-based architectures like Transformers?**
  - Basis in paper: [inferred] The methodology is derived specifically for "fully feedforward neural networks (FFNNs)" and the experimental section validates this on "standard image benchmarks" using MLPs, excluding CNN/Transformer architectures.
  - Why unresolved: The theoretical derivation relies on elementwise scalar dynamics ($x_{i}^{\ell+1} = f(a_{i}^{\ell+1}x_{i}^{\ell})$) and specific gain statistics that may not hold or require modification for spatial sharing of weights (CNNs) or attention mechanisms.
  - What evidence would resolve it: Reformulation of the gain distribution for structured layers (Conv2d, Linear projections in Attention) and empirical benchmarks on CIFAR/ImageNet using ResNets or ViTs.

- **Does the proposed initialization provide complementary benefits when combined with skip connections (ResNets) or other normalization techniques?**
  - Basis in paper: [inferred] The paper notes in the experimental setting (Appendix C) that all experiments are implemented "without skip connections," focusing on demonstrating stability in pure FFNNs without Batch Normalization.
  - Why unresolved: While the method stabilizes training in vanilla networks, the interaction with identity mappings in ResNets or the additive effects of LayerNorm in Transformers is unknown.
  - What evidence would resolve it: Ablation studies comparing the proposed initialization against standard initializations within ResNet architectures to determine if the signal preservation benefits are redundant with or complementary to skip connections.

## Limitations
- Theoretical claims rely on mean-field approximation that may break down at extreme depths or widths
- Method's effectiveness depends critically on correctly identifying odd-sigmoid activations and computing precise critical gain ω
- Empirical validation focuses primarily on feedforward architectures without skip connections or normalization layers
- Computational overhead of computing σ_z for each depth may be non-trivial for very deep networks

## Confidence
- **High Confidence**: Signal and gradient preservation mechanisms at initialization (forward pass and gradient norm stability)
- **Medium Confidence**: Training performance improvements and data efficiency gains (empirical results show benefits but depend on specific training protocols)
- **Low Confidence**: Claims about optimal negative rate calibration and theoretical guarantees for arbitrary odd-sigmoid functions (based on limited experimental validation)

## Next Checks
1. **Mean-field breakdown test**: Train networks with L ∈ {50, 100, 200, 500} and verify that gradient norms remain stable according to theoretical predictions. Plot gradient norm vs. depth on log scale to identify where mean-field assumptions break down.

2. **Activation class robustness**: Test the initialization on non-odd-sigmoid activations (ReLU, Leaky ReLU, Swish) to confirm that signal collapse occurs as predicted when ω is undefined. Measure activation variance preservation across depth.

3. **Width independence verification**: For fixed depth L=100, vary width N ∈ {128, 512, 2048} and verify that training dynamics (loss curves, gradient norms) remain consistent across widths, confirming the claimed width-independence property.