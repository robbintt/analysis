---
ver: rpa2
title: Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based
  Multi-Objective Reinforcement Learning
arxiv_id: '2601.14092'
source_url: https://arxiv.org/abs/2601.14092
tags:
- data
- collection
- devices
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an attention-based multi-objective reinforcement
  learning (MORL) framework for optimizing data collection and energy consumption
  in UAV-aided IoT networks. The method uses a permutation-invariant attention architecture
  to generalize across unseen environments without retraining, and incorporates preference
  vectors directly as input tokens for flexible trade-off control.
---

# Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.14092
- Source URL: https://arxiv.org/abs/2601.14092
- Authors: Babacar Toure; Dimitrios Tsilimantos; Omid Esrafilian; Marios Kountouris
- Reference count: 35
- This paper proposes an attention-based multi-objective reinforcement learning (MORL) framework for optimizing data collection and energy consumption in UAV-aided IoT networks.

## Executive Summary
This paper introduces MOSAC-ATT, an attention-based multi-objective reinforcement learning framework for optimizing data collection and energy consumption in UAV-aided IoT networks. The framework leverages permutation-invariant attention architecture and preference vectors to achieve flexible trade-offs without retraining. Extensive simulations demonstrate superior performance over state-of-the-art MORL methods in terms of sample efficiency, model compactness, and robustness to environmental variations.

## Method Summary
The proposed method employs an attention-based multi-objective soft actor-critic (MOSAC-ATT) framework that operates in continuous action spaces. The architecture uses a permutation-invariant attention mechanism to process variable numbers of IoT devices as input tokens, with preference vectors integrated directly as additional input tokens for flexible trade-off control. The model operates within a MOMDP formulation, where the UAV agent collects data from IoT devices while managing energy consumption. The attention mechanism ensures scalability across different numbers of devices, and the preference vector allows for on-the-fly adjustment of objective priorities without retraining.

## Key Results
- MOSAC-ATT achieves up to 87% data collection efficiency in previously unseen settings
- Model demonstrates 55% fewer parameters compared to state-of-the-art MORL methods
- Superior sample efficiency and multi-objective performance compared to existing methods
- Robustness to environmental variations without requiring retraining

## Why This Works (Mechanism)
The attention-based architecture enables the model to dynamically weigh the importance of different IoT devices based on their current state and predicted contribution to the objectives. By incorporating preference vectors as input tokens rather than conditioning mechanisms, the model can flexibly adjust trade-offs between data collection and energy consumption in real-time. The permutation-invariant design ensures consistent performance regardless of the order of input devices, making the policy robust to different device arrangements and scalable to varying numbers of devices.

## Foundational Learning
- **Multi-Objective Reinforcement Learning (MORL)**: Needed to handle conflicting objectives (data collection vs. energy consumption) simultaneously; quick check: verify the preference vector integration works as intended
- **Attention Mechanisms in RL**: Required for scalable processing of variable numbers of IoT devices; quick check: test permutation invariance property
- **Continuous Action Spaces**: Essential for smooth UAV trajectory control; quick check: validate action bounds and clipping
- **Soft Actor-Critic (SAC) Extension**: Needed for stable multi-objective learning; quick check: compare against standard SAC performance

## Architecture Onboarding

Component Map:
UAV Agent -> Attention Encoder -> Actor-Critic Network -> Continuous Actions -> Environment -> Reward Signals -> Memory Buffer

Critical Path:
The attention encoder processes IoT device states and preference vectors, which feed into the actor-critic network to produce continuous actions for UAV movement and data collection decisions. These actions interact with the environment to generate rewards, which update the policy through the SAC algorithm.

Design Tradeoffs:
The permutation-invariant attention architecture sacrifices some representational capacity for scalability and generalization across different device configurations. The fixed token limit (K_max) requires heuristic device selection when device count exceeds this threshold, potentially missing strategically important devices.

Failure Signatures:
- Degraded performance when device density significantly exceeds K_max
- Sensitivity to noise in device state information (partial observability)
- Potential overfitting to specific device distributions during training

First Experiments:
1. Test permutation invariance by shuffling device input order and measuring policy consistency
2. Evaluate performance degradation when device count exceeds K_max token limit
3. Compare sample efficiency against standard SAC with and without attention mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the attention-based MORL framework be extended to decentralized multi-UAV coordination without suffering from non-stationarity or credit assignment issues?
- Basis in paper: Section VI concludes that future work may "extend this framework to multi-UAV coordination."
- Why unresolved: The current study focuses on a single UAV agent; adding multiple agents introduces complex dynamics where one agent's actions alter the environment and rewards for others.
- What evidence would resolve it: Successful training of multiple agents in the same environment demonstrating collision avoidance and efficient load balancing without centralized control.

### Open Question 2
- Question: How does the policy performance degrade when the agent operates under partial observability (POMDP) rather than the full state assumptions used in the current MOMDP formulation?
- Basis in paper: Section VI suggests "integrate partial observability" as a direction for future work to approach real-world deployment.
- Why unresolved: The model currently assumes full knowledge of the environment state (positions, data, SNR of all devices), which is often unavailable in real-world scenarios due to sensor range or occlusion.
- What evidence would resolve it: Evaluation of the model's robustness when state inputs are noisy, incomplete, or memory-based estimations are required.

### Open Question 3
- Question: Does the inclusion of realistic wireless interference modeling significantly alter the trade-off strategies learned by the MOSAC-ATT agent?
- Basis in paper: Section VI identifies "model wireless interference" as necessary for real-world deployment.
- Why unresolved: The current channel model (Eq. 5) calculates SNR based primarily on distance and shadowing, potentially ignoring the impact of aggregate interference from dense IoT devices.
- What evidence would resolve it: Simulation results comparing policy performance under the current log-normal shadowing model versus a model incorporating inter-device interference.

### Open Question 4
- Question: Can the architecture effectively handle scenarios where the number of IoT devices significantly exceeds the fixed token limit (K_max) without relying on the naive SNR-based pre-selection?
- Basis in paper: Section V-B states that if devices exceed the limit, they are "naively selected," while Section IV-B sets a hard limit K_max.
- Why unresolved: The fixed input size forces a heuristic selection of "top K_max" devices by SNR, which might filter out strategically important devices (e.g., those with low current SNR but high data value).
- What evidence would resolve it: Performance metrics in environments with dense device deployments (e.g., K > K_max) comparing the naive selection against dynamic attention mechanisms that scale with input size.

## Limitations
- Evaluation limited to simulation environments without validation on real UAV-IoT deployments
- Claims about robustness to environmental variations lack rigorous statistical validation
- Comparison against only four existing MORL methods without including non-MORL approaches
- Scalability analysis focuses on single UAV scenarios without exploring multi-UAV coordination

## Confidence
- High confidence: The architectural innovation of permutation-invariant attention and preference vector integration as input tokens is technically sound and well-explained.
- Medium confidence: The performance improvements over existing MORL methods are demonstrated convincingly, though limited to simulations.
- Low confidence: Claims about robustness to environmental variations and model compactness benefits lack rigorous statistical validation.

## Next Checks
1. Implement a pilot test on a small-scale real UAV-IoT testbed to validate simulation results and assess practical challenges like sensor communication delays and UAV dynamics.
2. Conduct ablation studies removing the attention mechanism to quantify its specific contribution versus other components of the MORL framework.
3. Test the framework's generalization across different IoT device distributions and densities not seen during training, particularly focusing on edge cases where device density is either very low or extremely high.