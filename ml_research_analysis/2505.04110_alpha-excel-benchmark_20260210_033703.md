---
ver: rpa2
title: Alpha Excel Benchmark
arxiv_id: '2505.04110'
source_url: https://arxiv.org/abs/2505.04110
tags:
- excel
- challenges
- performance
- benchmark
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel benchmark for evaluating Large Language
  Models (LLMs) using 113 Financial Modeling World Cup Excel challenges converted
  to JSON format. The benchmark tests models across categories like financial modeling,
  game simulations, and data analysis, revealing that GPT-4o-mini achieved the highest
  overall accuracy, especially on structured financial tasks (20% accuracy).
---

# Alpha Excel Benchmark

## Quick Facts
- arXiv ID: 2505.04110
- Source URL: https://arxiv.org/abs/2505.04110
- Reference count: 3
- Primary result: GPT-4o-mini achieved highest accuracy (~80%) on 113 JSON-converted Excel challenges, excelling on structured financial tasks

## Executive Summary
This study introduces a novel benchmark for evaluating Large Language Models using 113 Financial Modeling World Cup Excel challenges converted to JSON format. The benchmark tests models across categories like financial modeling, game simulations, and data analysis, revealing that GPT-4o-mini achieved the highest overall accuracy, especially on structured financial tasks (>20% accuracy). Models performed better on financial and data analysis challenges than on complex game simulations requiring multi-step reasoning and state tracking. The benchmark provides a practical, business-oriented evaluation framework bridging academic AI metrics with real-world Excel usage, highlighting numerical reasoning and rule application as key limitations for current LLMs.

## Method Summary
The benchmark converts 113 FMWC Excel challenges into JSON format with metadata, problem statements, input data, and structured question-answer pairs. Three leading LLMs (GPT-4o-mini, Mistral, Qwen2.5) were evaluated using standardized API settings: temperature=0.2, 60-second timeout, randomized challenge order, and zero-shot prompting with minimal few-shot examples. The evaluation measures accuracy across four challenge categories (financial modeling, game simulation, data analysis, misc) with partial credit for multi-step problems. The dataset and evaluation harness are open-sourced for reproducibility.

## Key Results
- GPT-4o-mini achieved ~80% overall accuracy versus human expert ~85%
- All models performed significantly better on structured financial tasks (>20% accuracy) than game simulations
- Error analysis revealed consistent patterns: numerical reasoning errors, state tracking difficulties, and rule application challenges
- Financial modeling tasks showed the most consistent performance across models due to their structured nature

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured JSON conversion of spreadsheet challenges enables programmatic evaluation while preserving core problem complexity.
- **Mechanism:** The authors transform 113 Excel challenges from the FMWC into a standardized JSON schema containing metadata, problem statements, input data, and structured question-answer pairs. This removes Excel-specific implementation details while retaining cognitive demands.
- **Core assumption:** The JSON representation captures the essential difficulty of the original spreadsheet tasks without introducing systematic biases.
- **Evidence anchors:** [abstract] "We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats"; [Methods] "The transformation process involved determining which aspects of the original Excel-specific formulations were incidental to the implementation rather than central to the problem's demands."

### Mechanism 2
- **Claim:** Domain-specific challenge categories reveal distinct capability profiles in LLMs.
- **Mechanism:** The benchmark categorizes problems into financial modeling (35%), game simulation (40%), data analysis (15%), and misc tasks (10%). Evaluation across these categories shows models perform better on structured financial tasks than game simulations requiring state tracking and rule application.
- **Core assumption:** Category labels accurately reflect underlying cognitive demands, and performance differences stem from capability gaps rather than data distribution artifacts.
- **Evidence anchors:** [abstract] "Evaluation of three leading LLMs shows GPT-4o-mini achieving the highest accuracy at 80%, with all models performing better on structured financial tasks than game simulations"; [Results] "The most significant performance variations were observed in game simulation tasks... where models needed to track game states and apply complex rule sets."

### Mechanism 3
- **Claim:** Error analysis across failure modes provides diagnostic signals for model improvement.
- **Mechanism:** Systematic examination of incorrect solutions reveals that errors increase with calculation chain length (numerical reasoning), multi-rule scenarios (rule application), and technical terminology (context misunderstanding). GPT-4o-mini shows fewer numerical reasoning errors, suggesting architectural differences.
- **Core assumption:** Error taxonomies generalize across models and problem types, and specific failure modes are addressable through targeted interventions.
- **Evidence anchors:** [abstract] "The benchmark reveals consistent patterns in model limitations, including numerical reasoning errors, state tracking difficulties, and rule application challenges"; [Results/Methods] "These failure categories included calculation mistakes, arithmetic errors... Rule application errors occurred when models correctly understood problem statements but inconsistently applied or occasionally disregarded specified rules and constraints."

## Foundational Learning

- **Concept: Programmatic benchmark construction from human competitions**
  - Why needed here: The paper repurposes expert-designed Excel challenges into a standardized evaluation framework. Understanding how to preserve task validity during format conversion is essential for creating useful benchmarks.
  - Quick check question: When converting an interactive spreadsheet challenge to JSON, what information loss is acceptable versus critical to preserve?

- **Concept: State tracking in sequential reasoning**
  - Why needed here: Game simulation challenges expose LLM weaknesses in maintaining accurate representations across multiple state transitions. This concept underlies the error taxonomy's focus on multi-step problems.
  - Quick check question: In a game where position and score update each turn, what mechanism could help an LLM track state consistently across 10 iterations?

- **Concept: Partial credit scoring for multi-step problems**
  - Why needed here: The benchmark avoids binary correct/incorrect assessments, allocating points by solution quality. This enables finer-grained capability measurement.
  - Quick check question: If a model correctly applies 4 of 5 rules in a multi-rule problem but fails on the final aggregation, should it receive partial credit, and how would you quantify it?

## Architecture Onboarding

- **Component map:** Challenge dataset (113 JSON-encoded problems from FMWC) -> Evaluation framework (open-sourced testing harness) -> Model interface layer (API connections to GPT-4o-mini, Mistral, Qwen2.5) -> Scoring system (normalization procedures + partial credit rubric)

- **Critical path:**
  1. Acquire FMWC challenge set and validate JSON schema coverage
  2. Implement evaluation harness with API rate limiting and error handling
  3. Run baseline evaluation across target models
  4. Conduct error analysis using proposed taxonomy
  5. Iterate on prompt engineering or fine-tuning based on failure patterns

- **Design tradeoffs:**
  - JSON format enables automation but sacrifices visual/spatial dimensions of authentic Excel work
  - 60-second timeout accommodates API constraints but removes time-pressure dynamics present in human competitions
  - Temperature 0.2 balances determinism with solution diversity; may still allow drift in extended chains
  - Zero/few-shot prompting avoids solution contamination but may underrepresent model potential with examples

- **Failure signatures:**
  - Numerical drift: Small calculation errors propagating through multi-step chains, producing large final-value deviations
  - Rule inconsistency: Correct isolated rule application but failures when rules interact conditionally
  - Context misalignment: Solutions addressing different problems than specified, especially with domain-specific terminology
  - State degradation: Accuracy decline as simulation steps accumulate, indicating memory/attention limits

- **First 3 experiments:**
  1. **Category baseline:** Run all 113 challenges across 3 models with current settings; validate that financial tasks outperform game simulations as reported.
  2. **Chain length sensitivity:** Subset problems by calculation chain length (short/medium/long); measure error rate as function of steps to quantify numerical drift.
  3. **Rule complexity stress test:** Select game simulation challenges with varying rule counts (2, 4, 6+ interdependent rules); isolate whether failures scale with rule interactions or with state transitions independently.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid architectures that integrate symbolic mathematics engines with LLMs significantly reduce the numerical reasoning errors identified in the benchmark?
- **Basis in paper:** [explicit] The "Future Work" section suggests "hybrid approaches" combining language models with specialized numerical components to address the "inconsistent precision" noted in the results.
- **Why unresolved:** Current models struggle with "calculation mistakes" and "flawed mathematical operations" in extended chains, and it is unknown if external tools can be integrated without disrupting the LLM's contextual problem interpretation.
- **What evidence would resolve it:** Comparative evaluation on the Alpha Excel Benchmark showing a hybrid system achieving higher accuracy on financial modeling tasks than standalone LLMs.

### Open Question 2
- **Question:** Does a multi-turn, interactive evaluation protocol improve model performance on complex game simulation challenges compared to single-turn prompting?
- **Basis in paper:** [explicit] The authors propose "multi-turn evaluation protocols" and "simulated Excel environments" in "Future Work" to better mirror human iterative refinement and state tracking.
- **Why unresolved:** The current methodology uses static JSON prompts which remove the "rich interactive dimensions" of spreadsheet work, potentially hiding the models' true ability to correct "state tracking" errors via feedback.
- **What evidence would resolve it:** A new benchmark iteration where models interact with a live spreadsheet environment, specifically measuring error reduction in "Mortal Wombat" or "Ludo" simulations.

### Open Question 3
- **Question:** How does AI assistance impact the performance variance between Excel novices and seasoned professionals?
- **Basis in paper:** [explicit] The "Future Work" section calls for "collaborative benchmarks" to measure "human-AI collaboration outcomes" across "diverse user expertise levels."
- **Why unresolved:** While the paper notes LLMs excel at formula recall and humans at pattern identification, it remains unclear if these strengths combine to raise overall proficiency or if the gap between skill levels narrows.
- **What evidence would resolve it:** User studies measuring task completion time and accuracy on benchmark challenges for both novices and experts, with and without AI assistance.

## Limitations

- JSON conversion process may strip away spatial and visual dimensions inherent to Excel work, potentially introducing biases favoring certain problem types
- 60-second timeout constraint may artificially limit models that could succeed with longer reasoning chains, particularly in complex game simulations
- Error taxonomy lacks empirical validation through inter-rater reliability assessment, and partial credit scoring system is not fully specified

## Confidence

- **High Confidence:** The benchmark successfully distinguishes model performance across challenge categories, with GPT-4o-mini showing superior accuracy on structured financial tasks
- **Medium Confidence:** The assertion that JSON conversion preserves essential problem complexity while enabling programmatic evaluation
- **Low Confidence:** The specific error taxonomies and their implications for model improvement

## Next Checks

1. **Conversion Fidelity Test:** Select 10 challenges from each category and evaluate both the original Excel implementation and JSON version with human experts. Compare solution patterns and identify any systematic differences in problem-solving approaches to validate that JSON conversion preserves core cognitive demands.

2. **Error Taxonomy Validation:** Have three independent annotators categorize 100 random model failures using the proposed taxonomy. Calculate inter-rater reliability (Cohen's kappa) to assess whether the categories are well-defined and mutually exclusive, or whether they overlap significantly.

3. **State Tracking Stress Test:** Design a controlled experiment varying chain length and state complexity in game simulation challenges. Measure accuracy degradation as a function of both variables independently to determine whether memory/attention limits or rule application complexity drives the observed performance decline in these tasks.