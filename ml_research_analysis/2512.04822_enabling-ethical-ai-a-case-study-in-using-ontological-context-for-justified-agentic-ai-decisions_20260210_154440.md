---
ver: rpa2
title: 'Enabling Ethical AI: A case study in using Ontological Context for Justified
  Agentic AI Decisions'
arxiv_id: '2512.04822'
source_url: https://arxiv.org/abs/2512.04822
tags:
- server
- system
- monitoring
- your
- check
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a neuro-symbolic architecture that combines
  AI agents with human domain experts to create an inspectable semantic layer for
  Agentic AI. The approach involves AI agents proposing knowledge structures from
  diverse data sources, which domain experts then validate, correct, and extend.
---

# Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions

## Quick Facts
- arXiv ID: 2512.04822
- Source URL: https://arxiv.org/abs/2512.04822
- Reference count: 0
- One-line primary result: Context-enriched prompting significantly improves LLM accuracy and coherence in enterprise AI decisions

## Executive Summary
This study presents a neuro-symbolic architecture that combines AI agents with human domain experts to create an inspectable semantic layer for Agentic AI. The approach involves AI agents proposing knowledge structures from diverse data sources, which domain experts then validate, correct, and extend. This feedback loop improves subsequent models and captures tacit institutional knowledge.

The research validates that providing Large Language Models with ontology-derived context significantly enhances response quality. Empirical testing across five cycles with eight tests each showed that context-enriched prompting improves both accuracy and coherence of AI outputs. Statistical analysis confirmed that full contextual enhancement (including complex "super prompts") resulted in universal improvement across all model-cycle combinations for accuracy and coherence, with p < 0.0001 significance.

## Method Summary
The study evaluates whether ontology-derived contextual enhancement improves LLM response quality in enterprise Agentic AI scenarios. Three LLMs (GPT-4o, Gemini 2.0 Flash Thinking, Gemma3 27B) complete 8 test prompts with progressive context enrichment across 5 cycles, totaling 40 responses per model. Test 1 uses minimal context, Tests 2-6 incrementally add domain context, and Tests 7-8 are "super prompts" with full ontological detail. Responses are human-rated on accuracy (0-5), coherence (0-5), and relevance (0-5) scales, with Sign tests determining statistical significance.

## Key Results
- Super prompts with full ontological context showed universal improvement across all model-cycle combinations (p < 0.0001)
- Gradual context enhancement showed no statistical significance, with only 3 out of 15 model-cycle combinations improving
- The system enables justifiable Agentic AI decisions through a Toulmin argumentation framework grounded in explicit, inspectable evidence

## Why This Works (Mechanism)

### Mechanism 1: Semantic Layer Co-Construction
- **Claim:** A collaborative human-AI loop generates an inspectable semantic layer that captures tacit institutional knowledge more effectively than manual data archaeology.
- **Mechanism:** AI agents propose candidate knowledge structures from raw data (e.g., ontologies), which domain experts then validate, correct, and extend. This feedback is fed back into the system, refining the model.
- **Core assumption:** Domain experts possess tacit knowledge that can be externalized into formal structures, and AI can assist in the initial heavy-lifting of structuring raw data.
- **Evidence anchors:** [Abstract] "AI agents first propose candidate knowledge structures... domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models." [Page 2] "These human-derived corrections, in turn, train subsequent AI models... preventing knowledge loss (institutional amnesia)."
- **Break condition:** If domain experts cannot efficiently validate AI proposals due to volume or complexity, the "human-in-the-loop" becomes a bottleneck, failing to capture tacit knowledge.

### Mechanism 2: Context-Enriched Prompting (Super Prompts)
- **Claim:** Providing Large Language Models (LLMs) with dense, structured ontological context ("super prompts") significantly improves response accuracy and coherence compared to minimal or gradual context.
- **Mechanism:** Structured knowledge (ontologies) is injected into the LLM prompt. The empirical study showed that a "step change" in context volume (Tests 7-8 "super prompts") yielded universal improvement (p < 0.0001), whereas gradual increases (Tests 1-6) showed no statistical significance.
- **Core assumption:** The performance gain is driven by the semantic density and structure of the context, rather than just the quantity of tokens.
- **Evidence anchors:** [Page 16] "Gradual context enhancement... showed only 3 out of 15 model-cycle combinations improving... In contrast, the full contextual enhancement including super prompts... resulted in universal improvement... p < 0.0001." [Page 12] "Tests 7 and 8: 'super prompts'. These introduce a significantly higher volume of contextual detail all at once."
- **Break condition:** If the ontological context contains noise or contradictions, or if the context window limits are exceeded, the "super prompt" may degrade performance or hallucinate.

### Mechanism 3: Justification via Toulmin Argumentation
- **Claim:** Agentic decisions can be made inspectable and justifiable by structuring them using the Toulmin argumentation framework (Claim, Grounds, Warrant, Backing, Rebuttals).
- **Mechanism:** Instead of post-hoc explainability (e.g., saliency maps), the system requires the agent to construct a reasoning chain grounded in the semantic layer. This "justification" is generated as part of the decision process.
- **Core assumption:** Regulatory bodies and human operators value a logical argument structure (Claim/Grounds/Warrant) over purely statistical correlations or black-box explanations.
- **Evidence anchors:** [Page 10] "OntoKai justification approach... does not seek to explain how the AI system came up with a given result, instead focusing on whether the given result is justifiable... using the Toulmin schema." [Page 11] "OntoKai embeds this argumentation process within a continuous and auditable workflow that links decisions directly to structured ontological context."
- **Break condition:** If the domain problem is not suited to logical decomposition or lacks structured evidence, the Toulmin framework may produce a "justification" that is logically valid but factually hollow.

## Foundational Learning

- **Concept: Neuro-Symbolic Architecture**
  - **Why needed here:** The paper fundamentally relies on bridging the gap between neural networks (LLMs, which handle pattern recognition and language) and symbolic AI (Ontologies, which handle logic and rules). You cannot understand OntoKai/AIR without grasping this duality.
  - **Quick check question:** Can you explain why a pure LLM (neural) struggles with "institutional amnesia" or specific enterprise rules without the symbolic layer?

- **Concept: Ontological Context & Knowledge Graphs**
  - **Why needed here:** The "semantic layer" in the paper is implemented as an ontology. Understanding how data is structured as entities and relationships is crucial for the "super prompt" mechanism.
  - **Quick check question:** How does structuring data as a graph (ontology) differ from storing it in a relational database in terms of semantic meaning?

- **Concept: Toulmin Argumentation Model**
  - **Why needed here:** This is the specific theoretical framework the paper uses to solve the "interpretability gap." It replaces "explainability" with "justification."
  - **Quick check question:** Identify the difference between the "Warrant" (the bridge) and the "Grounds" (the evidence) in a Toulmin argument.

## Architecture Onboarding

- **Component map:** OntoKai (Knowledge Orchestration) -> Avantra AIR (Agentic AI) -> REST API queries -> Toulmin Justification Loop -> OntoKai storage
- **Critical path:**
  1. Ingest raw data/docs into **OntoKai**
  2. Run AI to generate candidate ontologies; **Domain Experts validate**
  3. **Avantra AIR** queries OntoKai for context ("Super Prompt")
  4. Agent executes decision based on enriched context
  5. System logs the decision with **Toulmin Justification** for audit
- **Design tradeoffs:**
  - **Super Prompts vs. Incremental Context:** The paper suggests "super prompts" (massive context injection) are statistically more effective than gradual context, but they consume more tokens/compute
  - **Justification vs. Explanation:** The system prioritizes logical justification (is it right?) over model explainability (how did it think?). This supports compliance but may hide model biases
- **Failure signatures:**
  - **Context Window Saturation:** "Super prompts" fail if the context exceeds the LLM's token limit
  - **Validation Bottleneck:** If human experts don't validate AI-generated ontologies, the semantic layer becomes noisy, degrading agent performance
  - **Hollow Justification:** The agent generates a valid Toulmin structure but links to non-existent or incorrect ontology nodes
- **First 3 experiments:**
  1. **Super Prompt Benchmark:** Take a dataset (e.g., shipping container logs) and query an LLM. Compare accuracy with zero-context vs. full-ontology context ("super prompt") to replicate the p < 0.0001 finding
  2. **Toulmin Audit Trail:** Simulate an agentic decision (e.g., "restart server"). Force the model to output the decision in Toulmin format (Claim, Grounds, Warrant) using a provided ontology snippet
  3. **Hallucination Check:** Provide the agent with a specific ontology containing a rule (e.g., "Max 10 containers in Warehouse 0024"). Ask a question that violates this rule and verify if the "Grounds" or "Rebuttal" in the justification correctly cites the ontology constraint

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the improvement in LLM performance stem from the semantic structure of ontologies or merely from the increased quantity of information provided?
- **Basis in paper:** [explicit] The authors note the experimental design "did not isolate the specific contribution of ontological structuring versus information quantity or relevance."
- **Why unresolved:** The study compared increasing levels of context but lacked a control group receiving unstructured text of equivalent length.
- **What evidence would resolve it:** A controlled A/B test comparing model outputs using structured ontological context versus unstructured text containing the same information.

### Open Question 2
- **Question:** How can a system identify which actions are suitable for autonomous justification versus those requiring human-in-the-loop validation?
- **Basis in paper:** [explicit] The authors ask how to "identify which actions can be justified by the system itself without a human in the loop, and how to abstract the justification outputs."
- **Why unresolved:** Scaling the human review process for massive datasets requires distinguishing low-risk actions that can be automated from high-risk ones.
- **What evidence would resolve it:** A classification framework or risk threshold that successfully categorizes actions into autonomous and human-review buckets.

### Open Question 3
- **Question:** Which consensus model is most appropriate for specific swarm configurations to balance system responsiveness with result consistency?
- **Basis in paper:** [explicit] The text highlights the open question of "which consensus model is most appropriate for each specific scenario" when coordinating multiple AI agents.
- **Why unresolved:** Strict consensus requirements can render systems unresponsive, while lenient requirements risk inconsistent or hallucinated results.
- **What evidence would resolve it:** Empirical testing of various consensus algorithms within agentic swarms to identify the optimal balance for enterprise tasks.

## Limitations
- The empirical validation focuses on a single domain (SAP server monitoring) and uses only three LLM models, limiting generalizability claims
- The "super prompt" mechanism's reliance on massive context injection raises practical concerns about token limits and computational costs in production systems
- Human validation of AI-generated ontologies introduces a potential bottleneck that could limit scalability

## Confidence

- **High:** The neuro-symbolic architecture combining ontologies with LLMs is technically sound and addresses documented interpretability gaps in current explainability methods
- **Medium:** The statistical evidence for super prompts improving accuracy and coherence is robust, but the single-domain focus limits generalizability claims
- **Medium:** The Toulmin argumentation framework provides a theoretically valid approach to justification, but practical effectiveness across diverse domains remains unproven

## Next Checks

1. **Cross-Domain Validation:** Test the super prompt mechanism across at least three diverse domains (e.g., healthcare diagnosis, financial compliance, manufacturing quality control) to verify generalizability beyond SAP monitoring

2. **Human Validation Bottleneck Assessment:** Measure the time and cognitive load required for domain experts to validate AI-generated ontologies, and evaluate whether this scales for enterprise-wide deployment with multiple concurrent agents

3. **Hallucination Vulnerability Test:** Systematically introduce controlled contradictions into the ontology and measure whether the Toulmin-based justification mechanism can detect and appropriately flag these inconsistencies in agent decisions