---
ver: rpa2
title: 'MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval'
arxiv_id: '2601.09562'
source_url: https://arxiv.org/abs/2601.09562
tags:
- query
- image
- retrieval
- images
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MM-BRIGHT is the first benchmark combining multimodal queries,
  reasoning-intensive technical domains, and multiple retrieval task variants. It
  contains 2,803 real-world queries spanning 29 diverse technical domains with four
  tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image,
  and multimodal-to-multimodal retrieval.'
---

# MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval

## Quick Facts
- arXiv ID: 2601.09562
- Source URL: https://arxiv.org/abs/2601.09562
- Reference count: 40
- First benchmark combining multimodal queries, reasoning-intensive technical domains, and multiple retrieval task variants

## Executive Summary
MM-BRIGHT introduces a multi-task multimodal benchmark for reasoning-intensive retrieval, containing 2,803 real-world queries across 29 technical domains. The benchmark evaluates four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. State-of-the-art models struggle across all tasks, with BM25 achieving only 8.5 nDCG@10 on text-only retrieval and the best multimodal model (Nomic-Vision) reaching just 27.6 nDCG@10 on multimodal-to-text retrieval, actually underperforming the best text-only model (DiVeR: 32.2).

## Method Summary
MM-BRIGHT contains 2,803 real-world queries spanning 29 technical domains including physics, bioinformatics, and engineering. The benchmark defines four retrieval tasks of increasing complexity: text-to-text (Task 1), multimodal-to-text (Task 2), multimodal-to-image (Task 3), and multimodal-to-multimodal (Task 4). Queries are manually annotated for domain, reasoning intensity, and image essentiality. The corpus includes 2.4M+ documents, with each query evaluated using nDCG@10 as the primary metric. Hard negatives are generated using GPT-4o to prevent models from relying on simple semantic similarity.

## Key Results
- BM25 achieves only 8.5 nDCG@10 on text-only retrieval (Task 1)
- Best multimodal model (Nomic-Vision) reaches 27.6 nDCG@10 on multimodal-to-text retrieval (Task 2), underperforming best text-only model (DiVeR: 32.2)
- All multimodal models perform worst when images are essential for understanding queries (15.0 avg nDCG@10 vs 20.4 for helpful images)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding images to queries degrades retrieval performance in current multimodal models because they are trained on surface-level image-text matching rather than visual reasoning.
- Mechanism: The paper reports that the best multimodal model (Nomic-Vision, 27.6 nDCG@10) underperforms the best text-only model (DiVeR: 32.2) on multimodal-to-text retrieval. The authors attribute this to training on simple correspondence tasks rather than reasoning-intensive visual understanding.
- Core assumption: Visual reasoning requires architectures trained to extract semantic relationships from diagrams, charts, and technical figures, not just object recognition.
- Evidence anchors:
  - [abstract] "while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval, actually underperforming the best text-only model (DiVeR: 32.2)"
  - [section 4.2] "we believe this is because current multimodal models are trained mainly on simple image-text matching rather than visual reasoning"
  - [corpus] MRMR benchmark (arxiv:2510.09510) confirms "reasoning-intensive demands" are overlooked in current multimodal retrieval evaluation

### Mechanism 2
- Claim: Image captions improve semantic retrievers but degrade reasoning-enhanced retrievers, suggesting different retrieval paradigms process visual information differently.
- Mechanism: Semantic embeddings (E5) gain +7.4 nDCG@10 from captions by expanding lexical coverage, while reasoning-enhanced models (DiVeR) lose -12.0 points, indicating that verbose descriptions disrupt multi-step inference processes.
- Core assumption: Reasoning retrievers rely on concise query structures to construct inference chains; caption noise interferes with chain construction.
- Evidence anchors:
  - [section 5] "semantic dense retrievers like E5 improve dramatically with captions (+7.4 nDCG@10), reasoning-enhanced models like DiVeR suffer severe performance degradation (-12.0 points)"
  - [section 5] "image captions, while providing useful semantic information, introduce noise that disrupts reasoning-based retrieval strategies"
  - [corpus] DiVeR paper (arxiv:2508.07995) describes multi-stage reasoning requiring clean query signals

### Mechanism 3
- Claim: Models perform worst on queries where images are essential, indicating current multimodal encoders fail to prioritize critical visual evidence.
- Mechanism: Analysis by image essentiality shows all multimodal models score lowest on "essential" images (15.0 avg nDCG@10) vs. "helpful" images (20.4). This inverted pattern suggests models rely on surface associations rather than extracting task-critical visual information.
- Core assumption: Essential images contain information that cannot be expressed in text (e.g., circuit configurations, microscopy features), requiring deeper visual parsing.
- Evidence anchors:
  - [section 5, figure 6] "all multimodal models perform worst when images are essential for understanding the query"
  - [section 5] "performance is highest for queries with helpful images (20.4 nDCG@10 on average), but drops sharply for essential images (15.0)"
  - [corpus] MR²-Bench (arxiv:2509.26378) notes existing benchmarks "fail to assess deeper reasoning" in multimodal contexts

## Foundational Learning

- Concept: nDCG@10 (Normalized Discounted Cumulative Gain at rank 10)
  - Why needed here: The primary evaluation metric across all four retrieval tasks; understanding it is essential to interpret the 8.5–45.6 score ranges reported.
  - Quick check question: Given BM25 scores 8.5 and DiVeR scores 32.2 on Task 1, what does this 3.8× improvement indicate about reasoning-intensity vs. lexical matching?

- Concept: Hard negative mining
  - Why needed here: The benchmark uses GPT-4o to generate search queries specifically designed to retrieve topically similar but irrelevant documents (Section A.7.1), preventing models from relying on simple semantic similarity.
  - Quick check question: Why would a document about "ammonium nitrate explosives" be a hard negative for a bioinformatics query about genome trees?

- Concept: Multimodal fusion architectures (CLIP-family vs. reasoning-enhanced)
  - Why needed here: The paper contrasts contrastive VLMs (CLIP, SigLIP) with multimodal embedding models (BGE-VL, Nomic-Vision, GME); understanding their differences explains performance gaps.
  - Quick check question: Why does GME-2B excel at image retrieval (Task 3: 45.6) but struggle with multimodal-to-text (Task 2: 19.5)?

## Architecture Onboarding

- Component map: Query encoder -> Document/image encoder -> Relevance scoring -> Hard negative generator
- Critical path: 1. Load MM-BRIGHT dataset (1,585 queries for Task 1; 1,218 for Tasks 2–4) 2. Encode queries with/without images depending on task 3. Retrieve from corpus (2.4M+ documents across domains) 4. Compute nDCG@10 with binary (Tasks 1–3) or graded relevance (Task 4)
- Design tradeoffs:
  - Caption-based augmentation: Quick fix for text-only retrievers but harms reasoning models
  - Query reformulation: Provides +3 nDCG for semantic retrievers but inconsistent for reasoning models
  - Model scale: Larger caption models (Qwen-72B) show diminishing returns over smaller ones (Qwen-3B) for retrieval tasks
- Failure signatures:
  - Multimodal model underperforms text-only baseline → likely surface-matching architecture, consider reasoning-aware training
  - Sharp drop on "essential" image queries → visual encoder not extracting task-critical features
  - Caption augmentation degrades performance → model expects concise reasoning inputs, not verbose descriptions
- First 3 experiments:
  1. Baseline reproduction: Run BM25 and E5 on Task 1 to verify 8.5 and 25.3 nDCG@10 scores match reported results.
  2. Essentiality analysis: Stratify Task 2 results by image essentiality labels to confirm inverted performance pattern.
  3. Caption ablation: Test E5 and DiVeR with captions from GPT-4o vs. Qwen-3B to quantify semantic gain vs. reasoning degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do current multimodal retrieval models perform worse when given access to images, with the best multimodal model (27.6 nDCG@10) underperforming the best text-only model (32.2)?
- Basis in paper: [explicit] "Adding images hurts retrieval performance instead of helping... The best multimodal model on Task 2 (Query+Image → Documents) is Nomic-Vision with 27.6 nDCG@10, which is lower than the best text-only model on Task 1 (DiVeR: 32.2)."
- Why unresolved: The authors hypothesize current models are trained on simple image-text matching rather than visual reasoning, but this remains untested.
- What evidence would resolve it: Training models on reasoning-intensive multimodal data and evaluating whether the performance gap closes.

### Open Question 2
- Question: Why do multimodal models perform worst when images are most essential for understanding the query?
- Basis in paper: [explicit] "All multimodal models perform worst when images are essential for understanding the query. Performance is highest for queries with helpful images (20.4 nDCG@10 on average), but drops sharply for essential images (15.0)."
- Why unresolved: The paper documents the inverted pattern but does not identify specific failure modes in visual reasoning mechanisms.
- What evidence would resolve it: Fine-grained analysis of model attention and intermediate representations on essential-image queries compared to helpful-image queries.

### Open Question 3
- Question: How can retrieval architectures natively process visual and textual signals without relying on intermediate text generation (e.g., image captions)?
- Basis in paper: [inferred] The appendix concludes: "Caption-based approaches offer a simple way to incorporate visual information into text-only retrievers, they fundamentally cannot replace true multimodal reasoning. Future work should explore retrieval architectures that can natively process both visual and textual signals."
- Why unresolved: All evaluated approaches either use caption-based augmentation or existing vision-language embeddings, neither of which effectively integrate visual reasoning with multi-step technical reasoning.
- What evidence would resolve it: Development and evaluation of architectures jointly trained on reasoning-intensive multimodal retrieval tasks.

### Open Question 4
- Question: What training data or objectives would enable multimodal models to leverage visual information for reasoning-intensive retrieval rather than relying on surface-level image-text associations?
- Basis in paper: [explicit] "We believe this is because current multimodal models are trained mainly on simple image-text matching rather than visual reasoning."
- Why unresolved: The benchmark reveals the failure mode but does not propose or evaluate alternative training paradigms.
- What evidence would resolve it: Comparison of models trained on different objective functions (e.g., reasoning-augmented multimodal contrastive learning) on MM-BRIGHT.

## Limitations
- Benchmark's strong reliance on GPT-4o for hard negative generation introduces critical dependency on model version quality
- "Reasoning-intensive" label depends entirely on expert annotation without standardized rubrics across 29 technical domains
- Performance gaps may reflect architectural constraints rather than reasoning deficiencies

## Confidence
- **High confidence**: Text-only retrieval results (BM25: 8.5, E5: 25.3) are reproducible baseline measurements with clear methodology.
- **Medium confidence**: Multimodal-to-text retrieval conclusions (Nomic-Vision: 27.6 < DiVeR: 32.2) require assumptions about model training objectives that aren't fully verified.
- **Low confidence**: Claims about visual reasoning deficits rely on expert annotations and untested counterfactuals (e.g., reasoning-trained models haven't been evaluated).

## Next Checks
1. **Hard negative robustness test**: Run the benchmark pipeline with GPT-3.5-turbo instead of GPT-4o for negative mining and measure changes in baseline model performance.
2. **Domain generalization experiment**: Evaluate the best-performing models on a held-out technical domain not present in the training corpus to assess whether performance gaps reflect domain adaptation issues rather than reasoning limitations.
3. **Reasoning model ablation**: Fine-tune a CLIP-based model with explicit visual reasoning objectives (e.g., diagram-to-diagram translation) and test whether it outperforms current multimodal baselines on essential image queries.