---
ver: rpa2
title: 'Neural Network-based Vehicular Channel Estimation Performance: Effect of Noise
  in the Training Set'
arxiv_id: '2502.06824'
source_url: https://arxiv.org/abs/2502.06824
tags:
- channel
- data
- estimation
- training
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines the effectiveness of training neural network-based
  vehicular channel estimators using mixed signal-to-noise ratio (SNR) datasets compared
  to high SNR datasets. The research addresses the challenge of accurate channel estimation
  in high-mobility vehicular environments by testing five different architectures:
  CNN-Transformer, TCN-DPA, STA-MLP, TRFI-MLP, and LSTM-DPA-TA.'
---

# Neural Network-based Vehicular Channel Estimation Performance: Effect of Noise in the Training Set

## Quick Facts
- arXiv ID: 2502.06824
- Source URL: https://arxiv.org/abs/2502.06824
- Authors: Simbarashe Aldrin Ngorima; Albert Helberg; Marelie H. Davel
- Reference count: 15
- Primary result: Training on mixed SNR datasets significantly improves generalization across all SNR ranges for most neural network channel estimators, with CNN-Transformer achieving the best overall performance.

## Executive Summary
This study examines how training neural network-based vehicular channel estimators on mixed signal-to-noise ratio (SNR) datasets compares to high SNR datasets. The research tests five different architectures (CNN-Transformer, TCN-DPA, STA-MLP, TRFI-MLP, and LSTM-DPA-TA) on IEEE 802.11p vehicular communications using the VTV-SDWW channel model. Results demonstrate that mixed SNR training provides substantial benefits for most architectures, particularly in low SNR conditions, with CNN-Transformer outperforming all other models including the current state-of-the-art LSTM-DPA-TA. The study concludes that SNR range should be treated as an important hyperparameter when training neural network channel estimators.

## Method Summary
The study evaluates five neural network architectures for IEEE 802.11p OFDM channel estimation in vehicular environments. Mixed SNR training uses 18,000 frames across 0-40 dB in 5 dB increments (2,000 frames per SNR level), while high SNR training uses 18,000 frames at 40 dB. Each frame contains 50 OFDM symbols with 52 subcarriers (48 data + 4 pilot), preprocessed into 52×100 matrices by separating real/imaginary components and interleaving. Models are optimized using Optuna TPE with 50 trials and 3 seeds, employing Adam (or AdamW for CNN-Transformer) with batch size 128 (16 for CNN-Transformer). Performance is evaluated using Bit Error Rate (BER) across 9 SNR levels (0-40 dB) on a test set of 2,000 independent frames.

## Key Results
- CNN-Transformer architecture achieves the best overall performance when trained on mixed SNR data, outperforming all other models including the current state-of-the-art LSTM-DPA-TA
- Mixed SNR training significantly improves generalization across all SNR ranges for CNN-Transformer, TCN-DPA, TRFI-MLP, and STA-MLP architectures
- LSTM-DPA-TA and STA-MLP show reduced performance with mixed SNR training, suggesting architecture-dependent benefits
- The SNR range in training datasets should be treated as an important hyperparameter for neural network channel estimators

## Why This Works (Mechanism)

### Mechanism 1
Training on mixed SNR datasets enables neural networks to generalize across both noise-dominated and signal-dominated regimes by learning a joint distribution of channel effects and noise artifacts. The network is exposed to the full variance of noise (0-40 dB) during optimization, forcing feature extractors to learn robust representations of the channel impulse response that are invariant to noise level rather than overfitting to clean channel statistics. This mechanism fails if network capacity is too low to separate the signal manifold from the increasing noise manifold.

### Mechanism 2
The CNN-Transformer architecture outperforms recurrent baselines in mixed SNR settings because self-attention mechanisms capture global frequency dependencies more effectively than sequential processing. The CNN layers extract local features while the Transformer's self-attention mechanism weighs the importance of all subcarriers globally, allowing the model to dynamically attend to reliable subcarriers and ignore noisy ones. This capability is distinct from the fixed-window or sequential forgetting characteristics of LSTMs/TCNs. Performance degrades if input sequence length is too short for the attention mechanism to infer global context.

### Mechanism 3
Specialized architectures (LSTM-DPA-TA, STA-MLP) show reduced performance with mixed SNR training because their inductive biases rely heavily on the temporal correlations of clean channel statistics. Models like STA-MLP use smoothing parameters that assume certain coherence time; when trained on high SNR, they learn ideal smoothing, but mixed SNR noise variance disrupts temporal correlation assumptions, causing smoothing logic to propagate errors rather than correct them. This mechanism fails if the channel changes very rapidly regardless of SNR, making mixed SNR training a secondary concern to model suitability.

## Foundational Learning

- **Concept: Orthogonal Frequency Division Multiplexing (OFDM) & 802.11p**
  - Why needed here: The entire input structure (52 subcarriers, pilot/data split) and nature of estimation problem are defined by this standard
  - Quick check question: Can you explain why channel estimation in OFDM requires interpolating estimates between "pilot" subcarriers to recover data on the remaining subcarriers?

- **Concept: Least Squares (LS) and Data Pilot-Aided (DPA) Estimation**
  - Why needed here: Deep learning models augment or correct classical DPA estimates; understanding DPA uses demapped data symbols as pseudo-pilots is crucial for interpreting training labels
  - Quick check question: How does DPA estimation differ from standard pilot-based estimation, and what specific error does the TCN-DPA model try to correct?

- **Concept: Signal-to-Noise Ratio (SNR) as a Hyperparameter**
  - Why needed here: The core thesis treats SNR range of training data as a tunable hyperparameter rather than fixed condition
  - Quick check question: Why did prior works assume training at high SNR (40 dB) was optimal, and how does mixed SNR approach challenge the "clean data = better generalization" intuition?

## Architecture Onboarding

- **Component map:** Input (52 subcarriers × 100 interleaved real/imaginary) → Preprocessing (complex → real/imaginary separation → interleaving) → Models (CNN-Transformer: 1D Convs → Transformer Encoder → Regression Head; TCN-DPA: Temporal Conv Net → DPA Loop; LSTM-DPA-TA: LSTM → DPA → Temporal Averaging) → Output (Channel estimate $\hat{H}$ used to equalize signal)

- **Critical path:** The data preprocessing step (Real/Imaginary interleaving) is critical; incorrect reshaping destroys phase information networks rely on. Following this, choice of optimizer (AdamW for Transformer, Adam for others) and mixed-SNR training schedule are primary performance drivers.

- **Design tradeoffs:**
  - Mixed vs. High SNR Training: Mixed SNR trades peak performance at very high SNR for robustness at low SNR
  - Transformer vs. LSTM: Transformer offers superior accuracy and robustness but typically requires more data and careful tuning; LSTM-DPA-TA is computationally expensive and struggles with mixed data but performs best in clean training regimes

- **Failure signatures:**
  - "High SNR Only" Collapse: Models trained only on 40 dB data failing to converge or predicting flat lines when tested at 0-10 dB
  - DPA Error Propagation: If initial TCN/LSTM stage is weak, DPA loop reinforces incorrect symbol decisions, causing BER to spiral
  - Negative Delta: If model performs worse on mixed data than high SNR data (like LSTM-DPA-TA), it indicates the architecture is memorizing noise patterns rather than filtering them

- **First 3 experiments:**
  1. Baseline Reproduction: Train CNN-Transformer on High SNR (40 dB) dataset only, evaluate on full 0-40 dB test set to confirm "generalization gap" in low SNR regions
  2. Mixed SNR Ablation: Train same CNN-Transformer on Mixed SNR dataset, plot BER delta against baseline to verify performance gain specifically in 0-15 dB range
  3. Architecture Sensitivity Check: Train LSTM-DPA-TA model on Mixed SNR dataset, verify "negative delta" phenomenon (performance degradation at high SNR compared to high-SNR-only training)

## Open Questions the Paper Calls Out

- **Open Question 1:** Why do only certain neural network architectures (CNN-Transformer, TCN-DPA, TRFI-MLP) benefit from mixed SNR training while others (LSTM-DPA-TA, STA-MLP) show reduced performance?
  - Basis in paper: The conclusion states "The exact reason why only certain models benefit remains unclear and requires further investigation"
  - Why unresolved: The paper demonstrates the empirical phenomenon but does not investigate architectural properties that determine whether a model benefits from mixed SNR training
  - What evidence would resolve it: An ablation study analyzing how different architectural components respond to noise variation during training, or theoretical analysis linking model capacity and noise robustness

- **Open Question 2:** How does mixed SNR training affect performance on other vehicular channel models beyond the VTV-SDWW scenario tested?
  - Basis in paper: The conclusion states "This will be verified in future research that will investigate the impact of mixed SNR training on other vehicular channel models"
  - Why unresolved: Only one channel model (Vehicle-To-Vehicle Expressway Same Direction with Wall) was tested, limiting generalizability conclusions
  - What evidence would resolve it: Replication of experiments across multiple standardized vehicular channel models with varying mobility and delay spread characteristics

- **Open Question 3:** What is the optimal distribution and granularity of SNR levels within a mixed training dataset?
  - Basis in paper: The paper concludes that "the SNR range in the training dataset should be treated as a hyperparameter" but only tests one specific configuration (0-40 dB in 5 dB increments with uniform sampling)
  - Why unresolved: The study does not explore whether non-uniform SNR sampling, different ranges, or finer/coarser granularity could yield better performance
  - What evidence would resolve it: A systematic study varying SNR distributions (weighted toward low SNR, weighted toward high SNR, different bin sizes) and comparing resulting generalization performance

## Limitations

- The findings depend critically on the specific VTV-SDWW channel model and IEEE 802.11p parameters used, which are not fully specified in the paper
- The mixed SNR dataset construction (18,000 frames across 0-40 dB in 5 dB increments) represents a specific sampling strategy that may not generalize to other SNR distributions
- The comparison with LSTM-DPA-TA relies on externally cited code that was not made available, potentially introducing implementation discrepancies

## Confidence

- **High Confidence:** The observation that CNN-Transformer outperforms other architectures when trained on mixed SNR data (supported by direct experimental results across multiple SNR ranges)
- **Medium Confidence:** The general recommendation that mixed SNR training improves robustness for most architectures (based on averaged results but with architecture-dependent variations)
- **Medium Confidence:** The claim that mixed SNR training is not universally beneficial (supported for LSTM-DPA-TA but limited to single architecture counterexample)

## Next Checks

1. **Architecture Sensitivity Analysis:** Systematically vary the number of SNR levels and their distribution in the mixed training dataset (e.g., uniform vs. Gaussian sampling, 0-30 dB vs. 10-40 dB ranges) to determine optimal SNR coverage for different architectures.

2. **Cross-Channel Generalization:** Validate the mixed SNR benefits on alternative vehicular channel models (e.g., V2V-Street, V2I Highway) and different mobility scenarios (varying Doppler shifts from 100-1000 Hz) to test robustness of the findings beyond the specific VTV-SDWW model.

3. **Capacity-Scaling Study:** Evaluate the mixed vs. high SNR performance tradeoff across multiple model sizes (reduced parameter counts) to determine if the benefits persist for resource-constrained deployment scenarios where network capacity is limited.