---
ver: rpa2
title: Generalization of Medical Large Language Models through Cross-Domain Weak Supervision
arxiv_id: '2502.00832'
source_url: https://arxiv.org/abs/2502.00832
tags:
- medical
- icft
- arxiv
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICFT, an Incremental Curriculum-Based Fine-Tuning
  framework for medical large language models. ICFT addresses the challenge of adapting
  general-purpose LLMs to the medical domain by progressively transitioning from general
  linguistic knowledge to domain-specific expertise.
---

# Generalization of Medical Large Language Models through Cross-Domain Weak Supervision

## Quick Facts
- arXiv ID: 2502.00832
- Source URL: https://arxiv.org/abs/2502.00832
- Authors: Robert Long; Eric Gonzalez; Harrison Fuller
- Reference count: 37
- Primary result: ICFT framework achieves up to 2.58 points improvement in ROUGE-1, 6.61% higher win rate in response generation, and reduces factual errors by 36.9% while training only 0.55% of parameters

## Executive Summary
This paper introduces ICFT (Incremental Curriculum-Based Fine-Tuning), a framework designed to adapt general-purpose LLMs to medical domains while preserving generalization and computational efficiency. ICFT employs curriculum learning to progressively transition from general linguistic knowledge to domain-specific expertise, combined with dual-stage memory coordination and parameter-efficient fine-tuning via LoRA. The framework addresses catastrophic forgetting and optimizes computational efficiency while achieving superior performance across medical NLP tasks including question answering, preference classification, and response generation.

## Method Summary
ICFT combines curriculum learning, dual-stage memory coordination, and LoRA-based parameter-efficient fine-tuning. The model progresses through training data ordered by complexity, maintains separate short-term and long-term memory buffers for context and factual knowledge, and updates only low-rank adapter matrices rather than full model weights. The architecture uses a domain adapter layer, attention-based memory retrieval, and consistency loss to preserve base model capabilities while incorporating medical knowledge.

## Key Results
- ICFT achieves up to 2.58 points improvement in ROUGE-1 score over baselines
- 6.61% higher win rate in medical response generation tasks
- Reduces factual errors by 36.9% while training only 0.55% of total parameters
- Achieves 91.38% STM and 93.12% LTM retrieval accuracy

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-Driven Knowledge Consolidation
Graduated exposure to domain complexity reduces overfitting and improves generalization by allowing models to anchor specialized knowledge onto stable linguistic foundations. This "weak-to-strong" transition simulates how medical expertise is hierarchically acquired.

### Mechanism 2: Dual-Process Memory Retrieval
Separating short-term dialogue context from long-term factual knowledge enhances retrieval precision and reduces factual errors by maintaining distinct buffers for transient interactions and established medical facts.

### Mechanism 3: Parameter Isolation via LoRA
Constraining weight updates to low-rank adapters mitigates catastrophic forgetting by preserving the pre-trained base model's reasoning structure while adding domain-specific adaptations.

## Foundational Learning

- **Concept: Catastrophic Forgetting** - Why needed: Primary failure mode ICFT solves; without it, efficiency gains appear as constraints rather than necessary features. Quick check: Fine-tuning on cardiology data - would general grammar degrade? (Answer: Yes, without parameter isolation).

- **Concept: Curriculum Learning** - Why needed: Central to "Incremental" part; shifts mental model from "more data is better" to "better data sequencing is better." Quick check: Train on complex diagnoses before basic anatomy? (Answer: No, simple → complex).

- **Concept: Retrieval-Augmented Generation vs. Parametric Memory** - Why needed: ICFT uses hybrid dual-stage memory combining explicit retrieval with parametric tuning. Quick check: Is medical fact stored in weights or retrieved from database? (Answer: Both; LTM retrieves explicit facts, LoRA tunes processing).

## Architecture Onboarding

- **Component map:** Input → Domain Adapter → [Query STM & LTM] → Attention Fusion → LoRA-tuned Transformer Blocks → Output

- **Critical path:** Input → Domain Adapter → [Query STM & LTM] → Attention Fusion → LoRA-tuned Transformer Blocks → Output

- **Design tradeoffs:**
  - Memory Capacity (K) vs. Latency: Increasing STM size improves context retention but increases attention complexity
  - LoRA Rank (r) vs. Specificity: Low r saves memory but risks underfitting rare medical jargon
  - Adapter size (r_adp) vs. Speed: Adds computational steps to every forward pass

- **Failure signatures:**
  - High hallucination rate: Likely indicates LTM retrieval failure or threshold misconfiguration
  - Loss of general fluency: Suggests LoRA parameters have drifted too far
  - Poor rare-disease performance: Indicates curriculum stalled at general knowledge

- **First 3 experiments:**
  1. LoRA Rank Ablation: Test r ∈ {4, 8, 16} to find efficiency/accuracy knee point
  2. Curriculum Order Validation: Shuffle training data vs. ICFT structure to verify performance drop
  3. Memory Capacity Stress Test: Vary STM capacity K to find retrieval accuracy degradation point

## Open Questions the Paper Calls Out

- **Open Question 1:** How does ICFT perform when extended to multi-modal medical data integrating textual and imaging information? (Basis: conclusion explicitly states future work will focus on this)

- **Open Question 2:** Can ICFT methodology be effectively transferred to other high-stakes domains outside healthcare while maintaining similar efficiency and error reduction rates? (Basis: conclusion proposes exploring application in other high-stakes domains)

- **Open Question 3:** Does consistency loss constraint hinder the model's ability to correct or override flawed pre-existing knowledge in the base LLM? (Basis: method penalizes deviation from base model outputs, assuming beneficial priors)

## Limitations

- Performance claims rely heavily on comparisons against narrow baseline set without considering more recent medical domain adaptation methods
- Curriculum construction process lacks sufficient detail on how "complexity" and "domain specificity" are quantified
- Dual-memory mechanism lacks ablation studies showing individual contribution of each memory type

## Confidence

- **High Confidence:** Core architecture combining curriculum learning with LoRA parameter isolation is technically sound and well-supported by ablation study
- **Medium Confidence:** Performance improvements over baselines are statistically significant with consistent trends across tasks
- **Low Confidence:** Generalization claims to "unseen data" are asserted but nature of this data is not described

## Next Checks

1. **Curriculum Ordering Validation:** Replicate curriculum learning ablation by training with randomly ordered vs. complexity-ordered medical datasets to verify performance drop replicates

2. **Memory Mechanism Isolation:** Design experiments disabling either STM or LTM individually while keeping other active to quantify marginal contribution of each component

3. **Cross-Domain Generalization Test:** Evaluate ICFT on medical data from completely different source than training to validate true generalization capability beyond in-domain test sets