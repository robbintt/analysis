---
ver: rpa2
title: Adaptive sampling using variational autoencoder and reinforcement learning
arxiv_id: '2512.03525'
source_url: https://arxiv.org/abs/2512.03525
tags:
- uni00000013
- reconstruction
- measurement
- uni0000004c
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an adaptive sparse sensing framework that
  combines a variational autoencoder (VAE) generative prior with reinforcement learning
  (RL) to improve signal reconstruction from limited measurements. Unlike classical
  compressed sensing (CS), which relies on generic bases and random sampling, or optimal
  sensor placement (OSP), which uses fixed linear patterns, the proposed method adaptively
  selects informative measurement locations for each signal using a learned policy.
---

# Adaptive sampling using variational autoencoder and reinforcement learning

## Quick Facts
- arXiv ID: 2512.03525
- Source URL: https://arxiv.org/abs/2512.03525
- Reference count: 13
- Key result: VAE-RL adaptive sampling significantly outperforms both OSP and VAE-based random sampling in MNIST reconstruction quality (MSE/SSIM) by concentrating measurements on digit-specific strokes.

## Executive Summary
This paper introduces an adaptive sparse sensing framework that combines a variational autoencoder (VAE) generative prior with reinforcement learning (RL) to improve signal reconstruction from limited measurements. Unlike classical compressed sensing (CS), which relies on generic bases and random sampling, or optimal sensor placement (OSP), which uses fixed linear patterns, the proposed method adaptively selects informative measurement locations for each signal using a learned policy. The RL agent is trained via proximal policy optimization (PPO) to sequentially choose where to sample next, guided by reconstruction performance measured through a pretrained CNN classifier. Experiments on the MNIST dataset show that the VAE-RL approach significantly outperforms both OSP and VAE-based random sampling in terms of mean squared error (MSE) and structural similarity index (SSIM), producing sharper and more accurate reconstructions by concentrating measurements on digit-specific strokes. The method demonstrates the benefits of tailoring sensing strategies to individual signals using generative models and adaptive decision-making.

## Method Summary
The method uses a two-stage approach: first, a convolutional VAE is trained on MNIST to learn a generative prior (encoder: 3 strided conv layers 1→32→64→128 channels, 4×4 kernels, stride 2, ReLU; latent dim z=64; decoder symmetric with sigmoid output). Second, a PPO-based RL agent is trained to sequentially select pixel locations to measure. The agent observes the current measurement state and outputs an action selecting one of 784 pixel locations. Reconstruction uses latent optimization (minimizing ||C·G(z) - y||²) with 20 ensemble samples from the VAE decoder. The RL reward is +1 for correct classification of the reconstructed image by a pretrained CNN classifier, -1 otherwise. The algorithm uses an initial probe stage with m_p random measurements, followed by m_a adaptive steps for a total of 60 measurements.

## Key Results
- VAE-RL adaptive sampling achieves significantly lower MSE and higher SSIM than both OSP and VAE-based random sampling on MNIST reconstruction
- The method produces sharper reconstructions that better preserve digit-specific strokes and details
- RL agent successfully learns to concentrate measurements on informative regions rather than uniformly sampling

## Why This Works (Mechanism)
The approach combines the expressive generative power of VAEs with the adaptive decision-making capabilities of RL. The VAE provides a structured prior that enables high-quality reconstruction from limited measurements, while the RL agent learns to select measurements that maximally improve reconstruction quality for each specific signal. By using a pretrained CNN classifier as the reward signal, the method directly optimizes for reconstruction quality in a way that aligns with downstream classification tasks. The adaptive nature allows the system to tailor the sensing strategy to each individual signal's characteristics, focusing on regions that are most informative for reconstruction.

## Foundational Learning
- **Variational Autoencoders**: Generative models that learn compressed representations of data; needed for providing a structured prior that enables reconstruction from limited measurements. Quick check: Verify VAE can reconstruct MNIST digits with reasonable quality from the latent space.
- **Proximal Policy Optimization**: RL algorithm that optimizes policies through policy gradient methods with clipping to ensure stable learning; needed for training the adaptive measurement selection policy. Quick check: Monitor policy entropy and reward during training to ensure learning is progressing.
- **Compressed Sensing**: Framework for reconstructing signals from limited linear measurements; provides the theoretical foundation for the sensing problem being addressed. Quick check: Verify that random sensing matrices can reconstruct MNIST digits with acceptable quality as a baseline.
- **Latent Optimization**: Iterative optimization procedure to find the latent code that best reconstructs given measurements; needed for performing reconstruction at each RL step. Quick check: Measure reconstruction time and quality for different numbers of optimization iterations.
- **Reinforcement Learning with Function Approximation**: Using neural networks to approximate value functions and policies in RL; needed for handling the high-dimensional action space (784 pixel locations). Quick check: Verify that the policy network can accurately predict action probabilities for different measurement states.

## Architecture Onboarding

Component map: VAE (encoder → latent space → decoder) -> RL agent (CNN feature extractor → policy head + value head) -> Measurement selection -> Latent optimization -> Reconstruction -> CNN classifier -> Reward

Critical path: State observation (C_t, y_t) -> CNN feature extraction -> Policy network -> Action selection -> Measurement acquisition -> Latent optimization -> Reconstruction -> CNN classification -> Reward computation

Design tradeoffs: The method trades computational complexity (expensive latent optimization at each step) for adaptive measurement selection. Using a pretrained CNN for rewards provides task-relevant feedback but may introduce bias. The ensemble decoding approach improves reconstruction quality but increases computational cost.

Failure signatures: Poor VAE reconstruction quality leads to suboptimal RL learning. RL agent collapsing to single action indicates insufficient exploration or reward signal issues. Slow RL convergence suggests latent optimization is too computationally expensive or reward signal is too sparse.

First experiments:
1. Train VAE on MNIST and validate reconstruction quality on held-out validation set
2. Implement and test latent optimization procedure for reconstruction from random measurements
3. Train CNN classifier on MNIST and verify it can correctly classify VAE reconstructions from probe measurements

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of latent optimization at each RL step makes real-time deployment challenging
- Performance depends heavily on the quality of the pretrained VAE and CNN classifier
- Method requires training separate models for each sensing task and signal domain

## Confidence
High confidence in the general framework and architectural specifications for both VAE and policy network, as these are explicitly detailed. Medium confidence in experimental results since the MNIST-specific parameters and optimization procedures contain gaps that could affect quantitative outcomes. Low confidence in exact reproduction of the RL training dynamics due to missing hyperparameters and optimization details.

## Next Checks
1. Implement and validate the VAE reconstruction quality on the MNIST validation set, ensuring MSE/SSIM match expected ranges for the specified architecture
2. Verify the RL reward computation pipeline by testing the pretrained CNN classifier accuracy on reconstructed images from the probe stage
3. Run a small-scale PPO training experiment (10-20 episodes) to confirm that the policy network can learn to select meaningful pixel locations and that reward signals are properly propagated through the latent optimization reconstruction step