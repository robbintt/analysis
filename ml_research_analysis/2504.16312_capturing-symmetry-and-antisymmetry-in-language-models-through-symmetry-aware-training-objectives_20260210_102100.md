---
ver: rpa2
title: Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware
  Training Objectives
arxiv_id: '2504.16312'
source_url: https://arxiv.org/abs/2504.16312
tags:
- language
- label
- relations
- retraining
- k-nn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of improving large language
  models' understanding of symmetric and antisymmetric relations, which is critical
  for tasks like relation extraction and natural language inference. The authors introduce
  a novel Wikidata-derived dataset for evaluating models on symmetric and antisymmetric
  relations and demonstrate that standard LLMs perform only at chance level on this
  benchmark.
---

# Capturing Symmetry and Antisymmetric Relations in Language Models through Symmetry-Aware Training Objectives

## Quick Facts
- **arXiv ID:** 2504.16312
- **Source URL:** https://arxiv.org/abs/2504.16312
- **Reference count:** 6
- **Primary result:** Proposed symmetry-aware training achieves 100% accuracy on symmetric/antisymmetric relation classification while mitigating catastrophic forgetting

## Executive Summary
This paper addresses the challenge of improving large language models' understanding of symmetric and antisymmetric relations, which is critical for tasks like relation extraction and natural language inference. The authors introduce a novel Wikidata-derived dataset for evaluating models on symmetric and antisymmetric relations and demonstrate that standard LLMs perform only at chance level on this benchmark. To address this gap, they propose retraining encoder models using contrastive learning with k-nearest neighbors (k-NN), incorporating symmetry-aware objectives. This approach achieves 100% accuracy on both lexicalized and delexicalized versions of the dataset, matching fine-tuned classification heads while requiring fewer training samples. Additionally, it shows better mitigation of catastrophic forgetting (up to 5.4% improvement) and greater efficiency in few-shot learning scenarios.

## Method Summary
The method retrains encoder models (RoBERTa-Large) using contrastive learning with a symmetry-aware distance metric derived from RotatE knowledge graph embeddings. The distance function d_l(p, h) = 1 - cos_sim(h, p ∘ l) breaks symmetry to distinguish directional relations. Training uses contrastive loss to minimize distances for same-label pairs and maximize with margin for different-label pairs. The approach employs k-NN inference over retrained embeddings, enabling label-flexible deployment without architectural changes. Three variants are evaluated: Random Label Embeddings (48 samples), k-NN with learned distance metric (64 samples), and k-NN with fixed symmetry-aware metric (400 samples).

## Key Results
- Achieved 100% accuracy on both lexicalized and delexicalized versions of the Wikidata-derived dataset
- Required fewer training samples compared to fine-tuning classification heads (48-400 vs typical fine-tuning)
- Improved catastrophic forgetting mitigation (5.8-7.7% performance drop vs 11.2% for fine-tuning)
- Demonstrated greater efficiency in few-shot learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetry-aware distance metrics enable encoders to distinguish directional relations that standard similarity measures conflate.
- Mechanism: The paper adapts the RotatE knowledge graph embedding approach to create a distance function d_l(p, h) = 1 - cos_sim(h, p ∘ l) where p is premise, h is hypothesis, and l is a label embedding. Unlike cosine similarity which is inherently symmetric (d(p,h) = d(h,p)), this composition operation breaks symmetry, allowing the model to capture that "A borders B" entails "B borders A" (symmetric) while "A is parent of B" contradicts "B is parent of A" (antisymmetric).
- Core assumption: The label embedding space can be structured such that composing premise with label and comparing to hypothesis reveals relational directionality.
- Evidence anchors:
  - [abstract] "retraining encoder models using contrastive learning with k-nearest neighbors (k-NN), incorporating symmetry-aware objectives"
  - [section 2.2] "Standard distance functions such as dot products and cosine similarity are not symmetry-aware (i.e., d(p, h) = d(h, p)). To address this, we propose a symmetry-aware distance metric function derived from the RotatE model"
  - [corpus] Weak direct relevance—corpus papers address geometric/physical symmetry (3D rotations, robotic manipulation) rather than linguistic relational symmetry; conceptual connection exists but empirical transfer is untested.
- Break condition: If relations require multi-hop reasoning beyond single-triple directionality, or if syntactic variation obscures the subject-object swap pattern, this mechanism may fail.

### Mechanism 2
- Claim: Encoder-only retraining with contrastive objectives preserves prior knowledge better than classification-head fine-tuning.
- Mechanism: Rather than adding task-specific parameters (classification head) that must learn from scratch and can overwrite encoder weights during backpropagation, the method keeps architecture fixed. The contrastive loss (Equation 5: minimize d+(l1,l2)² + max(0, margin - d-(l1,l2))²) shapes the encoder's embedding space directly. Fewer parameters change, and changes are constrained to geometric rearrangement of existing representations rather than new circuit formation.
- Core assumption: The pretrained encoder already contains sufficient relational knowledge; it merely needs spatial reorganization rather than new feature learning.
- Evidence anchors:
  - [abstract] "retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including... improved mitigation of catastrophic forgetting"
  - [section 3.2, Table 2] Catastrophic forgetting measured as MNLI performance drop: Random Label Embeddings ↓5.8%, k-NN ↓7.7%, Fine-Tuning ↓11.2%
  - [corpus] No corpus papers address catastrophic forgetting mitigation directly.
- Break condition: If the target task requires genuinely new relational patterns absent from pretraining data, encoder-only retraining may underfit compared to methods that add capacity.

### Mechanism 3
- Claim: k-NN inference over retrained embeddings enables label-flexible deployment without architectural changes.
- Mechanism: Training stores labeled embeddings; inference computes distance from test sample to all training samples and votes among k nearest. Adding new relation types requires only storing their examples—no model modification. This decouples the label space from model architecture (compare: fine-tuning requires reinitializing classification head for each label set).
- Core assumption: The embedding space is locally consistent—similar relations cluster such that nearest neighbors reliably share labels.
- Evidence anchors:
  - [abstract] "greater efficiency in few-shot learning"
  - [section 2.3] "One advantage of the k-NN approach is its flexibility in handling a varying number of labels... can adapt to new labels without extensive retraining"
  - [corpus] No corpus papers address k-NN for NLP tasks; mechanism appears novel to this paper's application domain.
- Break condition: If training samples are sparse or distributionally shifted from test, k-NN becomes unreliable; embedding quality dominates performance.

## Foundational Learning

- Concept: Symmetric vs. Antisymmetric Relations
  - Why needed here: The entire method hinges on distinguishing these—symmetric means r(x,y) ⟹ r(y,x) (bidirectional entailment), antisymmetric means r(x,y) ⟹ ¬r(y,x) (unidirectional). Without this distinction, the objective function and distance metric design make no sense.
  - Quick check question: Given "A caused B," what should the model infer about "B caused A"—entailment, contradiction, or neutral?

- Concept: Contrastive Learning Objectives
  - Why needed here: The training loss pulls same-label embeddings closer and pushes different-label embeddings apart with a margin. Understanding this clarifies why the embedding space geometry changes without new parameters.
  - Quick check question: If margin=0.5 and two different-label embeddings have distance 0.3, what term appears in the loss?

- Concept: Catastrophic Forgetting in Continual Learning
  - Why needed here: The paper positions encoder retraining as a solution; evaluating this claim requires understanding why standard fine-tuning causes performance drops on prior tasks (gradient updates overwrite previously learned weights).
  - Quick check question: If a model fine-tuned on task B drops from 85% to 74% on task A, what is the forgetting magnitude?

## Architecture Onboarding

- Component map:
Input: (premise, hypothesis) pair
  ↓
Encoder (RoBERTa-Large or similar)
  ↓
Embeddings: [p_emb, h_emb] (fixed dimension, e.g., 1024)
  ↓
Label Embedding Computation:
  - Random Label: Predefined l+, l- vectors
  - k-NN: l = h ⊘ p (elementwise division, from RotatE)
  ↓
Distance Computation: d_l(p, h) = 1 - cos_sim(h, p ∘ l)
  ↓
Training: Contrastive loss (Eq. 3 or 5)
  ↓
Inference: 
  - Random Label: argmin distance to l+ or l-
  - k-NN: majority vote among k nearest training samples

- Critical path:
  1. Premise-hypothesis tokenization (must preserve subject/object positions for swap detection)
  2. Encoder forward pass (no gradient during inference; gradients during training update encoder weights only)
  3. Label embedding construction (depends on method: random static vs. computed from pair)
  4. Distance computation using symmetry-aware metric (not standard cosine)
  5. Loss computation and backprop (encoder-only, no head)

- Design tradeoffs:
  - Random Label Embeddings vs. k-NN: Random labels require only 48 training samples but fix label semantics upfront; k-NN needs 64 samples but allows dynamic label extension. k-NN with learned distance metric needs 400 samples—highest flexibility, lowest efficiency.
  - Encoder-only vs. Fine-tuning: Encoder-only reduces catastrophic forgetting (5.8-7.7% drop vs. 11.2%) but may underfit on genuinely novel patterns. Fine-tuning adds capacity at knowledge retention cost.
  - Fixed vs. Learned Distance Metric: Fixed (Eq. 4) encodes domain knowledge (symmetry properties) explicitly; learned discovers optimal metric but needs more data (400 vs. 64 samples).

- Failure signatures:
  - Random baseline accuracy (~50%): Encoder not learning directional distinction; check if label embeddings are being used correctly in distance computation.
  - High catastrophic forgetting (>15%): Overtraining or learning rate too high; encoder weights changing too aggressively.
  - k-NN accuracy drops with new relations: Embedding space not generalizing; may need more diverse training relations or reconsider fixed metric assumptions.
  - Delexicalized much worse than lexicalized: Model overfitting to surface forms rather than learning abstract relational patterns.

- First 3 experiments:
  1. **Baseline probe**: Load pretrained RoBERTa-Large, evaluate on the Wikidata test set using 1-NN classifier (no retraining). Expect ~50% accuracy. This confirms the problem exists before any intervention.
  2. **Random Label Embeddings retraining**: Initialize two random label vectors (l_entail, l_contradict), train encoder with loss from Eq. 3 on 48 samples, evaluate on held-out test. Expect 100% if implementation correct. Measure MNLI drop to quantify forgetting.
  3. **k-NN comparison**: Train with contrastive loss (Eq. 5) on 64 sample pairs, store all label embeddings, evaluate with k=3 voting. Compare sample efficiency and forgetting against Random Label Embeddings. Check both lexicalized and delexicalized test sets to verify relational (not lexical) learning.

## Open Questions the Paper Calls Out

- Question: Does symmetry-aware encoder retraining transfer to improved performance on downstream tasks such as relation extraction, fact-checking, and natural language inference beyond the synthetic evaluation setting?
- Basis in paper: [explicit] The authors state their dataset's focus "may also overlook nuances present in other domains, potentially affecting performance in tasks like information extraction, question-answering, and natural language inference."
- Why unresolved: The evaluation is limited to a synthetic Wikidata-derived NLI task; no downstream task experiments were conducted.
- What evidence would resolve it: Experiments applying symmetry-aware encoders to established benchmarks (e.g., relation extraction datasets like TACRED, fact-checking datasets like FEVER) showing performance gains over standard encoders.

- Question: How robust is symmetry-aware retraining when evaluated on syntactically diverse natural language expressions of symmetric and antisymmetric relations?
- Basis in paper: [explicit] The authors acknowledge: "these datasets are factually accurate, they lack syntactic diversity, limiting the models' exposure to varied linguistic structures."
- Why unresolved: The Wikidata-derived dataset uses manually designed templates (Table 3), producing structurally uniform sentences; the 100% accuracy may reflect template exploitation rather than genuine relational understanding.
- What evidence would resolve it: Evaluation on a dataset with varied syntactic constructions expressing the same relations (passive voice, relative clauses, coreference, etc.), with analysis of performance degradation relative to template-based test sets.

- Question: Can symmetry-aware training objectives be effectively adapted for decoder-only large language models?
- Basis in paper: [inferred] The experiments are limited to encoder models (RoBERTa-Large, MiniLM variants); the authors do not test on decoder-only architectures despite these being dominant in current LLM research.
- Why unresolved: The contrastive learning approach with k-NN inference assumes sentence-level embeddings from encoders; decoder-only models process text differently and may require architectural modifications.
- What evidence would resolve it: Implementation of symmetry-aware objectives for decoder-only models (e.g., LLaMA, GPT) with comparative evaluation against encoder-based approaches on the same benchmark.

## Limitations
- Evaluation confined to synthetic Wikidata-derived dataset, limiting generalizability to naturally occurring asymmetric relations
- Limited comparison to fine-tuning without ablation showing whether encoder-only approach learns novel patterns versus reorganizing existing ones
- Catastrophic forgetting measurements limited to MNLI performance drops without examining selective forgetting of specific relation types

## Confidence
- Symmetry-aware distance metric effectiveness: **Medium** - Strong empirical results but mechanism relies on specific geometric assumptions
- Catastrophic forgetting mitigation: **Medium** - Clear improvements shown but limited to single downstream task
- Sample efficiency claims: **High** - Direct measurements support fewer training samples needed

## Next Checks
1. Evaluate on naturally occurring asymmetric relations (e.g., "X caused Y", "X before Y") from existing NLI datasets to test generalization beyond Wikidata predicates
2. Conduct ablation study comparing encoder retraining with and without the symmetry-aware distance metric to isolate its contribution
3. Measure forgetting on a subset of relations (e.g., symmetric relations) separately from overall MNLI performance to identify selective forgetting patterns