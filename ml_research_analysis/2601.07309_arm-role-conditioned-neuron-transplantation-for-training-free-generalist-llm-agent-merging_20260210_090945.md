---
ver: rpa2
title: 'ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist
  LLM Agent Merging'
arxiv_id: '2601.07309'
source_url: https://arxiv.org/abs/2601.07309
tags:
- backbone
- merging
- action
- neurons
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training-free model merging
  for interactive LLM agents across diverse environments. It proposes Agent-Role Merging
  (ARM), a three-step framework that first constructs merged backbones via weight-space
  merging, then selects the best backbone using role-conditioned activation overlap
  scores, and finally performs conflict-aware neuron transplantation to repair capability
  gaps.
---

# ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging

## Quick Facts
- arXiv ID: 2601.07309
- Source URL: https://arxiv.org/abs/2601.07309
- Authors: Zhuoka Feng; Kang Chen; Sihan Zhao; Kai Xiong; Yaoning Wang; Minshen Yu; Junjie Nian; Changyi Xiao; Yixin Cao; Yugang Jiang
- Reference count: 30
- Key outcome: ARM achieves highest average performance and worst-suite robustness on six benchmarks compared to training-free baselines while maintaining strong out-of-domain generalization.

## Executive Summary
This paper introduces ARM, a training-free framework for merging specialized LLM agents into a single generalist model. ARM operates in three stages: (1) construct merged backbones via weight-space merging, (2) select the best backbone using role-conditioned activation overlap scores, and (3) repair capability gaps through conflict-aware neuron transplantation. The method uses role-conditioned activation tracing to identify critical neurons for benchmark-specific behaviors while protecting neurons important for other benchmarks, preventing negative transfer. Evaluated on Qwen3-8B and Qwen2.5-7B expert pools across six benchmarks, ARM demonstrates superior average performance and robustness compared to existing training-free baselines.

## Method Summary
ARM merges multiple specialized LLM agents into a generalist model without gradient-based training. The framework constructs candidate backbones using various merge operators (averaging, task arithmetic, TIES, etc.), then selects the optimal backbone via Activation-Overlap Score (AOS) computed on a calibration set. AOS measures Jaccard similarity between role-salient neurons in the backbone and expert models. For weak benchmarks, ARM performs neuron transplantation by transferring top-k% donor neurons while excluding those in protection sets (neurons salient for other benchmarks). The process requires only forward-pass activation tracing and targeted neuron-level edits, with no retraining involved.

## Key Results
- ARM achieves the highest average accuracy across six benchmarks compared to training-free baselines
- ARM demonstrates superior worst-suite (WS) robustness, minimizing the minimum performance across all benchmarks
- The method maintains strong out-of-domain generalization capabilities
- Role-conditioned activation tracing reduces cross-benchmark neuron overlap from 61% to 41% on Qwen3-8B

## Why This Works (Mechanism)

### Mechanism 1
Role-conditioned activation tracing identifies benchmark-critical neurons more precisely than full-response tracing by restricting activation analysis to specific token spans (tool-call formatting, action serialization, final-answer JSON). This reduces cross-benchmark neuron overlap, producing more specialized neuron sets for transplantation. The core assumption is that benchmark-critical behaviors localize to specific token positions. Evidence shows role-conditioned tracing yields substantially lower cross-benchmark overlap (61% → 41%) and correlates with downstream performance. Break condition: If role spans cannot be deterministically identified, the mechanism degrades to full-response tracing.

### Mechanism 2
Activation-Overlap Score (AOS) serves as a lightweight proxy for merged backbone quality by measuring Jaccard similarity between role-salient neurons in the backbone and expert models. Higher overlap indicates better preservation of expert circuits, correlating with downstream performance. The core assumption is that preserving expert role-salient neurons predicts cross-benchmark generalization. Evidence shows Pearson correlation of 0.840 and Spearman correlation of 0.986 between AOS and average accuracy. Break condition: If calibration data is unrepresentative of evaluation distributions, AOS may select a suboptimal backbone.

### Mechanism 3
Conflict-aware transplantation reduces negative transfer by protecting neurons salient for other benchmarks. Before transplanting donor neurons for benchmark b, ARM computes a protection set containing neurons salient for all other benchmarks, then only transplants non-conflicting donor neurons. The core assumption is that overwriting protection set neurons causes regression on other benchmarks. Evidence shows the unprotected variant is more sensitive to top-k selection, with performance decreasing more rapidly as k grows. Break condition: If salient neuron sets are incomplete or noisy, protection may be insufficient or overly restrictive.

## Foundational Learning

- **Model merging (weight-space composition)**: Why needed here: ARM builds on existing merge operators to construct candidate backbones before selection and refinement. Quick check question: Can you explain why simple weight averaging can cause interference between fine-tuned experts?
- **Activation-based interpretability**: Why needed here: ARM relies on tracing MLP activations to identify role-salient neurons; understanding activation patterns is essential for diagnosing capability localization. Quick check question: What does a high mean activation at a specific neuron position suggest about that neuron's functional role?
- **Multi-turn agent trajectories**: Why needed here: ARM targets interactive agents where small errors on role-critical spans cascade into long-horizon failures. Quick check question: Why might a merged model succeed on single-turn NLP tasks but fail on multi-turn agent benchmarks?

## Architecture Onboarding

- **Component map**: Backbone Pool Construction (merge operators → candidate backbones) -> Backbone Selection (compute AOS → select highest mean AOS backbone) -> Neuron Transplantation (identify weak benchmarks → compute protection sets → transplant non-conflicting donor neurons)
- **Critical path**: Calibration data preparation → role span annotation → activation tracing → AOS computation → backbone selection → protection set computation → neuron transplantation. Errors in role span annotation propagate through the entire pipeline.
- **Design tradeoffs**: Top-k fraction affects conflict risk (higher k includes more neurons but increases conflict risk); calibration set size impacts saliency estimates (larger sets improve estimates but increase overhead); protection strictness uses hard exclusion (softer weighting could allow partial overlap).
- **Failure signatures**: High AOS but low performance (calibration data unrepresentative; role spans mislabeled); regression on protected benchmarks (protection set incomplete; shared neurons not captured); no improvement after transplantation (donor neurons already present; capability gap not neuron-localized); invalid action spikes (transplantation disturbed formatting circuits; reduce k or expand protection).
- **First 3 experiments**: 1) Sanity check: Run ARM on a single expert (merge with itself); verify AOS ≈ 1.0 and no transplantation occurs. 2) Ablation: role-conditioning: Compare role-conditioned vs. full-response tracing; measure cross-benchmark overlap reduction and downstream performance. 3) Sensitivity sweep: Vary top-k from 10% to 50% with and without conflict-aware protection; plot performance curves to validate robustness claims.

## Open Questions the Paper Calls Out

### Open Question 1
Can ARM be extended to merge heterogeneous expert models that possess different architectures or tokenizers? The authors state ARM requires "homologous expert checkpoints" and "does not directly apply to merging heterogeneous model families." The current neuron transplantation method relies on direct parameter correspondence, which is impossible across different architectures. A modified framework utilizing alignment techniques (e.g., git re-basing or neuron matching algorithms) evaluated on merging distinct model families could address this limitation.

### Open Question 2
Do more complex activation diagnostics improve the identification of role-salient neurons over the current mean-activation proxy? The paper notes "diagnostics tailored to multi-turn interactive agent behaviors remain relatively under-explored," suggesting the current tracing may be simplistic. The current method uses "expected per-role mean activation," which may not fully capture the complex causal circuits required for multi-step reasoning. Comparative ablations using causal tracing or attribution methods versus the proposed role-conditioned saliency could measure precision in neuron selection.

### Open Question 3
How robust is the Activation-Overlap Score (AOS) to reductions in calibration set size or shifts in data distribution? The method relies on a specific calibration set of 699 tasks to determine saliency, but the sensitivity of the AOS to this specific sample is not analyzed. Practical deployment may limit access to high-quality, representative calibration data; the minimum data requirements for reliable backbone selection remain unknown. Ablation studies varying the volume and domain composition of the calibration set could observe variance in selection accuracy and final merged performance.

## Limitations
- ARM requires homologous expert checkpoints and does not directly apply to merging heterogeneous model families with different architectures or tokenizers.
- The method depends heavily on the quality of role span annotations and assumes benchmark-critical behaviors localize to specific token positions.
- The protection set mechanism assumes salient neurons for other benchmarks are necessary and sufficient for those capabilities, which may not hold if salient neuron sets are incomplete.

## Confidence
- **High**: ARM's effectiveness in improving average and worst-suite performance over training-free baselines on Qwen3-8B and Qwen2.5-7B pools; correlation between AOS and downstream accuracy (Pearson: 0.840, Spearman: 0.986); conflict-aware transplantation reducing sensitivity to top-k selection.
- **Medium**: Claims about cross-benchmark neuron overlap reduction (61% → 41%) and the extent to which role-conditioning is necessary for ARM's success; the generality of protection set sufficiency across diverse agent tasks.
- **Low**: Generalizability to arbitrary open-domain tasks without structured outputs; robustness when calibration data is small or unrepresentative; sensitivity to span extraction errors in noisy or ambiguous agent interactions.

## Next Checks
1. **Role-span sensitivity**: Systematically perturb role span boundaries (e.g., ±1-2 tokens) for τ-bench, OfficeBench, and AgentBench, and measure the impact on cross-benchmark neuron overlap and downstream ARM performance.
2. **Calibration data size ablation**: Train ARM with varying calibration set sizes (e.g., 10%, 25%, 50%, 100%) and evaluate AOS correlation stability and final benchmark performance.
3. **Out-of-distribution robustness**: Evaluate ARM-merged models on benchmarks outside the expert pool (e.g., unseen tool types, new environments) to test claims of strong out-of-domain generalization.