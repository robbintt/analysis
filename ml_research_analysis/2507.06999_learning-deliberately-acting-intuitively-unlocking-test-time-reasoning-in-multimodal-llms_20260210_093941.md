---
ver: rpa2
title: 'Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in
  Multimodal LLMs'
arxiv_id: '2507.06999'
source_url: https://arxiv.org/abs/2507.06999
tags:
- reasoning
- answer
- arxiv
- training
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal reasoning
  in large language models (LLMs), specifically focusing on tasks that require both
  visual and textual understanding, such as mathematical problem-solving. Existing
  approaches often rely on extensive data annotation or complex reward systems, increasing
  training costs and limiting scalability.
---

# Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs

## Quick Facts
- arXiv ID: 2507.06999
- Source URL: https://arxiv.org/abs/2507.06999
- Reference count: 40
- Primary result: D2I framework improves multimodal reasoning accuracy by 7.5-14.0% over strong baselines

## Executive Summary
This paper addresses the challenge of improving multimodal reasoning in large language models for tasks requiring both visual and textual understanding, such as mathematical problem-solving. The authors propose the Deliberate-to-Intuitive (D2I) reasoning framework, which uses structured training with deliberate reasoning strategies (LOC, JUS, PAR) and rule-based rewards, then shifts to intuitive inference without format constraints. Experiments on Qwen2.5-vl-7B demonstrate consistent improvements across diverse benchmarks, with 60.6% accuracy on GEOQA-8K representing a 14.0% improvement over the base model.

## Method Summary
The D2I framework trains multimodal LLMs using GRPO with three deliberate reasoning strategies: LOC (bounding box localization), JUS (visual justification), and PAR (image parsing). During training, the model generates structured outputs with specific tags (<box>, <crucial>, <parse>) that are validated by rule-based format rewards. GRPO optimizes for both format compliance and answer correctness. At inference, the model switches to a simple prompt asking for step-by-step reasoning without any format constraints, allowing flexible, implicit reasoning. The approach is trained for 150 steps on GEOQA-8K with batch size 128 and evaluated across multiple benchmarks.

## Key Results
- D2I achieves 60.6% accuracy on GEOQA-8K, a 14.0% improvement over the base model
- Outperforms GRPO baseline by 7.5% on the same dataset
- Demonstrates consistent effectiveness across diverse benchmarks including MathVerse, MathVista, and MATH-Vision
- Shows complementary strengths of different strategies (LOC, JUS, PAR) across various task types

## Why This Works (Mechanism)
The framework works by first teaching the model structured reasoning patterns during training through explicit format constraints, then allowing it to apply these learned reasoning capabilities more flexibly at inference. The deliberate training phase forces the model to ground its reasoning in specific visual evidence (coordinates, regions, structures), building robust multimodal alignment. The intuitive inference phase removes these constraints, preventing the model from being locked into potentially incorrect intermediate outputs. This two-phase approach combines the benefits of supervised structure during learning with the flexibility needed for effective test-time reasoning.

## Foundational Learning

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR) / GRPO**
  - Why needed here: D2I builds on GRPO to train without human annotations using rule-based rewards for format and answer correctness
  - Quick check question: How does GRPO differ from standard supervised fine-tuning, and what role do the "groupwise advantage" and rule-based rewards play?

- **Concept: Multimodal Alignment and Grounding**
  - Why needed here: The deliberate strategies force better grounding by linking textual reasoning to specific visual evidence through coordinates and regions
  - Quick check question: What does "grounding" mean in a multimodal LLM context, and why is it a prerequisite for reliable reasoning?

- **Concept: Chain-of-Thought (CoT) and Test-Time Scaling**
  - Why needed here: D2I fits within the "slow-thinking" paradigm where reasoning is a multi-step process that can be scaled at test time
  - Quick check question: How does generating a chain of thought before a final answer improve a model's performance on complex tasks like math?

## Architecture Onboarding

- **Component map:**
  - Base Model: Qwen2.5-VL-7B-Instruct
  - Input: Question + Image + Strategy-Specific Prompt (LOC/JUS/PAR)
  - GRPO Rewards: Format Reward (binary, checks tag structure) + Answer Reward (binary, checks ground truth)
  - Output: Structured response with intermediate visual outputs
  - Inference: Simple prompt ("step-by-step reasoning...")

- **Critical path:**
  1. Select one of three strategies (LOC, JUS, PAR) or combine them
  2. Define rule-based format reward checking for specific tags and structure
  3. GRPO training loop: generate responses, score with rewards, update model
  4. Shift to intuitive prompt at inference, removing format constraints

- **Design tradeoffs:**
  - D2I vs D2D: D2I uses complex training but flexible inference, avoiding bad intermediate outputs at test time; D2D is more interpretable but brittle
  - Strategy Choice: LOC for spatial tasks, JUS for semantic alignment, PAR for structural relationships
  - D2I generally outperforms D2D for complex reasoning tasks by avoiding format overfitting

- **Failure signatures:**
  - Low-quality grounding: model produces correct format without correct content
  - Format overfitting: good D2D performance but poor D2I performance
  - Reward hacking: satisfying format reward without contributing to reasoning

- **First 3 experiments:**
  1. Ablation on strategy: Train separate models with only LOC, only JUS, only PAR; evaluate on diverse benchmarks to confirm complementary strengths
  2. D2I vs D2D comparison: Train with JUS strategy; evaluate same checkpoint with JUS prompt (D2D) vs intuitive prompt (D2I) on MATH-Vision
  3. Cross-domain generalization: Train on Doc-Mix dataset; evaluate on math benchmarks to test general reasoning improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks detailed GRPO hyperparameters (KL coefficient, clip parameter, sample count)
- Format reward implementation is underspecified (binary vs partial credit, weighting)
- Limited ablation studies on whether deliberate-to-intuitive transition is truly necessary
- Doesn't explore simpler alternatives that might achieve similar results

## Confidence
- **High confidence**: Experimental results showing D2I outperforming both base model and GRPO baseline on multiple benchmarks
- **Medium confidence**: Claim that deliberate-to-intuitive is superior to deliberate-to-deliberate for complex reasoning tasks
- **Low confidence**: Assertion that no single strategy is universally best without more comprehensive cross-domain validation

## Next Checks
1. Strategy ablation on diverse tasks: Train three separate models using only LOC, only JUS, only PAR on same dataset; evaluate all three on MME, SEED-Bench, MMMU to confirm complementary strengths
2. D2I vs D2D head-to-head: Train with JUS strategy; evaluate identical checkpoints using strategy-specific prompt (D2D) and intuitive prompt (D2I) on MATH-Vision
3. Format reward sensitivity analysis: Train identical models with varying format reward weights (0, 0.5, 1.0, 2.0); measure impact on training compliance and inference performance