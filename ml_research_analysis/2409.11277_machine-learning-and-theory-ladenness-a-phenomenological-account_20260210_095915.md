---
ver: rpa2
title: Machine Learning and Theory Ladenness -- A Phenomenological Account
arxiv_id: '2409.11277'
source_url: https://arxiv.org/abs/2409.11277
tags:
- theory
- data
- domain
- which
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that machine learning model-building practices
  in science are largely theory-indifferent, meaning they do not require explicit
  reference to domain-theory (scientific expertise of the field where ML is applied).
  The authors analyze ML models through comparison with phenomenological models, showing
  that MLMs are constructed primarily through data-fitting procedures and optimization
  tasks that are independent of theoretical considerations.
---

# Machine Learning and Theory Ladenness -- A Phenomenological Account

## Quick Facts
- arXiv ID: 2409.11277
- Source URL: https://arxiv.org/abs/2409.11277
- Reference count: 15
- Primary result: ML modeling practices are theory-indifferent, enabling easy domain transfer but raising normative questions about whether they should incorporate domain-theory.

## Executive Summary
This paper challenges the prevailing view in philosophy of science that scientific activities must be theory-laden by analyzing machine learning model-building practices. Through comparison with phenomenological models, the authors demonstrate that ML models are constructed primarily through data-fitting procedures and optimization tasks that are independent of explicit theoretical considerations. They introduce the concept of "theory-infection" to describe how domain-theory can be passively inherited from datasets without being necessary for model construction. The analysis reveals that ML modeling practices fundamentally differ from traditional modeling approaches, with important implications for scientific methodology and the philosophy of science.

## Method Summary
The paper employs a philosophical analysis comparing machine learning models to phenomenological models, examining the role of domain-theory in different stages of ML model construction. The authors analyze three key components: training samples (data), training engines (algorithms), and learned models (fitting curves/parameters). They trace the critical path from data curation through feature engineering, hyper-parameter specification, and weight-parameter optimization, arguing that each stage is either theory-infected or theory-indifferent. The analysis is primarily conceptual rather than empirical, drawing on established ML practices and philosophical frameworks to construct their argument about theory-ladenness.

## Key Results
- ML modeling practices are largely theory-indifferent, constructed through data-fitting procedures independent of explicit theoretical constraints
- Theory can be passively "inherited" from datasets (theory-infection) without requiring explicit incorporation into model construction
- Theory-indifference enables easy transfer of ML methods across scientific domains but raises normative questions about whether models should incorporate domain-theory
- The analysis challenges the "blanket view" that all scientific activities must be theory-laden, showing ML practices are fundamentally different

## Why This Works (Mechanism)
ML models achieve theory-indifference through their construction process: they optimize parameters to minimize loss functions on training data without requiring explicit theoretical constraints. The models learn statistical patterns from data rather than encoding theoretical principles, allowing them to transfer across domains by simply changing the input data while keeping the same architecture and optimization procedure.

## Foundational Learning
- **Theory-ladenness**: The idea that scientific observations and methods are influenced by theoretical assumptions. Why needed: Provides the philosophical framework against which ML's theory-indifference is evaluated. Quick check: Can you identify theoretical assumptions in a traditional scientific model like SIR?
- **Phenomenological models**: Models that describe observed phenomena without explaining underlying mechanisms. Why needed: Serves as the conceptual bridge between traditional theory-laden models and theory-indifferent ML models. Quick check: How does a phenomenological model differ from a mechanistic model?
- **Theory-infection**: Passive inheritance of theoretical assumptions through data curation rather than explicit model construction. Why needed: Explains how domain-theory can influence ML models without requiring explicit theoretical incorporation. Quick check: Can you identify potential theory-infection in a medical imaging dataset?

## Architecture Onboarding

- **Component map**: Data Curation -> Feature Engineering -> Hyper-parameter Specification -> Weight-parameter Optimization -> Learned Model
- **Critical path**: Data Curation (theory-infected, external) -> Feature Engineering (increasingly automated, theory-indifferent) -> Hyper-parameter Specification (engineering-driven, theory-indifferent) -> Weight-parameter Optimization (fully automated, theory-indifferent)
- **Design tradeoffs**: A key tradeoff is between "transferability" and "control." Theory-indifferent models transfer easily across domains (as-is transferability) but lack explicit theoretical grounding. Theory-informed models embed domain knowledge, potentially improving robustness and generalizability but sacrificing easy transfer.
- **Failure signatures**: A model achieves high in-distribution accuracy but fails on out-of-distribution data or provides no mechanistic insight. This indicates a theory-indifferent model that has learned statistical correlations rather than causal structure.
- **First 3 experiments**:
  1. **Replicate a theory-indifferent baseline:** Train a standard CNN on a curated scientific dataset (e.g., medical images) without domain-specific feature engineering. Measure baseline predictive accuracy.
  2. **Test for theory-infection:** Analyze the baseline model's errors. Do they correlate with known biases in the data curation process (e.g., specific subgroups over/under-represented in the training set)?
  3. **Provoke a mechanism failure:** Evaluate the baseline model on a slightly altered task (e.g., related but different pathology). Does it fail catastrophically, indicating a lack of learned generalizable (theoretical) concepts?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Should machine learning systems be explicitly designed to incorporate domain-theory, or should they remain theory-indifferent?
- **Basis in paper:** [explicit] Section 5 explicitly asks "is it desirable to infuse domain-theory in ML systems?" and whether models accounting for theory are better.
- **Why unresolved:** The paper focuses on the descriptive analysis that ML *is* largely theory-indifferent, deferring the normative question of whether this is "desirable" to future work.
- **What evidence would resolve it:** Comparative studies measuring the scientific utility, reliability, and validity of theory-infused models versus theory-indifferent models across various scientific tasks.

### Open Question 2
- **Question:** Does incorporating domain-theory into model construction actually improve epistemic desiderata such as robustness and out-of-distribution (OOD) generalization?
- **Basis in paper:** [explicit] Section 5 notes that while some argue "verticalization" helps OOD robustness, others support general-purpose models, leading to a "conflict between two different 'cultures'."
- **Why unresolved:** The authors identify this conflict and the need to argue whether theory is required for these specific goals, but they do not provide the argument or evidence themselves.
- **What evidence would resolve it:** Empirical benchmarks showing that theory-laden constraints significantly reduce error rates in OOD samples compared to purely data-driven, theory-indifferent baselines.

### Open Question 3
- **Question:** Does the "theory-indifference" of machine learning imply that practitioners require only general-purpose training rather than domain-specific curricula?
- **Basis in paper:** [explicit] Section 5 asks: "do we need ML curricula specific for a given discipline, or just one, general-purpose curriculum...?"
- **Why unresolved:** The question is raised as a direct consequence of the paper's thesis on transferability, but the authors do not analyze educational structures or outcomes.
- **What evidence would resolve it:** Longitudinal studies of ML practitioners determining if general-purpose training suffices for successful model deployment in specialized scientific fields.

## Limitations
- The paper's central claim about theory-indifference rests on philosophical analysis rather than empirical validation across diverse scientific domains
- The distinction between "theory-infection" and explicit theoretical incorporation remains somewhat nebulous in practice
- Data curation itself often involves theoretical decisions that may be opaque to model developers, complicating the theory-indifference claim

## Confidence

**Confidence Labels:**
- **High Confidence**: The descriptive claim that ML models are typically built through data-fitting procedures without explicit theoretical constraints. This is well-established in ML practice.
- **Medium Confidence**: The normative claim that theory-indifference enables easier domain transfer. While plausible, this requires more empirical validation across diverse scientific contexts.
- **Medium Confidence**: The philosophical distinction between theory-infected and theory-indifferent modeling practices. The conceptual framework is sound but may oversimplify the complex interplay between theory and data in scientific ML.

## Next Checks

1. **Empirical Transferability Test**: Systematically evaluate model performance when transferred across scientific domains with varying degrees of theoretical complexity, measuring both accuracy retention and generalization capacity.

2. **Theory-Infection Analysis**: Develop methods to quantify the extent of theoretical priors embedded in scientific datasets, distinguishing between curator-level theoretical assumptions and modeler-level theoretical inputs.

3. **Domain Expert Evaluation**: Conduct structured interviews with domain experts to assess whether theory-indifferent ML models provide sufficient explanatory value for scientific understanding, not just predictive accuracy.