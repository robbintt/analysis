---
ver: rpa2
title: Investigating Training and Generalization in Faithful Self-Explanations of
  Large Language Models
arxiv_id: '2512.07288'
source_url: https://arxiv.org/abs/2512.07288
tags:
- self-explanations
- training
- faithfulness
- style
- redacted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how training affects the faithfulness\
  \ of self-explanations generated by large language models (LLMs) and examines whether\
  \ these improvements generalize across different settings. The authors construct\
  \ pseudo-faithful self-explanations for three styles\u2014attribution, redaction,\
  \ and counterfactual\u2014using feature attribution methods under a one-word constraint."
---

# Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models

## Quick Facts
- arXiv ID: 2512.07288
- Source URL: https://arxiv.org/abs/2512.07288
- Reference count: 11
- Primary result: Training on pseudo-faithful self-explanations improves faithfulness, with generalization to unconstrained and unseen settings

## Executive Summary
This paper investigates how training affects the faithfulness of self-explanations generated by large language models (LLMs) and examines whether these improvements generalize across different settings. The authors construct pseudo-faithful self-explanations for three styles—attribution, redaction, and counterfactual—using feature attribution methods under a one-word constraint. They then train LLMs using these constructed explanations in a continual learning setup, mixing them with original instruction-tuning data to prevent catastrophic forgetting. Experiments with Tulu-2 models on three classification tasks show that training significantly improves faithfulness across all styles, with generalization observed to unconstrained multi-word settings and unseen tasks, particularly in the attribution style.

## Method Summary
The authors construct pseudo-faithful self-explanations using feature attribution methods under a one-word constraint for three explanation styles. These explanations are used to train LLMs in a continual learning setup that combines explanation data with original instruction-tuning data to prevent catastrophic forgetting. The study evaluates faithfulness improvements across three classification tasks using Tulu-2 models, examining both within-style and cross-style generalization effects.

## Key Results
- Training on pseudo-faithful explanations significantly improves faithfulness across all explanation styles
- Improvements generalize to unconstrained multi-word explanations and unseen tasks, especially for attribution style
- Cross-style generalization is observed, suggesting training benefits extend beyond the specific style used

## Why This Works (Mechanism)
Training on pseudo-faithful explanations provides LLMs with explicit examples of what constitutes faithful reasoning, allowing the models to internalize patterns that correlate with actual decision-making processes. The continual learning approach prevents catastrophic forgetting while integrating explanation generation into the model's existing capabilities. The one-word constraint simplifies the explanation generation task, making it easier for models to learn the fundamental relationship between inputs and outputs before generalizing to more complex explanation forms.

## Foundational Learning
- **Feature attribution methods**: Used to construct pseudo-faithful explanations by identifying important input features; needed to create training data for explanation generation
- **Continual learning**: Training paradigm that combines explanation data with original instruction data to prevent catastrophic forgetting; needed to maintain baseline capabilities while adding explanation generation
- **Faithfulness evaluation**: Metrics to assess whether explanations accurately reflect the model's reasoning; needed to quantify improvements in explanation quality
- **One-word constraint**: Simplification that restricts explanations to single words; needed to establish a tractable baseline for training
- **Cross-style generalization**: Ability of models trained on one explanation style to perform well on other styles; needed to assess the breadth of training benefits
- **Classification tasks**: Specific evaluation tasks used to measure explanation faithfulness; needed to provide concrete benchmarks for assessment

## Architecture Onboarding
The study uses Tulu-2 models as the base architecture, employing a continual learning approach where pseudo-faithful explanations are interleaved with original instruction-tuning data. The critical path involves constructing explanations via feature attribution, training the model on this combined dataset, and evaluating faithfulness across multiple settings. The design tradeoff involves using simplified one-word explanations for tractability versus the complexity of real-world explanations. Failure signatures would include no improvement in faithfulness after training or catastrophic forgetting of original capabilities. Three first experiments would be: 1) Evaluate faithfulness before and after training on attribution-style explanations, 2) Test generalization to unconstrained explanations, and 3) Assess cross-style generalization by training on one style and testing on another.

## Open Questions the Paper Calls Out
None

## Limitations
- Use of pseudo-faithful explanations rather than ground truth explanations may not fully capture true faithful self-explanation capabilities
- One-word constraint significantly simplifies the explanation generation task compared to real-world requirements
- Experimental scope limited to three classification tasks and Tulu-2 models, restricting generalizability

## Confidence
- **High confidence**: Training improves faithfulness in self-explanations within experimental conditions
- **Medium confidence**: Generalization to unconstrained multi-word settings and unseen tasks
- **Medium confidence**: Cross-style generalization benefits

## Next Checks
1. Test generalization across model scales (e.g., 7B, 13B, 70B parameter models) to assess whether training effects scale predictably with model size
2. Evaluate performance on open-ended generation tasks beyond classification to determine if findings extend to broader LLM capabilities
3. Compare pseudo-faithful training against training with human-annotated explanations to quantify the gap between constructed and ground truth explanations in terms of generalization performance