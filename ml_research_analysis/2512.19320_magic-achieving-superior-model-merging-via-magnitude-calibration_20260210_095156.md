---
ver: rpa2
title: 'MAGIC: Achieving Superior Model Merging via Magnitude Calibration'
arxiv_id: '2512.19320'
source_url: https://arxiv.org/abs/2512.19320
tags:
- task
- magnitude
- feature
- calibration
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model merging in deep learning,
  where the goal is to combine the capabilities of multiple specialized models into
  a single unified model. The authors identify that existing model merging approaches
  primarily focus on aligning the directional components of features, while neglecting
  the magnitude component.
---

# MAGIC: Achieving Superior Model Merging via Magnitude Calibration

## Quick Facts
- **arXiv ID:** 2512.19320
- **Source URL:** https://arxiv.org/abs/2512.19320
- **Reference count:** 40
- **Primary result:** Average +4.3% accuracy improvement on 8 computer vision datasets and +8.0% on Llama tasks

## Executive Summary
This paper addresses a critical limitation in model merging: the neglect of feature magnitude alignment during parameter fusion. While existing methods focus on directional alignment, the authors demonstrate that magnitude deviation between merged and specialized models significantly degrades performance. They propose MAGIC (MAGnItude Calibration), a framework that calibrates feature and weight magnitudes to realign merged models with their specialized counterparts. MAGIC includes three variants - Feature Space Calibration (FSC), Weight Space Calibration (WSC), and Dual Space Calibration (DSC) - achieving consistent improvements across computer vision and natural language processing tasks.

## Method Summary
MAGIC is a post-merging calibration framework that addresses magnitude deviation in merged models. It first extracts task vectors (Δθ = θ_finetuned - θ_pretrained) and applies a base merging method (TA, TIES, etc.). The framework then identifies magnitude-sensitive layers using a generic dataset, computes layer-wise scaling coefficients to minimize feature-norm discrepancies, and applies conservative scaling (only reducing magnitude for sensitive layers, only increasing for others). FSC uses unlabeled task data to compute coefficients via feature matching, WSC provides a data-free approximation via weight space constraints, and DSC applies both sequentially.

## Key Results
- Average +4.3% accuracy improvement on 8 computer vision datasets
- Average +8.0% improvement on Llama tasks
- Outperforms state-of-the-art merging methods across multiple architectures (CLIP, BERT, Llama)
- Data-free WSC variant provides reasonable performance without requiring task-specific samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merged models suffer from feature magnitude deviation that degrades performance, even when directional alignment is achieved.
- Mechanism: Merging operations alter the Lp-norm of task vectors. Under local linearity, weight magnitude changes propagate to feature magnitude changes. Weight disentanglement means task-irrelevant weights reduce feature magnitude by misaligning with input-specific Jacobians, independently of weight norm.
- Core assumption: First-order Taylor expansion approximates feature changes well; NTK approximation holds for task vector formation.
- Evidence anchors: [abstract] "features consist of two critical components: direction and magnitude"; [section II-B] Eq. (4-8) derive that "any fusion of task vectors inherently alters their magnitude"; [corpus] Related work on LoRA merging notes magnitude issues may be architecture-specific.
- Break condition: If task vectors are near-orthogonal AND merging preserves norms exactly, magnitude deviation becomes negligible.

### Mechanism 2
- Claim: Layer-wise scaling coefficients that minimize feature-norm discrepancy between merged and specialized models improve task performance.
- Mechanism: Under full weight disentanglement, optimal performance occurs when merged feature magnitude matches specialized model magnitude: ξ_l = ∥Δh_k∥ / ∥Δh_merge∥. FSC computes this coefficient per layer using unlabeled data; WSC approximates it via hyperellipsoid constraints in weight space.
- Core assumption: Weight disentanglement property holds sufficiently; minimizing magnitude discrepancy is a good proxy for preserving task-specific behavior.
- Evidence anchors: [section II-D] Theorem 2 and Eq. (16): "the merged model achieves its optimal task performance when the magnitude of its task feature matches that of the corresponding specialised model"; [section III-B] Algorithm 1 implements Eq. (18) with conservative XOR constraint for sensitive layers.
- Break condition: When task interference is severe (ε(x) dominates), matching magnitude alone is insufficient; directional conflicts remain.

### Mechanism 3
- Claim: Identifying magnitude-sensitive layers and suppressing their scaling prevents performance degradation from aggressive calibration.
- Mechanism: Layers in sharp minima of the loss landscape have high sensitivity s_l to magnitude perturbations. Theorem 1 shows sensitivity varies by layer. Using a general dataset, compute layer-wise sensitivity and suppress scaling (ξ<1 only) for top-α sensitive layers.
- Core assumption: Layer sensitivity is transferable across tasks/datasets; sensitive layers reside in sharp minima universally.
- Evidence anchors: [section II-C] Theorem 1: "different layers exhibit varying sensitivities to task vector scaling"; [section III-A] Eq. (17) defines magnitude-sensitive layer set A; Fig. 5 visualizes layer sensitivity consistency across tasks.
- Break condition: If α is set too high, overly conservative calibration underutilizes beneficial magnitude scaling; if too low, sensitive layers cause instability.

## Foundational Learning

- Concept: **Task Vectors and Task Arithmetic**
  - Why needed here: MAGIC builds on task vectors (Δθ = θ_finetuned - θ_pretrained) as the fundamental unit of merging. Understanding that task-specific knowledge is encoded in weight differences is prerequisite.
  - Quick check question: Given a pretrained model (70% acc) and fine-tuned model (95% acc), what does the task vector represent?

- Concept: **Feature Decomposition (Direction vs. Magnitude)**
  - Why needed here: The paper's core insight is that features have two components; prior work aligned direction but ignored magnitude. Understanding vector norms and unit vectors is essential.
  - Quick check question: If a feature vector has magnitude 5 and points at 45°, what happens to its magnitude if you scale by ξ=0.5?

- Concept: **Jacobian and First-Order Taylor Expansion**
  - Why needed here: The theoretical analysis relies on approximating feature changes via Jacobians: Δh ≈ J·Δθ. This connects weight perturbations to feature perturbations.
  - Quick check question: Why does the paper assume local linearity around θ_pretrained?

## Architecture Onboarding

- Component map: Input: {θ_pretrained, {θ_k}_{k=1 to K}, optional unlabeled data X} -> Task Vector Extraction: Δθ_k = θ_k - θ_pretrained for each k -> Base Merging: θ_merge = MERGE({Δθ_k}) [any existing method] -> Sensitive Layer Detection (uses generic dataset) -> Calibration Branch: WSC, FSC, or DSC -> Conservative Constraint -> Output: Calibrated merged model θ̂_merge

- Critical path: Sensitive layer identification (requires forward pass on generic data) -> Calibration coefficient computation (requires either feature extraction or weight decomposition) -> Conservative scaling application

- Design tradeoffs:
  - **WSC vs. FSC**: WSC is data-free but assumes weight-feature magnitude correlation holds (fails for Iso-C in Table II). FSC is more accurate but requires 1+ unlabeled samples per task.
  - **DSC complexity**: Applies both calibrations sequentially; marginal gains over FSC alone in some cases (Table II).
  - **α selection**: Controls sensitivity to magnitude amplification. Default α=10 works across backbones; smaller α is more conservative.

- Failure signatures:
  - WSC degrades performance when weight-feature correlation is weak (e.g., Iso-C w/ WSC in Table II: -0.8% avg)
  - Using task-agnostic data for FSC (DSC-A in Table VII) underperforms task-specific data (76.5% vs 78.9%)
  - Large α causes over-suppression; α=0 (no suppression) still works but less robust

- First 3 experiments:
  1. **Sanity check**: Reproduce baseline (TA, TIES) on ViT-B/32 with 8 vision tasks; verify merged model accuracy is within 2-3% of reported (69-72% avg)
  2. **Ablation on calibration variants**: Apply WSC-only, FSC-only, and DSC to TSV-M on same tasks; compare to Table II to validate implementation (expect ~1.3% gain for WSC, ~0.3% for FSC on TSV-M)
  3. **Data sensitivity test**: Run FSC/DSC with varying unlabeled samples per task (1, 5, 10); plot accuracy vs. sample count to reproduce Fig. 6 curve (sharp gain at 1 sample, marginal thereafter)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can calibration accuracy be achieved in a purely data-free manner that matches the performance of feature-based methods? The Conclusion states "questions remain regarding... the realisation of improved calibration under data-free conditions." The proposed Weight Space Calibration (WSC) serves as a data-free approximation but is less robust; Table II shows WSC actually degrades performance for Iso-C (-0.8%) and Iso-CTS (-1.1%) due to discrepancies between weight and feature magnitudes.

- **Open Question 2**: Can weight disentanglement be accurately assessed solely at the weight level without relying on feature activations? The Conclusion lists "questions remain regarding the assessment of weight disentanglement at the weight level" as a primary limitation. The paper establishes the importance of disentanglement but relies on feature-space analysis or geometric approximations to estimate it, lacking a direct theoretical metric derived from weights.

- **Open Question 3**: How can the conflict between weight-space and feature-space calibration coefficients be resolved when they indicate opposing adjustments? Figure 8 visualizes a "notable divergence" where weight space demands a reduction in magnitude while feature space requires an increase. The Dual Space Calibration (DSC) simply applies calibration sequentially, but Table II shows that for certain methods (Iso-C), applying WSC is detrimental, suggesting the conflict is not resolved but rather that FSC overrides or masks the error in the final DSC step.

## Limitations

- The paper assumes weight disentanglement holds sufficiently for magnitude matching to be a good proxy for task performance, but this assumption is not rigorously validated across all tested architectures.
- WSC's data-free calibration relies on a hyperellipsoid constraint (Eq. 22) that is not explicitly solved in closed form, requiring approximation methods that may affect reproducibility.
- Layer sensitivity transferability (Theorem 1) is empirically supported but theoretically limited to specific conditions (local linearity, sharp minima).

## Confidence

- **High**: The empirical improvements (+4.3% CV, +8.0% NLP) are well-supported by extensive experiments across multiple datasets and architectures.
- **Medium**: The theoretical framework (Theorems 1-2) provides intuitive justification but relies on assumptions that don't always hold (e.g., weight-feature magnitude correlation fails for Iso-C).
- **Low**: The mechanism explaining why magnitude deviation specifically causes degradation is primarily correlational rather than causal.

## Next Checks

1. **Ablation on weight-feature correlation**: Systematically test WSC vs. FSC across architectures where weight-feature correlation is known to be weak to validate the hyperellipsoid approximation's limitations.

2. **Sensitivity analysis beyond α=10**: Test layer sensitivity transferability across task domains by varying α and measuring performance degradation when using mismatched sensitivity datasets.

3. **Magnitude perturbation experiments**: Design controlled experiments where magnitude is artificially manipulated (without directional changes) to isolate its causal effect on task performance.