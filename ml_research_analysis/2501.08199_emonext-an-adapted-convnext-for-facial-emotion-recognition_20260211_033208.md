---
ver: rpa2
title: 'EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition'
arxiv_id: '2501.08199'
source_url: https://arxiv.org/abs/2501.08199
tags:
- facial
- recognition
- deep
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EmoNeXt, a deep learning framework for facial
  emotion recognition based on an adapted ConvNeXt architecture. The method integrates
  a Spatial Transformer Network (STN) to handle variations in facial images, Squeeze-and-Excitation
  blocks for adaptive channel-wise feature recalibration, and a self-attention regularization
  term to encourage compact feature generation.
---

# EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition

## Quick Facts
- arXiv ID: 2501.08199
- Source URL: https://arxiv.org/abs/2501.08199
- Reference count: 40
- Primary result: Achieves 76.12% accuracy on FER2013, surpassing state-of-the-art baselines

## Executive Summary
This paper presents EmoNeXt, a deep learning framework for facial emotion recognition based on an adapted ConvNeXt architecture. The method integrates a Spatial Transformer Network (STN) to handle variations in facial images, Squeeze-and-Excitation blocks for adaptive channel-wise feature recalibration, and a self-attention regularization term to encourage compact feature generation. The model was evaluated on the FER2013 dataset and achieved state-of-the-art performance with an accuracy of 76.12%, surpassing existing models including the Segmentation VGG-19 (75.97%) and the original ConvNeXt-XLarge (74.15%). The proposed architecture demonstrates superior emotion classification accuracy, making it a promising approach for facial expression recognition applications.

## Method Summary
EmoNeXt adapts the ConvNeXt architecture for facial emotion recognition by incorporating three key modifications: a Spatial Transformer Network for handling facial image variations, Squeeze-and-Excitation blocks for adaptive channel recalibration, and a self-attention regularization term. The model uses ImageNet-22k pretrained weights and is trained on the FER2013 dataset with AdamW optimizer, cosine decay schedule, and mixed precision training. The architecture processes 48×48 grayscale images resized to 224×224, employing various regularization techniques including Stochastic Depth, Label Smoothing, and EMA.

## Key Results
- Achieves 76.12% accuracy on FER2013 test set with EmoNeXt-XLarge
- Outperforms baseline ConvNeXt-XLarge (74.15%) and Segmentation VGG-19 (75.97%)
- Demonstrates consistent performance improvement across model variants (Tiny to XLarge)
- Shows robust emotion classification capability across seven expression categories

## Why This Works (Mechanism)

### Mechanism 1: Spatial Transformer Network for Alignment
The STN component improves robustness to pose and alignment variations in facial images. It learns a differentiable geometric transformation through localization, grid generation, and sampling sub-components, automatically normalizing face position, scale, and rotation during forward propagation.

### Mechanism 2: Channel-wise Feature Recalibration via SE Blocks
Squeeze-and-Excitation blocks enhance discriminative feature selection by adaptively weighting channel importance. Global average pooling followed by fully connected layers learns non-linear channel interdependencies, amplifying informative channels and suppressing redundant ones.

### Mechanism 3: Self-Attention Regularization for Compact Features
The self-attention regularization term encourages more balanced feature utilization by minimizing variance between individual attention weights and their mean. This prevents any single feature dimension from dominating the representation, improving generalization.

## Foundational Learning

- **Depthwise Separable Convolutions**
  - Why needed here: ConvNeXt backbone uses depthwise convolutions followed by pointwise convolutions. Understanding this decomposition is essential for debugging feature extraction.
  - Quick check question: Given an input of 64 channels and 3×3 kernel, how many parameters does a depthwise convolution require versus a standard convolution with 128 output channels?

- **Attention Mechanisms (Query-Key-Value paradigm)**
  - Why needed here: The self-attention regularization computes attention weights between features. Understanding Q, K, V formulation clarifies what the regularization actually constrains.
  - Quick check question: In the equation W(Q,K) = softmax(Q·K^T / √d), what does the √d scaling prevent?

- **Transfer Learning from ImageNet to Specialized Domains**
  - Why needed here: The paper uses ImageNet-22k pretrained ConvNeXt weights. Recognizing what transfers versus what doesn't informs fine-tuning strategy.
  - Quick check question: Why might early convolutional layers transfer well while later layers require more aggressive fine-tuning for emotion recognition?

## Architecture Onboarding

- **Component map:**
  Input (48×48 grayscale → resized to 224×224 RGB) → Spatial Transformer Network → Patchify Layer → Stage 1: ConvNeXt blocks → SE block → Stage 2-4: [ConvNeXt blocks → downsample → SE block] × 3 → Global Average Pooling → Classification head

- **Critical path:**
  1. STN convergence is critical—if the localization network produces degenerate transformations, downstream features remain unaligned
  2. SE block placement after each stage is the design choice; verify this matches implementation
  3. The self-attention branch must be disabled during inference—it only contributes to training loss

- **Design tradeoffs:**
  - Model size vs. accuracy: EmoNeXt-Tiny (73.34%) to XLarge (76.12%) shows ~2.8% accuracy gain but significant VRAM increase
  - STN overhead: Adds ~5-10% training time but eliminates need for external face alignment tools
  - SE block parameters: Adds roughly (C/r + C) parameters per block, approximately 1-2% total parameter increase
  - λ tuning: Too high → underfitting from over-regularization; too low → regularization ineffective

- **Failure signatures:**
  - STN produces blank or severely distorted outputs → learning rate for localization network too high
  - Validation accuracy plateaus 5-10% below paper results → check class imbalance handling
  - Attention weights collapse to near-uniform despite regularization → λ may be too low
  - Training loss decreases but validation accuracy fluctuates wildly → enable Label Smoothing

- **First 3 experiments:**
  1. Ablation baseline: Train EmoNeXt-XLarge with each component removed individually to isolate contribution of each mechanism
  2. Lambda sensitivity sweep: Train models with λ ∈ {0, 0.01, 0.1, 0.5, 1.0, 2.0} to identify the stability range
  3. Cross-dataset sanity check: Evaluate the best checkpoint from FER2013 on RAF-DB or AffectNet

## Open Questions the Paper Calls Out
- How does EmoNeXt perform across diverse facial expression databases beyond FER2013?
- Can the model effectively generalize to emotion recognition for elderly Alzheimer’s patients?
- What is the individual contribution of the STN, SE blocks, and self-attention regularization to the performance gain?

## Limitations
- Critical hyperparameters including batch size, training epochs, and λ value are not specified
- Class imbalance in FER2013 (547 disgust vs 8,989 happy samples) may impact minority class performance
- Claims of state-of-the-art performance require verification against recent FER2013 benchmarks

## Confidence
- **High Confidence:** The architectural integration of STN, SE blocks, and self-attention regularization is technically sound
- **Medium Confidence:** The claimed 76.12% accuracy is plausible but requires reproduction due to missing hyperparameters
- **Low Confidence:** The effectiveness of the self-attention regularization term specifically for FER is asserted but not independently validated

## Next Checks
1. Perform ablation study reproduction by training EmoNeXt-XLarge with each component removed individually
2. Conduct cross-dataset generalization evaluation by testing the best FER2013 model on RAF-DB or AffectNet
3. Execute hyperparameter sensitivity analysis by sweeping λ values in [0.01, 0.1, 1.0] while monitoring validation accuracy and attention weight variance