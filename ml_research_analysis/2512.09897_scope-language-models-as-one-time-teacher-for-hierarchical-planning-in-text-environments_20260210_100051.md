---
ver: rpa2
title: 'SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text
  Environments'
arxiv_id: '2512.09897'
source_url: https://arxiv.org/abs/2512.09897
tags:
- craft
- birch
- planks
- subgoals
- pink
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCOPE, a hierarchical planning method that
  uses LLM-generated subgoals only once at initialization to pretrain a student planner,
  rather than repeatedly querying the LLM during training. By extracting subgoals
  directly from demonstration trajectories and combining them with RL fine-tuning
  via world models, SCOPE reduces computational cost and eliminates the need for LLM
  inference at test time.
---

# SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments

## Quick Facts
- **arXiv ID:** 2512.09897
- **Source URL:** https://arxiv.org/abs/2512.09897
- **Reference count:** 40
- **Primary result:** LLM-generated subgoals used once at initialization enable hierarchical planner to achieve 0.56 success rate in TextCraft, outperforming LLM-based ADaPT (0.52) while reducing inference time from 164.4s to 3.0s

## Executive Summary
SCOPE introduces a hierarchical planning method that uses LLM-generated subgoals only once at initialization to pretrain a student planner, eliminating the need for repeated LLM queries during training and testing. By extracting subgoals directly from demonstration trajectories and combining them with RL fine-tuning via world models, SCOPE achieves significant computational savings while maintaining strong performance. The method demonstrates that even suboptimal, one-time LLM guidance can effectively support long-horizon text-based planning when paired with RL refinement, reducing inference time by over 50x compared to baseline approaches.

## Method Summary
SCOPE uses an LLM to generate subgoal decomposition logic from demonstration trajectories, then trains a two-level hierarchical planner (Manager/Employee) via supervised learning followed by RL fine-tuning in learned world models. The employee learns to execute primitive actions to achieve subgoals, while the manager learns to propose subgoals that compensate for employee imperfections. Both agents are trained on the same trajectories but at different abstraction levels, with the manager adapting to the employee's execution capabilities through simulated rollouts in the Manager World Model.

## Key Results
- Achieves 0.56 success rate on TextCraft, outperforming LLM-based ADaPT (0.52)
- Reduces inference time from 164.4 seconds to 3.0 seconds
- 2% performance drop compared to hand-engineered subgoals, demonstrating robustness of LLM-generated structure
- Manager RL fine-tuning is critical: ablation drops success from 0.56 to 0.24

## Why This Works (Mechanism)

### Mechanism 1
A one-time LLM query to generate subgoal decomposition function sufficiently initializes a hierarchical planner when combined with RL grounding. The LLM creates a Python function that parses trajectories into subgoals based on inventory states, used to distill knowledge into a student policy via behavioral cloning. This creates strong initialization avoiding online LLM latency. Core assumption: static demonstration trajectories contain sufficient signal for valid subgoal structures. Evidence: 2% performance gap vs hand-engineered subgoals while achieving 50x speedup.

### Mechanism 2
A high-level manager agent learns to compensate for low-level execution errors by adaptively proposing alternative subgoals. During RL fine-tuning, the manager interacts with a Manager World Model composed of the environment world model and current employee policy. If employee fails to achieve proposed subgoal, manager receives negative feedback and learns to propose "safer" subgoals through CEM optimization. Core assumption: employee achieves non-zero success rate on individual subgoals. Evidence: ablation shows manager RL fine-tuning critical (0.56 → 0.24).

### Mechanism 3
Decoupling world model training from policy training enables efficient offline-style refinement of hierarchical policies. An Elementary World Model trained on trajectories predicts next states and action validity, then frozen for employee RL. Manager's world model wraps EWM with trained employee policy, allowing manager to simulate "subgoal actions." Core assumption: text-based state space can be modeled accurately enough for useful learning signals. Evidence: world model construction described in section 4.2.

## Foundational Learning

**Concept: Hierarchical Reinforcement Learning (HRL)**
- Why needed: Entire architecture relies on two-level hierarchy (Manager/Employee) to solve long-horizon problems
- Quick check: Can you explain the difference between a "primitive action" (executed by Employee) and a "temporal abstract action" (executed by Manager)?

**Concept: Model-Based Reinforcement Learning (World Models)**
- Why needed: Policies trained via rollouts in learned model rather than live environment
- Quick check: How does training on a world model differ from training directly on demonstration data (behavioral cloning)?

**Concept: Cross-Entropy Method (CEM)**
- Why needed: Paper uses CEM for optimizing policy during RL fine-tuning stage
- Quick check: CEM is derivative-free optimization; why might this be chosen over standard policy gradient methods for this architecture?

## Architecture Onboarding

**Component map:**
LLM (Offline) -> Data Preprocessor -> Employee Agent -> World Models -> Employee RL -> Manager Agent -> Manager RL

**Critical path:**
1. Code Generation: Prompt LLM to generate Python decomposition code (f_dc)
2. Data Processing: Run f_dc on 500k trajectories to generate training pairs
3. Pretraining: Train Employee (BC) and Manager (BC) separately
4. World Model Training: Train EWM on state transitions
5. Employee RL: Fine-tune Employee in EWM to maximize subgoal success
6. Manager RL: Construct MWM (EWM + Frozen Employee). Fine-tune Manager in MWM

**Design tradeoffs:**
- Speed vs. Explainability: LLM-generated subgoals fast but "less interpretable" (vague inventory states) vs hand-engineered ones
- Model Bias: Learned world model enables fast simulation but risks "model exploitation" where agent learns to hack imperfect world model

**Failure signatures:**
- Inventory Hallucination: Employee attempts to craft items with insufficient resources (EWM validity check failure)
- Manager Deadlocks: Manager repeatedly proposes subgoal employee lacks skill to solve, fails to switch strategies
- Drift: Manager generates item names not recognized by environment (e.g., "oak plank" vs "planks")

**First 3 experiments:**
1. Validate Subgoal Logic: Run LLM-generated f_dc on held-out validation set to ensure valid subgoal sequences
2. World Model Accuracy: Test EWM's ability to predict next-state inventories; <90% accuracy likely causes RL failure
3. Ablation on Manager Adaptation: Compare Manager performance with fixed subgoals vs RL-adaptive subgoals to verify "imperfection compensation" mechanism

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can SCOPE generalize to more complex environments beyond TextCraft, particularly those with less structured state spaces and more ambiguous action semantics?
- Basis: Authors describe TextCraft as "simplified, text-only version of Minecraft" and call this "preliminary work"
- Why unresolved: Evaluation limited to one environment with well-defined crafting recipes
- Evidence needed: Successful application to diverse text environments (interactive fiction, multi-room navigation) or embodied settings

**Open Question 2**
- Question: Can subgoal-generation process be refined to produce more interpretable subgoals while maintaining or improving performance?
- Basis: Authors note LLM-generated subgoals are "far less interpretable" than hand-engineered ones
- Why unresolved: 2% gap suggests room for improvement but no method for enhancing interpretability explored
- Evidence needed: Variant producing more interpretable subgoals (e.g., natural language descriptions) with comparable or better success rates

**Open Question 3**
- Question: How does SCOPE perform when demonstration trajectories contain significantly higher noise rates or adversarial suboptimality?
- Basis: Trajectories generated with only 10% random action injection; real human demonstrations may be more variable
- Why unresolved: Robustness to noisier or misleading demonstrations not evaluated
- Evidence needed: Ablation studies varying noise rates in demonstrations, or evaluation on human-collected trajectories with documented quality variance

## Limitations

- Method's reliance on single LLM query is both strength and limitation; untested what happens when LLM generates entirely incorrect decomposition logic
- Assumption that 500k suboptimal trajectories contain sufficient signal for meaningful subgoal extraction is untested beyond current TextCraft domain
- World model's ability to generalize to states not seen in training trajectories remains unclear

## Confidence

**High confidence**: Speed improvement claims (164.4s → 3.0s) well-supported by elimination of runtime LLM inference. Manager adaptation mechanism directly validated through ablation (0.56 → 0.24).

**Medium confidence**: Effectiveness of one-time LLM guidance supported by 2% performance gap but relies heavily on assumption LLM can extract meaningful structure from suboptimal demonstrations.

**Low confidence**: World model's fidelity and impact on long-term planning not thoroughly evaluated; paper states EWM predicts next states but does not report accuracy metrics or test sensitivity to model errors.

## Next Checks

1. **LLM Failure Mode Analysis**: Systematically test SCOPE with LLM-generated decomposition functions containing increasing levels of error (random subgoals, cyclic dependencies, impossible subgoals) to establish failure threshold

2. **World Model Fidelity Audit**: Measure EWM prediction accuracy on held-out trajectory segments and quantify how prediction errors propagate through manager rollouts during training

3. **Cross-Domain Generalization**: Apply SCOPE to different text-based planning environment (e.g., text adventure games) with minimal architecture changes to test robustness of one-time LLM teaching approach