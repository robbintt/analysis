---
ver: rpa2
title: Incomplete In-context Learning
arxiv_id: '2505.07251'
source_url: https://arxiv.org/abs/2505.07251
tags:
- learning
- image
- retrieval
- database
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Incomplete In-context Learning
  (IICL), where the retrieval database used for Vision In-context Learning (VICL)
  contains labeled examples for only a subset of all possible classes, due to delayed
  updates, class imbalance, or incomplete annotation. The authors propose Iterative
  Judgments and Integrated Prediction (IJIP), a two-stage framework that first reformulates
  an m-class classification problem into a sequence of m binary classification tasks
  to ensure complete demonstrations for each class, and then refines predictions by
  combining image and iterative judgment outputs.
---

# Incomplete In-context Learning

## Quick Facts
- **arXiv ID:** 2505.07251
- **Source URL:** https://arxiv.org/abs/2505.07251
- **Reference count:** 40
- **Primary result:** Proposes IJIP framework achieving up to 93.9% accuracy for IICL by reformulating multi-class problems into binary sequences.

## Executive Summary
This paper addresses Incomplete In-context Learning (IICL), where Vision In-context Learning (VICL) retrieval databases contain labeled examples for only a subset of all possible classes. The authors propose Iterative Judgments and Integrated Prediction (IJIP), a two-stage framework that reformulates an m-class classification problem into a sequence of m binary classification tasks to ensure complete demonstrations for each class, and then refines predictions by combining image and iterative judgment outputs. IJIP achieves state-of-the-art performance, reaching up to 93.9% accuracy across two LVLMs and datasets, outperforming six baselines even under extreme label incompleteness (e.g., 88.5% accuracy when only one label is available).

## Method Summary
IJIP is a two-stage framework for Vision In-context Learning with incomplete retrieval databases. First, the Iterative Judgments Stage reformulates an m-class classification problem into m binary classification tasks ("Is this Class j?"), ensuring that even if the database lacks examples for Class j, it contains examples of other classes that can serve as "Not Class j" demonstrations. Second, the Integrated Prediction Stage analyzes the binary results: if zero positives are found, it falls back to standard multi-class VICL; if one positive, it accepts that label; if multiple positives, it refines by restricting to those candidate classes. This approach converts an incomplete IICL scenario into a complete VICL scenario through dynamic label remapping and consistency checking.

## Key Results
- IJIP achieves 93.9% accuracy on CIFAR-10 and Fashion-MNIST with complete databases
- Maintains 88.5% accuracy even when retrieval database contains data for only one class (99% missing)
- Outperforms six baselines including zero-shot and standard VICL approaches
- Demonstrates effectiveness across two LVLMs (InternVL 2.5-4B/8B) and multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating a multi-class problem into a sequence of binary classification tasks mitigates the absence of labeled data for specific classes in the retrieval database.
- **Mechanism:** By querying "Is this class $C_j$?" rather than "Which class is this?", the system transforms the problem space. Even if the retrieval database lacks images for class $C_j$, it contains images of other classes which serve as "Not $C_j$" demonstrations. This ensures that for every binary query, valid positive or negative demonstrations exist, converting an incomplete IICL scenario into a complete VICL scenario.
- **Core assumption:** The LVLM can effectively interpret "Not [Class]" labels as distinct negative concepts when applied to images of other classes.
- **Evidence anchors:** [abstract] "The Iterative Judgments Stage reformulates an m-class classification problem into a series of m binary classification tasks..."; [section 4.1] "Transforming Incomplete In-context Learning into In-context Learning... the retrieval database always contains at least one instance of $\bar{C}_j$ [negative class]."

### Mechanism 2
- **Claim:** Dynamic re-labeling of demonstrations allows a single incomplete database to serve all classification sub-tasks.
- **Mechanism:** The system does not statically store binary labels. Instead, it retrieves images based on visual similarity and dynamically assigns binary labels (e.g., "Dog" vs "Not Dog") based on the specific sub-question being asked in the Iterative Judgment Stage. This creates synthetic binary datasets on the fly from a multi-class source.
- **Core assumption:** Retrieving visually similar images is sufficient for effective in-context learning, even if their original ground-truth labels are mapped to a binary "Not" category for the current query.
- **Evidence anchors:** [section 3] "In the j-th subproblem... the demonstration format is modified... $y_i^j \in \{C_j, \bar{C}_j\}$."; [table 1] Shows mapping ground truth "Dog" to "Not cat, Not fish."

### Mechanism 3
- **Claim:** An integration stage that enforces consistency and narrows candidate sets improves upon the raw binary predictions.
- **Mechanism:** The "Integrated Prediction Stage" acts as a corrective filter. If the binary stage produces multiple positive candidates (contradictions), it forces a final selection among only those candidates. If all are negative, it falls back to a standard multi-class query. This mimics a "review" process where uncertain options are re-evaluated.
- **Core assumption:** The LVLM is better at choosing the correct label from a small subset (e.g., 2 or 3 candidates) than from the full set of $m$ classes directly.
- **Evidence anchors:** [section 4.2] "When $I_x > 1$... the LVLM employs the IICL of the u-class classification... candidate labels are the corresponding labels for $I_j^x = 1$."; [figure 2] "IJIP queries LVLMs at most twice... to refine the decision."

## Foundational Learning

- **Concept: Vision In-Context Learning (VICL)**
  - **Why needed here:** This is the base capability being debugged. You must understand that LVLMs can perform tasks by looking at examples (demonstrations) in the prompt without weight updates.
  - **Quick check question:** Can you explain how providing an image-label pair in a prompt changes an LVLM's output compared to a zero-shot query?

- **Concept: Retrieval Database & k-NN Search**
  - **Why needed here:** The core problem (IICL) is defined by the state of this database. You need to understand that these systems typically retrieve the "most similar" past examples to guide the current prediction.
  - **Quick check question:** If a database has labels for "Cats" and "Dogs" but not "Fish," what happens when standard VICL tries to classify a "Fish" image based on visual similarity?

- **Concept: Binary vs. Multi-Class Classification Decomposition**
  - **Why needed here:** The proposed solution (IJIP) relies on breaking one complex problem into many simple yes/no questions.
  - **Quick check question:** How does converting a 10-class problem into 10 binary "Is this X?" problems change the requirements for the labeled data?

## Architecture Onboarding

- **Component map:** CLIP encoder -> k-NN retrieval -> Prompt constructor -> LVLM inference -> Integration logic
- **Critical path:**
  1. Input Image → Retrieval (find neighbors)
  2. Loop/Sequence over $m$ classes (Iterative Judgments):
     - Construct "Is this Class $j$?" prompt
     - Inject neighbors with "Class $j$" or "Not Class $j$" labels
     - LVLM inference
  3. Aggregate Binary Results → Decision Logic (Integrated Prediction):
     - 0 positives? → Standard VICL
     - 1 positive? → Assign label
     - >1 positive? → Restricted VICL on positive subset

- **Design tradeoffs:**
  - **Latency vs. Robustness:** The Iterative Judgment Stage may query the model multiple times (or with a longer sequence of sub-questions), increasing inference cost compared to a single-shot VICL, but it recovers accuracy in incomplete data regimes
  - **Assumption 3.1:** The architecture bets that binary classification is easier for the model than multi-class, which may not hold for all model architectures or modalities

- **Failure signatures:**
  - **Constant Negatives:** The binary stage returns "Not X" for all classes, triggering fallback to standard VICL which will likely fail if the true class is missing from the database
  - **Oscillation:** Multiple conflicting positive binary results that the Integrated Prediction stage fails to resolve

- **First 3 experiments:**
  1. **Sanity Check (Complete DB):** Run IJIP on CIFAR-10 with a complete database. Verify it matches or exceeds standard VICL to ensure the binary reformulation doesn't degrade standard performance
  2. **Ablation (Missing Label):** Remove all training data for a single class (e.g., "Truck") from the retrieval database. Compare standard VICL accuracy vs. IJIP accuracy specifically on "Truck" test images
  3. **Stress Test (Single Label DB):** Populate the retrieval database with data for only one class (99% missing). Measure IJIP's ability to distinguish that class from others using only "Not [Class]" demonstrations

## Open Questions the Paper Calls Out
- **Open Question 1:** How does IJIP performance and latency scale when the number of classes ($m$) extends to large-scale datasets (e.g., >100 classes)? The paper only evaluates datasets with up to 24 classes, and as $m$ increases, prompt length grows significantly, potentially hitting context window limits or causing attention dilution.
- **Open Question 2:** Is IJIP robust to label noise within the available subset of the retrieval database? The paper assumes available annotations are correct, but real-world incomplete databases may contain mislabeled examples that could mislead binary judgments.
- **Open Question 3:** Does the sequence order of the binary sub-questions influence the final prediction accuracy? The paper reformulates the problem into a sequence of $m$ binary tasks but does not discuss ordering strategy, which might introduce positional bias in LLM responses.

## Limitations
- The binary reformulation assumes LVLMs can effectively learn "Not [Class]" concepts from visually dissimilar images, which may not hold for semantically similar classes
- Performance depends on the quality of negative demonstrations retrieved, which could be problematic when retrieval database is extremely sparse
- The framework's latency increases with the number of classes due to sequential binary queries, making it less suitable for large-scale classification tasks
- Assumes the LVLM can follow binary instructions and negation logic in prompts, which may not generalize across all model architectures

## Confidence
- **High Confidence:** The empirical results showing IJIP's superior performance in incomplete-label regimes are well-supported by the reported accuracy metrics across multiple datasets and model scales
- **Medium Confidence:** The mechanism explaining how binary reformulation solves the incomplete database problem is logically sound but lacks ablation studies to prove necessity
- **Low Confidence:** The claim that binary classification is inherently easier than multi-class classification for LVLMs is assumed rather than tested; no direct comparison of binary vs. multi-class performance on the same model/task is provided

## Next Checks
1. **Prompt Template Isolation:** Implement and test multiple prompt variations (strict binary instructions vs. open-ended) to determine if the binary formulation's success depends on prompt engineering rather than the conceptual framework
2. **Negative Sample Quality Analysis:** Systematically vary the visual similarity between "Not [Class]" demonstrations and query images to quantify how negative sample quality affects binary classification accuracy
3. **Binary vs. Multi-Class Direct Comparison:** On a complete database, compare IJIP's binary stage accuracy against a direct multi-class prompt to test the assumption that binary decomposition is easier for the model