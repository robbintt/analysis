---
ver: rpa2
title: Accelerating Inference for Multilayer Neural Networks with Quantum Computers
arxiv_id: '2510.07195'
source_url: https://arxiv.org/abs/2510.07195
tags:
- quantum
- circuit
- then
- lemma
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first fully coherent quantum implementation
  of multilayer neural networks with non-linear activation functions, specifically
  focusing on ResNet-like architectures with residual blocks containing multi-filter
  2D convolutions, sigmoid activations, skip-connections, and layer normalizations.
  The work analyzes inference complexity under three quantum data access regimes:
  with efficient quantum access to both inputs and weights (achieving polylogarithmic
  complexity in input dimension), with efficient quantum access to weights only (providing
  quartic speedup over classical methods), and with no quantum access assumptions
  (yielding quadratic speedup).'
---

# Accelerating Inference for Multilayer Neural Networks with Quantum Computers

## Quick Facts
- arXiv ID: 2510.07195
- Source URL: https://arxiv.org/abs/2510.07195
- Reference count: 40
- First fully coherent quantum implementation of multilayer neural networks with non-linear activation functions

## Executive Summary
This paper presents the first fully coherent quantum implementation of multilayer neural networks with non-linear activation functions, specifically focusing on ResNet-like architectures with residual blocks containing multi-filter 2D convolutions, sigmoid activations, skip-connections, and layer normalizations. The work analyzes inference complexity under three quantum data access regimes: with efficient quantum access to both inputs and weights (achieving polylogarithmic complexity in input dimension), with efficient quantum access to weights only (providing quartic speedup over classical methods), and with no quantum access assumptions (yielding quadratic speedup). Key technical contributions include developing a modular vector-encoding framework for quantum matrix-vector arithmetic, introducing a novel quantum algorithm for multiplying arbitrary dense matrices with element-wise squared vectors without Frobenius norm dependence, and providing a QRAM-free block-encoding for 2D multi-filter convolutions.

## Method Summary
The paper constructs quantum inference circuits for residual neural networks using a Vector-Encoding framework and Quantum Singular Value Transformation. The architecture consists of sequential residual blocks with 2D convolutions, erf activation functions, skip connections, and layer normalization. The inference process uses three distinct data access regimes: (1) efficient quantum access to both inputs and weights achieving polylogarithmic complexity, (2) efficient quantum access to weights only providing quartic speedup, and (3) no quantum access assumptions yielding quadratic speedup. The key innovation is maintaining coherence throughout the network layers without intermediate measurements or tomography, enabled by spectral normalization of weights and residual skip connections that preserve vector norms through non-linear transformations.

## Key Results
- Achieves polylogarithmic complexity in input dimension when efficient quantum access to both inputs and weights is available
- Provides quartic speedup over classical methods when only efficient quantum access to weights is available
- Introduces novel quantum algorithm for multiplying arbitrary dense matrices with element-wise squared vectors without Frobenius norm dependence
- Develops QRAM-free block-encoding for 2D multi-filter convolutions

## Why This Works (Mechanism)

### Mechanism 1: Arbitrary Matrix-Vector Squared Product without Frobenius Dependence
The paper derives a quantum subroutine to multiply an arbitrary full-rank, dense matrix W by the element-wise square of a vector g(|ψ⟩), circumventing the typical ||W||_F (Frobenius norm) complexity dependence. Instead of creating a block-encoding of W directly, the algorithm constructs a specific operator using importance weighting that queries columns of W weighted by the amplitudes of the input vector |ψ⟩. This treats W as a sum of rank-1 components and applies them via a "Matrix-Vector Squared" subroutine without normalizing by the total matrix norm.

### Mechanism 2: Norm Preservation via Residual Skip Connections
Residual (skip) connections are necessary to maintain the norm of the quantum state through multiple layers, preventing "norm decay" that would otherwise make deep networks intractable. In a quantum circuit, non-linear transformations can arbitrarily shrink the amplitude of the encoded vector. The skip connection adds the input vector to the transformed output, guaranteeing a lower bound on the output norm and stabilizing computational cost. This requires weight layers to be "spectrally regularized" (||W||_2 ≤ 1).

### Mechanism 3: Coherent Non-Linearity via Polynomial Approximation
Non-linear activation functions (specifically erf or sigmoid-like functions) can be applied coherently to a quantum state without collapsing the state via measurement, provided the circuit depth scales exponentially with the number of layers k. The paper uses Quantum Singular Value Transformation or similar polynomial approximation techniques to implement the non-linear function on the amplitudes of the state, though this requires input-dependent unitary construction that scales poorly with depth.

## Foundational Learning

**Concept: Quantum Random Access Memory (QRAM)**
- Why needed: The speedups are strictly conditional on data access speeds. QRAM provides O(polylog(N)) access to weights and inputs, which is the bottleneck that quantum algorithms speed up.
- Quick check: If you only have classical RAM (access cost O(N)), does the quadratic speedup in Regime 3 remain, or does it vanish?

**Concept: Block Encoding / Vector Encoding**
- Why needed: This is the data structure of the paper. A "Vector Encoding" (VE) is a unitary where a sub-block approximates a vector. All operations are constructed by manipulating these unitaries so the top-left block contains the desired result.
- Quick check: In a (1, a, ε)-VE for vector |ψ⟩, what does the parameter a represent and how does it affect the normalization of the encoded vector?

**Concept: Spectral Normalization**
- Why needed: The paper requires ||W||_2 ≤ 1 to prove norm stability. Without this, the quantum state amplitude could blow up or vanish, breaking the complexity proofs for residual blocks.
- Quick check: Why is spectral normalization used here instead of the Frobenius norm when regularizing convolutional layers?

## Architecture Onboarding

**Component map:**
Input Layer (QRAM or State Prep) -> Residual Blocks (Conv + Activation + Skip + Norm) -> Output Layer (Linear-Pooling + L2 Norm + Measurement)

**Critical path:**
The Norm Preservation flow. You must track the "norm lower bound" through every layer. If the bound breaks (due to bad polynomial approximation or un-normalized weights), the amplitude amplification step fails or becomes exponentially expensive.

**Design tradeoffs:**
- Regime Selection: Choose between speed and feasibility. Regime 1 (Full QRAM) is theoretically fastest but requires unproven hardware. Regime 3 (No QRAM) is hardware-feasible today but offers only quadratic speedup and requires loading input in O(N).
- Depth vs. Width: The paper advises against depth. Complexity scales with O(polylog^k) or similar exponential factors for k layers. Design must be "Wide and Shallow" (large channel count C, low layer count k).

**Failure signatures:**
- Vanishing Norm: Outputs are all zeros or measurement statistics are empty - norm decayed during non-linearities/skips
- Exponential Blowup: Circuit depth is massive for small N - too many layers (k is too high) or error tolerance ε is too tight
- Frobenius Bottleneck: Using Matrix-Vector Squared block but constructing block-encoding of W first, reintroducing ||W||_F cost

**First 3 experiments:**
1. Verify Single Block Stability: Implement General Skip Norm Block for one layer, verify output norm remains high
2. Compare Data Regimes: Benchmark Matrix-Vector Squared subroutine assuming O(1) QRAM vs O(N) state preparation
3. Polynomial Approximation Error: Implement erf via QSVT on single qubit, plot circuit depth vs approximation error ε

## Open Questions the Paper Calls Out

**Open Question 1:**
Is it provably impossible to coherently enact sequences of non-linear transformations without exponentially increasing circuit depth while maintaining polylogarithmic error-dependence? The authors suspect impossibility but lack a formal no-go theorem.

**Open Question 2:**
Can techniques based on Quantum Phase Estimation (QPE) be utilized to enact sequences of non-linearities without exponentially increasing circuit depth for "medium-depth" architectures (up to 25 layers)? This could combine QPE approaches with the current methodology.

**Open Question 3:**
Can quantum-inspired classical algorithms (dequantization) be applied to accelerate inference for the ResNet-style convolutional blocks presented, specifically those without the final full-rank linear layer? This could yield interesting classical acceleration techniques.

**Open Question 4:**
How can the QRAM-free block-encoding of 2D multi-filter convolutions be optimized to reduce the polynomial dependence on the number of channels (C) and filter size (D)? The current construction has potentially large polynomial overheads.

## Limitations
- Relies on idealized quantum access to data (QRAM) which remains a hardware challenge
- Exponential dependence on network depth makes approach practical only for shallow networks
- Requires strict spectral normalization of weights, limiting compatibility with standard pretrained networks
- Implementation of the "preprocessed matrix QRAM data structure" for dense matrices is not fully specified

## Confidence
- **High Confidence:** Polynomial approximation framework for coherent non-linear activations and basic vector-encoding methodology
- **Medium Confidence:** Norm preservation analysis via residual connections (assumes perfect approximations and ideal QRAM)
- **Medium Confidence:** Specific complexity bounds for Matrix-Vector Squared subroutine (depends on unspecified QRAM data structure details)

## Next Checks
1. Implement a single residual block (convolution + erf + skip) on a simulator to verify norm preservation holds in practice across the sequence of operations
2. Benchmark the "Matrix-Vector Squared" algorithm with varying matrix densities and dimensions to empirically verify the claimed speedup over classical dense matrix multiplication
3. Test the polynomial approximation accuracy vs. circuit depth tradeoff for the erf function to determine practical limits on network depth and error tolerance