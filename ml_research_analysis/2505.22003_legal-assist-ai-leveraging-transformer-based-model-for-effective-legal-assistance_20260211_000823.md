---
ver: rpa2
title: 'Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance'
arxiv_id: '2505.22003'
source_url: https://arxiv.org/abs/2505.22003
tags:
- legal
- assist
- indian
- assistance
- india
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Legal Assist AI, a transformer-based legal
  assistant model fine-tuned on extensive Indian legal datasets, including the Constitution,
  BNS, BNSS, and BSA. The model was evaluated against GPT-3.5 Turbo and Mistral 7B
  on the AIBE examination, achieving a 60.08% score, outperforming competitors in
  legal reasoning accuracy.
---

# Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance

## Quick Facts
- arXiv ID: 2505.22003
- Source URL: https://arxiv.org/abs/2505.22003
- Reference count: 22
- Primary result: Achieved 60.08% on AIBE exam, outperforming GPT-3.5 Turbo (58.72%) and Mistral 7B (23.48%)

## Executive Summary
This paper introduces Legal Assist AI, a transformer-based legal assistant model fine-tuned on extensive Indian legal datasets. The model employs a Retrieval-Augmented Generation (RAG) architecture to answer legal queries while avoiding hallucinations. When tested against GPT-3.5 Turbo and Mistral 7B on the AIBE examination, Legal Assist AI achieved a 60.08% score, demonstrating superior legal reasoning accuracy. The model also achieved a 76.90% BERT score on subjective legal queries from the Lawyer_GPT dataset, showing strong semantic understanding. A key innovation is the model's ability to respond "I don't know" when it lacks relevant information, rather than fabricating answers.

## Method Summary
The approach uses Llama 3.1 8B as the base model, quantized to Q4_0 (~4.7GB) for efficient deployment. Indian legal documents (Constitution, BNS, BNSS, BSA, labor laws, SEBI guidelines, judgments) were downloaded, chunked at 1000 characters with 20-character overlap, and embedded using sentence-transformers/all-MiniLM-L6-v2. A FAISS vector store enables semantic retrieval. The RAG architecture retrieves relevant document chunks before generation, with the system prompt enforcing legal constraints. The model was evaluated on the AIBE dataset (1156 MCQs) and Lawyer_GPT dataset (150 subjective questions), achieving 60.08% accuracy and 76.90% BERT score respectively.

## Key Results
- Achieved 60.08% accuracy on AIBE examination, surpassing GPT-3.5 Turbo (58.72%) and Mistral 7B (23.48%)
- Obtained 76.90% BERT score on subjective legal queries from Lawyer_GPT dataset
- Successfully avoided hallucinations by responding "I don't know" when lacking relevant information
- Demonstrated effective deployment with quantized 8B model (4.7GB) without accuracy loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific retrieval augmentation reduces hallucination by grounding responses in curated legal documents.
- Mechanism: The RAG architecture retrieves semantically similar document chunks from a FAISS-indexed vector store before generation. When the retrieved context lacks relevant information, the model is prompted to respond "I don't know" rather than fabricate.
- Core assumption: The curated legal corpus is sufficiently comprehensive for the query distribution.
- Evidence anchors: Abstract states "retrieves relevant legal information from a curated database and generates accurate responses"; section 4.1.2 shows reduced hallucinations with "I don't know" responses.

### Mechanism 2
- Claim: Fine-tuning on Indian legal texts improves jurisdiction-specific reasoning over general-purpose models.
- Mechanism: The base Llama 3.1 8B model was fine-tuned on a prepared vector store derived from Indian legal documents.
- Core assumption: The fine-tuning corpus quality and coverage directly enable the observed gains.
- Evidence anchors: Abstract mentions fine-tuning on "extensive datasets from the Indian legal domain"; section 3.1 describes downloading legal documents from governmental websites.

### Mechanism 3
- Claim: Quantization (Q4_0) preserves accuracy while enabling deployment in resource-constrained environments.
- Mechanism: The 8B-parameter model was quantized to ~4.7 GB using Q4_0, reducing memory footprint and improving inference speed.
- Core assumption: Quantization error does not materially affect legal reasoning quality.
- Evidence anchors: Section 3.2.3 states "The quantized version... brought down the model size to circa 4.7 GB, which had significantly improved the inference time and efficiency with no loss in high accuracy."

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper's architecture depends on retrieving legal document chunks before generation; understanding RAG is essential to debug retrieval failures and context integration.
  - Quick check question: Can you explain why RAG might reduce hallucination compared to a standalone LLM, and what failure modes it introduces?

- Concept: **Vector Embeddings and Similarity Search**
  - Why needed here: The system uses HuggingFaceEmbeddings (all-MiniLM-L6-v2) and FAISS for semantic retrieval; understanding embedding quality and index behavior is critical for troubleshooting poor retrievals.
  - Quick check question: What happens to retrieval quality if legal documents share similar phrasing but differ in legal effect?

- Concept: **Quantization for LLM Deployment**
  - Why needed here: The model uses Q4_0 quantization; engineers must understand the tradeoffs between memory savings and potential accuracy degradation.
  - Quick check question: What types of tasks are most likely to suffer performance degradation under 4-bit quantization?

## Architecture Onboarding

- Component map: Data Ingestion -> LangChain DirectoryLoader -> chunking (1000 chars, 20 overlap) -> sentence-transformers/all-MiniLM-L6-v2 -> FAISS index -> Retrieval QA Chain -> Llama 3.1 8B (Q4_0 quantized) -> Text response or "I don't know" fallback

- Critical path: Document chunking quality determines retrieval granularity; embedding model choice affects semantic matching; prompt engineering enforces refusal behavior; quantization level impacts inference quality.

- Design tradeoffs: Chunk size (1000 chars) balances context preservation vs. retrieval precision; Q4_0 quantization trades accuracy for deployability; curated corpus limits coverage.

- Failure signatures: Model responds "I don't know" to valid legal queries (retrieval gap or corpus incompleteness); model cites non-existent statutes (embedding or retrieval contamination); inconsistent answers for semantically similar queries (embedding instability or chunk boundary artifacts).

- First 3 experiments: 
  1. Retrieval ablation: Manually inspect top-k retrieved chunks for a sample of AIBE questions; measure retrieval relevance vs. final answer accuracy.
  2. Chunk size sweep: Test 500, 1000, 1500 character chunks on a held-out legal QA set; track BERT score and hallucination rate.
  3. Quantization comparison: Run identical evaluation on full-precision and Q4_0 models; quantify any accuracy delta on complex reasoning questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the volume of case-specific training data lead to performance gains or accuracy degradation in legal reasoning tasks?
- Basis in paper: [explicit] The Discussion section states that while generalization can be improved by adding case-specific data, "increasing the data amount may result in either significant improvements or potential decreases in accuracy."
- Why unresolved: The authors observe a non-linear relationship where more data does not guarantee better performance, yet they have not isolated the variables (data quality vs. quantity) to predict the outcome.
- What evidence would resolve it: An ablation study comparing the current baseline against models trained with incrementally larger case-specific datasets, measuring performance on the AIBE and Lawyer_GPT benchmarks.

### Open Question 2
- Question: How robust is the model's semantic understanding and retrieval accuracy when processing queries in non-English languages?
- Basis in paper: [explicit] The Abstract and Conclusion identify expanding multilingual datasets as a primary objective for future iterations to improve performance.
- Why unresolved: While the base model (Llama 3.1) supports multilingualism, the fine-tuning and vector store construction focused on standard Indian legal documents which are predominantly in English; performance in vernacular languages remains unquantified.
- What evidence would resolve it: Evaluation of the model on a translated subset of the Lawyer_GPT dataset or specific regional legal queries to calculate cross-lingual semantic scores.

### Open Question 3
- Question: How do automated metrics like BERT score correlate with human expert validation in assessing the logical validity of legal arguments?
- Basis in paper: [inferred] The Conclusion suggests supplementing data with "expert reviews," implying that current automated evaluation methods may not fully capture the nuance required for reliable legal assistance.
- Why unresolved: High BERT scores indicate semantic similarity to reference answers but do not necessarily validate the legal logic or the absence of subtle, context-specific errors.
- What evidence would resolve it: A study comparing model outputs rated by legal professionals against the automated BERT scores to determine the correlation coefficient between human judgment and semantic similarity.

## Limitations

- Fine-tuning hyperparameters and complete system prompt are not fully specified, making exact reproduction challenging
- Quantization claims of "no loss in high accuracy" are asserted but not empirically validated through direct comparison with full-precision models
- The curated corpus may have coverage gaps that could produce false refusals on valid but uncommon legal queries
- The 60.08% AIBE score remains below what would be considered expert-level performance (typically 70%+ for practicing lawyers)

## Confidence

- **High Confidence**: The RAG architecture's basic mechanism for reducing hallucinations is well-established in the literature, and the retrieval-refusal pattern is clearly demonstrated in evaluation.
- **Medium Confidence**: The claimed performance improvements over GPT-3.5 Turbo and Mistral 7B are based on specific evaluation datasets, but the fine-tuning effectiveness depends on undocumented hyperparameters and corpus quality assumptions.
- **Low Confidence**: The quantization claim of "no accuracy loss" lacks direct empirical validation, and the model's behavior on complex multi-step legal reasoning or edge cases remains largely untested.

## Next Checks

1. **Retrieval Quality Audit**: Manually examine top-5 retrieved document chunks for 50 randomly sampled AIBE questions to measure semantic relevance and identify potential retrieval failure modes.

2. **Quantization Impact Study**: Run identical evaluation on the full-precision and Q4_0 quantized versions across all metrics, focusing on complex reasoning questions where quantization artifacts are most likely to appear.

3. **Corpus Coverage Analysis**: Systematically test the model on a validation set of Indian legal questions spanning constitutional, criminal, civil, and procedural domains to identify coverage gaps and quantify refusal rates for valid versus invalid queries.