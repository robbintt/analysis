---
ver: rpa2
title: Disentangling Polysemantic Channels in Convolutional Neural Networks
arxiv_id: '2504.12939'
source_url: https://arxiv.org/abs/2504.12939
tags:
- channel
- channels
- concepts
- concept
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of interpreting convolutional\
  \ neural networks (CNNs) by tackling polysemantic channels\u2014those that encode\
  \ multiple unrelated concepts\u2014which hinder mechanistic interpretability. The\
  \ authors propose an algorithm to disentangle such channels into multiple monosemantic\
  \ channels, each responding to a single concept."
---

# Disentangling Polysemantic Channels in Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2504.12939
- Source URL: https://arxiv.org/abs/2504.12939
- Reference count: 29
- One-line primary result: An algorithm that disentangles polysemantic channels in CNNs into multiple monosemantic channels, improving interpretability.

## Executive Summary
This paper addresses the challenge of interpreting convolutional neural networks (CNNs) by tackling polysemantic channels—those that encode multiple unrelated concepts—which hinder mechanistic interpretability. The authors propose an algorithm to disentangle such channels into multiple monosemantic channels, each responding to a single concept. The method leverages the observation that different concepts within the same channel exhibit distinct activation patterns in the preceding layer. By restructuring weights in the CNN, the approach creates new channels that isolate individual concepts, enhancing interpretability and enabling clearer feature visualizations.

## Method Summary
The method identifies polysemantic channels by computing Average Relevance Vectors (ARVs) that capture which channels in layer l−1 contribute to activating a target channel c in layer l, averaged over images from each class. When ARVs for two classes have low cosine similarity (below threshold γ), it indicates the channel detects different concepts rather than a shared one. Disentanglement is achieved by inserting an artificial layer with selectively masked weights—a new layer l′ is inserted between l−1 and l. For each concept, weights from channels in l−1 that are predominantly relevant to the other concept are zeroed out, creating two monosemantic channels. A third "residual" channel with weight −1 cancels double-counted features.

## Key Results
- Disentangled channels produce more interpretable feature visualizations compared to original polysemantic channels
- Quantitative analysis confirms disentangled channels accurately mimic original channel's activation for one concept while suppressing the other
- Residual channel remains largely inactive, indicating minimal information loss after disentanglement
- Method successfully disentangles channels in ResNet-50 trained on ImageNet, with polysemanticity identified based on relevance to multiple classes

## Why This Works (Mechanism)

### Mechanism 1
Polysemantic channels can be identified by comparing how different classes activate the preceding layer. The method computes Average Relevance Vectors (ARVs) that capture which channels in layer l−1 contribute to activating a target channel c in layer l, averaged over images from each class. When ARVs for two classes have low cosine similarity (below threshold γ), it indicates the channel detects different concepts rather than a shared one. Distinct concepts rely on distinct activation patterns in the preceding layer; semantically similar concepts have similar ARVs.

### Mechanism 2
Inserting an artificial layer with selectively masked weights disentangles concepts without changing network output. A new layer l′ is inserted between l−1 and l. For each concept, weights from channels in l−1 that are predominantly relevant to the other concept are zeroed out, creating two monosemantic channels. A third "residual" channel with weight −1 cancels double-counted features. Attribution scores reliably indicate which preceding-layer channels contribute to each concept; attribution is approximately decomposable.

### Mechanism 3
The residual channel preserves the original activation by subtracting double-encoded features. Since both disentangled channels retain weights for features not exclusively associated with either concept, these are encoded twice. The residual channel connects with weight −1 and only keeps "neutral" edges, canceling the double-count and ensuring original channel activation is recoverable. The union of disentangled channels' activations equals original activation plus residual; residual is small if disentanglement captures most variance.

## Foundational Learning

- **Attribution/Relevance Methods (e.g., Input×Gradient, LRP, Integrated Gradients)**: The entire method relies on computing which preceding-layer channels are "relevant" to a target channel for specific classes. Quick check: Can you explain why Input×Gradient gives a signed relevance score and what a positive vs. negative value means?

- **Cosine Similarity**: Used to compare ARVs; low similarity indicates distinct activation patterns across classes. Quick check: Given two vectors [1,0,1] and [1,1,0], what is their cosine similarity? What does a value near 0 imply?

- **Superposition and Non-privileged Bases in Neural Networks**: Explains why polysemanticity arises—networks may encode more concepts than dimensions by superimposing them. Quick check: Why might a network prefer to superimpose features rather than dedicate one neuron per feature?

## Architecture Onboarding

- **Component map**: CNN -> Attribution Module -> ARV Computation -> γ-Polysemanticity Detector -> Disentanglement Engine -> Evaluation Pipeline
- **Critical path**: 1) Select layer l of interest (penultimate layer recommended for semantic richness). 2) For each class, compute per-channel relevance to final logits to find candidate channels. 3) For candidate channels, compute ARVs for relevant class pairs. 4) Apply γ threshold to identify polysemantic channels. 5) For each polysemantic channel, construct l′ with 3 replacement channels and selectively zero weights. 6) Verify via activation density plots and feature visualizations.
- **Design tradeoffs**: γ threshold: Lower values increase precision (fewer false positives) but reduce coverage; γ=0.5 was chosen based on WordNet similarity correlation. ρ parameter: Controls strictness of "concept-relevant" preceding channels; auto-tuned to maximize activation separation. Attribution method choice: Input×Gradient is simple but may be noisy; LRP/IG could improve accuracy at computational cost.
- **Failure signatures**: Disentangled channels still activate on both concepts: Suggests attribution failed to separate concepts or ρ is too permissive. Residual channel has high activation: Indicates significant features not captured by either disentangled channel. Network output changes: Weight restructuring bug; check identity/−1 mappings.
- **First 3 experiments**: 1) Replicate channel #1660 disentanglement: Apply pipeline to "digital clock" and "cauliflower" classes; verify activation density separation and MACO visualization improvement. 2) Ablate attribution method: Replace Input×Gradient with LRP or IG on a subset of channels; compare disentanglement quality (activation ratios). 3) Vary γ threshold: Sweep γ ∈ [0.3, 0.7] and measure precision/recall against WordNet similarity proxy; confirm 0.5 is reasonable or find task-specific optimum.

## Open Questions the Paper Calls Out

### Open Question 1
Can the disentanglement algorithm be generalized to handle channels representing more than two concepts simultaneously without requiring recursive application? The authors state the method is "Currently only applied to the common form of disentanglement of two concepts" and suggest recursive application as a future extension. Successful single-pass disentanglement of a channel relevant to three or more distinct classes into individual monosemantic channels would resolve this.

### Open Question 2
How can the critical hyperparameters (τ, γ, ρ) be set automatically to ensure robustness across different network architectures and layers? The conclusion notes that "Future work could further explore the impact of these hyperparameters and potential methods for setting them automatically." An adaptive mechanism for selecting these parameters that maintains quantitative disentanglement fidelity without manual tuning would resolve this.

### Open Question 3
Does the reliance on class labels for defining γ-polysemanticity limit the identification of polysemanticity in earlier layers where features are less semantically aligned with output classes? The authors acknowledge, "Our current formulation of polysemanticity is based on class labels, thus, our proposed disentanglement may work better in later layers." Analysis of disentanglement success rates in early layers compared to the penultimate layer, or an adaptation of the method using unsupervised clustering, would resolve this.

## Limitations

- The method relies heavily on Input×Gradient attributions, which may be noisy or fail to separate concepts cleanly
- Scalability to deeper architectures and earlier layers is uncertain, as results are demonstrated only on ResNet-50's penultimate layer
- The relationship between activation separation and human interpretability is not rigorously established beyond qualitative visualizations

## Confidence

- **Polysemantic channel identification**: High - The ARV-based identification method is well-grounded in the observation that distinct concepts activate different preceding-layer patterns
- **Disentanglement mechanism**: Medium - The weight restructuring approach is technically sound but confidence is reduced by sensitivity to attribution quality
- **Interpretability improvements**: Medium - Qualitative visualizations support the claim, but quantitative metrics beyond activation ratios are limited

## Next Checks

1. **Attribution method ablation**: Repeat the disentanglement pipeline using LRP and Integrated Gradients instead of Input×Gradient on the same set of polysemantic channels. Compare activation ratios and feature visualization quality to assess attribution method sensitivity.

2. **Residual channel analysis**: For each disentangled channel, measure the correlation between residual activation magnitude and classification accuracy drop. If residual activation correlates with performance degradation, this indicates the residual channel is capturing meaningful information that the disentangled channels miss.

3. **Cross-layer validation**: Apply the disentanglement method to intermediate layers (not just penultimate) and measure how γ-threshold performance changes. If polysemanticity detection accuracy varies significantly across layers, this suggests the method has layer-specific limitations that need addressing.