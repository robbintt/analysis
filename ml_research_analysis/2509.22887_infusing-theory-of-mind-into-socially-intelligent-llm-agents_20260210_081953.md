---
ver: rpa2
title: Infusing Theory of Mind into Socially Intelligent LLM Agents
arxiv_id: '2509.22887'
source_url: https://arxiv.org/abs/2509.22887
tags:
- goal
- toma
- agent
- agents
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ToMAgent (TOMA), a method for improving LLM
  social reasoning by integrating Theory of Mind (ToM). TOMA combines explicit ToM
  predictions with dialogue lookahead simulation to select mental states and utterances
  that best achieve social goals.
---

# Infusing Theory of Mind into Socially Intelligent LLM Agents

## Quick Facts
- arXiv ID: 2509.22887
- Source URL: https://arxiv.org/abs/2509.22887
- Reference count: 40
- One-line primary result: TOMA achieves up to 18.9% and 6.9% score improvements over best base model variants for Qwen2.5-3B and Qwen2.5-7B respectively on Sotopia benchmark

## Executive Summary
This paper introduces TOMA (ToM Agent), a method that improves LLM social reasoning by integrating Theory of Mind (ToM) with dialogue lookahead simulation. The approach generates multiple mental state hypotheses per dialogue turn, simulates future conversations using a partner model, and retains high-scoring state-utterance pairs for fine-tuning. Experiments on the Sotopia benchmark demonstrate that TOMA outperforms baseline models significantly, achieving substantial improvements in goal-oriented social dialogue while maintaining better relationships and exhibiting more strategic, long-horizon adaptation behaviors.

## Method Summary
TOMA combines explicit Theory of Mind predictions with dialogue lookahead simulation to select mental states and utterances that best achieve social goals. During training, the model generates K mental state hypotheses per dialogue turn, simulates 4 future turns using a partner model, and scores goal achievement for both agents. High-scoring pairs (average ≥9/10) are retained for fine-tuning. The training objective jointly optimizes for mental state prediction and utterance prediction conditioned on mental states, forcing the model to learn mental states that are both predictive of partner behavior and useful for generating effective utterances.

## Key Results
- TOMA achieves up to 18.9% and 6.9% score improvements over best base model variants for Qwen2.5-3B and Qwen2.5-7B respectively
- TOMA maintains better relationships and exhibits more strategic, long-horizon adaptation behaviors compared to baselines
- TOMA generates more first-order beliefs (+6.3%) and focuses on intentions over emotions, leading to improved social goal achievement across diverse scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit mental state generation improves goal achievement by making social reasoning tractable as an intermediate supervision signal.
- **Mechanism:** The model generates mental state hypotheses (covering beliefs, desires, intentions, emotions, knowledge) before utterances, creating a latent reasoning trace that conditions utterance generation. This decomposes the direct context→utterance mapping into context→mental states→utterances.
- **Core assumption:** Mental states provide a useful intermediate representation that captures social dynamics better than end-to-end utterance generation alone.
- **Evidence anchors:** [abstract] "ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals." [section 2.2] "We prompt an LM to generate K mental state hypotheses... The model is asked to ensure that each generated hypothesis covers at least three out of five ToM dimensions."
- **Break condition:** If mental states are too generic or hallucinated without grounding in conversation context, they provide no useful signal and may mislead utterance generation.

### Mechanism 2
- **Claim:** Dialogue lookahead simulation enables credit assignment from goal achievement back to earlier mental state-utterance pairs.
- **Mechanism:** For each (mental state, utterance) candidate pair, TOMA simulates 4 future turns using a partner model, then scores goal achievement for both agents. High-scoring pairs (avg ≥9/10) are retained for training. This creates outcome-supervised training data without human labels.
- **Core assumption:** The partner model (Qwen2.5-14B) and scorer (Gemini-Flash) provide sufficiently accurate simulations and evaluations to identify genuinely useful strategies.
- **Evidence anchors:** [abstract] "ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals." [section 2.2] "To identify useful mental states and utterances... we run short-horizon simulations... and assess how each pair influences the goal achievement."
- **Break condition:** If simulations diverge from real interaction dynamics (partner model too weak or scorer misaligned), selected pairs optimize for simulated rather than actual goal achievement.

### Mechanism 3
- **Claim:** Joint training on mental states and utterances creates an aligned internal policy where better ToM reasoning leads to better social outcomes.
- **Mechanism:** The training objective L = -log P(m|H) - log P(u|m, H) trains the model on both mental state prediction and utterance prediction conditioned on mental states. This forces the model to learn mental states that are both predictive of partner behavior AND useful for generating effective utterances.
- **Core assumption:** The selected high-scoring pairs contain learnable patterns that generalize beyond the training scenarios.
- **Evidence anchors:** [section 2.3] "We train the model to align with the joint behavior P(u, m|H) = P(u|m, H) · P(m|H) that led to high goal scores." [section 4.1] "TOMA... achieves the best of both worlds, effectively maintaining relationships, knowledge seeking, and goal-oriented behavior."
- **Break condition:** If the training set has low diversity (only 500 episodes sampled) or selection threshold is too permissive, the model may overfit to specific scenario patterns.

## Foundational Learning

- **Concept: Theory of Mind (ToM) dimensions**
  - Why needed here: TOMA explicitly models 5 dimensions—beliefs, desires, intentions, emotions, knowledge—as structured latent variables. Understanding these categories is essential for debugging mental state generation quality.
  - Quick check question: Given a negotiation scenario, can you distinguish an agent's *desire* (what they want) from their *intention* (what they plan to do)?

- **Concept: First-order vs. zero-order mental states**
  - Why needed here: The paper shows TOMA generates more first-order beliefs (about others' mental states) than baseline (+6.3%). This shift toward other-modeling is key to improved social reasoning.
  - Quick check question: "I want the blanket" is zero-order. What would a first-order belief about this scenario look like?

- **Concept: LoRA (Low-Rank Adaptation) fine-tuning**
  - Why needed here: TOMA uses LoRA for efficient fine-tuning of Qwen2.5-3B/7B. Understanding rank/alpha hyperparameters is necessary for reproducing or extending experiments.
  - Quick check question: What happens to model capacity if LoRA rank is set too low for a complex social reasoning task?

## Architecture Onboarding

- **Component map:** Sotopia-Pi dataset (scenarios + goals) -> Context sampling (scenario, goals, partial history) -> Mental state generation (K=2 hypotheses per turn) -> Utterance generation (J=2 per mental state) -> Dialogue simulation (4 turns, partner model) -> Goal scoring (Gemini-Flash, avg score threshold ≥9) -> Training data collection (high-scoring pairs) -> LoRA fine-tuning (joint MS + utterance objective)

- **Critical path:** The simulation-scoring loop (steps 4-6) determines training data quality. If this pipeline produces noisy or misaligned examples, downstream fine-tuning will amplify errors.

- **Design tradeoffs:**
  - K=2, J=2 keeps compute tractable but limits exploration. Paper notes this is a computational constraint.
  - 4-turn lookahead balances planning depth vs. simulation reliability (longer = more drift from partner model errors).
  - Score threshold ≥9 is strict; if no pairs qualify, top pair is kept (fallback may introduce noise).

- **Failure signatures:**
  - Model generates repetitive mental states → exploration insufficient, may need higher temperature or more hypotheses.
  - Goal scores high but relationship scores low → simulation rewards goal achievement at expense of social norms; scorer may need reweighting.
  - Base model outperforms TOMA on "easy" cooperation scenarios → overfitting to conflict/negotiation patterns in training data.

- **First 3 experiments:**
  1. **Ablate mental state training:** Train FT+Uttr only (utterances without MS supervision) on same data. Compare goal scores to TOMA to quantify MS contribution.
  2. **Vary lookahead depth:** Test K∈{1,2,4} and lookahead turns ∈{2,4,6} to find compute/performance sweet spot.
  3. **Cross-partner generalization:** Evaluate TOMA agent against partner models of different sizes (3B vs 32B) to test robustness beyond self-play setup.

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation pipeline reliability depends on partner model accuracy and LLM judge scoring quality
- Generalization across partner types remains unverified with limited testing against independent partners
- Computational constraints limited K/J values and lookahead depth, potentially preventing discovery of optimal configurations

## Confidence

**High confidence:** The methodology for generating mental state hypotheses and the joint training objective are clearly specified and technically sound. The improvement over baseline models (18.9% and 6.9% on average goal scores) is substantial and well-documented across multiple metrics.

**Medium confidence:** The simulation-based training pipeline produces reliable training signals. While the approach is logically sound and supported by related work on counterfactual simulation, the dependence on LLM judge quality and partner model behavior introduces potential variability.

**Low confidence:** Claims about TOMA's superiority over GPT-5-nano are based on very limited testing (few instances). The computational constraints that limited K/J values and lookahead depth may have prevented discovering optimal configurations.

## Next Checks

1. **Partner model robustness test:** Evaluate TOMA against a diverse set of partner models including GPT-4, Claude, and open-weight models of varying sizes (not just self-play with Qwen2.5-14B) to verify generalization beyond the training distribution.

2. **Training pipeline ablation:** Systematically vary K, J, and lookahead depth while measuring training data diversity and downstream performance to identify optimal computational tradeoffs and ensure the selected configuration isn't suboptimal.

3. **Cross-dataset generalization:** Test TOMA on established ToM benchmarks like Hintikkamult and SyntheticAvenue (not just Sotopia) to verify that improvements transfer to broader social reasoning tasks beyond the specific scenario types in the training data.