---
ver: rpa2
title: Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management
arxiv_id: '2602.02283'
source_url: https://arxiv.org/abs/2602.02283
tags:
- error
- page
- where
- q-learning
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces choice-model-assisted RL for revenue management\
  \ with delayed feedback, where a calibrated discrete choice model is used as a fixed\
  \ partial world model to impute delayed learning targets at decision time. In a\
  \ fixed-model deployment regime, the authors prove that tabular Q-learning with\
  \ model-imputed targets converges to an O(\u03B5/(1-\u03B3)) neighborhood of the\
  \ optimal Q-function, with an additional O(t^{-1/2}) sampling term."
---

# Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management

## Quick Facts
- **arXiv ID:** 2602.02283
- **Source URL:** https://arxiv.org/abs/2602.02283
- **Reference count:** 40
- **Primary result:** CA-DQN with model-imputed targets converges to O(ε/(1-γ)) neighborhood of optimal Q-function; no stationary difference from MB-DQN baseline, gains under in-family shifts (5/10 scenarios, up to 12.4%), degradation under structural misspecification (1.4-2.6% loss)

## Executive Summary
This paper introduces choice-model-assisted reinforcement learning for revenue management with delayed feedback, where a calibrated discrete choice model (DCM) is used as a fixed partial world model to impute delayed reward components at decision time. The approach enables immediate Q-learning updates without waiting for outcomes to mature, theoretically converging to an O(ε/(1-γ)) neighborhood of the optimal Q-function with O(t^{-1/2}) sampling noise. Experiments in a simulator calibrated from 61,619 hotel bookings show no stationary difference from a maturity-buffer DQN baseline, positive effects under in-family parameter shifts in 5 of 10 scenarios (up to 12.4% gains), and consistent degradation under structural misspecification (1.4-2.6% lower revenue).

## Method Summary
The method uses a pre-trained multinomial logit (MNL) discrete choice model to impute delayed reward components at decision time. For each state-action pair, synthetic samples are generated using the DCM to predict shock outcomes (cancellations, modifications) from order features available at booking time. These imputed transitions enable immediate Q-learning updates without waiting for outcomes to mature. The theoretical framework proves convergence of tabular Q-learning to an O(ε/(1-γ)) neighborhood of optimal Q-function, where ε represents DCM approximation error. Experiments use a two-layer MLP DQN (128 hidden units, ~22K parameters) with model-imputed sampling, comparing against a maturity-buffer baseline that waits for delayed outcomes before updating.

## Key Results
- No statistically detectable difference from maturity-buffer DQN baseline in stationary settings (TOST equivalence with ±5% margin)
- Positive effects under in-family parameter shifts in 5 of 10 scenarios after Holm-Bonferroni correction (up to 12.4% revenue gains)
- Consistent degradation under structural misspecification where DCM assumptions violated (1.4-2.6% lower revenue)

## Why This Works (Mechanism)

### Mechanism 1: Model-Imputed Sampling Enables Immediate Credit Assignment
A calibrated discrete choice model can impute delayed reward components at decision time, enabling Q-learning updates without waiting for outcomes to mature. The total learning label decomposes as r_t = r_imm^t + r_del^t, and the DCM predicts shock outcomes from order features known at booking time.

### Mechanism 2: Bounded Error Propagation via Simulation Lemma
With a fixed pre-trained DCM, tabular Q-learning converges to an O(ε/(1-γ)) neighborhood of the optimal Q-function, plus sampling noise O(t^{-1/2}). The approximate MDP induced by the DCM has bounded reward and transition error, which propagates to the Q-function via the simulation lemma.

### Mechanism 3: Parametric Structure Provides Extrapolation Guarantees Under In-Family Shifts
Under distributional shift with feature distance δ_φ from training support, DCM error grows as O(ε_train + (L_m + L_r)δ_φ), which is strictly better than worst-case O(R_max) when δ_φ is small. This explains gains under parameter shifts that preserve MNL structure.

## Foundational Learning

- **Q-learning convergence under approximate models**: Understanding how Q-learning behaves when transitions/rewards come from an approximate MDP rather than the true environment. Quick check: If DCM has 10% reward error, why is Q-function error O(ε/(1-γ)) rather than O(ε)?
- **Multinomial Logit (MNL) choice models**: The DCM uses MNL structure for both booking decisions and shock outcomes. Quick check: If two room types are perceived as similar (correlated utilities), which MNL assumption is violated and what would you expect empirically?
- **Delayed feedback and credit assignment**: The core problem is assigning credit to pricing actions when outcomes arrive 1-14 days later. Quick check: Without model imputation, what is the baseline approach and what is its sample-efficiency cost?

## Architecture Onboarding

- **Component map:** DCM (47-parameter MNL) -> Model-Imputed Sampler -> Q-Network (2-layer MLP, 128 units) -> Replay Buffer -> DQN Training Loop
- **Critical path:** 1) Pre-train DCM on historical bookings 2) At deployment: observe state, select action 3) Observe immediate outcomes, compute r_imm^t 4) Impute delayed component using DCM 5) Store synthetic (r'_t, s'_{t+1}) immediately 6) Standard DQN training loop
- **Design tradeoffs:** Fixed vs adaptive DCM (paper uses fixed for realistic recalibration intervals); model-imputed sampling vs doubly-robust targets (sampling chosen for simplicity); tabular theory vs DQN practice (gap acknowledged)
- **Failure signatures:** Structural misspecification (-1.4% to -2.6% underperformance); high demand/competition shifts (-3.9% to -9.6% degradation); calibration drift if ∥P_hat - P_observed∥₁ exceeds threshold
- **First 3 experiments:** 1) Stationary validation: CA-DQN vs MB-DQN on held-out episodes, expect no significant difference 2) In-family shift robustness: Test 10 shift scenarios, expect 5/10 improvements after Holm-Bonferroni correction 3) Misspecification stress test: Deploy against nested logit/mixture simulators, expect consistent underperformance

## Open Questions the Paper Calls Out

### Open Question 1
Does online adaptive DCM recalibration improve performance over fixed-DCM deployment in practice? The theoretical analysis covers online updates, but all experiments use fixed pre-trained DCMs. An empirical evaluation of online DCM updates in synthetic settings would strengthen theory-practice connection.

### Open Question 2
Why does CA-DQN degrade under high demand (+15%) and high competition (+15%) shifts while improving under low demand (-15%, -50%) and low competition (-15%, -30%) shifts? The paper documents this asymmetry but doesn't explain the mechanism, which is left for future work.

### Open Question 3
How should practitioners detect structural misspecification during deployment to trigger fallback to MB-DQN? The paper establishes misspecification causes degradation but doesn't validate automated detection mechanisms or specify thresholds for fallback rules.

## Limitations
- Theoretical convergence guarantee applies to tabular Q-learning, while experiments use neural DQN (approximation gap acknowledged)
- Structural misspecification effects only demonstrated against synthetic simulators, not real-world choice behavior
- Mechanism behind CA-DQN degradation under high-demand/competition shifts is unexplained
- Fixed-model deployment assumption limits applicability; adaptive variant not empirically validated

## Confidence

- **High:** Tabular Q-learning convergence with model-imputed targets; in-family shift robustness (5/10 gains); structural misspecification degradation
- **Medium:** DQN empirical results (neural approximation gap acknowledged); extrapolation bounds under distributional shift
- **Low:** Adaptive DCM variant (only theoretical); unexplained high-demand/competition shift degradation

## Next Checks

1. **Tabular vs Neural Gap:** Implement tabular Q-learning CA variant on simplified 2-state/2-action problem to verify O(ε/(1-γ)) convergence, then compare against DQN performance
2. **Real-World Misspecification Test:** Apply CA-DQN to publicly available choice modeling dataset and test against nested logit as ground truth to quantify IIA violation effects
3. **Adaptive DCM Validation:** Implement adaptive variant with parameter updates every T episodes, measure trade-off between DCM accuracy and Q-learning stability, validate Theorem 7 empirically