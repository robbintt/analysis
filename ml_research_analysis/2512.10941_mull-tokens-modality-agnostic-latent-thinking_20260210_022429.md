---
ver: rpa2
title: 'Mull-Tokens: Modality-Agnostic Latent Thinking'
arxiv_id: '2512.10941'
source_url: https://arxiv.org/abs/2512.10941
tags:
- reasoning
- image
- text
- answer
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mull-Tokens, a simple and effective method
  for multimodal reasoning using modality-agnostic latent tokens. Unlike prior approaches
  that require explicit interleaving of text and image thoughts or generating intermediate
  images, Mull-Tokens allow models to flexibly reason in a unified latent space.
---

# Mull-Tokens: Modality-Agnostic Latent Thinking

## Quick Facts
- arXiv ID: 2512.10941
- Source URL: https://arxiv.org/abs/2512.10941
- Reference count: 40
- Key outcome: +3% average accuracy gain on visual reasoning tasks, up to +16% on reasoning-heavy splits

## Executive Summary
Mull-Tokens introduces a method for multimodal reasoning using modality-agnostic latent tokens that flexibly interleave reasoning steps without explicit image generation or verbose text chains. The approach trains in three stages: multimodal warm-up aligns tokens with reasoning traces, relaxed fine-tuning optimizes only final answers, and GRPO refinement strengthens causally useful trajectories. Evaluated on spatial reasoning benchmarks, Mull-Tokens achieve significant accuracy gains while being faster than image generation baselines. The method is faster and more efficient than generating explicit images or verbose text chains, while maintaining high accuracy.

## Method Summary
Mull-Tokens adds K special latent tokens to a frozen multimodal encoder (Qwen2.5-VL) and processes them through self-attention to create a unified reasoning chain. Training occurs in three stages: (1) warm-up with interleaved text-image reasoning traces using dual supervision (cross-entropy for text, cosine similarity for images), (2) relaxed fine-tuning optimizing only final answers, and (3) GRPO refinement rewarding causally effective latent trajectories. The approach avoids explicit image generation or verbose text reasoning, enabling efficient multimodal reasoning.

## Key Results
- +3% average accuracy improvement on visual reasoning benchmarks
- Up to +16% improvement on reasoning-heavy splits (BLINK, SAT-Real)
- Outperforms explicit image generation and text-only chain-of-thought baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal anchoring during warm-up is necessary for effective latent reasoning tokens.
- Core assumption: Semantic alignment learned during supervised warm-up transfers to unsupervised optimization in Stage 2.
- Evidence: Text-only warm-up yields only +1.07% over baseline; multimodal warm-up yields +3.05% (Table 2).

### Mechanism 2
- Claim: Relaxing intermediate supervision enables the model to discover task-optimal latent trajectories.
- Core assumption: The warm-up phase provides sufficient initialization for the model to navigate the latent space meaningfully.
- Evidence: Performance scales with token count after Stage 2, suggesting learned utility (Figure 4b).

### Mechanism 3
- Claim: GRPO refinement strengthens the causal contribution of the latent chain to final answers.
- Core assumption: The reward signal is sufficient to shape latent representations without explicit supervision.
- Evidence: Performance scales more positively with token count after GRPO than after SFT alone (Figure 4c).

## Foundational Learning

- Concept: **Latent Reasoning / Pause Tokens**
  - Why needed here: Mull-Tokens build on prior work using pause tokens or latent tokens for reasoning. Understanding that tokens can serve as compute rather than output is essential.
  - Quick check question: Can you explain why adding tokens with no explicit semantic content could improve model performance?

- Concept: **Interleaved Multimodal Chain-of-Thought**
  - Why needed here: Stage 1 supervision comes from datasets where reasoning alternates between text descriptions and subgoal images.
  - Quick check question: How would you construct a training example where a reasoning step is an image rather than text?

- Concept: **Discrete vs. Continuous Latent Representations**
  - Why needed here: The paper explicitly compares discrete Mull-Tokens against continuous recurrent embeddings, finding discrete tokens superior in both performance and efficiency.
  - Quick check question: What are the tradeoffs between recurrent continuous embeddings and discrete tokens processed via self-attention?

## Architecture Onboarding

- Component map: Frozen Qwen2.5-VL image encoder + text tokenizer -> K Mull-Tokens appended after question -> Transformer backbone -> Answer head (and optional dual supervision heads in Stage 1)

- Critical path:
  1. Encode question + image via frozen encoder
  2. Append K Mull-Tokens to the sequence
  3. Pass through transformer; hidden states at Mull-Token positions become h_Mull
  4. Answer tokens attend to h_Mull via self-attention
  5. Loss: supervised on Mull-Tokens (Stage 1), only on answers (Stage 2), reward-based via GRPO (Stage 3)

- Design tradeoffs:
  - K=20 tokens used during training; 10-40 optimal at inference (Figure 4b)
  - Discrete tokens preferred over continuous embeddings for parallelism and stability
  - Must balance interleaved CoT data (for warm-up) with direct-answer data (for generalization)

- Failure signatures:
  - Performance drops if model is forced to use image thoughts explicitly (Table 4: -3% average)
  - Text-only warm-up yields marginal gains (+1.07% vs +3.05% for multimodal)
  - Too many latent tokens can hurt performance (Figure 4b shows degradation at high K)

- First 3 experiments:
  1. Baseline sanity check: Reproduce direct-answer fine-tuning and text-CoT baselines on a held-out split to verify measurement consistency.
  2. Warm-up ablation: Train Mull-Tokens with text-only CoT data vs. interleaved image-text data; compare performance on spatial reasoning benchmarks.
  3. Token count sweep: Vary K (5, 10, 20, 40) at inference time after Stage 2 and Stage 3 to verify scaling behavior matches Figure 4.

## Open Questions the Paper Calls Out

- Can Mull-Tokens generalize to modalities beyond image and text, such as 3D point clouds, audio, or structured data?
- Do Mull-Tokens transfer effectively to other model backbones and larger scales beyond Qwen2.5-VL (7B)?
- What semantic content do individual Mull-Tokens encode, and can they be meaningfully decoded into human-interpretable representations?
- Why does supervised text-based CoT fine-tuning hurt performance compared to direct answer fine-tuning on visual reasoning tasks?

## Limitations
- Warm-up stage requires rare interleaved multimodal reasoning traces, making training data construction challenging
- Only tested on Qwen2.5-VL (7B) architecture; generalization to other backbones and scales unverified
- Limited interpretability of latent tokens' semantic content despite task performance

## Confidence

- **High Confidence**: Mull-Tokens improve spatial reasoning accuracy when trained on interleaved multimodal CoT data; Stage 1 warm-up is necessary for performance gains
- **Medium Confidence**: Relaxed supervision in Stage 2 allows the model to discover better latent trajectories; GRPO in Stage 3 causally reinforces correct reasoning
- **Low Confidence**: General claims of modality-agnostic reasoning extend beyond spatial tasks; the superiority of discrete tokens over continuous embeddings is robustly justified

## Next Checks

1. **Warm-up Data Ablation**: Train Mull-Tokens using only text reasoning traces (no image subgoals) and compare spatial reasoning accuracy to fully interleaved training. Verify if +1% vs. +3% gain difference replicates.

2. **Stage 2 Token Scaling**: At inference, sweep Mull-Token count K from 5 to 40 after Stage 2. Confirm performance peaks around K=20 and degrades at higher counts, matching Figure 4b.

3. **Reward Function Impact**: Implement GRPO with multiple reward schemes (exact-match, normalized absolute error, smoothed rewards). Measure whether Stage 3 refinement consistently improves accuracy or if gains depend on reward design.