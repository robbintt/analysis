---
ver: rpa2
title: 'A*-Decoding: Token-Efficient Inference Scaling'
arxiv_id: '2505.13672'
source_url: https://arxiv.org/abs/2505.13672
tags:
- search
- decoding
- wang
- inference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes A-Decoding, a search-based inference-time strategy
  that builds on the A search algorithm to optimally utilize a fixed compute budget
  by prioritizing high-quality reasoning paths during language model generation. The
  method frames language model decoding as a structured search in a state space of
  partial solutions, applying the A transition model to identify promising continuations
  guided by an external process supervision signal.
---

# A*-Decoding: Token-Efficient Inference Scaling

## Quick Facts
- arXiv ID: 2505.13672
- Source URL: https://arxiv.org/abs/2505.13672
- Authors: Giannis Chatziveroglou
- Reference count: 40
- Primary result: A*-Decoding achieves competitive reasoning accuracy while generating up to 3x fewer tokens and 30% fewer inference passes under fixed compute budgets

## Executive Summary
A*-Decoding is a search-based inference-time scaling method that applies the A* search algorithm to structured language model generation. By framing decoding as a search in a state space of partial solutions, the method prioritizes high-quality reasoning paths using an external process supervision signal. This approach enables small language models to match or exceed the performance of much larger models on mathematical reasoning benchmarks while significantly reducing token generation and inference passes.

## Method Summary
A*-Decoding treats language model decoding as a structured search problem in the state space of partial solutions. It applies the A* transition model to identify promising continuations guided by an external process supervision signal. The method builds a search tree of partial solutions, expanding only the most promising paths based on their estimated quality. During inference, A*-Decoding maintains a priority queue of candidate states and expands them according to their heuristic scores, effectively focusing compute on high-quality reasoning trajectories while avoiding wasteful exploration of poor paths.

## Key Results
- A 1B-parameter Llama model matches the performance of a 70B-parameter baseline on MATH500 and AIME 2024 benchmarks
- A 1.7B-parameter Qwen model achieves OpenAI o1-like reasoning accuracy on the same benchmarks
- Generates up to 3x fewer tokens and 30% fewer inference passes compared to best-of-N, self-consistency, and particle filtering baselines under equivalent compute budgets

## Why This Works (Mechanism)
A*-Decoding leverages the A* search algorithm's ability to find optimal paths in state spaces by combining path cost with heuristic estimates of remaining distance to goal. In language model inference, each decoding step represents a state transition, and the algorithm maintains a priority queue of partial solutions ordered by their estimated total quality. The external process supervision signal provides the heuristic function that estimates how promising each partial solution is, allowing the search to focus computational resources on the most likely successful reasoning paths while pruning unpromising branches early.

## Foundational Learning
- **A* Search Algorithm**: An informed search algorithm that finds optimal paths by combining actual path cost with heuristic estimates of remaining distance. Needed to efficiently explore the solution space while avoiding exhaustive search. Quick check: Verify that the heuristic function is admissible (never overestimates the true cost).
- **Process Supervision**: External signals that evaluate intermediate reasoning steps rather than just final answers. Critical for guiding the search toward high-quality solution paths. Quick check: Ensure the supervision signal provides consistent quality assessments across different problem types.
- **Token-Efficient Inference**: Strategies that reduce the number of generated tokens while maintaining or improving accuracy. Essential for practical deployment where computational resources are limited. Quick check: Compare token counts against baseline methods under identical accuracy targets.
- **Structured Search in Language Generation**: Treating language model output as paths in a structured state space rather than independent token predictions. Enables more sophisticated search strategies beyond greedy or sampling-based approaches. Quick check: Verify that the state space representation captures all relevant aspects of the reasoning process.

## Architecture Onboarding

**Component Map**: Language Model -> A* Search Manager -> Priority Queue -> Process Supervision Signal -> Heuristic Function

**Critical Path**: Input prompt → Language Model → Token generation → State expansion → Heuristic evaluation → Priority queue update → Next state selection

**Design Tradeoffs**: 
- Search depth vs. computational overhead: Deeper searches find better solutions but require more memory and computation
- Heuristic quality vs. supervision cost: Better heuristics improve search efficiency but may require expensive supervision signals
- Model size vs. search complexity: Smaller models benefit more from search but may need more aggressive pruning

**Failure Signatures**:
- Getting stuck in local optima when the heuristic function is poorly calibrated
- Excessive memory usage when the priority queue grows too large
- Degraded performance when process supervision signals are noisy or inconsistent
- Slower convergence when the heuristic function provides weak guidance

**Exactly 3 First Experiments**:
1. Compare A*-Decoding performance with varying maximum search depths on a subset of MATH500 problems
2. Test the sensitivity of results to different heuristic functions (e.g., random vs. learned vs. rule-based)
3. Evaluate the impact of priority queue size limits on both accuracy and computational efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to mathematical reasoning benchmarks (MATH500, AIME 2024), with unknown performance on other domains
- Computational overhead and memory requirements not fully characterized, raising questions about practical deployment
- Strong dependence on the quality and availability of external process supervision signals

## Confidence
- High confidence in mathematical reasoning performance claims due to strong benchmark results
- Medium confidence in token efficiency claims, pending detailed runtime and memory profiling
- Low confidence in cross-domain generalization due to narrow evaluation scope
- Low confidence in supervision signal robustness across different problem types and model architectures

## Next Checks
1. **Domain Generalization Test**: Evaluate A*-Decoding on non-mathematical benchmarks such as code generation (HumanEval, MBPP), factual QA (NaturalQuestions), or commonsense reasoning (StrategyQA) to assess cross-domain performance and robustness.

2. **Overhead and Resource Profiling**: Conduct runtime and memory usage benchmarks comparing A*-Decoding to baseline methods on identical hardware, measuring both wall-clock time and peak memory to quantify practical efficiency gains.

3. **Supervision Signal Ablation**: Test the impact of varying the quality and type of process supervision signal (e.g., using different reward models or heuristic functions) on A*-Decoding's accuracy and efficiency to determine sensitivity to supervision quality.