---
ver: rpa2
title: 'EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training'
arxiv_id: '2512.12210'
source_url: https://arxiv.org/abs/2512.12210
tags:
- data
- distillation
- dataset
- pre-training
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EEG-DLite addresses the high computational cost of training large-scale
  EEG foundation models by introducing a data distillation framework that removes
  noisy and redundant samples from large unlabeled EEG datasets. The method encodes
  EEG segments into compact latent representations using a self-supervised autoencoder,
  then applies outlier filtering and diversity sampling to retain only the most informative
  samples.
---

# EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training

## Quick Facts
- arXiv ID: 2512.12210
- Source URL: https://arxiv.org/abs/2512.12210
- Authors: Yuting Tang; Weibang Jiang; Shanglin Li; Yong Li; Chenyu Liu; Xinliang Zhou; Yi Ding; Cuntai Guan
- Reference count: 6
- Primary result: 5% of 2,500-hour EEG dataset achieves comparable or better performance than full dataset across 4 downstream tasks

## Executive Summary
EEG-DLite introduces a data distillation framework that addresses the high computational cost of training large-scale EEG foundation models by removing noisy and redundant samples from unlabeled EEG datasets. The method uses a self-supervised autoencoder to encode EEG segments into compact latent representations, then applies outlier filtering and diversity sampling to retain only the most informative samples. Extensive experiments show that training on just 5% of a 2,500-hour EEG dataset achieves performance comparable to, or better than, training on the full dataset across multiple downstream tasks, reducing GPU pre-training time from 30 hours to 2 hours. This work provides the first systematic study of pre-training data distillation for EEG foundation models and demonstrates that data quality and diversity are more critical to model generalization than data quantity.

## Method Summary
EEG-DLite operates in three stages: (1) a multi-view autoencoder is trained on raw EEG, FFT magnitude, and FFT phase using a reconstruction loss plus inter-instance discrimination loss to learn 64-dimensional segment embeddings; (2) Histogram-Based Outlier Score (HBOS) removes the top τ% of samples with highest outlier scores based on latent representations; (3) k-center greedy diversity sampling selects η% of samples that maximize minimum distance to previously chosen centers. The distilled subset is then used to pre-train foundation models like LaBraM, achieving comparable or superior performance to models trained on the full dataset while reducing pre-training time from 30 hours to 2 hours.

## Key Results
- Training on 5% of a 2,500-hour EEG dataset distilled with EEG-DLite achieves performance comparable to, or better than, training on the full dataset
- GPU pre-training time reduced from 30 hours to 2 hours (15× speedup)
- OOD removal (0.05%) improves accuracy by eliminating noisy samples
- K-center diversity sampling preserves dataset diversity with ~20× compression

## Why This Works (Mechanism)

### Mechanism 1
Compressing EEG signals into learned latent representations before selection improves sample quality assessment compared to raw signal selection. A self-supervised autoencoder (6 transformer encoder layers, 2 decoder layers) maps multi-view EEG inputs to 64-dimensional embeddings. The training objective combines reconstruction loss (L_Rec) with inter-instance discrimination loss (L_IDC), which penalizes excessive token similarity across samples in a batch, encouraging feature diversity in the learned space. The compressed latent space preserves semantically meaningful structure while discarding high-dimensional noise that would otherwise confound sample selection.

### Mechanism 2
Removing outliers before diversity sampling improves downstream generalization by eliminating samples that degrade representation quality. HBOS computes per-sample OOD scores by summing log-probabilities across feature dimensions. Samples in the top τ% of OOD scores are excluded. Manual inspection confirms these often contain noise or artifacts. Anomalous samples in latent space correspond to low-quality EEG segments (artifacts, corruption) rather than rare but meaningful neural patterns.

### Mechanism 3
Divergence-based core-set selection preserves dataset diversity with ~20× compression while maintaining downstream performance. K-center greedy approximation iteratively selects points maximizing minimum distance to previously chosen centers. EEG datasets contain substantial redundancy (temporally adjacent segments capturing overlapping patterns); retaining diverse representatives is sufficient for foundation model pre-training.

## Foundational Learning

- **Self-supervised learning (SSL) for time-series representation**: The autoencoder must learn meaningful EEG representations without labels. Understanding contrastive vs. reconstruction objectives helps diagnose why L_IDC complements L_Rec. Quick check: Can you explain why penalizing token similarity across samples in a batch encourages more discriminative latent representations?

- **Core-set selection and facility location problems**: The k-center formulation is a classic combinatorial optimization problem. Understanding NP-hardness and greedy approximation bounds helps set expectations for solution quality. Quick check: What is the approximation guarantee of the greedy k-center algorithm (hint: 2-approximation for metric spaces)?

- **Out-of-distribution (OOD) detection via density estimation**: HBOS uses univariate histograms per feature dimension. Understanding its assumptions (feature independence) vs. alternatives (isolation forests, deep density models) informs when it may fail. Quick check: Why might HBOS underperform if latent dimensions are highly correlated?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Neural compressor (CNN encoders + transformer encoder + transformer decoder) -> Outlier module (HBOS) -> Diversity selector (k-center) -> Output (distilled subset)

- **Critical path**: 1) Train autoencoder (50 epochs, Adam, lr=0.001, gradient clipping=5.0) 2) Encode full dataset → Z 3) Filter outliers (τ=0.05% default) 4) Select η% via k-center (η=5% achieves near-full performance) 5) Use distilled subset for LaBraM foundation model pre-training

- **Design tradeoffs**: Latent dimension (d=64) balances speed vs. discriminative structure; distillation ratio (η) shows 5% is sufficient while 1% degrades performance; OOD threshold (τ) at 0.05% helps without removing rare patterns; selection method shows SSL embeddings outperform PCA for diversity representation

- **Failure signatures**: Distilled model underperforms random baseline (autoencoder failed to learn meaningful representations); OOD removal degrades performance (τ too aggressive or HBOS flagging meaningful patterns); high variance across seeds (selection is unstable); M3D synthetic baseline produces artifacts with "abrupt plateaus, flat transitions, blocky patterns"

- **First 3 experiments**: 1) Ablate η ∈ {1%, 5%, 10%, 25%} on SEED-V to confirm 5% sufficiency 2) Visualize latent space (UMAP) before/after OOD removal to verify outlier filtering 3) Compare SSL embeddings vs. PCA at fixed η to reproduce SSL superiority finding

## Open Questions the Paper Calls Out

- **Subject-aware pre-training**: Can subject-aware pre-training strategies leverage the inter-subject variability revealed by diversity sampling to improve foundation model generalization? The authors observe that diversity sampling creates subject-level imbalances but do not explore how to exploit this phenomenon.

- **Generative methods for EEG**: Can improved generative methods produce synthetic EEG data that matches real signal quality for foundation model pre-training? Current approaches produce artifacts unsuitable for EEG; the paper shows M3D generates "unnatural characteristics" with "abrupt plateaus, flat transitions, and repetitive blocky patterns."

- **Performance improvement mechanism**: What mechanisms explain why distilled subsets (5% of data) sometimes outperform training on the full dataset? Tables 1-2 show distilled data surpassing full data on TUEV, MoBI, and TUAB, but the paper does not identify which removed sample types drive improvement.

## Limitations

- Neural compressor architecture details remain underspecified, particularly CNN encoder configurations for processing three input views
- Limited experimental scope to LaBraM as foundation model architecture without exploration of generalization to other EEG foundation models
- Selection of τ=0.05% for outlier removal and η=5% for diversity sampling appears effective but lacks rigorous ablation studies across diverse EEG datasets
- Computational overhead of training the neural compressor (50 epochs) represents non-trivial upfront cost

## Confidence

- **High Confidence**: Core claim that 5% data distillation achieves comparable or superior downstream performance to full-dataset training is well-supported by extensive experimental results across four diverse EEG tasks with clear computational efficiency gains
- **Medium Confidence**: Claims about data quality/diversity being more critical than quantity and SSL embeddings outperforming PCA are supported but would benefit from more extensive ablation studies
- **Low Confidence**: Assertion that this represents the first systematic study of pre-training data distillation for EEG foundation models cannot be independently verified; generalizability of specific hyperparameters across different EEG datasets remains uncertain

## Next Checks

1. **Architecture Ablation**: Systematically vary neural compressor architecture parameters (latent dimension d ∈ {32, 64, 128}, number of transformer encoder layers ∈ {4, 6, 8}) to determine sensitivity and identify minimum viable architecture maintaining performance

2. **OOD Robustness Analysis**: Conduct controlled experiment introducing rare but clinically meaningful EEG patterns (e.g., epileptic spikes) into dataset, then evaluate whether HBOS outlier detection correctly preserves these patterns while removing actual artifacts

3. **Cross-Domain Generalization**: Apply EEG-DLite pipeline to completely different EEG dataset (different clinical domain or acquisition system) to evaluate whether same hyperparameters (τ=0.05%, η=5%) maintain effectiveness or require dataset-specific tuning