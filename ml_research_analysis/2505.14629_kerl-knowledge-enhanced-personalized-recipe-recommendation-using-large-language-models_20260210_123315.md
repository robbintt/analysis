---
ver: rpa2
title: 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language
  Models'
arxiv_id: '2505.14629'
source_url: https://arxiv.org/abs/2505.14629
tags:
- recipes
- recipe
- food
- generation
- ingredients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KERL, a unified food recommendation system
  that leverages food knowledge graphs and large language models to provide personalized
  recipe recommendations, generate cooking instructions, and produce detailed micro-nutritional
  information. The system extracts entities from natural language queries, retrieves
  relevant subgraphs from a food knowledge graph, and uses a multi-LoRA approach to
  fine-tune a base LLM for each task: recipe recommendation, recipe generation, and
  nutrition generation.'
---

# KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models

## Quick Facts
- arXiv ID: 2505.14629
- Source URL: https://arxiv.org/abs/2505.14629
- Reference count: 28
- Achieves 96% mean average precision on KGQA benchmark

## Executive Summary
KERL introduces a unified food recommendation system that leverages knowledge graphs and large language models to provide personalized recipe recommendations, generate cooking instructions, and produce detailed micro-nutritional information. The system uses subgraph retrieval from FoodKG to ground LLM responses and employs a multi-LoRA approach for efficient task-specific fine-tuning. KERL demonstrates significant improvements over baseline models across multiple food-related tasks while maintaining efficiency through parameter-efficient adaptation.

## Method Summary
KERL extracts entities from natural language queries, retrieves relevant subgraphs from FoodKG, and uses multi-LoRA adapters to fine-tune a base LLM for three distinct tasks: recipe recommendation, recipe generation, and nutrition estimation. The system formulates food recommendation as constrained question-answering over a knowledge graph, enabling precise handling of complex user preferences. The architecture uses a single Phi-3-mini base model with three task-specific LoRA adapters, allowing efficient deployment and activation of specialized capabilities as needed.

## Key Results
- Achieves 96% mean average precision on curated KGQA benchmark, outperforming baselines by over 50 percentage points
- Improves BLEU-1 by 20 points over base Phi-3 model for recipe generation
- Reduces mean absolute error by up to 50% compared to baselines for nutrition generation when filtering outlier samples

## Why This Works (Mechanism)

### Mechanism 1: Grounded Subgraph Retrieval
- Claim: Subgraph retrieval from FoodKG serves as grounded context, constraining LLM generation to factual recipes that satisfy user-defined constraints.
- Mechanism: The system parses natural language queries to extract entities, generates SPARQL queries to retrieve relevant subgraphs from FoodKG, and provides these serialized subgraphs as context to the LLM for reasoning and selection.
- Core assumption: FoodKG is comprehensive and accurate; SPARQL templates correctly capture query intent; base LLM can reason over textual subgraphs.
- Evidence anchors: Abstract states "retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints." Section 3.1 details the subgraph retrieval and serialization process.
- Break condition: Fails if FoodKG is incomplete or outdated, or if SPARQL templates cannot parse complex query structures.

### Mechanism 2: Task-Specific LoRA Fine-tuning
- Claim: Task-specific LoRA adapters create specialized, efficient modules for recommendation, recipe generation, and nutrition estimation using a single base LLM.
- Mechanism: Instead of training separate full models, KERL uses Phi-3-mini with three small task-specific LoRA adapters (KERL-Recom, KERL-Recipe, KERL-Nutri) to learn nuances of each distinct task efficiently.
- Core assumption: Tasks benefit from specialized fine-tuning while sharing a strong linguistic foundation; LoRA provides sufficient representational capacity.
- Evidence anchors: Abstract mentions "multi-LoRA approach to fine-tune a base LLM for each task." Section 5.1 describes the LoRA configuration and deployment strategy.
- Break condition: Fails if LoRA rank is too low to capture task complexity or if adapters interfere with each other.

### Mechanism 3: Constraint-Based Question Answering
- Claim: Formulating food recommendation as constrained QA over a KG enables precise handling of complex, multi-factor user preferences.
- Mechanism: The system models user requests as hard constraints (included/excluded ingredients, nutrient ranges) and trains to find the intersection of these conditions within retrieved subgraph context.
- Core assumption: User preferences can be expressed as hard logical constraints; sufficient training data with such examples is available.
- Evidence anchors: Abstract mentions "select the recipes that satisfy the constraints." Section 4.1 describes the constraint generation process for ingredients and nutrition.
- Break condition: Fails for soft preferences or when highly specific constraints yield no results in the KG.

## Foundational Learning

### Knowledge Graph Question Answering (KGQA)
- Why needed here: KERL's core task is KGQAâ€”retrieving structured information from a KG to answer natural language questions.
- Quick check question: How does KGQA differ from standard RAG over documents, and what are the advantages of using a KG for structured constraints?

### Low-Rank Adaptation (LoRA)
- Why needed here: The entire efficiency and modularity of KERL depends on LoRA for parameter-efficient fine-tuning.
- Quick check question: What are the primary benefits of using LoRA adapters over full fine-tuning for deploying multiple specialized tasks?

### Constrained Generation
- Why needed here: The system generates text that must strictly adhere to specified logical constraints derived from user queries.
- Quick check question: What is the challenge in ensuring an LLM's output strictly satisfies multiple simultaneous numerical and categorical constraints?

## Architecture Onboarding

### Component map:
User Query -> Parser -> SPARQL Engine -> Retriever -> Context Formation -> KERL-Recom Adapter -> Recipe List

### Critical path:
Query -> Parser -> SPARQL Engine -> Retriever -> Context Formation -> KERL-Recom Adapter -> Recipe List. This list can then be passed to Recipe/Nutri adapters.

### Design tradeoffs:
The system trades generative creativity for factual grounding and constraint satisfaction. It trades the potential power of a single large fine-tuned model for the efficiency and modularity of a multi-LoRA setup.

### Failure signatures:
The system may fail silently by returning an empty list if constraints are too strict, or fail loudly by hallucinating recipes not in the provided context. Nutrition generation may be highly inaccurate for outlier recipes with extreme values.

### First 3 experiments:
1. **Trace a Query**: Select a sample question from the benchmark, trace the SPARQL query it generates, examine the retrieved subgraph context, and manually verify if the model's recommended recipes satisfy all constraints.
2. **Adapter Validation**: Run the same input through the pipeline using the base model (no adapter) vs. the KERL-Recom adapter to quantify the performance gain attributable to the fine-tuned adapter.
3. **Constraint Stress Test**: Start with a simple query and progressively add constraints (e.g., add a disliked ingredient, tighten a nutrient range). Observe how the recommendations change and identify the point where the system fails to find any matches.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetic query generation, which may not capture real-world query complexity and ambiguity
- Reported nutrition generation improvements are qualified by "filtering out outlier samples," suggesting struggles with extreme cases
- System's real-world deployment readiness, handling diverse user queries and managing KG incompleteness, is not thoroughly addressed

## Confidence

**High Confidence**: The core mechanism of using subgraph retrieval from FoodKG as grounded context for LLM-based recommendation is well-supported by presented evidence. KGQA benchmark results showing 96% MAP are compelling.

**Medium Confidence**: The multi-LoRA approach's effectiveness is demonstrated, but evidence is less comprehensive. Improvements over baselines are reported without extensive ablation studies or comparison to alternative parameter-efficient fine-tuning methods.

**Low Confidence**: The system's real-world deployment readiness, particularly regarding handling diverse user queries, managing KG incompleteness, and scaling to different cuisines or dietary preferences, is not thoroughly addressed.

## Next Checks

1. **Real-World Query Evaluation**: Deploy the system on an actual user query dataset (not synthetically generated) to assess performance on ambiguous, conversational, or multi-intent requests that users might actually make.

2. **KG Completeness Stress Test**: Systematically identify and remove random subsets of recipes from FoodKG, then measure how recommendation accuracy degrades. This would quantify the system's dependence on KG coverage.

3. **Cross-Domain Transferability**: Attempt to apply the KERL architecture to a different knowledge domain (e.g., movie or restaurant recommendations) using the same approach. This would validate whether the success is due to the methodology or the specific characteristics of the FoodKG dataset.