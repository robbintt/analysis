---
ver: rpa2
title: 'RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language
  Models'
arxiv_id: '2601.15331'
source_url: https://arxiv.org/abs/2601.15331
tags:
- adversarial
- prompts
- prompt
- training
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently testing large
  language models (LLMs) for adversarial prompt vulnerabilities, which is critical
  for security evaluation but resource-intensive with existing methods like GCG, PEZ,
  and GBDA. The proposed method, RECAP, eliminates the need for retraining by retrieving
  pre-trained adversarial prompts from a categorized database, enabling faster and
  cheaper evaluation.
---

# RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models

## Quick Facts
- arXiv ID: 2601.15331
- Source URL: https://arxiv.org/abs/2601.15331
- Authors: Rishit Chugh
- Reference count: 12
- One-line primary result: RECAP achieves 33% attack success rate on Llama 3 8B with 4-minute inference time vs hours for retraining methods

## Executive Summary
This paper introduces RECAP, a resource-efficient method for evaluating large language models (LLMs) against adversarial prompts without requiring expensive retraining. The approach leverages a retrieval-augmented generation system that matches new prompts to a pre-compiled database of successful adversarial examples, appending the corresponding attack suffixes to bypass safety guardrails. Tested on Llama 3 8B, RECAP achieves competitive attack success rates while reducing inference time from hours to minutes, making scalable red-teaming more practical. The method also demonstrates transferability to black-box models like Gemini, though with lower success rates.

## Method Summary
RECAP addresses the computational expense of traditional adversarial prompting methods (GCG, PEZ, GBDA) by building a retrieval database of pre-trained successful attacks. The process involves classifying harmful prompts into 7 intent categories using Vicuna-7B, generating adversarial suffixes via multiple attack algorithms on Llama 3 8B, filtering successful examples through HarmBench Classifier, and creating a FAISS index for similarity search. During inference, new prompts are encoded, matched to the nearest database example, and the corresponding adversarial suffix is appended based on algorithm performance hierarchy per intent category.

## Key Results
- Average attack success rate of 33% on Llama 3 8B using 226 filtered adversarial examples
- Inference time reduced to 4 minutes per 20 prompts versus hours for traditional retraining methods
- Transferability to black-box Gemini model with 4-10% success rate when harm filters are disabled
- Resource efficiency achieved by eliminating retraining while maintaining competitive ASR

## Why This Works (Mechanism)
RECAP exploits the observation that adversarial prompts often share semantic patterns that can be captured through vector similarity. By pre-computing successful attacks and organizing them by intent category, the method avoids the computational cost of generating new adversarial examples for each test prompt. The hierarchical algorithm selection ensures that the most effective attack method for each intent category is used, while the FAISS index enables fast nearest-neighbor retrieval. This approach trades some attack success rate for dramatic improvements in efficiency and scalability.

## Foundational Learning

**FAISS similarity search** - Why needed: Enables fast retrieval of semantically similar adversarial prompts from large databases. Quick check: Verify that retrieved prompts are semantically relevant to the query prompt.

**Adversarial suffix generation** - Why needed: Creates the attack patterns that bypass safety guardrails. Quick check: Confirm that generated suffixes successfully attack the target model in initial validation.

**Intent classification** - Why needed: Organizes prompts into categories to enable algorithm performance hierarchy. Quick check: Audit sample classifications for accuracy and consistency.

**Vector embeddings** - Why needed: Converts text prompts into numerical representations for similarity comparison. Quick check: Test that semantically similar prompts have high cosine similarity in embedding space.

**HarmBench evaluation** - Why needed: Provides standardized binary success/failure metric for attacks. Quick check: Verify that HarmBench correctly identifies safety violations in LLM responses.

## Architecture Onboarding

**Component map:** Vicuna-7B (intent classification) -> PEZ/GBDA/GCG (adversarial generation) -> HarmBench (evaluation) -> FAISS (indexing) -> Llama 3 8B (target model)

**Critical path:** New prompt → Vicuna intent classification → FAISS similarity search → Retrieve adversarial suffix → Append to prompt → Evaluate with target LLM → Check HarmBench success

**Design tradeoffs:** Database size vs. retrieval accuracy vs. storage costs; algorithm hierarchy complexity vs. implementation simplicity; similarity threshold strictness vs. coverage completeness.

**Failure signatures:** Low ASR indicates poor database coverage or similarity matching; high inference time suggests inefficient FAISS indexing; incorrect intent classification leads to wrong algorithm selection.

**First experiments:** 1) Test FAISS retrieval accuracy on a small validation set of known prompt-suffix pairs. 2) Verify adversarial suffix transferability by testing on multiple target models. 3) Benchmark inference time improvement against baseline retraining methods.

## Open Questions the Paper Calls Out

**Open Question 1:** What specific factors cause the inconsistent results when using RECAP tokens to initialize GCG training, and can these factors be characterized predictably? The authors observed that RECAP-generated tokens sometimes accelerated training but other times reduced success rates, suggesting unmeasured correlations influencing performance.

**Open Question 2:** Will success rates follow the predicted logistic growth pattern as the retrieval database expands, and at what database size does performance plateau? The current 226-example database is acknowledged as limited, with theoretical logistic growth untested empirically.

**Open Question 3:** What mechanistic explanation accounts for the correlation between prompt harm category and optimal attack algorithm? The observed performance hierarchy across categories lacks theoretical grounding in embedding space structure or safety training patterns.

## Limitations
- Small database size (226 examples) may not provide comprehensive coverage for diverse attack scenarios
- Limited transferability testing to only one black-box model (Gemini) restricts generalizability
- Fixed 13-token constraint for GCG may bias database toward shorter attacks
- No analysis of potential biases in the initial prompt dataset affecting retrieval quality

## Confidence

**High Confidence:** Core methodology of using RAG for adversarial prompt retrieval is technically sound; reported 33% ASR and 4-minute inference time are specific and credible.

**Medium Confidence:** Resource efficiency claims are supported but trade-offs with full retraining approaches need more exploration; transferability to Gemini is presented with limited validation.

**Low Confidence:** "Scalable red-teaming" assertion is overstated given small database size and limited model coverage; potential dataset biases not addressed.

## Next Checks
1. Conduct semantic similarity analysis between database examples and held-out prompts to quantify coverage gaps and identify consistently failing prompt types.

2. Test RECAP's retrieval effectiveness across at least three additional LLM architectures (Claude, GPT-4, Mistral) to establish if Gemini transferability is representative.

3. Measure how ASR and inference efficiency change as database grows from 226 to 1000+ examples to determine optimal size and point of diminishing returns.