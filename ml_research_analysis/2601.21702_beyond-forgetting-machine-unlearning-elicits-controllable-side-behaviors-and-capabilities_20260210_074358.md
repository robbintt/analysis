---
ver: rpa2
title: 'Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors
  and Capabilities'
arxiv_id: '2601.21702'
source_url: https://arxiv.org/abs/2601.21702
tags:
- unlearning
- https
- machine
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores representation misdirection (RM), a class\
  \ of LLM unlearning methods that manipulates forget-representations toward a target\
  \ vector to achieve selective forgetting. While previous work used random vectors,\
  \ this paper revisits RM through the linear representation hypothesis, which suggests\
  \ that high-level concepts can be represented as one-dimensional vectors in the\
  \ model\u2019s latent space."
---

# Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities

## Quick Facts
- arXiv ID: 2601.21702
- Source URL: https://arxiv.org/abs/2601.21702
- Reference count: 40
- Primary result: Representation misdirection (RM) with targeted concept vectors elicits controllable side effects beyond forgetting, improving truthfulness (+5.3-12.7 points) and sentiment (+38.6-39.9 percentage points) while maintaining unlearning effectiveness.

## Executive Summary
This paper explores representation misdirection (RM) as a machine unlearning method that manipulates forget-representations toward a target vector. Unlike previous random-vector approaches, this work uses structured concept vectors aligned with high-level concepts (truth, sentiment, refusal) extracted via logistic regression probes. The authors introduce two conceptual models: representational addition (RAd) adds concept vectors to forget-representations, while representational ablation (RAb) removes components aligned with the concept direction. Extensive experiments on Zephyr-7B and Mistral-7B validate that these methods not only achieve unlearning but also elicit controllable side behaviors and enhanced capabilities aligned with the target concept.

## Method Summary
The method extracts concept directions via logistic regression probes trained on contrastive prompt pairs, then manipulates forget-representations using RAd (additive) or RAb (ablative) interventions. The losses are computed as ||λf_θ - (λf_ref + c·λ̄W)||² for RAd and ||λf_θ - (λf_ref - c⟨λf_ref,λ̄W⟩λ̄W)||² for RAb, with retain-representations similarly regularized. Fine-tuning uses AdamW (lr=5e-5, batch=4, T=500 steps) updating layers {7,6,5}. Concept directions are extracted from MLP output layer 7 representations.

## Key Results
- RAd with truthfulness direction improves TruthfulQA performance by 5.3-12.7 points while maintaining unlearning effectiveness
- RAd with sentiment direction increases positive sentiment by 38.6-39.9 percentage points
- RAd with refusal direction induces refusal even to harmless instructions
- RAd with task-specific directions improves in-context learning capabilities by 32.9-58.5 percentage points

## Why This Works (Mechanism)

### Mechanism 1: Representational Addition (RAd) Shifts Output Odds
- Claim: Adding a concept vector to forget-representations multiplies the odds of generating concept-aligned outputs by a monotonic factor.
- Mechanism: By linearity of logit measurement, additive intervention λ′ = λf + c·λ̄W increases the logit for target outcome proportionally to the inner product λ̄W⊤γ̄W > 0, shifting probability mass toward the concept.
- Core assumption: The concept vector λ̄W truly corresponds to the high-level concept (linear representation hypothesis holds).
- Evidence anchors: [section 3.2.1] Derivation shows monotonic increase; [Table 1] RAd with truthfulness improves TruthfulQA MC1 by +5.9 while maintaining unlearning; [corpus] Related work confirms concept removal is possible but doesn't address controlled side effects.

### Mechanism 2: Representational Ablation (RAb) Suppresses Concept-Aligned Outputs
- Claim: Removing components of forget-representations aligned with a concept vector exponentially decreases odds of concept-aligned outputs.
- Mechanism: Ablative intervention λ′ = λf - c⟨λf,λ̄W⟩λ̄W projects out the concept direction, reducing logit proportionally to the original alignment.
- Core assumption: Forget-representations contain positive evidence for the concept (λf⊤λ̄W > 0); otherwise ablation has limited effect.
- Evidence anchors: [section 3.2.2] Eq. 9 shows odds reduction; [Table 1] RAb with truthfulness degrades TruthfulQA MC1 by -12.9; [Table 5] RAb with refusal reduces refusal score from 90.3% to 49.0%.

### Mechanism 3: Random Vectors Are Nearly Orthogonal to Concept Directions in High Dimensions
- Claim: Randomly sampled target vectors in high-dimensional LLM representation spaces are unlikely to align with specific high-level concept directions.
- Mechanism: By Lévy's Lemma, for a random unit vector u and concept vector λ̄W ∈ ℝ^d, concentration bound guarantees near-orthogonality for large d.
- Core assumption: Representation dimensionality d is large (true for modern LLMs: d=4096 for 7B models).
- Evidence anchors: [section 3.2.3, Proposition 3.2] Formal proof using Lévy's Lemma; [Table 1] Random direction yields marginal TruthfulQA improvements (+2.0 avg) vs. concept-aligned (+6.6 avg).

## Foundational Learning

- **Concept: Linear Representation Hypothesis**
  - Why needed here: The entire theoretical framework assumes high-level concepts are encoded as one-dimensional vectors in latent space, enabling linear operations to control outputs.
  - Quick check question: If concept representations were nonlinear (e.g., curved manifolds), would additive intervention still shift output probabilities monotonically? (Answer: No, local linear approximation may fail globally.)

- **Concept: Logit-Linearity of Concept Measurement**
  - Why needed here: The proof that RAd/RAb shift output odds relies on Theorem 2.2 (logit P(Y=Y(1)|λ) = αλ⊤γ̄W), linking latent representations to output probabilities linearly.
  - Quick check question: Why does λ̄W⊤γ̄W > 0 matter for additive intervention to increase target outcome probability? (Answer: This positive inner product ensures the exponent in the odds ratio is positive.)

- **Concept: Representation Misdirection for Unlearning**
  - Why needed here: RAd/RAb are instances of RM—a class of methods that manipulate forget-representations toward a target vector to achieve selective forgetting.
  - Quick check question: How does RAd differ from random-vector RM methods like RMU? (Answer: RAd uses structured concept vectors instead of random noise, inducing predictable side effects beyond forgetting.)

## Architecture Onboarding

- **Component map:** Concept Direction Extractor -> Activation Hook (layer 7) -> RAd/RAb Loss Computer -> Fine-tuning Loop

- **Critical path:** Concept direction extraction → Layer selection (l=7) → Hyperparameter tuning (αf, αr, c) → Loss computation → Gradient update

- **Design tradeoffs:**
  - Higher scaling coefficient c: Stronger side effects but risk of destabilizing retain-performance (Table 6 shows task-specific ICL gains but MMLU drops 3-8 points)
  - Earlier layers (smaller l): Broader intervention but may affect general capabilities more
  - RAd vs. RAb: RAd enables controllable side effects; RAb provides cleaner suppression but is less effective at unlearning (WMDP: 32.9 vs. 28.2 for RAd with truthfulness)

- **Failure signatures:**
  - Gibberish outputs: c too large or λ̄W poorly extracted (check probe validation accuracy)
  - Poor unlearning: λ̄W doesn't align with forget-knowledge (verify via cosine similarity between λ̄W and forget-representations)
  - Collateral capability loss: Concept direction overlaps with retain-knowledge (monitor MMLU during training)
  - Knowledge recovery vulnerability: Finetuning on 5 forget-samples can restore WMDP accuracy to ~60% (Table 7)

- **First 3 experiments:**
  1. Validate concept direction extraction: Train probe on TruthfulQA dev set, check validation accuracy >75%, visualize λ̄W via UMAP against random vectors
  2. Sweep scaling coefficient c: Run RAd with truthfulness direction on WMDP-Biology, vary c ∈ {10, 14, 18, 23}, plot WMDP accuracy vs. TruthfulQA MC1 to find Pareto frontier
  3. Test robustness to benign perturbation: Evaluate RAd/RAb models on perturbed MMLU (replace incorrect choice with "SARS-CoV-2"), compare accuracy drop vs. base model (per Table 10, RAd drops to ~24% while RAb stays at ~53%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can concept vectors extracted from one model architecture effectively transfer to induce controllable side effects in different model architectures (e.g., from Zephyr-7B to Llama-3-8B)?
- Basis in paper: [explicit] "The linear representation transferability hypothesis (Bello et al., 2025) suggests that linear representations are transferable across models, meaning that a concept vector extracted from a model can be used in other models. Exploring the side effects of target direction in different model architectures and settings is a promising direction for future work."
- Why unresolved: All experiments used concept directions extracted from the same model being unlearned; cross-architecture transfer was not tested.
- What evidence would resolve it: Experiments showing whether a truthfulness direction extracted from Zephyr-7B can improve TruthfulQA performance when applied to Mistral-7B or Llama-3-8B.

### Open Question 2
- Question: Can the trade-off between unlearning effectiveness (favored by RAd) and robustness against benign perturbation (favored by RAb) be overcome through hybrid approaches?
- Basis in paper: [inferred] Section F shows RAd models achieve good unlearning but are "highly susceptible to benign perturbation, indicated by near-random accuracy on perturbed MMLU," while RAb models exhibit stability but "are less effective in unlearning performance."
- Why unresolved: The paper identifies the trade-off but does not propose or test methods to achieve both properties simultaneously.
- What evidence would resolve it: A modified RM method that achieves both low WMDP accuracy (effective unlearning) and minimal performance drop on perturbed MMLU (robustness).

### Open Question 3
- Question: Can RAd/RAb models be made robust to parameter modification attacks (finetuning, orthogonalization, pruning) while preserving unlearning effectiveness?
- Basis in paper: [explicit] "Attacks that directly modify model parameters, such as finetuning, orthogonalization, and pruning, can substantially restore forgotten knowledge, often recovering performance to near the base model's accuracy."
- Why unresolved: While RAd shows slightly better robustness than RAb against some attacks, both remain fundamentally vulnerable to parameter modification.
- What evidence would resolve it: Modified RAd/RAb methods where accuracy under finetuning attack remains substantially below base model accuracy while maintaining unlearning on WMDP benchmarks.

## Limitations

- The theoretical framework relies on the linear representation hypothesis, which may not hold for all high-level concepts across different model architectures
- Knowledge recovery attacks that modify model parameters can substantially restore forgotten knowledge, undermining the unlearning effectiveness
- The trade-off between unlearning effectiveness and robustness against benign perturbation remains unresolved, with RAd favoring the former and RAb the latter

## Confidence

**High Confidence**: Claims about RAd improving side behaviors (truthfulness, sentiment, refusal) while maintaining unlearning effectiveness. Supported by consistent improvements across both models in Tables 1 and 5.

**Medium Confidence**: Claims about RAb's effectiveness at suppressing behaviors and RAd's vulnerability to knowledge recovery. The ablation effects are clear, but the vulnerability results show mixed patterns (RAd shows slightly better robustness but still vulnerable).

**Low Confidence**: Claims about the general applicability of the linear representation hypothesis to arbitrary high-level concepts. While well-theorized, the paper only validates on a limited set of concepts.

## Next Checks

1. **Probe Validation Across Layers**: Test whether concept direction extraction quality varies significantly when extracting representations from different layers (not just layer 7). This would validate the assumption that MLP output layer captures the most discriminative concept information.

2. **Adversarial Concept Direction Testing**: Systematically evaluate whether adversarially crafted concept vectors (rather than randomly sampled ones) can induce unintended side effects in RAd models, testing the robustness of the "random vectors are orthogonal" claim.

3. **Cross-Domain Generalization**: Apply the same RAd/RAb methodology to a different unlearning task (e.g., copyright-protected code or medical data) to test whether the controllable side effect mechanism generalizes beyond the biology/cybersecurity domains used in the paper.