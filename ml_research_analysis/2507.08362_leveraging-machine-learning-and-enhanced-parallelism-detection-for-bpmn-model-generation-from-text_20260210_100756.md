---
ver: rpa2
title: Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model
  Generation from Text
arxiv_id: '2507.08362'
source_url: https://arxiv.org/abs/2507.08362
tags:
- dataset
- process
- bpmn
- extraction
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning pipeline for generating
  BPMN models from text. The key contribution is a new annotated dataset (LESCHNEIDER)
  that extends the PET dataset with 15 documents containing 32 additional parallel
  gateways, addressing a critical gap in existing data.
---

# Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text

## Quick Facts
- arXiv ID: 2507.08362
- Source URL: https://arxiv.org/abs/2507.08362
- Reference count: 28
- The pipeline achieved 89.2% F1 for elements and 73.7% F1 for relations across six test documents

## Executive Summary
This paper presents a machine learning pipeline for generating BPMN models from text, addressing the critical gap in existing datasets where parallel gateways are severely underrepresented. The key innovation is the LESCHNEIDER dataset, which extends the PET dataset with 15 documents containing 32 additional parallel gateways, enabling improved detection of AND Gateways that were previously statistically invisible. The pipeline uses BERT and RoBERTa for Named Entity Recognition and CatBoost for relation extraction, achieving strong results for element detection (89.2% F1) while still facing challenges with relation extraction (73.7% F1) and implicit gateway closures.

## Method Summary
The pipeline follows a standard NLP approach: text preprocessing (tokenization, sentence segmentation, special character removal), Named Entity Recognition using BERT-base-cased for token classification into BPMN elements, Relation Extraction using CatBoost classifier on mention-pair features with Random Over Sampling, optional Coreference Resolution, and BPMN diagram generation using NetworkX DiGraph. The method uses 5-fold cross-validation with batch size 8, training on L4 GPU Google Colab. The approach specifically targets the class imbalance problem where AND Gateways were severely underrepresented in existing datasets, using linguistic markers like "and simultaneously" to create synthetic examples for training.

## Key Results
- F1 score for B-AND Gateway improved from 0% to 23% through LESCHNEIDER dataset augmentation
- BERT-base-cased model outperformed BERT-large and RoBERTa-large variants on small dataset, avoiding overfitting
- The pipeline achieved 89.2% F1 for elements and 73.7% F1 for relations across six test documents
- Random Over Sampling of flow relations improved CatBoost performance without sacrificing other relation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting training data with targeted parallel gateway examples improves detection of underrepresented BPMN elements.
- Mechanism: The original PET dataset contained only 8 AND Gateway instances versus 117 XOR Gateway instances, creating severe class imbalance. By adding 15 manually annotated documents with 32 new AND Gateways using linguistic markers like "and simultaneously," the LESCHNEIDER dataset increases the relative frequency from 0.48% to 1.92%. This expanded training signal allows models to learn parallelism patterns that were previously statistically invisible.
- Core assumption: The linguistic markers used in synthetic examples generalize to real-world process descriptions with parallel structures.
- Evidence anchors: [abstract] "we augment the PET dataset with 15 newly annotated documents containing 32 parallel gateways for model training"; [Section 3.2] "The LESCHNEIDER dataset introduces 32 new AND Gateways... by focusing on parallel process structures using phrases like 'and simultaneously'"; [Section 5.2] "F1 score for B-AND Gateway from 0% to 23% and an increase in precision to 50%"

### Mechanism 2
- Claim: BERT-base-cased outperforms larger transformer variants for BPMN entity extraction on small datasets due to balanced model capacity.
- Mechanism: BERT's bidirectional contextual representations enable token-level classification for NER. The cased variant preserves capitalization cues that distinguish proper nouns (actors) from common terms. On the combined dataset (~1,100 entities across 79 documents), larger models (BERT-large, RoBERTa-large) overfit, while CRF lacks contextual understanding. BERT-base-cased provides sufficient representational capacity without exceeding the dataset's regularization threshold.
- Core assumption: The performance ranking holds for datasets of similar scale and annotation density; scaling data may favor larger models.
- Evidence anchors: [abstract] "The BERT-base-cased model outperformed others, achieving strong results across most labels"; [Section 5.3] "The BERT-base-cased model outperforms others, achieving the highest F1-scores across all types"; [Section 5.5] "larger models such as BERT-large and RoBERTa-large tend to overfit when trained on the relatively small dataset"

### Mechanism 3
- Claim: Random Over-Sampling (ROS) of the flow relation class improves CatBoost relation extraction without sacrificing other relation types.
- Mechanism: Relation extraction forms mention pairs with features (POS tags, token distances, dependency tags, previous/next tags). The flow relation is critical for BPMN sequence construction but underrepresented. ROS doubles flow examples, allowing CatBoost's gradient-boosted trees to better partition the feature space for this class. Unlike SMOTE (which synthesizes potentially invalid feature combinations) or negative sampling (which skews toward "no relation"), ROS preserves feature authenticity.
- Core assumption: Duplicating flow examples does not introduce spurious patterns; the feature space is sufficiently discriminative.
- Evidence anchors: [Section 4.2] "The flow class is prioritized because it fundamentally defines the sequential interaction of activities"; [Section 5.3] "ROS showed the best performance for the flow class while also maintaining strong results across other categories"; [Section 5.4] Pipeline achieves 73.7% F1 for relations versus 89.2% for elements

## Foundational Learning

- Concept: IOB2 Tagging Format
  - Why needed here: All NER annotations use Inside-Outside-Begin scheme (B-Activity, I-Activity, O). Understanding this is essential for interpreting Table 3 results and reproducing the pipeline.
  - Quick check question: Given tokens ["The", "staff", "member", "logs", "the", "book"], how would "staff member" (Actor) and "logs the book" (Activity) be tagged?

- Concept: BPMN Gateway Types (XOR vs AND)
  - Why needed here: The core contribution addresses AND Gateway detection. XOR represents exclusive choice (one path); AND represents parallel execution (all paths). Confusion between these invalidates the parallelism detection claim.
  - Quick check question: In "The system sends an email AND simultaneously updates the database," which gateway type applies?

- Concept: Class Imbalance Mitigation Techniques
  - Why needed here: The paper compares Negative Sampling, SMOTE, and ROS for relation extraction. Understanding tradeoffs is critical for adapting to new datasets.
  - Quick check question: Why might SMOTE generate implausible feature combinations for categorical features like "dependency tag"?

## Architecture Onboarding

- Component map: Pre-Processing -> NER Module (BERT-base-cased) -> Relation Extraction (CatBoost + ROS) -> Entity Resolution -> Diagram Generation
- Critical path: NER → RE → Diagram Generation. Errors in NER propagate to RE (missing/incorrect mentions cannot form correct relations). The paper notes this explicitly: "preventing error propagation that affects precision and recall."
- Design tradeoffs:
  - Model size vs. dataset scale: BERT-base over RoBERTa-large avoids overfitting on small data
  - Sampling strategy: ROS chosen for flow relation emphasis; SMOTE rejected for potential synthetic artifacts
  - Coreference resolution: Evaluated but not integrated by default; adds complexity with marginal gain
- Failure signatures:
  - B-AND Gateway F1 = 23% (improved from 0% but still low): Parallelism detection remains brittle
  - I-XOR Gateway F1 dropped in cross-dataset testing: Phrase-specific patterns ("case") don't generalize
  - CatBoost flow relation F1 = 62%: Limits diagram coherence; implicit gateway closures fail
- First 3 experiments:
  1. Baseline CRF validation: Train CRF on PET with 5-fold CV; replicate Table 3 column (a) to verify environment setup. Expected weighted F1 ≈ 0.72.
  2. Cross-dataset generalization test: Train on PET, test on LESCHNEIDER without LESCHNEIDER training data. Confirm performance drop (weighted F1 0.72 → 0.61) to understand domain shift.
  3. NER model comparison: Run 5-fold CV on combined dataset comparing BERT-base-cased vs. RoBERTa-base. Verify BERT-base-cased achieves higher macro F1 on AND Gateway specifically.

## Open Questions the Paper Calls Out

- Can large language models outperform CatBoost for relation extraction in BPMN model generation, particularly for flow relations? The current CatBoost model achieved only 62% F1 for flow relations, identified as a key limitation that hinders accurate BPMN diagram generation.
- What mechanisms can enable accurate detection of implicit gateway closures when explicit textual markers are absent? The authors identify as a limitation: "the models struggle with implicit gateway closures."
- Can hybrid rule-based and machine learning approaches improve generalization across diverse writing styles compared to purely ML-based methods? Future work includes "hybrid rule-based and ML approaches for better generalization."
- What dataset scale is required for larger pre-trained models to leverage their capacity without overfitting? BERT-large and RoBERTa-large "tended to overfit when trained on the relatively small dataset," while BERT-base-cased performed best, suggesting the combined 60-document dataset is insufficient.

## Limitations

- Dataset Size and Domain Specificity: The LESCHNEIDER dataset contains only 15 documents with 32 AND Gateway instances, creating a relatively small training corpus that limits generalizability to domains with different linguistic patterns for expressing concurrent activities.
- Performance Gap in Complex Structures: While F1 scores improve for AND Gateway detection (0% to 23%), this remains substantially below XOR Gateway performance (64.9% F1), and the pipeline struggles with implicit gateway closures and complex BPMN structures.
- Model Capacity Constraints: The finding that BERT-base-cased outperforms larger models is dataset-size dependent, and with 10x more data, larger models may show superior performance, suggesting current results are not architecture-optimal but rather regularization-appropriate.

## Confidence

- High Confidence: The mechanism by which augmenting training data with parallel gateway examples improves detection is well-supported by the 23 percentage point improvement in AND Gateway F1 scores.
- Medium Confidence: The claim that BERT-base-cased provides optimal capacity for this dataset size is supported by empirical comparison but remains conditional on dataset scale.
- Low Confidence: The generalization of linguistic markers for parallelism to diverse real-world process descriptions remains untested.

## Next Checks

1. Cross-Domain Parallelism Detection: Test the trained model on process descriptions from different domains (manufacturing, healthcare, software development) that may use distinct linguistic constructions for expressing parallel activities. Measure AND Gateway detection performance to validate marker generalization.

2. Dataset Scaling Experiment: Replicate the BERT model comparison with 10x the current training data (approximately 1,100 entities → 11,000 entities). This will determine whether the current architecture choice remains optimal as data scales or whether larger models become necessary.

3. Implicit Gateway Closure Testing: Construct test cases with implicit parallel structures lacking explicit linguistic markers (e.g., "Process A completes, then Process B and C begin"). Evaluate whether the pipeline can detect AND Gateways without the "simultaneously" type cues present in LESCHNEIDER.