---
ver: rpa2
title: Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge
  with Dual-Granularity Representations
arxiv_id: '2508.16634'
source_url: https://arxiv.org/abs/2508.16634
tags:
- fault
- learning
- class
- feature
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of Few-Shot Class-Incremental Fault
  Diagnosis (FSC-FD), where diagnostic models must continuously learn new fault types
  from very few samples while retaining knowledge of previously seen faults. The key
  challenge is preventing catastrophic forgetting of old knowledge while avoiding
  overfitting to scarce new data.
---

# Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations

## Quick Facts
- arXiv ID: 2508.16634
- Source URL: https://arxiv.org/abs/2508.16634
- Authors: Zhendong Yang; Jie Wang; Liansong Zong; Xiaorong Liu; Quan Qian; Shiqian Chen
- Reference count: 40
- Primary result: Dual-Granularity Guidance Network (DGGN) achieves superior diagnostic accuracy in Few-Shot Class-Incremental Fault Diagnosis (FSC-FD) with average classification accuracy improvements of over 4-5% in class-imbalanced settings and up to 17.77% in long-tailed scenarios.

## Executive Summary
This paper addresses the challenge of Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), where diagnostic models must continuously learn new fault types from very few samples while retaining knowledge of previously seen faults. The key challenge is preventing catastrophic forgetting of old knowledge while avoiding overfitting to scarce new data. The authors propose a Dual-Granularity Guidance Network (DGGN) that decouples feature learning into two parallel streams: a fine-grained, class-specific stream for capturing discriminative features from limited new samples using a Multi-Order Interaction Aggregation module, and a coarse-grained, class-agnostic stream for preserving stable, shared knowledge across all fault types. These two representations are dynamically fused via a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features. To further mitigate forgetting, the paper introduces a Boundary-Aware Exemplar Prioritization strategy and employs a decoupled Balanced Random Forest classifier to address classification bias from data imbalance.

## Method Summary
The Dual-Granularity Guidance Network (DGGN) uses two parallel 1D ResNet-18 backbones: a Class-Specific (CS) stream for discriminative features and a Class-Agnostic (CA) stream for stable, shared knowledge. The CS stream uses Multi-Order Interaction Aggregation (parallel depth-wise convolutions at dilation rates 1, 2, 3) to capture multi-scale context and reduce overfitting. The CA stream uses self-supervised contrastive learning (SimCLR/CaSSLe) to learn general fault features independent of class labels. Features are dynamically fused via multi-semantic cross-attention, where CA knowledge guides CS learning. Boundary-Aware Exemplar Prioritization (BAEP) selects challenging samples near decision boundaries for replay, and a Balanced Random Forest classifier addresses class imbalance. The model is trained on sequential sessions, with the CA branch frozen during CS updates to prevent semantic drift.

## Key Results
- DGGN achieves average classification accuracy improvements of over 4-5% in class-imbalanced settings and up to 17.77% in long-tailed scenarios compared to state-of-the-art FSC-FD methods
- Boundary-Aware Exemplar Prioritization (BAEP) outperforms standard replay strategies (Herding, Random) in maintaining decision boundary integrity
- The decoupled dual-branch architecture significantly reduces catastrophic forgetting compared to single-stream baselines
- Performance validated on both TEP benchmark and real-world MFF datasets with varying class imbalance conditions

## Why This Works (Mechanism)

### Mechanism 1: Semantic Anchoring via Class-Agnostic Decoupling
Isolating generalizable "fault signatures" in a parallel branch stabilizes the learning of new, specific fault types by providing a stable reference point. The architecture splits feature learning into a Class-Specific (CS) stream and a Class-Agnostic (CA) stream. The CA stream is trained via self-supervised contrastive learning (SimCLR/CaSSLe) to capture shared semantics (e.g., "vibration" vs. "silence") independent of class labels. During incremental sessions, the frozen or slowly evolving CA features serve as a "semantic anchor." The Multi-Semantic Cross Attention (MSCA) module queries these stable CA features to regularize the CS features, theoretically preventing the CS representation from drifting arbitrarily when encountering scarce new data.

### Mechanism 2: Multi-Order Context Aggregation for Overfitting Reduction
Aggregating multi-scale contextual information reduces the risk of overfitting to spurious local patterns in few-shot scenarios. The Multi-Order Interaction Aggregation (MOIA) module replaces standard convolutions with parallel depth-wise convolutions (DWConv) at varying dilation rates (1, 2, 3). This forces the model to simultaneously capture local details (low-order), mid-range dependencies, and global context (high-order). By integrating these multi-order features, the model is less likely to rely on a single, potentially coincidental feature found in a small sample size.

### Mechanism 3: Boundary-Aware Exemplar Selection for Decision Boundary Preservation
Prioritizing samples near the decision boundary for replay preserves inter-class separability more effectively than selecting average/centroid samples. Standard replay often selects "prototypical" samples (class centers). The Boundary-Aware Exemplar Prioritization (BAEP) strategy explicitly selects samples that maximize deviation from the class mean and other selected exemplars (Eq 21). By forcing the model to replay these "hard" or "edge" cases, the model maintains a tighter, more robust decision boundary between old and new classes, combating the bias towards new, data-rich classes.

## Foundational Learning

- **Concept: Self-Supervised Contrastive Learning (SimCLR/CaSSLe)**
  - **Why needed here:** This drives the Class-Agnostic branch. Without it, the model cannot learn the "stable" general features required to guide the specific branch.
  - **Quick check question:** Can you explain how contrastive loss pulls positive pairs (augmented views of same sample) together and pushes negative pairs apart without using ground truth labels?

- **Concept: Knowledge Distillation (Feature-level)**
  - **Why needed here:** Used in the Class-Specific branch to prevent catastrophic forgetting by forcing the current model's features to mimic the previous model's features for old data.
  - **Quick check question:** What is the difference between logit-based distillation and feature-based distillation, and why might feature-based be preferred for preserving spatial/sequential fault characteristics?

- **Concept: Class-Incremental Learning (CIL) Stability-Plasticity Dilemma**
  - **Why needed here:** The entire DGGN architecture is a solution to this dilemma.
  - **Quick check question:** In the context of DGGN, which component handles "plasticity" (learning new data) and which handles "stability" (retaining old knowledge)?

## Architecture Onboarding

- **Component map:** Input (Fault signal) -> Dual 1D ResNet-18 (CS + CA) -> MOIA (CS blocks) -> MSCA (Fusion) -> Replay Buffer (BAEP) -> Balanced Random Forest (Classifier)

- **Critical path:**
  1. Initialize and train both branches on Base Session (large data).
  2. For Incremental Session:
     - Update CA branch using self-supervision (predictor alignment).
     - Select Replay Exemplars using BAEP (Eq 21).
     - Train CS branch: Compute Supervised Contrastive Loss + Distillation Loss (from old frozen model) + KL Alignment Loss (from CA branch).
     - Update BRF classifier with new features + replay buffer.

- **Design tradeoffs:**
  - **Dual-Branch vs. Single-Branch:** Higher computational/memory cost for significantly better stability (proven by ablation w/o CA model).
  - **BRF vs. FC Classifier:** BRF handles class imbalance natively but adds inference latency and complexity compared to a simple FC layer.
  - **BAEP vs. Herding:** BAEP is computationally more intensive during sample selection (calculating distances) but yields better boundary preservation.

- **Failure signatures:**
  - **Rapid Accuracy Drop:** Check Class-Agnostic branch updates; if not frozen/regularized properly, semantic drift invalidates guidance.
  - **Confusion on New Classes:** Check MOIA module; if dilation rates are mismatched to signal frequency, feature extraction fails.
  - **Bias to Majority Class:** Check BRF ensemble voting or BAEP buffer balance.

- **First 3 experiments:**
  1. **Ablation of Guidance:** Run DGGN vs. DGGN without the Cross-Domain Knowledge Alignment (L_kl) to quantify the impact of the CA stream on the CS stream (Table III supports this).
  2. **Replay Strategy Comparison:** Compare BAEP vs. Random selection vs. Herding on a long-tailed dataset (Fig 6 supports this).
  3. **Generalization Test:** Train on TEP dataset, test on MFF (or varying fault types within TEP as per Table VI) to verify if CA features are truly domain-agnostic or just dataset-specific.

## Open Questions the Paper Calls Out

- **Cross-Device/Domain Adaptation:** How can the DGGN framework be effectively adapted for cross-device or cross-domain fault diagnosis scenarios? The paper explicitly states future work will focus on extending the framework to more complex cross-device or cross-domain scenarios, but current evaluation is limited to individual datasets.

- **Model Efficiency Optimization:** Can the dual-branch architecture be optimized to reduce computational overhead while maintaining the decoupled representation of class-agnostic and class-specific features? The authors identify optimizing the model architecture for greater efficiency as a primary goal for future research.

- **Dynamic Memory Management:** What dynamic memory management strategies can be developed to improve upon the fixed memory capacity constraint used in the Boundary-Aware Exemplar Prioritization (BAEP)? The paper identifies developing dynamic memory management methods as a key area for future work.

## Limitations
- The approach relies on the assumption that fault signals contain stable, class-agnostic features that remain relevant across incremental sessions, which may not hold for fundamentally different fault mechanisms
- The computational overhead of dual-branch training and the BRF classifier may hinder deployment in resource-constrained industrial settings
- The effectiveness of BAEP in high-dimensional or extremely noisy feature spaces is unverified due to limited cross-domain validation

## Confidence
- **High Confidence:** The experimental results demonstrating superior performance on TEP and MFF datasets compared to baseline methods
- **Medium Confidence:** The theoretical framework for decoupling class-specific and class-agnostic knowledge, given the reliance on SimCLR/CaSSLe for feature learning
- **Low Confidence:** The claim that boundary-aware exemplar prioritization universally outperforms other strategies across all fault domains, due to limited cross-domain validation

## Next Checks
1. **Cross-Domain Transfer:** Train DGGN on TEP and evaluate on a completely different fault diagnosis dataset (e.g., CWRU bearing dataset) to test the robustness of class-agnostic features
2. **Ablation of CA Guidance:** Remove the Cross-Domain Knowledge Alignment (L_kl) and measure the impact on forgetting in long-tailed incremental scenarios
3. **Failure Mode Analysis:** Deliberately introduce distribution shifts between sessions (e.g., new fault types with different physical origins) and assess whether the CA guidance degrades into noise