---
ver: rpa2
title: Goal inference with Rao-Blackwellized Particle Filters
arxiv_id: '2512.09269'
source_url: https://arxiv.org/abs/2512.09269
tags:
- intent
- agent
- particles
- estimator
- particle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a Rao-Blackwellized Particle Filter (RBPF) approach
  for inferring the eventual goal of a mobile agent from noisy trajectory observations.
  The method exploits the assumed closed-loop dynamics of the agent to analytically
  marginalize the linear-Gaussian state substructure, updating only particle weights
  rather than sampling both state and intent parameters.
---

# Goal inference with Rao-Blackwellized Particle Filters

## Quick Facts
- arXiv ID: 2512.09269
- Source URL: https://arxiv.org/abs/2512.09269
- Reference count: 37
- Primary result: Rao-Blackwellized Particle Filter with reduced estimator achieves nearly equivalent intent inference with significantly lower computational cost than complete estimator.

## Executive Summary
This work presents a Rao-Blackwellized Particle Filter (RBPF) approach for inferring the eventual goal of a mobile agent from noisy trajectory observations. The method exploits the assumed closed-loop dynamics of the agent to analytically marginalize the linear-Gaussian state substructure, updating only particle weights rather than sampling both state and intent parameters. This significantly improves sample efficiency compared to standard particle filters. Two density-based estimators are introduced: a complete estimator using all particles and a reduced estimator using only effective particles. The performance of these estimators is quantified using information-theoretic leakage metrics, specifically Kullback-Leibler divergence.

## Method Summary
The approach uses a Rao-Blackwellized Particle Filter to infer intent parameters θ = (goal location, radius, arrival time) from noisy trajectory observations. Particles are maintained over intent parameters, while the agent's kinematic state is propagated analytically via Kalman filtering for each particle. Particle weights update based on measurement likelihood p(y_k|ẑ_k). A reduced estimator using only effective particles is shown to perform nearly as well as the complete estimator with bounded information loss. Resampling based on effective sample size maintains particle diversity while concentrating computational effort on high-likelihood hypotheses.

## Key Results
- RBPF improves sample efficiency by analytically marginalizing linear-Gaussian state dynamics while sampling only non-linear intent parameters
- Reduced estimator (effective particles only) provides nearly equivalent intent inference to complete estimator with bounded information loss
- Numerical experiments demonstrate fast and accurate intent recovery, with goal inference well before halfway point of trajectory
- Reduced estimator consistently maintains lower final estimation errors while requiring less computational resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rao-Blackwellization improves sample efficiency by analytically marginalizing linear-Gaussian state dynamics while only sampling non-linear intent parameters.
- **Mechanism:** The filter maintains particles over intent θ = (goal location, radius, arrival time). For each particle, the agent's kinematic state is propagated analytically via Kalman filtering (Equations 7-9), exploiting the conditionally linear-Gaussian structure of the closed-loop dynamics. Particle weights update only from measurement likelihood p(y_k|ẑ_k).
- **Core assumption:** Agent closed-loop dynamics follow f_θ(x) = -λ(θ)(x - x(θ)) + disturbance, inducing conditional linear-Gaussian state evolution (Assumption 1, Section 2).
- **Evidence anchors:**
  - [abstract] "Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter."
  - [section 3.1] Equations 5-9 show the probabilistic transition model and Kalman update equations.
  - [corpus] Neighbor paper "Robust Position Estimation by Rao-Blackwellized Particle Filter" corroborates RBPF effectiveness for position estimation without integer ambiguity resolution.
- **Break condition:** If dynamics are not conditionally linear-Gaussian given intent, analytical marginalization fails; must fall back to full particle filter or different factorization.

### Mechanism 2
- **Claim:** The reduced estimator (effective particles only) provides nearly equivalent intent inference to the complete estimator with bounded information loss.
- **Mechanism:** Define effective sample set M_k as top N_eff particles by weight. The reduced estimator (Equation 14) computes weighted mixture only over M_k. Theorem 1 proves |H^com - H^red| ≤ Σ(½ - ½√(2N))log(C_ν), where the bound shrinks as particle count N grows.
- **Core assumption:** Ineffective particles (low weights) contribute negligibly to information about true intent; this is validated by the derived KL bounds.
- **Evidence anchors:**
  - [abstract] "We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one."
  - [section 4] Theorem 1 provides explicit lower bounds on KL divergence and upper bound on estimator difference.
  - [corpus] Limited direct corpus support for this specific estimator comparison; claim rests primarily on paper's theoretical derivation.
- **Break condition:** If particle weights become near-uniform across all particles, the effective/ineffective distinction breaks down; reduced estimator may lose significant information.

### Mechanism 3
- **Claim:** Resampling based on effective sample size (N_eff) maintains particle diversity while concentrating computational effort on high-likelihood hypotheses.
- **Mechanism:** N_eff = (Σω²)^(-1) tracks weight concentration. When N_eff < N_0, retain top N_0 particles, replicate proportionally to weights, and reinitialize remaining N - N_0 particles randomly (Section 3.1). This prevents weight collapse while exploring the intent space.
- **Core assumption:** Random reinitialization maintains coverage of intent space Θ; goal lies within the bounded domain defined in Equation 1.
- **Evidence anchors:**
  - [section 3.1] "When N_eff drops below a predefined threshold N_0, we retain the N_0 highest weight particles and discard those with negligible weights."
  - [section 5.2] "Across all runs, N_eff initially falls as weights become uneven, then rises after resampling, and oscillates during late-stage convergence."
  - [corpus] Standard particle filter resampling strategies; no novel mechanism here, but well-motivated in this context.
- **Break condition:** If resampling threshold N_0 is set too low, particle diversity collapses; if too high, computational waste on low-likelihood hypotheses.

## Foundational Learning

- **Concept: Rao-Blackwellized Particle Filtering**
  - Why needed here: Core algorithm enabling efficient joint estimation of non-linear intent and linear-Gaussian state.
  - Quick check question: Can you explain why marginalizing the linear-Gaussian substructure reduces variance compared to sampling all variables?

- **Concept: Kalman Filtering**
  - Why needed here: Provides analytical state updates for each intent particle; required to implement Equations 5-9.
  - Quick check question: Given prior covariance P⁻ and measurement noise Δ, can you derive the Kalman gain K = P⁻(P⁻ + Δ)⁻¹?

- **Concept: Kullback-Leibler Divergence for Gaussian Mixtures**
  - Why needed here: Quantifies information leakage; used to evaluate estimator quality and bound performance gaps.
  - Quick check question: For two Gaussians N(μ₁, σ²) and N(μ₂, σ²), what is the closed-form KL divergence? (Answer: (μ₁-μ₂)²/(2σ²))

## Architecture Onboarding

- **Component map:**
  Particle store -> Kalman updater -> Weight normalizer -> Resampler -> Estimator

- **Critical path:**
  1. Receive observation y_k
  2. For each particle: propagate state via dynamics (Eq 5-6), compute Kalman update (Eq 7-9)
  3. Update weights via likelihood p(y_k|ẑ_k^(i)) (Eq 10)
  4. Normalize weights, compute N_eff
  5. If N_eff < threshold: resample
  6. Output intent estimate (complete or reduced)

- **Design tradeoffs:**
  - Particle count N: Higher N improves coverage but linearly increases computation. Paper uses N=1200.
  - Resampling threshold N_0: Lower = more aggressive pruning, faster convergence but risk of mode collapse.
  - Noise parameter σ: Controls disturbance tail; affects exploration vs exploitation balance.
  - Estimator choice: Reduced is computationally cheaper; theoretical bound shows minimal information loss.

- **Failure signatures:**
  - Weight collapse: N_eff ≈ 1 early in trajectory; single particle dominates, all others near-zero weight.
  - Slow convergence: KL divergence remains high past mid-trajectory; check observation noise Δ vs process noise Σ.
  - Goal mislocalization: Posterior concentrates on wrong region; verify dynamics model matches agent behavior.
  - Radius/time estimation drift: Position converges but r, t estimates unstable; these require longer observation horizons.

- **First 3 experiments:**
  1. **Baseline validation:** Implement single-particle ground-truth test where true intent is known. Verify Kalman state estimates track actual trajectory within expected covariance bounds.
  2. **Convergence timing:** Run 100 trials varying goal distance (10m-50m), measure inference time T_inf. Verify exponential KL decay as shown in Figure 2; confirm reduced estimator matches complete within bound.
  3. **Noise robustness:** Systematically vary measurement noise Δ and process noise σ_d. Identify failure threshold where N_eff collapses before goal identification. Compare against theoretical sensitivity in Lemma 1's λ_θ bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can one design principled intent-obfuscating controllers that minimize the derived KL-based information leakage subject to task constraints and reference path tracking?
- Basis in paper: [explicit] Abstract and Conclusion ("motivating future work on designing intent-obfuscating controllers"; "principled design of obfuscation controllers that minimize KL-based leakage subject to task constraints").
- Why unresolved: The current work focuses solely on the inference mechanism (the "adversary" perspective) and assumes compliant agent behavior; it does not formulate or test counter-measures.
- What evidence would resolve it: A control policy derived to maximize the lower bounds on KL divergence defined in Theorem 1, demonstrated via simulations to maintain goal reachability while significantly increasing inference time $T_{inf}$.

### Open Question 2
- Question: Can the RBPF inference framework be extended to handle richer motion models (beyond the practical stability property) and non-Gaussian observation channels without losing the computational benefits of the analytical marginalization?
- Basis in paper: [explicit] Conclusion ("Future work includes richer motion models and non-Gaussian observation channels").
- Why unresolved: The current method relies on Assumption 1 (linearization via stable closed-loop dynamics) and Gaussian noise (Eq. 6) to facilitate the Kalman filter update; removing these breaks the specific linear-Gaussian substructure exploited by the RBPF.
- What evidence would resolve it: Derivation of a modified RBPF that handles non-linear dynamics or non-Gaussian noise, accompanied by empirical comparisons showing inference accuracy and runtime relative to the current Gaussian model.

### Open Question 3
- Question: How does the intent inference method scale and perform when extended to multi-agent settings?
- Basis in paper: [explicit] Conclusion ("extensions to multi-agent settings").
- Why unresolved: The formulation currently addresses a single mobile agent (Section 2), and particle filters often suffer from exponential complexity (curse of dimensionality) when estimating the joint intent of multiple agents.
- What evidence would resolve it: Theoretical or empirical analysis of the RBPF's accuracy and computational load in simulations involving multiple agents with potentially correlated or independent goals.

### Open Question 4
- Question: What is the sensitivity of the estimator to a mismatch between the adversary's assumed dynamic model (Eq. 2) and the agent's actual dynamics?
- Basis in paper: [inferred] Assumption 1 states the adversary estimates dynamics based on a specific provable practical stability property; however, real-world observers may estimate these dynamics incorrectly.
- Why unresolved: The paper derives KL lower bounds assuming the model matches the agent's behavior, but does not quantify performance degradation (robustness) if the assumed $\lambda(\theta)$ or disturbance bound $\bar{d}$ is incorrect.
- What evidence would resolve it: Numerical experiments measuring the increase in inference time and KL divergence error when the parameters used by the adversary's RBPF deviate from the true agent parameters.

## Limitations

- Assumes known agent dynamics and intent parameterization (closed-loop form with goal, radius, arrival time), which may not hold for real-world agents
- Performance bounds on estimator differences rely on specific Gaussian assumptions that may degrade with non-ideal particle distributions
- Limited empirical validation across diverse scenarios beyond the synthetic setup

## Confidence

- **RBPF mechanism**: High - backed by explicit equations and clear algorithmic structure
- **Reduced estimator equivalence**: Medium - supported by Theorem 1 bounds but limited empirical validation
- **Overall intent inference performance**: Medium-High for synthetic setup, untested on real-world scenarios

## Next Checks

1. **Noise sensitivity validation**: Systematically vary measurement noise Δ and process noise σ_d beyond nominal values to identify performance breakdown threshold where N_eff collapses before goal identification.

2. **Model mismatch testing**: Evaluate inference accuracy when assumed closed-loop dynamics deviate from true agent behavior (different damping coefficients or goal-seeking strategies).

3. **Real-world trajectory testing**: Apply method to actual pedestrian or vehicle trajectory datasets to assess performance with realistic noise patterns and behavior distributions.