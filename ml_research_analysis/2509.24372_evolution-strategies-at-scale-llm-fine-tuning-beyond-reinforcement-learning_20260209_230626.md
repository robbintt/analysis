---
ver: rpa2
title: 'Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning'
arxiv_id: '2509.24372'
source_url: https://arxiv.org/abs/2509.24372
tags:
- fine-tuning
- uni00a0
- learning
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Evolution strategies (ES) were successfully scaled to fine-tune
  the full parameters of billion-parameter large language models (LLMs), demonstrating
  that ES can efficiently optimize in extremely high-dimensional spaces and outperform
  established reinforcement learning (RL) methods in multiple respects: sample efficiency,
  robustness to long-horizon rewards, stability across runs, and reduced susceptibility
  to reward hacking. In the Countdown task with sparse outcome-only rewards, ES achieved
  substantially better accuracy than RL across various model families and sizes.'
---

# Evolution Strategies at Scale: LLM Fine Tuning Beyond Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.24372
- Source URL: https://arxiv.org/abs/2509.24372
- Reference count: 40
- Evolution strategies scaled to fine-tune full parameters of billion-parameter LLMs, outperforming RL in sample efficiency, robustness, and stability

## Executive Summary
Evolution Strategies (ES) have been successfully scaled to perform full-parameter fine-tuning of large language models (LLMs), demonstrating capabilities beyond traditional reinforcement learning approaches. The research team applied ES to diverse tasks including sparse reward environments, conciseness optimization, and mathematical reasoning, achieving state-of-the-art results while addressing key limitations of RL-based fine-tuning. The method showed particular strengths in sample efficiency, robustness to reward hacking, and stability across independent runs, establishing ES as a viable backpropagation-free alternative for LLM optimization.

## Method Summary
The research team developed a scalable implementation of Evolution Strategies (ES) for fine-tuning large language models with billions of parameters. The approach leverages parallelized evaluation of perturbed model parameters across GPU clusters, using Gaussian noise to explore the parameter space. The method was applied to various model families including T5, Gemma, and Llama across different sizes, with particular focus on tasks featuring sparse rewards and long-horizon optimization challenges. The implementation maintained computational efficiency through careful parallelization strategies while preserving the core evolutionary optimization mechanism.

## Key Results
- Achieved substantially better accuracy than RL in sparse reward Countdown tasks across multiple model families
- Found superior Pareto frontiers in conciseness fine-tuning without reward hacking or KL divergence penalties
- Matched or exceeded SOTA RL baselines on math reasoning benchmarks while solving challenging puzzle tasks that base models failed

## Why This Works (Mechanism)
ES operates by maintaining a population of parameter perturbations evaluated in parallel, allowing exploration of the high-dimensional parameter space without backpropagation. The evolutionary mechanism naturally handles sparse rewards through population-based sampling, while the noise-based gradient estimation provides robustness to local optima. The parallel evaluation architecture enables efficient scaling to billions of parameters by distributing the computational load across GPU clusters, with each worker independently evaluating a perturbed model instance.

## Foundational Learning
**Evolution Strategies**: Population-based optimization algorithm that uses stochastic perturbations to explore parameter space
- Why needed: Provides gradient-free optimization suitable for non-differentiable objectives and sparse reward settings
- Quick check: Can handle discontinuous reward landscapes that break traditional RL algorithms

**Gaussian Smoothing**: Technique for estimating gradients through random perturbations
- Why needed: Enables gradient estimation without backpropagation, critical for non-differentiable reward functions
- Quick check: Variance of gradient estimates scales with dimensionality but remains manageable through population size tuning

**Parallel Population Evaluation**: Strategy for distributing model evaluations across multiple compute nodes
- Why needed: Makes ES computationally feasible for billion-parameter models by parallelizing expensive forward passes
- Quick check: Linear speedup achievable with sufficient parallelization, though communication overhead grows with population size

**Sparse Reward Handling**: Capability to optimize policies in environments with infrequent positive feedback
- Why needed: Many real-world tasks provide rewards only at completion, challenging credit assignment in RL
- Quick check: Population-based sampling naturally captures rare successful trajectories

## Architecture Onboarding
**Component Map**: Data parallel workers -> Parameter perturbation generator -> Reward evaluation pipeline -> Selection mechanism -> Model update
**Critical Path**: Parameter generation → Parallel evaluation → Reward aggregation → Gradient estimation → Model update
**Design Tradeoffs**: Population size vs. convergence speed vs. computational cost; noise magnitude vs. exploration vs. stability
**Failure Signatures**: Premature convergence to suboptimal policies; excessive variance in gradient estimates; communication bottlenecks in distributed evaluation
**First Experiments**: 1) Verify gradient estimation accuracy on synthetic test functions 2) Benchmark sample efficiency on simple control tasks 3) Test scaling behavior on incrementally larger model sizes

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Computational intensity remains substantial despite efficiency gains, with fine-tuning runs requiring 24-48 hours on GPU clusters
- Evaluation scope concentrated on specific task types and model architectures, limiting generalizability claims
- Limited systematic probing of potential adversarial reward scenarios to test robustness claims

## Confidence
- ES scales effectively to full-parameter fine-tuning of billion-parameter models: High
- Superior sample efficiency compared to RL: High
- Reduced susceptibility to reward hacking: Medium
- Matches or exceeds SOTA RL baselines on math reasoning: Medium

## Next Checks
1. Systematic scaling experiments to establish relationship between model size and ES efficiency gains, particularly comparing ES to LoRA/QLoRA for parameter-efficient fine-tuning
2. Controlled studies investigating ES behavior under adversarial reward design to rigorously test robustness to reward hacking
3. Cross-architecture validation extending ES fine-tuning to multimodal models and different architectural families beyond current focus on encoder-decoder and decoder-only transformers