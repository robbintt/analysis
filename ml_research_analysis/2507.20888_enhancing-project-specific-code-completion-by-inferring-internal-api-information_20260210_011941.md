---
ver: rpa2
title: Enhancing Project-Specific Code Completion by Inferring Internal API Information
arxiv_id: '2507.20888'
source_url: https://arxiv.org/abs/2507.20888
tags:
- code
- information
- completion
- function
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to enhance project-specific code completion
  by inferring internal API information without relying on import statements. The
  approach constructs a knowledge base containing usage examples and functional semantic
  descriptions for each internal API, and uses a code draft generated by LLMs to guide
  the retrieval of necessary API information.
---

# Enhancing Project-Specific Code Completion by Inferring Internal API Information

## Quick Facts
- **arXiv ID:** 2507.20888
- **Source URL:** https://arxiv.org/abs/2507.20888
- **Reference count:** 40
- **Primary result:** 22.72% relative improvement in code exact match over baselines

## Executive Summary
This paper addresses the challenge of completing code that uses internal APIs not present in the LLM's training data. Traditional approaches rely on import statements to retrieve relevant API information, but this is problematic for in-progress code without imports. The proposed method constructs a knowledge base of internal APIs enriched with usage examples and functional descriptions, then uses an initial LLM-generated code draft to retrieve the necessary information for final completion. The approach significantly outperforms existing baselines in both code and identifier exact match metrics.

## Method Summary
The method builds a knowledge base containing usage examples and functional semantic descriptions for each internal API, then uses an initial LLM-generated code draft to guide retrieval of relevant API information. This approach bypasses the need for import statements, making it suitable for real-world development scenarios where code is incomplete. The system combines vector embeddings of API usage patterns and LLM-generated docstrings to retrieve the most relevant information for completing the code.

## Key Results
- Achieves 22.72% relative improvement in code exact match compared to existing baselines
- Achieves 18.31% relative improvement in identifier exact match
- When integrated with existing baselines, boosts performance by 47.80% in code match and 35.55% in identifier match on average

## Why This Works (Mechanism)

### Mechanism 1: Code Draft as API Retrieval Signal
The system generates an initial, imperfect LLM code draft that contains sufficient cues to identify relevant internal APIs. This draft is used to extract API call signatures and functional summaries as queries against specialized retrieval indexes.

### Mechanism 2: Dual-Channel API Knowledge Base
A structured knowledge base enriches raw API definitions with synthetic usage examples (generated via heuristics) and LLM-generated docstrings, enabling more robust matching than raw code alone.

### Mechanism 3: Import-Free Context Retrieval
The system retrieves API information based on semantic and syntactic content of unfinished code rather than import statements, making it more robust for real-world, in-progress development.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) for Code**
  - Why needed here: The entire method is built on the RAG paradigm, retrieving information to augment the LLM prompt
  - Quick check question: Why might a standard code LLM fail to complete a line calling a function from a different project file?

- **Concept: Vector Embeddings and Semantic Similarity**
  - Why needed here: API retrieval relies on embedding code into vectors and calculating cosine similarity to find "semantic" matches
  - Quick check question: How would you represent a function and its summary so a computer can compare their "meaning" rather than just text?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The system uses ICL to prompt an LLM to generate consistent docstrings for all APIs
  - Quick check question: If you wanted an LLM to summarize code in a specific format, what would you put in its prompt before giving it the code?

## Architecture Onboarding

- **Component map:**
  1. Static Analyzer -> Extracts raw API info (signatures, bodies, paths)
  2. Knowledge Base Constructor -> UE Generator (creates usage examples via heuristics) -> Docstring Generator (LLM creates functional descriptions using ICL) -> Embedder (creates vector embeddings)
  3. Code Completion Pipeline -> Draft Generator (creates initial code draft using in-file context) -> Retrieval System (UER matches draft API calls, FSR matches summarized draft intent, Similar Code Retriever fetches similar code) -> Final Generator (produces final completion using retrieved info)

- **Critical path:** The retrieval system's ability to correctly identify relevant APIs from a noisy code draft is the linchpin. A failure here propagates errors to the final generator.

- **Design tradeoffs:**
  - Heuristic UEs vs. Generating All Variants: Heuristics are fast but may miss obscure patterns
  - LLM-Generated vs. Human-Written Docstrings: LLMs offer scalability and consistency
  - Draft Quality vs. Performance: System is robust to imperfect drafts, but nonsensical drafts yield poor retrieval

- **Failure signatures:**
  - Incorrect API Retrieval: UER/FSR returns irrelevant APIs, leading to wrong completion
  - Docstring Inaccuracy: LLM generates misleading docstring, causing FSR failure
  - Draft Divergence: Initial draft is too different from ground truth, providing no retrieval signal
  - Context Overload: Retrieved info exceeds LLM's context window, forcing truncation

- **First 3 experiments:**
  1. Baseline Knowledge Base Construction: Measure quality of generated UEs and docstrings via manual inspection
  2. Ablation on Retrieval Channels: Evaluate system with only UER, only FSR, and both to quantify each channel's contribution
  3. End-to-End with Leaked Imports: Run full system on dataset where imports are not masked and compare against baseline that relies on imports

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of heuristic-based UE generation and LLM-generated docstrings lacks direct validation on this dataset
- The method's robustness for APIs with highly irregular or non-idiomatic usage patterns is not tested
- Reliance on UniXcoder embeddings introduces unstated dependency on embedding quality for internal APIs

## Confidence

- **High confidence:** The overall architecture is logically sound and the retrieval-augmented generation mechanism is established in prior literature
- **Medium confidence:** The effectiveness of heuristic-based UE generation and LLM-generated docstrings is plausible but lacks direct validation
- **Low confidence:** The robustness of import-free retrieval in the wild, particularly when the code draft is highly divergent from the target API's usage, is not tested

## Next Checks

1. **Retrieval Channel Ablation:** Run experiments isolating UER and FSR to quantify each channel's contribution and identify which dominates under which conditions

2. **UE Heuristic Robustness:** Test the system on a small, manually curated set of APIs with known unusual usage patterns to measure heuristic coverage

3. **Import-Free vs. Import-Using Baseline:** Compare the method against a strong baseline (e.g., RepoFuse) on code where imports are present to validate the practical advantage of import-free retrieval