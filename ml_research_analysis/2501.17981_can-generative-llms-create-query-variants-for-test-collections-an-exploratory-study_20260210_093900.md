---
ver: rpa2
title: Can Generative LLMs Create Query Variants for Test Collections? An Exploratory
  Study
arxiv_id: '2501.17981'
source_url: https://arxiv.org/abs/2501.17981
tags:
- query
- variants
- information
- sets
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study

## Quick Facts
- **arXiv ID**: 2501.17981
- **Source URL**: https://arxiv.org/abs/2501.17981
- **Reference count**: 25
- **Key outcome**: LLMs with one-shot prompting can generate query variants that partially replicate human query formulations and produce overlapping relevant document pools (up to 71.1% at depth 100)

## Executive Summary
This paper investigates whether generative LLMs can substitute for human-generated query variants in test collection construction. Using one-shot prompting with GPT-3.5, the study generates query variants from backstories (information need descriptions) and compares them to human-generated variants from the UQV100 test collection. While LLM-generated queries show only modest overlap with human queries (10-19% coverage ratio), they retrieve highly overlapping sets of relevant documents (71.1% overlap at pool depth 100), suggesting LLMs could significantly reduce human effort in test collection construction. The study systematically examines temperature settings and their impact on query diversity and pool properties.

## Method Summary
The method uses GPT-3.5 (text-davinci-003) with one-shot learning to generate query variants from backstories in the UQV100 collection. Each prompt contains a task description, a single example (Topic 275), and the target backstory. The generated queries are evaluated against human-generated variants using Jaccard Index and coverage ratios, while retrieval effectiveness is measured using BM25 on the ClueWeb12-B corpus with metrics including P@10, NDCG@10, and RBP. Document pool overlap is analyzed at various depths (10, 50, 100) to assess the practical utility of LLM-generated queries for test collection construction.

## Key Results
- GPT-generated queries show 10-19% coverage ratio when compared to human queries under relaxed text transformations
- Document pools from GPT and human queries overlap by 43.7% at depth 10 and 71.1% at depth 100 for relevant documents
- Human query sets yield significantly larger pools (~190 documents at depth 10) than GPT sets (~93-105 documents)
- Lower temperature (0.0) produces more query variants but potentially less natural variation compared to higher temperature (1.0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with one-shot prompting can generate query variants that partially replicate human query formulations from backstories.
- Mechanism: The LLM is conditioned via a prompt containing (a) a natural language task description specifying expected query distribution, (b) a single example backstory with human-generated output queries, and (c) the target backstory. This in-context learning leverages the model's pretrained understanding of query-backstory relationships.
- Core assumption: The LLM has internalized sufficient knowledge of how humans formulate search queries from information needs during pretraining.
- Evidence anchors:
  - [abstract] "We explore how similar the queries generated by the LLM are to those generated by humans."
  - [section 2.1] "We follow a one-shot learning approach, in which the prompt contains an example input backstory with its associated human-generated output queries."
  - [corpus] Neighbor paper "Demographically-Inspired Query Variants Using an LLM" extends this approach for demographic diversity, suggesting replicability.
- Break condition: If backstories are domain-specific or use terminology outside the LLM's training distribution, query quality degrades. Zero-shot produced "long variants that closely resembled natural language questions" (Section 2.1).

### Mechanism 2
- Claim: Query variant overlap translates into substantial relevant document pool overlap at sufficient retrieval depths.
- Mechanism: Even when GPT queries differ lexically from human queries, they retrieve overlapping relevant documents because keyword-based ranking models (BM25) treat semantically similar queries as functionally equivalent for retrieval purposes.
- Core assumption: BM25's term-matching behavior generalizes across query formulations expressing the same information need.
- Evidence anchors:
  - [abstract] "...they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100."
  - [section 3.2] "When considering relevant documents only, the overlap is considerably high... the pools overlap at 43.7% at depth 10. This increases to 71.1% when examining the pools at depth 100."
  - [corpus] Evidence limited; neighbor papers focus on query expansion rather than pool construction overlap.
- Break condition: If retrieval model is semantic/dense rather than lexical, overlap patterns may differ. Results specific to BM25 (b=0.4, k1=0.9) on ClueWeb12-B.

### Mechanism 3
- Claim: Temperature settings control query diversity but trade off against human-likeness and pool coverage.
- Mechanism: Higher temperature increases stochasticity in generation, producing fewer variants with less diversity (Table 1: temp=1.0 yields 2,725 variants vs. temp=0.0 yields 4,803). Lower temperature produces more variants but potentially less natural variation.
- Core assumption: Query diversity correlates positively with pool size growth and retrieval of diverse relevant documents.
- Evidence anchors:
  - [section 2.1] Table 1 shows variant counts decreasing with higher temperature.
  - [section 3.2] Figure 5 shows human set pool size grows faster than GPT sets, with human pool size ~190 vs GPT ~93-105 at depth 10.
  - [corpus] Not directly addressed in neighbor papers.
- Break condition: Optimal temperature is dataset-dependent; no single setting maximizes both human-similarity and pool diversity simultaneously.

## Foundational Learning

- Concept: **Cranfield Paradigm / Test Collections**
  - Why needed here: The entire paper frames LLM query generation as a potential solution for test collection construction, which relies on the Cranfield approach of using document pools from system runs.
  - Quick check question: Can you explain why document pooling is necessary for reusable test collections?

- Concept: **Query Variants**
  - Why needed here: The paper's core object of study is whether LLMs can substitute for human-generated query variants, which are alternative formulations of the same information need.
  - Quick check question: Given the backstory "You want to hike in Sangre de Cristo mountains in summer," what are two plausible query variants a user might issue?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The method relies on zero/one/few-shot prompting rather than fine-tuning, which is the technical enabler for rapid query generation without domain-specific training data.
  - Quick check question: What is the difference between zero-shot and one-shot learning in the context of this paper's prompting strategy?

## Architecture Onboarding

- Component map: Backstories → Prompt construction → LLM generation (temp=0.5 recommended) → BM25 retrieval → Pool construction → Overlap analysis vs. human baseline
- Critical path: Backstory → Prompt construction → LLM generation (temp=0.5 recommended) → BM25 retrieval → Pool construction → Overlap analysis vs. human baseline
- Design tradeoffs:
  - **Temperature selection**: Lower (0.0) = more variants, less natural; Higher (1.0) = fewer variants, more compact pools
  - **Shot count**: Zero-shot fails (produces questions not queries); One-shot works; Few-shot may improve but limited by available backstories
  - **Pool depth**: Deeper pools increase overlap (43.7% → 71.1% from depth 10 → 100) but require more computation
- Failure signatures:
  - Queries resemble natural language questions rather than keyword searches (zero-shot failure mode)
  - Pool size significantly smaller than human baseline (~50% of human pool size)
  - High proportion of unjudged documents (0.31-0.37 vs. 0.13 for human)
  - Statistically significant effectiveness drops (P@10: 0.384-0.393 vs. 0.443 human, p<0.01)
- First 3 experiments:
  1. **Reproduce baseline overlap metrics**: Run one-shot prompting with temp=0.5 on UQV100 (excluding Topic 275), compute Jaccard overlap with human queries under T0-T4 transformations. Target: 10-19% coverage ratio.
  2. **Vary temperature and measure pool properties**: Compare pool size, relevant document overlap, and unjudged proportion across temp={0.0, 0.5, 1.0} at depths 10, 50, 100.
  3. **Investigate unjudged documents**: Manually sample unjudged documents retrieved by GPT but not human queries to assess whether they contain relevant information missed by human pooling.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent do LLM-generated queries approximate human queries when assessed via human evaluation rather than automated metrics?
  - Basis in paper: [explicit] The authors state that incorporating human evaluation to assess the extent to which GPT sets approximate human queries "is a question to be explored in future research."
  - Why unresolved: The current study relied on automated text overlap metrics (Jaccard) and retrieval metrics, which may not fully capture semantic appropriateness or naturalness.
  - What evidence would resolve it: Human annotator ratings on the naturalness and utility of the LLM-generated queries compared to the human baseline.

- **Open Question 2**: Do the documents retrieved by LLM-generated queries but marked as "unjudged" in current test collections actually contain relevant information?
  - Basis in paper: [explicit] The authors note that "it would be interesting to further investigate the unjudged portion of the GPT sets to understand whether they retrieve relevant documents that were not found through the human set."
  - Why unresolved: The relevance judgments (qrels) used were based on pools formed by human variants; thus, documents retrieved exclusively by LLMs have no ground-truth label.
  - What evidence would resolve it: Manual relevance assessment of the documents retrieved uniquely by the LLM-generated query variants to determine if they are relevant.

- **Open Question 3**: How does generating query variants from backstories using LLMs compare to previous simulation methods that generate variants from source documents?
  - Basis in paper: [explicit] The paper suggests that "further research could... compare our approach of using an LLM to generate query variants from backstories with previous query simulation methods (e.g., [4, 11]), which were used to generate query variants from source documents."
  - Why unresolved: This study only compared the LLM approach against human-generated queries, not against other automated algorithmic approaches.
  - What evidence would resolve it: A comparative analysis of retrieval effectiveness and pool diversity between backstory-based LLM generation and document-based simulation methods.

- **Open Question 4**: Can advanced prompting techniques increase the diversity of LLM-generated queries to match the pool size growth of human variants?
  - Basis in paper: [explicit] The authors propose that "further research could explore advanced prompting techniques," as the current results show human variants yield significantly larger pool sizes due to higher diversity.
  - Why unresolved: The one-shot prompting strategy used resulted in less diverse queries and smaller document pools compared to the human set.
  - What evidence would resolve it: Experiments utilizing few-shot or chain-of-thought prompting that demonstrate a higher rate of pool growth and unique query terms.

## Limitations

- The study relies on BM25 (lexical matching) for retrieval, which may not generalize to semantic retrieval models
- Results are limited to a single test collection (UQV100) and corpus (ClueWeb12-B)
- Temperature optimization is not systematically explored across diverse information needs
- Potential biases in LLM-generated queries and their impact on downstream evaluation metrics are not addressed

## Confidence

- **High Confidence**: Query generation mechanism (one-shot prompting with temperature control) - the method is clearly specified and directly supported by results showing systematic effects of temperature on variant count and pool size.
- **Medium Confidence**: Document pool overlap claims (71.1% at depth 100) - results are reproducible but depend on specific BM25 parameters and corpus characteristics that may not generalize.
- **Low Confidence**: Generalizability to other test collections, retrieval models, or LLMs - the study provides no evidence beyond the specific UQV100/ClueWeb12-B/BM25 configuration examined.

## Next Checks

1. **Replication with Semantic Retrieval**: Re-run the experiment using a semantic retrieval model (e.g., DPR or BM25+semantic reranking) to determine if document pool overlap patterns persist when lexical matching is not the primary ranking mechanism.

2. **Cross-Collection Generalization**: Apply the same methodology to a different test collection (e.g., TREC Robust or Gov2-based collections) to assess whether temperature effects and pool overlap patterns hold across diverse information needs and domains.

3. **Bias Analysis of Generated Queries**: Conduct a systematic analysis of potential demographic, topical, or stylistic biases in LLM-generated query variants compared to human variants, including examining whether certain types of information needs are systematically under-represented.