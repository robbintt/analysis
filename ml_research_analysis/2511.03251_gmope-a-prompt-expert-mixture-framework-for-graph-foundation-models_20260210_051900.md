---
ver: rpa2
title: GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models
arxiv_id: '2511.03251'
source_url: https://arxiv.org/abs/2511.03251
tags:
- graph
- expert
- prompt
- learning
- gmope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GMoPE integrates graph prompting with Mixture-of-Experts to address
  challenges in cross-domain graph foundation models. The framework uses expert-specific
  prompts and structure-aware MoE routing to enable dynamic specialization and balanced
  expert utilization.
---

# GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models

## Quick Facts
- arXiv ID: 2511.03251
- Source URL: https://arxiv.org/abs/2511.03251
- Reference count: 15
- GMoPE integrates graph prompting with Mixture-of-Experts to address challenges in cross-domain graph foundation models

## Executive Summary
GMoPE is a novel framework that combines graph prompting with Mixture-of-Experts (MoE) to address the challenges of cross-domain graph foundation models. The framework uses expert-specific prompts and structure-aware MoE routing to enable dynamic specialization and balanced expert utilization. A soft orthogonality loss prevents expert collapse by encouraging prompt diversity. The method demonstrates strong performance across diverse graph datasets while requiring minimal adaptation parameters.

## Method Summary
GMoPE addresses cross-domain generalization challenges in graph foundation models by integrating graph prompting with Mixture-of-Experts. The framework employs expert-specific prompts that are routed through a structure-aware gating mechanism, allowing dynamic specialization based on input graph characteristics. A soft orthogonality loss is introduced to prevent expert collapse and maintain prompt diversity across experts. The routing mechanism uses gating scores to select expert subsets for each input, while the orthogonality constraint ensures that experts learn distinct representations rather than converging to similar solutions.

## Key Results
- GMoPE outperforms state-of-the-art baselines across citation, e-commerce, and molecular graph datasets
- Achieves parameter efficiency by requiring less than 1% of learnable parameters during downstream transfer
- Matches full fine-tuning performance while maintaining significantly lower adaptation overhead

## Why This Works (Mechanism)
GMoPE works by combining the strengths of graph prompting and MoE architectures. The graph prompting component provides task-specific guidance through learnable prompts, while the MoE structure enables dynamic specialization through expert routing. The soft orthogonality loss ensures that experts maintain distinct capabilities rather than collapsing to similar representations. The structure-aware gating mechanism routes inputs to appropriate experts based on graph characteristics, enabling the model to handle diverse graph types effectively. This combination allows for both specialization (through experts) and generalization (through prompting), addressing the fundamental challenge of cross-domain adaptation.

## Foundational Learning
- **Graph prompting**: Why needed - Provides task-specific guidance without modifying model parameters; Quick check - Verify prompts can be effectively learned for new tasks
- **Mixture-of-Experts**: Why needed - Enables dynamic specialization for different graph types; Quick check - Confirm experts learn distinct representations
- **Expert collapse**: Why needed - Prevents all experts from learning identical representations; Quick check - Monitor orthogonality loss during training
- **Structure-aware routing**: Why needed - Ensures appropriate expert selection based on graph characteristics; Quick check - Validate routing decisions on known graph types
- **Parameter-efficient adaptation**: Why needed - Reduces computational overhead for downstream tasks; Quick check - Measure parameter count versus performance trade-off
- **Soft orthogonality constraints**: Why needed - Maintains diversity in expert representations; Quick check - Verify expert activation patterns remain diverse

## Architecture Onboarding

**Component Map**: Input Graph -> Structure-aware Gating -> Expert Selection -> Prompt Application -> Output Prediction

**Critical Path**: The critical path involves graph input processing through the gating network, expert routing based on gating scores, prompt application to selected experts, and final prediction aggregation. The gating mechanism determines which experts are activated for each input, while the orthogonality loss ensures diverse expert representations throughout training.

**Design Tradeoffs**: The framework balances between expert specialization and generalization through the gating mechanism and orthogonality constraints. Using fewer experts reduces computational cost but may limit specialization capability. The orthogonality weight must be carefully tuned to prevent expert collapse without overly constraining expert learning. The gating temperature affects routing sharpness and expert utilization balance.

**Failure Signatures**: Expert collapse occurs when all experts learn similar representations, failing the orthogonality constraint. Poor routing happens when the gating mechanism consistently selects wrong experts for certain graph types. Prompt inefficiency manifests when learned prompts fail to provide meaningful task guidance. Over-regularization from excessive orthogonality loss can prevent experts from learning useful representations.

**First Experiments**:
1. Test expert collapse prevention by training without orthogonality loss and measuring expert similarity
2. Evaluate routing quality by examining expert activation patterns across different graph types
3. Measure parameter efficiency by comparing full fine-tuning versus GMoPE adaptation on downstream tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness on graph structures beyond citation, e-commerce, and molecular datasets remains unproven
- Routing mechanism sensitivity to hyperparameter choices (gating temperature, orthogonality weight) not thoroughly explored
- Limited depth of analysis regarding which graph properties contribute to generalization success versus failure

## Confidence
- High confidence in parameter efficiency claims and MoE gating mechanism
- Medium confidence in generalization across the three tested domains
- Medium confidence in the effectiveness of soft orthogonality loss

## Next Checks
1. Evaluate GMoPE on temporal graphs and heterogeneous graphs to assess domain generality beyond the current dataset scope
2. Perform systematic ablation studies varying the gating temperature, orthogonality weight, and expert count to identify sensitivity boundaries
3. Compare GMoPE against other parameter-efficient fine-tuning methods (LoRA, adapters, prefix tuning) on the same tasks to establish relative performance advantages