---
ver: rpa2
title: 'How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards
  Realistic Evaluation'
arxiv_id: '2510.06448'
source_url: https://arxiv.org/abs/2510.06448
tags:
- transferability
- accuracy
- score
- metrics
- site
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critiques the standard benchmarks used to evaluate
  Source Independent Transferability Estimation (SITE) metrics, which rank pre-trained
  models by predicted performance on target tasks without fine-tuning. The authors
  identify three critical flaws: (1) the model space is unrealistic, dominated by
  large variants of two architectures, (2) the benchmark is solved by a static ranking
  where one model consistently wins, and (3) score differences do not meaningfully
  correlate with accuracy gaps.'
---

# How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation

## Quick Facts
- **arXiv ID:** 2510.06448
- **Source URL:** https://arxiv.org/abs/2510.06448
- **Reference count:** 40
- **Key outcome:** Standard SITE benchmarks are broken due to unrealistic model spaces, static hierarchies, and lack of fidelity between scores and accuracy differences.

## Executive Summary
This paper critiques the standard benchmarks used to evaluate Source Independent Transferability Estimation (SITE) metrics, which rank pre-trained models by predicted performance on target tasks without fine-tuning. The authors identify three critical flaws: (1) the model space is unrealistic, dominated by large variants of two architectures, (2) the benchmark is solved by a static ranking where one model consistently wins, and (3) score differences do not meaningfully correlate with accuracy gaps. Through empirical ablation studies, they show that simple static heuristics outperform sophisticated SITE metrics, and that these metrics' performance drops significantly when the model space is diversified. They provide actionable best practices for constructing more robust benchmarks, including diversifying model and dataset spaces, ensuring rank dispersion, and releasing evaluation code and data.

## Method Summary
The authors conduct a systematic critique of SITE metric evaluation by constructing a controlled experiment with 11 ImageNet pre-trained models (ResNets, DenseNets, Inception, GoogLeNet, MobileNet, MNASNet) and 6 target datasets (CIFAR10, CIFAR100, Pets, Aircraft, Food, DTD). They generate ground truth by fine-tuning all models on all datasets using a hyperparameter grid search, then compare various SITE metrics' rankings against this ground truth using Weighted Kendall's Tau. The key diagnostic tool is progressive model zoo ablation, where dominant architectures are sequentially removed to test metric robustness. They also evaluate a static ranking heuristic (sort by parameter count) as a baseline to demonstrate that current benchmarks can be solved without understanding transferability.

## Key Results
- A simple static ranking heuristic based on model size outperforms all sophisticated SITE metrics on standard benchmarks
- SITE metrics exhibit sharp performance drops when the model zoo is diversified through ablation studies
- Score differences from SITE metrics show weak to no correlation with actual accuracy differences between models

## Why This Works (Mechanism)

### Mechanism 1: Inflation via Static Hierarchy Exploitation
Current benchmarks rely on model zoos dominated by scaled variants of a few architectures. Because larger variants almost always outperform smaller ones, a metric can achieve high correlation simply by proxying for model size, bypassing the need to compute complex feature-label alignment. When the model zoo is diversified to break this size-performance correlation, these metrics fail.

### Mechanism 2: Brittleness to Search Space Diversity
The reliability of SITE metrics is conditional on a redundant, homogeneous model space. When "easy" comparisons are removed via ablation (forcing the metric to discriminate between models of similar capacity but different architectures), the ranking coherence collapses. This brittleness reveals that metrics are not learning to estimate transferability but are instead exploiting static hierarchies.

### Mechanism 3: The Fidelity Disconnect
High rank correlation does not imply that score magnitudes are meaningful for decision-making. Evaluation protocols typically optimize for ranking order, but the internal score distributions of metrics are often non-linear or compressed relative to accuracy gains. Consequently, a large score gap may indicate a tiny accuracy bump, while a small gap may indicate a large performance shift.

## Foundational Learning

- **Concept: Source Independent Transferability Estimation (SITE)**
  - **Why needed here:** The core subject of the paper. Unlike standard transfer learning, this scenario assumes no access to the massive source dataset, only the pre-trained weights and the target data.
  - **Quick check question:** If you have a pre-trained model's weights but not its original training data, can you predict its accuracy on a new task without fine-tuning?

- **Concept: Weighted Kendall's Tau (τw)**
  - **Why needed here:** This is the standard evaluation metric critiqued in the paper. It measures rank correlation but prioritizes the top of the leaderboard over the bottom.
  - **Quick check question:** Does a τw of 0.9 guarantee that the metric correctly identifies the #1 model, or just that the general ordering is good?

- **Concept: Model Zoo Ablation**
  - **Why needed here:** The primary diagnostic tool used in the paper to expose benchmark flaws. It involves systematically removing specific model families to test metric robustness.
  - **Quick check question:** If a metric's performance crashes when "ResNet-152" is removed from the pool, what does that imply about the metric's independence from model size?

## Architecture Onboarding

- **Component map:** Model Zoo -> SITE Metric (SUT) -> Score (Tm) -> Evaluator (τw) -> Ground Truth Loop (Fine-tuning) -> Ground Truth (Gm)
- **Critical path:** Constructing the Model Zoo. If the zoo is homogeneous, the subsequent evaluation is invalid regardless of the metric's sophistication.
- **Design tradeoffs:**
  - **Homogeneity vs. Difficulty:** A zoo with clear scaling laws (large > small) makes evaluation "stable" but trivial (false positives). A diverse zoo (ViTs, MLPs, CNNs of similar size) is "noisy" but represents the true problem difficulty.
  - **Rank vs. Fidelity:** Optimizing a metric for high τw does not optimize it for score fidelity (magnitude meaning).
- **Failure signatures:**
  - **The "Static Winner":** One model wins 8/10 tasks regardless of the dataset.
  - **Heuristic Baseline Defeat:** A simple size-based static ranking outperforms the learned metric.
  - **Flat Δ Correlation:** Scatter plot of Score Diff vs. Acc Diff shows a cloud, not a line.
- **First 3 experiments:**
  1. **Run the "Strawman" Baseline:** Implement a static ranker (Sort by Param Count) on your benchmark. If it beats your SITE metric, the benchmark is broken.
  2. **Progressive Ablation Study:** Sequentially remove the largest/most dominant models from your zoo. Plot the metric's τw decay. If it drops sharply, the metric is overfitting to scale.
  3. **Fidelity Stress Test:** Pick two pairs of models where Pair A has a small score gap (ΔT ≈ 0.01) and Pair B has a large score gap (ΔT ≈ 0.5). Verify if the accuracy gaps (ΔAcc) reflect this magnitude ordering.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can SITE metrics be adapted to account for the influence of different finetuning strategies, optimizers, and hyperparameters during evaluation?
  - **Basis in paper:** Current methods do not account for these factors, despite research showing they significantly impact model performance.
  - **Why unresolved:** Current benchmarks and metrics assume a fixed training pipeline, creating a disconnect between the estimated score and the actual performance potential under different training configurations.
  - **What evidence would resolve it:** A SITE evaluation protocol or metric that remains robust and predictive across a search space containing varying optimizers and hyperparameter settings.

- **Open Question 2:** Can a SITE metric be developed that ensures fidelity, where score differences strictly correlate with accuracy gaps?
  - **Basis in paper:** The authors explicitly critique current metrics for lacking "fidelity to accuracy differences," noting that score magnitude is often uninterpretable.
  - **Why unresolved:** Current metrics optimize for rank correlation (Kendall's Tau) rather than the semantic mapping between score gaps and performance improvements.
  - **What evidence would resolve it:** A metric demonstrating a high linear correlation between pairwise score differences (ΔT) and pairwise accuracy differences (ΔAcc) across diverse datasets.

- **Open Question 3:** To what extent do the identified benchmark flaws (static leaderboards, unrealistic model spaces) persist in non-CV domains like NLP or object detection?
  - **Basis in paper:** Section 5.4 notes that similar patterns (e.g., RoBERTa always winning in NLP) appear in other domains but the full empirical critique was not conducted there.
  - **Why unresolved:** The paper's main ablation studies focus on computer vision image classification, leaving the universality of these structural benchmark flaws unverified in other modalities.
  - **What evidence would resolve it:** Replicating the paper's ablation studies on standard NLP or object detection benchmarks to see if metric performance drops similarly.

## Limitations

- **Ground truth fidelity uncertainty:** The study relies on a specific fine-tuning protocol that may not generalize across different experimental setups, and small variations in training could alter the ground truth rankings.
- **Limited corpus support:** While the paper makes compelling empirical claims, there is limited citation support for its critiques in the broader literature, with neighboring papers focusing on unrelated domains.
- **Metric generality concerns:** The paper tests a specific set of 11 models and 6 datasets, leaving unclear whether the identified flaws persist when applied to newer architectures (e.g., Vision Transformers, MLP-Mixers) or different task domains.

## Confidence

- **High Confidence:** The claim that static hierarchies artificially inflate metric performance is strongly supported by the ablation study showing size-based heuristics outperform sophisticated metrics.
- **Medium Confidence:** The claim about fidelity disconnect (score differences not correlating with accuracy differences) is supported by specific examples but would benefit from more extensive statistical analysis across all dataset pairs.
- **Medium Confidence:** The claim about brittleness to search space diversity is well-demonstrated for the tested models but may not fully generalize to other architectural families.

## Next Checks

1. **Architectural generalization test:** Repeat the ablation study on a zoo containing ViTs, MLPs, and EfficientNets of similar parameter counts to verify the static ranking heuristic fails when size is no longer predictive.

2. **Fidelity correlation analysis:** Compute the Pearson correlation between score differences and accuracy differences across all 66 model-dataset pairs (11 models × 6 datasets) to quantify the strength of the fidelity disconnect.

3. **Ground truth robustness check:** Vary the fine-tuning protocol (epochs, augmentation, hyperparameter ranges) and measure how much the ground truth rankings change, to assess whether the benchmark's conclusions are protocol-dependent.