---
ver: rpa2
title: 'CDT: A Comprehensive Capability Framework for Large Language Models Across
  Cognition, Domain, and Task'
arxiv_id: '2509.24422'
source_url: https://arxiv.org/abs/2509.24422
tags:
- data
- capability
- capabilities
- framework
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CDT, a comprehensive framework that evaluates
  large language models (LLMs) across three orthogonal dimensions: Cognition, Domain,
  and Task. The Cognition dimension incorporates the Cattell-Horn-Carroll theory,
  identifying 18 core cognitive abilities; the Domain dimension organizes 33 subdomains;
  and the Task dimension categorizes 16 task types.'
---

# CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task

## Quick Facts
- arXiv ID: 2509.24422
- Source URL: https://arxiv.org/abs/2509.24422
- Reference count: 40
- Introduces CDT framework evaluating LLMs across Cognition, Domain, and Task dimensions

## Executive Summary
This paper introduces CDT, a comprehensive framework that evaluates large language models (LLMs) across three orthogonal dimensions: Cognition, Domain, and Task. The Cognition dimension incorporates the Cattell-Horn-Carroll theory, identifying 18 core cognitive abilities; the Domain dimension organizes 33 subdomains; and the Task dimension categorizes 16 task types. The authors develop specialized tag models for each dimension to annotate instruction datasets and apply CDT to two key applications: dataset evaluation and data selection. Experimental results demonstrate that CDT metrics correlate well with downstream performance and guide effective dataset construction. In data selection, CDT improves model performance on both general and specific benchmarks, achieving scores of 44.3 and 45.4 with increases of 1.6 and 2.2 points over baselines, respectively. These results validate CDT's effectiveness and practicality in enhancing LLM capabilities through targeted data curation.

## Method Summary
The CDT framework establishes a three-dimensional taxonomy for evaluating LLM capabilities. The Cognition dimension maps to 18 core cognitive abilities derived from Cattell-Horn-Carroll theory. The Domain dimension organizes 33 subdomains spanning various knowledge areas. The Task dimension categorizes 16 different task types. The authors develop specialized tag models for each dimension to systematically annotate instruction datasets. This annotation pipeline enables comprehensive dataset evaluation and targeted data selection. For data selection, the framework filters instruction datasets based on specific capability requirements, selecting data that aligns with desired cognitive abilities, domains, and task types. The methodology demonstrates practical applications in both evaluating dataset quality and constructing optimized training data.

## Key Results
- CDT-guided data selection improves model performance to 44.3 and 45.4 on general and specific benchmarks
- Performance gains of 1.6 and 2.2 points over baseline models demonstrate effectiveness
- CDT metrics show strong correlation with downstream performance, validating framework utility

## Why This Works (Mechanism)
The framework works by providing a structured, multi-dimensional approach to evaluating and selecting training data for LLMs. By decomposing capabilities into orthogonal dimensions (Cognition, Domain, Task), CDT enables precise targeting of model weaknesses and systematic dataset curation. The use of established psychological theory (Cattell-Horn-Carroll) for cognitive abilities provides a theoretically grounded foundation. The tag model annotation pipeline allows for scalable and consistent dataset evaluation across the complex taxonomy. This structured approach enables more effective data selection compared to unstructured or single-dimension approaches.

## Foundational Learning

**Cattell-Horn-Carroll (CHC) Theory**: A comprehensive psychological framework for understanding cognitive abilities, providing the theoretical basis for identifying core cognitive abilities in LLMs. Needed to ground cognitive capability identification in established psychological research. Quick check: Verify that selected 18 abilities comprehensively cover cognitive domains relevant to LLM tasks.

**Orthogonal Dimensions**: The separation of Cognition, Domain, and Task as independent axes allows for precise targeting of model capabilities without conflating different types of requirements. Needed to enable systematic analysis and selection of training data. Quick check: Confirm that capabilities in one dimension don't overlap significantly with capabilities in other dimensions.

**Tag Model Annotation**: Specialized models that automatically annotate instruction datasets with CDT tags across all three dimensions. Needed to scale the framework to large datasets efficiently. Quick check: Evaluate annotation accuracy and consistency across different dataset types and sizes.

## Architecture Onboarding

**Component Map**: Raw Instruction Datasets -> Tag Model Annotation (Cognition/Domain/Task) -> CDT-annotated Datasets -> Dataset Evaluation Metrics -> Data Selection Filters -> Optimized Training Data -> Model Training -> Performance Evaluation

**Critical Path**: The core workflow follows: dataset annotation → metric calculation → data selection → model training → evaluation. Each step depends on the previous one's output quality.

**Design Tradeoffs**: The framework trades annotation complexity (three specialized tag models) for precision in capability targeting. Simpler annotation approaches would be faster but less discriminative.

**Failure Signatures**: Poor downstream performance despite CDT-guided selection suggests either annotation model inaccuracies, inappropriate capability targeting, or insufficient data volume in selected categories.

**Three First Experiments**:
1. Evaluate annotation model accuracy by manual verification on sample dataset
2. Test correlation between CDT metrics and baseline model performance
3. Compare CDT-guided selection against random selection on same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Framework primarily validated on Chinese datasets; generalizability to other languages uncertain
- Tag model accuracy and consistency across complex taxonomy not thoroughly validated
- Modest absolute improvements (1.6-2.2 points) suggest room for refinement
- Scalability to evolving LLM capabilities and emerging task types remains unproven

## Confidence
- High: The framework's logical structure and orthogonality of dimensions
- Medium: The correlation between CDT metrics and downstream performance
- Medium: The practical effectiveness of CDT-guided data selection
- Low: Generalizability to non-Chinese datasets and other domains

## Next Checks
1. Cross-lingual validation: Test the framework's effectiveness on English and other language datasets to assess generalizability
2. Long-term stability analysis: Evaluate whether CDT-guided dataset construction maintains performance advantages over multiple model iterations
3. Tag model accuracy audit: Conduct comprehensive validation of the annotation models' accuracy and inter-annotator agreement across all three dimensions