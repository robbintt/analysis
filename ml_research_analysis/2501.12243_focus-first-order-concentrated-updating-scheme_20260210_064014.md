---
ver: rpa2
title: 'FOCUS: First Order Concentrated Updating Scheme'
arxiv_id: '2501.12243'
source_url: https://arxiv.org/abs/2501.12243
tags:
- focus
- adam
- learning
- signum
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FOCUS, an optimizer designed to improve Signum's
  performance on narrowing valley loss landscapes by adding an attraction force toward
  moving averaged parameters. The authors hypothesize that the pre-training loss landscape
  features narrowing valleys, and through synthetic loss function experiments, discover
  that Adam's performance degrades when gradient noise is large relative to valley
  sharpness because it reduces effective step size too drastically.
---

# FOCUS: First Order Concentrated Updating Scheme

## Quick Facts
- **arXiv ID:** 2501.12243
- **Source URL:** https://arxiv.org/abs/2501.12243
- **Reference count:** 40
- **Primary result:** FOCUS achieves approximately 2x speedup over Adam in GPT-2 small training while maintaining lower final validation loss

## Executive Summary
This paper introduces FOCUS, an optimizer designed to address Adam's performance degradation on narrowing valley loss landscapes when gradient noise is high relative to valley sharpness. The authors hypothesize that pre-training loss landscapes exhibit this narrowing valley structure, causing Adam's adaptive step sizes to collapse under noise. FOCUS combines Signum's fixed-magnitude updates with a self-attraction force toward moving-averaged parameters, maintaining larger effective step sizes while stabilizing oscillations in sharp valleys. Experiments show FOCUS achieves ~2x speedup over Adam on GPT-2 small pre-training while being more stable in float16 precision.

## Method Summary
FOCUS is an optimizer that updates parameters using momentum sign plus an attraction term towards EMA parameters. The update rule is: θₜ₊₁ = θₜ - ηₜ(sign(mₜ) + γ·sign(θₜ - θ̂ₜ)), where mₜ is momentum, θ̂ₜ is bias-corrected EMA of parameters, and γ controls attraction strength. The method inherits Signum's fixed step size from sign(mₜ) while adding γ·sign(θₜ - θ̂ₜ) to pull parameters toward their historical center-of-mass, counteracting oscillation-induced drift. This allows larger effective step sizes in sharp valleys where Adam's adaptive normalization would collapse.

## Key Results
- FOCUS achieves ~2x speedup over Adam on GPT-2 small pre-training (125M params, 9B tokens)
- FOCUS demonstrates greater stability than Adam in float16 precision training
- Theoretical analysis shows FOCUS achieves O(√T) regret bound matching Adam's convergence rate on convex problems
- FOCUS extends Signum's noise tolerance to higher sharpness regimes via self-attraction

## Why This Works (Mechanism)

### Mechanism 1: Adam's Effective Step Size Collapse Under High Noise
When gradient noise exceeds a critical threshold relative to valley sharpness, Adam's adaptive normalization excessively reduces effective step size, slowing convergence. Adam computes updates as -η·m̂ₜ/√v̂ₜ. Along sharp directions, gradients oscillate around zero, making m̂ₜ small while v̂ₜ accumulates squared noise. When noise dominates sharpness, the denominator inflates disproportionately, collapsing the effective step size regardless of true gradient direction.

### Mechanism 2: Self-Attraction Force for Valley Confinement
Adding an attraction term toward exponentially-weighted parameter averages allows fixed-magnitude updates to remain stable in sharp valleys. FOCUS updates via -η(sign(mₜ) + γ·sign(θₜ - θ̂ₜ)). The sign(mₜ) term provides fixed-magnitude gradient steps. The attraction term γ·sign(θₜ - θ̂ₜ) pulls parameters toward their historical center-of-mass, counteracting oscillation-induced drift. This acts as a "soft constraint" keeping trajectories compact within narrowing valleys.

### Mechanism 3: Noise-to-Sharpness Ratio as Regime Determinant
The relative magnitude of gradient noise σ to valley sharpness a determines optimizer superiority (Adam vs. Signum-family). Define critical noise σc(a) where Adam and Signum performance equal. Below σc, Adam's adaptive step sizes exploit curvature information effectively. Above σc, noise corrupts v̂ₜ estimates, and fixed-magnitude Signum updates become superior. FOCUS extends Signum's noise tolerance to higher sharpness regimes via self-attraction.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA) with bias correction**
  - Why needed here: FOCUS uses two EMAs—momentum mₜ and parameter history θ̄ₜ—with bias correction on the latter since initialization at zero introduces bias
  - Quick check question: Why does θ₀=0 require bias correction but momentum sign computation does not?

- **Concept: Sign-based gradient methods (Signum/SignSGD)**
  - Why needed here: FOCUS inherits Signum's core update -η·sign(mₜ); understanding fixed-magnitude vs. adaptive step sizes is essential for grasping why FOCUS avoids Adam's step-size collapse
  - Quick check question: What information is lost when using sign(g) instead of g, and why might this be acceptable in high-noise regimes?

- **Concept: Online convex optimization and regret bounds**
  - Why needed here: Section 4 provides O(√T) regret analysis; understanding regret R(T) = Σₜ[Lₜ(θₜ) - Lₜ(θ*)] contextualizes worst-case guarantees
  - Quick check question: Why does FOCUS's O(√T) regret bound match Adam's but not explain FOCUS's empirical speedup?

## Architecture Onboarding

- **Component map:** Momentum EMA → Parameter EMA → Bias correction → Weight decay → Update with attraction
- **Critical path:** The attraction term γ·sign(θₜ - θ̂ₜ) must be computed after bias-corrected EMA θ̂ₜ is available but before parameter update. Memory: 2 extra vectors (mₜ, θ̄ₜ) beyond standard Signum; comparable to Adam's (mₜ, vₜ).
- **Design tradeoffs:**
  - β₂ (parameter EMA decay): Higher β₂ (e.g., 0.99) = longer memory, more stable attraction center; lower β₂ = faster adaptation but noisier center
  - γ (attraction strength): Paper uses 0.2; larger γ = stronger confinement but risks slowing progress; γ=0 reduces to Signum
  - Weight decay interaction: High weight decay can eliminate attraction advantage
- **Failure signatures:**
  - Training diverges with learning rates where Adam is stable → reduce η or increase γ
  - Slower than Adam at early training → expected; FOCUS initial effective step ≈ η(1-γ) when θₜ "ahead" of θ̂ₜ
  - No improvement over Signum → check if valley sharpness is actually low; FOCUS advantage requires sharp+noisy regime
  - Instability in float16 with standard hyperparameters → reduce learning rate
- **First 3 experiments:**
  1. **Baseline sanity check:** On synthetic valley (Eq. 1, a=10, c=0.1, σ=1.0), compare Adam/Signum/FOCUS across learning rates {10⁻³, ..., 1.0}; verify phase transition reproduces Figure 2a pattern
  2. **Ablation study:** On GPT-2 small (or smaller proxy), sweep γ ∈ {0, 0.1, 0.2, 0.4} with β₂ ∈ {0.9, 0.95, 0.99}; identify stability vs. speed frontier
  3. **Noise regime validation:** Train MLP on MNIST with varying batch sizes (replicating Figure 2c,d); confirm FOCUS maintains advantage over Adam as batch size decreases, and extends Signum's stability range

## Open Questions the Paper Calls Out
- **Open Question 1:** What is the critical batch size threshold for LLMs where gradient noise diminishes sufficiently to favor Adam over FOCUS? The authors explicitly state this is unknown while demonstrating FOCUS outperforms Adam with batch size 480.
- **Open Question 2:** Does FOCUS maintain its speedup and stability advantages when scaling to multi-billion parameter models? The paper validates FOCUS only on GPT-2 small with 125M parameters.
- **Open Question 3:** Are neural scaling laws primarily a result of training dynamics within narrowing valleys rather than model capacity limits? Appendix B hypothesizes this but lacks experimental evidence to separate optimization limits from model expressivity.

## Limitations
- Main empirical validation relies on a single GPT-2 small experiment with limited baseline comparisons
- The attraction mechanism's effectiveness depends on EMA parameters accurately representing valley floor, which isn't extensively validated
- The "2x speedup" claim compared to "Adam from literature" lacks specific baseline details for verification

## Confidence
**High confidence:** Theoretical regret analysis (O(√T) bound matching Adam), synthetic valley experiments demonstrating optimizer phase transitions, mathematical derivation of when Adam's step size collapses under noise.
**Medium confidence:** FOCUS's advantage on GPT-2 small training, claim that gradient noise is a limiting factor in LLM training, specific hyperparameter choices (γ=0.2, β₂=0.99) being optimal.
**Low confidence:** The "2x speedup" claim without specific baseline details, whether attraction mechanism generalizes beyond tested valley geometries, assertion that FOCUS is "more stable than Adam in float16" without systematic stability analysis.

## Next Checks
1. **Synthetic landscape ablation:** Systematically vary valley sharpness parameters (a, c) and noise levels (σ) to map FOCUS's performance across broader parameter space than Figure 2a.
2. **Architecture scaling study:** Validate FOCUS on GPT-2 medium/large and other LLM architectures (Llama, Mistral) to test whether gradient noise remains limiting factor across scales.
3. **Alternative noise regimes:** Test FOCUS under structured noise (e.g., curriculum learning, data augmentation) rather than just batch-size-reduced noise to verify noise-to-sharpness ratio framework holds under non-i.i.d. structure.