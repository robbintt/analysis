---
ver: rpa2
title: The Effect of State Representation on LLM Agent Behavior in Dynamic Routing
  Games
arxiv_id: '2506.15624'
source_url: https://arxiv.org/abs/2506.15624
tags:
- game
- agents
- agent
- behavior
- route
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically investigates how different natural language
  state representations affect the strategic behavior of large language model (LLM)
  agents in dynamic routing games. The authors propose a framework characterizing
  representations along three axes: action informativeness, reward informativeness,
  and prompting style.'
---

# The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games

## Quick Facts
- **arXiv ID:** 2506.15624
- **Source URL:** https://arxiv.org/abs/2506.15624
- **Reference count:** 10
- **Key outcome:** Summarized state representations lead to more stable, equilibrium-like behavior in LLM agents playing dynamic routing games compared to full-chat representations.

## Executive Summary
This study systematically investigates how different natural language state representations affect the strategic behavior of large language model (LLM) agents in dynamic routing games. The authors propose a framework characterizing representations along three axes: action informativeness, reward informativeness, and prompting style. Applying this framework to a repeated atomic selfish routing game with Braess's paradox, they find that summarized state representations lead to more stable and equilibrium-like behavior compared to full-chat representations. Specifically, providing agents with summarized historical information, regret-based feedback rather than raw payoffs, and limited information about others' actions results in behavior closer to theoretical equilibrium predictions and with more stable game play. These findings offer practical guidance for researchers designing LLM agents for strategic decision-making in dynamic environments.

## Method Summary
The paper simulates $n=18$ LLM agents playing a repeated atomic selfish routing game (Braess's paradox network) over $T=40$ rounds using GPT-4o with temperature=1. Agents receive game states encoded in one of eight representations varying along three axes: Action Informativeness (Own vs. Everyone), Reward Informativeness (Payoff vs. Regret), and Prompting Style (Full-chat vs. Summarized). The network has two games: Game A (2 routes) and Game B (3 routes) with cost functions $10x$ for main links and $210$ for crossing links. Performance is measured via equilibrium deviation, mean payoff, mean regret, mean switches, and convergence correlation.

## Key Results
- Summarized state representations produce significantly more stable behavior than full-chat representations, with agents showing less myopia and better historical pattern recognition
- Regret-based feedback leads to faster convergence to equilibrium and lower switching rates compared to raw payoff information
- Limiting action history to an agent's own actions (rather than all agents') reduces behavioral instability and deviation from equilibrium predictions
- The combination S-RO (Summarized-Regret-Own) produces the most stable and equilibrium-like behavior across both routing games

## Why This Works (Mechanism)

### Mechanism 1: Summarization Reduces Myopic Reasoning
- **Claim:** Summarized natural language representations of game history lead to more stable, equilibrium-like behavior than full-chat transcripts.
- **Mechanism:** Full-chat histories potentially dilute critical historical signals with conversational filler or recency bias (myopia), causing the model to react only to the immediate last round. Summarization (e.g., tabular formats) compresses time-series data, allowing the LLM to function as a statistical aggregator rather than a conversationalist, thereby better leveraging its in-context learning capabilities to identify trends over 40 rounds.
- **Core assumption:** The LLM's attention mechanism assigns weight more effectively to structured, compressed data than to sequential conversational logs, even when the full context fits in the window.
- **Evidence anchors:**
  - [abstract] "representations which provide agents with... summarized, rather than complete, natural language representations... lead to behavior that more closely matches game theoretic equilibrium predictions."
  - [section 5.1] "This finding likely stems from the simplification and structure that results in the context provided to the LLM agents... full-chat representations induce myopia."

### Mechanism 2: Counterfactual Feedback (Regret) Directs Gradient Descent
- **Claim:** Providing "regret" (the gap between realized payoff and optimal payoff) rather than raw "payoff" results in faster convergence to equilibrium and lower switching rates.
- **Mechanism:** Raw payoffs act as sparse rewards; an agent receiving a high payoff cannot easily distinguish if it was the *best* possible payoff without external calculation. Regret-based feedback acts as a pre-computed gradient signal, explicitly telling the agent the cost of its error. This offloads the "counterfactual reasoning" step from the LLM, allowing it to simply minimize the explicit regret value.
- **Core assumption:** LLMs struggle to implicitly calculate counterfactuals ("what would have happened if I took the other route") when given only raw outcomes, but can effectively minimize an explicitly provided "regret" scalar.
- **Evidence anchors:**
  - [abstract] "providing agents with... regrets, rather than raw payoffs... results in behavior closer to theoretical equilibrium predictions."
  - [section 5.1] "agents benefit from information that allows them to discern how to change their action, rather than just the outcome of the action they chose."

### Mechanism 3: Information Scarcity Reduces Strategic Noise
- **Claim:** Limiting action history to an agent's own actions (rather than all agents') reduces behavioral instability and deviation from equilibrium.
- **Mechanism:** Observing the actions of others appears to trigger flawed "anticipatory reasoning" in LLMs (e.g., attempting to predict crowd behavior or "follow the herd" erroneously). By restricting the view to "Own" actions, the system forces the agent to rely on its private reward/regret signal, which serves as a sufficient statistic for learning in this class of routing games, thereby reducing noise from social modeling errors.
- **Core assumption:** In the specific game class (potential games/routing games), the payoff or regret signal captures enough information about the environment (the "state") that observing peers' specific actions is redundant or actively confusing for an LLM.
- **Evidence anchors:**
  - [abstract] "limited information about others' actions results in behavior closer to theoretical equilibrium predictions."
  - [section 5.1] "providing information about everyone's actions could elicit counterfactual, anticipatory reasoning... [but] the agent might then... proceed to make incorrect inferences."

## Foundational Learning

- **Concept: Braess's Paradox & Nash Equilibrium**
  - **Why needed here:** This is the benchmark. You cannot evaluate if the LLM is "rational" or "performing well" without understanding the theoretical optimal outcome (Nash Equilibrium) and the counter-intuitive nature of the game (Braess's Paradox: adding capacity reduces performance).
  - **Quick check question:** If adding a new high-speed link to a network causes everyone's travel time to increase, what principle is demonstrated?

- **Concept: Regret vs. Payoff (Online Learning)**
  - **Why needed here:** The paper hinges on varying the *Reward Informativeness*. Understanding that "Regret" is a measure of error relative to the best fixed action in hindsight is crucial to understanding why it provides a better learning signal than absolute "Payoff".
  - **Quick check question:** Does an agent with zero regret necessarily have the highest possible payoff, or simply the best payoff achievable given the actions of others?

- **Concept: Stateless Agents & In-Context Learning (ICL)**
  - **Why needed here:** LLMs have no persistent state between API calls. "State" must be constructed via prompts. Understanding ICL is necessary to grasp why the *format* of the prompt (summarized vs. full-chat) acts as the "learning algorithm" for the agent.
  - **Quick check question:** Why does a stateless LLM require the entire game history to be resent in the prompt for every new round of a dynamic game?

## Architecture Onboarding

- **Component map:** Game Engine -> State Encoder -> LLM Agent -> Simulation Loop
- **Critical path:** The **State Encoder**. The paper demonstrates that the *architecture of the prompt* (the output of this encoder) is more predictive of success than the underlying game logic. Tuning the `action_informativeness` and `reward_informativeness` flags here determines system stability.
- **Design tradeoffs:**
  - **Summary vs. Full-Chat:** Summaries require pre-processing logic but yield stable equilibrium behavior. Full-chat is easier to implement (dump logs) but leads to myopic, unstable agents.
  - **Observability:** Showing "Everyone's Actions" increases token cost and context complexity for potentially *worse* outcomes (instability), whereas "Own Actions" is cheaper and often more effective in these specific games.
- **Failure signatures:**
  - **High Switch Rate:** Agents changing routes every round (instability).
  - **Braess Paradox Failure:** Agents failing to find the dominant route in Game B (low bridge usage) or failing to split evenly in Game A.
  - **Myopia:** Agents over-reacting to the immediate previous round's outcome rather than the historical aggregate.
- **First 3 experiments:**
  1. **Baseline Replication:** Run the `S-RO` (Summarized-Regret-Own) configuration on Game B to verify that agents converge to the Braess Paradox equilibrium (all 18 agents on the bridge route).
  2. **Ablation on Feedback:** Run `S-PO` (Summarized-Payoff-Own) vs. `S-RO` (Summarized-Regret-Own) to quantify the "Regret Advantage" in convergence speed.
  3. **Context Stress Test:** Increase the number of rounds (e.g., to 80) to see if the "Summarized" advantage persists or if the "Full-Chat" context eventually learns, checking for context window limits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does limiting the depth of history (history truncation) provided to the agent affect equilibrium convergence in games with longer time horizons?
- **Basis in paper:** [explicit] Section 5.5 states that "curtailing the depth of history provided to the agent" is a direction for future research, as the current study retained the entire history.
- **Why unresolved:** The authors focused on the format of the state (summarized vs. full) rather than the temporal window size, leaving the impact of memory depth unknown.
- **What evidence would resolve it:** Experiments varying the look-back window (e.g., last 5 rounds vs. last 40) in extended games (e.g., $T > 100$) to measure convergence speed and stability.

### Open Question 2
- **Question:** Do sequential reasoning models (e.g., OpenAI o1) demonstrate different sensitivities to full-chat versus summarized state representations compared to standard LLMs like GPT-4o?
- **Basis in paper:** [explicit] Section 5.5 suggests expanding the inquiry to "newer models, e.g., sequential reasoning models such as OpenAI's o1".
- **Why unresolved:** The findings are specific to the `gpt-4o-2024-08-06` model; models optimized for internal chain-of-thought might handle raw history more effectively.
- **What evidence would resolve it:** Replicating the specific Braess's Paradox routing experiments using sequential reasoning models and comparing the performance gap between S-RO and F-RO representations.

### Open Question 3
- **Question:** Are the observed strategic errors in full-chat representations caused by human-like behavioral biases (e.g., crowd following) or simply by failures in context retrieval?
- **Basis in paper:** [explicit] Section 5.5 calls for "sophisticated forensics on the internal behavior of these agents" to check consistency with "behavioral biases observed in real-world human strategic behavior".
- **Why unresolved:** The paper relies on "anecdotal observations" from chat logs to explain why agents make incorrect inferences when given full chat history.
- **What evidence would resolve it:** Causal analysis of LLM reasoning chains or intervention studies where specific heuristics are explicitly penalized to see if "incorrect inferences" decrease.

## Limitations
- The findings may be specific to potential games/routing games where payoff signals contain sufficient environmental information, limiting generalizability to other game types
- Computational burden of calculating optimal payoffs for regret signals may limit practical deployment in more complex games
- The choice of GPT-4o with temperature=1 may not represent broader LLM capabilities or different prompting strategies

## Confidence

**High Confidence:** The empirical finding that summarized state representations lead to more stable behavior than full-chat histories. This is well-supported by the experimental data and mechanism is plausible given known LLM attention limitations.

**Medium Confidence:** The specific advantage of regret-based feedback over raw payoffs. While the data supports this, the mechanism assumes LLMs struggle with counterfactual reasoning, which may vary with model capabilities.

**Medium Confidence:** The claim that limiting action observability to "Own" actions reduces instability. This appears robust within the routing game context but may not generalize to games requiring strategic modeling of opponents.

## Next Checks

1. **Generalization Test:** Apply the state representation framework to a different game class (e.g., Prisoner's Dilemma or auction games) to verify whether the "summarized" and "regret" advantages persist outside potential games.

2. **Complexity Scaling:** Systematically increase the number of agents (n>18) or rounds (T>40) to test whether the computational cost of regret calculation remains tractable and whether context window limitations emerge.

3. **Model Variation:** Repeat key experiments with different LLM models (e.g., Claude, Llama) and temperature settings to assess whether the observed effects are model-specific or represent more general LLM properties.