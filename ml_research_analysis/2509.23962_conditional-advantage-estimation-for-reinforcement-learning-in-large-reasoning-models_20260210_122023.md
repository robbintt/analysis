---
ver: rpa2
title: Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning
  Models
arxiv_id: '2509.23962'
source_url: https://arxiv.org/abs/2509.23962
tags:
- arxiv
- training
- reasoning
- performance
- advantage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CANON, a conditional advantage estimation\
  \ method for reinforcement learning in large reasoning models that regroups responses\
  \ based on metrics like entropy or length to identify beneficial behavioral trends\
  \ without presupposing their direction. By comparing inter-group and intra-group\
  \ advantages, CANON amplifies the impact of specific metrics\u2014improving math\
  \ reasoning accuracy by 1.9 points and high-complexity logic reasoning by 5.2 points\
  \ over DR.GRPO."
---

# Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models

## Quick Facts
- arXiv ID: 2509.23962
- Source URL: https://arxiv.org/abs/2509.23962
- Authors: Guanxu Chen; Yafu Li; Yuxian Jiang; Chen Qian; Qihan Ren; Jingyi Yang; Yu Cheng; Dongrui Liu; Jing Shao
- Reference count: 0
- One-line primary result: Conditional advantage estimation improves math reasoning accuracy by 1.9 points and high-complexity logic reasoning by 5.2 points over DR.GRPO

## Executive Summary
This paper introduces CANON, a conditional advantage estimation method for reinforcement learning in large reasoning models. CANON regroups responses based on metrics like entropy or length to identify beneficial behavioral trends without presupposing their direction. By comparing inter-group and intra-group advantages, CANON amplifies the impact of specific metrics, achieving better performance-efficiency trade-offs and stable exploration across diverse reasoning tasks.

## Method Summary
CANON extends DR.GRPO by introducing conditional regrouping of responses based on metrics like entropy or length. Responses are sorted by the target metric and split into two equal-sized groups. The method computes inter-group advantage (comparing responses against the opposite group's mean reward) and intra-group advantage (comparing within-group), then combines them via a weighted formula. This approach identifies which metric trend (higher or lower) correlates with higher accuracy, enabling data-driven direction selection without assuming the optimal direction a priori.

## Key Results
- Improves math reasoning accuracy by 1.9 points over DR.GRPO on MATH-500
- Achieves 5.2-point improvement on high-complexity logic reasoning tasks
- Demonstrates 45.5% token reduction while maintaining accuracy through length-based grouping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inter-group advantage identifies which metric trend correlates with higher accuracy, enabling data-driven direction selection.
- **Mechanism:** Responses are sorted by target metric and split into equal groups. Each response is compared against the opposite group's mean reward. If lower-entropy group has higher rewards, it favors low-entropy; if higher-entropy group wins, it favors exploration.
- **Core assumption:** The grouping metric correlates with performance differences in at least one direction.
- **Evidence anchors:** [abstract] "measures which metric trend contributes to better performance"; [section 4.2] Equation 3 defines inter-group advantage.
- **Break condition:** If metric has no correlation with rewards, inter-group advantage provides no useful signal.

### Mechanism 2
- **Claim:** Intra-group advantage prioritizes correct responses from lower-reward group, encouraging beneficial exploration.
- **Mechanism:** Compares each response to its own group's mean. Correct answers in lower-average-reward group receive larger advantages, amplifying exploratory behaviors when low-entropy group dominates.
- **Core assumption:** Valuable behaviors exist in both metric trends.
- **Evidence anchors:** [section 4.2] "prioritizes correct responses from the group with a lower average reward"; [section 5.1] 5.2-point improvement on complex logic.
- **Break condition:** If one group has near-zero correct responses, intra-group advantage provides noisy gradients.

### Mechanism 3
- **Claim:** Avoiding directional priors enables robust performance across tasks with different optimal behaviors.
- **Mechanism:** Prior methods add penalty terms with fixed direction. CANON groups by metric values but lets inter-group comparison determine beneficial direction dynamically.
- **Core assumption:** Optimal metric direction varies by task complexity and training stage.
- **Evidence anchors:** [abstract] "amplifying the impact of the target metric without presuming its direction"; [section 1] "Simple handcrafted priors... are hard to work in different scenarios".
- **Break condition:** If μ scheduling is poorly tuned or training is too short, benefits diminish.

## Foundational Learning

- **Concept:** GRPO/DR.GRPO advantage estimation
  - **Why needed here:** CANON is a direct extension; DR.GRPO is proven as special case.
  - **Quick check question:** Explain how DR.GRPO computes advantage as R_o - mean(rewards in group).

- **Concept:** Generation entropy in LLM RL
  - **Why needed here:** Primary grouping metric; high entropy = exploration, low entropy = certainty.
  - **Quick check question:** Why might high entropy help complex tasks but hurt simple ones?

- **Concept:** Pareto frontier interpretation
  - **Why needed here:** Key evaluation framework for efficiency-accuracy trade-off.
  - **Quick check question:** A method achieving 56% accuracy with 600 tokens vs 55% with 1200 tokens—which is closer to the frontier?

## Architecture Onboarding

- **Component map:** Rollout generation -> Metric computation -> Conditional regrouping -> Advantage computation -> Policy update -> Optional scheduling

- **Critical path:**
  1. Ensure context window ≥8192 tokens (paper expanded Qwen2.5-Math from 4096→16384)
  2. Use correct verifier (Math-Verify) for 0-1 rewards
  3. Remove KL loss and entropy loss
  4. Set clip-higher ε=0.28 for stability

- **Design tradeoffs:**
  - Inter-group (μ→1): Better for math accuracy, reduces entropy/length; may under-explore
  - Intra-group (μ→0): Better for high-complexity logic; encourages reflection but slower convergence
  - Length-based grouping with α<1: Aggressive efficiency gains (45.5% reduction) with potential accuracy drop
  - Entropy-based grouping: More stable across tasks; 1.9-point math gain over DR.GRPO

- **Failure signatures:**
  - >30% response truncation: Context window too small
  - Training reward stagnates while length explodes: Likely need more inter-group emphasis
  - Complex task accuracy flat but math improving: Increase intra-group weight in scheduling
  - Aggressive length penalty (α<0.8) causes performance collapse

- **First 3 experiments:**
  1. Baseline sanity check: Reproduce DR.GRPO (μ=0.5) on Qwen2.5-Math-7B; target ~55.7 math accuracy, ~1522 avg tokens
  2. Entropy-based CANON-Inter (μ=1): Verify 57.6 math accuracy; confirm entropy decreases during training
  3. Length-based CANON-Eff (α=0.96, μ=0.5): Measure token reduction (target ~26%) while maintaining ~56.2 accuracy

## Open Questions the Paper Calls Out

- **Open Question 1:** How can CANON be extended to condition on multiple metrics simultaneously rather than a single metric?
  - **Basis:** [explicit] Appendix A states the paper "considers only one metric at a time"
  - **Why unresolved:** Current implementation regroups based solely on sorting of a single metric
  - **What evidence would resolve it:** Formulation supporting multi-dimensional grouping with empirical results

- **Open Question 2:** Is CANON effective when conditioning on training metrics other than response length or entropy?
  - **Basis:** [explicit] Appendix A notes that "other training metrics remain unexplored"
  - **Why unresolved:** Experiments restricted to entropy and length
  - **What evidence would resolve it:** Experiments applying conditional grouping to metrics like uncertainty quantification

- **Open Question 3:** Can the conditional grouping mechanism handle complex conditions that are difficult to verify or non-numerical?
  - **Basis:** [explicit] Appendix A highlights the work "focuses on conditions that can be specified through numerical ordering"
  - **Why unresolved:** Method relies on sorting numerical values to create equal-sized groups
  - **What evidence would resolve it:** Theoretical extension or empirical demonstration using non-scalar conditions

## Limitations

- Context window dependency: Effectiveness relies on sufficient context to sample multiple responses per query
- Metric selection sensitivity: Applicability to metrics beyond entropy and length remains untested
- Computational overhead: Sampling 16 responses per query increases training inference cost

## Confidence

- **High confidence:** Mathematical framework for CANON advantage estimation is internally consistent and builds on GRPO/DR.GRPO foundations
- **Medium confidence:** Empirical improvements are reported with specific metrics but lack statistical significance testing
- **Low confidence:** CANON-Dynamic scheduling strategy's optimal μ trajectories are not fully specified

## Next Checks

1. **Ablation on group size:** Systematically vary responses per query (G=8, 16, 32) and split ratio (50-50 vs 60-40) to identify optimal configuration

2. **Statistical significance validation:** Run multiple training seeds for each CANON variant and compute 95% confidence intervals for accuracy improvements

3. **Cross-metric generalization:** Implement CANON using alternative grouping metrics such as solution diversity or reasoning depth to test generalizability beyond entropy and length