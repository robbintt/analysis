---
ver: rpa2
title: Entity Linking using LLMs for Automated Product Carbon Footprint Estimation
arxiv_id: '2502.07418'
source_url: https://arxiv.org/abs/2502.07418
tags:
- database
- process
- component
- information
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using LLMs to automatically map product components
  from Bills of Materials to Life Cycle Assessment database entries for carbon footprint
  estimation. The method extracts additional context about components using an LLM,
  which is then used to perform semantic similarity matching against LCA database
  entries.
---

# Entity Linking using LLMs for Automated Product Carbon Footprint Estimation

## Quick Facts
- arXiv ID: 2502.07418
- Source URL: https://arxiv.org/abs/2502.07418
- Authors: Steffen Castle; Julian Moreno Schneider; Leonhard Hennig; Georg Rehm
- Reference count: 3
- Primary result: LLM-enhanced BOM-to-LCA matching achieves Hits@5 of 0.48, matching human non-expert performance

## Executive Summary
This paper presents a novel approach to automate the entity linking task in carbon footprint estimation by mapping product components from Bills of Materials to Life Cycle Assessment database entries using Large Language Models. The method extracts additional context about components through LLM querying, which is then used to perform semantic similarity matching against LCA database entries. When evaluated on 21 BOM components, the approach achieved Hits@5 of 0.48, matching human non-expert performance, and Hits@1 of 0.24, exceeding human performance. This demonstrates that LLM-enhanced context can effectively assist in the entity linking task, reducing the need for manual expert intervention in carbon footprint estimation workflows.

## Method Summary
The pipeline operates in three stages: first, it retrieves relevant datasheets from a pool using cosine similarity between BOM entries and datasheet filenames/texts; second, it queries an LLM (Llama 3.1 8B) to generate a manufacturing process description from the BOM information and retrieved datasheet context; third, it performs semantic similarity matching using pre-embedded LCA database entries stored in FAISS, ranking candidates by cosine similarity to the LLM-generated description. The approach bridges the semantic gap between sparse BOM entries and technical LCA database descriptions by leveraging the LLM's ability to infer manufacturing processes from material codes.

## Key Results
- Hits@5 of 0.48 achieved, matching human non-expert performance
- Hits@1 of 0.24 exceeded human performance
- Datasheet integration improved top-1 matching accuracy from 0.19 to 0.24
- Semantic similarity baseline alone achieved only 0.05 Hits@5, demonstrating the value of LLM context expansion

## Why This Works (Mechanism)

### Mechanism 1
LLM-based context expansion bridges the semantic gap between sparse BOM entries and technical LCA database descriptions. Material codes in BOMs are often internal specifications that lack direct matches in LCA databases. The LLM leverages its pre-trained knowledge to infer the manufacturing process from these codes, generating a natural language description that shares vocabulary with database entries. Core assumption: The LLM's pre-training corpus contains sufficient information about manufacturing materials and processes to accurately decode obscure material specifications.

### Mechanism 2
Datasheet integration improves top-1 matching accuracy by providing grounded technical specifications. Datasheets contain material properties and processing details that constrain the LLM's generation, reducing ambiguity. The pipeline retrieves relevant datasheets via cosine similarity (threshold ≥0.5) and includes this context in the LLM prompt. Core assumption: Datasheet filenames and content contain enough overlapping text with BOM entries for reliable retrieval at the 0.5 threshold.

### Mechanism 3
Two-stage semantic similarity (datasheet retrieval + database matching) enables scalable nearest-neighbor search in high-dimensional embedding space. Both datasheets and LCA database entries are pre-embedded using gte-large-en-v1.5 and stored in FAISS. At inference, the LLM-generated description is embedded and compared via cosine similarity, returning ranked candidates. Core assumption: The embedding model's semantic space aligns manufacturing process descriptions with LCA database terminology sufficiently for cosine similarity to surface correct matches.

## Foundational Learning

- **Life Cycle Assessment (LCA) Databases**
  - Why needed here: LCA databases like ecoinvent contain process names, technical descriptions, and environmental impact data. Understanding their structure is essential for designing the matching objective.
  - Quick check question: Given a component "steel bracket," would you match it to "steel production, electric arc furnace" or "steel rolling, average"? What information would you need to decide?

- **Semantic Embeddings and Cosine Similarity**
  - Why needed here: The pipeline relies on embedding models to represent text in a shared vector space where similar meanings have high cosine similarity. Without this, you cannot implement the retrieval components.
  - Quick check question: If two texts have cosine similarity of 0.85, what does that mean? What might cause semantically different texts to have high similarity?

- **Entity Linking vs. Information Retrieval**
  - Why needed here: This task is a form of entity linking—mapping a mention (BOM component) to a knowledge base entry (LCA process). Distinguishing it from generic IR clarifies why context expansion matters.
  - Quick check question: In standard entity linking (e.g., linking "Apple" to a company vs. fruit), what contextual signals help disambiguation? How does this parallel the BOM-to-LCA mapping problem?

## Architecture Onboarding

- **Component map:**
BOM Entry (name, supplier, material) → Datasheet Selection (gte-large-en-v1.5, cosine ≥0.5) → LLM Querying (Llama 3.1 8B, prompt with BOM + datasheet context) → LLM Response (manufacturing process description) → Semantic Similarity Matching (gte-large-en-v1.5 → FAISS → cosine ranking) → Ranked LCA Database Candidates (Hits@5, Hits@1)

- **Critical path:** The LLM's ability to decode material codes into accurate process descriptions is the bottleneck. If this fails, even perfect similarity matching cannot recover the correct entry.

- **Design tradeoffs:**
  - LLM size vs. latency: Llama 3.1 8B balances capability and local inference; larger models may improve accuracy but increase latency and carbon footprint.
  - Datasheet inclusion vs. retrieval noise: Datasheets improve Hits@1 (0.19 → 0.24) but add retrieval complexity and may introduce irrelevant context if threshold is too low.
  - Pre-computed vs. real-time embeddings: Database embeddings are pre-computed; LLM response embeddings are computed at inference, adding latency.

- **Failure signatures:**
  - Low Hits@5 (<0.2): Likely LLM failing to generate relevant process descriptions; check prompt design and material code coverage.
  - High Hits@5 but low Hits@1: Embedding model or similarity metric not fine-grained enough; consider domain-specific fine-tuning.
  - Retrieval of irrelevant datasheets: Threshold too low; raise to 0.6-0.7 or add filename filtering.

- **First 3 experiments:**
  1. Ablate datasheet context: Run pipeline with and without datasheet retrieval on the 21-component eval set; quantify Hits@1 difference (expected: ~5 point gain with datasheets).
  2. Test embedding models: Compare gte-large-en-v1.5 against a domain-specific or multilingual model; measure Hits@5 changes.
  3. Error analysis on missed matches: For components where Hits@5 = 0, manually inspect LLM outputs and database entries to identify failure modes.

## Open Questions the Paper Calls Out

- How does the pipeline perform when evaluated on a significantly larger dataset of BOM components?
  - Basis in paper: The Conclusion states "Future work includes an expanded evaluation with a larger evaluation dataset."
  - Why unresolved: Due to data privacy and scarcity, the current study was restricted to a labeled set of only 21 components from 3 BOMs.

- Does integrating context from web search results improve the accuracy of the entity linking?
  - Basis in paper: The Conclusion proposes the "integration of additional context from sources such as web search results" as a future step.
  - Why unresolved: The current system relies solely on internal BOM data and available datasheets, leaving external web data untested.

- Is the approach robust across diverse industrial sectors and obscure material naming conventions?
  - Basis in paper: The Results section notes evaluation was limited to 21 components, and the Background notes that material names are often "internal name[s]... or specification code[s]."
  - Why unresolved: The small sample size (N=21) may not capture the full variance of proprietary naming conventions across different manufacturing industries.

## Limitations

- Evaluation on only 21 components from 3 BOMs provides limited generalizability to real-world manufacturing scenarios
- The approach assumes manufacturing processes can be accurately inferred from material codes, which may not hold for proprietary or novel materials
- The 0.5 cosine similarity threshold for datasheet retrieval may not generalize across different datasheet formats or quality levels

## Confidence

- **High confidence** in the core mechanism: LLM context expansion improves semantic matching (evidence from Hits@1 increase with datasheet context)
- **Medium confidence** in practical performance: While results exceed human non-expert performance, the small evaluation set (21 components) limits statistical power
- **Medium confidence** in scalability: The pipeline architecture is sound, but real-world deployment would require extensive validation on diverse BOMs

## Next Checks

1. **Scale evaluation to 100+ components**: Test the pipeline on a larger, more diverse set of BOM components from multiple industries to assess generalizability and identify failure patterns.

2. **Cross-database validation**: Evaluate performance using alternative LCA databases (e.g., Gabi, ELCD) to verify that the approach is not specific to ecoinvent's structure or terminology.

3. **Human expert comparison**: Conduct a blinded study comparing LLM-assisted linking against expert practitioners on the same component set to establish real-world utility and identify where human judgment remains essential.