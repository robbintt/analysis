---
ver: rpa2
title: 'Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act'
arxiv_id: '2506.01931'
source_url: https://arxiv.org/abs/2506.01931
tags:
- https
- systems
- avoision
- research
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a framework and taxonomy for analyzing \"\
  avoision\"\u2014conduct that walks the line between legal avoidance and evasion\u2014\
  that firms might engage in to minimize regulatory burden under the EU AI Act. The\
  \ authors identify three tiers of avoision: (1) strategies targeting the Act's scope,\
  \ (2) strategies exploiting exemptions, and (3) strategies targeting consequential\
  \ categories."
---

# Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act

## Quick Facts
- **arXiv ID:** 2506.01931
- **Source URL:** https://arxiv.org/abs/2506.01931
- **Reference count:** 40
- **Primary result:** Proposes a three-tiered taxonomy of "avoision" strategies firms might use to minimize regulatory burden under the EU AI Act while undermining its fundamental rights protections.

## Executive Summary
This paper presents a framework for analyzing "avoision"—conduct that exploits the gap between legal avoidance and evasion—that firms might engage in to minimize regulatory burden under the EU AI Act. The authors identify three tiers of avoision: (1) strategies targeting the Act's scope, (2) strategies exploiting exemptions, and (3) strategies targeting consequential categories. For each tier, they hypothesize specific organizational and technological forms of avoision, such as adding human veneers to AI systems to circumvent definitions, strategically positioning AI development outside the EU, exploiting research and open-source exemptions, and manipulating model classifications to avoid higher-risk categories. The paper grounds these hypotheses in empirical observations and historical precedent from other domains, arguing that while these behaviors may comply with the letter of the law, they undermine the Act's intent to protect fundamental rights and ensure trustworthy AI.

## Method Summary
The authors employ qualitative legal analysis and "red teaming" via analogical reasoning to construct a three-tiered taxonomy of avoision strategies. They map the EU AI Act's text against historical examples of corporate regulatory arbitrage from domains like environmental regulation and finance to hypothesize specific evasion tactics. The method involves analyzing the Act's definitional boundaries, exemption criteria, and risk classification thresholds to identify potential loopholes, then validating these hypotheses through reference to contemporary empirical observations and historical precedent.

## Key Results
- Identified three tiers of avoision: scope manipulation, exemption exploitation, and category gaming
- Hypothesized specific strategies including "human veneers," "open-washing," and "FLOP-gaming"
- Demonstrated how quantitative thresholds create optimization targets for regulatory burden reduction
- Grounded theoretical framework in historical precedents like VW emissions scandal and Chrysler light truck classification

## Why This Works (Mechanism)

### Mechanism 1: Definitional Veneers
- **Claim:** Firms may structurally modify AI systems to argue they fall outside the AIA's specific definition of an "AI system."
- **Mechanism:** The AIA defines AI systems as "machine-based" and "autonomous." By inserting a human decision-maker or a deterministic rule-based layer at the output stage, firms create a "veneer" that arguably breaks the chain of machine autonomy, placing the system outside the Act's scope.
- **Core assumption:** Regulators or courts will interpret the letter of the law based on the immediate interface layer rather than the underlying functional logic of the system.
- **Evidence anchors:**
  - [section 4.1.1] The paper details adding "human veneers" or "reverse AI-washing" (adding rule-based layers) to exploit the "machine-based" and "autonomy" requirements.
  - [corpus] "The New Anticipatory Governance Culture for Innovation" suggests regulatory frameworks often lag behind technological architectures, supporting the viability of architectural workarounds.
- **Break condition:** Courts apply a "substance over form" interpretation, looking through the veneer to the core capability.

### Mechanism 2: Exemption Capture
- **Claim:** Firms can minimize burden by exploiting exemptions intended for non-commercial or public-goods activities (research/open-source) without fulfilling the intent of those exemptions.
- **Mechanism:** The AIA provides total or partial exemptions for "scientific research" and "open source" AI to foster innovation. Firms can engage in "open-washing" (releasing models with restrictive/ineffective licenses) or classify commercial R&D as pure research, thereby securing the regulatory waiver while retaining proprietary control.
- **Core assumption:** The enforcement of exemptions relies on formal criteria (e.g., license type) rather than a substantive assessment of public value or openness.
- **Evidence anchors:**
  - [section 4.2.2] Discusses "open-washing," where models are released under licenses that allow access but create practical barriers (e.g., missing weights, high compute costs), undermining the "quid pro quo" of the exemption.
  - [corpus] "The Precautionary Principle and the Innovation Principle" highlights the tension between promoting innovation and ensuring safety, the exact tension these exemptions navigate.
- **Break condition:** Technical standards (currently in development) are updated to strictly define "open source" and "research" based on verifiable public utility rather than intent.

### Mechanism 3: Threshold Gaming
- **Claim:** Quantitative thresholds for "high risk" or "systemic risk" create optimization targets that allow firms to reduce regulatory burden without necessarily reducing actual model capability or risk.
- **Mechanism:** The AIA uses metrics like compute (FLOPs) to trigger "systemic risk" classifications. Firms can use techniques like model distillation or decentralized training to stay under specific compute thresholds, or "benchmark shop" to avoid specific capability triggers, effectively decoupling the regulatory grade from the real-world risk.
- **Core assumption:** The specified metrics (FLOPs, selected benchmarks) are imperfect proxies for risk and can be mathematically optimized against.
- **Evidence anchors:**
  - [section 4.3.2] Explicitly maps "FLOP-Gaming" via distillation and "Benchmark Shopping" as strategies to avoid systemic risk classification.
  - [corpus] "ARMs: Adaptive Red-Teaming Agent..." highlights the difficulty of robust safety evaluation, implying benchmarks are often gameable or incomplete.
- **Break condition:** Regulators shift from static, pre-defined metrics to dynamic, holistic evaluations of model capability.

## Foundational Learning

- **Concept: Regulatory Arbitrage (Avoision)**
  - **Why needed here:** The paper centers on "avoision"—behavior that exploits the gap between the letter and the spirit of the law.
  - **Quick check question:** Can you distinguish between a system that complies with a regulation's text and one that fulfills its intent?

- **Concept: Risk-Based Regulation**
  - **Why needed here:** The EU AI Act assigns obligations based on risk tiers (Minimal, High, Systemic). Understanding this tiering is essential to understanding why firms manipulate their categorization.
  - **Quick check question:** Does a "General Purpose" model face the same initial constraints as a "High-Risk" system in a specific domain like education?

- **Concept: Proxy Measures**
  - **Why needed here:** Many avoision strategies rely on the fact that regulations must use measurable proxies (e.g., FLOPs, license type) for abstract concepts (e.g., systemic risk, openness).
  - **Quick check question:** Why is the amount of compute used to train a model an imperfect proxy for the danger that model poses?

## Architecture Onboarding

- **Component map:** The "Red Teaming" framework consists of three defensive layers for a firm:
  1. **Scope Layer:** Is the system legally an "AI system" in the EU? (Veneers, Geography).
  2. **Exemption Layer:** Does the system qualify for a carve-out? (Research, Open Source).
  3. **Category Layer:** How is the system classified? (Risk tier, Operator role).

- **Critical path:**
  1. Analyze the product against the AIA definition of "AI System" (Scope).
  2. If in scope, assess eligibility for Research or Open Source exemptions (Exemptions).
  3. If not exempt, minimize classification (High-Risk vs. Minimal) via technical constraints or role definitions (Categories).

- **Design tradeoffs:**
  - **Compliance Cost vs. Legal Risk:** Aggressive avoision (e.g., "sandbagging" model performance) saves money but risks future enforcement if laws are reinterpreted.
  - **Innovation vs. Control:** Using the "Open Source" exemption requires giving up some IP control (weights/architecture), though "open-washing" mitigates this.

- **Failure signatures:**
  - **Re-characterization:** A court determines a "human veneer" is merely a rubber stamp and applies provider obligations.
  - **Threshold Updates:** The EU lowers the FLOP threshold or changes the benchmark list, suddenly capturing a "safe" model.

- **First 3 experiments:**
  1. **Interface Audit:** Review a high-risk system to see if a human or rule-based decision layer can be formally inserted to argue reduced autonomy (Test Mechanism 1).
  2. **License Stress Test:** Analyze an "open" license to see if it practically enables competitors to reproduce the model, or if it acts as a barrier (Test Mechanism 2).
  3. **Compute Distillation:** Calculate if a large model can be distilled to perform a task just as well while staying under the 10^25 FLOP threshold (Test Mechanism 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which avoision strategies will manifest empirically "in the wild" as firms adjust to the AIA?
- **Basis in paper:** [explicit] The authors note that because the AIA is relatively new, "there is (so far) little documentation of avoision in the wild," relying instead on hypotheses and historical precedent.
- **Why unresolved:** The Act is not yet fully in effect (projected for 2027), and firms are still formulating compliance strategies versus avoisive maneuvers.
- **What evidence would resolve it:** Longitudinal empirical studies or whistleblower reports documenting specific corporate behaviors and technical architectures deployed between 2024 and 2028.

### Open Question 2
- **Question:** How will judicial interpretation define "autonomy" and "machine-based" systems regarding "human veneer" architectures?
- **Basis in paper:** [explicit] The authors state that "Based on how courts interpret the AIA, certain strains of avoision might end up being implausible," specifically referencing the ambiguity of human-in-the-loop systems.
- **Why unresolved:** Courts have not yet ruled on whether adding human layers to AI systems effectively removes them from the AIA's scope or constitutes evasion.
- **What evidence would resolve it:** Case law or regulatory guidance establishing precedents on the "machine-based" definition when human agents are used as "wrappers."

### Open Question 3
- **Question:** Can technical standards effectively close loopholes for "FLOP-gaming" and "open-washing"?
- **Basis in paper:** [inferred] The paper identifies "FLOP-gaming" (hiding compute via decentralization) and "open-washing" (restrictive licenses) as major risks, while relying on undefined future technical standards (CEN/CENELEC) as the primary mitigation.
- **Why unresolved:** Current definitions for compute thresholds and open-source compliance are ambiguous and potentially gameable by distributed training methods or restrictive licensing.
- **What evidence would resolve it:** Analysis of final technical standards to see if they mandate transparent reporting of decentralized compute or specific openness criteria beyond just license type.

## Limitations

- The taxonomy relies heavily on analogical reasoning from historical regulatory precedents rather than direct empirical evidence of avoision under the EU AI Act
- Specific strategies proposed are theoretically plausible but not yet observed in practice
- Effectiveness depends on future regulatory and judicial interpretations that remain uncertain
- Analysis assumes static regulatory frameworks while the EU is actively developing implementing acts
- Does not fully address how cross-border enforcement mechanisms might limit geographical avoision strategies

## Confidence

- **High confidence:** The general conceptual framework of avoision as regulatory arbitrage is well-established and the historical analogies (VW emissions, Chrysler light trucks) are verifiable and directly support the theoretical foundation.
- **Medium confidence:** The specific AIA provisions cited (definitions, exemptions, risk categories) are accurately represented, but the practical viability of exploitation strategies depends on future technical standards and enforcement interpretations not yet established.
- **Low confidence:** Predictions about firm behavior and the actual prevalence of specific avoision strategies lack empirical grounding and may overestimate corporate willingness to engage in legally ambiguous practices.

## Next Checks

1. **Track implementing acts development:** Monitor the European Commission's progress on technical standards for AI system definitions and open-source exemptions to assess whether proposed loopholes are being closed legislatively.

2. **Industry behavior survey:** Conduct qualitative interviews with AI developers and legal counsel to determine whether firms are currently considering or implementing the proposed avoision strategies.

3. **Judicial interpretation analysis:** Review early enforcement cases under the AI Act and GDPR to identify how courts are interpreting definitional boundaries and exemption criteria, particularly regarding "autonomy" and "scientific research" concepts.