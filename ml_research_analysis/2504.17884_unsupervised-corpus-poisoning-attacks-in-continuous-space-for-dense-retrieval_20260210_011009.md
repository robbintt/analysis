---
ver: rpa2
title: Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval
arxiv_id: '2504.17884'
source_url: https://arxiv.org/abs/2504.17884
tags:
- adversarial
- attack
- retrieval
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses unsupervised corpus poisoning attacks in dense
  retrieval, where an adversary injects malicious documents to compromise ranking
  performance without prior knowledge of query distribution. The authors propose an
  optimization method that operates directly in the continuous embedding space, using
  a reconstruction model to recover documents from token embeddings and a perturbation
  model that maintains geometric distance between original and adversarial embeddings
  while maximizing token-level dissimilarity.
---

# Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval

## Quick Facts
- arXiv ID: 2504.17884
- Source URL: https://arxiv.org/abs/2504.17884
- Reference count: 40
- Authors propose optimization method operating directly in continuous embedding space for corpus poisoning attacks

## Executive Summary
This paper addresses unsupervised corpus poisoning attacks in dense retrieval systems, where adversaries inject malicious documents without prior knowledge of query distribution. The authors develop an optimization method that operates directly in continuous embedding space, using reconstruction and perturbation models to maintain geometric distance between original and adversarial embeddings while maximizing token-level dissimilarity. Their approach demonstrates significant improvements in computational efficiency and stealth compared to existing methods, while maintaining strong attack success rates across multiple benchmark datasets.

## Method Summary
The proposed method operates in continuous embedding space by first encoding documents into token embeddings, then optimizing these embeddings through a perturbation model that maintains geometric distance to original embeddings while maximizing token dissimilarity. A reconstruction model converts the optimized embeddings back into adversarial documents. The optimization objective balances three components: maintaining document similarity in embedding space, maximizing token-level differences, and ensuring semantic coherence. This approach enables unsupervised attacks without requiring query distribution knowledge or labeled data, addressing a key limitation of existing poisoning methods.

## Key Results
- Outperforms HotFlip-based approaches with 4x faster generation time (119.9s vs 512.1s per document)
- Achieves significantly lower perplexity (188.9 vs 6032.6), making attacks harder to detect
- Demonstrates strong transferability in black-box attacks across multiple datasets including TREC DL 19/20, Natural Questions, Quora, FiQA, and TouchÃ©-2020
- Achieves attack success rates up to 0.984 in white-box settings

## Why This Works (Mechanism)
The method exploits the geometric properties of dense retrieval embedding spaces by directly optimizing in continuous space rather than working with discrete tokens. By maintaining geometric distance between original and adversarial embeddings while maximizing token-level dissimilarity, the approach creates documents that appear semantically similar to retrieval systems but contain strategically placed perturbations. The reconstruction model bridges the continuous optimization space with discrete document representation, enabling gradient-based optimization techniques that would be impossible with discrete token operations.

## Foundational Learning
- **Dense retrieval embedding spaces**: Vector representations where semantic similarity is preserved through geometric proximity, necessary for understanding how document embeddings are used in retrieval systems
- **Geometric distance optimization**: Mathematical techniques for maintaining relationships between original and modified embeddings, critical for preserving document relevance while introducing malicious modifications
- **Token-level dissimilarity maximization**: Methods for identifying and modifying specific tokens to maximize perturbation impact while maintaining surface-level document coherence
- **Reconstruction model architecture**: Neural network designs capable of converting optimized embeddings back to coherent documents, essential for practical attack implementation
- **Unsupervised attack optimization**: Techniques for generating effective attacks without labeled query data, addressing real-world attack scenarios
- **Perplexity as stealth metric**: Statistical measure of language model confidence that indicates how natural generated text appears, used here to assess attack detectability

## Architecture Onboarding

**Component Map**: Document -> Encoder -> Token Embeddings -> Perturbation Model -> Optimized Embeddings -> Reconstruction Model -> Adversarial Document

**Critical Path**: The optimization loop where token embeddings flow through the perturbation model to generate adversarial embeddings, which are then reconstructed into documents and evaluated for attack success, with gradients flowing back to update the perturbation model parameters.

**Design Tradeoffs**: The method prioritizes computational efficiency and stealth over semantic fidelity, choosing to optimize in continuous space rather than discrete tokens. This enables faster generation and better stealth (lower perplexity) but may introduce semantic drift that isn't captured by current evaluation metrics. The reconstruction model serves as a critical bridge but may generate documents that deviate from original semantics.

**Failure Signatures**: Attacks may fail when reconstruction introduces semantic drift that breaks document relevance, when geometric distance constraints are too restrictive to allow effective perturbations, or when the perturbation model cannot find optimal modifications within computational constraints. High perplexity values would indicate detectable attacks.

**First 3 Experiments**:
1. Benchmark computational efficiency comparison with HotFlip on standard datasets to verify 4x speedup claim
2. Perplexity analysis comparing generated documents against original documents and HotFlip outputs
3. White-box attack success rate evaluation on TREC DL 19/20 benchmark with complete system access

## Open Questions the Paper Calls Out
None

## Limitations
- Semantic fidelity concerns: Reconstruction model may generate documents with substantial semantic deviation while maintaining surface-level similarity
- White-box assumption: High attack success rates (up to 0.984) may overestimate real-world effectiveness where adversaries lack complete system access
- Controlled evaluation: Results tested in relatively controlled settings without accounting for diverse query distributions or retrieval system configurations

## Confidence
- **High Confidence**: Computational efficiency claims (119.9s vs 512.1s per document) and perplexity metrics (188.9 vs 6032.6) are directly measurable and reproducible
- **Medium Confidence**: Attack success rates and ranking performance degradation are well-established through standard evaluation protocols
- **Low Confidence**: Semantic preservation claims require further validation, as reconstruction may introduce semantic drift not captured by current metrics

## Next Checks
1. Conduct user studies to evaluate whether humans can distinguish between original and reconstructed documents, providing empirical validation of stealthiness beyond perplexity metrics
2. Test attack transferability across diverse retrieval architectures (different dense retrievers, re-ranking models) and query distributions to establish robustness beyond controlled settings
3. Implement and evaluate defensive mechanisms that detect document reconstruction artifacts to assess whether low perplexity truly indicates stealthiness or reflects different detection vulnerabilities