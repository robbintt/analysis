---
ver: rpa2
title: Automated Triaging and Transfer Learning of Incident Learning Safety Reports
  Using Large Language Representational Models
arxiv_id: '2509.13706'
source_url: https://arxiv.org/abs/2509.13706
tags:
- reports
- learning
- inst
- performance
- bluebert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the use of transfer learning with a BlueBERT
  model to classify incident report severity across institutions in radiation oncology.
  The model was first fine-tuned on a large internal dataset (7,094 reports) and then
  on a smaller, multi-national SAFRON dataset (571 reports).
---

# Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models

## Quick Facts
- arXiv ID: 2509.13706
- Source URL: https://arxiv.org/abs/2509.13706
- Authors: Peter Beidler; Mark Nguyen; Kevin Lybarger; Ola Holmberg; Eric Ford; John Kang
- Reference count: 38
- Primary result: Transfer learning improved cross-institutional incident report severity classification AUROC from 0.56 to 0.78

## Executive Summary
This study demonstrates that transfer learning with BlueBERT significantly improves automated classification of radiation oncology incident report severity across institutions. The model first fine-tunes on a large internal dataset (7,094 reports) before adapting to a smaller multi-national SAFRON dataset (571 reports). Without transfer learning, BlueBERT achieved AUROC of 0.56 on SAFRON, but with transfer learning, performance improved to 0.78. The results suggest that incident report classification models can generalize across institutions when properly adapted, with performance comparable to human evaluators.

## Method Summary
The study used two datasets: an institutional dataset of 7,094 radiation oncology incident reports and a SAFRON dataset of 571 labeled reports. Both datasets were binarized for high-severity classification (scores ≥3 or categories "major" and above). SVM with TF-IDF and BlueBERT models were trained and evaluated using stratified 70/15/15 and 60/20/20 splits respectively. BlueBERT underwent sequential fine-tuning: first on the institutional dataset (learning rate 1e-6), then on SAFRON (learning rate 1e-8). Models were evaluated on AUROC, sensitivity, and specificity at different alert rates (20% and 50%).

## Key Results
- BlueBERT_TRANSFER achieved AUROC of 0.78 on SAFRON vs 0.56 without transfer learning
- SVM and BlueBERT achieved AUROC of 0.82 and 0.81 respectively on internal dataset
- Model performance on manually curated reports (AUROC 0.74-0.85) matched human evaluators (AUROC 0.81)
- At 20% alert rate: specificity 0.93, sensitivity 0.46; at 50% alert rate: specificity 0.64, sensitivity 0.79

## Why This Works (Mechanism)

### Mechanism 1: Sequential Transfer Learning for Cross-Institutional Adaptation
The model first learns general incident report patterns from 7,094 institutional reports, providing better initialization for the smaller SAFRON dataset. This enables adaptation to different severity scales and reporting styles with limited target data. Core assumption: source and target datasets share underlying semantic patterns related to severity. Evidence: AUROC improved from 0.56 to 0.78 with transfer learning. Break condition: source and target severity definitions are fundamentally incompatible.

### Mechanism 2: Biomedical Domain Pretraining Reduces Vocabulary Gap
BlueBERT's pretraining on 4+ billion PubMed words and 400+ million MIMIC-III words exposes the model to medical terminology before task-specific training. This reduces semantic distance between pretrained representations and radiation oncology incident language. Core assumption: incident reports share sufficient vocabulary with biomedical literature. Evidence: domain pretraining acknowledged as potentially limited for specialized radiation oncology terminology. Break condition: incident reports use highly specialized jargon absent from pretraining corpus.

### Mechanism 3: Human Inter-Rater Agreement Constrains Model Ceiling
Training labels show "fair agreement" (Krippendorff's α = 0.376), creating an irreducible error floor. Models cannot learn patterns that human experts apply inconsistently. The finding that models achieve similar AUROC to humans (0.74-0.85 vs 0.81) suggests models approach this ceiling. Core assumption: prior inter-rater reliability measurements accurately represent label noise. Evidence: models achieve similar performance to humans on manually curated reports. Break condition: if severity criteria become standardized with high inter-rater agreement.

## Foundational Learning

- **Concept: Transfer Learning (Sequential Fine-tuning)**
  - Why needed: The AUROC jump from 0.56 to 0.78 depends on understanding why pre-fine-tuning on large data helps small-data adaptation.
  - Quick check: If you trained BlueBERT only on the 571 SAFRON reports, would you expect performance closer to 0.56 or 0.78?

- **Concept: BERT Fine-tuning vs. Training from Scratch**
  - Why needed: The paper uses very low learning rates (10^-6, 10^-8) and freezes pretrained weights during adaptation.
  - Quick check: Why use learning rates of 10^-6 rather than 10^-3 when fine-tuning BlueBERT?

- **Concept: AUROC vs. Operational Thresholds**
  - Why needed: AUROC measures ranking quality, but deployment requires choosing alert rates.
  - Quick check: To reduce alert fatigue while maintaining reasonable sensitivity, should you raise or lower the alert rate threshold?

## Architecture Onboarding

- **Component map**: Incident Report (raw text) → Preprocessing (lowercase, acronym expansion, truncate at 150 tokens) → BlueBERT Encoder (~110M params, frozen weights) → Classification Head (1 random init layer, sigmoid activation) → Severity Probability → Apply threshold for alert decision

- **Critical path**: Fine-tune on Inst. first (7,094 reports, lr=10^-6), then on SAFRON (571 reports, lr=10^-8). Skipping or reversing this order breaks cross-institutional performance.

- **Design tradeoffs**:
  - SVM vs. BlueBERT: SVM achieves 0.82 AUROC on internal data vs. BlueBERT's 0.81, but on external data, BlueBERT_TRANSFER (0.78) vastly outperforms SVM (0.42). Use BlueBERT for cross-institution deployment; SVM may suffice for single-institution applications.
  - Alert rate: 20% alert rate yields specificity 0.93/sensitivity 0.46; 50% yields specificity 0.64/sensitivity 0.79. Calibrate to institutional tolerance for false positives vs. missed events.

- **Failure signatures**:
  - External AUROC near 0.5 without transfer learning indicates domain mismatch.
  - Validation-test gap suggests overfitting to source institution reporting style.
  - Performance well below human AUROC (0.81) indicates model or data issues.

- **First 3 experiments**:
  1. Reproduce transfer learning gap: Train BlueBERT on SAFRON alone; confirm AUROC drops to ~0.56 range.
  2. Threshold calibration: Sweep alert rates (10-90%) on validation set to establish sensitivity/specificity curves for operational planning.
  3. Minimal local data test: If deploying to new institution, measure zero-shot performance, then fine-tune on 50-100 locally labeled reports to quantify adaptation data requirements.

## Open Questions the Paper Calls Out

### Open Question 1
What specific alert rate thresholds maximize clinical utility while minimizing alert fatigue for different institutional contexts? The authors state that discussing acceptable alert rates with clinical representatives is critical, but the tested 20% and 50% rates are demonstrative rather than definitive. This requires prospective deployment studies measuring sensitivity, specificity, and user compliance/fatigue across various threshold settings.

### Open Question 2
To what extent does inter-rater variability and label noise limit the achievable performance ceiling of NLP models in this domain? The authors note that label inconsistency could limit model performance, citing a Krippendorff's alpha of 0.376 among human raters. Models were trained against "ground truth" labels that are acknowledged as inconsistent, making it difficult to distinguish between model error and label ambiguity. This requires comparison against rigorously consensus-derived gold standard datasets.

### Open Question 3
Can performance be enhanced by pretraining on corpora specific to radiation oncology operational data rather than general biomedical text? The authors suggest that weaker BlueBERT performance may be because domain-specific terminology was not well-represented in the pretraining corpus. This requires comparative studies evaluating radiation-oncology-specific language models against general biomedical BlueBERT on the same task.

## Limitations
- Datasets require external access (Institutional dataset from authors, SAFRON from IAEA), creating reproducibility barriers
- Limited SAFRON dataset size (571 reports) constrains statistical power of transfer learning improvements
- Manual curation process for 4,068 Institutional reports is not fully documented, raising potential selection bias concerns

## Confidence

- **High Confidence**: AUROC improvement from 0.56 to 0.78 with transfer learning (n=571 test reports) is statistically robust. Model-human performance similarity (0.74-0.85 vs 0.81) on manually curated reports strongly indicates label noise constrains performance.
- **Medium Confidence**: Sequential transfer learning mechanism is well-supported, but optimal fine-tuning sequence is not tested. Cross-institutional performance degradation without transfer is based on single dataset comparison.
- **Low Confidence**: Claims about BlueBERT's superiority over SVM are institution-dependent. On internal data, SVM slightly outperforms BlueBERT (0.82 vs 0.81), but on external data, BlueBERT_TRANSFER vastly outperforms SVM (0.78 vs 0.42). Study does not investigate whether SVM could match BlueBERT's external performance with similar transfer learning.

## Next Checks

1. **Transfer learning ablation**: Train BlueBERT exclusively on SAFRON (no transfer) and measure AUROC degradation to confirm the 0.56 baseline is reproducible.

2. **Cross-institutional transfer directionality**: Train BlueBERT on SAFRON first, then on Institutional data, to test whether transfer learning is asymmetric or bidirectional.

3. **Minimal adaptation data requirement**: Measure model performance on new institution data with varying amounts of local fine-tuning (0, 10, 50, 100 labeled reports) to quantify data efficiency for deployment.