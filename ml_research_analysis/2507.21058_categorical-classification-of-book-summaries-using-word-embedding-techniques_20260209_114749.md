---
ver: rpa2
title: Categorical Classification of Book Summaries Using Word Embedding Techniques
arxiv_id: '2507.21058'
source_url: https://arxiv.org/abs/2507.21058
tags:
- kullan
- kelime
- fland
- veri
- lmas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study aimed to classify book summaries into eight categories\
  \ (fantasy, science fiction, romance, history, crime, philosophy, cinema, and horror-thriller)\
  \ using Turkish text. The researchers compared three word embedding methods\u2014\
  One Hot Encoding, Word2Vec, and TF-IDF\u2014combined with various preprocessing\
  \ techniques (case normalization, punctuation removal, stemming, and stopword removal)\
  \ and seven machine learning models (KNN, Naive Bayes, Random Forest, Decision Trees,\
  \ SVM, Logistic Regression, and AdaBoost)."
---

# Categorical Classification of Book Summaries Using Word Embedding Techniques

## Quick Facts
- arXiv ID: 2507.21058
- Source URL: https://arxiv.org/abs/2507.21058
- Reference count: 0
- Primary result: TF-IDF with SVM achieved 80% accuracy/F-score for Turkish book summary classification

## Executive Summary
This study classified Turkish book summaries into eight categories (fantasy, science fiction, romance, history, crime, philosophy, cinema, and horror-thriller) using three word embedding methods and seven machine learning models. The researchers tested 16 preprocessing combinations systematically, finding that stemming consistently improved performance across all embedding techniques. TF-IDF combined with SVM produced the highest F-score (80%), while One-Hot Encoding with Naive Bayes achieved the best accuracy (81%). Word2Vec with Logistic Regression yielded 72% accuracy, underperforming compared to bag-of-words approaches.

## Method Summary
The study employed a systematic factorial design testing 16 preprocessing combinations (lowercase, punctuation removal, Turkish stemming, stopword removal) × 3 word embedding methods (One-Hot Encoding, TF-IDF, Word2Vec) × 7 classifiers (KNN, Naive Bayes, Random Forest, Decision Trees, SVM, Logistic Regression, AdaBoost) on Turkish book summaries. Data came from idefix.com with approximately 3,200 documents across 8 genres. Performance was evaluated using accuracy, F-score, precision, and recall metrics, with TF-IDF + SVM emerging as the recommended baseline combination.

## Key Results
- TF-IDF with SVM achieved the highest F-score of 80%
- One-Hot Encoding with Naive Bayes produced the best accuracy of 81%
- Stemming consistently improved classification performance across all embedding methods
- Word2Vec with Logistic Regression achieved 72% accuracy, underperforming other methods
- KNN showed consistently poor performance across all configurations (F-scores 0.06-0.16)

## Why This Works (Mechanism)

### Mechanism 1
TF-IDF combined with SVM produces superior classification because the weighting scheme captures genre-discriminative vocabulary while SVM handles high-dimensional sparse representations effectively. TF-IDF reduces common word influence while amplifying distinguishing terms, and SVM finds optimal separating hyperplanes in this weighted feature space.

### Mechanism 2
Stemming consistently improves performance because Turkish's agglutinative morphology creates many surface variants that fragment semantic signals. Collapsing morphological variants into single features reduces vocabulary dimensionality while concentrating discriminative signals at the root level.

### Mechanism 3
One-Hot Encoding with Naive Bayes achieves highest accuracy because explicit binary presence indicators align with Naive Bayes' independence assumptions for keyword-based genre detection. This approach works when specific keywords strongly indicate genres without requiring frequency weighting or contextual relationships.

## Foundational Learning

- **Agglutinative Morphology** (Turkish language structure)
  - Why needed here: Turkish forms words by stacking suffixes onto roots, creating many surface variants that inflate vocabulary and fragment features unless stemmed
  - Quick check question: Can you explain why "evlerimde" (in my houses) and "ev" (house) would be treated as the same feature after stemming but different features without it?

- **TF-IDF Weighting**
  - Why needed here: The study's best-performing embedding method; understanding why TF and IDF components matter explains its superiority over simple frequency counting
  - Quick check question: If the word "kitap" (book) appears frequently in every genre's summaries, would it receive a high or low IDF weight, and why does this matter for classification?

- **Preprocessing Combinations (Factorial Design)**
  - Why needed here: The study tests 16 preprocessing combinations (2^4), showing how different pipelines affect performance systematically
  - Quick check question: Why would testing preprocessing combinations separately matter, versus always applying all four preprocessing steps together?

## Architecture Onboarding

- Component map: Raw book summaries → Preprocessing Layer (16 combinations) → Embedding Layer (3 methods) → Classification Layer (7 models) → Evaluation (F-Score, Accuracy, Precision, Recall)

- Critical path: Preprocessing → TF-IDF embedding → SVM classifier achieves the recommended baseline (80% F-score). Stemming is the highest-impact preprocessing step across all configurations.

- Design tradeoffs:
  - One-Hot + NB: Highest accuracy (81%) but sparse representations scale poorly with vocabulary size
  - TF-IDF + SVM: Slightly lower accuracy (80%) but more robust to vocabulary growth and better documented in literature
  - Word2Vec + LR: Lower accuracy (72%) but produces dense representations that could transfer better to other tasks or smaller datasets

- Failure signatures:
  - KNN consistently fails across all embedding methods (F-scores 0.06-0.16 for One-Hot and TF-IDF), suggesting curse of dimensionality
  - AdaBoost shows unstable performance with high variance between precision and recall
  - Word2Vec underperforms relative to bag-of-words methods—possible causes: insufficient training corpus or genre classification depends more on keyword presence

- First 3 experiments:
  1. Reproduce baseline: Apply preprocessing combination 1011 with TF-IDF embedding and SVM classifier. Target: 80% F-score on the same 8-category Turkish book summary task
  2. Ablate stemming: Compare preprocessing combination 1011 vs. 1001 using TF-IDF + SVM to quantify stemming's contribution
  3. Test embedding-model pairs: Run the three best configurations side-by-side: (TF-IDF + SVM), (One-Hot + NB), (Word2Vec + LR) on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
Would deep learning models (e.g., CNN, LSTM, BERT-based Turkish models) outperform the classical machine learning approaches tested? The conclusion states that findings will "provide guidance for future work on larger datasets and advanced deep learning techniques in Turkish natural language processing applications." This remains unresolved as the study deliberately limited scope to seven classical ML models.

### Open Question 2
Does the relatively lower Word2Vec performance (72%) stem from insufficient training corpus size for learning quality embeddings? Word2Vec achieved only 72% accuracy versus 80-81% for TF-IDF and One-Hot Encoding, yet Word2Vec typically requires large corpora to learn meaningful semantic representations. The paper does not report embedding quality metrics or analyze corpus adequacy.

### Open Question 3
How robust are the reported accuracies across different train-test splits or cross-validation folds? The paper reports single accuracy/F-score values but does not mention cross-validation, statistical significance testing, or variance across multiple runs. Without confidence intervals, it is unclear whether differences between methods are meaningful or due to random variation.

### Open Question 4
Would combining word embedding methods (e.g., TF-IDF + Word2Vec concatenation) yield higher accuracy than any single method? The literature review mentions prior work combining Word2Vec and TF-IDF with positive results, but this study tested each embedding method independently. No hybrid embedding approaches were evaluated despite evidence from cited literature suggesting potential benefits.

## Limitations
- Exact dataset size remains ambiguous ("three thousand two hundred thousand" appears to be a mistranslation)
- Paper lacks specific details on train-test split ratios, Word2Vec hyperparameters, and SVM kernel selection
- Absence of cross-validation protocols limits confidence in result stability across different data partitions
- Relative underperformance of Word2Vec suggests potential issues with Turkish embedding quality or document representation methods

## Confidence

- **High Confidence**: TF-IDF with SVM achieving 80% accuracy/F-score; stemming consistently improving performance; One-Hot Encoding with Naive Bayes achieving 81% accuracy
- **Medium Confidence**: Word2Vec with Logistic Regression achieving 72% accuracy; relative ranking of all seven classifiers across different embedding methods
- **Low Confidence**: Exact magnitude of stemming's contribution to performance; generalizability to larger datasets or different Turkish text domains

## Next Checks

1. Reconstruct the exact preprocessing pipeline: Implement all 16 preprocessing combinations using Turkish-specific stemmers and stopwords, then verify that combination 1011 consistently outperforms other combinations when paired with TF-IDF embedding

2. Validate embedding-method performance rankings: Systematically test the three claimed top-performing configurations—(TF-IDF + SVM), (One-Hot + NB), (Word2Vec + LR)—on a held-out test set to confirm the paper's relative accuracy rankings

3. Quantify preprocessing impact: Conduct ablation studies comparing preprocessing combinations with and without stemming to measure its specific contribution to F-score improvement, as the paper claims "büyük bir değişikliğe sebebiyet vermedi" except for stemming