---
ver: rpa2
title: A Systematic Study of Compression Ordering for Large Language Models
arxiv_id: '2511.19495'
source_url: https://arxiv.org/abs/2511.19495
tags:
- quantization
- pruning
- compression
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates the impact of different orderings\
  \ of three compression techniques\u2014knowledge distillation, structured pruning,\
  \ and low-bit quantization\u2014on large language models. Using the Qwen2.5-3B model,\
  \ the authors test multiple compression pipelines and find that the sequence Pruning\u2192\
  Knowledge Distillation\u2192Quantization (P-KD-Q) achieves the best balance, yielding\
  \ a 3.68\xD7 compression ratio while preserving strong instruction-following and\
  \ language understanding capabilities."
---

# A Systematic Study of Compression Ordering for Large Language Models

## Quick Facts
- arXiv ID: 2511.19495
- Source URL: https://arxiv.org/abs/2511.19495
- Reference count: 40
- Primary result: Pruning→Knowledge Distillation→Quantization (P-KD-Q) achieves 3.68× compression with preserved capability

## Executive Summary
This study systematically evaluates how the ordering of compression techniques affects large language model performance. Using Qwen2.5-3B, the authors test multiple pipelines combining knowledge distillation, structured pruning, and low-bit quantization. They find that applying quantization last yields the best results, while early quantization causes irreversible information loss that impairs subsequent training. The P-KD-Q sequence achieves a 3.68× compression ratio while maintaining strong instruction-following and language understanding capabilities.

## Method Summary
The authors conducted controlled experiments on Qwen2.5-3B using three compression techniques in various sequences. They tested all six possible orderings of pruning, knowledge distillation, and quantization, measuring performance across instruction-following and language understanding benchmarks. Each pipeline was evaluated for compression ratio, task performance retention, and the impact of irreversible information loss from quantization. The study focused on identifying optimal ordering strategies while maintaining model capability.

## Key Results
- P-KD-Q ordering achieves 3.68× compression ratio while preserving instruction-following capability
- Early quantization severely degrades performance due to irreversible information loss
- Quantization applied last allows distillation and pruning to work effectively on high-precision weights

## Why This Works (Mechanism)
Compression ordering fundamentally affects how information is preserved and transformed throughout the pipeline. Knowledge distillation and pruning operate more effectively on high-precision weights, allowing them to capture and retain critical information before it's irreversibly lost through quantization. Early quantization forces these techniques to work with already-compromised representations, limiting their ability to preserve model capabilities. The sequential approach ensures each technique can operate under optimal conditions.

## Foundational Learning

**Knowledge Distillation**
- Why needed: Transfers knowledge from larger to smaller models while preserving capabilities
- Quick check: Compare student model performance against teacher across representative tasks

**Structured Pruning**
- Why needed: Removes redundant parameters while maintaining functional pathways
- Quick check: Verify pruned models retain task performance with reduced parameters

**Low-bit Quantization**
- Why needed: Reduces memory footprint and enables efficient inference
- Quick check: Measure accuracy drop versus compression ratio at different bit widths

## Architecture Onboarding

**Component Map**
Knowledge Distillation -> Structured Pruning -> Low-bit Quantization

**Critical Path**
The P-KD-Q sequence represents the critical path for optimal performance, where each stage builds upon the high-precision outputs of the previous stage.

**Design Tradeoffs**
Early quantization sacrifices long-term capability for immediate compression gains, while late quantization preserves information for downstream processes at the cost of computational efficiency during training.

**Failure Signatures**
Performance collapse occurs when quantization precedes distillation or pruning, manifesting as degraded task accuracy and inability to recover through subsequent training.

**First Experiments**
1. Test P-KD-Q ordering on different model architectures (LLaMA, Mistral)
2. Evaluate compressed models on code generation and mathematical reasoning tasks
3. Measure actual inference latency on edge devices for each compression pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to models larger than 30B parameters or different architectures
- Evaluation focuses primarily on instruction-following and language understanding tasks
- Does not investigate impact on inference latency or memory access patterns

## Confidence

**High confidence**: Quantization should be applied last in compression pipelines
**Medium confidence**: P-KD-Q ordering optimality across different model scales
**Medium confidence**: 3.68× compression ratio claims specific to tested model and evaluation suite

## Next Checks

1. Validate compression ordering across multiple model sizes (1B, 8B, 70B parameters)
2. Evaluate compressed models on code generation, mathematical reasoning, and domain-specific tasks
3. Measure actual inference latency, memory bandwidth usage, and power consumption on edge hardware