---
ver: rpa2
title: Latent Preference Bandits
arxiv_id: '2508.05367'
source_url: https://arxiv.org/abs/2508.05367
tags:
- latent
- reward
- state
- lpbts
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent bandits reduce exploration in personalized decision-making
  by exploiting latent states that determine reward distributions. However, they require
  full knowledge of the joint distribution of rewards and latent states, which is
  often unrealistic and limits generalization between instances with different reward
  scales.
---

# Latent Preference Bandits

## Quick Facts
- arXiv ID: 2508.05367
- Source URL: https://arxiv.org/abs/2508.05367
- Reference count: 40
- Latent preference bandits reduce exploration by exploiting latent states that define preference orderings over actions, allowing different reward scales while maintaining shared preferences.

## Executive Summary
Latent bandits reduce exploration in personalized decision-making by exploiting latent states that determine reward distributions. However, they require full knowledge of the joint distribution of rewards and latent states, which is often unrealistic and limits generalization between instances with different reward scales. We propose latent preference bandits (LPB), a variant where each latent state defines a preference ordering over actions rather than a full reward distribution. This allows instances with the same latent state to have different reward scales while sharing the same preferences. We present lpbTS, a Thompson sampling algorithm that exploits this structure by sampling from an approximate posterior over latent states, using isotonic regression to enforce preference constraints. Empirically, lpbTS matches the performance of latent bandits with full reward knowledge when instances share reward scales, and outperforms them when scales differ. The benefit grows as the number of arms increases relative to the number of latent states.

## Method Summary
Latent preference bandits (LPB) reformulate the latent bandit problem by having each latent state define a preference ordering over actions rather than a full reward distribution. This approach allows different instances with the same latent state to have different reward scales while maintaining shared preferences. The lpbTS algorithm implements Thompson sampling with an approximate posterior over latent states, using isotonic regression to enforce preference constraints. This structure enables better generalization across instances with varying reward scales while preserving the exploration benefits of latent bandit approaches.

## Key Results
- lpbTS matches latent bandit performance when instances share reward scales
- lpbTS outperforms latent bandits when reward scales differ between instances
- Performance advantage increases with more arms relative to number of latent states

## Why This Works (Mechanism)
The key insight is that many decision-making problems have latent structure that determines relative preferences rather than absolute rewards. By focusing on preference orderings rather than full reward distributions, the algorithm can share information across instances that have the same underlying preferences but different scales. This is particularly useful in personalization scenarios where user groups share similar taste profiles but have different intensity levels or baselines.

## Foundational Learning
- **Latent bandits**: Why needed - to reduce exploration in personalized decision-making by exploiting shared latent structure. Quick check - can we identify consistent latent states that explain reward patterns?
- **Thompson sampling**: Why needed - to maintain principled uncertainty quantification in bandit settings. Quick check - does the posterior sampling properly balance exploration and exploitation?
- **Isotonic regression**: Why needed - to enforce preference constraints while fitting reward estimates. Quick check - are the learned preferences consistent with observed data?
- **Preference orderings**: Why needed - to capture relative preferences without requiring absolute reward knowledge. Quick check - do instances with same latent state exhibit similar preference patterns?
- **Posterior approximation**: Why needed - to handle computational complexity of exact Bayesian inference. Quick check - does the approximation maintain sufficient uncertainty representation?
- **Reward scale invariance**: Why needed - to enable generalization across instances with different absolute reward levels. Quick check - do learned preferences transfer across different reward scales?

## Architecture Onboarding
Component map: Environment -> Preference Model -> Isotonic Regression -> Thompson Sampling -> Action Selection
Critical path: Latent state observation → Preference constraint enforcement → Posterior sampling → Action selection
Design tradeoffs: Preference-based representation enables better generalization but requires more complex constraint enforcement; Thompson sampling provides principled uncertainty but is computationally heavier than greedy approaches
Failure signatures: Inconsistent preference learning (algorithm fails to exploit shared structure), computational bottlenecks (isostatic regression scaling issues), reward scale mismatch (preferences don't transfer across scales)
First experiments:
1. Verify preference consistency within latent states using synthetic data with known preferences
2. Test isotonic regression enforcement on small-scale examples
3. Compare exploration efficiency against standard Thompson sampling baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those implicit in the limitations.

## Limitations
- Assumes consistent preference orderings across instances, which may not hold when preferences evolve
- Isotonic regression introduces computational overhead that grows with number of arms
- Empirical evaluation focuses on synthetic settings with known latent structure

## Confidence
High: Advantage of preference-based representation over full reward distributions when reward scales vary
Medium: Empirical results due to limited scope and synthetic data

## Next Checks
1. Test lpbTS on real-world datasets with naturally occurring preference structures (e.g., recommendation systems with user clusters) to assess practical performance
2. Evaluate computational scalability as the number of arms grows to thousands, measuring runtime and memory usage
3. Investigate robustness to misspecification by introducing noise or adversarial preference orderings