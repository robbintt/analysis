---
ver: rpa2
title: 'FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships
  and Failure Modes'
arxiv_id: '2506.03278'
source_url: https://arxiv.org/abs/2506.03278
tags:
- reasoning
- question
- failure
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FailureSensorIQ is a benchmark dataset designed to evaluate large
  language models' (LLMs) understanding of sensor-failure relationships in industrial
  assets. The dataset includes 8,296 multiple-choice questions across 10 asset types,
  generated from ISO documents and expert templates, focusing on sensor relevance
  for failure modes.
---

# FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes

## Quick Facts
- arXiv ID: 2506.03278
- Source URL: https://arxiv.org/abs/2506.03278
- Reference count: 40
- Primary result: Top models achieve only 53.5% accuracy on single-answer questions, revealing significant gaps in industrial diagnostic reasoning

## Executive Summary
FailureSensorIQ is a benchmark dataset designed to evaluate large language models' understanding of sensor-failure relationships in industrial assets. The dataset includes 8,296 multiple-choice questions across 10 asset types, generated from ISO documents and expert templates, focusing on sensor relevance for failure modes. Evaluation of over a dozen LLMs revealed significant performance gaps: top models achieved only 53.5% accuracy on single-answer questions and under 21% on multi-answer questions. Perturbation and uncertainty analyses showed models' fragility to input changes and calibration challenges. Expert human evaluation confirmed the task's complexity, with scores averaging 60.20%. The dataset also supports an LLM-based feature selection pipeline for real-world predictive maintenance tasks.

## Method Summary
The dataset consists of 8,296 multiple-choice questions (2,667 single-answer, 5,629 multi-answer) across 10 industrial asset types. Questions are generated from ISO documents and expert templates without LLM involvement. Models are evaluated using zero-shot direct prompting in a closed-book setting, with structured output for answer decoding. The evaluation includes six metrics: accuracy on original questions, perturbed accuracy, consistency-based accuracy, coverage rate, set size, and uncertainty-adjusted accuracy. Perturbation analysis uses SimplePert, ComplexPert, and OptionsPert to test model robustness to input variations.

## Key Results
- Top models achieve 53.5% accuracy on single-answer questions and under 21% on multi-answer questions
- Performance drops significantly under perturbation (5-20% accuracy loss), revealing surface-pattern reliance
- CoT prompting improves performance for medium-sized models (70B parameters) but has limited effect on frontier models
- Asset-specific performance varies substantially, correlating with public documentation availability
- Expert human evaluation achieves 60.20% average accuracy, confirming task difficulty

## Why This Works (Mechanism)

### Mechanism 1: Domain Knowledge Encoding via Pre-training Exposure
- Claim: LLMs accumulate industrial asset knowledge proportional to domain-specific documentation in training corpora
- Evidence: Correlation between external knowledge volume and per-asset accuracy; assets with richer documentation yield higher accuracy
- Core assumption: Correlation implies causation; models retrieve encoded knowledge rather than relying on pattern matching
- Break condition: If models trained on identical corpora show divergent asset-specific performance, or if retrieval-augmented models fail to improve

### Mechanism 2: Reasoning Strategy Amplification via Prompting Architectures
- Claim: Explicit reasoning scaffolds (Chain-of-Thought) improve performance by forcing intermediate causal chain construction
- Evidence: CoT@Standard lifting llama-3.3-70b from 41.69% to 51.18%, suggesting structured decomposition helps navigate fine-grained distractors
- Core assumption: Performance gains stem from genuine reasoning decomposition rather than prompt-format exploitation
- Break condition: If perturbed prompts preserving reasoning structure cause similar performance drops as non-reasoning prompts

### Mechanism 3: Fragility to Surface-Level Perturbations via Option Bias Exploitation
- Claim: Models rely partially on positional and formatting heuristics, causing 5-20% accuracy drops under perturbation
- Evidence: Performance drops under perturbation; models select more incorrect options when input formatting changes
- Core assumption: Performance drops reflect reliance on non-semantic cues rather than increased cognitive load
- Break condition: If models trained with extensive data augmentation against perturbations maintain robustness while preserving semantic understanding

## Foundational Learning

- **Concept: Failure Modes and Effects Analysis (FMEA)**
  - Why needed: The benchmark maps failure modes to sensors; understanding FMEA methodology is prerequisite to interpreting sensor relevance
  - Quick check: Given "bearing wear" failure in a rotating asset, which sensor types would FMEA prioritization logic suggest monitoring?

- **Concept: Conformal Prediction for Uncertainty Quantification**
  - Why needed: Section H uses conformal prediction to compute coverage guarantees and set sizes
  - Quick check: If a model's conformal threshold is set too conservatively on a calibration set with imbalanced asset types, how would this manifest in test-set coverage rates?

- **Concept: Perturbation-Invariant Evaluation**
  - Why needed: The paper's core contribution includes Acc@Consist, measuring accuracy preservation under format changes
  - Quick check: A model achieves 70% Acc@Original but 35% Acc@Consist under ComplexPert; what does this pattern suggest about knowledge representation versus surface-pattern exploitation?

## Architecture Onboarding

- **Component map:**
  ISO Documents + Expert Templates → Question Generation Pipeline → SC-MCQA (2,667 Q) + MC-MCQA (5,629 Q) → Perturbation Engine → SimplePert / ComplexPert / OptionsPert → Evaluation Layer → Acc@Original / Acc@Perturb / Acc@Consist / UAcc → Application Layer → LLMFeatureSelector

- **Critical path:**
  1. Start with llama-3.3-70b-instruct as baseline
  2. Run direct prompting on SC-MCQA → establish Acc@Original
  3. Apply ComplexPert pipeline → measure Acc@Consist delta
  4. If delta > 15%, investigate response patterns for over-selection
  5. Integrate LLMFeatureSelector with real dataset

- **Design tradeoffs:**
  - Closed-book vs. RAG evaluation: RAG degrades performance, suggesting benchmark tests internal reasoning
  - Reasoning prompt selection: CoT@Standard yields highest gains but adds inference cost
  - Multi-answer protocol: Strict evaluation penalizes partial credit while soft metrics may overstate capability

- **Failure signatures:**
  - High Acc@Original (55%+) but low Acc@Consist (<30%) → surface-pattern reliance
  - High set size (SS > 4) with low UAcc → model over-selecting options due to miscalibration
  - Asset-specific performance variance >20% → corpus exposure gaps
  - ReAct performance below direct prompting → retrieval interfering with reasoning

- **First 3 experiments:**
  1. Run mistral-large-instruct-2407 on SC-MCQA with direct prompting, compute all six metrics, compare set size and accuracy
  2. Apply ComplexPert to same model, compute Acc@Consist and PDR, analyze response patterns for over-selection
  3. Deploy LLMFeatureSelector on Air Compressor dataset, compute correlations between LLM top-5 recommendations and target variable

## Open Questions the Paper Calls Out

- **Temporal sensor-failure dynamics:** The benchmark focuses on static knowledge without modeling temporal sensor-failure dynamics; future work will extend with temporal reasoning tasks
- **Retrieval and reasoning architectures:** ReAct-based agents fail to deliver expected performance improvements; the method of search and reasoning is a critical underexplored dimension for tool-augmented agents
- **Domain-specific pre-training strategies:** Substantial performance variation across assets correlates with public documentation availability; targeted experiments needed to determine effective knowledge sources

## Limitations

- Dataset generation relies on human experts without specified inter-annotator agreement rates
- Perturbation techniques may not capture all real-world variations in question presentation
- Expert human evaluation dataset is relatively small (22 participants, 10 questions each)
- Benchmark only captures static relationships without modeling temporal fault progression

## Confidence

**High Confidence** (supported by multiple validation methods):
- Dataset successfully exposes LLM limitations in industrial sensor-failure reasoning
- Closed-book evaluation methodology is sound and reproducible
- Multi-answer format reveals different model capabilities than single-answer format

**Medium Confidence** (reasonable assumptions but limited validation):
- Correlation between external knowledge volume and accuracy implies knowledge encoding
- CoT prompting genuinely improves reasoning vs. format exploitation
- Perturbation fragility indicates surface-pattern reliance rather than semantic understanding

**Low Confidence** (requires further validation):
- Human expert performance (60.20%) represents true expert-level capability
- LLM-based feature selection will generalize to real-world predictive maintenance
- Benchmark comprehensively covers all relevant industrial asset types and failure modes

## Next Checks

1. Reproduce perturbation analysis with expanded variations including synonym replacement, sentence reordering, and noise injection
2. Validate human expert evaluation scale with larger-scale evaluation across diverse expertise levels
3. Test LLM-based feature selection in deployment on at least two additional real industrial datasets beyond the Air Compressor case study