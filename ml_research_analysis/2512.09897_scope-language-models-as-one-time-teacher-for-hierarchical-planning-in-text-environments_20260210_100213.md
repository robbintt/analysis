---
ver: rpa2
title: 'SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text
  Environments'
arxiv_id: '2512.09897'
source_url: https://arxiv.org/abs/2512.09897
tags:
- craft
- birch
- planks
- subgoals
- pink
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCOPE, a hierarchical planning method that
  uses LLM-generated subgoals only once at initialization to pretrain a student planner,
  rather than repeatedly querying the LLM during training. By extracting subgoals
  directly from demonstration trajectories and combining them with RL fine-tuning
  via world models, SCOPE reduces computational cost and eliminates the need for LLM
  inference at test time.
---

# SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments

## Quick Facts
- arXiv ID: 2512.09897
- Source URL: https://arxiv.org/abs/2512.09897
- Reference count: 40
- One-line primary result: SCOPE achieves 0.56 success rate in TextCraft, outperforming LLM-based ADaPT (0.52) while reducing inference time from 164.4s to 3.0s

## Executive Summary
SCOPE introduces a hierarchical planning method that uses LLM-generated subgoals only once at initialization to pretrain a student planner, eliminating repeated LLM queries during training. By extracting subgoals directly from demonstration trajectories and combining them with RL fine-tuning via world models, SCOPE reduces computational cost and eliminates the need for LLM inference at test time. On the TextCraft environment, SCOPE achieves strong performance while being 55x faster than baselines that query LLMs at each step.

## Method Summary
SCOPE uses an LLM to analyze 50 demonstration trajectories and generate Python functions for subgoal decomposition and completion checking. These subgoals pretrain a manager agent via supervised learning, which is then fine-tuned with RL. The employee agent is pretrained to imitate subgoal-conditioned trajectories and fine-tuned via model-based RL using an Employee World Model. The manager uses a Manager World Model constructed by composing the trained employee with the EWM. Both agents are trained via weighted imitation learning on successful rollouts, allowing the manager to discover achievable subgoals that compensate for employee imperfections.

## Key Results
- SCOPE achieves 0.56 success rate in TextCraft, outperforming ADaPT (0.52)
- Inference time reduced from 164.4 seconds to 3.0 seconds (55x speedup)
- Manager RL fine-tuning essential: without it, success drops from 0.56 to 0.24
- Subgoal alignment critical: 25% random item remapping drops success to 0.09

## Why This Works (Mechanism)

### Mechanism 1: Temporal Abstraction Through One-Shot Subgoal Injection
- **Claim:** Providing LLM-generated subgoals once at initialization enables a lightweight student planner to learn effective hierarchical decomposition without repeated LLM queries.
- **Mechanism:** The LLM analyzes 50 trajectory samples to produce a decomposition function f_dc that partitions trajectories into subtrajectories ending at subgoal states. These extracted subgoal sequences pretrain a manager agent via supervised learning, which is then fine-tuned with RL—transferring the LLM's semantic knowledge about task structure into a compact neural policy.
- **Core assumption:** Suboptimal subgoals from static demonstration analysis provide sufficient inductive bias for hierarchical learning, even without environment interaction.
- **Evidence anchors:**
  - [abstract]: "our method derives subgoals directly from example trajectories... removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals."
  - [section 3]: "by applying f_dc, we decompose tuple (g, I, T) into... state-action-subgoal-instruction tuples... compiled into the dataset Φ"
  - [corpus]: Weak direct support; neighbor papers focus on iterative LLM guidance, not one-time extraction.
- **Break condition:** If subgoals are misaligned with environment dynamics (e.g., >25% random item remapping), performance collapses (success drops to 0.09 vs. 0.56 baseline), showing alignment is critical.

### Mechanism 2: Manager-Employee Hierarchy with World-Model Rollouts
- **Claim:** A two-level hierarchy (manager proposes subgoals, employee executes) trained with model-based RL compensates for imperfections in both agents through iterative adaptation.
- **Mechanism:** The employee agent is pretrained to imitate subgoal-conditioned trajectories, then fine-tuned via CEM rollouts in an Employee World Model (EWM) that validates actions and predicts states. The manager agent is pretrained on LLM-subgoal sequences, then fine-tuned in a Manager World Model (MWM) constructed by composing the trained employee with EWM. The manager learns to propose achievable subgoals by receiving positive feedback only when subgoals succeed in the composed model.
- **Core assumption:** World models trained on suboptimal trajectories can support effective policy improvement via weighted imitation learning on successful rollouts.
- **Evidence anchors:**
  - [abstract]: "SCOPE pretrains a lightweight student planner, which is then fine-tuned using reinforcement learning and world models."
  - [section 4.1]: "EWM(s,a;I) first verifies whether a is valid... If valid, the model predicts the next state... We adopt Cross-Entropy Method"
  - [section 4.2]: "manager world model MWM(s, g̃)... implemented by roll[ing] out the employee policy"
  - [corpus]: "CoEx" paper supports learned models for planning but doesn't address one-shot distillation.
- **Break condition:** Removing manager RL fine-tuning drops success from 0.56 to 0.24 (Table 3)—static subgoal sequences cannot recover from execution failures.

### Mechanism 3: Compensation for Subgoal Imperfection Through RL Fine-Tuning
- **Claim:** RL fine-tuning allows the manager to discover alternative, achievable subgoals that compensate for employee imperfections and LLM-generated subgoal suboptimality.
- **Mechanism:** During manager RL training, failed subgoal proposals receive zero weight, forcing exploration of alternatives. Over iterations, the manager learns to propose subgoals the current employee can reliably achieve. Figure 6 shows steadily increasing success rates during this adaptation.
- **Core assumption:** The subgoal space is rich enough that alternative decompositions exist when initial proposals fail.
- **Evidence anchors:**
  - [section 5.3]: "RL-based finetuning enables the manager to adapt to and compensate for these imperfections... when a proposed subgoal is not achieved... it learns to propose alternative subgoals"
  - [section 5.3, Fig 6]: "Validation trajectory success rate for the manager agent during RL fine-tuning... steadily increasing"
  - [corpus]: Limited direct evidence; HRL papers support decomposition but not specifically teacher-imperfection compensation.
- **Break condition:** If subgoals are completely misaligned (100% item remapping), success collapses to 0.02—RL cannot recover from fundamentally wrong guidance.

## Foundational Learning

- **Concept: Hierarchical Reinforcement Learning (HRL)**
  - **Why needed here:** SCOPE implements temporal abstraction where a high-level manager operates over subgoals and a low-level employee executes primitive actions. Understanding options, semi-MDPs, and the curse of horizon is essential.
  - **Quick check question:** Why does decomposing a 50-step crafting task into 5 subgoals improve credit assignment?

- **Concept: Model-Based RL with World Models**
  - **Why needed here:** Both agents are fine-tuned via rollouts in learned world models using CEM. Understanding dynamics learning and planning with imperfect models is critical.
  - **Quick check question:** What is the risk of over-reliance on a world model trained on suboptimal trajectories, and how does CEM's elite selection mitigate this?

- **Concept: Knowledge Distillation from LLMs**
  - **Why needed here:** SCOPE distills LLM's subgoal-generation capability into a small policy network via supervised pretraining, removing inference-time LLM dependency.
  - **Quick check question:** Why might one-shot distillation yield lower explainability than adaptive teacher querying?

## Architecture Onboarding

- **Component map:**
  - LLM Teacher (One-Time) -> Python functions f_dc (subgoal decomposition) and f_sg (completion checker)
  - Trajectories + f_dc -> Dataset Φ (employee tuples) and Φ₀ (manager tuples)
  - Employee Agent (LSTM) <- Pretraining (BC on Φ) -> Fine-tuning (CEM in EWM)
  - Manager Agent (LSTM) <- Pretraining (BC on Φ₀) -> Fine-tuning (CEM in MWM)
  - EWM (Transformer) predicts action validity and next state
  - MWM = EWM + Frozen Employee Agent

- **Critical path:**
  1. Generate f_dc and f_sg via one-time LLM prompt.
  2. Build datasets Φ and Φ₀.
  3. Train EWM on trajectories.
  4. Pretrain then fine-tune employee via CEM in EWM.
  5. Construct MWM (employee + EWM).
  6. Pretrain then fine-tune manager via CEM in MWM.
  7. Deploy hierarchy.

- **Design tradeoffs:**
  - One-time LLM: 3.0s vs 164.4s inference, 11M vs 175B params, but 2% success gap and reduced explainability.
  - Subgoal alignment beats specificity: Vague aligned subgoals degrade moderately; misaligned ones collapse catastrophically.
  - Manager RL essential: Without it, success drops from 0.56 to 0.24.

- **Failure signatures:**
  - **Employee:** Confuses base vs. intermediate items, attempts infeasible crafts, triggered by unfamiliar inventory states.
  - **Manager:** Proposes infeasible or misaligned subgoals.
  - **World model:** EWM may predict invalid states; MWM may misestimate employee capability.
  - **Complete collapse:** >25% subgoal-environment misalignment → near-zero success.

- **First 3 experiments:**
  1. Ablate manager RL (static subgoals) → expect ~0.24 success.
  2. Degrade subgoal alignment (remapping p ∈ {0, 0.25, 0.5, 1.0}) → expect sharp drop at 0.25.
  3. Vary employee strength (partial checkpoints) → expect compounding effect on final success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCOPE's one-time LLM subgoal extraction approach generalize to more complex text environments beyond TextCraft, particularly those with larger action spaces, longer horizons, or partial observability?
- Basis in paper: [explicit] The authors describe TextCraft as "a simplified, text-only version of Minecraft" and note this is "preliminary work" with evaluation limited to a single benchmark. They also state "in more complex settings, LLM-generated subgoal decomposition may be the only practical option."
- Why unresolved: The experiments are confined to TextCraft, which has structured crafting dependencies and deterministic outcomes. Real-world text environments may have branching action spaces, stochastic transitions, and ambiguous state descriptions that could undermine the heuristic subgoal decomposition approach.
- What evidence would resolve it: Evaluation on additional text-based RL benchmarks (e.g., ALFWorld, ScienceWorld, or Interactive Fiction games) showing comparable success rate improvements over LLM-dependent baselines.

### Open Question 2
- Question: How can the suboptimality of LLM-generated subgoals be reduced when the LLM cannot interact with the environment?
- Basis in paper: [explicit] The abstract states subgoals are "suboptimal due to lack of environment interaction," and Section 5.2 shows hand-engineered subgoals outperform LLM-generated ones by 2%, suggesting room for improvement.
- Why unresolved: The paper uses a one-time extraction that produces subgoals lacking interpretability and sometimes alignment with optimal task decomposition, but does not propose mechanisms to refine them before training.
- What evidence would resolve it: Experiments incorporating iterative subgoal refinement (e.g., multiple LLM queries with simulated environment feedback, or trajectory-based verification) demonstrating reduced suboptimality gaps.

### Open Question 3
- Question: What are the minimum requirements for demonstration trajectory quality and quantity for SCOPE to achieve effective hierarchical planning?
- Basis in paper: [inferred] The method assumes access to "suboptimal example trajectories" from humans, with 500K generated trajectories used in experiments. The paper does not ablate how performance degrades with fewer or lower-quality demonstrations.
- Why unresolved: The reliance on demonstration data is a key assumption, but the sensitivity to dataset size, trajectory optimality, and diversity remains uncharacterized.
- What evidence would resolve it: Systematic ablation studies varying the number of trajectories (e.g., 1K, 10K, 100K, 500K) and the optimality ratio (e.g., varying the random action injection rate from 0% to 50%) and reporting resulting success rates.

## Limitations
- The scalability of one-time LLM subgoal extraction to environments with significantly longer horizons or more complex state spaces remains untested.
- The paper assumes access to 50 high-quality demonstration trajectories, but the impact of demonstration quantity/quality on subgoal extraction fidelity is not explored.
- While the approach reduces LLM inference time from 164.4s to 3.0s, the 0.56 vs 0.52 success rate trade-off may not justify adoption in all scenarios where LLM guidance is already available.

## Confidence
- **High Confidence**: The core mechanism of one-time subgoal extraction + RL fine-tuning is well-supported by ablation results (Table 3 shows manager RL is essential, and subgoal misalignment experiments in Figure 8 demonstrate the importance of alignment).
- **Medium Confidence**: The generalization of SCOPE to environments beyond TextCraft is plausible but unproven; the approach relies heavily on LLM's ability to extract meaningful subgoals from demonstrations, which may not transfer cleanly.
- **Medium Confidence**: The claim that SCOPE achieves "real-time inference" (3.0s) while ADaPT requires 164.4s is methodologically sound but depends on the specific TextCraft setup and may not scale to more complex environments.

## Next Checks
1. Test SCOPE on a longer-horizon or more combinatorially complex text environment (e.g., multi-room TextWorld games) to assess scalability.
2. Vary the number and quality of demonstration trajectories (e.g., 10 vs. 50, optimal vs. noisy) to quantify the robustness of subgoal extraction.
3. Implement a runtime comparison across different LLM sizes (e.g., 7B vs. 175B) to confirm the inference-time advantage holds in practical settings.