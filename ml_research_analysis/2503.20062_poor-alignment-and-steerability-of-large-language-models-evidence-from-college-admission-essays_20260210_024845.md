---
ver: rpa2
title: 'Poor Alignment and Steerability of Large Language Models: Evidence from College
  Admission Essays'
arxiv_id: '2503.20062'
source_url: https://arxiv.org/abs/2503.20062
tags:
- essays
- engineering
- identity
- have
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) fail to produce college application
  essays that resemble human writing, even when prompted with demographic identity
  details. Analysis of 87,696 essays (30,000 human + synthetic pairs) showed LLM outputs
  form distinct clusters from human essays in embedding space (PCA) and are easily
  classified (F1=0.998).
---

# Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays

## Quick Facts
- **arXiv ID**: 2503.20062
- **Source URL**: https://arxiv.org/abs/2503.20062
- **Reference count**: 40
- **Primary result**: LLM outputs form distinct semantic clusters from human essays, are easily classified (F1=0.998), and identity prompting fails to improve alignment.

## Executive Summary
This study reveals fundamental alignment failures in large language models when generating college application essays. Despite explicit demographic identity prompts, LLM outputs remain semantically distinct from human writing and fail to capture authentic personal narratives. The research demonstrates that LLMs default to abstract, conceptual language rather than the concrete, temporally-grounded storytelling characteristic of human reflection. Identity prompting only superficially modifies outputs without achieving meaningful alignment with specific demographic groups, suggesting current steering approaches are insufficient for high-stakes applications requiring authentic personal expression.

## Method Summary
The study compared 29,232 human-authored Common App essays (2019-2023) against synthetic essays generated from 8 different LLMs using two prompting strategies: default prompts and identity-enhanced prompts. Essays were encoded using T5-base sentence embeddings and TF-IDF vectors, then analyzed through logistic regression classification, PCA clustering, and pairwise cosine similarity measurements. The analysis focused on binary classification performance (Human vs LLM), semantic clustering patterns, and the effectiveness of demographic identity prompting in steering model outputs toward human-like expression patterns.

## Key Results
- LLM essays achieve F1=0.998 classification accuracy against human essays, forming distinct semantic clusters in embedding space
- Identity prompting fails to align LLM outputs with human demographic groups; ID-prompted essays cluster closer to default LLM outputs than to human essays
- LLM outputs show severe homogenization across demographics (cosine similarity 0.95+) compared to human diversity (similarity 0.88), favoring abstract language over personal narratives

## Why This Works (Mechanism)

### Mechanism 1: Semantic Generalization vs. Temporal Grounding
LLMs generate abstract, high-level conceptual language rather than concrete, temporally-grounded narratives. The model optimizes for plausible thematic continuations (e.g., "resilience," "growth") rather than specific, idiosyncratic memories, resulting in keyword replication without temporal dynamics (e.g., "year," "time," "friend") found in human essays. This reflects reliance on training data regularities over simulated personal experience.

### Mechanism 2: Ineffective Steerability via Identity Prompting
Explicit demographic prompting fails to align LLM outputs because the model's rigid latent writing style dominates prompt perturbations. When prompted with identity (e.g., "I am Asian"), the model inserts demographic markers verbatim but maintains the underlying semantic clustering of the base model. The high cosine similarity between ID-prompted and un-prompted LLM essays (0.95+) indicates a stylistic floor that simple prompting cannot penetrate.

### Mechanism 3: Homogenization via Preference Tuning (RLHF)
The distinct separation of LLM text results from RLHF's flattening effect, which optimizes for safe, positive, generic outputs at the cost of human-like variance. RLHF penalizes risk, leading to "upbeat narratives" and "generic quality," creating algorithmic monoculture where LLM output variance (0.952 similarity) is drastically lower than human output variance (0.88 similarity).

## Foundational Learning

- **PCA (Principal Component Analysis) in Embedding Space**
  - **Why needed here:** To visualize high-dimensional semantic differences between human and LLM essays. The paper relies on T5 embeddings reduced to 2D to prove LLMs form a distinct "island" separate from human writing.
  - **Quick check question:** If two text groups overlap in a PCA plot, does it mean they are lexically identical or just semantically similar in the dimensions captured?

- **F1 Score vs. Accuracy in Classification**
  - **Why needed here:** To evaluate AI text detectability. An F1 score of ~0.998 means the classifier found almost no false positives/negatives, proving LLM writing is fundamentally different, not just slightly off.
  - **Quick check question:** Why is F1 score preferred over simple accuracy when classes (Human vs AI) might be imbalanced or the cost of missing an AI essay is high?

- **Homogenization / Algorithmic Monoculture**
  - **Why needed here:** To understand societal risk. It's not just that LLMs write differently; they write identically to each other across demographics, reducing diversity.
  - **Quick check question:** Does high pairwise cosine similarity within a group indicate high diversity or high homogenization?

## Architecture Onboarding

- **Component map:** Human Essays + Synthetic Pairs (Default & ID-Prompted) -> T5 Transformer + TF-IDF -> PCA + Logistic Regression + Cosine Similarity
- **Critical path:** Data Preprocessing (removing wrappers/refusals) -> Encoding (T5/TF-IDF) -> Dimensionality Reduction (PCA) -> Metric Calculation (F1, Cosine Sim)
- **Design tradeoffs:**
  - **Lexical vs. Semantic:** TF-IDF captures specific words (good for spotting "challenge" overuse), while T5 captures meaning (good for spotting "inauthentic" narrative structure). The paper uses both to ensure robustness.
  - **Prompting Strategy:** The paper used "minimal" steering (simple identity facts). Tradeoff: Does not test "heavy" prompt engineering (e.g., "Write like a 17-year-old using slang"), leaving a gap in "max capability" steerability.
- **Failure signatures:**
  - **High Inter-LLM Similarity:** If LLM vs LLM cosine similarity > 0.95, the model is effectively "deaf" to the specific identity prompt.
  - **Abstract Lexical Bias:** Logistic regression coefficients heavily weighting "challenge/growth" (LLM) vs. "friend/time" (Human).
- **First 3 experiments:**
  1. Replicate the Classifier: Train Logistic Regression on TF-IDF vectors of Human vs GPT-4o essays to verify if the 0.998 F1 score holds on a sample of new prompts.
  2. Ablation on Identity: Generate essays with "Counterfactual" identities (e.g., prompt "Female" for a Male-authored essay context) to measure if the cosine similarity shift is purely syntactic or actually grounded in demographic reality.
  3. Temperature Sweeping: Vary generation temperature (0.0 to 1.0) to see if "creativity" settings reduce homogenization (lower the LLM-vs-LLM similarity) or just increase noise without bridging the gap to human writing.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can iterative human-AI co-writing produce college application essays that are linguistically indistinguishable from human-authored texts?
- **Basis in paper:** [explicit] The authors note their study focused on single-turn generation and suggest "future studies could examine human-AI co-writing behavior to complement our study."
- **Why unresolved:** The current study only evaluated fully synthetic text against fully human text, ignoring the hybrid workflows students likely use.
- **What evidence would resolve it:** A comparative analysis of linguistic markers and classifier detection rates on essays produced through collaborative drafting versus fully human drafts.

### Open Question 2
- **Question:** Does providing personal writing samples (few-shot prompting) succeed in steering LLMs to mimic individual voices where identity prompting failed?
- **Basis in paper:** [explicit] The limitations section states students may use "elaborate prompt engineering strategies... e.g., providing writing samples to convey their voice," which was not tested.
- **Why unresolved:** The paper only tested demographic identity prompts, which proved ineffective at aligning the model with specific human styles.
- **What evidence would resolve it:** An experiment comparing the alignment scores of essays generated using writing samples as context against those generated with demographic prompts.

### Open Question 3
- **Question:** Do LLM-generated essays exhibit structural narrative deficiencies (e.g., plot progression, temporal dynamics) that differ from human essays?
- **Basis in paper:** [explicit] The authors acknowledge focusing on "word usage and sentence-level embeddings" and suggest "future studies can explore how narratives presented by LLMs differ."
- **Why unresolved:** Embedding-based analysis captured semantic clustering but did not assess higher-order narrative structures or storytelling mechanics.
- **What evidence would resolve it:** Analysis of essay datasets using narrative theory frameworks (e.g., story arcs, tension graphs) to identify structural divergences.

## Limitations

- Sample representativeness concerns due to single admission cycle (2019-2023) and potential demographic skew in the human essay dataset
- Model version specificity limits generalizability to different LLM versions or fine-tuning approaches
- 1024-token embedding limit may truncate longer essays, affecting semantic representation accuracy

## Confidence

- **High confidence**: LLM outputs form distinct semantic clusters from human essays (F1=0.998), identity prompting fails to improve alignment, and LLMs show homogenization across demographic groups
- **Medium confidence**: Attribution of homogenization to RLHF preference tuning is supported by literature but not directly tested in this study
- **Low confidence**: Specific mechanisms of why LLMs default to abstract language over personal narratives remain speculative without deeper analysis of training data distributions

## Next Checks

1. **Temporal robustness test**: Generate essays using prompts from different years (2015-2018 vs 2020-2023) to determine if the alignment gap is consistent across writing style evolution periods
2. **Base model comparison**: Repeat the analysis using base models (without RLHF alignment) to isolate whether homogenization is driven by preference tuning versus pretraining data distribution
3. **Prompt engineering ablation**: Test alternative steerability approaches (persona cards, few-shot examples, detailed narrative instructions) to establish the boundary conditions of effective prompt-based alignment