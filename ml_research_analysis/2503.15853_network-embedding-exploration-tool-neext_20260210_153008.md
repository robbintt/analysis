---
ver: rpa2
title: Network Embedding Exploration Tool (NEExT)
arxiv_id: '2503.15853'
source_url: https://arxiv.org/abs/2503.15853
tags:
- features
- graph
- feature
- graphs
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEExT (Network Embedding Exploration Tool) addresses the challenge
  of analyzing collections of graphs when ground-truth labels are absent or when model
  interpretability is crucial. The framework generates graph embeddings using user-defined
  node features combined with fast embedding algorithms like Approximate Wasserstein,
  making it both interpretable and computationally efficient.
---

# Network Embedding Exploration Tool (NEExT)

## Quick Facts
- **arXiv ID:** 2503.15853
- **Source URL:** https://arxiv.org/abs/2503.15853
- **Reference count:** 34
- **Primary result:** NEExT achieves classification accuracy comparable to state-of-the-art methods on real-world datasets (IMDB, MUTAG, NCI1, BZR, PROTEINS) while maintaining interpretability.

## Executive Summary
NEExT (Network Embedding Exploration Tool) addresses the challenge of analyzing collections of graphs when ground-truth labels are absent or when model interpretability is crucial. The framework generates graph embeddings using user-defined node features combined with fast embedding algorithms like Approximate Wasserstein, making it both interpretable and computationally efficient. The core method computes interpretable node features (centrality measures, structural embeddings, custom metrics), then embeds graphs by measuring Wasserstein distances between node feature distributions. For supervised tasks, NEExT uses greedy or fast feature selection to identify the most predictive node features. For unsupervised scenarios, it selects features based on variance across graphs.

## Method Summary
NEExT generates graph embeddings by first computing interpretable node features (LSME, centrality measures, expansion, and custom metrics) for each graph in a collection. These node features form distributions for each graph, which are then embedded using Approximate Wasserstein distance to measure dissimilarity between distributions. The framework supports both supervised feature selection (greedy incremental accuracy or Random Forest importance) and unsupervised selection (variance-based). Node sampling (50-60%) can reduce computation time by 5-20% without significantly degrading classifier performance. The resulting fixed-dimensional embeddings can be used with standard machine learning classifiers like XGBoost.

## Key Results
- NEExT achieves classification accuracy comparable to state-of-the-art methods on real-world datasets (IMDB, MUTAG, NCI1, BZR, PROTEINS) while maintaining interpretability.
- Feature importance analysis reveals which structural properties drive predictions, with greedy selection achieving highest accuracy across all feature counts on NCI1 dataset.
- Experiments show embeddings capture graph structure and noise levels effectively across synthetic and real networks, with 50-60% node sampling reducing computation time by 5-20% without significantly impacting classifier performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Interpretable node features combined with distributional distance metrics produce graph embeddings that maintain predictive performance while remaining explainable.
- **Mechanism**: Each graph is represented as a distribution of node feature vectors. The Wasserstein distance (optimal transport) measures dissimilarity between these distributions. Linear Optimal Transport (LOT) maps each graph's distribution to a reference distribution, and SVD of the resulting transport plans yields fixed-dimensional embeddings.
- **Core assumption**: Node-level feature distributions capture graph-level properties relevant to downstream tasks; Wasserstein distance preserves these relationships in embedding space.
- **Evidence anchors**:
  - [abstract] "The core method computes interpretable node features (centrality measures, structural embeddings, custom metrics), then embeds graphs by measuring Wasserstein distances between node feature distributions."
  - [Section 2.3] "We use the implementation of this approach from the easy to use Vectorizers Python package, which solves the LOT and computes embeddings by computing the SVD of the optimal transport plans."
  - [corpus] Related work on Wasserstein embeddings (Togninalli et al., Kolouri et al.) cited but not in provided corpus; corpus evidence weak for this specific mechanism.
- **Break condition**: If node feature distributions are uninformative (e.g., all graphs have identical feature distributions despite different structures), embeddings will fail to discriminate.

### Mechanism 2
- **Claim**: Greedy feature selection identifies a subset of node features that maximizes incremental classification accuracy, yielding both efficient computation and interpretable feature importance rankings.
- **Mechanism**: Starting from an empty feature set, iteratively add the feature that provides the largest accuracy gain when combined with already-selected features. Each candidate subset generates embeddings via Approximate Wasserstein, trains XGBoost classifiers (50 seeds, 70/30 split), and evaluates average accuracy.
- **Core assumption**: Features contributing incrementally to accuracy on held-out data generalize to unseen graphs; greedy optimization approximates global optimum sufficiently.
- **Evidence anchors**:
  - [Section 2.4] "The main idea behind the first algorithm is to find a permutation of the features for which the order is determined in a greedy fashion by selecting features with the highest incremental contribution to the accuracy."
  - [Figure 7] Greedy method achieves highest accuracy across all feature counts on NCI1 dataset.
  - [corpus] No direct corpus evidence for greedy feature selection on graph embeddings.
- **Break condition**: If features are highly correlated or interactions are critical (non-additive contributions), greedy selection may miss optimal subsets.

### Mechanism 3
- **Claim**: Random node sampling (50-60%) reduces computation time by 5-20% without significantly degrading classifier performance because Wasserstein-based embeddings are robust to distribution perturbations.
- **Mechanism**: Sample nodes uniformly at random from each graph. Compute node features only on sampled nodes. Generate embeddings from sampled distributions. Approximate Wasserstein uses center-of-mass comparisons, which are statistically stable under uniform sampling.
- **Core assumption**: Uniform random sampling preserves the shape of node feature distributions sufficiently for downstream classification boundaries.
- **Evidence anchors**:
  - [Section 3.4.3] "In Figure 9...the quality of the classifiers built on those embeddings is not impacted [by sampling]."
  - [Section 3.4.3] "This has to do with the fact that classifiers look for dividing boundaries between classes, and as long as this boundary can be found, a classifier can perform well."
  - [corpus] No corpus evidence directly addressing sampling robustness in Wasserstein graph embeddings.
- **Break condition**: If graphs contain rare but informative nodes (outliers, critical connectors), uniform sampling may miss them, degrading performance.

## Foundational Learning

- **Concept**: Wasserstein distance / Optimal Transport
  - **Why needed here**: Core embedding mechanism; understanding that Wasserstein measures "work" to transform one distribution into another explains why it preserves structural relationships better than KL divergence or total variation.
  - **Quick check question**: Given two point clouds representing node degree distributions, explain why Wasserstein distance would be non-zero even if both have the same mean.

- **Concept**: Node-level vs. Graph-level embeddings
  - **Why needed here**: NEExT's architecture explicitly separates these: node features → node vectors → (via distribution embedding) → graph vectors. Confusing these levels will lead to implementation errors.
  - **Quick check question**: If you have 10 graphs with average 50 nodes each, and you compute 8-dimensional node features, what is the shape of the intermediate representation before graph embedding?

- **Concept**: Feature importance via permutation/incremental methods
  - **Why needed here**: Distinguishes Greedy (incremental accuracy) from Fast (Random Forest importance) methods; critical for interpreting outputs correctly.
  - **Quick check question**: Why might the Greedy method rank features differently than the Fast method on the same dataset?

## Architecture Onboarding

- **Component map**: GraphCollection -> Node feature generators -> Vectorizers library -> Feature selection -> Downstream classifier
- **Critical path**: Pre-processing → Node feature computation (potentially sampled) → Approximate Wasserstein embedding → Feature selection (if supervised) → Downstream classifier
- **Design tradeoffs**:
  - Wasserstein vs. ApproximateWasserstein: Accuracy vs. speed (use Approximate when k > m)
  - Greedy vs. Fast feature selection: Optimal ordering vs. computational cost (Fast is one-shot, Greedy iterates through all features)
  - Full graph vs. sampled nodes: Embedding quality vs. computation time (50% sampling ~5-20% speedup, minimal accuracy loss)
- **Failure signatures**:
  - All embeddings cluster together: Check if node features are being computed correctly; may indicate features have near-zero variance across graphs
  - Feature selection yields random performance: Labels may be uncorrelated with any structural features; consider external/injected features
  - Sampling degrades accuracy sharply: Graphs may have critical rare nodes; reduce sampling rate or use stratified sampling (not currently implemented)
- **First 3 experiments**:
  1. Replicate Experiment 1 (ABCD noise detection): Generate 100 graphs with ξ varying 0.1-0.5, compute Expansion + LSME features (k=4), embed with ApproximateWasserstein (d=2), train regression model. Verify correlation between embedding position and ξ.
  2. Run Greedy feature selection on a labeled dataset (e.g., NCI1): Confirm top features match Table 5; measure accuracy trajectory as features are added.
  3. Ablate sampling rates (0%, 25%, 50%, 75%) on PROTEINS dataset: Plot both Embedding Similarity Score and classifier accuracy to verify robustness claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid exploitation-exploration algorithm outperform the purely exploitative Greedy feature selection method?
- Basis in paper: [explicit] "It is important to note that the Greedy method uses only an exploitative method and may not select the absolute best set of features. A more robust algorithm could mix both exploitation and exploration to determine the most predictive set of features. This will be explored in future studies."
- Why unresolved: The current Greedy method greedily selects features with highest incremental accuracy but may miss globally optimal feature combinations.
- What evidence would resolve it: Comparing classification accuracy of embeddings from hybrid algorithms (e.g., combining Greedy with random exploration or beam search) against pure Greedy across multiple datasets.

### Open Question 2
- Question: How does integrating additional node and edge metadata affect NEExT's embedding quality and predictive performance?
- Basis in paper: [inferred] The paper notes that benchmark models "are trained on additional metadata available for nodes as well as edges. We do not do it at present but it would be easy to incorporate such additional information."
- Why unresolved: NEExT currently relies solely on structural node features; the impact of external attributes (e.g., text features, chemical properties) on embedding quality remains untested.
- What evidence would resolve it: Ablation studies on datasets with available metadata (e.g., NCI1, MUTAG) comparing embeddings with and without metadata integration.

### Open Question 3
- Question: What is the optimal per-feature sampling strategy to maximize embedding quality while minimizing computational cost?
- Basis in paper: [inferred] Experiments show "sampling has a more significant impact on embeddings generated using feature group FG-3 (LSME and Expansion)" compared to centrality-based features, and "each feature... will be impacted differently by the sampling rate."
- Why unresolved: Current sampling uniformly samples nodes across all features; different features may require different sampling rates to preserve distributional properties.
- What evidence would resolve it: Systematic experiments varying sampling rates per feature type and measuring Embedding Similarity Scores and downstream classifier performance.

### Open Question 4
- Question: Can adaptive sampling strategies identify and preserve nodes critical to maintaining embedding quality?
- Basis in paper: [inferred] The authors note "some graphs could have a small set of computationally expensive nodes (perhaps outliers), which contribute to the computational cost while not impacting the quality of the resulting classifier models."
- Why unresolved: Uniform random sampling may waste computation on uninformative nodes while discarding important structural information.
- What evidence would resolve it: Comparing uniform sampling against structure-aware sampling (e.g., preserving high-centrality nodes) on computational time and embedding quality metrics.

## Limitations
- Greedy feature selection may not find globally optimal feature subsets for non-additive or highly correlated features.
- The claim about 5-20% runtime reduction from 50-60% sampling is based on limited evidence without systematic ablation studies.
- Robustness to sampling assumes uniform random sampling is sufficient, which may not hold for graphs with critical rare nodes.

## Confidence
- **High Confidence**: The core algorithmic framework (Wasserstein-based graph embeddings with interpretable node features) is technically sound and well-established in the literature.
- **Medium Confidence**: The greedy feature selection method's optimality guarantees are limited; it may not find the globally optimal feature subset for non-additive or highly correlated features.
- **Low Confidence**: The claim about 5-20% runtime reduction from 50-60% sampling is based on a single illustrative figure without systematic ablation studies across datasets.

## Next Checks
1. **Ablation Study on Sampling Rates**: Systematically vary node sampling rates (0%, 25%, 50%, 75%, 100%) across all five benchmark datasets and plot both Embedding Similarity Score and classifier accuracy to validate the claimed robustness to sampling.
2. **Greedy vs. Exhaustive Feature Selection**: On a small dataset (e.g., MUTAG), compare the greedy feature selection results against an exhaustive search to quantify the approximation error and identify conditions where greedy selection fails.
3. **Feature Importance Cross-Validation**: Run the Fast (Random Forest) and Greedy feature selection methods on the same dataset (e.g., NCI1) and compare the top-ranked features to assess consistency and identify discrepancies due to non-additive feature contributions.