---
ver: rpa2
title: 'Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural
  Study on Phishing Detection'
arxiv_id: '2601.10524'
source_url: https://arxiv.org/abs/2601.10524
tags:
- data
- arxiv
- training
- performance
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-layered diagnostic framework to investigate
  generalization failures in fine-tuned LLMs. Fine-tuning large language models on
  specialized tasks often leads to brittle performance when faced with out-of-domain
  data, a critical issue in high-stakes applications like phishing detection.
---

# Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection

## Quick Facts
- **arXiv ID:** 2601.10524
- **Source URL:** https://arxiv.org/abs/2601.10524
- **Reference count:** 38
- **Primary result:** Introduces a multi-layered diagnostic framework combining SHAP and mechanistic interpretability to uncover root causes of LLM generalization failures in phishing detection

## Executive Summary
This study investigates why fine-tuned LLMs fail when encountering out-of-domain data in phishing detection tasks. Through a cross-architectural analysis of Llama 3.1 8B, Gemma 2 9B, and Mistral models fine-tuned via QLoRA on three stylistically diverse phishing datasets, the authors identify that generalization failures stem from a complex interplay between model architecture and data diversity rather than training data volume alone. The research reveals that Gemma 2 9B achieves superior generalization (>91% F1) by successfully integrating diverse features, while Llama 3.1 8B exhibits negative transfer when exposed to mixed data. The work also uncovers high rates of label noise in public datasets and demonstrates how models develop brittle heuristics by over-relying on stylistic artifacts rather than semantic content. By combining SHAP analysis with mechanistic interpretability, the study provides a concrete methodology for diagnosing and understanding these failures.

## Method Summary
The study fine-tunes three LLMs (Llama 3.1 8B, Gemma 2 9B, Mistral 12B) using QLoRA with rank 16 on three phishing datasets: Enron (formal corporate), SpamAssassin (diverse), and Modern Phishing (adversarial). Models are trained as "Specialists" on single datasets or "Generalists" on combined data. The training uses 4-bit quantization, constant learning rate of 1e-4, 3 epochs, batch size 2 with gradient accumulation of 8, targeting query and value projections. Performance is evaluated using Weighted F1 Score, with diagnostic analysis conducted via SHAP feature importance and attention heatmaps to identify failure mechanisms.

## Key Results
- Gemma 2 9B achieves >91% F1 on diverse data through successful feature integration
- Llama 3.1 8B exhibits negative transfer, degrading from 0.7828 to 0.6648 F1 when trained on combined datasets
- Public phishing datasets contain 17-22% label noise that confounds model training
- Models develop brittle heuristics by over-relying on stylistic artifacts (e.g., formal tone) rather than semantic threat content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generalization capability is contingent on a synergy between specific model architectures and stylistic data diversity, rather than data volume alone.
- **Mechanism:** Different architectures handle feature interference differently. Gemma 2 9B successfully integrates features from diverse datasets, while Llama 3.1 8B exhibits "negative transfer" by failing to reconcile features from different domains.
- **Core assumption:** The observed performance differences are intrinsic to the model architectures' inductive biases and not artifacts of specific QLoRA hyperparameters.
- **Evidence anchors:** Gemma improving significantly with diverse data (0.5974 → 0.9132 F1) while Llama degrades (0.7828 → 0.6648 F1).
- **Break condition:** If fine-tuning hyperparameters (e.g., LoRA rank) were increased significantly for Llama, the negative transfer might resolve.

### Mechanism 2
- **Claim:** Models develop "brittle heuristics" by overfitting to stylistic artifacts in training data, suppressing semantic understanding of security threats.
- **Mechanism:** The model learns shortcuts like "Formal/Professional style = Legitimate," causing it to override semantic spam signals when encountering professional phishing.
- **Core assumption:** SHAP values accurately reflect the model's decision process rather than correlating spurious features.
- **Evidence anchors:** SHAP analysis reveals the model equates professional, formal style with "legitimacy."
- **Break condition:** If training data is audited to remove style-label correlations, this heuristic should diminish.

### Mechanism 3
- **Claim:** Fine-tuning specialized attention heads creates distinct "detector circuits" for specific phishing cues.
- **Mechanism:** QLoRA modifies query/value projections, causing specific attention heads (e.g., Layer 2 Head 15) to specialize in recognizing structural threats like IP addresses in URLs.
- **Core assumption:** Identified attention patterns are causal mechanisms for detection and not just correlated activations.
- **Evidence anchors:** Layer 2, Head 15 treats IP address sequences as cohesive units linked to call-to-action tokens.
- **Break condition:** If attention head ablation does not reduce detection accuracy for IP-based phishing, the mechanism is likely distributive.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (QLoRA)**
  - **Why needed here:** The study relies on QLoRA to adapt massive models (8B-12B params) on limited hardware. Understanding that LoRA injects low-rank matrices into attention projections is critical to grasping where the mechanistic changes occur.
  - **Quick check question:** Does freezing the base model weights and only training adapters (LoRA matrices) limit the model's ability to unlearn pre-trained biases?

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the primary diagnostic tool used to validate why the model fails (e.g., showing that "professional" tokens push predictions toward "legitimate").
  - **Quick check question:** If a token has a high negative SHAP value, does it contribute to a "SPAM" or "LEGIT" classification in this paper's context?

- **Concept: Domain Shift / Distributional Shift**
  - **Why needed here:** The core problem is that training data (Enron) differs statistically from deployment data (Modern Phishing), causing the "generalization failure" the paper diagnoses.
  - **Quick check question:** Why would a model trained on formal corporate emails (Enron) likely fail on informal, modern phishing texts?

## Architecture Onboarding

- **Component map:**
  - Backbones: Llama 3.1 8B (Poor generalist) -> Gemma 2 9B (Robust generalist) -> Mistral 12B (Consistent)
  - Adapters: QLoRA (Rank 16) injected into Query/Value projections
  - Data: Enron (Formal) -> SpamAssassin (Diverse) -> Modern Phishing (Adversarial)
  - Diagnostics: SHAP (Feature importance) -> Attention Heatmaps (Mechanistic circuits)

- **Critical path:**
  1. Data Audit: Check for label noise (paper finds 17-22% potential noise) and "naturally occurring adversarial text"
  2. Specialist Training: Fine-tune on a narrow domain (Enron) to establish baseline
  3. Generalist Training: Fine-tune on combined datasets to test architectural synergy
  4. Failure Diagnosis: Use SHAP on False Negatives to identify over-reliance on style

- **Design tradeoffs:**
  - Specialist vs. Generalist: Training on diverse data improves Gemma but degrades Llama. You must validate architecture compatibility before mixing datasets.
  - CoT vs. Standard: CoT reasoning helps Mistral but hinders Gemma. Assumption: CoT constraints may limit the "synergy" capacity of high-performing architectures.

- **Failure signatures:**
  - High In-Domain / Low Out-of-Domain F1: Indicates overfitting to style (Llama Generalist)
  - "Professional Style" Bias: SHAP visualizations showing strong "LEGIT" weights on formatting/header tokens
  - Label Noise Confounding: Model correctly predicts "SPAM" but is penalized because the ground truth dataset label is incorrect

- **First 3 experiments:**
  1. Reproduce the Llama Failure: Train Llama 3.1 8B on the "Generalist" combined dataset and verify if F1 drops below the "Specialist" baseline (0.66 vs 0.78) to confirm architectural sensitivity.
  2. Data Ablation: Remove the "naturally occurring adversarial" examples (flagged by the auditor model) from the Enron set and measure if the Generalist model's robustness improves.
  3. Head Ablation: Locate the "Suspicious Link Detector" (Layer 2, Head 15) in the fine-tuned Gemma model and mask it to confirm the causal drop in URL-phishing detection accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Core claims rest on a limited sample size of three model families within the 8-12B parameter range
- QLoRA hyperparameters are held constant across architectures despite potential architectural differences in optimal fine-tuning strategies
- The study focuses exclusively on English text classification without addressing multilingual generalization

## Confidence

**High Confidence (4/5):**
- Gemma 2 9B achieves superior generalization performance (>91% F1) when trained on diverse data compared to single-domain training
- All three architectures exhibit domain shift failures when deployed on out-of-domain data
- Public phishing detection datasets contain significant label noise (17-22% estimated)

**Medium Confidence (3/5):**
- The architectural synergy between Gemma 2 9B and data diversity is intrinsic rather than hyperparameter-dependent
- Llama 3.1 8B exhibits negative transfer when exposed to diverse data
- Mistral demonstrates more consistent performance across domains

**Low Confidence (2/5):**
- The identified attention heads represent definitive "detector circuits" rather than correlated activations
- SHAP attributions accurately reflect causal decision mechanisms rather than spurious correlations
- The brittle heuristics identified through SHAP analysis are the primary failure modes rather than symptoms of deeper issues

## Next Checks

1. **Ablation Study for Mechanistic Claims:** Perform head-level ablation by zeroing out Layer 2 Head 15 in the Gemma model and measure the specific degradation in IP address detection accuracy. Compare this with random head ablation to establish whether the identified attention patterns are causal mechanisms or correlated activations.

2. **Architectural Hyperparameter Sensitivity:** Replicate the study with varied QLoRA configurations (ranks 8, 16, 32) for each architecture, particularly testing whether increasing Llama's LoRA rank capacity eliminates the observed negative transfer phenomenon.

3. **Label Noise Impact Quantification:** Systematically remove the estimated 17-22% noisy examples from the training datasets and retrain the "Generalist" models. Measure changes in both in-domain and out-of-domain performance to quantify how label noise confounds architectural comparisons.