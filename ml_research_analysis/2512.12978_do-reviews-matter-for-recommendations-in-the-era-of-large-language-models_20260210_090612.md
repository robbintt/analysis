---
ver: rpa2
title: Do Reviews Matter for Recommendations in the Era of Large Language Models?
arxiv_id: '2512.12978'
source_url: https://arxiv.org/abs/2512.12978
tags:
- reviews
- user
- review
- performance
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the contribution of textual
  reviews to recommendation performance in the era of large language models (LLMs).
  The authors compare deep learning methods with LLM-based approaches across eight
  public datasets, assessing performance under zero-shot, few-shot, and fine-tuning
  scenarios.
---

# Do Reviews Matter for Recommendations in the Era of Large Language Models?

## Quick Facts
- arXiv ID: 2512.12978
- Source URL: https://arxiv.org/abs/2512.12978
- Authors: Chee Heng Tan; Huiying Zheng; Jing Wang; Zhuoyi Lin; Shaodi Feng; Huijing Zhan; Xiaoli Li; J. Senthilnath
- Reference count: 40
- Key outcome: LLMs generally outperform traditional deep learning approaches, especially in sparse and cold-start scenarios, but removing or distorting textual reviews does not consistently degrade recommendation accuracy

## Executive Summary
This paper systematically evaluates the contribution of textual reviews to recommendation performance in the era of large language models (LLMs). The authors compare deep learning methods with LLM-based approaches across eight public datasets, assessing performance under zero-shot, few-shot, and fine-tuning scenarios. They introduce RAREval, a comprehensive evaluation framework that examines review removal, distortion, data sparsity, and cold-start user conditions. Their proposed REVLoRA fine-tuning method efficiently leverages review texts with LLMs. Results show that LLMs generally outperform traditional deep learning approaches, especially in sparse and cold-start scenarios. Notably, removing or distorting textual reviews does not consistently degrade recommendation accuracy, challenging assumptions about review utility.

## Method Summary
The study employs Amazon 5-core datasets (8 categories) with empty reviews removed and 80:10:10 train/val/test splits. Three LLM settings are evaluated: zero-shot (10 reviews per user/item, max 7680 tokens), few-shot (3-shot with 2 reviews per example), and REVLoRA (extraction-summarization to sentence-long profiles Su/Si, LoRA fine-tuning with regression head). Baselines include DeepCoNN, RGCL, and DIRECT with specified hyperparameters. LLMs tested include Llama 3.2 (1B/3B) and Qwen 2.5 (0.5B/3B). The RAREval framework assesses performance under review removal, distortion, sparsity, and cold-start conditions using MAE/MSE metrics.

## Key Results
- LLMs consistently outperform traditional deep learning methods across all datasets and scenarios
- REVLoRA achieves comparable accuracy to concatenation methods with 13× less computational cost
- Review removal does not consistently degrade performance; DeepCoNN even improves on Musical Instruments when reviews are removed
- LLMs show larger performance margins over baselines in cold-start scenarios (f≤5 interactions)
- Review distortion negatively impacts performance but the effect is dataset-dependent

## Why This Works (Mechanism)

### Mechanism 1: REVLoRA Compression-Fine-Tuning Pipeline
- Pre-compressing reviews into structured summaries before fine-tuning yields computational efficiency without accuracy loss
- The pipeline extracts rating-relevant features from raw reviews via LLM prompting, summarizes them into sentence-long user preferences (Su) and item descriptions (Si), then LoRA fine-tunes on compressed prompts
- Core assumption: extraction-summarization preserves sufficient predictive signal without harming rating prediction accuracy
- Evidence: REVLoRA achieves comparable MAE (0.5338) to extraction-only (0.5334) with 13× less runtime penalty on Musical Instruments

### Mechanism 2: Pre-trained World Knowledge for Cold-Start Generalization
- LLMs leverage pre-trained linguistic and behavioral patterns to generalize from sparse user history
- Pre-trained priors about product categories, user behavior patterns, and sentiment-rating relationships compensate for missing collaborative signals in cold-start scenarios
- Core assumption: Pre-training corpora contain sufficient domain-relevant behavioral and preference patterns transferable to recommendation tasks
- Evidence: REVLoRA consistently outperforms baselines at low interaction frequencies, with larger margins when f≤5

### Mechanism 3: Review-Rating Signal Asymmetry
- Explicit rating signals often dominate review-derived features; adding reviews can introduce noise
- Reviews contain contradictory signals (e.g., complaints with 5-star ratings), semantic ambiguity, and irrelevant content
- Core assumption: Rating distributions encode sufficient preference signal; review text adds marginal or redundant information in most cases
- Evidence: Zero-shot LLM generally performs worse with reviews, and DeepCoNN MAE improves by 0.51 on Musical Instruments when reviews are removed

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed: REVLoRA depends on LoRA to make fine-tuning tractable; understanding low-rank adapter injection is essential for reproducing results
  - Quick check: Can you explain why LoRA reduces trainable parameters while preserving model capacity?

- **Cold-Start Problem in Recommender Systems**
  - Why needed: RQ5 explicitly targets cold-start scenarios; understanding why sparse interaction histories challenge traditional methods clarifies why LLMs' pre-trained priors matter
  - Quick check: What is the fundamental signal limitation for a user with only 1-2 historical interactions?

- **Prompt Engineering for Zero-Shot/Few-Shot LLMs**
  - Why needed: Zero-shot and few-shot performance depends heavily on prompt structure; poor prompting can obscure LLM capabilities
  - Quick check: Why might including fewer target user reviews in few-shot prompts offset the benefit of demonstration examples?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Profile Generation (REVLoRA-only) -> Prompt Construction -> Model Backends -> Fine-Tuning Layer -> Evaluation
- **Critical path**: Preprocess dataset → generate Su/Si profiles (REVLoRA) OR format raw reviews (zero/few-shot) → construct prompts per setting → run inference or fine-tune → evaluate via RAREval framework
- **Design tradeoffs**: Prompt length vs. information density (concatenation preserves detail but is computationally infeasible; summarization enables efficiency but risks signal loss); Zero-shot vs. few-shot (few-shot provides calibration but reduces space for target user/item reviews); MAE vs. MSE loss (REVLoRA with MAE achieves best MAE; MSE-tuned variant underperforms)
- **Failure signatures**: Zero-shot performance degrades with reviews present (likely distracted by contradictory signals); REVLoRA underperforms on small dense datasets (may lack data to exploit pre-training priors); DeepCoNN improves without reviews (review noise exceeds signal under limited capacity)
- **First 3 experiments**: 1) Baseline replication: Run DeepCoNN, RGCL, DIRECT on one 5-core Amazon dataset with and without reviews; 2) REVLoRA ablation: Compare full REVLoRA vs. extraction-only vs. concatenation on same dataset; 3) Cold-start stratification: Evaluate zero-shot, few-shot, and REVLoRA on users grouped by f=1, 3, 5, 10 interactions

## Open Questions the Paper Calls Out
None

## Limitations
- Review removal does not consistently harm performance lacks clear mechanistic explanation - unclear whether this reflects genuine redundancy or limitations in current review extraction methods
- REVLoRA's summarization stage introduces potential information loss that is neither quantified nor empirically validated across diverse review domains
- Evaluation framework focuses narrowly on rating prediction accuracy, omitting downstream business metrics like diversity, serendipity, or user engagement
- Cold-start advantages are demonstrated but the specific pre-training priors responsible remain unidentified, making generalization to niche domains uncertain

## Confidence
- **High Confidence**: Comparative LLM vs. deep learning performance (RQ1-2) - demonstrated across multiple datasets and model sizes with consistent patterns
- **Medium Confidence**: REVLoRA compression benefits (RQ3) - runtime improvements are clear, but accuracy preservation requires further validation on datasets with richer review content
- **Medium Confidence**: Review removal effects (RQ4) - patterns are consistent but mechanistic explanations are incomplete
- **Medium Confidence**: Cold-start advantages (RQ5) - statistically significant but domain-transferability unproven
- **Low Confidence**: Review distortion impact (RQ4) - negative effects are shown but the underlying cognitive mechanism remains unclear

## Next Checks
1. **Mechanistic validation**: Conduct ablation studies isolating the contribution of individual review aspects (sentiment, aspect mentions, comparative language) to determine which review features, if any, provide unique predictive signal beyond ratings

2. **Cross-domain generalization**: Apply REVLoRA to non-Amazon domains (e.g., academic paper recommendations, music streaming) to test whether pre-trained priors transfer effectively to domains with different review writing conventions and preference structures

3. **Temporal stability analysis**: Evaluate whether review utility patterns (removal effects, cold-start advantages) remain stable across different time periods within the same dataset to assess whether observed patterns reflect persistent user behavior or dataset-specific artifacts