---
ver: rpa2
title: Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of
  L1-Dependent Biases
arxiv_id: '2502.14507'
source_url: https://arxiv.org/abs/2502.14507
tags:
- speaker
- language
- density
- agreement
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether Large Language Models (LLMs) can simulate
  non-native-like English dialogue patterns observed in human L2 learners. Using an
  information-theoretic framework and a comprehensive linguistic annotation scheme,
  we prompt seven LLMs to generate dialogues as L2 speakers with specific native languages
  (Korean, Mandarin, Japanese, Cantonese, Thai, Malay, Urdu) and compare outputs to
  real L2 data from the ICNALE dataset.
---

# Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases

## Quick Facts
- arXiv ID: 2502.14507
- Source URL: https://arxiv.org/abs/2502.14507
- Reference count: 40
- Key outcome: L1 knowledge injection prompting significantly improves LLM ability to simulate L2-English dialogue patterns across seven target languages.

## Executive Summary
This study investigates whether Large Language Models (LLMs) can simulate non-native-like English dialogue patterns observed in human L2 learners. Using an information-theoretic framework and comprehensive linguistic annotation, the researchers prompted seven LLMs to generate dialogues as L2 speakers with specific native languages (Korean, Mandarin, Japanese, Cantonese, Thai, Malay, Urdu) and compared outputs to real L2 data from the ICNALE dataset. Results show that prompting with L1 knowledge significantly improves LLMs' ability to replicate L1-dependent linguistic patterns, particularly in tense agreement, number agreement, subject-verb agreement, speech acts, and reference word usage. The findings indicate that LLMs can effectively mimic human L2 dialogue patterns when provided with appropriate L1 knowledge injection, with performance varying by both language pair and model size.

## Method Summary
The study used the ICNALE dataset containing 4,250 dialogues from 425 participants across seven L1 backgrounds. GPT-4o automated annotation was applied to both human and LLM-generated dialogues across eight linguistic features (tense agreement, number agreement, subject-verb agreement, modals, quantifiers, noun-verb collocations, reference words, speech acts). L1 knowledge injection prompts were constructed with meta-linguistic trait descriptions and bilingual dialogue examples for each target language. LLMs generated 20-turn dialogues at temperature=0, which were then evaluated using an information-theoretic distance metric comparing mutual information distributions between LLM outputs and human reference patterns.

## Key Results
- L1 knowledge injection prompting significantly improves LLM simulation of L2-specific linguistic patterns, with GPT-4o showing improved generation across all languages except quantifiers and numerals
- Information-theoretic mutual information distance effectively quantifies L2 simulation fidelity, with dbi distances consistently lower than dmono baselines across most features
- Model scale correlates with L2 simulation capability, with larger models (GPT-4o, DeepSeekV3) performing better than smaller models (LLaMA3-8B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: L1 knowledge injection prompting improves LLM simulation of L2-specific linguistic patterns
- Mechanism: Providing explicit meta-linguistic information about a target L1 conditions the LLM's prior distribution p(Y|X') to better approximate human L2 speaker distributions p(Y|D,X). The LLM leverages cross-linguistic knowledge already encoded in its parameters, steering generation toward L1-biased outputs without fine-tuning
- Core assumption: LLMs encode sufficient cross-linguistic knowledge during pre-training; prompting can surface this knowledge as controlled generation behavior
- Evidence anchors:
  - [abstract]: "prompting with L1 knowledge significantly improves LLMs' ability to replicate L1-dependent linguistic patterns"
  - [section 6.1, Table 3]: dbi distances are consistently lower than dmono baselines across almost all features and languages
- Break condition: If target L1 has very limited representation in pre-training data, prompting alone may fail

### Mechanism 2
- Claim: Information-theoretic mutual information distance quantifies L2 simulation fidelity
- Mechanism: The metric measures divergence between human L2 speaker distributions (conditioned on actual L1 prior X) and LLM-generated distributions (conditioned on prompted L1 prior X'). Lower distance indicates the LLM has captured L1-dependent biases observed in human data
- Core assumption: The annotated linguistic constructs sufficiently represent L1-L2 transfer phenomena; distributional similarity correlates with human-likeness
- Evidence anchors:
  - [section 3.2]: Full derivation of the distance metric from logarithmic loss and mutual information
  - [section 6.1, Table 3]: Quantitative results show dbi < dmono for most feature-language pairs
- Break condition: If annotation quality is low or constructs fail to capture critical transfer phenomena, distance scores may be uninformative

### Mechanism 3
- Claim: Model scale correlates with L2 simulation capability
- Mechanism: Larger models have greater cross-linguistic coverage and parameter capacity to represent fine-grained L1-specific patterns. Smaller models lack sufficient capacity, yielding weaker transfer even with identical prompts
- Core assumption: Scale-dependent multilingual competence exists; prompting effectiveness is mediated by pre-existing cross-lingual representations
- Evidence anchors:
  - [section 6.1]: "LLAMA3-8B performs the worst, although this is perhaps unsurprising since it's the smallest model"
  - [section 6.1, Table 5-8]: DeepSeekV3 and GPT-4o show consistently lower distances than LLaMA-8B across languages
- Break condition: If prompt engineering or L1 knowledge quality is poor, even large models may fail

## Foundational Learning

- Concept: **Second Language Acquisition (L2) Transfer Theory**
  - Why needed here: The paper's entire framework rests on the premise that L1 linguistic properties bias L2 production (e.g., SOV languages like Japanese affecting subject-verb agreement in English)
  - Quick check question: Can you explain why a Korean L1 speaker might systematically omit or misorder English articles compared to a Mandarin L1 speaker?

- Concept: **Information-Theoretic Distance Metrics (Mutual Information, KL-style divergences)**
  - Why needed here: The evaluation framework uses I(X;Y|D) to quantify how much information L1 shares with L2 linguistic features
  - Quick check question: Given two probability distributions over linguistic feature counts, how would you compute their mutual information distance conditioned on dialogue context D?

- Concept: **Prompt Engineering for Persona/Style Conditioning**
  - Why needed here: L1 knowledge injection relies on carefully constructed prompts with trait analysis, few-shot examples, and explicit linguistic instructions
  - Quick check question: What prompt components would you include to make an LLM generate Thai-L1-influenced English with appropriate politeness marker omissions?

## Architecture Onboarding

- Component map: ICNALE dataset -> GPT-4o annotation -> L1 knowledge injection prompts -> LLM generation -> Distance metric computation -> Density visualization
- Critical path:
  1. Select target L1 and extract human L2 reference data from ICNALE
  2. Construct L1 knowledge injection prompt (trait analysis + examples)
  3. Generate L2 dialogues with LLM (20 turns, temperature=0)
  4. Annotate both human and LLM outputs for 8 features
  5. Compute d_bi and d_mono distances; visualize density distributions
  6. Analyze feature-level gaps and L1-specific patterns
- Design tradeoffs:
  - Prompt length vs. specificity: Longer prompts improve accuracy but increase inference cost
  - Automated vs. human annotation: GPT-4o annotation is scalable (84.1% accuracy) but introduces model-dependent bias
  - Temperature=0 vs. higher values: Zero temperature ensures reproducibility but may underrepresent natural L2 variation
  - Single-turn evaluation vs. dialogue coherence: Current framework evaluates feature distributions but not discourse coherence
- Failure signatures:
  - Quantifiers/Numerals consistently fail across all models (LLMs over-omit these features)
  - Cantonese Noun-Verb Collocation shows dbi > dmono for GPT-4o, suggesting L1 injection hurts performance
  - Small models (LLaMA-8B) show inconsistent or reversed effects
  - Over-correction: Some LLM outputs appear too fluent, missing characteristic L2 errors
  - Template rigidity: Interview-based prompts may not generalize to spontaneous chit-chat
- First 3 experiments:
  1. Ablation study: Generate L2 dialogues with full L1 knowledge prompt, trait analysis only, examples only, and simple L1 mention only
  2. Language-pair expansion: Test framework on non-Asian L1 (e.g., Spanish, Arabic) to assess generalizability
  3. Human evaluation comparison: Conduct blinded human judgment study rating L2 authenticity

## Open Questions the Paper Calls Out

- **Question:** To what extent do LLM-generated L2 dialogues produce grammatically incorrect forms typical of human learners (error generation) versus simply avoiding complex constructs (avoidance)?
- **Question:** Can the findings regarding L1-dependent simulation be generalized to non-Asian native languages or typologically distinct language families?
- **Question:** How robust is the L1 knowledge injection prompting strategy in unscripted, open-domain dialogue scenarios compared to the template-based interviews used in this study?
- **Question:** How do socio-cultural biases influence the simulation performance beyond the strictly linguistic features analyzed in the current framework?

## Limitations
- Results may not generalize beyond interview-style dialogues to spontaneous conversational contexts
- The mechanism by which prompting improves L1-dependent pattern generation is inferred but not directly validated through controlled studies
- The information-theoretic metric assumes annotated features fully capture L1-L2 transfer phenomena

## Confidence
- High confidence: LLMs can simulate L2 patterns when provided with L1 knowledge (consistent dbi < dmono results)
- Medium confidence: Model scale directly correlates with L2 simulation capability (performance gaps but no external validation)
- Medium confidence: The information-theoretic distance metric validly measures L2 simulation fidelity (theoretically sound but no external validation corpus exists)

## Next Checks
1. Conduct ablation studies to isolate which prompt components drive L1 knowledge injection improvements
2. Test framework generalizability by applying it to non-Asian L1 backgrounds (e.g., Spanish, Arabic)
3. Perform blinded human evaluation studies comparing LLM-generated dialogues against human L2 speakers for perceived authenticity and L1-specific patterns