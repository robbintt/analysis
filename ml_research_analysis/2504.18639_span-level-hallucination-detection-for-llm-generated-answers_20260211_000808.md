---
ver: rpa2
title: Span-Level Hallucination Detection for LLM-Generated Answers
arxiv_id: '2504.18639'
source_url: https://arxiv.org/abs/2504.18639
tags:
- hallucination
- arabic
- detection
- spans
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses span-level hallucination detection in LLM-generated
  answers, focusing on English and Arabic texts. The proposed framework integrates
  Semantic Role Labeling (SRL) to decompose answers into atomic roles, retrieves relevant
  context via question-based LLM prompting, and evaluates semantic alignment using
  a DeBERTa-based textual entailment model.
---

# Span-Level Hallucination Detection for LLM-Generated Answers

## Quick Facts
- **arXiv ID**: 2504.18639
- **Source URL**: https://arxiv.org/abs/2504.18639
- **Reference count**: 4
- **Primary result**: Proposed framework achieves IoU of 0.358 (English) and 0.28 (Arabic) on Mu-SHROOM dataset for span-level hallucination detection.

## Executive Summary
This paper introduces a span-level hallucination detection framework for LLM-generated answers, focusing on English and Arabic texts. The approach integrates Semantic Role Labeling (SRL) to decompose answers into atomic semantic units, retrieves relevant context via question-based LLM prompting, and evaluates semantic alignment using a DeBERTa-based textual entailment model. Token-level confidence scores from output logits are combined with entailment scores to detect hallucinated spans. Experiments on the Mu-SHROOM dataset demonstrate competitive performance, with particularly notable results in Arabic using CamelParser for dependency parsing.

## Method Summary
The framework processes LLM-generated answers through a pipeline that begins with SRL decomposition into atomic semantic roles. For each role, context is retrieved using GPT-4 prompted with the original question. A DeBERTa entailment model evaluates whether each semantic unit is entailed, contradicted, or neutral with respect to the retrieved context. Token-level confidence scores derived from LLM output logits are combined with entailment scores using a weighted sum (α parameter). Units with combined scores below threshold 0.5 are flagged as hallucinated. For Arabic, CamelParser is used for dependency parsing followed by SRL extraction, showing superior performance compared to HanLP.

## Key Results
- English performance: IoU of 0.358 and Cor of 0.322 on Mu-SHROOM dataset
- Arabic performance: IoU of 0.28 and Cor of 0.21 using CamelParser
- CamelParser outperforms HanLP for Arabic dependency parsing (IoU 0.28 vs 0.205)
- Fact-checking of hallucinated spans using GPT-4 and LLaMA validates framework effectiveness

## Why This Works (Mechanism)

### Mechanism 1: SRL-Based Semantic Decomposition
Decomposing LLM-generated answers into atomic semantic units improves hallucination localization compared to sentence-level or token-level approaches. Semantic Role Labeling (SRL) extracts structured components—predicates, arguments (ARG0, ARG1), and modifiers (ARGM-TMP, ARGM-LOC)—which represent distinct factual claims. Each unit is then independently evaluated against retrieved context, enabling fine-grained detection rather than coarse sentence-level binary classification.

### Mechanism 2: Textual Entailment for Factual Alignment
Comparing each semantic unit against independently retrieved context via a DeBERTa entailment model identifies contradictions and neutral claims that correlate with hallucinations. After GPT-4 retrieves context for the question (not the answer), each SRL-extracted unit serves as a hypothesis evaluated against this context. The entailment model outputs probability distributions over [entailment, neutral, contradiction], where high contradiction or neutral scores indicate potential hallucination.

### Mechanism 3: Token-Level Confidence Refinement
Combining entailment scores with token-level confidence derived from LLM output logits improves detection accuracy over either signal alone. Logits from the original LLM output are converted to softmax probabilities per token, then averaged per semantic unit. Lower logit scores indicate reduced model certainty. The refined score = α × entailment + (1 − α) × confidence, where α controls weighting. Units below threshold 0.5 are flagged as hallucinated.

## Foundational Learning

- **Concept: Semantic Role Labeling (SRL)**
  - Why needed here: Core decomposition mechanism; you must understand how predicates and arguments are extracted to debug detection failures.
  - Quick check question: Given "Marie discovered radium in 1898," can you identify ARG0, the predicate, and ARGM-TMP?

- **Concept: Natural Language Inference / Textual Entailment**
  - Why needed here: Determines whether atomic claims follow from, contradict, or are neutral to retrieved context.
  - Quick check question: If context states "Paris is in France" and hypothesis is "Paris hosted the 2024 Olympics," what entailment label applies and why?

- **Concept: Softmax Probability from Logits**
  - Why needed here: Converts raw model logits into interpretable confidence scores; understanding normalization is essential for debugging score integration.
  - Quick check question: Given logits [2.0, 1.0, 0.1], compute the softmax probability for the first class.

## Architecture Onboarding

- **Component map**: Context Retrieval → SRL Decomposition → Entailment Scoring → Confidence Scoring → Score Integration → Span Classification
- **Critical path**: Context retrieval → SRL decomposition quality → Entailment model accuracy → Score integration weighting. Failures in SRL extraction (especially Arabic) cascade to all downstream components.
- **Design tradeoffs**:
  - GPT-4 retrieval vs. search-based retrieval: simpler but introduces dependency on another LLM's correctness.
  - CamelParser vs. HanLP for Arabic: CamelParser yields higher IoU (0.28 vs. 0.205) but requires more preprocessing.
  - Threshold selection: 0.5 is heuristic; tuning may improve precision/recall trade-offs.
- **Failure signatures**:
  - Low IoU on morphologically rich languages → likely SRL or dependency parsing failure.
  - High neutral entailment scores → retrieved context may be insufficient.
  - Mismatch between logit confidence and actual correctness → calibration issue with source LLM.
- **First 3 experiments**:
  1. **Ablate confidence scoring**: Set α = 1.0 to isolate entailment-only performance and quantify contribution of logit-based refinement.
  2. **Threshold sweep**: Test refined score thresholds from 0.3 to 0.7 to optimize IoU/Cor on validation set.
  3. **Arabic parser comparison**: Run parallel experiments with HanLP vs. CamelParser on identical Arabic samples to replicate reported IoU differences and analyze failure cases.

## Open Questions the Paper Calls Out

### Open Question 1
How can entailment models be enhanced to better handle syntactic structures such as Arabic nominal sentences and long complex sentences? The conclusion states: "Future work will focus on enhancing entailment models and addressing additional syntactic structures, such as nominal sentences in Arabic and long complex sentences." The current DeBERTa-based entailment model was not specifically designed for Arabic nominal sentence structures or long complex sentences, contributing to lower Arabic performance (IoU 0.28 vs. 0.358 for English).

### Open Question 2
Does the optimal α weighting for score integration differ between languages, particularly between English and morphologically rich languages like Arabic? The paper introduces α as a hyperparameter (Equation 2) but does not report language-specific tuning or ablation results for different α values. Arabic syntactic complexity may require different balance between entailment and confidence signals, yet a single α value was used.

### Open Question 3
Would replacing GPT-4-based context retrieval with traditional information retrieval or dense retrieval methods maintain or improve hallucination detection performance? The paper relies entirely on GPT-4 prompting for context retrieval without comparison to alternative retrieval approaches. The paper does not ablate or compare retrieval methods, leaving open whether simpler or open-source retrievers could achieve comparable results.

## Limitations
- Framework performance depends heavily on SRL decomposition quality, which may degrade on complex syntactic structures, particularly in Arabic.
- Reliance on GPT-4 for context retrieval introduces additional source of potential error and dependency on another black-box model.
- Reported metrics indicate moderate precision in span localization (IoU 0.358 for English, 0.28 for Arabic), suggesting room for improvement.

## Confidence
**High Confidence**: The SRL-based decomposition mechanism and its integration with entailment scoring is well-grounded in the methodology. The reported performance differences between CamelParser and HanLP for Arabic dependency parsing are statistically significant and reproducible.

**Medium Confidence**: The effectiveness of combining entailment scores with token-level confidence is supported by experimental results but lacks ablation studies showing the relative contribution of each component.

**Low Confidence**: The generalizability of results to languages beyond English and Arabic, to different question-answering datasets, and to more complex or longer-form documents remains uncertain.

## Next Checks
1. **Ablation Study of Confidence Integration**: Conduct experiments with α = 1.0 (entailment-only) and α = 0 (confidence-only) to quantify the contribution of each component to overall detection performance, and identify optimal weighting for different domains.

2. **Cross-Domain Generalization Test**: Evaluate the framework on at least two additional question-answering datasets from different domains (e.g., biomedical, technical support) to assess performance stability and identify domain-specific failure patterns.

3. **Parser Robustness Analysis**: Systematically compare CamelParser and HanLP on a diverse set of Arabic sentences with varying morphological complexity, and analyze error patterns to identify specific syntactic structures where SRL decomposition fails.