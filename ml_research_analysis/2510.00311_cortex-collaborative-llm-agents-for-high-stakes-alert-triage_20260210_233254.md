---
ver: rpa2
title: 'CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage'
arxiv_id: '2510.00311'
source_url: https://arxiv.org/abs/2510.00311
tags:
- user
- reasoning
- actionable
- agent
- workflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORTEX introduces a multi-agent LLM architecture for SOC alert
  triage, with specialized agents handling behavior analysis, evidence gathering,
  and decision synthesis. Unlike single-agent approaches, it grounds decisions in
  real tool outputs and structured evidence, improving auditability.
---

# CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage

## Quick Facts
- arXiv ID: 2510.00311
- Source URL: https://arxiv.org/abs/2510.00311
- Reference count: 40
- Primary result: Achieves actionable F1 of 0.78 and reduces false-positive rates from 24.9% to 14.2% compared to single-agent baselines

## Executive Summary
CORTEX introduces a multi-agent LLM architecture for SOC alert triage, with specialized agents handling behavior analysis, evidence gathering, and decision synthesis. Unlike single-agent approaches, it grounds decisions in real tool outputs and structured evidence, improving auditability. Evaluated on a fine-grained SOC dataset, CORTEX achieves an actionable F1 of 0.78 and reduces false-positive rates from 24.9% to 14.2% compared to single-agent baselines, while maintaining reasonable latency. The system also produces structured, auditable reports with extracted observables and follow-up recommendations.

## Method Summary
CORTEX uses a multi-agent architecture with OpenAI Agents SDK and Model Context Protocol (MCP) to decompose SOC alert triage into specialized roles: Behavior Analysis (routing), Evidence Acquisition (tool execution), and Reasoning & Coordination (synthesis). The system processes JSON alerts through seven predefined workflows, each with typed tools that return structured data. A conservative "escalate-on-any" policy ensures actionable alerts are not missed, while structured categorization reduces false positives. The approach is evaluated on a fine-grained SOC dataset with metrics including actionable F1, non-actionable F1, subclass F1, and false-positive rate.

## Key Results
- Achieves actionable F1 of 0.78 and non-actionable F1 of 0.86
- Reduces false-positive rates from 24.9% to 14.2% compared to single-agent baselines
- Maintains reasonable latency while using 5.6x more tokens (23,600 vs. 4,152) than baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing triage into specialized roles reduces error propagation compared to single-agent "end-to-end" approaches.
- **Mechanism:** Instead of one model handling log interpretation, context retrieval, and adjudication simultaneously, CORTEX separates these into distinct Behavior Analysis (routing), Evidence Acquisition (tool execution), and Reasoning & Coordination (synthesis) agents. This isolates failures; a routing error does not necessarily corrupt evidence gathering.
- **Core assumption:** The complexity of SOC triage exceeds the reliable capacity of a single LLM context window and reasoning step.
- **Evidence anchors:** [abstract] "...specialized agents handle behavior analysis, evidence gathering, and decision synthesis... improves investigation quality over state-of-the-art single-agent LLMs." [section 3.1] "CORTEX decomposes triage into four stages... specialized agents assume distinct roles." [corpus] Neighbor paper "Information-Dense Reasoning" confirms that verbose reasoning chains in security often incur "prohibitive latency," supporting the need for architectural decomposition.
- **Break condition:** If the routing agent (Behavior Analysis) consistently misclassifies alerts, the specialized evidence workflows will never trigger, breaking the pipeline.

### Mechanism 2
- **Claim:** Forcing agents to query typed tools rather than relying on parametric knowledge improves factual grounding and auditability.
- **Mechanism:** Agents do not guess; they must call typed tools (e.g., `getUserRecord`, `searchBehaviorEvents`) via the Model Context Protocol (MCP). The Reasoning Agent then synthesizes findings strictly from these returned JSON objects, explicitly linking claims to evidence IDs.
- **Core assumption:** External systems (SIEM, identity stores) return structured, deterministic data that is more reliable than the LLM's internal weights.
- **Evidence anchors:** [abstract] "...grounds decisions in real tool outputs and structured evidence, improving auditability." [section 3.1] "Typed tools ground reasoning... ensuring decisions remain auditable and reproducible." [corpus] "Agentic Observability" highlights similar success in using agents for API verification and log inspection in e-commerce triage.
- **Break condition:** If the tool schemas change or return unstructured errors, the LLM may hallucinate tool outputs ("tool hallucination"), leading to false conclusions.

### Mechanism 3
- **Claim:** A conservative "escalate-on-any" synthesis policy reduces false negatives while structured categorization manages false positives.
- **Mechanism:** The Reasoning & Coordination Agent aggregates outputs. If any workflow flags an alert as actionable, the final verdict is actionable. However, for non-actionable alerts, it enforces a specific category (Benign, FP-Logic, etc.), which appears to regularize the decision boundary and lower the False Positive Rate (FPR).
- **Core assumption:** In high-stakes environments, the cost of a missed threat (False Negative) outweighs the cost of investigating a false alarm (False Positive), but structure helps minimize the latter.
- **Evidence anchors:** [abstract] "...reduces false-positive rates from 24.9% to 14.2%..." [section 3.1] "CORTEX adopts a conservative decision policy: if any workflow escalates, the overall verdict is actionable." [corpus] "Decision-Aware Trust Signal Alignment" discusses calibrating confidence in SOCs, supporting the need for explicit decision policies found in CORTEX.
- **Break condition:** If multiple workflows are overly sensitive, the "escalate-on-any" policy could flood analysts with actionable alerts, negating the FPR gains.

## Foundational Learning

- **Concept: ReAct Pattern (Reason + Act)**
  - **Why needed here:** CORTEX agents and baselines rely on interleaving reasoning traces with tool calls. Understanding this loop is critical to debugging why an agent called `getUserRecord` vs. `runStructuredQuery`.
  - **Quick check question:** Can you trace a single step in the logs where an agent decided not to call a tool and explain why?

- **Concept: False Positive Rate (FPR) vs. Precision**
  - **Why needed here:** The primary success metric cited is a reduction in FPR (24.9% â†’ 14.2%). You must distinguish this from "Accuracy," as class imbalance in SOC alerts (mostly benign) makes raw accuracy misleading.
  - **Quick check question:** If the system marks 100 benign alerts as "actionable," is that a False Positive or a False Negative?

- **Concept: Model Context Protocol (MCP)**
  - **Why needed here:** The implementation uses OpenAI's Agents SDK with MCP for tool exposure. Understanding how tools are exposed as JSON-schema resources is necessary for adding new integrations.
  - **Quick check question:** How does the MCP server handle authentication when the `getUserRecord` tool is invoked?

## Architecture Onboarding

- **Component map:** Input (Alert JSON) -> Orchestrator (manages pipeline) -> Behavior Analysis Agent (routing) -> Evidence Acquisition Agents (tool execution) -> Reasoning & Coordination Agent (synthesis) -> Output (Structured Report)

- **Critical path:** The Evidence Acquisition phase. This is where external latency is introduced (tool calls) and where token counts spike (serializing tool outputs).

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** CORTEX is 3.4x slower than the ReAct baseline (152s vs. 44s) but gains +0.12 F1.
  - **Cost:** Uses 5.6x more tokens (23,600 vs. 4,152) due to multi-agent message passing and rich tool context serialization.

- **Failure signatures:**
  - **Infinite Loops:** Agents repeatedly querying the same tool without progressing (mitigated by turn caps in the Orchestrator).
  - **Context Overflow:** Very long tool outputs (e.g., massive raw logs) exceeding the LLM context window.
  - **Routing Drift:** New alert types that don't match the 7 predefined workflows falling into "Generic" and receiving poor triage.

- **First 3 experiments:**
  1. **Latency Profiling:** Measure the time contribution of the Orchestrator vs. Tool Calls vs. LLM Inference to identify the bottleneck.
  2. **Ablation on Synthesis Policy:** Switch the Reasoning Agent from "escalate-on-any" to "escalate-on-majority" to measure the impact on FPR and Recall.
  3. **Workflow Coverage:** Run the Behavior Analysis Agent on a hold-out set of alerts to verify distribution across the 7 workflows (ensure it isn't over-using the "Generic" fallback).

## Open Questions the Paper Calls Out

- **Distillation of multi-agent traces into a compact single-model policy for cost/latency reduction** - While CORTEX improves F1 scores, it incurs 3.4x higher latency and 5.6x more tokens than single-agent baselines, creating a deployment trade-off.

- **Sensitivity to adversarial prompt injections** - The paper lists "sensitivity to... prompt injection" as a limitation and calls for "expanded benchmarks for red-team robustness."

- **Generalization to novel attack scenarios or enterprise environments** - The authors acknowledge the limitation of "dataset coverage" and note that "agentic systems... can be sensitive to distribution shift."

## Limitations
- Dataset access and representativeness: No public dataset URL provided, preventing independent validation of alert distribution and tool output schemas
- LLM model and tool implementation details: "gpt-5-mini" appears to be a placeholder, with actual model/API unspecified and exact SIEM integrations not detailed
- Conservative policy tradeoff: The "escalate-on-any" policy reduces FPR but increases latency and token usage, with unclear impact on analyst workload

## Confidence
- **High confidence:** The architectural decomposition mechanism (Mechanism 1) is well-supported by the paper's design and comparison to single-agent baselines
- **Medium confidence:** The tool-grounding mechanism (Mechanism 2) is logically sound but real-world tool schema changes or "tool hallucination" risks are not empirically validated
- **Low confidence:** The scalability of the 7-workflow approach to new alert types is untested, and the lack of dataset access prevents validation of routing accuracy and coverage

## Next Checks
1. **Dataset verification:** Obtain or reconstruct the SOC dataset to validate the distribution of alerts across the 7 workflows and test routing accuracy on hold-out samples
2. **Tool hallucination audit:** Implement logging to cross-check every cited observable against actual tool call outputs, identifying any ungrounded claims in the reasoning traces
3. **Synthesis policy ablation:** Conduct a controlled experiment switching the Reasoning Agent from "escalate-on-any" to "escalate-on-majority" to measure the impact on FPR, Recall, and analyst workload in a simulated production setting