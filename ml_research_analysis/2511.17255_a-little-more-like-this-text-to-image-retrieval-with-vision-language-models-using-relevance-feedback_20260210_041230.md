---
ver: rpa2
title: 'A Little More Like This: Text-to-Image Retrieval with Vision-Language Models
  Using Relevance Feedback'
arxiv_id: '2511.17255'
source_url: https://arxiv.org/abs/2511.17255
tags:
- feedback
- relevance
- retrieval
- image
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for enhancing text-to-image
  retrieval by refining query representations at inference time using relevance feedback.
  It proposes four strategies: pseudo-relevance feedback (PRF), generative relevance
  feedback (GRF) using synthetic captions, an attentive feedback summarizer (AFS)
  that integrates multimodal fine-grained features, and simulated explicit feedback
  using ground-truth captions.'
---

# A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback

## Quick Facts
- **arXiv ID**: 2511.17255
- **Source URL**: https://arxiv.org/abs/2511.17255
- **Reference count**: 40
- **Key outcome**: Text-to-image retrieval improved by 3-5% in MRR@5 for smaller models and 1-3% for larger ones using relevance feedback at inference time.

## Executive Summary
This paper introduces a framework for enhancing text-to-image retrieval by refining query representations at inference time using relevance feedback. It proposes four strategies: pseudo-relevance feedback (PRF), generative relevance feedback (GRF) using synthetic captions, an attentive feedback summarizer (AFS) that integrates multimodal fine-grained features, and simulated explicit feedback using ground-truth captions. Experiments on Flickr30k and COCO with four vision-language models show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller models and 1-3% for larger ones compared to no feedback. AFS particularly mitigates query drift and is more robust in multi-turn retrieval settings. The approach consistently enhances retrieval across different VLMs without requiring model fine-tuning.

## Method Summary
The method employs four feedback strategies to refine query representations for text-to-image retrieval at inference time. First, pseudo-relevance feedback (PRF) uses the top-5 retrieved images' captions as pseudo-relevant examples to update the query via masked language modeling. Second, generative relevance feedback (GRF) generates synthetic captions using GPT-4o for the top-5 retrieved images, avoiding the need for ground-truth captions. Third, the attentive feedback summarizer (AFS) leverages CLIP image embeddings to identify fine-grained, salient image regions and generates multimodal feedback by combining image and caption features. Fourth, simulated explicit feedback uses ground-truth captions as perfect relevance signals. The feedback-enhanced query is then re-encoded and used for retrieval. Experiments compare these strategies against a baseline with no feedback across multiple VLMs, focusing on MRR@5 and R@5 metrics.

## Key Results
- GRF, AFS, and explicit feedback improve retrieval by 3-5% MRR@5 for smaller VLMs and 1-3% for larger VLMs versus no feedback.
- AFS mitigates query drift and is robust in multi-turn retrieval settings.
- The approach enhances retrieval consistently across different VLMs without fine-tuning.

## Why This Works (Mechanism)
The framework improves retrieval by iteratively refining query representations with feedback at inference time, using both synthetic and multimodal sources to better align user intent with image content. By leveraging relevance feedback—either through pseudo-relevant examples, synthetic captions, or fine-grained multimodal features—the model adapts its understanding of the query, capturing nuances that a single query encoding might miss. AFS, in particular, uses fine-grained image features to focus on salient regions, making it more robust to query drift and better at handling complex, multi-turn interactions. This approach addresses the limitation of fixed query encodings in vision-language models, enabling more accurate retrieval without the need for model retraining.

## Foundational Learning
- **Pseudo-Relevance Feedback (PRF)**: Using top-retrieved items as proxy relevance signals to refine queries. *Why needed*: Provides a lightweight way to adapt queries without explicit user feedback. *Quick check*: Verify PRF improves retrieval when top results are semantically relevant.
- **Multimodal Feature Integration**: Combining text and image features (e.g., via CLIP) to capture fine-grained visual cues. *Why needed*: Enhances query refinement by grounding updates in actual image content. *Quick check*: Ensure image features are meaningfully correlated with caption semantics.
- **Attention-Based Summarization**: Using attention mechanisms to select salient features from multimodal inputs. *Why needed*: Focuses query updates on the most informative image regions and text. *Quick check*: Confirm attention weights highlight relevant image-text pairs.
- **Synthetic Caption Generation**: Using generative models (e.g., GPT-4o) to produce synthetic relevance signals. *Why needed*: Bypasses the need for human-labeled feedback or ground-truth captions. *Quick check*: Evaluate synthetic captions for relevance and coherence.
- **Query Drift**: The degradation of retrieval relevance over multiple feedback rounds. *Why needed*: Identifying and mitigating drift is crucial for multi-turn retrieval. *Quick check*: Monitor retrieval metrics across query rounds for stability.

## Architecture Onboarding
- **Component Map**: User Query -> Retrieval Model -> Top-5 Images -> (PRF/GRF/AFS/Explicit Feedback) -> Feedback-Enhanced Query -> Re-Retrieval
- **Critical Path**: Query encoding → Retrieval → Feedback generation/summarization → Query refinement → Re-retrieval
- **Design Tradeoffs**: GRF trades synthetic caption quality for scalability (no ground-truth needed), while AFS trades higher computational cost for more robust, fine-grained feedback but may introduce noise from complex features.
- **Failure Signatures**: PRF/GRF may fail if initial retrieval is poor (garbage in, garbage out); AFS may overfit to irrelevant image regions or introduce noise if fine-grained features are noisy; explicit feedback is limited by ground-truth caption availability.
- **3 First Experiments**:
  1. Run PRF and GRF on a small subset of queries and compare retrieval gains with/without feedback.
  2. Test AFS with and without fine-grained image features to assess their contribution.
  3. Measure computational overhead (inference time, memory) for each feedback strategy on a single VLM.

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements are measured on clean benchmark datasets; real-world user queries may be noisier and degrade relative gains.
- Assumes caption availability for synthetic feedback and simulated explicit feedback, but ground-truth captions are rarely available in practice.
- Multi-turn retrieval relies on synthetic queries and simulated relevance feedback, which may not reflect real user interaction dynamics.
- Focus on MRR@5 and R@5 may not reflect broader user satisfaction in real-world tasks.
- Computational overhead of generating synthetic captions or fine-grained multimodal features is not quantified.

## Confidence
- **Improvement over no feedback baseline (3-5% MRR@5 for small VLMs, 1-3% for large VLMs)**: Medium confidence. Validated on benchmarks but not on real-world, noisy data or extended to more datasets/models.
- **AFS mitigates query drift and is robust in multi-turn retrieval**: Medium confidence. Supported by synthetic experiments, but real user behavior may differ significantly.
- **Consistent enhancement across different VLMs without fine-tuning**: Medium confidence. Evidence is strong within the tested scope but may not generalize to all VLMs or domains.

## Next Checks
1. Conduct experiments on real-world user query logs with noisy, unstructured text to test robustness outside of clean benchmark captions.
2. Quantify and compare computational costs (inference time, memory) of each feedback strategy, especially AFS and synthetic caption generation, across different hardware setups.
3. Test the proposed methods on additional vision-language models and diverse datasets (e.g., LAION, OpenImages, or domain-specific image collections) to assess scalability and generalization.