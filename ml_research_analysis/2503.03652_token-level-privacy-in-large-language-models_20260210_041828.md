---
ver: rpa2
title: Token-Level Privacy in Large Language Models
arxiv_id: '2503.03652'
source_url: https://arxiv.org/abs/2503.03652
tags:
- privacy
- stencil
- token
- mechanism
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving privacy when using
  large language models (LLMs) as remote services, where users must transmit sensitive
  data to external providers. Existing privacy-preserving NLP methods primarily focus
  on semantic similarity and neglect contextual information, which is crucial for
  maintaining utility.
---

# Token-Level Privacy in Large Language Models

## Quick Facts
- arXiv ID: 2503.03652
- Source URL: https://arxiv.org/abs/2503.03652
- Authors: Re'em Harel; Niv Gilboa; Yuval Pinter
- Reference count: 20
- Key outcome: dχ-STENCIL achieves 2ϵ-dχ-privacy with comparable or better utility-privacy tradeoffs than CUSTEXT+ and NOISE baselines across SST2, QNLI, SWAG, and MMLU datasets

## Executive Summary
This paper addresses token-level privacy preservation for LLMs when used as remote services, where users must transmit sensitive data to external providers. The authors introduce dχ-STENCIL, a novel privacy-preserving mechanism that combines contextual and semantic information while ensuring strong privacy guarantees under the dχ differential privacy framework. The method works by encoding neighboring token information, adding calibrated Laplacian noise, and selecting the closest token to the perturbed vector to preserve semantic integrity. Experimental results demonstrate that dχ-STENCIL achieves comparable or better utility-privacy tradeoffs compared to existing methods while maintaining low reconstruction rates.

## Method Summary
The dχ-STENCIL method operates on individual tokens by: (1) retrieving embeddings of L neighboring tokens from GloVe, (2) computing Gaussian-weighted sum with standard deviation σ to incorporate contextual information, (3) adding Laplacian noise sampled from Gamma(ℓ, 1/η) × uniform hypersphere, and (4) selecting the nearest vocabulary token via cosine similarity. The method achieves 2ϵ-dχ-privacy guarantees and operates at the token level to preserve privacy before transmission to untrusted LLM services. Skipwords are excluded from perturbation while OOV tokens remain unchanged.

## Key Results
- dχ-STENCIL achieves comparable or better utility-privacy tradeoffs compared to CUSTEXT+ and NOISE baselines
- Low reconstruction rates (Pr@5) are maintained across all tested datasets and models
- Contextual information integration enhances privacy preservation while maintaining utility
- Strong 2ϵ-dχ-privacy guarantees are achieved for both d₂ and dC distance functions

## Why This Works (Mechanism)

### Mechanism 1: Contextual Embedding Integration
- Claim: Encoding neighboring token information into each token's perturbation maintains downstream utility while providing additional privacy through contextual mixing
- Mechanism: For each token ti, the algorithm retrieves embeddings of L neighboring tokens, applies Gaussian-weighted summation (f(j,σ) = e^(-j²/2σ²)), creating a quasi-vector ϕ̃i that encodes local context before noise addition
- Core assumption: Contextual relationships are essential for LLM task performance, and mixing neighbor information distributes sensitive content across multiple perturbed tokens
- Evidence anchors: [abstract] "integrates contextual and semantic information while ensuring strong privacy guarantees under the dχ differential privacy framework"; [Section 3.1] "To derive the new token t̃i, information from its L neighboring tokens... is incorporated"
- Break condition: When σ is too high (≥1.0) or L too large, contextual mixing dilutes semantic signal, degrading utility without proportional privacy gain

### Mechanism 2: Calibrated Laplacian Noise Injection
- Claim: Adding noise sampled from a multivariate Laplacian distribution provides formal 2ϵ-dχ-privacy guarantees for the token perturbation mechanism
- Mechanism: Sample noise p from Lap_ℓ,ϵ(v) = ce^(-ϵ||v||) (implemented via Gamma × uniform product), add to context-weighted embedding, then map to nearest vocabulary token
- Core assumption: The dχ-privacy framework's requirement—output probability ratios bounded by e^(η·dist(x,x'))—is satisfied when noise follows this distribution
- Evidence anchors: [Section 3.4, Theorem 1] "the dχ-STENCIL mechanism with parameters L, N, E and Laplace noise with parameter ϵ is 2ϵ-dχ-private for both d₂ and dC distance functions"
- Break condition: When η is set too low (aggressive noise), semantic coherence collapses; when too high, reconstruction attacks succeed

### Mechanism 3: Nearest-Neighbor Un-embedding
- Claim: Mapping perturbed continuous vectors back to discrete vocabulary tokens preserves semantic integrity by exploiting embedding space geometry
- Mechanism: After context aggregation and noise addition, select t̃i = argmin_{tj∈V} dist(E[tj], ϕ̃i), where dist uses cosine similarity
- Core assumption: Semantic similarity in the embedding space correlates with task-relevant meaning, so nearby tokens are functionally interchangeable
- Evidence anchors: [Section 3.3] "locating the closest token... to the perturbed vector, thereby preserving semantic integrity"; [Section 4.2] Reconstruction attack (Pr@5) tests whether original token appears in top-5 nearest neighbors of perturbed token
- Break condition: When perturbation magnitude exceeds local embedding neighborhood radius, selected tokens become semantically unrelated

## Foundational Learning

- Concept: **Local Differential Privacy (LDP)**
  - Why needed here: The paper assumes an untrusted remote server scenario where privacy must be enforced client-side before transmission
  - Quick check question: Why is LDP preferred over central DP when users cannot trust the LLM service provider?

- Concept: **dχ-privacy (metric-local DP)**
  - Why needed here: Standard (ε,δ)-DP treats all inputs equally; dχ-privacy relaxes this to allow output distributions that scale with input distance, preserving semantic similarity
  - Quick check question: How does dχ-privacy differ from pure ε-DP in handling semantically similar inputs?

- Concept: **Embedding space geometry and distance metrics**
  - Why needed here: The mechanism relies on cosine/Euclidean distances in embedding space for both noise calibration and token selection
  - Quick check question: Why might cosine distance be preferred for semantic similarity in normalized word embeddings?

## Architecture Onboarding

- Component map: Input token -> GloVe embedding lookup -> Gaussian context aggregator -> Laplacian noise generator -> Nearest-neighbor search -> Privatized token output
- Critical path: Input token → Embedding lookup → Context window aggregation → Laplacian noise addition → Nearest-neighbor search → Privatized token output
- Design tradeoffs:
  - **Odd vs even L**: Odd L centers weight on original token (higher utility, higher reconstruction risk); even L splits weight (lower utility ceiling, better privacy)
  - **σ (Gaussian spread)**: Low σ (0.5) preserves original token; high σ (1.0) over-mixes context
  - **η (privacy parameter)**: Higher η = less noise = better accuracy but higher Pr@5 reconstruction rate
  - **Stopword handling**: Excluding stopwords improves utility but may leak structural information
- Failure signatures:
  - Pr@5 > 0.7 at target η: Noise insufficient; check η calibration
  - Accuracy >15% below clean baseline: Context mixing too aggressive; reduce L or increase η
  - High OOV rate (>10%): Embedding vocabulary mismatch; consider domain-specific embeddings
- First 3 experiments:
  1. Reproduce SST2 accuracy vs Pr@5 frontier using FLAN-T5 with dχ-STENCIL (L=4,5; σ=0.75; η=80-240) against NOISE and CUSTEXT+ baselines
  2. Ablate window parity: Compare L=4 vs L=5 (even vs odd) at matched reconstruction rates to validate utility ceiling difference
  3. Implement nearest-neighbor reconstruction attack: Given perturbed tokens, rank vocabulary by cosine similarity and report Pr@5 to verify privacy claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the privacy guarantee for the full sequence be theoretically tightened from $2\epsilon$ to $\epsilon$-d$\chi$-privacy by formally integrating the bounded contribution of internal tokens into the main proof?
- Basis in paper: [explicit] Remark 1 (p. 6) states the current $2\epsilon$ bound is "pessimistic for large N" and suggests a tighter bound exists for internal substrings
- Why unresolved: The current proof utilizes a loose upper bound ($C_j \leq 2$) for all tokens to simplify the mathematical derivation, ignoring that internal tokens contribute exactly 1
- What evidence would resolve it: A formal proof extending the tighter substring guarantee to the full sequence or a hybrid bound dependent on sequence position

### Open Question 2
- Question: How robust is d$\chi$-STENCIL against sophisticated practical attacks, such as attribute inference or linking attacks, compared to the simple nearest-neighbor reconstruction evaluated?
- Basis in paper: [explicit] Page 3 notes that for existing methods, "evaluation under practical attack scenarios are yet to be investigated"
- Why unresolved: The paper evaluates robustness primarily against a "Pr@5" nearest-neighbor inversion attack, which the authors classify as a "relatively simple adversary"
- What evidence would resolve it: Empirical results measuring the success rate of advanced adversarial attacks (e.g., training auxiliary classifiers to predict private attributes) on sanitized text

### Open Question 3
- Question: Does utilizing the target LLM's own contextualized embeddings instead of static GloVe embeddings improve the utility-privacy trade-off?
- Basis in paper: [inferred] The paper emphasizes the importance of context (Section 1) but restricts the methodology to static GloVe embeddings (Section 4) for the nearest-neighbor calculations
- Why unresolved: Static embeddings may not capture the specific semantic nuances utilized by the target generative models (FLAN-T5, QWEN), potentially creating a misalignment between the privatized input and the model's internal representation
- What evidence would resolve it: An ablation study comparing downstream task performance and reconstruction rates when using dynamic, context-aware embeddings for the noise mechanism

## Limitations

- The paper assumes token-level privacy is sufficient, but does not address whether token sequences or higher-order patterns could still leak sensitive information
- Results are validated primarily on benchmark datasets using two specific model architectures, limiting generalizability to domain-specific applications
- The method requires careful tuning of multiple interacting parameters (L, σ, η) with limited systematic optimization guidance

## Confidence

**High Confidence Claims**:
- The dχ-STENCIL mechanism can achieve the claimed 2ϵ-dχ-privacy guarantees when implemented with the specified parameters and distance metrics
- The method produces measurable privacy-utility tradeoffs that can be controlled via the η parameter
- Token reconstruction attacks (Pr@5) are a valid and effective method for evaluating token-level privacy

**Medium Confidence Claims**:
- Contextual information integration consistently improves privacy-utility tradeoffs compared to semantic-only methods
- The specific parameter ranges (L=4,5; σ=0.75; η=80-240) represent optimal tradeoffs for the tested models and datasets
- The method generalizes to different LLM architectures and task types

**Low Confidence Claims**:
- The privacy guarantees extend to all realistic adversarial scenarios, including adaptive attacks and model inversion
- The GloVe-based context aggregation is optimal for all embedding spaces and model architectures
- The method provides sufficient privacy for all sensitive data types and application contexts

## Next Checks

1. **Cross-model embedding compatibility test**: Implement dχ-STENCIL using the same GloVe embeddings but test with additional LLM architectures (e.g., LLaMA, Mistral) to verify whether contextual mixing remains effective across different embedding spaces. Measure both utility retention and reconstruction rates to identify potential embedding-space dependencies.

2. **Adversarial attack robustness evaluation**: Design and implement adaptive reconstruction attacks that exploit semantic and syntactic dependencies across multiple perturbed tokens. Test whether the dχ-privacy guarantees hold under these stronger attack models, and identify parameter regimes where privacy breaks down.

3. **Domain transfer validation**: Apply dχ-STENCIL to domain-specific datasets (medical, financial, legal text) with models fine-tuned on these domains. Compare privacy-utility tradeoffs to benchmark results to assess generalization and identify domain-specific parameter optimizations.