---
ver: rpa2
title: On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large
  Vision-Language Models
arxiv_id: '2510.09008'
source_url: https://arxiv.org/abs/2510.09008
tags:
- image
- visual
- object
- hallucination
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses object hallucination in large vision-language
  models (LVLMs), where models generate descriptions of objects not present in the
  input image. The core method identifies uncertain visual tokens in the vision encoder
  using adversarial perturbations, which are shown to correlate with hallucination
  occurrence.
---

# On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.09008
- **Source URL:** https://arxiv.org/abs/2510.09008
- **Reference count:** 40
- **Primary result:** Training-free method that reduces hallucinations in LVLMs by masking uncertain visual tokens, achieving significant drops in hallucination metrics (e.g., Cs↓38.4%, Ci↓23.8% on LLaVA-1.5-7B) while preserving caption quality.

## Executive Summary
This paper addresses object hallucination in large vision-language models (LVLMs), where models generate descriptions of objects not present in the input image. The core method identifies uncertain visual tokens in the vision encoder using adversarial perturbations, which are shown to correlate with hallucination occurrence. These uncertain tokens are then masked in intermediate self-attention layers of the vision encoder during inference. Extensive experiments demonstrate that this approach significantly reduces hallucination across multiple benchmarks (CHAIR, POPE, AMBER) and various LVLM architectures (e.g., LLaVA-1.5-7B: Cs↓38.4%, Ci↓23.8%) while preserving caption quality. The method is training-free and compatible with existing hallucination mitigation techniques.

## Method Summary
The method identifies uncertain visual tokens in the vision encoder using adversarial perturbations, then masks these tokens in intermediate self-attention layers during inference. Specifically, it extracts hidden states from early encoder layers (1-10) to compute uncertainty via adversarial perturbations, aggregates and thresholds these to create a binary mask, and applies this mask to self-attention outputs at middle layers (13-17 for LLaVA). This suppresses uncertain visual tokens without deleting them from the sequence, reducing hallucinations while preserving semantic quality.

## Key Results
- Significant hallucination reduction: LLaVA-1.5-7B Cs drops from ~47 to ~29, Ci from ~15 to ~9
- Strong correlation between uncertainty and hallucination: Spearman ρ=0.794 (CHAIRs), ρ=0.733 (CHAIRi), p<0.05
- Computational efficiency: Adversarial uncertainty maps at ~5× lower runtime than MC-dropout
- Cross-benchmark success: Effective on CHAIR, POPE, and AMBER benchmarks
- Architecture compatibility: Works with multiple LVLM architectures and complements existing hallucination mitigation methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual tokens with high epistemic uncertainty in the vision encoder positively correlate with object hallucination occurrence.
- **Mechanism:** The vision encoder produces token representations whose instability under small perturbations reflects unreliable visual grounding; these propagate to the LLM and surface as fabricated objects.
- **Core assumption:** Hallucinations originate significantly from vision-side uncertainty, not solely from language-model priors or decoding.
- **Evidence anchors:**
  - [abstract] "statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations"
  - [section 3.2.1] Spearman correlation ρ=0.794 (CHAIRs), ρ=0.733 (CHAIRi), p-value<0.05 across 1,000 MS-COCO samples
  - [corpus] Neighbors confirm visual-encoder contributions to hallucination (e.g., SHIELD, EAZY), but do not replicate the adversarial-uncertainty linkage directly
- **Break condition:** If vision tokens are already robust (low epistemic uncertainty), masking yields minimal gains; if hallucinations arise primarily from language priors, vision-side intervention is insufficient.

### Mechanism 2
- **Claim:** Adversarial perturbations efficiently approximate epistemic uncertainty by measuring hidden-state deviation.
- **Mechanism:** PGD perturbations maximize representation deviation in early vision-encoder layers; larger deviation implies higher differential-entropy upper bound (Theorem 3.2), serving as a proxy for token uncertainty.
- **Core assumption:** Small perturbations keep deviations locally Gaussian (Lemma 3.1); early layers remain in a linearizable regime.
- **Evidence anchors:**
  - [section 3.1.1] Theorem 3.2: "upper bound of the differential entropy of Z_i^{(t)} increases as E_ε[‖Z_i^{(t)}−z_i^{(t)}‖²] increases"
  - [figure 2 & appendix E.1] Uncertainty maps from adversarial attack closely match MC-dropout maps at ~5× lower runtime (12.4s → 2.43s)
  - [corpus] Corpus does not provide independent validation of adversarial-based uncertainty as proxy; related work focuses on decoding-level interventions
- **Break condition:** If perturbation strength is too large or applied too late in the encoder, deviation amplification no longer tracks uncertainty reliably (Figure 3 shows deviations increase non-linearly in later layers).

### Mechanism 3
- **Claim:** Token-wise attention masking at intermediate encoder layers reduces hallucinations while preserving semantic quality.
- **Mechanism:** A binary mask (from aggregated early-layer deviations) multiplies attention outputs in middle layers (13–17), suppressing influence of uncertain tokens without deleting them from the sequence.
- **Core assumption:** Intermediate layers balance suppressing uncertainty vs. retaining global semantics; earlier/later masking is less effective.
- **Evidence anchors:**
  - [section 3.2.2] Equation 4 defines Attention(Q,K,V)⊙M; applied within residual structure
  - [table 4] Ablation shows layers 13–17 yield best Cs=29.2, Ci=9.3 on LLaVA-1.5-7B vs. worse at 1–8 or 18–22
  - [corpus] Corpus neighbors propose complementary LLM-side methods (OPERA, VCD, PAI); no contradiction to encoder-side masking
- **Break condition:** If masking threshold σ_th is too aggressive, useful visual cues are lost (F1 may drop); if too permissive, uncertain tokens persist and hallucinations remain.

## Foundational Learning

- **Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The paper targets epistemic uncertainty (model-induced) of visual tokens, not input noise; approximated via representation deviation under adversarial perturbations.
  - Quick check question: Would adding Gaussian image noise directly estimate the same uncertainty this paper targets? (Answer: No—this method uses adversarial perturbations to probe encoder sensitivity, not input-level noise.)

- **Vision-Encoder Layer Roles**
  - Why needed here: The method extracts uncertainty from early layers (1–10) but applies masking at middle layers (13–17), reflecting different functional regimes.
  - Quick check question: Why not extract uncertainty from the final encoder layer? (Answer: Later layers show larger, non-linear deviations that violate the small-perturbation assumption; early layers provide more stable estimates.)

- **Self-Attention Masking vs. Token Removal**
  - Why needed here: The approach attenuates uncertain tokens within attention outputs rather than deleting them, preserving sequence structure.
  - Quick check question: What happens if you zero out uncertain tokens at the encoder input instead? (Answer: Table 5 shows input masking performs worse (Cs=47.4) than attention-level masking (Cs=29.2), likely due to lost context.)

## Architecture Onboarding

- **Component map:** Vision Encoder (ViT-L/14 or EVA-CLIP) -> PGD Adversarial Attack -> Uncertainty Aggregation -> Attention Masking (layers 13-17) -> LLM (Vicuna-7B/13B)
- **Critical path:**
  1. Run PGD attack on vision encoder to get perturbed features
  2. Compute layer-wise deviation maps from early layers
  3. Aggregate and threshold to produce binary mask M
  4. Inject M into self-attention outputs at specified intermediate layers during forward pass
  5. Proceed with standard LLM decoding
- **Design tradeoffs:**
  - Early vs. late uncertainty extraction: Early layers are theoretically grounded but may miss high-level semantic uncertainty
  - Masking strength (σ_th): Higher values mask more tokens (lower hallucination but potential F1 drop); tuned per baseline
  - Architectural compatibility: Q-Former models (MiniGPT-4) show smaller gains (Cs 28.6→27.4) since visual tokens are compressed before LLM
- **Failure signatures:**
  - Low mask consistency across seeds: Check perturbation strength k or PGD iterations
  - Minimal hallucination reduction on Q-Former architectures: Consider joint attack on queries (Table A9 shows improved Cs 27.0 with image+query perturbation)
  - F1 degradation: Reduce σ_th or narrow masking layer range
- **First 3 experiments:**
  1. Replicate CHAIR evaluation on LLaVA-1.5-7B with greedy decoding; confirm Cs drops from ~47 to ~29 with masking layers 13–17 and σ_th=1.1
  2. Ablate uncertainty extraction layers (1–10 vs. 11–20 vs. 21–22) to verify early-layer advantage (Table 3)
  3. Combine with one LLM-side method (e.g., VCD) to test additivity; expect incremental gains beyond either method alone (Table 1)

## Open Questions the Paper Calls Out
None

## Limitations
- The adversarial-uncertainty proxy is not independently validated against gold-standard epistemic uncertainty estimates (e.g., ensemble methods)
- The method's gains are smaller on Q-Former architectures (LLaVA vs. MiniGPT-4), suggesting architectural constraints
- The fixed masking layer range (13-17 for LLaVA) may not generalize optimally to all encoder depths or vision backbones

## Confidence
- **High confidence:** The positive correlation between early-layer token uncertainty and hallucination occurrence (ρ≈0.75, p<0.05), and the general effectiveness of masking uncertain tokens at intermediate layers to reduce hallucination metrics (Cs, Ci)
- **Medium confidence:** The adversarial perturbation approach as a computationally efficient proxy for epistemic uncertainty, since direct validation against MC-dropout or ensemble baselines is absent
- **Medium confidence:** The optimality of the chosen masking layer range (13-17), as ablation shows it outperforms extremes but may not be globally optimal

## Next Checks
1. Validate uncertainty proxy: Compare the adversarial-deviation uncertainty maps against gold-standard epistemic uncertainty estimates (e.g., MC-dropout or ensemble methods) on the same model and dataset to confirm they capture the same quantity
2. Cross-architecture robustness: Test the method on a wider range of LVLM architectures (e.g., BLIP-2, OFA) and vision backbones (e.g., Swin, ConvNeXt) to assess generalization beyond LLaVA and EVA-CLIP
3. Downstream task impact: Evaluate the effect of uncertainty masking on non-hallucination benchmarks (e.g., visual question answering accuracy, image captioning CIDEr) to ensure semantic quality is preserved beyond F1 metrics