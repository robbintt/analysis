---
ver: rpa2
title: A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance
  Judgment
arxiv_id: '2504.12408'
source_url: https://arxiv.org/abs/2504.12408
tags:
- relevance
- prompts
- llms
- prompt
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how prompt variations affect LLM-based relevance
  judgments in information retrieval. The authors collected 72 prompts from human
  experts and LLMs across three assessment approaches (binary, graded, pairwise) and
  tested them with three different LLMs on TREC Deep Learning datasets.
---

# A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment

## Quick Facts
- arXiv ID: 2504.12408
- Source URL: https://arxiv.org/abs/2504.12408
- Reference count: 40
- This study examines how prompt variations affect LLM-based relevance judgments in information retrieval, finding that graded relevance judgments are highly sensitive to prompt variations while binary and pairwise methods are more stable.

## Executive Summary
This study systematically evaluates how different prompts affect LLM-based relevance judgments across three assessment approaches: binary, graded, and pairwise. The authors collected 72 prompts from human experts and LLMs, testing them with three different LLMs (GPT-4o, LLaMA 3.2-3B, Mistral-7B) on TREC Deep Learning datasets. The results demonstrate that graded relevance judgments show high sensitivity to prompt variations, while binary and pairwise methods exhibit greater stability. LLM-generated prompts achieved higher average agreement with human annotations and showed less variance than human-crafted prompts, with no single prompt consistently performing well across all models.

## Method Summary
The study evaluated prompt sensitivity using TREC Deep Learning 2020 and 2021 datasets (54 and 53 queries respectively) with ground truth from NIST assessors. Three judge LLMs were used: GPT-4o via OpenAI API, and LLaMA 3.2-3B and Mistral-7B via Ollama, all with temperature=0. For GPT-4o, up to 10 documents per relevance level were sampled per query; for pairwise judgments, 10 pairs per comparison type were evaluated with order swapping to control positional bias. Binary relevance mapped levels 2-3 to relevant and 0-1 to non-relevant. The 72 prompts (12 human + 12 LLM per task type) were obtained from a GitHub repository and formatted for compatibility with the model APIs.

## Key Results
- Graded relevance judgments showed high sensitivity to prompt variations, while binary and pairwise methods were more stable across different prompts
- LLM-generated prompts achieved higher average agreement with human annotations and exhibited less variance than human-crafted prompts
- No single prompt consistently performed well across all models, though UMBRELA—a prompt that breaks relevance into specific aspects like trustworthiness—showed robust performance across different LLMs for graded assessments
- GPT-4o demonstrated consistently high performance with low variance, while LLaMA 3.2 and Mistral showed significant variability depending on the prompt and task

## Why This Works (Mechanism)
The study demonstrates that LLM-based relevance judgment is highly sensitive to how the relevance concept is operationalized in prompts. Different prompt formulations can lead to significantly different judgments even when assessing the same documents, particularly for graded relevance scales. This sensitivity appears to stem from how LLMs interpret and operationalize the relevance criteria specified in the prompt, with more specific, aspect-based prompts (like UMBRELA) showing greater robustness across different model architectures.

## Foundational Learning
- **Relevance judgment in IR**: Understanding that relevance assessment is fundamentally about matching user information needs to document content, with varying levels of granularity (binary vs. graded vs. pairwise).
- **Prompt engineering for LLMs**: Recognizing that small changes in prompt wording can significantly affect LLM outputs, particularly for subjective tasks like relevance assessment.
- **Cohen's Kappa and agreement metrics**: Understanding inter-annotator agreement measures that account for chance agreement, essential for evaluating consistency across prompts and models.
- **Ground truth construction in TREC**: Familiarity with how NIST assessors create graded relevance judgments as the gold standard for IR evaluation.
- **Cross-model generalization**: Recognizing that findings from one set of models may not generalize to others, particularly when comparing proprietary vs. open-source models.

## Architecture Onboarding

**Component Map**: Human prompts -> LLM judges -> Relevance labels -> Agreement metrics -> Analysis

**Critical Path**: Prompt creation → Document sampling → LLM judgment → Label cleaning → Agreement calculation → Sensitivity analysis

**Design Tradeoffs**: The study chose to evaluate only three LLMs and two TREC datasets to maintain manageable scope while still capturing cross-model patterns. Using temperature=0 ensures deterministic outputs but may not reflect typical usage scenarios.

**Failure Signatures**: High variance in LLM outputs across prompts indicates sensitivity; consistent agreement with human labels suggests robustness; positional bias in pairwise tasks manifests as order-dependent judgments.

**Three First Experiments**:
1. Run a small pilot with 3-5 prompts across all three LLMs to verify prompt formatting and label extraction works correctly
2. Test the UMBRELA prompt across all models on a single query to confirm its purported robustness
3. Compare binary vs. graded agreement for a single prompt across models to observe the sensitivity difference firsthand

## Open Questions the Paper Calls Out
- Does decomposing relevance into fine-grained aspects (e.g., trustworthiness, intent alignment) explain why prompts like UMBRELA achieve cross-model robustness, or are other structural properties responsible?
- How does prompt sensitivity in relevance judgment vary across LLMs beyond the three tested (GPT-4o, LLaMA 3.2-3B, Mistral-7B), particularly for newer or larger open-source models?
- Would prompts from non-expert human populations show different sensitivity patterns compared to the IR/NLP expert prompts studied here?

## Limitations
- The study evaluated only three LLMs, limiting generalizability of findings to other model families and sizes
- The specific prompt generation process and human expertise level may not reflect diverse real-world prompt creation practices
- The study did not explore prompt engineering techniques like chain-of-thought or few-shot examples that could affect sensitivity
- The pairwise agreement metric used for pairwise judgments is less established than Cohen's Kappa and may be more susceptible to noise

## Confidence
High confidence: Graded relevance judgments are more sensitive to prompt variations than binary or pairwise approaches
Medium confidence: LLM-generated prompts achieve higher average agreement with human annotations than human-crafted prompts
Medium confidence: UMBRELA demonstrates robust performance across different LLMs for graded assessments

## Next Checks
1. Test the sensitivity findings with additional state-of-the-art models (e.g., GPT-4 Turbo, Claude 3) to verify if observed patterns hold across a broader range of model capabilities
2. Implement and test prompt engineering techniques like chain-of-thought and few-shot examples to determine if they reduce sensitivity to prompt variations, particularly for graded assessments
3. Conduct a qualitative analysis of specific prompt variations that led to largest disagreements to identify linguistic or structural features most strongly influencing relevance judgments