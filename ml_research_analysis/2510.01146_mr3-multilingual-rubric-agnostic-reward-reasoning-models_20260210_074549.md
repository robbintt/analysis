---
ver: rpa2
title: 'mR3: Multilingual Rubric-Agnostic Reward Reasoning Models'
arxiv_id: '2510.01146'
source_url: https://arxiv.org/abs/2510.01146
tags:
- reasoning
- response
- language
- evaluation
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mR3 introduces a massively multilingual reward reasoning model
  trained on 72 languages, achieving the broadest language coverage in reward modeling
  to date. The core method involves standardizing input formats and using a dataset
  of 100k high-quality examples to train models that generate reasoning traces, explanations,
  and scores.
---

# mR3: Multilingual Rubric-Agnostic Reward Reasoning Models

## Quick Facts
- **arXiv ID:** 2510.01146
- **Source URL:** https://arxiv.org/abs/2510.01146
- **Reference count:** 40
- **Primary result:** Introduces mR3, the first massively multilingual reward reasoning model trained on 72 languages, outperforming larger models (e.g., GPT-OSS-120B) by up to 9x while being smaller.

## Executive Summary
mR3 presents a massively multilingual reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. The core method involves standardizing input formats and using a dataset of 100k high-quality examples to train models that generate reasoning traces, explanations, and scores. Key findings show that mR3 outperforms existing reward models and much larger models (e.g., GPT-OSS-120B) by up to 9x while being smaller, with consistent improvements across multilingual settings. Human evaluations confirm the quality of reasoning traces and rubrics, including for extremely low-resource languages unseen during training. The approach demonstrates effectiveness in off-policy preference optimization and provides interpretable, rubric-based evaluation across diverse languages.

## Method Summary
mR3 uses SFT on QWEN3 (4B/8B/14B) with a curated dataset of 100k examples across 72 languages. Data construction involves generating rubrics via GPT-4.1, distilling reasoning traces from GPT-OSS-120B, and filtering samples using GPT-OSS-20B consistency. Three aligned variants are created: ENG-ENG, TGT-ENG, TGT-TGT. Training employs easy-to-hard curriculum ordering and difficulty-based filtering. The model outputs rubric-compliant reasoning traces, explanations, and scores for point-wise, pair-wise, and binary evaluation. Training uses LLaMA-Factory with DeepSpeed Stage 3 + CPU offloading, 3 epochs, lr=1e-5, and evaluation via vLLM inference.

## Key Results
- mR3 achieves 9x smaller model size than GPT-OSS-120B while outperforming on multilingual benchmarks
- Target-language reasoning (TGT-TGT) shows largest relative gains over base models, even surpassing ENG-ENG performance
- Human evaluators prefer mR3 reasoning for cultural fit and factual correctness, including for unseen low-resource languages
- SFT training outperforms RLVR (GRPO) for rubric-based reasoning across all evaluation formats

## Why This Works (Mechanism)

### Mechanism 1
Multilingual reasoning supervision improves reward model performance across all target languages, with largest gains for target-language reasoning. The paper constructs three aligned dataset variants—ENG-ENG, TGT-ENG, TGT-TGT—by distilling outputs from a strong teacher (GPT-OSS-120B) and retaining only samples where all three strategies produce correct outputs. Training on this aligned multilingual data transfers reasoning ability across languages, with TGT-TGT showing the largest relative improvement over base models even though absolute scores remain highest for ENG-ENG.

### Mechanism 2
Difficulty-based data filtering improves training efficiency and downstream performance. The authors filter the initial dataset by discarding samples that GPT-OSS-20B can solve consistently (≤2/5 trials), focusing training on "hard" examples. This curriculum prioritizes informative gradients and avoids wasting capacity on already-learned patterns.

### Mechanism 3
Easy-to-Hard curriculum ordering improves validation and test performance over random shuffling or reverse ordering. The paper experiments with six curriculum strategies and finds that sorting training data from easiest to hardest—where difficulty is defined primarily by prediction consistency and secondarily by token length—yields the best HelpSteer3 validation performance and generalizes to test benchmarks.

### Mechanism 4
Rubric-based generative evaluation with explicit reasoning traces improves interpretability and enables flexible scoring across point-wise, pair-wise, and binary settings. mR3 follows the R3 framework, generating (trace, explanation, score) tuples conditioned on task instructions, inputs, candidate responses, and evaluation rubrics. The model outputs both a scalar/binary score and a natural language justification, making decisions more interpretable and rubric-controllable.

## Foundational Learning

- **Concept:** Cross-lingual transfer in LLMs
  - Why needed here: mR3's core hypothesis is that multilingual supervision improves reasoning across languages, including zero-shot transfer to unseen LRLs. Understanding cross-lingual transfer helps diagnose why TGT-TGT shows large relative gains and why human evaluators prefer mR3 reasoning even for unseen languages.
  - Quick check question: Can you explain why training on multilingual reasoning data might improve performance on a language the model has never seen during training?

- **Concept:** Supervised Fine-Tuning (SFT) vs. Reinforcement Learning for Reward Models
  - Why needed here: The paper explicitly compares SFT to RLVR (GRPO) and finds SFT superior for rubric-based reasoning. Understanding this trade-off is critical for choosing training objectives when building reward models.
  - Quick check question: Why might SFT be more effective than RLVR for learning to follow evaluation rubrics, and what are the efficiency implications?

- **Concept:** Curriculum Learning and Data Filtering
  - Why needed here: mR3's performance depends heavily on difficulty-based filtering and easy-to-hard ordering. These techniques are broadly applicable to data-efficient training of reasoning models.
  - Quick check question: How would you define "difficulty" for a multilingual reward model dataset, and what are the risks of over-filtering?

## Architecture Onboarding

- **Component map:** Source datasets → Rubric generation (GPT-4.1) → Output distillation (GPT-OSS-120B) → Alignment filtering (all 3 strategies correct) → Difficulty filtering (GPT-OSS-20B consistency) → Downsample to 100K → Curriculum ordering (Easy-to-Hard) → QWEN3 base (4B/8B/14B) → SFT on multilingual reasoning data → mR3 checkpoint → Evaluation: Pair-wise, Point-wise, Binary

- **Critical path:** 1) Construct aligned multilingual dataset (ENG-ENG, TGT-ENG, TGT-TGT) with rubric generation and teacher distillation 2) Filter by difficulty using weak model consistency 3) Train with Easy-to-Hard curriculum via SFT 4) Evaluate across multilingual benchmarks; validate reasoning quality via human evaluation

- **Design tradeoffs:** Data size vs. quality: 100K filtered samples outperform larger unfiltered pools; but filtering may reduce language diversity for LRLs. Teacher strength vs. cost: GPT-OSS-120B provides strong supervision but is expensive; weaker teachers (GPT-OSS-20B) degrade performance. ENG-ENG vs. TGT-TGT: ENG-ENG yields highest absolute scores; TGT-TGT shows largest relative gains and is preferred by human evaluators for cultural fit.

- **Failure signatures:** Over-filtering: If alignment or difficulty criteria are too strict, LRL samples may be excluded, harming zero-shot transfer. Rubric translation drift: Low-quality rubric translations may lead to inconsistent scoring. Curriculum mismatch: Hard-to-Easy or English-First curricula underperform on validation, suggesting ordering matters.

- **First 3 experiments:** 1) Replicate the data filtering pipeline on a small subset (e.g., 10K samples, 10 languages) to verify that difficulty-based filtering and alignment improve validation accuracy over random sampling. 2) Ablate curriculum strategies (Easy-to-Hard vs. Random vs. Hard-to-Easy) on a single model scale (e.g., QWEN3-4B) to confirm ordering effects. 3) Evaluate zero-shot transfer to 2–3 unseen LRLs (e.g., Albanian, Javanese, Telugu) to verify that TGT-TGT reasoning quality matches or exceeds ENG-ENG as reported in human evaluations.

## Open Questions the Paper Calls Out

1. Can iterative refinement of evaluation rubrics during training enhance the performance of multilingual reward models beyond static rubric generation? The current methodology relies on static rubrics generated once and does not investigate whether updating these rubrics based on model feedback improves alignment or reasoning quality.

2. To what extent does the scarcity of large-scale pre-training data in target languages limit the effectiveness of fine-tuning multilingual reasoning models? The study successfully improves fine-tuning strategies but identifies the "inherent capabilities" of base models (dominated by English pre-training) as the likely cause for the remaining performance gap, which was not empirically resolved.

3. How does the performance of mR3 compare against proprietary, closed-source LLM judges across diverse multilingual evaluation tasks? The lack of comparison with commercial state-of-the-art models (e.g., GPT-4, Claude) leaves the relative standing of the open-source mR3 models against the highest-performing proprietary judges undefined.

## Limitations

- Reliance on proprietary teacher models (GPT-OSS-120B, GPT-OSS-20B) for data distillation and filtering may limit reproducibility
- Human evaluation dataset only covers 6 low-resource languages, limiting generalizability to the full 72-language coverage claimed
- Study does not address potential bias introduced by English-centric rubrics even when translated, which could affect cultural relevance in target-language evaluation

## Confidence

- **High Confidence**: The core finding that SFT outperforms RLVR for rubric-based reward modeling, supported by direct ablation comparisons and consistent across multiple benchmarks
- **Medium Confidence**: The assertion that multilingual reasoning supervision improves performance across all target languages, particularly for target-language reasoning (TGT-TGT)
- **Low Confidence**: The generalizability of difficulty-based filtering and curriculum ordering to other multilingual reasoning tasks beyond the specific benchmarks tested

## Next Checks

1. Replicate the curriculum learning ablation on a subset of 5-10 languages with different linguistic families to verify that easy-to-hard ordering consistently outperforms random shuffling across diverse language pairs.

2. Test zero-shot transfer to 3-5 additional extremely low-resource languages not included in the human evaluation set (e.g., languages from Africa or indigenous Americas) to assess the claimed 72-language coverage.

3. Replace GPT-OSS-120B with an open-source alternative (e.g., LLaMA-3-70B) for data distillation and re-run the filtering pipeline to determine if the performance gains depend on proprietary model access.