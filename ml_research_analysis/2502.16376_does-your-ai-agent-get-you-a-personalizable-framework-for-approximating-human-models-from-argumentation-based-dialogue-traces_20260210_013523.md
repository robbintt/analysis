---
ver: rpa2
title: Does Your AI Agent Get You? A Personalizable Framework for Approximating Human
  Models from Argumentation-based Dialogue Traces
arxiv_id: '2502.16376'
source_url: https://arxiv.org/abs/2502.16376
tags:
- human
- persona
- argument
- probability
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Persona, a framework for approximating human
  models through argumentation-based dialogues. Persona combines Bayesian belief updates
  with a prospect theory-inspired probability weighting function to personalize interactions.
---

# Does Your AI Agent Get You? A Personalizable Framework for Approximating Human Models from Argumentation-based Dialogue Traces

## Quick Facts
- arXiv ID: 2502.16376
- Source URL: https://arxiv.org/abs/2502.16376
- Authors: Yinxu Tang; Stylianos Loukas Vasileiou; William Yeoh
- Reference count: 31
- Primary result: Persona achieves 0.40-0.47 Spearman correlation in human model approximation, significantly outperforming baselines

## Executive Summary
This paper introduces Persona, a framework that approximates human belief models through argumentation-based dialogues by combining Bayesian belief updates with prospect theory-inspired probability weighting. The method learns personalized parameters for each user from dialogue traces, capturing individual differences in probability perception and enabling more accurate belief state estimation. Experiments with 184 participants show significant improvements over state-of-the-art baselines, with Persona achieving correlation coefficients of 0.40-0.47 in human model approximation.

## Method Summary
Persona integrates a probability weighting function with Bayesian belief updates to approximate human models from argumentation traces. The system maintains a probability distribution over logical models, updates beliefs using observed arguments and human confidence scores, and learns personalized parameters (s, r) via correlation maximization. The framework requires three training rounds to optimize parameters before making predictions, achieving real-time performance of approximately 0.6 seconds per parameter evaluation.

## Key Results
- Persona achieved Spearman's rank correlation coefficients of 0.40-0.47 in human model approximation
- Personalization improved performance by 0.04-0.07 correlation points over generic baselines (p < 0.05)
- Three training rounds (D3) provided optimal performance, significantly outperforming single-round approaches
- Method successfully captured evolving human beliefs and facilitated personalized interactions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bayesian belief updating refines probability distributions over possible human models by increasing probability mass on models consistent with observed arguments.
- **Mechanism:** At each timestep ti, when argument Ai is observed, the system applies Equation 1: models where m |= Ai receive increased probability proportional to p(Ai), while inconsistent models receive decreased probability proportional to (1 - p(Ai)). This creates a likelihood-weighted update that progressively concentrates probability mass on models explaining the observed dialogue behavior.
- **Core assumption:** Human belief states can be adequately represented as probability distributions over propositional logic models, and argument acceptance reflects model consistency.
- **Evidence anchors:** [abstract] integration of probability weighting with Bayesian belief updates; [section] Page 3, Equation 1 describing conditional update rule.

### Mechanism 2
- **Claim:** The prospect-theory-inspired probability weighting function captures individual differences in how humans subjectively transform objective probabilities into reported confidence values.
- **Mechanism:** Equation 2 maps actual probability p(Ai) to subjective confidence σ(Ai) via parameters s ∈ (0,1) and r ∈ [1,∞). Parameter s determines confidence at p=0.5; parameter r controls nonlinear distortion (overweighting small probabilities, underweighting large ones). The inverse (Equation 3) recovers p(Ai) from observed σ(Ai) for use in Bayesian updates.
- **Core assumption:** Human probability perception follows prospect theory patterns in argumentative contexts, and this varies meaningfully across individuals.
- **Evidence anchors:** [abstract] draws on prospect theory and integrates probability weighting function; [section] Page 3-4, Equations 2-3 showing nonlinear transformation curves.

### Mechanism 3
- **Claim:** Personalized parameter learning via correlation maximization enables adaptation to individual users' probability perception patterns from limited dialogue data.
- **Mechanism:** For each participant i, the system searches over discrete parameter grids (s' ∈ {0.1,...,0.9}, r' ∈ {1,...,8}) to find (s*i, r*i) maximizing cumulative Spearman correlation between computed model rankings and user-provided ground truth rankings across observed rounds (Equation 5). Learned parameters then predict rankings in future rounds.
- **Core assumption:** Parameter (s, r) values that maximize historical correlation will generalize to future rounds within the same dialogue session.
- **Evidence anchors:** [section] Page 4, Equation 4-5 describing correlation maximization objective; [section] Page 5, Figure 3 and Table 1 showing D3 significantly outperforms D1/D2 with p-values <0.05.

## Foundational Learning

- **Concept: Probability distributions over logical models**
  - Why needed here: The framework represents uncertainty not over individual propositions but over complete world models (truth assignments to all atomic variables). Probability of any formula/argument is computed by summing distribution mass over models entailing it.
  - Quick check question: Given 3 binary propositional variables, how many models exist, and how would you compute P(a ∧ b) from a distribution over these models?

- **Concept: Structured deductive argumentation**
  - Why needed here: Arguments are defined as premise-claim pairs ⟨Φ, ϕ⟩ where Φ entails ϕ, Φ is consistent and minimal. Attack relations capture logical incompatibility between argument premises. This structure determines which models support/contradict each argument.
  - Quick check question: If argument A1 = ⟨{b, b→a}, a⟩ and A2 = ⟨{¬c, ¬c→¬a}, ¬a⟩, what makes them mutual attacks?

- **Concept: Prospect theory probability weighting**
  - Why needed here: Humans systematically deviate from linear probability assessment. The weighting function captures that subjective confidence ≠ objective probability, enabling more realistic modeling of reported beliefs.
  - Quick check question: If s=0.5 and r=2, would a user report higher or lower confidence than the actual probability for p(A)=0.8? What about p(A)=0.2?

## Architecture Onboarding

- **Component map:** Dialogue Trace Parser -> Confidence-to-Probability Converter -> Model Distribution Updater -> Ranking Generator -> Parameter Optimizer

- **Critical path:** Parameter learning -> requires historical dialogue traces + user-provided model rankings -> applies grid search -> produces personalized (s*, r*) -> enables real-time belief updates in subsequent rounds. Runtime ~0.6s per (s,r) pair per participant.

- **Design tradeoffs:**
  - Grid search vs continuous optimization: Current discrete grid (9×8=72 combinations) is tractable but may miss optima; gradient-based methods would require differentiable approximation
  - Per-user vs global parameters: Personalization improves accuracy (Table 2) but requires 3+ rounds of data; Generic baseline can deploy immediately
  - Uniform vs informed priors: Initial P_h^t0(m) = 1/|M| ignores domain knowledge; informative priors could accelerate convergence but risk bias

- **Failure signatures:**
  - **Degenerate distributions:** All probability mass concentrates on one model too quickly -> check r values (high r causes extreme distortion)
  - **Flat distributions:** No meaningful differentiation after updates -> confidence values may be clustered near 0.5 -> investigate data quality
  - **Negative correlations:** Computed rankings inverse to ground truth -> s parameter may be inverted or attack relations misconfigured

- **First 3 experiments:**
  1. Replicate Experiment 1 with synthetic dialogue traces: Generate traces from known ground-truth (s, r) values, verify parameter recovery accuracy across varying trace lengths and noise levels
  2. Ablation study on prior sensitivity: Test non-uniform initial distributions (e.g., biased toward simpler models) to quantify impact of prior misspecification
  3. Cross-domain transfer test: Train (s, r) on one argumentation scenario, evaluate on another to assess whether personalized parameters capture domain-general traits vs scenario-specific response patterns

## Open Questions the Paper Calls Out

- How can the learned human models be utilized to generate more persuasive arguments during interactions? The authors plan to investigate how these learned human models can be used to generate more persuasive arguments.

- How can the translation between natural language and the required propositional logic be automated to support real-time interactions? Additional time would be required for translating between natural language and logic, which is an area planned for future work.

- Can the Persona framework be effectively integrated into automated planning and scheduling systems? The authors list applying the learned models to other applications, including automated planning and scheduling, as a direction for future work.

## Limitations

- Model representation adequacy: The framework assumes human beliefs can be captured as probability distributions over propositional models, which may oversimplify complex, context-dependent reasoning patterns.

- Parameter generalization: The grid search optimization is computationally intensive and may not scale well to domains with larger model spaces or longer dialogues.

- Ground truth reliability: Model rankings provided by participants are subjective and may reflect presentation order effects or temporary preferences rather than stable belief structures.

## Confidence

- **High confidence**: The core mechanism combining Bayesian updating with prospect theory probability weighting is technically sound and well-supported by the mathematical framework and experimental results.

- **Medium confidence**: The claim that personalization significantly outperforms generic approaches is supported by statistical tests (p < 0.05), but the effect size improvements (0.04-0.07 Spearman correlation) may have limited practical significance.

- **Medium confidence**: The assertion that 3 training rounds provide optimal performance is based on observed correlations, but the underlying reasons for diminishing returns after 3 rounds are not fully explained.

## Next Checks

1. **Cross-scenario parameter transfer**: Test whether personalized (s, r) parameters learned in one argumentation scenario can predict behavior in novel scenarios to assess domain-general vs scenario-specific adaptation.

2. **Real-time deployment validation**: Implement the framework in an interactive dialogue system and measure whether personalized updates improve user satisfaction and argument persuasiveness beyond correlation metrics.

3. **Alternative probability weighting functions**: Compare prospect theory-based weighting against alternative models (e.g., rank-dependent utility, cumulative prospect theory) to determine if the specific two-parameter form is optimal for this application.