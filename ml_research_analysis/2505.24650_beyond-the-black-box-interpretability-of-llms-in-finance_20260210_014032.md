---
ver: rpa2
title: 'Beyond the Black Box: Interpretability of LLMs in Finance'
arxiv_id: '2505.24650'
source_url: https://arxiv.org/abs/2505.24650
tags:
- financial
- interpretability
- feature
- features
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mechanistic interpretability for large language
  models (LLMs) in finance, addressing the need for transparency in AI-driven decision-making.
  By reverse-engineering model internals, the approach dissects activations and circuits
  to reveal how specific features influence predictions, enabling both observation
  and intervention.
---

# Beyond the Black Box: Interpretability of LLMs in Finance

## Quick Facts
- arXiv ID: 2505.24650
- Source URL: https://arxiv.org/abs/2505.24650
- Authors: Hariom Tatsat; Ariye Shater
- Reference count: 9
- Primary result: Mechanistic interpretability methods improve transparency and fairness in financial AI decisions

## Executive Summary
This paper introduces mechanistic interpretability techniques to address the black-box problem in financial AI systems. By reverse-engineering model internals through sparse autoencoders, logit attribution, and activation patching, the authors demonstrate how to observe and modify model behavior in tasks like sentiment analysis, credit risk assessment, and bias detection. The approach enables targeted interventions without costly fine-tuning while providing transparency for regulatory compliance.

## Method Summary
The study applies three core interpretability techniques: sparse autoencoders to decompose neural activations into interpretable features, logit lens analysis to track prediction evolution across layers, and attribution patching to identify causally important components. Experiments use GPT-2 for basic mechanistic analysis and Gemma-2B with Gemma Scope SAEs for feature-level interventions. The methods are validated on financial tasks including sentiment classification, loan bias detection, and hallucination mitigation in regulatory question-answering.

## Key Results
- SAE feature steering reduced sentiment misclassification by up to 296 instances
- Logit Lens revealed layer 9-10 convergence for positive financial predictions in GPT-2
- Attribution patching identified layers 8 and 10 as most influential for financial predictions
- Threshold-based RAG retrieval reduced hallucination rates from 30% to 5% in regulatory QA

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders decompose polysemantic activations into monosemantic features by projecting to a higher-dimensional sparse latent space. The encoder applies ReLU activation with L1 penalty for sparsity, while the decoder reconstructs inputs. Financial concepts are assumed to be linearly representable in this space.

### Mechanism 2
Logit Lens tracks prediction evolution by projecting intermediate layer activations to vocabulary logits using the model's output projection matrix. Earlier convergence indicates simpler reasoning patterns, while later convergence suggests complex feature integration.

### Mechanism 3
Attribution patching identifies causally necessary components by comparing outputs between clean and corrupted inputs. Patching activations from clean to corrupted runs reveals which attention heads and layers are most influential for specific predictions.

## Foundational Learning

- **Superposition and Polysemanticity**: SAEs address superposition where multiple features compress into single neurons. Quick check: Can you explain why a single neuron activating for both "credit risk" and "stock ticker" would complicate interpretability?

- **Transformer Residual Stream and Attention Heads**: Patching and Logit Lens operate on residual stream activations. Quick check: If layer 8 attention heads show strong attribution for financial predictions but layer 6 shows suppression, what does this suggest about the model's processing?

- **Feature Activation Thresholds for Detection**: Bias detection relies on setting thresholds on SAE feature activations to trigger interventions. Quick check: Given a bias ratio of 0.40 vs. 0.25, what determines an appropriate threshold for flagging loan applications?

## Architecture Onboarding

- **Component map**: Financial prompts -> GPT-2/Gemma-2B -> TransformerLens/Neuronpedia -> SAE layer -> Interpretability tools -> Intervention layer

- **Critical path**: 1) Extract activations at target layers, 2) Train SAE on activations, 3) Identify domain-relevant features, 4) Validate against financial data, 5) Apply steering/interventions, 6) Measure output changes

- **Design tradeoffs**: Larger SAE dictionaries capture more features but increase labeling burden; steering magnitude affects output coherence; model scale impacts analysis feasibility

- **Failure signatures**: SAE features remain polysemantic; patching produces inconsistent results; steering creates unintended behavior changes; threshold triggers are too frequent or miss critical cases

- **First 3 experiments**: 1) Replicate Logit Lens analysis on financial prompts, 2) Discover SAE features for credit risk and validate on loan applications, 3) Apply minimal steering to credit risk feature and measure sentiment classification changes

## Open Questions the Paper Calls Out

- What is the optimal magnitude for steering SAE features to modify model behavior without introducing unintended trade-offs or entangled representations? (Section 6.3.1)

- Do mechanistic interpretability techniques developed on GPT-2 and Gemma models scale effectively to GPT-4-class and larger models used in production financial systems? (Section 7)

- Can ground-truth benchmarks be established for validating financial interpretability methods, enabling objective comparison across techniques? (Section 7)

## Limitations

- SAE feature labeling requires extensive manual effort and may not generalize across financial domains
- Current experiments limited to smaller models; scalability to GPT-4 remains unproven
- Lack of standardized benchmarks for validating financial interpretability methods

## Confidence

- **Method validity**: Medium - Theoretical framework established but financial domain validation limited
- **Experimental results**: Medium - Specific examples demonstrate effectiveness but sample size small
- **Scalability claims**: Low - Not tested on production-scale models
- **Regulatory impact**: Low - No real-world compliance testing performed

## Next Checks

1. Replicate SAE feature discovery on multiple financial datasets (sentiment, risk, compliance) to test cross-domain generalizability
2. Conduct ablation studies removing identified circuits to verify causal attribution claims
3. Test interpretability method robustness against adversarial financial prompts designed to break discovered circuits