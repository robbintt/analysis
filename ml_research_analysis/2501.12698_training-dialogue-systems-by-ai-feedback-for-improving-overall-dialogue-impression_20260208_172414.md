---
ver: rpa2
title: Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression
arxiv_id: '2501.12698'
source_url: https://arxiv.org/abs/2501.12698
tags:
- dialogue
- evaluation
- reward
- training
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates training dialogue systems to improve overall
  dialogue impression by using reinforcement learning from AI feedback (RLAIF). The
  authors prepare reward models corresponding to 12 dialogue impression metrics (e.g.,
  consistency, empathy, personality) through supervised fine-tuning of LLMs on human-annotated
  dialogue data.
---

# Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression

## Quick Facts
- arXiv ID: 2501.12698
- Source URL: https://arxiv.org/abs/2501.12698
- Reference count: 28
- Primary result: DPO-based RLAIF significantly improves dialogue systems across 12 impression metrics while PPO shows minimal gains

## Executive Summary
This paper demonstrates that dialogue systems can be effectively trained to improve overall dialogue impression using reinforcement learning from AI feedback (RLAIF). The authors develop reward models for 12 distinct dialogue impression metrics through supervised fine-tuning of LLMs on human-annotated dialogue data. These reward models then guide dialogue model training via both PPO and DPO approaches. The results show that DPO-based training significantly outperforms PPO, achieving substantial improvements in both automatic evaluation metrics and human assessments of naturalness and dialogue impression reflection.

## Method Summary
The method involves three key stages: (1) Training reward models by fine-tuning LLMs with a linear regression head on human-annotated dialogue sessions to predict 12 impression metrics; (2) Generating preference pairs using the reward model to label response pairs as accepted/rejected for each metric; (3) Training dialogue models via DPO (120 epochs) and PPO (2 epochs) using the preference data, with DPO showing superior performance. The approach uses Japanese dialogue data (JTransformer-Eval with 1,600 sessions, 12 metrics) and dialogue models (calm2-7b-chat, youri-7b-chat).

## Key Results
- DPO significantly outperforms PPO across all 12 dialogue impression metrics
- Reward model correlations reached up to 0.89 for the Trust metric after SFT training
- DPO improved AIF scores by ~0.5 average while PPO showed minimal change
- Human evaluations showed DPO models winning 65-84% of comparisons across different metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised fine-tuning of reward models substantially improves correlation with human dialogue impression judgments compared to zero-shot/few-shot prompting.
- **Mechanism**: Adding a linear regression head to an LLM and training on human-annotated dialogue scores (0-10 scale) enables the model to internalize evaluation criteria rather than rely on brittle prompting strategies.
- **Core assumption**: The 1,600 annotated dialogue sessions provide sufficient signal for generalizable reward prediction across 12 distinct impression metrics.
- **Evidence anchors**: SFT 7b model achieved correlations of 0.76-0.89 across all metrics, while prompting without SFT showed near-zero or negative correlations.

### Mechanism 2
- **Claim**: DPO-based training more effectively optimizes dialogue impression metrics while preserving fluency compared to PPO.
- **Mechanism**: DPO creates preference pairs offline (higher-scoring response = accepted, lower = rejected), avoiding the PPO instability from online reward model queries during training. This offline approach keeps the model anchored to its base distribution.
- **Core assumption**: The reward model's preference rankings are reliable enough that optimizing for them transfers to genuine human-perceived improvements.
- **Evidence anchors**: DPO improved AIF scores by ~0.5 average while PPO showed minimal change; DPO also achieved better perplexity.

### Mechanism 3
- **Claim**: Training against a single unified reward model across 12 metrics enables targeted improvement on specific dialogue impression dimensions.
- **Mechanism**: A single LLM with a regression head outputs 12 separate scores; training the dialogue model against any one metric's signal allows controlled optimization of that aspect without separate reward models per metric.
- **Core assumption**: Improving individual metrics contributes positively to overall dialogue quality without unintended negative interactions between metrics.
- **Evidence anchors**: Human rankings showed DPO models winning 65-84% of comparisons across different metrics.

## Foundational Learning

- **Concept: RLHF vs RLAIF distinction**
  - Why needed here: The paper builds on RLHF but substitutes human annotators with AI-based reward models; understanding this substitution is essential to interpret the methodology.
  - Quick check question: Can you explain why RLAIF might introduce different biases than RLHF, and what validation steps would detect them?

- **Concept: PPO vs DPO optimization**
  - Why needed here: The paper directly compares these methods; without understanding their mechanics, you cannot debug unexpected training behavior.
  - Quick check question: Why would PPO degrade perplexity while DPO improves it, as observed in this paper?

- **Concept: Reward model calibration**
  - Why needed here: The reward model outputs 0-10 scores; understanding whether these are well-calibrated affects how you interpret AIF improvements.
  - Quick check question: If a reward model systematically over-scores "safe" responses (e.g., "I see"), what failure mode would you expect during dialogue model training?

## Architecture Onboarding

- **Component map:** Reward Model (SFT LLM with regression head) -> Dialogue Model (calm2/youri-7b-chat) -> Preference Dataset Generator (reward model scoring) -> Evaluation Pipeline (PPL + AIF + human ranking)

- **Critical path:**
  1. Prepare dialogue evaluation data (JTransformer-Eval, 1,600 sessions, 12 metrics)
  2. Train reward model via SFT (regression, MSE loss, validate on held-out split)
  3. Generate preference pairs for target metric using reward model
  4. Train dialogue model with DPO (120 epochs, select by lowest training loss)
  5. Evaluate: perplexity + AIF automatic scores + human ranking

- **Design tradeoffs:**
  - Smaller reward model (1.8b) vs larger (7b): 7b achieved 0.77-0.89 correlation vs 0.32-0.55 for 1.8b
  - DPO epochs (120) vs PPO epochs (2): DPO needs more epochs but produces stable improvements
  - Single-metric optimization vs multi-metric: paper optimizes one metric at a time; joint optimization unexplored

- **Failure signatures:**
  - Dull response generation: "Yes" or "I think it's good" receive high naturalness scores from reward model despite low engagement
  - Reward hacking: Model exploits reward model blind spots rather than genuinely improving dialogue quality
  - PPL degradation without AIF gain: PPO showed this pattern, suggesting reward model optimization failing

- **First 3 experiments:**
  1. Validate reward model on held-out test split before any dialogue training; require >0.6 correlation with human scores for target metric
  2. Run DPO training on a single well-behaved metric (e.g., Trust, correlation 0.89) and verify both AIF improvement and PPL stability
  3. Conduct small-scale human evaluation (20-30 examples) comparing pre-training vs DPO outputs before full deployment; check for dull response patterns

## Open Questions the Paper Calls Out

- How can reward models be designed to penalize "dull responses" (e.g., "Yes," "I think it's good") that receive high naturalness scores but fail to meaningfully reflect targeted dialogue impression metrics?
- What methods can effectively increase the diversity of generated dialogue responses while maintaining optimization for specific dialogue impression metrics?
- Why does DPO significantly outperform PPO in both automatic metrics and human evaluation for dialogue impression optimization, despite PPO's theoretical advantages for online reward optimization?

## Limitations

- The paper doesn't validate reward model generalization across different dialogue domains or formats beyond the test split
- The "dull response" problem indicates potential reward hacking where models optimize for easily-gamed features rather than genuine quality
- Single-metric optimization leaves unexplored the potential negative interactions between optimizing different dialogue impression metrics simultaneously

## Confidence

**High Confidence**: The reward model training methodology (SFT with linear regression head) and its superior performance compared to prompting baselines.

**Medium Confidence**: The claim that DPO outperforms PPO for this task, though PPO results were minimal suggesting the comparison might be between effective vs ineffective methods.

**Low Confidence**: The transferability of improvements across all 12 metrics, as human evaluation showed varying success rates (65-84% wins) without analysis of which metrics are most effectively optimized.

## Next Checks

1. Test the trained reward model on dialogue data from different domains (customer service, casual conversation, task-oriented dialogue) to verify correlation stability above 0.6 across formats.

2. Implement a weighted sum of multiple reward signals and compare against single-metric optimization to detect potential negative interactions between metrics.

3. Collect human judgments specifically on DPO-generated responses flagged as potentially dull to verify whether high naturalness scores from the reward model correspond to actual human perceptions.