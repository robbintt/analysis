---
ver: rpa2
title: Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation
arxiv_id: '2510.20310'
source_url: https://arxiv.org/abs/2510.20310
tags:
- reasoning
- tooleqa
- question
- exploration
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolEQA, an embodied question answering agent
  that combines multi-step reasoning with external tools to improve both exploration
  efficiency and answer accuracy. ToolEQA uses a planner to decompose tasks, a controller
  to generate reasoning steps and code for tool invocation, and an executor to run
  tools in the environment.
---

# Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation

## Quick Facts
- arXiv ID: 2510.20310
- Source URL: https://arxiv.org/abs/2510.20310
- Authors: Mingliang Zhai; Hansheng Liang; Xiaomeng Fan; Zhi Gao; Chuanhao Li; Che Sun; Xu Bin; Yuwei Wu; Yunde Jia
- Reference count: 16
- Key outcome: Introduces ToolEQA, achieving 9.2-20.2% higher success rates than state-of-the-art baselines on EQA-RT and HM-EQA through multi-step reasoning and external tool augmentation.

## Executive Summary
This paper presents ToolEQA, an embodied question answering agent that integrates multi-step reasoning with external tools to enhance exploration efficiency and answer accuracy. The agent uses a planner to decompose tasks, a controller to generate reasoning steps and code for tool invocation, and an executor to run tools in the environment. A novel data generation pipeline creates 18K tasks with reasoning trajectories (EQA-RT), split into training and test sets. ToolEQA demonstrates significant performance improvements over state-of-the-art baselines on multiple benchmarks, including EQA-RT, HM-EQA, OpenEQA, and ExpressBench. Fine-tuned models show improved reasoning and tool usage capabilities.

## Method Summary
ToolEQA is an embodied question answering agent that combines multi-step reasoning with external tools. The agent consists of three components: a Planner LLM that decomposes tasks into structured sub-goals, a Controller VLM that generates reasoning steps and tool-invoking code, and an Executor that runs tools in the Habitat-Sim environment. The Controller is fine-tuned on a dataset of 18K tasks (EQA-RT) generated through a pipeline involving 3D detection and GPT-4o for task and trajectory generation. The fine-tuning uses LoRA on Qwen2.5-VL-7B with trajectory sampling to manage memory overhead.

## Key Results
- ToolEQA achieves 9.2-20.2% higher success rates than state-of-the-art baselines on EQA-RT and HM-EQA benchmarks.
- Fine-tuning increases thought length (90 to 116 tokens) and tool accuracy (58% to 69%).
- The agent demonstrates strong performance on OpenEQA and ExpressBench benchmarks.
- ToolEQA shows improved exploration efficiency with shorter path lengths compared to reactive VLM exploration.

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented State Estimation
External tools (e.g., `ObjectLocation3D`) provide more precise spatial attributes (size, distance) than direct VLM inference, reducing hallucination in comparison tasks. Instead of relying on the VLM to estimate object volume from pixels—which fails in Figure 6—the agent generates Python code to invoke a detector. This returns explicit numerical bounds, grounding the subsequent reasoning step.

### Mechanism 2: Plan-Guided Exploration Efficiency
Decomposing tasks into structured sub-goals via a Planner before exploration reduces navigational oscillation and path length compared to reactive VLM exploration. A dedicated Planner module first maps the question to a sequence of high-level sub-goals (e.g., "Go to living room," then "Check size"). This conditions the Controller's reasoning, preventing it from getting stuck in "blind exploration" loops common in End-to-End VLM agents.

### Mechanism 3: Trajectory-Conditioned Policy Distillation
Fine-tuning the VLM controller on A*-optimized reasoning traces enforces efficient exploration habits and correct tool syntax better than prompting. The data pipeline generates "gold" trajectories by computing the shortest physical path (A*) and using GPT-4o to inject reasoning. Training on this data distills the logic of when to stop exploring and how to aggregate observations.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Paradigm**
  - **Why needed here:** The Controller uses an iterative loop of Thought $\to$ Code $\to$ Observation. Understanding that the model must explicitly generate a reasoning trace before acting is crucial to debugging why it might skip steps.
  - **Quick check question:** Does the model output an action immediately, or does it first generate a "Thought:" block explaining the intent?

- **Concept: VLM Hallucination vs. Grounding**
  - **Why needed here:** The paper explicitly tackles VLM failures in size comparison (Figure 6). You must distinguish between when the model is "guessing" based on priors vs. "reporting" based on tool output.
  - **Quick check question:** If the `ObjectLocation3D` tool returns `None`, does the model admit ignorance or invent a size?

- **Concept: Habitat-Sim / HM3D Environment**
  - **Why needed here:** The agent interacts with a 3D simulator. Understanding the discrete navigation commands (MoveForward, TurnLeft) and the rendering pipeline is required to debug "Executor" failures.
  - **Quick check question:** Is the failure due to the physics engine (agent stuck) or the logic engine (wrong tool call)?

## Architecture Onboarding

- **Component map:** Input Question $Q$ + Scene ID $\to$ Planner LLM $\to$ Plan $p$ (List of sub-goals) $\to$ Controller VLM (takes $Q, p, \text{History } h$) $\to$ generates $(\text{Thought } t, \text{Code } c)$ $\to$ Executor $\to$ returns Observation $o$ $\to$ Memory stores trajectory $\{t, c, o\}$

- **Critical path:** The *Controller* is the inference bottleneck. It must parse the plan, look at the image history, and write syntactically correct Python code. If the Controller misses an object in the history, it may re-explore unnecessarily.

- **Design tradeoffs:**
  - **Code vs. JSON:** Authors chose Python code for flexibility (e.g., `math.prod` for volume) over rigid JSON schemas.
  - **Sampling Strategy:** Training uses "trajectory sampling" (random non-key steps) to fit long 12-step trajectories into GPU memory.

- **Failure signatures:**
  - **Redundant Exploration:** "I need to check the bedroom" $\to$ *Turn Left* $\to$ "Nothing here" $\to$ *Turn Right* (loop). (Fix: Check planner graph connectivity).
  - **Tool Syntax Error:** Code calls `VisualQA(question, image)` but passes wrong image path. (Fix: Executor exception handling).

- **First 3 experiments:**
  1. **Sanity Check (Zero-Shot):** Run GPT-4o as the Controller (no fine-tuning) on EQA-RT-Seen. Verify it can successfully call tools without training.
  2. **Ablation on Planning:** Remove the Planner module and feed the question directly to the Controller. Measure the drop in success rate and path efficiency ($L(m)$) to quantify the Planner's contribution.
  3. **Data Scaling:** Train the Controller on only 10% of EQA-RT-Train data. Check if tool usage (syntax) learns faster than reasoning logic (thought quality).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the random sampling of non-key reasoning steps during training impair the agent's ability to handle long-horizon dependencies or recover from navigation errors?
- **Basis in paper:** [Explicit] Page 7 states a "trajectory sampling strategy" is used where non-key steps are randomly sampled and key steps retained to reduce memory overhead, assuming non-key steps are redundant.
- **Why unresolved:** While efficient, removing context might prevent the model from learning how to recover from extended sequences of unsuccessful exploration.
- **What evidence would resolve it:** Ablation studies comparing success rates on tasks with varying trajectory lengths when trained with 100% of steps vs. the proposed sampling strategy.

### Open Question 2
- **Question:** How robust is the agent to failures in the initial static plan generated by the Planner?
- **Basis in paper:** [Inferred] Section 3 formulates the controller's decision based on an overall plan $p$ (Eq. 1), but the architecture does not explicitly describe a mechanism for re-planning if the initial high-level plan proves impossible.
- **Why unresolved:** If the environment changes or the initial plan contains logical errors, the controller may lack the capacity to update the high-level strategy, potentially leading to bottlenecks.
- **What evidence would resolve it:** Experiments introducing dynamic obstacles or invalidating initial plans to see if the Controller can diverge from the provided plan to solve the task.

### Open Question 3
- **Question:** How does the performance of ToolEQA degrade when the external tools (e.g., ObjectLocation3D, VisualQA) provide noisy or hallucinated outputs?
- **Basis in paper:** [Inferred] The system is evaluated in simulation (HM3D) where tools likely have high accuracy; however, the Controller's reliance on these tools for reasoning (Section 3) suggests it may be vulnerable to error propagation.
- **Why unresolved:** The paper does not analyze the sensitivity of the reasoning chain to imperfect tool observations, which is critical for real-world robotics.
- **What evidence would resolve it:** Testing the agent's success rate while systematically injecting noise or errors into the observations returned by the executor tools.

## Limitations
- **Data Quality Dependency:** The approach hinges on the quality of the EQA-RT dataset generated via GPT-4o and 3D detection models, with no precision reporting of automated pipelines.
- **Tool Reliability Assumption:** Assumes external tools provide reliable, structured outputs, but doesn't evaluate robustness when tools return `None` or hallucinate detections.
- **Simulator vs. Reality Gap:** Evaluation confined to Habitat-Sim/HM3D may overestimate tool invocation reliability and navigation efficiency compared to real-world deployments.

## Confidence
- **High Confidence:** Success rate improvements (9.2-20.2%) over baselines on EQA-RT and HM-EQA are directly measured and reported. The architectural contribution (Planner + Controller + Executor) is clearly specified.
- **Medium Confidence:** The claimed mechanism of tool-based state estimation (avoiding VLM hallucination) is supported by Figure 6 but relies on a single illustrative example.
- **Low Confidence:** The assertion that fine-tuning on A*-optimized trajectories enforces "efficient exploration habits" is inferred from increased thought length and tool accuracy, but lacks ablation showing correlation with actual path efficiency in novel environments.

## Next Checks
1. **Tool Failure Robustness:** Deliberately inject detector failures (return `None` or random outputs) for 10% of tool invocations during evaluation. Measure whether the Controller can recover or falls back to VLM-only reasoning.

2. **Real-World Transfer:** Deploy ToolEQA on a real robot platform (e.g., LoCoBot) in a controlled apartment testbed. Compare success rates and path lengths against Habitat-Sim to quantify the simulator-reality gap.

3. **Ablation on Tool Flexibility:** Replace Python code generation with a fixed JSON schema for tool calls. Measure the impact on success rate and reasoning depth to validate the claimed advantage of flexible code over rigid schemas.