---
ver: rpa2
title: 'Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression'
arxiv_id: '2505.11143'
source_url: https://arxiv.org/abs/2505.11143
tags:
- nash
- information
- regression
- side
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Neural Adaptive Shrinkage (Nash), a high-dimensional
  regression framework that integrates covariate-specific side information into the
  estimation process using neural networks. Nash adaptively learns structured penalties
  in a nonparametric fashion, enabling flexible regularization without the need for
  cross-validation.
---

# Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression

## Quick Facts
- **arXiv ID:** 2505.11143
- **Source URL:** https://arxiv.org/abs/2505.11143
- **Reference count:** 40
- **Primary result:** Introduces Nash, a framework using neural networks to adaptively learn structured penalties from covariate side information in high-dimensional regression

## Executive Summary
Nash is a high-dimensional regression framework that integrates covariate-specific side information into the estimation process using neural networks. It adaptively learns structured penalties in a nonparametric fashion, enabling flexible regularization without cross-validation. The method extends existing approaches by using neural networks to incorporate side information when learning the penalty function in linear regression.

## Method Summary
Nash uses split variational empirical Bayes (split VEB) to decouple prior learning from posterior inference, allowing for efficient and scalable optimization. The algorithm iteratively updates regression coefficients and learns the penalty structure through a neural network. It generalizes existing approaches that incorporate side information, offering a unified and more expressive framework. The method employs a mixture of Gaussians as the prior, where a neural network learns the mixture weights based on side information, effectively mimicking different penalty shapes for different covariate groups.

## Key Results
- On denoising MNIST images, Nash-fused achieved an RMSE of 0.058 compared to 0.062 for XGBoost and 0.441 for a standard MLP
- On TCGA gene expression prediction, Nash-mdn achieved an RMSE of 0.435 compared to 0.449 for mr.ash and 0.466 for Elastic Net
- Experiments demonstrate that Nash consistently performs competitively and can outperform existing methods tailored to handle specific types of side information

## Why This Works (Mechanism)

### Mechanism 1
Structured side information can be transformed into adaptive regularization strengths, potentially reducing the need for manual cross-validation. The architecture employs a neural network $g(d_j, \theta)$ to map covariate metadata to the parameters of a prior distribution over regression coefficients. By optimizing these parameters via empirical Bayes, the model effectively "learns" which covariates should be heavily shrunk and which should be spared, based on their metadata features. The core assumption is that side information contains predictive signal about the sparsity or magnitude of the true regression coefficients. This mechanism degrades to standard uniform shrinkage if side information is uninformative or the neural network fails to converge to a meaningful mapping.

### Mechanism 2
Decoupling coefficient inference from prior learning via "split VEB" lowers computational complexity relative to standard variational EM. The algorithm introduces a latent variable split, separating the optimization into two iterative steps: a fast coordinate ascent for $\beta$ and a supervised learning step for the neural network parameters $\theta$. This prevents the need to update the neural network multiple times per iteration, a bottleneck in related methods. The core assumption is that the latent space introduced by the split maintains a valid Evidence Lower Bound that approximates the true marginal likelihood well. If the coordinate ascent oscillates or fails to converge, it suggests the split approximation is too loose or the step sizes for the neural net updates are mismatched with the variance updates.

### Mechanism 3
Flexible mixture priors allow the model to approximate a wide range of penalty structures dynamically. Instead of hard-coding a specific penalty like $L_1$, the model uses a mixture of Gaussians (or Laplace distributions) as the prior. The neural network learns the mixture weights $\pi(d_j, \theta)$. This allows the "shrinkage operator" to effectively mimic different penalty shapes for different covariate groups based on the data. The core assumption is that the true underlying penalty structure is heterogeneous across covariates and can be represented by the mixture families defined. If the mixture components are poorly chosen, the model cannot adapt to the signal magnitudes, resulting in over-shrinking or under-shrinking.

## Foundational Learning

- **Variational Inference (VI) & ELBO:** Nash relies on maximizing an Evidence Lower Bound (ELBO) rather than exact Bayesian inference. Understanding the trade-off between computational speed and approximation accuracy is crucial for debugging convergence. *Quick check question:* Can you explain why we maximize the ELBO instead of the true log-likelihood in this high-dimensional setting?

- **Empirical Bayes (EB):** The model learns the hyperparameters (the neural network weights $\theta$) from the data. This differs from pure Bayesian approaches where hyperparameters might be fixed or integrated out. *Quick check question:* How does Empirical Bayes "borrow strength" across covariates to estimate the prior $g$?

- **Coordinate Ascent:** The "split VEB" algorithm iteratively updates $\beta$, then $b$, then $\theta$. Understanding that this is a hill-climbing procedure that optimizes one variable block at a time helps in diagnosing partial convergence issues. *Quick check question:* If the loss plateaus but $\beta$ is still changing, which component (the neural net or the likelihood) is likely lagging?

## Architecture Onboarding

- **Component map:** Standardized design matrix $X$ and side information matrix $D$ -> Coefficient Layer ($\beta$) -> Latent Prior Layer ($b, \theta$) -> Variance Layer ($\sigma^2, \sigma^2_0$)
- **Critical path:** The algorithm iterates: Compute residuals $\bar{r}$ -> Update $\beta$ using $\bar{r}$ and current prior means $\bar{b}$ -> Update Neural Net $\theta$ by maximizing the marginal likelihood of the current $\bar{\beta}$ -> Update posterior means $\bar{b}$ based on the new neural net prior -> Update global variances
- **Design tradeoffs:** Expressivity vs. Stability (deep MDN allows rich penalty shapes but risks overfitting vs. simple categorical network is more stable but less adaptive); Speed vs. Accuracy (split VEB is fast but assumes decoupling is valid vs. joint optimization might be needed for high correlations)
- **Failure signatures:** Variance Collapse ($\sigma_0^2$ goes to zero, forcing all $\beta$ to zero); Prior Overtting (neural net learns a prior that perfectly fits current $\beta$ estimates but fails to generalize); Non-convergence (oscillating RMSE suggests neural net learning rate is too high)
- **First 3 experiments:** 1) Sanity Check (No Side Info): Run Nash with constant side info;