---
ver: rpa2
title: 'ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive
  Step-level Guardrail and Feedback'
arxiv_id: '2601.10156'
source_url: https://arxiv.org/abs/2601.10156
tags:
- tool
- action
- safety
- agent
- guardrail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of ensuring safe tool invocation
  in LLM-based agents, which face security risks from malicious user requests and
  prompt injection attacks. To tackle this, the authors introduce TS-Bench, the first
  benchmark for step-level tool invocation safety detection, and develop TS-Guard,
  a guardrail model trained via multi-task reinforcement learning to proactively detect
  unsafe tool calls before execution.
---

# ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback

## Quick Facts
- arXiv ID: 2601.10156
- Source URL: https://arxiv.org/abs/2601.10156
- Reference count: 40
- Primary result: Reduces harmful tool invocations by up to 65% while improving benign task completion by ~10% under attack conditions

## Executive Summary
ToolSafe addresses safety risks in LLM-based agents caused by malicious user requests and prompt injection attacks during tool invocation. The authors introduce TS-Bench, the first benchmark for step-level tool invocation safety detection, and develop TS-Guard, a guardrail model trained via multi-task reinforcement learning to proactively detect unsafe tool calls before execution. They also propose TS-Flow, a guardrail-feedback-driven reasoning framework that guides agents toward safer tool use. Experiments show that TS-Flow reduces harmful tool invocations by up to 65% on average while improving benign task completion by approximately 10% under attack conditions.

## Method Summary
ToolSafe introduces a multi-component system for safe tool invocation in LLM-based agents. TS-Bench provides a step-level benchmark with 13,120 samples across 8 domains for training and evaluation. TS-Guard is a Qwen2.5-7B model trained via multi-task reinforcement learning that performs three sequential safety detection tasks: request harmfulness prediction, attack-action correlation detection, and final safety rating. TS-Flow is a runtime framework that intercepts candidate tool calls, uses TS-Guard to evaluate safety, and injects guardrail feedback into the agent's context to enable self-correction before execution.

## Key Results
- TS-Flow reduces harmful tool invocations by 65% on average while improving benign task completion by approximately 10% under attack conditions
- Multi-task reasoning in TS-Guard outperforms single-task classification for step-level safety detection
- Pre-execution feedback loops reduce unsafe invocations while preserving benign task completion better than abort-based defenses
- Guardrail feedback increases entropy in agent outputs at risky steps, preventing overconfident unsafe execution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task reasoning improves step-level safety detection over single-task classification.
- Mechanism: TS-Guard decomposes detection into three sequential tasks—request harmfulness prediction, attack-action correlation detection, and final safety rating—trained jointly via GRPO with a weighted multi-task reward. This forces the model to produce intermediate reasoning (`<Think>`) before judgments, improving generalization to novel attack patterns.
- Core assumption: Explicit intermediate supervision signals guide the model toward causal analysis rather than pattern matching.
- Evidence anchors:
  - [abstract] "It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback."
  - [section 4.1] Eq. (3) defines multi-task reward with w1=w2=w3=1/3; Figure 3(b) shows multi-task outperforms single-task reward.
  - [corpus] Related work "Think Twice Before You Act" similarly uses thought correction for behavioral safety.
- Break condition: If harmfulness and attack labels are mislabeled in training data, joint training could propagate errors across tasks.

### Mechanism 2
- Claim: Pre-execution feedback loops reduce unsafe invocations while preserving benign task completion better than abort-based defenses.
- Mechanism: TS-Flow intercepts candidate tool calls before execution, feeds TS-Guard's analysis back to the agent as context, and allows the agent to revise its action. This contrasts with LlamaFirewall's "detect-and-abort" paradigm, which terminates execution on detection.
- Core assumption: Agents can incorporate guardrail feedback to self-correct without external intervention.
- Evidence anchors:
  - [abstract] "TS-Flow... reduces harmful tool invocations of ReAct-style agents by 65 percent... and improves benign task completion by approximately 10 percent under prompt injection attacks."
  - [section 5.4.2, Table 4] TS-Flow with TS-Guard achieves 1.16% ASR and 42.78% Utility on AgentDojo vs. LlamaFirewall's 0.95% ASR and 20.79% Utility.
  - [corpus] Pro2Guard proposes probabilistic model checking for proactive runtime enforcement, aligning with pre-execution intervention.
- Break condition: If agents ignore or misinterpret feedback (limitation acknowledged in Section 9), safety gains degrade.

### Mechanism 3
- Claim: Guardrail feedback increases entropy in agent outputs at risky steps, preventing overconfident unsafe execution.
- Mechanism: Without guardrails, ReAct agents exhibit decreasing token-wise entropy during reasoning, indicating overconfidence. TS-Guard feedback injects uncertainty at detected-risky steps, raising entropy and encouraging exploration of safer trajectories.
- Core assumption: Higher entropy correlates with more careful reasoning and safer action selection.
- Evidence anchors:
  - [section 6.3, Figure 5] Shows entropy rising at tool-call boundaries when TS-Flow intervenes vs. monotonic decrease without guardrails.
  - [section 6.2, Figure 4] TS-Guard lowers entropy at final judgment tokens but preserves reasoning-step entropy.
  - [corpus] No direct corpus evidence on entropy-safety link for agents; this mechanism is paper-specific.
- Break condition: If high entropy leads to random or incoherent outputs rather than safer alternatives, task utility degrades.

## Foundational Learning

- **ReAct-style agents (Reasoning + Acting)**:
  - Why needed here: TS-Flow is designed for ReAct loops; understanding Thought/Action/Observation structure is prerequisite.
  - Quick check question: Can you trace how a ReAct agent updates its context after each tool observation?

- **Prompt injection taxonomy**:
  - Why needed here: The paper distinguishes malicious user requests vs. third-party prompt injection; defense strategies differ.
  - Quick check question: What's the difference between direct injection (DPI) and indirect injection (IPI) in tool outputs?

- **GRPO (Group Relative Policy Optimization)**:
  - Why needed here: TS-Guard is trained via GRPO, not SFT; understanding RL-based alignment is critical.
  - Quick check question: Why does the paper claim RL-only outperforms SFT+RL for this task?

## Architecture Onboarding

- **Component map**:
  - Agent generates tool call -> TS-Guard evaluates safety -> TS-Flow injects feedback -> Agent revises or proceeds

- **Critical path**:
  1. Agent generates candidate tool call (a_t).
  2. TS-Guard receives (a_t, H_t, u_t, T) and outputs safety analysis.
  3. If rating ≥ 0.5, feedback injected; agent re-reasons.
  4. Safe action executed; observation appended to history.

- **Design tradeoffs**:
  - **Latency vs. safety**: TS-Flow adds 36-89% context overhead (Table 10); acceptable for non-real-time agents.
  - **Strict vs. loose mode**: Strict mode treats "controversial" as unsafe (higher recall, more false positives); loose mode treats as safe.
  - **Abort vs. feedback**: Abort maximizes safety but destroys utility; feedback preserves utility but requires agent cooperation.

- **Failure signatures**:
  - High false positive rate on benign tasks with injected context (AgentDojo BTRA cases in Table 3).
  - Agent ignores feedback and repeats unsafe action (acknowledged limitation).
  - TS-Guard misclassifies novel attack patterns not in TS-Bench.

- **First 3 experiments**:
  1. **Guardrail model ablation**: Compare TS-Guard (RL-only) vs. SFT vs. SFT+RL on TS-Bench-eval strict mode F1.
  2. **Feedback richness study**: Run TS-Flow with full TS-Guard output vs. only safety rating; measure ASR and Utility delta (Table 5).
  3. **Entropy dynamics**: Log token-wise entropy for ReAct agent with/without TS-Flow on AgentDojo; verify entropy rise at intervention points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can guardrail models and agents be jointly trained to achieve tighter coupling between agent reasoning processes and guardrail safety judgments?
- Basis in paper: [explicit] The limitations section states that "the agent and the guardrail model are trained independently, without explicit coordination, which may lead to misalignment between the agent's reasoning process and the guard's safety judgments. Future work will explore joint training or tighter coupling."
- Why unresolved: The current design treats guardrail feedback as an external signal appended to context, which may not be reliably incorporated by agents. Joint training could improve alignment but requires novel training frameworks.
- What evidence would resolve it: A joint training framework demonstrating improved safety-utility trade-offs compared to independently trained TS-Guard and TS-Flow, with analysis of agent-guardrail alignment metrics.

### Open Question 2
- Question: What architectural mechanisms beyond appending feedback to context can ensure agents reliably incorporate guardrail safety signals?
- Basis in paper: [inferred] The paper notes "current LLM-based agents may occasionally fail to fully incorporate such feedback, limiting the effectiveness of step-level safety intervention." This methodological limitation suggests the feedback mechanism itself is suboptimal.
- Why unresolved: The simple append-to-context approach is model-agnostic but lacks guarantees that agents will attend to and act upon safety feedback appropriately.
- What evidence would resolve it: Comparison of alternative feedback integration mechanisms (e.g., attention steering, constrained decoding, explicit reasoning channels) showing higher compliance rates with safety feedback.

### Open Question 3
- Question: Why does SFT pre-training reduce the effectiveness of subsequent RL fine-tuning for guardrail models, and how can this be mitigated?
- Basis in paper: [explicit] The ablation study shows "RL-only shows superior generalization" while "SFT+RL" underperforms. The authors explain "entropy analysis reveals a decrease from 0.74 to 0.61, indicating reduced output diversity, which may limit subsequent RL gains."
- Why unresolved: The paper observes the phenomenon but does not provide a definitive explanation or solution. Understanding this interaction could improve training strategies across many safety applications.
- What evidence would resolve it: Controlled experiments isolating entropy effects, alternative SFT objectives that preserve diversity, or modified RL approaches that account for reduced exploration capacity.

### Open Question 4
- Question: Can step-level guardrails effectively defend against adversarial attacks specifically designed to evade detection (e.g., attacks aware of the guardrail model)?
- Basis in paper: [inferred] The paper evaluates against standard prompt injection attacks but does not test adaptive adversaries who know the guardrail model exists and attempt to craft inputs that bypass it while still achieving malicious tool invocations.
- Why unresolved: Security systems often face adaptive adversaries. The generalization claims (e.g., to unseen domains) may not hold under white-box attack scenarios.
- What evidence would resolve it: Red-teaming experiments using guardrail-aware attack strategies, demonstrating robustness or identifying failure modes under adaptive adversarial conditions.

## Limitations
- The generalizability of TS-Guard beyond the 13 domains in TS-Bench remains untested
- The assumption that agents will incorporate guardrail feedback is unverified in practice
- The entropy-safety correlation is asserted but not validated with ablation studies

## Confidence

- **High** confidence in the multi-task reasoning improvement claim (supported by controlled ablation in Table 2 and explicit reward design)
- **High** confidence in the pre-execution feedback loop's utility-safety tradeoff (clear quantitative comparison vs. LlamaFirewall in Table 4)
- **Medium** confidence in the entropy mechanism (mechanism-specific, with limited external validation)
- **Medium** confidence in the generalizability of results (TS-Bench domain coverage acknowledged as a constraint)

## Next Checks

1. **Cross-domain robustness test**: Evaluate TS-Guard on a held-out set of tool types and user domains not represented in TS-Bench training or validation data.
2. **Agent compliance study**: Measure the frequency and conditions under which agents ignore or misinterpret TS-Flow feedback, and quantify the impact on safety/utility.
3. **Entropy ablation**: Run TS-Flow with feedback stripped of all but the final safety rating to isolate the entropy effect from other feedback mechanisms.