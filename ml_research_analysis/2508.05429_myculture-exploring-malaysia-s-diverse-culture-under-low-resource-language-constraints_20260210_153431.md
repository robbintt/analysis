---
ver: rpa2
title: 'MyCulture: Exploring Malaysia''s Diverse Culture under Low-Resource Language
  Constraints'
arxiv_id: '2508.05429'
source_url: https://arxiv.org/abs/2508.05429
tags:
- llms
- mcqs
- open-ended
- cultural
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MyCulture, a benchmark to evaluate LLMs'
  understanding of Malaysia's diverse, multi-ethnic culture in the low-resource Bahasa
  Melayu language. To reduce format bias and guessing, it proposes an open-ended MCQ
  format with multi-answer, ordering, and matching types, without predefined options.
---

# MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints

## Quick Facts
- arXiv ID: 2508.05429
- Source URL: https://arxiv.org/abs/2508.05429
- Reference count: 10
- Key outcome: LLMs show minimum 17% performance drop when switching from closed-form to open-ended MCQs for Malaysia's multi-ethnic culture in Bahasa Melayu

## Executive Summary
This paper introduces MyCulture, a benchmark designed to evaluate large language models' understanding of Malaysia's diverse, multi-ethnic culture in the low-resource Bahasa Melayu language. The benchmark addresses the challenge of format bias and guessing in traditional multiple-choice questions by proposing an open-ended MCQ format featuring multi-answer, ordering, and matching question types without predefined options. Across 2,652 expert-validated questions spanning six cultural domains, the study reveals that LLMs experience a significant performance drop when transitioning from closed-form to open-ended formats, highlighting the limitations of current evaluation methods and the need for culturally grounded, linguistically inclusive approaches.

## Method Summary
MyCulture presents a benchmark with 2,652 expert-validated questions across six cultural domains in Bahasa Melayu. The key innovation is an open-ended MCQ format that eliminates predefined options to reduce format bias and guessing. Question types include multi-answer, ordering, and matching formats requiring open-ended responses. This approach contrasts with traditional closed-form MCQs by requiring models to generate answers rather than select from given choices. The benchmark specifically targets low-resource language constraints, using Malaysia's cultural context as a case study to evaluate how well LLMs can handle nuanced local cultural knowledge despite being trained primarily on high-resource languages.

## Key Results
- LLMs show minimum 17% performance drop when switching from closed-form to open-ended MCQs
- The open-ended format reveals inflated performance under standard MCQ formats
- Models trained on high-resource languages struggle with nuanced local cultural knowledge despite strong general performance

## Why This Works (Mechanism)
The mechanism behind MyCulture's effectiveness lies in its elimination of format bias inherent in traditional multiple-choice questions. By removing predefined options, the open-ended MCQ format forces models to demonstrate genuine understanding rather than exploiting patterns or making educated guesses. This approach better captures cultural nuances that require contextual knowledge and reasoning beyond surface-level pattern matching. The multi-answer, ordering, and matching question types further challenge models to engage with complex cultural relationships and hierarchies that cannot be adequately assessed through simple selection tasks.

## Foundational Learning
- **Low-resource language constraints**: Understanding how limited training data affects model performance in specific languages - needed to contextualize the Bahasa Melayu focus; quick check: compare model performance across languages with varying resource levels
- **Cultural domain expertise**: Knowledge of Malaysia's multi-ethnic cultural landscape across six domains - needed for question design and validation; quick check: verify domain coverage matches Malaysia's cultural diversity
- **MCQ format bias**: Recognition of how predefined options can inflate model performance through guessing - needed to justify open-ended approach; quick check: measure performance gap between open and closed formats
- **Expert validation processes**: Methods for ensuring cultural question accuracy and relevance - needed for benchmark credibility; quick check: review inter-rater agreement among cultural experts
- **Linguistic inclusivity in AI**: Understanding how language barriers affect model cultural understanding - needed for broader implications; quick check: test models across multiple low-resource languages

## Architecture Onboarding

**Component Map:**
Data Collection -> Question Design -> Expert Validation -> Benchmark Assembly -> Model Evaluation -> Performance Analysis

**Critical Path:**
Expert Validation -> Benchmark Assembly -> Model Evaluation

**Design Tradeoffs:**
- Open-ended format reduces guessing but increases grading complexity
- Bahasa Melayu focus provides cultural depth but limits generalizability
- Expert validation ensures quality but introduces potential selection bias
- Six cultural domains provide breadth but may miss niche cultural elements

**Failure Signatures:**
- High performance in closed-form but low in open-ended suggests format exploitation
- Domain-specific weaknesses indicate cultural knowledge gaps
- Language performance gaps reveal low-resource training limitations

**3 First Experiments:**
1. Replicate the 17% performance gap across different model sizes and architectures
2. Test a subset of questions with human graders to establish baseline performance
3. Evaluate models fine-tuned on Malaysian cultural data to measure improvement potential

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Exclusive focus on Bahasa Melayu may not generalize to other low-resource languages
- Limited pool of cultural experts could introduce selection bias in question design
- Open-ended MCQ grading may introduce measurement noise through ambiguous criteria
- Six cultural domains, while comprehensive, may not capture all relevant cultural nuances

## Confidence
- **High**: Core finding of 17% performance drop between closed-form and open-ended MCQs
- **Medium**: Claims about cultural grounding challenges being inferred from performance gaps
- **Medium**: Assertion that high-resource language models inherently struggle with nuanced local cultural knowledge based on single context

## Next Checks
1. Replicate the performance gap findings across at least two additional low-resource languages with distinct cultural contexts
2. Conduct inter-rater reliability analysis on the open-ended MCQ grading to quantify human annotation consistency
3. Test whether fine-tuning on domain-specific cultural data reduces the open-ended MCQ performance gap