---
ver: rpa2
title: Automatically Finding Rule-Based Neurons in OthelloGPT
arxiv_id: '2511.00059'
source_url: https://arxiv.org/abs/2511.00059
tags:
- neurons
- decision
- layer
- trees
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an automated approach to identify and interpret
  rule-based neurons in OthelloGPT, a transformer trained to predict legal moves in
  Othello. The method uses regression decision trees to map board states to neuron
  activations, then extracts high-activation decision paths to generate human-readable
  logical rules describing each neuron's behavior.
---

# Automatically Finding Rule-Based Neurons in OthelloGPT

## Quick Facts
- arXiv ID: 2511.00059
- Source URL: https://arxiv.org/abs/2511.00059
- Authors: Aditya Singh; Zihang Wen; Srujananjali Medicherla; Adam Karvonen; Can Rager
- Reference count: 15
- Key outcome: Automated method identifies interpretable rule-based neurons in OthelloGPT, with 913/2,048 neurons in layer 5 accurately described by compact decision trees (R² > 0.7), confirmed by causal interventions showing 5-10x performance degradation when ablating pattern-specific neurons.

## Executive Summary
This paper presents an automated approach to identify and interpret rule-based neurons in OthelloGPT, a transformer trained to predict legal moves in Othello. The method uses regression decision trees to map board states to neuron activations, then extracts high-activation decision paths to generate human-readable logical rules describing each neuron's behavior. The results show that approximately half of neurons in layer 5 can be accurately described by compact rule-based decision trees (R² > 0.7 for 913 of 2,048 neurons). Causal interventions confirm these neurons are mechanistically relevant - ablating neurons corresponding to specific patterns degrades the model's ability to predict legal moves along those patterns by 5-10 times more than control patterns. The paper also provides a Python tool mapping rule-based game behaviors to their implementing neurons, facilitating future interpretability research.

## Method Summary
The paper introduces an automated pipeline for discovering interpretable neurons in OthelloGPT by fitting regression decision trees that map board states to individual neuron activations. For each neuron, the method trains a decision tree regressor using board state features as inputs and neuron activation values as targets. High-activation decision paths are then extracted from these trees to form human-readable logical rules describing the neuron's behavior. The approach focuses on layer 5 neurons, identifying those that can be accurately described by compact trees (R² > 0.7). The method is validated through causal interventions - systematically ablating neurons associated with specific patterns and measuring performance degradation on those patterns versus control patterns. A Python tool is provided to map rule-based game behaviors to their implementing neurons.

## Key Results
- 913 out of 2,048 neurons in layer 5 are accurately described by compact decision trees (R² > 0.7)
- Causal ablation experiments show 5-10x greater performance degradation on targeted patterns versus control patterns
- The method successfully identifies interpretable patterns including corner-adjacent pieces and specific edge formations
- A Python tool is provided for mapping rule-based game behaviors to implementing neurons

## Why This Works (Mechanism)
The approach works by leveraging the fact that many neurons in OthelloGPT respond to specific, interpretable patterns in the game state. Decision trees can effectively capture these pattern-based relationships because they naturally represent hierarchical logical conditions. By fitting regression trees to neuron activations, the method discovers the specific board configurations that trigger each neuron. High-activation paths through the tree correspond to the most important conditions for that neuron's firing. The causal intervention validation confirms that these identified patterns are not merely correlations but mechanistically important for the model's performance on legal move prediction.

## Foundational Learning
- Decision tree regression: Used to map board states to neuron activations; needed because it can capture nonlinear relationships while remaining interpretable
- Quick check: Verify tree depth and R² values for each neuron to assess interpretability quality
- Othello game mechanics: Understanding legal moves and strategic patterns; needed as domain knowledge for interpreting neuron behaviors
- Quick check: Cross-reference identified patterns with known Othello strategy principles
- Neuron ablation: Systematic deactivation of neurons to test their functional importance; needed to validate whether identified patterns are mechanistically relevant
- Quick check: Compare performance degradation on targeted vs control patterns
- Activation mapping: Connecting neuron responses to specific board features; needed to bridge between raw activations and interpretable patterns
- Quick check: Visualize activation heatmaps for different board configurations

## Architecture Onboarding
Component map: Othello board states -> Transformer layers (12 total) -> Neuron activations -> Decision tree regression -> Rule extraction -> Causal validation
Critical path: Board state features -> Layer 5 neuron activations -> Decision tree fitting -> High-activation path extraction -> Rule generation
Design tradeoffs: The method prioritizes interpretability over completeness, successfully characterizing neurons with simple, pattern-based behaviors while leaving more complex neurons poorly understood. This represents a deliberate choice to focus on high-value interpretable components rather than attempting comprehensive characterization.
Failure signatures: Neurons with low R² values (R² < 0.7) indicate either diffuse activation patterns, highly nonlinear responses, or redundant representations that resist simple rule-based description.
First experiments:
1. Train decision tree regressor for a single neuron and examine the resulting rule structure
2. Perform ablation on neurons with highest-confidence rules and measure performance impact
3. Test rule generalization by applying extracted rules to held-out board states with systematic pattern variations

## Open Questions the Paper Calls Out
None

## Limitations
- The approach works best for neurons encoding simple, interpretable patterns and struggles with neurons showing diffuse or highly nonlinear activation patterns
- Many neurons remain poorly characterized, suggesting significant limitations in current interpretability methods for transformer-based game models
- The model maintains reasonable performance even after removing neurons described by high-quality decision trees, suggesting either redundancy or incomplete characterization

## Confidence
- High confidence in the identified rule-based neurons in layer 5, supported by both regression accuracy and causal intervention results
- Medium confidence in general claims about OthelloGPT interpretability due to the method's limitations with complex neurons
- Low confidence in completeness of the interpretability analysis given that many neurons remain uncharacterized

## Next Checks
1. Test whether extracted rules generalize to board states not seen during regression training using held-out validation sets with systematic pattern variations
2. Analyze whether combining multiple rule-based neurons (AND/OR logic) captures additional interpretable behaviors missed by individual neuron analysis
3. Apply the methodology to earlier and later layers to understand how rule-based representations emerge and transform through the network hierarchy