---
ver: rpa2
title: 'OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real
  World'
arxiv_id: '2508.03669'
source_url: https://arxiv.org/abs/2508.03669
tags:
- object
- shape
- omnishape
- pose
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OmniShape addresses the problem of joint pose and shape estimation
  from a single RGB-D image without assuming known object models or categories. The
  core method idea is to decouple shape completion into two distributions: (1) mapping
  images to partial pointclouds in a normalized object reference frame (NORF), and
  (2) completing full object shapes from these partial observations using triplanar
  neural fields.'
---

# OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World

## Quick Facts
- **arXiv ID**: 2508.03669
- **Source URL**: https://arxiv.org/abs/2508.03669
- **Authors**: Katherine Liu, Sergey Zakharov, Dian Chen, Takuya Ikeda, Greg Shakhnarovich, Adrien Gaidon, Rares Ambrus
- **Reference count**: 40
- **Primary result**: Zero-shot multi-hypothesis shape and pose estimation from RGB-D images using conditional diffusion models, achieving state-of-the-art performance on real-world benchmarks

## Executive Summary
OmniShape addresses the problem of joint pose and shape estimation from a single RGB-D image without assuming known object models or categories. The core method idea is to decouple shape completion into two distributions: (1) mapping images to partial pointclouds in a normalized object reference frame (NORF), and (2) completing full object shapes from these partial observations using triplanar neural fields. By modeling both distributions with separate conditional diffusion models, OmniShape enables sampling multiple hypotheses from the joint pose and shape distribution. The method achieves state-of-the-art performance on real-world datasets, with best-of-25 hypothesis results showing 0.2867 F1 score and 0.230 Chamfer distance on Ocrtoc3D, and 0.2571 F1 score and 0.272 Chamfer distance on Pix3D.

## Method Summary
OmniShape uses a two-stage diffusion-based approach to decouple pose and shape estimation. First, a diffusion model predicts NORF maps from images, which encode partial pointclouds in a normalized reference frame. Second, another diffusion model completes full shapes from these partial observations using triplanar neural fields. The NORF approach enables pose estimation through registration without requiring canonicalized training data. The method generates multiple hypotheses by sampling from both diffusion models independently, leveraging diversity to handle occlusion and symmetry. Training uses synthetic data from ShapeNet and Objaverse-LVIS with rendered normals, while inference runs DPM-Solver++ for ~3 seconds per hypothesis.

## Key Results
- Best-of-25 hypothesis results: 0.2867 F1 score and 0.230 Chamfer distance on Ocrtoc3D
- Best-of-25 hypothesis results: 0.2571 F1 score and 0.272 Chamfer distance on Pix3D
- Outperforms deterministic baselines through multi-hypothesis approach
- 45% relative improvement in F1 score when using 25 hypotheses vs. single hypothesis on Ocrtoc3D

## Why This Works (Mechanism)

### Mechanism 1: Distributional Decoupling via Chain Rule
- Claim: Joint pose-shape estimation from images can be tractably learned by factoring p(z,m|I) into two conditional distributions rather than modeling the joint directly.
- Mechanism: The chain rule decomposition p(z,m|I) = p(z|m)p(m|I) separates (1) predicting normalized partial pointclouds from images and (2) completing shapes from those partial observations. Each factor is modeled by an independent diffusion model, allowing specialized training for each subproblem.
- Core assumption: The NORF map m captures sufficient information about both pose and partial geometry to condition shape completion—i.e., shape does not depend directly on the image once m is known.
- Evidence anchors: [abstract] "OmniShape is based on the key insight that shape completion can be decoupled into two multi-modal distributions"; [Section II] Equation (1) and surrounding text explicitly state the factorization strategy; [corpus] MonoDiff9D and Diff9D use similar diffusion-based factorizations for pose estimation.

### Mechanism 2: NORF as Pose-Conditioned Canonical Space
- Claim: A learned normalized reference frame enables shape completion without strict dataset canonicalization while preserving differentiable pose recovery.
- Mechanism: The NORF map m ∈ R^(d×d×3) stores normalized 3D coordinates per pixel, providing dense correspondence between image pixels and a canonical object space. This allows the shape completion model to operate in a consistent reference frame without requiring pre-aligned training data. At test time, registering predicted NORF coordinates to observed depth recovers metric pose.
- Core assumption: The dataset has implicit alignment structure (even without explicit canonicalization) that diffusion models can learn—human-curated data exhibits regularities like "cups point upward."
- Evidence anchors: [Section II-B] "Fig. 3 shows cup openings pointing upwards and axis aligned airplanes, suggesting some inherent alignment rules"; [Section II-B] "The canonicalization and therefore pose distribution modeled is a function of the dataset"; [corpus] NOCS methods demonstrate similar coordinate regression for pose estimation.

### Mechanism 3: Multi-Hypothesis Sampling via Diffusion
- Claim: Modeling both distributions with diffusion enables sampling diverse hypotheses that capture multimodal uncertainty from occlusion and symmetry.
- Mechanism: Each diffusion model ϵθ learns to denoise from random noise to the target distribution. By running independent sampling chains, OmniShape generates multiple (m,z) pairs representing different plausible interpretations. The best-of-N results and inlier-based selection leverage this diversity.
- Core assumption: The diffusion models successfully capture the true multimodal structure of the conditional distributions, and sampling diversity aligns with task-relevant ambiguity.
- Evidence anchors: [abstract] "By training separate conditional diffusion models for these two distributions, we enable sampling multiple hypotheses"; [Section III-B, Table I] Best-of-25 achieves 0.2867 F1 vs. 0.1975 for first hypothesis on Ocrtoc3D—a 45% relative improvement demonstrating diversity benefits; [corpus] Uncertainty Quantification for Visual Object Pose Estimation emphasizes importance of multimodal pose distributions.

## Foundational Learning

- **Concept**: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: Both pipeline stages use DDPMs. Understanding the forward noising process, reverse denoising objective (Eq. 2), and classifier-free guidance is essential.
  - Quick check question: Given a noisy sample u_t at timestep t, what does the denoising network ϵθ predict?

- **Concept**: Triplanar Neural Fields / SDFs
  - Why needed here: Shapes are represented as triplanar latent grids decoded to signed distance fields. Understanding how 3D points project to feature planes and how SDFs define surfaces is critical.
  - Quick check question: How do you extract a mesh from a learned SDF, and why might triplanes be preferred over full 3D grids?

- **Concept**: Point Cloud Registration (Procrustes/RANSAC)
  - Why needed here: Pose estimation from NORF predictions requires registering predicted coordinates to observed depth. Understanding inlier counting as a hypothesis selection signal is key.
  - Quick check question: Given correspondences between NORF predictions and depth points, how would you robustly estimate the SE(3) transform?

## Architecture Onboarding

- **Component map**: RGB(+optional normals) → UNet ϵmθ → NORF map m → Ortho-NORF voxelization/projection → UNet ϵzθ → Triplane z → MLP decoder ξ → SDF → Marching cubes → Mesh; m + observed depth → RANSAC/Procrustes → SE(3) pose

- **Critical path**: 1) Train triplanes Z offline using Eq. 4 (SDF supervision + TV regularization); 2) Train ϵmθ on (image, normals) → NORF pairs; 3) Train ϵzθ on (partial NORF → triplane) pairs; 4) Inference: Sample N hypotheses → Register each → Select by inliers or other metric

- **Design tradeoffs**: Triplane resolution (p=5) vs. memory; CFG scale benefits dataset-dependent; hypothesis count vs. latency (25 hypotheses improve F1 by ~45% but require ~75s total); normal conditioning dropped with 50% probability during training

- **Failure signatures**: Incoherent geometry from shape completion when conditioning is ambiguous; poor hypothesis selection where inlier count doesn't correlate with shape quality; pose ambiguity from symmetry captured in multiple NORF hypotheses

- **First 3 experiments**: 1) Ablate hypothesis count (N=1,5,10,25) on held-out validation to quantify diversity benefits vs. inference cost; 2) Test NORF prediction with/without normal conditioning on real vs. synthetic data to assess robustness to normal noise; 3) Compare inlier-based selection against oracle to understand gap and motivate future selection metric development

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a learned hypothesis selection metric effectively close the performance gap between the current inlier-based heuristic and the oracle best-of-N results? Section IV lists "learning a hypothesis selection metric" as a specific future direction.

- **Open Question 2**: Does incorporating semantic feature conditioning (e.g., from DINOv2) improve the model's ability to resolve ambiguous geometries and detailed structures? Section IV explicitly suggests "using semantic feature conditioning" to address current limitations.

- **Open Question 3**: Can fine-tuning on zero-shot normal estimation models mitigate the domain gap caused by noisy real-world depth sensors? Section IV identifies that "noisy real-world normals appear out of distribution" and suggests "finetuning on zero-shot normal estimates" as a solution.

## Limitations
- The method's performance gains rely heavily on sampling multiple hypotheses, but the selection mechanism (inlier-based registration) may not correlate well with actual shape completion quality
- Dependence on synthetic training data with rendered normals raises concerns about real-world generalization when normals are noisy or absent
- Underspecified Ortho-NOR Frame voxelization and projection details make exact reproduction challenging

## Confidence

- **High confidence**: The diffusion-based factorization approach (p(z,m|I) = p(z|m)p(m|I)) is technically sound and well-supported by related work in conditional diffusion models and shape completion
- **Medium confidence**: Real-world performance improvements are demonstrated but may be influenced by dataset-specific factors (implicit alignment in ShapeNet/Objaverse) that don't generalize to truly uncanonicalized data
- **Low confidence**: The NORF map's ability to preserve sufficient geometric information for shape completion across diverse object categories, particularly in the presence of severe occlusion or when dataset alignment is minimal

## Next Checks

1. **Generalization to Truly Uncanonicalized Data**: Test on a dataset with random rotations (no consistent orientation patterns) to verify NORF predictions remain consistent and shape completion works without implicit alignment

2. **Alternative Selection Metrics**: Compare inlier-based selection against learned selection approaches or oracle selection to quantify the gap and evaluate whether registration-based metrics adequately capture shape completion quality

3. **Normal Conditioning Robustness**: Systematically vary normal noise levels in synthetic data and evaluate real-world performance with/without normal conditioning to assess the method's sensitivity to normal quality and distribution shifts