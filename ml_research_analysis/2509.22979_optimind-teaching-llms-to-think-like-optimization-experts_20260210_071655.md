---
ver: rpa2
title: 'OptiMind: Teaching LLMs to Think Like Optimization Experts'
arxiv_id: '2509.22979'
source_url: https://arxiv.org/abs/2509.22979
tags:
- city
- problem
- units
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of translating natural language
  optimization problems into precise mathematical models, a task requiring specialized
  expertise. The authors propose OptiMind, a framework that systematically integrates
  optimization expertise to improve formulation accuracy.
---

# OptiMind: Teaching LLMs to Think Like Optimization Experts
## Quick Facts
- arXiv ID: 2509.22979
- Source URL: https://arxiv.org/abs/2509.22979
- Reference count: 40
- Primary result: Improves formulation accuracy by 14-20 percentage points over strong base models

## Executive Summary
This work addresses the challenge of translating natural language optimization problems into precise mathematical models, a task requiring specialized expertise. The authors propose OptiMind, a framework that systematically integrates optimization expertise to improve formulation accuracy. Key innovations include class-based error analysis to identify and prevent common mistakes, semi-automated training data cleaning, and multi-turn inference with domain-informed prompts and solver feedback. Experiments across three cleaned benchmarks show that OptiMind improves formulation accuracy by 14-20 percentage points over strong base models and matches or exceeds performance of much larger proprietary models. The framework demonstrates consistent gains across multiple optimization problem classes and validates the importance of domain knowledge in making LLMs reliable for optimization tasks.

## Method Summary
OptiMind addresses the challenge of translating natural language optimization problems into precise mathematical models by integrating optimization expertise into the LLM workflow. The framework employs class-based error analysis to systematically identify and prevent common formulation mistakes, uses semi-automated training data cleaning to ensure high-quality training signals, and implements multi-turn inference with domain-informed prompts and solver feedback. The approach validates the importance of domain knowledge in making LLMs reliable for optimization tasks across multiple problem classes.

## Key Results
- Improves formulation accuracy by 14-20 percentage points over strong base models
- Matches or exceeds performance of much larger proprietary models
- Demonstrates consistent gains across multiple optimization problem classes

## Why This Works (Mechanism)
OptiMind's effectiveness stems from its systematic approach to error prevention and iterative refinement. The class-based error analysis identifies common formulation mistakes across different optimization problem types, allowing the framework to anticipate and prevent these errors through targeted prompts. The semi-automated training data cleaning ensures that LLMs learn from high-quality examples rather than noisy or incorrect formulations. The multi-turn inference framework allows for iterative refinement by incorporating solver feedback, enabling the model to correct mistakes and improve formulations progressively. Domain-informed prompting provides the necessary optimization-specific context that general-purpose LLMs lack, bridging the gap between natural language understanding and mathematical modeling.

## Foundational Learning
- **Optimization problem classes** - Understanding different types of optimization problems (linear, integer, combinatorial) is essential because each class has distinct formulation requirements and common error patterns.
- **Mathematical modeling translation** - The ability to convert natural language descriptions into precise mathematical formulations is the core skill being taught, requiring both domain knowledge and linguistic precision.
- **Error classification systems** - Systematic categorization of formulation errors enables targeted prevention strategies and improves the framework's ability to handle diverse problem types.
- **Solver feedback integration** - Incorporating solver responses into the formulation process creates a feedback loop that progressively refines solutions and catches errors that human reviewers might miss.

## Architecture Onboarding
- **Component Map**: Natural Language Input -> Class-based Error Analysis -> Domain-informed Prompting -> LLM Formulation -> Solver Feedback -> Refined Output
- **Critical Path**: The multi-turn inference loop (Prompting -> Formulation -> Solver Feedback -> Refinement) represents the core workflow where accuracy improvements occur.
- **Design Tradeoffs**: OptiMind trades computational efficiency for accuracy by using multiple inference turns rather than single-shot generation, but this investment pays off through significantly improved formulation quality.
- **Failure Signatures**: Common failure modes include incorrect constraint formulations, objective function misinterpretation, and variable type mismatches, all of which are systematically addressed through error analysis.
- **First Experiments**: 1) Compare single-turn vs multi-turn inference accuracy on benchmark problems, 2) Test error analysis effectiveness by measuring formulation improvement rates, 3) Evaluate domain-informed prompting against generic prompts across different problem classes.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability claim comparing to larger proprietary models relies on reported results rather than direct head-to-head evaluation
- Scalability of semi-automated training data cleaning approach untested on real-world optimization problems
- Framework performance on complex constraints and objectives beyond curated benchmarks remains unverified

## Confidence
- **High**: Controlled benchmark experiments show consistent 14-20 percentage point improvements
- **Medium**: Performance comparison to larger models based on reported results rather than direct testing
- **Low**: Scalability to real-world problems with complex constraints not yet validated

## Next Checks
1. Conduct ablation studies isolating the contribution of each framework component to verify their individual impact on formulation accuracy
2. Test OptiMind on optimization problems from real-world domains (e.g., supply chain, energy systems) to assess performance beyond curated benchmarks
3. Evaluate the framework's robustness when faced with ambiguous natural language descriptions where multiple valid formulations exist