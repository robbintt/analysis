---
ver: rpa2
title: 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision'
arxiv_id: '2509.01360'
source_url: https://arxiv.org/abs/2509.01360
tags:
- medical
- retrieval
- image
- modalities
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes M3Ret, a unified multimodal medical image retrieval
  framework that leverages self-supervised learning to achieve state-of-the-art zero-shot
  performance across diverse medical imaging modalities. The approach addresses the
  challenge of fragmented, modality-specific retrieval methods by training a single
  visual encoder on a large-scale hybrid dataset of 867,653 samples spanning X-ray,
  ultrasound, endoscopy videos, and CT scans.
---

# M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision
## Quick Facts
- arXiv ID: 2509.01360
- Source URL: https://arxiv.org/abs/2509.01360
- Reference count: 40
- Unified multimodal medical image retrieval framework achieving state-of-the-art zero-shot performance across diverse medical imaging modalities

## Executive Summary
M3Ret introduces a unified framework for zero-shot multimodal medical image retrieval that leverages self-supervised learning to achieve superior performance across diverse medical imaging modalities. The approach addresses the challenge of fragmented, modality-specific retrieval methods by training a single visual encoder on a large-scale hybrid dataset spanning X-ray, ultrasound, endoscopy videos, and CT scans. By employing MAE and SimDINO self-supervised learning paradigms without modality-specific customization, M3Ret demonstrates strong zero-shot generalization capabilities across multiple medical datasets while achieving cross-modal alignment without paired data.

## Method Summary
M3Ret is built on a hybrid dataset of 867,653 medical samples from four modalities: X-ray, ultrasound, endoscopy videos, and CT scans. The framework employs two self-supervised learning paradigms - MAE (Masked Autoencoder) and SimDINO (Simple DINO) - to train a unified visual encoder without modality-specific customization. The approach focuses exclusively on visual self-supervision without incorporating textual or clinical metadata. The model is evaluated on zero-shot image-to-image retrieval tasks across four medical datasets, demonstrating superior performance compared to strong baselines including DINOv3 and BMC-CLIP.

## Key Results
- Achieves state-of-the-art zero-shot performance on image-to-image retrieval tasks across four medical datasets
- Successfully generalizes to MRI tasks despite never observing MRI during pretraining
- Demonstrates strong cross-modal alignment without requiring paired multimodal data during training

## Why This Works (Mechanism)
The framework's success stems from leveraging large-scale self-supervised pretraining on diverse medical imaging data, which enables the model to learn rich visual representations that generalize across modalities. By avoiding modality-specific customization and relying purely on visual self-supervision, M3Ret develops robust feature representations that capture universal patterns in medical imaging. The combination of MAE and SimDINO paradigms provides complementary learning signals that enhance the model's ability to understand spatial relationships and contextual information across different imaging modalities.

## Foundational Learning
- Self-supervised learning (MAE, SimDINO) - Why needed: Enables learning from unlabeled medical images at scale; Quick check: Compare performance with supervised baselines on downstream tasks
- Multimodal fusion strategies - Why needed: Allows unified processing of diverse medical imaging modalities; Quick check: Evaluate modality-specific vs. unified encoder performance
- Zero-shot transfer learning - Why needed: Enables application to unseen medical tasks without fine-tuning; Quick check: Test on completely novel imaging modalities not in pretraining data

## Architecture Onboarding
- Component map: Raw images -> Visual encoder (MAE/SimDINO) -> Feature embeddings -> Retrieval system
- Critical path: Image input → Masked reconstruction (MAE) or instance discrimination (SimDINO) → Feature extraction → Similarity matching
- Design tradeoffs: Pure visual self-supervision vs. multimodal learning with text/clinical data
- Failure signatures: Poor performance on specialized modalities with limited representation in pretraining data; degradation in cross-modal retrieval tasks
- First experiments:
  1. Ablation study comparing MAE-only vs. SimDINO-only vs. combined pretraining approaches
  2. Evaluation of retrieval performance across different similarity metrics (cosine vs. Euclidean)
  3. Analysis of retrieval accuracy vs. dataset size and modality diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive reliance on visual self-supervision without incorporating textual or clinical metadata that could enhance semantic understanding
- Evaluation scope limited primarily to image-to-image retrieval tasks across specific modalities
- Claims of state-of-the-art performance based on comparisons with a limited set of baselines

## Confidence
Zero-shot multimodal retrieval performance: High
Generalization to unseen MRI: Medium
Scalability and modality unification: Medium

## Next Checks
1. Evaluate M3Ret's performance on a broader range of medical imaging modalities and clinical tasks, including histopathology, PET scans, and multimodal diagnosis scenarios involving both images and clinical reports.

2. Conduct ablation studies specifically examining the impact of dataset size, diversity, and modality distribution on zero-shot generalization performance across different medical domains.

3. Compare M3Ret against recent state-of-the-art multimodal medical vision models that incorporate both image and text understanding, such as RegionMed-CLIP and MedUnifier, on clinically relevant retrieval benchmarks.