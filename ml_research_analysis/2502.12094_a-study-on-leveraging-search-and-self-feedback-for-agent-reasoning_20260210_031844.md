---
ver: rpa2
title: A Study on Leveraging Search and Self-Feedback for Agent Reasoning
arxiv_id: '2502.12094'
source_url: https://arxiv.org/abs/2502.12094
tags:
- search
- feedback
- mcts
- tool
- username
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of self-feedback during search
  for agent reasoning, examining its effectiveness on mathematical reasoning and tool-calling
  tasks. While search algorithms like MCTS can improve performance, the experiments
  reveal that self-feedback alone is often unreliable for guiding the search process.
---

# A Study on Leveraging Search and Self-Feedback for Agent Reasoning

## Quick Facts
- arXiv ID: 2502.12094
- Source URL: https://arxiv.org/abs/2502.12094
- Reference count: 6
- Self-feedback during search is unreliable for complex tasks; majority voting outperforms self-feedback reward selection

## Executive Summary
This study investigates whether self-feedback can effectively guide search algorithms for agent reasoning in mathematical and tool-calling tasks. The experiments reveal that while ground-truth feedback enables MCTS to substantially improve reasoning performance, self-feedback alone is often unreliable. On GSM8K, majority voting across search tree nodes consistently outperforms self-feedback-based reward selection. For tool-calling tasks, self-feedback search can degrade performance due to tool parameter hallucinations. Various augmentation strategies were tested, with mixed results—some improving precision but reducing recall. The findings suggest that search algorithms require carefully designed, task-specific feedback mechanisms rather than relying solely on self-feedback.

## Method Summary
The paper evaluates MCTS with self-refine (MCTS-R) on GSM8K and ToolTalk datasets using Llama 3, Mistral, Claude Haiku, and Claude Sonnet models. The search algorithm generates candidate solutions at each node, which are then evaluated by a feedback model that provides self-generated critiques and ratings. Three answer selection strategies are compared: random selection, majority voting on final answers, and maximum self-feedback reward selection. For ToolTalk, additional augmentation strategies include anti-hallucination guidelines, ICL examples, and a hallucination detection module. Ground-truth feedback serves as an upper bound for comparison.

## Key Results
- On GSM8K, MCTS with ground-truth feedback achieves 0.958 accuracy (Llama 3) versus 0.813 no-search baseline
- Majority voting consistently outperforms maximum self-feedback reward selection across all tested models
- For tool-calling tasks, self-feedback search can degrade performance due to tool parameter hallucinations
- Hallucination detection modules improve precision but significantly reduce recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Search with ground-truth feedback substantially improves reasoning performance over direct generation.
- **Mechanism:** MCTS uses the UCT criterion to balance exploration and exploitation. Feedback signals initialize Q-values during node creation and update them via backpropagation. With ground-truth, the search can reliably prune incorrect paths and prioritize promising reasoning trajectories.
- **Core assumption:** Feedback quality directly determines search effectiveness; unreliable rewards misguide exploration.
- **Evidence anchors:**
  - [abstract]: "For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task."
  - [section]: Table 1 shows MCTS with ground-truth feedback achieves 0.958 accuracy on GSM8K (Llama 3) versus 0.813 no-search baseline—approximately 14.5 percentage point improvement.
  - [corpus]: Related work (Hao et al., 2023; Zhou et al., 2023) confirms MCTS can improve reasoning when feedback is available.
- **Break condition:** When ground-truth is unavailable and self-feedback rewards are noisy, UCT-guided exploration may amplify errors rather than correct them.

### Mechanism 2
- **Claim:** Majority voting across search tree nodes outperforms self-feedback-based reward selection when ground-truth is unavailable.
- **Mechanism:** Rather than relying on the model's self-assessment (maximum reward selection), majority voting aggregates final answers across all explored nodes and selects the most frequent. This leverages the intuition that correct answers cluster while errors disperse.
- **Core assumption:** The search process generates sufficient diversity that correct answers converge more frequently than incorrect ones.
- **Evidence anchors:**
  - [section]: Table 1 shows majority voting achieves 0.883 (Llama 3), 0.608 (Mistral), 0.905 (Haiku), and 0.786 (Sonnet)—consistently outperforming maximum reward selection (0.776, 0.469, 0.854, 0.685 respectively).
  - [section]: "Within the strategies that do not rely on ground-truth feedback, majority voting seems to be the only selection strategy that consistently improves over the no-search baseline."
  - [corpus]: Weak direct corpus support; neighbor papers focus on self-feedback for training rather than inference-time selection.
- **Break condition:** When the model has systematic biases producing consistent wrong answers, majority voting will amplify rather than correct errors.

### Mechanism 3
- **Claim:** Self-feedback is unreliable for complex tasks involving tool-calling because models fail to detect parameter hallucinations.
- **Mechanism:** In tool-calling, agents must ground parameters in conversation context. Self-feedback models evaluate their own outputs but miss hallucinated parameters because the feedback mechanism lacks explicit grounding verification. Augmentation strategies (ICL examples, hallucination detection modules) can partially address this but introduce tradeoffs.
- **Core assumption:** Models can verbally critique outputs but cannot reliably verify factual grounding without external support.
- **Evidence anchors:**
  - [abstract]: "For tool-calling tasks, self-feedback search can degrade performance due to tool parameter hallucinations."
  - [section]: Table 2 shows MCTS with self-feedback reduces Sonnet 3 F1 from 0.706 to 0.559. Table 3 shows hallucination detection module improves precision (0.754) but reduces recall (0.544).
  - [corpus]: Stechly et al. (2024) and Kambhampati et al. (2024) corroborate limitations in self-evaluation capabilities.
- **Break condition:** When hallucination detection becomes overly conservative, models request unnecessary confirmations, degrading user experience and recall.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - **Why needed here:** The paper builds on MCTSr framework; understanding selection, expansion, simulation, and backpropagation is essential to interpret results.
  - **Quick check question:** Can you explain how UCT balances exploration (visiting unexplored nodes) versus exploitation (visiting high-value nodes)?

- **Concept: Self-Feedback vs. Ground-Truth Feedback**
  - **Why needed here:** The core experimental design compares these two feedback sources; misunderstanding this distinction leads to misinterpreting the findings.
  - **Quick check question:** If a model rates its own solution as 9/10 but the answer is wrong, what type of feedback failure is this?

- **Concept: Precision-Recall Tradeoffs in Tool-Calling**
  - **Why needed here:** Augmentation strategies show precision gains but recall losses; practitioners must understand this tradeoff when designing feedback mechanisms.
  - **Quick check question:** Why might a hallucination detection module that catches more errors also cause the model to ask users for information they've already provided?

## Architecture Onboarding

- **Component map:** User Query → Agent (generates candidate solutions) → Search Algorithm (MCTS/DFS) → Feedback Model (self-feedback or augmented) → Reward Computation → Node Selection Strategy → Final Answer (majority voting / max reward / random)

- **Critical path:** Feedback quality determines reward reliability → Reward reliability determines node selection quality → Node selection determines final answer accuracy.

- **Design tradeoffs:**
  - Ground-truth feedback: Highest performance but requires task-specific verification (often unavailable in deployment)
  - Self-feedback: No external dependencies but unreliable for complex tasks
  - Majority voting: Robust to feedback noise but requires sufficient search diversity
  - Hallucination detection module: Improves precision but risks over-conservative behavior (unnecessary confirmations, reduced recall)

- **Failure signatures:**
  - Tool parameter hallucination: Model generates plausible-sounding but ungrounded parameters (e.g., "username: assistant_request")
  - Premature tool calling: Model makes tool calls with incomplete information instead of asking clarifying questions
  - Over-correction: Model requests confirmations or information already provided in context
  - Reward inversion: Self-feedback assigns high rewards to incorrect solutions

- **First 3 experiments:**
  1. **Establish baselines:** Run no-search direct generation and MCTS with ground-truth feedback on your target task to establish performance bounds.
  2. **Test selection strategies without ground-truth:** Compare majority voting vs. maximum reward selection vs. random selection using the same search tree to isolate the impact of selection method.
  3. **Diagnose failure modes:** On errors, manually annotate whether failures stem from search quality (no correct answer in tree) versus selection quality (correct answer exists but wasn't selected). This determines whether to improve the generator or the feedback/selection mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can carefully designed, task-specific feedback mechanisms achieve performance comparable to ground-truth feedback for search-based reasoning?
- Basis in paper: [explicit] The abstract states "For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task."
- Why unresolved: The augmentation strategies tested (guidelines, ICL, hallucination modules) showed mixed results—some improved precision but reduced recall, none matched ground-truth performance.
- What evidence would resolve it: A feedback mechanism design that achieves ≥95% of ground-truth MCTS performance on GSM8K without any ground-truth access.

### Open Question 2
- Question: Why does majority voting consistently outperform self-feedback-based reward selection, and does this pattern generalize across all reasoning domains?
- Basis in paper: [explicit] "Majority voting seems to be the only selection strategy that consistently improves over the no-search baseline" and "self-feedback may not be a reliable source for providing rewards."
- Why unresolved: The paper demonstrates the empirical finding but does not provide a theoretical explanation for why aggregating predictions works better than using the model's own quality assessments.
- What evidence would resolve it: Analysis across additional domains (code generation, logical reasoning) showing whether majority voting maintains its advantage, combined with theoretical analysis of self-feedback calibration.

### Open Question 3
- Question: How can the precision-recall tradeoff observed in hallucination detection modules be resolved for complex agentic tasks?
- Basis in paper: [inferred] Table 3 shows the hallucination detection module significantly increased precision (0.754 vs 0.502 for Sonnet) but substantially reduced recall (0.544 vs 0.630).
- Why unresolved: The paper identifies that models with hallucination detection become overly cautious, asking for unnecessary confirmation, but does not propose solutions to balance this tradeoff.
- What evidence would resolve it: A feedback augmentation strategy that improves both precision and recall simultaneously, or a tunable mechanism that allows practitioners to select an operating point on the precision-recall curve.

### Open Question 4
- Question: Can self-feedback be effectively leveraged for reasoning outside of the search paradigm, as suggested by recent work?
- Basis in paper: [explicit] The introduction suggests "alternative approaches for leveraging self-feedback outside of search" and cites Chen et al. (2025) in this context.
- Why unresolved: The paper focuses specifically on search-based methods and does not investigate whether self-feedback might be more reliable in non-search reasoning approaches.
- What evidence would resolve it: Comparative experiments showing self-feedback performance in iterative refinement vs. search-based approaches on identical tasks.

## Limitations
- Self-feedback is unreliable for complex tasks involving tool-calling due to parameter hallucinations
- Augmentation strategies show mixed results with precision-recall tradeoffs that cannot be resolved
- The findings may not generalize to other complex reasoning tasks beyond GSM8K and ToolTalk

## Confidence
- **High:** Ground-truth feedback superiority over self-feedback for both tasks; majority voting outperforming max-reward selection on GSM8K; the precision-recall tradeoff with hallucination detection modules.
- **Medium:** Self-feedback degradation in tool-calling tasks; effectiveness of augmentation strategies (due to limited ablation studies).
- **Low:** The generalizability of these findings to other complex reasoning tasks beyond GSM8K and ToolTalk.

## Next Checks
1. **Ablation Study on Augmentation Strategies:** Systematically isolate the impact of each augmentation component (guidelines, ICL examples, hallucination detection module) on tool-calling performance to determine which components drive precision gains and recall losses.
2. **Search Diversity Analysis:** Quantify the diversity of solutions generated in the search tree for both tasks to determine if majority voting's success is driven by correct-answer clustering or simply by the number of candidates explored.
3. **Error Analysis of Tool Hallucinations:** Manually categorize tool-calling errors into hallucination types (parameter fabrication, missing parameters, premature calls) to determine if the hallucination detection module addresses the most critical failure modes.