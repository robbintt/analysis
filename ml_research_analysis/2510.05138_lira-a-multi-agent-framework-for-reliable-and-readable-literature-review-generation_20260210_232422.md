---
ver: rpa2
title: 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review
  Generation'
arxiv_id: '2510.05138'
source_url: https://arxiv.org/abs/2510.05138
tags:
- review
- literature
- lira
- reviews
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiRA is a multi-agent framework for automated literature review
  generation that addresses the challenge of maintaining comprehensive, up-to-date
  systematic reviews amid growing scientific publications. The framework employs specialized
  agents for outlining, subsection writing, editing, and reviewing to produce cohesive
  review articles while emphasizing readability and factual accuracy.
---

# LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation

## Quick Facts
- **arXiv ID**: 2510.05138
- **Source URL**: https://arxiv.org/abs/2510.05138
- **Reference count**: 28
- **Primary result**: Multi-agent framework for automated literature review generation with SME-preferred structural coherence and competitive ROUGE scores

## Executive Summary
LiRA addresses the challenge of maintaining comprehensive, up-to-date systematic literature reviews amid growing scientific publications. The framework employs specialized agents for outlining, subsection writing, editing, and reviewing to produce cohesive review articles while emphasizing readability and factual accuracy. Evaluated on SciReviewGen and ScienceDirect datasets, LiRA outperformed baselines like AutoSurvey and MASS-Survey across multiple metrics, achieving ROUGE scores of 0.13 (SciReviewGen) and 0.13 (ScienceDirect), with citation quality F1-scores of 0.76 and 0.73 respectively. SME evaluations showed LiRA's structural coherence was preferred over baselines while maintaining competitive coverage.

## Method Summary
The framework uses a multi-agent architecture where each agent specializes in a specific task: OutlineAgent generates hierarchical section structures from literature, SubSectionAgent writes individual subsections, EditAgent refines generated content for readability and accuracy, and ReviewAgent performs quality checks and feedback integration. The system leverages Llama 3.2 3B and Qwen2.5 3B models, employing a search-then-rerank pipeline for document retrieval from arXiv and Semantic Scholar. Each agent follows detailed prompt templates with examples and follows a iterative review-edit-review cycle to ensure quality. The framework demonstrates robustness in real-world retrieval scenarios and claims no domain-specific tuning is required.

## Key Results
- LiRA achieved ROUGE-1/L scores of 0.13/0.05 on SciReviewGen and 0.13/0.05 on ScienceDirect datasets
- Citation quality F1-scores reached 0.76 (SciReviewGen) and 0.73 (ScienceDirect)
- SME evaluations showed LiRA's structural coherence was preferred over baselines while maintaining competitive coverage
- The framework outperformed AutoSurvey and MASS-Survey baselines across multiple evaluation metrics

## Why This Works (Mechanism)
The multi-agent architecture enables specialization where each component focuses on a specific aspect of review generation - outlining, writing, editing, and quality checking. This division of labor allows for more focused prompt engineering and error correction cycles. The iterative review-edit-review process creates multiple quality checkpoints, while the use of separate models for different tasks prevents catastrophic forgetting and maintains task-specific capabilities. The framework's emphasis on readability alongside factual accuracy addresses the common weakness of AI-generated reviews being either too technical or lacking depth.

## Foundational Learning
- **Multi-agent coordination**: Why needed - Enables specialization and parallel processing; Quick check - Can agents work independently without conflicts
- **Iterative quality control**: Why needed - Catches errors at multiple stages; Quick check - Does each review cycle improve output quality
- **Citation verification**: Why needed - Ensures factual accuracy in scientific claims; Quick check - Are all claims properly attributed to source papers
- **Readability assessment**: Why needed - Makes reviews accessible to broader audience; Quick check - Can non-experts understand the generated content
- **Domain adaptation**: Why needed - Scientific reviews vary across fields; Quick check - Does framework work across different scientific domains
- **Long-form coherence**: Why needed - Reviews require logical flow across sections; Quick check - Do subsections connect logically

## Architecture Onboarding

**Component Map**: arXiv/SS retrieval -> OutlineAgent -> SubSectionAgent -> EditAgent -> ReviewAgent -> ReviewAgent feedback -> SubSectionAgent refinement -> final integration

**Critical Path**: Document retrieval → Outline generation → Subsection writing → Editing → Quality review → Output generation

**Design Tradeoffs**: Uses smaller 3B models for cost efficiency vs larger models for potentially better quality; prioritizes readability over technical depth; emphasizes automated metrics vs human-only evaluation

**Failure Signatures**: 
- Retrieval failures lead to incomplete topic coverage
- OutlineAgent errors cause structural incoherence
- SubSectionAgent mistakes create factual inaccuracies
- EditAgent limitations reduce readability improvements
- ReviewAgent oversights allow quality issues to persist

**First Experiments**:
1. Test framework on single-topic review generation with known ground truth
2. Evaluate component isolation by running each agent independently
3. Measure retrieval effectiveness by comparing retrieved vs. all relevant papers

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though it acknowledges limitations regarding automated metric sufficiency and the need for more diverse domain testing.

## Limitations
- Heavy reliance on automated metrics (ROUGE, citation scores) may not fully capture nuanced review quality
- Claims of "no domain-specific tuning" require validation across diverse scientific fields
- Does not address handling of controversial or rapidly evolving research areas where consensus hasn't formed

## Confidence
- Multi-agent architecture effectiveness: High
- Domain generalization: Medium
- SME evaluation validity: Medium

## Next Checks
1. Conduct cross-domain evaluation: Test LiRA's performance on literature reviews from at least three distinct scientific fields (e.g., biology, computer science, social sciences) to validate true domain generalization claims.
2. Longitudinal consistency analysis: Generate reviews on the same topics at different time intervals (e.g., 6-month gaps) to assess the framework's ability to maintain consistency while incorporating new research.
3. Error propagation study: Systematically introduce controlled errors at different agent levels to measure how robustly the review-editing-review cycle catches and corrects mistakes, quantifying the effectiveness of the multi-agent quality control mechanism.