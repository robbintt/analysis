---
ver: rpa2
title: Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning
  Models
arxiv_id: '2509.25402'
source_url: https://arxiv.org/abs/2509.25402
tags:
- search
- parallel
- state
- edge
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PACHS, a parallel best-first search framework
  that leverages both actor and critic networks from trained SAC models for efficient
  inference. By integrating the actor for action generation and the critic as a learned
  heuristic, PACHS enables multi-step reasoning and backtracking during deployment,
  addressing limitations of standard RL rollout strategies.
---

# Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models

## Quick Facts
- arXiv ID: 2509.25402
- Source URL: https://arxiv.org/abs/2509.25402
- Reference count: 32
- Primary result: PACHS achieves 100% success rates across collision-free motion planning and contact-rich manipulation tasks while significantly improving generalization and closed-loop execution performance compared to single and parallel rollout baselines.

## Executive Summary
This work introduces PACHS, a parallel best-first search framework that leverages both actor and critic networks from trained SAC models for efficient inference. By integrating the actor for action generation and the critic as a learned heuristic, PACHS enables multi-step reasoning and backtracking during deployment, addressing limitations of standard RL rollout strategies. The algorithm employs multi-level parallelization—batch-level GPU processing for neural network inference and thread-level CPU distribution for search expansion—making it computationally practical. Experimental results across collision-free motion planning and contact-rich manipulation tasks demonstrate PACHS achieves 100% success rates in multiple environments while significantly improving generalization, especially in obstacle scenarios not seen during training. The approach shows substantial improvements in closed-loop execution performance compared to single and parallel rollout baselines.

## Method Summary
PACHS is a parallel best-first search framework that uses trained actor-critic SAC models for deployment-time inference. The actor network generates continuous actions while the critic provides cost-to-go estimates to guide the search. The algorithm employs a two-level parallelization strategy: GPU-batched neural network inference for actor action generation and critic Q-value computation, combined with multi-threaded CPU distribution for graph expansion and edge evaluation. The search uses a priority function f(e) = g(s) + w·Q_φ(s,a) where Q-values guide the search toward promising action branches. The method is evaluated on robotic manipulation tasks including Panda-Shelf collision-free motion planning and Push-T object manipulation across multiple variants.

## Key Results
- PACHS achieves 100% success rates across all tested environments including Panda-Shelf and Push-T variants
- Significant improvements in generalization, with PACHS maintaining ~85% success in obstacle scenarios versus ~50% for parallel rollout baselines
- Closed-loop execution success rates substantially exceed rollout baselines (93% → 100% in PushT-Fixed, 81% → 100% in PushT-Rand)
- PACHS uses fewer evaluated nodes per expanded node compared to search using state-only heuristics

## Why This Works (Mechanism)

### Mechanism 1: Critic Network as Learned Heuristic Function
The critic network's Q-values provide action-specific priority estimates that guide search more effectively than state-only heuristics. The priority function f(e) = g(s) + w·Q_φ(s,a) incorporates the critic's estimate of expected cumulative cost-to-go for each specific action, creating differentiated priorities that focus search on promising action branches before expensive edge evaluation.

### Mechanism 2: Actor-Sampled Continuous Action Spaces Replace Hand-Crafted Primitives
The actor network generates domain-appropriate continuous actions that implicitly encode learned dynamics and task structure. Instead of discrete motion primitives, the actor π_θ(s) samples actions from a learned distribution conditioned on the policy's learned representation of robot-object interactions, dynamics, and environmental constraints.

### Mechanism 3: Two-Level Parallelization (GPU Batches + CPU Threads)
Decoupling neural inference (GPU-batched) from graph expansion (CPU-threaded) makes search with learned modules computationally practical. Worker threads handle edge evaluation while GPU batches process actor action generation and critic Q-value computation. Lock-free expensive operations plus locked data structure updates prevent race conditions while maximizing throughput.

## Foundational Learning

- **Best-First Search (A*) and Priority Functions**: Understanding standard f=g+h formulation is prerequisite since PACHS modifies A* priority functions to use learned Q-values. Quick check: Can you explain why admissible heuristics guarantee optimality in A*, and what happens when heuristics are weighted (w > 1)?
- **Actor-Critic Architecture and Q-Functions**: The method requires understanding what Q_φ(s,a) represents and how it differs from V(s) or policy outputs. Quick check: In SAC, what does Q(s,a) estimate, and why does the critic train separately from the actor?
- **Parallel Search Data Structures (OPEN/CLOSED Lists)**: Thread-safe manipulation of shared priority queues is central to the parallelization strategy. Quick check: What race conditions can occur when multiple threads pop from and insert into a shared priority queue without locks?

## Architecture Onboarding

- **Component map**: Plan() [Main Thread] -> ExpandEdgeThread() [Worker Threads] -> ExpandState() (actor+critic) or ExpandEdge() (simulation) -> OPEN list updates
- **Critical path**: Insert start state as dummy edge → Loop: pop highest-priority edge → if dummy, expand state (actor+critic); if real, evaluate edge (simulation) → On goal satisfaction, backtrack parent pointers
- **Design tradeoffs**: Heuristic weight w (higher w increases greediness but weakens optimality bounds), action batch size (larger batches improve GPU utilization), thread count (more threads help but increase lock contention), planning budget vs. execution horizon
- **Failure signatures**: OPEN list exhaustion without goal reach (search space disconnected), all edges from state have similar f-values (critic not differentiating actions), planning time dominated by lock waits (too many threads or small batches), closed-loop success much lower than planning success (sim-to-real gap)
- **First 3 experiments**: 1) Validate baseline equivalence: Run single rollout vs. PACHS on PushT-Fixed—confirm PACHS achieves ≥ rollout success rate. 2) Ablate priority function: Compare f(e) = g(s) + w·Q(s,a) against f(e) = g(s) + w·h(s)—expect fewer edges evaluated with full Q-values. 3) Test generalization: Train SAC in obstacle-free environment, deploy PACHS in obstacle environment—expect PACHS to maintain higher success rates.

## Open Questions the Paper Calls Out

- **Can PACHS maintain its generalization and planning improvements during sim-to-real transfer?** The paper plans to deploy PACHS to real robots but all experiments were conducted in simulation environments without physical validation.

- **Can formal theoretical guarantees be derived for PACHS when using potentially inadmissible learned critics?** The algorithm relies on a learned critic that is typically overestimating and inadmissible, so standard optimality guarantees do not strictly apply.

- **How can additional heuristics be effectively integrated with the actor-critic architecture?** The paper proposes to investigate additional heuristics but does not explore hybrid approaches combining learned modules with domain-specific or hand-crafted heuristics.

## Limitations

- The paper does not specify critical hyperparameters including heuristic weight w, number of actions sampled per state, or batch size for GPU inference.
- Generalization results in obstacle scenarios rely on a single obstacle configuration, limiting claims about robustness.
- Simulation environment details are underspecified, particularly regarding edge evaluation costs and collision checking granularity.

## Confidence

- **High**: PACHS framework architecture and parallelization strategy; improved closed-loop execution success rates vs. rollout baselines.
- **Medium**: Claims about fewer edges evaluated per expanded node due to critic-guided search; effectiveness of actor-sampled actions vs. handcrafted primitives.
- **Low**: Quantitative generalization claims without ablation on critic weight w or actor sampling diversity; scalability to more complex environments.

## Next Checks

1. **Ablate heuristic weight w**: Sweep w ∈ [1,3,5] in PushT-Obs and measure edges evaluated per expanded node and success rate to quantify critic contribution.
2. **Ablate action sampling**: Compare PACHS with actor sampling (N=8) vs. uniform random actions—test whether learned actions provide meaningful search guidance.
3. **Test sim-to-real transfer**: Deploy trained models and PACHS in a noisy physics simulator to assess robustness beyond ideal simulation.