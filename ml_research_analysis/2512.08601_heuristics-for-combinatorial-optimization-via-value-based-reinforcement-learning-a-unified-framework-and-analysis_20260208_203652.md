---
ver: rpa2
title: 'Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning:
  A Unified Framework and Analysis'
arxiv_id: '2512.08601'
source_url: https://arxiv.org/abs/2512.08601
tags:
- such
- which
- theorem
- problem
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a unified theoretical framework linking
  combinatorial optimization (CO) problems to Markov decision processes (MDPs) and
  analyzes value-based reinforcement learning methods for solving them. The key contributions
  include: CO-to-MDP Translation: A general framework translating CO problems to equivalent
  undiscounted MDPs using Karp-Held theory, with conditions ensuring convergence of
  value iteration to optimal solutions.'
---

# Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis

## Quick Facts
- arXiv ID: 2512.08601
- Source URL: https://arxiv.org/abs/2512.08601
- Reference count: 40
- Primary result: Establishes theoretical framework linking CO problems to MDPs with convergence guarantees for fitted value iteration

## Executive Summary
This paper presents a unified theoretical framework connecting combinatorial optimization problems to Markov decision processes, enabling the application of value-based reinforcement learning methods to CO. The authors establish conditions under which finite CO problems can be formulated as equivalent undiscounted MDPs using Karp-Held theory, and analyze fitted value iteration with sample average approximation to provide sample and iteration complexity bounds. The framework separates approximation error (from hypothesis class restriction) from estimation error (from sampling), offering theoretical justification for the success of deep Q-learning in CO while quantifying its limitations through error propagation analysis.

## Method Summary
The method translates combinatorial optimization problems into undiscounted MDPs using Karp-Held equivalence relations, then applies fitted value iteration with sample average approximation. States are defined as equivalence classes of partial solutions under a right congruence relation, with deterministic transitions inherited from the CO structure. The value function is approximated using a parametric family VΘ (e.g., affine functions of state embeddings), and each iteration solves a sample average approximation problem using projected gradient descent. The framework provides theoretical guarantees on convergence and optimality gap bounds when the projected Bellman operator is contractive under τ-weighted norms.

## Key Results
- CO-to-MDP translation framework with convergence guarantees for undiscounted MDPs under specific equivalence relation conditions
- Error decomposition showing fitted value iteration's total error separates into approximation error and estimation error, enabling separate analysis
- Sample complexity bounds showing O(|S|) samples needed for small error tolerance, effectively lower bounding estimation error
- Empirical validation on Knapsack and TSP problems demonstrating affine approximation schemes satisfy contraction assumptions in practice

## Why This Works (Mechanism)

### Mechanism 1: CO-MDP Equivalence via Karp-Held Translation
Finite combinatorial optimization problems can be translated into equivalent undiscounted MDPs whose optimal value function yields optimal CO solutions. The translation defines states as equivalence classes of partial solutions under a right congruence relation, with the Bellman operator becoming a contraction under τ-weighted ℓ∞ norm due to the layer structure forcing convergence despite no discounting. The core assumption requires the equivalence relation ∼ to be a right congruence of finite rank refining ∼Π, with s∞ not refined. If the CO problem has infinite feasible solutions or the equivalence relation fails to satisfy right congruence, the translation collapses.

### Mechanism 2: Approximation-Estimation Error Separation
Fitted value iteration's total error decomposes into approximation error (from restricting the hypothesis class VΘ) and estimation error (from finite-sample SAA), enabling separate analysis. This works by interpreting FVI as projected value iteration where Πσ_ΘB projects Bellman updates onto VΘ. Under the assumption that Πσ_ΘB is a γ-contraction in τ-weighted ∞-norm, the PVI fixed point satisfies bounds isolating approximation error, while SAA analysis bounds estimation error via sub-Gaussian concentration. The decomposition breaks if Πσ_ΘB is not contractive, as is known to fail for general MDPs.

### Mechanism 3: Deterministic Transitions Enable FVI Over FQI
For CO-derived MDPs with deterministic transitions, fitted value iteration on state-value functions suffices, avoiding the larger state-action space of fitted Q-iteration. This works because deterministic p(s'|s,a) = δ_{s',λ(s,a)} makes the expectation and maximization operators commute, collapsing the bias-variance decomposition so the loss function simplifies without the cross-term present in stochastic MDPs. This reduction fails if stochastic transitions are introduced, such as for robustness modeling.

## Foundational Learning

- **Markov Decision Processes and Bellman Operators**: Essential for understanding the entire framework translation and convergence proofs. Quick check: Can you explain why the undiscounted Bellman operator fails to be a contraction in sup-norm but succeeds under τ-weighted norms for proper-policy MDPs?

- **Sample Average Approximation (SAA) and Concentration**: Critical for understanding the estimation error analysis relying on SAA consistency theory and sub-Gaussian tail bounds. Quick check: Given n samples from σ, what rate does SAA achieve for minimizing f_t(θ) = E_σ[(BṼ_θ_t(s) - V_θ(s))²]?

- **Projected Gradient Descent and Convex Optimization**: Necessary for understanding the assumption that PGD solves each FVI iteration's SAA subproblem and the iteration complexity analysis. Quick check: If Θ has diameter D_Θ and loss is κ*-Lipschitz, how many PGD steps ensure ε-suboptimality?

## Architecture Onboarding

- **Component map**: CO Problem Instance (A, Π, g) → Equivalence Relation ∼ → State space S = A*/∼ → MDP Construction (transitions λ, rewards r) → Approximation Scheme VΘ (feature embedding ϕ, parametrized v) → FVI Loop (sample states, compute targets, fit via SAA+PGD)

- **Critical path**: (1) Verify CO problem satisfies Assumption 3 (extendability); (2) Construct right congruence ∼ satisfying Assumption 4; (3) Design compact embedding ϕ with sufficient expressivity; (4) Validate Assumption 6 empirically (contractive Πσ_ΘB); (5) Run FVI with increasing n_t, c_t per Theorem 4.11

- **Design tradeoffs**:
  - Coarse vs. fine ∼: Coarse equivalence yields smaller |S| (faster VI) but may violate Theorem 3.5(ii); fine equivalence increases state space exponentially
  - Low vs. high K: Smaller K improves contraction probability but increases approximation error ∥V* - Πσ_ΘV*∥
  - Exact vs. approximate PGD: Exact optimization reduces estimation error but is often intractable; approximate PGD introduces residual error captured in Theorem 4.11's bound

- **Failure signatures**:
  - Non-convergent PVI: γ ≥ 1 in Eq. (45)—suggests embedding or τ weights violate Assumption 6
  - Infeasible greedy extraction: Greedy sequence (29) hits s∞ prematurely—indicates V approximation has large errors on critical states or Assumption 5 is violated
  - Sample explosion: n_t = O(|S|) for small ε_t—practical limit on achievable optimality gap

- **First 3 experiments**:
  1. Validate Assumption 6 on synthetic KSP/TSP: Implement affine scheme, sample random σ, τ, θ_0; compute γ via Eq. (45); verify γ < 1 in >95% of runs
  2. Measure optimality gap vs. ε: Run FVI with decreasing ε_t; extract greedy solution; compare objective to exact solver; verify linear relationship with d_Π·ε
  3. Ablate embedding dimension K: For fixed (COP, d), compare relative optimality gap for K = d/2 vs. K = d; expect no significant difference due to O(|S|/K) dominance

## Open Questions the Paper Calls Out

### Open Question 1
Can the unified framework be extended to analyze generalization performance across parameterized reward functions c ∈ C, rather than for fixed problem instances? The current theoretical analysis assumes a fixed parameter realization c to establish convergence and optimality bounds. What evidence would resolve it: Theoretical bounds on the optimality gap when the value function is trained on a distribution of parameters and tested on unseen instances.

### Open Question 2
Does focusing exploration-exploitation on high-value states mitigate the exponential explosion in sample size required to decrease the estimation error? The provided sample complexity bounds rely on the weighted infinity norm over the entire state space, necessitating large sample sizes (O(|S|)) as error tolerance decreases. What evidence would resolve it: Derivation of refined sample complexity bounds that depend on the sparsity or visitation frequency of states under an optimal or near-optimal policy.

### Open Question 3
How does the contraction modulus γ of the projected Bellman operator depend on the problem input size (dimension d) and the state-embedding ratio |S|/K? While convergence is guaranteed if γ < 1, a global bound on γ relative to input scaling is required to guarantee polynomial runtime complexity, which is currently missing. What evidence would resolve it: Empirical scaling laws or theoretical bounds linking γ to d for specific problem classes.

## Limitations
- Framework applicability constrained by requirement for deterministic transitions in CO-derived MDPs, limiting extension to stochastic optimization problems
- Exponential state space growth remains a fundamental barrier despite compact embeddings
- Affine approximation scheme's success depends critically on specific structure of Karp-Held equivalence relations, with no guarantees for arbitrary CO problems

## Confidence
- CO-MDP Translation Theory (High): Theorems 3.5 and 3.8 provide rigorous conditions with constructive proofs
- Error Decomposition Framework (Medium): Novel theoretical contribution but relies on unverified Assumption 6 across problem classes
- Empirical Validation (Low-Medium): Limited to KSP and TSP with small problem sizes (d ≤ 18), no comparison to exact solvers or alternative heuristics

## Next Checks
1. Test Assumption 6 contraction across diverse CO problems beyond KSP/TSP (e.g., Vehicle Routing, Set Cover) to assess framework generality
2. Implement exact solver baseline for small instances to quantify optimality gap distribution rather than single-point comparisons
3. Evaluate computational complexity scaling beyond d=18 to identify practical limits of the Karp-Held translation approach