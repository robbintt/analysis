---
ver: rpa2
title: Gradient flow in parameter space is equivalent to linear interpolation in output
  space
arxiv_id: '2408.01517'
source_url: https://arxiv.org/abs/2408.01517
tags:
- gradient
- flow
- space
- output
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves that the standard gradient flow in parameter space
  underlying many training algorithms in deep learning can be continuously deformed
  into an adapted gradient flow that yields constrained Euclidean gradient flow in
  output space. The authors show that for the L2 loss, if the Jacobian of the outputs
  with respect to the parameters is full rank, then the time variable can be reparametrized
  so that the resulting flow is simply linear interpolation, and a global minimum
  can be achieved.
---

# Gradient flow in parameter space is equivalent to linear interpolation in output space

## Quick Facts
- arXiv ID: 2408.01517
- Source URL: https://arxiv.org/abs/2408.01517
- Reference count: 4
- Key outcome: Standard gradient flow in parameter space can be continuously deformed into an adapted gradient flow yielding constrained Euclidean gradient flow in output space, with time reparametrization producing linear interpolation for L2 loss.

## Executive Summary
This work establishes a fundamental connection between gradient flow in parameter space and linear interpolation in output space for neural network training. The authors prove that under a full-rank condition on the Jacobian of outputs with respect to parameters, the standard gradient flow can be continuously deformed into an adapted gradient flow that simplifies to linear interpolation in output space after appropriate time reparametrization. For the L2 loss, this means gradient flow achieves global minimum through simple linear interpolation when the Jacobian remains full rank. For cross-entropy loss with softmax, the authors derive an explicit formula for the unique global minimum when labels have positive components. These results highlight the critical role of the Jacobian rank (equivalently, the neural tangent kernel) in determining optimization dynamics.

## Method Summary
The authors analyze gradient flow dynamics by comparing standard gradient flow in parameter space with an "adapted" gradient flow that uses the Moore-Penrose pseudoinverse of the Jacobian as a preconditioner. They establish a homotopy equivalence between these flows, prove that under full-rank Jacobian conditions the output space dynamics reduce to Euclidean gradient flow, and show that for L2 loss this can be reparametrized to yield exact linear interpolation. For cross-entropy loss with softmax, they characterize the dynamics on invariant affine hyperplanes and derive explicit formulas for global minima. The analysis assumes smooth activations and overparameterization (more parameters than training points).

## Key Results
- Standard gradient flow and adapted gradient flow in parameter space share identical critical points and are homotopy-equivalent via a one-parameter family of vector fields
- For L2 loss with full-rank Jacobian, time reparametrization transforms gradient flow in output space into exact linear interpolation between initial and target outputs
- For cross-entropy loss with softmax, gradient flow is constrained to invariant affine hyperplanes and converges to an explicit global minimum when labels have strictly positive components
- The rank of the Jacobian (or equivalently, the neural tangent kernel) fundamentally determines whether output space flow is Euclidean or sub-Riemannian

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard gradient flow and adapted gradient flow in parameter space share identical critical points and are homotopy-equivalent.
- Mechanism: A one-parameter family of vector fields V_{θ,α} = -A_{θ,α}∇_θC interpolates between -∇_θC (α=0) and -(D^T D)^+∇_θC (α=1). The matrix A_{θ,α} = α(D^T D)^+ + (1-α)I_{K×K} remains positive-definite for α∈[0,1), and at α=1, ker((D^T D)^+) = ker((D)^+D) ensures singularities are preserved.
- Core assumption: Lipschitz continuous derivatives of x(θ); overparameterization (K ≥ QN) when discussing adapted flow.
- Break condition: If the Jacobian D loses rank, the constrained flow operates on a sub-Riemannian structure rather than full Euclidean space; equilibrium characterization requires projection P_{range(DD^T)}.

### Mechanism 2
- Claim: Under full-rank Jacobian and L² loss, time reparametrization transforms gradient flow in output space into exact linear interpolation between initial and target outputs.
- Mechanism: Euclidean gradient flow ∂_s x = -∇_x C = (1/N)(x(s) - y) has solution x(s) = y + e^{-s/N}(x_0 - y). Setting t = 1 - e^{-s/N} yields ẋ(t) = -(1/(1-t))(x(t) - y), solved by x(t) = y + (1-t)(x_0 - y).
- Core assumption: rank(D[θ(s)]) = QN ≤ K for all s along the trajectory; y must be in the image of x.
- Break condition: If rank(D) < QN, deviation from linear interpolation occurs via explicit formula involving projection onto orthogonal complement of range(DD^T).

### Mechanism 3
- Claim: For cross-entropy loss with softmax, gradient flow is constrained to invariant affine hyperplanes H_c = {z : z·u_Q = c} and converges to an explicit global minimum when labels have strictly positive components.
- Mechanism: The Hessian D²ℓ|_z = diag(σ(z)) - σ(z)σ(z)^T has rank Q-1 and kernel spanned by u_Q = (1,...,1)^T, making the loss convex on each hyperplane H_c. The equilibrium σ(f*) = y has explicit solution f*_{c,j} = log y_j + (1/Q)∑_i(f_0(x_i) - log y_i).
- Core assumption: Labels y_j > 0 for all j; this excludes one-hot encoding and requires positive probability assignments to all classes.
- Break condition: If any y_j = 0, equilibrium σ(f*) = y is unattainable; gradient flow cannot converge to a finite critical point within H_c.

## Foundational Learning

- Concept: Jacobian matrix D[θ] ∈ R^{QN×K} and Neural Tangent Kernel Θ = DD^T
  - Why needed here: The rank of D determines whether output space flow is Euclidean (full rank) or sub-Riemannian (rank-deficient); NTK eigenvalues govern optimization geometry.
  - Quick check question: Given a network with K=10⁶ parameters and N=10⁴ training points with Q=10 classes, is the Jacobian likely full rank at initialization?

- Concept: Moore-Penrose pseudoinverse (D)^+ and projection operators P_{range(D)}
  - Why needed here: The adapted gradient flow uses (D^T D)^+ as a preconditioner; understanding P_{range(D)}v = D(D)^+v is essential for interpreting constrained dynamics.
  - Quick check question: If D ∈ R^{m×n} with m < n and rank(D) = m, what is the simplified form of (D)^+?

- Concept: Time reparametrization of ODEs
  - Why needed here: The key result that gradient flow ≡ linear interpolation relies on the substitution t = 1 - e^{-s/N}; this technique converts asymptotic convergence to finite-time arrival.
  - Quick check question: Under reparametrization t = φ(s), how does ∂_t x transform in terms of ∂_s x?

## Architecture Onboarding

- Component map: D(s) = D[θ(s)] -> (D)^+, (D^T D)^+ -> P_{range(DD^T)} -> H_c
- Critical path:
  1. Initialize θ₀ such that D[θ₀] has full rank (random initialization typically suffices for overparameterized networks)
  2. For adapted flow: compute ∂_s θ = -(D^T D)^+∇_θ C at each step; verify P_{range(DD^T)}∇_xC ≈ ∇_xC
  3. For L² loss: track deviation from linear interpolation; if deviation grows, rank loss is occurring
- Design tradeoffs:
  - Standard gradient flow: O(K) memory, simple implementation, but output dynamics coupled through DD^T metric
  - Adapted gradient flow: Trivialized output dynamics, but requires pseudoinverse computation (expensive for large K) and full-batch Jacobian
  - Assumption: Full-rank maintenance may require explicit regularization or architecture choices
- Failure signatures:
  - rank(D) < QN: Deviation from linear interpolation (L²) or convergence to non-interpolating critical point
  - Zero label components (cross-entropy): Flow diverges; f_j(x) → -∞ for classes with y_j = 0
  - Jacobian rank drop mid-training: ∂_s x exits range(D); prescribed paths become infeasible
- First 3 experiments:
  1. Rank monitoring: On a small MLP (K ~ 10⁴, QN ~ 10²), track rank(D) throughout training via SVD; verify full rank correlates with monotonic loss decrease.
  2. Linear interpolation verification: For L² loss on synthetic data, compare x(s(t)) against (1-t)x₀ + ty; measure deviation ∥x(s(t)) - ((1-t)x₀ + ty)∥².
  3. Cross-entropy hyperplane constraint: Train with label smoothing (y_j ≥ ε > 0) vs. one-hot; verify convergence to explicit f*_{c} formula and measure final loss ∝ -y·log y.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the homotopy equivalence and linear interpolation framework be meaningfully extended to probability manifolds within information geometry?
- Basis in paper: The authors state: "Whether the ideas presented here can be meaningfully extended to this setting remains an open question, which we leave for future investigation" regarding gradient flows on probability manifolds.
- Why unresolved: The current analysis treats outputs as unconstrained vectors in $\mathbb{R}^Q$, but probability outputs lie on the simplex, requiring different geometric structures.
- What evidence would resolve it: A proof that the homotopy equivalence extends to manifolds with Fisher-Rao or related metrics, or a counterexample showing fundamental obstructions.

### Open Question 2
- Question: How do the rank properties of the Jacobian and achievable output paths depend on parameter initialization, even for fixed initial outputs?
- Basis in paper: Remark 3.1 notes that "properties of the Jacobian matrix $D[\theta(s)]$ that determine the dynamics of $x(s)$, such as rank and the subspace $\text{range}(DD^T)$, might depend on the initialization even within a class $\{\theta_0 : x(\theta_0) = x_0\}$."
- Why unresolved: Multiple parameter configurations can yield the same initial output but different local geometries, yet the dependence is not characterized.
- What evidence would resolve it: Theoretical characterization or empirical study of how rank varies across the fiber $x^{-1}(x_0)$ for typical network architectures.

### Open Question 3
- Question: What are the convergence properties of the cross-entropy gradient flow when labels have zero components (one-hot encoding)?
- Basis in paper: The explicit global minimum formula (Proposition 2.8) requires $y_j > 0$ for all $j$. Footnote 2 notes one-hot encoding is common but leads to different behavior not analyzed here.
- Why unresolved: When $y_j = 0$, the equilibrium condition $\sigma(f^*(x)) = y$ requires $f_j^*(x) \to -\infty$, which is unbounded.
- What evidence would resolve it: Characterization of asymptotic convergence rates and limiting behavior for one-hot labels, potentially involving barrier functions or constrained analysis.

### Open Question 4
- Question: Do the homotopy equivalence results persist under discrete-time gradient descent with finite learning rates?
- Basis in paper: All analysis concerns continuous-time gradient flow; discretization effects are not addressed despite practical algorithms using discrete steps.
- Why unresolved: Discretization introduces stability constraints and can cause divergence, oscillation, or convergence to different equilibria than the continuous flow.
- What evidence would resolve it: Bounds on learning rates that preserve the homotopy equivalence property, or analysis of discretization error in the output space trajectories.

## Limitations
- The full-rank Jacobian condition is critical but difficult to verify and maintain in practice for large networks
- Pseudoinverse computation (DᵀD)⁺ becomes prohibitively expensive for modern deep networks with millions of parameters
- The analysis assumes smooth activations, which may not hold for typical neural network activation functions like ReLU without approximation

## Confidence
- **High confidence**: The mathematical derivations for time reparametrization yielding linear interpolation are rigorous and follow standard ODE techniques
- **Medium confidence**: The homotopy equivalence between standard and adapted gradient flows is theoretically valid but requires Lipschitz smoothness that may not hold for typical neural network activations
- **Low confidence**: Practical applicability to large-scale deep learning, as the rank condition and pseudoinverse computation present significant barriers not addressed in the paper

## Next Checks
1. **Rank stability monitoring**: Implement systematic rank tracking of D[θ] throughout training across different architectures (MLPs, CNNs) and initialization schemes to identify when/how rank drops occur
2. **Computational scaling analysis**: Benchmark pseudoinverse computation costs versus standard gradient descent for networks of increasing size (K = 10², 10³, 10⁴ parameters) to establish practical limits
3. **Smooth activation impact**: Compare trajectories using exact ReLU versus smooth approximations (Softplus) to quantify how activation non-smoothness affects the validity of the main theorems