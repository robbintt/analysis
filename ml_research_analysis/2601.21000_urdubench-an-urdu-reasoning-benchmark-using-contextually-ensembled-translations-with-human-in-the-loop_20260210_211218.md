---
ver: rpa2
title: 'UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations
  with Human-in-the-Loop'
arxiv_id: '2601.21000'
source_url: https://arxiv.org/abs/2601.21000
tags:
- urdu
- translation
- reasoning
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UrduBench, a benchmark suite for evaluating
  Urdu reasoning capabilities in large language models (LLMs). The authors propose
  a context-aware ensemble translation framework with human-in-the-loop validation
  to translate four widely-used reasoning datasets (MGSM, MATH-500, CommonSenseQA,
  and OpenBookQA) into high-quality Urdu while preserving contextual and structural
  integrity.
---

# UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop

## Quick Facts
- arXiv ID: 2601.21000
- Source URL: https://arxiv.org/abs/2601.21000
- Reference count: 40
- This paper introduces UrduBench, a benchmark suite for evaluating Urdu reasoning capabilities in large language models (LLMs)

## Executive Summary
This paper introduces UrduBench, a benchmark suite for evaluating Urdu reasoning capabilities in large language models (LLMs). The authors propose a context-aware ensemble translation framework with human-in-the-loop validation to translate four widely-used reasoning datasets (MGSM, MATH-500, CommonSenseQA, and OpenBookQA) into high-quality Urdu while preserving contextual and structural integrity. The study evaluates multiple reasoning-oriented and instruction-tuned LLMs across various prompting strategies, revealing that multi-step and symbolic reasoning tasks pose significant challenges in Urdu.

## Method Summary
The authors developed UrduBench by translating four English reasoning datasets into Urdu using a context-preserving ensemble translation framework. The method concatenates question stems with all answer choices before translation, then uses four translation systems (IndicTrans2, NLLB, Qwen-3-30B, Gemini-2.5-Pro) followed by GPT-5.1 synthesis and human validation. The benchmark evaluates 10 LLMs across Direct, Chain-of-Thought, and Few-Shot + CoT prompting strategies, measuring both accuracy and language consistency (Urdu-only vs. code-switching).

## Key Results
- Context-preserving translation prevents semantic fragmentation in multiple-choice benchmarks
- Higher Urdu language consistency correlates with improved reasoning accuracy, particularly on MGSM under Chain-of-Thought prompting
- Larger models with multilingual pretraining (Gemma-3-12B-it) generally perform better, but model size alone is insufficient without targeted multilingual coverage

## Why This Works (Mechanism)

### Mechanism 1: Context-Preserving Translation Ensemble
Translating question-choice pairs as unified units reduces semantic fragmentation in multiple-choice benchmarks. Four translation candidates generate independent translations; GPT-5.1 synthesizes best features; native annotators select final output. Questions and choices are concatenated pre-translation, then programmatically separated. Ensemble diversity + human validation compensates for individual translator weaknesses in low-resource settings.

### Mechanism 2: Language Consistency as Reasoning Prerequisite
Higher target-language adherence (lower code-switching) correlates with improved reasoning accuracy in low-resource languages. Models that maintain Urdu-only outputs during Chain-of-Thought reasoning show higher accuracy on MGSM; code-switching disrupts intermediate computation coherence. Language mixing reflects underlying representation instability rather than intentional multilingual reasoning strategies.

### Mechanism 3: Multilingual Pretraining × Task Complexity Interaction
Reasoning-oriented distillation transfers partially to low-resource languages, but multilingual grounding determines commonsense reasoning performance. Reasoning-specialized models show gains on MGSM arithmetic but instruction-tuned models with multilingual exposure outperform on CommonsenseQA/OpenBookQA. Arithmetic reasoning patterns are more language-agnostic than commonsense knowledge retrieval.

## Foundational Learning

- **Context-Preserving Translation**: Independent translation of question-choice pairs causes grammatical misalignment. Quick check: Given a translated question stem, can you predict whether choices were translated with or without context by examining grammatical agreement?

- **Language Consistency / Code-Switching Detection**: Measuring whether models stay in-target-language is essential for diagnosing reasoning failures in multilingual settings. Quick check: If a model switches to English mid-reasoning on an Urdu math problem, where in the computation chain does accuracy typically degrade?

- **Difficulty-Level Stratification**: MATH-500 L1-L5 reveals that some models retain performance at high difficulty while others collapse. Quick check: Why might a 4B reasoning-specialized model outperform a 7B general model on L5 problems but not L1 problems?

## Architecture Onboarding

- **Component map**: Dataset selection → context-aware concatenation → parallel translation → GPT-5.1 fusion → heuristic validation → human annotation → model inference → accuracy + consistency metrics

- **Critical path**: 1) Dataset selection → context-aware concatenation → parallel translation → ensemble fusion; 2) Heuristic filtering (empty outputs, missing fields, excessive English ratio >0.3); 3) Human validation with rubric (Table 7: completeness, grammar, cultural equivalence); 4) Model inference with fixed prompts (Figure 5) → accuracy + consistency metrics

- **Design tradeoffs**: Multi-translator ensemble increases cost/latency but reduces single-point-of-failure risk; GPT-5.1 as fusion judge introduces dependency on proprietary model; human validation is bottleneck but essential for low-resource quality; Few-Shot + CoT provides highest ceiling but introduces instability for smaller models

- **Failure signatures**: Context fragmentation: Answer choices grammatically mismatch question; Language collapse: Model outputs English or mixed-language responses on >20% of queries; Difficulty cliff: Sharp accuracy drop between L3→L4 indicates shallow reasoning generalization

- **First 3 experiments**: 1) Ablate translation ensemble: Compare single-translator (NLLB only) vs. full ensemble on a 100-sample subset; measure semantic drift via human evaluation; 2) Cross-prompt robustness: Run same model with Direct vs. CoT vs. FS+CoT on MGSM; identify models where few-shot degrades performance; 3) Consistency intervention: For models with <90% Urdu consistency, prepend explicit language instruction to prompt; measure accuracy change

## Open Questions the Paper Calls Out
None

## Limitations
- Translation quality uncertainty: Methodology doesn't quantify inter-annotator agreement or provide error analysis for translation failures
- Generalizability constraints: Results specific to four datasets and five models without exploration of other reasoning tasks
- Prompt sensitivity: Fixed prompt templates may advantage certain model types without systematic variation testing

## Confidence
- High Confidence: Translation context-preservation mechanism shows clear technical implementation and logical necessity for MCQ benchmarks
- Medium Confidence: Multilingual pretraining × task complexity interaction claims are supported by comparative results but lack causal isolation
- Low Confidence: Ensemble diversity + human validation compensation assertion lacks quantitative validation

## Next Checks
1. **Translation Quality Audit**: Conduct inter-annotator reliability analysis on 100 randomly sampled translations with Cohen's kappa; perform error type classification to identify systematic failure modes

2. **Prompt Robustness Testing**: Systematically vary few-shot example quality, temperature settings (0.0-1.0), and prompt phrasing across all models; measure variance in accuracy and consistency

3. **Model Architecture Generalization**: Test UrduBench on additional model families including sparse models (Mixtral), specialized reasoning architectures (AlphaGeometry), and non-transformer approaches; compare scaling laws across model types