---
ver: rpa2
title: Deep Reinforcement Learning-Based User Scheduling for Collaborative Perception
arxiv_id: '2502.10456'
source_url: https://arxiv.org/abs/2502.10456
tags:
- scheduling
- perception
- vehicle
- collaborative
- slot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of user scheduling in vehicle-to-everything
  (V2X) communications for collaborative perception in autonomous driving systems.
  Due to limited communication resources, it is impractical for all connected and
  autonomous vehicles (CAVs) and roadside units (RSUs) to transmit their sensing data,
  necessitating an efficient scheduling mechanism.
---

# Deep Reinforcement Learning-Based User Scheduling for Collaborative Perception

## Quick Facts
- arXiv ID: 2502.10456
- Source URL: https://arxiv.org/abs/2502.10456
- Reference count: 40
- Key result: DRL-based user scheduling framework achieving up to 16.3% improvement in AP@0.70 over nearest neighbor scheduling for collaborative perception

## Executive Summary
This paper addresses the critical challenge of user scheduling in vehicle-to-everything (V2X) communications for collaborative perception in autonomous driving systems. The authors propose SchedCP, a deep reinforcement learning-based framework that leverages both channel state information (CSI) and semantic information to schedule which connected and autonomous vehicles (CAVs) or roadside units (RSUs) should transmit their perception data to improve the ego vehicle's perception accuracy. The framework uses a double deep Q-network (DDQN) with a label-free reward function that enables training without requiring perceptual labels, making it practical for real-world deployment.

## Method Summary
The framework transforms point clouds from collaborators into BEV features using PointPillars, then generates spatial confidence maps indicating object detection probability. A DDQN agent selects which collaborator to activate per scheduling slot based on a state representation combining channel conditions (large-scale fading α and small-scale fading h) and confidence information (sum and max of residual confidence values). The label-free reward function uses transmission rate and a utility metric measuring confidence value changes to guide training. Feature selection prioritizes grids where collaborators have high confidence but the ego vehicle has low confidence, ensuring complementary information transmission. The system is trained on the V2X-Sim dataset for 30,000 episodes using experience replay and target network updates.

## Key Results
- Achieves up to 16.3% improvement in AP@0.70 compared to nearest neighbor scheduling
- Demonstrates 4.0% improvement over maximum rate scheduling
- Shows robustness across different bandwidth conditions (200-600kHz)
- Maintains effectiveness with very short sensor sampling intervals (20ms)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prioritizing transmission of BEV grids where collaborators have high confidence and the ego vehicle has low confidence improves perception accuracy under bandwidth constraints.
- **Mechanism:** The spatial confidence map τ ∈ R^(H×W) assigns probability values to each grid cell indicating object presence. Feature selection mask M_j^t = TOPK(τ_j(x,y)²[1 - τ_e^0(x,y)]|B_j^t) selects the top B_j^t grids maximizing collaborator confidence weighted by ego uncertainty. This ensures collaborators transmit precisely the regions where they offer complementary information, avoiding redundant transmission of areas the ego already perceives well.
- **Core assumption:** Semantic information encoded in spatial confidence maps correlates with actual detection improvement potential. Assumption: Confidence values accurately reflect perceptual utility.
- **Evidence anchors:**
  - [abstract] "Incorporating both channel state information (CSI) and semantic information, we develop a double deep Q-Network (DDQN)-based user scheduling framework"
  - [Section III-A] Eq. (13): Feature selection rule explicitly uses τ_j²[1 - τ_e^0] to prioritize complementary grids
  - [corpus] Limited direct corpus support for this specific confidence-based selection; "Wireless Communication as an Information Sensor" survey mentions semantic importance but not this exact formulation
- **Break condition:** If confidence maps become unreliable under adversarial conditions or severe occlusion (see corpus paper "Uncertainty Quantification for Collaborative Object Detection"), the selection mechanism may degrade. Also breaks if B_j^t is too small to capture critical regions.

### Mechanism 2
- **Claim:** A label-free reward function based on changes in the ego vehicle's spatial confidence map can substitute for label-dependent average precision metrics during training.
- **Mechanism:** The AP metric requires ground-truth labels O_true, impractical for real-time training. Through empirical observations (Observations 3.1-3.3), the authors establish: ∆AP ∝ ∆L_det ∝ max(∆L_cls, ϱ), and that collaboration rarely causes True-to-False prediction shifts (<1% probability per Table I). The label-free reward R_t = λ_{r,nl}C_r^t + λ_u Σ_{(x,y)∈Ξ_t} max[T_t(x,y), G_t(x,y)] uses: (1) transmission rate C_r^t for channel awareness, and (2) utility measuring confidence value changes exceeding threshold ξ, encouraging prediction state transitions without requiring labels.
- **Core assumption:** Assumption: Well-behaved collaborative perception systems satisfy Observations 3.1-3.3 consistently. Assumption: The heuristic utility correlates sufficiently with AP for training guidance.
- **Evidence anchors:**
  - [abstract] "we reformulate the conventional label-dependent objective into a label-free goal, based on characteristics of 3D object detection"
  - [Section III-C, Table I] Empirical validation shows True-to-False probability <1% and Observation 3.3 violation probability <1% with ζ=0.05
  - [Section IV-B, Fig. 8-9] Detection/classification loss trends correlate with AP; utility values generally follow AP trends except minor deviation in high bandwidth (>500kHz) where localization loss dominates
- **Break condition:** If the collaborative perception system is not "well-behaved" (e.g., adversarial attacks, severe sensor miscalibration), observations may not hold. High-bandwidth scenarios where localization loss L_loc dominates classification loss L_cls may reduce label-free reward effectiveness (noted in Section IV-B).

### Mechanism 3
- **Claim:** DDQN with carefully designed state representation learns adaptive scheduling that jointly optimizes for channel conditions and perceptual semantics.
- **Mechanism:** The state S_t = {sum(R_j^t²), max(R_j^t²), {α_j, h_{j}^{t,0}}_{j∈N}} encodes: (1) total and maximum residual confidence per collaborator (semantic importance), and (2) large-scale fading α_j and current small-scale fading h (channel quality). DDQN learns Q(s,a;θ) to maximize cumulative discounted reward, using target network θ^- and experience replay to stabilize training. The agent balances scheduling collaborators with good channels versus those with high complementary semantic value.
- **Core assumption:** Assumption: The state representation captures sufficient information for optimal scheduling without requiring raw features or full confidence maps. Assumption: First-order Markov process adequately models small-scale fading dynamics.
- **Evidence anchors:**
  - [abstract] "double deep Q-Network (DDQN) to adaptively schedule which CAVs or RSUs should transmit"
  - [Section III-B, Eq. (15-16, 26-27)] State design, reward formulation, and DDQN loss function details
  - [Section IV-C, Fig. 12] Case study shows CAV3 scheduled first (high semantic value for occluded vehicles), then CAV1 (highest total confidence), then decisions influenced by channel rate variations
  - [corpus] "AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks" supports DRL applicability to V2X resource allocation
- **Break condition:** If the number of collaborators N increases substantially, the action space grows, potentially requiring alternative RL algorithms. If channel dynamics violate the Markov assumption (e.g., sudden interference), performance may degrade.

## Foundational Learning

- **Concept: Bird's Eye View (BEV) Representation**
  - Why needed here: All perception data is transformed to a common BEV coordinate system (H×W grids) before feature extraction and fusion. Understanding BEV is essential for grasping how collaborators align features and how spatial confidence maps operate.
  - Quick check question: Can you explain why point clouds from different vehicles must be transformed to the ego vehicle's coordinate system before BEV encoding?

- **Concept: Double Deep Q-Network (DDQN)**
  - Why needed here: The core scheduling algorithm uses DDQN with experience replay and target networks. Understanding Q-learning, the overestimation problem DDQN solves, and the role of target networks is prerequisite for implementing or modifying SchedCP.
  - Quick check question: Why does DDQN use separate Q-network and target Q-network parameters (θ vs θ^-), and how does this reduce overestimation compared to standard DQN?

- **Concept: Spatial Fading Channels in Vehicular Networks**
  - Why needed here: The system model explicitly separates large-scale fading (path loss, shadowing) from small-scale fading following a first-order Markov process. Understanding how channel capacity C relates to bandwidth, power, and fading gain is essential for interpreting transmission rate calculations and scheduling constraints.
  - Quick check question: Given the Markov fading model h_{j}^{t,ts} = μ_j·h_{j}^{t,ts-1} + e, what does the correlation coefficient μ_j = J_0(2πv_j f_c ∆t/c) tell you about channel predictability at different vehicle speeds?

## Architecture Onboarding

- **Component map:**
  Collaborators/RSUs (N units) -> Point clouds χ_j -> Feature Encoder (Φ_enc, shared PointPillars) -> BEV features F_j ∈ R^(H×W×D) -> Confidence Generator (Φ_gen) -> Spatial confidence maps τ_j^0 -> [Low overhead transmission to ego] -> Ego Vehicle Scheduler (DDQN Agent) -> CSI: {α_j, h_j^0} + Confidence: {τ_j^0, τ_e^t} -> Action: λ_j^t ∈ {0,1} -> Selected Collaborator -> Feature Selection (Φ_sel via Eq. 13) -> Masked BEV features F_j^t -> Ego Vehicle Fusion (Φ_fus) -> Fused feature F_f^t -> Detection Head (Φ_dec → Φ_pred) -> Detection results Ô_pred^T

- **Critical path:**
  1. Pre-train PointPillars-based collaborative perception backbone (DiscoNet in paper) on detection task
  2. Initialize DDQN with 3-layer fully-connected network (500-250-125 neurons)
  3. For each episode (sensor sampling interval T=200ms):
     - Gather τ_j^0 from all collaborators (negligible overhead)
     - For each scheduling slot (T slots × 5ms each):
       - Compute state S_t from CSI and confidence scores
       - Select action via ε-greedy (ε decays 1→0.02 over 16K episodes)
       - Transmit features, compute reward, store transition
     - Update Q-network via mini-batch gradient descent on Eq. (26)
     - Copy θ→θ^- every 10 episodes

- **Design tradeoffs:**
  - Label-dependent vs. label-free reward: SchedCP-wl (with labels) achieves ~2-3% higher AP@0.70 but requires ground truth; SchedCP (label-free) enables practical deployment with minimal performance gap
  - Scheduling slot length: 5ms balances channel coherence (small-scale fading updates at 1ms) vs. decision overhead; shorter slots increase overhead
  - Feature selection bandwidth: More grids B_j^t improves perception but requires better channel; paper tests 200-600kHz range
  - State representation: Compressed confidence scores (sum, max) vs. full maps trades information for tractability

- **Failure signatures:**
  - AP drops significantly below nearest-neighbor baseline → Check feature selection mask generation; τ_j^0 may be unreliable
  - Reward curve diverges after initial convergence → Target network update frequency may be too low; try reducing E in Algorithm 1
  - Utility value increases but AP decreases (especially at high bandwidth) → Label-free reward may not capture localization improvements; consider hybrid reward combining classification and localization losses
  - Scheduling always selects same collaborator → Exploration insufficient; verify ε-greedy schedule or check if one collaborator dominates confidence scores

- **First 3 experiments:**
  1. Reproduce baseline comparison at 300kHz bandwidth: Train SchedCP for 30K episodes on V2X-Sim training split, test on 500 frames. Compare AP@0.50 and AP@0.70 against Nearest, RR, Max Rate baselines. Expected: ~12-16% improvement over Nearest, ~3-4% over Max Rate.
  2. Ablate label-free reward components: Run (a) with only transmission rate reward (λ_u=0), (b) with only utility reward (λ_{r,nl}=0), (c) with both. Measure convergence speed and final AP to validate joint optimization necessity.
  3. Stress test under short sensor sampling intervals: Reduce T from 40 slots (200ms) to 4 slots (20ms) at 200kHz bandwidth. Verify SchedCP maintains performance advantage over baselines, demonstrating robustness to time-constrained scenarios per Fig. 10.

## Open Questions the Paper Calls Out

- **Question:** How can the label-free reward function be refined to explicitly optimize localization accuracy in high-bandwidth regimes?
- **Question:** Does the SchedCP policy generalize to real-world environments with imperfect channel state information (CSI) and heterogeneous perception backbones?

## Limitations

- The exact implementation details of confidence map generation and feature selection are not fully specified, creating uncertainty about faithful reproduction
- The label-free reward function's correlation with AP metric weakens at high bandwidths where localization loss dominates
- Performance is validated only on the V2X-Sim dataset with specific traffic and channel conditions

## Confidence

**High Confidence:** The DDQN framework architecture, state representation design, and feature selection mechanism are well-defined and theoretically sound. The scheduling performance improvements over baseline methods are empirically demonstrated.

**Medium Confidence:** The label-free reward formulation and its correlation with AP metric is reasonably justified but has acknowledged limitations in high-bandwidth scenarios. The assumption that well-behaved collaborative perception systems consistently satisfy Observations 3.1-3.3 is empirically supported but not rigorously proven.

**Low Confidence:** The exact implementation details of confidence map generation, specific hyperparameter values, and performance under adversarial conditions or extreme environmental scenarios are either unspecified or untested.

## Next Checks

1. **Reward Alignment Validation:** During training, periodically evaluate the agent's policy and plot AP vs. the average label-free reward. Investigate and address any divergence between reward and AP metrics, particularly at higher bandwidths where localization loss dominates.

2. **Adversarial Robustness Test:** Implement and test the framework under simulated adversarial attacks on confidence maps or channel state information. Evaluate whether the scheduling decisions remain optimal when collaborators provide misleading semantic information or when channel dynamics violate the Markov assumption.

3. **Cross-Dataset Generalization:** Validate the trained model on a different collaborative perception dataset (e.g., V2X-Sim with different environmental conditions or a completely different dataset) to assess the framework's generalization capability beyond the original training distribution.