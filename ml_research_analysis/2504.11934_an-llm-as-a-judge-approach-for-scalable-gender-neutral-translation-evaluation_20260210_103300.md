---
ver: rpa2
title: An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation
arxiv_id: '2504.11934'
source_url: https://arxiv.org/abs/2504.11934
tags:
- gender
- language
- evaluation
- phrases
- gendered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores the use of large language models (LLMs) as
  evaluators of gender-neutral translation (GNT), addressing the lack of scalable
  evaluation methods in this domain. Two prompting approaches were tested: sentence-level
  assessment only, and a chain-of-thought approach that first generates phrase-level
  annotations before sentence-level judgment.'
---

# An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation

## Quick Facts
- arXiv ID: 2504.11934
- Source URL: https://arxiv.org/abs/2504.11934
- Authors: Andrea Piergentili; Beatrice Savoldi; Matteo Negri; Luisa Bentivogli
- Reference count: 32
- Key outcome: LLM evaluators with chain-of-thought prompting achieve up to 81.37% accuracy on gender-neutral translation evaluation across multiple languages

## Executive Summary
This work addresses the scalability challenge in gender-neutral translation (GNT) evaluation by proposing LLM-as-a-judge approaches. The authors test two prompting strategies - sentence-level assessment only versus chain-of-thought with phrase-level annotations - across five models (GPT-4o, Qwen 2.5, Mistral Small 3, DeepSeek) and multiple languages. Results show that requiring phrase-level gender annotations before sentence-level judgments consistently improves accuracy across all models and languages. This approach provides a more scalable and flexible alternative to existing classifier-based methods, enabling GNT evaluation across new languages without requiring dedicated fine-tuning data.

## Method Summary
The method involves four prompt variants: MONO-L (target sentence → label), MONO-P+L (target → phrases + label), CROSS-L (source+target → label), and CROSS-P+L (source+target → phrases + correctness + label). The P+L variants require phrase-level gender annotations (M/F/N per phrase) before sentence-level classification. Evaluation uses the mGeNTE dataset (en→it/es/de, 1,500 sentences per pair) with 8-shot in-context learning using balanced exemplars. Models include GPT-4o, Qwen 2.5 32B/72B, Mistral Small 3 24B, and DeepSeek-R1-Distill-Qwen-32B. Structured JSON output ensures consistent formatting. Accuracy is measured against gold references, with additional analysis on automatic GNTs for precision/recall/F1.

## Key Results
- Chain-of-thought prompting (P+L) consistently improves accuracy across all models and languages compared to sentence-level only
- GPT-4o achieves highest accuracy at 81.37% on Italian mGeNTE reference set
- Source-target evaluation shows lower accuracy than target-only, indicating cross-lingual reasoning limitations
- Phrase-level annotation accuracy remains unverified due to lack of gold phrase labels in mGeNTE

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Annotation Improves Classification Accuracy
- Claim: Intermediate phrase-level annotations guide model attention to individual human references, reducing default-to-gendered labeling
- Core assumption: Phrase-level annotations force explicit reasoning about each human referent phrase
- Evidence anchors: "prompting for phrase-level annotations before sentence-level assessments consistently improves the accuracy of all models"
- Break condition: If models cannot reliably identify human-referent phrases in target language

### Mechanism 2: Structured Output Constraints Enable Reliable Parsing
- Claim: JSON schema constraints ensure consistent output formatting without post-processing
- Core assumption: Constrained generation doesn't degrade assessment quality
- Evidence anchors: "This ensures that all models' outputs adhere to the same formats without the need for post-processing"
- Break condition: If JSON schema conflicts with model's natural generation patterns

### Mechanism 3: Source-Target Pairing Enables Correctness Assessment
- Claim: Providing source and target sentences allows evaluation of whether gender expression is correct relative to source gender cues
- Core assumption: LLMs can reliably map English gender cues to target-language gender inflections
- Evidence anchors: "WRONGLY GENDERED if the target's gender does not match the source or the target adds gender information when the source lacks it"
- Break condition: If source sentences contain ambiguous or culturally-specific gender cues

## Foundational Learning

- Concept: Grammatical vs. social gender distinction
  - Why needed here: Prompts require identifying social gender independently of grammatical gender markings
  - Quick check question: In Spanish, would "la persona que enseña" be grammatically feminine, socially neutral, or both?

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding why P+L prompts outperform L-only prompts
  - Quick check question: If a model defaults to GENDERED labels without intermediate analysis, what cognitive shortcut might it be applying?

- Concept: LLM-as-a-Judge paradigm limitations
  - Why needed here: Source-target evaluation shows lower accuracy than target-only
  - Quick check question: Why might an LLM evaluate translation quality better with a reference than with the source alone?

## Architecture Onboarding

- Component map: Input layer (source sentence optional, target sentence) -> Prompt variants (MONO-L, MONO-P+L, CROSS-L, CROSS-P+L) -> Structured generation (JSON schema constraint) -> Output layer (sentence-level label plus optional phrase annotations)

- Critical path: Start with CROSS-P+L prompt - achieves highest accuracy in most configurations; use 8 balanced exemplars for in-context learning

- Design tradeoffs:
  - MONO vs. CROSS: MONO requires gold source labels; CROSS enables standalone evaluation but with lower accuracy
  - L vs. P+L: P+L adds inference cost but consistently improves accuracy
  - Open vs. proprietary: GPT-4o performs best; Qwen 72B is strongest open alternative

- Failure signatures:
  - High REF-G + low REF-N accuracy indicates default-to-GENDERED bias
  - Low recall on neutral sentences suggests phrase-level annotation is not guiding correctly
  - Source-target accuracy much lower than target-only indicates cross-lingual reasoning failure

- First 3 experiments:
  1. Replicate target-only evaluation on mGeNTE Italian with MONO-L and MONO-P+L using GPT-4o; compare against classifier baseline (81.37% accuracy)
  2. Test CROSS-P+L on Spanish/German source-target pairs to confirm cross-lingual accuracy degradation pattern
  3. Ablate structured generation to verify JSON constraints do not degrade assessment quality

## Open Questions the Paper Calls Out

- Question: How can automatic GNT evaluation be expanded to assess semantic adequacy and fluency of neutral text?
  - Basis in paper: Section 7 states proposed approaches "do not factor in an important aspect of GNT: the acceptability of neutral text"
  - Why unresolved: Current evaluation confined to discrete labels cannot determine if neutral translation is readable or faithful
  - What evidence would resolve it: Development and validation of evaluation systems against fine-grained human reference annotations focused on acceptability

- Question: What is the accuracy of intermediate phrase-level annotations generated by P+L prompts?
  - Basis in paper: Section 7 notes authors "left aside the phrase-level annotations" because mGeNTE lacks gold phrase labels
  - Why unresolved: While phrase-level reasoning improves sentence-level accuracy, intermediate reasoning steps remain unverified
  - What evidence would resolve it: Dataset with gold-standard phrase-level spans and gender labels to directly evaluate models' annotation capabilities

- Question: How can evaluation frameworks account for cases where preserving gender is semantically essential?
  - Basis in paper: Footnote mentions "instances where gender is essential to the meaning of a sentence... we retain them in our experiments and leave this analysis to future work"
  - Why unresolved: Current system treats all neutral outputs as correct in Set-N scenarios, potentially penalizing necessary gender preservation
  - What evidence would resolve it: Framework integrating translation adequacy metrics to distinguish between unnecessary gendering and contextually required gender preservation

## Limitations

- Performance ceiling limitations: Best model (GPT-4o) achieves only 81.37% accuracy, leaving significant room for improvement
- Data imbalance handling: Automatic GNTs evaluation reveals severe class imbalance with precision/recall/F1 scores of 0.28/0.02/0.04 for neutral translations
- Methodological scope constraints: Study focuses on en→it/es/de language pairs only, limiting generalizability to other language families

## Confidence

**High Confidence Claims:**
- LLMs can serve as GNT evaluators - Consistent accuracy improvements across multiple models and languages
- P+L prompting consistently improves accuracy over L-only prompting - Demonstrated across all tested models and language pairs
- Structured JSON output enables reliable parsing without post-processing - Explicitly validated through consistent output formatting

**Medium Confidence Claims:**
- Chain-of-thought annotation reduces default-to-gendered labeling - Inferred from accuracy patterns but mechanism not directly observed
- Source-target evaluation is inherently harder than target-only - Supported by lower scores but causal factors not fully isolated

**Low Confidence Claims:**
- This approach scales to arbitrary languages without fine-tuning - Limited evidence; only 4 languages tested
- JSON schema constraints don't degrade assessment quality - No direct comparison with unconstrained generation provided

## Next Checks

1. **Cross-linguistic generalization test**: Evaluate MONO-P+L prompt on additional language pairs (en→fr, en→ru) to assess whether accuracy improvements generalize beyond tested languages; compare against baseline classifier performance

2. **Prompt ablation study**: Systematically remove or modify components of P+L prompt structure (e.g., remove correctness assessment, change phrase annotation format) to isolate which elements drive accuracy improvements and validate essential mechanisms

3. **Structured vs. unstructured generation comparison**: Run identical evaluations using same prompts but with unconstrained generation output; measure both assessment accuracy and parsing success rates to definitively determine whether JSON schema constraints improve reliability without compromising evaluation quality