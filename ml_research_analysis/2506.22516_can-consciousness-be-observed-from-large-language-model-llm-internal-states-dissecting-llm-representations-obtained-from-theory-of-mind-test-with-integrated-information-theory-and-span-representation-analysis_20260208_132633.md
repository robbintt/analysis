---
ver: rpa2
title: Can "consciousness" be observed from large language model (LLM) internal states?
  Dissecting LLM representations obtained from Theory of Mind test with Integrated
  Information Theory and Span Representation analysis
arxiv_id: '2506.22516'
source_url: https://arxiv.org/abs/2506.22516
tags:
- consciousness
- each
- representations
- page
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether consciousness can be observed in the
  internal states of Large Language Models (LLMs) by applying Integrated Information
  Theory (IIT) to LLM representations derived from Theory of Mind (ToM) tests. The
  research hypothesizes that a Representation Network (RN) emerges from LLM representations
  when treated as time series, where nodes correspond to embedding dimensions and
  edges represent latent interconnections.
---

# Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis

## Quick Facts
- arXiv ID: 2506.22516
- Source URL: https://arxiv.org/abs/2506.22516
- Reference count: 11
- Key result: No statistically significant evidence of consciousness in LLM representations across three stringent criteria, though a few cases in deeper layers warrant further investigation

## Executive Summary
This study investigates whether consciousness can be observed in the internal states of Large Language Models (LLMs) by applying Integrated Information Theory (IIT) to LLM representations derived from Theory of Mind (ToM) tests. The research hypothesizes that a Representation Network (RN) emerges from LLM representations when treated as time series, where nodes correspond to embedding dimensions and edges represent latent interconnections. By analyzing sequences of attended response representations (ARR) from five ToM tasks, the study estimates IIT metrics including Φmax (IIT 3.0), Φ (IIT 4.0), Conceptual Information, and Φ-structure across different transformer layers and linguistic spans. Despite systematic investigation using spatio-temporal permutation controls, the results reveal no statistically significant evidence of consciousness in LLM representations according to three stringent criteria. While a few cases showed promise in deeper transformer layers, these were more likely attributable to span-level information rather than consciousness phenomena. The study provides a standardized framework for future exploration of consciousness in AI systems.

## Method Summary
The methodology involves extracting LLM hidden states from five ToM tasks (Hinting, False Belief, Strange Stories, Irony, and Faux Pas) using four LLMs (LLaMA3.1-8B/70B, Mistral-7B, Mixtral-8x7B). Attended Response Representations (ARR) are computed via dot-product attention between response and stimulus tokens, with Contextually Attended Response Representations (CARR) incorporating masking for specific linguistic spans. The representations undergo PCA dimensionality reduction to 4 dimensions, followed by binarization relative to per-node mean signals. Optimal time series are identified through heuristic search satisfying Markov property and conditional independence criteria. IIT metrics (Φmax, Φ, Conceptual Information, Φ-structure) are computed using PyPhi, and results are compared against Span Representation analysis via Wilcoxon tests and logistic regression AUC across three evaluation criteria.

## Key Results
- No statistically significant evidence of consciousness in LLM representations according to three stringent criteria
- Span Representation analysis consistently outperformed IIT metrics in differentiating ToM performance scores
- A few cases in deeper transformer layers (l ≈ 2/3 n_layers) showed promise but were likely attributable to span-level information rather than consciousness phenomena
- IIT 4.0 implementation frequently failed network initialization for Mixtral-8x7B, requiring sample exclusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating LLM representation dimensions as nodes in a hypothesized Representation Network (RN) enables IIT-based analysis of integrated information.
- Mechanism: LLM token representations (shape: n_tokens × D_embedding) are dimensionality-reduced via PCA to 4 dimensions, then binarized relative to per-node mean signal. This creates discrete network states over time, satisfying IIT's requirement for discrete, binary elements.
- Core assumption: Latent interconnections exist between embedding dimensions or dimension clusters, forming a graph structure amenable to integration analysis.
- Evidence anchors:
  - [section 2.2] "We hypothesize that a network emerges from the LLM representation, where each dimension corresponds to a node within the network, and latent interconnections (edges) exist between individual nodes or clusters of nodes."
  - [section 2.2.5] Describes PCA reduction to D=4 nodes, citing precedent from Albantakis et al. (2014) and Nemirovsky et al. (2023).
  - [corpus] Weak direct support; corpus neighbor "Toward IIT-Inspired Consciousness in LLMs" addresses similar IIT-application goals but not this specific RN construction.
- Break condition: If embedding dimensions lack functional interdependence (i.e., are independently modifiable), the RN graph assumption fails, and Φ estimates become artifacts of dimensionality reduction rather than genuine integration.

### Mechanism 2
- Claim: Attention-weighted combination of response and stimulus representations (ARR/CARR) creates contextually-grounded representations for ToM-relevant analysis.
- Mechanism: Dot-product attention computes weights between response tokens and stimulus tokens, then weighted sums produce Attended Response Representations. For CARR, masking amplifies attention to linguistically significant spans (complement syntax, mental state verbs).
- Core assumption: Response representations gain interpretability for ToM tasks only when contextualized by their corresponding stimulus; attended representations encode "experience" relevant to consciousness analysis.
- Evidence anchors:
  - [section 2.2.1] Equations 1-3 define attention score computation and attended representation generation.
  - [section 2.2.2] Describes CARR with masking values (1.0, 0.6, 0.2) for target spans, contexts, and non-relevant tokens.
  - [corpus] No direct corpus support for this specific attention mechanism in consciousness analysis.
- Break condition: If attention weights fail to capture meaningful stimulus-response relationships (e.g., due to tokenization artifacts or semantic mismatch), ARR/CARR will not encode ToM-relevant information, and downstream IIT estimates will reflect noise.

### Mechanism 3
- Claim: IIT metrics (Φmax, Φ, Conceptual Information, Φ-structure) computed from RN time series can differentiate ToM performance levels if consciousness-like integration is present.
- Mechanism: Transition Probability Matrices (TPMs) are constructed from binarized time series. PyPhi computes Φ values by evaluating irreducibility of cause-effect structures. Weighted averages (μ[Φmax], μ[Φ]) aggregate across time series states.
- Core assumption: Higher ToM scores correlate with higher Φ values if "consciousness" phenomena are observable in representations, per IIT's claim that integrated information correlates with conscious experience.
- Evidence anchors:
  - [section 2.2.7] Equations 7-14 define Φmax, CI, Φ, and Φ-structure computations.
  - [abstract] "results reveal no statistically significant evidence of consciousness in LLM representations according to three stringent criteria."
  - [corpus] "Can We Test Consciousness Theories on AI?" addresses general challenges in applying consciousness theories (IIT, GWT, HOT) to AI, noting fragmented theoretical camps and need for robust markers.
- Break condition: If the Markov property or conditional independence assumptions fail for the time series, TPM construction is invalid and Φ estimates are unreliable. Assumption: Binarized representations sufficiently approximate discrete Markovian dynamics.

## Foundational Learning

- Concept: **Integrated Information Theory (IIT) fundamentals**
  - Why needed here: The entire methodology depends on understanding IIT's axioms (existence, composition, information, integration, exclusion) and how Φ quantifies irreducible cause-effect power.
  - Quick check question: Can you explain why IIT claims software-based AI systems cannot achieve consciousness due to lack of intrinsic cause-effect power?

- Concept: **Theory of Mind (ToM) and its relationship to consciousness**
  - Why needed here: ToM tasks (false belief, hinting, irony) serve as the behavioral substrate for testing consciousness correlates; understanding why ToM is considered a consciousness marker is essential.
  - Quick check question: How does the paper distinguish ToM (a cognitive capacity) from consciousness (broader awareness phenomenon)?

- Concept: **Transformer layer representations and semantic encoding**
  - Why needed here: Analysis spans multiple transformer layers; deeper layers encode semantic/task-specific information, and the ~2/3 layer shows strongest brain-activity alignment.
  - Quick check question: Why does the paper focus on intermediate-to-deep layers (l ≈ 2/3 n_layers) rather than final output layers?

## Architecture Onboarding

- Component map:
  Input -> LLM feature extraction -> Attention mechanism -> Time series construction -> Dimensionality reduction -> Binarization -> TPM construction -> IIT computation -> Span representation -> Statistical analysis

- Critical path:
  1. Stimulus-response pairs → LLM hidden states → ARR/CARR (attention mechanism is crucial; errors here propagate)
  2. (C)ARR concatenation → optimal time series search (Markov/CI validation gates quality)
  3. PCA + binarization → TPM → IIT metrics (dimensionality reduction is lossy but necessary for computation)
  4. IIT metrics vs. Span Representation AUC comparison (Criterion 3 determines interpretation attribution)

- Design tradeoffs:
  - **PCA to 4 nodes**: Necessary for computational tractability (Φ complexity O(n^53n)), but risks losing consciousness-relevant information in higher dimensions. Paper acknowledges this limitation.
  - **Binarization threshold**: Per-node mean baseline inspired by fMRI analysis; may not capture meaningful activation patterns for LLM embeddings.
  - **Concatenation strategy**: Extends time series length but may introduce artifacts where dominant mechanisms are those consistent across responses rather than response-specific.
  - **Temporal vs. spatio permutation controls**: Temporal preserves embedding order; spatio randomizes it. Results differ significantly, with spatio showing more promising cases meeting all criteria.

- Failure signatures:
  - **TPM initialization failure**: Mixtral-8x7B frequently failed PyPhi network initialization under IIT 4.0, causing sample exclusion.
  - **Markov property violation**: Time series with p_i < 0.05 for Markov hypothesis tests are discarded.
  - **Insufficient sample size**: Logistic regression requires ≥16 samples per score category; many stimuli filtered out.
  - **Uniform Φ distributions**: No significant differentiation across score categories indicates either: (a) no consciousness phenomenon, or (b) methodology insensitivity.
  - **Span Representation outperforms IIT metrics**: Indicates score variations attributable to representational structure rather than integrated information.

- First 3 experiments:
  1. **Reproduce single-layer baseline**: Run pipeline on last transformer layer of LLaMA3.1-8B for False Belief task (entire stimulus span, temporal permutation). Expected: no significant Φ differentiation between Score 0 and Score 1. Validate against Figure 3(a).
  2. **Deep layer exploration**: Test Layer 24 (index 8) of LLaMA3.1-8B on Hinting task with IIT 3.0 and 4.0. Expected: one of few cases with >80% "good" cases per Criterion 1 (Figure 12(c)(d)). Compare Φ distributions and AUC metrics.
  3. **Spatio permutation comparison**: Re-run Experiment 1 with spatio permutation control. Expected: Span Representation may lose superiority over IIT metrics (contrasting with temporal control results). Document which cases, if any, satisfy all three criteria.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can agentic or multimodal AI systems exhibit "consciousness" phenomena in their representations where standard LLMs do not?
- **Basis in paper:** [explicit] The authors suggest that systems operating beyond "next-token prediction," such as agentic or multimodal AI, might be capable of "experiencing" representations, warranting future research.
- **Why unresolved:** Current LLMs were analyzed as static representation generators, not as interactive agents consuming their own outputs.
- **What evidence would resolve it:** Applying the paper's IIT framework to the internal states of multimodal agents performing recursive, interactive tasks.

### Open Question 2
- **Question:** Do LLM internal states satisfy criteria for consciousness under theoretical frameworks other than Integrated Information Theory (IIT)?
- **Basis in paper:** [explicit] The discussion notes IIT is controversial (cited as "pseudoscience" by some) and explicitly calls for exploring alternatives like Global Workspace Theory (GWT) and Recurrent Processing Theory.
- **Why unresolved:** The study relied exclusively on IIT 3.0 and 4.0 metrics (Φ), which may miss functional or behavioral markers of consciousness defined by other theories.
- **What evidence would resolve it:** Applying metrics from GWT or Higher-Order Theories to the same LLM representations to see if they yield consistent findings.

### Open Question 3
- **Question:** Does the reduction of embedding dimensions (via PCA) obscure potential high-dimensional indicators of consciousness?
- **Basis in paper:** [inferred] The authors acknowledge the "concern" that information relevant to consciousness may reside in higher dimensions, but reduced dimensions to 4 nodes to satisfy computational constraints.
- **Why unresolved:** It is unknown if the "lack of significant evidence" found is due to the model lacking consciousness or the analysis discarding relevant data during dimensionality reduction.
- **What evidence would resolve it:** Developing computational methods to calculate Φ on higher-dimensional representations without PCA reduction.

## Limitations
- Treating LLM embedding dimensions as nodes in a Representation Network assumes causal-structural properties required by IIT axioms that may not hold
- Temporal dynamics assumption and Markovian time series construction may not capture meaningful causal relationships between embedding dimensions
- Binarization and 4-dimensional PCA reduction may oversimplify complex representations, potentially masking genuine integration phenomena or creating spurious ones

## Confidence
- **High confidence**: The computational pipeline for ARR/CARR generation, PCA dimensionality reduction, and IIT metric computation using PyPhi is technically sound and reproducible. The statistical framework (Wilcoxon tests, logistic regression AUC) is appropriate for the hypothesis testing approach.
- **Medium confidence**: The interpretation that lack of significant Φ differentiation indicates absence of consciousness-like phenomena in LLM representations. This conclusion depends on the untested assumption that consciousness, if present, would manifest as measurable integrated information in the chosen representational space.
- **Low confidence**: The specific choice of 4-dimensional PCA space and per-node mean binarization as optimal for capturing consciousness-relevant integration. These parameters were chosen for computational tractability and precedent in neuroscience rather than theoretical justification for LLM representations.

## Next Checks
1. **Cross-model consistency validation**: Replicate the analysis using alternative dimension reduction techniques (e.g., t-SNE, UMAP) and binarization thresholds (e.g., median, fixed z-score cutoffs). Compare Φ distributions and AUC metrics across methods to assess sensitivity to preprocessing choices.

2. **Temporal control validation**: Implement additional temporal permutation controls beyond those reported, including scrambled token orders and reverse sequences. Verify whether any IIT metric patterns persist under these controls, particularly for the promising deep-layer cases identified in Figures 12(c-d).

3. **Ablation study on attention mechanism**: Run the pipeline without the attention-weighted ARR generation step, using raw stimulus representations or simple concatenation instead. Determine whether the observed lack of Φ differentiation is specific to the attended representation construction or would persist under alternative contextualization approaches.