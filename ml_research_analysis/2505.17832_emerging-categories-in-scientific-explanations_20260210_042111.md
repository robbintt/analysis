---
ver: rpa2
title: Emerging categories in scientific explanations
arxiv_id: '2505.17832'
source_url: https://arxiv.org/abs/2505.17832
tags:
- explanations
- categories
- dataset
- which
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the lack of large-scale datasets of human-like,
  human-generated explanations in scientific literature. The authors extracted 340
  explanatory sentences from biotechnology and biophysics literature, then performed
  inductive classification to derive six explanation types: causation, mechanistic
  causation, contrastive, correlation, functional, and pragmatic approach.'
---

# Emerging categories in scientific explanations

## Quick Facts
- arXiv ID: 2505.17832
- Source URL: https://arxiv.org/abs/2505.17832
- Authors: Giacomo Magnifico; Eduard Barbu
- Reference count: 25
- Primary result: 272 high-quality explanatory sentences annotated with 6 categories, achieving Krippendorff's alpha of 0.667 after consolidation to 3 classes

## Executive Summary
This study addresses the lack of large-scale datasets of human-like, human-generated explanations in scientific literature. The authors extracted 340 explanatory sentences from biotechnology and biophysics literature, then performed inductive classification to derive six explanation types: causation, mechanistic causation, contrastive, correlation, functional, and pragmatic approach. To evaluate annotator consensus, they conducted a classification study on Prolific with 120 annotators rating 272 high-quality sentences. Initial Krippendorff's alpha showed poor agreement between closely related categories, but improved to 0.667 after grouping into strong relation (causation, mechanistic causation), weak relation (correlation, functional, pragmatic), and multipath relation (contrastive) categories. This 0.667 value, while only slightly above the 0.667 threshold for good agreement, represents acceptable consensus and validates the dataset quality.

## Method Summary
The methodology involved extracting explanatory sentences with explicit explananda from biotechnology and biophysics literature, followed by deductive coding to identify six explanation categories. The authors then conducted crowdsourced annotation via Prolific, collecting 10 annotations per sentence from 120 annotators. After filtering for high-quality annotations and removing statistical outliers, they calculated Krippendorff's alpha for agreement. Due to poor agreement between closely related categories, they consolidated the six categories into three based on causal strength: strong relation (causation, mechanistic causation), weak relation (correlation, functional, pragmatic), and multipath relation (contrastive).

## Key Results
- Six explanation categories emerged from deductive coding: causation, mechanistic causation, contrastive, correlation, functional, and pragmatic approach
- Initial Krippendorff's alpha showed poor agreement between closely related categories
- After consolidation to three categories (strong, weak, multipath relation), agreement reached 0.667, meeting the threshold for good agreement
- Dataset contains 272 high-quality explanatory sentences with explicit explananda

## Why This Works (Mechanism)
The approach works by leveraging inductive classification from actual scientific texts to derive explanation categories that reflect real-world usage patterns. The consolidation of categories based on causal strength addresses the inherent ambiguity in distinguishing between similar explanatory intents, improving annotator agreement by reducing the cognitive load of fine-grained distinctions.

## Foundational Learning
- **Inductive classification**: Deriving categories from observed data rather than imposing predetermined taxonomies; needed to ensure categories reflect actual scientific explanation patterns; quick check: verify categories emerge consistently from multiple domain samples
- **Krippendorff's alpha**: Statistical measure of inter-annotator agreement that accounts for chance agreement; needed to validate category definitions and annotation instructions; quick check: calculate alpha for different category granularities
- **Explicit explanandum requirement**: Focusing on sentences with clearly stated explananda to avoid multi-sentence explanations; needed to maintain annotation consistency; quick check: count excluded implicit cases in the corpus
- **Crowdsourced annotation**: Using Prolific participants to scale annotation efforts; needed for sufficient annotation volume; quick check: verify annotator quality through screening questions
- **Category consolidation**: Merging similar categories to improve agreement; needed when fine-grained distinctions prove too ambiguous; quick check: measure agreement before and after consolidation
- **Scientific explanation types**: Understanding different ways scientists explain phenomena (causation, correlation, functional, etc.); needed to build interpretable explanation systems; quick check: map categories to explanation patterns in new domains

## Architecture Onboarding
**Component Map**: Text extraction -> Deductive coding -> Crowdsourced annotation -> Agreement calculation -> Category consolidation
**Critical Path**: The pipeline flows from raw text extraction through multiple refinement stages, with the annotation and agreement calculation stages being most critical for validation
**Design Tradeoffs**: Fine-grained categories vs. annotator agreement (resolved by consolidation), explicit vs. implicit explananda (resolved by constraint), domain specificity vs. generalizability (acknowledged limitation)
**Failure Signatures**: Low Krippendorff's alpha indicates category ambiguity or poor instructions; high disagreement in closely related categories suggests need for consolidation; systematic bias in annotations suggests unclear definitions
**3 First Experiments**:
1. Calculate Krippendorff's alpha for the original six-category schema to quantify initial agreement levels
2. Test category assignment consistency by having annotators re-annotate the same sentences after a time interval
3. Apply the consolidated three-category schema to a small sample from a different scientific domain to test generalizability

## Open Questions the Paper Calls Out
**Open Question 1**: Can the six original explanation categories achieve acceptable inter-annotator agreement without consolidation into three broader categories?
**Open Question 2**: Do the identified explanation categories generalize to scientific domains beyond biotechnology and biophysics?
**Open Question 3**: How prevalent are scientific explanations with implicit explananda, and do they require different categorical treatment?

## Limitations
- Krippendorff's alpha of 0.667 represents only minimal acceptable agreement, suggesting category definitions may be inherently ambiguous
- The consolidation of six categories into three was a post-hoc adjustment rather than pre-registered analysis, raising concerns about overfitting
- Lack of detailed information about annotator selection criteria and specific definitions makes independent replication challenging

## Confidence
- **High confidence**: The existence of the dataset and basic crowdsourcing methodology
- **Medium confidence**: The reported Krippendorff's alpha value of 0.667 for the collapsed 3-class schema
- **Low confidence**: The interpretability and stability of the six original categories

## Next Checks
1. Replicate the Krippendorff's alpha calculation using publicly available annotation data to verify the 0.667 agreement value for the collapsed 3-class schema, including confidence intervals
2. Analyze the distribution of annotations across the six original categories to quantify confusion between closely related categories
3. Conduct a small-scale validation study with domain experts to assess whether the six original categories represent meaningful distinctions in scientific explanation