---
ver: rpa2
title: Unsupervised Word-level Quality Estimation for Machine Translation Through
  the Lens of Annotators (Dis)agreement
arxiv_id: '2505.23183'
source_url: https://arxiv.org/abs/2505.23183
tags:
- metrics
- association
- translation
- computational
- xcomet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates unsupervised word-level quality estimation
  (WQE) metrics across 14 metrics, 12 translation directions, and multiple human annotation
  sets. The authors investigate whether signals derived from model internals (like
  predictive uncertainty and attention) can effectively identify translation errors
  without requiring costly human-labeled data or LLM prompting.
---

# Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement

## Quick Facts
- arXiv ID: 2505.23183
- Source URL: https://arxiv.org/abs/2505.23183
- Reference count: 40
- This study evaluates unsupervised word-level quality estimation (WQE) metrics across 14 metrics, 12 translation directions, and multiple human annotation sets, finding that uncertainty-based metrics like Surprisal MCD VAR perform competitively with supervised approaches.

## Executive Summary
This paper investigates whether unsupervised word-level quality estimation metrics derived from model internals can effectively identify translation errors without requiring costly human-labeled data or LLM prompting. The authors evaluate 14 unsupervised metrics across 12 translation directions using three datasets with human error annotations. They find that metrics based on output distribution uncertainty, particularly Monte Carlo Dropout variance, achieve strong correlation with human error annotations. The study also demonstrates that incorporating multiple human annotation sets improves evaluation robustness and that confidence-weighted XCOMET variants significantly outperform default binary versions.

## Method Summary
The paper evaluates unsupervised WQE metrics by force-decoding annotated translations through MT models and extracting various interpretability signals. These include token surprisal, output entropy, Monte Carlo Dropout variance (n=10 samples), LogitLens projections, attention entropy, and BLOOD. The extracted metrics are compared against human error annotations using Average Precision (AP) and Optimal F1 (F1*) across binarization thresholds. The evaluation spans three datasets: DivEMT with mBART-50 outputs, WMT24 with Aya23-35B outputs, and QE4PE with NLLB-3.3B outputs. Supervised baselines include XCOMET-XL/XXL metrics, with a proposed confidence-weighted variant (XCOMET_CONF) that sums error type probabilities.

## Key Results
- Surprisal MCD VAR (variance of surprisal across 10 MCD samples) achieves strong correlation with human error annotations, approaching performance of supervised metrics
- XCOMET_CONF (confidence-weighted version summing MINOR/MAJOR/CRITICAL probabilities) consistently outperforms default XCOMET across all datasets
- Incorporating multiple human annotation sets increases metric evaluation robustness, with correlations improving monotonically with number of annotators
- The best unsupervised metrics (Surprisal MCD VAR) perform competitively with default XCOMET, though confidence-weighted supervised metrics remain strongest

## Why This Works (Mechanism)

### Mechanism 1: Predictive Distribution Variance Captures Epistemic Uncertainty
Unsupervised metrics derived from model output distributions—particularly Monte Carlo Dropout variance—detect translation errors by quantifying epistemic uncertainty. At inference time, MCD samples multiple forward passes with different dropout masks, producing a distribution of surprisal values. High variance across these samples indicates model uncertainty about the prediction, which correlates with translation errors. This reflects the model's lack of confident knowledge rather than inherent data ambiguity. The paper shows Surprisal MCD VAR has steeper correlation increase with more annotators, suggesting epistemic uncertainty anticipates aleatoric (human) disagreement.

### Mechanism 2: Confidence-Weighted Supervised Metrics Enable Calibration
Default XCOMET metrics have low recall because they output discrete error labels; using continuous confidence scores (summing error type probabilities) enables threshold tuning. XCOMET_CONF = p(MINOR) + p(MAJOR) + p(CRITICAL) as a continuous score allows practitioners to select operating points along the precision-recall curve based on use case. This allows for calibration where high precision is prioritized for comprehensive post-editing assistance. The paper demonstrates XCOMET_CONF shows consistent improvements over default XCOMET across all datasets and covers the full recall range.

### Mechanism 3: Human Label Variation Sets Performance Ceiling
Single-annotator evaluations are brittle because individual annotator preferences create confounders; aggregating multiple annotations reveals metrics' true alignment with human judgment. Different annotators mark different spans as errors due to subjective thresholds. By computing edit counts (how many of L annotators marked a token as error), the paper shows metric correlations increase with more annotators. Metrics that capture uncertainty (especially MCD VAR) better predict this annotation disagreement, revealing the true performance ceiling of WQE metrics.

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** The paper explicitly claims MCD VAR captures epistemic uncertainty (model's knowledge gaps) which correlates with aleatoric uncertainty (human annotator disagreement). Understanding this distinction is required to interpret why variance-based metrics work.
  - **Quick check question:** If you ran MCD on a model with random weights, would the variance be higher or lower than a trained model? Why?

- **Concept: Precision-Recall Tradeoff and Calibration**
  - **Why needed here:** The paper's key practical contribution is showing that default XCOMET has high precision but low recall, and confidence weighting enables operating point selection.
  - **Quick check question:** For a post-editing assistance tool where missing an error is worse than flagging a correct token, should you optimize for precision or recall?

- **Concept: Force-Decoding for Metric Extraction**
  - **Why needed here:** To extract unsupervised metrics that align with annotated spans, the paper force-decodes the exact annotated translations. This ensures token-level alignment.
  - **Quick check question:** Why can't you simply generate translations freely and then align them to annotations post-hoc?

## Architecture Onboarding

- **Component map:** Source text + MT model + human annotations -> Inseq extraction (surprisal, entropy, MCD, LogitLens, attention) -> XCOMET classification -> token-level alignment -> AP/F1* computation

- **Critical path:**
  1. Obtain MT outputs with human error annotations (DivEMT, WMT24, or QE4PE formats)
  2. Load the same MT model used to generate translations (mBART-50, NLLB-3.3B, or Aya23)
  3. Force-decode annotated translations while extracting metrics via Inseq
  4. For MCD: run n=10 forward passes per token, compute mean and variance of surprisal
  5. Binarily classify tokens as error/not-error, compute AP and optimal F1

- **Design tradeoffs:**
  - **MCD vs. simple surprisal:** MCD VAR outperforms but requires 10x forward passes and only works on models with dropout layers
  - **Supervised vs. unsupervised:** XCOMET_CONF is strongest but requires 3.5B+ parameter model download; unsupervised metrics require only the MT model you already have
  - **Binary vs. continuous outputs:** Binary labels can't be calibrated; continuous scores require threshold selection

- **Failure signatures:**
  - Tokenization mismatch between MT model and annotations (paper uses character-to-token mapping: label token as error if ≥1 character marked)
  - Aya23 lacks dropout → MCD variants unavailable for this model
  - Default XCOMET appears to "fail" not because it's wrong, but because it's miscalibrated (high precision, low recall)

- **First 3 experiments:**
  1. **Reproduce Surprisal MCD VAR on a single language pair:** Use DivEMT EN→IT, extract surprisal variance with n=10 MCD samples, compute AP against human post-edits. Compare to random baseline.
  2. **Test calibration impact:** Run both default XCOMET and XCOMET_CONF on same data. Plot precision-recall curves. Identify threshold where F1 is maximized.
  3. **Measure annotator sensitivity:** If you have QE4PE data with 6 annotators, compute metric correlations separately for each annotator's labels. Calculate variance in AP across annotators to quantify brittleness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced interpretability methods (e.g., tuned vocabulary projections, confidence neuron identification) close the remaining performance gap between unsupervised and supervised WQE metrics?
- **Basis in paper:** [explicit] The Limitations section states: "future work should leverage the latest insights from more advanced techniques requiring, for example, the tuning of vocabulary projections (Belrose et al., 2023; Yom Din et al., 2024) or the identification of 'confidence neurons' modulating predictive entropy (Stolfo et al., 2024)."
- **Why unresolved:** Current unsupervised metrics, while promising (Surprisal MCD VAR matches default XCOMET), still lag behind confidence-weighted supervised methods; the paper only tested simpler, easier-to-implement interpretability approaches.
- **What evidence would resolve it:** Systematic comparison of WQE performance using advanced interpretability techniques on the same datasets, showing whether these methods can match or exceed XCOMET-CONF performance.

### Open Question 2
- **Question:** Can MCD-based uncertainty estimation be adapted for computationally efficient application to large decoder-only LLMs?
- **Basis in paper:** [explicit] The Limitations section notes: "unsupervised methods based on MCD to require substantial computational resources, and as such we could not evaluate them on Aya23 35B."
- **Why unresolved:** The best-performing unsupervised metric (Surprisal MCD VAR) cannot be practically applied to large LLMs due to computational constraints, limiting its real-world applicability for state-of-the-art systems.
- **What evidence would resolve it:** Development and evaluation of efficient MCD approximation methods for large LLMs, demonstrating comparable performance to full MCD with reduced computational cost.

### Open Question 3
- **Question:** How does incorporating fine-grained error severity levels affect WQE metric performance and calibration?
- **Basis in paper:** [inferred] The Limitations section acknowledges: "Our evaluation for WMT24 treats available error spans as binary labels and does not directly account for error severity in human-annotated spans."
- **Why unresolved:** Error severity (MINOR/MAJOR/CRITICAL) is annotated but collapsed to binary labels in the study; it is unclear whether metrics differentially detect errors of varying severity or whether severity-weighted evaluation would change metric rankings.
- **What evidence would resolve it:** Re-evaluation of metrics using severity-weighted annotations, with analysis of whether metrics show differential sensitivity to minor vs. critical errors.

## Limitations
- **MCD implementation variability**: The paper uses n=10 MCD samples but doesn't specify dropout rate or which layers to apply dropout to, which could significantly impact variance estimates and cross-model comparability.
- **Tokenization alignment assumptions**: The binary labeling approach (token is error if ≥1 character marked) is pragmatic but may misalign with annotation intent, especially for subword tokenization artifacts.
- **Confidence calibration validation**: While XCOMET_CONF shows consistent improvement, the paper doesn't validate whether these confidence scores are properly calibrated probabilities or just monotonically related to error likelihood.

## Confidence

- **High confidence**: The core finding that unsupervised metrics (especially Surprisal MCD VAR) perform competitively with supervised approaches is well-supported by AP/F1* metrics across multiple datasets.
- **Medium confidence**: The claim about human label variation affecting metric evaluation is convincing but limited by the relatively small number of annotators (6 max) in available datasets.
- **Low confidence**: The mechanism linking epistemic uncertainty to translation errors is plausible but not definitively proven—correlation doesn't establish that MCD variance actually captures "knowledge gaps" rather than other factors.

## Next Checks

1. **MCD hyperparameter sensitivity test**: Run the same experiments with n=5 and n=20 MCD samples to determine if variance estimates are stable or hyperparameter-dependent.
2. **Calibration analysis for XCOMET**: Compute expected calibration error (ECE) for XCOMET_CONF scores to verify they represent true probabilities rather than just ordinal rankings.
3. **Cross-lingual generalization**: Test whether the relative performance ranking of metrics (MCD VAR > Surprisal > Entropy) holds across typologically diverse language pairs not in the original datasets.