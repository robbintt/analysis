---
ver: rpa2
title: 'Persistent Instability in LLM''s Personality Measurements: Effects of Scale,
  Reasoning, and Conversation History'
arxiv_id: '2508.04826'
source_url: https://arxiv.org/abs/2508.04826
tags:
- perplexity
- question-level
- size
- history
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PERSIST evaluates 25 LLM models (1B-685B parameters) across 2M+
  responses using personality questionnaires under varied conditions. Question reordering
  alone caused large shifts in personality measurements.
---

# Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History

## Quick Facts
- arXiv ID: 2508.04826
- Source URL: https://arxiv.org/abs/2508.04826
- Reference count: 40
- **Primary result:** Even 400B+ parameter models show standard deviations >0.3 on 5-point scales, indicating current LLMs lack architectural foundations for behavioral consistency.

## Executive Summary
PERSIST evaluates 25 LLM models (1B-685B parameters) across 2M+ responses using personality questionnaires under varied conditions. Question reordering alone caused large shifts in personality measurements. Scaling provided limited stability gains—even 400B+ models showed standard deviations >0.3 on 5-point scales. Chain-of-thought reasoning increased variability while decreasing perplexity. Detailed persona instructions had mixed effects, with misaligned personas increasing variability. LLM-adapted questionnaires showed comparable instability to human-centric versions. Conversation history amplified variability for smaller models. These findings indicate current LLMs lack architectural foundations for behavioral consistency, challenging their safe deployment in high-stakes applications.

## Method Summary
PERSIST evaluates 25 LLM models (1B-685B parameters) using BFI-44 and SD3 personality questionnaires under varied conditions: question reordering (250 permutations), paraphrasing (100 variants), personas (5 types), reasoning modes, and conversation history. Models respond with single-digit Likert scores (1-5). Instability is measured as standard deviation across 250 runs per question. Temperature=0 for most experiments, 0.6 for reasoning. Perplexity and statistical tests (Wilcoxon, Kruskal-Wallis, Mann-Whitney U) supplement analysis.

## Key Results
- Question reordering alone caused large shifts in personality measurements across all model sizes.
- Chain-of-thought reasoning increased variability while decreasing perplexity.
- Larger models (>50B) showed marginally better stability, but even 400B+ models had SD > 0.3.
- Conversation history amplified variability for smaller models but improved consistency for larger ones.
- LLM-adapted questionnaires showed comparable instability to human-centric versions.

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Superposition and Contextual Sampling
LLMs do not possess a single stable "personality" but rather a superposition of myriad personas derived from training data, differentially activated by context changes (e.g., question order). The model samples from a distribution of behavioral patterns, and when context shifts, the attention mechanism activates different prior contexts or "personas" from the training distribution, resulting in inconsistent trait expression. This implies "behavioral consistency" is not an inherent architectural property but a context-dependent retrieval state.

### Mechanism 2: Reasoning-Induced Path Dependence
Chain-of-thought (CoT) reasoning increases behavioral variability because the act of generating justifications creates divergent paths that condition the final output. CoT forces the model to generate intermediate reasoning tokens. If the model samples different "reasoning steps" (stochasticity in explanation), these steps act as new prompts that steer the final answer. This creates a "butterfly effect" where the justification causally determines the personality score, rather than revealing a pre-existing stable trait.

### Mechanism 3: Priming via Conversation History
Conversation history acts as a strong primer that narrows or shifts the behavioral distribution, but this effect is unstable for smaller models which lack the capacity to integrate history consistently. For smaller models (<50B parameters), the context window fills with previous turns that may contain noise or subtle inconsistencies. The model overfits to this immediate history ("priming"), amplifying drift. Larger models may use history to "lock in" a persona, but smaller models degrade into noise.

## Foundational Learning

- **Concept: Psychometric Stability vs. Validity**
  - **Why needed here:** The paper measures *stability* (consistency of scores across runs), not just *validity* (whether the test measures what it claims). A model could be stably wrong or unstably "correct."
  - **Quick check question:** If a model gives the same personality score for "Extraversion" regardless of whether it's prompted to be a "Monk" or a "Comedian," does it have high or low stability? (High stability, but arguably low validity/controllability).

- **Concept: Perplexity-Behavior Dissociation**
  - **Why needed here:** The paper finds that reasoning lowers perplexity (model is "confident") but raises variability (model is "unstable"). Engineers often use perplexity as a proxy for quality, but this paper proves it is a poor proxy for behavioral consistency.
  - **Quick check question:** Can a model have low perplexity (high confidence) on its tokens but still produce completely different answers on the same question run twice? (Yes, as shown with CoT reasoning).

- **Concept: Positional and Order Sensitivity**
  - **Why needed here:** The finding that "question reordering alone caused large shifts" implies the model lacks invariance to trivial input modifications.
  - **Quick check question:** Why should shuffling the order of 44 questions change the measured "Openness" of a model if the model truly *has* a trait? (It shouldn't, unless the model is suffering from priming or recency bias).

## Architecture Onboarding

- **Component map:** vLLM inference -> BFI-44/SD3 questionnaires -> 250 permutations -> single-digit Likert scores (1-5) -> Standard Deviation calculation -> Trait-level instability measurement

- **Critical path:** 1) Select model and persona prompt 2) Run 250 permutations of question order (Temperature=0) 3) Extract single-digit Likert scores (1-5) 4) Calculate Standard Deviation (SD) of scores for each question across the 250 runs 5) Aggregate SD to trait level to measure "Instability"

- **Design tradeoffs:**
  - **Temperature:** The authors used Temp=0.6 for reasoning models because they "perform poorly at temp 0." This introduces a confound—does variability come from "reasoning" or "higher temperature"? The paper attempts to isolate this by comparing reasoning/non-reasoning at the same temp where possible, but the core result relies on this distinction.
  - **LLM-adapted Questions:** Designed to remove human-centric concepts (e.g., "depressed"), but results showed instability persisted, validating the use of standard human psychometrics for testing.

- **Failure signatures:**
  - **SD > 0.3:** The threshold identified for "significant instability" even in 400B+ models.
  - **Inverse Correlation:** Reasoning ON + Perplexity DOWN + Variability UP. This is the counter-intuitive failure mode to watch for.

- **First 3 experiments:**
  1. **Order Sensitivity Baseline:** Run a 10-question personality subset on a 7B and 70B model with 50 shuffle permutations at Temp=0. Verify if SD drops with scale.
  2. **Reasoning Paradox Check:** Enable CoT on a model (Temp=0.6) vs Standard (Temp=0.6) on the same questions. Check if the reasoning runs show higher SD despite "feeling" more coherent.
  3. **History Overflow:** Feed a 20-turn conversation history to a <10B model and ask a personality question. Compare the SD of the answer vs. a zero-history context to verify the "small model amplification" effect.

## Open Questions the Paper Calls Out

### Open Question 1
Does reduced variability in larger LLMs reflect genuine behavioral stability or merely ceiling effects from extreme trait scores? The authors state: "Moreover, as model size increases, trait scores increasingly diverge from human population norms, which may reduce variability through ceiling effects rather than genuine behavioral stability." This remains unresolved because the current analysis cannot disentangle whether lower SDs in larger models stem from architectural improvements or constrained response ranges due to extreme mean scores.

### Open Question 2
Can models with situational awareness strategically modify personality responses during evaluation? The limitations section notes: "Strategic deception cannot be entirely ruled out. If models possess sufficient situational awareness to recognize evaluation contexts, they might modify responses accordingly." This remains unresolved because the study did not test whether models recognize evaluation contexts or whether random permutations can resist alignment faking.

### Open Question 3
How well do self-report questionnaire instabilities translate to actual behavioral inconsistencies in deployed applications? The authors acknowledge: "Our focus on self-report measures may not fully capture how instabilities manifest in actual model behavior." While recent evidence suggests correlations between self-reports and behavioral outputs exist, the validity of self-report instruments for evaluating LLMs has been challenged.

### Open Question 4
Do LLM-adapted personality instruments achieve construct validity comparable to human-validated instruments? The authors state: "Both traditional and LLM-adapted instruments lack formal psychometric validation for LLM use; this would require other analysis such as factor loading and internal consistency through Cronbach's α." The LLM-adapted questionnaires were constructed to improve ecological validity but lack established reliability and validity metrics.

## Limitations

- **Architectural Basis Uncertainty:** The paper demonstrates instability but does not definitively establish whether this stems from fundamental architectural limitations or training artifacts.
- **Temperature Confound:** Reasoning experiments use temperature 0.6 while non-reasoning uses 0.0, creating ambiguity about whether observed variability differences stem from reasoning mechanisms or stochasticity.
- **Single Invalid Response Rule:** Discarding entire runs based on one invalid response (0-0.35% occurrence rate) may introduce selection bias, particularly for smaller models that showed higher invalid rates.

## Confidence

- **High Confidence:** Question reordering causing instability, CoT increasing variability despite lowering perplexity, and larger models showing marginally better stability than smaller models.
- **Medium Confidence:** Conversation history effects being model-size dependent, and LLM-adapted questionnaires showing comparable instability to human-centric versions.
- **Low Confidence:** The fundamental claim about architectural limitations precluding behavioral consistency. While the evidence strongly suggests this direction, alternative explanations (training data, fine-tuning procedures) cannot be ruled out with current experiments.

## Next Checks

1. **Temperature Isolation Experiment:** Replicate the reasoning vs. non-reasoning comparison at temperature 0.0 for both conditions to isolate whether reasoning or stochasticity drives variability increases.

2. **Architectural Ablation Study:** Test whether adding explicit memory mechanisms (e.g., long-term memory layers) to smaller models reduces history-induced variability, providing evidence for architectural rather than training-based explanations.

3. **Cross-Lingual Stability Assessment:** Apply the same stability tests to multilingual models across different language pairs to determine if instability is universal or culturally/linguistically specific.