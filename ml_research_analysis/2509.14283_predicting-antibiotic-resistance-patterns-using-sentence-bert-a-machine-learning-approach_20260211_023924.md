---
ver: rpa2
title: 'Predicting Antibiotic Resistance Patterns Using Sentence-BERT: A Machine Learning
  Approach'
arxiv_id: '2509.14283'
source_url: https://arxiv.org/abs/2509.14283
tags:
- antibiotic
- clinical
- resistance
- neural
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of antibiotic resistance in
  sepsis by developing machine learning models to predict antibiotic susceptibility
  patterns using clinical documentation. The approach involves generating Sentence-BERT
  embeddings from clinical notes in the MIMIC-III database and applying Neural Networks
  and XGBoost models to classify antibiotic susceptibility.
---

# Predicting Antibiotic Resistance Patterns Using Sentence-BERT: A Machine Learning Approach

## Quick Facts
- arXiv ID: 2509.14283
- Source URL: https://arxiv.org/abs/2509.14283
- Reference count: 0
- Primary result: XGBoost model achieved average F1 score of 0.86 and AUC of 0.78 for predicting antibiotic susceptibility from clinical notes

## Executive Summary
This study addresses the critical challenge of antibiotic resistance in sepsis by developing machine learning models that predict antibiotic susceptibility patterns using only clinical documentation. The approach leverages Sentence-BERT embeddings to transform unstructured clinical notes from the MIMIC-III database into fixed-dimensional vectors, which are then used to classify antibiotic resistance for seven common antibiotics. The XGBoost model demonstrated superior performance with an average F1 score of 0.86 and AUC of 0.78, while Neural Networks achieved slightly lower metrics. Meropenem resistance prediction showed the highest performance for both models, reaching F1 scores of 0.91. The study demonstrates the potential of document embeddings in clinical settings for improving antimicrobial stewardship decisions, though results are limited to a single hospital system.

## Method Summary
The study extracted microbiological cultures with antibiotic susceptibility results from the MIMIC-III database and linked them to clinical notes recorded on or before the culture collection day. Sentence-BERT embeddings were generated from these clinical notes to capture semantic information about patient conditions, prior treatments, and risk factors. Two machine learning models were trained to predict binary antibiotic susceptibility (sensitive vs. resistant/intermediate): a Neural Network with one hidden layer (100 neurons, ReLU activation) and an XGBoost ensemble. Models were evaluated using stratified 10-fold cross-validation across seven antibiotics, reporting both F1 scores and AUC metrics. The approach intentionally used only clinical documentation, excluding structured tabular data like vital signs and lab results.

## Key Results
- XGBoost achieved average F1 score of 0.86 and AUC of 0.78 across seven antibiotics
- Neural Networks achieved average F1 score of 0.84 and AUC of 0.76
- Meropenem resistance prediction showed highest performance with F1 score of 0.91 for both models
- Ceftriaxone showed lowest performance with F1 score of 0.73 for XGBoost and 0.71 for Neural Networks
- Study demonstrates document embeddings can capture resistance-relevant signals in clinical text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-BERT embeddings extracted from clinical notes encode latent phenotypic signals associated with antibiotic resistance patterns.
- Mechanism: S-BERT transforms unstructured clinical documentation into fixed-dimensional vector representations that capture semantic relationships in patient histories, comorbidities, and prior healthcare exposures—factors known to correlate with resistance risk.
- Core assumption: Clinical documentation contains sufficiently consistent and predictive signals that map to resistance outcomes, and these signals are preserved through the embedding transformation.
- Evidence anchors:
  - [abstract] "we generated Sentence-BERT embeddings from clinical notes and applied Neural Networks and XGBoost to predict antibiotic susceptibility"
  - [methods] "These notes were processed using Sentence-BERT (S-BERT) to generate embedding vectors that encapsulate the clinical context"
  - [corpus] Limited direct corpus validation; related work on AMR prediction uses genomic/tabular features rather than document embeddings, suggesting this approach remains relatively unexplored.
- Break condition: If documentation variability across providers or institutions introduces noise exceeding the signal strength, embedding quality degrades and predictive performance collapses toward baseline.

### Mechanism 2
- Claim: Aggregating clinical notes recorded on or before the culture collection day provides temporally valid predictive signal for resistance classification.
- Mechanism: Historical documentation captures prior antibiotic exposures, hospitalization duration, nursing facility residence, and comorbidities—established risk factors for resistant organisms—enabling prediction before culture results are available.
- Core assumption: Resistance-determining patient characteristics are documented prior to or on the day of culture collection, not only in response to culture results.
- Evidence anchors:
  - [methods] "clinical notes recorded on or before the culture day"
  - [results] "median number of clinical documentation entries per subject was 3 (IQR: 1-8)"
  - [corpus] "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis" (neighbor paper) supports temporal signal utility for MDR prediction.
- Break condition: If critical resistance predictors are only documented after culture day or are captured in structured fields not included in notes, the model will underperform on edge cases.

### Mechanism 3
- Claim: XGBoost's gradient-boosted tree ensemble slightly outperforms a single-hidden-layer MLP for classification on document embeddings in this task.
- Mechanism: XGBoost handles high-dimensional embedding features through iterative error correction and feature interaction learning, while the MLP's single 100-neuron layer may underfit the embedding space complexity.
- Core assumption: The embedding-to-resistance mapping contains non-linear feature interactions better captured by tree ensembles than a shallow neural architecture.
- Evidence anchors:
  - [results] "XGBoost achieved an average AUC of 0.78 (SD 0.03) and an average F1 score of 0.86... Neural Networks had an average AUC of 0.76 (SD 0.03) and an average F1 score of 0.84"
  - [results] "XGBoost achieved the highest AUC of 0.81 and the highest F1 score of 0.91 for Meropenem"
  - [corpus] "Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble" supports XGBoost utility in AMR tasks.
- Break condition: If deeper neural architectures or attention mechanisms over embeddings are introduced, the performance gap may shift; current comparison is limited to shallow MLP.

## Foundational Learning

- Concept: **Sentence-BERT (S-BERT) and Document Embeddings**
  - Why needed here: The entire pipeline depends on converting unstructured clinical text into fixed vectors; understanding what semantic information is preserved vs. lost is critical for debugging performance.
  - Quick check question: Given a 768-dimensional S-BERT embedding, would you expect two notes describing the same patient condition with different vocabulary to have high cosine similarity?

- Concept: **Stratified K-Fold Cross-Validation for Imbalanced Clinical Outcomes**
  - Why needed here: Resistance patterns are often imbalanced (fewer resistant cases); stratified sampling ensures each fold maintains class proportions for reliable performance estimation.
  - Quick check question: If resistant cases comprise 15% of your dataset, what would happen to your F1 estimate if you used standard (non-stratified) 10-fold CV?

- Concept: **F1 Score vs. AUC for Clinical Classification Tasks**
  - Why needed here: The paper reports both metrics; F1 emphasizes performance at a fixed threshold (critical for binary clinical decisions), while AUC captures ranking ability across thresholds.
  - Quick check question: In a clinical deployment scenario where you must make a binary resistant/not-resistant call, which metric better reflects operational performance?

## Architecture Onboarding

- Component map:
  - MIMIC-III database -> clinical notes + microbiological cultures -> Sentence-BERT embeddings -> XGBoost or Neural Network -> antibiotic susceptibility prediction

- Critical path:
  1. Extract microbiological cultures with antibiotic susceptibility results from MIMIC-III
  2. Retrieve all clinical notes documented on or before each culture day
  3. Generate S-BERT embeddings for each note (aggregation method for multiple notes per patient not specified—assumption: concatenation, mean pooling, or per-note prediction)
  4. Train classifiers to predict susceptibility (S vs. R/I) using embeddings as features
  5. Evaluate via stratified 10-fold CV; report per-antibiotic and average metrics

- Design tradeoffs:
  - Notes-only vs. multimodal: Paper acknowledges excluding structured/tabular data; this sacrifices potential signal from labs, vitals, and demographics
  - Binary classification: Intermediate susceptibility merged with resistant; this may inflate "resistant" class size but clinically conservative
  - Single hidden layer MLP: Simple architecture limits capacity; tradeoff between interpretability and expressiveness
  - XGBoost hyperparameters: Defaults (lr=0.3, max_depth=6) are aggressive; may overfit on smaller antibiotic-specific subsets

- Failure signatures:
  - Performance drops significantly on antibiotics with fewer resistant cases (class imbalance)
  - High variance across folds suggests overfitting to specific documentation patterns
  - Site-specific documentation norms limit generalization (acknowledged: single hospital system, one region)
  - Missing documentation for patients with short stays (median 3 notes, IQR 1-8) may underrepresent their risk factors

- First 3 experiments:
  1. **Embedding extraction validation**: Pull a sample of 100 clinical notes from MIMIC-III, generate S-BERT embeddings, and verify output dimensions; manually inspect nearest neighbors in embedding space to confirm semantic clustering.
  2. **Baseline reproduction with single antibiotic**: Train XGBoost on Meropenem susceptibility (highest reported F1=0.91) using 10-fold stratified CV; confirm F1 and AUC fall within reported ranges before expanding to all antibiotics.
  3. **Multi-note aggregation ablation**: Test whether mean-pooling vs. max-pooling vs. using only the most recent note affects performance; the paper does not specify aggregation, so this is a critical unknown.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance of the S-BERT based prediction model be maintained when validated on external healthcare systems with different antimicrobial stewardship patterns?
- Basis in paper: [explicit] The authors state, "The next steps in our study include validating the model retrospectively and prospectively on Duke Health data."
- Why unresolved: The current study utilized only the MIMIC-III database, which represents a single hospital system and region, limiting the generalizability of the findings.
- What evidence would resolve it: Retrospective and prospective validation results showing similar F1 and AUC scores when applied to the Duke Health dataset.

### Open Question 2
- Question: What explainability methods can be effectively integrated to clarify the reasoning behind the model's predictions for clinical end-users?
- Basis in paper: [explicit] The paper notes that "adoption of these tools at bedside may face challenges related to the lack of explainability; therefore, we plan to explore effective methods for providing clear reasoning."
- Why unresolved: Document embeddings and Neural Networks act as "black boxes," making it difficult for clinicians to trust or verify the specific factors driving a resistance prediction.
- What evidence would resolve it: The development and testing of an interpretability layer (e.g., feature importance or attention visualization) that allows clinicians to identify which text segments influenced the classification.

### Open Question 3
- Question: Does the integration of structured tabular data (e.g., vital signs, lab results) with unstructured clinical notes improve prediction accuracy compared to text embeddings alone?
- Basis in paper: [inferred] The authors acknowledge a limitation: "other tubular data can carry important information does not exist in documentation."
- Why unresolved: The study isolated clinical notes to test S-BERT efficacy, leaving the potential performance gain from a multimodal data fusion approach untested.
- What evidence would resolve it: A comparative study where models are trained on both S-BERT embeddings and structured patient data to see if performance metrics exceed the current baseline of 0.86 F1.

## Limitations

- Single-center study using only MIMIC-III database, limiting generalizability to other healthcare systems
- Performance may be inflated on antibiotics with low resistance prevalence due to class imbalance
- Critical implementation details missing: S-BERT model variant, note aggregation method, and training hyperparameters

## Confidence

- High confidence in the general approach validity (document embeddings can capture resistance-relevant signals)
- Medium confidence in specific performance metrics (F1=0.86, AUC=0.78) due to undisclosed implementation details
- Medium confidence in clinical utility given single-site data and limited external validation

## Next Checks

1. Verify temporal integrity by confirming no notes from after culture collection are included in training data
2. Replicate results on a single antibiotic (Meropenem) to validate the core methodology before scaling
3. Test alternative document aggregation strategies (mean vs. max pooling) to determine optimal handling of multiple notes per patient