---
ver: rpa2
title: 'Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery
  in Large Language Models'
arxiv_id: '2509.23108'
source_url: https://arxiv.org/abs/2509.23108
tags:
- imagery
- reasoning
- mental
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study tested whether large language models (LLMs) can perform\
  \ mental imagery tasks traditionally thought to require visual imagery. Researchers\
  \ created 60 novel mental imagery tasks\u201448 newly designed items and 12 adapted\
  \ from prior work\u2014where participants followed written instructions to transform\
  \ imagined shapes and letters and identify the resulting object."
---

# Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.23108
- **Source URL:** https://arxiv.org/abs/2509.23108
- **Authors:** Morgan McCarty; Jorge Morales
- **Reference count:** 40
- **Primary result:** Best LLMs (GPT-5, o3, o3-Pro) significantly outperformed humans on mental imagery tasks without using visual input

## Executive Summary
This study tested whether large language models can perform mental imagery tasks traditionally thought to require visual imagery. Researchers created 60 novel mental imagery tasks where participants followed written instructions to transform imagined shapes and letters and identify the resulting object. Human subjects averaged 54.7% performance, while the best LLMs—GPT-5 and OpenAI's o3/o3-Pro—significantly outperformed humans at 67.0%, 64.1%, and 66.6% respectively (p < .00001). Image-aided reasoning did not improve results and sometimes reduced performance. These findings suggest that LLMs can complete imagery-dependent tasks using language alone, challenging the idea that such tasks require pictorial mental imagery.

## Method Summary
The study evaluated LLMs on a mental imagery object reconstruction task adapted from Finke et al. (1989), where models must follow 3-5 written instructions to mentally transform and combine letters/shapes, then label the resulting object. Zero-shot inference was performed via API using 60 instruction sets (48 novel, 12 adapted) involving 2-4 transformation steps. Performance was measured by a weighted score derived from human evaluators rating how well model labels matched reference images on a 5-point scale. OpenAI models used `reasoning_effort='high'` with temperature fixed at ~1.0, while other models used `temperature=0.1`. Claude models were allocated 4000-9000 tokens for extended thinking.

## Key Results
- GPT-5 achieved 67.0% accuracy, significantly outperforming human baseline of 54.7% (p < .00001)
- OpenAI's o3 and o3-Pro scored 64.1% and 66.6% respectively, also significantly above human performance
- Image-aided reasoning decreased performance (o3 dropped 8.8% with images), suggesting compositional generation errors offset potential benefits
- Higher reasoning token allocation improved results, supporting the mechanism of intermediate state tracking

## Why This Works (Mechanism)

### Mechanism 1: Propositional Reasoning Without Pictorial Representation
- **Claim:** LLMs solve imagery tasks through propositional/language-based reasoning alone
- **Mechanism:** Text-only models manipulate linguistic tokens to represent and transform spatial relationships without pictorial mental images
- **Evidence:** Abstract states LLMs can complete imagery-dependent tasks using language alone, challenging pictorial imagery requirements
- **Break condition:** If models required multimodal visual input to achieve comparable performance

### Mechanism 2: Reasoning Token Scaling
- **Claim:** Performance improves when models allocate more tokens to intermediate reasoning steps
- **Mechanism:** Extended reasoning traces allow models to maintain and manipulate intermediate representations across multiple transformation steps
- **Evidence:** Abstract and section 3.2 show higher reasoning effort led to improved results
- **Break condition:** If increasing reasoning tokens degraded performance or showed no effect

### Mechanism 3: Spatial-Relational Extraction from Text
- **Claim:** LLMs extract and manipulate spatial relations from textual descriptions without forming object-level visual representations
- **Mechanism:** Models acquire spatial concept representations through training on procedural descriptions
- **Evidence:** Section 4 references neurological evidence of preserved spatial reasoning despite lost conscious imagery
- **Break condition:** If LLMs failed specifically on tasks requiring spatial transformations

## Foundational Learning

- **Concept: Pictorial vs. Propositional Imagery Debate**
  - **Why needed:** The paper's core claim depends on understanding this 50-year debate about whether mental imagery requires picture-like representations or can be accomplished through language-like propositions
  - **Quick check:** Can you explain why Finke et al.'s object reconstruction task was designed to require pictorial imagery?

- **Concept: Aphantasia and Cognitive Compensation**
  - **Why needed:** Aphantasics' near-normal performance on imagery tasks provides the human analog to LLMs' text-only architecture
  - **Quick check:** What verbal strategies might an aphantasic use to solve "imagine a D rotated 90° left with a J attached to its bottom"?

- **Concept: Reasoning Token Budget Allocation**
  - **Why needed:** Understanding how hyperparameters like `reasoning_effort` affect model behavior is critical for reproducing results
  - **Quick check:** What happened to performance when reasoning effort was set from "high" to "minimal" in GPT-5?

## Architecture Onboarding

- **Component map:** 60 instruction sets -> Text-only LLM inference (o3, GPT-5, Claude, Gemini) -> Human evaluation (crowd + experts) -> Weighted scoring
- **Critical path:** Design novel instruction sets -> Run models with high reasoning effort -> Collect human baseline -> Grade responses -> Compare using chi-square tests
- **Design tradeoffs:** Single vs. multiple context showed no significant difference; multiple-context preferred for cost efficiency; image augmentation decreased performance by 8.8%
- **Failure signatures:** Hyper-literal responses graded highly by crowd but penalized by experts; zero uncertainty expression in LLMs vs. 54 human "I don't know" responses
- **First 3 experiments:** 1) Replicate o3 high-reasoning vs. human baseline on 48 novel items; 2) Test whether chain-of-thought prompting matches native reasoning token allocation; 3) Design spatial-only variant to isolate spatial vs. object imagery contributions

## Open Questions the Paper Calls Out

- **What specific strategies do aphantasics use to complete mental imagery tasks, and do these strategies parallel the computational mechanisms employed by LLMs?**
  - **Basis:** Paper suggests follow-up study comparing aphantasic strategies to LLM approaches
  - **Why unresolved:** One aphantasic subject performed 5th highest among humans, but strategies couldn't be determined
  - **Evidence needed:** Comparative think-aloud protocols or cognitive process tracing from both aphantasics and LLM chain-of-thought analysis

- **Can mechanistic interpretability methods identify propositional, iconic, or spatial representational formats within the weights of open-weight models that succeed on this task?**
  - **Basis:** Paper proposes examining model weights for direct evidence of representational formats
  - **Why unresolved:** Current frontier models are closed-weight, preventing direct examination
  - **Evidence needed:** Replication with high-performing open-weight models using probing techniques

- **Why does image-aided reasoning decrease performance on this task, and under what conditions might external visual scaffolding help rather than hinder?**
  - **Basis:** Paper found consistent performance drops with image generation but couldn't determine underlying causes
  - **Why unresolved:** Couldn't distinguish between compositional generation failures, attention misallocation, or modality conflicts
  - **Evidence needed:** Controlled experiments varying image generation timing, quality, and integration method

- **What is the upper bound of LLM propositional reasoning capacity when freed from human working memory constraints?**
  - **Basis:** Current task designed around human cognitive limits; frontier models may have untapped capacity
  - **Why unresolved:** Unknown how models perform with many more than 3-5 steps and 4 objects
  - **Evidence needed:** Systematic scaling of instruction set complexity until performance plateaus or declines

## Limitations
- Reliance on zero-shot inference without iterative refinement or fine-tuning limits understanding of learned strategies
- Human baseline of 54.7% accuracy may reflect task difficulty rather than inherent imagery capability
- Figurative labeling introduces semantic ambiguity that may advantage models trained on rich linguistic associations

## Confidence

- **High confidence:** LLMs can outperform humans on the specific mental imagery object reconstruction task when using text-only reasoning (p < .00001)
- **Medium confidence:** Performance differences reflect propositional rather than pictorial reasoning, not requiring visual imagery generation
- **Medium confidence:** Increased reasoning token allocation improves performance through better intermediate state tracking
- **Low confidence:** Results generalize to broader mental imagery capabilities or challenge fundamental theories of human visual cognition

## Next Checks
1. **Internal representation analysis:** Use attention visualization and feature attribution methods to examine whether models activate visual-like representations during imagery tasks, even when responding with text labels
2. **Aphantasic population replication:** Test the same task with aphantasic individuals using controlled verbal strategies to determine if their performance patterns match LLM approaches
3. **Transfer learning experiment:** Train models on simpler spatial transformation tasks and measure whether this improves imagery task performance, distinguishing between learned reasoning strategies versus inherent capabilities