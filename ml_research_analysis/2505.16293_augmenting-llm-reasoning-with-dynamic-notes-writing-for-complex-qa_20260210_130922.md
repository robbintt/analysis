---
ver: rpa2
title: Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA
arxiv_id: '2505.16293'
source_url: https://arxiv.org/abs/2505.16293
tags:
- search
- first
- arxiv
- action
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NotesWriting, a method to enhance the effective
  context length of iterative retrieval-augmented generation (RAG) systems by generating
  concise, query-relevant notes from retrieved documents at each reasoning step. This
  reduces noise, avoids context overload, and improves LLM planning and reasoning
  abilities.
---

# Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA

## Quick Facts
- **arXiv ID**: 2505.16293
- **Source URL**: https://arxiv.org/abs/2505.16293
- **Reference count**: 40
- **Primary result**: NotesWriting improves multi-hop QA performance by 15.6 percentage points while reducing token usage

## Executive Summary
This paper introduces NotesWriting, a method to enhance iterative retrieval-augmented generation (RAG) systems for complex multi-hop question answering. The approach generates concise, query-relevant notes from retrieved documents at each reasoning step, reducing noise and avoiding context overload. By replacing raw documents with distilled notes in the LLM context, the method improves planning and reasoning abilities while being framework-agnostic. Experiments on four multi-hop QA datasets using two models show substantial performance gains with reduced token consumption and fewer reasoning steps.

## Method Summary
NotesWriting enhances iterative RAG systems by introducing a note-taking LLM (LMnotes) that processes each retrieved document to extract concise, query-relevant information. During each iteration, the system retrieves top-5 Wikipedia pages per query, converts them to Markdown, and passes them through the notes-writing module. The notes-writing LLM generates YES#/NO# formatted notes from each document, which are aggregated and provided as observation to the main LLM instead of raw documents. This approach is integrated with ReAct framework where after each search action, documents are retrieved, notes are generated, and notes are returned as observation until the finish action or maximum iterations are reached.

## Key Results
- Achieves an average improvement of 15.6 percentage points in performance across four multi-hop QA datasets
- Reduces input tokens to the main LLM by replacing full documents with concise notes
- Decreases average number of reasoning steps needed to answer questions
- In ReAct setting (ReNAct), improves efficiency, redundancy, and coherence of reasoning chains

## Why This Works (Mechanism)
The method works by addressing the core challenge of iterative RAG systems: context overload from passing full retrieved documents to the main LLM. By distilling documents into concise, query-relevant notes at each iteration, NotesWriting reduces noise while preserving essential information. This allows the main LLM to focus on reasoning rather than processing irrelevant document content. The aggregated notes serve as a cleaner observation that guides subsequent reasoning steps more effectively than raw documents.

## Foundational Learning
- **Iterative RAG**: Why needed - to handle complex questions requiring multiple retrieval steps; Quick check - can retrieve and process documents in sequence until answer found
- **Context window management**: Why needed - LLMs have limited token capacity; Quick check - monitor token counts per iteration
- **Document summarization**: Why needed - to extract relevant information from retrieved content; Quick check - verify notes contain query-relevant facts
- **Framework integration**: Why needed - to work with existing RAG architectures; Quick check - can plug into ReAct, IRCoT, or FLARE workflows

## Architecture Onboarding

**Component Map**: Query -> Wikipedia API -> Document retrieval (top-5) -> LMnotes -> Note generation -> Note aggregation -> Main LLM (ReAct/IRCoT/FLARE) -> Answer

**Critical Path**: The core workflow involves: (1) retrieve documents via Wikipedia API, (2) generate notes using LMnotes, (3) aggregate notes, (4) provide notes as observation to main LLM, (5) continue until finish action or max iterations.

**Design Tradeoffs**: 
- Pros: Reduces context window usage, improves reasoning efficiency, framework-agnostic
- Cons: Introduces dependency on note-writing LLM quality, potential information loss during summarization

**Failure Signatures**: 
- Context window exceeded (baseline methods fail on ~80% of FanoutQA/FRAMES questions)
- Hallucination in notes (note-taking LLM may extract incorrect information)
- Conflicting information from multiple documents

**3 First Experiments**:
1. **Token Reduction Validation**: Compare input token counts with and without NotesWriting across all datasets
2. **Performance Baseline**: Run ReAct baseline without notes to establish performance floor
3. **Note Quality Check**: Manually evaluate sample notes for accuracy and relevance

## Open Questions the Paper Calls Out
1. **Model Transferability**: Does performance improvement transfer to smaller open-source models (<8B parameters), given the extraction task requires distinct instruction-following capabilities? The study only validates on GPT-4o-mini and Llama-3.1-70B.

2. **Conflict Detection**: How can NotesWriting be enhanced to detect and resolve conflicting information or hallucinations from the note-taking LLM? Current method lacks verification steps.

3. **Data Heterogeneity**: Is NotesWriting robust when applied to unstructured web data compared to structured Wikipedia pages? Current evaluation is limited to Wikipedia API.

## Limitations
- Performance depends on the quality of the notes-writing LLM, introducing an additional failure point
- Evaluated only on English Wikipedia-based datasets, limiting generalizability to other knowledge sources
- May lose critical context when distilling documents into concise notes
- Requires careful tuning to balance conciseness with completeness

## Confidence
- **High Confidence**: Claims about reducing input token counts to main LLM are strongly supported by empirical results
- **Medium Confidence**: 15.6 percentage point average improvement is well-documented but may vary with different models and datasets
- **Low Confidence**: Claims about framework-agnostic applicability are theoretical rather than empirically validated across diverse RAG architectures

## Next Checks
1. **Robustness Testing**: Evaluate performance when notes-writing LLM produces erroneous or incomplete notes to understand failure modes
2. **Ablation Study**: Test impact of varying retrieved document count (k) and maximum iterations (T) to identify optimal configurations
3. **Cross-Domain Evaluation**: Assess performance on non-Wikipedia knowledge sources (scientific literature, news articles) to validate generalizability