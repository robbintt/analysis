---
ver: rpa2
title: 'Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better
  Reasoning Ability'
arxiv_id: '2512.01848'
source_url: https://arxiv.org/abs/2512.01848
tags:
- safety
- reasoning
- arxiv
- preprint
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates safety limitations of supervised fine-tuning
  (SFT) for large reasoning models (LRMs) and introduces reinforcement learning (RL)
  as a superior alternative. While SFT on safety-oriented long chain-of-thought datasets
  yields inconsistent safety gains, degrades reasoning performance, and lacks cross-model
  generalization, RL optimization achieves stronger and more consistent safety improvements
  while preserving or enhancing reasoning ability.
---

# Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability

## Quick Facts
- **arXiv ID:** 2512.01848
- **Source URL:** https://arxiv.org/abs/2512.01848
- **Reference count:** 0
- **Primary result:** RL optimization achieves stronger and more consistent safety improvements than SFT while preserving or enhancing reasoning ability in large reasoning models.

## Executive Summary
This paper investigates safety limitations of supervised fine-tuning (SFT) for large reasoning models (LRMs) and introduces reinforcement learning (RL) as a superior alternative. While SFT on safety-oriented long chain-of-thought datasets yields inconsistent safety gains, degrades reasoning performance, and lacks cross-model generalization, RL optimization achieves stronger and more consistent safety improvements while preserving or enhancing reasoning ability. Empirical results across multiple model families (DeepSeek-R1-distilled, Qwen3, Granite-4.0-Tiny-Preview) and benchmarks (AttaQ, AIR-Bench, GPQA-Diamond, MATH500, AIME24/25) show RL-trained models outperform both SFT and base models on safety while maintaining or improving reasoning accuracy. Fine-grained analysis of reflection dynamics reveals RL suppresses unsafe exploratory reasoning while preserving reflective depth in legitimate tasks.

## Method Summary
The study compares SFT and RL approaches for aligning LRMs on safety while preserving reasoning capability. SFT uses OpenRLHF with 5 epochs on STAR-1 dataset, while RL employs REINFORCE++ algorithm with token-level KL penalties, PPO-style clipping, and mini-batch updates. The RL training runs for 500 episodes using the Skywork-Reward-V2 reward model on STAR-1 prompts. Both methods are evaluated on safety benchmarks (AttaQ, AIR-Bench) and reasoning benchmarks (MATH500, AIME24/25, GPQA-Diamond) across multiple model families. The analysis includes reflection-token entropy measurement to understand how each method affects reasoning exploration dynamics.

## Key Results
- RL-trained models achieve better safety-reasoning trade-offs than SFT across all tested model families
- RL maintains reasoning accuracy (GPQA-Diamond: 49.24→49.06) while improving safety (AttaQ: 0.37→0.92), whereas SFT degrades reasoning (GPQA-Diamond: 49.24→47.54)
- RL generalizes across model architectures, unlike SFT which shows distribution-dependent performance based on pretraining compatibility
- Entropy analysis shows RL reduces unsafe exploration (AttaQ: 0.24→0.09) while preserving legitimate reasoning exploration (AIME24: 3.12→3.00)

## Why This Works (Mechanism)

### Mechanism 1: Direct Policy Optimization vs. Trajectory Imitation
RL achieves stronger and more consistent safety gains than SFT by directly optimizing reasoning trajectories with reward feedback, rather than mimicking fixed supervision signals. Policy gradient optimization (REINFORCE++) shapes the conditional distribution πθ(·|x) over reasoning trajectories t and final answers y, maximizing expected safety reward R(x, t+y). This allows the model to discover safe reasoning paths dynamically rather than being constrained to pre-defined trajectories.

### Mechanism 2: Differential Entropy Regulation
RL suppresses unsafe exploratory reasoning (lower entropy) while preserving reflective depth for legitimate reasoning tasks (maintained entropy). Token-level entropy Ht at reflection tokens indicates reasoning forks. On unsafe prompts, RL reduces entropy from 0.24 (base) to 0.09, collapsing harmful exploration paths. On reasoning prompts, RL maintains entropy near base levels (3.00 vs 3.12), preserving multi-branch exploration needed for complex problem-solving.

### Mechanism 3: Architecture-Agnostic Reward Optimization
RL generalizes across model architectures without requiring data-model distribution alignment, unlike SFT which depends on pretraining compatibility. SFT's effectiveness correlates with memorization scores—DeepSeek models show stronger memorization of STAR-1 data than GRANITE, explaining SFT's weak transfer. RL's reward-driven optimization bypasses this by learning from outcome feedback rather than trajectory imitation.

## Foundational Learning

- **Concept: Policy Gradient / REINFORCE++**
  - **Why needed here:** Understanding how RL optimizes reasoning trajectories without a critic network, using token-level KL penalties and PPO-style clipping for stability.
  - **Quick check question:** Can you explain why REINFORCE++ removes the need for a critic while maintaining stable optimization compared to PPO?

- **Concept: Next-Token Entropy as Exploration Measure**
  - **Why needed here:** Interpreting entropy Ht at reflection tokens as a diagnostic for reasoning exploration vs. suppression, and understanding the differential behavior on safety vs. reasoning tasks.
  - **Quick check question:** What does low entropy at a "wait" token indicate about the model's reasoning state? How should this differ between a harmful-request prompt and a math problem?

- **Concept: Catastrophic Forgetting in Sequential Fine-Tuning**
  - **Why needed here:** Understanding why SFT degrades reasoning ability and how RL mitigates this through reward-guided rather than imitation-based learning.
  - **Quick check question:** Why does excessive SFT on safety data cause reasoning degradation, and what property of RL prevents this?

## Architecture Onboarding

- **Component map:** Base LRM (e.g., DeepSeek-R1-Distill-Qwen-7B, Qwen3-8B) -> Reward model (Skywork-Reward-V2-Llama-3.1-8B) -> REINFORCE++ optimizer -> Evaluation suite

- **Critical path:**
  1. Verify base model reasoning capability on MATH500/AIME before alignment
  2. Validate reward model calibration on safety benchmark subset
  3. Train RL policy for 500 episodes over STAR-1 prompt distribution
  4. Monitor entropy at reflection tokens during training (target: low on unsafe prompts, maintained on reasoning)
  5. Evaluate safety-reasoning trade-off; if reasoning degrades >2%, reduce KL penalty weight

- **Design tradeoffs:**
  - Episode count: 500 episodes balances safety gains vs. compute; fewer risks under-alignment, more risks over-regularization
  - Reward model choice: Skywork-Reward-V2 is state-of-the-art on RewardBench but may have domain gaps; alternative reward models may require recalibration
  - Prompt distribution: Using STAR-1 prompts ensures fair comparison with SFT baselines but may limit coverage of novel attack vectors

- **Failure signatures:**
  - Safety improves but AIME24/MATH500 drops >5%: Likely over-regularization; reduce episode count or increase KL penalty weight
  - Safety scores inconsistent across AttaQ categories: Reward model may have category-specific biases; audit reward model on failing categories
  - Cross-architecture transfer fails: Verify reward model outputs are meaningful for target architecture; may need architecture-specific reward calibration

- **First 3 experiments:**
  1. **Baseline replication:** Train SFT on STAR-1, evaluate on AttaQ + AIME24. Confirm SFT improves safety (AttaQ: 0.37→0.76) but may degrade reasoning (GPQA: 49.24→47.54).
  2. **RL safety-reasoning validation:** Train REINFORCE++ on STAR-1 prompts, measure safety (AttaQ, AIR-Bench) and reasoning (AIME24, MATH500, GPQA-Diamond). Expect safety ≥ SFT baseline with reasoning ≥ base model.
  3. **Entropy diagnostic:** Compute reflection-token entropy on held-out unsafe (AttaQ) and reasoning (AIME24) prompts. Verify RL reduces entropy on safety (target <0.15) while maintaining on reasoning (target >2.8).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does RL-based safety alignment scale to frontier models (70B+ parameters), and do optimization dynamics or safety–reasoning trade-offs change at larger scales?
- **Basis in paper:** "our experiments are conducted on medium-scale LRMs (up to 8B parameters); extending the analysis to larger frontier models (e.g., 70B or above) may reveal different optimization dynamics and scaling behaviors."
- **Why unresolved:** The study only validates RL safety training on 7B–8B models; scaling properties for much larger LRMs remain unknown.
- **What evidence would resolve it:** Replicate the RL alignment protocol on 70B+ LRMs (e.g., DeepSeek-R1-70B, Llama-4 variants) and compare safety/reasoning trajectories against smaller-scale findings.

### Open Question 2
- **Question:** How robust is RL-based safety alignment against targeted adversarial attacks and red-team prompts that probe deeper failure modes beyond harmful request compliance?
- **Basis in paper:** "our evaluation primarily focuses on harmful request compliance and does not include targeted adversarial attacks or red-team prompts that probe deeper failure modes."
- **Why unresolved:** The paper evaluates general safety benchmarks (AttaQ, AIR-Bench) but not structured adversarial or jailbreak attacks tailored to exploit reasoning pathways.
- **What evidence would resolve it:** Test RL-aligned LRMs against reasoning-level adversarial benchmarks (e.g., BadChain, DarkMind, Shadow-CoT attacks) and measure attack success rates relative to SFT baselines.

### Open Question 3
- **Question:** To what extent do reward model biases and limited coverage shape RL safety outcomes, and can more fine-grained reward signals improve reasoning-process regulation?
- **Basis in paper:** "our approach relies on existing reward models, whose biases and coverage may affect training outcomes."
- **Why unresolved:** The paper uses SKYWORK-REWARD-V2 as a fixed reward model without analyzing how its particular failure modes or domain gaps influence the learned safety behaviors.
- **What evidence would resolve it:** Conduct ablations with multiple reward models of varying coverage/quality; analyze correlation between reward model blind spots and residual unsafe behaviors in RL-aligned models.

## Limitations

- Findings are based on specific model families (DeepSeek-R1-distilled, Qwen3, Granite-4.0-Tiny-Preview) and may not generalize to other architectures
- Entropy analysis methodology for identifying reflection tokens is not fully specified, limiting reproducibility
- Evaluation focuses on short-term safety and reasoning performance without addressing potential long-term safety implications of RL optimization

## Confidence

**High Confidence:** The empirical finding that RL outperforms SFT on safety benchmarks while maintaining or improving reasoning performance. This is supported by direct experimental comparisons across multiple model families and benchmarks with clear, measurable outcomes.

**Medium Confidence:** The mechanism explaining why RL achieves better safety-reasoning trade-offs through direct policy optimization versus SFT's trajectory imitation. While the theoretical framework is sound, specific implementation details could affect the magnitude of observed benefits.

**Low Confidence:** The entropy analysis interpretation and its implications for understanding reflection dynamics. The methodology for extracting and analyzing reflection tokens is insufficiently detailed, making independent verification challenging.

## Next Checks

1. **Cross-Architecture Reward Model Validation:** Test whether the Skywork-Reward-V2 model provides consistent safety judgments across different model architectures by evaluating the same reasoning trajectories from different base models and comparing reward scores.

2. **Long-Tail Safety Scenario Testing:** Design and evaluate novel safety prompts that were not present in the training datasets to test whether RL's safety improvements generalize beyond the specific distribution used for training.

3. **Ablation Study on REINFORCE++ Components:** Systematically remove or modify individual components of the REINFORCE++ algorithm (e.g., token-level KL penalty, PPO-style clipping, normalized advantage estimation) to isolate which mechanisms contribute most to the observed safety-reasoning trade-offs.