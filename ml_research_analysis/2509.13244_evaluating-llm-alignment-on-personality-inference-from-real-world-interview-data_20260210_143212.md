---
ver: rpa2
title: Evaluating LLM Alignment on Personality Inference from Real-World Interview
  Data
arxiv_id: '2509.13244'
source_url: https://arxiv.org/abs/2509.13244
tags:
- personality
- traits
- llms
- five
- trait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating LLM alignment
  on personality trait prediction using real-world interview data with validated Big
  Five trait scores. The authors compare zero-shot and chain-of-thought prompting,
  LoRA-based fine-tuning on RoBERTa and LLaMA, and static embedding regression.
---

# Evaluating LLM Alignment on Personality Inference from Real-World Interview Data

## Quick Facts
- **arXiv ID:** 2509.13244
- **Source URL:** https://arxiv.org/abs/2509.13244
- **Reference count:** 12
- **Primary result:** Current LLMs fail to achieve strong alignment for personality inference from interview transcripts, with correlations below 0.26 and MAE above 1.0.

## Executive Summary
This paper introduces a benchmark for evaluating large language model alignment on inferring Big Five personality traits from real-world interview transcripts. The study compares zero-shot and chain-of-thought prompting, LoRA-based fine-tuning on RoBERTa and LLaMA models, and static embedding regression approaches. Despite using validated BFI-10 scores from 518 clinical interviews, all methods fail to achieve strong correlation with ground truth, highlighting significant challenges in LLM alignment for personality assessment. The work establishes baseline performance metrics and identifies promising directions for improvement.

## Method Summary
The study evaluates personality trait prediction from semi-structured interview transcripts using multiple approaches. Five methods are compared: GPT-4.1 Mini with zero-shot and chain-of-thought prompting, LoRA fine-tuning on RoBERTa-base and Llama-3.1-8B-Instruct, and static embedding regression using sentence transformers. The dataset consists of 518 real-world interview transcripts paired with validated BFI-10 scores, split 80/20 for training and testing. Models are evaluated using Pearson correlation and mean absolute error, with target thresholds of r > 0.5 (strong) or > 0.3 (moderate) and MAE < 0.8.

## Key Results
- All methods achieve Pearson correlations below 0.26 and MAE above 1.0, failing to meet strong or moderate performance thresholds
- Chain-of-thought prompting provides minimal improvement over zero-shot approaches
- Encoder-based models (RoBERTa) outperform decoder-based models (LLaMA) in trait prediction accuracy
- Openness is the easiest trait to predict, while Extraversion remains particularly challenging across all methods

## Why This Works (Mechanism)
The study establishes a systematic evaluation framework for assessing LLM alignment in personality inference tasks. By using real clinical interview data with validated personality scores, it provides a realistic benchmark that captures the complexity of natural language patterns in personality expression. The multi-method comparison isolates the impact of different architectural choices (encoder vs decoder), prompting strategies, and fine-tuning approaches on model performance.

## Foundational Learning
- **Big Five Personality Traits:** Five-factor model (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) used as evaluation target
  - *Why needed:* Provides standardized framework for personality assessment and evaluation metrics
  - *Quick check:* Verify understanding of trait definitions and scoring scales (1-5 Likert)

- **Chain-of-Thought Prompting:** Intermediate reasoning steps injected between prompt and response
  - *Why needed:* Tests whether explicit reasoning improves complex inference tasks
  - *Quick check:* Compare zero-shot vs CoT outputs for consistency and depth

- **LoRA Fine-tuning:** Parameter-efficient adaptation using low-rank matrix decomposition
  - *Why needed:* Enables efficient model customization without full fine-tuning
  - *Quick check:* Verify rank selection impact on downstream performance

## Architecture Onboarding

**Component Map:** Interview Transcripts -> Embedding/Encoding -> Personality Prediction -> Correlation/MAE Evaluation

**Critical Path:** Data Preprocessing -> Model Inference -> Metric Calculation -> Performance Analysis

**Design Tradeoffs:** Encoder models (RoBERTa) provide better contextual understanding for personality inference compared to decoder models (LLaMA), but require careful chunking strategies for long interviews. Static embeddings offer faster inference but lose sequential information critical for personality expression.

**Failure Signatures:** Near-zero Pearson correlations indicate model collapse to mean predictions; high MAE suggests systematic bias in trait estimation. Minimal CoT improvements reveal limitations in prompting for complex personality inference.

**First Experiments:**
1. Implement Ridge regression baseline with text-embedding-3-small to establish performance floor
2. Test RoBERTa chunking with 512-token windows and 128-token stride to optimize context retention
3. Compare GPT-4o-mini against GPT-4.1 Mini using identical prompts to validate model substitution impact

## Open Questions the Paper Calls Out
- Can trait-specific prompting strategies with tailored instructions per dimension improve LLM alignment for personality inference?
- Would multimodal signal integration (speech prosody, facial expressions, interaction dynamics) substantially improve inference for socially-expressed traits like Extraversion and Agreeableness?
- Do domain-adapted mental healthâ€“oriented LLMs provide better alignment for traits linked to emotional vulnerability (Neuroticism)?

## Limitations
- Dataset access restricted due to IRB protections on clinical interview transcripts
- RoBERTa chunking strategy lacks specific implementation details for optimal context encoding
- GPT-4.1 Mini model version may be unavailable, requiring substitution with similar models

## Confidence
- **High Confidence:** Encoder vs decoder performance trends, trait difficulty hierarchy
- **Medium Confidence:** Metric threshold interpretations and their absolute values
- **Low Confidence:** Chain-of-thought effectiveness claims and LoRA rank selection rationale

## Next Checks
1. Validate data access by confirming transcript availability or obtaining IRB permissions for PANDORA dataset substitution
2. Test multiple RoBERTa chunking configurations to determine optimal context window and stride settings
3. Substitute GPT-4o-mini for GPT-4.1 Mini while preserving prompt templates and comparing performance outcomes