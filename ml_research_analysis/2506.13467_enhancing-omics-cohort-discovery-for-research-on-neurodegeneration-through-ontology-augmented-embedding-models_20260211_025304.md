---
ver: rpa2
title: Enhancing Omics Cohort Discovery for Research on Neurodegeneration through
  Ontology-Augmented Embedding Models
arxiv_id: '2506.13467'
source_url: https://arxiv.org/abs/2506.13467
tags:
- metadata
- cohorts
- terms
- retrieval
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We developed NeuroEmbed, a methodology for semantically indexing
  and retrieving neurodegenerative disease cohorts from omics repositories. The approach
  uses ontology-based synonym expansion and fine-tuning of biomedical embeddings to
  enhance search precision.
---

# Enhancing Omics Cohort Discovery for Research on Neurodegeneration through Ontology-Augmented Embedding Models

## Quick Facts
- arXiv ID: 2506.13467
- Source URL: https://arxiv.org/abs/2506.13467
- Reference count: 0
- 2,801 GEO cohorts processed; retrieval precision increased from 0.277 to 0.866 after fine-tuning

## Executive Summary
NeuroEmbed is a methodology for semantically indexing and retrieving neurodegenerative disease cohorts from omics repositories. The approach uses ontology-based synonym expansion and fine-tuning of biomedical embeddings to enhance search precision. Applied to 2,801 GEO cohorts, the method normalized 1,700 heterogeneous tissue labels into 326 ontology-aligned concepts and expanded metadata terms by 2.7-20 fold. After fine-tuning PubMedBERT on a natural language query dataset, retrieval precision increased from 0.277 to 0.866 and mean percentile rank from 0.355 to 0.896. The method enables accurate retrieval of relevant ND cohorts using free-text queries and supports automated bioinformatics pipelines. The resulting catalog is publicly available.

## Method Summary
The methodology processes GEO cohorts through ontology-based normalization and synonym expansion, then fine-tunes a biomedical embedder for improved retrieval. The pipeline begins with data acquisition via MeSH queries, followed by ontology matching to normalize heterogeneous metadata into canonical concepts. A QA dataset is automatically generated from stratified metadata combinations using six natural language templates. PubMedBERT is fine-tuned using contrastive learning with in-batch negatives (MNRL loss) for 2 epochs. The resulting embedder enables cosine similarity-based retrieval of cohorts matching natural language queries.

## Key Results
- Normalized 1,700 heterogeneous tissue labels into 326 unique ontology-aligned concepts
- Expanded metadata terms by 2.7-20 fold through synonym augmentation
- Retrieval precision increased from 0.277 to 0.866 after fine-tuning PubMedBERT
- Mean percentile rank improved from 0.355 to 0.896

## Why This Works (Mechanism)

### Mechanism 1
Ontology-based synonym expansion enables the model to recognize semantically equivalent terms that differ lexically. Raw metadata values are matched against biomedical ontologies using exact matching first, then fuzzy matching (Levenshtein distance ≥80%). Each matched term is replaced with its canonical ontology label and expanded with all synonyms. This creates a shared vocabulary where "brain cortex" and "cerebral cortex" map to the same concept.

### Mechanism 2
Contrastive learning with in-batch negatives trains the embedder to distinguish relevant query-cohort pairs from distractors without requiring explicit negative labels. MultipleNegativesRankingLoss implements InfoNCE: for each (query, cohort) positive pair in a mini-batch, the model maximizes their cosine similarity while treating all other cohorts in the batch as implicit negatives. This creates dense representations where clinically similar terms cluster together.

### Mechanism 3
Template-based NLQ generation from stratified metadata combinations creates a labeled training set that covers the query space without manual annotation. The 774 augmented metadata values are split 80/20 into train/test synonym sets. NLQs are generated by randomly combining 1-4 metadata dimensions, filtering to queries with at least one matching cohort, and applying one of six natural language templates. Train and test sets use disjoint synonym vocabularies to evaluate generalization.

## Foundational Learning

- **Concept: Contrastive Learning / InfoNCE Loss**
  - **Why needed here:** The fine-tuning step uses MNRL/InfoNCE to train the embedder. Understanding how positive pairs are pulled together and negatives pushed apart is essential for debugging retrieval failures.
  - **Quick check question:** Given a batch of 32 (query, cohort) pairs, how many implicit negatives does MNRL create for each positive pair? (Answer: 31 per pair)

- **Concept: Ontology-Based Normalization vs. Augmentation**
  - **Why needed here:** The paper distinguishes normalization (mapping variants to canonical IDs) from augmentation (expanding with synonyms). This affects both the vocabulary size and the semantic coverage of the embedding space.
  - **Quick check question:** If "cerebral cortex" maps to UBERON:0000956 with 4 synonyms, what is the effect on the QA dataset size if this term appears in 200 cohorts?

- **Concept: Stratified Train/Test Split on Vocabulary**
  - **Why needed here:** The split is performed on the synonym vocabulary, not on cohorts or queries directly. This ensures test queries use unseen synonyms, evaluating generalization to new terminology rather than memorization.
  - **Quick check question:** Why would a random split on queries (rather than vocabulary) potentially overestimate test performance?

## Architecture Onboarding

- **Component map:** Data Acquisition -> Ontology Matcher -> QA Generator -> Fine-Tuner -> Retriever
- **Critical path:** Ontology matching quality → QA dataset coverage → Embedder fine-tuning → Retrieval precision. Errors in synonym expansion propagate to the QA dataset and cannot be recovered by fine-tuning.
- **Design tradeoffs:**
  - Fuzzy threshold (80%): Lower values increase recall but risk spurious matches; higher values reduce augmentation fold-change
  - Subsampling ratio (4:1 train:test): Balances overfitting risk against training signal; may discard useful training examples
  - Template diversity (6 templates): Limited template variety may not cover real query phrasing; adding templates increases engineering effort
- **Failure signatures:**
  - Precision near 0.0 but high MPR: Target cohort ranked high but not top-1; likely due to partial matches scoring higher
  - Training loss plateaus early: Indicates QA dataset lacks diversity; needs harder negatives or more lexical variants
  - Test precision degrades for specific dimensions: Suggests ontology coverage gaps in that dimension
- **First 3 experiments:**
  1. Ablate synonym expansion: Train on original metadata (no ontology augmentation) and compare retrieval precision
  2. Hard negative mining: Identify cohorts that score high on cosine similarity but are semantically irrelevant; add as explicit negatives
  3. Cross-domain transfer: Apply the same pipeline to a non-ND domain (e.g., oncology) to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating raw omics profiles (e.g., gene expression vectors) with text-based metadata embeddings improve retrieval performance for complex, multi-modal queries? The current NeuroEmbed implementation operates solely on metadata descriptions; the technical approach for aligning heterogeneous text embeddings with high-dimensional molecular data is not yet developed.

### Open Question 2
Can ontological disambiguation strategies effectively resolve retrieval errors in two-term queries containing semantically proximate but distinct entities? Error analysis revealed that residual failures concentrate in two-term queries with ambiguous combinations (e.g., specific primate species pairs), suggesting improvements require ontological disambiguation rather than architectural changes.

### Open Question 3
Does scaling the retrieval framework from 2,801 cohorts to over 150,000 individual samples maintain retrieval precision and computational efficiency? While the methodology works for repository-level metadata, applying it to sample-level attributes increases noise and vector space complexity, potentially degrading the Mean Percentile Rank.

## Limitations
- Critical hyperparameters for PubMedBERT fine-tuning are not specified, creating reproducibility constraints
- Exact MeSH query strings and GEO access date remain unspecified, making independent dataset reconstruction difficult
- Methodology validated only on neurodegenerative disease cohorts from GEO; performance on other domains untested
- Template-based NLQs may not capture full diversity of actual user queries

## Confidence

- **High confidence**: Core methodology of ontology-based synonym expansion combined with contrastive fine-tuning is technically sound with clear quantitative improvements
- **Medium confidence**: Specific choice of fuzzy matching threshold and stratified train/test split methodology may require tuning for different domains
- **Medium confidence**: Reported performance metrics based on generated QA dataset; real-world performance with actual user queries may vary

## Next Checks

1. **Ablation study**: Train the embedder on original metadata without ontology expansion to quantify the precise contribution of synonym augmentation to retrieval performance.
2. **Cross-domain transfer**: Apply the complete pipeline to a different omics domain (e.g., oncology cohorts) to test generalizability beyond neurodegenerative diseases.
3. **Real user query evaluation**: Test the system with actual natural language queries from domain experts to assess whether template-based query performance translates to real-world usage.