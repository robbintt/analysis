---
ver: rpa2
title: 'GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network'
arxiv_id: '2504.04862'
source_url: https://arxiv.org/abs/2504.04862
tags:
- trajectory
- prediction
- mechanism
- gamdtp
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GAMDTP is a novel trajectory prediction model that fuses Graph
  Attention Networks (GAT) and Mamba-SSM through a gate mechanism to capture both
  local spatial interactions and global temporal dependencies. The model encodes HD
  map data and historical trajectory information, generating multi-modal future trajectories
  for traffic agents.
---

# GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network

## Quick Facts
- **arXiv ID**: 2504.04862
- **Source URL**: https://arxiv.org/abs/2504.04862
- **Reference count**: 40
- **One-line primary result**: Achieves state-of-the-art performance on Argoverse and INTERACTION datasets with minADE improvements from 0.7612 to 0.7603 and gains of 0.0223 in minJointADE on INTERACTION

## Executive Summary
GAMDTP introduces a novel trajectory prediction model that fuses Graph Attention Networks (GAT) and Mamba-SSM through a learned gate mechanism to capture both local spatial interactions and global temporal dependencies. The model encodes HD map data and historical trajectory information to generate multi-modal future trajectories for traffic agents. A key innovation is the quality scoring mechanism that evaluates prediction reliability during both proposal and refinement stages of a two-stage framework, improving refinement performance. Experiments demonstrate superior accuracy and reliability compared to baseline methods on Argoverse and INTERACTION datasets.

## Method Summary
GAMDTP employs a two-stage proposal-refinement framework where historical trajectories and HD map data are encoded through MLP layers. The core architecture features three sequential Graph Attention Mamba (GAM) modules: Agent GAM for spatial interactions, Historical Prediction GAM for temporal consistency, and Mode GAM for multimodal interactions. A learned sigmoid gate dynamically balances features from GAT (capturing spatial attention) and Mamba2 (capturing long-range temporal dependencies). The model generates K trajectory proposals with associated quality scores that guide refinement by prioritizing trajectories with greater improvement potential. Training uses a combined loss of regression (Huber), classification (CE), and quality score prediction (L1) with winner-takes-all strategy.

## Key Results
- Achieves state-of-the-art minADE of 0.7603 on Argoverse (improvement from 0.7612)
- Improves MR from 0.5514 to 0.5509 on Argoverse
- Gains 0.0223 in minJointADE and 0.0923 in minJointFDE on INTERACTION dataset
- Ablation shows gate mechanism removal increases minJointADE from 0.2529 to 0.2641
- Single Mamba layer performs best; deeper layers degrade performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing GAT and Mamba-SSM via a learned gate mechanism improves trajectory feature extraction by dynamically balancing local spatial interactions with global temporal dependencies.
- Mechanism: Node features are processed in parallel through GAT (capturing inter-agent spatial attention) and Mamba2 (capturing long-range temporal dependencies with linear complexity). A sigmoid gate G = σ(FC(P^M + P^A)) learns to weight their contributions: P^G = P + G·P^A + (1-G)·P^M. This allows the model to adaptively emphasize spatial versus temporal features per node.
- Core assumption: The optimal balance between local graph attention and global sequence modeling varies across nodes and timesteps, and a data-dependent gate can learn this balance better than fixed weighting.
- Evidence anchors:
  - [abstract]: "fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately"
  - [section 3.3]: Equations 5-8 detail the gate computation; ablation (Table 3) shows removal increases minJointADE from 0.2529 to 0.2641
  - [corpus]: HAMF paper also proposes hybrid attention-mamba fusion for motion forecasting, suggesting this architecture pattern has emerging support
- Break condition: If gate values saturate near 0 or 1 consistently across samples, the fusion degenerates to a single branch—indicating redundancy or insufficient gradient signal to the gate.

### Mechanism 2
- Claim: A quality scoring mechanism during proposal and refinement stages improves final predictions by prioritizing high-quality trajectory proposals for refinement.
- Mechanism: During training, ground-truth yields a quality score q = |dp - dr| / (|dmax - dr| + ε), where dp and dr are proposal/refinement errors. A Mamba2 layer + MLP predicts this score from proposal embeddings. The score guides refinement focus toward trajectories with greater improvement potential.
- Core assumption: Proposal trajectories with larger error reduction potential (dp - dr) are more valuable to refine, and this property is learnable from input embeddings.
- Evidence anchors:
  - [abstract]: "design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes"
  - [section 3.4]: Algorithm 1 and Equation 9 define the scoring; ablation shows removal increases minJointADE by 0.0014
  - [corpus]: Weak direct corpus evidence—no neighbor papers explicitly cite quality scoring for refinement; this appears novel to this work
- Break condition: If predicted scores show low correlation with actual improvement (|dp - dr|), the mechanism adds computational overhead without benefit—monitor score-to-improvement correlation on validation.

### Mechanism 3
- Claim: Hierarchical GAM modules (Agent GAM → Historical Prediction GAM → Mode GAM) capture multi-level interactions necessary for multi-agent trajectory prediction.
- Mechanism: Three sequential GAM modules: (1) Agent GAM models agent-agent interactions using graph attention over spatial neighbors; (2) Historical Prediction GAM encodes temporal consistency between successive predictions; (3) Mode GAM captures interactions among K future trajectory modes. Each module uses the GAT-Mamba fusion from Mechanism 1.
- Core assumption: Interactions exist at multiple abstraction levels (spatial neighbors, temporal predictions, multimodal futures) and sequential processing captures these hierarchically.
- Evidence anchors:
  - [section 3.2]: "Agent GAM first input the prediction embeddings... Then Historical Prediction GAM inputs the result... Finally, results... entered into Mode GAM"
  - [section 4.4]: 1 Mamba layer performs best; more layers cause degradation (minJointADE: 0.2529→0.2706 with 5 layers)
  - [corpus]: DyG-Mamba and other Mamba-graph hybrids support the sequential SSM-on-graph approach
- Break condition: If removing any single GAM module causes <5% metric degradation, that module may be redundant—test via ablation.

## Foundational Learning

- **Concept: Graph Attention Networks (GAT)**
  - Why needed here: Core spatial modeling component—attention weights edges between agents/lanes based on learned importance
  - Quick check question: Can you explain how attention coefficients α_ij are computed from node features and edge features?

- **Concept: State Space Models (SSM/Mamba)**
  - Why needed here: Replaces LSTM/Transformer for temporal modeling with O(n) complexity; Mamba2 adds selectivity via input-dependent parameters
  - Quick check question: What is the computational complexity difference between self-attention and SSM for sequence length T?

- **Concept: Winner-Takes-All Training**
  - Why needed here: Multi-modal prediction optimization strategy—only the trajectory mode closest to ground truth receives gradient
  - Quick check question: Why does WTA prevent mode collapse compared to averaging loss over all K modes?

## Architecture Onboarding

- **Component map**:
  Input (HD map + agent history) → MLP Encoder (embeddings E_a, E_m, E_e) → Agent GAM (spatial interactions) → Historical Prediction GAM (temporal consistency) → Mode GAM (multimodal interactions) → Decoder (K trajectories + probabilities) → Score Decoder (quality scores for refinement) → Refinement Stage (offset prediction)

- **Critical path**:
  1. Verify embedding shapes: E_a ∈ R^(T×N×D), E_m ∈ R^(M×D), E_e ∈ R^(Y×D)
  2. Gate output P^G must stay in [P_min, P_max] range—monitor for gradient flow
  3. Score prediction loss L_s uses L1; if unstable, check ε in denominator

- **Design tradeoffs**:
  - 1 Mamba layer vs. multiple: Paper shows >1 layer degrades performance (Table 3)—likely over-smoothing
  - K=6 modes: Standard for Argoverse; increasing K improves coverage but increases computation
  - Gate mechanism adds ~15% parameters but enables adaptive fusion

- **Failure signatures**:
  - Gate values → all 0.5: Insufficient gradient signal; consider gate initialization or loss weighting
  - Score prediction → constant: Check if ground-truth scores have sufficient variance
  - Mode collapse (all K trajectories similar): Verify WTA loss is correctly selecting argmin mode

- **First 3 experiments**:
  1. Reproduce ablation in Table 3: Train without gate mechanism, confirm minJointADE degrades to ~0.2641
  2. Visualize gate values G across validation samples: Check if distribution is bimodal (learning) vs. uniform (not learning)
  3. Correlation analysis: Plot predicted quality scores vs. actual |dp - dr| on held-out set; target r > 0.3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed fusion of GAT and Mamba-SSM actually provide computational speedups (latency/FPS) suitable for real-time systems compared to standard Transformer baselines?
- Basis in paper: [inferred] The introduction claims the model is "suited for real-time trajectory prediction" due to Mamba's linear complexity, yet the experimental section (Tables 1-3) only reports accuracy metrics (minADE, MR) without providing inference time or FLOPs comparisons.
- Why unresolved: The theoretical efficiency of Mamba is established, but the practical overhead of the gate mechanism, graph construction, and two-stage refinement in GAMDTP is not quantified.
- What evidence would resolve it: Benchmarks of inference latency (ms) and memory consumption on identical hardware compared against baseline models like HPNet or QCNet.

### Open Question 2
- Question: Why does increasing the number of Mamba layers lead to performance degradation and convergence difficulties in this specific architecture?
- Basis in paper: [explicit] Section 4.4 (Ablation Study) notes that using 3 or 5 Mamba layers causes performance to drop (e.g., minJointADE increases to 0.2706), attributing this to "computational redundancy."
- Why unresolved: This contradicts the general deep learning principle that deeper networks often capture more complex features; the specific mechanism causing this redundancy is not analyzed.
- What evidence would resolve it: A study on gradient flow or feature rank in deeper layers, or an analysis of the data scale relative to model capacity.

### Open Question 3
- Question: How well-calibrated is the Quality Scoring Mechanism in predicting the actual error of proposals during inference when ground truth is unavailable?
- Basis in paper: [inferred] The paper trains a score predictor ($L_s$) to match a ground-truth quality metric (Eq. 9), but evaluates only the refined trajectory error (Table 3), not the accuracy of the score prediction itself.
- Why unresolved: It is unclear if the predicted score reliably identifies poor proposals or if the improvement comes simply from the additional regularization of the auxiliary task.
- What evidence would resolve it: A correlation analysis (e.g., Spearman's rank) between the predicted quality scores and the actual endpoint displacement errors on the validation set.

## Limitations

- Key architectural hyperparameters (hidden dimensions, attention heads, GAT layer counts, Mamba2 state dimension, conv kernel sizes) are unspecified
- Training hyperparameters (learning rate, batch size, optimizer, weight decay, λ value) are not provided
- Neighbor selection radius and specific attention implementation details are absent
- No runtime efficiency comparisons or computational complexity analysis provided

## Confidence

- **High confidence**: Core mechanism of GAT-Mamba fusion through learned gate (ablation shows degradation from 0.2529 to 0.2641 when removed)
- **Medium confidence**: Quality scoring mechanism (ablation shows 0.0014 minJointADE degradation when removed)
- **Low confidence**: Specific architectural hyperparameters and training configurations needed for faithful reproduction

## Next Checks

1. Reproduce the gate mechanism ablation: Train GAMDTP without the gate fusion and verify minJointADE degrades to approximately 0.2641 as reported in Table 3.

2. Analyze gate value distributions: Plot the learned gate values G across validation samples to confirm they are neither saturated (near 0 or 1) nor uniform (near 0.5), indicating the gate is learning meaningful spatial-temporal tradeoffs.

3. Validate quality score prediction: Compute the Pearson correlation between predicted quality scores and actual improvement (|dp - dr|) on a held-out validation set, targeting a correlation coefficient above 0.3 to confirm the scoring mechanism is learning useful signals.