---
ver: rpa2
title: Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations,
  Cognitive Biases, or Phobias in Bayesian Reasoning?
arxiv_id: '2503.15268'
source_url: https://arxiv.org/abs/2503.15268
tags:
- reasoning
- blocks
- they
- natural
- card
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how Large Language Models (LLMs) with Chain-of-Thought
  (CoT) reasoning handle Bayesian reasoning problems, specifically examining their
  use of ecologically valid strategies like natural frequencies, whole objects, and
  embodied heuristics. Using four stages of prompts, the research tested three LLMs
  (GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1) on a Bayesian lie detection
  problem designed for elementary students.
---

# Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?

## Quick Facts
- arXiv ID: 2503.15268
- Source URL: https://arxiv.org/abs/2503.15268
- Reference count: 40
- Key outcome: LLMs with Chain-of-Thought reasoning persistently avoid natural frequencies and embodied heuristics when solving Bayesian problems, even when explicitly prompted to use these strategies.

## Executive Summary
This study investigates how Large Language Models with Chain-of-Thought (CoT) reasoning handle Bayesian problems, specifically testing whether they use intuitive strategies like natural frequencies and embodied heuristics that are pedagogically valuable for students. Using a Bayesian lie detection problem designed for elementary students and a four-stage progressive prompting approach, the research found that three tested LLMs (GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1) consistently defaulted to symbolic probability notation rather than intuitive reasoning strategies. Even with explicit prompts to use natural frequencies and colored blocks, the models showed inconsistent compliance and frequently reverted to formal probabilistic reasoning, suggesting a persistent cognitive bias or "phobia" toward ecologically valid approaches.

## Method Summary
The study employed a four-stage progressive prompting approach with three commercial LLMs (GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1) on a Bayesian lie detection problem involving spatial relationships between characters (cat, books, etc.) with probabilistic clues. The baseline prompt tested autonomous strategy selection, followed by prompts explicitly requesting natural frequencies, whole objects/blocks, and colored blocks/embodied heuristics. The analysis focused on qualitative assessment of reasoning traces (CoT output) to determine whether models used requested ecological strategies versus symbolic probability/Bayes' theorem, examining both correctness of conclusions and fidelity to the specified reasoning format.

## Key Results
- All three LLMs consistently defaulted to symbolic probability notation rather than natural frequencies or embodied heuristics, even when explicitly prompted to use intuitive strategies.
- When prompted to use natural frequencies or blocks, models showed inconsistent compliance and frequently reverted to probabilistic notation mid-reasoning, displaying an Einstellung-like effect.
- DeepSeek R1 produced extensive CoT traces (up to 8457 words) but remained superficial in strategy adoption, demonstrating the complexity of achieving genuine strategy compliance.
- Models that reached correct Bayesian conclusions often did so through formal probability calculations rather than the requested intuitive reasoning processes.

## Why This Works (Mechanism)

### Mechanism 1: Training Data Composition Bias Toward Symbolic Reasoning
- Claim: LLMs default to symbolic probability notation because their training corpora predominantly consists of expert-edited academic papers and formal mathematical content, not novice reasoning traces.
- Core assumption: Training data composition directly shapes which reasoning formats the model finds "natural" during generation.
- Evidence anchors: [section 4]: "LLMs exhibit these cognitive biases towards symbolic reasoning... probably because they are trained predominantly on expert-edited papers and professionally written computer codes, neglecting the messy, intuitive processes underlying human thought."

### Mechanism 2: Einstellung Effect—Ingrained Probability Format Reversion
- Claim: Even when explicitly prompted to use natural frequencies, LLMs exhibit an Einstellung-like effect where they revert to probability notation mid-reasoning.
- Core assumption: The phenomenon resembles human cognitive inertia where prior learning impedes alternative problem-solving approaches.
- Evidence anchors: [section 4]: "This behavior mirrors the Einstellung effect in humans, where prior learning of formally trained strategies hinders problem-solving"; [section 3.2.3]: DeepSeek "mentioned using natural frequencies but continued with percentages... said 'This is getting too tangled. Let's simplify with natural frequencies.' However, then it returned to probabilities"

### Mechanism 3: Absence of Sensorimotor Grounding for Embodied Heuristics
- Claim: LLMs cannot genuinely employ embodied heuristics because they lack the perceptual-motor systems these strategies are designed to exploit.
- Core assumption: The pedagogical value of embodied heuristics derives from actual sensorimotor engagement, not linguistic description of engagement.
- Evidence anchors: [section 1]: "They are by-products of millions of years of biological and cultural evolution... Based on intuition... the critical task for the brain is to control the body"; [section 4]: "LLMs either ignored these strategies or applied them inconsistently... their CoT responses frequently reverted to probabilistic notation, bypassing more intuitive, pedagogically valuable approaches"

## Foundational Learning

- **Concept: Natural Frequencies vs. Probability Format**
  - Why needed here: The paper's core finding is that LLMs avoid natural frequencies despite their proven pedagogical value over abstract probability notation.
  - Quick check question: Given a medical test with 90% sensitivity and 1% base rate, would you compute with decimals (0.9 × 0.01) or imagine 1000 patients and count?

- **Concept: Bayesian Posterior Probability**
  - Why needed here: The lie-detection task requires computing P(card near cat | cat says "not next to me")—the posterior probability that balances prior evidence with new evidence.
  - Quick check question: If you initially believe X is 90% likely, then receive evidence that's 80% reliable suggesting otherwise, how do you combine these?

- **Concept: Chain-of-Thought (CoT) as Simulated Inner Speech**
  - Why needed here: CoT reveals the model's "internal deliberation" process. The paper analyzes CoT traces to diagnose where reasoning diverges from human-like strategies.
  - Quick check question: When you solve a complex problem, what represents your "inner voice"—and how might an LLM's generated text differ from genuine internal reasoning?

## Architecture Onboarding

- **Component map:** Prompt layer -> CoT generation -> Strategy compliance detector -> Evaluation layer
- **Critical path:** 1) Design Bayesian problem with ecological validity, 2) Run model with baseline prompt—observe autonomous strategy selection, 3) Apply strategy-inducing prompts progressively—measure compliance depth and persistence, 4) Analyze CoT for format reversion events
- **Design tradeoffs:** 
  - Prompt specificity vs. ecological validity: More detailed prompts may elicit surface-level compliance without genuine strategy adoption
  - Model scale vs. interpretability: DeepSeek R1 produced 8457-word CoT enabling rich analysis but creating pedagogical usability challenges
  - Multiple runs vs. reproducibility: GEMINI produced different conclusions across runs, creating classroom opportunities but reliability concerns
- **Failure signatures:**
  - Format reversion: Model begins with "100 blocks" then switches to "P(A|B) = 0.67"
  - Inconsistent coloring: Model describes red/white blocks but forgets color assignments mid-reasoning
  - Correct answer, wrong argument: Model reaches Bayesian posterior correctly via Bayes' theorem but cannot explain via natural frequencies
  - Simulation without comprehension: Model describes counting blocks but makes logical errors in block-to-probability mapping
- **First 3 experiments:**
  1. Baseline strategy audit: Run 5 diverse Bayesian problems across 3 models with zero-shot CoT—code each for autonomous use of natural frequencies, whole objects, or embodied heuristics. Expect <10% ecological strategy appearance.
  2. Prompt persistence test: Apply strategy-inducing prompts at 3 points (start, mid-reasoning, pre-conclusion). Measure whether mid-reasoning prompts reduce format reversion rates compared to start-only prompts.
  3. Cross-domain generalization: After prompting natural frequencies on lie-detection, test whether same model autonomously applies natural frequencies to medical diagnosis problem without re-prompting. Assess strategy transfer.

## Open Questions the Paper Calls Out

- Can training Large Language Models on "synchronized thinking datasets" (where humans verbalize raw, unfiltered thoughts) eliminate the observed bias against natural frequencies and embodied heuristics?
- Is the identified "phobia" of natural frequencies consistent across diverse Bayesian contexts, or is it specific to spatial/embodied tasks like the lie-detection problem used here?
- Is the models' resistance to ecologically valid strategies better characterized as a fixable "Einstellung effect" or a fundamental architectural inability to simulate embodied cognition?

## Limitations

- The study relies on a single Bayesian problem design, limiting generalizability of the cognitive bias findings to other Bayesian reasoning contexts.
- The analysis only examines text-based CoT traces without external validation of whether surface-level compliance with embodied heuristics produces genuine conceptual understanding.
- The extensive CoT length required for ecological strategies (up to 8457 tokens) suggests the models may struggle with procedural reasoning rather than strategy preference.

## Confidence

- **High:** Models consistently default to symbolic probability notation even when prompted for natural frequencies; this finding is robust across all three tested models.
- **Medium:** The Einstellung effect interpretation is plausible but not definitively proven—the format reversion could reflect training data bias rather than cognitive inertia.
- **Low:** Claims about the pedagogical implications for K-12 education require field testing; the study only examines model behavior, not actual student learning outcomes.

## Next Checks

1. Multi-problem generalization test: Apply the same four-stage prompting protocol to five diverse Bayesian problems (medical diagnosis, legal reasoning, environmental risk assessment) to verify the cognitive bias pattern persists across domains.
2. Cognitive load analysis: Compare CoT token counts and reasoning errors between models using natural frequencies versus symbolic probabilities to quantify whether format choice affects reasoning efficiency.
3. Student simulation validation: Conduct think-aloud protocols with K-12 students on the same problems, then compare human reasoning traces with LLM CoT outputs to assess pedagogical alignment claims.