---
ver: rpa2
title: Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation
arxiv_id: '2512.14954'
source_url: https://arxiv.org/abs/2512.14954
tags:
- encoding
- vocabulary
- distillation
- token
- cover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of cross-tokenizer likelihood scoring
  in language model distillation, where teacher and student models use different vocabularies.
  The core idea leverages the recursive structure of Byte-Pair Encoding (BPE) to create
  a probabilistic framework for computing sequence likelihoods across vocabularies.
---

# Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation

## Quick Facts
- arXiv ID: 2512.14954
- Source URL: https://arxiv.org/abs/2512.14954
- Authors: Buu Phan; Ashish Khisti; Karen Ullrich
- Reference count: 40
- Key outcome: Improves GSM8K accuracy by >2% in cross-tokenizer distillation through exact likelihood scoring

## Executive Summary
This work addresses the fundamental problem of computing sequence likelihoods when teacher and student language models use different token vocabularies. The core innovation leverages the recursive structure of Byte-Pair Encoding (BPE) to create a probabilistic framework for computing sequence likelihoods across vocabularies. For the subset case where the student vocabulary is contained within the teacher's, the method computes exact likelihoods with constant model evaluations per token. For arbitrary vocabularies, it provides both a lossless recursive algorithm and a fast beam-search approximation. Applied to cross-tokenizer distillation for mathematical reasoning, the approach achieves >2% accuracy improvement over state-of-the-art methods on GSM8K.

## Method Summary
The method exploits BPE's recursive merge structure to compute cross-tokenizer likelihoods. For the subset case (student vocab ⊆ teacher vocab), it uses relative alphabet marginalization to compute exact likelihoods in O(1) time per token by summing probabilities over all valid "cover encodings." For the general case, it implements a recursive probability decomposition algorithm that subtracts alternative merge paths, combined with a beam search approximation for tractability. The framework enables distillation through Partial KL (PKL) divergence minimization, where the teacher's byte-level probabilities are converted to student token probabilities for the loss calculation.

## Key Results
- Improves GSM8K accuracy by more than 2% over current state-of-the-art cross-tokenizer distillation
- Achieves up to 12% memory reduction through vocabulary trimming while improving baseline performance by up to 4%
- Demonstrates exact likelihood computation with O(1) model evaluations for subset vocabularies
- Shows RMSE = 0.015 for beam search approximation on GSM8K, closely matching ground truth probabilities

## Why This Works (Mechanism)

### Mechanism 1: Relative Alphabet Marginalization (Full-to-Subset)
The algorithm treats the student vocabulary as a "relative alphabet" of the teacher, aggregating probability mass of all "relative cover encodings" - valid token sequences in the teacher's vocabulary that decode to the string prefix defined by student tokens. This enables exact likelihood computation with O(1) model evaluations when the student vocabulary is a strict subset of the teacher's.

### Mechanism 2: Recursive Probability Decomposition (Subset-to-Full)
When converting from smaller to larger vocabularies, the method calculates token probabilities by recursively subtracting the probability of alternative merge paths from base components. This leverages the marginalization principle where token probability equals the sum of all underlying byte-strings that manifest as that token.

### Mechanism 3: Beam Search Approximation (General Case)
For arbitrary vocabularies where exact recursion is intractable, beam search approximates the likelihood correction term by exploring high-probability string continuations from byte-level prefixes. It assumes frequent delimiters (whitespace/EOS) bound correction set complexity, with low-probability continuations contributing negligibly.

## Foundational Learning

- **Concept:** Byte-Pair Encoding (BPE) Merges
  - **Why needed:** The framework rests on BPE's recursive structure - you cannot define "relative alphabets" without understanding how tokens compose from simpler tokens/bytes
  - **Quick check:** Given merges $a+b \to ab$ and $ab+c \to abc$, what is the valid decomposition of $abc$?

- **Concept:** Marginalization in Probability
  - **Why needed:** Cross-tokenizer scores are marginal probabilities - the sum of probabilities of all underlying byte-strings that manifest as the desired token sequence
  - **Quick check:** If $P(S) = 0.6$ and $P(S') = 0.4$, and both encode to token $T$, what is $P(T)$?

- **Concept:** Tokenization Bias
  - **Why needed:** This is the core problem being solved - understanding why standard LMs cannot output token $B$ if previous token $A$ implicitly contains $B$'s bytes
  - **Quick check:** In Llama3, why is sequence `[12, 2]` impossible if `122` is a single token?

## Architecture Onboarding

- **Component map:** Tokenizer Analyzer → Cover Encoder (Algorithm 2) → Recursive Scorer (Algorithm 1) → Distillation Engine
- **Critical path:** Identify vocab relationship → If $V_{M'} \subset V_M$: Use Algorithm 2 + Equation (9) for exact scoring → Else: Use Algorithm 1 + Beam Search approximation → Feed into PKL Loss (Equation 37)
- **Design tradeoffs:** Exact vs. Approximate - use O(1) method only with subset vocabularies (e.g., trimming); otherwise accept ~0.5s/token approximation overhead. Memory vs. Performance - trimming trades token capacity for efficient distillation requiring warm-up phase.
- **Failure signatures:** High RMSE indicates beam search too narrow or delimiters unsuitable for language; vanishing probabilities suggest structural impossibility; negative probabilities indicate incomplete approximation set.
- **First 3 experiments:**
  1. Verify `RelativeCoverSearch` by checking if `cover[1->2]([a, ab])` correctly returns `{[a, ab], [a, aba]}` as per Example 1
  2. Replicate Figure 4; compare Algorithm 1 probabilities against ground-truth for GSM8K tokens ensuring RMSE < 0.02
  3. Distill Qwen2.5-1.5B to 32k vocab subset using exact O(1) method and confirm Table 3 metrics (approx 61% GSM8K accuracy)

## Open Questions the Paper Calls Out
1. Can the computational efficiency of the general-case beam-search approximation be improved to reduce the current latency of 0.5 seconds per token?
2. How can this cross-tokenizer likelihood framework be adapted for preference optimization tasks?
3. Does this method yield significant performance improvements when applied to tokenization adaptation for specific target domains?

## Limitations
- Approximation quality heavily depends on delimiter assumptions that may not generalize to code or logographic languages
- Exact recursive algorithm has exponential complexity O(exp(M-M')) making it impractical for large vocabulary differences
- Method assumes standard BPE merge structures - deviations from this break the mathematical framework
- Single validation point (GSM8K) for approximation quality across different language domains

## Confidence
**High Confidence:** Theoretical framework for exact likelihood computation in subset case (Lemma 1, O(1) complexity) and vocabulary trimming results (12% memory reduction, 4% performance improvement)
**Medium Confidence:** Beam search approximation's general effectiveness across diverse vocabulary pairs and languages (RMSE = 0.015 needs broader validation)
**Low Confidence:** Scalability claims for exact recursive algorithm in practice (exponential complexity not empirically validated)

## Next Checks
1. Test beam search approximation on non-English datasets (CodeXGLUE, multilingual benchmarks) to verify delimiter-based pruning across different tokenization patterns
2. Systematically evaluate approximation quality as vocabulary size difference increases (32k→16k, 32k→8k, 32k→4k) to identify RMSE degradation breaking points
3. Measure actual wall-clock time for both exact and approximate methods across different vocabulary pairs and sequence lengths to validate O(1) claims and quantify practical overhead