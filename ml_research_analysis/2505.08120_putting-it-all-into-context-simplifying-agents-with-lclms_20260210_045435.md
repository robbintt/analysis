---
ver: rpa2
title: 'Putting It All into Context: Simplifying Agents with LCLMs'
arxiv_id: '2505.08120'
source_url: https://arxiv.org/abs/2505.08120
tags:
- code
- solve
- context
- files
- lclms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether complex agent scaffolding is necessary
  for software engineering tasks like SWE-bench. The authors propose a simplified
  approach using long-context language models (LCLMs) that directly process the entire
  codebase without specialized tools or pipelines.
---

# Putting It All into Context: Simplifying Agents with LCLMs

## Quick Facts
- arXiv ID: 2505.08120
- Source URL: https://arxiv.org/abs/2505.08120
- Reference count: 40
- Primary result: Gemini-1.5-Pro achieves 38% solve rate on SWE-bench, comparable to complex agent scaffolds at 32%

## Executive Summary
This paper challenges the assumption that complex agent scaffolding is necessary for software engineering tasks by proposing a simplified approach using long-context language models (LCLMs). The authors demonstrate that directly processing the entire codebase with an LCLM can achieve competitive results on SWE-bench without specialized tools or pipelines. Their approach uses two key prompting techniques - code restatement for in-context retrieval and chain-of-thought prompting - to achieve results comparable to carefully engineered agent scaffolds. The paper also introduces a two-stage approach combining LCLM compression with short-context LM repair, achieving 48.6% solve rate.

## Method Summary
The authors propose a simplified approach for software engineering tasks that bypasses complex agent scaffolding by directly processing entire codebases with long-context language models. The method uses two key prompting techniques: code restatement for in-context retrieval, where the model is prompted to rewrite relevant code sections for better retrieval, and chain-of-thought prompting to enable systematic problem-solving. They also introduce a two-stage approach that first uses an LCLM to compress and analyze the codebase, then passes the results to a short-context language model for detailed bug fixing. This monolithic approach achieves competitive performance with less complexity than traditional agent scaffolds.

## Key Results
- Gemini-1.5-Pro achieves 38% solve rate on SWE-bench, comparable to complex agent scaffolds at 32%
- Gemini-2.5-Pro attains 50.8% solve rate using the simplified LCLM approach
- Two-stage approach (LCLM compression + short-context LM repair) achieves 48.6% solve rate
- Simplified LCLM approach matches or exceeds performance of carefully engineered agent scaffolds

## Why This Works (Mechanism)
The success of this approach stems from the increasing context window capabilities of modern language models, which can now process entire codebases without the need for specialized retrieval or filtering mechanisms. By presenting the complete context to the model, it can perform its own retrieval and reasoning, eliminating the overhead and potential errors introduced by complex scaffolding systems. The code restatement technique helps the model better understand and organize the relevant code sections, while chain-of-thought prompting enables systematic problem-solving approaches that mirror how human developers tackle software bugs.

## Foundational Learning

**Long-Context Language Models (LCLMs)**
*Why needed:* Traditional language models have limited context windows, requiring complex retrieval mechanisms for large codebases.
*Quick check:* Verify the model can process the entire repository without truncation or summarization.

**Chain-of-Thought Prompting**
*Why needed:* Enables systematic reasoning and step-by-step problem-solving in complex software engineering tasks.
*Quick check:* Ensure the model generates intermediate reasoning steps before producing final solutions.

**Code Restatement for In-Context Retrieval**
*Why needed:* Helps the model better organize and understand relevant code sections within the full context.
*Quick check:* Compare solution quality with and without code restatement prompting.

**Two-Stage Approach**
*Why needed:* Combines the broad context understanding of LCLMs with the precision of short-context models for detailed work.
*Quick check:* Measure performance improvement when adding the short-context refinement stage.

## Architecture Onboarding

**Component Map**
Long-Context LM (Full Codebase) -> Prompting Techniques (Code Restatement + CoT) -> Solution Generation

**Critical Path**
1. Load entire codebase into LCLM context window
2. Apply code restatement and chain-of-thought prompting
3. Generate solution or pass to short-context LM for refinement

**Design Tradeoffs**
- Monolithic simplicity vs. specialized tool integration
- Full context processing vs. targeted retrieval efficiency
- Prompt engineering vs. pipeline complexity

**Failure Signatures**
- Context window overflow errors
- Incomplete code restatement leading to missing context
- Chain-of-thought breakdowns resulting in incomplete reasoning
- Over-reliance on model capabilities without error handling

**Three First Experiments**
1. Compare solve rates of LCLM vs. traditional agent scaffolds on identical SWE-bench tasks
2. A/B test code restatement prompting effectiveness by measuring solution quality with and without it
3. Evaluate two-stage approach performance across different bug complexity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to more complex software engineering tasks beyond SWE-bench scope
- Neither approach (LCLM nor agent scaffolds) achieves satisfactory solve rates for practical deployment
- Paper lacks extensive exploration of failure modes and bug category analysis

## Confidence

**High confidence:** Long-context models can achieve competitive results on SWE-bench without specialized scaffolding (38% vs 32% solve rates).

**Medium confidence:** Simpler monolithic approaches may replace complex agent scaffolding as LCLM capabilities improve.

**Medium confidence:** Two-stage approach (LCLM compression + short-context LM repair) achieving 48.6% solve rate is effective but needs broader validation.

## Next Checks
1. Test the simplified LCLM approach on more complex software engineering benchmarks requiring multi-file reasoning and architectural understanding.

2. Conduct ablation studies to identify which components of agent scaffolds provide the most value and test hybrid system integration.

3. Perform detailed error analysis to categorize bug types solved by each approach and identify inherent strengths of monolithic vs. scaffolded methods.