---
ver: rpa2
title: Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization
arxiv_id: '2509.17405'
source_url: https://arxiv.org/abs/2509.17405
tags:
- bosw
- optimization
- bayesian
- directions
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Bayesian optimization (BO) to learn projection
  directions for sliced Wasserstein (SW) distance estimation, rather than relying
  on random or quasi-Monte Carlo sampling. The core idea is that in settings where
  SW appears inside an optimization loop, BO can adaptively select informative projection
  directions by treating the 1D Wasserstein cost as a black-box function on the sphere.
---

# Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization

## Quick Facts
- **arXiv ID:** 2509.17405
- **Source URL:** https://arxiv.org/abs/2509.17405
- **Reference count:** 32
- **Key outcome:** Bayesian optimization can adaptively select informative projection directions for sliced Wasserstein distance estimation, outperforming random sampling in optimization-in-the-loop scenarios while maintaining modest runtime overhead.

## Executive Summary
This paper introduces Bayesian optimization (BO) methods for selecting projection directions in sliced Wasserstein (SW) distance computation. Instead of using random or quasi-Monte Carlo sampling, the authors treat the 1D Wasserstein cost as a black-box function on the sphere and use BO to find high-signal directions. Four variants are proposed: BOSW (one-shot), RBOSW (periodic refresh), ABOSW (QSW-seeded refinement), and ARBOSW (restarted hybrid). These methods integrate into existing SW pipelines without altering downstream losses or gradients. Experiments on point-cloud interpolation, image style transfer, and deep point-cloud autoencoders show that ABOSW and ARBOSW achieve convergence competitive with the best quasi-Monte Carlo baselines while offering modest runtime overhead.

## Method Summary
The paper proposes using Bayesian optimization to learn projection directions for sliced Wasserstein distance estimation. The core idea is that in settings where SW appears inside an optimization loop, BO can adaptively select informative projection directions by treating the 1D Wasserstein cost as a black-box function on the sphere. The method treats f(θ; μ, ν) = W_p^p(θ♯μ, θ♯ν) as a black-box on S^(d-1) and models it using a Gaussian process with angular RBF kernel. Four variants are introduced: BOSW (one-shot), RBOSW (periodic refresh), ABOSW (QSW-seeded refinement), and ARBOSW (restarted hybrid). These methods integrate into existing SW pipelines without altering downstream losses or gradients.

## Key Results
- ABOSW and ARBOSW achieve convergence comparable to the best QSW variants with modest runtime overhead
- BOSW outperforms QSW on optimization-in-the-loop tasks but underperforms on pure approximation tasks
- ABOSW achieves lowest reconstruction loss (SW²=1.81±0.02, W²=9.01±0.03) in deep point-cloud autoencoders
- RBOSW shows high early performance but degrades over time due to variance from frequent refreshes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BO identifies high-signal projection directions by modeling the 1D Wasserstein cost as a structured landscape on the sphere, concentrating samples where they provide most information.
- **Mechanism:** The method treats f(θ; μ, ν) = W_p^p(θ♯μ, θ♯ν) as a black-box on S^(d-1). A Gaussian process with angular RBF kernel (geodesic distance-based) models this landscape. UCB acquisition (β=0.7) balances exploration/exploitation: α_t(θ) = μ_{t-1}(θ) + βσ_{t-1}(θ). From a candidate pool (n_c=4096), the top-b proposals are selected, with near-duplicates suppressed (cosine similarity >0.98 threshold).
- **Core assumption:** The Wasserstein cost f(θ) is non-uniform on the sphere—certain directions reveal substantially more about distributional differences than others, creating exploitable structure.
- **Evidence anchors:**
  - [abstract] "BO can adaptively select informative projection directions by treating the 1D Wasserstein cost as a black-box function on the sphere"
  - [section 3.1] Details GP with angular RBF kernel k(θ,θ') = exp(-½(d_S(θ,θ')/ℓ)²), UCB with β=0.7, candidate pool n_c=4096, batch b=5
  - [section 4.1, Figure 1] Synthetic experiment: BO reaches ≈1.90 best fitness on Peaks landscape at L=20 vs ≈1.2 for strongest QSW; finds near-optimal projections by L=10 on Quadratic (2.69 vs ≤1.0 for QSW)
  - [corpus] "Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance" (arxiv 2509.10166) confirms sphere sampling for SW as active research direction
- **Break condition:** When distributions have uniform structure across all directions (no discriminative projections), BO finds nothing to exploit—Section 4.2 shows BOSW underperforms QSW on approximation error tasks with uniform point clouds.

### Mechanism 2
- **Claim:** Hybrid QSW-seeding with BO refinement combines uniform sphere coverage with task-specific adaptation, achieving better convergence than either approach alone.
- **Mechanism:** ABOSW initializes from a QSW set (spiral/Coulomb designs), providing low-discrepancy baseline coverage. Then r=2 BO rounds with batch size b=5 replace at most 10 directions (≤10% of L=100 set), focusing BO on local refinement rather than global search. The "worst" directions (lowest f-values) are replaced by BO proposals.
- **Core assumption:** QSW provides good global coverage; the signal is concentrated in specific regions where BO refinement adds value without destabilizing the quadrature.
- **Evidence anchors:**
  - [abstract] "ABOSW and ARBOSW can achieve convergence comparable to the best QSW variants with modest runtime overhead"
  - [section 3.2] "ABOSW begins with a strong QSW set... and performs a few BO rounds to lightly adjust it... at most br≤10 directions change when L=100"
  - [section 4.5, Table 2] ABOSW achieves lowest reconstruction loss (SW²=1.81±0.02, W²=9.01±0.03 at epoch 400), outperforming CQSW (SW²=1.84, W²=9.06)
  - [corpus] No direct corpus evidence for QSW-BO hybrid; appears to be a novel contribution
- **Break condition:** When unbiased estimation is required—Appendix D proves BOSW with constant β does not converge to uniform coverage (biased estimator). For unbiased needs, must use randomized QSW or annealed β schedule.

### Mechanism 3
- **Claim:** Periodic direction refresh tracks evolving distribution geometry in gradient-flow settings, but harms convergence when distributions are stable.
- **Mechanism:** RBOSW re-runs full BO from scratch every R steps on current distributions. ARBOSW periodically re-seeds from QSW then runs short BO refinement. Key difference: RBOSW discards previous surrogate; ARBOSW restarts from deterministic baseline.
- **Core assumption:** Optimal projection directions shift as distributions evolve during optimization; static sets become stale.
- **Evidence anchors:**
  - [section 4.3, Table 1] "RBOSW is strongest early (best at steps 100 and 200)" with W²=4.155/0.115; "ARBOSW achieves best value at step 500" with W²=0.003±0.000
  - [section 4.5] Observed reversal: "in autoencoders, the distribution is large and stable, so maintaining consistent projection sets (BOSW/ABOSW) is more effective than frequent refreshes"
  - [section 4.2] "BOSW trails (consistent with the need for fresh directions as the flow evolves)"
  - [corpus] "Markovian sliced wasserstein distances" (Nguyen et al., cited in paper) addresses similar adaptive direction selection problem
- **Break condition:** When distributions are stable (e.g., large-scale training), refresh introduces variance—Table 2 shows RBOSW/ARBOSW (1.90/1.85) lag behind ABOSW (1.81) on autoencoder reconstruction. Also breaks when R is too small: RBOSW incurs 44.45s runtime vs 3.86s for ABOSW.

## Foundational Learning

- **Concept: Sliced Wasserstein Distance and 1D Projection**
  - Why needed here: SW reduces d-dimensional OT to averaging 1D costs via sorting; understanding this O(n log n) mechanism is essential for grasping why direction selection matters.
  - Quick check question: For two point clouds X, Y ∈ R^d, why does computing SW require sorting the projections θ^T x_i before computing distances, and what is the per-direction complexity?

- **Concept: Bayesian Optimization with Gaussian Process Surrogates**
  - Why needed here: BO is the core direction-selection mechanism; requires understanding how GP posteriors enable sample-efficient optimization via acquisition functions.
  - Quick check question: Given GP posterior mean μ_t and variance σ_t^2, how does UCB acquisition α(θ) = μ_t(θ) + βσ_t(θ) balance exploration vs exploitation, and what happens as β→0 or β→∞?

- **Concept: Quasi-Monte Carlo and Low-Discrepancy Sequences on Spheres**
  - Why needed here: QSW baselines and ABOSW/ARBOSW initialization depend on deterministic low-discrepancy point sets; must understand why these outperform random MC for integration.
  - Quick check question: Why does QSW achieve O(L⁻¹) error convergence vs MC's O(L⁻¹/²), and why does randomized QSW (RQSW) restore unbiasedness while retaining uniformity?

## Architecture Onboarding

**Component map:**
Input distributions (μ, ν) → Direction Selector Module:
├── BOSW: Fresh BO run → fixed Θ_L (one-shot, biased)
├── RBOSW: BO rerun every R steps → adaptive (high overhead)
├── ABOSW: QSW seed → r=2 BO rounds → fixed Θ_L (hybrid)
└── ARBOSW: QSW seed → BO refine → restart every K steps
      ↓
SW Computation Loop (for θ ∈ Θ_L):
  Project: [θ^T x_1, ..., θ^T x_n], [θ^T y_1, ..., θ^T y_n]
  Sort: O(n log n) → [x_θ^(1), ..., x_θ^(n)]
  1D cost: (1/n) Σ|x_θ^(i) - y_θ^(i)|^p
      ↓
Aggregate: dSW_p^p(μ,ν; Θ_L) = (1/L) Σ W_p^p(θ_ℓ♯μ, θ_ℓ♯ν)
      ↓
Downstream: SW enters gradient/loss unchanged (drop-in replacement)

**Critical path:**
1. BO candidate scoring is O(n_c × n_t) kernel evaluations—parallelizable on GPU
2. SW computation bottleneck is O(L × n log n) sorting per iteration
3. Refresh interval R/refresh strategy is the key hyperparameter for adaptive variants

**Design tradeoffs:**
- BOSW vs QSW: Task-adaptive but biased vs unbiased but task-agnostic
- ABOSW vs ARBOSW: Stability (fixed after refinement) vs tracking (periodic restart)
- Candidate pool n_c: 4096 optimal for hybrids; 1024 for BOSW alone (Figure 11)
- Cosine threshold: 0.98 balances diversity vs redundancy (Figure 10)

**Failure signatures:**
- High approximation error on uniform/regular data → mechanism 1 has nothing to exploit; switch to QSW
- Autoencoder instability → excessive refresh variance; use ABOSW not ARBOSW
- Runtime blowup → check RBOSW refresh interval R; reduce n_c to 1024-2048

**First 3 experiments:**
1. Replicate synthetic landscape experiment (Figure 1) on Peaks/Ridge/Quadratic with L∈{5,10,15,20} to confirm BO finds informative directions when structure exists.
2. Run approximation error benchmark (Figure 2) on ShapeNet point cloud pairs at L∈{10,100,1000,10000} to validate where BOSW underperforms and why.
3. Execute point-cloud interpolation gradient flow (Table 1) for 500 steps with L=100, comparing all four BO variants against QSW baselines on convergence speed vs runtime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do BO-based selectors perform in high-dimensional ambient spaces ($d \gg 3$)?
- Basis in paper: [explicit] The authors state in the conclusion, "We would like to test our methods on higher-dimensional datasets in future work."
- Why unresolved: The experiments are restricted to low dimensions ($d=3$ for point clouds and RGB images), and BO traditionally struggles with dimensionality, though recent work suggests this may be mitigated.
- Evidence would resolve it: Empirical benchmarks on datasets with significantly higher ambient dimensions (e.g., $d \geq 50$).

### Open Question 2
- Question: Can finite-sample error bounds be derived for BOSW using concepts like information gain or regret minimization?
- Basis in paper: [explicit] Appendix D states, "Bounding the performance of BOSW for finite $L$ is an interesting open question," suggesting information gain as a potential avenue.
- Why unresolved: The theoretical analysis is currently limited because the UCB acquisition function introduces a bias that invalidates standard SW convergence assumptions.
- Evidence would resolve it: A formal derivation bounding the error $|SW_L(\hat{\Theta}_T) - SW_p(\mu, \nu)|$ for a fixed budget $L$.

### Open Question 3
- Question: Can an annealing schedule for the UCB parameter $\beta$ ensure asymptotic convergence to the true Sliced Wasserstein distance?
- Basis in paper: [inferred] Appendix D notes that with constant $\beta$, BOSW "will not converge in the limit" and informally sketches an annealing schedule to fix this bias.
- Why unresolved: The paper implements only a constant $\beta$; the annealing approach is proposed as a concept to harmonize fast early convergence with eventual uniform coverage but is not proven or tested.
- Evidence would resolve it: A modified algorithm with $\beta \to 0$ that empirically achieves $O(L^{-1/2})$ convergence error in static approximation tasks.

### Open Question 4
- Question: Does incorporating prior knowledge or constraints into the BO acquisition function provide significant gains over data-agnostic methods like QSW?
- Basis in paper: [explicit] The conclusion lists "Adding constraints or information to BO... would give our methods further advantage" as an opportunity for future improvement.
- Why unresolved: The current implementation treats the 1D Wasserstein cost as a black-box function without leveraging specific structural properties of the problem.
- Evidence would resolve it: Experiments comparing constrained BO variants against the standard BOSW and QSW baselines on complex optimization landscapes.

## Limitations
- BOSW introduces bias that prevents convergence to true SW distance, limiting use in unbiased estimation scenarios
- Performance improvements in autoencoders are modest (≈2% gain) and may stem from initialization rather than genuine signal
- Hyperparameter sensitivity to n_c, β, and batch size requires careful tuning for new applications
- RBOSW incurs significant runtime overhead (44.45s vs 3.86s) from frequent full BO restarts

## Confidence
**High Confidence:** The core mechanism of BO finding informative directions on structured landscapes (Mechanism 1) is well-supported by synthetic experiments (Figure 1) showing BO outperforming QSW when discriminative projections exist. The mathematical framework (GP with angular RBF kernel, UCB acquisition) is sound.

**Medium Confidence:** The hybrid ABOSW/ARBOSW approaches work as described, with moderate improvements in reconstruction loss (2% gain). However, the explanation for why periodic refresh hurts in autoencoders but helps in interpolation (Mechanism 3) relies on qualitative reasoning about "stable" vs "evolving" distributions without rigorous quantification of when each regime applies.

**Low Confidence:** The claim that ABOSW achieves "convergence comparable to the best QSW variants" is context-dependent. On the original QSW benchmark suite, ABOSW matches or outperforms only in optimization-in-the-loop scenarios. For pure approximation tasks (Figure 2), BOSW underperforms QSW, contradicting the idea that BO is universally beneficial.

## Next Checks
1. **Bias Quantification Study:** Systematically measure the bias introduced by BOSW across different distribution families and L values. Compare with an annealed β schedule that provably converges to uniform coverage, measuring the trade-off between bias reduction and performance loss in downstream tasks.

2. **Architecture Ablation:** For the autoencoder task, run controlled ablations: (a) test ABOSW initialization from random vs QSW seeds, (b) measure impact of BO refinement rounds r∈{0,1,2,3}, (c) compare with standard SW initialization but better optimizer. This would isolate whether BO provides genuine signal or just better starting conditions.

3. **Hyperparameter Sensitivity Analysis:** Conduct a grid search over (n_c ∈ {512,1024,2048,4096,8192}, β ∈ {0.1,0.3,0.5,0.7,0.9}, b ∈ {1,3,5,10}) for ABOSW on a representative task (e.g., ShapeNet interpolation). Report performance sensitivity and identify robust default settings, addressing the current lack of hyperparameter guidance.