---
ver: rpa2
title: 'AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning
  in Real-World Scenarios'
arxiv_id: '2508.19988'
source_url: https://arxiv.org/abs/2508.19988
tags:
- reasoning
- answer
- step
- compositional
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AgentCoMa, a benchmark that requires large
  language models (LLMs) to combine commonsense and mathematical reasoning in realistic
  agentic scenarios. Each question in the dataset contains two reasoning steps: a
  commonsense choice followed by a single arithmetic operation.'
---

# AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios

## Quick Facts
- arXiv ID: 2508.19988
- Source URL: https://arxiv.org/abs/2508.19988
- Reference count: 40
- Primary result: LLMs show ~30% performance drop on compositional reasoning tasks compared to individual steps

## Executive Summary
This paper introduces AgentCoMa, a benchmark designed to evaluate large language models' ability to combine commonsense and mathematical reasoning in realistic agentic scenarios. Each question requires two reasoning steps: a commonsense choice followed by a single arithmetic operation. When evaluated on 61 contemporary LLMs, all models achieve high accuracy on individual steps but experience a significant ~30% drop in performance when both steps are composed together. This compositionality gap is much larger than that seen in prior benchmarks combining multiple steps of the same reasoning type. Human annotators, in contrast, perform similarly on both individual steps and compositional questions.

## Method Summary
AgentCoMa consists of 260 human-written samples across 5 agentic scenarios, each containing a compositional question, commonsense sub-question, math sub-question, and ground-truth answers. The benchmark was evaluated using 2-shot CoT prompting with greedy decoding (temperature=0, top-k=1, max_tokens=2048) on 61 contemporary LLMs. Numerical answers were extracted via regex and exact-matched, while non-numerical answers were evaluated by GPT-4o as an LLM-as-a-judge. The key metric is the compositionality gap, defined as the difference between compositional accuracy and the proportion where both steps succeed independently.

## Key Results
- All 61 evaluated LLMs achieve high accuracy (>80%) on individual commonsense and math steps
- Models experience a ~30% drop in performance when both steps are composed together
- This compositionality gap is significantly larger than that seen in prior benchmarks combining multiple steps of the same reasoning type
- Human annotators perform similarly on both individual steps and compositional questions

## Why This Works (Mechanism)
The benchmark reveals that LLMs struggle with compositional reasoning when different reasoning types must be combined. Analysis suggests that mixed-type reasoning tasks are relatively rare in LLM training data, leading models to activate only the neural circuits relevant to one reasoning type when solving compositional questions. This selective activation prevents effective integration of both commonsense and mathematical reasoning required for the compositional task.

## Foundational Learning
- Compositional reasoning: Combining multiple reasoning types (why needed: core capability being measured; quick check: can models integrate commonsense with math?)
- Compositionality gap: Performance difference between individual and combined steps (why needed: quantifies difficulty of integration; quick check: measure gap size)
- Mixed-type reasoning: Tasks requiring different reasoning modalities (why needed: rare in training data; quick check: analyze training corpus statistics)

## Architecture Onboarding
**Component map:** Compositional question -> Commonsense step -> Mathematical step -> Final answer

**Critical path:** 2-shot CoT prompt → Model inference → Answer extraction → Evaluation (regex for numerical, GPT-4o judge for commonsense)

**Design tradeoffs:** Balanced scenario distribution vs. comprehensive coverage; strict evaluation vs. flexible interpretation

**Failure signatures:** 
- High individual step accuracy but low compositional accuracy
- Inconsistent reasoning across steps within same question
- Selective activation of only one reasoning type's neural circuits

**3 first experiments:**
1. Compare performance across different model families to identify architectural patterns
2. Test varying prompt contexts to control for attention lookback effects
3. Analyze error patterns to identify specific failure modes in compositional reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-as-a-judge evaluation introduces potential subjectivity without human validation
- The interpretation of compositionality gaps relies on post-hoc explanations about neural circuit activation
- The benchmark's 260 samples may be insufficient to fully characterize model capabilities
- Potential confounding effect of prompt context length on compositional performance due to reduced lookback attention

## Confidence
- **High confidence:** Individual step performance metrics and human baseline comparisons
- **Medium confidence:** Compositionality gap quantification and magnitude relative to prior work
- **Low confidence:** Causal explanation of why compositionality gaps occur and their relationship to neural circuit activation

## Next Checks
1. Replicate the compositionality gap measurement using models with extended context windows to control for attention lookback effects
2. Conduct human evaluation of GPT-4o's LLM-as-a-judge decisions on a stratified sample of 100 compositional responses
3. Perform ablation studies varying the proportion of commonsense vs. mathematical reasoning in training data to test the "rare mixed-type reasoning" hypothesis