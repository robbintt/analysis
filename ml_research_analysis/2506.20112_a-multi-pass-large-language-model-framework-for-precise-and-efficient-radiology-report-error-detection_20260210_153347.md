---
ver: rpa2
title: A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology
  Report Error Detection
arxiv_id: '2506.20112'
source_url: https://arxiv.org/abs/2506.20112
tags:
- framework
- error
- radiology
- report
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduced a three-pass large language model (LLM) framework
  for radiology report error detection, significantly improving precision while reducing
  operational costs. The framework employs a lightweight preprocessor to clean report
  content, followed by stepwise error detection and false-positive verification using
  advanced LLMs.
---

# A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection

## Quick Facts
- arXiv ID: 2506.20112
- Source URL: https://arxiv.org/abs/2506.20112
- Authors: Songsoo Kim; Seungtae Lee; See Young Lee; Joonho Kim; Keechan Kan; Dukyong Yoon
- Reference count: 0
- Three-pass LLM framework improves precision while reducing operational costs for radiology report error detection

## Executive Summary
This study introduces a three-pass large language model (LLM) framework designed to enhance the precision and efficiency of radiology report error detection. The framework strategically employs a lightweight preprocessor for content cleaning, followed by stepwise error detection and false-positive verification using advanced LLMs. Tested on 1,000 radiology reports from MIMIC-III and external datasets (CheXpert, Open-i), the approach achieves a positive predictive value (PPV) of 0.159—more than double that of single-pass baselines—while maintaining stable true positive rates. Operational costs are reduced by up to 42.6%, and the number of reports requiring human review drops by 54%, demonstrating a scalable, cost-effective solution for AI-assisted quality assurance in radiology.

## Method Summary
The proposed three-pass LLM framework systematically improves radiology report error detection through a staged approach. Initially, a lightweight preprocessor cleans report content to enhance downstream model performance. The first pass applies a lightweight LLM for initial error detection, followed by a second pass using a more advanced LLM to identify additional errors. A third verification pass filters false positives, further refining results. This multi-pass strategy is evaluated across three datasets (MIMIC-III, CheXpert, Open-i), demonstrating improved precision and cost-efficiency compared to single-pass baselines, while maintaining robust performance in external validation.

## Key Results
- Three-pass framework achieves PPV of 0.159, more than double single-pass baselines
- Operational costs reduced by up to 42.6%, from USD 9.72 to USD 5.58 per 1,000 reports
- 54% reduction in reports requiring human review, while maintaining stable true positive rates (0.012–0.014)

## Why This Works (Mechanism)
The three-pass LLM framework enhances precision and efficiency by strategically layering lightweight and advanced models. The preprocessor reduces noise, allowing the first pass to quickly identify clear errors with minimal cost. The second pass leverages a more powerful LLM to detect subtler issues, while the third pass acts as a quality gate, filtering out false positives before human review. This staged approach minimizes unnecessary high-cost LLM queries, reduces false positives, and maintains high recall, resulting in significant cost savings and improved operational efficiency.

## Foundational Learning
- **Preprocessing importance**: Why needed - Reduces noise and standardizes report content for better LLM performance; Quick check - Compare preprocessing impact on baseline vs. multi-pass accuracy
- **Multi-pass error detection**: Why needed - Allows for targeted use of advanced models, balancing precision and cost; Quick check - Measure cost vs. precision trade-off across pass combinations
- **False-positive filtering**: Why needed - Critical for reducing unnecessary human review and operational costs; Quick check - Assess reduction in human review workload vs. drop in recall

## Architecture Onboarding
- **Component map**: Preprocessor -> Pass 1 (lightweight LLM) -> Pass 2 (advanced LLM) -> Pass 3 (verification LLM)
- **Critical path**: Preprocessor → Pass 1 → Pass 2 → Pass 3 (error verification)
- **Design tradeoffs**: Uses lightweight models early to reduce costs, reserves advanced models for verification to balance precision and expense
- **Failure signatures**: Over-reliance on preprocessing may miss context-dependent errors; aggressive filtering in Pass 3 may reduce recall
- **First experiments**: 1) Measure cost reduction vs. baseline single-pass; 2) Validate PPV improvement on external datasets; 3) Test scalability with larger report volumes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to US-centric datasets; generalizability to other healthcare systems and languages uncertain
- Moderate PPV (0.159) still results in ~84% false positives requiring human verification
- Cost savings depend on specific LLM pricing model; may vary with different providers or regions

## Confidence
- Framework effectiveness and cost reduction: High - Well-supported by controlled experiments across multiple datasets with clear metrics
- External validity: Medium - Performance holds across different datasets but all share similar clinical contexts and language
- Clinical utility and workflow integration: Low - No evaluation of actual radiologist acceptance, impact on clinical decision-making, or long-term sustainability

## Next Checks
1. Test framework performance on radiology reports from non-US healthcare systems and in multiple languages to assess true generalizability
2. Conduct a prospective clinical study measuring actual time savings and error reduction in a real radiology workflow with practicing radiologists
3. Evaluate model performance across different LLM providers and pricing tiers to establish cost-effectiveness boundaries and identify optimal configurations for different healthcare settings