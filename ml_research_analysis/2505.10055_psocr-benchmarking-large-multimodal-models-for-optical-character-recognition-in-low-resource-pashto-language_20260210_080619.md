---
ver: rpa2
title: 'PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition
  in Low-resource Pashto Language'
arxiv_id: '2505.10055'
source_url: https://arxiv.org/abs/2505.10055
tags:
- text
- pashto
- dataset
- performance
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PsOCR, a large-scale synthetic dataset for
  evaluating Large Multimodal Models (LMMs) on Optical Character Recognition (OCR)
  in the low-resource Pashto language. To address the lack of annotated Pashto text
  data, the authors generated one million synthetic images annotated at word, line,
  and document levels, covering 1,000 font families and diverse layouts.
---

# PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language

## Quick Facts
- **arXiv ID**: 2505.10055
- **Source URL**: https://arxiv.org/abs/2505.10055
- **Reference count**: 40
- **Primary result**: PsOCR dataset with 1M synthetic images; Gemini best overall (CER: 0.10, WER: 0.31), Qwen-7B best open-source (CER: 0.34, WER: 0.73)

## Executive Summary
This paper introduces PsOCR, a large-scale synthetic dataset for evaluating Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. To address the lack of annotated Pashto text data, the authors generated one million synthetic images annotated at word, line, and document levels, covering 1,000 font families and diverse layouts. A benchmark subset of 10,000 images was used to assess seven open-source and four closed-source LMMs under zero-shot settings. Results show that Gemini achieved the best performance (CER: 0.10, WER: 0.31), while Qwen-7B was the top-performing open-source model (CER: 0.34, WER: 0.73). The study highlights the effectiveness of current LMMs for OCR in cursive, low-resource languages and provides a foundation for future research in similar scripts. The PsOCR dataset is publicly available.

## Method Summary
The authors created PsOCR by generating 1 million synthetic Pashto text images using CSS-based rendering across 1,000 font families, with sizes 11-30px and widths 200-800px. Each image was annotated at word, line, and document levels with bounding boxes. A benchmark subset of 10,000 images was evaluated using eight LMMs (4 open-source: Llama-3.2-11B-Vision-Instruct, Florence-2-large, Qwen2.5-VL-3B/7B; 4 closed: GPT-4o, Gemini-2.0-flash, Claude-3.7-sonnet, Grok) in zero-shot settings. Models were prompted to extract text only, with outputs post-processed to remove commentary. CER and WER were computed against ground truth, supplemented by BLEU, METEOR, and BoW similarity metrics.

## Key Results
- Gemini-2.0-flash achieved the best overall performance (CER: 0.10, WER: 0.31)
- Qwen2.5-VL-7B was the top-performing open-source model (CER: 0.34, WER: 0.73)
- All models showed significant improvement over random baseline (CER: 0.81, WER: 0.94)
- Zero-shot LMMs demonstrated strong capabilities for OCR in cursive, low-resource languages

## Why This Works (Mechanism)
The PsOCR dataset addresses the fundamental challenge of data scarcity in low-resource languages by synthetically generating diverse, annotated text images. The zero-shot evaluation approach leverages the multimodal capabilities of modern LMMs to perform OCR without task-specific fine-tuning, demonstrating that these models have learned generalizable visual-text representations. The comprehensive annotation scheme (word, line, document levels) enables detailed performance analysis and supports future fine-tuning efforts.

## Foundational Learning
- **Synthetic data generation**: Why needed: Real annotated Pashto text data is scarce. Quick check: Verify dataset covers diverse fonts, sizes, and layouts as claimed.
- **Zero-shot evaluation**: Why needed: Tests general model capability without task-specific adaptation. Quick check: Confirm all models were evaluated under identical conditions.
- **OCR metrics (CER/WER)**: Why needed: Standard measures of text recognition accuracy. Quick check: Validate metric calculations against ground truth.
- **Bounding box annotation**: Why needed: Enables spatial understanding and region-specific evaluation. Quick check: Ensure bounding boxes align correctly with text regions.
- **Multimodal prompting**: Why needed: Guides models to extract text rather than perform other tasks. Quick check: Verify prompts are consistent across models and effective at text extraction.

## Architecture Onboarding

**Component Map**: Data Generation -> Model Inference -> Post-processing -> Metric Computation

**Critical Path**: The evaluation pipeline flows from synthetic image generation through LMM inference, output post-processing, and finally to error rate calculations. Each stage must function correctly for valid results.

**Design Tradeoffs**: Synthetic data ensures scale and control but may not capture real-world variability. Zero-shot evaluation provides fair comparison but may underestimate achievable performance with fine-tuning. Open-source vs closed-source model access creates different inference environments.

**Failure Signatures**: 
- Models returning translations instead of raw text indicates prompt misinterpretation
- High variance in output format suggests insufficient post-processing
- Safety filter rejections (GPT-4o) result in missing responses
- Inconsistent bounding box alignment affects spatial metrics

**First Experiments**:
1. Run inference on a small PsOCR subset to verify model compatibility and prompt effectiveness
2. Test post-processing pipeline on sample outputs to ensure clean text extraction
3. Compute CER/WER on the validation set to confirm metric implementation

## Open Questions the Paper Calls Out
- How does the performance of state-of-the-art LMMs differ between synthetic printed Pashto text and cursive handwritten Pashto script?
- To what extent do complex backgrounds, textures, and variable lighting conditions degrade the OCR accuracy of current LMMs for Pashto?
- Can fine-tuning open-source models on the full PsOCR training set bridge the performance gap with closed-source models like Gemini?

## Limitations
- Synthetic data may not capture real-world document variability and noise
- Zero-shot evaluation potentially underestimates model performance achievable through fine-tuning
- Limited model diversity (8 total) may not represent the full landscape of LMM capabilities

## Confidence
- **High confidence**: PsOCR dataset creation methodology and public availability are well-documented and reproducible
- **Medium confidence**: Relative ranking of LMMs in zero-shot Pashto OCR is likely accurate, though absolute performance numbers may be optimistic
- **Low confidence**: Claims about specific model capabilities should be interpreted cautiously given synthetic dataset constraints and lack of fine-tuning

## Next Checks
1. Test the same models on a small set of real Pashto document images to assess generalization from synthetic to authentic data
2. Systematically vary prompts across models to determine how much performance varies with prompt engineering versus model capability
3. Implement and evaluate traditional OCR approaches (e.g., Tesseract with Pashto-trained models) on the same dataset to establish whether LMMs truly outperform specialized OCR solutions for this task