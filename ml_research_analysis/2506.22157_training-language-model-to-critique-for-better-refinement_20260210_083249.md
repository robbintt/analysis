---
ver: rpa2
title: Training Language Model to Critique for Better Refinement
arxiv_id: '2506.22157'
source_url: https://arxiv.org/abs/2506.22157
tags:
- critique
- answer
- your
- code
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Refinement-oriented Critique Optimization
  (RCO), a method for training critic models that generate critiques specifically
  aimed at improving actor model responses. The core idea is to use critique utility
  (CU) - calculated as the preference ratio of refined responses to initial responses
  - as a reward signal to train the critic model, eliminating the need for direct
  critique preference assessment.
---

# Training Language Model to Critique for Better Refinement
## Quick Facts
- arXiv ID: 2506.22157
- Source URL: https://arxiv.org/abs/2506.22157
- Reference count: 40
- Primary result: RCO achieves 81.1 CU and 6.49 RQS average across 5 tasks

## Executive Summary
This paper introduces Refinement-oriented Critique Optimization (RCO), a method for training critic models that generate critiques specifically aimed at improving actor model responses. The core idea is to use critique utility (CU) - calculated as the preference ratio of refined responses to initial responses - as a reward signal to train the critic model, eliminating the need for direct critique preference assessment. RCO significantly outperforms traditional methods and open-source models across five tasks: dialog generation, summarization, question answering, mathematical reasoning, and code generation.

## Method Summary
RCO trains critic models using a refinement-oriented objective where critique utility serves as the reward signal. The method evaluates critiques based on their ability to help an actor model improve its responses through refinement. By focusing on the downstream utility of critiques rather than direct preference assessment, RCO can be trained without requiring human-labeled critique preferences. The approach demonstrates strong weak-to-strong generalization capabilities and achieves superior performance across multiple language tasks.

## Key Results
- RCO achieves 81.1 CU and 6.49 RQS average across 5 tasks
- Outperforms traditional methods and open-source models on dialog generation, summarization, QA, mathematical reasoning, and code generation
- Demonstrates strong weak-to-strong generalization capabilities
- Human evaluation shows RCO excels in generating more accurate, thorough, clear, and constructive critiques

## Why This Works (Mechanism)
The core mechanism works by aligning critique generation with refinement utility rather than direct preference matching. By using the preference ratio of refined responses as the reward signal, RCO creates a feedback loop where critics learn to generate critiques that are actually useful for improving responses. This indirect optimization through refinement sidesteps the need for costly human annotation of critique preferences while still producing high-quality critiques.

## Foundational Learning
- Critique utility calculation: Why needed - to measure critique effectiveness; Quick check - preference ratio of refined vs initial responses
- Actor-critic framework: Why needed - to enable iterative improvement; Quick check - critic guides actor refinement
- Weak-to-strong generalization: Why needed - to ensure robustness; Quick check - performance on stronger actors than training

## Architecture Onboarding
- Component map: Critic -> Actor -> Refined Response -> Utility Score -> Reward Signal
- Critical path: Critique generation → Actor refinement → Response evaluation → Utility calculation → Critic update
- Design tradeoffs: Direct preference learning vs. refinement-based learning; computational cost vs. annotation savings
- Failure signatures: Low CU scores indicate ineffective critiques; high variance suggests instability
- First experiments: 1) Baseline comparison on CU scores, 2) Human evaluation of critique quality, 3) Cross-task generalization test

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on automatic metrics and synthetic data generation
- Focus on refinement as primary utility signal may not generalize to all critique scenarios
- Method depends on having an actor model to generate refinements
- Scalability to larger models and more diverse tasks remains untested

## Confidence
- High: Technical implementation and superiority over baselines in reported metrics
- Medium: Human evaluation results with limited sample sizes; generalization claims
- Low: Long-term effectiveness in real-world deployment scenarios

## Next Checks
1. Expand human evaluation to include larger sample sizes (500-1000 samples per task) and diversify evaluator pool
2. Cross-task generalization test by applying RCO-trained critics to substantially different tasks (creative writing, legal document review)
3. Real-world deployment pilot to test practical utility in production environments