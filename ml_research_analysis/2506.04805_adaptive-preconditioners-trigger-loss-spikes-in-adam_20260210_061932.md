---
ver: rpa2
title: Adaptive Preconditioners Trigger Loss Spikes in Adam
arxiv_id: '2506.04805'
source_url: https://arxiv.org/abs/2506.04805
tags:
- loss
- spike
- training
- gradient
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates loss spikes in Adam optimization, a phenomenon
  where training loss suddenly surges and subsides during neural network training.
  While previous explanations attributed spikes to loss landscape geometry, the authors
  demonstrate that Adam's adaptive preconditioners themselves can trigger these spikes.
---

# Adaptive Preconditioners Trigger Loss Spikes in Adam

## Quick Facts
- **arXiv ID:** 2506.04805
- **Source URL:** https://arxiv.org/abs/2506.04805
- **Reference count:** 40
- **Primary result:** Adam's adaptive preconditioners can independently trigger loss spikes through β₂-exponential decay, not just landscape geometry

## Executive Summary
This paper reveals that loss spikes in Adam optimization stem from the optimizer's own adaptive preconditioners rather than solely from loss landscape geometry. When squared gradients become much smaller than second-moment estimates, the latter undergoes β₂-exponential decay, decoupling from current gradient information. This pushes the maximum eigenvalue of the preconditioned Hessian beyond the stability threshold 2/η for sustained periods, inducing instability. The authors identify a five-phase progression from stable loss decrease to spike onset and resolution, and propose λgrad as a precise predictor for spike occurrences.

## Method Summary
The paper investigates Adam's spike mechanism through theoretical analysis and extensive experiments. The core approach involves monitoring adaptive preconditioner dynamics during training, particularly when ∥g_t∥ ≪ ∥√v̂_t∥ triggers v_t's β₂-exponential decay. The authors develop a novel predictor λgrad that captures gradient-directional curvature exceeding the stability threshold 2/η. Experiments span 1D quadratic functions, fully connected networks, CNNs, and Transformers, validating the mechanism across architectures. Hessian eigenvalues are computed via power iteration with Hessian-vector products, and various ablation studies test the robustness of findings.

## Key Results
- Adaptive preconditioners alone can trigger loss spikes through β₂-exponential decay when squared gradients become substantially smaller than second-moment estimates
- λgrad predictor accurately forecasts spike occurrences by measuring gradient-directional curvature exceeding 2/η threshold
- Five-phase spike progression (stable decrease → preconditioner decay → spike onset → preconditioner growth → loss decay) is consistently observed across architectures
- Spikes occur precisely when gradient aligns with maximum curvature direction and λgrad exceeds the threshold, making λmax(Ĥt) insufficient alone for prediction

## Why This Works (Mechanism)

### Mechanism 1: Preconditioner Decay-Coupled Instability
- **Claim:** Adam's adaptive preconditioners can trigger loss spikes independently of loss landscape geometry when second-moment estimates undergo β₂-exponential decay.
- **Mechanism:** When squared gradients become substantially smaller than second-moment estimates (∥g_t∥ ≪ ∥√v̂_t∥), v_t enters a β₂-dominant exponential decay regime (v_t ≈ β₂v_{t-1}) and becomes decoupled from current gradient information. This decay reduces the effective preconditioner strength, pushing the maximum eigenvalue of the preconditioned Hessian λ_max(Ĥ_t) beyond the classical stability threshold 2/η for sustained periods.
- **Core assumption:** Quadratic approximation of loss landscape holds locally; β₂ is sufficiently large (typically [0.95, 0.9999]).
- **Evidence anchors:**
  - [abstract]: "Specifically, we identify a critical regime where squared gradients become substantially smaller than the second-order moment estimates, causing the latter to undergo a β₂-exponential decay and to respond sluggishly to current gradient information."
  - [Section 4.2]: Fig. 3 shows experimental validation on quadratic functions demonstrating that β₂ = 0.99 produces pronounced spikes while β₂ = 0.9 produces only minor oscillations.
  - [corpus]: SPAM optimizer paper addresses similar spike-aware momentum reset mechanisms; SoftSignSGD proposes alternatives to Adam for spike minimization.
- **Break condition:** If gradients remain non-negligible relative to √v_t throughout training, β₂-exponential decay never initiates and spikes are suppressed.

### Mechanism 2: Gradient-Directional Curvature as Precise Spike Predictor
- **Claim:** λ_max(Ĥ_t) > 2/η alone is insufficient to predict spikes; spikes occur specifically when gradient-directional curvature λ_grad(Ĥ_t) exceeds the threshold for a sustained duration.
- **Mechanism:** In high dimensions, λ_max(Ĥ_t) > 2/η creates instability primarily along the maximum eigendirection, but a spike requires the gradient to align with this direction. The predictor λ_grad(Ĥ_t) := (∇L(θ_t)^T Ĥ ∇L(θ_t)) / ∥∇L(θ_t)∥² captures this alignment. Transient threshold crossings do not trigger spikes—sustained periods above 2/η are necessary.
- **Core assumption:** Hessian eigenvectors remain approximately stable across consecutive steps; gradient aligns progressively with maximum eigendirection during instability.
- **Evidence anchors:**
  - [abstract]: "A loss spike occurs precisely when the gradient-directional curvature exceeds 2/η."
  - [Section 4.3]: Fig. 4 demonstrates that λ_max(H_t) can exceed 2/η without triggering spikes, while λ_grad(H_t) > 2/η corresponds precisely to spike onset.
  - [corpus]: Limited direct corpus support for gradient-directional curvature predictor specifically.
- **Break condition:** If gradient never aligns with maximum eigendirection, or if threshold crossing is transient rather than sustained, no spike occurs.

### Mechanism 3: Five-Phase Spike Formation-Resolution Cycle
- **Claim:** Loss spikes follow a predictable five-phase progression tied to feedback dynamics between gradients and adaptive preconditioners.
- **Mechanism:** Phase 1 (stable loss decrease) → Phase 2 (v_t exponential decay as ∥g_t∥ ≪ ∥√v_t∥) → Phase 3 (spike onset when λ_grad(Ĥ_t) > 2/η sustained) → Phase 4 (gradient norm grows, eventually impacting v_t, causing v_t to increase) → Phase 5 (λ_grad(Ĥ_t) falls below 2/η, loss resumes decreasing). The sluggish response of v_t to gradient changes creates the characteristic spike shape.
- **Core assumption:** The feedback loop between gradient norm growth and v_t adjustment operates with characteristic delays governed by β₂.
- **Evidence anchors:**
  - [Section 4.4]: Explicitly defines all five phases with mechanistic description.
  - [Section 5.1-5.3]: Validates the progression across FNNs, CNNs, and Transformers with consistent phase timing.
  - [corpus]: Limited corpus support; this five-phase model appears novel to this work.
- **Break condition:** If β₂ is reduced or ε is increased sufficiently, the decay-growth feedback loop is disrupted and spikes are suppressed.

## Foundational Learning

- **Concept: Adam's second-moment estimate (v_t) dynamics**
  - **Why needed here:** Understanding how v_t = β₂v_{t-1} + (1-β₂)g²_t transitions between gradient-responsive and β₂-dominant regimes is essential to grasp the spike mechanism.
  - **Quick check question:** When ∥g_t∥ ≪ √v_t, what asymptotic behavior does v_t approach?

- **Concept: Stability threshold 2/η for iterative optimization**
  - **Why needed here:** The threshold 2/η determines when eigenvalues of the (preconditioned) Hessian cause iterative updates to diverge rather than converge.
  - **Quick check question:** For learning rate η = 0.001, what is the stability threshold, and what happens when λ_max exceeds it?

- **Concept: Preconditioned Hessian eigenvalues**
  - **Why needed here:** The preconditioned Hessian Ĥ = diag(1/√v̂_t + ε) H determines Adam's effective curvature, not the raw Hessian H.
  - **Quick check question:** How does decreasing v_t affect the eigenvalues of Ĥ, and what implication does this have for stability?

## Architecture Onboarding

- **Component map:**
  - Adam update: θ_{t+1} = θ_t - η·m̂_t / (√v̂_t + ε)
  - First moment: m_t = β₁m_{t-1} + (1-β₁)g_t (momentum)
  - Second moment: v_t = β₂v_{t-1} + (1-β₂)g²_t (adaptive preconditioner)
  - Preconditioned Hessian: Ĥ_t = (1/(1-β₁^t))·((1-β₁)/(1+β₁))·diag(1/√v̂_t + ε)·H_t
  - Spike predictor: λ_grad(Ĥ_t) = (∇L^T Ĥ ∇L) / ∥∇L∥²

- **Critical path:**
  1. Monitor ∥g_t∥ relative to √v̂_t during training (especially when loss decreases steadily).
  2. When ∥g_t∥ ≪ √v̂_t, check if v_t is decaying at rate ≈ β₂.
  3. Track λ_grad(Ĥ_t) approaching 2/η threshold.
  4. If sustained λ_grad > 2/η, expect spike onset within iterations.

- **Design tradeoffs:**
  - **Higher β₂ (0.999):** Smoother convergence but higher spike risk due to slower v_t response.
  - **Lower β₂ (0.9):** More responsive v_t, suppressed spikes, but potentially inferior final convergence.
  - **Larger ε:** Suppresses spikes by bounding 1/√v̂_t, but may slow overall convergence.
  - **Learning rate η:** Higher η increases effective step size, lowering 2/η threshold and making instability more likely.

- **Failure signatures:**
  - Sudden loss increase after sustained decrease with ∥g_t∥ ≪ √v̂_t
  - v_t decaying exponentially at rate ≈ β₂ while gradient remains small
  - λ_grad(Ĥ_t) sustained above 2/η for multiple iterations
  - Gradient aligning with maximum Hessian eigendirection (cosine similarity → 1)

- **First 3 experiments:**
  1. **Reproduce spike on quadratic objective:** Optimize f(θ) = ½θ² with Adam (β₁=0.9, β₂=0.99, η=0.001-1.0). Verify that spike timing corresponds to η/√v̂_t reaching threshold ≈38 (from Section 3, Fig. 2c).
  2. **Validate λ_grad predictor on FNN:** Train two-layer FNN (width 1000) on 200 samples of f(x)=sin(x)+sin(4x) with Adam (η=0.01, β₁=0.9, β₂=0.999, full-batch). Compute λgrad(Ĥt) and λmax(Ĥt) via power iteration; confirm spikes only when λgrad(Ĥt) exceeds 2/η while λmax(Ĥt)>2/η alone is insufficient (Fig. 4c–d).
  3. **Test spike suppression strategies:** Compare (a) increasing ε to 0.1 at spike onset, (b) clipping v_t minimum, (c) reducing β₂ to 0.9. Measure impact on spike frequency and final convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the individual contributions of loss landscape geometry versus adaptive preconditioner dynamics be disentangled when they jointly produce loss spikes in large-scale models?
- **Basis in paper:** [explicit] "Disentangling their individual contributions and attributing different spike mechanisms remains an open direction for future work."
- **Why unresolved:** The paper demonstrates preconditioners can independently trigger spikes, but acknowledges both factors may interact in complex real-world training scenarios.
- **What evidence would resolve it:** Controlled experiments isolating each mechanism, or theoretical analysis quantifying their relative contributions under varying conditions.

### Open Question 2
- **Question:** What precise conditions determine whether a loss spike will be beneficial, neutral, malignant, or catastrophic for generalization performance?
- **Basis in paper:** [explicit] "Determining the precise conditions under which a spike becomes beneficial or malignant remains an open question for future research."
- **Why unresolved:** While the paper identifies four spike types through empirical observation, it does not establish predictive criteria for which type will occur.
- **What evidence would resolve it:** Systematic experiments tracking training/test loss dynamics across spike events, combined with analysis of landscape properties before/after spikes.

### Open Question 3
- **Question:** How can gradient-directional curvature λ_grad and preconditioned Hessian eigenvalues be efficiently approximated in large-scale models where exact Hessian computation is prohibitive?
- **Basis in paper:** [explicit] "Developing efficient algorithms to approximate the maximum eigenvalue of the Hessian and the eigenvalues in the gradient direction represents a critical direction for future work."
- **Why unresolved:** Power iteration with Hessian-vector products becomes impractical for models with billions of parameters.
- **What evidence would resolve it:** Development of low-cost proxies or stochastic estimators that correlate strongly with exact eigenvalue computations while scaling to large models.

### Open Question 4
- **Question:** Can computationally efficient real-time criteria be developed to distinguish between the four spike types during training for adaptive intervention?
- **Basis in paper:** [explicit] "Developing robust, computationally efficient criteria to distinguish between these categories would significantly enhance our ability to detect and appropriately respond to different spike types during training."
- **Why unresolved:** Current spike prediction requires expensive Hessian computations and post-hoc analysis of generalization effects.
- **What evidence would resolve it:** Identification of lightweight signals (e.g., gradient statistics, loss ratios) that predict spike type before or during occurrence.

## Limitations

- The mechanism assumes quadratic approximation of loss landscape holds locally, which may not capture complex non-convex behavior in deep networks
- Limited validation of the five-phase model beyond the specific architectures examined (FNNs, CNNs, one Transformer configuration)
- Practical implementation of λgrad predictor requires expensive Hessian computations that don't scale to very large models
- The work doesn't fully characterize how spikes interact with other training phenomena like learning rate schedules or data augmentation

## Confidence

- **High Confidence:** The core observation that Adam's adaptive preconditioners can trigger loss spikes through v_t decay mechanisms. This is well-supported by theoretical analysis and multiple experimental validations across architectures.
- **Medium Confidence:** The λ_grad predictor as the definitive signal for spike onset. While experiments show strong correlation, the predictor's performance across diverse loss landscapes and training dynamics warrants additional validation.
- **Medium Confidence:** The five-phase spike formation-resolution cycle. The mechanistic description is internally consistent, but real-world training may exhibit more complex dynamics than the simplified model captures.

## Next Checks

1. **Cross-architecture generalization test:** Validate the λ_grad predictor on architectures not examined in the paper (e.g., Vision Transformers, diffusion models) to assess whether the spike mechanism generalizes beyond FNNs, CNNs, and the specific Transformer configuration studied.

2. **Dynamic landscape variation experiment:** Design training scenarios where the loss landscape itself changes significantly (e.g., curriculum learning, data augmentation schedules) to distinguish between spikes triggered by preconditioner decay versus landscape-induced instabilities.

3. **Real-time intervention validation:** Implement online monitoring of λ_grad during training and test whether immediate interventions (preemptively adjusting β₂, ε, or learning rate when λ_grad approaches 2/η) can prevent spikes without degrading final performance, providing practical validation of the theoretical framework.