---
ver: rpa2
title: 'Quantization for OpenAI''s Whisper Models: A Comparative Analysis'
arxiv_id: '2503.09905'
source_url: https://arxiv.org/abs/2503.09905
tags:
- whisper
- quantization
- speech
- latency
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study analyzes OpenAI''s Whisper and its variants for automated
  speech recognition, focusing on quantization''s impact on latency and model size.
  Three Whisper variants were compared: standard Whisper, whisper-timestamped (with
  word-level timestamps), and WhisperStreaming (optimized for real-time transcription).'
---

# Quantization for OpenAI's Whisper Models: A Comparative Analysis

## Quick Facts
- **arXiv ID:** 2503.09905
- **Source URL:** https://arxiv.org/abs/2503.09905
- **Reference count:** 30
- **Primary result:** INT4 quantization reduced model size by 45% and latency by 19% while maintaining transcription accuracy

## Executive Summary
This study analyzes quantization's impact on OpenAI's Whisper ASR models, focusing on three variants: standard Whisper, whisper-timestamped (with word-level timestamps), and Whisper_Streaming (optimized for real-time transcription). Using the LibriSpeech dataset, the research evaluates INT4, INT5, and INT8 quantization methods on the Whisper base model. Results demonstrate that quantization significantly reduces both model size and latency while preserving transcription accuracy, with the encoder identified as the primary computational bottleneck. The findings support deployment on resource-constrained devices like mobile phones and IoT systems without sacrificing performance.

## Method Summary
The study benchmarked OpenAI's Whisper base model using post-training integer quantization (INT4/INT5/INT8) implemented through whisper.cpp, comparing against standard Whisper implementation. Evaluation used the LibriSpeech dataset, specifically the test-clean subset (10 files for quantitative benchmarks, 25 total files mentioned elsewhere). The research measured Word Error Rate (WER), model size reduction, and latency improvements on an Intel Xeon CPU @ 2.20GHz. Three Whisper variants were tested: standard Whisper, whisper-timestamped (using Dynamic Time Warping on cross-attention weights for word-level timestamps), and Whisper_Streaming (using buffer-based processing with local agreement policies for real-time transcription).

## Key Results
- Quantization reduced model size by 45% (Q4: 44MB vs base: 140MB)
- Average latency reduction of 19% across quantization levels
- WER maintained at 0.0199 (98% accuracy), with INT4 surprisingly showing improved WER of 0.0159 on small sample
- Whisper encoder identified as primary bottleneck (4600ms vs decoder's 226ms)

## Why This Works (Mechanism)

### Mechanism 1: Integer Quantization
Post-training integer quantization (INT4/INT8) compresses model weights with minimal impact on Word Error Rate for clean speech. The process maps continuous floating-point weights to discrete lower-bit integer representations, reducing memory footprint and accelerating inference through hardware integer arithmetic. Core assumption: reduced precision retains sufficient information to approximate original decision boundaries for target audio distribution.

### Mechanism 2: Buffer-based Streaming
`Whisper_Streaming` divides audio into short segments, processes them with a sliding context window, and holds recent text in a buffer. It finalizes output only when transcription stabilizes ("local agreement"), trading slight latency for coherence. Core assumption: underlying non-causal model can process short chunks effectively with sliding context.

### Mechanism 3: DTW Timestamp Alignment
`whisper-timestamped` uses cross-attention maps from decoder to encoder, applying Dynamic Time Warping to warp alignments and correct internal timing to match actual speech signal. Core assumption: cross-attention weights contain learnable linear projection correlating strongly with temporal audio position.

## Foundational Learning

- **Concept:** Integer Quantization (INT4/INT8)
  - **Why needed:** Central optimization technique reducing model size (44MB vs 140MB) and latency
  - **Quick check:** Why does reducing bit-width of model weights (16-bit to 4-bit) directly lower inference latency on compatible hardware?

- **Concept:** Word Error Rate (WER)
  - **Why needed:** Primary accuracy metric penalizing substitutions, deletions, and insertions
  - **Quick check:** If a model transcribes "the cat sat" as "the cat stands," would the WER increase? (Answer: Yes, due to substitution)

- **Concept:** Encoder-Decoder Architecture
  - **Why needed:** Whisper is a Transformer Encoder-Decoder; latency breakdown shows encoder as heavier compute burden
  - **Quick check:** Based on latency breakdown, which part of Whisper architecture should be primary optimization target for long audio files?

## Architecture Onboarding

- **Component map:** Input (Audio) → Log-Mel Spectrogram → Whisper Core (Encoder + Decoder) → Quantization Layer (whisper.cpp/ggml) → Variant Layer (DTW alignment or streaming buffer)
- **Critical path:** Encode step is bottleneck (4600ms vs decoder's 226ms); optimizing or quantizing encoder is highest-leverage action for reducing total latency
- **Design tradeoffs:**
  - INT4 vs INT8: INT4 offers smallest size (44MB) and fastest speed but risks higher hallucination rates
  - Streaming vs Accuracy: Whisper_Streaming prioritizes real-time responsiveness (3.3s latency) but may miss context
- **Failure signatures:**
  - Hallucinations: Generating text not present in audio (e.g., "Subtitles by Amara.org")
  - Timestamp Drift: In Whisper_Streaming, timestamps occasionally resetting to 0.00s or undercounting phrase duration
- **First 3 experiments:**
  1. Reproduce WER vs Quantization on complete LibriSpeech test-clean set to verify INT4 advantage
  2. Profile whisper.cpp on target hardware to confirm encoder remains dominant time sink
  3. Test quantized models on silent audio/noise to check hallucination rates vs baseline

## Open Questions the Paper Calls Out
- Can quantization benefits generalize to other ASR architectures beyond Whisper model family?
- Does quantization specifically mitigate or exacerbate hallucinated content in transcriptions?
- How do latency/accuracy trade-offs translate to actual resource-constrained edge hardware vs high-performance CPUs?

## Limitations
- Small evaluation subset (10 files) limits statistical significance of WER improvements
- Lack of evaluation on noisy speech conditions or low-resource languages
- Absence of hardware-specific benchmarking beyond Intel Xeon CPU limits generalizability to mobile/embedded deployment

## Confidence
- **High Confidence:** Model size reduction (45%), latency reduction (19%), encoder as bottleneck
- **Medium Confidence:** INT4 maintaining accuracy, Whisper_Streaming real-time performance, whisper-timestamped timestamp accuracy
- **Low Confidence:** INT4 improving WER (sample size too small), DTW alignment effectiveness, streaming buffer coherence in complex scenarios

## Next Checks
1. Run WER tests on complete LibriSpeech test-clean set (2,620 files) to verify if INT4 accuracy advantage holds
2. Benchmark quantized models on ARM-based mobile hardware to validate 19% latency reduction in resource-constrained environments
3. Evaluate quantization performance on noisy audio (Librispeech test-other) and accented speech samples to test challenging acoustic conditions