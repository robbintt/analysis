---
ver: rpa2
title: Knowledge Base Construction for Knowledge-Augmented Text-to-SQL
arxiv_id: '2505.22096'
source_url: https://arxiv.org/abs/2505.22096
tags:
- knowledge
- base
- text-to-sql
- queries
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KAT-SQL, a method for knowledge base construction
  and augmentation to improve text-to-SQL performance. The core idea is to automatically
  build a comprehensive knowledge base from available query-schema pairs and their
  associated knowledge, then retrieve and refine relevant knowledge to assist SQL
  generation.
---

# Knowledge Base Construction for Knowledge-Augmented Text-to-SQL

## Quick Facts
- arXiv ID: 2505.22096
- Source URL: https://arxiv.org/abs/2505.22096
- Reference count: 29
- Primary result: Achieves 41.18% exact match accuracy on BIRD (overlap) outperforming baselines

## Executive Summary
KAT-SQL introduces a knowledge base construction method for text-to-SQL that automatically builds a comprehensive repository from query-schema pairs and their associated knowledge. The system retrieves and refines relevant knowledge to assist SQL generation, addressing the limitation of prior work that generates only a few pieces of knowledge per query. Experiments show substantial improvements over baselines across multiple datasets including BIRD, Spider, and CSTINSIGHT, with retrieval accuracy reaching 92.65% MRR in overlapping domains and 83.47% in non-overlapping settings.

## Method Summary
KAT-SQL operates in two phases: offline knowledge base construction and online inference. The construction phase uses an LLM to generate knowledge entries from query-schema pairs, starting with existing knowledge annotations and expanding through iterative few-shot prompting. The inference phase employs a fine-tuned retriever (MPNet/TAS-B) to fetch top-j relevant entries, which are then refined by a refiner LLM into a single tailored knowledge piece. This refined knowledge, combined with the query and schema, is fed to an SQL generator LLM to produce the final SQL query. The retriever is trained with contrastive learning to improve retrieval accuracy.

## Key Results
- KAT-SQL achieves 41.18% exact match accuracy on BIRD (overlap), outperforming DELLM (34.70%) and no-knowledge baseline (23.76%)
- Retrieval accuracy reaches 92.65% MRR when test and training domains overlap, 83.47% under non-overlap settings
- Generalizes well to unseen domains, achieving 24.19% exact match on non-overlapping databases
- Consistently improves performance across different LLMs including Llama, Granite, and Mixtral
- Sets new state-of-the-art when combined with existing text-to-SQL models

## Why This Works (Mechanism)
The method works by creating a reusable knowledge repository that captures domain-specific patterns and relationships from query-schema pairs, rather than generating ad-hoc knowledge for each query. The retrieval component finds relevant examples based on semantic similarity in embedding space, while the refinement step synthesizes multiple retrieved entries into context-specific knowledge tailored to the target schema. This approach enables knowledge sharing across similar queries and databases, reducing redundancy while maintaining coverage.

## Foundational Learning

- **Dense Retrieval with Bi-Encoders**: Why needed here - The system uses embedding models (MPNet, TAS-B) to encode queries and knowledge entries into vector space for similarity-based retrieval. Understanding how these embeddings represent semantic meaning is crucial for debugging and improving the retrieval component. Quick check question: Given a query and two knowledge strings, can you predict which one would likely have a higher cosine similarity to the query's embedding?

- **LLM In-Context Learning with RAG**: Why needed here - The final SQL generation is performed by an LLM conditioned on a prompt containing the query, schema, and refined knowledge. The performance hinges on the LLM's ability to attend to and integrate the retrieved knowledge at inference time without weight updates. Quick check question: How does the order of retrieved context in the prompt affect an LLM's reliance on it?

- **Synthetic Data Generation with LLMs**: Why needed here - The knowledge base is expanded by prompting an LLM to generate new knowledge entries from query-schema pairs. This requires understanding prompting strategies (e.g., few-shot examples) to control output quality and format. Quick check question: If an LLM generates a knowledge entry that is syntactically correct but semantically incorrect for a database schema, how might you detect it automatically?

## Architecture Onboarding

- **Component map**: Knowledge Base Construction -> Retriever (MPNet/TAS-B) -> Refiner LLM -> SQL Generator LLM -> SQL Output
- **Critical path**: The correctness of the final SQL is most sensitive to the quality of the Refiner's output. The retriever's job is to provide good raw material, but the refiner must correctly interpret it in the context of the database schema.
- **Design tradeoffs**: KB Size vs. Retrieval Noise - A larger, more comprehensive knowledge base increases coverage but may also introduce more irrelevant entries, making the retriever's job harder. Refinement vs. Cost - Adding the refinement LLM step improves accuracy but adds latency and computational cost.
- **Failure signatures**: Hallucinated Knowledge - If the refiner produces knowledge not grounded in either the retrieved entries or the schema, the generator may produce syntactically correct but semantically wrong SQL. Retrieval Miss - If the retriever fails to find any relevant entries, the refiner has no useful signal, potentially degrading to a weaker zero-shot or purely schema-based generation.
- **First 3 experiments**:
  1. Ablation on Refinement: Run inference directly with the top-1 retrieved knowledge versus the refined knowledge to quantify the refiner's contribution.
  2. Retrieval Model Comparison: Compare a baseline retriever (e.g., off-the-shelf MPNet) against the contrastively-trained retriever on a held-out set of query-knowledge pairs using MRR.
  3. KB Coverage Analysis: For a sample of test queries, check if the gold knowledge exists in the KB. Then, for queries where it's missing, analyze if similar knowledge is present and if the refiner successfully adapts it.

## Open Questions the Paper Calls Out
- How can knowledge base coverage be improved to close the substantial performance gap between generated knowledge and oracle human-annotated knowledge? The authors state there is still room to improve coverage with advanced knowledge base construction methods.
- What is the optimal balance between knowledge base size/comprehensiveness and computational overhead for text-to-SQL systems? The paper constructs KBs with 86,254-117,328 entries but does not systematically analyze the size-performance trade-off curve.
- How can retrieval accuracy and generalization be improved for non-overlapping database domains where relevant knowledge is scarce? Retrieval MRR drops significantly from overlap to non-overlap settings, indicating a generalization challenge.

## Limitations
- The evaluation does not provide detailed analysis of whether generated knowledge is semantically correct or relevant to the specific schema
- Knowledge base construction relies heavily on LLM-generated synthetic data, introducing potential for hallucinated or schema-inconsistent knowledge entries
- Performance drops substantially on non-overlapping databases (24.19% EM vs 41.18% EM for overlapping), raising questions about scalability to truly unseen domains

## Confidence
- **High Confidence**: The core methodology of building a reusable knowledge base and retrieving/refining knowledge for text-to-SQL tasks is well-described and the reported improvements over baselines are statistically significant
- **Medium Confidence**: The retrieval accuracy claims (92.65% MRR for overlap, 83.47% for non-overlap) appear robust based on the contrastively trained retriever
- **Medium Confidence**: The generalization results on non-overlapping databases demonstrate practical utility, though the substantial performance gap suggests limitations in domain transfer

## Next Checks
1. Conduct a qualitative analysis of 50 randomly sampled knowledge entries comparing generated vs. gold knowledge for semantic correctness and schema alignment, measuring the rate of hallucinated information
2. Perform an ablation study varying the knowledge base size (different numbers of generated entries) to identify the optimal tradeoff between coverage and retrieval noise, measuring both retrieval accuracy and final SQL generation performance
3. Test the system on a truly out-of-domain database (e.g., from a completely different application domain than BIRD training) to assess real-world generalization beyond the controlled non-overlapping experiments