---
ver: rpa2
title: 'Retrieve-Refine-Calibrate: A Framework for Complex Claim Fact-Checking'
arxiv_id: '2601.16555'
source_url: https://arxiv.org/abs/2601.16555
tags:
- evidence
- claim
- fact-checking
- framework
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fact-checking complex claims using a Retrieve-Refine-Calibrate
  (RRC) framework that avoids claim decomposition. The framework retrieves evidence
  based on entities in the claim, refines it to reduce noise, and calibrates predictions
  by re-evaluating low-confidence outputs.
---

# Retrieve-Refine-Calibrate: A Framework for Complex Claim Fact-Checking

## Quick Facts
- arXiv ID: 2601.16555
- Source URL: https://arxiv.org/abs/2601.16555
- Reference count: 29
- Primary result: RRC framework outperforms baselines on HOVER (72.55 Macro-F1) and FEVEROUS-S (60.95 Macro-F1 on 4-hop subset) by avoiding claim decomposition

## Executive Summary
This paper presents a three-stage Retrieve-Refine-Calibrate (RRC) framework for complex claim fact-checking that avoids the decomposition paradigm. Instead of splitting complex claims into sub-claims, RRC anchors retrieval on entities extracted directly from the original claim, then refines evidence through LLM compression, and calibrates low-confidence predictions. The approach demonstrates that maintaining claim semantic unity while integrating claim-specific evidence improves accuracy and robustness in multi-hop fact-checking tasks.

## Method Summary
The RRC framework operates through three stages: (1) Entity extraction using LUKE to identify up to 3 key entities, followed by BM25 retrieval of k=10 documents for HOVER and k=5 for FEVEROUS-S; (2) LLM-based evidence refinement that compresses and paraphrases retrieved documents into a concise, claim-relevant paragraph; (3) Verification with confidence threshold θ=0.90 for HOVER and θ=0.85 for FEVEROUS-S, triggering a calibrator module for low-confidence predictions. The framework uses DeepSeek-Coder or Qwen3-32B as backbone models and achieves state-of-the-art results without requiring explicit claim decomposition.

## Key Results
- Achieves 72.55 Macro-F1 on HOVER benchmark, outperforming decomposition-based baselines
- Scores 60.95 Macro-F1 on 4-hop subset of FEVEROUS-S, showing consistent gains across backbone model sizes
- Ablation studies show each component (refinement, calibration) contributes measurable performance improvements
- Evidence refinement improves factuality and completeness while slightly reducing fluency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Avoiding claim decomposition reduces noise propagation in multi-hop fact-checking.
- **Mechanism**: Anchors retrieval on entities extracted directly from the original claim rather than splitting it into sub-claims, maintaining semantic unity.
- **Core assumption**: The claim contains sufficient specific entities to anchor necessary evidence; explicit decomposition introduces more noise than reasoning value.
- **Evidence anchors**: Abstract states decomposition may introduce noise degrading verification accuracy; Section 1 shows decomposition mistakenly links actors to unrelated individuals.
- **Break condition**: If a claim is abstract or lacks specific named entities, entity-based retrieval may yield insufficient or irrelevant evidence.

### Mechanism 2
- **Claim**: LLM-based evidence refinement filters noisy retrieval artifacts better than raw concatenation.
- **Mechanism**: Generative LLM compresses and paraphrases retrieved documents into a concise, claim-relevant paragraph that removes irrelevant sentences.
- **Core assumption**: LLM has sufficient parametric knowledge to identify relevant context within noisy documents without hallucinating new facts.
- **Evidence anchors**: Section 2.3 describes the refinement process; Table 4 shows refined evidence improves Factuality and Completeness vs. original retrieval.
- **Break condition**: If the Refiner LLM is too small or aggressive, it may drop crucial nuance required for 4-hop reasoning.

### Mechanism 3
- **Claim**: Conditional calibration on low-confidence predictions mitigates hallucination without unnecessary compute.
- **Mechanism**: Checks verifier's confidence score; if below threshold θ, a secondary LLM re-evaluates the reasoning chain and evidence.
- **Core assumption**: Low confidence correlates strongly with reasoning errors or hallucinations, while high confidence correlates with correctness.
- **Evidence anchors**: Section 2.4 states low confidence indicates potential flaws; Table 3 shows performance drop without calibration in complex scenarios.
- **Break condition**: If the model is "confidently wrong," the calibration module is bypassed and the error persists.

## Foundational Learning

- **Concept**: **Multi-hop Reasoning**
  - **Why needed here**: Target datasets require synthesizing facts from multiple documents to verify a single claim.
  - **Quick check question**: If evidence A links to B, and B links to C, can the model trace A→C if A and C are in different documents?

- **Concept**: **Confidence Calibration**
  - **Why needed here**: Raw LLM probabilities are often misaligned with actual correctness; framework relies on confidence scores to trigger costly re-evaluation.
  - **Quick check question**: Does a probability of 0.9 from the model mean the answer is correct 90% of the time?

- **Concept**: **Entity Linking**
  - **Why needed here**: "Entity-based Retrieval" module relies on LUKE to extract entities before BM25 retrieval.
  - **Quick check question**: Can the system disambiguate "Apple" (fruit) from "Apple" (company) based solely on claim context?

## Architecture Onboarding

- **Component map**: Input (Complex Claim) → Entity Extractor (LUKE) → Retriever (BM25) → Refiner (LLM) → Verifier (LLM) → Calibrator (LLM)
- **Critical path**: Entity Extraction and Refinement stages; if LUKE misses an entity, evidence is never retrieved; if Refinement strips crucial detail, Verifier hallucinates.
- **Design tradeoffs**: Threshold θ set high (0.90 for HOVER) vs. lower (0.85 for FEVEROUS-S); Docs count k=10 for HOVER vs k=5 for FEVEROUS-S.
- **Failure signatures**: Low Recall (Entity Extractor missed a name); Compressed Hallucination (Refiner LLM hallucinated during paraphrasing); Entity Ambiguity (Refiner failed to filter context).
- **First 3 experiments**: 1) Module Ablation (run RRC, then remove Calibrator, then remove Refiner); 2) Threshold Sweep (vary θ from 0.7 to 0.95); 3) Refinement Quality Check (inspect 10 cases where "w/o Refinement" failed but "w/ Refinement" succeeded).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the evidence refinement process be modified to maintain or improve linguistic fluency while preserving the gains in factuality and conciseness?
- Basis in paper: Analysis section states refined evidence lags behind original retrieval in fluency, attributing this to refinement emphasizing factual consistency.
- Why unresolved: Current LLM-based refinement successfully compresses evidence and improves factuality but inadvertently degrades linguistic naturalness.
- What evidence would resolve it: Modification to refinement prompt or objective function yielding G-Eval fluency scores equal to or higher than original evidence while maintaining high factuality and conciseness.

### Open Question 2
- Question: Can a hybrid approach that utilizes decomposition for simpler claims and RRC for complex claims outperform the current universal strategy?
- Basis in paper: Main Results section notes improvement on 2-hop subset is limited because such claims decompose more easily, favoring decomposition methods.
- Why unresolved: While RRC excels at complex multi-hop reasoning, it appears at a relative disadvantage for simpler claims compared to decomposition baselines.
- What evidence would resolve it: Experiments demonstrating complexity-classifier can successfully route 2-hop claims to decomposition methods and 3/4-hop claims to RRC to achieve state-of-the-art performance across all subsets.

### Open Question 3
- Question: Is it possible to dynamically determine the optimal confidence threshold (θ) for calibration rather than relying on dataset-specific manual tuning?
- Basis in paper: Section 3.4 analyzes impact of threshold θ, noting performance peaks at 0.90 for HOVER but drops if threshold is too high, whereas FEVEROUS-S behaves differently.
- Why unresolved: Framework currently requires manual selection of θ (0.90 vs 0.85) to balance trade-off between correcting unreliable predictions and preserving stable ones.
- What evidence would resolve it: Development of adaptive thresholding mechanism that automatically adjusts θ based on retrieval density or claim ambiguity, resulting in robust performance without per-dataset hyperparameter search.

## Limitations

- Entity disambiguation strategy not detailed when LUKE extracts ambiguous entities with same surface form
- Confidence extraction mechanism remains underspecified - unclear whether γ is parsed from LLM output or derived from logits
- Prompt templates for verifier, refiner, and calibrator modules are critical missing components for faithful reproduction

## Confidence

- **High confidence**: Avoiding claim decomposition reduces noise propagation (well-supported by cited abstract and section 1 arguments)
- **Medium confidence**: LLM-based evidence refinement filters noisy retrieval artifacts (supported by Table 4 results but depends on unprovided prompt details)
- **Medium confidence**: Conditional calibration mechanism's effectiveness (demonstrated empirically but theoretical justification indirectly cited)

## Next Checks

1. Implement confidence score distribution logging to verify threshold θ triggers calibration at expected rate (target 10-15% for HOVER)
2. Conduct controlled ablation testing: run RRC with/without calibration on 100 random samples to measure precision-recall trade-off
3. Perform entity extraction validation: manually verify LUKE correctly identifies and disambiguates all named entities in 20 diverse complex claims from HOVER