---
ver: rpa2
title: Distilling Normalizing Flows
arxiv_id: '2506.21003'
source_url: https://arxiv.org/abs/2506.21003
tags:
- student
- flows
- flow
- distillation
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces knowledge distillation techniques for normalizing
  flows, a class of explicit density models that learn tractable probability distributions
  through invertible transformations. The authors address the challenge that normalizing
  flows are difficult to train and often have lower sampling quality compared to implicit
  models like GANs.
---

# Distilling Normalizing Flows

## Quick Facts
- arXiv ID: 2506.21003
- Source URL: https://arxiv.org/abs/2506.21003
- Reference count: 37
- Primary result: Knowledge distillation techniques for normalizing flows achieve up to 35% FID improvement on CelebA while using only 13% of teacher parameters

## Executive Summary
This paper introduces knowledge distillation techniques for normalizing flows, addressing the challenge that these models are difficult to train and often have lower sampling quality than implicit models like GANs. The authors propose three distillation methods—Latent Knowledge Distillation (LKD), Intermediate Latent Knowledge Distillation (ILKD), and Synthesized Knowledge Distillation (SKD)—that leverage the unique property that intermediate layers in normalizing flows contain meaningful probability distributions. Experiments demonstrate that these techniques significantly improve student model performance across tabular datasets (POWER, GAS, HEPMASS, MINIBOONE, BSDS300) and image datasets (CIFAR-10, CelebA), with ILKD reducing CIFAR-10 FID from 71.177 to 69.371 while using only 25% of the teacher's parameters.

## Method Summary
The paper proposes distilling knowledge from a pre-trained teacher normalizing flow to a smaller student flow using three complementary losses. The core idea exploits the fact that intermediate layers in normalizing flows represent valid probability distributions along the transformation path. LKD matches the final latent distribution, ILKD aligns intermediate layer outputs between teacher and student, and SKD distills knowledge via the generative (inverse) path using random latent samples. The student is trained with a combined loss that includes negative log-likelihood on the data plus distillation terms weighted by hyperparameters (typically λ_NLL=0.9, λ_ILKD=0.1 for ILKD). The method works with GLOW and MAF architectures, showing consistent improvements in both density estimation (tabular) and sampling quality (images).

## Key Results
- ILKD achieves 54.480 FID on CelebA vs 68.127 for baseline student (35% improvement) with only 13% of teacher parameters
- CIFAR-10 FID improves from 71.177 to 69.371 using ILKD with 25% of teacher parameters
- SKD yields highest log-likelihood on tabular datasets (GAS: 6.515 vs Teacher 6.604) but exhibits instability
- Consistent improvements across different flow architectures (GLOW and MAF) for ILKD method

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Trajectory Alignment (ILKD)
Normalizing flows are composable diffeomorphic functions where intermediate layers represent valid probability distributions that transform gradually toward the target base distribution. By minimizing L1 loss between intermediate student and teacher activations at specific levels, the student is constrained to mimic the geometric trajectory of the teacher's change-of-variables mapping. This forces the student to learn a compressed but faithful representation of the probability path. The method assumes the student architecture has sufficient capacity to approximate the teacher's trajectory. Evidence shows ILKD student achieves 54.480 FID on CelebA vs 68.127 for baseline student (>35% improvement). Break condition: student model is too shallow relative to teacher, causing gradient instability.

### Mechanism 2: Synthesized Distribution Matching (SKD)
SKD samples from the base distribution and passes these through the inverse of teacher and student, constraining the generated outputs to be similar. This ensures the student covers the generative manifold of the teacher, transferring the "shape" of the learned distribution rather than point-wise density estimates. The method assumes latent spaces are roughly aligned to prevent comparing outputs from disparate regions. Evidence: SKD yields highest log-likelihood on tabular datasets (GAS: 6.515 vs Teacher 6.604) but results in unstable training for MAF architectures. Break condition: training divergence if student's latent space deviates significantly from teacher's before SKD loss is applied.

### Mechanism 3: Capacity Reallocation via Diffeomorphism
Knowledge distillation allows aggressive parameter reduction (shrinking depth) without proportional performance degradation by offloading the "learning burden" of the trajectory to the teacher. Standard NF training must discover both the probability path and transformation parameters simultaneously. With distillation, the student primarily learns to fit trajectory parameters to match the teacher's path. Because the transformation is invertible and smooth (diffeomorphic), a shorter chain of bijectors can approximate a longer chain if intermediate targets are explicitly provided. Evidence: CIFAR student is a quarter the size of teacher yet has <2% difference in sampling quality. Break condition: bijector expressiveness is insufficient to perform required "compressed" transformation.

## Foundational Learning

- **Concept: Change of Variables Formula & Jacobians**
  - Why needed: The premise relies on transforming probability densities (p_x(x) = p_u(u)|det J|^(-1)). Understanding how log-likelihood is computed and why Jacobian determinant matters is essential for distillation losses.
  - Quick check: If a bijector halves the volume of a region in space, what must happen to the probability density in that region?

- **Concept: Invertibility & Bijectors**
  - Why needed: The paper leverages that NFs are fully reversible. SKD explicitly uses the inverse function (f^(-1)). You must distinguish between generative direction (inverse) and density estimation direction (forward).
  - Quick check: In a GLOW model, which direction is typically parallelized during training, and which is sequential in autoregressive flows?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed: You need to understand the standard paradigm (soft targets, hint learning) to see how authors deviate (using intermediate latent states rather than logits or feature maps).
  - Quick check: Why might matching intermediate "hint" features of a network be more effective for NFs than for a standard ResNet classifier?

## Architecture Onboarding

- **Component map:** Teacher Flow (pre-trained, fixed large NF) -> Student Flow (smaller NF) -> Distillation Connector (maps intermediate outputs) -> Loss Aggregator (combines NLL, LKD, ILKD, SKD)

- **Critical path:**
  1. Initialize Student with standard flow setup (ActNorm, Coupling layers)
  2. For data batch x, compute forward pass for Teacher (stores all intermediate z_t^i) and Student (stores z_s^j)
  3. Calculate NLL loss using standard change of variables
  4. Align shapes: If Teacher has 32 steps and Student has 8, align Student step 1 with Teacher step 4, Student 2 with Teacher 8, etc.
  5. Compute L1 loss between aligned pairs (L_ILKD)
  6. Update Student using combined gradient: ∇(λ_NLL L_NLL + λ_ILKD L_ILKD)

- **Design tradeoffs:**
  - ILKD vs SKD: ILKD is stable and works well for image data (CNN-based flows). SKD offers better density estimation on tabular data but is prone to numerical instability, especially with MAF
  - Alignment Frequency: Aligning every layer may over-constrain student; aligning too sparsely loses "trajectory" benefit. Paper suggests aligning at "levels" (splits) rather than every flow step

- **Failure signatures:**
  - NaNs during training: Often caused by SKD instability or exploding gradients in Jacobian calculation. Lower learning rates or gradient clipping required
  - Over-regularization: If λ_ILKD is too high, student perfectly mimics teacher's intermediate states but fails to model data distribution accurately (low likelihood)
  - MAF Collapse: SKD specifically caused training failures with MAF models (Table 1 has no SKD data for MAF)

- **First 3 experiments:**
  1. Sanity Check (Tabular): Train small GLOW on POWER dataset without KD, then with ILKD. Verify log-likelihood improves (e.g., moves from -0.228 toward 0.143)
  2. Ablation on Alignment: On CIFAR-10, test aligning at every step vs. aligning only at split levels. Confirm "level-only" alignment provides best compute-to-performance ratio
  3. Stress Test (SKD): Attempt SKD on GLOW-CIFAR setup. Introduce gradient clipping and monitor for instability mentioned in text to establish "safe" operating region for λ_SKD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the instability of Synthesized Knowledge Distillation (SKD) be mitigated to allow smaller autoregressive flows (like MAF) to benefit from this method without training collapse?
- Basis: Table 1 shows SKD failed to produce results for MAF students, and text notes SKD is "often unstable" due to normalizing flows' difficulty with backward inference
- Why unresolved: Authors identified instability and speculated it stems from poor sampling quality, but offered no solution for stabilizing method on weaker architectures
- Evidence: Successful training runs of MAF student models utilizing SKD that achieve stable convergence and density estimation improvements comparable to GLOW

### Open Question 2
- Question: Can intermediate latent distillation techniques (ILKD) be successfully adapted for diffusion models given their shared properties with normalizing flows?
- Basis: Introduction states "insights gained from this work may inspire research for architectures such as diffusion models [10, 32] which share some, but not all, properties"
- Why unresolved: Current work restricted experiments to GLOW and MAF architectures, leaving applicability to diffusion models as suggestion for future work
- Evidence: Study applying ILKD to student diffusion model, demonstrating improved generation quality or density estimation compared to baseline student

### Open Question 3
- Question: What are theoretically optimal loss functions and hyperparameters for these distillation methods, as opposed to arbitrary or empirically "stable" choices presented?
- Basis: Paper states "We do not seek to find optimal settings or methods of distillation," and regarding loss, "This loss may be arbitrarily, but we use a L1 loss"
- Why unresolved: Authors focused on establishing general framework and proving capacity for distillation rather than optimizing specific loss mechanisms
- Evidence: Ablation study varying λ weights and loss functions (e.g., L2 vs. L1) that identifies configuration yielding statistically significant improvements over reported baselines

## Limitations

- SKD instability: The method can lead to training instability, particularly with MAF architectures, and was not even attempted for MAF models due to anticipated instability
- Architectural generality: While demonstrated on GLOW and MAF flows, the paper does not establish whether these techniques generalize to other normalizing flow architectures like RealNVP or Neural Spline Flows
- Alignment strategy: The paper suggests aligning intermediate layers at "levels" rather than every flow step but does not provide rigorous justification for this choice

## Confidence

- **High Confidence**: Core ILKD method and empirical improvements on both tabular and image datasets are well-established with theoretically sound mechanism
- **Medium Confidence**: Capacity reallocation claims (achieving similar performance with 25% of parameters) are supported by CIFAR-10 results but rely on extrapolating from single dataset
- **Low Confidence**: SKD's effectiveness is questionable due to acknowledged instability, lack of image results, and explicit warnings about numerical issues

## Next Checks

1. **SKD Failure Analysis**: Systematically investigate conditions under which SKD causes training divergence by varying batch size, learning rate, and alignment frequency to identify failure threshold and develop stabilization techniques

2. **Architectural Stress Test**: Apply ILKD to diverse set of flow architectures (RealNVP, NSF, Sylvester flows) on both tabular and image datasets to determine which properties make a flow amenable to knowledge distillation

3. **Alignment Frequency Grid Search**: Conduct comprehensive ablation study on CIFAR-10 and CelebA varying alignment frequency from every flow step to only final layer to quantify tradeoff between computational overhead and performance gains