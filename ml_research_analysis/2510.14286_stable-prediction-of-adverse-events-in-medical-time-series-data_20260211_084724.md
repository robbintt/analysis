---
ver: rpa2
title: Stable Prediction of Adverse Events in Medical Time-Series Data
arxiv_id: '2510.14286'
source_url: https://arxiv.org/abs/2510.14286
tags:
- prediction
- stability
- data
- clinical
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAREBench, a benchmark for evaluating early
  event prediction (EEP) in healthcare using multi-modal data (EHR, waveforms, and
  clinical text). The key innovation is incorporating temporal stability as an evaluation
  metric alongside accuracy, using a local Lipschitz constant to measure short-term
  variability in risk trajectories.
---

# Stable Prediction of Adverse Events in Medical Time-Series Data

## Quick Facts
- arXiv ID: 2510.14286
- Source URL: https://arxiv.org/abs/2510.14286
- Reference count: 40
- Primary result: No single model excels across all metrics; classical models show excellent stability but moderate accuracy, while GPT-2 achieves best accuracy and GPT2AR offers exceptional stability at accuracy cost

## Executive Summary
This paper introduces CAREBench, a benchmark for evaluating early event prediction in healthcare that incorporates temporal stability alongside accuracy. The key innovation is using local Lipschitz constants to quantify short-term variability in risk trajectories, penalizing abrupt oscillations that would undermine clinical trust. Evaluating six clinical tasks across three public datasets, the authors find that classical models demonstrate excellent stability while deep learning models and LLMs achieve better accuracy but with significantly higher instability metrics.

## Method Summary
The method involves defining early event prediction tasks with specific forecasting horizons, constructing cohorts using propensity-based sampling to match time-since-admission distributions, and training three model families: classical models (XGBoost, Random Forest) on 30 aggregated features, deep learning models (Mamba, GPT-2) with custom tokenizers, and zero-shot LLMs (Qwen3-32B) with chain-of-thought prompting. The evaluation combines accuracy metrics (AUROC, AUPRC, F1) with a stability metric based on local Lipschitz constants computed over 10-minute windows, plus flip count to measure threshold crossing frequency.

## Key Results
- Classical models (XGBoost, Random Forest) show near-zero Lipschitz constants (<0.03) but moderate accuracy across tasks
- GPT-2 achieves the best overall AUROC performance but with moderate stability (Lc ~ 0.015)
- GPT2AR autoregressive variant achieves near-zero stability on short horizons but degrades on longer horizons (6-12 hours)
- LLMs (Qwen3-32B) show poor stability with Lc values 10-11x higher than other models, despite good accuracy on text-heavy tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal stability of risk scores can be quantified via local Lipschitz constants, enabling systematic comparison of model behavior beyond per-window accuracy.
- Mechanism: The stability metric computes the average rate of change in risk scores over short time windows (c = 10 minutes), penalizing abrupt oscillations that lack clinical justification. Lower Lc values indicate smoother trajectories; higher values indicate erratic fluctuations.
- Core assumption: Clinically meaningful risk changes should correlate with new evidence, not arbitrary input perturbations or measurement noise.
- Evidence anchors:
  - [abstract] "We propose a stability metric that quantifies short-term variability in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz constants."
  - [section 2.3] Formula: Lc = (1/|Tc|) Σ |f(x≤t) - f(x≤t')| / |t - t'|
  - [corpus] Weak corpus support; neighbor papers focus on prediction accuracy, not stability metrics.
- Break condition: If stability scores conflate legitimate rapid clinical deterioration with noise, the metric may penalize clinically appropriate risk changes.

### Mechanism 2
- Claim: Input representation granularity drives the accuracy-stability tradeoff across model families.
- Mechanism: Classical models operate on derived statistics (30 aggregated features), limiting sensitivity to individual measurements. Deep learning models and LLMs process raw tokens, where each new observation modifies the sequence and can shift predictions, creating higher Lipschitz constants.
- Core assumption: Feature aggregation dampens noise but may discard discriminative signal; token-level processing preserves signal but amplifies noise sensitivity.
- Evidence anchors:
  - [section 5.3] "Classical models exhibit stable predictions, primarily because their inputs consist of derived statistics rather than raw data. Limiting the input to 30 features further enhances stability."
  - [section 5.3] Qwen3-32B stability scores: 10.67 (hyperkalemia), 11.68 (hypoglycemia) vs. <0.03 for other models.
  - [corpus] No direct corpus evidence on stability-accuracy tradeoffs in medical time-series.
- Break condition: If task-relevant signal requires fine-grained temporal patterns, aggressive aggregation will harm accuracy without improving meaningful stability.

### Mechanism 3
- Claim: Autoregressive prediction enforces implicit smoothing by constraining risk estimates to evolve through predicted intermediate states.
- Mechanism: GPT2AR iteratively predicts x[N+1:N+h] before classification, creating a smoothing prior where risk scores must traverse plausible intermediate futures rather than jumping directly.
- Core assumption: Intermediate predictions act as regularization, reducing sensitivity to small input perturbations.
- Evidence anchors:
  - [section 4] "The auto-regressive approach aggregates logits from each unrolled prediction step."
  - [section 5.3] "GPT2AR achieves markedly better stability, with near-zero Lipschitz constants for hyperkalemia and hypoglycemia tasks."
  - [corpus] No corpus evidence on autoregressive smoothing in clinical prediction.
- Break condition: If the forecasting horizon exceeds model capacity (6-12 hours in MIMIC-IV tasks), autoregressive error accumulation degrades accuracy without stability gains.

## Foundational Learning

- Concept: **Early Event Prediction (EEP) vs. Time-Series Forecasting**
  - Why needed here: EEP predicts *whether* an event occurs within horizon h (binary classification), not the future value of a signal. This distinction shapes label construction and evaluation.
  - Quick check question: Given patient data up to time T, should the model output a continuous heart rate forecast or a probability of sepsis onset in the next 90 minutes?

- Concept: **Local Lipschitz Constant**
  - Why needed here: This metric quantifies how much a function's output changes relative to small input changes, applied here to risk trajectories over short time windows.
  - Quick check question: If a model's risk score jumps from 0.3 to 0.8 in 5 minutes without new clinical evidence, would the local Lipschitz constant be low or high?

- Concept: **Multi-Modal Temporal Alignment**
  - Why needed here: EHR (sparse labs), waveforms (continuous), and text (irregular notes) arrive at different cadences with varying missingness patterns.
  - Quick check question: Should a model treat a lab result timestamped 2 hours ago differently from an ECG reading from 30 seconds ago?

## Architecture Onboarding

- Component map: MC-MED (EHR + waveforms) -> MIMIC-IV (EHR + text) -> EHRShot (EHR only) -> Preprocessing (propensity sampling) -> Model families (Classical, Sequence, LLM) -> Evaluation (Accuracy + Stability)
- Critical path:
  1. Task definition: Select event E, horizon h, context window a
  2. Cohort construction: Filter patients, apply propensity sampling
  3. Input encoding: Tokenize modality-specific data (custom tokenizers for EHR codes)
  4. Model training: Pretrain (next-token prediction) -> Fine-tune (classification)
  5. Stability evaluation: Compute Lc over [T-b, T+b] with local pairing window c=10min
- Design tradeoffs:
  - Accuracy vs. stability: GPT-2 achieves best AUROC but moderate stability; GPT2AR achieves near-zero Lc but lower accuracy on long horizons
  - Modality coverage: EHR-only (classical models) vs. multi-modal (deep learning) vs. text-heavy (LLMs excel on MIMIC-IV mortality with radiology reports)
  - Computational cost: Autoregressive unrolling (GPT2AR) requires h forward passes per prediction
- Failure signatures:
  - High flip count (>2 per patient): Model produces oscillating alerts unsuitable for clinical workflows
  - Lc > 1.0: Risk scores change faster than clinically plausible given measurement frequency
  - Strong AUROC with poor AUPRC on rare events: Model may be miscalibrated at high-precision operating points
- First 3 experiments:
  1. Reproduce stability metric on held-out fold: Compute Lc for XGBoost vs. GPT-2 on sepsis task to verify reported gap (0.0494 vs. 0.0150)
  2. Ablate feature aggregation: Train XGBoost with 30 vs. 100 features to quantify stability-accuracy tradeoff
  3. Test autoregressive horizon sensitivity: Evaluate GPT2AR at h=1.5h vs. h=6h to observe accuracy degradation on longer horizons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be effectively adapted to structured tabular and waveform data for clinical EEP tasks?
- Basis in paper: [explicit] Section 5.2 states: "These findings suggest that further research is needed to effectively leverage LLMs for structured tabular and waveform data."
- Why unresolved: Zero-shot LLMs performed well only on text-inclusive tasks (ICU transfer, mortality) but poorly on purely structured data tasks like hyperkalemia and hypoglycemia.
- What evidence would resolve it: Demonstrating supervised fine-tuning or specialized prompting techniques that achieve competitive AUROC/AUPRC on EHR-only tasks comparable to classical models.

### Open Question 2
- Question: How can the stability metric be refined to account for clinically justified risk changes versus erratic oscillations?
- Basis in paper: [explicit] Section 6 states: "The current metric does not account for underlying changes in a patient's condition, which can result in substantial increases or decreases in actual risk."
- Why unresolved: The local Lipschitz constant penalizes all variability equally, even when rapid risk changes may be clinically appropriate.
- What evidence would resolve it: A refined metric that correlates with clinician judgments of trajectory plausibility and normalizes for patient condition severity.

### Open Question 3
- Question: Can a single model architecture jointly optimize both accuracy and temporal stability across diverse clinical EEP tasks?
- Basis in paper: [explicit] Abstract: "Across tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy and stability."
- Why unresolved: Classical models show excellent stability but moderate accuracy; GPT-2 achieves best accuracy; GPT2AR offers stability at accuracy cost. No model excels at both.
- What evidence would resolve it: A model achieving top-quartile AUROC and top-quartile stability simultaneously across all six CAREBench tasks.

### Open Question 4
- Question: Do models trained and evaluated on CAREBench generalize to new hospital systems under distribution shift?
- Basis in paper: [explicit] Section 6 states: "A clinically useful future direction is to conduct a multi-center study to evaluate how models generalize under distribution shifts across hospital sites."
- Why unresolved: All three datasets (MC-MED, MIMIC-IV, EHRShot) represent single institutions; cross-site robustness remains untested.
- What evidence would resolve it: External validation showing consistent AUROC and stability metrics when models are applied to an independent hospital system not represented in training data.

## Limitations

- The stability metric implementation requires exact matching of the propensity-based sampling algorithm, which is not fully specified in the paper
- Zero-shot LLM evaluation depends heavily on prompt engineering details that are only partially provided, with apparent inconsistencies in decoding parameters
- The autoregressive GPT2AR variant's training procedure, particularly how logits are aggregated across unrolled steps, needs clearer documentation

## Confidence

- **High confidence**: The existence of the stability-accuracy tradeoff across model families, supported by multiple model comparisons showing consistent patterns
- **Medium confidence**: The specific magnitude of stability differences between models, given implementation dependencies on sampling and tokenization
- **Medium confidence**: The autoregressive smoothing mechanism's effectiveness, limited by unclear training details

## Next Checks

1. Implement and validate the exact propensity-based sampling algorithm to ensure comparable stability scores across model evaluations
2. Replicate the stability metric computation on a held-out validation set to verify the reported gap between classical models (Lc ~ 0.05) and LLMs (Lc ~ 10-11)
3. Test GPT2AR performance at varying autoregressive horizons to characterize accuracy degradation beyond 3-4 hours