---
ver: rpa2
title: 'Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation'
arxiv_id: '2510.15304'
source_url: https://arxiv.org/abs/2510.15304
tags:
- layer
- layers
- pruning
- come
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoMe compresses large language models through a novel concatenation-based
  layer merging approach combined with hierarchical distillation. It introduces a
  channel sensitivity metric to identify critical parameters and progressively merges
  adjacent layers while preserving essential information.
---

# Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation

## Quick Facts
- arXiv ID: 2510.15304
- Source URL: https://arxiv.org/abs/2510.15304
- Authors: Fei Wang; Li Shen; Liang Ding; Chao Xue; Ye Liu; Changxing Ding
- Reference count: 40
- Key outcome: CoMe achieves 83% accuracy retention when pruning 30% of LLaMA-2-7b's parameters, outperforming existing methods by 2-4% in accuracy and 4.7+ in perplexity

## Executive Summary
CoMe introduces a novel approach to LLM compression through concatenation-based layer merging combined with hierarchical distillation. Unlike traditional weight averaging methods that cause over-smoothing, CoMe identifies and preserves the most critical channels from adjacent layers while discarding less important ones. The method uses a channel sensitivity metric that considers both weight magnitudes and activation intensities to make pruning decisions, then recovers performance through knowledge distillation. CoMe achieves state-of-the-art compression performance across multiple models while maintaining better perplexity scores than competing approaches.

## Method Summary
CoMe operates through a four-stage process: (1) compute channel sensitivity using activation intensity and weight norms on calibration data, (2) identify low-importance layer groups via SBI scores, (3) concatenate top-k channels from adjacent layers based on heuristic retention ratios, and (4) apply hierarchical KL distillation to recover performance. The method targets 10-30% sparsity by merging 2 layers at a time using a skewness exponent to determine channel preservation ratios. Post-training distillation can be performed either in single-process (CoMe-sp) or multi-process (CoMe-mp) modes, with CoMe-sp offering better performance through global optimization.

## Key Results
- Retains 83% of original accuracy when pruning 30% of LLaMA-2-7b's parameters
- Outperforms existing methods by 2-4% in accuracy across 7 benchmarks
- Achieves 4.7+ improvement in perplexity compared to state-of-the-art pruning approaches
- Demonstrates consistent performance across LLaMA-2-7b/13b, LLaMA-3-8b, Vicuna-7b, Mistral-7b, Qwen-2.5-7b, and Qwen-3-4b

## Why This Works (Mechanism)

### Mechanism 1
Concatenation-based merging preserves model capacity better than linear weight aggregation for LLM compression. Instead of averaging weights (which causes "over-smoothing" in Feed-Forward Networks), CoMe selects the most critical channels from adjacent layers based on a sensitivity metric and concatenates them into a single new layer. This retains the specific mapping functions of the pruned sections.

### Mechanism 2
Channel sensitivity metrics effectively decouple parameter importance from input distribution variance. The metric estimates the expected L1-norm of output perturbation when a specific channel is pruned, weighing both weight magnitude and activation intensity observed on a calibration dataset.

### Mechanism 3
Hierarchical distillation recovers performance by exploiting the layer correspondences established during the pruning process. CoMe-sp optimizes all merged layers simultaneously using KL divergence against the corresponding layer groups in the original model, aligning feature representations globally rather than sequentially.

## Foundational Learning

- **Concept: Structured Pruning vs. Weight Aggregation**
  - Why needed here: The paper critiques two existing paradigms: Direct Layer Pruning (DLP) which loses info, and Weight Sum-based Layer Pruning (WSLP) which blurs info. You must understand this dichotomy to grasp why "concatenation" is proposed as a middle path.
  - Quick check question: Why does averaging weights (WSLP) fail specifically for Feed-Forward Networks (FFN) according to the text? (Answer: It leads to over-smoothing because FFN weights lack explicit feature correspondence across layers.)

- **Concept: Channel-wise Sensitivity**
  - Why needed here: The core technical contribution is selecting *which* pieces of the puzzle to keep. This requires understanding how to quantify "importance" at a sub-layer granularity.
  - Quick check question: How does the channel sensitivity metric in Equation 1 utilize the calibration dataset D?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: Pruning is only the first half; recovery is the second. The paper relies on feature-based distillation to realign the pruned model.
  - Quick check question: What is the difference between CoMe-mp and CoMe-sp in terms of optimization scope?

## Architecture Onboarding

- **Component map:** Calibration Engine -> Pruning Controller -> Merge Operator -> Distillation Trainer
- **Critical path:** The Concatenation-based Merge logic. This is where the dimensionality reduction happens without averaging weights. Ensuring the column-wise concatenation W^(merge) aligns correctly with the new reduced input/output dimensions is the highest implementation risk.
- **Design tradeoffs:**
  - Merge Granularity (m): Merging 2 layers at a time is stable; merging >2 degrades performance significantly
  - Distillation Strategy: CoMe-sp (single-process) offers better performance/faster convergence but requires ~|P| times more storage for optimizer states than CoMe-mp (multi-process)
  - Calibration: Larger calibration sets (>128 samples) stabilize selection but increase initial overhead
- **Failure signatures:**
  - Perplexity Explosion: If merging >2 layers or if calibration data is too small/irrelevant
  - Recovery Stagnation: If using CoMe-mp, convergence is slower; may look like a performance ceiling compared to CoMe-sp
- **First 3 experiments:**
  1. Sanity Check (Merge vs. Avg): Replicate Fig. 3/Fig. 11. Merge two adjacent layers on a small model using averaging vs. CoMe concatenation and compare PPL
  2. Sensitivity Stability: Ablate the calibration sample count (Fig. 10) to determine the minimal data required for your specific model domain
  3. Distillation Comparison: Compare CoMe-mp vs. CoMe-sp recovery on a 10% pruned model to measure the memory/performance trade-off for your hardware

## Open Questions the Paper Calls Out

- Can a non-heuristic, adaptive parameter preservation ratio improve performance across diverse Transformer architectures? The current method relies on a fixed skewness exponent p to determine how many channels to keep from each layer, which may not be optimal for all model types.

- How can the progressive merging strategy be generalized to fuse more than two layers simultaneously without the observed performance degradation? The current concatenation approach is iterative (merging 2 layers at a time), which is robust but slow.

- Can the concatenation-based merging approach be effectively extended to Mixture-of-Experts (MoE) models? MoE models possess sparse expert structures (FFNs) and routing mechanisms where the importance of a channel may be conditional on the router state.

## Limitations

- The method's effectiveness depends heavily on using representative calibration data, but no systematic ablation shows performance degradation with out-of-distribution data
- The approach targets transformer-based LLMs with specific FFN/MHA layer structures, with performance on other architectures remaining untested
- While the paper shows CoMe outperforms baselines, it lacks ablations for key hyperparameters like the retention ratio œÅ range or sensitivity metric alternatives

## Confidence

- **High confidence**: The core mechanism of concatenation-based merging preserving more information than weight averaging is well-supported by perplexity comparisons
- **Medium confidence**: The channel sensitivity metric's effectiveness is supported by sample size ablation but lacks validation across diverse data distributions
- **Medium confidence**: Hierarchical distillation recovery shows consistent improvements over baseline pruning methods

## Next Checks

1. Test CoMe's performance when using calibration data from different domains (e.g., MMLU for pre-training compression) to quantify sensitivity to data distribution mismatch
2. Apply CoMe to non-standard transformer variants (e.g., Mamba, RWKV) or smaller models to test whether concatenation-based merging provides similar benefits
3. Compare the proposed L1-norm-based sensitivity metric against alternative importance measures (e.g., Taylor expansion, path-based attribution) to validate whether the specific formulation is critical to CoMe's success