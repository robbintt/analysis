---
ver: rpa2
title: 'From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit
  Generation'
arxiv_id: '2512.22031'
source_url: https://arxiv.org/abs/2512.22031
tags:
- hit-like
- gsk-3
- generative
- molecules
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether generative models can directly
  produce hit-like molecules suitable for early-stage drug discovery, bypassing traditional
  high-throughput screening. Three graph-based models (MolRNN, GraphINVENT, DiGress)
  were trained and evaluated using a novel framework combining physicochemical filtering,
  distributional similarity, and target-specific docking.
---

# From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation

## Quick Facts
- arXiv ID: 2512.22031
- Source URL: https://arxiv.org/abs/2512.22031
- Reference count: 28
- Key outcome: Generative models trained on hit-like data can produce biologically relevant molecules, with autoregressive models outperforming diffusion-based approaches in constrained chemical spaces.

## Executive Summary
This study investigates whether generative models can directly produce hit-like molecules suitable for early-stage drug discovery, bypassing traditional high-throughput screening. Three graph-based models (MolRNN, GraphINVENT, DiGress) were trained and evaluated using a novel framework combining physicochemical filtering, distributional similarity, and target-specific docking. Models trained or fine-tuned on hit-like data generated chemically valid, diverse molecules that passed stringent drug-likeness filters and achieved docking scores comparable to or better than known ligands across seven targets. Prospective in vitro testing confirmed GSK-3β activity for a novel hit (IC₅₀ = 314 nM), validating the biological relevance of generated compounds. Autoregressive models outperformed diffusion-based approaches in target-specific fine-tuning, and evaluation metrics like VUN, FCD, and scaffold similarity showed limited correlation with predicted bioactivity. Key limitations include data scarcity for hit-like chemical space and poor adaptation of diffusion models to low-data regimes. The work highlights the potential of generative models to accelerate hit identification while underscoring the need for better datasets and bioactivity-aware evaluation frameworks.

## Method Summary
The study compares three graph-based generative models (MolRNN, GraphINVENT, DiGress) trained under three regimes: REINVENT dataset (general ChEMBL), hit-like dataset (58,837 molecules filtered for drug-likeness), and REINVENT→hit-like fine-tuning. Models generate molecules evaluated through a three-layer framework: validity/uniqueness/novelty metrics (VUN), distributional similarity metrics (FCD, BRICS, scaffolds, SNN, diversity), and hit-like filter compliance plus target-specific docking scores. Seven protein targets were used for docking validation (D3R, ADORA2A, HSP90α, GSK-3β, SRC, Thrombin, PPARα), with prospective in vitro testing of top candidates for GSK-3β.

## Key Results
- Autoregressive models (MolRNN, GraphINVENT) achieved >99.9% validity and up to 76.9% hit-like filter compliance when trained directly on hit-like data
- Direct hit-like training produced higher filter compliance (MolRNN: 76.87% vs 11.46% from REINVENT) than fine-tuning approaches
- DiGress (diffusion model) lagged with 70-76% validity and poorer filter adaptation, particularly in low-data regimes
- MolRNN best matched ligand docking score distributions (KL divergence 0.061±0.062), with >30% of generated molecules beating ligand medians for 6/7 targets
- Prospective in vitro testing confirmed GSK-3β activity for a novel hit (IC₅₀ = 314 nM)

## Why This Works (Mechanism)

### Mechanism 1
Training on filtered hit-like datasets produces higher filter compliance than training on general-purpose datasets or fine-tuning from them. The hit-like dataset (2.68% of ChEMBL, 58,837 molecules) constrains the learned chemical space to structures satisfying MW (150–350 Da), logP (1–3), synthetic accessibility (SAS ≤5), and ring/element constraints. Models learn this distribution directly rather than attempting to approximate it from a broader prior. Core assumption: The filtered hit-like dataset sufficiently represents the target chemical space; no critical substructures are excluded by filtering. Evidence anchors: [abstract] "MolRNN and GraphINVENT achieved high validity (>99.9%) and hit-like compliance (up to 76.9%)"; [Table 1] MolRNN (Hit-like): 76.87% filter compliance vs. MolRNN (REINVENT): 11.46%. Break condition: If hit-like filters exclude chemotypes that are actually viable for specific targets, direct training will underrepresent those regions.

### Mechanism 2
Autoregressive models outperform diffusion-based models in validity and hit-like filter compliance under constrained chemical spaces. Autoregressive models (MolRNN, GraphINVENT) enforce chemical validity at each generation step via valence rules and chemically-guided action probabilities. Diffusion models (DiGress) apply global denoising without step-wise validity constraints, making them more sensitive to distributional sparsity and strict post-hoc filters. Core assumption: The superiority is attributable to step-wise validity enforcement rather than architectural differences in encoder capacity or training dynamics. Evidence anchors: [abstract] "DiGress lagged due to lower validity (70–76%) and poorer filter adaptation"; [Section 3.1] "We attribute this primarily to the challenges of discrete diffusion in low-data, highly constrained regimes... diffusion-based approaches rely on global denoising and appear more sensitive to distributional sparsity". Break condition: If diffusion models incorporate validity-constrained denoising or guidance mechanisms, this gap may close.

### Mechanism 3
Distributional alignment (KL divergence) between generated molecule docking scores and known ligand docking scores predicts whether models generate biologically relevant candidates. Models trained directly on hit-like data produce docking score distributions closely matching target-specific ligand distributions (KL <0.01). Fine-tuned or REINVENT-trained models show higher divergence (0.08–0.10), reflecting broader chemical priors. Core assumption: Docking score distributions of known ligands are a meaningful proxy for bioactivity potential; low KL divergence implies functional relevance. Evidence anchors: [abstract] "Docking analysis showed MolRNN best matched ligand score distributions (KL divergence 0.061±0.062)"; [Table 3a] Hit-like training yields KL divergence <0.01 for all three models. Break condition: If docking scores poorly correlate with actual binding affinity (a known limitation), KL divergence becomes a proxy for chemical similarity rather than bioactivity.

## Foundational Learning

- **Graph-based molecular representations**: Understanding nodes=atoms, edges=bonds, features=atom/bond types is prerequisite to interpreting how all three models operate on molecular graphs through message-passing and graph transformers. Quick check: Can you explain how a GRU-based RNN would sequentially build a molecular graph vs. how a diffusion model would denoise one?

- **Autoregressive vs. diffusion-based generation**: The paper's central comparison hinges on step-wise sequential generation (MolRNN, GraphINVENT) vs. one-shot denoising (DiGress); this determines validity enforcement and data efficiency. Quick check: Which paradigm would you expect to be more sample-efficient under a narrow, highly-constrained distribution?

- **Hit-like chemical space and drug-likeness filters**: The filtering pipeline (MW, logP, SAS, severity score, ring constraints) defines the target distribution; understanding these constraints is essential for interpreting filter compliance results. Quick check: Why might a model trained on general ChEMBL data produce only ~11% hit-like compliant molecules?

## Architecture Onboarding

- **Component map**: MolRNN: GNN layers → GRU for graph-level state → MLP decoder → sequential atom/bond addition actions; GraphINVENT: MPNN encoder → Readout block → Action Probability Distribution (APD) logits → autoregressive generation; DiGress: Graph transformer → discrete Markov noise transitions → denoising network with structural/spectral features (cycle counts, Laplacian eigenvalues); Shared: All use maximum likelihood training objectives; MolRNN and GraphINVENT use BFS-based preprocessing for action sequences.

- **Critical path**: 1. Dataset construction: Filter ChEMBL for hit-like criteria → 58,837 molecules; 2. Training: REINVENT pretraining → hit-like fine-tuning (or direct hit-like training); 3. Generation: Sample until target count of VUN + hit-like filtered molecules reached; 4. Evaluation: VUN metrics → distributional metrics → hit-like filters → docking → visual inspection → synthesis.

- **Design tradeoffs**: Validity vs. diversity: Autoregressive models enforce validity but may reduce novelty (MolRNN novelty dropped to 64.6% on hit-like data); Data efficiency vs. generative breadth: Direct hit-like training is data-efficient but may limit extrapolation; REINVENT pretraining provides broader priors but requires fine-tuning; Metric alignment: FCD and scaffold similarity don't correlate with bioactivity (explicitly noted in conclusion).

- **Failure signatures**: Low validity + high MW/ring violations → diffusion model struggling with constrained space (DiGress pattern); High validity + low novelty → overfitting to small hit-like dataset (MolRNN pattern on hit-like); Low docking score improvement over ligand median → training data mismatch (PPARα, SRC showed <1% improvement due to scarce hit-like ligands).

- **First 3 experiments**: 1. Replicate VUN + filter compliance comparison using the three models on a held-out subset of the hit-like dataset to validate reported validity and compliance gaps; 2. Ablate training regime: Compare direct hit-like training vs. REINVENT→hit-like fine-tuning for each model on a single target (e.g., GSK-3β) to isolate fine-tuning benefit; 3. Validate docking-bioactivity correlation: Dock a sample of generated molecules and known ligands for one target, then compare docking score rankings against literature IC50 values to assess whether KL divergence predicts actual potency.

## Open Questions the Paper Calls Out

- **Can the development of richer, explicitly curated hit-like datasets resolve the performance degradation observed in targets with scarce ligand data, such as PPAR$\alpha$ and SRC?** The authors explicitly state that "performance on targets such as PPAR$\alpha$ and SRC was constrained by the scarce high-quality hit-like data" and that "future progress will require richer, better-curated hit-like datasets." This remains unresolved because current public databases lack sufficient molecules that satisfy both the strict hit-like physicochemical filters and the specific activity profiles required for these difficult targets. Benchmarking current models against newly compiled, high-density datasets for PPAR$\alpha$/SRC would resolve this by observing if KL divergence and docking success rates improve significantly.

- **What evaluation frameworks can effectively correlate standard distributional metrics with actual bioactivity?** The study concludes that "widely used evaluation metrics such as VUN, FCD, and scaffold similarity did not consistently correlate with predicted bioactivity," highlighting a disconnect between chemical validity and biological function. A molecule can score well on structural validity and novelty (high VUN/FCD) yet fail to bind to the target, making current benchmarks insufficient for predicting experimental success. A new benchmark where high scores on a proposed metric statistically predict low IC50 values or high docking scores across a diverse panel of protein targets would resolve this question.

- **Can diffusion-based architectures be modified to handle the strict constraints of hit generation more effectively than the tested DiGress model?** The authors note that DiGress "lagged" and struggled with fine-tuning due to "higher data demands and sensitivity to dataset size" compared to autoregressive models. The paper suggests that the one-shot, global denoising approach of diffusion models may be fundamentally misaligned with the narrow, highly constrained chemical space of hit-like molecules without additional architectural constraints. A study introducing chemical validity constraints or hybrid autoregressive-diffusion mechanisms that achieve validity rates >99% and hit-like compliance comparable to MolRNN would resolve this question.

## Limitations

- The study's findings are limited by the small size of hit-like datasets (58,837 molecules), raising concerns about overfitting and generalizability, particularly for rare targets with limited ligand availability.
- The assumption that docking score distributions correlate with bioactivity remains unproven, as the study relies on docking as a proxy for binding affinity without extensive experimental validation across multiple targets.
- The hit-like chemical space definition may exclude viable chemotypes for specific targets, potentially biasing models toward narrow chemical scaffolds and limiting their applicability to diverse drug discovery scenarios.

## Confidence

- **High Confidence**: The superiority of autoregressive models over diffusion-based approaches in validity and filter compliance (based on direct comparisons with clear numerical gaps: 99.9% vs 70-76% validity)
- **Medium Confidence**: The distributional alignment mechanism predicting biological relevance (supported by docking score distributions but weakened by known limitations of docking as a bioactivity predictor)
- **Low Confidence**: The extrapolation of fine-tuning benefits from limited target-specific data to broader chemical space (only 7 targets tested with varying ligand availability)

## Next Checks

1. **Target Expansion Validation**: Test the same model comparison framework on 3-5 additional targets with varying ligand availability to assess whether the autoregressive advantage persists across different chemical spaces and whether fine-tuning benefits scale with dataset size.

2. **Docking-to-Bioactivity Correlation**: For at least two targets, compare generated molecule docking scores against actual IC50 measurements from in vitro testing to validate whether KL divergence in docking distributions correlates with true bioactivity.

3. **Chemotype Coverage Analysis**: Systematically analyze whether the hit-like filtering criteria exclude known active chemotypes by comparing the chemical space coverage of hit-like vs. non-hit-like compounds across all 7 tested targets.