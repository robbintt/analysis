---
ver: rpa2
title: Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice
  QA
arxiv_id: '2509.25941'
source_url: https://arxiv.org/abs/2509.25941
tags:
- reasoning
- cots
- answer
- correct
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to explicitly model the solvability of multiple-choice
  questions to improve chain-of-thought reasoning in large language models. The key
  insight is that unsolvable questions often produce spurious chains of thought, leading
  to false positives, and there is an intermediate regime where learning is most effective.
---

# Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA

## Quick Facts
- arXiv ID: 2509.25941
- Source URL: https://arxiv.org/abs/2509.25941
- Reference count: 40
- Primary result: Explicitly modeling solvability improves process-correct chain-of-thought reasoning by down-weighting CoTs from unsolvable questions

## Executive Summary
This paper proposes modeling question solvability to improve chain-of-thought reasoning in large language models for multiple-choice QA. The key insight is that unsolvable questions often produce spurious reasoning chains that yield correct answers by accident (false positives). By estimating solvability using Bayesian inference on sampled CoTs, the authors modify both outcome-supervised reward models and reinforcement learning to down-weight contributions from unsolvable questions. Experiments show these modifications consistently yield higher rates of process-correct reasoning and, in RL, improved answer accuracy. The results highlight solvability as a key factor for reducing hallucinations and increasing reliability in CoT reasoning.

## Method Summary
The method introduces solvability estimation via Bayesian inference on sampled CoTs per question. For each question, G CoTs are sampled and the observed success rate is computed. Using a Beta(1,1) prior, a posterior Beta distribution is derived, and solvability probability is calculated as the survival function above the random guessing baseline. This probability is then used to modify two training approaches: (1) for outcome-supervised reward models, labels are softened by solvability probability, down-weighting likely false-positive CoTs; (2) for reinforcement learning with group-relative advantage, advantages are scaled by solvability, focusing learning on questions with optimal "learning potential" (product of novelty and solvability). The approach is evaluated on math and multimodal datasets.

## Key Results
- MCQ-ORM consistently improves process accuracy across AQuA, MATH, and GSM8K (e.g., 70.0 vs 67.3 on AQuA with Llama3 1B)
- MCQ-DrGRPO improves both process accuracy (65.0 vs 63.7 on AQuA) and answer accuracy (59.5 vs 58.9)
- Learning potential peaks at intermediate solvability, confirmed by controlled finetuning experiment showing accuracy improvement peaks at intermediate #answer-correct CoTs
- Performance improves with more samples (G), with clearer solvable/unsolvable separation at higher G

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Estimating question solvability via Bayesian inference identifies CoTs that are likely false positives (answer-correct but process-incorrect).
- **Mechanism:** Sample G CoTs per question and compute the observed success rate μ_observed. Using a Beta(1,1) prior, derive the posterior Beta(α, β). The solvability probability p_solvable is the survival function above the random-guessing baseline μ_random = 1/|c|. When few samples are correct, p_solvable → 0, indicating the model is guessing rather than reasoning.
- **Core assumption:** The binary outcome of answering correctly follows a Bernoulli distribution with an unobservable true rate that can be estimated from repeated sampling.
- **Evidence anchors:**
  - [abstract] "By estimating the solvability of each question, we uncover an intermediate regime where learning is most effective."
  - [Section 2, Figure 1] "The p_solvable line closely follows the empirical data in the bar chart, showing it is a good predictor of whether the model is able to generate a correct thought process."
  - [corpus] Related work on consistency evaluation (CoRA metric, arxiv 2511.21860) similarly exploits multiple samples per question to improve reliability, though not via Bayesian solvability.

### Mechanism 2
- **Claim:** Softening ORM training labels by solvability probability down-weights likely false-positive CoTs during reward model training.
- **Mechanism:** Replace binary labels z_ij ∈ {0,1} with z_ij = p_solvable(q_i) if answer-correct, else 0. CoTs from low-solvability questions receive lower target scores, teaching the ORM to rank them below CoTs from genuinely solvable questions.
- **Core assumption:** Answer-correct CoTs from low-solvability questions are more likely process-incorrect; this correlation holds across domains.
- **Evidence anchors:**
  - [Section 4, Equation 12] "This gives lower weight to CoTs that are likely false positive and therefore should receive a lower score when ranking."
  - [Table 1] MCQ-ORM consistently outperforms baseline ORM on process accuracy across AQuA, MATH, and GSM8K (e.g., 70.0 vs 67.3 on AQuA with Llama3 1B).
  - [corpus] The "Rewarding Curse" paper (arxiv 2503.05188) analyzes reward modeling issues for CoT reasoning but does not incorporate solvability.

### Mechanism 3
- **Claim:** Scaling RL advantages by solvability focuses learning signal on questions with high "learning potential" — the product of novelty and solvability.
- **Mechanism:** Define p_novel = fraction of incorrect answers. Learning potential LP = p_novel × p_solvable. The modified advantage A_MCQ-DrGRPO = p_solvable × A_DrGRPO. This down-weights advantages for unsolvable questions (where the model is guessing) and over-solvable questions (where there's little to learn).
- **Core assumption:** Learning follows an inverted-U curve: neither trivially easy nor impossibly hard questions provide useful gradient signal.
- **Evidence anchors:**
  - [Section 5.1, Figure 3] Controlled finetuning experiment shows accuracy improvement peaks at intermediate #answer-correct CoTs; the LP curve matches this distribution.
  - [Table 2] MCQ-DrGRPO improves both process accuracy (65.0 vs 63.7 on AQuA) and answer accuracy (59.5 vs 58.9).
  - [corpus] GRPO-lead (arxiv 2504.09696) re-weights by difficulty alone, without the novelty-solvability trade-off.

## Foundational Learning

- **Concept: Bayesian posterior with conjugate priors**
  - Why needed here: The solvability estimator uses a Beta-Bernoulli model; understanding how prior + likelihood → posterior is essential.
  - Quick check question: Given 4 correct out of 16 samples with Beta(1,1) prior, what are the posterior α and β parameters?

- **Concept: Policy gradient advantage estimation**
  - Why needed here: The paper modifies DrGRPO advantages; you need to understand baseline subtraction and why relative advantages matter.
  - Quick check question: Why does subtracting the group mean reward stabilize policy gradient variance?

- **Concept: Multiple-choice QA evaluation metrics**
  - Why needed here: The paper distinguishes answer accuracy (A-Acc) from process accuracy (P-Acc); knowing when each applies prevents misinterpretation.
  - Quick check question: If a CoT gets the right answer via incorrect reasoning, is it a false positive for A-Acc or P-Acc?

## Architecture Onboarding

- **Component map:** Solvability Estimator -> MCQ-ORM / MCQ-DrGRPO -> Evaluation (A-Acc, P-Acc)
- **Critical path:**
  1. Sample G CoTs per question with temperature 1.0
  2. Compute p_solvable for each question (requires knowing |c| choices)
  3. For ORM: train with softened labels; for RL: scale advantages before policy update
  4. Evaluate on held-out sets using both A-Acc and P-Acc (latter requires LLM judge)

- **Design tradeoffs:**
  - Larger G → more reliable p_solvable but higher compute; Figure 1 shows G=32 gives clear solvable/unsolvable separation
  - LLM judge for P-Acc introduces its own error; Appendix D shows 97% agreement with humans but some false negatives
  - MCQ-ORM improves P-Acc but Table 8 shows slight A-Acc regression; may need separate reward models for each metric

- **Failure signatures:**
  - All questions classified as unsolvable → advantages collapse to zero → no learning
  - Judge is too strict → P-Acc floor effects → hard to measure improvement
  - Answer choices are poorly constructed (e.g., symmetric distractors) → model exploits bias, solvability estimate becomes meaningless

- **First 3 experiments:**
  1. Reproduce Figure 1 on your target dataset: sample 32 CoTs per question, plot p_solvable vs #answer-correct CoTs, verify the curve shape matches.
  2. Ablate G: train MCQ-DrGRPO with G ∈ {4, 8, 16, 32} and plot both A-Acc and P-Acc; expect widening gap at higher G (Figure 6).
  3. Test out-of-domain generalization: train on one dataset (e.g., AQuA), evaluate on another (e.g., MATH); compare MCQ-DrGRPO vs DrGRPO on P-Acc drop.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the solvability estimation framework be adapted for open-ended generation tasks where the random guessing baseline ($\mu_{random}$) is undefined?
- **Basis in paper:** [Explicit] The paper states it focuses on MCQA "to avoid confounding factors of noisy answer extraction and matching," noting that solvability (Eq. 5) relies on the number of answer choices $|c_i|$.
- **Why unresolved:** The mathematical definition of solvability relies on a fixed denominator ($1/|c_i|$) to calculate the survival function of the Beta distribution, which does not exist in open-ended formats.
- **What evidence would resolve it:** A reformulation of the solvability metric using semantic entropy or density-based clustering for open-ended responses, demonstrating improved process correctness on datasets like GSM8K without multiple-choice options.

### Open Question 2
- **Question:** Does the reduced answer variance observed in MCQ-DrGRPO limit the model's ability to discover novel or distinct reasoning paths compared to standard DrGRPO?
- **Basis in paper:** [Inferred] The analysis notes that DrGRPO achieves higher Answer-Pass@4 scores than MCQ-DrGRPO, indicating that "the variance of answers is higher using DrGRPO," while MCQ-DrGRPO prioritizes reliable signal over "diverse but potentially noisy signal."
- **Why unresolved:** While the paper shows MCQ-DrGRPO improves accuracy and process-correctness, it does not analyze if the down-weighting of "unsolvable" questions restricts the exploration of diverse strategies that might eventually become valid.
- **What evidence would resolve it:** A diversity analysis (e.g., measuring distinct solution clusters or semantic entropy) of the generated CoTs for MCQ-DrGRPO versus the baseline over the course of training.

### Open Question 3
- **Question:** To what extent are the reported improvements in process-correctness contingent upon the specific biases or error modes of the GPT-4 judge used for evaluation?
- **Basis in paper:** [Explicit] The authors acknowledge that using LLMs as judges "has to be handled with care" and report a 3% disagreement rate with human annotators in their meta-evaluation.
- **Why unresolved:** The paper relies on a single judge architecture for both determining the ground truth for the "sweet spot" analysis and evaluating the final process accuracy, creating a risk of circularity or bias amplification.
- **What evidence would resolve it:** A robustness check where the RL models are evaluated by a panel of human experts or an ensemble of structurally different LLM judges to confirm the increase in process correctness.

## Limitations

- **Sample size dependency:** The solvability estimation requires sufficient samples per question (G=32 used), increasing computational cost and potentially limiting applicability to datasets with many unique questions.
- **Judge reliability:** Process accuracy evaluation depends on LLM judging, which introduces potential bias despite 97% human agreement reported; the 3% discrepancy could affect results.
- **Domain specificity:** Evaluation focuses on math and adapted general QA datasets; effectiveness on other domains like commonsense reasoning or code generation remains unexplored.

## Confidence

- **High Confidence:** The Bayesian solvability estimation mechanism and its use to identify guessing behavior is mathematically sound and well-supported by empirical evidence.
- **Medium Confidence:** The empirical improvements in both process accuracy and answer accuracy are demonstrated, though the magnitude varies and some ablation studies are limited in scope.
- **Low Confidence:** The out-of-domain generalization results are not thoroughly explored, limiting claims about the approach's broader applicability.

## Next Checks

1. **Sample Size Sensitivity Analysis:** Systematically vary G from 4 to 64 samples per question and measure both p_solvable estimation stability and downstream performance metrics. Plot the trade-off between computation cost and accuracy improvement to identify the optimal G for different dataset sizes.

2. **Judge Cross-Validation:** Evaluate the same CoTs using multiple LLM judges (e.g., GPT-4.1, Claude 3.5, Llama 3-70B) and human annotators on a held-out subset. Measure inter-judge agreement rates and analyze cases of disagreement to quantify the reliability of process accuracy measurements.

3. **Domain Transfer Experiment:** Train MCQ-DrGRPO on AQuA, then fine-tune on MATH, and finally evaluate on a completely different domain (e.g., strategyQA or Codex). Measure both answer accuracy and process accuracy degradation to assess whether solvability modeling generalizes beyond math problems.