---
ver: rpa2
title: Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language
  Translation and Research
arxiv_id: '2511.08507'
source_url: https://arxiv.org/abs/2511.08507
tags:
- dataset
- bangla
- sign
- language
- gloss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Bangla Sign Language (BdSL)
  translation, a low-resource NLP task due to the lack of large-scale sentence-level
  datasets. The authors introduce Bangla-SGP, a novel dataset of 1,000 human-annotated
  Bangla sentence-gloss pairs, augmented with ~3,000 synthetically generated pairs
  using rule-based morphological transformations, masked-token substitution, and a
  Retrieval-Augmented Generation (RAG) pipeline.
---

# Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research

## Quick Facts
- arXiv ID: 2511.08507
- Source URL: https://arxiv.org/abs/2511.08507
- Reference count: 0
- Primary result: mBART-50 achieves BLEU-4 of 27.31 and COMET of 0.8261 on augmented Bangla sentence-to-gloss translation

## Executive Summary
This paper addresses the challenge of Bangla Sign Language (BdSL) translation by introducing Bangla-SGP, a novel dataset of 1,000 human-annotated Bangla sentence-gloss pairs augmented to ~4,000 pairs using rule-based morphological transformations, masked-token substitution, and a Retrieval-Augmented Generation (RAG) pipeline. The dataset is designed to overcome the low-resource nature of BdSL translation by providing high-quality training data for transformer-based models. The study demonstrates that multilingual transformers fine-tuned on augmented data significantly outperform base models, with mBART-50 achieving the best performance metrics. The work establishes a foundation for future BdSL research and applications while highlighting the effectiveness of data augmentation in low-resource NLP settings.

## Method Summary
The Bangla-SGP dataset was created through human annotation of 1,000 Bangla sentences into gloss sequences by a professional signer, then augmented to ~4,000 pairs using three methods: rule-based morphological transformations (tense-based patterns), masked-token substitution with Bangla-BERT, and a RAG pipeline using LaBSE embeddings and GPT-4.1-nano. Transformer models including mBART-50, mT5-small, and NLLB-200 with LoRA were fine-tuned on the augmented dataset with specific hyperparameters for each model. The RAG pipeline retrieves similar annotated examples above 0.5 similarity threshold and uses two-stage prompting (tense identification → rule + example injection) to generate glosses for new sentences.

## Key Results
- mBART-50 achieved best performance with BLEU-4 of 27.31 and COMET of 0.8261 on augmented dataset
- Data augmentation improved BLEU-4 scores by ~6 points across models (mBART-50: 21.06→27.31, mT5: 17.56→20.28)
- Cohen's κ agreement of 0.7489 for binary classification and 0.3496 for quality scoring between annotators
- RAG pipeline generated ~2,000 pairs with 75.3% validation rate by professional signers

## Why This Works (Mechanism)

### Mechanism 1: Rule-based Morphological Transformation for Data Augmentation
- Claim: Tense-based morphological rules can systematically generate valid sentence-gloss pairs from limited annotations
- Mechanism: Extract consistent tense transformation patterns from human-annotated pairs, then apply these rules to convert existing sentences across tenses (e.g., present → future: verb to root form + "will be")
- Core assumption: BdSL gloss translation follows regular, codifiable tense patterns that generalize across sentences
- Evidence anchors: Abstract mentions "syntactic and morphological rules through a rule-based RAG pipeline"; section 3.2.1 discusses consistent tense patterns; corpus paper arXiv:2504.02293 mentions grammatical rule-based gloss generation

### Mechanism 2: RAG-based Few-Shot Prompting for Gloss Generation
- Claim: Retrieving similar annotated examples grounds LLM generation, improving gloss quality for unseen sentence structures
- Mechanism: Embed 800 training pairs using LaBSE, store in Pinecone. For new sentences, retrieve examples above 0.5 similarity threshold, inject into GPT-4.1-nano prompt with two-stage prompting (tense identification → rule + example injection)
- Core assumption: Similar sentences have similar gloss patterns; LLMs can generalize from 1-20 retrieved examples even without comprehensive rule coverage
- Evidence anchors: Abstract mentions "syntactic and morphological rules through a rule-based RAG pipeline"; section 3.2.3 discusses RAG effectiveness in low-resource settings; corpus paper arXiv:2510.19367 explores gloss annotation challenges

### Mechanism 3: Multilingual Transformer Transfer Learning with Augmented Data
- Claim: Multilingual pre-trained transformers fine-tuned on augmented data outperform base-data training for low-resource gloss translation
- Mechanism: Leverage cross-lingual representations from mBART-50 (610M params), mT5-small (300M params), NLLB-200 (1.3B params with LoRA). Data augmentation (1K→4K pairs) provides sufficient signal for task-specific adaptation
- Core assumption: Multilingual pre-training captures transferable linguistic structure; synthetic data quality is sufficient to improve rather than degrade performance
- Evidence anchors: Abstract reports mBART-50 BLEU-4 of 27.31 with COMET of 0.8261; section 4.3 Table 2 shows mBART-50 improvement from 21.06 to 27.31; corpus paper arXiv:2504.02293 specifically studies mBART for Bangla text-to-gloss translation

## Foundational Learning

- Concept: **Sign Language Gloss Notation**
  - Why needed here: Glosses are intermediate textual representations of sign language that follow sign grammar, not spoken language syntax. They reorder words, omit articles, and use different temporal markers. Misunderstanding this leads to expecting word-for-word translation.
  - Quick check question: If the Bangla sentence is "আমি কাল স্কুলে গেলাম" (I went to school yesterday), would the gloss be "I went to school yesterday" as a direct mapping?

- Concept: **Data Augmentation in Low-Resource NLP**
  - Why needed here: With only 1,000 annotated pairs, transformer models cannot learn robust patterns. The paper's core contribution is demonstrating that rule-based + RAG + masked substitution can expand data 4× without proportional expert effort.
  - Quick check question: Why doesn't simply duplicating the 1,000 pairs improve model performance?

- Concept: **BLEU vs. COMET Evaluation Metrics**
  - Why needed here: BLEU measures n-gram overlap (surface-level similarity), while COMET uses neural evaluation (semantic similarity). High BLEU-1 with low BLEU-4 indicates word-level but not phrase-level alignment. The paper reports both to capture different quality dimensions.
  - Quick check question: A model scores 67.99 BLEU-1 but 16.67 BLEU-4. What does this indicate about translation quality?

## Architecture Onboarding

- Component map: Bangla sentences → Professional signer → Gloss annotation (1K pairs) → Rule Engine (~500 pairs) + Bangla-BERT (~500 pairs) + RAG Pipeline (~2K pairs) → Training Layer (mBART-50/mT5/NLLB-200) → Evaluation (BLEU-1/2/3/4 + COMET + Cohen's Kappa)

- Critical path:
  1. Professional annotation (bottleneck: certified BdSL signer required)
  2. Rule extraction from annotated pairs (manual pattern analysis)
  3. RAG index construction (LaBSE embedding + Pinecone setup)
  4. Two-stage prompting (tense detection → targeted rule + example injection)
  5. Fine-tuning with early stopping (monitor BLEU-4 on held-out test set)

- Design tradeoffs:
  - Quality vs. Scale: Human annotation (κ=0.7489 binary agreement) vs. RAG generation (κ=0.3496 quality agreement, 75.3% validation rate)
  - Model size vs. Compute: NLLB-200 (1.3B) couldn't run on augmented 4K dataset; mBART-50 (610M) achieved best results with full data
  - Rule specificity vs. Coverage: Tense rules are precise but narrow; RAG covers broader patterns but with variable quality

- Failure signatures:
  - BLEU-4 << BLEU-1: Model captures words but not phrase structure (e.g., NLLB-200: BLEU-1=67.99, BLEU-4=16.67)
  - Augmented data worse than base: RAG quality too low; fallback rules insufficient
  - Cohen's κ < 0.4 for quality: Annotators disagree on gloss accuracy—requires manual review
  - No retrieval matches (similarity < 0.5): Corpus domain mismatch with training data

- First 3 experiments:
  1. Baseline vs. Augmented comparison: Fine-tune mBART-50 on 1K human pairs only, then on 4K augmented data. Expect ~6-point BLEU-4 improvement (21.06→27.31 per paper). Use same test set for fair comparison.
  2. Ablation study: Train separate models with each augmentation method independently (rules-only, masked-only, RAG-only) to isolate contribution. Measure BLEU-4 delta for each.
  3. Validation gate: Before training on augmented data, sample 15% and have 2 professional signers rate quality (1-5 scale). Only proceed if Cohen's κ ≥ 0.3 for quality and ≥70% validation rate (paper achieved 75.3%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a stable, automated pipeline be developed to generate continuous 3D sign language animations from the Bangla-SGP glosses without transition artifacts?
- Basis in paper: Section 7.2 states the authors plan to create a pipeline for 3D representations but currently face "unstable transitions between consecutive signs."
- Why unresolved: The authors attempted a preliminary exploration but lacked a comprehensive video dictionary to smooth transitions between frames.
- What evidence would resolve it: A functional pipeline producing smooth 3D animations with consistent object files across gloss transitions.

### Open Question 2
- Question: To what extent does the inclusion of non-manual markers (facial expressions, head movements) improve translation accuracy over gloss-only representations?
- Basis in paper: Section 7.2 notes that Phase 2 will collect synchronized video samples to annotate these markers, acknowledging they are critical to grammar.
- Why unresolved: The current Phase 1 dataset is limited to text-to-gloss pairs and lacks the multimodal data required to train or evaluate these features.
- What evidence would resolve it: Comparative benchmark results showing performance deltas between Phase 1 (gloss-only) and Phase 2 (multimodal) models.

### Open Question 3
- Question: How does single-signer annotation bias affect the generalizability of models trained on Bangla-SGP?
- Basis in paper: Section 7.1 identifies the reliance on a single expert as a limitation that may include "signer bias."
- Why unresolved: The dataset does not currently contain sufficient samples from multiple signers to test or mitigate idiosyncratic annotation patterns.
- What evidence would resolve it: A study measuring inter-annotator agreement and model performance variance when trained on data annotated by diverse signers.

## Limitations

- The RAG-based data augmentation pipeline relies heavily on GPT-4.1-nano availability and cost, with exact prompt templates and rule definitions not published, limiting reproducibility.
- Human annotation quality showed moderate inter-annotator agreement (κ=0.7489 binary, 0.3496 quality), suggesting potential variability in gloss annotations that could affect model training and evaluation.
- The study focuses on specific BdSL gloss patterns and may not capture full linguistic complexity, particularly for idiomatic expressions, domain-specific terminology, or complex sentence structures.
- Computational constraints prevented evaluation of NLLB-200 on augmented dataset, leaving gaps in understanding how larger multilingual models perform with this task and data augmentation approach.

## Confidence

- Data augmentation effectiveness: High (multiple experiments show 6+ point BLEU-4 improvements across models)
- mBART-50 superiority: Medium (achieved best results, but NLLB-200 couldn't be evaluated on augmented data due to computational constraints)
- RAG pipeline effectiveness: Medium (contributed ~2,000 pairs with 75.3% validation rate, but quality agreement was low at κ=0.3496)

## Next Checks

1. **Cross-validation with independent annotators**: Have 2-3 additional professional BdSL signers independently annotate 100 randomly sampled sentence-gloss pairs from the augmented dataset to verify consistency and identify systematic generation errors.

2. **Out-of-domain robustness testing**: Evaluate fine-tuned models on BdSL sentences from different domains (e.g., news articles, social media, technical documents) to assess generalization beyond the training corpus and identify domain-specific weaknesses.

3. **End-to-end BdSL video translation pipeline**: Integrate the best-performing sentence-to-gloss model with existing Bangla text-to-sign video generation systems to assess practical utility and identify any quality degradation when moving from intermediate gloss representation to final video output.