---
ver: rpa2
title: 'AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology'
arxiv_id: '2506.18156'
source_url: https://arxiv.org/abs/2506.18156
tags:
- moral
- cognitive
- llms
- human
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models exhibit human-like cognitive tendencies
  across four psychological frameworks: thematic apperception, framing bias, moral
  foundations, and cognitive dissonance. In TAT experiments, models generated coherent
  narratives with varied emotional and moral tones.'
---

# AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology

## Quick Facts
- **arXiv ID:** 2506.18156
- **Source URL:** https://arxiv.org/abs/2506.18156
- **Reference count:** 40
- **Primary result:** LLMs exhibit human-like cognitive patterns across TAT, framing bias, moral foundations, and cognitive dissonance frameworks.

## Executive Summary
This study applies four psychological frameworks—Thematic Apperception Test, Framing Bias, Moral Foundations Theory, and Cognitive Dissonance—to evaluate cognitive tendencies in large language models. The findings reveal that models generate coherent narratives with varied moral tones, show strong optimism bias under negative framing, exhibit elevated sensitivity to liberty/oppression themes likely due to alignment training, and rarely contradict themselves but provide complex rationalizations when faced with cognitive dissonance. These patterns suggest LLMs mirror human cognitive biases shaped by their training and alignment methods, with implications for AI transparency and ethical deployment.

## Method Summary
The study evaluates four LLMs (GPT-4o, LLaMA 3.3, DeepSeek V3, and others) across four psychological frameworks using automated LLM judging via LLaMA 3.1 405B. For TAT, 30 ambiguous images were presented and narratives scored using SCORS-G rubric. Framing bias used 460 question pairs across 46 categories. Moral Foundations Theory employed 360 situational questions. Cognitive dissonance was tested with 200 scenario variations. All prompts were processed with default decoding parameters, and outputs were automatically scored by the judge model using custom rubrics.

## Key Results
- TAT narratives showed varied emotional and moral tones with SCORS-G scores across 8 dimensions (1-5 scale)
- Framing bias revealed 58.37% positive entailment vs 15.65% negative entailment in DeepSeek-v3, aligning with prospect theory's risk-aversion
- Liberty/Oppression moral foundation scores (3.8-4.7) significantly exceeded human baseline (2.3), likely due to RLHF alignment
- Cognitive dissonance showed low contradiction rates (<1.5) but high rationalization complexity across all models

## Why This Works (Mechanism)

### Mechanism 1: Alignment Training Amplifies Liberty/Oppression Sensitivity
The paper posits that RLHF optimizes models toward "safe" and "fair" outputs, encoding heightened sensitivity to oppression dynamics. This causes models to over-index on Liberty/Oppression themes compared to human baselines. The core assumption is that this sensitivity emerges from the alignment phase rather than the pre-training corpus. Evidence shows models scoring 3.8-4.7 on Liberty/Oppression versus human baseline of 2.3. This mechanism could break if base models without instruction tuning show similar sensitivity.

### Mechanism 2: Framing Bias as Linguistic Optimism
Models exhibit linguistic "optimism bias" where they maintain positive, helpful stances even under negative framing, effectively ignoring negative frames to produce socially desirable outputs. This mirrors human Prospect Theory's risk-aversion patterns. The core assumption is that this behavior stems from training objectives to be helpful rather than pure logic. Evidence shows DeepSeek-v3 with 58.37% positive entailment vs 15.65% negative entailment. This could break if purely logical models showed higher rates of negative entailment under negative framing.

### Mechanism 3: Dissonance Masked by Rationalization Complexity
When presented with conflicting beliefs, models rarely output direct contradictions but instead generate verbose explanations to bridge logical gaps, maintaining internal coherence. This emergent property prioritizes plausible-sounding text over strict logical consistency. The core assumption is that this stems from instruction tuning for helpfulness. Evidence shows rationalization complexity is fairly high while contradiction scores remain below 1.5. This could break if explicit wordiness penalties revealed higher underlying contradiction rates.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** It's the hypothesized driver for exaggerated Liberty/Oppression moral scores. Understanding RLHF explains why models might "fake" alignment with human moral ideals.
  - **Quick check question:** Does the model display this moral sensitivity in a base (pre-aligned) version, or only after RLHF?

- **Concept: Prospect Theory (Framing Effects)**
  - **Why needed here:** The paper explicitly maps model's "positive entailment" preference to human risk-aversion defined in Prospect Theory.
  - **Quick check question:** If a prompt frames a loss as a "gain," does the model's decision flip, or does it remain stubbornly optimistic?

- **Concept: Projective Testing (Thematic Apperception Test)**
  - **Why needed here:** To understand how ambiguous inputs elicit latent "psychological" states or biases in models without explicit questioning.
  - **Quick check question:** Does the model project consistent "personality" traits across different ambiguous images, or is it just pattern-matching the prompt style?

## Architecture Onboarding

- **Component map:** Subject Models (GPT-4o, LLaMA 3.3, DeepSeek) -> Judge/Evaluator (LLaMA 3.1 405B) -> Automated Scoring (SCORS-G, Likert scales)

- **Critical path:**
  1. Generate stimuli: Create framed question pairs or select TAT images
  2. Elicit response: Query subject models with default temperature settings
  3. Automated scoring: Pass subject model output + rubric to evaluator model

- **Design tradeoffs:**
  - LLM-as-Judge vs Human Eval: Uses LLaMA 405B for scalability but may introduce "machine bias"
  - Scenario-based vs Direct Questioning: Situational dilemmas reduce refusals but make it harder to separate moral code from narrative ability

- **Failure signatures:**
  - "AI Disclaimer" Loop: Models refuse to answer rather than engaging with framing bias
  - Sycophancy: Models agree with user's frame regardless of prompt logic

- **First 3 experiments:**
  1. Isolate RLHF Influence: Compare base vs instruct-tuned models on MFT to verify alignment effect
  2. Judge Validation: Compare LLaMA 405B scoring against human expert sample for TAT narratives
  3. Temperature Sweep: Test framing bias across multiple temperature settings to assess robustness

## Open Questions the Paper Calls Out

- **Open Question 1:** Do elevated Liberty/Oppression scores result from RLHF alignment strategies or emulation of inferred social desirability? The precise cause remains inconclusive and requires ablation studies comparing base and aligned models.

- **Open Question 2:** Do observed cognitive biases persist or shift in dynamic, multi-turn interactions compared to static prompts? Current methodology used single-turn prompts, but future research should explore interactive game environments.

- **Open Question 3:** Are moral foundations and cognitive biases culturally invariant, or do they exhibit language-dependent variations? All tests were conducted in English, leaving potential multilingual differences unexplored.

## Limitations

- Automated evaluation via LLM judge (Llama 3.1 405B) introduces potential judge bias and calibration issues
- Specific TAT image set and exact dataset prompts were not fully specified, limiting exact replication
- Attribution of elevated Liberty/Oppression scores to RLHF is speculative without base model comparisons

## Confidence

- **High Confidence:** General methodology of applying projective tests and framing bias tasks to LLMs is sound with technically detailed automated annotation pipeline
- **Medium Confidence:** Observation that models show systematic framing bias and produce coherent TAT narratives is replicable, though psychological interpretations are inferred
- **Low Confidence:** Attribution of Liberty/Oppression scores specifically to RLHF alignment is speculative; psychological interpretations of dissonance rationalizations are largely qualitative

## Next Checks

1. **Isolate RLHF Influence:** Run MFT battery on base model (e.g., LLaMA 3.1 70B base) vs instruct-tuned counterpart to verify if Liberty/Oppression sensitivity is truly an artifact of alignment

2. **Judge Validation:** Compare LLaMA 3.1 405B automated SCORS-G scoring of TAT narratives against small human expert sample to calibrate automated rubric and quantify judge bias

3. **Temperature Sweep:** Re-run framing bias experiment at multiple temperature settings (0.1, 0.7, 1.5) to determine if positive entailment preference is robust or decoding artifact