---
ver: rpa2
title: 'Evaluating Human Trust in LLM-Based Planners: A Preliminary Study'
arxiv_id: '2502.20284'
source_url: https://arxiv.org/abs/2502.20284
tags:
- trust
- planner
- planning
- plan
- planners
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates human trust in LLM-based planners versus
  classical planners through a user study in a PDDL domain. The research compares
  a language-model-based planner (GPT-4o) with a traditional graph-search-based planner
  (Fast Downwards) using subjective trust measures and objective evaluation accuracy.
---

# Evaluating Human Trust in LLM-Based Planners: A Preliminary Study

## Quick Facts
- **arXiv ID**: 2502.20284
- **Source URL**: https://arxiv.org/abs/2502.20284
- **Reference count**: 40
- **Primary result**: PDDL solver achieved highest correctness and trust scores; explanations improved accuracy but not trust; plan refinement showed potential for overtrust

## Executive Summary
This study investigates human trust in LLM-based planners versus classical planners through a controlled user study in a PDDL domain. The research compares GPT-4o with Fast Downwards planner across subjective trust measures and objective evaluation accuracy. The study found that correctness is the primary driver of trust, with the classical PDDL solver outperforming LLM-based approaches in both accuracy and trust scores. While explanations provided minimal trust benefits, plan refinement showed potential to increase trust without corresponding accuracy improvements, suggesting users may overtrust LLM refinements.

## Method Summary
The study employed a within-subject design with 23 participants evaluating plans generated by four different planners: PDDL solver, LLM planner, LLM with explanation, and LLM with refinement. Participants completed 8 tasks (2 per planner) in a controlled PDDL domain, spending 6 minutes per plan. Trust was measured using a 7-point scale and the Trust in Automation scale, while correctness was assessed through participant evaluations of task completion. The study also analyzed the propensity to trust based on subjective trust scores and objective evaluation accuracy.

## Key Results
- PDDL solver achieved highest evaluation accuracy (1.76/2 correct tasks) and trust scores (5.68/7)
- Explanations improved evaluation accuracy but had minimal impact on trust scores
- Plan refinement showed potential to increase trust without significantly enhancing evaluation accuracy
- Correctness emerged as the primary driver of trust across all conditions

## Why This Works (Mechanism)
The study demonstrates that human trust in planning systems is primarily driven by observed correctness rather than explanatory features. The mechanism suggests that users form trust judgments based on tangible outcomes and task completion success, while supplementary features like explanations provide limited additional trust benefits. The overtrust phenomenon in plan refinement indicates that users may attribute increased credibility to refined outputs regardless of actual accuracy improvements.

## Foundational Learning
- **PDDL (Planning Domain Definition Language)**: Formal language for representing planning problems and domains; needed for standardized task representation across planners
- **Trust propensity analysis**: Method for examining the relationship between subjective trust and objective performance; needed to validate trust-correctness correlation
- **Within-subject design**: Experimental approach where same participants experience all conditions; needed to control for individual trust baselines
- **Plan refinement**: Iterative improvement process for generated plans; needed to test whether enhanced outputs increase user trust
- **Trust in Automation scale**: Standardized questionnaire for measuring trust in automated systems; needed for consistent trust measurement

## Architecture Onboarding
**Component Map**: PDDL domain → Task generation → Plan generation (4 planners) → User evaluation → Trust/accuracy measurement
**Critical Path**: Domain definition → Task creation → Plan generation → User assessment → Trust scoring
**Design Tradeoffs**: Simplified domain (reduced realism) vs. controlled variables (improved measurement precision)
**Failure Signatures**: Low evaluation accuracy correlates with low trust; explanation features show limited impact when base accuracy is poor
**3 First Experiments**:
1. Test PDDL solver vs. LLM planner on identical simple tasks
2. Evaluate explanation impact by comparing LLM vs. LLM+explanation conditions
3. Assess refinement effects by comparing LLM vs. LLM+refinement conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=23) limits statistical power and generalizability
- Homogeneous participant pool (university affiliates) may not represent broader populations
- Controlled PDDL domain setting may not generalize to complex real-world planning scenarios
- Short task completion time (6 minutes) may not capture sustained trust development

## Confidence
- **High confidence**: PDDL solver's superior correctness and trust scores compared to LLM-based approaches
- **Medium confidence**: Explanations have minimal impact on trust scores
- **Medium confidence**: Plan refinement may lead to overtrust without accuracy improvement
- **Low confidence**: Generalizability to real-world planning scenarios and more complex tasks

## Next Checks
1. Replicate the study with a larger, more diverse participant pool (n>100) across different demographics and professional backgrounds
2. Test the same methodology with more complex planning domains and tasks with multiple solution paths
3. Conduct a longitudinal study examining how trust evolves over multiple interactions with LLM-based planners in real-world settings