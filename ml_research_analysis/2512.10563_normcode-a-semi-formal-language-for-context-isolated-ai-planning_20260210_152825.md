---
ver: rpa2
title: 'NormCode: A Semi-Formal Language for Context-Isolated AI Planning'
arxiv_id: '2512.10563'
source_url: https://arxiv.org/abs/2512.10563
tags:
- normcode
- number
- data
- step
- execution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'NormCode addresses context pollution in LLM-based planning by
  enforcing explicit data isolation: each inference operates only on explicitly passed
  inputs, eliminating cross-step contamination. It separates semantic operations (LLM
  reasoning) from syntactic operations (deterministic data flow), enabling precise
  cost attribution and auditability.'
---

# NormCode: A Semi-Formal Language for Context-Isolated AI Planning

## Quick Facts
- arXiv ID: 2512.10563
- Source URL: https://arxiv.org/abs/2512.10563
- Authors: Xin Guan; Yunshan Li
- Reference count: 40
- Primary result: 100% accuracy on base-X addition across arbitrary digit lengths with successful self-hosted compiler pipeline

## Executive Summary
NormCode addresses context pollution in LLM-based planning by enforcing explicit data isolation: each inference operates only on explicitly passed inputs, eliminating cross-step contamination. It separates semantic operations (LLM reasoning) from syntactic operations (deterministic data flow), enabling precise cost attribution and auditability. The language exists in multiple isomorphic formats (.ncds for drafting, .ncd for execution, .ncn for verification) supporting progressive formalization from sketch to production. A four-phase compiler pipeline transforms natural language intent into executable JSON repositories. Validation shows 100% accuracy on base-X addition across arbitrary digit lengths and successful self-hosting of the compiler pipeline, demonstrating both correctness and expressive completeness.

## Method Summary
NormCode uses a four-phase compiler pipeline (Derivation → Formalization → Post-Formalization → Activation) to transform natural language intent in .ncds files into executable JSON repositories. The system enforces explicit data isolation where each inference receives only declared inputs via sealed execution contexts, preventing cross-step contamination. An orchestrator manages dependency-driven scheduling through Waitlist, Blackboard, and Repository components, with SQLite checkpointing enabling resumability. Agent Sequences handle different inference types (Imperative, Judgement, Assigning, Grouping, Timing, Looping), while Paradigm JSON files configure LLM calls. A Canvas App provides visual debugging with real-time graph inspection, breakpoints, and tensor visualization.

## Key Results
- 100% accuracy on base-X addition across arbitrary digit lengths (tested up to 150-digit numbers)
- Successful self-hosting of the compiler pipeline (~50 inferences across 4 phases)
- Demonstrated separation of semantic (LLM) and syntactic (deterministic) operations enabling precise cost attribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit data isolation reduces hallucination and constraint drift in multi-step LLM workflows.
- Mechanism: Each inference receives only declared inputs via a sealed execution context; no implicit context bleeding occurs between steps. This prevents early-step hallucinations from propagating into later prompts.
- Core assumption: LLM errors compound when prior context is visible; isolation localizes failures.
- Evidence anchors:
  - [abstract] "each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design"
  - [Section 1.1] "context pollution—the accumulation of information across reasoning steps—causes models to hallucinate, confuse intermediate outputs with inputs, and lose track of original task constraints"
  - [corpus] Related work on reliability issues in agentic systems (arXiv:2601.17435) corroborates that implicit state management causes hallucinated actions, but does not directly test isolation as a remedy.
- Break condition: If downstream reasoning genuinely requires accumulated context (e.g., incremental summarization across all prior steps), isolation may impede rather than help; the paper does not evaluate such scenarios.

### Mechanism 2
- Claim: Separating semantic (LLM) from syntactic (deterministic) operations enables precise cost and reliability attribution.
- Mechanism: Syntactic operators (grouping, assigning, timing, looping) perform tensor algebra without LLM calls; only imperatives and judgements invoke models. The orchestrator tags each step by type, producing an audit trail distinguishing probabilistic from deterministic computation.
- Core assumption: Most data plumbing in workflows is structural and does not require AI reasoning; confining LLM calls to marked steps exposes where tokens burn and where non-determinism enters.
- Evidence anchors:
  - [abstract] "separates semantic operations (LLM reasoning) from syntactic operations (deterministic data flow), enabling precise cost attribution and auditability"
  - [Section 5.3] "In a typical NormCode plan, the majority of steps are syntactic... Only the 'thinking' steps (imperatives and judgements) invoke an LLM"
  - [corpus] No direct corpus evidence evaluates this separation pattern; related agent frameworks (GraphicBench, FinRobot) do not isolate semantic/syntactic layers.
- Break condition: If semantic content is deeply interleaved with structural decisions (e.g., LLM decides loop termination dynamically), the clean separation may blur; the paper acknowledges but does not deeply test such hybrid cases.

### Mechanism 3
- Claim: Progressive formalization through a multi-format ecosystem improves human-AI collaboration in plan authoring and verification.
- Mechanism: Plans traverse .ncds (human draft) → .ncd (formal IR) → .ncn (natural language narrative) → executable JSON. Each phase adds precision while preserving review opportunities; the .ncn format allows domain experts to verify logic without mastering formal syntax.
- Core assumption: Stakeholders (developers, auditors, domain experts) need different views of the same underlying plan; strict upfront formalization would block iteration.
- Evidence anchors:
  - [abstract] "supports progressive formalization from sketch to production"
  - [Section 7.1] "Each stage progressively adds rigor while preserving opportunities for human review"
  - [corpus] Neighbor papers on declarative agentic layers (arXiv:2601.17435) suggest structured IRs help reliability, but do not test multi-format human verification.
- Break condition: If compilation fails or generates incorrect .ncn narratives, users may mistrust the system; the paper notes compiler robustness remains an open challenge (Section 10).

## Foundational Learning

- Concept: Intermediate Representations (IRs)
  - Why needed here: NormCode's .ncd format is an IR between natural intent and executable JSON; understanding IRs (e.g., LLVM) clarifies why structure is extracted before execution.
  - Quick check question: Can you explain why compilers use IRs instead of translating directly from source to machine code?

- Concept: Hierarchical Task Network (HTN) Planning
  - Why needed here: NormCode's inference decomposition mirrors HTN's recursive task breakdown; knowing HTN helps map plan hierarchies to execution.
  - Quick check question: How does HTN differ from classical STRIPS-style planning in representing tasks?

- Concept: Tensors with Named Axes
  - Why needed here: NormCode's Reference system uses named-axis tensors to track data shape and flow; this prevents "shape mismatch" errors in multi-step workflows.
  - Quick check question: Why might named axes (e.g., ['document', 'feature']) be more robust than positional indices for tracking data transformations?

## Architecture Onboarding

- Component map:
  - Author intent in .ncds → Compiler Pipeline (Derivation → Formalization → Post-Formalization → Activation) → Executable JSON → Orchestrator (Waitlist, Blackboard, Repositories) → Execution via CLI or Canvas App

- Critical path:
  1. Author intent in .ncds (natural language with minimal markers)
  2. Run compiler: Derivation → Formalization (adds flow indices, sequence types) → Post-Formalization (paradigm bindings) → Activation (JSON repos)
  3. Load concept.json and inference.json into Orchestrator
  4. Execute via CLI or Canvas App; inspect Blackboard state at each flow index

- Design tradeoffs:
  - Verbosity vs. Auditability: Explicit data flow produces longer plans but enables precise debugging
  - Structure vs. Flexibility: Semi-formal syntax allows natural language content; compiler enforces only what's needed for execution
  - Overhead vs. Resumability: SQLite checkpointing adds I/O cost but supports long-running, forkable workflows

- Failure signatures:
  - Context pollution (traditional systems): Early hallucinations leak forward; NormCode isolates by design but compilation errors can reintroduce implicit dependencies
  - Dependency cycles: Orchestrator detects at runtime, not compile time
  - Paradigm mismatch: If vertical/horizontal specs don't match available tools, MFP or TVA steps fail
  - Perceptual sign resolution failure: Missing files or untransmutable signs cause MVP errors

- First 3 experiments:
  1. Implement a minimal 3-step inference plan (e.g., load document → summarize → extract keywords) in .ncds; compile and run via CLI to observe flow indices and Blackboard updates
  2. Intentionally introduce a dependency error (reference a concept before it's computed) and verify Orchestrator detection; inspect Canvas App for visual failure signal
  3. Fork a completed run at a mid-point checkpoint, modify a paradigm parameter (e.g., change LLM model), and resume to compare outputs—demonstrating checkpoint-based experimentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NormCode compare to baseline approaches (direct prompting, LangChain, AutoGPT) on standardized benchmarks in terms of accuracy, token cost, and debugging time?
- Basis in paper: [explicit] "Comparative evaluation against direct prompting, LangChain, and other frameworks on established benchmarks is planned for future work" (Section 8.4); "Benchmarking against baseline approaches... would quantify cost-accuracy tradeoffs" (Section 10).
- Why unresolved: Current validation uses only two case studies (self-hosted compiler, base-X addition) without comparative baselines or statistical analysis.
- What evidence would resolve it: Controlled experiments on benchmarks like HumanEval or GAIA measuring accuracy, token consumption, latency, and developer effort across frameworks.

### Open Question 2
- Question: Can fine-tuned models or automated validation improve the robustness of natural language to .ncd derivation?
- Basis in paper: [explicit] "The natural language to .ncd derivation phase remains the most fragile component, relying on carefully designed prompts that are sensitive to instruction complexity" (Section 10).
- Why unresolved: The derivation phase uses hand-crafted prompts without systematic error recovery or validation.
- What evidence would resolve it: Evaluation of derivation accuracy across varying instruction complexities, with comparisons between prompted LLMs and fine-tuned models on human-authored plans.

### Open Question 3
- Question: What mechanisms can enable true multi-agent coordination within NormCode's isolation constraints?
- Basis in paper: [explicit] "The current implementation supports multiple Subjects... but real multi-agent coordination remains unexplored" (Section 10).
- Why unresolved: Current multi-agent support handles different tool bodies but lacks negotiation protocols, delegation mechanisms, and conflict resolution.
- What evidence would resolve it: Implementation and testing of protocols for agent-to-agent task allocation and contradiction handling in workflows requiring coordination.

## Limitations

- No public implementation code or complete schema examples for Paradigm files and compiler pipeline, making faithful reproduction challenging
- Limited empirical validation using only two case studies without comparative baselines or statistical analysis
- The natural language to .ncd derivation phase remains fragile, relying on hand-crafted prompts without systematic error recovery

## Confidence

- **High Confidence**: The core mechanism of explicit data isolation reducing context pollution is well-grounded in established LLM reliability literature
- **Medium Confidence**: The semantic/syntactic separation claim has theoretical support but lacks direct empirical evidence and acknowledges hybrid cases may blur the separation
- **Low Confidence**: The progressive formalization's practical effectiveness for human-AI collaboration lacks user studies or empirical evidence demonstrating improved collaboration outcomes

## Next Checks

1. Implement and test base-X addition with 150-digit numbers using the described four-phase compiler pipeline to verify the 100% accuracy claim and orchestrator's dependency resolution capabilities.

2. Reproduce the self-hosting experiment by compiling NormCode's own compiler pipeline and verify all 50+ inferences complete successfully with identical outputs to the original.

3. Stress-test context isolation by designing a multi-step plan where early-step hallucinations would normally propagate and verify that explicit data isolation truly prevents contamination versus traditional implicit context approaches.