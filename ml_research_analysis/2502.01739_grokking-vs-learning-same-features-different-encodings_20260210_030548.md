---
ver: rpa2
title: 'Grokking vs. Learning: Same Features, Different Encodings'
arxiv_id: '2502.01739'
source_url: https://arxiv.org/abs/2502.01739
tags:
- grokking
- learning
- ising
- compressibility
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grokking and steady learning lead to the same dataset features
  but different model compressions. In modular addition, a compressive training regime
  emerges with a linear trade-off between model loss and compressibility, enabling
  up to 25x compression compared to 5x in grokking.
---

# Grokking vs. Learning: Same Features, Different Encodings

## Quick Facts
- **arXiv ID:** 2502.01739
- **Source URL:** https://arxiv.org/abs/2502.01739
- **Reference count:** 40
- **Primary result:** Grokked and steadily trained models learn identical features but exhibit different compression profiles.

## Executive Summary
This paper resolves the paradox of grokking by showing that grokked and steadily trained models learn the same underlying features despite vastly different training trajectories. The key distinction is that steady learning enters a "compressive regime" where models achieve up to 25x compression with a linear trade-off between loss and compressibility, compared to 5x in grokking. Using information-geometric measures, the authors show that grokking trajectories follow straight paths dominated by weight decay, while learning trajectories exhibit different geometric patterns depending on the task.

## Method Summary
The authors train two types of models on interpretable tasks: Ising phase classification using a 2-layer CNN and modular addition using a 1-layer MLP. They control the training path (grokking vs. steady) by varying weight initialization scale ($w_0$). Grokking occurs at high $w_0$ (≥3 for Ising, ≥0.4 for modular addition), while steady learning emerges at low $w_0$ (≤0.4 for Ising, ≤0.3 for modular addition). They analyze feature learning through correlation with ground-truth features (energy/magnetization for Ising, Fourier modes for modular addition) and measure compressibility via magnitude pruning. Fisher Information Metric (FIM) trajectories quantify the geometric properties of training paths.

## Key Results
- Grokked and steadily trained models learn identical features despite different training speeds
- Steady learning achieves 25x compression vs 5x in grokking through a linear loss-compressibility trade-off
- Grokking trajectories follow straight lines in information space dominated by weight decay
- Modular addition shows feature refinement during the plateau, while Ising shows none

## Why This Works (Mechanism)

### Mechanism 1: Task-Dependent Feature Determinism
- **Claim:** If a model is trained on interpretable tasks (e.g., phase classification, modular arithmetic), the learned features are determined by the data structure rather than the specific training trajectory (grokking vs. steady).
- **Mechanism:** The optimization landscape contains a "natural basis" (e.g., energy or Fourier modes). Regardless of the path speed or "grokking" delay, gradient descent converges to these dominant representations to minimize loss.
- **Core assumption:** The tasks possess a known, ground-truth interpretable structure that the network is capable of representing.
- **Evidence anchors:** [abstract] "grokked and steadily trained models learn the same features"; [section 4.1] "every neuron in the final layer is either perfectly correlated or anti-correlated with the energy"

### Mechanism 2: Initialization-Scaled Compressive Regime
- **Claim:** If the weight initialization scale ($w_0$) is reduced to induce steady learning, models enter a "compressive regime" where compression factors increase linearly as training loss decreases.
- **Mechanism:** Lower initialization scales prevent the "memorization" circuit formation typical of grokking. This allows the optimizer to find solutions that lie on a manifold where model loss and complexity (measured by pruning vulnerability) trade off linearly.
- **Core assumption:** The "steady learning" regime is accessible via initialization tuning and is not an artifact of specific optimizer settings.
- **Evidence anchors:** [abstract] "linear trade-off between model loss and compressibility... 25x compression compared to 5x in grokking"; [section 5] "compressive regime of steady training... absent in grokking"

### Mechanism 3: Weight Decay as a Geometric Driver
- **Claim:** If a model undergoes grokking (specifically in the Ising task), the trajectory follows a straight line in model space, dominated by uniform weight decay rather than feature learning.
- **Mechanism:** Large initial weights place the model in a memorization basin. Weight decay exerts a uniform force toward the origin, causing the model to travel a "straight" path (geodesic) in Fisher-Rao metric space until it hits the generalization manifold.
- **Core assumption:** The Fisher Information Metric (FIM) accurately captures the "straightness" of the trajectory in high-dimensional parameter space.
- **Evidence anchors:** [abstract] "grokking trajectories follow approximately straight paths in model space"; [section 6.3] "dominant effect before the grokking transition is weight decay... trajectory... points towards the origin"

## Foundational Learning

- **Concept: Magnitude Pruning**
  - **Why needed here:** The paper relies on pruning (setting smallest weights to zero) to define "compressibility" and distinguish the efficiency of grokked vs. steady models.
  - **Quick check question:** Can you explain why a model that resists magnitude pruning (maintains accuracy despite weight removal) is considered "more compressible"?

- **Concept: Fisher Information Metric (FIM)**
  - **Why needed here:** The paper uses FIM to quantify the "straightness" of training trajectories, serving as a novel progress measure that generalizes across tasks.
  - **Quick check question:** How does the FIM differ from Euclidean distance when measuring "steps" in parameter space, and why does the paper claim FIM is better?

- **Concept: Fourier Basis / Inverse Participation Ratio (IPR)**
  - **Why needed here:** To verify that Modular Addition models learn the correct features, the paper measures how localized the weights are in the Fourier domain using IPR.
  - **Quick check question:** If a neuron has a high IPR in the Fourier domain, what does that imply about the specific frequency it processes?

## Architecture Onboarding

- **Component map:**
  - Ising Task: 2-Layer CNN [2,4 channels, stride 2, kernel 2] + 1 FC Layer (ReLU) -> Energy/Magnetization features
  - Modular Addition Task: 1-Layer MLP [226→512→113] (ReLU) -> Fourier modes features
  - Control Knob: Weight Multiplier ($w_0$) at initialization (tunes Grokking vs. Steady)
  - Analysis Layer: Magnitude Pruning & FIM trajectory logging

- **Critical path:**
  1. Initialize weights with specific multiplier $w_0$ (e.g., $w_0 > 3$ for grokking, $w_0 < 0.4$ for steady)
  2. Train using Adam with weight decay
  3. Evaluate terminal model on pruning curves (Area Under Curve) and feature correlation (IPR/Energy)

- **Design tradeoffs:**
  - **Steady Learning ($w_0$ low):** Faster convergence, 5x higher compressibility, but requires careful tuning to avoid the "breakdown" of feature representation
  - **Grokking ($w_0$ high):** Slow convergence, robust feature formation, but lower compressibility and compute efficiency

- **Failure signatures:**
  - **No Compressive Regime:** If batch size is too high (>200), the linear trade-off between loss and compression vanishes
  - **Representation Breakdown:** If weight decay is too aggressive ($3 \times 10^{-4}$) in the steady regime, Fourier localization (IPR) collapses, and the model fails to generalize despite compressing well

- **First 3 experiments:**
  1. **Replicate the Sweep:** Train the Modular Addition MLP across weight multipliers (0.1 to 10) to observe the phase transition from steady learning to grokking in test accuracy curves
  2. **Verify Compression:** Run magnitude pruning on the checkpoints from Exp 1; plot "Accuracy vs. % Weights Pruned" to confirm that steady learning models (low $w_0$) maintain accuracy longer than grokked models
  3. **FIM Trajectory Check:** Compute the FIM-cosine similarity between consecutive steps for the Ising task to verify that grokking trajectories approach a similarity of 1.0 (straight line) while learning trajectories do not

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are there tasks which can only be grokked, or conversely, can only be steadily learned?
- **Basis in paper:** [explicit] "However, it is unknown if there are tasks which can only be grokked, or, vice versa, can only be steadily learned."
- **Why unresolved:** The authors only studied two toy tasks where both grokking and steady learning were achievable by tuning initialization weights. Whether some tasks force one path over the other remains unexplored.
- **What evidence would resolve it:** A systematic survey of diverse tasks (algorithmic, linguistic, physical) testing whether both regimes are reachable under varied hyperparameter configurations.

### Open Question 2
- **Question:** What is the mechanistic nature of the "compressive regime," and can it be exploited to compress practical large-scale models?
- **Basis in paper:** [explicit] "Second, what is the nature of the compressive regime, and can it be used to compress practical models?"
- **Why unresolved:** The authors discovered a regime in modular addition where steady learning achieves 25x compression with a linear loss-compressibility trade-off, but the underlying mechanism and generalizability to larger architectures is unknown.
- **What evidence would resolve it:** Experiments testing whether the compressive regime emerges in larger transformer or CNN architectures, combined with analysis of weight distributions and circuit structure in compressed models.

### Open Question 3
- **Question:** Can information-geometric measures be developed to provide general-purpose, scalable interpretability tools?
- **Basis in paper:** [explicit] "Finally, can information geometry be used to help scale interpretability?"
- **Why unresolved:** Task-specific interpretability measures had to be hand-constructed for each task, while FIM-based measures were task-agnostic but only demonstrated on small models. Whether these geometric measures scale to large models is unknown.
- **What evidence would resolve it:** Application of FIM trajectory measures to large language models or vision transformers to detect learning transitions, grokking-like phenomena, or circuit formation.

### Open Question 4
- **Question:** Do models in the compressive regime display significantly higher superposition than models trained outside it?
- **Basis in paper:** [explicit] "Since models are known to compress features by superposition, it would be particularly interesting to investigate in further work whether models in this regime display significantly higher superposition than models trained outside of it."
- **Why unresolved:** The compressive regime achieves high compression factors, but whether this reflects representational superposition or a different encoding strategy was not investigated.
- **What evidence would resolve it:** Probing experiments measuring feature polysemanticity and superposition metrics across models trained inside and outside the compressive regime.

## Limitations

- **Feature Equivalence Generalization:** The paper's claims about feature equivalence rely on specific interpretable tasks with known ground-truth features, limiting generalizability to tasks without such structure.
- **Compressive Regime Robustness:** The compressive regime's existence depends on precise hyperparameter tuning (initialization scale, batch size), and the paper doesn't explore the full hyperparameter landscape.
- **FIM Metric Validation:** The geometric interpretation relies on Fisher Information Metric without comparison to alternative trajectory measures or validation on more complex tasks.

## Confidence

- **High Confidence:** Feature correlation results for Ising (energy) and modular addition (Fourier modes) - clear quantitative evidence (IPR values, correlation coefficients)
- **Medium Confidence:** Compressive regime trade-off - quantified 25x vs 5x compression factor, but linear trade-off assumption needs validation across wider range
- **Low Confidence:** FIM trajectory analysis - geometric interpretation relies on single metric without comparison to alternatives

## Next Checks

1. **Cross-Task Feature Generalization:** Apply feature equivalence analysis to CIFAR-10 classification to test whether mechanism generalizes beyond interpretable problems
2. **Compressive Regime Robustness:** Systematically vary weight decay, learning rate, and batch size in modular addition to map boundaries of compressive regime
3. **Alternative Trajectory Metrics:** Compare FIM-based trajectory straightness against Euclidean distance, gradient alignment, and information bottleneck measures