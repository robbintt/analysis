---
ver: rpa2
title: Improving Coverage in Combined Prediction Sets with Weighted p-values
arxiv_id: '2505.11785'
source_url: https://arxiv.org/abs/2505.11785
tags:
- coverage
- prediction
- aggregation
- weighted
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a weighted p-value framework for aggregating\
  \ conformal prediction sets, improving coverage guarantees beyond the standard 1-2\u03B1\
  \ bound. The method assigns weights to individual prediction sets based on their\
  \ importance, allowing the overall coverage guarantee to interpolate between 1-2\u03B1\
  \ and 1-\u03B1 depending on the weight distribution."
---

# Improving Coverage in Combined Prediction Sets with Weighted p-values

## Quick Facts
- arXiv ID: 2505.11785
- Source URL: https://arxiv.org/abs/2505.11785
- Reference count: 40
- Primary result: Weighted p-value framework improves worst-slice coverage beyond standard 1-2α bounds for conformal prediction sets

## Executive Summary
This paper introduces a weighted p-value framework for aggregating conformal prediction sets, achieving coverage guarantees that interpolate between 1-2α and 1-α depending on weight distribution. The method assigns data-dependent weights to individual prediction sets based on their importance, enabling adaptive coverage in settings like mixture-of-experts. By introducing a correction factor m* for data-dependent weights, the approach maintains finite-sample validity while improving worst-slice coverage on regression and classification tasks. Experiments demonstrate more consistent coverage across demographic groups compared to standard split conformal methods.

## Method Summary
The method aggregates K conformal prediction sets by computing weighted averages of their p-value functions: p̄(x,y) = Σv_k·p̂_k(x,y). For data-independent weights, this yields coverage guarantees between 1-2α and 1-α depending on the maximum weight. For data-dependent weights from routing networks, a correction factor m* = sup_δ[F_Pall(δ)/δ] is estimated from a merging set and applied to maintain validity. The approach uses split conformal as the base method, with calibration data for p-value computation and a separate merging set for m* estimation.

## Key Results
- Weighted aggregation achieves coverage guarantees between 1-2α and 1-α, tighter than standard aggregation when weights are informative
- Data-dependent routing weights enable prediction sets with improved worst-slice coverage compared to marginal methods
- Empirical evaluation shows better coverage consistency across demographic groups in real datasets
- The correction factor m* converges with <160 samples in the merging set, providing practical efficiency

## Why This Works (Mechanism)

### Mechanism 1: Weighted p-value aggregation with improved coverage bounds
- **Claim:** Aggregating p-values with asymmetric weights yields coverage guarantees between 1-2α and 1-α, depending on weight distribution.
- **Mechanism:** The method computes a weighted average of p-value functions: p̄(x,y) = Σv_k·p̂_k(x,y). Per Vovk and Wang [19], the coverage bound depends on the maximum weight v = max{v_1,...,v_K}: coverage ≥ 1 - min{1/v, 2}·α. When one predictor dominates (v > 1/2), the guarantee exceeds 1-2α; when v = 1, it recovers 1-α.
- **Core assumption:** Data-independent weights (fixed a priori or expert-assigned); p-values may have arbitrary dependence.
- **Evidence anchors:**
  - [abstract] "achieving tighter coverage bounds that interpolate between the 1-2α guarantee of the combined models and the 1-α guarantee of an individual model depending on the distribution of weights"
  - [Section 4, Prop. 4.1] Formal statement of the coverage guarantee with weight-dependent bound
  - [corpus] Neighbor paper "Aggregating Conformal Prediction Sets via α-Allocation" addresses similar aggregation challenges but via allocation strategies rather than p-value weighting
- **Break condition:** When weights are symmetric (v_k ≈ 1/K), the bound reverts to 1-2α with no improvement over standard aggregation.

### Mechanism 2: Correction factor m* for data-dependent weights
- **Claim:** A linear scaling transformation restores valid p-variable status for weighted averages with data-dependent weights.
- **Mechanism:** When weights W depend on data, P_all = ΣW_k·P_k is not a valid p-variable. The method learns m* = sup_δ[F_Pall(δ)/δ] from a merging set S_merge, then uses m*·p_all as the thresholded p-variable. This satisfies P{m*·P_all ≤ α} ≤ α for all α ∈ (0,1).
- **Core assumption:** A designated merging set S_merge is available; exchangeability holds across calibration, merge, and test data.
- **Evidence anchors:**
  - [Section 5, Prop. 5.1] "The scaling defined in (7) transforms the weighted average P_all into a valid p-variable, recovering a coverage guarantee"
  - [Section 5, Eq. 7-9] Definition of m* and its empirical estimator m̂*
  - [corpus] Limited direct corpus evidence; most related work (e.g., "Class conditional conformal prediction for multiple inputs") assumes data-independent aggregation
- **Break condition:** If |S_merge| is too small, the empirical CDF F̂_Pall is noisy, causing m̂* to be overly conservative (overcoverage) or unstable.

### Mechanism 3: Adaptive local coverage via data-dependent routing
- **Claim:** Data-dependent weights from routing networks enable prediction sets with improved worst-slice coverage compared to marginal methods.
- **Mechanism:** In MoE, routing network outputs W(x) adaptively weight experts per input. The weighted p-value p_all(X_{n+1}, y; W^{(n+1)}) naturally up-weights experts relevant to the test point. After m* correction, the prediction set achieves better coverage on difficult slices because experts suited to those regions contribute more.
- **Core assumption:** Routing weights meaningfully reflect expert relevance; the relationship between input features and expert quality is learnable.
- **Evidence anchors:**
  - [Section 6] "data-dependent weights allow the influence of each model to be adjusted based on how well it performs for a specific data point... a form of locally conditional coverage"
  - [Fig. 3] WA methods achieve WS coverage closer to nominal while split conformal undercovers on WS slabs
  - [corpus] "Colorful Pinball" and "WQLCP" address adaptive coverage via different mechanisms (density-weighted quantiles, weighted adaptive CP under distribution shift)
- **Break condition:** If routing weights are uninformative (e.g., uniform or mis-specified), adaptivity degrades to marginal behavior without improving local validity.

## Foundational Learning

- **Concept: Split conformal prediction**
  - **Why needed here:** The paper builds on split conformal as the base method for generating individual prediction sets and p-value functions. Understanding the calibration/proper training split and quantile construction is essential.
  - **Quick check question:** Given calibration scores R_1,...,R_n and significance level α = 0.1, what is the prediction set threshold Q̂⁺_α?

- **Concept: p-variables and p-values**
  - **Why needed here:** The core technical contribution reformulates conformal prediction via p-value functions and relies on Vovk and Wang's p-variable averaging results. The distinction between conservative and precise p-variables matters for understanding coverage tightness.
  - **Quick check question:** If P is a p-variable, what inequality must it satisfy for all α ∈ (0,1)?

- **Concept: Coverage types (marginal, conditional, worst-slice)**
  - **Why needed here:** The paper claims improved local validity via WS coverage. Distinguishing marginal guarantees (true for any test point averaged over distribution) from conditional guarantees (true for specific X values) clarifies what the method can and cannot achieve.
  - **Quick check question:** Why is distribution-free X-conditional coverage impossible with finite samples?

## Architecture Onboarding

- **Component map:**
  Training data → S_train (fit predictors) + S_cal (compute p-value functions p̂_k) + S_merge (estimate m̂*)
  
  At inference:
  1. For test input X_{n+1}: routing network → weights W^{(n+1)}
  2. Compute p_all(X_{n+1}, y; W^{(n+1)}) = Σ W_k · p̂_k(X_{n+1}, y) for candidate y
  3. Threshold: Ĉ_α(X_{n+1}) = {y : m̂* · p_all(X_{n+1}, y; W^{(n+1)}) > α}

- **Critical path:**
  1. Partition data into train/cal/merge splits (avoid data leakage)
  2. Fit K predictors and routing network on S_train
  3. Compute p-value functions p̂_k on S_cal for each predictor
  4. Compute m̂* from S_merge using empirical CDF of weighted p-values (Eq. 9)
  5. At test time, compute weighted aggregate p-value and apply correction

- **Design tradeoffs:**
  - **Merge set size |S_merge|:** Larger sets → more precise m̂* but require more data. Paper finds ~160 samples sufficient for <3% overcoverage (Fig. 4).
  - **WA variant:** "WA targeted (0, α']" guarantees coverage for α ∈ (0, α'] (more conservative); "WA precise α'" guarantees only at α = α' (tighter coverage, weaker guarantee).
  - **Score function:** Can combine with CQR scores for additional adaptivity, trading simplicity for improved local coverage.

- **Failure signatures:**
  - **Excessive overcoverage:** Likely due to small |S_merge| or overly conservative m̂* estimation. Check empirical CDF stability.
  - **Undercoverage on merge set:** Indicates exchangeability violation or implementation error in m̂* computation.
  - **No improvement over split conformal on WS coverage:** Routing weights may be uninformative; verify that different experts have meaningfully different performance across input regions.

- **First 3 experiments:**
  1. **Synthetic ablation (varying |S_merge|):** Replicate Fig. 4 to validate m̂* stability on your data scale. Start with |S_merge| ∈ {40, 80, 160, 320}.
  2. **Feature overlap study:** Replicate Fig. 5 to understand how expert feature sharing affects coverage and efficiency in your domain. Compare "no overlap" vs. "share 1/2" vs. "features 15/16" configurations.
  3. **Real data WS coverage comparison:** On a dataset with known heterogeneous difficulty (e.g., Communities with demographic subgroups), compare split conformal, CQR, and WA variants. Verify that WA achieves smaller Δ coverage (gap between marginal and WS coverage) as in Fig. 10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of the merging set $|S_{merge}|$ to balance coverage accuracy and data efficiency?
- Basis in paper: [inferred] The paper notes that "even a modest number of samples (<200) is sufficient for accurate coverage" and Figure 4 shows coverage improves with larger $|S_{merge}|$, but no theoretical guidance is provided for selecting this hyperparameter.
- Why unresolved: The paper empirically demonstrates the trade-off but does not derive sample complexity bounds or optimal allocation between calibration and merging sets.
- What evidence would resolve it: A theoretical analysis of how the correction factor $\hat{m}^*$ converges to $m^*$ as a function of $|S_{merge}|$, or empirical validation across diverse dataset sizes and noise levels.

### Open Question 2
- Question: How does weighted p-value aggregation perform with nonlinear routing networks and deep expert models, beyond the linear models tested?
- Basis in paper: [explicit] "In our experiments... we use linear models for both the experts and the routing networks. For split conformal, we use the weighted sum of experts as a standard black-box predictor."
- Why unresolved: The authors restrict experiments to linear models "for simplicity," leaving open whether the theoretical guarantees and empirical benefits transfer to complex, nonlinear architectures common in modern MoE systems.
- What evidence would resolve it: Experiments applying the method to transformer-based MoE models or deep neural network experts with learned routing.

### Open Question 3
- Question: Can tighter theoretical bounds be derived for the correction factor $m^*$ when the distribution of weights or structure of expert disagreement is known?
- Basis in paper: [inferred] The paper derives $m^* = \sup_{\delta > 0} F_{P_{all}}(\delta)/\delta$ but relies on empirical estimation; Proposition 5.2 provides finite-sample guarantees via DKW but these may be loose.
- Why unresolved: The worst-case DKW correction ($\epsilon$) is noted as "extremely conservative in practice" in Appendix C.4 when discussing ECDF-DKW, suggesting room for tighter analysis.
- What evidence would resolve it: Derivation of problem-dependent bounds that exploit structure (e.g., bounded weight variance, expert independence assumptions) with empirical validation showing tighter coverage.

### Open Question 4
- Question: What is the computational and statistical cost of weighted aggregation as the number of experts $K$ scales to very large ensembles?
- Basis in paper: [inferred] The complexity analysis (Appendix D) states $O(KLN)$ but does not empirically study scaling behavior or whether the correction factor $m^*$ degrades with large $K$.
- Why unresolved: Modern MoE systems may have hundreds or thousands of experts; whether coverage guarantees and efficiency scale gracefully remains untested.
- What evidence would resolve it: Scaling experiments varying $K$ from small to large values (e.g., 4 to 100+ experts), measuring coverage, prediction set size, and computational cost.

## Limitations

- Theoretical guarantees are weaker for data-dependent weights, only proven at specified significance level or interval rather than uniformly over all α
- Empirical validation scope is limited to synthetic data and UCI datasets with moderate dimensionality, not high-dimensional problems or severe covariate shift
- Worst-slice coverage metric is heuristic with no rigorous statistical testing of significance between methods

## Confidence

- **High confidence**: Coverage bounds for data-independent weights (Section 4), m* correction mechanism for data-dependent weights (Section 5), and basic experimental methodology (Section 6)
- **Medium confidence**: Empirical improvements in WS coverage (Fig. 3, 10) given the heuristic nature of the metric and limited statistical validation
- **Low confidence**: The relationship between merge set size and coverage stability (Fig. 4) requires more extensive evaluation across different data regimes

## Next Checks

1. **Statistical significance testing**: Apply paired t-tests or bootstrap confidence intervals to coverage improvements reported in Figures 3 and 10 to establish whether observed differences are statistically significant.

2. **Scaling study for m* estimation**: Systematically vary |S_merge| across multiple orders of magnitude (e.g., 40, 80, 160, 320, 640) on a single dataset to verify the claimed convergence behavior and identify the minimum sufficient sample size for stable correction factor estimation.

3. **Expert feature overlap sensitivity**: Replicate the feature overlap study (Fig. 5) with additional intermediate overlap levels and more diverse expert architectures (e.g., nonlinear experts) to better characterize the tradeoff between coverage tightness and prediction set efficiency.