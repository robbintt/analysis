---
ver: rpa2
title: 'ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over
  Knowledge Graphs'
arxiv_id: '2602.02382'
source_url: https://arxiv.org/abs/2602.02382
tags:
- reasoning
- query
- logical
- complex
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROG, a retrieval-augmented LLM reasoning
  framework for answering first-order logic queries over incomplete knowledge graphs.
  ROG addresses the challenge of complex logical reasoning by decomposing queries
  into single-operator sub-queries and grounding each step in query-relevant neighborhood
  evidence retrieved from the KG.
---

# ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs

## Quick Facts
- arXiv ID: 2602.02382
- Source URL: https://arxiv.org/abs/2602.02382
- Reference count: 12
- Outperforms embedding-based baselines by up to 35% MRR on complex FOL queries over incomplete KGs

## Executive Summary
ROG introduces a retrieval-augmented LLM reasoning framework that answers first-order logic queries over incomplete knowledge graphs by decomposing complex queries into single-operator sub-queries and grounding each step in compact, query-relevant neighborhood evidence. The method uses entity/relation abstraction to force reasoning from retrieved evidence rather than parametric memorization, caches intermediate answer sets to stabilize multi-step inference, and achieves consistent MRR gains over strong embedding-based baselines on standard benchmarks like FB15k and NELL995, particularly on high-complexity and negation-heavy query types.

## Method Summary
ROG parses FOL queries into deterministic execution plans, decomposes them into single-operator sub-queries, and retrieves compact k-hop neighborhoods filtered by query-relevant relations. Each sub-query is answered by an LLM (ChatGLM) using prompts that serialize evidence with abstract entity/relation IDs, forcing reasoning from provided facts rather than memorized knowledge. Intermediate answer sets are cached and injected as placeholders into subsequent prompts, preventing re-computation errors. The final answer sets are de-abstracted to surface entity names for evaluation.

## Key Results
- Achieves up to 35% MRR improvement over embedding-based baselines on complex FOL queries
- Consistent gains across query types, especially on negation-heavy queries (2in, 3in, inp, pin, pni)
- Outperforms GQE and Query2Box on FB15k and NELL995 benchmarks while avoiding learned embedding training

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition with Intermediate Caching
Decomposing multi-operator queries into single-operator sub-queries with explicit caching reduces compounding errors in deep reasoning chains. Each step produces an intermediate answer set that is materialized, cached by step type and arguments, and injected into subsequent prompts via placeholders—preventing re-derivation of earlier results.

### Mechanism 2: Query-Aware Neighborhood Retrieval
Restricting LLM context to compact, relation-filtered neighborhoods improves grounding and reduces distractor-induced errors. For each sub-query, k-hop expansion retrieves triples around grounded entities, prioritizing edges whose relations appear in the query, with truncation to fit context windows.

### Mechanism 3: Entity/Relation Abstraction to Identifiers
Replacing surface-form names with abstract identifiers forces reasoning from retrieved evidence rather than parametric memorization. All entity and relation names are mapped to unique IDs, removing semantic cues the LLM could use to hallucinate from pretrained knowledge.

## Foundational Learning

- **Concept: First-Order Logic Operators over KGs (P, ∧, ∨, ¬)**
  - Why needed here: ROG's decomposition relies on understanding operator semantics—projection follows relations, intersection/union combine sets, negation subtracts.
  - Quick check question: Given entity set {A, B} and relation r, what set does P(r, {A, B}) represent?

- **Concept: k-Hop Neighborhood Expansion**
  - Why needed here: Retrieval granularity directly controls evidence quality. Understanding what 1-hop vs 2-hop returns is essential for tuning retrieval and diagnosing sparse evidence failures.
  - Quick check question: If entity E has 3 outgoing edges, how many triples maximum appear in its 1-hop neighborhood?

- **Concept: Chain-of-Thought Prompting with State Accumulation**
  - Why needed here: ROG is fundamentally sequential inference where each prompt references prior outputs. Understanding CoT helps debug where reasoning diverges.
  - Quick check question: Why does caching intermediate sets improve consistency compared to asking the LLM to recompute them?

## Architecture Onboarding

- **Component map:**
  KG Store (Neo4j) -> Query Parser -> Decomposition Engine -> Neighborhood Retriever -> Prompt Constructor -> LLM (ChatGLM) -> Answer Cache -> Output Parser

- **Critical path:**
  1. Parse FOL query -> generate deterministic execution plan
  2. For each sub-query: retrieve filtered neighborhood -> serialize to prompt -> invoke LLM -> parse output set -> cache
  3. Later sub-queries reference cached sets by placeholder
  4. Final answer set assembled and de-abstracted to entity names

- **Design tradeoffs:**
  - Neighborhood size vs. context noise: larger k-hop captures more evidence but increases distractors
  - Decomposition granularity: finer decomposition = more LLM calls but simpler per-step tasks
  - Multi-agent consensus: improves robustness on ambiguous queries but multiplies inference cost

- **Failure signatures:**
  - Empty intermediate sets (LLM outputs NONE) -> evidence may be insufficient or truncation too aggressive
  - Entities in output not present in retrieved evidence -> abstraction may have failed or LLM ignored constraints
  - Negation errors magnified -> inspect "to-be-removed" set computation step

- **First 3 experiments:**
  1. Validate 1p baseline: Run single-projection queries through full pipeline; verify retrieval returns expected triples and LLM output matches ground truth.
  2. Test cache propagation: Run a 2p query; manually inspect that step-1 answer set appears correctly in step-2 prompt placeholder.
  3. Probe negation handling: Run a negation-heavy query (e.g., 2in); check that subtraction logic correctly removes entities from the candidate pool.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What retrieval policies can optimize the trade-off between context compactness and evidence sufficiency to minimize LLM hallucination without losing answer recall?
- Basis in paper: [explicit] The Conclusion states future work includes "more efficient retrieval policies."
- Why unresolved: The current method relies on k-hop expansion and simple truncation, which may either include distractors or exclude necessary distant evidence.
- What evidence would resolve it: Ablation studies comparing semantic similarity-based pruning versus structural connectivity heuristics on complex multi-hop queries.

### Open Question 2
- Question: How can symbolic execution plans be dynamically coupled with prompting to allow for plan revision during inference?
- Basis in paper: [explicit] The Conclusion proposes "tighter coupling between symbolic execution plans and prompting" as future work.
- Why unresolved: ROG currently uses a deterministic, static execution order; if an intermediate step fails or yields low confidence, the system lacks a mechanism to revise the plan.
- What evidence would resolve it: Implementation of a feedback loop where LLM uncertainty scores trigger query re-decomposition or re-retrieval, improving accuracy on ambiguous queries.

### Open Question 3
- Question: How does the query-time latency of ROG compare to embedding-based baselines, and can it be optimized for real-time applications?
- Basis in paper: [inferred] The framework shifts complexity from training to "query-time reasoning" (Section 2), utilizing sequential LLM calls and database retrieval.
- Why unresolved: While effective, the chain-of-thought decomposition implies significantly higher inference costs than single-pass embedding lookups, potentially limiting deployment in latency-sensitive environments.
- What evidence would resolve it: Detailed latency benchmarks (ms/query) on standard hardware comparing ROG against GQE and Query2Box across varying query depths.

## Limitations
- Dependence on retrieved neighborhood quality—sparse or noisy KGs may yield insufficient evidence for correct LLM outputs
- Runtime cost scales with query depth and neighborhood retrieval overhead, limiting web-scale deployment
- Current design does not address reasoning over dynamic or streaming KGs

## Confidence
- **High confidence**: Query decomposition with intermediate caching reduces compounding errors; entity/relation abstraction improves portability and reduces hallucination; MRR gains are statistically significant vs. embedding baselines.
- **Medium confidence**: Neighborhood retrieval prioritization strategy meaningfully improves grounding; multi-agent consensus variant provides robustness on ambiguous queries.
- **Low confidence**: Abstraction technique robustness across heterogeneous KGs; scalability of retrieval overhead in production scenarios.

## Next Checks
1. Probe retrieval sufficiency: Systematically vary k-hop depth and triple cap; measure impact on MRR for each query type to identify thresholds where evidence becomes insufficient.
2. Stress-test abstraction: Replace the fixed ID scheme with a randomized mapping per run; verify that output consistency and correctness are preserved, confirming reasoning depends on evidence, not memorized associations.
3. Benchmark against state-of-the-art: Compare ROG's performance on negation-heavy query types (2in, 3in, inp, pin, pni) against Neural-Symbolic Message Passing and recent embedding-based methods on the same benchmarks to quantify relative gains.