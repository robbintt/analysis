---
ver: rpa2
title: Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness
  Evaluation
arxiv_id: '2511.18958'
source_url: https://arxiv.org/abs/2511.18958
tags:
- graph
- reward
- learning
- robustness
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cutter, a dual-agent reinforcement learning
  framework for graph compression that preserves structural robustness. The method
  addresses the challenge of evaluating large graphs under adversarial attacks, which
  is computationally expensive.
---

# Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation

## Quick Facts
- arXiv ID: 2511.18958
- Source URL: https://arxiv.org/abs/2511.18958
- Reference count: 12
- This paper introduces Cutter, a dual-agent reinforcement learning framework for graph compression that preserves structural robustness while significantly improving evaluation efficiency.

## Executive Summary
This paper addresses the challenge of evaluating large graphs under adversarial attacks, which is computationally expensive. Cutter is a dual-agent reinforcement learning framework that collaboratively identifies structurally vital and redundant nodes for guided compression. The method preserves robustness degradation patterns under various attacks while achieving significant compression ratios. Experiments on five real-world datasets demonstrate that Cutter achieves high RPS (Robustness Preservation Similarity) scores, outperforming baselines even under aggressive compression, with RPS of 0.832 at 0.1 compression ratio on Cora.

## Method Summary
Cutter uses a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA) to collaboratively identify critical and redundant nodes for compression. The framework incorporates three key strategies: trajectory-level reward shaping with affine alignment, prototype-based shaping using contrastive learning from high- and low-return trajectories, and cross-agent active-follow exploration where agents alternate leader-follower roles. The dual agents share a GCN encoder and operate through task-specific sub-encoders to predict Q-values for node removal decisions, with rewards designed to preserve robustness profiles while removing redundancy.

## Key Results
- Cutter achieves RPS of 0.832 at 0.1 compression ratio on Cora dataset
- Outperforms baselines even under aggressive compression ratios
- Significantly improves evaluation efficiency without compromising assessment fidelity
- Maintains high RPS scores across five different real-world datasets

## Why This Works (Mechanism)

### Mechanism 1: Dual-Agent Role Specialization with Asymmetric Objectives
Using two specialized agents—one identifying vital nodes (VDA), another identifying redundant nodes (RDA)—enables compression while preserving robustness profiles better than single-agent approaches. VDA maximizes connectivity degradation to identify critical nodes while RDA minimizes penalties combining connectivity loss, vital-node deletion, and embedding distortion. The asymmetric but coupled objectives create complementary learning signals.

### Mechanism 2: Trajectory-Level Reward Shaping with Affine Return Alignment
Dense step-wise rewards derived from trajectory-level supervision improve learning efficiency while preserving optimal policy equivalence. A neural reward network predicts step-wise rewards that are aligned to true return scale via affine transformation, ensuring predicted rewards span the same range as true returns while preserving policy invariance.

### Mechanism 3: Prototype-Based Contrastive Shaping for Critical Decision Points
Extracting behavioral prototypes from trajectory "phase transitions" (abrupt connectivity changes) provides fine-grained guidance that trajectory-level rewards miss. The method encodes n-step context windows using GRU to form positive and negative prototypes, rewarding new decisions based on cosine similarity to positive minus similarity to negative prototypes.

### Mechanism 4: Cross-Agent Active-Follow Exploration
Alternating leader-follower roles between agents enables safer exploration and transfers structural priors across asymmetric tasks. The framework alternates between VDA leading and RDA following (with top-15% importance set), then reverses, storing trajectories in both agents' buffers for mutual learning.

## Foundational Learning

- **Concept: Markov Decision Processes and Deep Q-Networks**
  - Why needed here: Graph compression is formulated as sequential node removal requiring understanding of MDP tuples, Q-functions, and experience replay
  - Quick check question: Given the Bellman equation Q*(s,a) = R(s,a) + γ·max_a' Q*(s',a'), why does the paper use separate Q-networks for VDA and RDA instead of a shared Q-network?

- **Concept: Graph Convolutional Networks (GCN) for Node Embedding**
  - Why needed here: The architecture uses H^(0) = σ(AXW) as the shared encoder, requiring understanding of how adjacency matrix multiplication propagates neighborhood information
  - Quick check question: If node features X are all-ones (no semantic information), what structural properties does the operation AX capture that enables the encoder to distinguish nodes?

- **Concept: Reward Shaping and Policy Invariance**
  - Why needed here: The paper's theoretical foundation relies on preserving optimal policies while adding dense intermediate rewards
  - Quick check question: Why does the affine transformation R_affine(x) = αx + β preserve the optimal policy when α > 0? What would happen if α < 0?

## Architecture Onboarding

- **Component map**: Input: A (adjacency), X (all-ones features) → Shared Encoder: f_shared → H^(0), z^(0) → Task-Specific Encoders: f_vda, f_rda → H_vda^(ℓ), z_vda / H_rda^(ℓ), z_rda → Q_vda decoder / Q_rda decoder → VDA actions / RDA actions → Reward Networks: R_vda_ϕ, R_rda_ϕ → Prototype Module: GRU encoder → h_pos, h_neg

- **Critical path**: Graph encoding through shared + task-specific encoders → Q-value computation for all candidate nodes → ε-greedy action selection and node removal → Reward network evaluation and buffer storage → Cross-agent trajectory sharing (Phase I ↔ Phase II alternation) → Batch training: Q-networks, reward networks, prototype encoder

- **Design tradeoffs**: Shared vs. task-specific encoders (shared first layer reduces parameters but may limit task specialization); prototype window size n (larger n captures more context but may dilute critical decision signal); importance set threshold (15%) affects RDA's exploration freedom; cross-agent vs. independent training adds synchronization complexity but enables transfer

- **Failure signatures**: Reward network divergence (L_reward plateaus or increases); VDA under-exploration (P_conn remains low across episodes); RDA vital-node deletion (P_delete high); Prototype collapse (h_pos and h_neg converge); Compression ratio drift (actual ρ deviates from target)

- **First 3 experiments**:
  1. Reward shaping ablation: Train RDA with/without reward network on Cora; measure average return over training episodes
  2. Cross-agent transfer quantification: Train three configurations (VDA-only, RDA-only, full Cutter) and evaluate RPS at different compression ratios
  3. Prototype sensitivity analysis: Vary K and n values on PubMed; plot RPS vs. hyperparameters to identify optimal operating points

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Cutter be effectively extended to graphs with rich node and edge attributes while preserving robustness profiles? The current framework operates on topology-only graphs with uniform all-one feature matrices, deliberately isolating structural compression. The shared encoder architecture uses only adjacency information, and the reward functions do not account for attribute preservation.

- **Open Question 2**: How does Cutter's robustness preservation generalize to attack strategies beyond node removal, such as edge-level perturbations or adversarial edge additions? The evaluation exclusively uses node removal attacks based on centrality measures and learning-based node ranking. Real-world adversarial scenarios may involve edge manipulations.

- **Open Question 3**: What is the computational break-even point where the cost of training Cutter's dual-agent framework is offset by downstream robustness evaluation savings? The paper claims improved "evaluation efficiency" but provides no analysis of training overhead, including wall-clock time, memory requirements, and number of episodes needed for convergence.

## Limitations

- Critical hyperparameters are missing, preventing exact reproduction (hidden dimensions, learning rates, weight values, etc.)
- Limited ablation studies leave relative contribution of three mechanisms unclear
- Prototype-based shaping relies on behavioral patterns that may not generalize across graph types
- Cross-agent transfer assumes asymmetric objectives share sufficient structural signal

## Confidence

- **High**: RPS metric validity (directly measured), compression ratio implementation, dual-agent framework architecture
- **Medium**: Trajectory-level reward shaping effectiveness (based on formal proof), cross-agent exploration benefit (ablation shows improvement)
- **Low**: Prototype-based shaping generalization (weak corpus support), relative contribution of three components (no individual ablations)

## Next Checks

1. Implement hyperparameter sensitivity analysis for ω₁/ω₂/ω₃ weights and K/n values to identify robust operating points
2. Conduct ablation study isolating trajectory-level reward shaping, prototype-based shaping, and cross-agent exploration contributions
3. Validate prototype encoder behavior by visualizing h_pos/h_neg distributions across trajectory types to ensure they capture meaningful behavioral distinctions