---
ver: rpa2
title: 'CLASH: A Benchmark for Cross-Modal Contradiction Detection'
arxiv_id: '2511.19199'
source_url: https://arxiv.org/abs/2511.19199
tags:
- conflict
- caption
- object
- question
- conflicting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLASH addresses the critical gap in evaluating multimodal models'
  ability to detect contradictions across visual and textual inputs. While existing
  benchmarks assume input consistency or treat one modality as authoritative, CLASH
  tests spontaneous conflict detection in dual ground truth scenarios where both modalities
  are valid but contradictory.
---

# CLASH: A Benchmark for Cross-Modal Contradiction Detection

## Quick Facts
- arXiv ID: 2511.19199
- Source URL: https://arxiv.org/abs/2511.19199
- Reference count: 40
- Primary result: Leading closed-source models achieve over 85% conflict detection accuracy while most open-source models struggle with near-zero detection rates.

## Executive Summary
CLASH addresses the critical gap in evaluating multimodal models' ability to detect contradictions across visual and textual inputs. While existing benchmarks assume input consistency or treat one modality as authoritative, CLASH tests spontaneous conflict detection in dual ground truth scenarios where both modalities are valid but contradictory. The benchmark features controlled object- and attribute-level contradictions in MS COCO images paired with modified captions, accompanied by targeted questions in multiple-choice and open-ended formats.

The key finding reveals a stark performance divide: leading closed-source models (GPT-5, Gemini 2.5 Pro) achieve over 85% conflict detection accuracy, while most open-source models struggle with near-zero detection rates. Systematic modality biases emerge across architectures, with some models favoring text over images and vice versa. Category-specific analysis shows environmental attributes are most challenging, while colors are more easily detected. Most significantly, targeted fine-tuning on CLASH dramatically improves conflict detection capabilities, with models like LLaVa-1.5-7b showing improvements from 0% to 77% accuracy.

## Method Summary
CLASH uses MS COCO images paired with synthetically modified captions containing controlled object-level or attribute-level contradictions. The dataset includes ~15k filtered training samples and 1289 human-verified test samples (655 object-level, 634 attribute-level). Questions target the contradictory element, with answers in multiple-choice (4 options) and open-ended formats. Models are evaluated using bootstrap-estimated conflict detection accuracy and modality bias metrics. LoRA fine-tuning with specific hyperparameters is used to improve conflict detection capabilities, with quality control through automated validation and human verification.

## Key Results
- Closed-source models (GPT-5: 86.78%, Gemini 2.5 Pro: 88.48%) achieve over 85% conflict detection accuracy, while open-source models struggle with near-zero detection rates
- Systematic modality biases emerge: LLaVa-OneVision-7b shows 81.45% image preference, while mPLUG-Owl-2 shows 97.03% text bias
- Environmental attributes are most challenging (Gemini 2.5 Pro: 66.58%), while colors are more easily detected (93.50%)
- Targeted fine-tuning on CLASH dramatically improves conflict detection, with LLaVa-1.5-7b improving from 0% to 77% accuracy

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Exposure Training
- **Claim:** Targeted fine-tuning on controlled contradictions enables models to learn cross-modal comparison as a distinct capability
- **Mechanism:** By presenting image-text pairs with precisely one contradicting element, models receive dense supervision for the "compare and flag" operation
- **Evidence:** LLaVa-1.5-7b improves from 0.00% to 76.86% with filtered training data; fine-tuning on 15k filtered data outperforms 30k unfiltered data

### Mechanism 2: Modality Bias Quantification Through Forced Choice
- **Claim:** Systematic biases emerge when models must choose between equally valid but contradictory sources
- **Mechanism:** The four-option design forces models to commit, exposing default preferences when cross-modal comparison is insufficient
- **Evidence:** LLaVa-OneVision-7b shows 81.45% image preference while mPLUG-Owl-2 shows 97.03% text bias; closed-source models show minimal bias

### Mechanism 3: Difficulty Hierarchy via Attribute Type
- **Claim:** Conflict detection difficulty correlates with attribute perceptual/conceptual complexity
- **Mechanism:** Colors are directly observable and unambiguous; environmental attributes require context integration and may have fuzzy boundaries
- **Evidence:** Environmental attributes prove most challenging (66.58% for Gemini 2.5 Pro) while colors are more easily detected (93.50%)

## Foundational Learning

- **Dual Ground Truth Paradigm**:
  - **Why needed here:** CLASH assumes both image and text are valid sources, unlike traditional VQA or hallucination benchmarks
  - **Quick check question:** When a caption says "blue car" but the image shows a red car, should the model (a) trust the image, (b) trust the text, or (c) flag the conflict?

- **Modality Bias vs. Cross-Modal Reasoning**:
  - **Why needed here:** The paper distinguishes between bias (systematic preference) and reasoning failure
  - **Quick check question:** If InternVL 1.5 answers text-grounded 52.40% of the time on CLASH, is this a vision failure or a training bias?

- **Conflict Detection vs. Hallucination Detection**:
  - **Why needed here:** Hallucination benchmarks test whether models generate false content; CLASH tests whether models recognize when authoritative sources disagree
  - **Quick check question:** Why is POPE (object presence evaluation) insufficient for evaluating CLASH-style contradictions?

## Architecture Onboarding

- **Component map:** Caption modification -> Question generation -> Answer generation -> Quality control
- **Critical path:** Caption modification quality → question relevance → answer distinctness → human verification
- **Design tradeoffs:** Synthetic contradictions enable controlled experiments but may not reflect real-world complexity; object/attribute vs. spatial/relational conflicts; multiple-choice vs. open-ended formats
- **Failure signatures:** High "Incorrect" rate indicates instruction-following failure; near-zero conflict detection with strong modality preference indicates bias-dominated architecture; performance drop on general benchmarks indicates overfitting
- **First 3 experiments:**
  1. Run your model on the 1289-sample diagnostic set to establish conflict detection rate and modality bias profile
  2. Compare filtered (15k) vs. raw (30k) training data to verify quality-over-quantity finding
  3. Analyze performance on environmental attributes vs. colors to identify category-specific weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do conflict detection capabilities learned through fine-tuning on object- and attribute-level contradictions transfer to more complex contradiction types such as spatial, relational, temporal, or causal conflicts?
- **Basis:** The authors state they focus on object- and attribute-level contradictions rather than spatial, relational, temporal or causal conflicts
- **Evidence needed:** Fine-tuning models on CLASH and evaluating on benchmarks containing spatial/relational/temporal conflicts

### Open Question 2
- **Question:** What architectural or training factors—beyond model scale—enable robust cross-modal conflict detection?
- **Basis:** The authors observe that larger models do not consistently improve conflict detection and that scale alone does not guarantee better performance
- **Evidence needed:** Controlled ablation studies varying one factor at a time (architecture, training objectives, data composition)

### Open Question 3
- **Question:** Can conflict detection capabilities learned from synthetic everyday scenes in MS COCO transfer to specialized domains such as medical imaging, scientific figures, or technical documentation?
- **Basis:** The authors state extending to specialized domains like medical, scientific, or technical content remains an important direction
- **Evidence needed:** Zero-shot or few-shot evaluation of CLASH-fine-tuned models on contradiction detection benchmarks from specialized domains

## Limitations
- Synthetic vs. natural contradictions: CLASH relies on LLM-generated contradictions rather than naturally occurring inconsistencies
- Closed-source model dependence: Performance comparisons heavily rely on GPT-5 and Gemini 2.5 Pro results that cannot be independently verified
- Category-specific generalization: The difficulty hierarchy is observed but not explained mechanistically

## Confidence
- **High Confidence:** Targeted fine-tuning enables learning of cross-modal comparison; systematic modality biases emerge when forced to choose; performance improves with high-quality filtered training data
- **Medium Confidence:** Difficulty hierarchy across attribute types correlates with complexity; spatial conflict detection is harder than object/attribute detection; false positive rates remain low
- **Low Confidence:** Closed-source model performance levels; generalization of attribute difficulty patterns; long-term effectiveness of fine-tuning without degradation

## Next Checks
1. Evaluate CLASH models on naturally occurring multimodal contradictions from real-world sources to assess whether synthetic training generalizes to authentic conflicts
2. Systematically vary the number and types of contradictory attributes in test samples to determine whether difficulty hierarchy reflects architectural limitations or data distribution effects
3. Investigate whether conflict detection capability can be maintained while simultaneously training on general multimodal tasks, addressing the performance degradation observed with current targeted fine-tuning approaches