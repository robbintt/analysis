---
ver: rpa2
title: 'MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended
  QA Evaluation with LLMs'
arxiv_id: '2506.15215'
source_url: https://arxiv.org/abs/2506.15215
tags:
- answer
- question
- evaluation
- factoid
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MinosEval introduces a novel evaluation framework for open-ended
  question answering that addresses the limitations of existing methods by distinguishing
  between factoid and non-factoid questions. For factoid questions, it employs an
  adaptive key-point scoring strategy that extracts key information from reference
  answers and uses natural language inference to assess how well model responses capture
  these points.
---

# MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs

## Quick Facts
- **arXiv ID:** 2506.15215
- **Source URL:** https://arxiv.org/abs/2506.15215
- **Reference count:** 26
- **Primary result:** Novel evaluation framework achieving superior alignment with human annotations compared to traditional metrics and LLM-based evaluation methods.

## Executive Summary
MinosEval introduces a novel evaluation framework for open-ended question answering that addresses the limitations of existing methods by distinguishing between factoid and non-factoid questions. For factoid questions, it employs an adaptive key-point scoring strategy that extracts key information from reference answers and uses natural language inference to assess how well model responses capture these points. For non-factoid questions, it applies an instance-aware listwise ranking strategy that generates silver answer instances at different quality levels to improve ranking performance. Experiments across four datasets, including two self-built ones, demonstrate that MinosEval achieves superior alignment with human annotations compared to traditional metrics and LLM-based evaluation methods. The approach provides more interpretable results through clear evaluation guidelines and shows better cost-effectiveness than specialized factoid/non-factoid evaluation methods while maintaining robustness across different datasets.

## Method Summary
MinosEval is a novel evaluation framework for open-ended QA that first classifies each question-answer pair as factoid or non-factoid using an LLM-based classifier. For factoid questions, it extracts key points from reference answers using an LLM and scores candidate responses by calculating entailment scores between responses and each key point using an NLI model. The final score is the average of these NLI scores. For non-factoid questions, it generates five exemplar answers (from "Excellent" to "Bad") as silver instances, then uses these to guide an LLM ranker in ordering candidate responses. The framework is designed to maximize alignment with human annotations through tailored evaluation strategies for each question type.

## Key Results
- Achieved superior Spearman's Rho and Kendall's Tau correlation with human annotations compared to traditional metrics and LLM-based evaluation methods
- Demonstrated 97% classification accuracy for factoid/non-factoid detection in few-shot settings
- Showed improved cost-effectiveness compared to specialized factoid/non-factoid evaluation methods while maintaining robustness across different datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distinguishing between factoid and non-factoid questions allows for applying tailored evaluation strategies, which improves alignment with human judgment.
- **Mechanism:** An LLM-based classifier (fact detection module) uses in-context learning with demonstration examples to label each question-answer pair as factoid or non-factoid. This routing step enables the system to apply a specialized sub-mechanism for each type.
- **Core assumption:** The definitions of factoid (answers contain explicit, verifiable information) and non-factoid (answers are more creative and less constrained) are sufficiently clear for a capable LLM to distinguish with high accuracy.
- **Evidence anchors:**
  - [abstract] "...distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies."
  - [section] "We employ an in-context learning approach to develop a simple yet effective fact detection module..." (3.2.1).
  - [corpus] Related work like "Typed-RAG" and "Long-context Non-factoid Question Answering" also focuses on decomposing or addressing non-factoid questions, supporting the premise that distinct handling is valuable.
- **Break condition:** If the classifier's accuracy drops significantly (e.g., below 90%), the wrong evaluation strategy will be applied, leading to poor rankings (e.g., trying to extract key points from a creative writing prompt). Experiments show ~97% accuracy with few-shot learning on some datasets (Table 6), but performance is lower in zero-shot settings.

### Mechanism 2
- **Claim:** For factoid questions, decomposing the reference answer into key points and using Natural Language Inference (NLI) to score model responses provides a granular, interpretable, and effective evaluation.
- **Mechanism:** The "Adaptive Key-Point Scoring" strategy first uses an LLM to extract a list of key points from the reference answer. Then, an NLI model calculates an entailment score for each candidate response against each key point. The final response score is the average of these NLI scores.
- **Core assumption:** The NLI task, which measures logical entailment between a premise (the model response) and a hypothesis (a key point), is a good proxy for whether a response correctly contains the required factual information.
- **Evidence anchors:**
  - [abstract] "For factoid questions, it employs an adaptive key-point scoring strategy that extracts key points from reference answers and uses natural language inference to assess model responses."
  - [section] The formal definitions in Formulas 1, 2, and 3 describe the scoring function `S_f` based on NLI scores (3.2.2).
  - [corpus] Weak direct evidence. A neighbor paper, "An Empirical Study of Evaluating Long-form Question Answering," critiques n-gram metrics, indirectly supporting semantic-based methods like NLI.
- **Break condition:** This mechanism fails if the key points extracted are incomplete, redundant, or not truly critical. It also assumes the NLI model (e.g., mDeBERTa) generalizes well to the domain of the questions. The paper notes this limitation: "performance of such generalized models may be constrained in more specialized evaluation scenarios."

### Mechanism 3
- **Claim:** For non-factoid questions, providing an LLM ranker with "silver" answer instances of varying quality levels improves the stability and accuracy of the listwise ranking process.
- **Mechanism:** The "Instance-Aware Listwise Ranking" strategy uses an LLM to generate five exemplar answers (from "Excellent" to "Bad") for the given question, based on the reference answer. These silver instances serve as an in-context rubric. The LLM ranker then orders the candidate model responses alongside its understanding of these quality levels.
- **Core assumption:** Exposing the ranking LLM to concrete, generated examples of different answer qualities helps calibrate its internal judgment, reducing positional bias and other ranking inconsistencies.
- **Evidence anchors:**
  - [abstract] "For non-factoid questions, it applies an instance-aware listwise ranking strategy that generates silver answer instances to enhance ranking performance."
  - [section] Formula 4 defines `R_nf` and notes `Ii = generateInstance(qi, ai)` (3.2.3).
  - [corpus] Weak direct evidence. The NTCIR-18 AEOLLM task overview discusses automatic LLM evaluation challenges, which this method attempts to address, but does not directly validate the silver-instance approach.
- **Break condition:** The mechanism is sensitive to the quality and relevance of the generated silver instances. If the instances are poor or not representative of the quality spectrum, they could misguide the ranker. The error analysis identifies "errors from low-quality instances (LIQ)" as a failure mode (Figure B3).

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** It is the core scoring engine for the factoid evaluation branch. Understanding that NLI models output probability scores for entailment, contradiction, and neutrality is essential for interpreting the MinosEval scores.
  - **Quick check question:** Given a candidate response "The event was in 1995" and a key point "The event occurred in the mid-90s," what would an NLI model likely output high probability for?

- **Concept: Question Type Taxonomy (Factoid vs. Non-Factoid)**
  - **Why needed here:** This is the foundational decision point of the entire architecture. Grasping the distinction—factoid questions seek verifiable facts/entities, non-factoid questions seek explanations, opinions, or creative content—is critical for understanding why the pipeline splits.
  - **Quick check question:** Is "Explain the symbolism in *The Great Gatsby*" a factoid or non-factoid question according to the paper's definition?

- **Concept: Silver Instance Generation**
  - **Why needed here:** This is the novel augmentation for the non-factoid ranking branch. One must understand it as a form of synthetic data or in-context example generation used to guide the LLM's judgment, not as ground-truth data.
  - **Quick check question:** Why are the generated answer instances called "silver" and not "gold"?

## Architecture Onboarding

- **Component map:** Input -> Fact Detection Module -> Factoid Path (Key-Point Extraction LLM -> NLI Scoring Model -> Score Aggregator) OR Non-Factoid Path (Silver Instance Generator LLM -> Listwise Ranking LLM) -> Output

- **Critical path:** The path from Input to the Fact Detection Module is the most critical, as a misclassification sends the entire sample down the wrong evaluation branch, likely leading to a meaningless result. Ensuring the robustness of this classifier is paramount.

- **Design tradeoffs:**
  - **LLM for NLI vs. Specialized NLI Model:** The paper uses a specialized model (mDeBERTa) for NLI in the factoid branch, likely for cost and speed, but acknowledges performance limits. A larger LLM could be used at higher cost.
  - **Manual vs. Automatic Classification:** The paper provides results for both ("MinosEval" vs. "MinosEval†"). Automatic is scalable but introduces error; manual is more accurate but not scalable. The default is automatic.
  - **Silver Instance Quality vs. Cost:** Generating more nuanced instances might improve ranking but increases latency and cost for each evaluation query.

- **Failure signatures:**
  - **Factoid Misclassification:** A creative writing prompt is scored on key-point entailment, resulting in nonsensical rankings.
  - **Poor Key Point Extraction:** Essential facts are missed, causing good responses to be scored low.
  - **Uninformative Silver Instances:** Generated instances don't span the quality spectrum, providing poor guidance to the ranker LLM.
  - **LLM Ranker Bias:** Despite instances, the ranker still exhibits positional bias or verbosity bias.

- **First 3 experiments:**
  1. **Validate the Fact Detection Module:** Measure classification accuracy (Zero-shot vs. Few-shot) on a held-out set of manually labeled factoid/non-factoid questions. Goal: Ensure >90% accuracy with the few-shot prompt.
  2. **Ablate the Factoid Scoring Path:** On a dataset of purely factoid questions, compare ranking performance using (a) the full MinosEval factoid path, (b) direct NLI scoring between response and full reference answer (no key-point extraction), and (c) a baseline listwise LLM ranker. Goal: Isolate the contribution of key-point extraction.
  3. **Ablate the Non-Factoid Ranking Path:** On a dataset of purely non-factoid questions, compare ranking performance using (a) the full MinosEval non-factoid path (with silver instances), (b) the same listwise ranker *without* silver instances, and (c) a baseline pairwise LLM evaluator. Goal: Quantify the impact of the instance-aware augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a hybrid strategy combining adaptive key-point scoring and instance-aware listwise ranking improve evaluation performance for questions that straddle the factoid and non-factoid boundary?
- **Basis in paper:** [explicit] The "Limitations" section states, "the boundary between factoid and non-factoid problems can be ambiguous... In such cases, combining the two strategies of MinosEval may be beneficial, offering a potential direction for future research."
- **Why unresolved:** The current architecture enforces a hard classification into one of two clusters ($C_f$ or $C_{nf}$), causing potential error propagation if the question type is ambiguous or exhibits mixed characteristics.
- **What evidence would resolve it:** An experiment using a "soft" or weighted routing mechanism that applies both strategies simultaneously to ambiguous samples, compared against the current hard-switching baseline.

### Open Question 2
- **Question:** To what extent does training a specialized Natural Language Inference (NLI) model for atomic fact entailment improve upon the performance of the generalized mDeBERTa-v3-base-mnli-xnli used in MinosEval?
- **Basis in paper:** [explicit] The authors explicitly note, "We did not train a specialized NLI model, instead relying on the widely-used mDeBERTa... The performance of such generalized models may be constrained in more specialized evaluation scenarios."
- **Why unresolved:** General-purpose NLI models may struggle with the nuance of "key points" extracted from specific reference answers, potentially limiting scoring accuracy in domain-specific factoid QA.
- **What evidence would resolve it:** A comparative study fine-tuning an NLI model on QA-specific entailment tasks and measuring the Kendall's Tau and Spearman's Rho against the current baseline on specialized datasets.

### Open Question 3
- **Question:** Is MinosEval robust to lower-quality "silver instances" generated by weaker or smaller open-source LLMs, or is high-quality instance generation a bottleneck for the non-factoid ranking strategy?
- **Basis in paper:** [inferred] The paper relies on advanced LLMs (GPT-4o) for generating silver instances. The "Robustness and Cost" analysis discusses computational cost, but implies reliance on the base LLM's capability to generate distinct quality levels (Excellent to Bad).
- **Why unresolved:** If the LLM generating the silver instances cannot reliably differentiate between "Good" and "Fair" answers, the listwise ranking alignment for non-factoid questions may degrade.
- **What evidence would resolve it:** An ablation study substituting the instance-generation LLM with smaller models (e.g., Llama-3-8B) and measuring the correlation drop in the final rankings of non-factoid answers.

## Limitations

- The factoid/non-factoid classifier shows significant accuracy drops in zero-shot settings, raising concerns about scalability and generalization
- The reliance on generated silver instances for non-factoid evaluation introduces variability and potential reproducibility issues
- The NLI-based scoring for factoid questions assumes the NLI model generalizes well to specialized domains, which may limit performance in domain-specific contexts

## Confidence

- **High Confidence:** The experimental methodology and results demonstrating improved Spearman's Rho and Kendall's Tau over baseline metrics are well-documented and reproducible from the code repository.
- **Medium Confidence:** The theoretical justification for using NLI as a scoring mechanism for factoid questions is reasonable, but the paper lacks strong empirical validation that NLI scores correlate with human judgments of factual correctness in all contexts.
- **Low Confidence:** The scalability claims for the automatic classification approach are weakened by the significant performance drop in zero-shot settings, and the paper does not thoroughly address how this might affect real-world deployment.

## Next Checks

1. **Classifier Robustness Test:** Evaluate the factoid/non-factoid classifier on a diverse, held-out test set with varying question types and domains to measure real-world accuracy, particularly in zero-shot settings.

2. **Ablation of NLI Scoring:** Compare the adaptive key-point scoring strategy against direct reference-response NLI scoring on factoid questions to isolate the contribution of key-point extraction to improved performance.

3. **Silver Instance Sensitivity Analysis:** Test the non-factoid ranking strategy with varying numbers and quality levels of silver instances to determine the minimum effective set size and identify failure modes when instances are poor quality.