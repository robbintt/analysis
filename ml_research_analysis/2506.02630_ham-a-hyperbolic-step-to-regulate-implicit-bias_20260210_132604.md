---
ver: rpa2
title: 'HAM: A Hyperbolic Step to Regulate Implicit Bias'
arxiv_id: '2506.02630'
source_url: https://arxiv.org/abs/2506.02630
tags:
- uni00000013
- learning
- gradient
- uni0000001a
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HAM is a new optimization algorithm that alternates between a\
  \ standard optimizer step and a hyperbolic mirror step to regulate implicit bias\
  \ in deep learning. It addresses the vanishing artificial learning rate problem\
  \ of the pointwise overparameterization m \u2299 w, which slows down convergence\
  \ and impedes sign learning."
---

# HAM: A Hyperbolic Step to Regulate Implicit Bias

## Quick Facts
- arXiv ID: 2506.02630
- Source URL: https://arxiv.org/abs/2506.02630
- Authors: Tom Jacobs; Advait Gadhikar; Celia Rubio-Madrigal; Rebekka Burkholz
- Reference count: 40
- Primary result: HAM improves dense training, boosts state-of-the-art sparsification methods, and enhances performance across vision, graph, and LLM tasks with minimal computational overhead.

## Executive Summary
HAM is a novel optimization algorithm that alternates between a standard optimizer step and a hyperbolic mirror step to regulate implicit bias in deep learning. It addresses the vanishing artificial learning rate problem of pointwise overparameterization, which slows down convergence and impedes sign learning. HAM is theoretically grounded via Riemannian gradient flows and natural gradient descent, showing faster convergence and controllable implicit bias that interpolates between L2 and L1 regularization. Empirically, HAM demonstrates consistent improvements across diverse tasks including vision, graph/node classification, and LLM fine-tuning, while complementing existing techniques like sharpness-aware minimization.

## Method Summary
HAM is an optimizer modification that alternates between a standard optimizer step and a hyperbolic mirror step (HYP*) to regulate implicit bias. After a standard optimizer update, weights are modified by multiplying with exp(-η(α sign(x_{k+1/2})∇f(x_k) + β)), where α controls bias strength (high α → L1 bias, low α → L2 bias) and β is explicit regularization. The method requires zero extra memory by reusing gradient and current weights, unlike doubling-parameter approaches. It's applied to all weights except BatchNorm/LayerNorm layers, and shows plug-and-play compatibility with existing optimizers like SGD and Adam.

## Key Results
- Improves dense training: +0.8% accuracy on CIFAR100 with ResNet18, +0.7% on ImageNet with ResNet50
- Boosts sparsification: Enhances Ac/Dc, RiGL, and STR methods across multiple vision benchmarks
- Cross-domain success: Improves performance on graph/node classification (OGB benchmarks) and LLM fine-tuning (LlaMA 3.2)
- Complements existing methods: Further gains when combined with sharpness-aware minimization (SAM)

## Why This Works (Mechanism)

### Mechanism 1: Correction of the Vanishing Artificial Learning Rate
HAM mitigates the "vanishing artificial learning rate" problem inherent in pointwise overparameterization (m ⊙ w), accelerating convergence. Standard gradient descent has uniform metric I=1, while m ⊙ w induces metric I⁻¹ = √(x²+γ²) that shrinks near zero, slowing learning. HAM induces metric I⁻¹ = 1 + α|x| ≥ 1, ensuring the artificial learning rate never vanishes. This is proven in Theorem 4.3 showing convergence rates driven by (1 + α min |xᵢ|) under PL-inequality assumptions.

### Mechanism 2: Facilitation of Sign Flips via Hyperbolic Geometry
The hyperbolic update step facilitates "sign flips" (crossing zero) more effectively than standard dense or m ⊙ w training. The hyperbolic step (HYP) moves incorrect weights exponentially fast toward zero - if the gradient sign opposes the weight sign, the update exp(-η(...)) aggressively dampens weight magnitude, allowing it to cross zero and stabilize at a new sign. This addresses the sign learning bottleneck for generalization in sparse training.

### Mechanism 3: Controllable Implicit Bias Interpolation
HAM provides implicit bias that smoothly interpolates between L2 and L1 regularization, controlled by hyperparameter α. Derived as mirror descent with Bregman function R_α, Theorem 4.6 shows that as α → 0, R_α ~ L2 (dense), and as α → ∞, R_α ~ L1 (sparse). This allows geometry to mimic sparsity-inducing overparameterization without explicit parameter doubling, providing theoretical control over the asymptotic solution.

## Foundational Learning

- **Concept: Mirror Descent & Riemannian Gradient Flows**
  - Why needed here: HAM is derived as a specific discretization of Riemannian gradient flow. Understanding that optimizers implicitly follow "warped" geometry (defined by mirror map) is essential to grasp why HAM regulates bias.
  - Quick check question: Can you explain how the "metric" or "Fisher Information" of a parameter space changes the effective step size of a gradient update?

- **Concept: Implicit Bias in Overparameterized Models**
  - Why needed here: Core motivation is regulating this bias. In overparameterized regimes (infinite solutions), the optimizer (not just loss) picks final solution (e.g., minimum norm vs sparse).
  - Quick check question: In a linear system with more parameters than data, why does Gradient Descent tend to find minimum L2 norm solution, and how might different optimizer change that?

- **Concept: Sparsification (Pruning at Initialization vs Dense-to-Sparse)**
  - Why needed here: Paper evaluates HAM on these tasks. "Pruning at Initialization" often fails due to optimization difficulties (stuck signs), explaining why HAM is proposed as fix.
  - Quick check question: Why is "sign learning" particularly difficult when training network that is sparse from very beginning?

## Architecture Onboarding

- **Component map:** Input weights xₖ and gradients ∇f(xₖ) → Standard optimizer step → xₖ₊₁/₂ → Hyperbolic step → xₖ₊₁
- **Critical path:** Alternating nature is strict. Gradient ∇f(xₖ) must be calculated before standard optimizer modifies weights, but sign used in hyperbolic step comes from post-optimizer intermediate weight xₖ₊₁/₂.
- **Design tradeoffs:**
  - α (Hyperbolic awareness): Controls geometry. High α → L1 bias (sparsity) but potential instability/discretization error. Low α → L2 bias (standard behavior).
  - β (Explicit regularization): Controls weight decay magnitude directly in hyperbolic step.
  - Memory: HAM requires zero extra memory (reuses gradient and current weights), unlike m ⊙ w which doubles parameters.
- **Failure signatures:**
  - Stalling: If α extremely high without learning rate adjustment, discrete steps may diverge from continuous flow theory, failing to converge.
  - Over-regularization: If β set too high relative to task, weights may collapse to zero too aggressively.
- **First 3 experiments:**
  1. Dense ImageNet Validation: Run ResNet50 with HAM on ImageNet. Verify if validation accuracy increases (target: ~76.7% → ~77.5%) and confirm "plug-and-play" nature by swapping SGD for HAM step.
  2. Linear Regression Sanity Check: Implement underdetermined linear regression setup (Appendix D.1) to visually confirm HAM converges to "sparser" ground truth vector than standard GD.
  3. Sparsification Integration: Apply HAM to existing sparsification method (e.g., Ac/Dc) on CIFAR100 to verify performance gains specifically in "Dense-to-Sparse" training regime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can different mirror maps be designed to create better task-specific or optimizer-specific awareness?
- Basis: The conclusion states, "It remains an interesting open question if different mirror maps could create better task and optimizer-specific awareness."
- Why unresolved: HAM currently utilizes specific hyperbolic mirror map; potential of other geometries (e.g., incorporating data structure or momentum) remains unexplored.

### Open Question 2
- Question: Why does using negative hyperparameter (α < 0) in HAM improve performance on heterophilic graph datasets?
- Basis: Appendix D.4.3 notes that success of negative α on heterophilic benchmarks is an "intriguing phenomenon" that "requires further investigation."
- Why unresolved: Theoretical derivation of HAM's Bregman function (Lemma 4.4) assumes α > 0, leaving dynamics and implicit bias of "inverse" bias (α < 0) theoretically undefined.

### Open Question 3
- Question: Does HAM guarantee convergence in general non-convex settings without relying on Polyak-Lojasiewicz (PL) inequality?
- Basis: Theorem 4.3 assumes loss function satisfies PL-inequality or is convex, assumptions that do not hold for general deep learning optimization landscapes.
- Why unresolved: While empirical results on neural networks are positive, theoretical guarantees are limited to restricted settings (convex/PL).

## Limitations

- Theoretical guarantees are limited to simplified settings (linear regression, PL-inequality landscapes) and don't fully characterize dynamics in deep non-convex networks
- Empirical evaluation focuses heavily on vision and structured data tasks; generalization to reinforcement learning or other domains remains untested
- Optimal hyperparameter settings (α, β) are task-dependent and derived from tuning rather than theory, limiting prescriptive guidance

## Confidence

- **High Confidence:** Core mechanism of correcting vanishing artificial learning rate is theoretically sound and well-supported by linear regression and convergence rate analysis
- **Medium Confidence:** Empirical improvements on dense training and sparsification are compelling but rely on hyperparameter tuning; benefits may not generalize without careful calibration
- **Medium Confidence:** Claim of interpolating implicit bias between L2 and L1 is rigorously proven in Bregman function limit but practical impact on generalization is primarily demonstrated empirically

## Next Checks

1. **Hyperparameter Sensitivity Study:** Systematically vary α and β across multiple tasks to map stability and performance landscape, confirming theoretical tradeoff between bias strength and convergence

2. **Non-Convex Landscape Analysis:** Apply HAM to synthetic deep learning task with known loss landscape properties (e.g., shallow CNN on controlled dataset) and visualize trajectory to confirm mechanism holds beyond linear models

3. **Cross-Domain Generalization:** Test HAM on reinforcement learning benchmark (e.g., Atari) to verify benefits extend beyond supervised and structured prediction tasks