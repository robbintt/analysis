---
ver: rpa2
title: 'Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are
  Information Peaks in LLM Reasoning'
arxiv_id: '2506.02867'
source_url: https://arxiv.org/abs/2506.02867
tags:
- reasoning
- step
- sample
- value
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reasoning dynamics of large reasoning
  models (LRMs) by tracking mutual information (MI) between intermediate representations
  and the correct answer during the reasoning process. The authors discover an "MI
  peaks" phenomenon where specific reasoning steps exhibit sudden, significant increases
  in MI with the correct answer.
---

# Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning

## Quick Facts
- **arXiv ID**: 2506.02867
- **Source URL**: https://arxiv.org/abs/2506.02867
- **Reference count**: 40
- **Primary result**: LRMs exhibit "MI peaks" at reasoning steps that correlate with thinking tokens; suppressing these tokens impairs performance; two training-free methods improve reasoning accuracy

## Executive Summary
This paper investigates reasoning dynamics in Large Reasoning Models by tracking mutual information (MI) between intermediate representations and correct answers. The authors discover that specific reasoning steps exhibit sudden, significant increases in MI with the answer—termed "MI peaks"—which predominantly correspond to "thinking tokens" like "Hmm," "Wait," and "Therefore." Theoretical analysis establishes that higher cumulative MI correlates with lower prediction error probabilities. Based on these findings, the authors propose two training-free methods: Representation Recycling, which reuses high-MI representations to improve accuracy by up to 20% relatively on AIME24, and Thinking Token based Test-time Scaling, which leverages thinking tokens during extended inference to achieve steady performance improvements as token budgets increase.

## Method Summary
The authors extract last-layer representations at each generation step from reasoning models during inference, then estimate MI between these representations and golden answer representations using HSIC with Gaussian kernels. They identify MI peaks using IQR-based outlier detection and decode these representations to token space to identify thinking tokens. Two training-free methods are proposed: Representation Recycling hooks into the forward pass at target layers to reprocess high-MI representations, while Thinking Token based Test-time Scaling monitors token budgets and appends thinking tokens when extending generation.

## Key Results
- MI peaks account for 0.51%-4.8% of reasoning steps across tested LRMs, with intervals ranging 27-87 steps
- Thinking tokens like "So," "Hmm," "Wait," and "Therefore" predominantly correspond to MI peaks
- Suppressing thinking tokens significantly degrades performance while suppressing random tokens has minimal effect
- Representation Recycling improves accuracy by up to 20% relatively on AIME24
- TTTS achieves steady performance improvements as token budgets increase

## Why This Works (Mechanism)

### Mechanism 1
Certain reasoning steps concentrate disproportionate information about the correct answer, manifest as MI peaks. During auto-regressive generation, hidden representations at specific steps exhibit sudden, significant increases in mutual information with the ground-truth answer. These peaks are sparse (≤5% of all steps) and non-uniformly distributed, suggesting they emerge at key reasoning junctures rather than uniformly throughout generation.

### Mechanism 2
Thinking tokens ("Wait," "Hmm," "Therefore," "So") serve as linguistic and representational anchors that correlate with MI peaks. When decoding MI peak representations back to token space via the output head, the highest-probability tokens are predominantly connective/reflective expressions. These tokens serve dual functions: linguistically prompting continued deliberation, and encoding high-MI hidden states.

### Mechanism 3
Higher cumulative MI between intermediate representations and the answer tightens theoretical bounds on prediction error probability. Theorems establish that prediction error p_e is bounded both above and below by functions involving cumulative MI. Intuitively, representations that capture more information about the target reduce uncertainty. MI peaks contribute to raising cumulative MI throughout the trajectory.

## Foundational Learning

- **Concept: Mutual Information (MI)**
  - **Why needed here:** The entire analysis hinges on tracking how much information each representation shares with the correct answer. Without understanding MI, you cannot interpret the peaks phenomenon or the theoretical bounds.
  - **Quick check question:** If I(h_t; y) increases while I(h_{t+1}; y) decreases, what does that imply about the information encoded at step t versus t+1?

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - **Why needed here:** Direct MI estimation in high dimensions is intractable. The paper uses HSIC as a practical proxy; understanding its kernel-based formulation is necessary to implement or validate the MI tracking pipeline.
  - **Quick check question:** Why does HSIC use kernel matrices K_X and K_Y rather than directly computing joint densities?

- **Concept: Transformer Hidden Representations per Layer**
  - **Why needed here:** Representation Recycling operates on layer-level outputs. You need to know which layers to target (middle-to-high per prior work) and how to intercept/modify the forward pass.
  - **Quick check question:** If you recycle a representation at layer ℓ*, which subsequent computations change versus the standard forward pass?

## Architecture Onboarding

- **Component map:** MI Tracker -> Peak Detector -> Token Decoder -> RR hook or TTTS intervention
- **Critical path:** MI extraction → Peak detection → Thinking token identification → RR hook placement or TTTS intervention
- **Design tradeoffs:** RR adds one forward pass through a single layer per peak—minimal latency overhead but requires identifying which layers benefit most. TTTS extends generation length, trading compute for accuracy. Using last-layer representations follows prior semantic-richness claims but may miss multi-layer information patterns.
- **Failure signatures:** If MI trajectory appears flat without distinct peaks, the model may not be an LRM. If RR degrades performance, likely recycling at wrong layer or too frequently. If TTTS causes repetitive outputs, the thinking token set may be triggering stuck patterns.
- **First 3 experiments:**
  1. MI trajectory visualization: Sample 20-50 problems from MATH training split; extract and plot MI per step to confirm peaks exist in your target model
  2. Token suppression ablation: Suppress top-5 thinking tokens vs. same count of random tokens; verify differential performance drop on GSM8K/MATH500
  3. RR layer sweep: Test recycling at layers {L/3, L/2, 2L/3, L-1} on a held-out set; measure accuracy vs. latency overhead

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms during reasoning-intensive training (e.g., reinforcement learning vs. distillation) are responsible for inducing the distinct MI peaks phenomenon observed in Large Reasoning Models? The authors state the underlying mechanisms that give rise to these peaks remain underexplored and leave deeper analysis of their origin to future work.

### Open Question 2
Does analyzing the reasoning process at the level of semantic units or logical steps, rather than individual tokens, reveal different patterns or "super-peaks" in mutual information? The authors note that alternative granularities such as dividing reasoning steps by semantic units may reveal additional insights.

### Open Question 3
To what extent does the specific vocabulary of "thinking tokens" depend on the foundation model's pre-training data versus the subsequent reasoning-intensive training? The authors hypothesize that the distribution of tokens at MI peaks may be influenced by factors such as the nature of the foundation LLM.

## Limitations
- HSIC approximation error may affect reliability of MI peak identification across different kernel bandwidths or batch sizes
- Causal mechanism linking thinking tokens to reasoning success remains correlative rather than proven
- Findings may not generalize beyond DeepSeek-R1-Distill and Qwen-based model families

## Confidence
- **High Confidence**: Observation that LRMs exhibit more pronounced MI peaks than base models, and that these peaks correlate with thinking tokens
- **Medium Confidence**: Theoretical bounds relating cumulative MI to prediction error probability
- **Low Confidence**: Claim that thinking tokens are the primary functional mechanism enabling reasoning improvements

## Next Checks
1. **HSIC Stability Analysis**: Run MI estimation on 100 randomly sampled problems with 5 different random seeds for kernel bandwidth selection and batch ordering. Report mean and standard deviation of peak detection rates and MI values.
2. **Alternative Token Suppression Test**: Identify the top-10 non-thinking tokens that co-occur with MI peaks. Perform the same suppression ablation as in Section 3.2. If suppressing these tokens has similar or greater performance impact than thinking tokens, the thinking token hypothesis needs revision.
3. **Cross-Architecture Peak Analysis**: Test at least two non-DeepSeek LRM variants (e.g., OpenAI o1/o3-mini, Google Gemini Flash Thinking) on the same MATH samples. Compare MI peak ratios, intervals, and thinking token distributions.