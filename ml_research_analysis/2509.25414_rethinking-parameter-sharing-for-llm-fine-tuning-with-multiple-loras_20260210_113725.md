---
ver: rpa2
title: Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs
arxiv_id: '2509.25414'
source_url: https://arxiv.org/abs/2509.25414
tags:
- learning
- fine-tuning
- lora
- multi-task
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits parameter sharing in multi-LoRA fine-tuning
  by analyzing the learning dynamics of LoRA modules. The authors find that the similarity
  of $A$ matrices stems from identical initialization rather than shared knowledge,
  with $B$ playing a more critical role in encoding domain knowledge.
---

# Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs

## Quick Facts
- **arXiv ID**: 2509.25414
- **Source URL**: https://arxiv.org/abs/2509.25414
- **Reference count**: 40
- **Primary result**: Parameter sharing in multi-LoRA fine-tuning should be reconsidered; A-matrices share similarity due to initialization, not knowledge, while B-matrices encode domain knowledge more critically

## Executive Summary
This paper challenges conventional wisdom about parameter sharing in multi-LoRA fine-tuning by revealing that similarity between A-matrices across different LoRA modules stems from identical initialization rather than shared knowledge. Through extensive experiments across commonsense reasoning, math reasoning, and multi-task NLP datasets, the authors demonstrate that B-matrices play a more critical role in encoding domain-specific knowledge. Based on these insights, they propose ALoRA, which uses multiple A-matrices with a single shared B-matrix, and Fed-ALoRA, a federated learning variant that communicates only B-matrices. Their methods achieve more balanced performance with reduced communication costs of up to 75% in heterogeneous settings.

## Method Summary
The authors introduce ALoRA and Fed-ALoRA as novel approaches to multi-LoRA fine-tuning. ALoRA uses multiple A-matrices and a single shared B-matrix, directly addressing the initialization-dependent similarity of A-matrices observed across tasks. Fed-ALoRA extends this to federated learning by communicating only B-matrices across clients, significantly reducing communication overhead. Both methods are evaluated across multiple datasets including commonsense reasoning, math reasoning, and multi-task NLP benchmarks. The key insight driving these architectures is that B-matrices, not A-matrices, are primarily responsible for encoding domain knowledge, making them more valuable to share across tasks or clients.

## Key Results
- ALoRA improves average ROUGE-1 by +0.68 and reduces Î”m% by -1.94 compared to traditional multi-LoRA approaches
- Fed-ALoRA reduces communication costs by up to 75% in heterogeneous federated learning settings while maintaining comparable accuracy
- Multiple A-matrices with shared B-matrix configuration achieves more balanced performance across diverse tasks compared to independent LoRA modules

## Why This Works (Mechanism)
The mechanism behind ALoRA's effectiveness lies in correctly identifying which parameters carry task-specific knowledge. While A-matrices in LoRA modules are initialized identically and thus show high similarity across tasks, B-matrices encode the actual domain knowledge learned during fine-tuning. By sharing B-matrices across tasks while maintaining separate A-matrices, ALoRA leverages the common knowledge while preserving task-specific transformations. This architecture prevents catastrophic interference between tasks while reducing the number of parameters that need to be updated independently, leading to more efficient learning and better generalization.

## Foundational Learning

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices. *Why needed*: Enables efficient adaptation of large language models without full fine-tuning. *Quick check*: Verify that rank-r decomposition approximates weight updates effectively.

**Parameter Sharing in Multi-Task Learning**: The practice of sharing parameters across different tasks to leverage common knowledge. *Why needed*: Reduces computational overhead and prevents overfitting in multi-task scenarios. *Quick check*: Confirm that shared parameters capture universal features rather than task-specific noise.

**Federated Learning**: A distributed learning paradigm where clients train models locally and communicate updates to a central server. *Why needed*: Enables privacy-preserving collaborative learning across decentralized data sources. *Quick check*: Ensure communication efficiency without compromising model accuracy.

## Architecture Onboarding

**Component map**: Input -> Multiple A-matrices (task-specific) -> Shared B-matrix (common) -> Output

**Critical path**: The shared B-matrix serves as the primary knowledge transfer mechanism, with task-specific A-matrices providing the necessary differentiation for individual tasks.

**Design tradeoffs**: Sharing B-matrices reduces parameter count and enables knowledge transfer, but may limit task-specific optimization. The tradeoff favors shared B-matrices because B-matrices encode more domain knowledge than A-matrices.

**Failure signatures**: If B-matrices are not truly encoding common knowledge, sharing them will lead to performance degradation across all tasks. Task-specific A-matrices must be sufficiently expressive to compensate for the shared B-matrix.

**First 3 experiments to run**:
1. Ablation study comparing fully independent LoRAs, shared A with independent B, and ALoRA's configuration
2. Communication cost analysis in federated setting comparing Fed-ALoRA vs traditional federated LoRA
3. Cross-task transfer learning evaluation measuring knowledge transfer effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based on a single 7B parameter model (Llama-2) may not generalize to larger models or different architectures
- Initialization-dependent A-matrix similarity requires validation across diverse model families and sizes
- Observed B-matrix importance in encoding domain knowledge needs systematic ablation studies to confirm causal relationships

## Confidence

**High confidence**: Empirical findings on A-matrix similarity stemming from initialization rather than shared knowledge, supported by consistent observations across multiple datasets and tasks

**Medium confidence**: Proposed ALoRA and Fed-ALoRA architectures and their theoretical advantages, as practical deployment may reveal additional challenges not captured in controlled experiments

**Medium confidence**: Communication cost reduction claims for Fed-ALoRA, as real-world federated settings may introduce additional constraints and overhead

## Next Checks

1. **Cross-model validation**: Test initialization-dependency hypothesis on models ranging from 1B to 70B parameters across different architectures (LLaMA, Mistral, Qwen) to verify generalizability

2. **Long-term stability analysis**: Evaluate performance degradation over extended fine-tuning periods to assess whether shared B-matrix approaches maintain advantages compared to fully independent LoRAs

3. **Resource efficiency benchmarking**: Conduct controlled experiments measuring actual GPU memory usage and training time for ALoRA versus traditional multi-LoRA approaches under identical hardware constraints