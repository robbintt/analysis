---
ver: rpa2
title: 'SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating
  Data Quality and Diversity'
arxiv_id: '2503.01506'
source_url: https://arxiv.org/abs/2503.01506
tags:
- data
- quality
- sampling
- diversity
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SampleMix, a sample-wise pre-training data
  mixing strategy that addresses limitations in existing domain-wise methods by evaluating
  individual sample quality and diversity. The approach uses a bottom-up paradigm
  that assigns sampling weights to each document based on both quality (using a 7-dimensional
  evaluation) and diversity (using cluster compactness and separation metrics).
---

# SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity

## Quick Facts
- arXiv ID: 2503.01506
- Source URL: https://arxiv.org/abs/2503.01506
- Authors: Xiangyu Xi; Deyang Kong; Jian Yang; Jiawei Yang; Zhengyu Chen; Wei Wang; Jingang Wang; Xunliang Cai; Shikun Zhang; Wei Ye
- Reference count: 40
- Key outcome: SampleMix achieves 47.77% average accuracy across 8 downstream tasks with 1.9x fewer training steps than baseline

## Executive Summary
SampleMix introduces a sample-wise pre-training data mixing strategy that addresses limitations in existing domain-wise methods by evaluating individual sample quality and diversity. Unlike traditional uniform sampling within domains, SampleMix performs global cross-domain sampling to construct an optimal training dataset. The approach uses a 7-dimensional quality evaluation (trained on GPT-4o annotations) combined with cluster-based diversity metrics to assign sampling weights to each document. Experiments demonstrate that SampleMix achieves the best average accuracy (47.77%) across 8 downstream tasks and lowest perplexity scores (25.63 on Pile, 46.38 on xP3). Most notably, it reaches baseline performance with 1.9x fewer training steps, showcasing substantial efficiency gains.

## Method Summary
SampleMix is a sample-wise pre-training data mixing strategy that evaluates individual document quality and diversity for weighted sampling. The method uses a 7-dimensional quality evaluation (clarity, coherence, structure, credibility, significance, knowledge richness, analytical depth) trained via ordinal regression on GPT-4o-annotated data. Diversity is computed using K-means clustering on L2-normalized embeddings, measuring cluster compactness × separation. Sampling weights combine quality and diversity with α=0.8, normalized with temperature τ=0.2. The approach constructs training datasets from 503M SlimPajama documents targeting 100B tokens, achieving 47.77% average accuracy on 8 downstream tasks with 1.9x fewer training steps than baseline.

## Key Results
- Achieves best average accuracy (47.77%) across 8 downstream tasks
- Lowest perplexity scores (25.63 on Pile, 46.38 on xP3)
- Reaches baseline performance with 1.9x fewer training steps
- Outperforms domain-wise methods on larger 8B models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample-wise quality evaluation enables more precise data selection than domain-level heuristics.
- Mechanism: A 7-dimensional ordinal regression model (trained on GPT-4o annotations) predicts quality scores (0-10) per document using thresholded binary classifications.
- Core assumption: The 7 quality dimensions predict downstream task performance and are learnable from GPT-4o labels.
- Evidence anchors: 83.37% close accuracy (±1 tolerance) on ordinal regression task.

### Mechanism 2
- Claim: Cluster-based diversity metrics ensure training data covers semantic space, preventing redundancy.
- Mechanism: K-means clustering on L2-normalized embeddings groups semantically similar documents; diversity = compactness × separation.
- Core assumption: Cluster structure in embedding space reflects meaningful semantic diversity for generalization.
- Evidence anchors: Accuracy increases from 45.53% (quality-only) to 47.77% when including diversity (α=0.8).

### Mechanism 3
- Claim: Combining quality and diversity with temperature-controlled sampling optimizes token budget utilization.
- Mechanism: Sampling weight = α·diversity + (1-α)·quality, with softmax temperature τ controlling concentration.
- Core assumption: Linear combination with α=0.8 appropriately balances diversity vs. quality for pre-filtered datasets.
- Evidence anchors: Adaptive sampling discards 18.2% lowest-quality data when T_tgt = T_src.

## Foundational Learning

- **Ordinal Regression**
  - Why needed here: Quality scores are ordered (0→10) but not equally spaced; standard classification ignores this structure.
  - Quick check question: Can you explain why MSE is more informative than accuracy for ordered scores?

- **K-means Clustering with Faiss**
  - Why needed here: Scaling to 500M+ documents requires efficient clustering; Faiss enables GPU-accelerated k-means.
  - Quick check question: What happens to cluster quality if you don't L2-normalize embeddings before k-means?

- **Softmax Temperature Scaling**
  - Why needed here: Controls sampling concentration—low τ emphasizes top samples, high τ flattens distribution.
  - Quick check question: If τ→0, what happens to the sampling distribution? If τ→∞?

## Architecture Onboarding

- **Component map:**
  1. Quality Evaluator: gte-en-mlm-base backbone + 10 ordinal layers (one per threshold)
  2. Diversity Evaluator: Embedding model (princeton-nlp/unsup-simcse-bert-base) → K-means → compactness/separation calc
  3. Weight Calculator: Combines quality + diversity with α, applies softmax with τ
  4. Sampler: Converts weights to integer/fractional sampling counts

- **Critical path:**
  1. Generate embeddings for all documents (most compute-intensive)
  2. Train quality evaluator on GPT-4o annotations (requires labeling budget)
  3. Run K-means clustering (√n clusters)
  4. Compute sampling weights and build training dataset

- **Design tradeoffs:**
  - Higher α prioritizes diversity (good for already-filtered data); lower α prioritizes quality (better for noisy data)
  - Lower τ concentrates sampling on top samples; risks overfitting to evaluation criteria
  - Cluster count k = √n balances granularity vs. computation; may miss fine-grained structure

- **Failure signatures:**
  - Quality evaluator MAE > 1.0: Labels or model capacity insufficient
  - Sampling weights all near uniform: α/τ misconfigured or quality/diversity distributions collapsed
  - Perplexity increases: Over-concentration on high-quality but narrow samples

- **First 3 experiments:**
  1. Validate quality evaluator: Sample 100 documents, compare GPT-4o scores vs. model predictions; target CACC > 80%
  2. Ablation on α: Train small models (100M params, 10B tokens) with α ∈ {0.0, 0.5, 0.8, 1.0}; verify α≈0.8 is optimal for your dataset
  3. Budget adaptation test: Run with T_tgt = 0.2×T_src vs. T_tgt = T_src; confirm sampling counts adapt appropriately (Figure 6 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic evaluation metrics derived from the model's perspective effectively replace or complement the current manually designed 7-dimensional quality measures?
- Basis in paper: The conclusion states future interest in "incorporating automatic evaluation metrics derived from the model’s perspective to complement the current manually designed measures."
- Why unresolved: The current study relies on manual definition of quality dimensions which may introduce human bias or miss features relevant to the model's learning dynamics.
- What evidence would resolve it: Experiments comparing model-derived quality scorers against current manual annotation method in terms of downstream task performance.

### Open Question 2
- Question: How does the sample-wise mixing strategy perform when applied to code data, which possesses distinct structural and semantic properties compared to natural text?
- Basis in paper: The authors exclude the GitHub domain and note that "Investigating code data mixing remains an avenue for future research."
- Why unresolved: Diversity and quality metrics are tailored for natural language and may not capture syntactic correctness or logic required for optimal code pre-training.
- What evidence would resolve it: Applying SampleMix to datasets like The Stack and evaluating on code-specific benchmarks (HumanEval, MBPP).

### Open Question 3
- Question: Is the K-Means clustering approach computationally scalable to web-scale datasets (e.g., >1T tokens) without significant loss in precision of diversity estimation?
- Basis in paper: The method requires generating embeddings and clustering the entire source dataset to determine sampling weights; computational overhead on larger datasets is not analyzed.
- Why unresolved: Clustering complexity grows with data size, potentially making data preparation prohibitively expensive for trillion-token corpora.
- What evidence would resolve it: Analysis of runtime and memory costs on datasets larger than SlimPajama, or comparison with approximate clustering methods.

### Open Question 4
- Question: Can the optimal weighting factor (α) and temperature (τ) be predicted or transferred to new datasets without extensive grid search?
- Basis in paper: The Limitations section notes optimal hyperparameters were identified specifically for SlimPajama and "may not directly transfer to other datasets."
- Why unresolved: Sensitivity of quality-diversity balance to different data distributions remains unquantified beyond specific SlimPajama distribution.
- What evidence would resolve it: Transferability study showing how optimal α shifts when source data's average quality score changes.

## Limitations
- Quality evaluator's robustness beyond SlimPajama dataset remains unverified
- Clustering-based diversity metric may not capture all semantic diversity relevant for downstream tasks
- Method requires extensive computational resources for embedding generation and clustering on large datasets

## Confidence

**High**: Training efficiency gains (1.9x fewer steps), perplexity improvements (25.63 on Pile), average accuracy (47.77%)

**Medium**: Quality evaluator performance (83.37% CACC), diversity metric effectiveness (α=0.8 optimal)

**Low**: Generalizability to other datasets, optimal hyperparameter selection (α, τ), cluster count choice

## Next Checks

1. **Quality Evaluator Generalization**: Evaluate the 7-dimensional quality model on a held-out domain (e.g., GitHub from original SlimPajama) and measure prediction accuracy degradation compared to the original test set.

2. **Hyperparameter Sensitivity**: Systematically vary α (0.0, 0.5, 0.8, 1.0) and τ (0.1, 0.2, 0.5) on a smaller model (100M parameters) and measure impact on downstream task performance and training convergence.

3. **Cluster Robustness**: Test the diversity metric with different cluster counts (k=1000, 10000, 50000) on a subset of the data and measure stability of sampling weights and downstream task performance.