---
ver: rpa2
title: 'More than a Moment: Towards Coherent Sequences of Audio Descriptions'
arxiv_id: '2510.25440'
source_url: https://arxiv.org/abs/2510.25440
tags:
- description
- visual
- candidate
- descriptions
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating coherent audio descriptions
  (ADs) for videos, which are used to help visually impaired audiences follow visual
  content. Existing methods generate each AD independently, often leading to repetitive
  and incoherent descriptions that fail to advance the narrative.
---

# More than a Moment: Towards Coherent Sequences of Audio Descriptions

## Quick Facts
- arXiv ID: 2510.25440
- Source URL: https://arxiv.org/abs/2510.25440
- Authors: Eshika Khandelwal, Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Andrew Zisserman, Gül Varol, Makarand Tapaswi
- Reference count: 24
- Primary result: Training-free pipeline CoherentAD achieves 0% exact repetition and 5.21% partial repetition on CMD-AD while maintaining high StoryRecall (2.63)

## Executive Summary
This paper addresses the problem of generating coherent audio descriptions (ADs) for videos, which are used to help visually impaired audiences follow visual content. Existing methods generate each AD independently, often leading to repetitive and incoherent descriptions that fail to advance the narrative. To solve this, the authors propose CoherentAD, a training-free method that first generates multiple candidate descriptions for each video interval, then selects a coherent, non-redundant sequence using four scoring criteria: adherence to AD guidelines, redundancy, story advancement, and visual element counts.

## Method Summary
CoherentAD is a training-free 4-stage pipeline: (1) Qwen2-VL-7B extracts structured visual descriptions from 16 sampled frames per interval; (2) LLaMA-3.1-Instruct-8B summarizes into concise paragraphs; (3) LLM generates up to 5 candidate ADs; (4) auto-regressive selection using 4 LLM scorers (weights: adherence 0.40, redundancy 0.25, story advancement 0.40, counts 0.29) conditioned on 3 prior ADs. The method runs on NVIDIA A6000 and achieves ~22 sec/AD on CMD-AD and TV-AD datasets.

## Key Results
- CoherentAD achieves 0% exact repetition and 5.21% partial repetition on CMD-AD (vs. 0.42% and 15.31% for Shot-by-Shot)
- StoryRecall score of 2.63 on CMD-AD (human-level is ~3.0)
- Fine-tuned AutoAD-III achieves lower StoryRecall (2.11) than training-free CoherentAD (2.63)
- On TV-AD dataset, CoherentAD achieves StoryRecall 2.33 with 7.07% partial repetition

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage decomposition improves output quality over single-model generation. The pipeline separates visual extraction, narrative summarization, candidate generation, and scoring into distinct stages, preventing instruction overload that smaller models struggle with when handling complex, multi-constraint tasks in a single pass.

### Mechanism 2
Auto-regressive selection with prior context reduces redundancy. Each candidate is scored against the previous 3 selected ADs, creating a dependency chain where earlier selections constrain later ones while maintaining local coherence.

### Mechanism 3
Multi-criteria scoring with weighted combination balances competing objectives. Four independent LLM scorers evaluate adherence to guidelines, redundancy, story advancement, and visual element counts, preventing any single criterion from dominating.

## Foundational Learning

- **Auto-regressive sequence modeling**: Selection at each interval depends on prior selections; understanding this dependency is essential for debugging why certain candidates are chosen. *Quick check*: Can you explain why increasing context window r from 1 to 3 improves StoryRecall but plateaus afterward?

- **Prompt engineering for constrained generation**: Each stage uses detailed prompts with explicit constraints; understanding how to structure these affects pipeline quality. *Quick check*: What happens if the summarization prompt fails to remove inferred emotions—how does this propagate to candidate scoring?

- **Evaluation metrics beyond n-gram overlap**: StoryRecall uses LLM-based comparison of narrative content, allowing paraphrasing and reordering while penalizing exact repetition. *Quick check*: Why does the paper allow paraphrasing and reordering in StoryRecall but penalize it in traditional captioning metrics?

## Architecture Onboarding

- Component map: Video → [VLM: Structured Description] → [LLM: Summarization] → [LLM: Candidate Generation (m≤5)] → [4× LLM Scorers] → Weighted Average → Auto-regressive Selection → AD Output

- Critical path: VLM extraction (~15s per interval) is the bottleneck—this is where missed details are unrecoverable; summarization ensures candidates are grounded and concise; scoring is parallelizable across candidates; selection is sequential.

- Design tradeoffs: m=5 candidates vs. fewer (more candidates increase diversity but add scoring cost); r=3 context vs. longer (longer context improves coherence marginally but increases latency); training-free vs. fine-tuned (avoids dataset requirements but limits domain adaptation).

- Failure signatures: High partial repetition (>10%) suggests redundancy scorer is underweighting; low StoryRecall with high visual element counts suggests candidates are detailed but miss narrative structure; AD guideline violations suggest summarization stage failed.

- First 3 experiments: (1) Ablate context window: Run with r={1,2,3,5} on held-out subset; expect StoryRecall to plateau at r=3. (2) Vary candidate count: Test m={1,3,5,7}; hypothesis is m=3 may be sufficient for simpler scenes. (3) Cross-dataset weight validation: Test current weights on TV-AD; monitor if StoryRecall gap widens.

## Open Questions the Paper Calls Out

None

## Limitations

- The training-free design inherits biases from underlying VLMs and LLMs, which may not be optimized for AD-specific requirements
- Dataset coverage gaps: TV-AD lacks explicit gender representation analysis, and CMD-AD shows ~55% male character mentions without exploring demographic skew impact
- Evaluation relies on LLM-based StoryRecall, which may not fully capture human perception of AD quality

## Confidence

**High confidence**: Multi-stage decomposition improves AD quality over single-model generation (well-supported by ablation and controlled experiments)

**Medium confidence**: Auto-regressive selection with r=3 context effectively reduces repetition while maintaining narrative coherence (strong evidence but generalizability to different narrative structures not fully established)

**Medium confidence**: Weighted multi-criteria scoring balances competing objectives effectively (stability demonstrated across weight configurations but optimal weights may vary with different models)

## Next Checks

1. **Cross-dataset weight validation**: Test current scoring weights on TV-AD and held-out CMD-AD subset to verify generalization

2. **Context window ablation**: Systematically evaluate r={1,2,3,5} on diverse videos to identify diminishing returns and failure modes

3. **Fine-tuning vs. training-free comparison**: Implement training-free pipeline and fine-tune smaller model on CMD-AD; compare StoryRecall and repetition metrics to quantify trade-offs