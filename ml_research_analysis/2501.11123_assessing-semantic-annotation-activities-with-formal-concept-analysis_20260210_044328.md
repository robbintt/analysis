---
ver: rpa2
title: Assessing Semantic Annotation Activities with Formal Concept Analysis
arxiv_id: '2501.11123'
source_url: https://arxiv.org/abs/2501.11123
tags:
- annotation
- ontology
- concept
- concepts
- ontologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using Formal Concept Analysis (FCA) to assess
  how domain experts' ontologies are used during semantic annotation of digital resources.
  Annotators tag resources using concepts from provided taxonomies, and FCA generates
  concept lattices from these annotations.
---

# Assessing Semantic Annotation Activities with Formal Concept Analysis

## Quick Facts
- arXiv ID: 2501.11123
- Source URL: https://arxiv.org/abs/2501.11123
- Reference count: 0
- Domain experts can assess annotation quality and refine ontologies using FCA-generated concept lattices

## Executive Summary
This paper proposes using Formal Concept Analysis (FCA) to assess how domain experts' ontologies are used during semantic annotation of digital resources. Annotators tag resources using concepts from provided taxonomies, and FCA generates concept lattices from these annotations. These lattices visually reveal patterns of concept usage, such as frequently combined concepts or unused concepts, enabling domain experts to guide annotators and refine ontologies. The method was implemented in @note, a tool for annotating literary texts, and applied to annotate Borges' "The Library of Babel." Evaluation with domain experts showed the approach effectively supports ontology refinement and annotator guidance.

## Method Summary
The approach uses FCA to analyze semantic annotation activities by generating concept lattices from annotation data. Annotators apply concepts from a hierarchical ontology to digital resources, and the system aggregates these annotations into a formal context where resources are objects and applied concepts (including parent concepts) are attributes. The FCA engine then transforms this context into a concept lattice, which domain experts can visually inspect to identify patterns of proper or improper concept usage. The system was implemented in the @note annotation tool and tested with domain experts annotating Borges' "The Library of Babel," demonstrating its effectiveness for ontology refinement and annotator guidance.

## Key Results
- FCA-generated concept lattices effectively reveal patterns of concept usage in semantic annotations
- Domain experts successfully used lattices to identify improper annotation practices and refine ontologies
- The approach is feasible for domain experts with limited technical background

## Why This Works (Mechanism)
The method works because FCA transforms annotation data into a formal context where objects (resources) and attributes (concepts) create a mathematical structure that reveals inherent relationships. When annotators consistently misuse concepts or combine them inappropriately, these patterns emerge as new formal concepts in the lattice that wouldn't exist in the original ontology. The lattice's visual structure makes these anomalies immediately apparent to domain experts who can then trace back to the underlying annotation data and correct the issues.

## Foundational Learning
- **Formal Context Construction**: Maps annotations to binary relations between resources and concepts, including parent concepts. Why needed: Without proper context building, the FCA engine cannot generate meaningful lattices. Quick check: Verify that annotating C-1-1 includes both C-1-1 and C-1 as attributes in the formal context.
- **Hasse Diagram Interpretation**: Understanding that bottom nodes represent unused concepts and merged nodes indicate problematic co-occurrence. Why needed: Domain experts must correctly interpret lattice patterns to identify issues. Quick check: Confirm that unused concepts appear at the bottom and that consistently co-occurring concepts merge into single nodes.
- **Ontology Refinement Loop**: The process of using lattice insights to modify the ontology, then re-annotating. Why needed: Assessment is only valuable if it leads to actionable improvements. Quick check: Track whether identified issues (e.g., ambiguous concepts) are resolved through ontology changes.

## Architecture Onboarding

- **Component map:** Ontology Editor -> Annotation Interface -> Formal Context Builder -> FCA Engine -> Lattice Viewer
- **Critical path:** The core data flow is: `Ontology -> Annotation Activity -> Formal Context -> Concept Lattice -> Expert Assessment -> Ontology Refinement`. The most critical step for an engineer to understand is the `Formal Context Builder`. Its logic must correctly map an annotation like `C-1-1` to both the specific concept `C-1-1` *and* its parent `C-1` in the formal context's attributes. A failure here would produce an incorrect lattice that misrepresents the ontology's usage.
- **Design tradeoffs:**
  -   **Simplicity vs. Expressiveness**: The system trades the rich semantics of a full OWL ontology for the usability and editability of a simple taxonomy. This makes it easier for domain experts but limits the types of knowledge that can be represented.
  -   **Post-hoc Analysis vs. Real-time Guidance**: The assessment is a batch process performed after an annotation activity is complete. This allows for a comprehensive view but prevents immediate intervention. The paper notes that experts requested "dynamic/continuous assessment" as a future improvement.
  -   **Automation vs. Expert Interpretation**: The system automates data aggregation and lattice generation but relies on human experts to interpret the results and decide on corrective actions. This avoids brittleness from hard-coded rules but creates a potential bottleneck and requires expert availability.
- **Failure signatures:**
  -   **Unused Concepts:** Concepts appearing at the very bottom of the lattice indicate they were never applied to any resource.
  -   **Unexpected Co-occurrence:** Concepts that always appear together, merging into a single node in the lattice, may signal that annotators cannot distinguish between them or that they represent a single underlying idea.
  -   **Unintended Combinations:** The appearance of new formal concepts in the lower lattice that combine concepts from different branches of the original ontology can indicate either a novel, valid insight or a misuse of the ontology.
- **First 3 experiments:**
  1.  **Simple Lattice Generation**: Create a minimal ontology (3 concepts, 1 parent, 2 children) and a set of 5-10 mock annotations. Manually construct the formal context and trace how the FCA algorithm would generate the concept lattice. Verify the resulting structure matches expectations.
  2.  **Assess "Improper" Use**: Introduce a deliberate misuse pattern, such as always tagging with two concepts from different categories (e.g., `Character` and `Setting`). Observe how this pattern manifests as a new formal concept in the generated lattice and what it would look like to an expert.
  3.  **Validate Refinement Loop**: After identifying the misuse in step 2, simulate an expert's corrective action: either merge the concepts in the ontology or create a new rule. Re-run the "assessment" mentally. How would the lattice change? This tests the entire feedback loop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sophisticated visualization techniques effectively scale concept lattices for larger ontologies while remaining usable for domain experts?
- Basis: [explicit] The authors state in Section 7 that "whether the visual representation of the lattice scales well for larger ontologies" is a weakness and requires research into whether advanced techniques will be "well received by domain experts."
- Why unresolved: The current case study involved a relatively small ontology; the visual complexity of Hasse diagrams may become overwhelming with denser data.
- What evidence would resolve it: A user study with domain experts utilizing large-scale ontologies to assess readability and navigation of complex lattices.

### Open Question 2
- Question: To what extent can automated rules support the semi-automatic assessment of annotation activities within concept lattices?
- Basis: [explicit] Section 5 and Section 7 note that experts suggested "the inclusion of semi-automatic assessment support (e.g., by adding rules)" and that this "deserves further investigation."
- Why unresolved: The current method relies heavily on the manual visual inspection of the lattice by the domain expert.
- What evidence would resolve it: Implementation of a rule-based engine that flags improper concept combinations, validated by comparing its findings against expert judgment.

### Open Question 3
- Question: Can the ontological assumption be relaxed to allow complex relationships beyond simple taxonomies without compromising the tool's usability for non-technical experts?
- Basis: [explicit] Section 7 identifies the constraint of "taxonomical arrangements of atomic concepts" as a weakness, asking "whether this assumption can be relaxed without compromising usability."
- Why unresolved: The current limitation (is-a relationships only) was intentionally imposed to ensure domain experts could author ontologies without advanced computer science knowledge.
- What evidence would resolve it: Comparative experiments where domain experts attempt to construct and utilize ontologies with richer relationship types (e.g., part-whole).

## Limitations
- Scalability concerns for larger ontologies and annotation datasets remain untested
- Manual expert interpretation introduces potential subjectivity and reproducibility issues
- Limited evaluation scope (single domain expert, single text) prevents generalization

## Confidence
- **High**: The mathematical foundations of FCA are well-established and clearly explained
- **Medium**: Practical utility for ontology refinement needs broader testing across different domains and user groups
- **Medium**: Claims about domain expert usability require more rigorous testing with diverse user populations

## Next Checks
1. Conduct a controlled experiment with multiple domain experts independently assessing the same annotation data to measure inter-rater reliability and identify subjective interpretation patterns.
2. Test the approach with progressively larger ontologies (10, 50, 100+ concepts) and larger annotation datasets to establish clear scalability boundaries and performance metrics.
3. Implement and evaluate the proposed "dynamic/continuous assessment" feature to determine if real-time feedback improves annotation quality compared to the current post-hoc approach.