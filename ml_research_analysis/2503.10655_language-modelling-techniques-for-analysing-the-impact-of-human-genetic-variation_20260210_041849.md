---
ver: rpa2
title: Language modelling techniques for analysing the impact of human genetic variation
arxiv_id: '2503.10655'
source_url: https://arxiv.org/abs/2503.10655
tags:
- variant
- prediction
- language
- protein
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This review synthesizes over a decade of language model applications\
  \ in variant effect prediction, focusing on the post-2017 Transformer era. It identifies\
  \ trends across DNA, RNA, and protein sequence modeling, highlighting that while\
  \ Transformer-based foundation models like ESM-1b, DNABERT-2, and Nucleotide Transformer\
  \ achieve state-of-the-art performance\u2014especially in protein pathogenicity\
  \ classification (AUROC 0.8)\u2014non-coding and RNA variant prediction remain challenging."
---

# Language modelling techniques for analysing the impact of human genetic variation

## Quick Facts
- **arXiv ID:** 2503.10655
- **Source URL:** https://arxiv.org/abs/2503.10655
- **Reference count:** 40
- **Primary result:** This review synthesizes over a decade of language model applications in variant effect prediction, focusing on the post-2017 Transformer era.

## Executive Summary
This comprehensive review synthesizes over a decade of language model applications in variant effect prediction, with particular focus on post-2017 Transformer architectures. The paper identifies that while Transformer-based foundation models achieve state-of-the-art performance in protein pathogenicity classification (AUROC > 0.8), non-coding and RNA variant prediction remain challenging. The review highlights computational efficiency gains from post-Transformer architectures like Caduceus and Evo, though their predictive accuracy lags behind Transformers. Key limitations include lack of benchmark standardization, demographic bias in training data, and insufficient evaluation of multi-base-pair or non-substitution variants.

## Method Summary
The review conducts a systematic analysis of language models for variant effect prediction across DNA, RNA, and protein sequences. It evaluates models using standard pipelines: sequence tokenization (BPE or k-mer) → pre-training (Masked Language Modeling) → fine-tuning or zero-shot inference. The study examines datasets including ClinVar, gnomAD, UniProt, and benchmark suites like BEND and Genome Understanding Evaluation. Performance metrics include AUROC, Spearman correlation, MCC, and Perplexity. The analysis covers both Transformer architectures and emerging state-space models, comparing their efficiency and accuracy trade-offs.

## Key Results
- Transformer-based models like ESM-1b, DNABERT-2, and Nucleotide Transformer achieve state-of-the-art performance in protein pathogenicity classification (AUROC > 0.8)
- Post-Transformer architectures such as Caduceus and Evo offer computational efficiency gains but have lower predictive accuracy than Transformers
- Non-coding DNA and RNA variant prediction tasks remain challenging with current models, showing limited performance improvement
- Significant issues include lack of benchmark standardization, demographic bias in training data, and limited evaluation of multi-base-pair variants

## Why This Works (Mechanism)

### Mechanism 1
Mapping genetic sequences to numerical vector spaces allows models to capture functional constraints analogous to semantic meaning in natural language. Tokenization breaks sequences into discrete units, and pre-training forces the model to learn high-dimensional embeddings where functionally similar elements cluster together. The functional impact of a variant is largely determined by statistical regularities and co-occurrence patterns learned during large-scale pre-training.

### Mechanism 2
Efficient state-space models (SSMs) like Mamba and Hyena extend the effective receptive field to capture long-range genomic interactions without quadratic Transformer costs. Unlike standard attention, SSMs use structured state spaces to compress sequence history, allowing reference to distant regulatory elements when predicting variant effects. Critical biological signals for non-coding variants often reside in long-range dependencies that short-context Transformers miss.

### Mechanism 3
Transfer learning from foundation models enables accurate pathogenicity prediction with limited labeled clinical data. Models pre-trained on massive datasets learn general constraints that can be adapted to distinguish pathogenic variants from benign ones. The evolutionary constraints learned during pre-training are functionally relevant to human disease pathogenicity.

## Foundational Learning

- **Byte-Pair Encoding (BPE) vs. K-mer Tokenization**: BPE balances vocabulary frequency while k-mers create sparse vocabularies. Quick check: Why might a fixed-length k-mer tokenizer struggle with rare sequence patterns like CG dinucleotides compared to BPE? (Answer: K-mers produce heterogeneous token frequencies, potentially biasing training toward frequent patterns).

- **Encoder-Only (BERT) vs. Decoder-Only (GPT) Architectures**: Most variant effect predictors use Encoder-only architectures for bidirectional context. Quick check: Why is bidirectionality (Encoder-only) preferred for classifying a variant's pathogenicity over the unidirectional approach of GPT? (Answer: Predicting mutation effects requires context from both sides of the nucleotide).

- **Zero-Shot Prediction**: Recent models attempt to predict variant effects without task-specific fine-tuning. Quick check: What is the primary trade-off when using a foundation model for zero-shot prediction versus fine-tuning it on a labeled dataset? (Answer: Zero-shot avoids scarce labeled data but generally underperforms specialized supervised models).

## Architecture Onboarding

- **Component map**: Input: DNA/RNA/Protein Sequence → Tokenizer (BPE/K-mer) → Embedding Layer → Backbone (Transformer Encoder OR State Space Model) → Task Head (Linear Layer → Softmax/Regression)

- **Critical path**: 1) Data Curation: Ensure data is free of circular labels 2) Tokenization: Select BPE for balancing vocab or K-mer for fixed resolution 3) Backbone Selection: Use Transformer for high-accuracy protein tasks; use Mamba/Hyena for long-range non-coding DNA tasks

- **Design tradeoffs**: Accuracy vs. Efficiency: Transformers offer high accuracy but scale quadratically; Mamba/Hyena scale linearly but are newer and less validated. MSA vs. Sequence-only: MSA improves conservation awareness but adds computational overhead; sequence-only models can match MSA performance with lower cost.

- **Failure signatures**: Demographic Bias: Model performs well on European-ancestry genomes but fails on others. Circularity Artifacts: High accuracy on validation sets but poor performance on "de novo" variants. Context Truncation: Performance degradation on variants far from TSS if context length is insufficient.

- **First 3 experiments**: 1) Baseline Fine-Tuning: Take pre-trained foundation model and fine-tune on ClinVar dataset, measure AUROC. 2) Context Length Stress Test: Compare Enformer vs. Caduceus on predicting non-coding variant effects for variants >50k base pairs from TSS. 3) Ablation on Input: Compare model performance using MSA input vs. Sequence-only input for protein fitness prediction task.

## Open Questions the Paper Calls Out

- Can computationally efficient architectures like Mamba or Hyena improve prediction accuracy for non-coding DNA variants where current Transformer models struggle? (Basis: The review concludes future work should prioritize non-coding regions and highlights current models show low performance on these tasks).

- How can the field establish standardized benchmarks to resolve performance ambiguity caused by inconsistent evaluation datasets and metrics? (Basis: Authors state lack of standard evaluation datasets makes performance comparison difficult and suggest a CASP-like competition is needed).

- To what extent do demographic biases in training data impact the clinical generalizability of variant effect predictors? (Basis: Authors warn that training on ancestrally homogenous datasets risks losing valuable features and prioritize equitable frameworks).

## Limitations

- Performance comparisons between models are uncertain due to lack of uniform benchmarking across studies
- Demographic bias remains critical as most training data derives from European-ancestry populations
- Insufficient evaluation of non-substitution variants and multi-base-pair changes represents clinically important gaps

## Confidence

**High Confidence** - Transformer-based models (ESM-1b, DNABERT-2) achieving state-of-the-art performance in protein pathogenicity classification, supported by specific AUROC metrics (>0.8) and multiple validation studies.

**Medium Confidence** - Characterization of post-Transformer architectures (Caduceus, Evo) as computationally efficient alternatives, based on theoretical scaling advantages and preliminary performance data.

**Low Confidence** - Predictions about clinical applicability and real-world implementation, given acknowledged absence of clinically relevant benchmarks and limited evaluation of diverse population datasets.

## Next Checks

1. Benchmark Standardization Assessment: Replicate performance evaluations using unified benchmark suite (BEND or Genome Understanding Evaluation) across multiple model types to establish direct, comparable performance metrics.

2. Demographic Bias Validation: Train and evaluate variant effect prediction models on ancestry-balanced datasets to quantify performance disparities across ancestral groups.

3. Multi-Base-Pair Variant Testing: Design experiments specifically targeting insertion, deletion, and structural variant prediction using existing foundation models to assess capability beyond single nucleotide substitutions.