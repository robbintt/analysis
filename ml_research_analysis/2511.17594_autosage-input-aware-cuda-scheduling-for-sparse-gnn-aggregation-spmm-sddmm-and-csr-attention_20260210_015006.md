---
ver: rpa2
title: 'AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM)
  and CSR Attention'
arxiv_id: '2511.17594'
source_url: https://arxiv.org/abs/2511.17594
tags:
- baseline
- autosage
- spmm
- sddmm
- guardrail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoSAGE addresses the performance variability of sparse GNN aggregations
  (CSR SpMM/SDDMM) on GPUs due to degree skew, feature width, and micro-architecture
  differences. The method introduces an input-aware CUDA scheduler that selects optimal
  tiling and mapping strategies per input using lightweight estimates refined by on-device
  micro-probes, with a guardrail to safely fall back to vendor kernels and persistent
  caching for deterministic replay.
---

# AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention

## Quick Facts
- arXiv ID: 2511.17594
- Source URL: https://arxiv.org/abs/2511.17594
- Authors: Aleksandar Stankovic
- Reference count: 26
- Primary result: Input-aware CUDA scheduler achieves up to 4.7× speedup on synthetic stress tests and matches vendor baselines on real GNN datasets.

## Executive Summary
AutoSAGE introduces an input-aware CUDA scheduler for sparse GNN aggregations (CSR SpMM/SDDMM) that adapts to degree skew, feature width, and micro-architecture differences. The system uses lightweight estimates refined by on-device micro-probes on an induced subgraph, with a guardrail to safely fall back to vendor kernels and persistent caching for deterministic replay. It covers SpMM and SDDMM operations and composes into a CSR attention pipeline. On real datasets Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and achieves gains at smaller widths; on synthetic stress tests with skew or extreme sparsity, it achieves up to 4.7× kernel-level speedups. The system is released with CUDA sources, Python bindings, reproducible harness, and replayable cache logs.

## Method Summary
AutoSAGE implements an input-aware CUDA scheduler that selects optimal tiling and mapping strategies per input using lightweight estimates refined by on-device micro-probes, with a guardrail to safely fall back to vendor kernels and persistent caching for deterministic replay. The scheduler covers SpMM and SDDMM operations and composes into a CSR attention pipeline. On real datasets Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and achieves gains at smaller widths; on synthetic stress tests with skew or extreme sparsity, it achieves up to 4.7× kernel-level speedups.

## Key Results
- Matches vendor baselines at bandwidth-bound feature widths and achieves gains at smaller widths on real datasets
- Achieves up to 4.7× kernel-level speedups on synthetic stress tests with skew or extreme sparsity
- Provides non-regression guarantee via thresholded comparison to baseline vendor kernels

## Why This Works (Mechanism)

### Mechanism 1: Input-Aware Kernel Selection via Micro-Probing
- Claim: Lightweight on-device timing on an induced subgraph predicts optimal kernel configuration for the full graph.
- Mechanism: Extract input features (degree quantiles, nnz/rows, F), shortlist candidates via roofline-style estimate, then time top-k kernels on a 2–3% induced subgraph (min 512 rows) with iteration cap. Cache the winner keyed by (device, graph signature, F, op).
- Core assumption: Subgraph timing correlates with full-graph relative performance; sparsity patterns are locally representative.
- Evidence anchors:
  - [abstract] "lightweight estimates refined by on-device micro-probes"
  - [Section 4.2] "time the top-k on an induced subgraph (default 2–3% rows, min 512) for n iterations with a wall-time cap"
  - [corpus] Weak direct evidence; Fused3S uses similar SDDMM→softmax→SpMM decomposition but does not address micro-probing.
- Break condition: Subgraph is unrepresentative (e.g., localized structure anomalies); probe budget too low for high-variance kernels.

### Mechanism 2: Guardrail Fallback for Non-Regression
- Claim: A thresholded comparison to baseline vendor kernels prevents regressions.
- Mechanism: Let t_b = baseline latency, t* = best candidate. Accept candidate iff t* ≤ α·t_b (α=0.95 default). Else fall back to cuSPARSE/gather–dot baseline.
- Core assumption: Baseline is stable and correctly measured; α correctly balances exploration vs safety.
- Evidence anchors:
  - [abstract] "guardrail to safely fall back to vendor kernels"
  - [Section 4.2, Proposition 1] "With α≤1, the chosen runtime t_chosen ≤ t_b"
  - [corpus] No direct analogs in neighbors; non-regression guarantee is AutoSAGE-specific.
- Break condition: Baseline measurement noise exceeds α margin; vendor kernel improves across driver versions.

### Mechanism 3: Hub-Aware CTA-per-Hub Split for Degree Skew
- Claim: Assigning one CTA per heavy ("hub") row improves load balance under power-law degree distributions.
- Mechanism: Detect rows exceeding hubT threshold; schedule one CTA per hub row vs warp-per-row for others. Enables parallelism within high-degree rows.
- Core assumption: Hub rows are identifiable via threshold; intra-row parallelization overhead is outweighed by better occupancy.
- Evidence anchors:
  - [Section 4.1] "hub-split: CTA-per-hub for heavy rows, warp-per-row for others"
  - [Table 10] Hub-skew synthetics show 2.2×–3.4× speedup over baseline at F=128
  - [corpus] Libra addresses SpMM heterogeneity but via Tensor Core / CUDA core partitioning, not hub-split.
- Break condition: Threshold misconfigured; hub rows too few or too many; small F where overhead dominates.

## Foundational Learning

- Concept: CSR SpMM/SDDMM semantics and memory-bound behavior
  - Why needed here: AutoSAGE schedules these exact kernels; understanding row-wise access and degree skew effects is prerequisite.
  - Quick check question: Given A∈R^{N×N} sparse CSR and B∈R^{N×F} dense, which dimension dominates memory traffic in C=AB?

- Concept: GPU execution model (warps, CTAs, occupancy, coalescing)
  - Why needed here: Kernel variants differ by warp-per-row vs CTA-per-hub mapping; requires mental model of scheduling and shared memory constraints.
  - Quick check question: If one row has 10× average degree, how does warp-per-row mapping hurt load balance?

- Concept: Autotuning and cost-model vs empirical selection
  - Why needed here: AutoSAGE uses estimate→micro-probe hybrid rather than pure cost model; understanding tradeoffs informs tuning.
  - Quick check question: Why probe on an induced subgraph instead of full graph?

## Architecture Onboarding

- Component map: spmm_rows.cu -> spmm_hub.cu -> sddmm_csr.cu -> Launcher + PyTorch bindings -> Scheduler -> Cache
- Critical path: (1) Input arrives -> (2) Check cache -> (3) If miss, extract features -> (4) Shortlist -> (5) Micro-probe on subgraph -> (6) Guardrail decision -> (7) Cache and execute.
- Design tradeoffs:
  - Probe fraction vs overhead: 2–3% rows gives 3–9% overhead; smaller fractions increase variance.
  - Guardrail α: 0.95 accepts small wins; 0.98 is more conservative (flips to baseline often).
  - Vec4: requires F mod 4 = 0 and 16B alignment; can help or hurt depending on dataset (see Table 9).
- Failure signatures:
  - Cache miss on every call → probe overhead dominates latency.
  - Misaligned vec4 paths → silent correctness issues or slowdown.
  - Hub threshold too low → excessive splitting; too high → load imbalance remains.
- First 3 experiments:
  1. Baseline comparison on Reddit/OGBN-Products with F=64,128,256; verify guardrail selects AutoSAGE at F=64 and baseline at large F.
  2. Ablate probe fraction (0.01, 0.02, 0.05) and measure overhead vs selection stability.
  3. Synthetic hub-skew sweep: vary hub fraction and threshold; confirm CTA-per-hub activates and measure speedup.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope is narrow: only two public datasets and one GPU model (A100) are tested, limiting generalizability.
- The subgraph micro-probing relies on an unproven assumption that local graph structure correlates with full-graph performance; this may fail on graphs with localized degree anomalies.
- The hub threshold tuning is heuristic and may require per-workload calibration.

## Confidence
- Guardrail mechanism: High
- Performance gap identification: High
- Universal robustness across unseen graph topologies: Medium
- Vec4 optimization complexity: Medium

## Next Checks
1. Run AutoSAGE on additional GNN datasets (e.g., Papers100M, Amazon-Products) and heterogeneous GPU architectures (e.g., H100, RTX 4090) to test portability.
2. Generate synthetic graphs with localized high-degree clusters to validate or break the subgraph micro-probing assumption.
3. Stress-test the guardrail by injecting controlled baseline latency noise and measuring regression rates across α settings.