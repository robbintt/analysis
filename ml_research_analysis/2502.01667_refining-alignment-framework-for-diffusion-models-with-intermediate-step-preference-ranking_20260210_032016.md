---
ver: rpa2
title: Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference
  Ranking
arxiv_id: '2502.01667'
source_url: https://arxiv.org/abs/2502.01667
tags:
- reward
- preference
- diffusion
- samples
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies inherent issues in existing DPO frameworks
  for diffusion models, where trajectory-level preference ranking and step-level optimization
  are mismatched. The authors propose TailorPO, a framework that generates paired
  noisy samples from the same intermediate input and directly ranks their step-wise
  rewards for optimization.
---

# Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference Ranking

## Quick Facts
- arXiv ID: 2502.01667
- Source URL: https://arxiv.org/abs/2502.01667
- Reference count: 28
- Primary result: Proposes TailorPO framework that improves aesthetic scores to 6.96 vs 5.79 baseline through step-wise preference ranking and gradient guidance

## Executive Summary
This paper identifies fundamental issues in existing Diffusion Policy Optimization (DPO) frameworks where trajectory-level preference ranking mismatches step-level optimization. The authors propose TailorPO, which generates paired noisy samples from the same intermediate input and directly ranks their step-wise rewards for optimization. They also introduce gradient guidance to enhance sample diversity and training effectiveness. Experiments show significant improvements in aesthetic scores, human preference alignment, and reward model optimization compared to previous methods.

## Method Summary
TailorPO fine-tunes text-to-image diffusion models by generating paired noisy samples from identical intermediate states at each denoising step, then ranking them using step-wise rewards approximated by predicted clean images. The framework resolves gradient direction issues through same-input conditioning and incorporates gradient guidance to enhance sample diversity. The method uses LoRA fine-tuning on 5 uniformly sampled steps with DDIM sampling, achieving up to 6.96 aesthetic score versus 5.79 for baseline methods.

## Key Results
- Aesthetic score improvement from 5.79 to 6.96 compared to baseline
- 59.33% win rate in human preference comparisons against existing methods
- Significant improvement in ImageReward, HPSv2, and PickScore metrics
- Effective gradient guidance increases training efficiency and sample diversity

## Why This Works (Mechanism)

### Mechanism 1: Step-wise Preference Ranking
Intermediate noisy samples should be ranked by their expected future reward, not by the final image reward from a single trajectory. The method formulates denoising as an MDP where action value Q(s,a) = E[r(c,x₀)|c,xₜ₋₁] simplifies to an approximation using the predicted clean image: rₜ(c,xₜ) ≈ r(c, x̂₀(xₜ)). This enables direct preference ranking at each step without training a separate step-wise reward model.

### Mechanism 2: Same-Input Conditioning for Gradient Alignment
Generating paired samples from identical input xₜ ensures the DPO gradient correctly increases probability of preferred samples while decreasing dis-preferred samples. By conditioning both π(xʷₜ₋₁|xₜ,c) and π(xˡₜ₋₁|xₜ,c) on the same xₜ, the gradient becomes ∇θL(θ) ∝ ∇θμθ(xₜ)(xʷₜ₋₁ - xˡₜ₋₁), which directly points toward preferred samples.

### Mechanism 3: Gradient Guidance for Diversity Enhancement
Applying reward model gradients to modify the preferred sample before training increases sample diversity and training effectiveness. The update x⁺ₜ₋₁ ← xₜ₋₁ - ηₜ∇ₓₜ₋₁(rₕᵢ₍ₕ - rₜ(c,xₜ₋₁))² pushes the preferred sample toward higher reward regions, enlarging the reward gap between paired samples.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation of diffusion**
  - Why needed here: Provides the formal framework to derive step-wise rewards and understand denoising as sequential decision-making, enabling the theoretical justification for intermediate-step preference ranking.
  - Quick check question: Why does the action value Q(s,a) simplify to E[r(c,x₀)|c,xₜ₋₁] rather than requiring sum over all future timesteps?

- **Concept: DPO gradient mechanics and the log-likelihood derivative**
  - Why needed here: Essential for understanding why conditioning on different inputs corrupts the gradient direction, and how same-input conditioning restores the intended optimization behavior.
  - Quick check question: In Eq. 8, why does the gradient term contain μθ(xˡₜ) - μθ(xʷₜ), and under what condition would this push the model toward dis-preferred outputs?

- **Concept: Classifier/reward guidance in diffusion sampling**
  - Why needed here: Provides the theoretical basis for understanding how gradient guidance modifies the sampling distribution to increase expected rewards, enabling the TailorPO-G enhancement.
  - Quick check question: How does the gradient update in Eq. 14 modify xₜ₋₁, and why does this push samples toward higher-reward regions of the distribution?

## Architecture Onboarding

- **Component map:**
  Input: Text prompt c, pretrained diffusion model πθ
  Initialize: xT ~ N(0,I)
  For each denoising step t = T,...,1:
    1. Generate: Sample x⁰ₜ₋₁, x¹ₜ₋₁ ~ πθ(·|xₜ,c)
    2. Rank: Compute rₜ(c,x⁰ₜ₋₁), rₜ(c,x¹ₜ₋₁) via Eq. 12; assign xʷₜ₋₁, xˡₜ₋₁
    3. [TailorPO-G]: Apply gradient guidance to xʷₜ₋₁ if reward model differentiable
    4. Optimize: Compute loss Eq. 10, update θ
    5. Continue: Set xₜ₋₁ ← xʷₜ₋₁ for next step
  Output: Fine-tuned model πθ, generated image x₀

- **Critical path:**
  1. Step-wise reward computation via predicted clean image x̂₀ (Eq. 12)—this is the core approximation enabling intermediate-step ranking
  2. Paired sample generation from same xₜ (not separate trajectories)—ensures gradient alignment
  3. Loss computation with same-input conditioning (Eq. 10)—applies corrected DPO formulation
  4. LoRA fine-tuning on T_fine-tune=5 uniformly sampled steps (efficiency optimization)

- **Design tradeoffs:**
  - T_fine-tune selection (Table 8): 5 steps gives best performance-efficiency balance; 10 steps slightly worse, 3 steps underfits
  - ηₜ scheduling (Table 9): Cosine schedule [0.1, 0.2] outperforms fixed values—larger guidance at smaller t (closer to output) works better
  - TailorPO vs TailorPO-G: TailorPO-G achieves higher scores (6.96 vs 6.66 aesthetic) but requires differentiable rewards; use TailorPO for non-differentiable reward models

- **Failure signatures:**
  - Reward hacking (Figure 11): Optimizing JPEG compressibility produces blank backgrounds—model overfits to reward proxy rather than true image quality
  - Low diversity gradient collapse: When xʷₜ₋₁ ≈ xˡₜ₋₁ at late timesteps, gradient magnitude approaches zero; gradient guidance (Table 10) mitigates this
  - Estimation error drift: Tables 6-7 show small but non-zero approximation error; may accumulate over training if reward model is poorly calibrated

- **First 3 experiments:**
  1. Baseline replication: Fine-tune SD v1.5 on animal prompts with aesthetic scorer reward. Compare TailorPO vs D3PO vs DDPO using Table 2 metrics. Verify gradient direction improvement via loss curve analysis.
  2. Component ablation (following Table 10): Test three configurations—(a) step-level ranking only, (b) + same-input conditioning, (c) + gradient guidance. Measure aesthetic score and ImageReward to isolate each component's contribution.
  3. Step-wise reward validation (Appendix D.2 methodology): Sample 100 (c, xₜ) pairs at timesteps {12, 8, 4, 1}. Compute ground-truth E[r(c,x₀)|c,xₜ] via 100 trajectory samples per pair, compare against approximation r(c, x̂₀(xₜ)). Verify relative error < 5% at critical timesteps.

## Open Questions the Paper Calls Out

- **Question:** Can the integration of reward model ensembles effectively mitigate the risk of reward hacking in TailorPO without compromising alignment performance?
- **Question:** How can the hyperparameter $\beta$ be dynamically adjusted during training to optimally balance high reward scores with the preservation of natural image distributions?
- **Question:** To what extent does the Jensen gap in the step-wise reward approximation ($r_t(c, x_t) \approx r(c, \hat{x}_0(x_t))$) impact performance when aligning with highly complex or non-linear reward functions?

## Limitations

- The Jensen gap approximation lacks formal error bounds, making it unclear when preference rankings become unreliable
- Small sample size (25 participants) in human preference tests limits generalization claims
- Missing analysis of model diversity and mode coverage under extended sampling

## Confidence

**High confidence:**
- Step-wise preference ranking mechanism and gradient alignment theory (Section 3.2)
- Improved aesthetic scores over baseline (Table 2, Figure 3)
- Component-wise ablation showing contributions of each innovation (Table 10)

**Medium confidence:**
- Generalization across different reward models (aesthetic scorer vs ImageReward)
- User preference win rates vs existing methods
- Gradient guidance effectiveness for sample diversity

**Low confidence:**
- Robustness to different prompt distributions beyond animal/NSFW domains
- Long-term stability of fine-tuned models under extended inference
- Scalability to larger diffusion models (SDXL, etc.)

## Next Checks

1. **Formal Jensen gap analysis:** Derive upper bounds on approximation error as function of noise level σ and reward model smoothness; validate with controlled experiments
2. **Step selection sensitivity:** Systematically test T_fine-tune ∈ {3,5,8,10,15} across 3 different reward models; report performance-efficiency tradeoffs
3. **Human evaluation replication:** Conduct preference studies with n=50 participants across diverse prompt categories (portraits, landscapes, abstract); include explicit diversity metrics