---
ver: rpa2
title: 'Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path
  Visual Physical CoT'
arxiv_id: '2505.24182'
source_url: https://arxiv.org/abs/2505.24182
tags:
- reasoning
- step
- object
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVPBench, a benchmark designed to evaluate
  visual physical reasoning in multimodal large language models (MLLMs). It addresses
  the gap in current benchmarks by focusing on real-world visual physics problems
  that require grounded, multi-step reasoning over visual evidence, going beyond surface-level
  image description.
---

# Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT

## Quick Facts
- **arXiv ID:** 2505.24182
- **Source URL:** https://arxiv.org/abs/2505.24182
- **Reference count:** 40
- **Primary result:** Current MLLMs show poor visual reasoning accuracy and weak image-text alignment in physical domains

## Executive Summary
This paper introduces MVPBench, a benchmark designed to evaluate visual physical reasoning in multimodal large language models (MLLMs). MVPBench addresses critical gaps in current evaluation frameworks by focusing on real-world visual physics problems that require grounded, multi-step reasoning over visual evidence, going beyond surface-level image description. The benchmark includes 1,211 carefully curated examples across four domains: physics experiments, exam-style physics problems, spatial relations, and dynamic prediction.

The paper proposes a novel graph-based CoT consistency metric suite to evaluate reasoning fidelity, visual grounding, and path diversity. This metric represents each reasoning chain as a directed acyclic graph of atomic facts and assesses step-wise fidelity through exact or fuzzy graph matching. Experimental results reveal that even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains, while surprisingly showing that reinforcement learning-based post-training alignment often harms spatial reasoning performance.

## Method Summary
MVPBench introduces a comprehensive benchmark for evaluating visual physical reasoning in MLLMs through a graph-based Chain-of-Thought consistency metric. The methodology involves constructing a dataset of 1,211 examples across four domains requiring multi-step reasoning over visual evidence. Each example features interleaved multi-image inputs demanding coherent, step-by-step reasoning paths grounded in evolving visual cues. The evaluation framework represents reasoning chains as directed acyclic graphs of atomic facts, using both exact and fuzzy matching algorithms to assess step-wise fidelity. The benchmark also investigates the impact of temporal context by comparing model performance with and without full image sequences.

## Key Results
- Current MLLMs exhibit poor visual reasoning accuracy in physical domains, with significant performance gaps in spatial reasoning tasks
- Reinforcement learning-based post-training alignment, commonly believed to improve visual reasoning, actually harms spatial reasoning performance
- Providing models with full image sequences boosts performance by up to 21 percentage points, highlighting the importance of temporal context
- The graph-based CoT consistency metrics effectively capture reasoning fidelity and visual grounding deficiencies in current MLLM approaches

## Why This Works (Mechanism)
The paper's approach works by addressing the fundamental mismatch between current MLLM evaluation practices and the complex nature of visual physical reasoning. By requiring models to process multi-image sequences and generate coherent reasoning paths, MVPBench forces models to demonstrate genuine understanding rather than surface-level pattern matching. The graph-based consistency metrics capture the nuanced relationships between visual evidence and reasoning steps, revealing weaknesses that traditional accuracy metrics miss. The temporal context experiments demonstrate that visual reasoning requires understanding of state evolution, which current models struggle to maintain across multiple steps.

## Foundational Learning

**Visual Physics Reasoning**
- Why needed: Real-world physical reasoning requires understanding of spatial relationships, causality, and temporal dynamics
- Quick check: Can the model predict outcomes of physical interactions from visual evidence?

**Multi-path CoT Evaluation**
- Why needed: Single-path evaluation misses the diversity of valid reasoning approaches in complex physical scenarios
- Quick check: Does the metric capture multiple valid solution paths to the same problem?

**Graph-based Consistency Metrics**
- Why needed: Traditional sequence metrics fail to capture the structural dependencies in physical reasoning chains
- Quick check: Can the metric distinguish between superficially different but logically equivalent reasoning paths?

## Architecture Onboarding

**Component Map:** Input Images -> Visual Encoder -> Feature Fusion -> Reasoning Graph Generator -> Graph Consistency Evaluator -> Performance Metrics

**Critical Path:** Multi-image input sequence → Visual feature extraction → Atomic fact generation → Graph construction → Consistency evaluation → Performance assessment

**Design Tradeoffs:** The framework trades computational complexity for comprehensive evaluation depth, requiring graph matching algorithms that are computationally intensive but provide more nuanced insights than simple accuracy metrics.

**Failure Signatures:** Poor visual grounding manifests as disconnected reasoning graphs, while temporal reasoning failures appear as inconsistent state representations across image sequences.

**3 First Experiments:**
1. Compare exact vs. fuzzy graph matching performance on a subset of benchmark examples
2. Evaluate model performance with vs. without temporal context using the same reasoning architecture
3. Test different visual feature extraction methods on spatial reasoning task accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies heavily on synthetic ground truth generation through GPT-4, which may introduce systematic biases in how reasoning chains are constructed and evaluated
- The dataset size (1,211 examples) may be insufficient to capture the full complexity and diversity of real-world visual physics problems
- The graph-based consistency metrics depend on the quality of atomic fact extraction and matching algorithms, which may not fully capture nuanced reasoning errors

## Confidence

**High confidence:** Claims about the need for better visual physics reasoning benchmarks and the observation that temporal context improves performance are well-supported by the experimental design

**Medium confidence:** Claims about RL post-training alignment harming spatial reasoning are based on the presented experiments but would benefit from additional ablation studies across different model architectures

**Medium confidence:** Claims about poor visual grounding in MLLMs are reasonable but limited by the specific evaluation methodology and dataset

## Next Checks

1. Conduct cross-validation using human-annotated ground truth reasoning chains for a subset of the benchmark to verify the consistency metrics' reliability
2. Expand the dataset size and diversity by including additional physical scenarios and edge cases to test the robustness of current MLLM performance claims
3. Implement additional control experiments comparing different alignment strategies beyond RL-based approaches to better understand the spatial reasoning degradation phenomenon