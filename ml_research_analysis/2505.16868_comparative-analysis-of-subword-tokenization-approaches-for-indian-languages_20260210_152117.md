---
ver: rpa2
title: Comparative analysis of subword tokenization approaches for Indian languages
arxiv_id: '2505.16868'
source_url: https://arxiv.org/abs/2505.16868
tags:
- translation
- tokenization
- machine
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the impact of three subword tokenization\
  \ methods\u2014SentencePiece, Byte Pair Encoding (BPE), and WordPiece\u2014on machine\
  \ translation for 11 Indian languages. The evaluation covers statistical, neural,\
  \ and multilingual neural machine translation models using standard metrics including\
  \ BLEU, TER, METEOR, CHRF, RIBES, and COMET."
---

# Comparative analysis of subword tokenization approaches for Indian languages

## Quick Facts
- arXiv ID: 2505.16868
- Source URL: https://arxiv.org/abs/2505.16868
- Reference count: 40
- This study investigates the impact of three subword tokenization methods—SentencePiece, Byte Pair Encoding (BPE), and WordPiece—on machine translation for 11 Indian languages.

## Executive Summary
This research systematically evaluates three subword tokenization approaches (SentencePiece, BPE, and WordPiece) for machine translation across 11 Indian languages. The study compares performance across statistical, neural, and multilingual neural machine translation models using multiple evaluation metrics. Results show SentencePiece excels in bilingual settings for morphologically complex languages, while BPE performs better in multilingual contexts. A consistent finding is the directional asymmetry where translating from Indian languages to English yields higher quality than the reverse direction.

## Method Summary
The study uses the Samanantar dataset (49.6M sentence pairs) for training and Flores200 for testing. Three tokenization methods were implemented: BPE via subword-nmt, SentencePiece via its library, and WordPiece via HuggingFace tokenizers. Preprocessing includes Unicode normalization, diacritic handling, lowercasing, and Indic script conversion. Models include Moses for statistical machine translation and Fairseq Transformer (6 encoder/decoder layers, 8 attention heads) for neural models. All 22 translation directions (11 languages × 2 directions) were evaluated using BLEU, TER, METEOR, CHRF, RIBES, and COMET metrics.

## Key Results
- SentencePiece consistently outperformed other tokenizers in statistical and neural machine translation models
- BPE showed superior results in multilingual neural machine translation settings
- Translations from Indian languages to English generally achieved higher scores than the reverse direction
- WordPiece generally underperformed compared to both SentencePiece and BPE across all settings

## Why This Works (Mechanism)

### Mechanism 1: Language-Agnostic Segmentation for Morphological Complexity
SentencePiece treats input as a raw stream where spaces are regular characters, allowing effective segmentation of agglutinative languages without enforcing arbitrary word boundaries. This better captures prefixes, suffixes, and stems compared to whitespace-dependent tokenizers.

### Mechanism 2: Vocabulary Sharing in Multilingual Contexts
BPE's frequency-driven approach to building vocabulary creates a more stable shared subword space for massively multilingual models compared to SentencePiece's probabilistic unigram approach, facilitating better knowledge transfer between high-resource and low-resource language pairs.

### Mechanism 3: Directional Translation Asymmetry
ILs are morphologically rich while English is morphologically simpler. It is computationally easier to "collapse" rich morphology into a simpler target (IL→En) than to "generate" complex morphological inflections from a simpler source (En→IL).

## Foundational Learning

**Concept: Subword Tokenization**
- Why needed: To solve the Out-of-Vocabulary (OOV) problem inherent in word-level models, especially for ILs with vast vocabularies due to inflection
- Quick check: Why does splitting a word like "unhappiness" into "un", "##happi", "##ness" help a low-resource translation model?

**Concept: Agglutinative Morphology**
- Why needed: To understand why ILs need specialized tokenizers; words are formed by gluing morphemes (e.g., "book" + "plural" + "case"), creating sparse data in naive models
- Quick check: How does a tokenizer handle a word that consists of a root + 3 suffixes if the exact word never appears in the training set?

**Concept: Vocabulary Construction (BPE vs. Unigram)**
- Why needed: The paper pivots on the difference between BPE (iterative merging of frequent pairs) and SentencePiece (Unigram likelihood)
- Quick check: Does BPE prioritize the most frequent *character pairs* or the most likely *subword units* when building a vocabulary?

## Architecture Onboarding

**Component map:**
Samanantar dataset -> Preprocessor (normalizer) -> Tokenizer (SentencePiece/BPE/WordPiece) -> Model (Transformer/Moses) -> Evaluator (BLEU/COMET/CHRF)

**Critical path:**
The choice of Tokenizer dictates the Vocabulary Size and Sequence Length, which directly impacts the Self-Attention Mechanism in the Transformer. Misalignment here causes the failure modes seen in WordPiece results.

**Design tradeoffs:**
- SentencePiece: Best for capturing linguistic nuance in bilingual pairs; requires training a separate model for each language direction or a joint model
- BPE: Best for massive multilingual models (MNMT) where script-agnostic subwords aid transfer; may miss fine-grained morphological boundaries compared to SentencePiece

**Failure signatures:**
- Over-segmentation: Words reduced to single characters (indicates vocabulary too small)
- Under-segmentation: High OOV rate (indicates vocabulary too large or data too sparse)
- Halant/Nukta errors: Incorrect handling of Indic diacritics leads to semantic drift

**First 3 experiments:**
1. Replicate the Hindi-English (Hi-En) NMT result using SentencePiece to verify the "morphological fit" hypothesis against a standard BPE baseline
2. Train an MNMT model on 3 distinct scripts (e.g., Hindi, Tamil, Bengali) using BPE to confirm vocabulary sharing efficiency vs. SentencePiece
3. Inspect token splits for a complex agglutinative word (e.g., in Tamil or Telugu) to visually verify if SentencePiece preserves the root while BPE splits based on character frequency

## Open Questions the Paper Calls Out

**Open Question 1:** How do different subword tokenization strategies impact translation quality for Indic-to-Indic language pairs?
- Basis: The conclusion states this tokenizer can be used to translate Indic to Indic languages and the translation quality can be evaluated further
- Why unresolved: Current study is restricted to English-to-Indic and Indic-to-English pairs

**Open Question 2:** What linguistic or architectural mechanisms cause BPE to outperform SentencePiece in Multilingual NMT despite SentencePiece's superiority in bilingual settings?
- Basis: The paper highlights the divergent outcome where BPE excels in MNMT while SentencePiece excels in SMT/NMT, without explaining the underlying cause
- Why unresolved: Authors provide empirical results but don't investigate whether this is due to vocabulary sharing efficiency

**Open Question 3:** Can optimized tokenization mitigate the performance disparity between English-to-Indic and Indic-to-English translation directions?
- Basis: Authors note that translations to English consistently surpassed translations from English, suggesting current tokenization or modeling approaches struggle with morphological complexity
- Why unresolved: Study establishes disparity exists across all models but doesn't isolate whether tokenization granularity is a primary bottleneck

## Limitations

- The study relies on a single large dataset (Samanantar) which may not capture the full diversity of translation scenarios for Indian languages
- Vocabulary sizes for each tokenizer are not specified, which is a critical hyperparameter affecting results
- The preprocessing pipeline for handling Indic diacritics and halants is described but not fully detailed, potentially affecting reproducibility
- No statistical significance testing is provided for the observed differences between tokenization methods

## Confidence

**High Confidence:** The general directional asymmetry finding (IL→En > En→IL) and the overall ranking of SentencePiece superiority in bilingual settings
**Medium Confidence:** The specific mechanism by which SentencePiece handles morphological complexity better than BPE in bilingual settings
**Low Confidence:** The claim that WordPiece is "generally underperforming" lacks detailed comparative analysis to explain why this specific tokenizer performs worse

## Next Checks

1. **Vocabulary Size Sensitivity Analysis:** Conduct experiments varying vocabulary sizes (8k, 16k, 32k, 64k) for each tokenizer to determine optimal sizes and assess whether performance differences persist across configurations

2. **Token Segmentation Pattern Analysis:** For morphologically complex words in Tamil/Telugu, visually compare and statistically analyze the actual token splits produced by each method to validate the claimed mechanism of SentencePiece preserving morphological boundaries better than BPE

3. **Statistical Significance Testing:** Apply paired t-tests or bootstrap confidence intervals on BLEU scores across multiple random seeds to determine whether the observed performance differences between tokenizers are statistically significant rather than due to random variation