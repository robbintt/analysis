---
ver: rpa2
title: Automatically discovering heuristics in a complex SAT solver with large language
  models
arxiv_id: '2507.22876'
source_url: https://arxiv.org/abs/2507.22876
tags:
- solver
- heuristics
- function
- solvers
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents AutoModSAT, the first successful framework for
  optimizing complex SAT solvers using large language models. The key innovation is
  a modular solver architecture combined with automated prompt optimization and a
  presearch strategy that reduces search space complexity.
---

# Automatically discovering heuristics in a complex SAT solver with large language models

## Quick Facts
- arXiv ID: 2507.22876
- Source URL: https://arxiv.org/abs/2507.22876
- Reference count: 40
- Primary result: 50% performance improvement over baseline SAT solvers using LLM-discovered heuristics

## Executive Summary
This work presents AutoModSAT, the first successful framework for optimizing complex SAT solvers using large language models. The key innovation is a modular solver architecture combined with automated prompt optimization and a presearch strategy that reduces search space complexity. AutoModSAT achieves 50% performance improvement over baseline solvers and 30% superiority against state-of-the-art competitors across 11 diverse datasets. It also delivers 20% average speedup compared to parameter-tuned alternatives. The framework discovers novel heuristics beyond human-designed search spaces, demonstrating LLM's potential to transcend traditional solver limitations and providing a transferable methodology for optimizing other complex solvers.

## Method Summary
AutoModSAT optimizes SAT solvers by modifying seven heuristic functions (restart, rephase, clause database reduction, and variable bumping) through LLM-guided search. The approach uses a modular solver architecture (ModSAT) with three design principles: simple focused functions, class variables for shared information, and proactive bug prevention. An entropy-based prompt optimization process maximizes diversity of generated code embeddings, while a presearch strategy evaluates functions on compact dataset subsets to identify the most impactful candidates. The evolutionary search then iteratively refines top candidates using LLM calls, with automatic compilation checking and performance evaluation guiding the optimization process.

## Key Results
- 50% improvement in PAR-2 score over baseline ModSAT solver
- 30% superiority against state-of-the-art SAT solvers across 11 benchmark datasets
- 20% average speedup compared to parameter-tuned alternatives

## Why This Works (Mechanism)

### Mechanism 1: Modular Solver Architecture Enables Targeted LLM Modification
Modularizing complex SAT solvers into focused, isolated functions allows LLMs to generate correct heuristic modifications without breaking unrelated components. Three design principles—simple focused functions, class variables for shared information, and proactive bug prevention—create an LLM-compatible codebase that reduces the token context burden and enables localized heuristic changes.

### Mechanism 2: Entropy-Based Prompt Optimization Increases Heuristic Diversity
Maximizing Shannon entropy of clustered code embeddings from LLM outputs produces more diverse heuristic candidates, improving search coverage. CodeT5+ generates embeddings for synthesized code → K-Means++ clusters samples → entropy measures diversity → prompts are iteratively refined to maximize entropy and compilation success rate.

### Mechanism 3: Presearch Strategy Reduces Combinatorial Search Complexity
A two-phase presearch strategy reduces the search space from O(n² + nλ) to approximately 75% fewer iterations by pruning low-impact functions early. Phase 1 evaluates each candidate function on 50% compact dataset subset using PAR-2 → top ~4 high-impact functions retained. Phase 2 runs (1+λ)EA evolutionary search only on the refined subset.

## Foundational Learning

- **CDCL (Conflict-Driven Clause Learning) Algorithm**: AutoModSAT operates on ModSAT, a CDCL-based solver. Understanding unit propagation, conflict analysis, clause learning, and backtracking is essential to interpret which heuristic functions matter. Quick check: Can you explain why learned clauses improve search efficiency, and what happens if too many are retained?

- **SAT Solver Heuristics (branching, restart, rephase, clause database reduction)**: The seven LLM-modifiable functions each control critical solver behavior. Understanding their roles is necessary to diagnose discovered heuristics. Quick check: What is the tradeoff between aggressive restarts (escaping local minima) and preserving learned clauses?

- **LLM Context Length and Code Generation Limitations**: The paper explicitly addresses token limits (~200k for LLMs vs. 250k+ for SOTA solvers). Understanding why modularization is necessary requires grasping this constraint. Quick check: Why can't an LLM simply read and modify the entire Kissat codebase in one prompt?

## Architecture Onboarding

- **Component map**: ModSAT (modularized CDCL solver) → Prompt Optimizer (entropy-based loop) → Presearch Module (compact dataset evaluator) → (1+λ)EA Search (evolutionary algorithm) → LLM Agents (Coder, Evaluator, Repairer) → PAR-2 evaluation → Solver update

- **Critical path**: Input dataset → Presearch identifies top-4 functions → Prompt optimization → LLM generates code → Compilation check (Repairer if needed) → Execution on dataset → PAR-2 evaluation → Update solver if improved → Iterate until budget exhausted

- **Design tradeoffs**: ModSAT's baseline is sub-SOTA (tradeoff: accessibility for LLM modification vs. raw starting performance); Presearch uses 50% subset for speed (risks non-representative pruning); Budget of 50 LLM calls limits exploration

- **Failure signatures**: Compilation errors (Repairer attempts fix → if fails, skip iteration); Synonymous code (Evaluator detects no semantic change → forces regeneration); Performance degradation (not integrated); Stagnation (consider increasing prompt diversity or budget)

- **First 3 experiments**: 1) Reproduce baseline comparison: Run ModSAT vs. AutoModSAT on a single dataset with budget=10 to verify pipeline produces executable, improved code; 2) Ablate presearch: Disable presearch and run full (1+λ)EA on all 7 functions; compare iteration count to reach same PAR-2 improvement; 3) Analyze discovered heuristics: Take the restart_function from Figure 3, manually trace its behavior on a small SAT instance, and compare to original implementation to understand why it improves performance

## Open Questions the Paper Calls Out

- **Performance baseline enhancement**: How can the baseline performance of modularized solvers be enhanced to better compete with state-of-the-art (SOTA) solvers prior to LLM optimization? The paper notes this as a primary challenge limiting the absolute ceiling of the framework.

- **Search feedback utilization**: How can feedback from the search process be utilized more effectively to guide subsequent heuristic generation? The authors identify this as a key challenge in leveraging search process feedback.

- **Long-context capability**: Can improved long-context capabilities allow LLMs to generate complex code for solvers without manual modularization? The paper notes that long-context capability needs improvement to generate more complex and high-quality codes.

## Limitations

- ModSAT baseline performance is sub-SOTA, limiting absolute ceiling before LLM optimization
- Presearch strategy assumes compact subsets maintain the same heuristic importance ranking as full datasets
- LLM-specific optimizations (entropy-based prompt optimization, agent framework) haven't been validated on other solver types

## Confidence

- **High confidence**: PAR-2 improvement metrics and modular solver architecture principles
- **Medium confidence**: 50% absolute improvement and 30% SOTA superiority claims
- **Low confidence**: Claims about LLM-discovered heuristics being "beyond human-designed search spaces" and generalizability to other complex solvers

## Next Checks

1. **Ablation study on presearch**: Disable the presearch strategy and run (1+λ)EA on all 7 functions for the same total number of LLM calls. Compare total iterations to reach 50% PAR-2 improvement and whether pruned functions would have contributed significantly.

2. **Cross-dataset function importance consistency**: For each of the 11 datasets, record which functions presearch identifies as top-4 contributors. Calculate Jaccard similarity between function sets across datasets to quantify pruning decision consistency.

3. **Heuristic behavior analysis**: Take one discovered heuristic (e.g., restart_function from Figure 3) and instrument ModSAT to log restart decisions during solving. Compare distribution of restart frequencies and timing against original implementation on 3 different problem types to explain performance difference.