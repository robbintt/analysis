---
ver: rpa2
title: On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization
arxiv_id: '2511.11362'
source_url: https://arxiv.org/abs/2511.11362
tags:
- mezo
- memory
- fine-tuning
- arxiv
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes memory-efficient on-device fine-tuning for
  large language models (LLMs) using zeroth-order optimization (MeZO) versus conventional
  backpropagation (BP). BP requires storing intermediate activations and gradients,
  severely limiting model size on memory-constrained devices.
---

# On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2511.11362
- Source URL: https://arxiv.org/abs/2511.11362
- Reference count: 0
- One-line primary result: MeZO enables 2×-25× larger model fine-tuning than BP under fixed memory constraints

## Executive Summary
This paper analyzes memory-efficient on-device fine-tuning for large language models (LLMs) using zeroth-order optimization (MeZO) versus conventional backpropagation (BP). BP requires storing intermediate activations and gradients, severely limiting model size on memory-constrained devices. MeZO avoids this overhead by estimating gradients via forward passes alone, enabling deployment of larger models within the same memory budget. Theoretical analysis shows MeZO can accommodate at least 2× larger models than BP, with potential savings up to 25× for long contexts. Experimental results on BoolQ task validate these findings: under equivalent memory constraints (~17 GB), MeZO with Llama2-7B or Llama2-13B achieves higher accuracy than BP with GPT2-medium, despite slower convergence. After ~2 hours, MeZO reaches ~82% accuracy versus <75% for BP.

## Method Summary
The method implements MeZO (Memory-efficient Zeroth-Order optimization) for on-device fine-tuning by estimating gradients through simultaneous perturbation stochastic approximation (SPSA). Instead of backpropagation, MeZO applies random perturbations to model weights and estimates gradients via finite differences from two forward evaluations. The implementation uses vanilla SGD with fixed learning rates, grid-searching MeZO LR [5e-7, 5e-8, 5e-9] and BP LR [5e-4, 5e-5, 5e-6]. Five perturbations per step are used for gradient estimation. The BoolQ binary QA task serves as the evaluation benchmark, with context length N=1024 and batch size B=8, running on a single H100 GPU with ~17GB memory constraint.

## Key Results
- Under 17GB memory constraint, MeZO with Llama2-13B achieves higher accuracy than BP with GPT2-medium
- MeZO memory advantage ranges from 2× at moderate contexts to 25× for N=32768
- After ~2 hours fine-tuning, MeZO reaches ~82% accuracy vs <75% for BP on BoolQ
- Theoretical analysis shows MeZO can fit models at least 2× larger than BP under same memory budget

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MeZO approximates gradients using only forward passes, eliminating backpropagation's memory overhead.
- Mechanism: The method applies random perturbations to model weights and estimates gradients via finite differences from two forward evaluations (simultaneous perturbation stochastic approximation). No backward pass or activation caching is required.
- Core assumption: Strong pre-training reduces the effective dimensionality of the parameter space, making gradient-free optimization tractable for fine-tuning.
- Evidence anchors:
  - [abstract] "MeZO alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states."
  - [section 1] "MeZO provides a memory-efficient alternative by estimating gradients through function evaluations rather than explicit derivatives."
  - [corpus] ConMeZO (arXiv:2511.02757) confirms ZO methods "eliminates the memory overhead of backpropagation" but notes slow convergence from "curse of dimensionality."
- Break condition: If the fine-tuning objective requires optimization in a high-dimensional subspace not already structured by pre-training, gradient estimates become too noisy for practical convergence.

### Mechanism 2
- Claim: Under fixed memory budgets, MeZO can accommodate models at least 2× larger than BP, with savings scaling up to 25× for long contexts.
- Mechanism: BP memory scales with activation storage (quadratic in context length per equation 1), while MeZO memory is dominated by parameters alone. The ratio $M_{BP}/M_{MeZO}$ improves as context length $N$ or hidden dimension $D$ increases.
- Core assumption: The implementation-specific factor $L'$ in equation 3 can be kept small (optimal: $L'=0$), meaning minimal intermediate tensor buffering.
- Evidence anchors:
  - [section 2.4] "The memory savings afforded by MeZO range from 2× at moderate contexts to up to 25× for N=32768."
  - [section 3] Under 17GB constraint, BP fits GPT2-medium while MeZO fits Llama2-13B (with $L'/L=0.15$).
  - [corpus] QuZO (arXiv:2502.12346) reports first-order optimizers like Adam "increases memory usage to more than 10 times the inference level."
- Break condition: If implementation requires substantial activation buffering ($L'/L \to 1$), MeZO memory advantage erodes toward parameter-only savings (~2×).

### Mechanism 3
- Claim: Given sufficient wall-clock time, larger models fine-tuned with MeZO achieve higher accuracy than smaller models fine-tuned with BP under identical memory constraints.
- Mechanism: The larger effective model capacity (enabled by memory savings) compensates for slower convergence per step. Accuracy gains emerge after extended training time.
- Core assumption: Wall-clock time is available as a fungible resource; the task benefits from larger model capacity more than faster gradient estimates.
- Evidence anchors:
  - [section 3] "After about 2 hours, the accuracy of MeZO is around 82%, while that of BP remains below 75%."
  - [abstract] "MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning."
  - [corpus] FZOO (arXiv:2506.09034) addresses ZO's "slow convergence" issue, suggesting speed is an active research frontier.
- Break condition: If wall-clock time is tightly bounded (e.g., real-time adaptation requirements), MeZO may not reach convergence before the deadline.

## Foundational Learning

- Concept: **Simultaneous Perturbation Stochastic Approximation (SPSA)**
  - Why needed here: MeZO's gradient estimation is derived from SPSA; understanding why two forward passes suffice for gradient approximation is essential for debugging convergence issues.
  - Quick check question: Can you explain why perturbing all parameters simultaneously (rather than one at a time) yields a valid gradient estimate with only $O(1)$ function evaluations?

- Concept: **Activation memory in Transformers**
  - Why needed here: The paper's core claim hinges on quantifying BP's activation overhead; understanding how $A = BLN D[2 + 16b + (2b+1)NH/D]$ scales helps predict memory bottlenecks.
  - Quick check question: If context length doubles from 2048 to 4096, what happens to activation memory for a 32-layer model?

- Concept: **Memory-computation tradeoffs in optimization**
  - Why needed here: MeZO represents a specific point on the spectrum trading memory for wall-clock time; engineers must evaluate where their constraints fall.
  - Quick check question: Given a 16GB memory budget and 1-hour time budget, which factors determine whether BP or MeZO is preferable?

## Architecture Onboarding

- Component map: Forward pass engine -> Loss computation -> Perturbation generator -> Second forward pass -> Gradient estimator -> SGD updater
- Critical path: Forward pass → loss computation → perturbation injection → second forward pass → gradient estimation → weight update. The perturbation injection must occur without materializing a full copy of perturbed weights.
- Design tradeoffs:
  - Number of perturbations: Paper uses 5; more perturbations reduce gradient variance but increase wall-clock time linearly.
  - Learning rate scale: MeZO requires smaller learning rates than BP (paper uses 1e-7 to 1e-9 vs 1e-4 to 1e-6).
  - $L'/L$ ratio: Implementation efficiency vs memory overhead; 0 is ideal but may require custom memory allocators.
  - Checkpointing vs pure MeZO: For edge cases where partial BP is acceptable, hybrid approaches (e.g., LoHo per citation 14) may optimize convergence.
- Failure signatures:
  - Exploding loss with reasonable learning rates: Perturbation scale $\epsilon$ may be too large relative to weight magnitudes.
  - No convergence after extended training: Effective dimensionality may be too high; consider parameter-efficient variants (AdaZeta, Sparse MeZO).
  - Memory usage matches BP: $L'/L$ is too high; audit memory allocator or switch frameworks.
  - Gradient estimates near zero consistently: Loss landscape may be flat in perturbation directions; check task difficulty and pre-training quality.
- First 3 experiments:
  1. **Memory baseline**: Profile peak memory for inference-only forward pass on target model; verify this matches $12bLD^2 + 2bVD$.
  2. **Convergence sweep**: On a small validation task (e.g., BoolQ subset), grid search learning rate × perturbation count × perturbation scale; identify stable convergence regime.
  3. **Memory-constrained comparison**: Fix memory budget, fit largest BP-compatible and MeZO-compatible models, compare accuracy at fixed wall-clock intervals (15min, 30min, 1hr, 2hr).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MeZO-based fine-tuning be specialized for neuromorphic systems and architectures with dynamic activation sparsity?
- Basis in paper: [explicit] The conclusion states: "More research is required to specialize the main conclusions to neuromorphic systems and other models with dynamic sparsity at the level of activations."
- Why unresolved: The current analysis and experiments focus on standard Transformer architectures; neuromorphic systems have fundamentally different computational paradigms and memory access patterns.
- What evidence would resolve it: Theoretical memory analysis adapted for neuromorphic architectures, plus empirical validation on neuromorphic hardware or sparsity-aware models.

### Open Question 2
- Question: Does the memory advantage of MeZO over backpropagation hold for mixture-of-experts (MoE) and sliding-window attention architectures?
- Basis in paper: [inferred] The analysis in Section 2 explicitly excludes these modern architectures, noting they were not studied despite claiming generalization should be possible "without affecting the general conclusions."
- Why unresolved: MoE architectures have sparse activation patterns and expert routing that may alter activation memory dynamics differently for BP versus MeZO.
- What evidence would resolve it: Extended theoretical analysis incorporating expert count and window-size variables, followed by empirical validation on MoE models like Mixtral.

### Open Question 3
- Question: What are optimal strategies for managing the implementation-specific memory allocation parameter L′ across different model architectures and hardware configurations?
- Basis in paper: [inferred] The paper acknowledges L′ depends on "memory allocation and management policies" and evaluates only two arbitrary values (0.15 and 0.41), but provides no principled method for selecting it.
- Why unresolved: The choice of L′ directly impacts the feasible model size under fixed memory budgets, yet no systematic study of optimal L′ selection is presented.
- What evidence would resolve it: Ablation studies across hardware platforms and model architectures identifying optimal L′ values, or an adaptive algorithm for dynamic L′ adjustment.

### Open Question 4
- Question: Can hybrid approaches combining MeZO with occasional backpropagation (e.g., LoHo) achieve better accuracy-time tradeoffs for on-device fine-tuning?
- Basis in paper: [inferred] The paper mentions LoHo as an extension that "integrates occasional BP-based gradient updates" but does not evaluate it, while the experiments show MeZO requires ~2 hours to outperform BP.
- Why unresolved: Pure MeZO trades longer wall-clock time for memory efficiency; hybrid approaches might achieve intermediate time-accuracy operating points suitable for different edge deployment scenarios.
- What evidence would resolve it: Comparative benchmarks of MeZO, BP, and hybrid methods (LoHo, Sparse MeZO, AdaZeta) under identical memory constraints, measuring accuracy as a function of fine-tuning time.

## Limitations

- Real-world edge hardware may have different memory hierarchies and allocation overhead that affect the theoretical memory savings
- Convergence robustness depends heavily on perturbation scale and task complexity, with no systematic characterization provided
- The accuracy advantage is demonstrated on a single task (BoolQ) and may not generalize to diverse NLP tasks or real-world edge scenarios

## Confidence

**High confidence** in the memory efficiency claims: The theoretical analysis of memory savings is sound, and the experimental demonstration on BoolQ task shows clear memory advantages allowing larger model deployment under fixed budgets.

**Medium confidence** in the accuracy claims: While the experimental results show MeZO achieving higher accuracy after ~2 hours compared to BP, this is demonstrated on a single task (BoolQ) with specific models and configurations.

**Medium confidence** in practical deployment viability: The paper acknowledges the "slow convergence" issue and suggests wall-clock time as a compensating factor, but many edge applications have strict latency requirements.

## Next Checks

1. **Cross-task generalization study**: Validate MeZO's accuracy advantage across diverse NLP tasks (GLUE, SuperGLUE, code generation) and with different model families (Mistral, Gemma) to assess whether the BoolQ results generalize beyond the specific experimental setup.

2. **Edge hardware profiling**: Implement MeZO on representative edge hardware (e.g., mobile SoCs, edge TPU) to measure actual memory savings and convergence behavior in production-like environments, accounting for memory allocation overhead and I/O constraints.

3. **Hybrid optimization analysis**: Experiment with adaptive perturbation strategies and hybrid approaches that combine MeZO with checkpointing or parameter-efficient fine-tuning methods to quantify potential improvements in convergence speed while maintaining memory efficiency advantages.