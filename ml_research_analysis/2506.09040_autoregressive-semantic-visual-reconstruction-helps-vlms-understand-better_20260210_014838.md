---
ver: rpa2
title: Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better
arxiv_id: '2506.09040'
source_url: https://arxiv.org/abs/2506.09040
tags:
- visual
- semantic
- asvr
- arxiv
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Autoregressive Semantic Visual Reconstruction (ASVR) addresses
  the limitation of typical large vision-language models (LVLMs), which apply autoregressive
  supervision only to textual outputs, overlooking fine-grained visual information.
  ASVR introduces a unified autoregressive framework that jointly trains visual and
  textual modalities by autoregressively reconstructing the semantic content of input
  images using a pretrained semantic visual tokenizer.
---

# Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

## Quick Facts
- arXiv ID: 2506.09040
- Source URL: https://arxiv.org/abs/2506.09040
- Authors: Dianyi Wang; Wei Song; Yikun Wang; Siyuan Wang; Kaicheng Yu; Zhongyu Wei; Jiaqi Wang
- Reference count: 20
- Primary result: Autoregressive Semantic Visual Reconstruction (ASVR) improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks

## Executive Summary
Autoregressive Semantic Visual Reconstruction (ASVR) addresses a critical limitation in large vision-language models (LVLMs) by extending autoregressive supervision beyond textual outputs to include fine-grained visual information. The approach introduces a unified autoregressive framework that jointly trains visual and textual modalities by reconstructing semantic content of input images using a pretrained semantic visual tokenizer. This enables effective reconstruction of discrete semantic tokens from continuous image features, leading to significant improvements in multimodal comprehension across diverse conditions.

The method demonstrates consistent performance gains across varying data scales (556k-2M samples), different visual input types, and multiple visual supervision strategies. ASVR outperforms existing denoising-based visual reconstruction approaches and shows robust generalization across different LLM backbones and high-resolution scenarios, making it a promising advancement for enhancing LVLM capabilities.

## Method Summary
ASVR introduces a unified autoregressive framework that jointly trains visual and textual modalities by autoregressively reconstructing the semantic content of input images using a pretrained semantic visual tokenizer. The approach treats visual reconstruction as an autoregressive task where the model learns to predict discrete semantic tokens that represent the visual content. This is achieved by encoding images into continuous features, which are then decoded into semantic tokens that capture the fine-grained visual information. The model is trained to jointly optimize both visual and textual autoregressive objectives, allowing it to better align visual and language representations. The framework demonstrates flexibility across different data scales, visual input types, and model architectures while maintaining consistent performance improvements.

## Key Results
- ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks
- Consistent improvements observed across varying data scales (556k-2M samples)
- Outperforms denoising-based visual reconstruction methods
- Demonstrates robust generalization across different LLM backbones and high-resolution scenarios

## Why This Works (Mechanism)
ASVR addresses the fundamental limitation of typical LVLMs where autoregressive supervision is applied only to textual outputs while visual information remains underutilized. By introducing joint autoregressive training for both modalities, the model learns to better align visual and language representations through the reconstruction of semantic visual tokens. The pretrained semantic visual tokenizer provides a discrete vocabulary that captures meaningful visual concepts, enabling the model to learn fine-grained visual understanding that directly benefits multimodal comprehension. The autoregressive nature of the reconstruction task forces the model to maintain coherent visual-semantic relationships throughout the generation process, leading to more robust cross-modal alignment.

## Foundational Learning

**Semantic Visual Tokenization**: Converting continuous image features into discrete semantic tokens that capture meaningful visual concepts. Needed because discrete tokens provide a stable representation for autoregressive reconstruction. Quick check: Verify tokenizer can reconstruct original images from semantic tokens with reasonable fidelity.

**Autoregressive Visual Reconstruction**: Predicting visual tokens sequentially based on previous predictions and context. Needed to maintain coherent visual-semantic relationships throughout the reconstruction process. Quick check: Measure reconstruction accuracy and coherence of generated visual tokens.

**Cross-Modal Alignment**: Joint training of visual and textual modalities to ensure consistent representations. Needed to improve multimodal understanding by aligning visual and language spaces. Quick check: Evaluate performance on multimodal benchmarks requiring cross-modal reasoning.

## Architecture Onboarding

**Component Map**: Image Features -> Semantic Visual Tokenizer -> Discrete Semantic Tokens -> Autoregressive Decoder -> Visual Reconstruction + Textual Output

**Critical Path**: Input Image → Visual Encoder → Semantic Visual Tokenizer → Autoregressive Reconstruction → Output Alignment with Text

**Design Tradeoffs**: The choice between autoregressive and denoising approaches involves balancing reconstruction quality with computational efficiency. Autoregressive methods provide better sequential coherence but may be slower, while denoising methods are faster but may lose fine-grained visual details.

**Failure Signatures**: Poor visual reconstruction quality leads to degraded multimodal performance. Tokenization errors propagate through the autoregressive process. Imbalanced training between visual and textual objectives can cause modality collapse.

**First Experiments**: 1) Test visual reconstruction quality on held-out images. 2) Evaluate cross-modal alignment using visual-linguistic similarity metrics. 3) Benchmark multimodal comprehension on standard datasets before and after ASVR integration.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on LLaVA-1.5 with limited ablation studies on architectural components
- Claims about scale robustness based on narrow range (556k-2M samples) without testing extreme scales
- Efficiency comparison with denoising methods lacks detailed analysis of computational overhead and training stability
- Generalization claims across architectures tested on limited set of backbones and resolution conditions
- 5% average improvement may mask performance variations across specific task types

## Confidence

**High confidence**: The core methodology of autoregressive visual reconstruction is technically sound and well-implemented.

**Medium confidence**: The performance improvements on tested benchmarks are reproducible but may not generalize to all LVLM variants.

**Medium confidence**: Claims about superior performance to denoising methods are supported but lack comprehensive efficiency analysis.

## Next Checks

1. Conduct extensive ablation studies isolating the contribution of autoregressive visual supervision versus other architectural modifications in the ASVR framework.

2. Evaluate performance across a broader range of data scales (both smaller and larger than 556k-2M) to verify claims about scale robustness.

3. Perform detailed efficiency analysis comparing ASVR with denoising-based methods, including training stability, computational overhead, and memory requirements across different hardware configurations.