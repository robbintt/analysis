---
ver: rpa2
title: 'Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models
  on Text-to-JQL Tasks'
arxiv_id: '2509.23579'
source_url: https://arxiv.org/abs/2509.23579
tags:
- language
- natural
- semantically
- execution
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jackal is the first open, real-world, execution-based benchmark
  for mapping natural language to Jira Query Language (JQL). It addresses the lack
  of evaluation resources for text-to-JQL by providing 100,000 validated natural language-JQL
  pairs and execution results on a live Jira instance with over 200,000 issues.
---

# Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks

## Quick Facts
- **arXiv ID:** 2509.23579
- **Source URL:** https://arxiv.org/abs/2509.23579
- **Reference count:** 37
- **Primary result:** First open, real-world, execution-based benchmark for mapping natural language to Jira Query Language (JQL)

## Executive Summary
Jackal addresses the lack of evaluation resources for text-to-JQL tasks by providing 100,000 validated natural language-JQL pairs and execution results on a live Jira instance with over 200,000 issues. Each query is paired with four user query variants to reflect real-world usage patterns. The benchmark includes an execution-based scoring toolkit and a static snapshot for reproducibility. Evaluation on Jackal-5K (5,000 pairs) with 23 models shows the best model (Gemini 2.5 Pro) achieves 60.3% execution accuracy overall, with significant performance variation across user query types. Exact match and canonical exact match metrics remain near zero, confirming the necessity of execution-based evaluation.

## Method Summary
Jackal is the first open, real-world, execution-based benchmark for mapping natural language to Jira Query Language (JQL). It addresses the lack of evaluation resources for text-to-JQL by providing 100,000 validated natural language-JQL pairs and execution results on a live Jira instance with over 200,000 issues. Each query is paired with four user query variants (Long NL, Short NL, Semantically Similar, and Semantically Exact) to reflect real-world usage patterns. The benchmark includes an execution-based scoring toolkit and a static snapshot for reproducibility. Evaluation on Jackal-5K (5,000 pairs) with 23 models shows the best model (Gemini 2.5 Pro) achieves 60.3% execution accuracy overall, with significant performance variation across user query types: 86.0% for Long NL, 35.7% for Short NL, 22.7% for Semantically Similar, and 99.3% for Semantically Exact. Exact match and canonical exact match metrics remain near zero, confirming the necessity of execution-based evaluation. Jackal exposes the limitations of current state-of-the-art LLMs and sets a new standard for domain-specific semantic parsing research.

## Key Results
- Gemini 2.5 Pro achieves 60.3% execution accuracy overall on Jackal-5K
- Performance varies significantly by query type: 86.0% for Long NL, 35.7% for Short NL, 22.7% for Semantically Similar, 99.3% for Semantically Exact
- Exact match and canonical exact match metrics remain near zero across all models
- Best model achieves only 35.7% accuracy on Short NL queries, highlighting difficulty with concise inputs

## Why This Works (Mechanism)
Jackal works by providing an execution-based evaluation methodology that tests whether generated JQL queries return the correct set of issues, rather than relying on syntactic matching. The benchmark's four query variant types capture different real-world usage patterns, from verbose detailed queries to semantically similar rephrasings. By evaluating on a live Jira instance with over 200,000 issues, the benchmark ensures that generated queries are tested against realistic data distributions and edge cases that would not be captured in synthetic test environments.

## Foundational Learning
- **Jira Query Language (JQL):** Domain-specific language for querying issues in Jira; needed because it's the target semantic representation for text-to-query tasks; quick check: can generate queries for filtering issues by status, assignee, and date range.
- **Execution-based evaluation:** Scoring method that runs generated queries and compares results to ground truth; needed because syntactic matching fails to capture semantic equivalence in query languages; quick check: does the generated query return the same issue set as the reference query.
- **Semantic parsing:** Task of mapping natural language to formal representations; needed as the core NLP problem Jackal addresses; quick check: can map "show me open bugs from last week" to appropriate JQL syntax.
- **Query variants:** Multiple natural language formulations of the same intent; needed to evaluate model robustness to phrasing variations; quick check: does model handle both verbose and concise versions of the same query.
- **Domain-specific benchmarks:** Evaluation datasets tailored to specific application domains; needed because general NLP benchmarks don't capture domain nuances; quick check: does the benchmark reflect realistic usage patterns in the target domain.

## Architecture Onboarding

**Component map:** Natural Language Input -> Text-to-JQL Model -> Generated JQL -> Jira Execution Engine -> Result Set -> Ground Truth Comparison

**Critical path:** Natural language query → Model generation → JQL execution → Result validation

**Design tradeoffs:** Execution-based evaluation provides objective ground truth but requires access to live Jira instances and may not scale to extremely large datasets. The four query variant types capture real-world usage but increase annotation and evaluation complexity. Static snapshots enable reproducibility but may become outdated as Jira schemas evolve.

**Failure signatures:** Models may generate syntactically valid JQL that returns incorrect results due to semantic misunderstandings. Short NL variants consistently show lower performance, suggesting models struggle with concise inputs. Semantically Similar variants reveal limitations in understanding paraphrased queries.

**3 first experiments:**
1. Evaluate baseline models on Long NL variants to establish upper performance bounds
2. Test execution-based scoring on synthetic JQL with known result sets
3. Compare performance across query variants to identify which types pose the greatest challenge

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on a single Jira instance, limiting generalizability to other issue-tracking systems or organizational contexts
- Performance gap between query variants suggests potential sensitivity to prompt engineering rather than genuine semantic understanding
- The evaluation set (5,000 pairs) may not fully capture edge cases or rare query patterns

## Confidence
- **Execution-based methodology:** High - provides objective ground truth through actual query results
- **Performance metrics:** Medium - controlled environment but potential confounding factors from variant-specific prompt formulations
- **Limitation claims:** High - consistent pattern of low performance across semantically similar variants

## Next Checks
1. Test model performance on Jira instances with different schemas, issue volumes, and custom field configurations to assess generalizability
2. Conduct ablation studies on the four query variant types to determine whether performance differences stem from semantic complexity or surface-level prompt variations
3. Evaluate whether fine-tuning approaches on Jackal data improve performance on semantically similar variants, distinguishing between memorization and genuine semantic understanding