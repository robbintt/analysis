---
ver: rpa2
title: 'Big Reasoning with Small Models: Instruction Retrieval at Inference Time'
arxiv_id: '2510.13935'
source_url: https://arxiv.org/abs/2510.13935
tags:
- reasoning
- instruction
- instructions
- knowledge
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling small language models
  (SLMs) to perform complex reasoning tasks that typically require large-scale models.
  The proposed solution, instruction retrieval at inference time, involves augmenting
  SLMs with structured reasoning procedures rather than relying on the models to generate
  these procedures from scratch.
---

# Big Reasoning with Small Models: Instruction Retrieval at Inference Time

## Quick Facts
- **arXiv ID:** 2510.13935
- **Source URL:** https://arxiv.org/abs/2510.13935
- **Reference count:** 40
- **Key outcome:** Small language models (3B-14B parameters) achieve up to 9.4% accuracy gains on complex reasoning tasks by retrieving structured instructions at inference time, outperforming GPT-4o on knowledge-intensive benchmarks.

## Executive Summary
This paper addresses the challenge of enabling small language models (SLMs) to perform complex reasoning tasks that typically require large-scale models. The proposed solution, instruction retrieval at inference time, involves augmenting SLMs with structured reasoning procedures rather than relying on the models to generate these procedures from scratch. An Instruction Corpus is constructed by clustering similar training questions and generating step-by-step instructions via a larger model. At inference, the SLM retrieves the most relevant instructions and follows them to solve the problem. This method is evaluated on three challenging benchmarks: MedQA (medical board exams), MMLU Professional Law, and MathQA, using models ranging from 3B to 14B parameters without additional fine-tuning. The results show consistent improvements across all tasks: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise instructions outperform longer ones, and the benefits depend more on the model's ability to follow structured guidance than on parameter count alone. Notably, the 14B-parameter models with retrieved instructions surpass GPT-4o's zero-shot performance on knowledge-intensive tasks.

## Method Summary
The paper introduces a novel approach called instruction retrieval at inference time, which augments small language models with structured reasoning procedures. The method involves constructing an Instruction Corpus by clustering similar training questions and generating step-by-step instructions using a larger teacher model (Qwen2.5-72B-Instruct). At inference, the SLM retrieves the most relevant instructions and follows them to solve the problem. This approach is evaluated on three challenging benchmarks: MedQA, MMLU Professional Law, and MathQA, using models ranging from 3B to 14B parameters without additional fine-tuning. The results demonstrate consistent improvements across all tasks, with the 14B-parameter models with retrieved instructions surpassing GPT-4o's zero-shot performance on knowledge-intensive tasks.

## Key Results
- 9.4% accuracy improvement on MedQA (medical board exams)
- 7.9% accuracy improvement on MMLU Professional Law
- 5.1% accuracy improvement on MathQA
- 14B-parameter models with retrieved instructions outperform GPT-4o's zero-shot performance on knowledge-intensive tasks

## Why This Works (Mechanism)
The method works by providing small language models with structured reasoning procedures at inference time, rather than relying on the models to generate these procedures from scratch. This approach leverages the strengths of large language models in generating high-quality instructions while allowing small models to benefit from the structured guidance without the computational cost of fine-tuning or the need for large-scale parameters.

## Foundational Learning
- **Instruction Retrieval:** The process of retrieving relevant instructions from a pre-constructed corpus at inference time. *Why needed:* To provide small models with structured guidance without relying on their own reasoning capabilities. *Quick check:* Ensure the retrieval mechanism can efficiently find the most relevant instructions for a given query.
- **Instruction Corpus Construction:** Clustering similar training questions and generating step-by-step instructions using a larger teacher model. *Why needed:* To create a repository of high-quality, task-specific instructions that small models can leverage. *Quick check:* Validate the quality and relevance of the generated instructions.
- **Structured Reasoning Procedures:** Step-by-step instructions that guide the model through complex reasoning tasks. *Why needed:* To break down complex problems into manageable steps, improving the model's ability to solve them. *Quick check:* Test the effectiveness of different instruction formats and lengths.

## Architecture Onboarding
- **Component Map:** Question -> Instruction Retrieval -> Instruction Following -> Answer Generation
- **Critical Path:** The model receives a question, retrieves the most relevant instructions from the corpus, follows the instructions to solve the problem, and generates an answer.
- **Design Tradeoffs:** Using a larger teacher model to generate instructions allows for high-quality guidance but introduces dependencies on the teacher model's capabilities and biases.
- **Failure Signatures:** Retrieval failures (irrelevant or misleading instructions) can lead to incorrect answers, and the method's effectiveness is sensitive to the quality of the instruction corpus.
- **Three First Experiments:**
  1. Test the retrieval mechanism's ability to find the most relevant instructions for a diverse set of queries.
  2. Evaluate the impact of instruction length and format on model performance.
  3. Analyze the computational overhead introduced by instruction retrieval at inference time.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a large teacher model (Qwen2.5-72B-Instruct) to generate the instruction corpus, which may limit scalability and introduce biases.
- Lack of error analysis for retrieval failures and cases where instructions are irrelevant or misleading.
- Limited exploration of generalization to other domains or larger model architectures.
- No analysis of computational overhead or latency introduced by retrieval at inference time.

## Confidence
- **High confidence** in the observed performance improvements on tested benchmarks, as results are consistent and statistically significant.
- **Medium confidence** in the generality of the approach, given the limited scope of tasks and models evaluated.
- **Low confidence** in the scalability and robustness of the method, due to lack of analysis on retrieval failure modes and computational costs.

## Next Checks
1. Conduct a thorough error analysis on retrieval failures to understand when and why the method breaks down.
2. Test the approach on a broader range of tasks and domains, including those outside the medical, legal, and mathematical reasoning domains.
3. Evaluate the computational overhead and latency introduced by instruction retrieval at inference time, especially in real-time or resource-constrained settings.