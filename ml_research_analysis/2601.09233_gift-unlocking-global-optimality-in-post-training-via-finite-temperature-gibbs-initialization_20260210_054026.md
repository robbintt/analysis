---
ver: rpa2
title: 'GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature
  Gibbs Initialization'
arxiv_id: '2601.09233'
source_url: https://arxiv.org/abs/2601.09233
tags:
- base
- gift
- arxiv
- preprint
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the optimization mismatch between Supervised
  Fine-Tuning (SFT) and Reinforcement Learning (RL) in post-training Large Reasoning
  Models, where SFT's rigid supervision causes distributional collapse that limits
  subsequent RL exploration. The authors propose GIFT (Gibbs Initialization with Finite
  Temperature), which reformulates SFT as a finite-temperature energy potential rather
  than zero-temperature collapse, creating a distributional bridge between stages.
---

# GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization

## Quick Facts
- arXiv ID: 2601.09233
- Source URL: https://arxiv.org/abs/2601.09233
- Reference count: 37
- Primary result: Achieves 52.43% average pass@1 on mathematical reasoning benchmarks with Qwen2.5-7B, outperforming standard SFT by 3.85%

## Executive Summary
This paper addresses the optimization mismatch between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in post-training Large Reasoning Models. The authors identify that SFT's rigid zero-temperature supervision causes distributional collapse that severely limits subsequent RL exploration capabilities. GIFT (Gibbs Initialization with Finite Temperature) reformulates SFT as a finite-temperature energy potential, creating a distributional bridge between SFT and RL stages. The method demonstrates significant performance improvements on mathematical reasoning benchmarks and better generalization to out-of-distribution tasks.

## Method Summary
GIFT introduces a novel approach to post-training by reformulating the SFT objective as a finite-temperature Gibbs distribution rather than traditional zero-temperature maximum likelihood. The method initializes model parameters using this Gibbs distribution, which maintains diversity in the output space while still respecting the supervision signal. This creates a smoother energy landscape that better bridges the gap between SFT's rigid constraints and RL's exploration needs. The temperature parameter controls the trade-off between following supervision and maintaining output diversity, with the optimization process gradually annealing toward zero temperature as training progresses.

## Key Results
- Achieves 52.43% average pass@1 on mathematical reasoning benchmarks with Qwen2.5-7B
- Outperforms standard SFT (48.58%) and other variants by 3.85% on benchmark tasks
- Shows superior generalization on out-of-distribution tasks (64.10% vs 59.78% for standard SFT)
- Maintains better geometric and distributional consistency throughout post-training pipeline

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental optimization gap between SFT and RL. Standard SFT collapses the model distribution to a single deterministic output per input, creating a sharp energy landscape that prevents effective RL exploration. GIFT's finite-temperature initialization creates a smoother energy potential that preserves distributional diversity while still respecting supervision. This allows RL to effectively explore and optimize within a more meaningful solution space rather than starting from a collapsed distribution. The gradual temperature annealing ensures the model eventually converges to deterministic outputs while maintaining the benefits of the initial diversity preservation.

## Foundational Learning
**Gibbs Distribution**: A probability distribution that assigns probabilities proportional to the exponential of a negative energy function. Why needed: Provides the mathematical framework for finite-temperature optimization. Quick check: Verify the partition function normalization in the implementation.

**KL Divergence**: Measures the difference between two probability distributions. Why needed: Used to quantify distributional collapse and consistency. Quick check: Ensure symmetric KL is used for bidirectional consistency measurement.

**Energy-Based Models**: Models that define a scalar energy function over variable states. Why needed: Forms the theoretical foundation for GIFT's temperature-based approach. Quick check: Validate that energy gradients properly reflect supervision signal strength.

**Distributional Collapse**: The phenomenon where model outputs converge to a single deterministic distribution. Why needed: The core problem GIFT addresses. Quick check: Monitor entropy of model outputs during training.

**Temperature Annealing**: Gradual reduction of temperature parameter during training. Why needed: Allows smooth transition from diverse initialization to deterministic convergence. Quick check: Verify annealing schedule matches theoretical convergence guarantees.

## Architecture Onboarding
**Component Map**: SFT dataset -> Gibbs Temperature Initialization -> Energy Potential Optimization -> RL Fine-tuning -> Final Model

**Critical Path**: The initialization phase is critical - poor temperature selection or incorrect energy landscape formulation will propagate errors through the entire post-training pipeline.

**Design Tradeoffs**: Higher temperatures provide better diversity preservation but may slow convergence; lower temperatures risk premature collapse but enable faster optimization.

**Failure Signatures**: Premature distributional collapse (indicated by KL divergence spike), unstable RL training (indicated by reward variance), or poor generalization (indicated by performance gap between training and OOD tasks).

**First Experiments**: 1) Ablation study varying temperature schedules across reasoning domains, 2) Distributional consistency validation using Wasserstein distance metrics, 3) Performance comparison on diverse OOD reasoning tasks with systematic domain shifts.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- Claims about distributional bridging enabling RL exploration are supported by correlational rather than causal evidence
- Temperature selection mechanism and sensitivity to different reasoning domains remains underexplored
- The claim of "unlocking global optimality" is somewhat overstated as the method improves local optima quality but doesn't guarantee true global optimality

## Confidence
- Mathematical formulation and theoretical framework: High
- Empirical performance improvements on benchmark tasks: Medium
- Claims about distributional bridging enabling RL exploration: Low to Medium
- Generalization to out-of-distribution tasks: Low

## Next Checks
1. Conduct ablation studies varying temperature schedules across different reasoning domains to determine optimal temperature dynamics
2. Test distributional consistency claims using alternative metrics beyond KL divergence, such as Wasserstein distance or empirical coverage analysis
3. Evaluate performance on a broader range of OOD reasoning tasks with systematic domain shifts to validate generalization claims