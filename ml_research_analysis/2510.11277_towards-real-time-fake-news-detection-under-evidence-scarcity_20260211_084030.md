---
ver: rpa2
title: Towards Real-Time Fake News Detection under Evidence Scarcity
arxiv_id: '2510.11277'
source_url: https://arxiv.org/abs/2510.11277
tags:
- evidence
- news
- fake
- reasoning
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of real-time fake news detection
  under evidence scarcity, where emerging news events lack sufficient supporting evidence.
  The authors propose EASE (Evaluation-Aware Selection of Experts), a novel framework
  that dynamically evaluates evidence sufficiency and adaptively selects from three
  expert modules: evidence-based, reasoning-based, and sentiment-based.'
---

# Towards Real-Time Fake News Detection under Evidence Scarcity

## Quick Facts
- arXiv ID: 2510.11277
- Source URL: https://arxiv.org/abs/2510.11277
- Reference count: 40
- EASE achieves 75.6% accuracy on RealTimeNews-25 vs 69.8% for best baseline

## Executive Summary
This paper addresses the challenge of detecting fake news in real-time when emerging events lack sufficient supporting evidence. The authors propose EASE (Evaluation-Aware Selection of Experts), a framework that dynamically evaluates evidence sufficiency and adaptively selects from three expert modules: evidence-based, reasoning-based, and sentiment-based. EASE uses instruction tuning with pseudo labels to create interpretable evaluators that assess evidence reliability, reasoning quality, and sentiment cues. The framework integrates these evaluations with news content for robust decision-making.

## Method Summary
EASE employs a sequential evaluation mechanism comprising three independent perspectives: evidence-based, reasoning-based, and sentiment-based. Each perspective includes an agent (for evidence retrieval or reasoning generation), an evaluator (for sufficiency assessment), and an expert (for decision-making). The framework uses instruction tuning with pseudo labels to train evaluators that assess evidence reliability, reasoning quality, and sentiment cues. These evaluators guide expert selection, with the evidence expert activated when evidence is sufficient, the reasoning expert when evidence is insufficient but reasoning is reliable, and the sentiment expert as a fallback when both evidence and reasoning are unreliable.

## Key Results
- EASE achieves state-of-the-art performance on historical datasets (Weibo, Weibo21, GossipCop)
- On RealTimeNews-25 benchmark, EASE reaches 75.6% accuracy compared to 69.8% for best baseline
- Single-expert performance is significantly worse (EE-only 0.607, RK-only 0.576, SA-only 0.535 on RealTimeNews-25), validating cascading design

## Why This Works (Mechanism)

### Mechanism 1
Cascading fallback through evidence→reasoning→sentiment improves robustness under evidence scarcity. Evidence evaluator checks sufficiency → if insufficient, reasoning evaluator checks LLM inference reliability → if unreliable, sentiment expert analyzes emotional/stylistic cues → each stage gates the next, preventing low-quality signals from contaminating decisions. Core assumption: Evidence reliability correlates with decision quality; when evidence fails, reasoning provides backup; when reasoning hallucinates, sentiment cues remain diagnostic.

### Mechanism 2
Instruction tuning with pseudo-labels improves evaluator reliability over raw LLM outputs. ChatGPT-4 generates sufficiency labels + rationales for evidence → these become training tuples (X, E, C_label, C_rationale) → LoRA fine-tunes Qwen2.5-14B on this supervision → fine-tuned evaluator better aligns with task criteria. Core assumption: ChatGPT's evaluation criteria are learnable and transferable; pseudo-labels capture task-relevant patterns.

### Mechanism 3
Evaluation-aware expert design (integrating evaluator rationale with content) outperforms content-only decisions. Evaluator outputs (sufficiency label + rationale R_t) → evidence analyzer encodes [E_t; R_t] → coordinator models news-evidence interaction via cross-attention → classifier integrates refined news representation with evidence-aware representation. Core assumption: Explicitly modeling evidence-quality assessment helps experts calibrate reliance on noisy information.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Fine-tuning 14B LLM evaluators efficiently; freezing base parameters prevents catastrophic forgetting. Quick check: If LoRA rank is too low, what happens to the evaluator's ability to learn task-specific patterns?

- **Cross-attention for multimodal fusion**: News-evidence coordinator uses cross-attention to model semantic dependencies; Q=news, K=V=evidence. Quick check: Why use cross-attention instead of concatenation for news-evidence interaction?

- **Pseudo-labeling / knowledge distillation**: Transferring evaluation capability from ChatGPT (black-box) to open-source LLMs. Quick check: What happens if pseudo-label noise exceeds model's learning capacity?

## Architecture Onboarding

- **Component map**: Evidence Agent → Evidence Evaluator → Evidence Expert → Coordinator → Classifier (parallel paths for Reasoning and Sentiment modules)
- **Critical path**: 1. Evidence retrieval (1:23 min, dominant cost) → 2. Evidence evaluation (0:05 min) → 3. If evidence insufficient → reasoning generation (0:08 min) → reasoning evaluation (0:05 min) → 4. If reasoning unreliable → sentiment analysis (0:07 min) → 5. Expert prediction (<0:01 min)
- **Design tradeoffs**: Multi-iteration retrieval (accuracy vs speed: Table 8 shows retrieval is 1:23 of total); pseudo-label quality vs annotation cost (human eval shows 93% consistency, but not perfect); cascading vs parallel expert fusion (sequential gating prevents noisy signals but increases latency)
- **Failure signatures**: Contradictory evidence: Evaluator marks as unreliable → reasoning activated but may also be inconsistent; Neutral-toned fake news: Sentiment expert misclassifies when no emotional manipulation cues; Evidence scarcity: RealTimeNews-25 scarcity ratio 0.37 vs 0.15-0.21 for historical datasets; expect more fallback activations
- **First 3 experiments**: 1. Evaluator alignment check: Sample 100 examples, compare fine-tuned evaluator vs ChatGPT pseudo-labels vs human judgment; target >90% agreement 2. Expert ablation: Test single-expert performance to validate cascading design 3. Inference time profiling: Measure evidence retrieval iterations (1 vs 2 vs 3), check accuracy-speed tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
What additional decision-making perspectives beyond sentiment analysis can improve robustness for objectively written fake news lacking strong emotional cues? Basis: Section 6 states future work will explore additional decision-making perspectives. Why unresolved: Sentiment expert fails on neutrally-toned fake news where fallback mechanism misclassifies. What evidence would resolve it: Systematic comparison of alternative fallback strategies evaluated on neutrally-written fake news samples.

### Open Question 2
Does the 7% misalignment between ChatGPT-generated pseudo labels and human judgment introduce systematic biases in evaluator training? Basis: Section 5.4.2 reports 93% consistency between pseudo labels and human annotators. Why unresolved: Paper doesn't analyze whether misaligned cases share common characteristics that could skew downstream expert decisions. What evidence would resolve it: Error analysis of 7% misaligned samples followed by ablation experiments comparing evaluator performance when trained on human-corrected vs. original pseudo labels.

### Open Question 3
How does the sequential cascaded evaluation architecture compare to parallel or integrated multi-perspective evaluation? Basis: Section 3 describes evidence→reasoning→sentiment as fixed sequential pipeline. Why unresolved: Sequential evaluation may miss beneficial interactions between perspectives. What evidence would resolve it: Architectural ablation comparing sequential vs. parallel evaluation vs. unified multi-perspective attention mechanisms.

### Open Question 4
What is the optimal trade-off between evidence retrieval iterations and detection accuracy/latency for real-time deployment? Basis: Section 5.1 arbitrarily limits retrieval to 3 iterations. Why unresolved: Paper doesn't systematically evaluate whether fewer iterations maintain acceptable accuracy. What evidence would resolve it: Controlled experiments varying iteration counts (1–5) across datasets with different evidence scarcity ratios.

## Limitations

- Evidence quality bottleneck: Framework effectiveness depends on iterative retrieval and LLM summarization, but no direct validation of evidence fidelity is provided
- Cascading failure risk: Sequential gating may introduce cascading failures when evaluators make incorrect sufficiency judgments
- ChatGPT dependency: Reliance on ChatGPT-generated pseudo-labels for training evaluators, while validated via human consistency (93%), doesn't guarantee these criteria generalize to edge cases

## Confidence

- **High Confidence**: Evidence-aware expert design integration, cascading fallback architecture effectiveness on benchmark datasets, RealTimeNews-25 benchmark creation methodology
- **Medium Confidence**: Pseudo-label quality and generalizability, evaluator training effectiveness across domains, sentiment expert reliability for neutral-toned misinformation
- **Low Confidence**: Evidence retrieval quality control, cascading failure modes under contradictory evidence, long-term performance degradation with domain shift

## Next Checks

1. **Evidence Quality Audit**: Manually verify 50 evidence retrieval outputs against original sources to quantify hallucination/factuality rates; current framework assumes retrieved evidence is trustworthy

2. **Cascading Failure Stress Test**: Create adversarial test cases with deliberately misleading evidence followed by contradictory reasoning, measure how often cascading decisions fail versus parallel expert voting

3. **Cross-Domain Generalization**: Deploy trained evaluators on news domains not seen during pseudo-label generation (e.g., finance, sports) to measure domain shift impact on the 75.6% RealTimeNews-25 accuracy claim