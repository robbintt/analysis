---
ver: rpa2
title: On the Factual Consistency of Text-based Explainable Recommendation Models
arxiv_id: '2512.24366'
source_url: https://arxiv.org/abs/2512.24366
tags:
- metrics
- factual
- explainable
- recommendation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for evaluating the factual consistency
  of text-based explainable recommendation models. The authors design a prompting-based
  pipeline that extracts atomic explanatory statements from user reviews, creating
  ground-truth explanations that focus on factual content.
---

# On the Factual Consistency of Text-based Explainable Recommendation Models

## Quick Facts
- arXiv ID: 2512.24366
- Source URL: https://arxiv.org/abs/2512.24366
- Reference count: 35
- Key outcome: Text-based explainable recommendation models achieve high semantic similarity (BERTScore F1: 0.81-0.90) but low factual consistency (LLM-based precision: 4.38%-32.88%).

## Executive Summary
This paper introduces a framework for evaluating the factual consistency of text-based explainable recommendation models. The authors design a prompting-based pipeline that extracts atomic explanatory statements from user reviews, creating ground-truth explanations that focus on factual content. Applying this pipeline to five Amazon Reviews categories, they construct augmented benchmarks for fine-grained evaluation. They propose statement-level alignment metrics combining LLM- and NLI-based approaches to assess factual consistency and relevance. Across experiments on six state-of-the-art models, they find that while models achieve high semantic similarity scores, factuality metrics reveal alarmingly low performance, highlighting the critical gap between surface-level fluency and factual accuracy in explainable recommendation systems.

## Method Summary
The authors construct a pipeline to extract atomic explanatory statements from user reviews, creating ground-truth explanations for evaluation. They use LLM prompting to decompose reviews into statement-topic-sentiment triplets, then aggregate these into explanatory paragraphs. Six state-of-the-art text-based explainable recommendation models are evaluated using both LLM-based and NLI-based statement-level alignment metrics. The LLM-based metrics score each generated statement against the full ground-truth explanation, while NLI-based metrics assess entailment and coherence between statement pairs. The evaluation is conducted across five Amazon Reviews categories with six different recommendation models.

## Key Results
- Models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90) across all tested datasets
- Factuality metrics reveal low performance (LLM-based precision: 4.38%-32.88%), indicating significant factual inconsistency
- NLI-based metrics show high variance across statement pairs, suggesting reliability concerns for fine-grained evaluation
- Current explainable recommendation models prioritize fluency over factual accuracy

## Why This Works (Mechanism)

### Mechanism 1: Atomic Statement Extraction via LLM Prompting
Decomposing reviews into atomic statement-topic-sentiment triplets isolates factual content from noise, enabling fine-grained evaluation. The pipeline prompts Llama-3-8B-Instruct with domain-specific topic sets and sentiment labels to extract polarized facts about single item attributes. A rule-based aggregation procedure then reconstructs these into ground-truth explanations.

### Mechanism 2: LLM-Based Statement-to-Explanation Scoring
Scoring each generated statement against the full reference explanation captures factual consistency better than pairwise NLI comparison. St2Exp-P computes average LLM scores for each generated statement given the ground-truth explanation as context. St2Exp-R reverses this, scoring each ground-truth statement against the generated explanation.

### Mechanism 3: NLI-Based Entailment and Coherence Metrics
NLI models provide efficient pairwise statement comparison, detecting both entailment and contradiction at scale. StEnt metrics use entailment probability; StCoh metrics compute entailment-minus-contradiction scores. DeBERTa-large-mnli scores statement pairs, with max-pooling across pairs for aggregate metrics.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: The NLI-based metrics (StEnt, StCoh) rely on understanding entailment, contradiction, and neutral relationships between statement pairs.
  - Quick check question: Given statement A: "The material feels cheap" and statement B: "The fabric quality is poor," would an NLI model predict entailment, contradiction, or neutral?

- **Concept: Semantic Similarity vs. Factual Consistency**
  - Why needed here: The paper's central finding is that high BERTScore (0.81-0.90) coexists with low factuality (4-33%). Understanding why embedding-based similarity fails to capture factual accuracy is essential.
  - Quick check question: Why might two texts have high BERTScore while one contains hallucinated claims not present in the other?

- **Concept: Atomic Fact Decomposition**
  - Why needed here: The extraction pipeline depends on breaking reviews into minimal, verifiable units. Understanding what makes a statement "atomic" determines evaluation granularity.
  - Quick check question: Is "The product is great and arrived on time" an atomic statement? If not, how would you decompose it?

## Architecture Onboarding

- **Component map:** Review text → Triplet extraction → Ground-truth construction → (Generated explanation → Statement extraction) → LLM/NLI scoring → Metric aggregation
- **Critical path:** Review text → Triplet extraction → Ground-truth construction → (Generated explanation → Statement extraction) → LLM/NLI scoring → Metric aggregation
- **Design tradeoffs:** LLM-based metrics preserve full context but are computationally expensive; NLI metrics scale efficiently but lose context. Rule-based ground-truth aggregation preserves all statements but may not reflect natural explanation structure. Topic pre-specification (10 per domain) enables filtering but risks missing emergent topics.
- **Failure signatures:** High BERTScore + low St2Exp-P indicates fluent hallucination. Negative StCoh scores indicate systematic contradiction. Large variance in NLI metrics (e.g., std. ~0.30) suggests unreliable pairwise comparison.
- **First 3 experiments:** 1) Baseline validation: Run all six models on one dataset to reproduce the semantic vs. factuality gap. 2) LLM scorer calibration: Manually verify 50-100 LLM-based consistency scores against human judgment. 3) Ablation on topic count: Test whether 10 topics per domain is sufficient by measuring statement coverage.

## Open Questions the Paper Calls Out

### Open Question 1
Does replacing rule-based aggregation with a learning-based selection approach for atomic statements result in ground-truth explanations that better reflect natural user preferences? The current rule-based procedure preserves content but may not reflect natural structure.

### Open Question 2
What specific training objectives or architectural innovations are required to close the performance gap between semantic similarity and factual consistency? Current state-of-the-art models optimize for surface-level text quality, resulting in high fluency but low precision on statement-level factuality metrics.

### Open Question 3
To what extent does the inclusion of human verification in the statement extraction pipeline improve the reliability of the factual consistency evaluation? The framework currently relies entirely on automated LLM extraction to define the ground truth.

## Limitations
- Dependence on LLM-based triplet extraction and scoring introduces potential systematic errors in ground-truth construction and metric computation
- Rule-based aggregation procedure is underspecified, making it difficult to assess whether ground-truth explanations faithfully represent review content
- LLM scorer's calibration and potential bias toward certain statement types remains unvalidated

## Confidence
- **High confidence:** The empirical finding that semantic similarity metrics (BERTScore F1: 0.81-0.90) can coexist with low factuality scores (LLM-based precision: 4.38%-32.88%) is robust across multiple datasets and models
- **Medium confidence:** The decomposition approach and statement-level metrics provide valuable fine-grained insights, though their absolute reliability depends on LLM performance quality
- **Medium confidence:** The claim that current explainable recommendation models suffer from significant factual inconsistency is well-supported, though the exact magnitude depends on the evaluation pipeline's accuracy

## Next Checks
1. **Manual validation study:** Conduct human evaluation of 200 randomly sampled LLM-based consistency scores to establish ground truth and calibrate the scoring function
2. **Ground-truth construction audit:** Systematically analyze 100 extracted triplets per dataset to quantify extraction errors and assess rule-based aggregation quality
3. **Cross-scorer comparison:** Test multiple LLM scorers (e.g., GPT-4, Claude) and NLI models to measure consistency across different model choices and identify potential scorer-specific biases