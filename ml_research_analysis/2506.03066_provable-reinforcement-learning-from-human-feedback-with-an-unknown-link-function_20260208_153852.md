---
ver: rpa2
title: Provable Reinforcement Learning from Human Feedback with an Unknown Link Function
arxiv_id: '2506.03066'
source_url: https://arxiv.org/abs/2506.03066
tags:
- function
- policy
- value
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel reinforcement learning from human feedback
  (RLHF) algorithm called ZSPO (Zeroth-Order Sign Policy Optimization) that does not
  require knowledge of the link function relating human preferences to reward values.
  The core method estimates the sign of value function differences directly from human
  feedback instead of estimating the full value function difference, avoiding the
  need to know the link function.
---

# Provable Reinforcement Learning from Human Feedback with an Unknown Link Function

## Quick Facts
- arXiv ID: 2506.03066
- Source URL: https://arxiv.org/abs/2506.03066
- Authors: Qining Zhang; Lei Ying
- Reference count: 40
- Key outcome: Proposes ZSPO (Zeroth-Order Sign Policy Optimization) that achieves O(d/T + 1/σ1(0)N^(1/4) + H/D^(1/4)) convergence without knowing the link function

## Executive Summary
This paper introduces a novel reinforcement learning from human feedback (RLHF) algorithm called ZSPO that provably learns optimal policies without requiring knowledge of the link function relating human preferences to rewards. The key insight is to estimate the sign of value function differences directly from human feedback rather than estimating full value differences, avoiding the need to know the exact preference model. Under smoothness assumptions, ZSPO achieves a convergence rate that includes terms for zeroth-order optimization error, majority vote approximation error, and panelist distinguishability limits.

## Method Summary
ZSPO (Zeroth-Order Sign Policy Optimization) is a policy optimization algorithm that uses only binary human preferences as feedback. At each iteration, it perturbs the current policy parameter by a random vector, collects trajectory batches under both perturbed and original policies, and queries human feedback to determine which policy is preferred. The algorithm uses majority voting over N batch comparisons to estimate the sign of the value function difference, which is then used to construct a gradient estimator for policy updates. The method avoids the need to know the link function by working directly with preference signs rather than absolute reward values.

## Key Results
- ZSPO achieves O(d/T + 1/σ1(0)N^(1/4) + H/D^(1/4)) convergence rate for finding ε-stationary policies
- The algorithm is robust to unknown link functions, performing comparably under both Bradley-Terry and linear preference models
- Empirically outperforms baseline methods (RM+PPO, DPO, Online DPO, ZPG) on a stochastic GridWorld task when there's model mismatch
- Convergence rate analysis identifies tradeoffs between zeroth-order optimization error, majority vote approximation error, and panelist distinguishability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating the sign of value function differences from human feedback eliminates the need to know the link function
- Mechanism: The algorithm perturbs the current policy parameter θ_t by a random vector v_t with distance μ, then uses majority voting over N trajectory-batch comparisons to determine whether the perturbed policy is better. This binary signal (sign of value difference) is sufficient to construct a gradient estimator ĝ_t = sign[Σ(o_{t,n} - 1/2)]·v_t that is positively correlated with the true policy gradient
- Core assumption: The link function σ(·) is strictly monotonic (Assumption 1), so preference probability increases with reward difference, but its exact form is unknown
- Break condition: If the perturbation distance μ is too small, the value function difference falls below the panelist distinguishability threshold ε*_D, and majority voting yields random noise rather than the true sign

### Mechanism 2
- Claim: Majority voting over N batch-pairs converges to the expected preference sign with error O(ς^{-1}(√(2/N)))
- Mechanism: For each of N iterations, sample D trajectories from both the current and perturbed policies to form batches, query a panelist, and accumulate binary outcomes o_{t,n}. The sign of (Σo_{t,n} - N/2) approximates the sign of the expected preference deviation, which (by Assumption 1) aligns with the sign of the value function difference
- Core assumption: The link function σ(·) is L-smooth with σ'(0) > 0 (Assumption 2), ensuring the deviation function ς is well-behaved near the origin
- Break condition: If the number of batches N is too small, majority vote error dominates; if panelists have inconsistent or adversarial preferences, the assumption of monotonic σ(·) is violated

### Mechanism 3
- Claim: The gradient estimator ĝ_t = sign[Σ(o_{t,n} - 1/2)]·v_t has positive correlation with ∇_θ V(π_θ) in expectation
- Mechanism: When the perturbation v_t aligns with the gradient direction (positive inner product), the perturbed policy tends to have higher value, so majority vote yields positive sign, and the update moves along v_t. When v_t opposes the gradient, the sign flips, and the update moves along -v_t (still gradient-aligned). This sign-weighting approximates |⟨∇V, v_t⟩| scaling in expectation
- Core assumption: The value function V(π_θ) is L-smooth in θ (Assumption 3), so linear approximation holds locally
- Break condition: If the perturbation μ is too large, linearization error exceeds the value difference; if μ is too small, distinguishability fails

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: ZSPO operates without gradient access, using only function-value comparisons (via preferences) to estimate descent directions
  - Quick check question: Given a function f, explain how you would estimate ∇f at point x using only f(x) and f(x + μv) for random v

- Concept: Link functions in RLHF (Bradley-Terry, Thurstone, etc.)
  - Why needed here: The paper's core innovation is avoiding link-function specification
  - Quick check question: Under the Bradley-Terry model, what is P(τ_1 > τ_0) when r(τ_1) - r(τ_0) = 1 and the scale parameter γ = 1?

- Concept: Episodic MDPs and value functions
  - Why needed here: The algorithm optimizes V(π_θ) = E_{s~μ_0}[V^π_1(s)], the expected return under policy π_θ
  - Quick check question: In an episodic MDP with horizon H, why is the value function bounded in [0, H] when per-step rewards are bounded in [0, 1]?

## Architecture Onboarding

- Component map: Policy network N_θ -> Perturbation generator -> Trajectory sampler -> Preference oracle -> Aggregator -> Updater
- Critical path: 1. Hyperparameter selection (μ, α_t, N, D) 2. Perturbation sampling and trajectory generation 3. Preference collection and majority voting 4. Parameter update
- Design tradeoffs: μ too large → linearization error; μ too small → indistinguishable policies; N large → better majority vote but more queries; D large → lower ε*_D but cognitive limits
- Failure signatures: Plateau at high gradient norm (μ too small); oscillation (N too small); divergence (α_t too large); systematic bias (link function non-smooth)
- First 3 experiments: 1. GridWorld validation comparing ZSPO vs baselines under different link functions 2. Ablation study on perturbation distance μ 3. Sensitivity analysis to panelist quality (σ'(0))

## Open Questions the Paper Calls Out

- Question: Can the global convergence of ZSPO be theoretically guaranteed without assuming convexity or the Polyak-Łojasiewicz (PL) condition?
- Question: Is the method of using majority voting over batches to estimate the sign of value function differences the optimal approach for utilizing preference panelists?
- Question: Can the perturbation distance μ be selected optimally without prior knowledge of the unknown link function σ(·) or the distinguishability constant ε_D^*?

## Limitations

- Theoretical assumptions require smoothness of value function and link function, which may not hold in practice
- Convergence rate depends critically on hyperparameter choices (μ, N, D) that are left as open problems
- Analysis assumes access to a perfect preference oracle, but real-world panelists may provide inconsistent or adversarial feedback
- The method may struggle with non-smooth or discontinuous link functions that arise in practical human feedback scenarios

## Confidence

- **High confidence**: The mechanism of using sign-based gradient estimation to avoid link-function specification is technically sound
- **Medium confidence**: The convergence rate is derived under stated assumptions, but practical performance depends heavily on hyperparameter tuning
- **Low confidence**: The claim that ZSPO is robust to link-function mismatch requires empirical validation across diverse preference elicitation scenarios

## Next Checks

1. Implement hyperparameter sensitivity analysis to identify optimal μ, N, D values empirically and compare to Corollary 1's theoretical guidance
2. Test ZSPO with real human feedback to verify the monotonic link function assumption and identify failure modes from inconsistent preferences
3. Extend the convergence analysis to handle non-smooth or discontinuous link functions that may arise in practical human feedback scenarios