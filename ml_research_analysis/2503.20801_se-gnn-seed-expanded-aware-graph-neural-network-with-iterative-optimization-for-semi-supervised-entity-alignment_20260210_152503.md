---
ver: rpa2
title: 'SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative Optimization
  for Semi-supervised Entity Alignment'
arxiv_id: '2503.20801'
source_url: https://arxiv.org/abs/2503.20801
tags:
- entity
- seed
- alignment
- information
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SE-GNN addresses the problem of entity alignment in knowledge graphs
  by proposing a seed-expanded-aware graph neural network with iterative optimization.
  The key challenge is the heavy reliance on pre-aligned seed pairs and the underestimation
  of embedding distortion caused by noisy seed pairs in existing methods.
---

# SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative Optimization for Semi-supervised Entity Alignment

## Quick Facts
- **arXiv ID:** 2503.20801
- **Source URL:** https://arxiv.org/abs/2503.20801
- **Authors:** Tao Meng; Shuo Shan; Hongen Shao; Yuntao Shou; Wei Ai; Keqin Li
- **Reference count:** 40
- **Primary result:** Reduces dependence on pre-aligned seeds and eliminates embedding distortion from noisy seeds through seed expansion and iterative Xavier correction

## Executive Summary
SE-GNN addresses entity alignment in knowledge graphs by combining semantic attributes and structural features for seed expansion, then employing local and global awareness mechanisms with iterative optimization. The approach reduces reliance on scarce pre-aligned seed pairs while addressing embedding distortion caused by noisy seeds through Xavier reinitialization. Experiments on multiple public datasets demonstrate superior performance over existing methods.

## Method Summary
SE-GNN uses a three-stage approach: seed expansion combining BGE semantic embeddings with GCN-aggregated neighborhood semantics via CSLS-adjusted similarity matrices; local-global awareness mechanism (LGAM) that jointly propagates relation-aware local information and semantic-aware global information; and iterative optimization with threshold nearest neighbor embedding correction strategy (TNECS) that resets embeddings every 30 epochs to eliminate distortion from noisy seeds.

## Key Results
- Achieves significant performance improvements across DBP15K, SRPRS, and DWY100K datasets
- Demonstrates robustness to varying seed ratios (10-40%) with minimal performance variance
- Ablation studies show LGAM contributes +32% Hit@1 improvement over GCN-Align baseline
- TNECS correction strategy maintains performance stability across iterative cycles

## Why This Works (Mechanism)

### Mechanism 1
- Combining semantic attributes with structural features improves initial seed pair quality over single-source structural information alone.
- Seed expansion uses BGE embeddings for entity semantics, aggregates neighborhood semantic embeddings via GCN, computes CSLS-adjusted similarity matrices for both, and fuses them with parameter ε to select initial potential seed pairs through bidirectional nearest neighbor filtering with threshold θ_sem.
- Core assumption: Equivalent entities exhibit semantic similarity in both their attributes and their neighbors' attributes, and CSLS local scaling improves match differentiation when candidate similarities are uniformly high.
- Evidence: [abstract] introduces seed expansion strategy combining semantic attributes and structural features; [section III.C] Equations 1-10 define semantic embedding via BGE, neighborhood aggregation via GCN, CSLS adjustment, matrix fusion, and bidirectional nearest neighbor selection.

### Mechanism 2
- Joint propagation of local relation information and global semantic information alleviates structural heterogeneity across knowledge graphs.
- Local Relation Awareness aggregates neighbor and relation embeddings through attention-weighted differences, stacking layers to expand receptive field. Global Entity Awareness selects top-K semantic high-order neighbors via CSLS-computed distances and aggregates them through attention-weighted summation. Final embeddings concatenate local and global representations.
- Core assumption: Entities with high semantic similarity provide meaningful high-order structural signals even without direct graph connectivity, and relation-specific attention captures heterogeneity in edge semantics.
- Evidence: [abstract] employs local and global awareness mechanism to obtain comprehensive entity embedding representation; [section III.D] Equations 11-21 detail local relation attention coefficients, global entity attention, multi-layer stacking, and final concatenation.

### Mechanism 3
- Embedding correction via Xavier reinitialization eliminates distortion accumulation from noisy seed pairs during semi-supervised iteration.
- TNECS selects optimized potential seed pairs using CSLS-adjusted final embedding similarity, bidirectional nearest neighbor constraints, and threshold θ_fin. After updating expanded seed set, Xavier initialization resets embeddings to uniformly distributed state before re-entering LGAM, breaking distortion propagation cycles.
- Core assumption: Noisy seed pairs cause embedding distances to shrink erroneously during loss minimization, and periodic reinitialization prevents this distortion from persisting across iterations while preserving learned structural patterns through re-training.
- Evidence: [abstract] utilizes threshold nearest neighbor embedding correction strategy to select iterative potential seed pairs and eliminate embedding distortion; [section III.E] Algorithm 1 and Eq. 25 define Xavier correction with optimization round interval set to 30 and 3 total TNECS updates.

## Foundational Learning

- **Cross-domain similarity local scaling (CSLS)**
  - Why needed: Standard cosine similarity produces uniform high scores when source entities match many candidates; CSLS adjusts by averaging Top-Q neighbor distances to improve differentiation.
  - Quick check: Given similarity matrix M_cos, can you compute M_csls = 2*M_cos - M_avg1 - M_avg2^T where M_avg1 averages each row's Top-Q values?

- **Semi-supervised iterative seed expansion**
  - Why needed: Pre-aligned seeds are scarce and expensive; iterative expansion leverages model predictions to grow training data while risking noise accumulation.
  - Quick check: How does bidirectional nearest neighbor filtering (requiring i→j AND j→i mutual nearest status) reduce false positives compared to unidirectional selection?

- **Xavier initialization for embedding correction**
  - Why needed: Noisy seeds cause gradient descent to shrink embedding distances erroneously; Xavier resets weights to uniform distribution with variance 2/(fan_in + fan_out), stabilizing activations.
  - Quick check: Why is Xavier preferred over random reinitialization for preventing gradient vanishing in deep GNN layers?

## Architecture Onboarding

- **Component map:**
  - Seed Expansion: BGE semantic encoder → GCN neighborhood aggregator → CSLS similarity matrices → Threshold + bidirectional NN selector → Initial potential seeds S_I
  - Local Global Awareness Mechanism (LGAM): Local Relation Attention + Global Entity Attention → Concatenation → Final embeddings H_final
  - Iterative Optimization: TNECS selector → Xavier correction → Updated seed set S_E → Re-feed to LGAM

- **Critical path:**
  1. Preprocess: Standardize entity names to English, compute BGE embeddings, aggregate neighborhood semantics via GCN
  2. Seed expansion: Fuse semantic and neighborhood matrices, apply CSLS, filter with θ_sem=0.01 and bidirectional NN → S_I
  3. LGAM training: Input S_E to local/global attention modules, train with LogSumExp loss using RMSprop, lr=0.01
  4. Every 30 epochs: Execute TNECS → select S_O with θ_fin=0.05 → Xavier correct embeddings → update S_E
  5. After 3 TNECS cycles: Output final embeddings and compute alignment ranking

- **Design tradeoffs:**
  - High-order neighbor count K: Larger K captures more global context but increases O(|E|×K) aggregation cost; ablation shows K=15 optimal
  - GNN depth l: Deeper networks expand receptive field but risk over-smoothing; l=2 balances local/global information
  - Seed ratio: Lower ratio reduces labeling cost but may starve initial training; experiments show robustness from 10-40%

- **Failure signatures:**
  - Hit@1 plateaus early: Check if θ_sem too high (insufficient seed expansion) or CSLS Q parameter misconfigured
  - MRR degrades across iterations: Noise seeds accumulating; verify Xavier correction executing at correct round intervals
  - Sparse graph performance collapse (SRPRS): Global entity awareness critical; confirm K≥15 and semantic neighbors selected via CSLS not raw cosine

- **First 3 experiments:**
  1. Ablate LGAM components: Train GCN-Align baseline on DBP15K, incrementally add Local Relation Awareness then Global Entity Awareness to isolate contribution
  2. Vary seed ratio robustness: Test SE-GNN(semi) on DBP15K with 10%, 20%, 30%, 40% pre-aligned seeds to verify low dependency claim
  3. TNECS interval sweep: Run optimization round intervals 10, 20, 30, 40 on DBP15K to identify peak and validate noise correction timing

## Open Questions the Paper Calls Out

None

## Limitations

- **Memory complexity concerns:** Full |E1|×|E2| similarity matrix computation may cause OOM on large datasets like DWY100K without sparse approximations
- **Translation mechanism uncertainty:** Exact method for standardizing entity names across languages for BGE encoding is unspecified
- **Optimizer state reset ambiguity:** Unclear whether RMSprop momentum buffers are reset during Xavier embedding correction

## Confidence

- **High** for LGAM architecture design and seed expansion strategy combining semantic/structural features
- **Medium** for TNECS noise correction mechanism, as ablation results show effectiveness but implementation nuances are unclear
- **Low** for DWY100K results, given memory complexity analysis suggests OOM conditions without sparse approximations

## Next Checks

1. Implement and verify CSLS-adjusted similarity matrix computation (M_csls = 2*M_cos - M_avg1 - M_avg2^T) on small synthetic dataset
2. Test TNECS Xavier correction with varying round intervals (10, 20, 30, 40) on DBP15K to replicate Fig. 5b trends
3. Reproduce ablation study from Table IV by incrementally adding LRA and GEA components to GCN-Align baseline