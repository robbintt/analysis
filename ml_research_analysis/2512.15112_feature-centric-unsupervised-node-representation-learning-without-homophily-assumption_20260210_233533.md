---
ver: rpa2
title: Feature-Centric Unsupervised Node Representation Learning Without Homophily
  Assumption
arxiv_id: '2512.15112'
source_url: https://arxiv.org/abs/2512.15112
tags:
- graph
- node
- fuel
- separability
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FUEL tackles the challenge of unsupervised node representation
  learning in non-homophilic graphs by adaptively adjusting the degree of graph convolution
  usage. Instead of relying on a fixed amount of neighborhood aggregation, FUEL learns
  to use graph convolution only when it improves class separability.
---

# Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption

## Quick Facts
- **arXiv ID**: 2512.15112
- **Source URL**: https://arxiv.org/abs/2512.15112
- **Authors**: Sunwoo Kim; Soo Yong Lee; Kyungho Kim; Hyunjin Hwang; Jaemin Yoo; Kijung Shin
- **Reference count**: 6
- **Primary result**: FUEL achieves best average ranking across 14 datasets and 15 baselines in both node classification and clustering tasks

## Executive Summary
FUEL addresses unsupervised node representation learning in non-homophilic graphs by adaptively controlling graph convolution usage. Instead of fixed neighborhood aggregation, it learns when to use graph convolution based on whether it improves class separability. The method treats feature-based clusters as proxy classes and optimizes convolution weights to enhance intra-cluster cohesion and inter-cluster separation. FUEL demonstrates superior performance across diverse real-world datasets with varying homophily.

## Method Summary
FUEL operates in two steps: (1) an adaptive graph convolution model learns optimal mixing coefficients between raw features and neighborhood-aggregated features, guided by a clustering-based loss that treats feature-based clusters as proxy classes; (2) a refinement step further boosts latent-class separability using nearest-neighbor similarity in the intermediate embedding space. The method adaptively determines the degree of graph convolution usage rather than relying on fixed aggregation, making it effective for both homophilic and non-homophilic graphs.

## Key Results
- Achieves best average ranking across 14 datasets in both node classification and clustering tasks
- Outperforms 15 baseline methods on datasets with varying homophily ratios
- Ablation study shows refinement step significantly improves performance (w/o Step 2 shows degradation)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptively weighting graph convolution prevents embedding of dissimilar nodes from collapsing in heterophilic graphs
- **Mechanism:** FUEL learns scalar coefficients (α₀, α₁, α₂) determining linear mix of raw features (X), 1-hop neighbors (ÃX), and 2-hop neighbors (Ã²X). This effectively "turns down" neighbor smoothing when local topology is noisy or dissimilar
- **Core assumption:** Node features contain primary signal for class membership even when graph topology connects dissimilar nodes
- **Evidence anchors:** Learns to use graph convolution only when it improves class separability; contains learnable parameters that directly control impact of graph convolution
- **Break condition:** If node features are completely uncorrelated with class labels, adaptive weights may converge to arbitrary values

### Mechanism 2
- **Claim:** Optimizing for "latent-class separability" serves as functional proxy for learning class separability without ground truth labels
- **Mechanism:** Uses feature-based clusters as pseudo-labels and maximizes Calinski-Harabasz index via custom loss function (L_clus) to force learned embeddings to group similar nodes while pushing dissimilar ones apart
- **Core assumption:** Underlying class structure roughly aligns with natural clustering of node features in feature space
- **Evidence anchors:** Treats feature-based clusters as proxy classes and optimizes graph convolution weights to enhance intra-cluster cohesion
- **Break condition:** If true class boundaries are highly non-convex or overlap significantly with other classes in feature space

### Mechanism 3
- **Claim:** Refinement step treating nearest neighbors in intermediate embedding space as positive pairs sharpens class boundaries
- **Mechanism:** After determining optimal convolution weights, feed-forward network (f_θ) further processes embeddings by minimizing distance loss (L_dist) that pulls node closer to N nearest neighbors while pushing away from non-neighbors
- **Core assumption:** Intermediate embeddings from Step 1 are of sufficient quality that nearest neighbors likely belong to same semantic class
- **Evidence anchors:** Refines node embeddings to further enhance separability among clusters; ablation shows performance degradation without Step 2
- **Break condition:** If intermediate embeddings are highly entangled, reinforcing neighbor similarity could inadvertently bridge distinct classes

## Foundational Learning

- **Concept: Homophily vs. Heterophily**
  - **Why needed here:** Architecture built to handle graphs where "connected nodes are dissimilar"; standard GNNs fail in heterophily because they over-smooth features
  - **Quick check question:** Does dataset have edge homophily ratio closer to 0.0 (heterophily) or 1.0 (homophily)?

- **Concept: Polynomial Graph Filters**
  - **Why needed here:** FUEL uses sum of normalized adjacency powers (I, Ã, Ã²); understanding that Ã² aggregates information from 2-hop neighbors explains why model might prefer α₂ over α₁ in specific structural contexts
  - **Quick check question:** What structural information does 2-hop aggregation capture that 1-hop aggregation misses?

- **Concept: Clustering as Proxy Labels**
  - **Why needed here:** Unsupervised learning requires signal; method uses geometric properties of feature space (clusters) to generate that signal
  - **Quick check question:** Why is Calinski-Harabasz index used here instead of simple Euclidean distance?

## Architecture Onboarding

- **Component map:** Input (Node Features X & Adjacency A) -> Step 1 (Adaptive Conv: calculates ÃX and Ã²X; mixes via learnable α weights → Intermediate Embeddings H) -> Step 1 (Clustering Head: assigns H to cluster centroids; calculates L_clus; backprops to α weights) -> Step 2 (Refinement: MLP (f_θ) takes H; minimizes distance to N nearest neighbors → Final Embeddings Z)

- **Critical path:** Estimation of α coefficients in Step 1. If these weights are wrong (e.g., over-weighting neighbors in heterophilic graph), intermediate embeddings H will be noisy, and refinement step may fail to recover signal

- **Design tradeoffs:**
  - Simplicity vs. Expressiveness: Convolution model is simple linear mix (Eq. 2), computationally efficient but may fail to capture complex non-linear topology filters
  - Cluster Sensitivity: Method requires setting number of clusters (C); paper claims low sensitivity, but extreme mismatch between C and actual classes could degrade proxy signal

- **Failure signatures:**
  - Weight Collapse: If α₀ goes to 0, model ignores graph entirely (reverts to raw features)
  - Cluster Collapse: If entropy loss (L₂) is too weak, all nodes might be assigned to single cluster, breaking proxy signal

- **First 3 experiments:**
  1. Homophily Stress Test: Run FUEL on synthetic graphs where you tune homophily ratio from 0.1 to 0.9. Plot learned α weights against homophily ratio to verify they adapt as expected (high α₁ for high homophily, high α₀ for low homophily)
  2. Cluster Count Ablation: Vary number of clusters C (e.g., 50%, 100%, 200% of actual classes) on benchmark dataset to confirm claim of low sensitivity
  3. Refinement Isolation: Visualize embeddings using t-SNE before Step 2 (intermediate H) and after Step 2 (final Z) to qualitatively assess "sharpening" of class boundaries

## Open Questions the Paper Calls Out
None

## Limitations
- Refinement network architecture (f_θ) is underspecified, making exact reproduction difficult
- Hyperparameter sensitivity (temperature τ, neighbor count N, loss coefficients) is not fully explored
- Cluster count C selection impact is only qualitatively addressed

## Confidence

- **High Confidence**: Adaptive convolution mechanism works as described for heterophilic graphs
- **Medium Confidence**: Clustering-based proxy learning provides valid signal when features correlate with labels
- **Medium Confidence**: Refinement step improves separability when intermediate embeddings are of reasonable quality
- **Low Confidence**: Method's robustness to extreme feature-class misalignment is not demonstrated

## Next Checks
1. **Homophily-Weight Correlation**: Plot learned α weights against ground-truth homophily ratios on synthetic datasets to verify adaptive behavior
2. **Feature-Label Alignment Stress Test**: Evaluate performance degradation on datasets where node features are intentionally decorrelated from labels
3. **Cluster Sensitivity Analysis**: Systematically vary cluster count C (50%, 100%, 200% of actual classes) to quantify impact on downstream performance