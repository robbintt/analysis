---
ver: rpa2
title: 'SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python
  Code'
arxiv_id: '2512.05954'
source_url: https://arxiv.org/abs/2512.05954
tags:
- reasoning
- accuracy
- problem
- question
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SymPyBench, a dynamic benchmark for evaluating
  scientific reasoning in physics with 15,045 parameterized problems. Each problem
  includes step-by-step reasoning and executable Python code that produces ground-truth
  solutions for any parameter set.
---

# SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code

## Quick Facts
- **arXiv ID:** 2512.05954
- **Source URL:** https://arxiv.org/abs/2512.05954
- **Reference count:** 32
- **Key outcome:** Introduces SymPyBench with 15,045 parameterized physics problems, three novel evaluation metrics, and demonstrates significant performance differences across state-of-the-art LLMs in scientific reasoning tasks.

## Executive Summary
SymPyBench is a dynamic benchmark designed to evaluate scientific reasoning capabilities in physics through 15,045 parameterized problems with executable Python code providing ground-truth solutions. The benchmark features three complementary formats—MC-Symbolic, MC-Numerical, and free-form questions—that test different aspects of reasoning ability. Beyond standard accuracy metrics, the authors introduce three novel evaluation measures: Consistency Score, Failure Rate, and Confusion Rate, which quantify model variability and uncertainty across problem variants.

The benchmark reveals substantial performance gaps between top LLMs, with Anthropic Sonnet-3.7 and Llama4-Maverick achieving over 64% exact match accuracy. However, Consistency Scores range from 5.66% to 42.42%, highlighting significant differences in generalization ability. The results show that MC-Symbolic questions are generally easier than MC-Numerical due to computational challenges, while free-form questions present the greatest difficulty due to structural complexity and the requirement for answer generation rather than selection.

## Method Summary
SymPyBench employs a parameterized problem generation approach where each problem can be instantiated with different parameter values while maintaining the same underlying reasoning structure. The benchmark includes 15,045 problems across physics domains, with each problem featuring step-by-step reasoning and executable Python code that generates ground-truth solutions for any parameter set. The three question formats serve complementary purposes: MC-Symbolic tests symbolic manipulation and algebraic reasoning, MC-Numerical evaluates computational accuracy and numerical precision, and free-form questions assess complete problem-solving capabilities including solution structure and presentation. The dynamic nature allows systematic evaluation of model behavior under controlled perturbations, revealing both strengths and limitations in scientific reasoning capabilities.

## Key Results
- Top models like Anthropic Sonnet-3.7 and Llama4-Maverick achieve over 64% exact match accuracy on SymPyBench
- Consistency Scores vary widely from 5.66% to 42.42% across models, indicating different levels of robustness
- MC-Symbolic questions are easier than MC-Numerical due to computational challenges in numerical evaluation
- Free-form questions are most difficult due to structural complexity and answer generation requirements

## Why This Works (Mechanism)
SymPyBench's effectiveness stems from its dynamic parameterization approach that enables systematic evaluation of model behavior across controlled variations. The executable Python code provides precise ground-truth solutions, eliminating ambiguity in evaluation. The three-format design captures different dimensions of scientific reasoning: symbolic manipulation, numerical computation, and complete problem-solving. The novel evaluation metrics go beyond simple accuracy to reveal model reliability, uncertainty patterns, and consistency across problem variants.

## Foundational Learning
- **Parameterized Problem Generation** - Creating problem families with shared structure but different parameters; needed to systematically evaluate generalization; quick check: verify all variants share same solution methodology
- **Executable Ground Truth Generation** - Using Python code to produce exact solutions; needed for unambiguous evaluation; quick check: confirm code produces correct answers across parameter ranges
- **Consistency Metrics** - Measuring model reliability across problem variants; needed to assess robustness; quick check: calculate consistency scores for simple model behaviors
- **Three-Format Design** - Complementary evaluation of symbolic, numerical, and free-form reasoning; needed to capture full reasoning spectrum; quick check: verify each format tests distinct reasoning skills

## Architecture Onboarding
**Component Map:** Problem Generator -> Question Formatter -> Python Executor -> Answer Evaluator -> Consistency Analyzer

**Critical Path:** Parameterized problem generation → Multiple question format instantiation → Python code execution for ground truth → Model response evaluation → Consistency and uncertainty metric calculation

**Design Tradeoffs:** Dynamic parameterization enables systematic evaluation but may create artificial scenarios; executable Python provides precise ground truth but may bias toward computational approaches; three-format design captures complexity but increases evaluation overhead

**Failure Signatures:** Low Consistency Scores indicate poor generalization across variants; High Failure Rates suggest fundamental misunderstanding of problem structure; High Confusion Rates reveal uncertainty in model decision-making; Performance gaps between MC-Symbolic and MC-Numerical indicate computational limitations

**First Experiments:** 1) Test single model on all three formats to establish baseline performance patterns, 2) Evaluate model consistency across parameter variations for specific problem types, 3) Compare performance on symbolic versus numerical variants of identical problems

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on Python execution may not capture full spectrum of scientific reasoning approaches
- Benchmark coverage limited to specific physics domains, potentially missing other scientific reasoning areas
- Parameterized generation may create artificial scenarios that don't reflect real-world problem complexity
- Focus on SymPy-compatible solutions may exclude valid scientific reasoning approaches using alternative mathematical frameworks

## Confidence
**High Confidence:** Benchmark construction methodology and evaluation metrics are well-documented and mathematically sound
**Medium Confidence:** Claims about relative difficulty of question formats are supported but require further validation
**Low Confidence:** Generalization to broader scientific reasoning beyond physics remains uncertain

## Next Checks
1. Test model performance on SymPyBench problems after systematic removal of Python execution component to assess ground truth generation bias
2. Expand benchmark coverage to include chemistry and biology problems to evaluate cross-domain generalization
3. Conduct human expert evaluation comparing model solutions against expert problem-solving approaches to identify gaps between benchmark performance and authentic scientific reasoning