---
ver: rpa2
title: Ontology Generation using Large Language Models
arxiv_id: '2503.05388'
source_url: https://arxiv.org/abs/2503.05388
tags:
- ontology
- generation
- llms
- prompting
- ontologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two new prompting techniques, Memoryless CQbyCQ
  and Ontogenia, for automated ontology generation from competency questions and user
  stories using Large Language Models (LLMs). The authors conduct experiments on a
  benchmark dataset of ten ontologies with 100 competency questions and 29 user stories,
  comparing three LLMs (GPT-4, o1-preview, Llama-3.1-405B).
---

# Ontology Generation using Large Language Models

## Quick Facts
- arXiv ID: 2503.05388
- Source URL: https://arxiv.org/abs/2503.05388
- Reference count: 40
- New prompting techniques (Memoryless CQbyCQ, Ontogenia) for automated ontology generation from competency questions and user stories using LLMs

## Executive Summary
This paper introduces two novel prompting techniques, Memoryless CQbyCQ and Ontogenia, for automated ontology generation using Large Language Models. The authors conduct experiments on a benchmark dataset of ten ontologies with 100 competency questions and 29 user stories, comparing three LLMs (GPT-4, o1-preview, Llama-3.1-405B). Their multi-dimensional evaluation approach combines OOPS! ontology metrics, structural analysis of superfluous elements, and expert qualitative assessment. The results demonstrate that o1-preview with Ontogenia produces ontologies meeting ontology engineers' requirements, significantly outperforming novice engineers in modeling ability, while GPT-4 with Memoryless CQbyCQ achieves comparable results. Both techniques correctly model 84-91% of competency questions, though they generate some superfluous elements and occasional modeling errors, particularly in complex competency questions involving reification and restrictions.

## Method Summary
The authors propose two prompting techniques for ontology generation: Memoryless CQbyCQ processes each competency question independently, concatenating all questions with the ontology-in-progress and relying on the LLM's memory for context, while Ontogenia processes one competency question at a time with explicit instructions to retain learned information. Both techniques use the same foundational prompt that includes ontology definitions, competency questions, user stories, and instructions for class definitions and restrictions. The evaluation framework employs a multi-dimensional approach combining OOPS! ontology metrics (detecting modeling errors), structural analysis of superfluous elements, and qualitative expert assessment. Experiments are conducted on three LLMs (GPT-4, o1-preview, Llama-3.1-405B) using a benchmark of ten ontologies with 100 competency questions and 29 user stories.

## Key Results
- o1-preview with Ontogenia significantly outperforms novice engineers in modeling ability, producing ontologies meeting ontology engineers' requirements
- GPT-4 with Memoryless CQbyCQ achieves comparable results to o1-preview, correctly modeling 84-91% of competency questions
- Both techniques generate some superfluous elements and occasional modeling errors, particularly for complex competency questions involving reification and restrictions
- Memoryless CQbyCQ has lower latency (41-89 seconds) compared to Ontogenia (88-95 seconds) due to the absence of tree-building operations

## Why This Works (Mechanism)
The paper demonstrates that LLMs can effectively generate ontologies by leveraging their reasoning capabilities and knowledge of semantic web standards. The two prompting techniques work by breaking down complex ontology generation tasks into manageable components - either processing all questions together (Memoryless CQbyCQ) or sequentially with knowledge retention (Ontogenia). The multi-dimensional evaluation approach provides a comprehensive assessment by combining automated metrics (OOPS!), structural analysis, and expert judgment. The success of o1-preview suggests that LLMs with stronger reasoning capabilities perform better at complex ontology modeling tasks, particularly those requiring understanding of domain semantics and relationships.

## Foundational Learning

**Ontology engineering** - Why needed: Provides the theoretical foundation for understanding ontology structures and requirements