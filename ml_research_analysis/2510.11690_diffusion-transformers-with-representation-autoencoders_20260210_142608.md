---
ver: rpa2
title: Diffusion Transformers with Representation Autoencoders
arxiv_id: '2510.11690'
source_url: https://arxiv.org/abs/2510.11690
tags:
- diffusion
- training
- representation
- generation
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using pretrained representation encoders (e.g.,
  DINO, SigLIP, MAE) with trained decoders as autoencoders for diffusion models, instead
  of traditional VAEs. The authors argue that VAEs have limitations such as outdated
  backbones, low-dimensional latents, and weak representations, which limit generative
  quality.
---

# Diffusion Transformers with Representation Autoencoders

## Quick Facts
- **arXiv ID**: 2510.11690
- **Source URL**: https://arxiv.org/abs/2510.11690
- **Reference count**: 40
- **Primary result**: RAE-based DiT achieves 1.51 FID at 256×256 (no guidance) and 1.13 at both 256×256 and 512×512 (with guidance)

## Executive Summary
This paper proposes replacing traditional VAEs with representation autoencoders (RAEs) in diffusion transformers. RAEs use frozen pretrained representation encoders (DINO, SigLIP, MAE) paired with trained lightweight decoders. The authors argue that VAEs have limitations such as outdated backbones, low-dimensional latents, and weak representations that limit generative quality. RAEs provide semantically rich latent spaces and high-fidelity reconstructions. The paper introduces a new DiT variant, DiT DH, with a lightweight, wide DDT head to handle the higher dimensionality of RAE latents, achieving strong image generation results on ImageNet.

## Method Summary
The method replaces VAE latents in diffusion transformers with latents from representation autoencoders. RAEs use frozen pretrained encoders (DINOv2, SigLIP, MAE) that capture global semantic structure through self-supervised objectives. These are paired with trained lightweight decoders for high-fidelity reconstruction. The paper introduces DiT DH, a new variant with a 2-layer, 2048-dim DDT head designed for high-dimensional RAE latents. Key innovations include a dimension-dependent noise schedule shift and noise-augmented decoder training to improve generation quality by making decoders robust to noisy diffusion outputs.

## Key Results
- RAE-based DiT achieves 1.51 FID at 256×256 (no guidance) and 1.13 at both 256×256 and 512×512 (with AutoGuidance)
- 47× training speedup vs SiT-XL on VAE latents, with comparable or better generation quality
- DINOv2-B encoder provides best balance of reconstruction and generation quality (rFID 0.49, gFID 4.28)
- DiT width must match or exceed token dimensionality for convergence (Theorem 1, Figure 3)

## Why This Works (Mechanism)

### Mechanism 1
Frozen pretrained representation encoders provide semantically richer latent spaces than VAEs. Pretrained encoders capture global semantic structure through self-supervised objectives, enabling high-fidelity reconstruction and faster diffusion convergence without auxiliary alignment losses. Semantic structure in latent space correlates with generation quality and learning efficiency. Evidence: RAE-based DiT achieves 47× training speedup; DINOv2-B achieves 84.5% linear probing accuracy vs SD-VAE's 8.0%; RAE achieves strong gFID scores.

### Mechanism 2
DiT width must match or exceed token dimensionality for diffusion to converge. Gaussian noise injection extends the data distribution's support to full space, requiring model capacity proportional to full dimensionality. When width < token dim, the low-rank bottleneck prevents loss convergence. Evidence: Theorem 1 proves lower bound on loss when d < n; DiT-B (d=768) succeeds with DINOv2-B (n=768) but DiT-S (d=384) fails; DiT-S only succeeds with DINOv2-S (n=384).

### Mechanism 3
Noise-augmented decoder training improves generation quality by making decoders robust to out-of-distribution latents. RAE decoders trained only on clean discrete latents struggle with continuous diffusion outputs. Adding Gaussian noise during decoder training smooths the latent distribution, improving generalization to noisy diffusion samples at cost of slightly worse exact reconstruction. Evidence: Noise augmentation improves gFID from 4.81 to 4.28 while rFID degrades slightly from 0.49 to 0.57; trade-off consistent across τ values.

## Foundational Learning

- **Latent Diffusion Models**: Why needed: RAEs replace VAEs as the latent space; understanding why diffusion operates in latent space (efficiency, semantic structure) is prerequisite. Quick check: Can you explain why diffusion in a compressed latent space is more efficient than pixel-space diffusion?

- **Vision Transformer (ViT) Architecture and Patch Embeddings**: Why needed: Both RAE encoders (frozen ViTs) and DiT backbone use transformer architectures with patch-based tokenization; token count and dimensionality are critical design parameters. Quick check: Given a 256×256 image with patch size 14, how many tokens does a ViT produce?

- **Flow Matching / Rectified Flow**: Why needed: Paper uses flow matching with linear interpolation x_t = (1-t)x + tε for training; understanding velocity prediction is essential for implementation. Quick check: What does the velocity field v(x_t, t) = E[ε - x | x_t] represent in flow matching?

## Architecture Onboarding

- **Component map**: Input Image → Frozen Encoder (DINOv2/SigLIP/MAE) → 256 tokens × d_dim → LayerNorm → DiT Backbone → DDT Head → predicted velocity → Diffusion Training: Flow Matching Loss. At Inference: Sampled Latents → Noise-augmented Decoder → Reconstructed Image.

- **Critical path**:
  1. Encoder selection and freezing: DINOv2-B recommended; freeze weights, apply per-token LayerNorm without affine params
  2. DiT width sizing: Must satisfy width ≥ token dimensionality; for DINOv2-B (d=768), minimum DiT-B; DiT-XL provides margin
  3. Noise schedule shift: Apply dimension-dependent shift t_m = α·t_n / (1 + (α-1)·t_n) where α = √(m/n)
  4. Noise-augmented decoder: Train decoder with z + n where n ~ N(0, τ²I), τ=0.8 default
  5. DDT head attachment: 2-layer, 2048-dim head on top of DiT for scalable width without quadratic cost

- **Design tradeoffs**:
  - Encoder choice: MAE-B gives best rFID (0.16) but worst gFID; DINOv2-B balances both
  - Decoder capacity: ViT-XL (106.7 GFLOPs) vs ViT-B (22.2 GFLOPs)—1/3 of SD-VAE cost with better quality
  - DDT head depth vs width: Wide-shallow (2-layer, 2048-dim) outperforms narrow-deep (6-layer, 1152-dim) at similar FLOPs
  - Noise augmentation τ: Higher τ → better gFID, worse rFID; τ=0.8 is default

- **Failure signatures**:
  - Loss not converging on single-image overfit: DiT width < token dimensionality
  - Good reconstruction but poor generation: Missing noise-augmented decoder training
  - Artifacts at generation time despite low training loss: Decoder not robust to noisy latents; increase τ
  - DiT worse on VAE latents: DDT head provides no benefit for low-dimensional spaces; only use with RAE
  - Slow convergence: Missing dimension-dependent noise schedule shift

- **First 3 experiments**:
  1. Validate RAE reconstruction: Train ViT-B decoder with DINOv2-B encoder on small subset; target rFID < 0.6; verify per-token LayerNorm implementation
  2. Single-image overfit sanity check: Take one image, encode with DINOv2-B (n=768), train DiT-B (d=768) to overfit; verify loss converges to <0.01. Then try DiT-S (d=384) to confirm failure mode
  3. Noise augmentation ablation: Train two decoders on ImageNet subset—one clean (τ=0), one noise-augmented (τ=0.8); compare gFID after 20 epochs of DiT-XL diffusion training; expect ~0.5-0.6 gFID gap

## Open Questions the Paper Calls Out

### Open Question 1
Why do representation encoders like MAE, which achieve superior reconstruction fidelity (rFID), fail to match the generative quality (gFID) of encoders like DINOv2? The paper identifies the paradox where MAE-B achieves the best rFID (0.16) but significantly worse gFID than DINOv2, but does not isolate the specific geometric or statistical properties of the latent space that drive the divergence in generative performance.

### Open Question 2
Can the RAE framework scale efficiently to ultra-high-resolution synthesis (e.g., 1024px and beyond) without the aggressive spatial compression utilized by traditional VAEs? The RAE's lack of spatial compression creates a sequence length bottleneck that standard DiT scaling laws may not overcome, challenging the claim that RAE is "the new default" for large-scale generative modeling.

### Open Question 3
Is there a theoretical justification for the empirical finding that the diffusion head must be shallow rather than deep to effectively handle high-dimensional latents? Section 4.1 provides theoretical bounds mandating that model width must match latent dimensionality, yet Section G.3 empirically shows a shallow (2-layer) head outperforms deep (6-layer) heads without offering a theoretical explanation for this depth constraint.

## Limitations
- Encoder diversity: All tested encoders are image-level models; performance with other modalities or token-level objectives remains unexplored
- DDT head design: Rationale for 2-layer, 2048-dim DDT head is primarily empirical; theoretical benefits versus deeper alternatives not fully characterized
- High-resolution scaling: RAE's lack of spatial compression creates sequence length bottleneck that challenges scalability to resolutions ≥1024px

## Confidence
- **High confidence**: Core claim that pretrained representation encoders outperform VAEs in latent diffusion due to richer semantic structure
- **Medium confidence**: Theoretical analysis of DiT width requirements (Theorem 1) and its direct applicability to practical training dynamics
- **Medium confidence**: Noise-augmented decoder training demonstrably improves generation quality, but exact mechanism and optimal noise schedule not fully explained

## Next Checks
1. **Single-image overfit test**: Train DiT on latents from a single image encoded by DINOv2-B. Verify convergence only occurs when DiT width ≥768 (token dimension).
2. **Noise augmentation ablation**: Train two decoders—one with clean latents, one with noise-augmented latents (τ=0.8)—and compare gFID after diffusion training. Expect ~0.5-0.6 gFID gap.
3. **Encoder ablation study**: Train diffusion models using latents from DINO, SigLIP, and MAE encoders (all frozen) and compare gFID. Validate that DINOv2-B consistently outperforms others.