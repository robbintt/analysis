---
ver: rpa2
title: Comparing Neural Network Encodings for Logic-based Explainability
arxiv_id: '2505.20269'
source_url: https://arxiv.org/abs/2505.20269
tags:
- explanations
- constraints
- anns
- encodings
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two logical encodings of artificial neural
  networks (ANNs) for computing minimal explanations of model predictions. The first
  encoding uses logical implications to represent ReLU activation functions, while
  the second, adapted from prior work, achieves the same behavior without implications
  and uses fewer variables and constraints.
---

# Comparing Neural Network Encodings for Logic-based Explainability

## Quick Facts
- **arXiv ID**: 2505.20269
- **Source URL**: https://arxiv.org/abs/2505.20269
- **Reference count**: 19
- **Primary result**: Adapted encoding without implications achieves up to 18% faster constraint building and 16% overall speedup over implication-based encoding for logic-based ANN explainability.

## Executive Summary
This paper compares two logical encodings of ReLU neural networks for computing minimal explanations of model predictions. The first encoding uses logical implications to represent ReLU activations, while the second, adapted from prior work, achieves the same behavior without implications and uses fewer variables and constraints. Experiments across 12 datasets and various ANN architectures show that both encodings have similar running times for computing explanations, but the adapted encoding performs up to 18% better in building logical constraints and up to 16% better in overall time. The adapted method is particularly more efficient for ANNs with a single hidden layer. These results suggest the adapted encoding offers a more scalable approach for logic-based explainability in ANNs.

## Method Summary
The study evaluates two logical encodings of ReLU neural networks for computing minimal explanations via MILP. The first encoding (Fischetti and Jo [5]) uses indicator constraints with implications to model ReLU activation, while the second (adapted Tjeng et al. [17]) avoids implications and uses fewer variables/constraints. Both methods compute neuron bounds via MILP optimization. The experiments use 12 datasets (UCI and Penn ML Benchmarks, 9–32 features, 156–691 instances) and train ReLU feedforward networks with up to 2 hidden layers of 20 neurons each. Preprocessing includes one-hot encoding for categorical features and normalization to [0,1] for continuous but not integer features. The adapted encoding is up to 18% faster in building constraints and 16% faster overall, with most benefit for single-hidden-layer networks.

## Key Results
- Both encodings achieve similar explanation computation times.
- Adapted encoding builds logical constraints up to 18% faster and is up to 16% faster overall.
- Adapted encoding is most efficient for single-hidden-layer ANNs.
- Fewer variables and constraints in the adapted encoding improve scalability.

## Why This Works (Mechanism)
The adapted encoding replaces logical implications with linear constraints and big-M formulations, reducing the number of variables and constraints required to represent ReLU activations in the MILP model. This reduction directly translates into faster constraint building and potentially faster solving, especially when neuron bounds are tight. Both encodings compute minimal explanations by iteratively removing features and checking unsat via MILP, ensuring correctness guarantees.

## Foundational Learning
- **ReLU neural networks**: Needed for modeling non-linear decision boundaries in ANNs; quick check: verify network uses ReLU activation.
- **Logical implications vs. linear constraints**: Implications model ReLU exactly but increase complexity; quick check: confirm adapted encoding avoids implications.
- **MILP (Mixed-Integer Linear Programming)**: Used to encode network behavior and compute explanations; quick check: ensure CPLEX/DOcplex correctly handles indicator constraints.
- **Neuron bound computation**: Tight bounds reduce search space in MILP; quick check: verify bounds are included in build time.
- **Feature preprocessing**: Categorical one-hot, continuous normalized to [0,1], integers left as-is; quick check: confirm integer features are not scaled.

## Architecture Onboarding
- **Component map**: Datasets -> Preprocessing (one-hot, scaling) -> ANN training (ReLU, 2x20) -> MILP encoding (with/without implications) -> Explanation computation (iterative MILP unsat checks)
- **Critical path**: Preprocessing → Training → Encoding (bounds + constraints) → Explanation (iterative MILP)
- **Design tradeoffs**: Implication-based encoding is more direct but uses more variables/constraints; adapted encoding is leaner but requires careful big-M handling.
- **Failure signatures**: MILP solver time blows up for larger ANNs; explanation correctness fails if integer features are incorrectly normalized.
- **First experiments**:
  1. Train a small ReLU network on breast-cancer dataset and compute one explanation.
  2. Compare build time for both encodings on a single dataset.
  3. Verify that explanations are minimal by checking iterative removal.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing software versions (TensorFlow, CPLEX, DOcplex, Python).
- Hardware specs and solver parameters not provided.
- Results may not generalize to larger networks or different solvers.
- No analysis of random seed effects on performance.

## Confidence
- **Performance comparison**: Medium
- **Correctness of encodings**: High
- **Scalability claims**: Medium

## Next Checks
1. Verify neuron bound computation time is included in "build time" for both encodings.
2. Test performance on datasets with more features and instances to check scalability.
3. Confirm that integer features are not normalized to [0,1], to preserve formal guarantees.