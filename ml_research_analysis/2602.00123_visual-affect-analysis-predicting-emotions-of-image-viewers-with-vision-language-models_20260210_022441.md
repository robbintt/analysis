---
ver: rpa2
title: 'Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language
  Models'
arxiv_id: '2602.00123'
source_url: https://arxiv.org/abs/2602.00123
tags:
- affective
- emotion
- image
- ratings
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks nine state-of-the-art vision-language models
  on predicting human affective ratings using three psychometrically validated image
  datasets: the International Affective Picture System, the Nencki Affective Picture
  System, and the Library of AI-Generated Affective Images. Models performed emotion
  classification and continuous rating prediction in a zero-shot setting, and the
  impact of rater-conditioned prompting was evaluated on the LAI-GAI dataset.'
---

# Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models

## Quick Facts
- arXiv ID: 2602.00123
- Source URL: https://arxiv.org/abs/2602.00123
- Reference count: 40
- Primary result: VLMs achieve 60-80% emotion classification accuracy and r > 0.75 alignment with human ratings in zero-shot affective computing

## Executive Summary
This paper presents a comprehensive benchmark of nine state-of-the-art vision-language models for predicting human affective responses to images. Using three psychometrically validated datasets - IAPS, NAPS, and LAI-GAI - the study evaluates both emotion classification and continuous rating prediction in zero-shot settings. The research demonstrates that VLMs can effectively predict emotional responses with strong performance in emotion classification (60-80% accuracy) and moderate-to-strong alignment with human ratings (r > 0.75), while also revealing systematic limitations in rating intensity calibration and specific affect dimensions like arousal and surprise.

## Method Summary
The study benchmarks nine VLMs across three datasets: IAPS (1,196 images), NAPS (1,356 images), and LAI-GAI (10,000 images). Models are evaluated in zero-shot settings for both emotion classification and continuous rating prediction tasks. The LAI-GAI dataset specifically tests rater-conditioned prompting, where models predict ratings based on information about individual raters. Performance is measured through accuracy, Spearman correlation, and Root Mean Square Error, with particular attention to systematic biases in rating predictions.

## Key Results
- Emotion classification accuracy ranges from 60-80% across all models and datasets
- Spearman correlation with human ratings exceeds 0.75 for most models
- Systematic overestimation of rating intensity observed across all models
- Rater-conditioned prompting shows minimal impact on prediction quality

## Why This Works (Mechanism)
Vision-language models leverage their pretraining on multimodal data to capture the relationship between visual content and associated language describing emotional responses. The zero-shot capability emerges from the models' ability to generalize from their pretraining distribution to emotionally evocative images, even without task-specific fine-tuning. The strong correlation with human ratings suggests that VLMs have learned meaningful representations of affective content that align with human perception.

## Foundational Learning

**Affective Computing**: Study of computational recognition and synthesis of human emotion - needed to understand the research domain and evaluation metrics; quick check: Can you explain valence/arousal dimensions?

**Zero-shot Learning**: Model inference without task-specific training - needed to understand the experimental setup and generalizability claims; quick check: What distinguishes zero-shot from few-shot?

**Psychometric Validation**: Statistical methods ensuring measurement instruments measure what they claim - needed to evaluate dataset quality and results reliability; quick check: Why are validated datasets critical for affective computing?

## Architecture Onboarding

**Component Map**: Image Encoder -> Text Encoder -> Cross-Attention Layers -> Output Head

**Critical Path**: Image input → visual feature extraction → cross-modal fusion → textual prediction generation → affect rating output

**Design Tradeoffs**: Zero-shot evaluation prioritizes generalizability over peak performance; rater-conditioned prompting adds complexity with minimal gains; systematic intensity overestimation suggests calibration tradeoffs

**Failure Signatures**: Consistent overestimation of rating intensity; weaker performance on arousal and surprise dimensions; minimal benefit from rater-specific conditioning

**First Experiments**: 1) Test model calibration across rating intensity ranges, 2) Compare zero-shot vs. fine-tuned performance on same datasets, 3) Evaluate alternative prompt engineering strategies for rater conditioning

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Zero-shot evaluation limits understanding of fine-tuning potential and performance ceilings
- Minimal effects from rater-conditioned prompting may reflect limited prompt engineering exploration
- Systematic overestimation of rating intensity suggests calibration challenges for real-world deployment
- Weaker performance on arousal and surprise dimensions indicates affect category-specific limitations

## Confidence

| Claim | Confidence |
|-------|------------|
| Emotion classification accuracy (60-80%) | High |
| Continuous rating alignment (r > 0.75) | Medium |
| Systematic overestimation of intensity | Medium |
| VLM performance on arousal/surprise | Medium |

## Next Checks

1. Conduct fine-tuning experiments on the same datasets to establish performance ceilings and compare against zero-shot baselines
2. Test alternative prompt engineering strategies for rater-conditioned prompting to determine if more sophisticated approaches yield measurable improvements
3. Validate model calibration by examining prediction errors across different rating intensity ranges to identify systematic biases in intensity estimation