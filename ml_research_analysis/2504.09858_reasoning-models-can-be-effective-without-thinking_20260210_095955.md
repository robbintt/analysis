---
ver: rpa2
title: Reasoning Models Can Be Effective Without Thinking
arxiv_id: '2504.09858'
source_url: https://arxiv.org/abs/2504.09858
tags:
- thinking
- nothinking
- tokens
- reasoning
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the necessity of lengthy thinking processes
  in reasoning models. The authors show that bypassing explicit thinking via simple
  prompting ("NoThinking") can be surprisingly effective, often outperforming traditional
  thinking approaches when controlling for token usage, especially in low-budget settings
  (e.g., 51.3% vs 28.9% accuracy on AMC23 with 700 tokens).
---

# Reasoning Models Can Be Effective Without Thinking

## Quick Facts
- arXiv ID: 2504.09858
- Source URL: https://arxiv.org/abs/2504.09858
- Reference count: 40
- Primary result: NoThinking often outperforms traditional thinking approaches when controlling for token usage, especially in low-budget settings (e.g., 51.3% vs 28.9% accuracy on AMC23 with 700 tokens).

## Executive Summary
This paper challenges the necessity of lengthy thinking processes in reasoning models. The authors show that bypassing explicit thinking via simple prompting ("NoThinking") can be surprisingly effective, often outperforming traditional thinking approaches when controlling for token usage, especially in low-budget settings. Building on this, they demonstrate that parallel scaling with NoThinking and best-of-N aggregation achieves better accuracy-latency tradeoffs than sequential thinking approaches, with up to 9× lower latency while maintaining comparable accuracy. The findings suggest rethinking the necessity of lengthy thinking processes and provide a competitive reference for efficient reasoning.

## Method Summary
The study compares "Thinking" (generating explicit chain-of-thought) against "NoThinking" (bypassing CoT via prompt prefilling) on reasoning benchmarks. NoThinking is implemented by prefilling the assistant response with a dummy thinking block ("Okay, I think I have finished thinking."). Budget forcing is used to control token usage - if generation hits the token limit, it forces the end of thinking and requires a final answer. The evaluation uses pass@k metric (probability of at least one correct answer among k samples) across diverse datasets including AIME, AMC, OlympiadBench, and formal theorem proving tasks. Parallel sampling and aggregation methods (verifiers for formal tasks, confidence-based selection for others) are used to compare latency-accuracy tradeoffs.

## Key Results
- NoThinking outperforms Thinking across seven reasoning datasets when controlling for token usage, especially in low-budget settings
- The performance advantage of NoThinking increases with pass@k as k increases, suggesting better diversity scaling
- Parallel scaling with NoThinking achieves up to 9× lower latency than sequential Thinking while maintaining comparable accuracy
- On MiniF2F/ProofNet with verifiers, NoThinking uses 3.3-3.7× fewer tokens while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Token Budget Reallocation
Traditional Thinking models allocate tokens to lengthy reasoning traces before solution generation. NoThinking prefills a dummy thinking block, forcing the model to skip this process and allocate tokens directly to the solution. When controlling for token usage, NoThinking often achieves higher accuracy because tokens are spent on solution attempts rather than intermediate reasoning steps.

### Mechanism 2: Pass@k Scaling with Sampling Diversity
Without lengthy thinking traces constraining the generation path, NoThinking may produce more uniformly diverse outputs across different questions. Higher diversity increases the probability that at least one of k samples is correct. NoThinking shows lower variance in answer entropy across questions, suggesting more consistent diversity.

### Mechanism 3: Parallel Scaling Advantage
Sequential Thinking requires generating long chains before producing an answer, creating latency bottlenecks. NoThinking generates shorter responses that can be sampled in parallel. Since parallel latency is determined by the longest single sample, NoThinking's shorter generations yield lower wall-clock time.

## Foundational Learning

- **Concept: Pass@k metric**
  - Why needed here: The paper's central claim hinges on pass@k performance varying with k. Understanding this metric is essential to interpret why NoThinking becomes more competitive as k increases.
  - Quick check question: If a model generates 10 samples per problem and gets 3 correct, what is pass@5? (Answer: 1 - C(7,5)/C(10,5) ≈ 0.917)

- **Concept: Budget forcing**
  - Why needed here: The paper uses budget forcing to control token usage when comparing Thinking vs. NoThinking. Understanding this technique is critical for reproducing the low-budget experiments.
  - Quick check question: If a model hits a 500-token budget while still in its thinking block, what must be appended before forcing "Final Answer:"? (Answer: The `end of thinking` tag)

- **Concept: Best-of-N aggregation methods**
  - Why needed here: The parallel scaling results depend on aggregation methods—verifiers for formal tasks or confidence-based selection for others.
  - Quick check question: For a coding task without exact-match outputs, which aggregation method does the paper fallback to, and why is it less reliable? (Answer: Selecting the highest-confidence response; it underperforms voting because voting requires exact-match outputs)

## Architecture Onboarding

- **Component map**: User Prompt -> NoThinking Prefill (dummy thinking block) -> Model Generation (skips thinking, directly outputs solution) -> Parallel Sampling (N independent samples) -> Aggregation (verifier for formal tasks / confidence-based selection for others) -> Final Output

- **Critical path**:
  1. Implement the dummy thinking block prefill (exact string in Appendix C)
  2. Integrate budget forcing to control token usage
  3. Set up parallel sampling infrastructure
  4. Implement aggregation: verifiers for formal tasks, confidence-based selection for others
  5. Measure latency as max tokens per sample

- **Design tradeoffs**:
  - NoThinking advantages: Lower latency (up to 9×), better pass@k in low-budget settings, 4× fewer tokens with verifiers
  - NoThinking disadvantages: May underperform at pass@1 in high-budget regimes; depends on effective aggregation methods
  - Task dependency: Formal tasks with verifiers benefit most; coding tasks without exact-match verification are harder

- **Failure signatures**:
  - Pass@1 underperformance: On some benchmarks, NoThinking lags at k=1 in high-budget settings
  - Selection method breakdown: Confidence-based selection without voting underperforms
  - Budget forcing artifacts: Setting max tokens too high causes "babbling"

- **First 3 experiments**:
  1. Token budget sweep on AMC23/AIME: Compare Thinking vs. NoThinking at controlled token budgets (500, 700, 1000, 1500, 2000, 3000) to identify crossover point where NoThinking outperforms at pass@1.
  2. Parallel scaling with verifiers (MiniF2F): Implement NoThinking with N=32 parallel samples, use Lean compiler as verifier. Compare latency-accuracy curve against sequential Thinking.
  3. Aggregation method ablation (OlympiadBench): Compare majority voting, confidence+highest, and confidence+voting for NoThinking with N=16 samples.

## Open Questions the Paper Calls Out

- Does NoThinking's effectiveness generalize to other reasoning models beyond DeepSeek-R1-Distill-Qwen, including closed-source models like OpenAI o1?
- What underlying mechanisms explain why NoThinking achieves strong performance on formal theorem proving at pass@1 despite using 3.3–3.7× fewer tokens?
- Can improved aggregation methods for coding tasks close the performance gap between NoThinking and Thinking on benchmarks like LiveCodeBench?
- At what token budget threshold does Thinking begin to outperform NoThinking at pass@1 across different task types?

## Limitations

- The paper does not specify sampling hyperparameters (temperature, top-p) used for pass@k experiments
- Exact system prompts or user prompt wrappers for specialized benchmarks are not provided
- The extent to which NoThinking advantages transfer to other reasoning models remains unknown
- The exact boundaries where Thinking overtakes NoThinking at high budgets are not systematically mapped

## Confidence

- **High Confidence**: The core empirical findings showing NoThinking's effectiveness at pass@k ≥ 16 with controlled token budgets and the parallel scaling results with verifiers
- **Medium Confidence**: The mechanism explanations for why NoThinking works (token reallocation vs. diversity effects)
- **Low Confidence**: The generalizability of NoThinking across all reasoning tasks, particularly for coding without exact-match verification

## Next Checks

1. **Sampling Strategy Ablation**: Run identical NoThinking experiments on AMC23 with multiple sampling temperatures (0.1, 0.7, 1.0, 1.5) and top-p values (0.9, 0.95). Measure how pass@k curves change across k=1, 4, 16, 64.

2. **Cross-Model Validation**: Implement NoThinking on at least two additional reasoning models (e.g., Qwen2.5-32B-Instruct and QwQ-32B-Preview). Compare pass@k performance on the same benchmark (e.g., AIME 2024) with identical token budgets and sampling parameters.

3. **Task-Specific Prompting Study**: For LiveCodeBench, systematically vary the prompt template while keeping NoThinking intervention constant. Test: (a) standard chat template, (b) task-specific instructions, (c) chain-of-thought prefill. Measure how much of the performance gap is due to NoThinking vs. prompt quality.