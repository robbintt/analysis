---
ver: rpa2
title: 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE'
arxiv_id: '2510.13344'
source_url: https://arxiv.org/abs/2510.13344
tags:
- music
- training
- speech
- generation
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniMoE-Audio, a unified model for speech
  and music generation that addresses the challenges of task conflict and data imbalance
  through a novel Dynamic-Capacity Mixture-of-Experts (MoE) architecture and a three-stage
  training curriculum. The model employs a Top-P routing strategy for dynamic expert
  allocation and a hybrid expert design with routed, shared, and null experts to achieve
  functional decoupling.
---

# UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE

## Quick Facts
- arXiv ID: 2510.13344
- Source URL: https://arxiv.org/abs/2510.13344
- Reference count: 40
- Key result: State-of-the-art unified speech and music generation with UTMOS of 4.36 for English speech synthesis

## Executive Summary
UniMoE-Audio introduces a unified model for speech and music generation that addresses the challenges of task conflict and data imbalance through a novel Dynamic-Capacity Mixture-of-Experts (MoE) architecture and a three-stage training curriculum. The model employs a Top-P routing strategy for dynamic expert allocation and a hybrid expert design with routed, shared, and null experts to achieve functional decoupling. Experimental results demonstrate superior performance on major speech and music generation benchmarks, achieving state-of-the-art results with a UTMOS of 4.36 for English speech synthesis and strong aesthetic quality metrics for text-to-music generation.

## Method Summary
The paper proposes a unified audio generation model that leverages a Dynamic-Capacity MoE architecture to handle the distinct characteristics of speech and music generation tasks. The model employs a three-stage training curriculum: independent specialist training, MoE integration with warmup, and synergistic joint training on a balanced dataset. A Top-P routing strategy dynamically allocates experts based on input complexity, while a hybrid expert design combines routed, shared, and null experts to achieve functional decoupling between speech and music generation tasks.

## Key Results
- Achieves state-of-the-art UTMOS of 4.36 for English speech synthesis on major benchmarks
- Demonstrates superior aesthetic quality metrics for text-to-music generation (PC: 6.00, PQ: 7.77, CE: 7.34)
- Effectively mitigates performance degradation typically observed in naive joint training approaches

## Why This Works (Mechanism)
The Dynamic-Capacity MoE architecture addresses the fundamental challenge of task conflict in unified audio generation by providing specialized processing pathways for speech and music while maintaining shared representations where beneficial. The three-stage training curriculum allows the model to first develop task-specific expertise before learning to integrate these capabilities, preventing catastrophic forgetting and ensuring balanced performance across both domains. The Top-P routing strategy ensures computational efficiency by activating only the most relevant experts for each input, while the hybrid expert design allows for both task-specific specialization and cross-task knowledge sharing.

## Foundational Learning
**MoE Architecture**
- Why needed: Enables specialized processing for different audio tasks while maintaining efficiency
- Quick check: Verify that routing mechanism activates appropriate experts for speech vs. music inputs

**Three-Stage Training Curriculum**
- Why needed: Prevents catastrophic forgetting and ensures balanced performance across tasks
- Quick check: Confirm performance improvement at each training stage

**Top-P Routing Strategy**
- Why needed: Dynamically allocates computational resources based on input complexity
- Quick check: Measure activation patterns across different audio types

## Architecture Onboarding
**Component Map**
Input -> Tokenizer -> MoE Layer (Top-P Routing) -> Expert Blocks (Routed/Shared/Null) -> Output Layer

**Critical Path**
Audio input → Tokenizer → Dynamic expert routing → Task-specific expert processing → Audio synthesis

**Design Tradeoffs**
- Flexibility vs. efficiency: Dynamic expert allocation improves performance but adds routing overhead
- Specialization vs. generalization: Hybrid expert design balances task-specific optimization with shared knowledge
- Training complexity vs. model performance: Three-stage curriculum requires more training time but yields superior results

**Failure Signatures**
- Routing imbalance: Uneven expert activation indicating potential design flaws
- Task interference: Performance degradation in one task when optimizing for the other
- Convergence issues: Difficulties in training due to conflicting optimization objectives

**First Experiments**
1. Routing analysis: Visualize expert activation patterns for different audio types
2. Ablation study: Compare performance with different expert configurations
3. Cross-task transfer: Test model's ability to leverage speech knowledge for music generation

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation primarily on controlled benchmark scenarios with relatively short audio sequences (3-4 seconds for speech, 5-7 seconds for music)
- Computational efficiency gains from MoE structure not thoroughly analyzed
- Limited analysis of model's performance on extended audio generation tasks and real-world usage scenarios

## Confidence
- High confidence: Technical implementation of Dynamic-Capacity MoE architecture and three-stage training curriculum
- Medium confidence: Claims about generalization to arbitrary audio lengths based on benchmark results
- Medium confidence: Superiority claims relative to state-of-the-art models within tested constraints

## Next Checks
1. Evaluate model performance on extended audio sequences (30+ seconds) to assess temporal consistency and coherence in long-form generation
2. Conduct ablation studies comparing training/inference efficiency metrics (FLOPs, memory usage) against baseline models across different expert configurations
3. Perform user studies with domain experts to validate the perceptual quality improvements beyond automated metrics, particularly for music generation across diverse genres