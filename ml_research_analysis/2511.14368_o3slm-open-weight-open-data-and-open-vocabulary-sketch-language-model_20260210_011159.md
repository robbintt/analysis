---
ver: rpa2
title: 'O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model'
arxiv_id: '2511.14368'
source_url: https://arxiv.org/abs/2511.14368
tags:
- sketch
- sketches
- image
- object
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling large vision-language
  models to understand and reason with hand-drawn sketches, which are abstract and
  variable visual inputs that current models struggle with. The authors propose a
  large-scale dataset called SketchVCL, containing image-sketch-instruction triplets,
  and train a new model, O3SLM, on this dataset.
---

# O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model

## Quick Facts
- **arXiv ID:** 2511.14368
- **Source URL:** https://arxiv.org/abs/2511.14368
- **Authors:** Rishi Gupta; Mukilan Karuppasamy; Shyam Marjit; Aditay Tripathi; Anirban Chakraborty
- **Reference count:** 22
- **Key outcome:** Large vision-language model achieving state-of-the-art performance on sketch-based object localization, counting, image retrieval, and VQA, with generalization to unseen sketch styles.

## Executive Summary
O3SLM addresses the challenge of enabling large vision-language models to understand hand-drawn sketches, which are abstract and variable visual inputs that current models struggle with. The authors propose a large-scale dataset called SketchVCL containing image-sketch-instruction triplets, and train a new model on this dataset. The model is evaluated on sketch-based object localization, counting, image retrieval, and visual question answering, achieving state-of-the-art performance compared to existing open-weight LVLMs. Notably, O3SLM also generalizes to unseen sketch styles and can handle fine-grained queries combining sketches and text.

## Method Summary
O3SLM uses a two-stage training approach on LLaVA-1.5 architecture with CLIP ViT-L/336, 2-layer MLP multimodal connector, and Vicuna v1.5 backbone. Stage I involves pretraining on 600K image-sketch-text triplets to align abstract sketch representations with natural images and text. Stage II fine-tunes on four downstream tasks (object localization, counting, VQA, SBIR) with task-specific prefixes. Sketches are generated via SAM2 object segmentation, Pix2Pix sketch abstraction, and edge detection, creating 32M synthetic sketches from Objects365 and OpenImages datasets. The model uses LoRA fine-tuning (rank 64) with cosine learning rate decay.

## Key Results
- Achieves 82.7% detection Acc@0.5 on SketchVCL validation set, outperforming baselines by 5-10%
- 47.0% accuracy on unseen TU-Berlin sketches, demonstrating generalization to hand-drawn styles
- 55-65% SBIR Acc@K (K=1,5,10), showing strong sketch-based image retrieval performance
- Outperforms existing open-weight LVLMs on all sketch-specific benchmarks while maintaining competitive image-only performance

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training for Cross-Modal Alignment
Large-scale pretraining on sketch-image-text triplets followed by task-specific instruction tuning enables sketch understanding in LVLMs. Stage I aligns abstract sketch representations with natural images and text through 600K descriptive triplets. Stage II fine-tunes on four downstream tasks with task-specific prefixes to specialize reasoning capabilities.

### Mechanism 2: Sketch Abstraction via SAM2-Pix2Pix Pipeline
Instance-level sketches generated from segmentation masks + edge extraction provide scalable training data that generalizes to human-drawn styles. SAM2 extracts object masks → background removal → Pix2Pix generates base sketch → morphological edge detection enhances contours.

### Mechanism 3: Unified Token Space with Self-Attention Fusion
Concatenating sketch, image, and text tokens before the LLM enables implicit cross-modal alignment through self-attention, outperforming explicit cross-attention designs. CLIP-L/336 encodes both sketch and image → 2-layer MLP projects to LLM embedding space → tokens concatenated → LLM self-attention learns relationships implicitly.

## Foundational Learning

- **Cross-Modal Token Alignment**
  - Why needed here: Understanding how different modalities are projected into a shared embedding space before LLM processing. The paper relies on CLIP's vision encoder + MLP projector to map both sketches and images to the same token space.
  - Quick check question: Can you explain why using the same CLIP encoder for both sketches and images (vs. separate encoders) might benefit alignment?

- **Instruction Tuning with Task Prefixes**
  - Why needed here: The paper uses task-specific string prefixes (COUNT, BBOX, VQA, SBIR) rather than special tokens to guide output formatting. This follows Molmo's approach and affects how you design prompts.
  - Quick check question: What is the tradeoff between using string prefixes vs. adding special tokens to the vocabulary?

- **Zero-Shot Generalization to Unseen Domains**
  - Why needed here: The model must generalize to TU-Berlin sketches (excluded from training) and fine-grained text-sketch queries (not explicitly trained). Understanding what enables this transfer is critical.
  - Quick check question: Why might pretraining on synthetic sketches transfer to human-drawn sketch styles not seen during training?

## Architecture Onboarding

- **Component map:**
Input: [Sketch Image, Natural Image, Text Prompt] → Visual Encoder: CLIP ViT-L/336 (shared for both image types) → Multimodal Connector: 2-layer MLP (projects to LLM embedding dim) → Token Concatenation: [sketch_tokens | image_tokens | text_tokens] → LLM Backbone: Vicuna v1.5 (initialized from LLaVA-1.5-13B) → Output: Task-specific responses (bbox coords, counts, yes/no, descriptions)

- **Critical path:**
  1. Sketch quality from generation pipeline → affects all downstream performance
  2. Multimodal connector training → Figure 6 shows this is more impactful than model size
  3. Two-stage training order → Pretraining must precede instruction tuning (Figure 5)

- **Design tradeoffs:**
  - Synthetic vs. human sketches: Scale (32M) vs. authenticity
  - Shared vs. separate encoders: Simplicity vs. potential specialization
  - Task prefixes vs. special tokens: No vocabulary modification vs. explicit task signaling
  - LoRA fine-tuning: Memory efficiency vs. full parameter updates

- **Failure signatures:**
  - Low SBIR accuracy without pretraining (~10-15% vs. 55-65% with pretraining)
  - Degraded image-only performance (<5% drop on VQAv2, MME benchmarks)
  - Hallucinated bounding boxes when many overlapping objects present (Figure 9 limitation)
  - Confusion between structurally similar classes in retrieval (Figure 16)

- **First 3 experiments:**
  1. Validate sketch generation pipeline: Generate sketches for a held-out image set, manually inspect if structural features (pose, shape, contours) are preserved. Check for systematic artifacts.
  2. Ablate pretraining: Train with only Stage II (instruction tuning) and compare SBIR/counting accuracy against full two-stage training on a validation split.
  3. Test multimodal connector: Compare frozen vs. tuned projector on detection Acc@0.5 across different sketch datasets (Sketchy, QuickDraw!, TU-Berlin) to confirm Figure 6 findings.

## Open Questions the Paper Calls Out
None

## Limitations

- **Generalization to Human Sketches**: While achieving 47.0% accuracy on unseen TU-Berlin sketches, there's a significant performance gap compared to synthetic SketchVCL data, suggesting limitations in transferring from SAM2-Pix2Pix generated sketches to human-drawn styles.
- **Fine-Grained Reasoning**: The model struggles with structurally similar classes (confusion between cat and dog) and overlapping object scenarios (hallucinated bounding boxes), indicating sketch abstraction may lose discriminative features needed for precise classification.
- **Dataset Construction Assumptions**: The sketch generation pipeline relies on SAM2 segmentation masks that may not perfectly align with hand-drawn object boundaries, and caption generation depends on unspecified prompt templates that could introduce biases.

## Confidence

- **High Confidence (Mechanism 1 - Two-Stage Training)**: Multiple ablation studies demonstrate pretraining's critical role, particularly for SBIR tasks. The architecture design follows established LVLM patterns with clear performance improvements.
- **Medium Confidence (Mechanism 2 - Sketch Abstraction Pipeline)**: While the pipeline generates 32M sketches and achieves transfer to unseen styles, there's no direct comparison with human-drawn sketches during training. The 47.0% TU-Berlin performance suggests some transfer but with significant room for improvement.
- **Low Confidence (Mechanism 3 - Unified Token Space)**: The architectural claim that concatenation outperforms cross-attention is supported by tuning results but lacks comparison to alternative alignment approaches. The assumption that self-attention can implicitly learn sketch-image relationships without specialized modules needs further validation.

## Next Checks

1. **Synthetic-to-Human Transfer Study**: Generate a small set of human-drawn sketches for held-out Objects365 classes, evaluate O3SLM performance, and compare against synthetic SketchVCL performance on the same classes. This would quantify the gap between pipeline-generated and human-drawn sketch understanding.

2. **Cross-Attention Ablation**: Implement an alternative architecture with explicit cross-attention between sketch and image tokens, train on SketchVCL, and compare SBIR/BBOX performance against the current concatenation approach. This would validate the architectural choice empirically.

3. **Caption Bias Analysis**: Analyze the distribution of generated captions from DeepSeek-VL2 for systematic biases (e.g., object co-occurrence patterns, attribute frequency). Test if models trained on filtered/unbiased caption sets show different generalization patterns to fine-grained queries.