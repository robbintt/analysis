---
ver: rpa2
title: The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks
arxiv_id: '2504.15521'
source_url: https://arxiv.org/abs/2504.15521
tags:
- multilingual
- benchmarks
- language
- languages
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks

## Quick Facts
- **arXiv ID:** 2504.15521
- **Source URL:** https://arxiv.org/abs/2504.15521
- **Authors:** Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang
- **Reference count:** 34
- **Primary result:** STEM-related benchmarks correlate more strongly with human judgments (0.70-0.85) than traditional NLP tasks (0.11-0.30) across languages

## Executive Summary
This comprehensive analysis of 2,024 multilingual benchmarks reveals fundamental insights about cross-lingual evaluation. The study finds that STEM-related benchmarks demonstrate strong correlations with human judgments (0.70-0.85) while traditional NLP tasks show much weaker alignment (0.11-0.30). Machine-translated evaluation data often fails to capture cultural nuances, resulting in localized benchmarks achieving significantly higher alignment with local human judgments (0.68) compared to translated versions (0.47).

## Method Summary
The study analyzed 2,024 papers from arXiv cs.CL (2021-2024), filtering from 370K using Qwen 2.5-7B-Instruct and manual review. Researchers evaluated 8 multilingual benchmarks (XNLI, ARC, HellaSwag, TruthfulQA, MMLU, GlobalMMLU, XQuAD, MGSM) across 5 languages using 30 LLMs with Chatbot Arena Elo scores. They computed Spearman's rank correlation between benchmark rankings and human Elo rankings to assess alignment, and analyzed user instruction distributions from Chatbot Arena and WildChat to identify task preferences.

## Key Results
- STEM tasks (ARC, MGSM) achieve correlation coefficients of 0.70-0.85 with human judgments, while NLU tasks like XQuAD show only 0.11-0.30
- Localized benchmarks demonstrate 44% higher alignment with local human judgments (0.68) than machine-translated counterparts (0.47)
- High-resource languages (Chinese, Spanish, French, German, Russian) dominate benchmarks, with English appearing most frequently even after excluding English-only benchmarks
- Large datasets (>100K) grew 3x from 2021-2024 but cost ~$0.10/example per evaluation

## Why This Works (Mechanism)

### Mechanism 1
STEM-related benchmarks correlate more strongly with human judgments than traditional NLP tasks across languages. STEM tasks rely on language-agnostic cognitive operations that remain intact across translation, while language-understanding tasks require nuanced cultural and linguistic knowledge that degrades under translation. Core assumption: Human evaluators prioritize reasoning capability over linguistic fluency when judging model quality. Evidence: ARC and MGSM show correlation coefficients of 0.70-0.85, while XQuAD shows only 0.11-0.30. Break condition: If human evaluators in different cultures weight fluency/linguistic-naturalness higher than reasoning accuracy.

### Mechanism 2
Localized (culturally-native) benchmarks align better with local human judgments than translated benchmarks. Translation artifacts create evaluation signals that don't reflect how native speakers actually use or judge language. Native benchmark content captures pragmatic expectations that translation cannot preserve. Core assumption: Human judgment of model quality incorporates culturally-specific knowledge and communication norms. Evidence: CMMLU shows correlation of 0.682 with Chinese human judgments vs. 0.473-0.487 for translated MMLU. Machine-translated data preserves source language syntactic structures while failing to capture idioms and region-specific knowledge. Break condition: If target culture has heavily absorbed source-culture conceptual frameworks, translated benchmarks may achieve parity with localized ones.

### Mechanism 3
Benchmark-human alignment varies inconsistently across languages for the same task. Machine translation quality varies by language pair, and benchmark tasks may embed source-culture assumptions that interact unpredictably with target-culture knowledge structures. Core assumption: Variation in correlation reflects evaluation artifact rather than genuine cross-linguistic capability differences. Evidence: XNLI shows 0.233 correlation for Chinese but 0.588 for Russian; XQuAD shows 0.110 for Chinese but 0.301 for German. Substantial variation in correlation strengths across different languages for the same benchmark. Break condition: If specific languages have systematically different user expectation distributions in Chatbot Arena, observed variance may reflect evaluator pool differences rather than benchmark quality.

## Foundational Learning

- **Concept: Spearman's rank correlation (ρ)**
  - Why needed here: Paper uses Spearman's ρ to measure alignment between benchmark rankings and human Elo rankings. Understanding that ρ measures monotonic relationship and that 0.7+ indicates strong alignment while 0.1-0.3 indicates near-random ordering is essential for interpreting results.
  - Quick check question: If benchmark A has ρ=0.75 and benchmark B has ρ=0.25 with human judgments, what does this tell you about their practical utility for model selection?

- **Concept: Benchmark contamination**
  - Why needed here: Paper identifies contamination-free as a core benchmark requirement. Contamination inflates metrics without improving real capability, creating false signals of multilingual competence.
  - Quick check question: Why might cross-lingual contamination (training on language A test data, evaluating on language B) be particularly hard to detect?

- **Concept: Resource-level language taxonomy**
  - Why needed here: Paper's entire analysis framework depends on high-resource (HRL) vs. low-resource (LRL) language distinctions. HRLs dominate benchmarks; LRLs remain critically underrepresented, creating capability gaps.
  - Quick check question: The paper shows English appears most frequently even after excluding English-only benchmarks—what mechanism explains this paradox?

## Architecture Onboarding

- **Component map:**
  User Needs Analysis (Section 5.1) -> [Writing: 30-45%, Commonsense, Programming] -> Benchmark Selection ←→ Human Judgment Alignment (ρ scores) -> ┌────────────────┬────────────────────┐ -> │ STEM Tasks │ NLU Tasks │ -> │ (ARC, MGSM) │ (XNLI, XQuAD) │ -> │ ρ: 0.70-0.85 │ ρ: 0.11-0.30 │ -> └────────────────┴────────────────────┘ -> Localization Decision: Native content (61.4% of benchmarks) → Higher alignment; Machine translation → Lower alignment (0.47 vs 0.68); Human translation → Intermediate (GlobalMMLU > MT-MMLU)

- **Critical path:**
  1. Define target languages and user populations (G5 countries = 40%+ of existing benchmarks)
  2. Identify task types matching user needs (writing/reasoning dominate cross-linguistically)
  3. Choose localization strategy: native curation (recommended) vs. translation (cost-effective but misaligned)
  4. Validate against human judgment corpus (Chatbot Arena Elo rankings)
  5. Check for contamination across all target languages

- **Design tradeoffs:**
  | Dimension | Option A | Option B | Guidance |
  |-----------|----------|----------|----------|
  | Content source | Native curation | Translation | Native shows 44% higher alignment (0.68 vs 0.47); use translation only for rapid prototyping |
  | Task focus | STEM/reasoning | Traditional NLU | STEM tasks validate reliably; NLU tasks may require per-language calibration |
  | Dataset size | Large (>100K) | Medium (1K-10K) | Large datasets grew 3x from 2021-2024 but cost ~$0.10/example; prioritize coverage over scale for LRLs |
  | Domain | Public (news, social) | Specialized (health, law) | Public domains = 40% of benchmarks; specialized domains underrepresented but higher practical value |

- **Failure signatures:**
  - **Signature 1:** Benchmark rankings contradict human preference rankings (ρ < 0.3) → Task may not capture what users value; re-examine task-user alignment
  - **Signature 2:** Large cross-language variance in ρ for same task → Suspect translation quality or cultural mismatch; consider localization
  - **Signature 3:** Model performance saturates (>90% accuracy) → Benchmark no longer differentiates models; need harder or more diverse tasks

- **First 3 experiments:**
  1. **Translation vs. Native Pilot:** For one target language, create 100-example test sets via (a) professional translation, (b) machine translation, (c) native curation. Measure ρ against local human preferences. Hypothesis: Native > Human translation > Machine translation.
  2. **Task-Type Correlation Audit:** Run existing multilingual benchmarks (ARC, XQuAD, XNLI, MGSM) on 10+ models; compute ρ per task against Chatbot Arena Elo for each language. Confirm STEM-task advantage replicates.
  3. **Contamination Cross-Check:** For top-3 target languages, scan training corpora of evaluated models for benchmark content (including translations). Flag models where contamination may explain high scores with low human-alignment.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the field develop scalable evaluation frameworks for Natural Language Generation (NLG) tasks across diverse languages to address the current imbalance where discriminative tasks (66.5%) significantly outnumber generative tasks (23.5%)?
**Basis in paper:** [explicit] Section 6.2 identifies "Natural Language Generation" as a critical research direction, noting that NLG capabilities remain "significantly underassessed" despite the prevalence of generative applications in real-world deployments.
**Why unresolved:** Current benchmarks focus heavily on classification and QA, creating a gap in assessing free-form text generation quality across languages.
**What evidence would resolve it:** The creation and validation of new multilingual benchmarks specifically designed for generative tasks (e.g., summarization, creative writing) that demonstrate high alignment with human judgment.

### Open Question 2
**Question:** Can "LLM-as-a-Judge" methodologies be effectively calibrated to mitigate language-specific biases and provide reliable evaluation for low-resource languages?
**Basis in paper:** [explicit] Section 6.2 highlights "LLM-as-a-Judge" as a promising but challenging future direction. The authors explicitly state that deploying LLMs as judges introduces "unique challenges, including potential evaluation biases that mirror the language disparities in the judge models themselves."
**Why unresolved:** While LLM-as-a-Judge works well for English, it remains unclear if these models possess the cross-lingual capabilities required to fairly evaluate nuanced text in languages where training data is scarce.
**What evidence would resolve it:** A study demonstrating that an LLM judge's scores for low-resource language generation correlate strongly with native human evaluators, comparable to its performance in English.

### Open Question 3
**Question:** What specific methodologies are required to capture "cultural authenticity" in benchmarks, and can these methods bridge the alignment gap observed between localized benchmarks (correlation 0.68) and machine-translated ones (correlation 0.47)?
**Basis in paper:** [explicit] The paper concludes in Section 6.2 under "Localized Benchmarking" that "there remains significant room for benchmarks that assess models on their ability to handle locally meaningful applications."
**Why unresolved:** The analysis proves that translation is insufficient, but establishing a standardized method for sourcing and validating "culturally authentic" data for hundreds of language groups remains an open operational challenge.
**What evidence would resolve it:** A new benchmark suite where performance on "culturally authentic" tasks shows a significantly higher predictive validity for user satisfaction in specific regions compared to translated standard benchmarks like MMLU.

## Limitations
- Correlation Interpretation Uncertainty: The paper demonstrates strong correlations but doesn't fully establish causality between STEM benchmarks and human judgments
- Translation Quality Variability: Analysis assumes consistent translation quality across language pairs, but translation artifacts vary substantially by source-target combination
- Temporal Stability: Chatbot Arena Elo rankings evolve continuously, yet the benchmark correlation analysis captures only a snapshot

## Confidence
- **STEM tasks show superior human alignment (0.70-0.85 vs 0.11-0.30):** High confidence - supported by robust correlation analysis across multiple languages and tasks
- **Localized benchmarks outperform translated versions (0.68 vs 0.47):** Medium confidence - strong empirical support but limited to specific benchmark pairs and languages
- **High-resource language dominance persists despite growth:** High confidence - clearly documented trend across the 2,000+ benchmark dataset

## Next Checks
1. **Replication Study with Different Human Judgment Corpus:** Validate findings using an independent human preference dataset (e.g., MT-Bench, LMSYS curated human preferences) to confirm that STEM task advantage and localization benefits replicate beyond Chatbot Arena.
2. **Controlled Translation Quality Experiment:** Systematically vary translation quality (human vs. machine vs. hybrid) for identical benchmark content and measure correlation degradation. This would isolate translation artifacts from task-content effects.
3. **Longitudinal Correlation Tracking:** Recompute benchmark-human correlations quarterly over 12 months to assess temporal stability and identify whether specific tasks maintain alignment as models and user expectations evolve.