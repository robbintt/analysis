---
ver: rpa2
title: 'Tower+: Bridging Generality and Translation Specialization in Multilingual
  LLMs'
arxiv_id: '2506.17080'
source_url: https://arxiv.org/abs/2506.17080
tags:
- translation
- data
- tower
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TOWER+ introduces a suite of multilingual language models that\
  \ achieve state-of-the-art translation performance while maintaining strong general-purpose\
  \ capabilities, addressing the common trade-off where specialized fine-tuning degrades\
  \ instruction-following and reasoning abilities. The authors develop a multi-stage\
  \ post-training pipeline\u2014comprising continued pretraining with multilingual\
  \ and instruction-following data, supervised fine-tuning with curated responses\
  \ from multiple top models, weighted preference optimization, and reinforcement\
  \ learning with verifiable rewards\u2014to jointly improve translation accuracy\
  \ and general chat abilities."
---

# Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs

## Quick Facts
- arXiv ID: 2506.17080
- Source URL: https://arxiv.org/abs/2506.17080
- Reference count: 40
- Tower+ models achieve state-of-the-art translation performance while maintaining strong general-purpose capabilities

## Executive Summary
Tower+ introduces a suite of multilingual language models that achieve state-of-the-art translation performance while maintaining strong general-purpose capabilities, addressing the common trade-off where specialized fine-tuning degrades instruction-following and reasoning abilities. The authors develop a multi-stage post-training pipeline—comprising continued pretraining with multilingual and instruction-following data, supervised fine-tuning with curated responses from multiple top models, weighted preference optimization, and reinforcement learning with verifiable rewards—to jointly improve translation accuracy and general chat abilities. Evaluated across translation (WMT24++, XCOMET-XXL), general chat (M-ArenaHard), and a novel IF-MT benchmark combining both, Tower+ models (2B, 9B, 72B) outperform or match larger general-purpose open-weight and proprietary models such as Llama 3.3 70B and GPT-4o, with the 72B variant achieving best-in-class translation for high-resource languages and top scores on IF-MT. The work demonstrates that it is possible to rival frontier models in general capabilities while optimizing for translation and localization use cases, providing a blueprint for balancing domain specialization with broad utility in LLMs.

## Method Summary
Tower+ employs a four-stage post-training pipeline to balance translation specialization with general-purpose capabilities. The process begins with continued pretraining (CPT) using a carefully curated multilingual corpus comprising 66% monolingual text, 33% parallel data filtered via COMETKiwi, and 1% instruction-following data across 27 languages. This is followed by supervised fine-tuning (SFT) on 1.3M samples selected through multi-teacher answer selection using reward models. The third stage applies weighted preference optimization (WPO) using mixed preference data from on-policy and off-policy sources, with preference selection via INF-ORM reward model. The final stage employs GRPO with verifiable rewards combining Tülu 3 data and custom translation-verification tasks. Base models include Gemma 2 9B and Qwen 2.5 72B, with the checkpoint merging technique used after CPT to preserve general capabilities.

## Key Results
- Tower+ 72B achieves best-in-class translation performance for high-resource languages on WMT24++ and XCOMET-XXL benchmarks
- Tower+ models outperform or match larger general-purpose models (Llama 3.3 70B, GPT-4o) on M-ArenaHard general chat benchmark
- Tower+ 72B attains top scores on the novel IF-MT benchmark combining instruction-following and translation tasks
- The 9B variant demonstrates strong performance while being significantly more efficient than larger models

## Why This Works (Mechanism)
The success of Tower+ stems from its multi-stage approach that carefully balances translation specialization with general capability preservation. By starting with continued pretraining that includes both parallel translation data and instruction-following content, the model maintains its foundational understanding while gaining translation proficiency. The multi-teacher selection approach during SFT ensures high-quality responses by leveraging the strengths of multiple top models rather than relying on a single source. The preference optimization stage refines the model's responses based on human preferences and model-generated rankings, while the GRPO stage with verifiable rewards ensures the model can handle complex translation tasks with proper verification. The checkpoint merging after CPT is particularly crucial, as it preserves the general capabilities that would otherwise degrade during translation-focused training.

## Foundational Learning
- **Continued Pretraining (CPT)**: Extends the pretraining phase with curated multilingual data to adapt the model to specific domains while maintaining general knowledge. Needed to establish a strong foundation before specialization. Quick check: Monitor perplexity on held-out validation sets during CPT.
- **Supervised Fine-Tuning (SFT)**: Trains the model on high-quality, curated responses to improve instruction-following capabilities. Needed to teach the model how to respond appropriately to various prompts. Quick check: Evaluate on IFEval benchmark after SFT.
- **Preference Optimization**: Uses pairwise comparisons to refine the model's response quality based on human preferences. Needed to align the model with human values and expectations. Quick check: Compare model outputs using reward model scores.
- **Reinforcement Learning with Verifiable Rewards (GRPO)**: Optimizes the model using reward signals that can be automatically verified. Needed for tasks where correctness can be programmatically checked. Quick check: Test on translation-verification tasks with automatic scoring.
- **Reward Model Selection**: Different reward models (Gemma-based vs Llama-based) have different biases and preferences. Needed to avoid reward hacking and ensure diverse, high-quality outputs. Quick check: Compare outputs from different reward models on the same prompts.
- **Checkpoint Merging**: Combines the specialized model with the original base model to preserve general capabilities. Needed to prevent catastrophic forgetting of general knowledge. Quick check: Compare performance on general benchmarks before and after merging.

## Architecture Onboarding

**Component Map:** Base Model -> CPT (32B tokens) -> Checkpoint Merge -> SFT (1.3M samples) -> WPO (mixed preferences) -> GRPO (verifiable rewards) -> Tower+ Model

**Critical Path:** The most critical sequence is CPT followed by Checkpoint Merge, as this establishes the foundation for all subsequent specialization while preserving general capabilities. Without proper checkpoint merging, the model risks catastrophic forgetting of general knowledge during the translation-focused CPT phase.

**Design Tradeoffs:** The authors chose a multi-stage approach over single-stage fine-tuning to better balance specialization with general capability preservation. While more complex and computationally expensive, this approach yields superior results compared to simpler methods. The use of multiple reward models (Gemma-based for SFT, Llama-based for WPO) addresses the bias issue where reward models tend to prefer outputs similar to their own architecture.

**Failure Signatures:** If CPT degrades general capabilities, the model will show declining performance on M-ArenaHard during training. If GRPO overfits to IFEval, the model may perform well on that specific benchmark but poorly on general translation tasks. If reward model bias is not addressed, the model may generate outputs that are overly similar to the reward model's architecture.

**First Experiments:**
1. Run CPT with only the 66%/33%/1% data mixing ratio and basic translation instruction templates to verify baseline performance
2. Test the checkpoint merging technique by comparing performance before and after merging on both translation and general benchmarks
3. Compare outputs from Gemma-based and Llama-based reward models on the same preference optimization tasks to quantify reward model bias

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation demonstrates strong performance but the multi-stage training pipeline creates significant barriers to exact replication due to proprietary components and unspecified hyperparameters
- The reliance on multiple reward models with different biases suggests results may be sensitive to reward model selection
- GRPO with translation-verification data yielded limited gains due to poor prompt formatting in the source Tülu 3 RLVR data

## Confidence

**Confidence Labels:**
- Translation performance claims (WMT24++, XCOMET-XXL): **High** - Extensive evaluation across 15+ languages with established benchmarks
- General capability maintenance (M-ArenaHard, IFEval): **Medium** - Strong performance but limited comparison to some recent models
- IF-MT benchmark results: **Medium** - Novel benchmark with internal consistency, but limited external validation
- Reproducibility of the full pipeline: **Low** - Multiple proprietary components and unspecified hyperparameters

## Next Checks

1. Implement a minimal version using only the specified 66%/33%/1% data mixing ratio and basic translation instruction templates to verify CPT + SFT baseline performance before adding WPO and GRPO stages
2. Conduct ablation studies removing the proprietary translation data to quantify its contribution versus the publicly available components
3. Test alternative reward models (both Gemma and Llama-based) on the same preference optimization tasks to measure sensitivity to reward model selection