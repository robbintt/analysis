---
ver: rpa2
title: 'EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via
  Step-wise Intention-Driven Product Association'
arxiv_id: '2505.15196'
source_url: https://arxiv.org/abs/2505.15196
tags:
- script
- product
- step
- products
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "E-commerce script planning is challenging due to LLMs\u2019 inability\
  \ to generate coherent scripts while retrieving relevant products, semantic gaps\
  \ between planned actions and search queries, and lack of large-scale benchmarks.\
  \ This paper addresses these by defining a three-step discriminative task framework\
  \ and introducing an intention-driven product alignment strategy."
---

# EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association

## Quick Facts
- arXiv ID: 2505.15196
- Source URL: https://arxiv.org/abs/2505.15196
- Reference count: 40
- LLMs struggle with e-commerce script planning even after fine-tuning, but injecting purchase intentions significantly improves performance

## Executive Summary
This paper addresses the challenge of e-commerce script planning by defining a three-step discriminative task framework and introducing an intention-driven product alignment strategy. The authors build a large-scale dataset (EcomScriptBench) with 605K scripts, 2.4M products, and 24M purchase intentions derived from Amazon reviews, creating a benchmark with 15K human-annotated samples. Experiments show that LLMs struggle on these tasks even after fine-tuning, but injecting purchase intentions significantly improves performance. The work provides both a new task definition and the first large-scale evaluation benchmark for e-commerce script planning.

## Method Summary
The method constructs scripts and products from Amazon review data using GPT-4o-mini to generate objectives, scripts, and purchase intentions. Products are aligned to script steps via SentenceBERT-based step-intention similarity matching with a threshold of 0.45. The benchmark defines three sequential discriminative tasks: script verification (T1), step-product discrimination (T2), and script-products verification (T3). Models are fine-tuned using LoRA (rank=16, α=32) with Adam optimizer (lr=5e-5), batch size 8, for 3 epochs, with max sequence length 300. Human annotations validate script and product quality, while intention injection involves continued pre-training on domain-specific knowledge bases.

## Key Results
- LLMs struggle with all three discriminative tasks even after fine-tuning, showing consistent performance gaps
- Injecting purchase intention knowledge via continued pre-training significantly improves model performance on product association tasks
- The three-step discriminative framework successfully decomposes the complex generative task into evaluable components, revealing specific failure modes in script planning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping products to script steps via "purchase intentions" reduces semantic gaps that hinder direct action-to-product retrieval
- **Mechanism:** Instead of matching product metadata to script steps, the system extracts "purchase intentions" (the "why" behind a buy) and calculates semantic similarity between these intentions and script steps, bridging the disconnect between "doing" and "buying"
- **Core assumption:** The vector space of "intentions" overlaps more effectively with "actionable steps" than product titles do
- **Evidence anchors:** Abstract states semantic gaps between planned actions and search queries; Section 4.3 explains intention alignment helps overcome semantic discrepancy; corpus suggests semantic grounding is critical
- **Break condition:** If intentions are overly generic or step descriptions are ambiguous, cosine similarity scores may fail to distinguish relevant products from noise

### Mechanism 2
- **Claim:** Decomposing generative script planning into discriminative verification tasks allows for more robust evaluation and error isolation
- **Mechanism:** Instead of asking an LLM to generate a complex product-enriched script end-to-end, the framework splits the problem into three sequential discriminative tasks that identify specific failure points in reasoning or retrieval
- **Core assumption:** High performance on discriminative sub-tasks correlates with the ability to generate coherent, helpful scripts
- **Evidence anchors:** Abstract defines three-step discriminative task framework; Section 3.1 explains why one-step generative formulation is challenging
- **Break condition:** If step-level correctness doesn't guarantee global coherence, modular evaluation provides false security

### Mechanism 3
- **Claim:** Injecting explicit purchase intention knowledge into models via continued pre-training enhances their capacity for product association
- **Mechanism:** General LLMs lack deep e-commerce specific knowledge; fine-tuning on intention knowledge bases before the target task teaches latent relationships between product features and user goals
- **Core assumption:** Knowledge of "why a product is bought" is transferable to "planning what products are needed"
- **Evidence anchors:** Abstract states intention injection significantly improves performance; Section 5.3 explains aligning LLMs with e-commerce products' use cases enhances ability to identify useful products
- **Break condition:** If intention data is noisy or differs significantly in distribution from target script domain, transfer learning may yield minimal gains or negative transfer

## Foundational Learning

- **Concept: Semantic Similarity & Vector Embeddings**
  - **Why needed here:** The core "Step-Intention Alignment" relies entirely on Sentence-BERT embeddings to calculate similarity between script steps and product intentions
  - **Quick check question:** If two sentences have a cosine similarity of 0.85, are they necessarily semantically equivalent in a specific domain context?

- **Concept: Goal-Oriented Script Planning**
  - **Why needed here:** The benchmark is built around "scripts" (sequences of actions toward a goal); understanding the difference between valid plans and random sequences is required to interpret Task 1 results
  - **Quick check question:** Does the order of steps matter for "Script Verification" task, or only the presence of necessary steps?

- **Concept: Knowledge Distillation**
  - **Why needed here:** Dataset construction uses GPT-4o-mini to "distill" purchase intentions from raw product data; understanding this pipeline is critical for assessing quality and potential biases
  - **Quick check question:** What are the risks of using a proprietary LLM to generate ground truth labels for a benchmark intended to evaluate LLMs?

## Architecture Onboarding

- **Component map:** Data Generator (GPT-4o-mini) -> Retrieval Engine (Sentence-BERT) -> Evaluator (Discriminative Classifiers)
- **Critical path:** The "Step-Intention Alignment" is the bottleneck; if similarity threshold τ (set to 0.45) is misconfigured, candidate dataset quality for Task 2 and Task 3 collapses
- **Design tradeoffs:**
  - **Synthetic vs. Real:** Trade authenticity of human-written scripts for scale of LLM-generated ones (605K scripts), necessitating rigorous human verification
  - **Discriminative vs. Generative:** Chose discriminative tasks for evaluation robustness, potentially sacrificing ability to measure "creativity" of planner
- **Failure signatures:**
  - **Semantic Drift:** Retrieving products matching keywords but violating global objective (e.g., heavy parka for "running" step because it "fits body")
  - **Product Conflict:** Passing Task 2 but failing Task 3 due to incompatible items across steps
- **First 3 experiments:**
  1. **Baseline Verification:** Run RoBERTa-Large on Task 2 (Step-Product Discrimination) to establish non-LLM baseline
  2. **Intention Ablation:** Compare product retrieval using direct keyword matching vs. intention-alignment strategy to quantify "semantic gap"
  3. **Knowledge Injection:** Fine-tune Llama-3-8B model on FolkScope dataset before EcomScript task to reproduce intention injection performance gain

## Open Questions the Paper Calls Out

- **Open Question 1:** How can model architectures be adapted to perform E-commerce script planning as a unified single-step generative task rather than relying on the proposed sequential discriminative pipeline?
  - **Basis in paper:** Authors state in conclusion that greater efforts should be directed toward achieving automated e-commerce script planning in a single-step generative manner
  - **Why unresolved:** Paper defines task via three sub-tasks to make evaluation feasible, but ultimate goal is coherent, single-generation plan which current models fail to produce effectively
  - **What evidence would resolve it:** A generative model architecture that outputs full script and associated products in one pass, achieving higher utility scores than current "fine-tuned discrimination" baselines

- **Open Question 2:** To what extent does integrating multi-modal signals (e.g., product images and structured metadata) mitigate the "wrong understanding of products" error and improve performance in ambiguous categories like "Beauty and Personal Care"?
  - **Basis in paper:** Error analysis and category-wise performance sections note that multi-modal product images or more detailed attributes can be incorporated to address the 68% error rate caused by false understanding of product features
  - **Why unresolved:** Current experiments are text-only, and performance dips significantly in categories with subtle or overlapping text descriptors
  - **What evidence would resolve it:** Benchmarks of multi-modal LLMs (e.g., GPT-4o with vision) on the dataset showing statistically significant accuracy improvements in text-ambiguous product categories

- **Open Question 3:** Can automated strategies be developed to verify product compatibility across different script steps, a task currently limited to human annotation due to complexity of feature interactions?
  - **Basis in paper:** Limitations section states authors do not implement any strategies within data collection framework for conflict verification because detecting conflicting products is complicated
  - **Why unresolved:** Paper relies on humans to check if products from different steps work together; automating this is necessary for scalable, reliable planning
  - **What evidence would resolve it:** A proposed logic-based or reasoning module capable of identifying cross-step product conflicts (e.g., voltage mismatches, software incompatibility) with high agreement compared to human gold labels

## Limitations

- **Dataset accessibility:** The ECOM SCRIPT BENCH dataset is not publicly available due to privacy concerns, creating fundamental reproducibility barrier despite detailed construction methodology
- **Synthetic data quality:** Dataset relies on GPT-4o-mini to generate both scripts and purchase intentions, potentially introducing distributional biases where model-generated content differs systematically from human-authored scripts
- **Evaluation scope:** Three discriminative tasks focus on binary classification correctness but don't directly measure generative quality of complete product-enriched scripts; high scores may not translate to practical utility

## Confidence

- **High confidence:** Core observation that LLMs struggle with script planning tasks even after fine-tuning (Section 5.2) - well-supported by empirical results showing consistent performance gaps across all three tasks
- **Medium confidence:** Effectiveness of intention-driven product alignment - while paper shows improved performance with intention injection (Section 5.3), comparison is primarily against keyword-based retrieval without ablation studies across different semantic similarity methods
- **Medium confidence:** Decomposition into three discriminative tasks improves evaluation robustness - modular approach is theoretically sound, but paper doesn't demonstrate that T2 and T3 performance actually correlates with end-to-end script generation quality

## Next Checks

1. **Threshold sensitivity analysis:** Systematically vary similarity threshold τ from 0.3 to 0.6 and measure impact on Task 2 and Task 3 performance to reveal whether fixed threshold of 0.45 is optimal or domain-dependent

2. **Human evaluation of script coherence:** Conduct small-scale human study (n=50) where annotators rate plausibility and helpfulness of product-enriched scripts generated by top-performing models to validate whether high discriminative scores translate to practical utility

3. **Cross-domain generalization test:** Apply fine-tuned models to scripts from different e-commerce domain (e.g., fashion vs. electronics) and measure performance degradation to reveal whether models have learned domain-specific patterns or general script planning capabilities