---
ver: rpa2
title: 'Rehearse With User: Personalized Opinion Summarization via Role-Playing based
  on Large Language Models'
arxiv_id: '2503.00449'
source_url: https://arxiv.org/abs/2503.00449
tags:
- user
- summary
- reviews
- product
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Rehearsal, a personalized opinion summarization
  framework via LLMs-based role-playing. To address the challenge of LLMs struggling
  with long-text personalization tasks, the method introduces a multi-agent framework
  that separates product and user information processing.
---

# Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models

## Quick Facts
- arXiv ID: 2503.00449
- Source URL: https://arxiv.org/abs/2503.00449
- Reference count: 40
- Key outcome: Proposed Rehearsal framework achieves 87.32% on GPT-3.5 and 91.07% on GPT-4o for personalized opinion summarization

## Executive Summary
This paper introduces Rehearsal, a personalized opinion summarization framework that leverages large language models (LLMs) through a role-playing approach. The framework addresses the challenge of LLMs struggling with long-text personalization tasks by implementing a multi-agent system that separates product and user information processing. The method employs a three-stage process: generating a general summary, using a user agent to provide personalized modification suggestions through role-playing, and performing retrieval-augmented rewriting. Experiments on the PerSum dataset demonstrate that Rehearsal outperforms baseline methods, achieving higher personalization scores while maintaining general summarization capabilities.

## Method Summary
Rehearsal operates through a multi-agent framework consisting of three main components: a product agent, a user agent, and a rewriter. The framework first generates a general summary of the product reviews, then the user agent engages in role-playing to understand the target user's preferences and provides personalized modification suggestions. Finally, the rewriter performs retrieval-augmented rewriting to incorporate these suggestions into the final personalized summary. The role-playing mechanism involves supervision and practice to improve the LLM's understanding of user interests, with the user agent generating questions and answers about user preferences based on the user profile and product information.

## Key Results
- Achieves 87.32% overall score on GPT-3.5 and 91.07% on GPT-4o
- Demonstrates 3.52% improvement over two-stage methods for GPT-3.5 in user-related metrics
- Shows 2.77% improvement over two-stage methods for GPT-4o in user-related metrics
- Outperforms baseline methods while maintaining general summarization capabilities

## Why This Works (Mechanism)
The approach works by breaking down the complex personalization task into manageable sub-tasks through its multi-agent framework. By separating product information processing from user information processing, the system can focus on understanding user preferences in a more targeted manner. The role-playing mechanism allows the user agent to actively engage with user profiles and product information, generating contextually relevant questions and answers that help the model better understand user interests. The retrieval-augmented rewriting step ensures that the personalized modifications are grounded in the original review content while incorporating user-specific preferences.

## Foundational Learning
- **Multi-agent framework**: Separates concerns between product processing and user understanding; quick check: verify distinct agent roles and information flow
- **Role-playing mechanism**: Enables active engagement with user profiles; quick check: validate question-answer pairs generated by user agent
- **Retrieval-augmented rewriting**: Ensures personalization is grounded in original content; quick check: confirm retrieval relevance and integration quality
- **Supervision and practice**: Improves model understanding of user interests; quick check: assess quality of generated questions and answers
- **Two-stage personalization**: First general summary, then personalized modifications; quick check: verify logical progression between stages

## Architecture Onboarding
**Component Map**: Product Reviews → Product Agent → General Summary → User Agent (Role-playing) → Personalized Suggestions → Rewriter (Retrieval-augmented) → Final Personalized Summary

**Critical Path**: The critical path flows from product review input through the product agent to generate a general summary, which is then passed to the user agent for role-playing and personalized suggestions, before final rewriting produces the personalized output.

**Design Tradeoffs**: The approach trades computational efficiency for improved personalization quality through its multi-agent framework. While simpler two-stage methods may be faster, the separation of concerns and role-playing mechanism provide better user understanding and more tailored outputs.

**Failure Signatures**: Potential failures include: user agent misunderstanding preferences leading to irrelevant modifications, general summary errors propagating to final output, or retrieval-augmented rewriting introducing factual inconsistencies.

**First Experiments**: 1) Validate user agent's ability to generate relevant questions from user profiles; 2) Test general summary quality across different product categories; 3) Assess personalized summary quality against human preferences.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o for both generation and evaluation introduces potential bias and circularity
- Initial general summary errors may propagate through the personalization pipeline
- Role-playing effectiveness depends heavily on user agent's understanding quality
- Computational intensity of multi-agent framework may limit real-time applications

## Confidence
- **High**: Basic multi-agent framework design and separation of product/user information processing
- **Medium**: Reported performance improvements on the PerSum dataset
- **Medium**: Effectiveness of role-playing for personalization

## Next Checks
1. Conduct human evaluation studies to validate the GPT-4o-based scoring metrics and assess whether the personalization actually aligns with human preferences
2. Test the framework's robustness across different domains and product types beyond the PerSum dataset
3. Evaluate the computational efficiency and scalability of the multi-agent approach compared to simpler methods, particularly for real-time applications