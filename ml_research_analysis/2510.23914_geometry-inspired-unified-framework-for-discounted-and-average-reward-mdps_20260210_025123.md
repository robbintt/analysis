---
ver: rpa2
title: Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs
arxiv_id: '2510.23914'
source_url: https://arxiv.org/abs/2510.23914
tags:
- case
- policy
- value
- reward
- average-reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work unifies the theoretical analysis of discounted-reward
  and average-reward Markov Decision Processes (MDPs) by extending a geometric interpretation
  framework to the average-reward case. The authors introduce new action and value
  vectors that enable a consistent geometric view across both settings, addressing
  the singularity issue that arises in average-reward MDPs when using traditional
  value functions.
---

# Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs

## Quick Facts
- arXiv ID: 2510.23914
- Source URL: https://arxiv.org/abs/2510.23914
- Reference count: 7
- Key outcome: Unifies theoretical analysis of discounted and average-reward MDPs through geometric interpretation, proving Value Iteration achieves geometric convergence in average-reward case under unichain assumption

## Executive Summary
This paper establishes a unified theoretical framework for analyzing both discounted and average-reward Markov Decision Processes through geometric interpretation. The authors introduce modified value vectors that remain well-defined even when the traditional Bellman equation becomes singular at γ=1, enabling consistent geometric analysis across both reward formulations. Under the assumption of a unique optimal policy inducing a unichain Markov chain, they prove Value Iteration achieves geometric convergence in the average-reward setting with a convergence rate bounded by γ^N τ where τ∈(0,1).

## Method Summary
The authors extend a geometric interpretation framework from discounted to average-reward MDPs by introducing new action and value vectors. Traditional value functions become undefined at γ=1 due to singularity in the Bellman equation, so they redefine values using a modified linear system that remains invertible under unichain assumptions. The framework uses span seminorm contraction arguments, showing that after normalization (optimal actions have zero reward, suboptimal have negative), the Bellman operator contracts geometrically. The key insight is that after N iterations, the product of transition operators can be decomposed to show uniform contraction across all states.

## Key Results
- Proves Value Iteration achieves geometric convergence in average-reward MDPs under unichain assumption
- Shows convergence rate bounded by γ^N τ where τ∈(0,1)
- Establishes unified geometric framework working for both discounted and average-reward settings
- Addresses singularity issue that arises when using traditional value functions at γ=1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modified value vectors enable geometric analysis at γ=1 where traditional values become undefined
- Mechanism: Redefines values via R^π = (I + E - γP^π)v^π/C where E = 11^T, avoiding singularity when γ=1
- Core assumption: Policy π induces a unichain Markov chain
- Evidence anchors: [Section 3.1] action vector definition, [Lemma 3.5] invertibility equivalence, [corpus] weak neighbors focus on algorithms
- Break condition: Multichain policies where I + E - P^π is non-invertible

### Mechanism 2
- Claim: Span seminorm contracts geometrically during Value Iteration under new formulation
- Mechanism: Update v_{t+1} = γ(P'_t - E)v_t holds after normalization; product expansion shows contraction via ergodicity
- Core assumption: Unique optimal policy inducing unichain with ergodic optimal chain
- Evidence anchors: [Theorem 4.3] convergence result, [Lemma 4.2] product expansion, [corpus] "Faster Fixed-Point Methods" notes lack of contractivity
- Break condition: Multiple optimal policies or non-ergodic optimal chains

### Mechanism 3
- Claim: Advantage function adv(a, π) = a^+ · v^+_π unifies both reward formulations
- Mechanism: Same inner product structure works for both discounted and average cases via constant cancellation
- Core assumption: Value functions satisfy respective Bellman equations in each case
- Evidence anchors: [Section 3.2] discounted case proof, [Section 3.3, Lemma 3.3] average case connection, [corpus] neighbors reference advantage functions without geometric formulation
- Break condition: Multichain cases where bias is not uniquely defined

## Foundational Learning

- **Span Seminorm** sp(v) = max_i v(i) - min_j v(j)
  - Why needed here: Captures translational invariance inherent in average-reward problems
  - Quick check question: Why does sp(v + c·1) = sp(v) for any constant c?

- **Unichain vs. Multichain MDPs**
  - Why needed here: Invertibility of I + E - P^π depends critically on unichain structure
  - Quick check question: If MDP has states {s_1, s_2, s_3, s_4} with transitions s_1 ↔ s_2 and s_3 ↔ s_4 forming two disconnected components, what is dim(ker(I - P^π))?

- **Ergodicity Coefficient and Doeblin's Condition**
  - Why needed here: Proof requires (P^*)^N to have all positive entries with minimum ω > 0
  - Quick check question: For stochastic matrix P, if P^k has all entries ≥ ε > 0, what can you conclude about lim_{t→∞} P^t v_0?

## Architecture Onboarding

- Component map:
  - Action Vector Constructor -> Policy Value Solver -> Advantage Computer -> VI Span Tracker

- Critical path: Initialize v_0 arbitrarily → Compute greedy policy π_t → Solve (I + E - γP^{π_t})v_{t+1} = R^{π_t} · C → Check sp(v_{t+1}) → Repeat

- Design tradeoffs:
  - Unichain assumption vs. generality: Current implementation requires unichain; multichain needs fallback to Relative Value Iteration
  - Exact solve vs. iterative refinement: Solving linear system exactly each iteration is expensive; iterative methods trade accuracy for speed
  - Span threshold vs. policy stability: Convergence on sp(v_t) may precede policy convergence; check both conditions

- Failure signatures:
  - Singularity error: Matrix (I + E - P^π) is singular → policy induces multichain
  - Non-convergence: sp(v_t) plateaus above threshold → possibly multiple optimal policies
  - Numerical instability: C = nγ + (1-γ) approaches zero when γ ≈ 1 and n small

- First 3 experiments:
  1. Unichain verification: Generate random MDPs with n ∈ {5, 10, 20} states; verify (I + E - P^{π^*}) is invertible and measure condition number
  2. Convergence rate validation: On 2-state MDPs with known optimal policy, run VI and measure actual sp(v_N)/sp(v_0) vs. theoretical bound γ^{Nτ}
  3. Ablation on ergodicity: Construct MDPs where (P^*)^N has varying minimum entry ω ∈ {0.001, 0.01, 0.1}; measure iterations to reach sp(v_t) < 10^{-6}

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can geometric convergence of Value Iteration be proven for multichain MDPs within this unified geometric framework?
- Basis: [explicit] Conclusion states future work might "extend the analysis to more general settings, including multichain structures."
- Why unresolved: Current proof relies on Lemma 3.5 establishing invertibility of I+E-P^π only under unichain assumption
- What evidence would resolve it: Theoretical proof extending Theorem 4.3 to multichains without requiring invertibility of I+E-P^π, or counterexample showing non-convergence

### Open Question 2
- Question: Does Value Iteration maintain geometric convergence if optimal policy is not unique?
- Basis: [explicit] Conclusion suggests relaxing assumptions to include "broader classes of policies," and Assumption 4.1 requires "unique optimal policy π^*"
- Why unresolved: Theorem 4.3 depends on gap δ > 0 defined by maximum advantage of non-optimal actions
- What evidence would resolve it: Proof showing span seminorm contracts geometrically even when advantage gap δ is zero, or empirical study on MDPs with multiple optimal policies

### Open Question 3
- Question: Can framework be extended to non-ergodic dynamics where optimal gain varies by initial state?
- Basis: [explicit] Conclusion lists "non-ergodic dynamics" as target for future analysis
- Why unresolved: Paper assumes optimal policy implies unichain MRP leading to state-independent gain
- What evidence would resolve it: Generalization of Lemma 3.3 and value definition v_π that accounts for state-dependent gains

## Limitations
- Unichain assumption restricts applicability to MDPs with single closed irreducible class
- Convergence rate bound relies on ergodicity of optimal policy's transition matrix
- O(n³) complexity per iteration from solving linear system limits scalability

## Confidence
- **High Confidence**: Geometric interpretation framework and application to discounted MDPs is well-established
- **Medium Confidence**: Convergence proof for Value Iteration in average-reward case follows logically from framework
- **Low Confidence**: Claim of unified theory across both reward settings may have gaps in edge cases

## Next Checks
1. Unichain robustness test: Generate MDPs with controlled unichain violations and measure convergence rate degradation
2. Rate bound verification: Empirically measure convergence rate on MDPs with known optimal policies and compare against theoretical bound
3. Multichain extension attempt: Attempt to extend framework to multichain MDPs by applying geometric interpretation to each closed class separately