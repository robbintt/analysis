---
ver: rpa2
title: 'MORPH: PDE Foundation Models with Arbitrary Data Modality'
arxiv_id: '2509.21670'
source_url: https://arxiv.org/abs/2509.21670
tags:
- data
- morph
- foundation
- datasets
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MORPH is a modality-agnostic foundation model for PDEs that handles\
  \ 1D\u20133D data, mixed scalar/vector fields, and varying resolutions through a\
  \ unified convolutional vision transformer backbone. It uses component-wise convolutions\
  \ for local interactions, inter-field cross-attention for selective information\
  \ propagation across physical fields, and axial attention for efficient spatiotemporal\
  \ modeling."
---

# MORPH: PDE Foundation Models with Arbitrary Data Modality

## Quick Facts
- arXiv ID: 2509.21670
- Source URL: https://arxiv.org/abs/2509.21670
- Reference count: 40
- Key outcome: MORPH outperforms scratch-trained models and matches or surpasses strong baselines on seven downstream PDE tasks.

## Executive Summary
MORPH is a modality-agnostic foundation model for partial differential equations (PDEs) that handles 1D–3D data, mixed scalar/vector fields, and varying resolutions through a unified convolutional vision transformer backbone. It uses component-wise convolutions for local interactions, inter-field cross-attention for selective information propagation across physical fields, and axial attention for efficient spatiotemporal modeling. Pretrained on diverse PDE datasets and fine-tuned on seven downstream tasks, MORPH demonstrates strong performance and positive cross-modality transfer, with zero-shot gap-closure ratios above 0.5 on fluid-based targets.

## Method Summary
MORPH processes PDE data in UPTF-7 format (B, T, F, C, D, H, W) through a convolutional preprocessing layer, patching, and a transformer backbone with axial attention. The model learns autoregressive dynamics for next-step prediction, using cross-attention to fuse multi-field representations. It is pretrained on heterogeneous PDE datasets (MHD-3D, DR-2D, CFD-1D, etc.) and fine-tuned on downstream tasks (DR-1D, CFD-2D, etc.) using LoRA adapters. Training uses AdamW with ReduceLROnPlateau, and performance is evaluated via NRMSE, VRMSE, and gap-closure ratios for zero-shot transfer.

## Key Results
- MORPH outperforms models trained from scratch on seven downstream PDE tasks.
- Zero-shot gap-closure ratios exceed 0.5 on fluid-based targets, indicating strong cross-modality transfer.
- LoRA fine-tuning recovers most performance gains with far fewer trainable parameters (77M vs. 480M).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lightweight convolutional preprocessing improves accuracy on higher-dimensional PDE data with modest compute overhead.
- **Mechanism**: A 3D convolutional operator acts on the component dimension (C) of the UPTF-7 tensor, jointly processing scalar and vector channels to capture local interactions before patching and transformer attention. Default uses 8 filters with LeakyReLU activations.
- **Core assumption**: Local spatial structure contains useful inductive bias that pure attention cannot efficiently extract from raw patch tokens.
- **Evidence anchors**:
  - [abstract] "component-wise convolution, which jointly processes scalar and vector channels to capture local interactions"
  - [Table 4 ablation] Adding 8-filter conv reduces MHD3D loss from 0.18376→0.16851 (~8.3%) at 2.76 GFLOP cost; DR2D from 0.00787→0.00747 (~5.1%)
  - [Table 4 ablation] 1D data shows negligible benefit (0.17185→0.17348), suggesting mechanism is dimension-dependent
  - [corpus] PDE-Transformer (Holzschuh et al.) similarly combines architectural improvements for physics simulations
- **Break condition**: Expect diminishing returns on 1D or very low-resolution data where local structure is already captured by patching.

### Mechanism 2
- **Claim**: Cross-attention-based field fusion outperforms naive concatenation for multi-field PDE systems by selectively propagating relevant inter-field information.
- **Mechanism**: A learned query token attends over F field embeddings, performing content-based pooling that weights field contributions dynamically. This replaces static concat+dense mixing with attention-weighted fusion, producing a single fused representation.
- **Core assumption**: Different physical fields (e.g., velocity, pressure, density) have varying relevance to each other that benefits from adaptive weighting rather than uniform mixing.
- **Evidence anchors**:
  - [abstract] "inter-field cross-attention, which models and selectively propagates information between different physical fields"
  - [Table 4 ablation] Cross-attention reduces loss vs. concat+dense: MHD3D 0.17101→0.16851, DR2D 0.00827→0.00747 (~9.7%), CFD1D 0.18141→0.17348 (~4.4%)
  - [Section 3.2] "prioritizes relevant inter-field interactions and selectively propagates information, enabling careful feature fusion while suppressing spurious activations arising from scale mismatches"
  - [corpus] No direct corpus comparison; mechanism appears novel to this work
- **Break condition**: Single-field datasets will not benefit; overhead may hurt if fields are uncorrelated or noisy.

### Mechanism 3
- **Claim**: Axial factorization of spatiotemporal attention provides superior accuracy-compute trade-offs versus full or sparse attention, especially at high token counts.
- **Mechanism**: Full 4D attention O((TDHW)²) is replaced by four sequential 1D attentions along time, depth, height, and width axes, reducing complexity to O(TDHW(T+D+H+W)). Each axis folds others into batch dimension.
- **Core assumption**: Spatiotemporal correlations factorize reasonably along individual axes without catastrophic information loss.
- **Evidence anchors**:
  - [abstract] "axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity"
  - [Table 4 ablation] At ps=8, Np=4096: axial achieves 0.00812 loss at 1.88 GFLOPs vs. full attention 0.01228 at 5.91 GFLOPs (~3.1× cheaper, ~34% lower loss)
  - [Section 3.2] "limits the computational cost to O(T² + D² + H² + W²), in contrast to full spatiotemporal attention with cost O((TDHW)²)"
  - [corpus] Axial attention principle from Ho et al. 2019; concurrent PDE-Transformer explores similar efficiency gains
- **Break condition**: Highly coupled 4D dynamics where cross-axis interactions dominate may underperform; expect degradation when T, D, H, W are all small (trivial factorization gains).

## Foundational Learning

- **Autoregressive dynamics (AR(1) vs. AR(p))**:
  - Why needed here: MORPH learns F_θ: X^t → X^{t+1} as a one-step autoregressive map aligned with initial-value-problem semantics.
  - Quick check question: Can you explain why AR(1) preserves compositional PDE structure better than high-order AR(16)?

- **Cross-attention mechanics**:
  - Why needed here: Field fusion uses query-key-value attention where a learned query aggregates F field representations.
  - Quick check question: Why does omitting field-wise positional encodings make the module permutation-invariant?

- **Foundation model transfer paradigm**:
  - Why needed here: MORPH pretrains on heterogeneous PDEs then fine-tunes; zero-shot GCR quantifies transfer quality.
  - Quick check question: If GCR = 0.6, what fraction of the scratch-to-naive performance gap does zero-shot recover?

## Architecture Onboarding

- **Component map**:
  - Encoder: UPTF-7 input → 3D component-wise conv (8 filters) → patching (size 8) → linear projection
  - Field fusion: Cross-attention (learned query, F keys/values) → fused single representation
  - Backbone: 4× axial attention blocks (time/depth/height/width) with pre-norm, MLP, residuals
  - Decoder: Linear projection back to data space
  - Variants: Ti(7M)/S(30M)/M(126M)/L(480M) scaled via attention dims, heads, depth

- **Critical path**:
  1. Data arrives as (B, T, F, C, D, H, W) → on-the-fly UPTF-7 conversion
  2. Conv operates on C dimension → produces F feature maps
  3. Patching → up to 4096 tokens (max constraint)
  4. Cross-attention fuses F fields → single sequence
  5. Axial attention over (T, D, H, W) → autoregressive prediction

- **Design tradeoffs**:
  - Patch size 8: larger patches reduce tokens but lose resolution; smaller increase memory
  - AR(1) vs. AR(p): AR(1) respects IVP semantics; higher-order may improve rollouts but departs from PDE structure
  - LoRA fine-tuning: 77M trainable params (vs. 480M full) recovers most gains—use for large models with limited data

- **Failure signatures**:
  - 1D data with conv preprocessing shows no gain or slight degradation
  - Single-field datasets waste cross-attention overhead
  - High token counts (>4096) hit memory ceiling; reduce patch size or crop spatial dims
  - Unstable rollouts may indicate need for longer AR context or learning rate tuning

- **First 3 experiments**:
  1. **Validate UPTF-7 conversion**: Load sample from each dataset (1D/2D/3D, scalar/vector), confirm tensor shape mapping preserves field semantics
  2. **Ablate axial vs. full attention**: On a 2D dataset with Np≥1024, compare loss and GFLOPs; expect axial ~3× cheaper with lower loss
  3. **Test zero-shot transfer**: Pretrain on 2D-CFD-IC only, evaluate GCR on 3D-MHD and 1D-DR without fine-tuning; expect GCR>0.5 on fluid targets

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the effect of LoRA adapter rank on fine-tuning performance across different PDE systems with varying complexity and data modalities?
  - **Basis in paper**: Authors state: "Future work will examine LoRA-based fine-tuning more extensively, including the effect of adapter rank across different datasets."
  - **Why unresolved**: Only default ranks (r_attn=16, r_mlp=12) were evaluated; no systematic study of rank sensitivity across the heterogeneous pretraining corpus was conducted.
  - **What evidence would resolve it**: A sweep of LoRA ranks (e.g., r ∈ {4, 8, 16, 32, 64}) across multiple downstream tasks (1D, 2D, 3D) reporting NRMSE/VRMSE and trainable parameter counts.

- **Open Question 2**: How can MORPH be extended to handle unstructured meshes and irregular geometries without abandoning the convolutional patching scheme?
  - **Basis in paper**: Authors state: "the convolutional preprocessing and patching scheme is designed for regular grids, which limits direct applicability to unstructured meshes or irregular geometries."
  - **Why unresolved**: The component-wise convolutions and patch-based axial attention fundamentally assume regular Cartesian grids; no alternative tokenization for unstructured data was proposed.
  - **What evidence would resolve it**: Demonstration of MORPH variants on benchmark unstructured mesh problems (e.g., finite element triangulations, particle-based simulations) with comparable accuracy to structured-grid counterparts.

- **Open Question 3**: What modifications to training protocols (hyperparameter tuning, training duration, learning rate schedules) enable predictable performance scaling from ~7M to ~500M parameters?
  - **Basis in paper**: Authors note: "For a fixed training budget...we do not observe clear gains when scaling from Ti to L. This suggests that mild model-specific hyperparameter tuning and/or extended training is needed."
  - **Why unresolved**: All variants used identical training hyperparameters; scaling benefits may be obscured by suboptimal configurations for larger models.
  - **What evidence would resolve it**: Scaling curves showing validation loss vs. model size under tuned hyperparameters per variant, with training budgets extended to convergence.

- **Open Question 4**: Can representations learned on non-fluid physics (elasticity, wave propagation, fluid-structure interaction) transfer effectively to fluid-dominated systems, achieving bidirectional cross-physics generalization?
  - **Basis in paper**: Zero-shot transfer was tested only from 2D incompressible Navier-Stokes to other fluid-based targets; the pretraining corpus is fluid-dominated, and authors explicitly note the need for "broader range of physics" for a general-purpose PDE foundation model.
  - **Why unresolved**: The current pretraining set lacks solid mechanics and coupled multiphysics systems, so reverse transfer (solid→fluid) and bidirectional transfer remain untested.
  - **What evidence would resolve it**: Pretraining MORPH on elasticity and wave propagation datasets, followed by zero-shot evaluation on fluid targets, with GCR > 0.5 indicating bidirectional cross-physics transfer.

## Limitations
- Transfer generality: Limited ablation of pretraining mix; medium confidence in robustness across all task combinations.
- Ablation granularity: Component-wise convolution and cross-attention ablations are per-dataset snapshots; causal attribution of gains is medium-confidence.
- Compute and memory scaling: 4096-token cap may bottleneck 3D high-res tasks; memory scaling with large T,D,H,W is not quantified.

## Confidence
- **High**: Core architecture design (UPTF-7 → conv → patch → axial backbone) and pretraining/finetuning pipeline reproducibility.
- **Medium**: Claims on component-wise convolution benefit (only partially ablated, inconsistent across dimensions).
- **Medium**: Cross-attention vs. concat performance (novel mechanism, ablation consistent but no external validation).
- **Medium**: Axial attention vs. full attention efficiency/accuracy trade-off (well-supported but limited to fixed token count).

## Next Checks
1. **Joint ablation study**: Remove conv preprocessing AND cross-attention simultaneously; confirm their combined effect is additive on multi-field datasets.
2. **Memory scaling test**: Run MORPH-M on 3D-MHD with increasing patch sizes (4→8→16) and record GFLOPs/token; verify O(TDHW(T+D+H+W)) scaling empirically.
3. **Pretraining mix sensitivity**: Train MORPH-Ti on subsets of pretraining datasets (e.g., only 2D-CFD-IC) and test GCR on held-out 3D-MHD; measure sensitivity of transfer to pretraining diversity.