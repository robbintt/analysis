---
ver: rpa2
title: Improving Language and Modality Transfer in Translation by Character-level
  Modeling
arxiv_id: '2505.24561'
source_url: https://arxiv.org/abs/2505.24561
tags:
- latn
- languages
- translation
- speech
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a character-level encoder for better cross-lingual\
  \ and cross-modal transfer in translation. By using SONAR\u2019s fixed-size embedding\
  \ space and a teacher-student approach, it learns a character-level encoder that\
  \ embeds sentences in a multilingual space, enabling effective language and modality\
  \ transfer."
---

# Improving Language and Modality Transfer in Translation by Character-level Modeling

## Quick Facts
- arXiv ID: 2505.24561
- Source URL: https://arxiv.org/abs/2505.24561
- Reference count: 24
- Primary result: Character-level encoder enables zero-shot speech-to-text translation for over 1,000 languages, outperforming subword-based models on FLORES+ and achieving state-of-the-art results on FLEURS

## Executive Summary
This work introduces charSONAR, a character-level encoder that improves cross-lingual and cross-modal transfer in translation. By using a teacher-student approach with SONAR's fixed-size embedding space, the model learns to embed sentences in a multilingual space that enables effective language and modality transfer. The key innovation is a lightweight cross-modal adapter that connects a massively multilingual ASR model to the character encoder, enabling speech translation from over 1,000 languages with minimal supervision. The character-based approach outperforms subword-based models in translation quality and similarity search, particularly for low-resource and zero-shot languages.

## Method Summary
The method uses SONAR's fixed-size embedding space (1024-dim mean-pooled vectors) as a target for training a character-level encoder via teacher-student distillation. The charSONAR encoder is trained with an interpolated MSE objective that maps between source and target language embeddings, finding intermediate regions better suited for both translation and similarity search. For speech translation, a cross-modal adapter connects the frozen MMS acoustic encoder to charSONAR using CTC-based character predictions. The adapter can be pretrained on ASR data for zero-shot translation or trained with parallel speech-translation data for better performance. Family tokens replace language tokens during training to enable zero-shot transfer to unseen languages.

## Key Results
- On FLORES+, charSONAR outperforms subword-based models in translation quality and similarity search, especially for low-resource and zero-shot languages
- On FLEURS, the pretrained adapter achieves 0.812 COMET with zero training data, surpassing Whisper (0.714) and state-of-the-art supervised models
- Character-level sequences are only 10% slower in inference despite being 3.2× longer, thanks to the fixed-size bottleneck
- Interpolated MSE objective (0.931 COMET) outperforms reconstruction (0.929) and translation (0.924) objectives on low-resource pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Character-level tokenization improves cross-lingual transfer for low-resource and unseen languages compared to subword-based models.
- **Mechanism:** Characters create a shared vocabulary across languages, enabling knowledge transfer through common sub-lexical units. The paper suggests that subword tokenizers create language-specific vocabulary splits that hinder transfer, while characters provide a universal representation that allows morphologically similar patterns to share representations across related languages.
- **Core assumption:** Character-level representations capture transferable morphological and phonetic patterns that subwords fragment or obscure.
- **Evidence anchors:**
  - [abstract] "demonstrating better zero-shot generalizability to unseen languages"
  - [Section 5.4/Table 4] English-only training shows no advantage for character-level models (0.015 BLEU behind), confirming gains come from cross-lingual transfer, not increased compute
  - [corpus] Weak direct evidence; neighbor papers focus on ASR/AST systems rather than character-level transfer mechanisms
- **Break condition:** If target languages share no script or character inventory with training languages, character-level advantages may not apply.

### Mechanism 2
- **Claim:** Interpolated embedding objectives create a more transferable semantic space than standard translation or reconstruction objectives.
- **Mechanism:** The interpolated objective trains the encoder to map to the midpoint between source and target language embeddings (e_s + e_t)/2. The paper hypothesizes this finds "regions in between the languages that are better suited for both translation and similarity search" (Section 5.1), creating a language-agnostic representation rather than source- or target-biased embeddings.
- **Core assumption:** The SONAR embedding space contains underutilized intermediate regions that are more transferable than language-specific regions.
- **Evidence anchors:**
  - [Section 5.1/Table 1] Interpolated MSE (0.931 COMET) outperforms reconstruction (0.929) and translation (0.924) objectives
  - [Section 5.1/Table 2] Decoding from averaged embeddings improves COMET by +0.076 for low-resource pairs
  - [corpus] No corpus validation; this is a novel finding specific to this work
- **Break condition:** If source and target embeddings are too distant in the embedding space, interpolation may produce incoherent representations.

### Mechanism 3
- **Claim:** Pretrained cross-modal adapters enable near-zero-shot speech translation by leveraging CTC-to-character alignment.
- **Mechanism:** The adapter uses MMS's CTC classification layer (which outputs character-level predictions) to project acoustic features directly onto the charSONAR embedding space via softmax-weighted embedding lookup. This creates a "soft prediction" that aligns with how charSONAR expects character inputs, enabling transfer without parallel speech-translation data.
- **Core assumption:** CTC outputs from MMS provide sufficiently accurate character-level predictions to serve as soft targets for the text encoder.
- **Evidence anchors:**
  - [Section 3.3.2] Adapter uses softmax(AW) × Emb to connect MMS vocabulary (64 tokens) to charSONAR embeddings
  - [Section 6.1/Table 6] Pretrained adapter (0.894 avg COMET) outperforms randomly initialized adapter (0.882) with same data
  - [Section 6.2/Table 7, row 11] Pretrained adapter achieves 0.812 COMET with zero training data, surpassing Whisper (0.714)
  - [corpus] Neighbor paper "Bi-directional Context-Enhanced Speech LLMs" uses character-level context for ASR, suggesting character-level speech representations have broader validity
- **Break condition:** If CTC predictions are poor (high character error rate), the pretrained adapter will propagate errors; languages with poor ASR quality may not benefit.

## Foundational Learning

- **Concept: SONAR fixed-size embedding space**
  - **Why needed here:** Understanding that SONAR compresses sentences into single 1024-dim vectors via mean-pooling, which decouples encoder sequence length from decoder computation. This is critical for understanding why character-level sequences (3.2× longer) only add 10% inference overhead.
  - **Quick check question:** If SONAR used full-sequence attention instead of mean-pooled embeddings, how would character-level sequences affect inference cost?

- **Concept: CTC (Connectionist Temporal Classification)**
  - **Why needed here:** MMS outputs CTC predictions over character vocabularies with <blank> tokens for frame alignment. The cross-modal adapter exploits CTC's character-level output space—you must understand CTC compression (collapsing repeated tokens, removing blanks) to understand the adapter design.
  - **Quick check question:** Why does the adapter drop frames labeled as <blank> before passing to charSONAR?

- **Concept: Teacher-student distillation**
  - **Why needed here:** The character encoder is trained to reproduce SONAR embeddings, not to optimize translation directly. This preserves SONAR's embedding geometry while adapting the input representation. MSE loss is sufficient because the bottleneck already exists.
  - **Quick check question:** Why can the paper use simple MSE loss while ZeroSwot (referenced in Section 2.2) required Wasserstein distance?

## Architecture Onboarding

- **Component map:** Raw text → Character tokenizer → charSONAR encoder (24 layers, 1024-dim) → Mean pool → Embedding (1024-dim) → SONAR decoder (24 layers) → Translation
- **Speech path:** Audio → MMS acoustic encoder (48 layers, 1280-dim, frozen) → CTC compression → Cross-modal adapter (2.5M params) → charSONAR encoder (frozen) → Embedding → SONAR decoder → Translation
- **Critical path:** The pretrained adapter branch is the key innovation. Without it, you need substantial ASR data to train the adapter. With it, the system works zero-shot.
- **Design tradeoffs:**
  - Dual adapter vs. pretrained-only: Dual adds 2.3M random parameters for high-resource languages; pretrained-only (~200K params) is data-efficient for low-resource
  - Interpolation vs. translation objective: Interpolation better for similarity search; translation may be better for specific direction translation (not tested separately)
  - Family tokens vs. language tokens: Family tokens enable zero-shot for unseen languages but may dilute language-specific signals
- **Failure signatures:**
  - High xSIM++ error with low COMET: Embedding space degraded, check training stability
  - Speech translation fails while text works: Adapter not converging—check ASR quality (CER) for that language
  - Zero-shot language performs poorly: Family token assignment may be wrong; verify language family classification
- **First 3 experiments:**
  1. **Validate pretrained adapter zero-shot capability:** Run inference with no adapter training on 3-5 languages with varying ASR quality. If correlation between MMS CER and translation COMET is weak, the mechanism may not be working as claimed.
  2. **Ablate interpolation objective:** Train with only translation MSE (no interpolation) on a subset of 10 languages. Compare zero-shot performance on held-out related languages to quantify interpolation's contribution.
  3. **Test script transfer hypothesis:** Select 2 zero-shot languages sharing script with training languages (e.g., new Cyrillic language) and 2 with different scripts. Character-level should help script-sharing languages more; if not, the mechanism may be different than claimed.

## Open Questions the Paper Calls Out
- Can character-level modeling be effectively extended to target-side decoding in translation without incurring prohibitive computational inefficiency?
- Can a unified acoustic model architecture replace language-specific CTC layers to enable true zero-shot generalization to unseen speech languages?
- Do the performance gains of charSONAR hold when applied to traditional encoder-decoder architectures without a fixed-size embedding bottleneck?

## Limitations
- The approach is limited by language-specific CTC layers in MMS, requiring language-specific adapter training
- Character-level decoding on the target side is problematic and relatively inefficient
- The fixed-size bottleneck reduces model capacity compared to traditional encoder-decoder architectures

## Confidence
**High confidence** in:
- The architectural design and implementation details (charSONAR encoder, cross-modal adapter, family tokens)
- The computational efficiency claims (fixed-size bottleneck, inference overhead)
- The overall zero-shot speech translation capability (the mechanism is sound even if transfer gains vary)

**Medium confidence** in:
- The magnitude of character-level transfer improvements (evidence is indirect, mechanism not fully isolated)
- The embedding interpolation contribution (novel finding, no corpus validation)
- The family token zero-shot mechanism (family definitions not fully specified)

**Low confidence** in:
- Cross-script zero-shot transfer capabilities (not tested)
- Whether gains come from character-level modeling vs. other design choices (mechanism not isolated)
- Generalization to languages outside FLORES+/FLEURS domains

## Next Checks
1. **Isolate the transfer mechanism:** Train two ablations on the same data—one with character-level tokenization but translation-only MSE (no interpolation), another with subword tokenization and interpolation MSE. Compare zero-shot performance on held-out related languages to determine whether characters or interpolation drives gains.

2. **Validate ASR quality dependency:** For the 3 lowest-performing zero-shot languages on FLEURS, obtain or estimate MMS CER scores. Correlate CER with COMET scores across all 33 FLEURS languages. A strong negative correlation (>0.7) would confirm the adapter's dependence on ASR quality.

3. **Test script transfer hypothesis:** Select 4 zero-shot languages: 2 sharing script with training languages (e.g., new Cyrillic language) and 2 with different scripts (e.g., Thai, Amharic). Character-level should show clear advantage for script-sharing languages; if not, the transfer mechanism may be different than claimed.