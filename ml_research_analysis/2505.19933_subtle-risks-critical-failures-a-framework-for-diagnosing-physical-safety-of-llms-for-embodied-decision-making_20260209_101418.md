---
ver: rpa2
title: 'Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety
  of LLMs for Embodied Decision Making'
arxiv_id: '2505.19933'
source_url: https://arxiv.org/abs/2505.19933
tags:
- action
- object
- agent
- goal
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces SAFEL, a safety assessment framework for
  evaluating physical safety in LLM-based embodied decision-making. SAFEL assesses
  LLMs via two tests: Command Refusal (rejecting unsafe commands) and Plan Safety
  (generating safe, executable plans), decomposed into Goal Interpretation, Transition
  Modeling, and Action Sequencing.'
---

# Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making

## Quick Facts
- arXiv ID: 2505.19933
- Source URL: https://arxiv.org/abs/2505.19933
- Reference count: 39
- Key outcome: SAFEL framework reveals LLMs excel at refusing overt harms (82.8-99.1% recall) but struggle with safe planning (44.75% max success rate), with Missing Step errors dominating.

## Executive Summary
This paper introduces SAFEL, a safety assessment framework for evaluating physical safety in LLM-based embodied decision-making. SAFEL assesses LLMs via two tests: Command Refusal (rejecting unsafe commands) and Plan Safety (generating safe, executable plans), decomposed into Goal Interpretation, Transition Modeling, and Action Sequencing. To support SAFEL, the authors create EMBODYGUARD, a PDDL-grounded benchmark of 942 scenarios (541 malicious, 402 situational) covering overt and subtle hazards. Evaluation across 13 state-of-the-art LLMs reveals that while models excel at refusing overtly harmful commands (recall 82.8-99.1%), they struggle with safe planning, especially in Transition Modeling and Action Sequencing. Even the best model, o1, succeeds in safe plan execution only 44.75% of the time, with missing-step errors being the dominant failure mode. These results highlight critical limitations in current LLMs for real-world physical safety.

## Method Summary
The SAFEL framework evaluates LLM physical safety through two tests: Command Refusal (rejecting unsafe commands) and Plan Safety (generating safe plans). The Plan Safety test is decomposed into three functional modules: Goal Interpretation (parsing instructions into symbolic goals), Transition Modeling (predicting action preconditions/effects), and Action Sequencing (ordering actions safely). The EMBODYGUARD benchmark provides 942 PDDL-grounded scenarios (541 malicious, 402 situational) with formal representations of actions, states, and hazards. Evaluation uses symbolic execution to validate plan executability and detect safety violations. LLMs are prompted to generate responses for each module, which are then validated against ground truth using planners and symbolic executors.

## Key Results
- Models achieve high malicious command refusal recall (82.8-99.1%) but struggle with safe planning
- Best model (o1) succeeds in safe plan execution only 44.75% of the time
- Missing Step errors dominate failure modes (29-42% across top models)
- Action Sequencing shows largest performance gap between best and worst models
- Smaller-scale models (1-8B) exhibit nearly 0% success rate in action sequencing

## Why This Works (Mechanism)

### Mechanism 1: Modular Decomposition Enables Localized Failure Diagnosis
Breaking embodied safety evaluation into distinct functional modules allows precise identification of where models fail, rather than reporting aggregate success rates. SAFEL separates Command Refusal from Plan Safety, then further decomposes Plan Safety into Goal Interpretation, Transition Modeling, and Action Sequencing. Each module is evaluated independently. Safety failures originate from specific reasoning stages rather than monolithic model incompetence.

### Mechanism 2: PDDL Grounding Bridges Natural Language to Executable Formalism
Converting natural language instructions into PDDL representations enables simulation-based verification of whether plans would cause physical harm when executed. Each scenario pairs a natural language instruction with a PDDL problem (objects, initial state, goal conditions). A symbolic planner validates executability, and a symbolic executor tracks state transitions to detect safety violations like missing steps, affordance errors, or temporal misordering.

### Mechanism 3: Malicious vs. Situational Risk Taxonomy Captures Distinct Failure Modes
Separating overtly malicious commands (EMBODYGUARD-mal) from contextually hazardous instructions (EMBODYGUARD-sit) reveals that models refuse obvious harms but fail on subtle, situation-dependent risks. Malicious scenarios test refusal (recall: 82.8-99.1%); situational scenarios require models to accept the command but generate safe plans. Situational hazards are embedded in initial conditions (e.g., live wire on floor) rather than explicit instruction harm.

## Foundational Learning

- **PDDL (Planning Domain Definition Language)**: Understanding how actions, preconditions, effects, and states are formally represented is essential to interpreting benchmark construction and evaluation. Quick check: Given an action "microwave(sandwich)" with precondition "not(wrapped, sandwich)" and effect "cooked(sandwich)," what happens if the precondition is violated?

- **Symbolic Execution for Plan Validation**: SAFEL's Action Sequencing module uses symbolic execution to detect intermediate safety violations, not just goal achievement. Quick check: If a plan's goal is achieved but a safety constraint (e.g., "not(dangerous, wire)") is violated mid-execution, would symbolic execution flag this?

- **Recall vs. Precision in Safety Contexts**: The paper reports recall for refusal and goal interpretation; understanding why recall (not precision) is prioritized matters for interpreting results. Quick check: If a model refuses 100% of malicious commands but also refuses 50% of safe commands, what metric tradeoff does this represent?

## Architecture Onboarding

- **Component map**: EMBODYGUARD Benchmark (malicious scenarios test refusal, situational scenarios test safe planning) -> SAFEL Framework (Command Refusal Test -> Plan Safety Test -> Goal Interpretation -> Transition Modeling -> Action Sequencing -> Symbolic Executor -> Error classification)

- **Critical path**: Situational scenarios → Goal Interpretation → Transition Modeling → Action Sequencing → Symbolic Executor → Error classification. This is where the paper's key findings (44.75% max success rate, 29-42% missing step errors) are generated.

- **Design tradeoffs**: PDDL vs. natural language evaluation (PDDL enables precise failure attribution but requires manual/semi-automated formalization; may miss nuances natural language captures); Recall vs. precision focus (paper prioritizes recall, appropriate for safety-critical contexts where misses are costly); Simulation vs. real-world execution (simulation validates plan logic but cannot capture perception, actuation, or environmental noise).

- **Failure signatures**: Missing Step Error (dominant, 29-42%) - model omits safety-preserving action; Affordance Error (0-7.75%) - applying action to incompatible object; Reasoning model degradation - extended rethinking leads to incorrect additions/omissions in transition modeling.

- **First 3 experiments**:
  1. Reproduce transition modeling on primitive vs. new actions: Compare Ap-Scores vs. An-Scores across models to validate whether newly defined actions systematically degrade performance.
  2. Error analysis on missing step patterns: For the top 3 models, manually inspect 20 missing-step failures to identify whether they cluster around specific hazard types (electrical, fire, spillage).
  3. Cross-benchmark validation: Run SAFEL evaluation on existing embodied planning benchmarks (e.g., BEHAVIOR subset) to assess whether failure patterns generalize beyond EMBODYGUARD's synthetic scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
Can extended reasoning mechanisms in models (like o1 or R1) be calibrated to improve Transition Modeling accuracy without "overthinking" and hallucinating false preconditions or effects? The paper identifies this "overthinking" behavior as a cause of failure but does not propose or test methods to regulate reasoning depth to align with ground truth PDDL constraints.

### Open Question 2
What reinforcement learning (RL) reward structures are most effective for improving LLM performance in the Transition Modeling and Action Sequencing modules of the SAFEL framework? The authors evaluate current capabilities but leave the development of specific training algorithms to correct the identified "Missing Step" and "Affordance" errors as future work.

### Open Question 3
How can the dataset construction process be scaled to verify commonsense physical safety (avoiding "Unreasonable object states") without relying on expensive, time-consuming manual expert annotation? The current pipeline requires filtering 1.4K scenarios down to 0.9K via human review to ensure physical realism, creating a bottleneck for expanding the hazard taxonomy.

## Limitations
- PDDL grounding may not fully capture real-world physical complexity, continuous dynamics, or stochastic environmental effects
- Distinction between malicious and situational hazards creates an artificial binary that doesn't reflect continuous real-world risk assessment
- Results may not generalize beyond the iGibson domain and specific hazard types covered in EMBODYGUARD

## Confidence

- **High Confidence**: Modular decomposition approach and error analysis methodology are well-grounded; Missing Step errors as dominant failure mode is supported by robust statistical evidence.
- **Medium Confidence**: PDDL grounding adequately represents physical safety constraints for evaluated scenarios; claim that current LLMs cannot reliably plan safe physical actions is well-supported.
- **Low Confidence**: Generalizability beyond iGibson domain and specific hazard types; refusal rates may be inflated by model-level safety filters.

## Next Checks
1. Cross-Domain Validation: Evaluate SAFEL framework on embodied planning benchmarks from different domains to test whether failure patterns persist across physical contexts.
2. Human Expert Validation: Have safety experts review a stratified sample of 50 plan executions to verify PDDL-based safety violations align with human judgments about physical risk.
3. Continuous State Extension: Extend EMBODYGUARD with continuous physical state representations and re-evaluate whether Missing Step errors persist when safety constraints become quantitative rather than binary.