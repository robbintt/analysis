---
ver: rpa2
title: Mixture of Small and Large Models for Chinese Spelling Check
arxiv_id: '2506.06887'
source_url: https://arxiv.org/abs/2506.06887
tags:
- relm
- llms
- small
- zhang
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic mixture approach that integrates
  BERT-based models with large language models (LLMs) during beam search decoding
  for Chinese spelling check. By combining the precise correction capabilities of
  fine-tuned small models and the fluency strengths of LLMs, the method achieves state-of-the-art
  results on multiple benchmark datasets.
---

# Mixture of Small and Large Models for Chinese Spelling Check

## Quick Facts
- arXiv ID: 2506.06887
- Source URL: https://arxiv.org/abs/2506.06887
- Reference count: 40
- Combines BERT-based small models with LLMs during beam search decoding for Chinese spelling check

## Executive Summary
This paper introduces a dynamic mixture approach that integrates BERT-based models with large language models (LLMs) during beam search decoding for Chinese spelling check. By combining the precise correction capabilities of fine-tuned small models and the fluency strengths of LLMs, the method achieves state-of-the-art results on multiple benchmark datasets. Experiments show that the approach significantly improves both sentence- and character-level F1 scores across domains, including SIGHAN, ECSpell, and LEMON, without requiring LLM fine-tuning. The method also reduces false positive rates while maintaining strong cross-domain generalization.

## Method Summary
The approach fine-tunes BERT-based models on synthetic and in-domain data, then integrates them with frozen LLMs during inference using a custom beam search scoring function. The scoring combines the LLM's autoregressive probability, the small model's character-level correction probability, and a distortion model based on phonetic/visual similarity. Dynamic weights based on LLM entropy adjust the influence of each component during decoding. The method uses predefined probabilities for 5 error types and requires no LLM fine-tuning, saving time and facilitating domain adaptation.

## Key Results
- Achieves state-of-the-art F1 scores on multiple Chinese spelling check benchmarks
- Reduces false positive rates while maintaining high precision and recall
- Works across multiple LLM families without requiring LLM fine-tuning
- Shows strong cross-domain generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
Combining probability distributions from BERT-based models and LLMs during beam search balances the strengths of precise corrections and fluent language generation. The method integrates a small model's log probability into the beam search scoring function of an LLM. The LLM generates candidates autoregressively, while the BERT model provides token-aligned probabilities and a Distortion Model ensures faithfulness to the input. These scores are combined using weighted coefficients and a dynamic entropy-based faithfulness reward.

### Mechanism 2
Dynamic weighting, based on LLM entropy, adaptively adjusts the influence of the small model and distortion model during decoding. The term `(1 + H_LLM(·))` scales the weights of the distortion and small model components. `H_LLM` is the entropy of the LLM's next-token probability distribution. If the LLM is uncertain (high entropy), the weights for distortion and small model components increase, giving more influence to the corrective signals.

### Mechanism 3
Using a prompt-free, training-free LLM as a pure language model preserves its cross-domain generalization capability. By treating the LLM as a pure language model and not using any prompts or instruction fine-tuning, the model relies solely on its vast pre-trained knowledge. This avoids the issue of "over-polishing" or changing the semantic meaning, which can occur when LLMs are prompted to act as correctors.

## Foundational Learning

**Concept: Beam Search Decoding**
Why needed here: This is the core inference algorithm where the mixture of models happens. Understanding how candidates are generated and scored is essential to grasp how probabilities are merged.
Quick check question: How does increasing the beam size affect both the diversity of candidate sentences and the computational cost?

**Concept: Probability Distribution & Log-Likelihood**
Why needed here: The method operates on the log probabilities of tokens from different models. Understanding how to combine these scores linearly is fundamental to the architecture.
Quick check question: Why are log probabilities used instead of raw probabilities when combining scores from multiple models?

**Concept: Tokenization Alignment**
Why needed here: BERT-based models and LLMs use different tokenizers. The paper explicitly describes a method to align and compute probabilities for multi-character LLM tokens using the BERT model's character-level predictions.
Quick check question: If an LLM generates a token representing three characters, how is the probability from the character-level BERT model calculated for that token?

## Architecture Onboarding

**Component map**: LLM (Language Model) -> Beam Search Decoding -> BERT-based Small Model (SM) -> Distortion Model (DM)

**Critical path**: The critical path for implementation is the inference loop:
1. For each step in beam search, generate top-K next tokens from the LLM
2. For each candidate token, calculate its LLM log-probability
3. Align the token to character(s) and get the log-probability from the fine-tuned BERT model for each character
4. Calculate the DM log-probability for the character transformation
5. Compute the final score using Eq. 1, including entropy calculation for the LLM's output distribution
6. Rank and prune candidates

**Design tradeoffs**:
- **Latency vs. Performance**: The paper notes only a 15-17% slowdown compared to running the LLM alone. However, running beam search with a large beam size (e.g., 12) is inherently slow
- **Model Choice**: The paper shows performance varies by LLM family and size, with no clear "larger is better" trend (Table 3). This requires empirical selection
- **Hyperparameters (α, β)**: The optimal weights for the DM and SM components need tuning, typically α ≈ 0.5 and β ≈ 0.9 based on experiments (Section 6.2)

**Failure signatures**:
- **Over-correction**: If the LLM weight is too high or the BERT model's correction signal is too weak, the output may be fluent but semantically different from the original input
- **Under-correction**: If the LLM's language modeling is too strong and confident, it may ignore the BERT model's correction signals, leaving errors uncorrected
- **Length Inconsistency**: This can still occur if the alignment and scoring functions do not sufficiently penalize token additions/deletions
- **Incompatibility with ICL/Chat Models**: Using instruction-tuned or chat versions of LLMs degrades performance (Table 6, Section 6.5)

**First 3 experiments**:
1. **Baseline Reproduction**: Reproduce the single-model baseline scores for the chosen BERT (e.g., ReLM) and LLM (e.g., Baichuan2-7B base) on a validation set (e.g., rSIGHAN15) to ensure environment and models are loaded correctly
2. **Mixture Integration**: Implement the combined scoring function from Eq. 1. Start with fixed weights (α=0.5, β=0.9) and a beam size of 4 to verify that the integration logic works and shows a performance uplift over the single BERT model
3. **Hyperparameter Sweep**: Run a grid search over α and β (e.g., α ∈ [0.3, 0.7], β ∈ [0.5, 1.1]) with a larger beam size (e.g., 12) on a validation set to find the optimal weighting for your specific model combination and domain

## Open Questions the Paper Calls Out
None

## Limitations
- Token alignment complexity between multi-character LLM tokens and character-level BERT probabilities is not fully specified
- Performance on complex semantic errors or out-of-vocabulary words is less clear than on phonetic/visual errors
- Computational overhead claims depend on specific hardware and implementation optimizations

## Confidence

**High Confidence**:
- The mixture approach achieves state-of-the-art results on established benchmarks
- The method reduces false positive rates compared to single-model approaches
- The approach works across multiple LLM families without requiring LLM fine-tuning

**Medium Confidence**:
- The entropy-based dynamic weighting mechanism provides meaningful improvements over fixed weighting
- The prompt-free LLM approach preserves cross-domain generalization better than instruction-tuned alternatives
- The method generalizes well to out-of-domain datasets

**Low Confidence**:
- The specific values of α and β are optimal across all possible model combinations and domains
- The method's performance scales predictably with LLM size
- The 15-17% computational overhead is representative across different hardware configurations

## Next Checks

1. **Token Alignment Verification**: Implement and test the token alignment mechanism with multiple LLM-tokenizer pairs. Measure alignment accuracy and its correlation with final CSC performance to validate this critical component.

2. **Cross-Domain Robustness Test**: Apply the method to a diverse set of CSC datasets including those with heavy semantic errors, domain-specific terminology, and out-of-vocabulary words. Compare performance degradation against the small model baseline to quantify the method's generalization limits.

3. **Hyperparameter Sensitivity Analysis**: Conduct a systematic grid search over α and β across at least three different domain datasets. Analyze how performance varies with these parameters and whether the entropy-based dynamic weighting consistently outperforms fixed weighting across different error distributions.