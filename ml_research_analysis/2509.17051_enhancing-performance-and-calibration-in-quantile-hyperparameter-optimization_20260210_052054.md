---
ver: rpa2
title: Enhancing Performance and Calibration in Quantile Hyperparameter Optimization
arxiv_id: '2509.17051'
source_url: https://arxiv.org/abs/2509.17051
tags:
- quantile
- performance
- hyperparameter
- search
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates enhancing conformalized quantile hyperparameter
  optimization (HPO) through acquisition function diversification, expanded surrogate
  architectures, and improved conformal calibration techniques. Key contributions
  include benchmarking quantile regression with adaptive conformal intervals (DtACI)
  and sample-efficient cross-validation (CV+), exploring additional acquisition functions
  like Expected Improvement and Optimistic Bayesian Sampling, and introducing novel
  surrogate architectures including quantile ensembles and Gaussian Processes.
---

# Enhancing Performance and Calibration in Quantile Hyperparameter Optimization

## Quick Facts
- arXiv ID: 2509.17051
- Source URL: https://arxiv.org/abs/2509.17051
- Authors: Riccardo Doyle
- Reference count: 35
- Ensemble quantile models (QE) with Optimistic Bayesian Sampling achieve state-of-the-art performance, significantly outperforming traditional Gaussian Processes, Tree Parzen Estimators, and SMAC in both general and challenging heteroskedastic/asymmetric settings.

## Executive Summary
This study investigates enhancing conformalized quantile hyperparameter optimization (HPO) through acquisition function diversification, expanded surrogate architectures, and improved conformal calibration techniques. Key contributions include benchmarking quantile regression with adaptive conformal intervals (DtACI) and sample-efficient cross-validation (CV+), exploring additional acquisition functions like Expected Improvement and Optimistic Bayesian Sampling, and introducing novel surrogate architectures including quantile ensembles and Gaussian Processes. Experiments across diverse tabular and image datasets show that ensemble quantile models (QE) with Optimistic Bayesian Sampling achieve state-of-the-art performance, significantly outperforming traditional Gaussian Processes, Tree Parzen Estimators, and SMAC in both general and challenging heteroskedastic/asymmetric settings. Conformalization provides strong calibration benefits under Expected Improvement but mixed results under Thompson Sampling.

## Method Summary
The paper enhances quantile HPO by diversifying acquisition functions (adding EI, OBS), expanding surrogate architectures (introducing QGP and QE ensembles), and improving conformal calibration (CV+, DtACI). It trains quantile surrogates via pinball loss, calibrates predictions using split conformal or cross-conformal methods, and adapts the confidence level dynamically during sequential optimization. Experiments compare these components across tabular and image datasets using multiple benchmarks, evaluating both performance and calibration metrics.

## Key Results
- Ensemble quantile models (QE) with Optimistic Bayesian Sampling achieve state-of-the-art performance across benchmarks
- Conformalization significantly improves calibration under Expected Improvement but has mixed impact under Thompson Sampling
- Quantile regression with adaptive conformal intervals (DtACI) and sample-efficient cross-validation (CV+) enhance coverage and efficiency
- Gaussian Process surrogates underperform in categorical-heavy spaces, while tree-based and ensemble approaches excel

## Why This Works (Mechanism)

### Mechanism 1: Conformal Calibration of Quantile Surrogates
- Claim: Conformalized quantile regression (CQR) provides finite-sample coverage guarantees that improve both calibration metrics and search performance under EI-based acquisition.
- Mechanism: Train quantile estimators via pinball loss on (configuration, performance) pairs, compute non-conformity scores from calibration set miss-coverage, then adjust prediction intervals by the (1−α) quantile of these scores. This corrects miscalibration without assuming normality.
- Core assumption: The paper assumes exchangeability of non-conformity scores holds approximately after adaptive correction (ACI/DtACI); sequential acquisition inherently violates strict exchangeability.
- Evidence anchors:
  - [abstract] "Conformalized quantile regression can address these estimation weaknesses, while still providing robust calibration guarantees."
  - [section 7.1, Table 1] Conformal variants achieve lower calibration error ranks than unconformalized baselines; DtACI further improves over ACI.
  - [corpus] Neighbor papers mention conformal prediction for distribution-free coverage, but direct evidence on HPO-specific conformalization is limited.
- Break condition: If the loss surface is extremely noisy or non-stationary beyond what DtACI can track, coverage guarantees may degrade.

### Mechanism 2: Quantile Ensembling via Linear Stacking (QE)
- Claim: Ensembling heterogeneous quantile surrogates (QGBM, QL, QGP) improves search robustness across diverse hyperparameter spaces, particularly for categorical-heavy or non-normal surfaces.
- Mechanism: Generate cross-validated holdout predictions from each base quantile estimator, stack them as features for a Quantile Lasso meta-learner trained via pinball loss with positive weight constraints. This aggregates complementary strengths (tree-based categorical handling, linear regularization, GP distributional priors).
- Core assumption: Assumption: Base learners make uncorrelated errors and the meta-learner can generalize on very small training sets typical of early HPO iterations.
- Evidence anchors:
  - [section 4.2] Describes quantile linear stacking with Lasso meta-learner and positive weight constraint.
  - [section 7.3, Figure 3] QE consistently outperforms individual surrogates across acquisition functions.
  - [corpus] No direct corpus evidence on quantile stacking in HPO; neighbor papers focus on single-model quantile approaches.
- Break condition: If base learners are all misspecified in similar ways, ensembling provides limited benefit and may add noise.

### Mechanism 3: Acquisition Function Alignment with Conformal Framework
- Claim: Thompson Sampling variants (OBS, TS) outperform Expected Improvement when using quantile-based surrogate distributions, but EI benefits more from conformal calibration.
- Mechanism: EI requires accurate tail estimates; discretized quantile distributions may poorly capture extremes, harming EI. Thompson Sampling draws from the full distribution, reducing granularity dependence. OBS floors samples at the conditional expectation, encouraging exploration.
- Core assumption: Assumption: The discretized quantile distribution adequately approximates the true conditional distribution for Thompson Sampling but is insufficient for EI's tail-sensitive calculation.
- Evidence anchors:
  - [section 3.1-3.2] Describes EI and OBS formulations under quantile framework.
  - [section 7.2, Figure 1] EI underperforms TS approaches; OBS slightly outperforms standard TS.
  - [section 7.3, Figure 4] Conformalization significantly benefits EI but has mixed/negative impact on TS.
  - [corpus] Neighbor paper on quantile-scaled BO mentions rank-based feedback but doesn't directly compare acquisition functions.
- Break condition: If quantile density is dramatically increased (M→large), EI may approach TS performance, though empirical results show diminishing returns for TS.

## Foundational Learning

- Concept: Quantile Regression and Pinball Loss
  - Why needed here: The core surrogate models predict conditional quantiles rather than point estimates; understanding how pinball loss trains these quantiles is essential.
  - Quick check question: Given a target quantile β=0.9 and prediction error u=5, what is the pinball loss?

- Concept: Conformal Prediction (Split Conformal, CV+, ACI/DtACI)
  - Why needed here: The paper's calibration improvements depend on conformal methods providing finite-sample coverage guarantees under exchangeability.
  - Quick check question: Why does splitting data into train/calibration sets trade training quality for calibration, and how does CV+ mitigate this?

- Concept: Bayesian Optimization Acquisition Functions (EI, TS, UCB, OBS)
  - Why needed here: Acquisition functions drive exploration-exploitation; the paper shows their performance varies significantly with surrogate type and conformalization.
  - Quick check question: How does Thompson Sampling differ from Expected Improvement in its use of the posterior distribution?

## Architecture Onboarding

- Component map:
  - Surrogate layer: QGBM (tree-based, categorical-strong), QL (linear, regularized), QGP (distributional, small-data-strong), QRF (forest-based), QE (ensemble via stacking)
  - Calibration layer: Split Conformal Prediction (SCP), Cross-Conformal (CV+), Adaptive (ACI), Dynamic-Tunable Adaptive (DtACI)
  - Acquisition layer: Expected Improvement (EI), Thompson Sampling (TS), Optimistic Bayesian Sampling (OBS), Upper Confidence Bound (UCB)
  - Data flow: Warm-start configurations → surrogate training → quantile prediction → conformal adjustment → acquisition scoring → next configuration selection

- Critical path:
  1. Initialize with 15 random configurations
  2. Train quantile surrogates on accumulated (X, Y) pairs
  3. Compute non-conformity scores on calibration fold (SCP) or via CV+
  4. Adjust quantile predictions using conformal intervals
  5. Apply DtACI to adapt α_t based on sequential coverage feedback
  6. Score candidates via chosen acquisition function (OBS or TS recommended)
  7. Select next configuration, evaluate, add to dataset, repeat

- Design tradeoffs:
  - CV+ vs SCP: CV+ uses all data for training (better sample efficiency) but increases runtime K-fold; SCP is faster but wastes calibration data. Paper uses heuristic: CV+ for t<50, SCP thereafter.
  - More quantiles (higher M): Improves EI (better tail capture) but shows no benefit for TS; adds computational cost.
  - Ensemble vs single surrogate: QE is most robust but requires training and maintaining multiple models; QGBM alone is competitive and simpler.
  - Conformalization: Essential for EI, may harm TS if model misspecification happens to suit the environment.

- Failure signatures:
  - Poor calibration on 25% intervals: Noisy non-conformity scores at narrow intervals; consider higher minimum confidence levels.
  - GP underperformance on categorical-heavy spaces: Use QGBM or QE instead.
  - Degraded performance after conformalization under TS: Calibration may be correcting useful misspecification; evaluate without conformalization or switch to EI.
  - Coverage drift in long horizons: DtACI should adapt; if not, check learning rate parameters (γ candidates, η, σ).

- First 3 experiments:
  1. Replicate calibration analysis (Table 1) on a single dataset: Compare unconformalized, SCP+DtACI, and CV++DtACI across 25%/50%/75% intervals to validate your implementation.
  2. Ablate surrogate architectures on LCBench-L subset: Run QGBM, QGP, and QE with OBS acquisition for 100 iterations, comparing final performance ranks.
  3. Test conformalization impact by acquisition function: For QGBM surrogate, compare with/without conformalization under EI, TS, and OBS to reproduce Figure 4 patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical results focus on moderate-sized tabular and image tasks; generalization to extremely high-dimensional hyperparameter spaces remains uncertain
- Conformalization shows context-dependent benefits—essential for EI but potentially harmful for TS, suggesting the need for problem-specific calibration
- DtACI adaptation relies on heuristic parameters (γ, η, σ) that may require tuning per problem; robustness without broader empirical sweeps is uncertain

## Confidence
- QE + OBS as state-of-the-art: High
- Conformalization universally beneficial: Medium (context-dependent)
- DtACI robustness: Medium (relies on heuristic parameters)
- Generalization to high-dimensional spaces: Low (limited empirical testing)

## Next Checks
1. Test DtACI sensitivity: Run QE + OBS with DtACI on a challenging heteroskedastic dataset, sweeping the γ and σ parameters to confirm adaptive coverage without degrading performance.
2. High-dimensional stress test: Apply QE + OBS to a benchmark with >50 hyperparameters to verify that ensembling still improves over single surrogates in large search spaces.
3. Long-horizon stability: Extend experiments to 500 iterations on a single dataset, monitoring coverage drift and final regret to validate DtACI and ensemble stability over time.