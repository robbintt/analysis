---
ver: rpa2
title: Evaluating the Systematic Reasoning Abilities of Large Language Models through
  Graph Coloring
arxiv_id: '2502.07087'
source_url: https://arxiv.org/abs/2502.07087
tags:
- math
- problems
- friends
- error
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Graph coloring tasks expose significant gaps in large language\
  \ models\u2019 (LLMs) systematic reasoning capabilities, with even advanced models\
  \ achieving less than 40% accuracy on moderately complex problems. Six models\u2014\
  four standard LLMs and two large reasoning models (LRMs)\u2014were evaluated on\
  \ procedurally generated graph coloring problems across multiple semantic frames."
---

# Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring

## Quick Facts
- arXiv ID: 2502.07087
- Source URL: https://arxiv.org/abs/2502.07087
- Reference count: 40
- Even advanced LLMs achieve less than 40% accuracy on moderately complex graph coloring problems

## Executive Summary
This paper evaluates the systematic reasoning capabilities of large language models (LLMs) using graph coloring tasks. Six models—four standard LLMs and two large reasoning models (LRMs)—were tested on procedurally generated graph coloring problems across multiple semantic frames. The results reveal significant gaps in LLM reasoning abilities, with standard models showing error rates exceeding 60% on difficult problems. While LRMs demonstrate improved performance, they still struggle with complex problems and exhibit substantial framing effects. The study provides a novel benchmark for assessing reasoning capabilities and highlights both recent progress and persistent limitations in LLM reasoning.

## Method Summary
The study evaluates six models on procedurally generated graph coloring problems using five problem sets with varying vertex and color counts. Models include Llama 3.1 405B, GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, o1-mini, and DeepSeek-R1. Problems are generated with specific difficulty metrics based on greedy algorithm success rates. Each problem is tested across four semantic frames (Math, Cities, Friends, Math-demanding) with five repetitions per combination. Error rates are calculated for different problem types and difficulty levels.

## Key Results
- Standard LLMs show error rates exceeding 60% on difficult graph coloring problems
- LRMs improve performance but still achieve less than 40% accuracy on complex problems
- No model achieves perfect accuracy even on simple 4-vertex, 2-color problems
- Semantic framing significantly affects performance across all models
- Error rates increase dramatically as problem complexity grows, especially for LRMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard LLMs rely on "greedy" algorithms, solving problems step-by-step without robust backtracking.
- **Core assumption:** The model's forward generation process struggles to explore alternative paths once an initial trajectory is "locked in."
- **Evidence:** Error rates spike when greedy score drops, particularly from g ≥ 0.9 to 0.5 ≤ g < 0.9.

### Mechanism 2
- **Claim:** LRMs improve by extending possibility space exploration but this capability saturates as complexity increases.
- **Core assumption:** Reinforcement learning training incentivizes longer chains of thought that mimic search, but the mechanism doesn't guarantee complete traversal of the solution tree.
- **Evidence:** LRMs show lower error rates (>10-15% on difficult problems vs >60% for standard) but still fail to reach perfect accuracy.

### Mechanism 3
- **Claim:** Reasoning reliability is contaminated by semantic framing, causing models to treat logically identical problems differently.
- **Core assumption:** The model's attention mechanism is distracted or biased by semantic tokens that don't contribute to the logical solution.
- **Evidence:** Substantial but varying framing effects across different problem types and semantic contexts.

## Foundational Learning

- **Concept: NP-Completeness & Greedy Heuristics**
  - **Why needed here:** Graph coloring is NP-complete, and understanding greedy algorithm failures is essential for interpreting difficulty metrics.
  - **Quick check question:** If a problem has a "greedy score" of 0.1, does a greedy algorithm solve it frequently or rarely? (Answer: Rarely)

- **Concept: Autoregressive generation**
  - **Why needed here:** Explains why standard LLMs behave like greedy algorithms and struggle with backtracking.
  - **Quick check question:** Why does the autoregressive nature of LLMs make "backtracking" inherently difficult without external mechanisms?

- **Concept: Semantic Frames / Prompt Engineering**
  - **Why needed here:** Helps diagnose whether model failures are due to logic limits or context confusion.
  - **Quick check question:** If a model solves a puzzle in "colored balls" frame but fails in "meeting schedules" frame, which limitation is exposed?

## Architecture Onboarding

- **Component map:** Procedural Generator -> Difficulty Filter -> Frame Renderer -> Evaluator
- **Critical path:** Dataset generation is computationally cheap, but evaluation scales with models, frames, and repetitions
- **Design tradeoffs:** Temperature settings differ between standard LLMs (0) and LRMs (0.5-1.0) due to API constraints
- **Failure signatures:** Greedy Trap, False Negative, Semantic Drift
- **First 3 experiments:**
  1. Run 4v2c set on target model; if error rate > 0, model lacks fundamental reliability
  2. Plot accuracy vs. greedy score; linear drop indicates greedy heuristics, flat indicates systematic search
  3. Test 7v3c problem across all frames; calculate variance to determine semantic bias susceptibility

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does LRM performance exhibit a rapid drop toward chance-level at some complexity threshold, or does it decline gradually?
- **Basis:** Tested range (n ≤ 8) is restricted, limiting visibility into scaling behavior
- **What would resolve it:** Testing on systematically larger graphs (n > 8, k > 4) to identify accuracy cliffs

### Open Question 2
- **Question:** Why does error rate jump disproportionately between medium and low greedy-score categories?
- **Basis:** Jump from g ≥ 0.9 to 0.5 ≤ g < 0.9 is larger than expected from greedy reasoning alone
- **What would resolve it:** Correlating error rates with additional difficulty metrics like backtracking depth

### Open Question 3
- **Question:** Can architectural innovations yield fundamental improvements in LRM reasoning robustness?
- **Basis:** Current LRMs show improved linear reasoning but remain weak at possibility space exploration
- **What would resolve it:** Testing next-generation LRMs with novel architectures on the same benchmark

## Limitations
- Focus on small-scale problems (n ≤ 8 vertices) may not capture real-world reasoning capabilities
- API-constrained temperature settings introduce variance that complicates direct model comparisons
- Lenient parsing approach may obscure subtle failure modes in model outputs
- Semantic frame construction depends on subjective mappings that could influence results

## Confidence
- **High Confidence:** Standard LLMs exhibit significant error rates (>60%) on complex graph coloring problems
- **Medium Confidence:** Error patterns stem from "greedy" reasoning and insufficient possibility space exploration
- **Low Confidence:** Specific quantitative claims about error rate increases are sensitive to sampling and parsing methods

## Next Checks
1. Replicate study on larger graph instances (n = 10-15 vertices) to assess reasoning limitations at scale
2. Conduct ablation study using both strict and lenient parsing to quantify impact on error measurements
3. Systematically vary semantic frame construction to isolate semantic bias from core reasoning limitations