---
ver: rpa2
title: Low Rank Gradients and Where to Find Them
arxiv_id: '2510.01303'
source_url: https://arxiv.org/abs/2510.01303
tags:
- data
- have
- gradient
- then
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the low-rank structure of gradients in two-layer
  neural networks under relaxed isotropy assumptions. The key finding is that the
  gradient with respect to input weights is approximately rank-two, dominated by two
  rank-one components: one aligned with the bulk data residue (S1) and another with
  the rank-one spike in input data (S2).'
---

# Low Rank Gradients and Where to Find Them

## Quick Facts
- arXiv ID: 2510.01303
- Source URL: https://arxiv.org/abs/2510.01303
- Authors: Rishi Sonthalia; Michael Murray; Guido Montúfar
- Reference count: 40
- Key outcome: The gradient with respect to input weights in two-layer neural networks is approximately rank-two, dominated by two rank-one components (S1 aligned with bulk data residue, S2 aligned with rank-one spike), with prominence depending on data properties, network scaling, activation functions, and regularization.

## Executive Summary
This paper analyzes the low-rank structure of gradients in two-layer neural networks under relaxed isotropy assumptions. The key finding is that the gradient with respect to input weights is approximately rank-two, dominated by two rank-one components: one aligned with the bulk data residue (S1) and another with the rank-one spike in input data (S2). The relative prominence of these components depends on data properties, network scaling, activation functions, and regularization. For small-to-moderate data spikes (ν<0.5), the gradient is approximately rank-two; for large spikes (ν≥0.5), it becomes approximately rank-one. ReLU activations suppress the residue component compared to smooth activations. Regularization techniques like weight decay, input noise, and Jacobian penalties selectively modulate these components. The analysis covers both mean-field and neural-tangent-kernel scaling regimes, revealing different gradient alignments and training dynamics. Theoretical predictions are validated on synthetic and real datasets (MNMNIST, CIFAR-10).

## Method Summary
The authors develop a theoretical framework to analyze gradient low-rank structure in two-layer neural networks, extending beyond isotropic data assumptions. They decompose the input-weight gradient into rank-one components aligned with data structure (S1 for bulk residue, S2 for spike component). The analysis covers two scaling regimes: mean-field (parameter scaling with data) and neural-tangent-kernel (parameters fixed, width scaling). They examine how different activation functions (ReLU vs smooth) and regularization methods (weight decay, input noise, Jacobian penalties) affect the relative strength of these components. The theoretical predictions are validated through experiments on synthetic data with controlled correlation structures and real datasets including MNMIST and CIFAR-10.

## Key Results
- Gradient with respect to input weights is approximately rank-two, dominated by S1 (bulk data residue) and S2 (rank-one spike) components
- For small-to-moderate data spikes (ν<0.5), gradient is approximately rank-two; for large spikes (ν≥0.5), becomes approximately rank-one
- ReLU activations suppress the residue component (S1) compared to smooth activations
- Regularization techniques (weight decay, input noise, Jacobian penalties) selectively modulate S1 and S2 components
- Different scaling regimes (mean-field vs NTK) reveal distinct gradient alignments and training dynamics

## Why This Works (Mechanism)
The low-rank gradient structure emerges from the interaction between input data structure and network dynamics. The rank-one spike in input data creates a dominant gradient component (S2), while the bulk data residue forms the second component (S1). Network scaling determines which component dominates: in mean-field scaling, both components contribute significantly, while NTK scaling emphasizes the spike component. Activation functions modulate this structure by their interaction with the data geometry - ReLU's piecewise linearity suppresses the residue component by zeroing out negative activations, while smooth activations maintain both components. Regularization techniques selectively amplify or attenuate these components, providing a mechanism to control training dynamics through gradient structure manipulation.

## Foundational Learning
- **Low-rank structure in neural networks**: Neural network gradients often exhibit low-rank structure due to data geometry and network architecture. Understanding this structure is crucial for optimization efficiency and generalization.
  - Why needed: Low-rank gradients enable more efficient optimization and provide insights into training dynamics.
  - Quick check: Verify that gradient matrices have significantly fewer non-zero singular values than their full rank.

- **Two-layer network analysis**: Shallow networks serve as tractable models for understanding deeper architectures, with analytical solutions possible for gradient structure.
  - Why needed: Two-layer networks capture essential phenomena while remaining mathematically tractable.
  - Quick check: Confirm that theoretical predictions match empirical observations on simple datasets.

- **Scaling regimes (mean-field vs NTK)**: Different parameter scaling leads to distinct learning dynamics and gradient structures in neural networks.
  - Why needed: Scaling determines whether networks behave more like kernel methods or adaptive feature learners.
  - Quick check: Compare training dynamics under different parameter scaling schemes.

- **Activation function effects**: Different activation functions interact differently with input data structure, affecting gradient composition.
  - Why needed: Activation choice significantly impacts learning dynamics and generalization.
  - Quick check: Measure how gradient components change when switching between ReLU and smooth activations.

## Architecture Onboarding

**Component Map**: Data Spike -> Network Parameters -> Gradient Decomposition (S1, S2) -> Training Dynamics

**Critical Path**: Data structure → Network initialization → Gradient computation → Component decomposition → Training optimization

**Design Tradeoffs**: 
- Shallow networks provide analytical tractability but may not capture deep network phenomena
- Mean-field scaling enables feature learning but requires more parameters; NTK scaling is more parameter-efficient but behaves like kernel methods
- Smooth activations maintain both gradient components but may slow training; ReLU suppresses residue but can cause dead neurons

**Failure Signatures**: 
- When rank-one spike assumption fails, the S2 component may not dominate as predicted
- In deep networks, gradient low-rank structure may become more complex than rank-two
- Strong regularization may eliminate both components, preventing effective learning

**3 First Experiments**:
1. Verify rank-two gradient decomposition on synthetic data with controlled rank-one spike structure
2. Compare S1/S2 component strengths when switching between ReLU and smooth activations
3. Measure how weight decay regularization selectively affects the residue component (S1)

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies heavily on rank-one spike assumption for input data structure, which may not capture real-world dataset complexity
- Approximation quality of rank-two gradient decomposition lacks rigorous bounds on approximation error
- Study focuses on shallow two-layer networks, leaving unclear how findings extend to deep architectures
- Theoretical framework assumes specific initialization schemes and activation functions, with ReLU analysis showing suppression of residue component but not fully characterizing activation-dependent effects

## Confidence
- High confidence: The theoretical framework for rank-two gradient decomposition under stated assumptions is mathematically sound
- Medium confidence: Empirical validation on synthetic data supports theoretical predictions, though extension to complex real datasets needs more thorough testing
- Low confidence: Claims about training dynamics and optimization implications require further empirical validation, particularly regarding how rank structure affects convergence rates and generalization

## Next Checks
1. Test the rank-two gradient decomposition approximation error bounds on diverse real-world datasets with varying input correlation structures beyond the rank-one spike model

2. Extend the analysis to deeper network architectures (3+ layers) to determine if rank-two structure persists or transforms in more complex models

3. Conduct controlled experiments varying initialization schemes, activation functions, and regularization combinations to systematically map their effects on S1 and S2 component strengths