---
ver: rpa2
title: Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization
arxiv_id: '2510.06732'
source_url: https://arxiv.org/abs/2510.06732
tags:
- ranking
- rank
- token
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work exposes a critical vulnerability in LLM-based reranking
  systems: small, natural-sounding prompt modifications can consistently elevate target
  items in rankings. We propose Rank Anything First (RAF), a two-stage token optimization
  framework that generates adversarial prompts through a combination of gradient-based
  shortlisting and dynamic entropy-weighted evaluation.'
---

# Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization

## Quick Facts
- arXiv ID: 2510.06732
- Source URL: https://arxiv.org/abs/2510.06732
- Authors: Tiancheng Xing; Jerry Li; Yixuan Du; Xiyang Hu
- Reference count: 7
- Primary result: Small, natural-sounding prompt modifications can consistently elevate target items in LLM-based reranking systems

## Executive Summary
This work exposes a critical vulnerability in LLM-based reranking systems: small, natural-sounding prompt modifications can consistently elevate target items in rankings. We propose Rank Anything First (RAF), a two-stage token optimization framework that generates adversarial prompts through a combination of gradient-based shortlisting and dynamic entropy-weighted evaluation. Experiments across multiple LLMs show RAF achieves lower average ranks (e.g., 3.26 vs 5.54 baseline on Coffee Machine) and higher fluency (lower perplexity) while maintaining stealth. RAF prompts are also transferable across models, making this vulnerability practical and systemic. The findings highlight that LLM rerankers are inherently susceptible to adversarial manipulation, raising urgent challenges for trustworthiness and robustness in modern retrieval systems.

## Method Summary
The Rank Anything First (RAF) framework generates adversarial prompts to manipulate LLM rerankers through a two-stage token optimization process. Stage 1 uses gradient-based shortlisting to efficiently identify top candidate tokens from the full vocabulary, combining target ranking and readability gradients with fixed weights. Stage 2 evaluates exact losses on this smaller candidate set using entropy-based dynamic weighting that adapts between ranking and readability objectives based on model uncertainty. The framework samples final tokens from a temperature-controlled softmax distribution over combined losses, iteratively building up to 30-token adversarial prompts. This approach balances effectiveness (rank manipulation), fluency (natural language), and stealth (undetectability) while maintaining transferability across different LLM models.

## Key Results
- RAF achieves average rank reduction from 5.54 to 3.26 on Coffee Machine category
- Generated prompts show lower perplexity (10.89 vs 17.73 baseline) indicating higher fluency
- RAF demonstrates strong transferability with rank deltas <0.6 across different models
- Two-stage optimization with dynamic weighting outperforms single-stage and fixed-weight approaches

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Token Optimization with Dual Objectives
Separating candidate shortlisting from exact evaluation improves both efficiency and stability over single-stage optimization. Stage 1 uses gradient approximation to quickly filter the full vocabulary to top-B candidates. Stage 2 computes exact losses on this smaller set, combining them with dynamic weights. This avoids the instability of optimizing directly over |V| tokens while ensuring precise objective evaluation. Core assumption: gradient approximations correlate sufficiently with true loss rankings to preserve good candidates in the shortlist.

### Mechanism 2: Entropy-Based Dynamic Weighting
Adjusting the balance between ranking and readability objectives based on model uncertainty produces more natural and effective adversarial prompts than fixed weights. At each token position, compute Shannon entropy of the next-token distribution. When entropy is low (model confident), upweight readability to maintain fluency. When uncertain, upweight the ranking objective. Formula: w_read = β · (H_max - H(p_read)) / H_max. Core assumption: entropy signals meaningfully indicate when optimization should prioritize one objective over the other.

### Mechanism 3: Temperature-Controlled Stochastic Sampling
Sampling from a softmax distribution over combined losses prevents brittle local optima and improves transferability. After computing L_comb for all candidates, sample the final token from softmax(-L_comb/τ). Temperature τ controls exploration: higher τ increases randomness, lower τ approaches greedy selection. This mimics natural language variation while maintaining optimization pressure. Core assumption: stochasticity helps escape local optima that would trap purely greedy methods.

## Foundational Learning

- **Concept: Greedy Coordinate Gradient (GCG)**
  - Why needed here: Stage 1 relies on gradient-based shortlisting. Understanding how gradients over discrete tokens approximate token impact is essential for debugging candidate selection.
  - Quick check question: Given a vocabulary of 50K tokens, how does GCG efficiently identify promising candidates without evaluating all of them?

- **Concept: Cross-Entropy Loss for Sequence Generation**
  - Why needed here: The ranking objective L_tar uses token-level cross-entropy between predicted probabilities and the target output sequence.
  - Quick check question: Why is cross-entropy preferred over MSE for language model objectives, and what does minimizing it imply about the target rank?

- **Concept: Perplexity as Fluency Proxy**
  - Why needed here: The readability objective and evaluation metric both use perplexity. Understanding its relationship to entropy and fluency is critical for interpreting results.
  - Quick check question: If a prompt has perplexity 15 vs. 150, what does this indicate about its linguistic naturalness, and why might lower perplexity correlate with stealthiness?

## Architecture Onboarding

- **Component map**: Product description + current attack sequence + candidate token -> Stage 1 (gradient computation + shortlist) -> Stage 2 (exact loss evaluation + dynamic weighting + sampling) -> Selected token -> Output

- **Critical path**: 1. Initialize attack sequence empty 2. Stage 1: Compute gradients, shortlist top-512 candidates 3. Stage 2: Compute exact losses with entropy weighting, sample token via softmax 4. Check convergence 5. Append selected token, repeat until 30 tokens or convergence

- **Design tradeoffs**: Shortlist size B (512 used), temperature τ for sampling, prompt length (30 tokens default), target vs readability weights (fixed in Stage 1: 300, Stage 2: 40)

- **Failure signatures**: Non-convergence without readability objective, low transferability with greedy selection, high perplexity indicating poor fluency, high bad word ratio indicating stealth failure

- **First 3 experiments**: 1. Reproduce main results on Llama-3.1-8B with STSData Coffee Machine category, verify rank reduction and perplexity 2. Ablate dynamic weighting, compare with fixed weights vs target-only 3. Test transferability, optimize on Llama-3.1-8B, evaluate on Mistral-7B and DeepSeek-7B

## Open Questions the Paper Calls Out

1. What specific defensive mechanisms can effectively mitigate RAF attacks without degrading the legitimate performance of LLM rerankers? The paper calls for "systematic defenses" but does not evaluate robustness against countermeasures like adversarial training or input purification.

2. Can RAF maintain its effectiveness when transferred to commercial, closed-source LLMs (e.g., GPT-4, Claude) typically used in production? The study validates transferability only across open-weight models; behavior on large, aligned proprietary APIs remains unknown.

3. Does the vulnerability extend to non-listwise reranking paradigms, specifically pairwise or pointwise methods? The RAF optimization objective minimizes loss of generating target item as first token in a list; unclear if gradient-based manipulation works when model outputs relevance scores or binary comparisons instead.

## Limitations

- Transferability ceiling remains non-trivial (+0.55 rank delta on Coffee Machine category)
- Token shortlist size sensitivity not characterized; B=512 not theoretically justified
- Prompt length constraints not fully addressed; 30-token default may exceed practical deployment limits
- Stealth claims rely heavily on perplexity without human evaluation or detection-system testing

## Confidence

**High Confidence**: Two-stage optimization architecture demonstrably improves efficiency and stability over single-stage methods. Experimental evidence consistently shows rank reduction from ~5.5 to ~3.3 with corresponding perplexity improvements. Core vulnerability robustly established.

**Medium Confidence**: Entropy-based dynamic weighting mechanism's contribution to fluency and transferability is supported by comparative experiments but lacks mechanistic explanation. Temperature-controlled sampling's role in preventing local optima is theoretically sound but empirically demonstrated primarily through transfer gap reduction.

**Low Confidence**: Stealth claims rely heavily on perplexity as proxy for naturalness without human evaluation. Bad word ratio metric mentioned but not prominently featured in results. Practical exploitability in real-world retrieval systems remains speculative.

## Next Checks

1. **Shortlist Size Sensitivity Analysis**: Systematically vary B from 64 to 2048 candidates in Stage 1 and measure impact on rank effectiveness, perplexity, and computational cost to characterize gradient approximation correlation strength.

2. **Human Evaluation of Stealth**: Conduct blinded human assessments comparing RAF-generated prompts against natural product descriptions and baseline adversarial prompts to validate perplexity-based stealth claims.

3. **Real-World Retrieval System Integration**: Implement RAF in simulated e-commerce retrieval pipeline with adversarial prompts injected into product feeds, measuring not just rank manipulation but also detection rates and impact on overall retrieval quality metrics.