---
ver: rpa2
title: 'Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and
  Ethics of Low-Rank LLMs'
arxiv_id: '2511.22099'
source_url: https://arxiv.org/abs/2511.22099
tags:
- chat
- low-rank
- base
- proj
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the trustworthiness of large
  language models (LLMs) under low-rank compression, examining privacy, adversarial
  robustness, fairness, and ethics. Low-rank factorization (SVD, Basel, FWSVD) reduces
  model size while maintaining performance, but its safety implications were unknown.
---

# Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs

## Quick Facts
- **arXiv ID**: 2511.22099
- **Source URL**: https://arxiv.org/abs/2511.22099
- **Reference count**: 40
- **Primary result**: Low-rank compression reduces training data leakage but may expose more PII; preserves or improves adversarial robustness while degrading fairness and ethics.

## Executive Summary
This study systematically evaluates the trustworthiness of large language models (LLMs) under low-rank compression, examining privacy, adversarial robustness, fairness, and ethics. Low-rank factorization (SVD, Basel, FWSVD) reduces model size while maintaining performance, but its safety implications were unknown. Results show compressed models leak less training data but may expose more personally identifiable information in certain settings. Adversarial robustness is preserved or improved, while fairness and ethical reasoning degrade under compression. Fine-tuning enhances task performance but increases vulnerability to attacks, especially in larger models. Layerwise attribution identifies embedding and down-projection layers as critical to trustworthiness. Overall, low-rank compression offers efficiency gains with mixed trustworthiness trade-offs, requiring careful evaluation before deployment.

## Method Summary
The study applies low-rank factorization methods (SVD, Basel, FWSVD) to compress LLaMA2 and Qwen2.5 models at 30%, 50%, and 70% parameter retention. Models are evaluated on privacy (Enron Email dataset), adversarial robustness (SST-2, QQP, MNLI with AdvGLUE++ perturbations), fairness (UCI Adult dataset), and ethics (ETHICS-Short). Layerwise attribution using gradient sensitivity identifies critical components for robustness. Fine-tuned variants for math and code tasks are also tested under compression.

## Key Results
- Low-rank compression reduces training data leakage but increases conversational PII exposure in zero-shot settings
- Adversarial robustness is preserved or improved, with k=70 showing best performance on SST-2 and MNLI
- Fairness metrics (Mdpd, Meod) degrade significantly under compression, with largest effects at k=30
- Ethics reasoning declines in zero-shot but recovers in few-shot prompting
- Embedding and down-projection layers are identified as most critical for robustness

## Why This Works (Mechanism)

### Mechanism 1: Training-Data Privacy Improvement via Representational Constriction
Low-rank compression reduces leakage of memorized training data by constraining representational capacity. Singular value truncation removes low-magnitude dimensions that encode fine-grained training examples, limiting memorization pathways unlike fine-tuning which reshapes existing parameters.

### Mechanism 2: Adversarial Robustness Preservation via Implicit Regularization
Compression preserves or improves robustness by pruning brittle, high-variance feature directions while retaining dominant semantic ones. This implicit regularization keeps model stability under adversarial perturbations even with substantial parameter reduction.

### Mechanism 3: Fairness Degradation via Bias Amplification in Dominant Subspaces
Low-rank compression degrades fairness by retaining biased directions while removing corrective variance. SVD preserves directions with largest singular values, and if societal biases correlate with high-variance features, compression amplifies them by removing competing low-variance fairness signals.

## Foundational Learning

- **Singular Value Decomposition (SVD)**: Decomposes weight matrices W ≈ UΣV^T. Essential for understanding how rank truncation affects model properties. Quick check: If a weight matrix has 100 singular values and you retain only the top 20, what types of information are most likely preserved vs. lost?

- **Gradient-based Attribution**: Used to identify layers contributing to robustness through a_i = ||∂ℓ/∂h_i · h_i||_2. Informs compression strategies. Quick check: A layer with high attribution sensitivity (large Δ_i = |a_clean - a_adv|) under adversarial inputs suggests what about its role in robustness?

- **Trustworthiness Taxonomy (DecodingTrust Framework)**: Evaluation structure following four dimensions: training-data privacy, PII protection, adversarial robustness, and ethics/fairness. Each requires different evaluation protocols. Quick check: Why might a model that rejects adversarial prompts (high robustness) still fail on fairness metrics?

## Architecture Onboarding

- **Component map**: embed_tokens -> Attention layers (k_proj, q_proj, v_proj, o_proj) -> MLP layers (gate_proj, up_proj, down_proj)

- **Critical path**: embed_tokens (first transformation point; adversarial perturbations begin here) -> down_proj (MLP bottleneck that compresses and transmits non-linear transformations) -> k_proj/q_proj (more important in 13B models than 7B models)

- **Design tradeoffs**:
  | Compression Level | Privacy | Robustness | Fairness | Ethics (zero-shot) |
  |-------------------|---------|------------|----------|-------------------|
  | k=70 (mild)       | ↑↑      | ↑          | ↓        | ↓                 |
  | k=50 (moderate)   | ↑↑      | ↑          | ↓↓       | ↓↓                |
  | k=30 (aggressive) | ↑↑      | ~          | ↓↓↓      | ↓↓↓               |

- **Failure signatures**: Fairness collapse (Mdpd > 5 or Meod > 10 indicates over-compression), Ethics degradation (zero-shot accuracy near 0% on ETHICS-Short with partial recovery in few-shot), PII over-exposure (leak rate > 40% in zero-shot PII tests)

- **First 3 experiments**:
  1. **Layer-sensitivity profiling**: Apply gradient attribution on your specific model + task before compression to identify which layers to preserve (focus on embed_tokens and down_proj analogues)
  2. **Compression sweep at k∈{70, 50, 30}**: Measure all four trustworthiness dimensions; do not rely on benign accuracy alone
  3. **Few-shot ethics recovery test**: If zero-shot ethics degrades, verify that few-shot prompting restores performance; if not, compression ratio may be too aggressive for your safety requirements

## Open Questions the Paper Calls Out

### Open Question 1
Can compression techniques be modified to prevent the decline in fairness observed in low-rank models? The paper quantifies fairness degradation but does not propose a mechanism to mitigate this bias amplification during factorization.

### Open Question 2
Why does low-rank compression improve training data privacy while simultaneously weakening protection against conversational PII leakage? The authors identify this divergence but do not fully explain the representational mechanisms behind this mixed result.

### Open Question 3
Does excluding high-attribution layers (e.g., `embed_tokens` and `down_proj`) from compression improve the trustworthiness-efficiency trade-off? While the paper identifies critical layers, it does not test a non-uniform compression strategy that preserves sensitive layers.

## Limitations
- The paper documents fairness degradation but lacks mechanistic explanation for why bias amplification occurs
- Basel and FWSVD implementations are referenced without full specification of key parameters
- Results may not generalize to other model architectures beyond LLaMA2 and Qwen2.5

## Confidence
- **High confidence**: Privacy improvements under compression, adversarial robustness preservation at k≥50, layer attribution rankings (embed_tokens and down_proj consistently critical)
- **Medium confidence**: Ethics degradation patterns, fairness metric increases, rank-dependent tradeoffs (k=70 optimal for robustness, k=30 too aggressive)
- **Low confidence**: Basel and FWSVD relative performance differences, exact mechanisms of fairness degradation, cross-model generalization of attribution patterns

## Next Checks
1. **Mechanistic ablation study**: Run controlled experiments isolating embedding vs down_proj compression to quantify each layer's contribution to fairness and ethics degradation. Compare singular value distributions in compressed vs uncompressed models to identify bias-amplifying directions.

2. **Cross-architecture replication**: Apply identical compression and evaluation pipeline to GPT-style models (e.g., Mistral) and domain-specific LLMs (e.g., clinical or legal models). Verify whether embed_tokens/down_proj attribution patterns persist and whether privacy/robustness tradeoffs remain consistent.

3. **Bias intervention test**: Apply post-compression bias mitigation (e.g., weighted training data, adversarial debiasing) to determine if fairness degradation is reversible or represents fundamental information loss. Compare Mdpd/Meod recovery rates across k values.