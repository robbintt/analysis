---
ver: rpa2
title: Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge
arxiv_id: '2510.19266'
source_url: https://arxiv.org/abs/2510.19266
tags:
- attention
- distillation
- training
- student
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of efficiently transferring\
  \ knowledge from pretrained Transformers to state-space models (SSMs) like Mamba,\
  \ which is difficult due to architectural differences and high computational costs.\
  \ The proposed method, Cross-architecture Distillation via Attention Bridge (CAB),\
  \ introduces a lightweight MLP-based attention bridge to align token-level attention\
  \ projections (B and C in Mamba) with the teacher\u2019s key and query representations,\
  \ enabling fine-grained, data-efficient supervision."
---

# Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge

## Quick Facts
- arXiv ID: 2510.19266
- Source URL: https://arxiv.org/abs/2510.19266
- Authors: Penghao Wang; Yuhao Zhou; Mengxuan Wu; Panpan Zhang; Zhangyang Wang; Kai Wang
- Reference count: 40
- Primary result: Introduces a lightweight MLP-based attention bridge to align token-level projections in Mamba with Transformer key/query representations, enabling data-efficient cross-architecture knowledge distillation.

## Executive Summary
This paper addresses the challenge of transferring knowledge from pretrained Transformers to state-space models (SSMs) like Mamba, which is difficult due to architectural differences and high computational costs. The proposed method, Cross-architecture Distillation via Attention Bridge (CAB), introduces a lightweight MLP-based attention bridge to align token-level attention projections (B and C in Mamba) with the teacher's key and query representations, enabling fine-grained, data-efficient supervision. This approach avoids the quadratic overhead of full attention matrix alignment and supports flexible layer-wise mapping between heterogeneous architectures. Experiments on ImageNet and language modeling tasks show that CAB consistently outperforms standard and cross-architecture distillation baselines, achieving up to 16.3% accuracy gains under limited data (1%-20% of ImageNet) and lower perplexity in language tasks. CAB also accelerates convergence and reduces memory usage by 10× compared to full-matrix methods.

## Method Summary
The method introduces a lightweight MLP-based attention bridge to align token-level attention projections (B and C in Mamba) with the teacher's key and query representations, enabling fine-grained, data-efficient supervision. This approach avoids the quadratic overhead of full attention matrix alignment and supports flexible layer-wise mapping between heterogeneous architectures. The core innovation is initializing the Mamba state transition matrix A ≈ 0 (forcing Ā ≈ I) to structurally align with linear attention, then using learnable 2-layer MLPs (φ_B, φ_C) to project student B and C projections into the teacher's representation space, minimizing MSE between these vectors.

## Key Results
- CAB achieves up to 16.3% accuracy gains under limited data (1%-20% of ImageNet) compared to standard distillation
- The method shows lower perplexity in language tasks and accelerates convergence
- Memory usage is reduced by 10× compared to full-matrix alignment methods
- CAB consistently outperforms both standard distillation and cross-architecture distillation baselines across vision and language tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning token-level projections (B, C) in Mamba with Key/Query (K, Q) vectors in Transformers transfers relational inductive biases without the quadratic cost of full attention matrix distillation.
- **Mechanism:** The paper posits that Mamba's continuous-time projections B and C function as "implicit attention carriers" (analogous to K and Q). A lightweight MLP bridge (φ_B, φ_C) projects the student's B and C into the teacher's representation space, minimizing the Mean Squared Error (MSE) between these vectors.
- **Core assumption:** The structural equivalence between Mamba's recurrence and linear attention (when state transition Ā ≈ I) holds sufficiently for B and C to carry semantic relational information similar to K and Q.
- **Evidence anchors:** [abstract] "CAB introduces a lightweight MLP-based attention bridge to align token-level attention projections (B and C in Mamba) with the teacher's key and query representations..." [section 3.1] Eq. 5 establishes the structural link between recurrence and linear attention. [corpus] Neighbor paper "On Structured State-Space Duality" supports the theoretical equivalence between scalar-identity SSMs and masked attention.
- **Break condition:** This mechanism may fail if the SSM requires complex state decay dynamics (non-identity Ā) for the specific task, rendering the linear attention approximation invalid.

### Mechanism 2
- **Claim:** Initializing the Mamba state transition matrix A ≈ 0 (forcing Ā ≈ I) stabilizes the structural alignment during distillation.
- **Mechanism:** By initializing Ā to identity, the recurrence simplifies to an additive form (h_t ≈ h_{t-1} + ...), structurally mimicking linear attention. This reduces the optimization gap between the Transformer teacher and the Mamba student during the alignment phase.
- **Core assumption:** The initial training phase benefits more from structural similarity to the teacher than from the SSM's native temporal dynamics (standard S4D initialization).
- **Evidence anchors:** [section 4.3] Table 5 shows "Align B+C w/ Ā ≈ I (Ours)" achieving 49.2% accuracy vs 43.1% for default initialization. [section 4.1] "Initializing A≈0... simplifies the recurrence into a form structurally equivalent to linear attention..."
- **Break condition:** If the target task relies heavily on long-horizon state decay features that are suppressed by the identity initialization, performance may degrade unless the model can relearn these dynamics later.

### Mechanism 3
- **Claim:** Proportional layer mapping enables effective knowledge transfer when student and teacher depths differ.
- **Mechanism:** A mapping function g(l) = ⌊l/L · T⌋ connects student layer l to teacher layer g(l). This allows fine-grained supervision even when the student is shallower or deeper than the teacher.
- **Core assumption:** Semantic features evolve linearly across layers in both architectures, allowing a proportional index to map analogous processing stages.
- **Evidence anchors:** [section 3.2] Eq. 7 defines the layer mapping function. [abstract] "...supports flexible layer-wise mapping between heterogeneous architectures."
- **Break condition:** If the teacher and student process information in different orders (e.g., one aggregates local features early and the other late), proportional mapping may align mismatched semantic levels.

## Foundational Learning

- **Concept:** Linear Attention & Recurrent Formulation
  - **Why needed here:** The core theoretical bridge relies on understanding that standard attention (QK^T V) can be reformulated as a recurrence (h_t = h_{t-1} + K_t^T V_t). Without this, the alignment of B, C to K, Q appears arbitrary.
  - **Quick check question:** Can you explain why linear attention reduces complexity from O(L^2) to O(L) and how that relates to a recurrent state h_t?

- **Concept:** Knowledge Distillation (Logit vs. Feature)
  - **Why needed here:** CAB moves beyond standard soft distillation (logits) to feature-level distillation (intermediate projections). Distinguishing these is necessary to implement the loss functions correctly.
  - **Quick check question:** What is the difference between minimizing KL divergence of output logits and MSE of intermediate feature maps?

- **Concept:** Mamba Architecture (SSMs)
  - **Why needed here:** Identifying and extracting the specific B and C projection vectors requires familiarity with the Selective State Space Model mechanics (discretization and token-dependent parameters).
  - **Quick check question:** In Mamba, how do the B and C matrices differ functionally from the learned transition matrix A?

## Architecture Onboarding

- **Component map:** Teacher (Transformer) -> Bridge MLPs (φ_B, φ_C) -> Student (Mamba/Vim)
- **Critical path:**
  1. Extract K, Q from Teacher layer g(l)
  2. Extract B, C from Student layer l
  3. Pass B, C through the Bridge MLPs
  4. Compute MSE loss between φ_B(B) and K, and φ_C(C) and Q
  5. Combine with standard distillation loss (KL divergence)

- **Design tradeoffs:**
  - **Initialization:** Initializing Ā ≈ I improves alignment but may temporarily reduce native SSM temporal modeling capacity
  - **Bridge Complexity:** The paper uses a 2-layer MLP; deeper bridges might overfit or hinder gradient flow
  - **Training Schedule:** Under full data (100% ImageNet), continuous alignment over-constrains the student (performance drop). An "early-stop" strategy (switching to KL only after ~50 epochs) is required

- **Failure signatures:**
  - **Performance Collapse:** Directly copying attention weights or forcing alignment without the Bridge MLP leads to instability (Fig 1b)
  - **Over-constraint:** In full-data regimes, maintaining the alignment loss for too long (e.g., 300 epochs) degrades final accuracy compared to early stopping (Table 7)

- **First 3 experiments:**
  1. **Sanity Check (Projection):** Implement the Bridge MLPs and verify dimension matching between a random Teacher K and Student B. Loss should converge to near zero on dummy data.
  2. **Low-Data Baseline:** Train Vim-Tiny on 1% ImageNet with standard Soft Distillation vs. CAB. Verify the claimed ~7-8% accuracy gap (Table 3).
  3. **Ablation on Initialization:** Compare training convergence with default S4D initialization vs. Ā ≈ I initialization on a small subset (10% data) to confirm optimization stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed attention bridge mechanism be generalized to other state-space model variants or hybrid architectures that do not share Mamba's specific structural equivalence to linear attention?
- **Basis in paper:** [explicit] The conclusion states, "our approach currently focuses on Mamba and Transformers, and extending it to other SSM variants or more complex hybrid architectures may require further investigation."
- **Why unresolved:** The method relies on aligning Mamba's B and C projections with Transformer K and Q, treating them as "implicit attention carriers." It is unclear if this specific token-level alignment strategy is effective for SSMs with different parameterizations (e.g., global state decay) or hybrid models that combine attention and recurrence.
- **What evidence would resolve it:** Experiments applying the CAB framework to distill knowledge into alternative architectures like RWKV or attention-augmented hybrids (e.g., Jamba), showing consistent performance gains without architectural modifications.

### Open Question 2
- **Question:** Is there a principled, adaptive mechanism to determine the optimal scheduling of attention alignment versus soft distillation to prevent over-constraining the student model?
- **Basis in paper:** [explicit] Appendix A.1 notes that enforcing alignment throughout training degrades performance, suggesting "excessive attention supervision can over-constrain the mismatch between Transformer attention and the SSM latent space."
- **Why unresolved:** While the authors successfully employ a manual "early-stopped alignment" strategy (e.g., stopping at 35 epochs), they do not define a theoretical criterion or automated schedule for when to switch from alignment to pure KL distillation.
- **What evidence would resolve it:** Ablation studies comparing fixed early-stopping heuristics against dynamic schedulers (based on gradient magnitude or validation metrics) across diverse datasets and training durations.

### Open Question 3
- **Question:** Does initializing the transition matrix Ā ≈ I to facilitate alignment hinder the student model's ability to learn complex temporal dynamics available to native SSMs?
- **Basis in paper:** [explicit] Section 4.3 mentions that while the simplified initialization improves transfer, the default S4D initialization is "designed to capture diverse temporal dynamics" which "may not align well with attention-based supervision."
- **Why unresolved:** There is a potential trade-off between maximizing knowledge transfer (by mimicking linear attention structure) and preserving the unique representational capacities of the SSM (captured by the standard S4D initialization), which was not fully evaluated on long-range reasoning tasks.
- **What evidence would resolve it:** Comparative evaluation of CAB-distilled models on benchmarks specifically designed for long-context retrieval or state-tracking, where the unique dynamics of native SSM initializations typically outperform linear attention.

## Limitations

- The method relies on structural equivalence between Mamba's recurrence and linear attention, which may break when tasks require complex state decay dynamics beyond linear attention approximation
- Initializing Ā ≈ I may temporarily suppress the SSM's native temporal modeling capacity and potentially limit long-term generalization to tasks requiring complex state decay
- Proportional layer mapping assumes semantic features evolve linearly across layers in both architectures, which may fail when teacher and student process information in fundamentally different orders

## Confidence

- **High Confidence:** The empirical results showing CAB's superiority over standard distillation baselines (16.3% accuracy gain under 1% ImageNet, lower perplexity in language tasks) are well-supported by the experimental section. The architectural implementation details (MLP bridge, loss formulation) are clearly specified.
- **Medium Confidence:** The mechanism explanations (token-level projection alignment, structural equivalence to linear attention) are theoretically grounded but rely on assumptions about the SSM's behavior that may not hold universally across all tasks.
- **Low Confidence:** The long-term generalization of models trained with Ā ≈ I initialization is unclear. The paper doesn't investigate whether these models can recover complex temporal dynamics after the distillation phase or if they are permanently biased toward linear attention patterns.

## Next Checks

1. **Structural Alignment Stress Test:** Implement the Bridge MLPs and verify dimension matching between a random Teacher K and Student B. Loss should converge to near zero on dummy data. Then, train on a small subset (10% data) with default Mamba initialization vs. Ā ≈ I to confirm optimization stability and identify if performance collapse occurs without proper initialization.

2. **Over-constraint Diagnostic:** Monitor test loss during training; if accuracy plateaus early or degrades in full-data regimes (100% ImageNet), adopt an "early-stop" strategy for the alignment loss (e.g., stop alignment at epoch 35-50, switch to pure soft distillation). Compare final accuracy with continuous alignment to validate the claim that over-constraint degrades performance.

3. **Generalization Capacity Test:** After distillation with Ā ≈ I initialization, fine-tune the student on a task requiring long-range temporal dependencies (e.g., character-level language modeling with >1024 context length). Measure whether the student can recover complex state decay dynamics or remains biased toward linear attention patterns, testing the long-term validity of the initialization strategy.