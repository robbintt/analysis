---
ver: rpa2
title: 'TSGM: Regular and Irregular Time-series Generation using Score-based Generative
  Models'
arxiv_id: '2511.21335'
source_url: https://arxiv.org/abs/2511.21335
tags:
- time-series
- score
- generation
- data
- irregular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TSGM, the first score-based generative model
  framework for universal time-series synthesis. The key innovation is designing an
  autoregressive denoising score matching loss that enables SGM to handle the sequential
  dependencies inherent in time-series data.
---

# TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models

## Quick Facts
- **arXiv ID**: 2511.21335
- **Source URL**: https://arxiv.org/abs/2511.21335
- **Reference count**: 40
- **Primary result**: Introduces TSGM, the first score-based generative model framework for universal time-series synthesis, achieving state-of-the-art performance on both regular and irregular time-series across four real-world datasets

## Executive Summary
This paper presents TSGM, a score-based generative model framework for time-series synthesis that handles both regular and irregular data. The key innovation is an autoregressive denoising score matching loss that enables score-based models to capture sequential dependencies in time-series data. TSGM uses a two-phase approach: pre-training an autoencoder (RNN-based for regular, continuous-time for irregular series) followed by training a conditional score network. Experiments on Stock, Energy, Air, and AI4I datasets demonstrate superior performance compared to 11 baselines across discriminative and predictive scores, with strong results even at 70% missing rates for irregular time-series.

## Method Summary
TSGM employs a two-phase training approach for time-series generation. First, an autoencoder (GRU-based for regular series, NCDE+GRU-ODE for irregular) is pre-trained using MSE reconstruction loss to learn latent representations. Second, a conditional score network (modified 1D U-Net) is trained using an autoregressive denoising score matching loss on the latent space. For irregular time-series, the model handles missing data by randomly dropping observations during training and generating complete sequences during evaluation. Sampling uses reverse-time diffusion with predictor-corrector methods, starting from Gaussian noise and progressively denoising to generate synthetic time-series.

## Key Results
- Achieves state-of-the-art discriminative scores on Energy dataset: 0.198±0.025 (subVP) vs TimeGAN's 0.236±0.012
- Maintains predictive scores nearly identical to original data (Energy: 0.252±0.000 vs 0.250±0.003)
- Shows superior diversity through t-SNE and KDE visualizations
- Performs well under irregular settings with 30%, 50%, and 70% missing rates
- Outperforms 11 baselines including TimeGAN, TimeVAE, GT-GAN, and diffusion-based methods

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Denoising Score Matching
- Claim: Score-based generative models can capture sequential dependencies in time-series by learning conditional score functions with an autoregressive denoising loss.
- Mechanism: The paper derives an autoregressive denoising score matching loss (Theorem 1) that proves L₁ = L_score, allowing the model to learn ∇logp(x^s_{1:n}|x^0_{1:n}) instead of the computationally prohibitive ∇logp(x^s_{1:n}|x^0_{1:n-1}). The score network M_θ(s, x^s_{1:n}, x^0_{1:n-1}) learns the gradient of the conditional log-likelihood given past observations.
- Core assumption: RNN-based encoders satisfy the Markovian property, meaning h_n captures all information from x_{1:n}, which allows the conditional score to depend only on the current and previous hidden states.
- Evidence anchors:
  - [abstract]: "designing an autoregressive denoising score matching loss that enables SGM to handle the sequential dependencies inherent in time-series data"
  - [Section II-A3]: "we derive an autoregressive denoising score matching... connecting SGMs to time-series generation domain"
  - [corpus]: Weak corpus support—related papers focus on diffusion for time-series forecasting/imputation, not synthesis; no direct corroboration of this specific autoregressive loss formulation.
- Break condition: If the encoder fails to compress temporal history into h_n (e.g., non-Markovian dynamics, very long sequences), the conditional score approximation degrades.

### Mechanism 2: Latent Space Diffusion with Conditional Score Network
- Claim: Performing diffusion in a learned latent space rather than directly on raw time-series improves both efficiency and generation quality.
- Mechanism: The framework pre-trains an autoencoder (L_ed loss), then trains a conditional score network M_θ on the latent space using L^H_score. The encoder embeds x^0_{1:n} → h^0_n, and the score network learns ∇logp(h^s_n|h^0_{n-1}). During sampling, the reverse SDE denoises latent vectors, which are decoded to time-series via the pre-trained decoder.
- Core assumption: The latent space preserves sufficient temporal structure for the score network to learn meaningful gradients, and the encoder-decoder reconstruction error is small enough not to propagate artifacts.
- Evidence anchors:
  - [abstract]: "The framework consists of an autoencoder (RNN-based for regular, continuous-time for irregular series) and a conditional score network based on a modified U-Net architecture."
  - [Section III-C]: "we modify the popular U-net architecture for our purposes... 1-dimensional convolution layers to handle time-series observations"
  - [corpus]: Corpus papers (e.g., Diff-MN, diffusion for irregular time-series) support latent diffusion for time-series but don't validate this specific U-Net modification.
- Break condition: If the latent space dimension is too low, temporal correlations are lost; if too high, the score network training becomes unstable.

### Mechanism 3: Irregular Time-series Handling via Continuous-time Methods
- Claim: Continuous-time encoder/decoder architectures enable handling of irregularly sampled time-series with missing values by modeling the underlying continuous trajectory.
- Mechanism: For irregular series, the encoder uses Neural Controlled Differential Equations (NCDEs) that evolve hidden states via Riemann-Stieltjes integrals over interpolated paths. The decoder uses GRU-ODE with jump layers at observation times. During training, observations are randomly dropped (30%/50%/70%); at inference, complete sequences are generated.
- Core assumption: Missing observations are missing-at-random or missing-not-at-random but informative enough that the continuous path interpolation (natural cubic spline) recovers plausible dynamics.
- Evidence anchors:
  - [Section III-B]: "NCDEs typically use the natural cubic spline algorithm to define X(t), which is twice differentiable"
  - [Table V]: TSGM-subVP maintains strong performance even at 70% missing rates (e.g., Energy discriminative score 0.235±0.123 vs. baselines >0.3)
  - [corpus]: "A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking" (arXiv:2510.06699) corroborates diffusion-based completion for irregular data.
- Break condition: If missing patterns are highly non-random or gaps exceed the interpolation's ability to reconstruct, generated sequences may diverge from true distribution.

## Foundational Learning

- Concept: **Stochastic Differential Equations (SDEs) for Diffusion**
  - Why needed here: The forward/reverse processes are defined via SDEs with drift f(s,x_s) and diffusion g(s). Understanding VP vs. subVP SDE types is essential for hyperparameter selection.
  - Quick check question: Can you explain why subVP generally achieves better NLL than VP, and when VE might be preferred?

- Concept: **Score Matching and Denoising Score Matching**
  - Why needed here: The core training objective replaces the intractable score function with a tractable denoising score. Understanding why ∇logp(x_s|x_0) is analytically computable (Gaussian transition kernel) is critical.
  - Quick check question: Why does denoising score matching avoid the partition function problem that plagues explicit density estimation?

- Concept: **Neural CDEs and GRU-ODE for Irregular Time-series**
  - Why needed here: For irregular settings, understanding how NCDEs handle continuous paths and how GRU-ODE jump layers incorporate observations at irregular times is essential for debugging.
  - Quick check question: What is the role of the cubic spline interpolation in NCDEs, and why must X(t) be twice differentiable?

## Architecture Onboarding

- Component map:
  - **Encoder**: GRU-based for regular time-series (h_n = e(h_{n-1}, x_n)); NCDE-based for irregular (integrates over continuous path)
  - **Decoder**: GRU-based for regular (x̂_n = d(h_n)); GRU-ODE for irregular (continuous-time evolution with jump layers)
  - **Conditional Score Network M_θ**: Modified 1D U-Net taking (s, h^s_n, h^0_{n-1}), outputs score estimate
  - **Pre-training phase**: Encoder-decoder trained with L_ed (MSE reconstruction)
  - **Main training phase**: Score network trained with L^H_score on frozen encoder outputs

- Critical path:
  1. Pre-train autoencoder (iter_pre: 50k–100k iterations)
  2. Encode training data to latent vectors h^0_{1:N}
  3. Train score network with autoregressive denoising loss L^H_score
  4. Sample: initialize h^1_1 ~ N(0,I), denoise via reverse SDE using M_θ, decode to time-series

- Design tradeoffs:
  - **use_alt hyperparameter**: Alternating training vs. freezing encoder after pre-training—paper finds dataset-dependent optimal choice
  - **SDE type**: VP more stable, subVP better NLL; paper uses subVP for main experiments
  - **Sampling steps**: 1000 steps default; can reduce to 100 for subVP with minimal degradation (Table VIII)
  - **Memory vs. quality**: TSGM requires ~1.9x memory vs. TimeGAN for training; sampling is ~200x slower (fundamental SGM limitation)

- Failure signatures:
  - **Mode collapse indicator**: Discriminative score near 0.5 (random classifier) with poor predictive score—check if encoder reconstruction loss is too high
  - **Irregular setting degradation**: If VP performance drops sharply at 70% missing while subVP remains stable, this is expected per paper observations
  - **Diversity collapse**: t-SNE shows synthesized samples clustered in narrow region—may indicate insufficient sampling steps or score network undertraining

- First 3 experiments:
  1. **Reproduce regular time-series baseline on Energy dataset**: Use Table VI hyperparameters (dim(h)=56, use_alt=False, iter_pre=100k). Target: discriminative score ~0.198±0.025 (subVP). This validates the full pipeline.
  2. **Ablate pre-training**: Train encoder, decoder, and score network jointly from scratch. Compare to pre-trained baseline per Table IX. Expect ~30-50% discriminative score degradation on Energy.
  3. **Test irregular setting with 50% missing on Stock dataset**: Use NCDE encoder + GRU-ODE decoder with Table VI irregular hyperparameters (D_hidden=48, dim(h)=24). Target: predictive score ~0.011±0.000 (close to original 0.011±0.002). This validates continuous-time components.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sampling efficiency of TSGM be significantly improved to match GAN-based methods without sacrificing generation quality?
- Basis in paper: [explicit] Section IV-E explicitly identifies the sampling time (e.g., 86.32s vs 0.47s for baselines) as a "fundamental drawback" of the SGM framework.
- Why unresolved: The authors note that while techniques exist (referenced as [27, 47]), they were not implemented or evaluated in this work.
- What evidence would resolve it: A study applying fast sampling solvers or distillation techniques to TSGM and reporting the resulting speed/quality trade-off curves.

### Open Question 2
- Question: Does TSGM effectively capture "informative missingness" where data absence is correlated with specific states, as opposed to the random dropping strategy used in experiments?
- Basis in paper: [inferred] The introduction cites "informative missingness" [8] as a key property of irregular time-series (e.g., medical events), yet the methodology (Section IV-A1) simulates irregularity by randomly dropping 30-70% of observations.
- Why unresolved: It is unclear if the model learns to generate missing patterns that are structurally meaningful or if it treats them merely as noise to be interpolated.
- What evidence would resolve it: Experiments on datasets with naturally occurring, non-random missingness (e.g., PhysioNet) comparing the generated missing patterns against the ground truth distribution.

### Open Question 3
- Question: How does TSGM scale to very long time-series sequences regarding memory consumption and the preservation of long-term temporal dependencies?
- Basis in paper: [inferred] The evaluated datasets (Table VII) utilize short sequence lengths (24 to 28 time steps), but the autoregressive U-Net architecture may struggle with the receptive field requirements of longer sequences.
- Why unresolved: The paper does not analyze the computational complexity or performance degradation as the sequence length $N$ increases significantly.
- What evidence would resolve it: Benchmarking TSGM on datasets with sequence lengths in the hundreds or thousands to observe memory limits and discriminative score trends.

## Limitations

- Sampling time is ~200x slower than GAN-based methods due to the iterative denoising process, identified as a fundamental drawback of the SGM framework
- The autoregressive denoising loss formulation relies on encoder Markovian assumptions that may not hold for long-range temporal dependencies
- The model uses random observation dropping to simulate irregularity, which may not capture informative missingness patterns present in real-world data

## Confidence

- **High**: TSGM framework design and experimental methodology are clearly specified and reproducible. The two-phase training procedure (pre-training + score network training) is well-defined.
- **Medium**: The autoregressive denoising score matching loss formulation and its connection to time-series generation are theoretically justified but depend on encoder properties not fully validated.
- **Medium**: Performance improvements over baselines are significant and consistent across datasets, though exact replication requires filling in architectural details.

## Next Checks

1. **Ablation study on pre-training**: Train encoder, decoder, and score network jointly from scratch vs. pre-trained baseline to quantify the impact of the two-phase approach on discriminative/predictive scores.

2. **Architecture sensitivity analysis**: Systematically vary the 1D U-Net depth, channel dimensions, and residual connection configurations to identify optimal design choices and robustness.

3. **Memory-time tradeoff evaluation**: Quantify the practical impact of SGM's ~200x slower sampling time vs. GAN/VAE alternatives on real-world deployment scenarios, particularly for irregular time-series with missing data.