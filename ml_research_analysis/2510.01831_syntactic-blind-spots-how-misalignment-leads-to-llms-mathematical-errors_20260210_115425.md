---
ver: rpa2
title: 'Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors'
arxiv_id: '2510.01831'
source_url: https://arxiv.org/abs/2510.01831
tags:
- complexity
- syntactic
- rephrasing
- correct
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a class of reasoning failures in large language
  models (LLMs) termed syntactic induction failures. These occur when models misapply
  familiar solution strategies to problems that are semantically simple but syntactically
  unfamiliar.
---

# Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors

## Quick Facts
- arXiv ID: 2510.01831
- Source URL: https://arxiv.org/abs/2510.01831
- Reference count: 40
- Primary result: DLT scores correlate with LLM failure rates; rephrasing fixes 15.5% of errors

## Executive Summary
This paper identifies a class of reasoning failures in large language models (LLMs) termed syntactic induction failures. These occur when models misapply familiar solution strategies to problems that are semantically simple but syntactically unfamiliar. Using Dependency Locality Theory (DLT) to quantify syntactic complexity, the authors show that higher DLT scores correlate with increased failure rates across multiple math reasoning datasets (GSM8K, SVAMP, MultiArith, ASDiv). By rephrasing syntactically complex questions to match the structure of correctly answered examples, they recover accuracy on previously failed questions, with improvements up to 15.5% on some models and datasets. These findings suggest that many LLM reasoning errors stem from structural misalignment rather than conceptual difficulty, and that syntax-aware interventions can mitigate these inductive failures.

## Method Summary
The authors employ Dependency Locality Theory (DLT) to quantify syntactic complexity by measuring the integration cost of parsing each token based on its syntactic dependencies. They analyze multiple mathematical reasoning datasets (GSM8K, SVAMP, MultiArith, ASDiv) across various LLM architectures, correlating DLT scores with failure rates. The core intervention involves rephrasing syntactically complex questions to match the structural patterns of questions that models answer correctly, effectively reducing the DLT score while preserving semantic content. They evaluate performance improvements through controlled experiments comparing original versus rephrased questions.

## Key Results
- Higher DLT scores correlate with increased failure rates across all tested datasets and models
- Rephrasing syntactically complex questions recovers accuracy on previously failed questions
- Accuracy improvements of up to 15.5% achieved through syntactic paraphrasing
- Models show better performance on questions with syntactic structures similar to their training data

## Why This Works (Mechanism)
None

## Foundational Learning
- Dependency Locality Theory (DLT): A psycholinguistic framework measuring syntactic integration costs based on dependency distances
  - Why needed: Provides quantitative metric for predicting which syntactic structures cause reasoning failures
  - Quick check: Verify DLT scores align with human difficulty ratings for sentence comprehension

- Syntactic induction failures: When models misapply familiar solution strategies to semantically simple but syntactically unfamiliar problems
  - Why needed: Identifies specific failure mode distinct from conceptual misunderstanding
  - Quick check: Test if rephrasing fixes errors without changing mathematical content

- Structural alignment: The degree to which problem syntax matches patterns seen during training
  - Why needed: Explains why models succeed on some structurally similar problems but fail on others
  - Quick check: Compare DLT distributions between training and test sets

## Architecture Onboarding

**Component map:** Input question -> DLT parser -> Syntax complexity score -> Failure prediction -> Paraphrasing engine -> Rephrased question -> LLM inference

**Critical path:** DLT complexity measurement → Failure prediction → Targeted rephrasing → Performance recovery

**Design tradeoffs:** 
- DLT provides interpretable complexity scores but may not capture all failure-inducing features
- Paraphrasing preserves semantics while reducing complexity, but requires template engineering
- Focus on inference-time fixes rather than training-time solutions limits long-term robustness

**Failure signatures:** 
- High DLT scores (> threshold) predict increased failure probability
- Models fail consistently on specific syntactic patterns despite semantic similarity to successful examples
- Chain-of-thought traces show disrupted reasoning flow on high-DLT inputs

**First 3 experiments to run:**
1. Measure DLT scores across all test questions and correlate with failure rates for each model
2. Apply paraphrasing to high-DLT failed questions and measure accuracy recovery
3. Test whether models trained on rephrased examples show improved zero-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do syntactic induction failures observed in mathematical reasoning generalize to other domains such as logical entailment, commonsense reasoning, or code generation?
- Basis in paper: The conclusion states, "While our experiments focus on mathematical benchmarks, the implications are broader. Syntactic induction failures may underlie reasoning brittleness across domains."
- Why unresolved: The empirical evaluation was restricted to math word problem datasets (GSM8K, SVAMP, MultiArith, ASDiv), leaving the cross-domain applicability of the DLT-based framework untested.
- What evidence would resolve it: Applying the DLT scoring metric and rephrasing intervention to non-math benchmarks (e.g., Big-Bench Hard, HumanEval) and observing similar accuracy recovery rates.

### Open Question 2
- Question: How does syntactic misalignment affect the fidelity of intermediate reasoning steps (Chain-of-Thought) rather than just the final answer?
- Basis in paper: The limitations section notes, "We evaluate only final-answer accuracy, without analyzing intermediate reasoning."
- Why unresolved: It is unclear if rephrasing fixes a superficial "schema trigger" failure or if syntactic complexity causes cascading errors in the step-by-step generation logic.
- What evidence would resolve it: A comparative analysis of the logical consistency of Chain-of-Thought traces for original high-DLT questions versus their rephrased counterparts.

### Open Question 3
- Question: Can "syntactic curriculum learning" (training on gradually increasing syntactic complexity) permanently resolve these blind spots without requiring inference-time rephrasing?
- Basis in paper: Section 6.4 and Section 7 propose future work on "Syntactic curriculum learning: Gradually exposing models to varied syntactic structures during training to improve generalization."
- Why unresolved: The current work demonstrates a "lightweight" inference-time fix (rephrasing) but does not explore if the internal representation can be made robust to syntax through training modifications.
- What evidence would resolve it: Training a model on a curriculum of DLT-scaled examples and evaluating its zero-shot robustness on structurally complex, unrephrased test sets.

## Limitations
- The causal mechanism linking DLT complexity to failures remains partially unclear
- Paraphrasing effectiveness may not generalize beyond tested datasets and models
- Focus on arithmetic reasoning limits generalizability to broader mathematical domains

## Confidence
- High confidence: The correlation between DLT scores and failure rates across multiple datasets and models is robust and well-demonstrated
- Medium confidence: The effectiveness of syntactic paraphrasing as a mitigation strategy, though results are promising, may not generalize universally
- Medium confidence: The interpretation that failures primarily stem from structural misalignment rather than conceptual difficulty, as this may oversimplify the interaction between syntax and semantics

## Next Checks
1. Test the DLT-based failure prediction and paraphrasing approach on non-arithmetic mathematical domains (algebra, geometry, calculus) to assess generalizability
2. Conduct ablation studies comparing DLT-based paraphrasing with alternative syntactic complexity measures to validate that DLT specifically captures the relevant failure-inducing features
3. Evaluate model performance after fine-tuning on paraphrased examples versus just prompting with paraphrased questions to determine the most effective intervention point