---
ver: rpa2
title: 'SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data'
arxiv_id: '2510.03962'
source_url: https://arxiv.org/abs/2510.03962
tags:
- time
- series
- soft
- anomaly
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SPEAR, a soft prompt enhanced anomaly recognition
  method for time series data that adapts small language models (LLMs) for anomaly
  detection through learnable soft prompts and quantization. SPEAR quantizes time
  series data into discrete tokens, combines them with soft prompt embeddings, and
  feeds them into frozen LLMs to detect anomalies via binary classification.
---

# SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data

## Quick Facts
- arXiv ID: 2510.03962
- Source URL: https://arxiv.org/abs/2510.03962
- Reference count: 33
- SPEAR-BERT achieves 0.93 accuracy and 0.93 F1-score on MIMIC-IV, outperforming zero-shot methods and LSTM baselines

## Executive Summary
SPEAR introduces a parameter-efficient approach for time series anomaly detection using soft prompts with frozen LLMs. The method quantizes continuous time series data into discrete tokens, concatenates them with learnable soft prompt embeddings, and processes them through frozen encoder models. Evaluated on MIMIC-IV, NASA, and NAB datasets, SPEAR-BERT demonstrated superior performance to zero-shot methods and larger LLMs, achieving 0.93 accuracy/F1 on MIMIC-IV and strong AUROC/AUPR scores on NASA. The approach proves that small models with soft prompts can effectively detect anomalies while maintaining computational efficiency.

## Method Summary
SPEAR adapts frozen LLMs for time series anomaly detection through learnable soft prompts and quantization. The process involves: (1) min-max scaling continuous time series to [0,1], then quantizing into discrete bins; (2) initializing soft prompts as trainable embedding vectors; (3) concatenating soft prompts with token embeddings as input to frozen LLM; (4) adding binary classification head; (5) training only soft prompts and classifier for 40 epochs using cross-entropy loss. The method uses T-SMOTE for class imbalance and evaluates with accuracy, F1, AUROC, and AUPR metrics.

## Key Results
- SPEAR-BERT achieved 0.93 accuracy and 0.93 F1-score on MIMIC-IV, outperforming zero-shot methods and LSTM baselines
- On NASA satellite telemetry, SPEAR-BERT reached 0.91 accuracy with strong AUROC (0.90) and AUPR (0.70) scores
- SPEAR demonstrated efficient memory usage, with soft prompts requiring only 0.06-0.16 MB versus full model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Driven Soft Prompt Calibration
Learnable soft prompt embeddings adapt frozen LLMs to time series anomaly detection without modifying pretrained weights. Soft prompts are updated via binary cross-entropy loss while the LLM remains frozen, modulating attention patterns across layers.

### Mechanism 2: Discretization-Enabled Token Alignment
Quantizing continuous time series into discrete bins enables compatibility with token-based LLM architectures. Min-max scaling to [0,1] followed by binning transforms continuous signals into the discrete token vocabulary space LLMs expect.

### Mechanism 3: Encoder-Only Architecture Advantage for Variable-Length Context
BERT-style encoder models outperform autoregressive models when adapted via soft prompts for time series classification. Bidirectional context representations created through masked language modeling are more receptive to soft prompt guidance.

## Foundational Learning

- **Concept: Soft Prompt Tuning**
  - Why needed: Core technique enabling parameter-efficient adaptation without modifying all weights
  - Quick check: Can you explain why gradients flow through soft prompts but not the frozen LLM backbone?

- **Concept: Quantization Binning Strategies**
  - Why needed: Converts continuous time series to discrete tokens compatible with LLMs
  - Quick check: What happens to anomaly detection when a critical threshold falls within a single bin?

- **Concept: Class Imbalance Metrics (AUROC/AUPR)**
  - Why needed: NASA and NAB datasets are highly imbalanced; accuracy alone is misleading
  - Quick check: Why does AUPR matter more than accuracy when anomaly prevalence is <5%?

## Architecture Onboarding

- **Component map:**
  Time Series → [Scaling + Quantization] → [Tokenizer] → [Token Embeddings E]
                                                                     ↓
  Soft Prompts P (trainable) ─────────────────────────→ [Concatenation] → [Frozen LLM] → [Classification Head] → Binary Output

- **Critical path:**
  1. Hyperparameter selection (prompt size m=20, bins N, epochs=40)
  2. Preprocessing pipeline (T-SMOTE for imbalance, context anomaly labeling)
  3. Soft prompt initialization and training loop
  4. Validation-based early stopping on AUROC/AUPR

- **Design tradeoffs:**
  - Prompt size: 10→20 tokens improves accuracy (0.89→0.93 on MIMIC-IV); 30 tokens shows degradation (0.915)
  - Base model: BERT-110M outperforms Gemma-2B despite 18× fewer parameters
  - Quantization bins: Paper doesn't specify N values per dataset—requires empirical tuning

- **Failure signatures:**
  - Overprediction mode: Zero-shot models show recall=1.0, precision<0.1 on imbalanced datasets
  - Underfitting: Training loss plateaus but AUROC/AUPR still improving after 10 epochs
  - Architecture mismatch: SPEAR-Gemma consistently underperforms SPEAR-BERT

- **First 3 experiments:**
  1. Replicate zero-shot vs. SPEAR-BERT comparison on MIMIC-IV subset to verify 0.90+ F1 reproduction
  2. Test N∈{50, 100, 200, 500} on NASA dataset, measuring AUROC/AUPR to identify information loss threshold
  3. Train soft prompts on MIMIC-IV, evaluate frozen on NASA without further training—tests domain-general temporal anomaly patterns

## Open Questions the Paper Calls Out

- **Open Question 1:** Do soft prompts remain effective for time series anomaly detection when applied to significantly larger state-of-the-art foundation models (e.g., Llama 3, DeepSeek) or domain-specific LLMs?
  - Basis: Authors state they plan to evaluate larger models like Llama 3/4 and DeepSeek as future work
  - Unresolved: Study restricted to BERT-Base and Gemma-2B due to hardware constraints

- **Open Question 2:** To what extent does the use of statistical heuristics for labeling "context anomalies" in MIMIC-IV impact model generalization compared to clinically validated ground truth?
  - Basis: Authors note labels are "proxy anomalies rather than medically validated events"
  - Unresolved: Current labels rely on simple algorithms that may capture mathematical outliers that are clinically irrelevant

- **Open Question 3:** Is the performance degradation observed in SPEAR-Gemma (compared to SPEAR-BERT) a fundamental limitation of soft prompts on larger architectures, or is it specific to the Gemma-2B architecture?
  - Basis: Paper observes "SPEAR-BERT consistently outperforms SPEAR-Gemma" but cause remains unclear
  - Unresolved: Counter-intuitive that larger model performs worse when both use same adaptation technique

## Limitations

- Optimal number of quantization bins (N) per dataset is unspecified, creating a critical hyperparameter requiring empirical tuning
- Learning rate and batch size for SPEAR training are not explicitly stated, though batch size 32 can be inferred from LSTM comparisons
- Zero-shot prompt templates used for baseline comparisons with GPT-4 are not documented

## Confidence

- **High Confidence:** The mechanism of soft prompt tuning for parameter-efficient adaptation
- **Medium Confidence:** The quantization approach for token alignment
- **Low Confidence:** Claims about encoder-only architecture advantages without controlled experiments

## Next Checks

1. **Quantization Sensitivity Analysis:** Systematically test N ∈ {50, 100, 200, 500} on NASA dataset to identify the point where information loss begins degrading AUROC/AUPR performance

2. **Cross-Domain Transferability:** Train soft prompts on MIMIC-IV, then evaluate frozen on NASA without further training to test whether prompts capture domain-general temporal anomaly patterns versus dataset-specific overfitting

3. **Architecture Ablation Study:** Compare SPEAR-BERT performance against a modified SPEAR-Gemma with identical prompt sizes and training epochs to isolate whether the performance gap stems from architecture or insufficient prompt adaptation for larger models