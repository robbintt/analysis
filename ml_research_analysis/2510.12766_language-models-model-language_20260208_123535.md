---
ver: rpa2
title: Language Models Model Language
arxiv_id: '2510.12766'
source_url: https://arxiv.org/abs/2510.12766
tags:
- language
- nczak
- llms
- linguistic
- witold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper reframes language models through Witold Ma\xB4nczak's\
  \ empiricist linguistics, which defines language as the totality of all that is\
  \ written and spoken, governed primarily by frequency of use. Ma\xB4nczak argued\
  \ that language is best understood through statistical analysis of actual texts\
  \ rather than abstract theoretical constructs."
---

# Language Models Model Language

## Quick Facts
- arXiv ID: 2510.12766
- Source URL: https://arxiv.org/abs/2510.12766
- Authors: Łukasz Borchmann
- Reference count: 11
- Key outcome: Language models succeed by modeling frequency-based patterns in actual language use, validating Mańczak's empiricist linguistics.

## Executive Summary
This paper reframes language models through Witold Mańczak's empiricist linguistics, which defines language as the totality of all that is written and spoken, governed primarily by frequency of use. Mańczak argued that language is best understood through statistical analysis of actual texts rather than abstract theoretical constructs. The paper challenges critics who claim LLMs lack "deep structure" or "grounded meaning," arguing instead that frequency-based pattern recognition—the core mechanism of LLMs—is the primary organizing force of language itself. Empirical evidence shows that LLMs improve performance with more training data because they better capture the frequency structure of language. The success of LLMs validates Mańczak's view that language competence emerges from recognizing and applying high-frequency patterns, not from innate grammatical rules.

## Method Summary
The paper presents a theoretical argument synthesizing existing evidence rather than conducting original experiments. It validates the central claim that LLM success supports Mańczak's empiricist linguistics through citations of scaling law studies (Kaplan et al. 2020; Hoffmann et al. 2022) and frequency effects in human language processing (Saffran et al. 1996; Ellis 2002). No specific experiments, datasets, model architectures, or hyperparameters are provided. The paper is a position/theoretical argument, not an experimental study, and proposes three conceptual validation approaches: reproducing scaling law experiments, testing frequency retention in embeddings, and comparing grammar rule systems against actual text generation capability.

## Key Results
- LLM performance scales smoothly with training data quantity because larger corpora better capture language's frequency structure
- Modern LLMs succeed where n-gram models failed due to operating on learned vector representations that enable analogical reasoning
- Language competence emerges from recognizing and applying high-frequency patterns rather than innate grammatical rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance improves with more training data because larger corpora better capture the frequency structure of language.
- Mechanism: Pretraining minimizes expected next-token surprisal (cross-entropy), pushing the model's conditional predictions toward matching empirical next-token frequencies. Larger training sets sharpen frequency estimation, especially in the long tail.
- Core assumption: Language is fundamentally organized by frequency of use; high-frequency patterns constitute "rules" while low-frequency patterns are "exceptions" on the same continuum.
- Evidence anchors:
  - [abstract] "Empirical evidence shows that LLMs improve performance with more training data because they better capture the frequency structure of language."
  - [section 4] "LLM performance increases smoothly with the amount of pretraining data (Kaplan et al., 2020; Hoffmann et al., 2022)... Estimation of the language's frequency structure improves and sharpens with a larger training set, especially in the long tail."
- Break condition: If language actually requires innate grammatical structures or external grounding that cannot be derived from textual frequency patterns alone, the mechanism is incomplete.

### Mechanism 2
- Claim: The key architectural leap enabling genuine linguistic generalization is operating on sequences of learned vector representations rather than surface-level token counts.
- Mechanism: Transformers use high-dimensional embeddings that are dynamically adjusted based on context. When presented with novel problems, the model uses its internalized map of relationships to find and apply the closest learned analogy.
- Core assumption: Analogy—recognizing and applying relational patterns—is the essence of linguistic competence, not rule-following.
- Evidence anchors:
  - [abstract] "Frequency-based pattern recognition—the core mechanism of LLMs—is the primary organizing force of language itself."
  - [Table 2] Compares n-gram (no analogical capability), CBOW (external analogical capability), and LLMs (inherent analogical capability as core mechanism).
- Break condition: If analogical reasoning cannot account for systematicity in language (e.g., novel sentence comprehension), additional architectural mechanisms are needed.

### Mechanism 3
- Claim: Meaning in LLMs is primarily relational—derived from webs of connections between terms—rather than requiring external grounding.
- Mechanism: Most words can be defined using other words in the language, with some primitives taken as axiomatic (self-evident). An LLM using "justice" correctly requires only mastery of the multidimensional web connecting it to "fairness," "law," "equality," etc.
- Core assumption: The demand for "grounded meaning" applies a double standard not required of other formal systems (calculators, theorem provers).
- Evidence anchors:
  - [section 2.4] "Mańczak argued that attempting to describe language without external reference... was a descent into a 'nebulous darkness.'"
  - [section 2.4] "The relevant test for LLM quality is not whether the model has access to an outside world, but whether it has mastered the internal, relational logic of the textual world it was given."
- Break condition: If certain linguistic tasks (e.g., reference to physical objects, temporal-indexical reasoning) demonstrably require non-textual grounding, the mechanism is incomplete for those capabilities.

## Foundational Learning

- **Frequency effects in cognition**
  - Why needed here: The paper's core thesis rests on frequency of use being a primary organizing principle of language, supported by cognitive science research.
  - Quick check question: Can you explain why high-frequency words are processed faster and learned earlier by children?

- **N-gram vs. embedding representations**
  - Why needed here: Understanding the architectural evolution from atomic symbols to dense vectors is essential for grasping why modern LLMs succeed where earlier models failed.
  - Quick check question: Why can't an n-gram model recognize that "Anna likes cats" and "Lily loves dogs" are analogous sentences?

- **The "synthesis validates analysis" principle**
  - Why needed here: This is Mańczak's criterion for theoretical validity—a theory must enable reconstruction of what it claims to explain, not just describe it.
  - Quick check question: Why does the paper claim generative grammar failed this criterion?

## Architecture Onboarding

- **Component map**: Input tokens → Tokenization → Embedding layer (dense vectors, frequency info retained) → Transformer layers (context-dependent representation updates) → Output layer (next-token probability distribution)

- **Critical path**: Training objective (cross-entropy minimization) → frequency pattern capture → embedding space organization → analogical generalization at inference.

- **Design tradeoffs**:
  - Larger training corpus = better frequency structure estimation, but may include unrepresentative/duplicated texts
  - Higher-dimensional embeddings = richer relational maps, but increased compute
  - Assumption: The paper advocates for "rational selection of texts" based on circulation and influence rather than raw scale alone.

- **Failure signatures**:
  - Confident factual errors (hallucination): LLM models text, not truth; "imperfect not because they fail to model language but because they only model language."
  - Poor performance on low-frequency patterns: Expected given frequency-based organization; "exceptions" are on continuum with "rules."
  - Domain mismatch: If training corpus doesn't represent target domain's frequency structure, performance degrades.

- **First 3 experiments**:
  1. **Frequency analysis probe**: Extract token frequencies from training corpus; compare model's conditional probability estimates against empirical frequencies. Verify that surprisal correlates with frequency (lower-frequency tokens should have higher surprisal).
  2. **Analogy detection test**: Construct sentence pairs with parallel structure (e.g., "X verbs Y" / "A verbs B") where X≠A and Y≠B. Measure whether model treats them as similar via embedding cosine similarity or attention pattern overlap.
  3. **Long-tail scaling check**: Train models on progressively larger corpora; evaluate performance stratified by token/construct frequency. Expect greatest gains in low-frequency regions as corpus grows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "rational selection of texts" based on circulation and influence be operationalized to create training corpora that better represent the "totality of all that is said and written"?
- Basis in paper: [explicit] Section 4 (Limitations), response to the objection "LLMs are trained on unrepresentative corpora."
- Why unresolved: The author proposes "principled, frequency-weighted corpus construction" to address data incompleteness but provides no concrete methodology for weighting or selecting texts.
- What evidence would resolve it: A comparative study showing improved linguistic competence or reduced bias in models trained on rationally selected corpora versus raw web scrapes.

### Open Question 2
- Question: Can linguistic creativity be formally characterized as the recombination of high-frequency templates rather than the generation of statistically rare exceptions?
- Basis in paper: [explicit] Section 4, response to the question "how do you explain creativity?"
- Why unresolved: The paper asserts that creativity is "pattern mastery," but lacks a formal mechanism explaining how high-frequency patterns result in genuinely novel or unexpected outputs.
- What evidence would resolve it: A statistical decomposition of "creative" LLM outputs demonstrating that they derive from high-frequency structural templates rather than low-probability token sequences.

### Open Question 3
- Question: Does defining LLMs as direct models of textual statistics (rather than flawed cognitive simulations) yield distinct ethical frameworks for mitigating harm?
- Basis in paper: [inferred] Section 4, response to "Your paper ignores urgent ethical dilemmas."
- Why unresolved: The author argues that defining the technology is a prerequisite for ethical debate, but explicitly defers the development of the resulting ethical guidelines.
- What evidence would resolve it: The derivation of an ethical framework from the Mańczakian perspective that resolves specific safety issues (e.g., hallucination) differently than "grounded meaning" approaches.

## Limitations
- The paper remains largely theoretical without direct empirical validation of its core claims, particularly regarding relational meaning and analogical reasoning mechanisms.
- It doesn't address potential limitations of Mańczak's framework itself, such as whether treating language as purely textual artifacts excludes important aspects of linguistic meaning and evolution.
- No concrete methodology is provided for operationalizing "rational selection of texts" or testing the frequency-structure hypothesis across typologically diverse languages.

## Confidence
- Frequency-structure mechanism: High
- Analogical reasoning mechanism: Medium
- Relational-meaning claim: Low

## Next Checks
1. **Systematicity test**: Design experiments where LLMs must generalize grammatical patterns to novel constructions. If models fail systematic generalization despite frequency-based training, this would challenge the completeness of the frequency-based mechanism.

2. **Cross-linguistic frequency analysis**: Compare how well the frequency-structure hypothesis explains linguistic phenomena across typologically diverse languages. If frequency effects vary significantly across language families, this would suggest additional organizing principles beyond textual frequency.

3. **External grounding evaluation**: Test LLM performance on tasks requiring real-world knowledge (e.g., spatial reasoning, temporal-indexical references). If these tasks consistently underperform relative to frequency-based predictions, this would indicate genuine limitations of purely textual grounding.