---
ver: rpa2
title: 'Concept Probing: Where to Find Human-Defined Concepts (Extended Version)'
arxiv_id: '2507.18681'
source_url: https://arxiv.org/abs/2507.18681
tags:
- concept
- concepts
- representations
- layer
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an information-theoretic method to automatically\
  \ select the most suitable layer in a neural network for probing a given human-defined\
  \ concept. The approach evaluates each layer\u2019s representations based on two\
  \ characteristics: informativeness (mutual information with the concept) and regularity\
  \ (ease of prediction via a simple classifier)."
---

# Concept Probing: Where to Find Human-Defined Concepts (Extended Version)
## Quick Facts
- arXiv ID: 2507.18681
- Source URL: https://arxiv.org/abs/2507.18681
- Reference count: 19
- The paper proposes an information-theoretic method to automatically select the most suitable layer in a neural network for probing a given human-defined concept.

## Executive Summary
This paper introduces an information-theoretic approach to automatically identify the optimal layer in a neural network for concept probing. The method evaluates each layer's representations based on informativeness (mutual information with the concept) and regularity (ease of prediction via a simple classifier). By combining these measures, the approach enables more accurate and efficient concept probing. The technique is validated across six models and four datasets, demonstrating significant improvements over baseline methods.

## Method Summary
The proposed method selects probing layers by combining two information-theoretic metrics: informativeness and regularity. Informativeness measures the mutual information between layer representations and the target concept, while regularity assesses how well a linear classifier can predict the concept from the representations. The method evaluates each layer independently, ranking them based on a weighted combination of these metrics. This approach enables automated selection of the most suitable layer for concept probing without requiring exhaustive experimentation across all layers.

## Key Results
- Proposed method achieves 90.1% average probe accuracy across six models and four datasets
- Outperforms layer-wise average baseline (77.6% accuracy) by 12.5 percentage points
- Approaches oracle performance (91.8% accuracy) with only 1.7 percentage points difference

## Why This Works (Mechanism)
The method works by leveraging information theory to quantify both the presence of concept-relevant information in layer representations and the predictability of that information. By combining mutual information (measuring information content) with classifier-based regularity (measuring prediction ease), the approach captures both what information is available and how accessible it is. This dual consideration enables more reliable layer selection than either metric alone, as it accounts for both information richness and the practical feasibility of extracting that information through simple probes.

## Foundational Learning
- **Mutual Information**: Measures the amount of information one random variable contains about another. Why needed: Quantifies how much concept-relevant information is present in each layer's representations. Quick check: Verify MI values increase monotonically from shallow to deep layers for relevant concepts.
- **Information Regularity**: Assesses how predictable a concept is from layer representations using simple classifiers. Why needed: Ensures selected layers have concepts that are practically extractable, not just theoretically present. Quick check: Compare linear classifier accuracy against more complex models for regularity measurement.
- **Concept Probing**: Technique for analyzing neural network representations by training simple classifiers to detect human-defined concepts. Why needed: Provides interpretable analysis of what models learn at different layers. Quick check: Validate probe accuracy correlates with human judgment of concept presence.

## Architecture Onboarding
- **Component Map**: Data -> Model Layers -> Representation Extraction -> MI/Regularity Computation -> Layer Ranking -> Probe Training
- **Critical Path**: The sequence from representation extraction through metric computation to layer selection determines final probe performance
- **Design Tradeoffs**: Balancing between informativeness (favoring deeper layers with more abstract features) and regularity (favoring shallower layers with cleaner representations)
- **Failure Signatures**: Poor performance occurs when MI estimates are unreliable (small datasets) or when concept is distributed across multiple layers requiring ensemble approaches
- **First Experiments**: 1) Validate MI estimation accuracy on synthetic data with known information content, 2) Test regularity metric sensitivity to classifier complexity choices, 3) Compare layer rankings on a simple vision task with known hierarchical feature structure

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Results may not generalize to all neural network architectures beyond the six tested models
- Method's effectiveness could be sensitive to dataset size and quality, particularly for reliable MI estimation
- The approach assumes concepts can be captured by single-layer probes, potentially missing distributed representations

## Confidence
- High: The core methodology combining informativeness and regularity for layer selection is sound and well-implemented
- Medium: The experimental results showing consistent improvement over baselines are reliable within the tested scope
- Low: Claims about the method's general applicability across all neural network architectures and domains

## Next Checks
1. Test the method on transformer-based language models with diverse linguistic tasks to assess cross-domain applicability
2. Evaluate performance with limited training data to understand sensitivity to dataset size constraints
3. Compare results against alternative layer selection methods using different theoretical foundations (e.g., feature importance or gradient-based approaches)