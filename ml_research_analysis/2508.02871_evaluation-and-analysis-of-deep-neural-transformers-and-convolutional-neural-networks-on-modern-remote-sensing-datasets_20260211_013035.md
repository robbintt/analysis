---
ver: rpa2
title: Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural
  Networks on Modern Remote Sensing Datasets
arxiv_id: '2508.02871'
source_url: https://arxiv.org/abs/2508.02871
tags:
- performance
- transformer
- detection
- dataset
- detr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates transformer-based neural networks against
  convolutional neural networks for object detection in high-resolution electro-optical
  satellite imagery. Eleven detection algorithms, including five transformer-based
  and six convolutional architectures, are trained and tested on three open-source
  overhead imagery datasets of varying sizes and complexities.
---

# Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets

## Quick Facts
- arXiv ID: 2508.02871
- Source URL: https://arxiv.org/abs/2508.02871
- Reference count: 35
- Primary result: Transformer-based models outperform CNNs on overhead imagery datasets, with SWIN and CO-DETR showing highest accuracy

## Executive Summary
This study systematically compares transformer-based neural networks against convolutional neural networks for object detection in high-resolution satellite imagery. Eleven detection algorithms—five transformer-based and six convolutional architectures—are trained and evaluated on three open-source overhead imagery datasets (RarePlanes, DOTA, xView) of varying sizes and complexities. The results demonstrate that transformer models, particularly SWIN and CO-DETR, consistently achieve superior detection accuracy across all datasets, with transformer architectures showing more consistent performance across different datasets compared to CNNs. However, this accuracy advantage comes at the cost of longer training times and lower real-time inference speeds.

## Method Summary
The study trains and evaluates 11 object detection models (6 CNNs and 5 transformers) on three overhead imagery datasets using the MMDetection framework. Models are trained for 200 epochs with CNNs using SGD optimizer and transformers using AdamW. Learning rate decay is applied in the final 20 and 40 epochs. Datasets are preprocessed into 512×512 tiles with 15% overlap using a semi-label-aware chipping algorithm. Pretrained weights from ImageNet-1k or COCO are used for initialization. Performance is evaluated using COCO metrics (AP, AP50, AR, AR50) and Optimal F1 scores.

## Key Results
- Transformer models (particularly SWIN and CO-DETR) outperform CNNs on all three datasets (RarePlanes, DOTA, xView)
- Transformer performance is more consistent across datasets with less average rank variation compared to CNNs
- Larger datasets (xView) show greater benefit from transformer architectures
- CO-DETR achieves highest AR50 scores, indicating superior recall in dense scenes
- Transformers require longer training times and have lower real-time FPS compared to CNNs

## Why This Works (Mechanism)

### Mechanism 1
Transformers lack CNNs' inductive spatial biases toward locality, allowing them to model global context without fixed priors. This flexibility enables better adaptation to diverse scene structures in overhead imagery where object orientation and context vary significantly. The superior consistency across datasets stems from the attention mechanism's ability to weigh relationships between all patches globally rather than relying on fixed spatial assumptions.

### Mechanism 2
SWIN's hierarchical attention with shifting windows balances global context with localization precision, enabling superior handling of multi-scale objects common in remote sensing. The shifting window approach reduces computational complexity from quadratic to linear while maintaining hierarchical feature extraction, leading to enhanced multi-scale feature extraction that benefits scale-variant objects like aircraft versus airports.

### Mechanism 3
CO-DETR's collaborative hybrid assignment strategy improves training efficiency and recall in dense scenes by mitigating the sparse positive sample problem. The one-to-many label assignment forces the model to learn features from more positive samples during training, directly increasing Recall and F1 scores in complex datasets like xView.

## Foundational Learning

- **Inductive Bias vs. Data Hunger**: CNNs have strong spatial priors that make them faster to train but less flexible; transformers have weak priors requiring more data but offering greater adaptability. Quick check: Would you choose a pure ViT or ResNet for 5,000 satellite chips? Why?

- **Transfer Learning in Remote Sensing**: Pretrained weights significantly impact performance, with overhead imagery's distinct characteristics from ground photos making the source of pretraining crucial. Quick check: Why might ImageNet weights be suboptimal for xView, and how does the paper address this?

- **Evaluation Metrics (AP vs. F1 vs. AR)**: Different metrics capture different aspects of performance—AP measures precision-recall trade-off, AR measures recall at fixed IoU, and Optimal F1 finds the best balance. Quick check: What does a high AR50 score specifically indicate about a model's error profile?

## Architecture Onboarding

- **Component map**: Backbone (ResNet/ConvNeXt for CNN; ViT/SWIN for Transformers) -> Neck (FPN) -> Head (RPN + ROI for Faster R-CNN; MLP for DETR)

- **Critical path**: 
  1. Preprocess large satellite scenes into 512×512 tiles with overlap
  2. Load pretrained weights (COCO > ImageNet for detection)
  3. Select optimizer (AdamW for Transformers, SGD for CNNs)
  4. Train for 200 epochs with LR decay in final epochs

- **Design tradeoffs**: 
  - Accuracy vs. Speed: SWIN/CO-DETR offer peak accuracy (F1 ~70-80%) but low FPS (<16); YOLOv3/SSD offer speed (FPS 30-45) but lower accuracy (F1 ~60%)
  - Dataset Size vs. Architecture: Use CNNs/ConvNeXt for smaller datasets (<50k samples); use Transformers/SWIN for larger datasets (>250k samples)

- **Failure signatures**: 
  - ViT on Small Data: Expect low F1 due to inability to learn spatial relations
  - DETR Convergence: May show high instability on dense scenes
  - Compute Bottleneck: CO-DETR may OOM on standard GPUs with batch sizes >2

- **First 3 experiments**: 
  1. Train YOLOX and SWIN-T on 10% of target dataset to measure "data hunger" gap
  2. Benchmark inference FPS and training time per epoch on specific hardware
  3. Compare training from scratch vs. using provided pretrained weights on a single class

## Open Questions the Paper Calls Out

- **Question**: Do the transformer weights pre-trained on overhead imagery significantly improve convergence and accuracy for downstream remote sensing tasks compared to standard ImageNet/COCO weights?
- **Basis**: Authors generated weights to improve future overhead CV applications but did not validate their transfer utility on new tasks
- **Why unresolved**: Study validates performance on training datasets but not generalizability to distinct downstream tasks
- **What evidence**: Experiments initializing training on separate remote sensing dataset using authors' released weights versus standard weights

- **Question**: What specific characteristics of the DOTA dataset cause FCOS to underperform drastically compared to its performance on RarePlanes and xView?
- **Basis**: FCOS "heavily struggles" on DOTA (40.34% F1), dropping significantly from RarePlanes performance (68.23%)
- **Why unresolved**: Authors acknowledge poor performance but don't isolate whether failure is due to object scale, orientation, or anchor-free mechanism's interaction with DOTA's features
- **What evidence**: Ablation study on FCOS focusing on object scale and rotation distributions within DOTA

- **Question**: How can transformer architectures be modified to maintain their superior detection accuracy while achieving real-time frame rates necessary for operational remote sensing?
- **Basis**: Transformers require longer training times and suffer from lower real-time FPS (5-15 FPS) compared to CNNs like YOLOv3 (45 FPS)
- **Why unresolved**: Paper establishes trade-off but doesn't investigate compression techniques or lightweight transformer variants
- **What evidence**: Comparative benchmarks of lightweight transformer models or optimized inference engines applied to same datasets

## Limitations
- Evaluation limited to three public datasets, which may not represent full diversity of real-world applications
- Semi-label-aware chipping algorithm critical for preprocessing is not fully detailed, affecting reproducibility
- Performance metrics heavily favor Optimal F1, potentially missing operational needs like inference speed or energy efficiency
- Reported FPS values for transformers are significantly lower than CNNs, raising practical deployment feasibility questions

## Confidence

- **High confidence**: Transformer superiority in detection accuracy across all datasets; consistency advantage of transformers across varied remote sensing datasets
- **Medium confidence**: Data-hunger hypothesis for transformers; multi-scale feature handling mechanism
- **Low confidence**: Exact contribution of CO-DETR's label assignment strategy versus SWIN-L backbone capacity; generalization to non-public or domain-specific remote sensing datasets

## Next Checks

1. Replicate top-3 models (SWIN, CO-DETR, YOLOX) on a held-out private remote sensing dataset with similar object density and scale variation to verify consistency claims
2. Conduct ablation studies comparing SWIN with and without shifting window attention on xView to isolate multi-scale feature extraction contribution
3. Benchmark same models on edge hardware (e.g., NVIDIA Jetson) to quantify real-world FPS and power consumption trade-offs between accuracy and deployment feasibility