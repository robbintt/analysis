---
ver: rpa2
title: 'PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting
  their Performances in Benchmarks'
arxiv_id: '2404.04671'
source_url: https://arxiv.org/abs/2404.04671
tags:
- genes
- matrix
- similarity
- figure
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhyloLM adapts phylogenetic algorithms to study relationships among
  Large Language Models (LLMs) by treating generated tokens as alleles and contexts
  as genes. It computes phylogenetic distances based on similarity of LLM outputs,
  constructs dendrograms, and predicts benchmark performance via logistic regression.
---

# PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks

## Quick Facts
- **arXiv ID:** 2404.04671
- **Source URL:** https://arxiv.org/abs/2404.04671
- **Reference count:** 40
- **Primary result:** Recovers known family relationships and predicts benchmark scores with ~50% variance explained (Pearson r ≈ 0.68)

## Executive Summary
PhyloLM adapts phylogenetic algorithms to study relationships among Large Language Models (LLMs) by treating generated tokens as alleles and contexts as genes. It computes phylogenetic distances based on similarity of LLM outputs, constructs dendrograms, and predicts benchmark performance via logistic regression. On 111 open-source and 45 closed models, it recovers known family relationships and predicts benchmark scores with ~50% variance explained (Pearson r ≈ 0.68 across benchmarks). The method is ~10× less costly than standard benchmarks and works even without training transparency.

## Method Summary
PhyloLM treats LLM outputs as genetic sequences to infer evolutionary relationships. It samples contexts ("genes") from recent benchmarks, queries each model N times per gene to generate tokens ("alleles"), computes pairwise similarity matrices using Nei's genetic distance formula, optionally constructs phylogenetic trees via Neighbor Joining, and predicts benchmark performance using logistic regression on ICA-reduced distance vectors. The method requires 128 genes and 32 probes per gene per model, generating ~4096 queries per model, and achieves ~50% variance explanation in benchmark scores.

## Key Results
- Perfect reconstruction of Mistral family tree (ground truth known)
- ~50% variance explained in benchmark prediction (Pearson r ≈ 0.68)
- Method is ~10× less costly than standard benchmarking approaches
- Works across diverse gene sets including math, Chinese poetry, and Wikipedia contexts

## Why This Works (Mechanism)

### Mechanism 1: Output Distribution Similarity as Proxy for Lineage
Models that share training data or fine-tuning ancestors produce statistically similar token distributions across shared contexts. Nei's genetic distance formula computes similarity by comparing token probability distributions across contexts. Models inherit statistical patterns from ancestors; distance correlates with functional similarity. Output distributions capture training lineage; shared training data/weights → similar P(token|context). If models converge to identical outputs without shared ancestry (e.g., different architectures trained on same data), distance metrics may indicate false relatedness.

### Mechanism 2: Context Diversity Enables Discriminative Power
Genes must exhibit moderate variance across models to separate families while preserving lineage signal. Contexts sampled from recent benchmarks avoid contamination and capture reasoning/coding evolution. Truncation (20-100 chars) balances informativeness vs. cost. Domain-specific contexts reveal evolutionary changes in model capabilities; contamination-prone contexts yield low variance. If test benchmarks become contaminated in training data, variance collapses and discriminative power degrades.

### Mechanism 3: Dimensionality Reduction Preserves Predictive Signal
ICA reduction to 15 components from high-dimensional similarity matrices preserves sufficient information for benchmark prediction. Similarity matrices reduced via ICA to 15 independent components; logistic regression maps these to benchmark scores via leave-one-family-out cross-validation. Benchmark performance correlates with phylogenetic position; family-level generalization requires components capturing cross-family variance. If new families diverge substantially from training distribution, extrapolation may fail.

## Foundational Learning

- **Concept: Nei's genetic distance**
  - **Why needed here:** Core similarity metric; interprets LLMs as probability distributions over token completions.
  - **Quick check question:** Given two models with identical top-1 tokens but different full distributions, would Nei distance capture this difference?

- **Concept: Neighbor Joining (NJ) algorithm**
  - **Why needed here:** Converts distance matrices to dendrograms; produces unrooted trees suitable for LLM genealogy.
  - **Quick check question:** Why might rooted trees be inappropriate for LLM phylogeny reconstruction?

- **Concept: Leave-one-family-out cross-validation**
  - **Why needed here:** Evaluates generalization to unseen model families; prevents overfitting to family-specific correlations.
  - **Quick check question:** If benchmark scores correlate within families, why is standard leave-one-out insufficient?

## Architecture Onboarding

- **Component map:** Gene sampler → truncates benchmark texts (20-100 chars) → Probing loop → queries each LLM N times per gene, extracts first 4 chars → Similarity matrix → Nei distance computation across all model pairs → Dimensionality reduction → ICA to 15 components → Predictor → logistic regression with sigmoid output scaling

- **Critical path:**
  1. Select gene set (G=128) from uncontaminated benchmarks
  2. For each model, probe N=32 times per gene, record 4-char completions
  3. Compute P(a|g) frequency distributions
  4. Apply Nei formula for pairwise similarity
  5. Optional: NJ tree construction for visualization
  6. Train predictor on similarity matrix with leave-one-family-out

- **Design tradeoffs:**
  - G vs. N: Paper shows G controls variance; N controls identity-matrix bias. Optimal G=128, N=32 balances cost (~4096 queries/model).
  - Gene length: Short genes (5 tokens) less informative; long genes (>200 chars) add cost without benefit. 20-100 chars recommended.
  - Completion comparison: 4 chars chosen empirically to maximize matrix contrast vs. tokenization alignment issues.

- **Failure signatures:**
  - Identity matrix output → N too low; increase probes
  - No family clustering → genes too contaminated or uniform; try different benchmark source
  - Chat/completion mixing artifacts → API adds prompt tokens, breaking gene correspondence; separate analysis
  - Quantization distance → larger models show smaller quantization effects; expect ~0.1-0.2 similarity drop for 4-bit

- **First 3 experiments:**
  1. Replicate Mistral family tree reconstruction (5 models, ground truth known) using math genes to validate pipeline.
  2. Test sensitivity: vary G={64,128,256} with fixed N=32 on a subset of 10 models; measure similarity matrix variance.
  3. Benchmark prediction ablation: compare ICA(15) vs. PCA(15) vs. raw similarity features on held-out family; report per-benchmark r values.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does varying the sampling temperature during allele generation affect the stability of the phylogenetic distance metric?
- **Basis in paper:** [explicit] The authors state in the Discussion, "Our study did not explore the effect of temperature."
- **Why unresolved:** Temperature controls output diversity; different settings might alter the "genetic" distribution estimated for each model, potentially introducing noise or artificial distance.
- **What evidence would resolve it:** A systematic sweep of temperatures (e.g., 0.0 to 1.5) quantifying the variance in resulting distance matrices and dendrogram topologies.

### Open Question 2
- **Question:** Can phylogenetic algorithms be modified to correctly place "living" ancestor models at internal tree nodes rather than forcing them to the leaves?
- **Basis in paper:** [inferred] The paper notes the "inherent limitations" of applying algorithms designed for biology (where ancestors are extinct) to LLMs, where "common ancestors are present among the studied models."
- **Why unresolved:** Standard algorithms like Neighbor-Joining place all input taxa at the leaves, misrepresenting the structural reality of finetuning chains where the parent model is included in the analysis.
- **What evidence would resolve it:** An algorithm capable of identifying known base models (e.g., Llama-2) as internal nodes in the tree structure relative to their finetuned derivatives.

### Open Question 3
- **Question:** Does integrating multiple distinct sets of "genes" (e.g., combining math and code contexts) significantly improve the prediction accuracy of benchmark scores?
- **Basis in paper:** [explicit] The authors note that while 50% variance is explained, "it remains room for improvement (a possible venue being using multiple sets of genes in the evaluation)."
- **Why unresolved:** The current study evaluates single gene sets (math or code) separately; it is unknown if a composite genetic profile captures a more holistic view of model capabilities.
- **What evidence would resolve it:** Experiments training the regression predictor on concatenated similarity matrices derived from heterogeneous gene sets to measure increases in Pearson correlation against benchmarks.

## Limitations

- The method assumes test benchmark contexts remain uncontaminated in training data, but this assumption becomes increasingly fragile as field evolves.
- Nei distance metric may capture convergent performance optimization rather than true training lineage, especially across different architectures.
- Extrapolation to entirely novel architectures or training paradigms (e.g., quantum-inspired models) remains untested and potentially unreliable.

## Confidence

- **High Confidence:** Phylogenetic reconstruction accuracy for known families and computational efficiency gains are directly measurable and well-supported by the data.
- **Medium Confidence:** Output distribution similarity proxies for training lineage holds for studied models but may not generalize to future architectures or training paradigms.
- **Low Confidence:** Predicting performance of entirely novel architectures or training approaches remains untested; the method may not work for non-transformer architectures.

## Next Checks

1. **Cross-Architecture Validation:** Test PhyloLM on diverse architectures including RNNs, convolutional networks, and hybrid approaches to verify output distribution similarity mechanism works beyond standard LLM families.

2. **Contamination Sensitivity Analysis:** Systematically introduce varying degrees of benchmark contamination into training data and measure degradation in both phylogenetic reconstruction accuracy and benchmark prediction performance.

3. **Temporal Validation:** Apply PhyloLM to historical snapshots of model families (e.g., GPT-2 → GPT-3 → GPT-3.5) to verify phylogenetic distances increase monotonically with architectural changes and training iterations.