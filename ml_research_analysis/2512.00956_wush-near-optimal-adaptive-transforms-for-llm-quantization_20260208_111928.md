---
ver: rpa2
title: 'WUSH: Near-Optimal Adaptive Transforms for LLM Quantization'
arxiv_id: '2512.00956'
source_url: https://arxiv.org/abs/2512.00956
tags:
- quantization
- wush
- transform
- error
- transforms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WUSH introduces a closed-form optimal blockwise transform for LLM
  quantization, combining a Hadamard backbone with a data-aware second-moment component
  to reduce quantization error from outliers. It is near-optimal for both floating-point
  and integer quantizers, provable under mild assumptions, and supports efficient
  GPU implementation.
---

# WUSH: Near-Optimal Adaptive Transforms for LLM Quantization

## Quick Facts
- arXiv ID: 2512.00956
- Source URL: https://arxiv.org/abs/2512.00956
- Reference count: 40
- One-line primary result: WUSH improves 4-bit quantization accuracy on Llama-3.1-8B-Instruct by up to +2.8 average points over Hadamard baselines

## Executive Summary
WUSH introduces a closed-form optimal blockwise transform for LLM quantization that combines a Hadamard backbone with a data-aware second-moment component to reduce quantization error from outliers. The transform is near-optimal for both floating-point and integer quantizers, provable under mild assumptions, and supports efficient GPU implementation. Empirically, WUSH improves 4-bit quantization accuracy on Llama-3.1-8B-Instruct by up to +2.8 average points over Hadamard-based baselines and achieves up to 6.6× throughput over BF16, making MXFP formats competitive with NVFP.

## Method Summary
WUSH constructs a data-aware blockwise transform Twush(i) = H·S^(-1/2)·U^T·W'^T for each weight block, where W' is the Cholesky factor of the block's second-moment matrix, and U,S come from SVD of W'^T·X' (weights times calibration activations). The method integrates with GPTQ by processing blocks in reverse order, applying intra-block GPTQ on transformed weights with transformed Hessian, and propagating inter-block errors via Y updates. Inference uses a fused CUTLASS kernel that applies Twush(i), quantizes, and performs low-precision MatMul. Block size is 32 for MXFP4, 16 for NVFP4.

## Key Results
- Improves 4-bit quantization accuracy on Llama-3.1-8B-Instruct by up to +2.8 average points over Hadamard baseline
- Achieves up to 6.6× per-layer throughput over BF16 via FP4 MatMul
- Makes MXFP formats competitive with NVFP4 while maintaining near-optimal error bounds

## Why This Works (Mechanism)

### Mechanism 1
A data-aware, non-orthogonal blockwise transform can reduce weight-activation quantization error more effectively than fixed, data-agnostic transforms like the Hadamard rotation. The transform Twush(i) = H S^(-1/2)(i) U^⊤(i) W'^⊤(i) combines a fixed Hadamard matrix (H) with a data-dependent scaling component derived from the singular values (S) of the interaction between weights and calibration activations. This component rescales dimensions based on their importance to the output, reshaping the error distribution to align with the data's intrinsic geometry rather than spreading error uniformly.

### Mechanism 2
The Hadamard matrix's role as a "backbone" is justified as the optimal data-agnostic component within the broader data-aware transform. Theoretical analysis identifies U' = H as the optimal orthogonal mixing matrix for the transformed space because it evenly distributes signal energy across dimensions. This "spreading" property is beneficial for the subsequent data-dependent scaling step, which can then act on a more uniform distribution of variance.

### Mechanism 3
The primary efficiency gain comes from enabling 4-bit matrix multiplications, not from the transform computation itself. Although WUSH uses a distinct, data-aware transform per block (increasing computational complexity compared to a global Hadamard), the custom GPU kernel is designed to minimize this overhead. The kernel fuses the per-block transform with the quantization step and leverages cache locality so that the memory access cost for loading per-block transforms is largely hidden.

## Foundational Learning

- **Concept: Block Quantization (RTN with AbsMax Scaling)** - Why needed: WUSH is derived specifically for this quantization paradigm. Understanding that values are grouped, and the scale for each group is set by its maximum absolute value, is essential to grasp why outlier values are so detrimental and how the transform aims to mitigate them. Quick check: For a vector `[1.0, 0.1, 0.01]`, what is the AbsMax scale, and why does the `1.0` value reduce the precision for the `0.01` value after quantization?

- **Concept: Cholesky Decomposition & Singular Value Decomposition (SVD)** - Why needed: These are the core linear algebra operations used to construct the WUSH transform. Cholesky provides a square-root of the second-moment matrix, and SVD extracts the principal directions (U, V) and scaling (S) of the weight-activation interaction, which directly parameterize Twush. Quick check: Given a positive semi-definite matrix M, what does M = W'W'^⊤ (Cholesky) tell you about W'? For a matrix A, what do the singular values in the diagonal matrix S from A = USV^⊤ represent geometrically?

- **Concept: Hadamard Transform** - Why needed: It is the fixed, data-agnostic backbone of WUSH. One must understand it as an orthogonal matrix that performs a specific "spreading" or "mixing" of input dimensions, which is a key step before the data-aware scaling is applied. Quick check: How does multiplying a vector by a normalized Hadamard matrix affect the sum of its squared elements? What effect does this have on the distribution of energy across vector components?

## Architecture Onboarding

- **Component Map**: Calibration Phase (Offline) -> Transform Construction -> GPTQ Integration -> Inference Kernel (Online)
- **Critical Path**: The correctness of the entire system hinges on the fused inference kernel. If the kernel fails to correctly apply the per-block transform Twush(i) before quantization, or if it introduces numerical errors, the theoretical error minimization guarantees are void.
- **Design Tradeoffs**:
  - Accuracy vs. Calibration Cost: Using GPTQ with WUSH increases calibration time and complexity compared to RTN but improves accuracy
  - Throughput vs. Memory: Storing per-block transforms (G × G × C) increases model memory footprint compared to a single global Hadamard, but the paper shows this overhead is small and runtime is dominated by other factors
  - Generality vs. Specificity: The transform is provably near-optimal for AbsMax-scaled RTN quantizers. Its benefits may not transfer directly to other quantization schemes
- **Failure Signatures**:
  - Accuracy Collapse: If calibration data is insufficient or non-representative, the data-aware component will be misestimated, leading to poor transforms
  - Kernel Performance Regression: If the fused kernel is not optimized for cache reuse, throughput may drop below the Hadamard baseline
  - Format Mismatch: Applying a transform optimized for MXFP4 to pure INT4 or NVFP4 may yield sub-optimal results
- **First 3 Experiments**:
  1. End-to-End Validation: Reproduce the main result on Llama-3.1-8B-Instruct with MXFP4 RTN to verify the claimed +2.8 average point improvement over the Hadamard baseline
  2. Ablation on Transform Components: Run experiments with (a) WUSH without Hadamard (WUS), (b) Hadamard-only, and (c) WUSH. Compare layerwise losses and final accuracy to isolate the contribution of each component
  3. Kernel Profiling: Measure the per-layer throughput of the fused "WUSH + Quant" kernel against the "H + Quant" baseline on target hardware. Verify the <1.3% average throughput difference claimed in the paper

## Open Questions the Paper Calls Out

- Can the WUSH transform be approximated using a diagonal matrix to reduce inference-time computational overhead while maintaining accuracy? The current optimal construction requires a full matrix multiplication per block, which is memory-bound.

- How sensitive is the closed-form WUSH transform to the choice and volume of calibration data? The transform is derived from the Cholesky decomposition of second-moment statistics obtained from calibration activations, but the paper does not analyze the stability of these statistics with small sample sizes.

- Can the WUSH transform be extended to quantize KV caches in addition to linear layer weights and activations? The paper focuses on linear layers and pre-quantized weights, but does not explicitly address the quantization of KV caches, which is a critical bottleneck in LLM inference.

## Limitations

- Scope of optimality claims: Theoretical guarantees assume Gaussian/Laplacian distributions for INT quantizers and bounded tails for NVFP, which may not hold universally across all model layers or datasets.

- Kernel implementation dependency: The claimed throughput advantages rely heavily on the fused inference kernel implementation. The paper asserts the per-block transform overhead is negligible due to caching, but this may not generalize to different architectures.

- Calibration sensitivity: The data-aware component depends on representative calibration data. The paper uses 32 samples from FineWeb-Edu but doesn't explore sensitivity to calibration set size or distribution mismatch.

## Confidence

**High Confidence**: The mathematical derivation of the WUSH transform formula is rigorous and the blockwise quantization framework is well-established.

**Medium Confidence**: The empirical results showing 2.8 average point improvements and 6.6× throughput gains are compelling but rely on specific implementation details that may be difficult to reproduce exactly.

**Low Confidence**: The claim that WUSH "outperforms all existing transforms" is not directly supported by head-to-head comparisons against all relevant baselines on the same models/tasks.

## Next Checks

1. **Ablation study verification**: Replicate the ablation experiments comparing WUSH vs. Hadamard-only vs. WUSH without Hadamard (WUS) on Llama-3.1-8B-Instruct with MXFP4 to isolate whether the data-aware scaling component provides the claimed benefits.

2. **Distribution sensitivity analysis**: Test WUSH on weight distributions that deviate from the assumed Gaussian/Laplacian models (e.g., highly skewed, multimodal, or heavy-tailed distributions) to reveal whether theoretical assumptions break down in practice.

3. **Kernel overhead measurement**: Profile the fused "WUSH + Quant" kernel to directly measure the transform computation time versus quantization time across different block sizes and model layers to validate whether the transform overhead is truly negligible as claimed.