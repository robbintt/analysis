---
ver: rpa2
title: Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning
arxiv_id: '2503.18063'
source_url: https://arxiv.org/abs/2503.18063
tags:
- uni00000048
- uni00000015
- task
- uni00000056
- uni00000047
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic Task Vector Grouping (DTVG) addresses the challenge of
  negative transfer in multi-task prompt tuning by dynamically selecting and grouping
  source tasks that are most relevant to each target task. Instead of using all source
  tasks or a single most-similar task, DTVG employs task prompt vectors to measure
  task similarity and group source tasks based on target similarity and knowledge
  consistency.
---

# Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning

## Quick Facts
- **arXiv ID**: 2503.18063
- **Source URL**: https://arxiv.org/abs/2503.18063
- **Reference count**: 40
- **Primary result**: Achieves SOTA performance on 26 NLP datasets by dynamically grouping source tasks based on target similarity and knowledge consistency.

## Executive Summary
Dynamic Task Vector Grouping (DTVG) addresses negative transfer in multi-task prompt tuning by dynamically selecting and grouping source tasks most relevant to each target task. The method uses task prompt vectors—the difference between tuned and initial prompts—to measure task similarity and group sources based on target similarity and knowledge consistency. This approach is updated iteratively during target task fine-tuning to adapt to changing task relationships. Experiments on 26 NLP datasets demonstrate DTVG achieves state-of-the-art performance while maintaining parameter efficiency.

## Method Summary
DTVG operates in two stages: first, it trains soft prompts independently for all source tasks to extract Task Prompt Vectors (TPVs) representing the change from initialization. Second, during target task fine-tuning, it dynamically groups source tasks at each training step based on their similarity to the current target vector and knowledge consistency among source tasks. The method merges these vectors to initialize the target prompt, optimizing only the target prompt and scaling terms. This iterative approach adapts to shifting task relationships during training while avoiding negative transfer through the knowledge consistency filter.

## Key Results
- Achieves state-of-the-art performance across 26 NLP datasets from GLUE, SuperGLUE, MRQA, and other benchmarks
- Significantly outperforms existing methods including SPoT, particularly in low-resource settings
- Maintains parameter efficiency by tuning only soft prompts rather than full model parameters
- Dynamic grouping improves performance over static approaches, with iterative updates adapting to changing task relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Task Prompt Vectors (TPV)—the difference between tuned and initial prompts—measure task similarity more reliably than raw soft prompts.
- **Mechanism**: By calculating $T = P^* - P_{init}$, DTVG isolates optimization direction in weight space rather than absolute prompt position. The dot product of these vectors serves as the similarity metric.
- **Core assumption**: Tasks with aligned optimization directions in prompt weight space exhibit positive transfer regardless of absolute prompt values.
- **Evidence anchors**: Abstract mentions measuring task similarity with task vectors instead of soft prompts; Section 3.2 explains TPV represents parameter change and optimization direction alignment.

### Mechanism 2
- **Claim**: Grouping source tasks based on internal "Knowledge Consistency" (KC) reduces negative transfer better than relying solely on target similarity.
- **Mechanism**: KC is defined as average pairwise similarity among candidate source tasks. By maximizing Target Similarity (TS) + KC, the method filters out conflicting source tasks that would cause gradient interference.
- **Core assumption**: Conflicting knowledge in source tasks degrades target performance more than missing a relevant but conflicting source task.
- **Evidence anchors**: Abstract mentions grouping based on knowledge consistency; Section 4.3 ablation shows KC improves SuperGLUE performance from 74.8 to 75.1 even without Target Similarity.

### Mechanism 3
- **Claim**: Relevance of source tasks to target tasks shifts during fine-tuning, necessitating dynamic, iterative re-grouping.
- **Mechanism**: DTVG re-evaluates source group at each training step as target prompt updates. Early similarity measurements are noisy and become more accurate as training progresses.
- **Core assumption**: Low-resource target tasks prevent sufficient convergence at initialization, making early similarity measurements unreliable.
- **Evidence anchors**: Abstract mentions similarity changes dynamically during fine-tuning; Figure 1 shows RTE's most similar source task shifting from QNLI to MNLI over time.

## Foundational Learning

- **Concept**: **Task Arithmetic**
  - **Why needed here**: DTVG relies on "Task Prompt Vectors," derived from task arithmetic principles (subtracting initialized weights from fine-tuned weights to isolate task-specific knowledge).
  - **Quick check question**: How does normalizing or scaling a task vector typically affect the magnitude of its contribution when merged with another vector?

- **Concept**: **Soft Prompt Tuning (PEFT)**
  - **Why needed here**: This is the substrate of the method. Understanding that "soft prompts" are continuous vectors prepended to input is necessary to grasp how they can be treated as vectors for arithmetic operations.
  - **Quick check question**: Why is the initialization strategy ($P_{init}$) critical for the comparability of task vectors across different runs?

- **Concept**: **Negative Transfer**
  - **Why needed here**: The primary problem DTVG solves is negative transfer (source tasks hurting target performance). Understanding that "more data isn't always better" in multi-task learning explains the need for the KC mechanism.
  - **Quick check question**: In what scenario would adding a high-resource source dataset degrade performance on a target dataset?

## Architecture Onboarding

- **Component map**: TPV Learner -> Similarity Engine -> Grouper (Heuristic) -> Merger -> Target Prompt Update
- **Critical path**: The iterative loop in Stage II. Specifically, the Grouper runs every training step. This is the computational bottleneck but also the source of the method's adaptability. The heuristic algorithm stops adding tasks to S' if the Knowledge Consistency score drops.
- **Design tradeoffs**:
  - **Accuracy vs. Speed**: Dynamic grouping every step yields SOTA results but adds computational overhead (approx. 8.7% decrease in training speed noted in Appendix J).
  - **Exploration vs. Stability**: The dynamic nature allows exploration of better source tasks but may introduce noise if the target vector changes too drastically between steps.
- **Failure signatures**:
  - **Unstable Loss**: If the target vector $T_t$ oscillates, the dynamic group $S'$ may switch rapidly (flapping), preventing convergence.
  - **Degradation to Single-Task**: If the KC weight ($\lambda$) is too high, or similarity threshold is too strict, $S'$ may end up empty or containing only one task, negating multi-task benefits.
  - **OOM (Out of Memory)**: While parameter efficient, storing all source TPVs and computing pairwise similarities for large source pools ($n$) in memory must be managed.
- **First 3 experiments**:
  1. **Metric Ablation**: Compare transfer performance using Cosine Similarity (SPoT style) vs. TPV Dot Product (DTVG style) to validate the core similarity claim.
  2. **Dynamic vs. Static**: Run DTVG with dynamic updates vs. a "Fix Group" version (ablation in Figure 5) to quantify the value of the iterative update.
  3. **Lambda Sweep**: Vary the tradeoff parameter $\lambda$ (balancing Target Similarity vs. Knowledge Consistency) to identify the sensitivity of the grouping logic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the length of the soft prompt affect the stability and performance of the Task Prompt Vector (TPV) grouping mechanism?
- **Basis in paper**: [explicit] Section 6 (Limitations) notes experiments fixed prompt length to 100 based on prior work, leaving the impact of varying prompt lengths on DTVG "unknown."
- **Why unresolved**: The relationship between prompt dimensionality and task vector similarity metrics has not been ablated or analyzed.
- **What evidence would resolve it**: Experiments applying DTVG to target tasks using varying prompt lengths (e.g., 10, 20, 50, 200) to measure performance correlation and grouping stability.

### Open Question 2
- **Question**: What is the optimal frequency for performing source task grouping to balance performance gains with computational efficiency?
- **Basis in paper**: [explicit] Section 6 (Limitations) identifies the computational burden of calculating task combinations at every iteration and suggests "exploring the feasible maximum value of K" (grouping every $K$ steps) as future work.
- **Why unresolved**: The current implementation updates the group every single step, achieving high performance but incurring training speed decrease.
- **What evidence would resolve it**: Experiments analyzing the trade-off curve between training throughput and final task accuracy for different grouping intervals (values of $K$).

### Open Question 3
- **Question**: How does the DTVG algorithm scale with the size of the source task pool, and does the heuristic algorithm fail with significantly larger task sets?
- **Basis in paper**: [explicit] Section 6 acknowledges using 6 high-resource tasks and that "the impact of the number of source tasks remains unknown."
- **Why unresolved**: The source task grouping relies on a greedy heuristic to solve an NP-Hard problem. It is unclear if this heuristic degrades or if "knowledge consistency" becomes harder to maintain as the search space expands.
- **What evidence would resolve it**: Evaluation of DTVG performance when the source pool is expanded to include all datasets in the P3 or Natural Instructions benchmarks.

## Limitations
- The exact value of the lambda hyperparameter balancing target similarity and knowledge consistency is not specified in the main text
- The greedy grouping algorithm has no theoretical guarantees for optimality and may degrade with larger source pools
- All experiments use T5-base; scaling behavior with larger models or different architectures is not investigated

## Confidence

**High Confidence**:
- **Task Prompt Vectors as similarity metric**: Well-supported by internal ablation studies and aligns with established task arithmetic principles
- **Negative transfer reduction**: Demonstrably reduces negative transfer, evidenced by consistent improvements across all benchmark suites when KC is included
- **Dynamic grouping improves performance**: Iterative re-evaluation shows clear correlation with improved validation performance

**Medium Confidence**:
- **State-of-the-art performance claims**: While DTVG achieves top results on 26 datasets, direct comparisons with concurrent methods are limited
- **Parameter efficiency**: Maintains efficiency by tuning only soft prompts, but full computational cost including dynamic grouping overhead is not fully characterized

**Low Confidence**:
- **Generalization to arbitrary source pools**: Performance with heterogeneous or unrelated source tasks is not tested
- **Robustness to initialization variance**: Assumption that TPV arithmetic is comparable across different initialization strategies is not validated

## Next Checks

1. **Ablation: Static vs Dynamic TPV Generation**
   - Run DTVG with two variants: (a) TPVs computed from fixed checkpoints (static) vs (b) TPVs recomputed every N steps during target training (dynamic). Compare performance to isolate whether the benefit comes from dynamic grouping or dynamic TPV computation.

2. **Stress Test: Unrelated Source Tasks**
   - Add 3-5 semantically unrelated source tasks (e.g., image captioning prompts, code generation tasks) to the source pool. Measure whether DTVG's KC mechanism successfully filters them out or whether negative transfer occurs.

3. **Scaling Study: Source Pool Size**
   - Systematically increase the number of source tasks (e.g., 6 → 12 → 24) and measure: (a) computational overhead of dynamic grouping, (b) performance saturation point, and (c) whether the greedy algorithm's optimality degrades with larger pools.