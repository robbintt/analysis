---
ver: rpa2
title: 'SAND Challenge: Four Approaches for Dysartria Severity Classification'
arxiv_id: '2512.02669'
source_url: https://arxiv.org/abs/2512.02669
tags:
- features
- each
- class
- speech
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified study of four distinct modeling approaches
  for classifying dysarthria severity in the SAND challenge. All models tackle the
  same five-class classification task using a common dataset of speech recordings.
---

# SAND Challenge: Four Approaches for Dysarthria Severity Classification

## Quick Facts
- arXiv ID: 2512.02669
- Source URL: https://arxiv.org/abs/2512.02669
- Reference count: 0
- Primary result: Hierarchical XGBoost ensemble achieves macro-F1 0.86; best deep learning model (BiLSTM) achieves 0.70 F1

## Executive Summary
This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the SAND challenge. All models tackle the same five-class classification task using a common dataset of speech recordings. The approaches include a ViT-OF method leveraging a Vision Transformer on spectrogram images, a 1D-CNN approach using eight 1-D CNNs with majority-vote fusion, a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and a Hierarchical XGBoost ensemble that combines glottal and formant features through a two-stage learning framework. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.

## Method Summary
The paper evaluates four distinct approaches for 5-class dysarthria severity classification using 219 training speakers and 53 validation speakers. Each speaker provides 8 utterances (5 vowels: A/E/I/O/U, 3 syllables: KA/PA/TA). The hierarchical XGBoost approach uses 12 acoustic features (5 formants F1-F5, 7 glottal parameters via SEDREAMS) plus age/gender, training 8 binary classifiers stratified by gender/age in Stage 1, then combining outputs with a decision tree in Stage 2. The deep learning approaches include ViT-B16 on 512×256 spectrograms with probability averaging, 1D-CNN ensemble with 54-dim phase features using majority voting, and BiLSTM ensemble on STFT spectrograms with majority voting. All approaches use multi-utterance fusion strategies to stabilize predictions.

## Key Results
- Hierarchical XGBoost ensemble achieves macro-F1 0.86, outperforming all deep learning approaches
- ViT-AVE achieves 0.68 F1 using pre-trained vision model on spectrograms
- BiLSTM-OF achieves 0.70 F1 after pruning from 9 to 4 models
- 1D-CNN achieves 0.64 F1 using phase feature-based ensemble
- All approaches show confusion primarily between adjacent severity classes

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical binary decomposition with domain-specific features
- **Claim:** Hierarchical binary decomposition with domain-specific features outperforms end-to-end learning in low-data, imbalanced settings.
- **Mechanism:** Eight binary XGBoost classifiers specialize in sub-problems (e.g., "Class 3 vs 4&5 for females"), then a Stage-2 decision tree integrates outputs. Formant frequencies (F1–F5) and glottal pulse parameters capture known acoustic biomarkers of dysarthria.
- **Core assumption:** Dysarthria manifestations differ by gender and age, so stratified models can isolate decision boundaries that a monolithic classifier would blur.
- **Evidence anchors:** XGBoost ensemble achieves macro-F1 0.86 vs. 0.64–0.70 for neural models; Stage 1 has models stratified by gender/age; confusion matrix shows only adjacent-class confusion.
- **Break condition:** If feature extraction pipelines (SEDREAMS glottal analysis) fail on noisy recordings or voiceless utterances, the approach degrades.

### Mechanism 2: Multi-utterance late fusion stabilizes predictions
- **Claim:** Multi-utterance late fusion stabilizes predictions across variable-quality recordings.
- **Mechanism:** ViT-AVE averages class probabilities across 8 utterances per speaker; BiLSTM-OF and 1D-CNN use majority voting across utterance-specific models. This reduces variance from any single utterance.
- **Core assumption:** Speaker-level severity label is consistently reflected across all utterances, even if individual signal quality varies.
- **Evidence anchors:** All approaches use fusion strategies for the 8 utterances per speaker; pruning BiLSTM ensemble from 9 to 4 models improved F1 from 0.62 to 0.70.
- **Break condition:** If severity is utterance-type dependent, uniform fusion weights may dilute discriminative signal.

### Mechanism 3: Transfer learning from pre-trained vision models
- **Claim:** Transfer learning from pre-trained vision models to spectrograms provides usable representations when training data is scarce.
- **Mechanism:** ViT-B16 (pre-trained on ImageNet) processes 512×256 spectrogram images; the model leverages learned attention patterns for spectral texture despite domain shift.
- **Core assumption:** Spectrograms share structural regularities (edges, textures) with natural images that transfer meaningfully.
- **Evidence anchors:** ViT-AVE achieved 0.68 F1 despite "relatively few training samples"; data augmentation applied to spectrograms during fine-tuning.
- **Break condition:** If spectrogram patterns relevant to dysarthria are not captured by ImageNet-derived attention heads, transfer provides limited benefit.

## Foundational Learning

- **Concept: Class-weighted loss functions**
  - Why needed here: Severe imbalance (4 Class 1 vs 86 Class 5 speakers) causes models to bias toward majority class.
  - Quick check question: If you train without class weighting, what F1 would a trivial majority-class predictor achieve on this 5-class task?

- **Concept: Spectrogram representation of speech**
  - Why needed here: Converts 1D audio into 2D time-frequency images for ViT input; BiLSTM uses STFT magnitude as frame sequences.
  - Quick check question: Given 20ms window, 10ms hop, and 8kHz sampling, how many frames represent 1 second of audio?

- **Concept: Majority voting vs. probability averaging (late fusion)**
  - Why needed here: Paper compares soft fusion (ViT probability averaging) vs. hard fusion (CNN/BiLSTM majority vote); understanding calibration tradeoffs informs design choices.
  - Quick check question: If 8 models output class probabilities [0.9, 0.1, 0, 0, 0] and [0.6, 0.4, 0, 0, 0] for two classes, which fusion method would be more sensitive to confidence calibration?

## Architecture Onboarding

- **Component map:**
  ```
  Audio (8 utterances/speaker)
      │
      ├─→ ViT-AVE: Spectrogram (512×256) → ViT-B16 → 5-class logits → Avg probabilities
      │
      ├─→ 1D-CNN: Phase features (54-dim frames) → 8 CNNs → Majority vote
      │
      ├─→ BiLSTM-OF: STFT spectrogram (129 bins) → 9 BiLSTMs → Majority vote
      │
      └─→ XGBoost: 12 acoustic features + age/gender → 8 binary XGB → Decision Tree → 5-class
  ```

- **Critical path:** Feature extraction quality → Class imbalance handling → Fusion strategy → Macro-F1 evaluation (not accuracy, due to imbalance).

- **Design tradeoffs:**
  | Approach | Interpretability | Data Efficiency | Compute | Feature Engineering |
  |----------|------------------|-----------------|---------|---------------------|
  | XGBoost | High (SHAP, decision paths) | High | Low | High (manual features) |
  | ViT-AVE | Low | Medium (pre-training helps) | High | Low |
  | BiLSTM-OF | Medium | Low | Medium | Low |
  | 1D-CNN | Medium | Low | Medium | Medium (phase features) |

- **Failure signatures:**
  - XGBoost: Poor glottal extraction on voiceless utterances (PA/TA excluded).
  - ViT: Noisy validation curves; sensitivity to training fluctuations.
  - BiLSTM/CNN: Low F1 when using all utterance models indiscriminately (confusion from weak vowel models).

- **First 3 experiments:**
  1. **Baseline replication:** Implement XGBoost with 12 features on a 3-class subset (merge Classes 1-2, 3, 4-5) to verify hierarchical decomposition benefits before scaling to 5 classes.
  2. **Ablation on fusion:** Compare soft (probability averaging) vs. hard (majority vote) fusion on ViT outputs using held-out speakers to determine which better handles calibrated vs. uncalibrated confidence.
  3. **Feature importance audit:** Run SHAP on XGBoost Stage-1 models to confirm which of the 12 features drive predictions; cross-reference with speech science literature to validate biological plausibility before trusting model generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a hybrid architecture combining domain-specific features with data-driven neural networks improve robustness over single-approach models?
- Basis in paper: The conclusion states that "future work could explore hybrid models that blend these approaches, such as feeding engineered features into neural networks or using neural outputs as features in boosting."
- Why unresolved: The study evaluates feature-engineered (XGBoost) and deep learning models (ViT, BiLSTM) as independent, parallel strategies rather than integrated pipelines.
- What evidence would resolve it: Performance metrics from a combined model that inputs glottal/formant features into the transformer or BiLSTM architectures.

### Open Question 2
- Question: To what extent would detailed annotations of rhythmic syllable repetitions improve the utility of ASR-based techniques for this task?
- Basis in paper: Remark 3 notes that "if these rhythm sounds were annotated to indicate how many times each speaker uttered these, then ASR techniques could have been used to better advantage."
- Why unresolved: The provided dataset lacks transcription details regarding the count of repeated syllables (PA, TA, KA), preventing ASR models from leveraging rhythmic consistency as a feature.
- What evidence would resolve it: A comparative study evaluating ASR classification performance with and without ground-truth syllable count annotations.

### Open Question 3
- Question: How does the alignment of loss function design with the manual grading protocol impact classification accuracy?
- Basis in paper: Remark 3 highlights that "exact methodology of manual classification would have provided a better background for the design of the loss function," noting the authors had to assume classification was based on averaging utterances.
- Why unresolved: The "gold standard" labeling process is opaque; therefore, current loss functions (e.g., averaged loss) may not mathematically reflect the clinical decision process (e.g., prioritizing specific syllables).
- What evidence would resolve it: A study correlating model performance when optimizing different loss functions against the specific aggregation rules used by human clinicians.

## Limitations

- XGBoost superiority may not generalize beyond SAND challenge dataset due to reliance on manual feature engineering
- Feature extraction pipelines (SEDREAMS glottal analysis) not fully specified, making exact reproduction challenging
- ViT transfer learning benefit lacks validation against baseline models or domain-specific pre-training alternatives
- Noisy validation curves suggest model sensitivity to training randomness, raising questions about result stability

## Confidence

- XGBoost superiority over deep learning: **Medium** (strong F1 difference but relies on unvalidated feature engineering)
- Transfer learning benefit for ViT: **Low** (no ablation against baseline models or alternative pre-training)
- Multi-utterance fusion effectiveness: **Medium** (supported by results but mechanism not rigorously tested)

## Next Checks

1. **Ablation study**: Compare XGBoost with only raw acoustic features (no formants/glottal) against the full 12-feature version to quantify feature engineering contribution versus model architecture.

2. **Cross-dataset validation**: Test the best-performing XGBoost model on an independent dysarthria dataset (e.g., TORGO or UASpeech) to assess generalizability beyond SAND challenge data.

3. **Ensemble fusion analysis**: Systematically compare soft (probability averaging) versus hard (majority vote) fusion across all four approaches using identical utterance subsets to determine optimal fusion strategy independent of model architecture.