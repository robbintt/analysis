---
ver: rpa2
title: 'CATTO: Balancing Preferences and Confidence in Language Models'
arxiv_id: '2601.23096'
source_url: https://arxiv.org/abs/2601.23096
tags:
- confidence
- calibration
- catto
- loss
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CATTO addresses miscalibration in language models by embedding\
  \ a differentiable per-token calibration objective into preference optimization,\
  \ aligning predicted confidence with empirical correctness while preserving preference\
  \ ordering. It reduces Expected Calibration Error (ECE) by 2.22%\u20137.61% in-distribution\
  \ and 1.46%\u201310.44% out-of-distribution versus DPO, and maintains or slightly\
  \ improves accuracy on five datasets."
---

# CATTO: Balancing Preferences and Confidence in Language Models

## Quick Facts
- arXiv ID: 2601.23096
- Source URL: https://arxiv.org/abs/2601.23096
- Reference count: 40
- Primary result: Reduces Expected Calibration Error by 2.22%–7.61% in-distribution and 1.46%–10.44% out-of-distribution versus DPO while maintaining accuracy

## Executive Summary
CATTO addresses a critical gap in language model preference optimization by embedding per-token calibration objectives directly into the DPO training loop. While DPO aligns model preferences with human judgments, it often produces overconfident predictions that poorly reflect true correctness probabilities. CATTO solves this by jointly optimizing preference alignment and calibration, producing models that not only rank responses correctly but also assign confidences that match empirical accuracy.

The method introduces Confidence@k, a test-time scaling rule that leverages calibrated confidences for optimal candidate selection. Across five datasets, CATTO reduces Expected Calibration Error by 2.22%–7.61% in-distribution and 1.46%–10.44% out-of-distribution compared to DPO, while maintaining or slightly improving accuracy. This dual capability—preserving preference ordering while improving confidence calibration—addresses both practical deployment needs and theoretical concerns about model reliability.

## Method Summary
CATTO modifies the standard DPO objective by adding a per-token calibration loss that penalizes mismatches between predicted confidence and empirical correctness. The calibration loss uses a differentiable form of Expected Calibration Error computed over individual tokens rather than entire sequences. During training, the model jointly optimizes preference alignment (via pairwise ranking losses) and calibration (via token-level confidence matching). The calibration component uses entropy-based scaling to transform raw logits into calibrated confidences, which are then used at test time through Confidence@k—a selection rule that chooses the top-k candidates based on calibrated rather than raw confidences. This integration allows CATTO to maintain preference ordering while ensuring that confidence scores better reflect actual correctness probabilities.

## Key Results
- Reduces Expected Calibration Error by 2.22%–7.61% in-distribution versus DPO
- Improves out-of-distribution calibration by 1.46%–10.44% on multiNLI-hard
- Maintains or slightly improves accuracy across five benchmark datasets
- Introduces Confidence@k scaling rule for Bayes-optimal candidate selection

## Why This Works (Mechanism)
CATTO works by recognizing that preference optimization and confidence calibration address complementary aspects of model quality. DPO focuses on getting the relative ranking of responses correct but doesn't constrain the absolute confidence values, leading to systematic overconfidence. By adding a differentiable calibration loss, CATTO creates a feedback signal that directly penalizes when predicted confidence diverges from empirical correctness at the token level. The key insight is that this calibration objective can be integrated without disrupting the preference alignment—the model still learns to rank preferred responses higher, but now does so while maintaining calibrated confidence estimates. The entropy-based scaling in Confidence@k then exploits these calibrated confidences for optimal decision-making at test time.

## Foundational Learning
- Expected Calibration Error (ECE): Measures discrepancy between predicted confidence and empirical accuracy; needed to quantify calibration quality, quick check: compute as weighted average of bin-wise accuracy-confidence differences
- Preference Optimization: Framework for aligning model outputs with human preferences via pairwise ranking; needed to ensure responses match human judgments, quick check: verify preference ranking loss decreases during training
- Differentiable Calibration Loss: Making calibration objectives compatible with gradient-based optimization; needed to integrate calibration into training loop, quick check: ensure gradients flow through calibration component
- Entropy-based Scaling: Transformation from raw logits to calibrated probabilities; needed for Confidence@k to work effectively, quick check: verify entropy increases as confidence decreases
- Bayes-optimal Selection: Decision rule that maximizes expected utility; needed for Confidence@k to provide theoretical guarantees, quick check: confirm selection rule maximizes calibrated confidence

## Architecture Onboarding

Component Map:
Input -> Encoder -> Preference Loss + Calibration Loss -> Calibrated Confidence Output

Critical Path:
During training: Input sequences → Encoder → Logits → (Preference ranking + Token-level calibration) → Updated weights
During inference: Input → Encoder → Calibrated confidences → Confidence@k selection

Design Tradeoffs:
- Adding calibration loss vs. pure preference optimization: balances confidence quality against ranking accuracy
- Token-level vs. sequence-level calibration: finer-grained control but higher computational cost
- Fixed vs. learned scaling constants (α, β): simplicity vs. potential for better optimization

Failure Signatures:
- Calibration loss dominates preference loss → poor ranking performance
- Preference loss dominates → minimal calibration improvement
- Improper scaling of calibration component → numerical instability

First Experiments:
1. Train with only preference loss (baseline DPO) to establish reference performance
2. Train with only calibration loss to verify it improves calibration independently
3. Joint training with varying α, β ratios to find optimal balance point

## Open Questions the Paper Calls Out
The paper acknowledges that while ECE improvements are measurable, the translation to meaningful real-world decision benefits remains unclear since downstream utility is not directly measured. It also notes that the choice of entropy-based scaling for Confidence@k, while "natural," lacks empirical justification against alternatives. The sensitivity of calibration gains to the scaling constants α and β is unexplored, leaving open whether current values are optimal.

## Limitations
- ECE improvements may not translate to meaningful real-world utility gains
- Limited OOD validation (only multiNLI-hard) weakens robustness claims
- Incomplete ablation on scaling constants (α, β) leaves optimization potential unexplored
- No direct measurement of downstream task performance or user decision benefits

## Confidence

High confidence:
- ECE reduction metrics (2.22%–7.61% in-distribution, 1.46%–10.44% OOD)
- Accuracy preservation/maintenance across five datasets
- Core mechanism of integrating per-token calibration loss into preference optimization

Medium confidence:
- OOD robustness claims (based only on multiNLI-hard)
- Optimality of entropy-based scaling in Confidence@k
- Generalizability of calibration improvements to other domains

Low confidence:
- Whether calibration improvements meaningfully impact real-world utility
- Sensitivity of results to hyperparameter choices (α, β)
- Performance on adversarial or out-of-domain distributions beyond tested datasets

## Next Checks
1. Validate OOD robustness on broader suite of datasets (adversarial NLI, out-of-domain QA)
2. Perform ablation study varying α and β to determine sensitivity and optimization potential
3. Conduct downstream task evaluation or user study to verify real-world utility of calibration improvements