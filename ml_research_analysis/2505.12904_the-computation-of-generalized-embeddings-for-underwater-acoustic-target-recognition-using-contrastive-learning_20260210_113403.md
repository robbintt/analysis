---
ver: rpa2
title: The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition
  using Contrastive Learning
arxiv_id: '2505.12904'
source_url: https://arxiv.org/abs/2505.12904
tags:
- data
- supervised
- unsupervised
- learning
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised contrastive learning framework
  for underwater acoustic target recognition (UATR), addressing the scarcity of labeled
  data by leveraging abundant unlabeled recordings from a single hydrophone. The method
  uses a Conformer-based encoder optimized with a Variance-Invariance-Covariance Regularization
  (VICReg) loss function to generate generalized embeddings from unlabeled spectrograms,
  which are then used for classification tasks.
---

# The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning

## Quick Facts
- **arXiv ID:** 2505.12904
- **Source URL:** https://arxiv.org/abs/2505.12904
- **Reference count:** 11
- **Primary result:** Unsupervised contrastive learning framework achieves competitive performance with supervised methods on ship and marine mammal classification tasks while being less dependent on labeled data availability.

## Executive Summary
This paper proposes an unsupervised contrastive learning framework for underwater acoustic target recognition (UATR) that addresses the scarcity of labeled data by leveraging abundant unlabeled recordings from a single hydrophone. The method uses a Conformer-based encoder optimized with a Variance-Invariance-Covariance Regularization (VICReg) loss function to generate generalized embeddings from unlabeled spectrograms, which are then used for classification tasks. The approach demonstrates competitive performance with supervised methods on ship type and marine mammal vocalization classification tasks, achieving accuracy scores of 54.87% and 86.10% respectively on benchmark datasets.

## Method Summary
The framework employs unsupervised contrastive learning to create generalized embeddings from unlabeled underwater acoustic data. A Conformer-based encoder processes 128-band Mel spectrograms extracted from 2-second audio windows. The model is pre-trained using VICReg loss on unlabeled Ocean Network Canada hydrophone data, then transferred to downstream classification tasks using logistic regression on frozen embeddings. Domain-specific augmentations exploit known acoustic properties of ship signatures, while the hybrid Conformer architecture captures both fine-grained spectral patterns and long-range temporal dependencies.

## Key Results
- Unsupervised Conformer achieves 54.87% accuracy on Deepship ship classification vs. 50.34% for unsupervised ResNet18
- On ShipsEar dataset, unsupervised Conformer reaches 57.42% accuracy compared to 42.25% for ResNet18
- Performance remains stable with reduced labeled data availability, outperforming supervised models when labels are scarce
- Cross-dataset generalization validated across multiple geographically and acoustically distinct datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VICReg loss prevents embedding collapse without requiring negative sample pairs, enabling unsupervised learning on completely unlabeled underwater acoustic data.
- **Mechanism:** Three loss components operate jointly: (1) **Invariance** pushes augmented views of the same sample together via negative cosine similarity; (2) **Variance** maintains embedding diversity by penalizing dimensions with standard deviation below a threshold (γ=1); (3) **Covariance** decorrelates embedding dimensions by penalizing off-diagonal terms in the covariance matrix, preventing informational collapse.
- **Core assumption:** Underwater spectrograms have lower inherent contrast than natural images, making them more susceptible to dimensional collapse where all embeddings converge to similar values.
- **Evidence anchors:**
  - [abstract]: "Conformer-based encoder is optimized by the so-called Variance-Invariance-Covariance Regularization loss function on these lower-quality unlabeled data"
  - [section 2.5]: "Since there is no information on the content of this recording available, it is difficult to assign negative samples as proposed in the original SimCLR paper. For this reason, the Variance-Invariance Covariance Regularization (VICReg) loss function is implemented."
  - [corpus]: Weak direct corpus evidence on VICReg for UATR; corpus papers focus on attention mechanisms and few-shot learning without negative-sample-free contrastive approaches.
- **Break condition:** If weight parameters follow computer vision defaults (λ=25, μ=25, ν=1), accuracy collapses to 23.81%; underwater acoustics require higher relative covariance weighting (λ=5, μ=5, ν=1).

### Mechanism 2
- **Claim:** Domain-specific augmentations create semantically meaningful positive pairs by exploiting known acoustic properties of ship signatures.
- **Mechanism:** Four augmentation functions define positive pairs: (1) identity passthrough; (2) Gaussian noise injection (0.3-0.5 dB SNR) simulating environmental variation; (3) low-pass filtering at 1kHz cutoff preserving discriminative ship frequencies; (4) MixUp combining low-frequency content (50Hz-1kHz) from one sample with high-frequency content (1-8kHz) from a temporally-adjacent sample drawn from N(center_time, 50s).
- **Core assumption:** The most discriminative ship classification information resides in 50Hz-1kHz frequency bands, and temporally proximate recordings share acoustic environmental characteristics.
- **Evidence anchors:**
  - [section 2.2]: "As stated in (Hummel et al., 2024), the most discriminative information for the classification of the type of ships lies between 50 Hz and 1 kHz. Therefore, a low-pass filter is a representative augmentation function."
  - [section 3.3]: "The highest accuracy scores on both Deepship and Watkin's are achieved by the LowPass filter. This augmentation function alone achieves performance similar to that of the model trained utilizing the full augmentation family."
  - [corpus]: Corpus papers mention data augmentation but lack frequency-band-specific analysis for UATR applications.
- **Break condition:** Adding speech-based augmentations (pitch shift, reverb, gain) improves supervised models but degrades unsupervised cross-dataset performance—ShipsEar drops from 57.42% to 50.47%.

### Mechanism 3
- **Claim:** Conformer architecture captures both fine-grained spectral patterns and long-range temporal dependencies required for generalizable acoustic embeddings.
- **Mechanism:** Each of four Conformer blocks combines (1) feedforward layers, (2) 4-head multi-head self-attention for global temporal relationships, and (3) convolution modules with kernel size 31 for local spectral pattern detection. The hybrid design addresses the dual requirement of detecting narrowband ship signatures while modeling extended temporal context.
- **Core assumption:** Ship acoustic signatures require simultaneous local frequency resolution and global temporal context modeling that pure CNN or pure Transformer architectures cannot efficiently provide.
- **Evidence anchors:**
  - [section 2.4]: "Previous research has shown the potential of the Conformer model in audio classification on AudioSet (Srivastava et al., 2022)"
  - [section 3.1]: "Unsupervised Conformer (LARS) achieves 54.87% accuracy on Deepship vs. 50.34% for unsupervised ResNet18; on ShipsEar, Conformer achieves 57.42% vs. 42.25%."
  - [corpus: paper 35281 "Automated data curation"]: Uses self-supervised learning for underwater acoustics but doesn't specifically validate Conformer vs. CNN tradeoffs.
- **Break condition:** For supervised training on small datasets, larger Conformer underperforms—supervised Conformer achieves lower Deepship accuracy than supervised ResNet18 when labeled data is limited (section 3.3.2).

## Foundational Learning

- **Concept:** Contrastive Learning Objective Functions
  - **Why needed here:** Understanding why VICReg works without negative samples is essential for debugging collapse and tuning loss weights.
  - **Quick check question:** Explain why SimCLR requires explicit negative pairs while VICReg achieves similar objectives through variance and covariance regularization alone.

- **Concept:** Mel-Scale Spectrogram Representations
  - **Why needed here:** All model inputs are 128-band Mel spectrograms; understanding frequency warping affects augmentation and failure mode analysis.
  - **Quick check question:** Why might Mel scale (emphasizing lower frequencies) be advantageous for ship classification compared to linear frequency scales?

- **Concept:** Time-wise vs. Random Train-Test Splits
  - **Why needed here:** The paper argues time-wise splits better reflect real-world deployment where models encounter previously unseen ships and seasonal variations.
  - **Quick check question:** Why does a random split inflate reported accuracy while potentially reducing generalization to geographically different datasets?

## Architecture Onboarding

- **Component map:** Raw audio → 16kHz downsampling → 2-sec non-overlapping windows → augmentation family → 128-band Mel spectrogram → 4× Conformer blocks (feedforward + 4-head attention + conv kernel 31) → flatten → 2048-dim embedding → 2-layer MLP expander → 8196-dim space → logistic regression classifier

- **Critical path:**
  1. Preprocess ONC hydrophone data: sample 5-min recordings across months/times, ensure temporal diversity
  2. Training loop: batch size 2048, LARS optimizer (lr=0.01 with 10× decay on plateau), VICReg loss with λ=5, μ=5, ν=1
  3. Transfer: freeze encoder, train linear classifier on labeled Deepship/ShipsEar/Watkin's embeddings

- **Design tradeoffs:**
  - Embedding dimension: 512 sufficient; 2048 shows minimal gain but higher covariance loss risk
  - Architecture capacity: Conformer (101M params) generalizes better than ResNet18 (17M params) for unsupervised, but supervised small-data scenarios favor ResNet18
  - Inference: Conformer is 6× larger but only 2× slower (5.3ms vs. 2.8ms per sample) due to parallelization

- **Failure signatures:**
  - Accuracy ~24% → VICReg weights wrong; use λ=5, μ=5, ν=1 (not CV defaults)
  - High Deepship accuracy, poor ShipsEar → overfitting to source domain; reduce model capacity or expand augmentation diversity
  - Covariance loss plateau during training → embedding collapse imminent; increase ν weight or reduce embedding dimension

- **First 3 experiments:**
  1. **VICReg weight ablation:** Train on ONC subset with (λ=5, μ=5, ν=1) vs. (λ=25, μ=25, ν=1); expect ~55% vs. ~24% Deepship accuracy
  2. **Augmentation sensitivity:** Train with LowPass-only vs. full family; expect matched Deepship performance but ~8% ShipsEar degradation for LowPass-only
  3. **Labeled data robustness:** Train supervised and unsupervised models with 10%/25%/50%/100% Deepship labels; expect unsupervised to maintain stable performance while supervised degrades sharply below 50% labels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this unsupervised contrastive learning framework effectively generalize to event-based underwater acoustic analysis tasks, such as climate change monitoring or nuclear test detection?
- **Basis in paper:** [explicit] The Highlights and Discussion sections explicitly state that the framework creates potential for translation to tasks like "climate change monitoring and nuclear bomb test detection," though the current study only validates it for ship and marine mammal classification.
- **Why unresolved:** The paper's experiments are limited to recognizing continuous sound sources (ships) and vocalizations (mammals). It is unknown if the "generalized embeddings" capture the transient or distinct features necessary for event detection.
- **What evidence would resolve it:** Benchmarking the pre-trained encoder on datasets containing geophysical or anthropogenic events to evaluate detection performance without fine-tuning.

### Open Question 2
- **Question:** Does incorporating diverse unlabeled data from multiple hydrophones significantly improve generalization compared to training on data from a single hydrophone?
- **Basis in paper:** [explicit] The Discussion notes the framework can be extended by "incorporating more diverse underwater acoustical data from multiple publicly available hydrophones," suggesting the current single-hydrophone approach may be a limiting factor.
- **Why unresolved:** The model is currently trained only on data from a specific location (ONC hydrophone near Vancouver). While it generalizes well to other datasets, utilizing multi-location data for pre-training might yield a more universally robust embedding space.
- **What evidence would resolve it:** A comparison of embeddings learned from a multi-hydrophone dataset versus the single-hydrophone dataset when evaluated on geographically distinct downstream tasks.

### Open Question 3
- **Question:** Would defining classification targets based on physical ship characteristics (e.g., engine type, blade count) result in higher accuracy than the current AIS-based categorization?
- **Basis in paper:** [explicit] The Discussion suggests that current AIS classes suffer from high intra-class spread and overlap, proposing "a solution to this problem could be to create classes based on the characteristics of the ship."
- **Why unresolved:** The paper demonstrates the model can classify standard AIS categories, but it remains untested whether the learned embeddings cluster more naturally around physical acoustic signatures rather than functional ship types.
- **What evidence would resolve it:** Re-evaluating the model's embedding space using a dataset labeled with detailed physical propulsion metadata to measure class separation.

## Limitations

- **Computational overhead:** The Conformer architecture is substantially larger than alternative CNN models (6× parameter count), potentially limiting deployment in resource-constrained underwater platforms.
- **Hyperparameter sensitivity:** VICReg loss requires careful hyperparameter tuning specific to underwater acoustics, as using default computer vision weights leads to complete model failure.
- **Limited validation scope:** While the approach shows robustness across multiple datasets, the evaluation primarily focuses on ship classification and marine mammal vocalization tasks, with limited validation on other underwater acoustic targets.

## Confidence

- **High Confidence:** VICReg preventing embedding collapse without negative samples is well-supported by ablation study showing catastrophic performance degradation with incorrect weights.
- **Medium Confidence:** Domain-specific augmentations creating meaningful positive pairs is supported by experiments showing LowPass filter alone achieves comparable performance to full augmentation families on Deepship.
- **Low Confidence:** Conformer architecture being optimal for capturing both fine-grained spectral patterns and long-range temporal dependencies lacks direct comparison to other hybrid architectures.

## Next Checks

1. **Loss Weight Sensitivity Analysis:** Systematically vary VICReg weights (λ, μ, ν) across a broader range to identify optimal values for different underwater acoustic scenarios and validate whether the proposed weights (λ=5, μ=5, ν=1) are universally optimal.

2. **Cross-Modal Augmentation Testing:** Evaluate the impact of speech-based augmentations (pitch shift, reverb, gain) on cross-dataset performance, particularly investigating why these augmentations degrade performance on ShipsEar despite improving supervised models.

3. **Real-Time Deployment Benchmark:** Measure actual inference latency and energy consumption on embedded underwater acoustic processing hardware to validate the practical feasibility of the 6× larger Conformer architecture compared to ResNet18 alternatives.