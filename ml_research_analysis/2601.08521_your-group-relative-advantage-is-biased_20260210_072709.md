---
ver: rpa2
title: Your Group-Relative Advantage Is Biased
arxiv_id: '2601.08521'
source_url: https://arxiv.org/abs/2601.08521
tags:
- advantage
- ha-dw
- grpo
- estimation
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Group-relative RL algorithms like GRPO suffer from biased advantage
  estimation that systematically underestimates advantages for hard prompts and overestimates
  them for easy prompts. This bias degrades training performance by skewing exploration-exploitation
  balance.
---

# Your Group-Relative Advantage Is Biased

## Quick Facts
- arXiv ID: 2601.08521
- Source URL: https://arxiv.org/abs/2601.08521
- Reference count: 40
- Group-relative RL algorithms suffer from biased advantage estimation that degrades training performance

## Executive Summary
Group-relative reinforcement learning algorithms like GRPO suffer from biased advantage estimation that systematically underestimates advantages for hard prompts while overestimating them for easy prompts. This bias skews the exploration-exploitation balance during training and degrades overall performance. The proposed History-Aware Adaptive Difficulty Weighting (HA-DW) addresses this issue by dynamically adjusting advantage weights using an evolving difficulty anchor that incorporates long-term reward trends and historical training information.

HA-DW is designed as a plug-and-play module that can be integrated into GRPO and its variants. Extensive experiments across five mathematical reasoning benchmarks using models from Qwen and LLaMA families demonstrate consistent performance improvements over original algorithms. Notably, HA-DW achieves superior results even when compared with GRPO using more rollouts, highlighting the importance of correcting biased advantage estimation for robust and efficient RLVR training.

## Method Summary
The paper addresses the fundamental issue of biased advantage estimation in group-relative RL algorithms, where rewards are normalized within groups leading to systematic underestimation for hard prompts and overestimation for easy prompts. HA-DW introduces a dynamic difficulty weighting mechanism that maintains an evolving difficulty anchor incorporating both historical and current reward trends. This anchor serves as a reference point for adjusting advantage weights during training, allowing the algorithm to better balance exploration and exploitation. The method is implemented as a modular component that can be integrated into existing GRPO frameworks without requiring architectural changes.

## Key Results
- HA-DW consistently improves performance across five mathematical reasoning benchmarks compared to original GRPO algorithms
- The method achieves superior results even when compared with GRPO using more rollouts, demonstrating computational efficiency
- Performance improvements are demonstrated across both Qwen and LLaMA model families, showing broad applicability

## Why This Works (Mechanism)
The core mechanism works by correcting the systematic bias in advantage estimation that arises from group-relative reward normalization. In standard GRPO, rewards are normalized within groups, which creates a bias where easy prompts appear to have higher advantages than they should, while hard prompts appear to have lower advantages. HA-DW introduces a difficulty-aware weighting scheme that uses an evolving difficulty anchor to recalibrate these advantage estimates. This anchor tracks long-term reward trends and historical performance, allowing the algorithm to maintain more accurate advantage estimates across different difficulty levels and training stages.

## Foundational Learning
- **Group-relative advantage estimation**: Why needed - to enable comparison across different prompts; Quick check - verify reward normalization within groups is correctly implemented
- **Difficulty-aware weighting**: Why needed - to correct systematic bias in advantage estimation; Quick check - ensure difficulty anchor evolves with training progress
- **Historical reward tracking**: Why needed - to maintain context across training stages; Quick check - validate that historical information is properly incorporated
- **Plug-and-play module design**: Why needed - for broad applicability across different RLVR frameworks; Quick check - confirm compatibility with different model architectures

## Architecture Onboarding

**Component Map:** Input -> Reward Normalization -> Difficulty Anchor Update -> Advantage Weighting -> Output

**Critical Path:** The critical path involves processing rewards through normalization, updating the difficulty anchor based on historical trends, applying adaptive weighting to advantages, and using these weighted advantages for policy updates.

**Design Tradeoffs:** The main tradeoff involves balancing computational overhead from maintaining historical information against the benefits of more accurate advantage estimation. HA-DW opts for additional computation to maintain accuracy across training stages.

**Failure Signatures:** Potential failures include incorrect difficulty anchor updates leading to improper weighting, failure to adapt to changing difficulty distributions, or computational overhead becoming prohibitive for very large-scale training.

**First 3 Experiments:**
1. Verify basic functionality by comparing HA-DW against standard GRPO on a simple mathematical reasoning task
2. Test adaptive weighting behavior by analyzing advantage estimates across different difficulty levels
3. Validate plug-and-play compatibility by integrating HA-DW into different GRPO variants

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Limited evaluation scope focused on mathematical reasoning tasks, with unclear generalizability to other RLVR domains like code generation or logical reasoning
- Theoretical foundation connecting adaptive difficulty weighting to advantage estimation properties needs stronger justification
- Computational efficiency claims require more systematic exploration across different rollout configurations and reward structures

## Confidence
- HA-DW improves mathematical reasoning RLVR performance: High
- HA-DW is universally plug-and-play across all GRPO variants: Medium
- HA-DW consistently outperforms more rollouts: Medium

## Next Checks
1. Test HA-DW on non-mathematical reasoning tasks including code generation, logical reasoning, and real-world applications to verify domain generalizability
2. Conduct ablation studies isolating the contribution of each component in HA-DW's difficulty weighting mechanism
3. Perform extensive hyperparameter sensitivity analysis across different model sizes and training configurations to identify robustness boundaries