---
ver: rpa2
title: 'LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large
  Language Models'
arxiv_id: '2511.18966'
source_url: https://arxiv.org/abs/2511.18966
tags:
- code
- security
- generated
- cwes
- vulnerabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models

## Quick Facts
- arXiv ID: 2511.18966
- Source URL: https://arxiv.org/abs/2511.18966
- Reference count: 40
- Key outcome: LLMs generate insecure C/C++ code with CWE-787 (Out-of-bounds Write) being the most prevalent vulnerability across models

## Executive Summary
This paper evaluates the security of C/C++ code generated by 10 Large Language Models (LLMs) using 84 prompts derived from MITRE CWE examples. The study systematically compares code generation security between baseline prompts ("Code Generator") and security-focused prompts ("Secure Code Generator") across models including GPT-3.5, Gemini, Llama, Mistral, and Code-specific variants. Using multiple static analysis tools (CodeQL, Snyk Code, CodeShield), the research identifies CWE patterns and maps them to CVEs to assess real-world exploitability. The findings reveal that security-focused system prompts do not consistently reduce vulnerability generation and that CWE-787 (Out-of-bounds Write) is the most prevalent vulnerability across models.

## Method Summary
The study employs a systematic approach: 84 CWE-derived prompts are fed to 10 LLMs in two modes (baseline CG and security-focused SCG). Generated code is analyzed using three static analysis tools (CodeQL with 804 CWE coverage, Snyk Code with 31 C/C++ rules, and CodeShield with 23 LLM-focused rules). Results are aggregated via a custom SarifMiner tool and mapped to CVEs from the NVD database. The methodology emphasizes reproducibility through open-source tooling and provides insights into how different prompting strategies affect code security.

## Key Results
- CWE-787 (Out-of-bounds Write) is the most prevalent vulnerability across all models
- Security-focused system prompts (SCG) do not consistently reduce vulnerability generation and sometimes increase it
- Multi-tool SAST analysis reveals limited overlap, with CodeQL detecting 33 unique CWEs while Snyk and CodeShield detect only 10 and 1 respectively
- Model refusal behavior is significant in SCG mode, with Llama2-7B refusing to generate code for 22/84 prompts

## Why This Works (Mechanism)

### Mechanism 1: CWE-to-Prompt Mapping for Systematic Vulnerability Elicitation
Structured prompts derived from MITRE CWE examples elicit vulnerable code patterns from LLMs in a reproducible manner. The authors analyze CWE code examples from MITRE, distill them into natural language prompts (e.g., "ask the user → password → store in a buffer → clear the buffer"), and use these prompts to generate code that replicates the root vulnerability pattern. Core assumption: LLMs trained on public code corpora have internalized vulnerable patterns present in real-world codebases and will reproduce them when prompted without explicit security guidance. Evidence: CWE-120 (buffer copy without size check) appeared frequently across models, with Mistral-7B showing 7 occurrences in CG and 9 in SCG. Break condition: If LLMs refuse to generate code for security-sensitive prompts (as observed with Llama2-7B SCG refusing 22/84 prompts), the mechanism produces false negatives rather than eliciting vulnerabilities.

### Mechanism 2: Multi-Tool SAST Overlap for Broader Vulnerability Coverage
Using multiple static analysis tools with complementary rule sets increases detection coverage of LLM-generated vulnerabilities. CodeQL (804 CWE coverage), Snyk Code (31 C/C++ rules), and CodeShield (23 LLM-focused rules) detect different vulnerability categories. The authors aggregate SARIF outputs via SarifMiner to identify overlapping and unique CWEs. Core assumption: Each SAST tool's rule set captures distinct vulnerability patterns, and false positives are acceptable tradeoffs for comprehensive detection. Evidence: CodeQL identified 33 unique CWEs. Snyk Code and CodeShield had narrower coverage, detecting 10 and 1 unique CWEs, respectively... there was no overlap between Snyk Code and CodeShield, and no single common CWE was detected by all three tools. Break condition: If tools flag non-exploitable patterns as vulnerabilities (false positives), security posture assessments become inflated. The paper does not report manual validation of SAST findings.

### Mechanism 3: System Prompt Security Directive with Limited Effectiveness
Explicit security-focused system prompts (SCG) do not consistently reduce vulnerability generation compared to baseline prompts (CG). Two assistants per model—CG ("You are a helpful assistant") and SCG ("You're really good at keeping things safe online")—generate code for identical prompts. Vulnerability counts are compared to isolate the effect of security framing. Core assumption: Security-focused system prompts activate latent security knowledge in LLMs without additional fine-tuning or few-shot examples. Evidence: Mistral-7B SCG showed CWE-120 increasing from 7 (CG) to 9 (SCG); CodeGemma-7B SCG showed CWE-119 increasing from 8 to 16. CodeShield analysis: "phi3-3.8B's CG codebase had zero CWEs, but surprisingly, its SCG codebase had 2." Break condition: If models interpret security directives as refusal-to-generate signals (Llama2-7B SCG generated only 74% of requested files), the comparison conflates reduced output with improved security.

## Foundational Learning

- **Common Weakness Enumeration (CWE) and CVE Mapping**: Why needed here: The entire evaluation framework depends on understanding CWE taxonomy (weakness types) and CVE mapping (real-world exploit frequency). Without this, you cannot interpret Table 4's criticality rankings (e.g., CWE-119 with 11,480 CVEs vs. CWE-14 with 0). Quick check question: Given CWE-787 with 10,462 associated CVEs, what does this suggest about its criticality in LLM-generated code?

- **Static Application Security Testing (SAST) Limitations**: Why needed here: Results differ significantly across CodeQL, Snyk Code, and CodeShield. Understanding that SAST tools produce false positives/negatives and have language-specific rule coverage is essential for interpreting Figure 8's limited overlap. Quick check question: Why might CodeQL detect 37 unique CWEs in CG while Snyk Code detects only 10 for the same codebase?

- **LLM Code Generation Context Windows and Refusal Behavior**: Why needed here: Models like Llama2-7B SCG refuse to generate code for 22/84 prompts, citing security concerns. Understanding alignment training and refusal mechanisms explains why "secure assistant" sometimes means "non-generating assistant." Quick check question: If a model refuses to generate code for a prompt, should this be counted as "secure" or "untested"?

## Architecture Onboarding

- **Component map**: MITRE CWE Database → Prompt Engineering (84 prompts) → LLM Generation (10 models × 2 modes) → SAST Tools (CodeQL/Snyk/CodeShield) → SARIF Reports → SarifMiner → NVD CVE Mapping → Analysis

- **Critical path**: Prompt quality and SAST tool configuration determine vulnerability detection coverage. The SarifMiner aggregation step is the bottleneck for cross-tool analysis.

- **Design tradeoffs**:
  - **CodeQL compilation requirement**: Only successfully compiled C/C++ files are analyzed. Gemini, Llama2, and CodeLlama had highest non-compilable file rates, reducing effective sample size.
  - **Tool coverage vs. precision**: CodeQL has broadest coverage (804 CWEs) but may produce more false positives; CodeShield has narrowest (23 rules) but focuses on LLM-specific patterns.
  - **SCG directive simplicity**: The secure prompt ("You're really good at keeping things safe online") is intentionally minimal; stronger prompts or few-shot examples might improve results but would complicate controlled comparison.

- **Failure signatures**:
  - **Non-compilation**: Files with empty content, syntax errors, or unsupported libraries fail CodeQL database creation (see `compile_errors.log` references).
  - **No-code generation**: Models return explanations or refusals instead of code (Table 5). This is more common in SCG mode.
  - **SCG vulnerability inflation**: Secure mode shows higher CWE counts for some models (e.g., Mistral-7B CWE-120: 7→9), suggesting security directives may alter code structure without improving security.

- **First 3 experiments**:
  1. **Reproduce baseline with single model**: Select one model (e.g., GPT-3.5), run the 84 prompts from the GitHub dataset, and verify CodeQL vulnerability counts match Table 1 (28 vulnerable files for CG, 24 for SCG). This validates your SAST pipeline.
  2. **Ablate security directive strength**: Replace the SCG prompt with explicit few-shot examples of secure C/C++ patterns (e.g., bounded string operations, input validation). Compare vulnerability rates to baseline SCG.
  3. **Validate SAST findings via manual review**: Randomly sample 10 CWE-787 or CWE-120 detections from CodeQL reports and manually verify they are true positives exploitable in a real execution context. Document false positive rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do advanced prompting strategies, specifically zero-shot, few-shot, and Chain-of-Thought (CoT) prompting, impact the security and functional correctness of generated C/C++ code compared to the simple system prompts used in this study?
- Basis in paper: [explicit] The "Future Work" section explicitly calls for investigating the impact of these specific prompting techniques on code quality and security.
- Why unresolved: The current study only utilized a basic "helpful assistant" and a "secure assistant" system prompt without exploring complex prompting paradigms.
- What evidence would resolve it: A comparative study measuring CWE density and code validity across the same models using these advanced prompting techniques.

### Open Question 2
- Question: Can an iterative prompt refinement framework effectively enable LLMs to identify and self-correct insecure code, thereby reducing vulnerability density?
- Basis in paper: [explicit] The authors propose developing a comprehensive prompt framework that allows models to iteratively refine insecure code to evaluate self-correction capabilities.
- Why unresolved: The current methodology treats code generation as a single-turn interaction (prompt -> code -> analysis) without testing the model's ability to fix its own errors based on feedback.
- What evidence would resolve it: Experiments where models are fed their own static analysis reports and prompted to patch the code, followed by a re-evaluation of vulnerability counts.

### Open Question 3
- Question: Do the vulnerability patterns and security weaknesses identified in C/C++ code generation persist in languages with different memory safety models, such as Rust or Go?
- Basis in paper: [explicit] The "Future Work" section suggests extending the study to include additional programming languages to evaluate how newer LLM versions perform across diverse ecosystems.
- Why unresolved: The study is strictly limited to C/C++, which is prone to specific memory-related vulnerabilities (e.g., buffer overflows) that may not apply to other languages.
- What evidence would resolve it: Applying the same prompt dataset and static analysis methodology to models generating code in memory-safe languages and comparing the resulting CWE distributions.

### Open Question 4
- Question: How do dynamic analysis (DAST) or runtime testing results compare to the static analysis (SAST) findings used in this paper regarding the detection of vulnerabilities in LLM-generated code?
- Basis in paper: [inferred] The "Threats to Validity" section notes that the study relied entirely on static analysis tools (CodeQL, Snyk, CodeShield), which are constrained by their specific rule sets and may miss vulnerabilities.
- Why unresolved: Static analysis tools can produce false positives and false negatives; runtime behavior of the generated code remains unverified.
- What evidence would resolve it: Running the generated codebases in a sandboxed environment or fuzzer to confirm exploitability of the detected CWEs and identify missed runtime errors.

## Limitations
- Heavy reliance on SAST tools may produce significant false positives without manual validation of detected vulnerabilities
- Model refusal behavior in SCG mode confounds security assessment with reduced output rather than improved code quality
- No ground truth validation of SAST findings limits confidence in actual security posture of LLM-generated code

## Confidence
- **High confidence**: CWE-to-prompt mapping mechanism (systematic and reproducible as demonstrated by consistent vulnerability patterns across models)
- **Medium confidence**: Multi-tool SAST overlap finding (robust methodology but unverified ground truth for detected vulnerabilities)
- **Low confidence**: System prompt security directive effectiveness (counterintuitive results where SCG sometimes increased vulnerabilities, with no manual validation)

## Next Checks
1. **Manual validation of SAST findings**: Select 20 randomly sampled CWE-787 and CWE-120 detections from CodeQL reports and verify through code review whether they represent truly exploitable vulnerabilities or false positives.
2. **Controlled prompt ablation study**: Replace the minimal SCG prompt with explicit few-shot examples of secure coding patterns (bounded string operations, input validation) and compare vulnerability rates to baseline CG/SCG conditions.
3. **Threshold sensitivity analysis**: Vary SAST tool thresholds and configuration parameters to quantify how many detected vulnerabilities are false positives versus true security issues.