---
ver: rpa2
title: 'BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian
  Personal Income Tax Law, including case law'
arxiv_id: '2505.15916'
source_url: https://arxiv.org/abs/2505.15916
tags:
- legal
- questions
- answers
- dataset
- references
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BR-TaxQA-R, a dataset for question answering
  with references in Brazilian personal income tax law. It combines 715 official tax
  questions, statutory regulations, and CARF case law rulings.
---

# BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law

## Quick Facts
- **arXiv ID:** 2505.15916
- **Source URL:** https://arxiv.org/abs/2505.15916
- **Reference count:** 22
- **Key outcome:** Custom RAG pipeline outperforms commercial tools in Response Relevancy (0.829) but lags in Factual Correctness; highlights trade-off between legal traceability and fluency

## Executive Summary
BR-TaxQA-R introduces a dataset for question answering with references in Brazilian personal income tax law, combining 715 official IRS questions with statutory regulations and CARF case law rulings. The authors implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings and GPT-4o-mini, comparing sliding-window and hierarchical segmentation strategies. Their custom RAG system outperforms commercial tools like ChatGPT and Perplexity.ai in response relevancy, while commercial models lead in factual correctness and fluency. The study highlights a trade-off between legal traceability and linguistic fluency, emphasizing the need for human expert evaluation to ensure legal validity in high-stakes domains.

## Method Summary
The BR-TaxQA-R dataset combines 715 Brazilian IRS (RFB) questions with 478 statutory documents and 7,204 CARF case law rulings from 2023. The RAG pipeline uses OpenAI text-embedding-3-small for dense retrieval with FAISS IndexFlatL2, comparing sliding-window segmentation (2048-token windows, 1024-token stride) against Langchain Recursive Character Text Splitter with legal separators. GPT-4o-mini generates answers with Chain-of-Thought prompting and citation constraints. The system is evaluated using RAGAS metrics (Response Relevancy, Factual Correctness, Semantic Similarity) plus BLEU and ROUGE-L.

## Key Results
- Custom RAG pipeline achieves highest Response Relevancy (0.829) with sliding-window segmentation plus case law
- Commercial tools lead in Factual Correctness (Perplexity.ai: 0.469) and Semantic Similarity (ChatGPT: 0.793)
- Trade-off exists between legally grounded generation and conversational fluency
- Case law contributes +0.016 to Response Relevancy but adds significant indexing complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sliding-window segmentation combined with jurisprudential case law improves query-answer alignment in legal domains.
- Mechanism: 2048-token overlapping windows (1024-token stride) preserve contextual boundaries that might otherwise be severed mid-provision. Adding CARF administrative rulings provides concrete application examples that bridge abstract statutory language and real-world tax scenarios, helping models interpret regulatory intent.
- Core assumption: Legal questions benefit from both normative text and real-world interpretive precedents; semantic similarity alone is insufficient for legal relevance.
- Evidence anchors:
  - [abstract] "We compare different text segmentation strategies...Results show that our custom RAG pipeline outperforms commercial systems in Response Relevancy"
  - [section 5] "sliding-window segmentation with case law achieved the highest score in Response Relevancy (0.829), outperforming all other systems, including commercial tools"
  - [corpus] MetaGen Blended RAG and AccurateRAG papers corroborate RAG effectiveness for specialized domain QA; corpus evidence on segmentation-specific legal effects is limited
- Break condition: When case law documents are temporally stale (no longer align with current interpretations) or when statutory language has no relevant jurisprudence coverage.

### Mechanism 2
- Claim: Domain-specific prompt engineering with citation constraints produces legally traceable answers at the cost of conversational fluency.
- Mechanism: Prompts explicitly constrain generation to retrieved context only, require standardized citation formatting (e.g., "Decreto Nº 9.580" not "RIR/2018"), and trigger fallback responses when context is insufficient. Chain-of-thought prompting guides intermediate reasoning before final answers.
- Core assumption: Legal adequacy requires explicit grounding in authoritative sources, even when this reduces linguistic naturalness.
- Evidence anchors:
  - [abstract] "trade-off between legally grounded generation and linguistic fluency"
  - [section 4.1] "responses derived solely from provided context...Include citations of applicable legal sources (norms and articles only) at the end"
  - [corpus] Attribution Techniques survey confirms citation-based approaches mitigate hallucination in RAG systems
- Break condition: When retrieved context lacks sufficient coverage for the query, triggering fallback; or when prompt constraints cause overly terse responses that omit necessary explanation.

### Mechanism 3
- Claim: Dense retrieval with hierarchy-aware document segmentation balances retrieval recall and precision for structured legal corpora.
- Mechanism: OpenAI text-embedding-3-small generates dense vectors; FAISS IndexFlatL2 performs L2 nearest-neighbor search. Recursive character splitter uses legal document hierarchy (articles, paragraphs, sections) as splitting boundaries, preserving structural coherence per chunk.
- Core assumption: Semantic similarity in embedding space correlates with legal relevance; structure-aware splitting produces more meaningful retrieval units.
- Evidence anchors:
  - [abstract] "RAG pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation"
  - [section 4.1] "customized separators list, including the documents containing statutory law hierarchy with many splitting points"
  - [corpus] eSapiens and Metadata-Driven RAG papers demonstrate similar hybrid retrieval approaches; direct evidence on hierarchy-aware splitting efficacy in this paper is observational, not ablated
- Break condition: When legal concepts exhibit high semantic variability across documents (same concept, different phrasing) causing embedding mismatch.

## Foundational Learning

- **Dense Passage Retrieval (DPR)**
  - Why needed here: Core retrieval mechanism embedding queries and document chunks for similarity-based search.
  - Quick check question: Why might L2 distance on embeddings miss legally relevant passages that use different terminology for the same concept?

- **Chunking Strategies for Legal Documents**
  - Why needed here: Determines how statutes and rulings are segmented before indexing; poor splitting severs provisions mid-clause.
  - Quick check question: What is the trade-off between sliding-window overlap and storage/indexing costs in production?

- **RAGAS Evaluation Metrics**
  - Why needed here: Automated framework for measuring Response Relevancy, Factual Correctness, and Semantic Similarity against ground truth.
  - Quick check question: Why might high Semantic Similarity scores fail to capture legal adequacy?

## Architecture Onboarding

- **Component map:**
  Data Layer: 478 statutory documents + 7,204 CARF rulings → preprocessed text
  Segmentation Layer: Sliding-window (2048/1024) OR recursive splitter with legal separators
  Indexing Layer: OpenAI text-embedding-3-small → FAISS IndexFlatL2
  Retrieval Layer: Query embedding → top-k chunk retrieval
  Generation Layer: GPT-4o-mini + legal prompting → answer + structured citations

- **Critical path:** Document preprocessing → Segmentation → Embedding generation → FAISS indexing → Query retrieval → Context assembly → Prompt construction → Generation → Citation extraction

- **Design tradeoffs:**
  Sliding-window vs. recursive: Overlap preserves context but increases index size ~2x
  With vs. without case law: +0.016 Response Relevancy gain but +7,204 documents to index
  Constrained vs. free-form prompting: Traceability vs. fluency (ChatGPT scored 0.793 Semantic Similarity vs. 0.768 for best RAG)

- **Failure signatures:**
  Fallback response triggered ("system still learning") → retrieval returned insufficient context
  Missing/hallucinated citations → prompt constraints violated or context gap
  High Relevancy + low Factual Correctness → retrieved chunks aligned with query but not ground truth
  Overly terse legalistic responses → prompt over-constraining natural language generation

- **First 3 experiments:**
  1. Replicate sliding-window + case law baseline on 50-question subset; measure Response Relevancy and Factual Correctness to validate reported 0.829/0.327 scores.
  2. Ablate case law component (source documents only); quantify jurisprudence contribution to Response Relevancy delta.
  3. Test recursive segmentation with custom legal separators against sliding-window; evaluate whether hierarchy-aware chunking improves Factual Correctness without degrading Relevancy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating multi-year CARF decisions (beyond 2023) improve RAG system performance on tax law question answering?
- Basis in paper: [explicit] "Only 2023 rulings were processed, with previous years potentially added in a future dataset release" and "Future work includes incorporating multi-year CARF decisions"
- Why unresolved: The current dataset is limited to a single year of jurisprudence; the impact of temporal breadth remains untested.
- What evidence would resolve it: Comparative evaluation of RAG performance when the case law set is expanded to include rulings from 2019–2023 versus the 2023-only baseline.

### Open Question 2
- Question: What human-in-the-loop evaluation protocols can validly capture legal adequacy, traceability, and user trust in tax QA systems?
- Basis in paper: [explicit] "We also aim to refine human-in-the-loop evaluation protocols to better capture legal adequacy, traceability, and user trust"
- Why unresolved: Automated metrics (BLEU, ROUGE, semantic similarity) do not guarantee legal validity, but no validated protocol yet exists for this domain.
- What evidence would resolve it: A standardized human evaluation framework with documented inter-annotator agreement and correlation with expert legal assessments.

### Open Question 3
- Question: Can a single RAG architecture simultaneously achieve both high response relevancy (>0.82) and high factual correctness (>0.45) on BR-TaxQA-R?
- Basis in paper: [inferred] Results show a trade-off: custom RAG excels at Response Relevancy (0.829) while commercial tools lead in Factual Correctness (Perplexity.ai: 0.469). The paper notes this as "a trade-off between legally grounded generation and linguistic fluency."
- Why unresolved: No evaluated system dominates both dimensions; it is unclear whether the trade-off is fundamental or can be overcome with improved retrieval or generation strategies.
- What evidence would resolve it: A model achieving scores above both thresholds on the same test set would demonstrate that the trade-off is not inherent.

## Limitations
- Limited evaluation scope: Expert legal review was not conducted due to resource constraints, leaving factual correctness and legal adequacy unverified by domain specialists
- Temporal sensitivity: CARF case law from 2023 may not reflect current interpretations, creating potential accuracy decay over time
- Commercial system comparison: ChatGPT and Perplexity.ai results may not generalize to other models or updated versions

## Confidence
- **High Confidence:** Sliding-window segmentation with case law improves Response Relevancy (0.829 score) compared to other RAG approaches
- **Medium Confidence:** Custom RAG pipeline outperforms commercial tools in Response Relevancy while commercial models lead in Semantic Similarity and Factual Correctness
- **Low Confidence:** Claim that legal traceability inherently trades off with conversational fluency without comprehensive human evaluation

## Next Checks
1. Conduct expert legal review of 50 sample responses to validate Factual Correctness scores and identify hallucination patterns in citations
2. Test retrieval performance with updated CARF case law (2024) to quantify temporal degradation in legal relevance
3. Implement ablation study comparing hierarchy-aware recursive segmentation against sliding-window approach across multiple legal domains to validate structural coherence benefits