---
ver: rpa2
title: 'Survey and Experiments on Mental Disorder Detection via Social Media: From
  Large Language Models and RAG to Agents'
arxiv_id: '2504.02800'
source_url: https://arxiv.org/abs/2504.02800
tags:
- social
- mental
- media
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically examines the use of large language models
  (LLMs), retrieval-augmented generation (RAG), and agentic systems for mental disorder
  detection on social media. The authors review existing literature across internalizing,
  psychotic, and externalizing disorders, highlighting the limitations of traditional
  methods and the potential of LLMs for semantic understanding and reasoning.
---

# Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents

## Quick Facts
- arXiv ID: 2504.02800
- Source URL: https://arxiv.org/abs/2504.02800
- Reference count: 40
- Primary result: LLMs excel at semantic understanding for mental health detection, but RAG improves long-text tasks while degrading short-text performance; agentic systems show promise for complex multi-step reasoning.

## Executive Summary
This survey systematically examines the use of large language models (LLMs), retrieval-augmented generation (RAG), and agentic systems for mental disorder detection on social media. The authors review existing literature across internalizing, psychotic, and externalizing disorders, highlighting the limitations of traditional methods and the potential of LLMs for semantic understanding and reasoning. They introduce a unified benchmarking framework and conduct empirical studies evaluating seven LLMs (e.g., GPT-4o, DeepSeek, Llama variants) on six datasets. Results show that while LLMs excel at understanding complex text, their direct application to challenging classification tasks remains suboptimal, with performance varying significantly by task complexity, dataset length, and model architecture. RAG techniques improve accuracy for long-text tasks but may degrade performance for short, sparse posts. The survey identifies future research directions, including multimodal integration, RAG-driven clinical interpretability, and autonomous agentic reasoning.

## Method Summary
The study evaluates seven large language models (GPT-4o, DeepSeek, Llama variants) on six benchmark datasets for mental disorder detection including CSSRS-Suicide, STRD, SDCNL, DepSeverity, Dreaddit, and SWMH. The authors compare zero-shot, few-shot, and RAG-enhanced prompting strategies using balanced accuracy and weighted F1-score as primary metrics. RAG implementations include both chunk-based NaiveRAG and graph-based GraphRAG using LightRAG and HippoRAG frameworks. Knowledge bases are constructed from clinical literature using Qwen3-32B for extraction and bge-m3 embeddings. The survey also introduces a unified benchmarking framework and conducts ablation studies on retrieval techniques and model architectures.

## Key Results
- LLMs demonstrate strong semantic understanding but struggle with fine-grained multi-class classification tasks
- RAG consistently degrades performance on short, sparse social media posts but improves accuracy for long-text narratives
- Graph-based RAG outperforms chunk-based retrieval by preserving symptom relationships and enabling differential diagnosis
- Few-shot prompting can introduce bias and degrade performance on complex multi-class tasks compared to zero-shot
- Model performance varies significantly by task complexity, dataset length, and specific LLM architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-Augmented Generation (RAG) enhances detection accuracy only when input text is sufficiently long and context-rich; it degrades performance on short, sparse posts.
- **Mechanism:** RAG retrieves external clinical knowledge (e.g., DSM-5 criteria) to augment the prompt. In long narratives, this provides necessary "signal density" to align symptoms with definitions. In short posts, the retrieved information lacks anchoring points in the input, acting as noise that disrupts the model's reasoning.
- **Core assumption:** The LLM has sufficient context window and attention mechanism to reconcile the user's historical posts with the retrieved clinical snippet.
- **Evidence anchors:** [abstract] Mentions RAG limitations and the need to address reliability. [section V-D] "On the other six datasets, all tested RAG variants led to a discernible degradation... brevity... leads to sparse signals that hinder symptom identification."

### Mechanism 2
- **Claim:** Graph-based RAG (GraphRAG) outperforms chunk-based RAG by preserving semantic relationships between symptoms and diagnostic criteria.
- **Mechanism:** Unlike chunk-based retrieval which isolates text fragments, GraphRAG traverses a knowledge graph to capture symptom interrelations (e.g., linking "insomnia" to both anxiety and depression definitions). This allows the model to perform differential diagnosis rather than simple keyword matching.
- **Core assumption:** The knowledge graph construction accurately extracts entities and relations from the clinical corpus without significant information loss.
- **Evidence anchors:** [section V-D] "Graph-based RAG methods outperform traditional chunk-based RAG... [GraphRAG] can traverse nodes and relations to retrieve... a more structured, holistic context."

### Mechanism 3
- **Claim:** Agentic systems enable autonomous handling of complex, multi-step diagnostic workflows that static LLMs cannot manage.
- **Mechanism:** By decomposing complex user timelines or multimodal inputs into sub-tasks (planning), maintaining state (memory), and using tools (retrieval), agents can model the progression of mental states over time rather than classifying isolated posts.
- **Core assumption:** The specific clinical guidelines and safety protocols (e.g., crisis detection routines) can be effectively encoded into the agent's toolset or system prompt.
- **Evidence anchors:** [abstract] "...agentic systems for autonomous reasoning and multi-step intervention." [section II-E] "Agentic systems... can maintain state and memory... enabling them to decompose complex user goals... into a series of manageable subtasks."

## Foundational Learning

- **Concept:** Balanced Accuracy vs. Weighted F1-Score
  - **Why needed here:** Mental health datasets are inherently imbalanced (e.g., far fewer "severe" cases than "moderate"). Standard accuracy is misleading.
  - **Quick check question:** If a model predicts "healthy" for 95% of users in a dataset with 5% depression, what is the Accuracy vs. the Balanced Accuracy?

- **Concept:** Zero-shot vs. Few-shot Complexity Trap
  - **Why needed here:** Intuition suggests more examples (few-shot) always help. This paper proves few-shot can inject bias and degrade performance on fine-grained, subjective multi-class tasks.
  - **Quick check question:** Why did few-shot prompting improve Binary classification but degrade performance on the 4-level Depression Severity task?

- **Concept:** Signal Density in RAG
  - **Why needed here:** To diagnose when RAG is "poisoning" the prompt versus helping it.
  - **Quick check question:** Given a 10-word Tweet and a retrieved 500-word clinical document, why might the model fail to associate the two?

## Architecture Onboarding

- **Component map:** User Historical Posts -> Retriever (Vector Database/Knowledge Graph) -> Reasoner (LLM) -> Diagnosis Label and Explanation
- **Critical path:**
  1. Tokenization & Length Check: If tokens < Threshold X, skip RAG to avoid noise
  2. Retrieval: If tokens > Threshold X, perform Graph-based retrieval to get structured symptom context
  3. Reasoning: LLM synthesizes the input and context to output a JSON object with severity and evidence

- **Design tradeoffs:**
  - ChunkRAG vs. GraphRAG: ChunkRAG is easier to implement but loses semantic links; GraphRAG requires upfront entity extraction cost but yields better differential diagnosis
  - General LLM vs. Reasoning Model: Reasoning models (DeepSeek-R1) excel at complex multi-class logic but may be slower/overkill for simple binary stress detection

- **Failure signatures:**
  - "Noise Injection": Sudden drop in Balanced Accuracy on short-text datasets after enabling RAG
  - "Bias Locking": Model strictly adheres to few-shot example phrasing, ignoring nuanced user input in multi-class tasks

- **First 3 experiments:**
  1. Length Threshold Analysis: Run RAG vs. Zero-shot on the same dataset while progressively truncating input tokens to find the exact "break point" where RAG stops helping
  2. Symptom Overlap Ablation: Construct a dataset with high symptom overlap and compare ChunkRAG vs. GraphRAG accuracy to verify the "differentiation" mechanism
  3. Few-shot Stability Test: Vary the random seed for few-shot example selection on the DepSeverity dataset to measure variance

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical findings are based on a limited set of six public datasets, which may not fully represent real-world social media mental health data diversity
- The study focuses primarily on English-language data and text-only inputs, leaving questions about multilingual and multimodal performance unanswered
- The survey does not address potential biases in LLMs trained on web data that may affect mental health detection

## Confidence

**High Confidence:** The general superiority of LLMs over traditional ML methods for semantic understanding in mental health detection. The observation that few-shot prompting can introduce bias and degrade performance on complex multi-class tasks. The finding that RAG consistently degrades performance on short, sparse social media posts.

**Medium Confidence:** The specific performance rankings of different LLM architectures across all six datasets. The magnitude of improvement from GraphRAG over chunk-based RAG. The claim that reasoning models (DeepSeek-R1) consistently outperform standard LLMs on multi-class classification tasks.

**Low Confidence:** The survey's predictions about agentic systems' future performance in autonomous mental health detection, as these remain largely theoretical with limited empirical validation. The optimal token length threshold for triggering RAG is not empirically determined but rather suggested based on observed patterns.

## Next Checks
1. Cross-dataset Generalization Test: Validate the survey's findings by testing the same LLM+RAG pipeline on a new, independent mental health dataset to determine if the observed performance patterns hold across different data distributions.

2. Knowledge Base Ablation Study: Systematically vary the composition and quality of the clinical knowledge base used for RAG to quantify how knowledge base quality affects detection accuracy and identify which types of clinical information provide the most value.

3. Multimodal Extension Validation: Replicate the survey's methodology on a multimodal dataset to test whether the identified mechanisms (signal density thresholds, GraphRAG advantages) extend to multimodal contexts where visual cues may provide additional diagnostic information.