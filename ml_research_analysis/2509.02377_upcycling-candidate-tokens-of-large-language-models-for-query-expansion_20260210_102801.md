---
ver: rpa2
title: Upcycling Candidate Tokens of Large Language Models for Query Expansion
arxiv_id: '2509.02377'
source_url: https://arxiv.org/abs/2509.02377
tags:
- query
- ctqe
- retrieval
- tokens
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of query expansion in information
  retrieval, where the goal is to improve search performance by enriching user queries
  with related terms. While large language models (LLMs) have shown promise for query
  expansion, existing methods face a trade-off between generating diverse terms and
  computational cost.
---

# Upcycling Candidate Tokens of Large Language Models for Query Expansion

## Quick Facts
- arXiv ID: 2509.02377
- Source URL: https://arxiv.org/abs/2509.02377
- Reference count: 35
- Key outcome: Proposes CTQE method that achieves competitive query expansion performance with significantly lower computational cost by leveraging unselected LLM candidate tokens

## Executive Summary
This paper addresses query expansion in information retrieval by introducing Candidate Token Query Expansion (CTQE), a method that extracts valuable expansion signals from unselected candidate tokens during LLM decoding. Unlike existing approaches that require multiple LLM calls or expensive inference, CTQE achieves both relevance and diversity without additional computational cost by utilizing the top-k candidate tokens already computed during autoregressive generation. The method demonstrates strong performance across 10 benchmarks, particularly excelling on low-resource datasets while maintaining efficiency.

## Method Summary
CTQE leverages unselected candidate tokens from LLM decoding passes to expand user queries. During autoregressive generation, LLMs compute probability distributions over vocabulary at each step, and CTQE extracts the top-k=20 unselected tokens, filtering them to first-position candidates only, removing duplicates and tokens shorter than 2 characters. These candidate tokens are then used as expansion signals alongside generated keywords, with scores interpolated (α=0.9 for BM25) to balance relevance preservation with expansion benefits. The method integrates seamlessly with both sparse (BM25) and dense (BGE) retrievers, achieving competitive performance with significantly lower computational cost than multi-pass approaches.

## Key Results
- CTQE outperforms existing keyword-based query expansion methods (Q2K) across 10 benchmarks
- Achieves up to 65.0 NDCG@10 on TREC19 with minimal token usage (16 tokens)
- Particularly effective on low-resource datasets while maintaining efficiency
- Competes with more expensive approaches like Q2D while using fewer tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Candidate tokens from LLM decoding capture query-relevant semantic signals without additional inference cost.
- Mechanism: During autoregressive decoding, LLMs compute full vocabulary distributions at each step. Unselected top-k tokens are already conditioned on query context through attention mechanism, representing semantically plausible alternatives the model considered.
- Core assumption: Alternative tokens ranked highly by LLM reflect genuine semantic relatedness to query, not just syntactic fluency or positional bias.
- Evidence anchors: [abstract] "These tokens, though not part of the final output, are conditioned on the full query and capture useful information." [section 2.2] "At each position j of a keyword wi, the LLM evaluates multiple alternatives before choosing the final token wi,j."

### Mechanism 2
- Claim: First-position candidate tokens provide higher lexical diversity than subsequent positions because they are less conditioned on preceding token choices.
- Mechanism: At position j > 1, candidate tokens are conditioned on specific first token wi,1 selected, constraining semantic space. First-position candidates branch directly from query context before any token commitment, yielding broader semantic coverage.
- Core assumption: Diversity at first token position translates to semantically distinct expansion terms rather than morphological variants or synonyms.
- Evidence anchors: [section 2.2] "Since candidate tokens in C′i,j for j > 1 are strongly conditioned on the first token wi,1, they tend to offer limited semantic diversity." [figure 2 center] "Dedup + 1st pos" yields substantial gains over using all candidates or deduplication alone.

### Mechanism 3
- Claim: Score interpolation between keyword-expanded queries and candidate-token signals balances relevance preservation with expansion benefits.
- Mechanism: Rather than concatenating all signals, CTQE maintains separate scoring channels—Sexpan for keyword-augmented retrieval and SC for candidate tokens—then interpolates with α weighting. This prevents noisy candidates from overwhelming precise query signals.
- Core assumption: Candidate tokens provide complementary rather than redundant signals to generated keywords; optimal α is stable across query types.
- Evidence anchors: [section 2.2] "The final CTQE score is computed by interpolating the two signals: SCTQE(d) = α · Sexpan(d) + (1 − α) · SC(d)" [table 1] CTQE with α = 0.9 consistently outperforms Q2K across 10 datasets.

## Foundational Learning

- Concept: Autoregressive decoding and logit distributions
  - Why needed here: CTQE relies on understanding that LLMs compute full vocabulary distributions at each step, not just output token. Without this, "candidate token" concept is opaque.
  - Quick check question: When an LLM outputs "cat", what information about alternative tokens was computed but discarded?

- Concept: Lexical vs. semantic retrieval paradigms
  - Why needed here: CTQE applies to BM25 (lexical), SPLADE (learned sparse), and BGE (dense), each requiring different integration strategies.
  - Quick check question: Why would expanding query with keywords help BM25 but potentially hurt dense retriever if naively concatenated?

- Concept: Pseudo-relevance feedback (PRF)
  - Why needed here: CTQE compared against RM3 and PRF variants; understanding why LLM-based expansion differs from document-based feedback clarifies method's contribution.
  - Quick check question: What information source does RM3 use that CTQE does not require?

## Architecture Onboarding

- Component map: Query → LLM → keywords + candidate tokens → Filter Module → Dual-Index Retrieval → Score Fusion → Final ranking
- Critical path: Query → LLM → keywords + candidate tokens (single forward pass) → Candidates → first-position filter → deduplication → C → Original query + keywords → Sexpan; C → SC → Score interpolation → final ranking
- Design tradeoffs:
  - k selection: Higher k increases diversity but risks noise; paper uses k=20 (API maximum), but optimal k may vary by domain
  - α tuning: 0.9 preserves query signal strength; lower α helps when queries are underspecified but risks drift
  - First-position heuristic: Reduces noise but discards potentially useful continuation candidates; ablation shows this is critical
- Failure signatures:
  1. Performance drops below Q2K baseline: Check if candidate token filtering is applied; "All" configuration in ablation underperforms Q2K
  2. Minimal improvement on high-resource datasets: Expected per table 1; CTQE excels on low-resource domains where candidate tokens capture domain vocabulary
  3. Latency exceeds Q2K despite single pass: Likely issue in dual-index construction or score fusion implementation
- First 3 experiments:
  1. Reproduce filtering ablation: Run CTQE with "All", "Dedup", and "Dedup + 1st pos" configurations on single BEIR dataset to validate filtering pipeline
  2. Sensitivity sweep on α: Test α ∈ {0.7, 0.8, 0.9, 0.95} on TREC19 to understand robustness
  3. Token budget comparison: Compare CTQE (16 tokens) vs. Q2D (16, 32, 64, 128 tokens) on latency and NDCG@10 to reproduce efficiency frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CTQE performance scale with access to more than top-20 candidate tokens per decoding step?
- Basis in paper: [inferred] Authors note they set k=20 because this is "maximum provided by OpenAI API," suggesting external constraint rather than theoretically motivated choice.
- Why unresolved: Relationship between candidate token pool size and retrieval effectiveness remains unexplored; larger k values might capture additional relevant terms or introduce noise.
- What evidence would resolve it: Experiments using open-source LLMs (e.g., LLaMA) where full vocabulary distributions are accessible, testing k values such as 50, 100, or complete vocabulary.

### Open Question 2
- Question: Can candidate tokens from positions beyond first (j > 1) provide value under alternative filtering or weighting schemes?
- Basis in paper: [inferred] Paper states tokens at j > 1 "tend to offer limited semantic diversity" and are discarded, but this claim supported only by ablation showing first-position tokens work well, not that later-position tokens are universally useless.
- Why unresolved: Later-position tokens may contain useful n-gram completions or domain-specific terminology; current filtering may be overly aggressive.
- What evidence would resolve it: Systematic analysis of second-position and later candidate tokens, potentially with probability-weighted scoring or contextual filtering based on generated keyword prefixes.

### Open Question 3
- Question: How robust is CTQE across multilingual and cross-lingual retrieval scenarios?
- Basis in paper: [inferred] All experiments are conducted on English datasets (TREC DL, BEIR); no multilingual evaluation is mentioned despite method's potential applicability to any language supported by LLM.
- Why unresolved: Tokenization differences, vocabulary sizes, and LLM capability variations across languages may affect candidate token quality and filtering effectiveness.
- What evidence would resolve it: Evaluation on multilingual benchmarks (e.g., CLEF, Mr. TyDi, or BEIR's multilingual extensions) using multilingual LLMs.

## Limitations
- Token filtering assumptions rely heavily on first-position candidates without theoretical grounding, which could break down for queries requiring multi-word expansions
- All experiments use English-only queries and Western-centric corpora; effectiveness on non-English languages or specialized technical domains remains unverified
- LLM API dependency means cost-effectiveness argument may not hold for alternative providers with different logprobs availability or pricing

## Confidence
- High confidence: Core claim that unselected candidate tokens contain useful expansion signals is well-supported by ablation studies; efficiency argument is directly verifiable
- Medium confidence: Filtering heuristic (first-position tokens only) is supported by ablation but lacks theoretical justification; effectiveness on low-resource datasets is supported but could benefit from more systematic experiments
- Low confidence: Superiority over more expensive methods like Q2D is based on single datapoints without statistical significance testing or comprehensive tradeoff analysis

## Next Checks
1. Replicate the filtering ablation across all 10 datasets using exact API parameters (k=20, temperature=0, prompt structure) to verify "All" configuration consistently underperforms "Dedup + 1st pos"
2. Conduct systematic sensitivity analysis on interpolation weights (α for BM25, β values for neural retrievers) across 5 diverse datasets to test whether performance degrades significantly outside reported values
3. Compare CTQE's efficiency-accuracy tradeoff against Q2D at multiple token budgets (16, 32, 64, 128) on TREC19, measuring both NDCG@10 and wall-clock latency to verify claimed efficiency frontier