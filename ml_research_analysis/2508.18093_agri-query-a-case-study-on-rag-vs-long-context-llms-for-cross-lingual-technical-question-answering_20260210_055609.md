---
ver: rpa2
title: 'Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical
  Question Answering'
arxiv_id: '2508.18093'
source_url: https://arxiv.org/abs/2508.18093
tags:
- question
- long-context
- context
- manual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a case study comparing large language models
  (LLMs) with 128K-token context windows against Retrieval-Augmented Generation (RAG)
  approaches for cross-lingual technical question answering using agricultural machinery
  manuals in English, French, and German. The study evaluates nine long-context LLMs
  using direct prompting and three RAG strategies (keyword, semantic, hybrid) with
  an LLM-as-a-judge for evaluation.
---

# Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering

## Quick Facts
- arXiv ID: 2508.18093
- Source URL: https://arxiv.org/abs/2508.18093
- Reference count: 23
- Primary result: Hybrid RAG outperforms long-context LLMs for cross-lingual technical question answering

## Executive Summary
This paper presents a comprehensive case study comparing nine long-context LLMs with retrieval-augmented generation (RAG) approaches for cross-lingual technical question answering using agricultural machinery manuals in English, French, and German. The study evaluates direct prompting versus three RAG strategies (keyword, semantic, hybrid) with an LLM-as-a-judge for evaluation. Results demonstrate that Hybrid RAG consistently outperforms direct long-context prompting, with models like Gemini 2.5 Flash and Qwen 2.5 7B achieving over 85% accuracy across all languages.

The research highlights that smaller models perform well with RAG augmentation, effectively addressing the "Lost in the Middle" effect commonly observed in long-context approaches. The study also demonstrates robust cross-lingual retrieval capabilities when using multilingual embedding models, providing practical insights for deploying technical question-answering systems in multilingual contexts.

## Method Summary
The study evaluates nine long-context LLMs (128K-token context windows) against three RAG strategies (keyword, semantic, hybrid) using agricultural machinery manuals in three languages. An LLM-as-a-judge evaluates responses, comparing direct prompting with RAG-augmented approaches. The methodology tests cross-lingual capabilities and identifies performance differences between approaches, particularly examining the "Lost in the Middle" effect in long-context models.

## Key Results
- Hybrid RAG consistently outperforms direct long-context prompting across all evaluated models
- Gemini 2.5 Flash and Qwen 2.5 7B achieve over 85% accuracy with Hybrid RAG across all three languages
- Smaller models demonstrate strong performance when augmented with RAG, addressing context window limitations
- Cross-lingual retrieval proves robust using multilingual embedding models

## Why This Works (Mechanism)
Hybrid RAG combines the strengths of both keyword-based and semantic search approaches, enabling more precise retrieval of relevant information from multilingual technical documents. The retrieval component filters and ranks relevant passages before they enter the LLM context, mitigating the "Lost in the Middle" effect where information becomes diluted or overlooked in long contexts. By providing focused, relevant content rather than overwhelming the model with entire documents, Hybrid RAG allows even smaller LLMs to generate accurate technical responses. The multilingual embedding models ensure effective cross-lingual retrieval, maintaining performance across English, French, and German manuals.

## Foundational Learning

**Long-context LLMs** - Models with extended context windows (128K tokens) that can process entire documents at once
- Why needed: To evaluate whether direct document processing can match retrieval-augmented approaches
- Quick check: Verify context window size and document chunking strategy

**RAG strategies** - Three approaches: keyword-based, semantic search, and hybrid combining both
- Why needed: To compare different information retrieval methods for technical QA
- Quick check: Test retrieval accuracy for each strategy on sample queries

**Multilingual embeddings** - Models that generate comparable vector representations across languages
- Why needed: To enable effective cross-lingual retrieval between English, French, and German documents
- Quick check: Verify embedding model supports all target languages

**LLM-as-a-judge** - Using LLMs to automatically evaluate answer quality
- Why needed: To provide scalable, consistent evaluation across large test sets
- Quick check: Compare LLM scores with human judgments on sample responses

## Architecture Onboarding

Component map: Query -> Embedding Model -> Retriever -> RAG Pipeline -> LLM -> Judge LLM

Critical path: User query → Embedding generation → Document retrieval → Context construction → Answer generation → Quality evaluation

Design tradeoffs: Long-context LLMs offer simplicity but suffer from information dilution; RAG adds complexity but provides focused, relevant context that improves accuracy, especially for smaller models.

Failure signatures: Long-context approaches show "Lost in the Middle" effect with declining performance as document length increases; RAG failures typically stem from poor retrieval quality or incorrect context selection.

First experiments:
1. Compare keyword vs. semantic retrieval accuracy on a small document set
2. Test cross-lingual retrieval performance with multilingual embeddings
3. Evaluate "Lost in the Middle" effect by varying document position of answer-relevant content

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to three languages (English, French, German) and agricultural machinery domain
- Relies on LLM-as-a-judge evaluation, which may introduce scoring biases
- Does not explore computational efficiency trade-offs between approaches

## Confidence
High: Hybrid RAG consistently outperforms long-context approaches
Medium: Cross-lingual retrieval capabilities generalize beyond tested domain
Low: Computational efficiency comparisons not provided

## Next Checks
1. Test methodology across multiple technical domains beyond agriculture to assess domain transfer capabilities
2. Implement human evaluation for a subset of responses to validate LLM-as-a-judge approach
3. Conduct latency and cost analysis comparing long-context LLMs versus RAG implementations