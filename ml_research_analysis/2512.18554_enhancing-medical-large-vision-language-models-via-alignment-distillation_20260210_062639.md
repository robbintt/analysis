---
ver: rpa2
title: Enhancing Medical Large Vision-Language Models via Alignment Distillation
arxiv_id: '2512.18554'
source_url: https://arxiv.org/abs/2512.18554
tags:
- visual
- attention
- alignment
- layer
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEDALIGN addresses the problem of hallucinations in medical large
  vision-language models (Med-LVLMs) by improving visual-text alignment. It transfers
  visual representation and attention alignment knowledge from a domain-specific expert
  CLIP model (UniMed-CLIP) to Med-LVLMs through lightweight alignment distillation.
---

# Enhancing Medical Large Vision-Language Models via Alignment Distillation

## Quick Facts
- **arXiv ID:** 2512.18554
- **Source URL:** https://arxiv.org/abs/2512.18554
- **Reference count:** 23
- **Primary result:** MEDALIGN achieves 10.31 BLEU, 29.01 ROUGE-L, 35.22 METEOR, 88.66 BERTScore, and 55.62 CheXbert scores on IU-Xray report generation, outperforming strong baselines.

## Executive Summary
MEDALIGN addresses the problem of hallucinations in medical large vision-language models (Med-LVLMs) by improving visual-text alignment. It transfers visual representation and attention alignment knowledge from a domain-specific expert CLIP model (UniMed-CLIP) to Med-LVLMs through lightweight alignment distillation. The method uses two distillation losses: a spatial-aware visual alignment loss based on similarity structures among image patches, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Applied at intermediate layers of the Med-LVLM, MEDALIGN does not require model retraining or fine-grained supervision.

Experiments on medical report generation and VQA benchmarks show consistent improvements: on IU-Xray report generation, MEDALIGN achieves 10.31 BLEU, 29.01 ROUGE-L, 35.22 METEOR, 88.66 BERTScore, and 55.62 CheXbert scores, outperforming strong baselines. On SLAKE VQA, it achieves 86.85% open-ended and 92.39% close-ended accuracy, demonstrating both enhanced performance and improved visual grounding.

## Method Summary
MEDALIGN transfers visual representation and attention alignment knowledge from a domain-specific expert CLIP model (UniMed-CLIP) to Med-LVLMs through lightweight alignment distillation. The method employs two distillation losses: a spatial-aware visual alignment loss that captures similarity structures among image patches, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. These losses are applied at intermediate layers of the Med-LVLM without requiring model retraining or fine-grained supervision.

## Key Results
- MEDALIGN achieves 10.31 BLEU, 29.01 ROUGE-L, 35.22 METEOR, 88.66 BERTScore, and 55.62 CheXbert scores on IU-Xray report generation
- On SLAKE VQA, MEDALIGN achieves 86.85% open-ended and 92.39% close-ended accuracy
- Consistent improvements across multiple medical vision-language benchmarks demonstrate enhanced performance and improved visual grounding

## Why This Works (Mechanism)
The method works by transferring domain-specific visual knowledge from UniMed-CLIP to Med-LVLMs through alignment distillation. The spatial-aware visual alignment loss captures similarity structures among image patches, while the attention-aware distillation loss guides the model to focus on diagnostically relevant regions. By applying these distillation losses at intermediate layers, MEDALIGN effectively improves visual-text alignment without requiring full model retraining or fine-grained supervision.

## Foundational Learning
- **Medical Large Vision-Language Models (Med-LVLMs):** Specialized models that process medical images and text together, needed for tasks like report generation and visual question answering in healthcare contexts.
- **CLIP Models:** Contrastive language-image pre-training models that learn joint representations, required here as expert teachers for domain-specific knowledge transfer.
- **Alignment Distillation:** Knowledge transfer technique that aligns intermediate representations between teacher and student models, enabling improved performance without full retraining.
- **Spatial-aware Visual Alignment:** Loss function that captures patch-level similarity structures in images, critical for maintaining spatial relationships in medical imaging.
- **Attention-aware Distillation:** Loss function that guides attention mechanisms toward diagnostically relevant regions, essential for reducing hallucinations in medical contexts.

## Architecture Onboarding
- **Component map:** Input images → UniMed-CLIP (teacher) → Med-LVLM (student) with alignment distillation losses → Output predictions
- **Critical path:** Image features extracted → Spatial alignment loss computed → Attention alignment loss computed → Distillation applied at intermediate layers → Final predictions generated
- **Design tradeoffs:** Lightweight distillation vs. full model retraining; spatial vs. global alignment approaches; attention guidance vs. standard training
- **Failure signatures:** Poor performance on spatial reasoning tasks; attention focusing on irrelevant regions; failure to generalize beyond radiology
- **First experiments:** 1) Test spatial alignment loss impact in isolation, 2) Evaluate attention distillation without spatial alignment, 3) Compare different intermediate layer choices for distillation application

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to other medical domains beyond radiology (e.g., pathology, dermatology) is untested
- Reliance on UniMed-CLIP as the expert teacher may limit applicability if such domain-specific CLIP models are unavailable
- No assessment of computational overhead during inference or behavior with different Med-LVLM architectures
- Lack of direct evaluation of hallucination frequency or types despite implied reduction through improved alignment

## Confidence
- **High confidence:** The reported benchmark improvements are consistent across multiple metrics and datasets, with clear superiority over strong baselines.
- **Medium confidence:** The effectiveness of the two distillation losses is supported by ablation studies, though the exact contribution of each component could benefit from further analysis.
- **Medium confidence:** The claim of improved visual grounding is supported by performance gains but lacks direct qualitative or quantitative assessment of hallucination reduction.

## Next Checks
1. Evaluate MEDALIGN on non-radiology medical domains (e.g., pathology or dermatology datasets) to assess cross-domain generalizability.
2. Conduct direct hallucination analysis comparing baseline Med-LVLMs and MEDALIGN outputs to quantify hallucination reduction.
3. Measure inference-time computational overhead and memory requirements to assess practical deployment feasibility.