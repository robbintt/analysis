---
ver: rpa2
title: 'Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic
  Interpolant Policy'
arxiv_id: '2511.20906'
source_url: https://arxiv.org/abs/2511.20906
tags:
- policy
- euler
- performance
- inference
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DA-SIP, a difficulty-aware stochastic interpolant
  policy that adaptively adjusts computational resources during robot control based
  on task complexity. The method uses a difficulty classifier to dynamically select
  solver type, step count, and ODE/SDE integration at each control cycle, enabling
  efficient allocation of computational resources where they are most needed.
---

# Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy

## Quick Facts
- **arXiv ID:** 2511.20906
- **Source URL:** https://arxiv.org/abs/2511.20906
- **Reference count:** 40
- **Primary result:** Achieves 2.6-4.4× reduction in computation time while maintaining task success rates comparable to fixed maximum-compute baselines.

## Executive Summary
This paper introduces DA-SIP, a difficulty-aware stochastic interpolant policy that adaptively adjusts computational resources during robot control based on task complexity. The method uses a difficulty classifier to dynamically select solver type, step count, and ODE/SDE integration at each control cycle, enabling efficient allocation of computational resources where they are most needed. Experiments across six manipulation tasks show DA-SIP achieves 2.6-4.4× reduction in total computation time while maintaining task success rates comparable to fixed maximum-computation baselines. The framework unifies diffusion and flow-based policies under the stochastic interpolant formulation, providing a spectrum of training and inference configurations. Performance remains robust across different classifier approaches, with fine-tuned VLMs offering an optimal balance between reliability and adaptability.

## Method Summary
DA-SIP combines a stochastic interpolant policy with difficulty-aware dynamic compute allocation. The policy learns either velocity or score functions for interpolating between initial and goal states, allowing flexible switching between ODE and SDE solvers at inference time. A difficulty classifier (CNN or VLM) processes observations in real-time to categorize the current state into one of six manipulation phases (Initial, Near, Grabbing, Stochastic, Continuous, End). Each phase maps to an inference configuration triple (step count, solver type, ODE/SDE) based on empirical performance-efficiency trade-offs. During control, the system dynamically adjusts these parameters each cycle, allocating high compute to precision phases and minimal compute to exploratory or stable phases.

## Key Results
- Achieves 2.6-4.4× reduction in total computation time across six manipulation tasks
- Maintains task success rates comparable to fixed maximum-compute baselines
- Outperforms both fixed-step baselines and prior adaptive methods like Diffusion Policy
- Shows robust performance across different classifier approaches (CNN, few-shot VLM, fine-tuned VLM)

## Why This Works (Mechanism)

### Mechanism 1: Difficulty Classification Drives Compute Allocation
Real-time difficulty classification of observations enables efficient, task-appropriate compute allocation without sacrificing success rates. A classifier processes the current observation and predicts a difficulty level, which maps to an inference configuration triple (step count, solver type, ODE/SDE). Easy states receive 1–5 steps with Euler-ODE; hard states receive 50–100 steps with Heun/RK4-SDE. The core assumption is that task difficulty at a given state correlates with the computational resources needed for accurate action generation.

### Mechanism 2: Stochastic Interpolant Unifies ODE/SDE with Post-Training Flexibility
The stochastic interpolant formulation allows a single trained policy to switch between deterministic (ODE) and stochastic (SDE) integration at inference time without retraining. The framework learns either velocity v(x,t,o) or score s(x,t,o), which are mathematically interconvertible. The reverse-time SDE includes an adjustable diffusion coefficient wt ≥ 0, enabling ODE (wt=0) or SDE (wt>0) sampling from the same network.

### Mechanism 3: State Phase Categorization Generalizes Compute Needs Across Tasks
A standardized categorization of robot states into six phases enables cross-task generalization of optimal compute configurations. Instead of tuning configurations per task, phases are mapped to compute levels based on general manipulation characteristics. This avoids impractical per-task optimization by assuming similar manipulation phases across different tasks have similar computational requirements.

## Foundational Learning

- **Score-based generative modeling (score function s(x,t) = ∇log p(x,t))**: Why needed here: DA-SIP learns either velocity or score; understanding the score-distribution relationship is essential for interpreting the ODE/SDE connection. Quick check: Can you explain why adding stochasticity (SDE) can improve sample quality or diversity compared to deterministic ODE sampling?

- **Numerical integration methods (Euler, Heun, RK4)**: Why needed here: The solver type directly affects accuracy vs. speed; Heun (2nd order) is more accurate than Euler (1st order) but slower. Quick check: Why might higher-order solvers like RK4 not always improve performance in exploratory manipulation tasks?

- **Variance-preserving vs. linear interpolation schedules**: Why needed here: The choice of interpolant affects training dynamics and inference quality; VP recovers standard diffusion formulations. Quick check: What is the key difference between Linear and VP interpolation in terms of noise-to-signal ratio over time?

## Architecture Onboarding

- **Component map**: Observation ot → Difficulty Classifier → Configuration Mapper → SIP Policy Network → Integration (ODE/SDE) → Action sequence

- **Critical path**: Observation → Classifier → Configuration selection → SIP inference → Action execution. Classifier accuracy and latency directly impact overall efficiency gains.

- **Design tradeoffs**:
  - CNN classifier: Fast (20ms) but requires 300 annotated images per task; less adaptable to new tasks
  - VLM (few-shot): No training required, but slow (500–1000ms) and lower accuracy (45.3% avg)
  - VLM (fine-tuned): Better accuracy and speed (300–400ms), but requires annotation and fine-tuning effort
  - Step count: More steps improve precision but increase latency linearly
  - ODE vs. SDE: SDE adds stochasticity, beneficial for exploratory tasks; ODE is deterministic and faster

- **Failure signatures**:
  - Misclassification of difficulty leads to under- or over-computation
  - Excessive steps on exploratory tasks can degrade performance
  - Classifier latency (especially few-shot VLM) can dominate overall cycle time

- **First 3 experiments**:
  1. Validate baseline: Train SIP policy on a simple task (Can or Lift) and confirm 1-step ODE achieves ~100% success
  2. Test classifier accuracy: Train CNN classifier with 300 annotated images per task and measure accuracy on held-out test set
  3. Measure adaptive compute: Run DA-SIP on a precision task (Push-T or Block Push) and compare total computation time vs. fixed 100-step baseline

## Open Questions the Paper Calls Out
- Can DA-SIP's adaptive inference framework scale effectively to large Vision-Language-Action (VLA) foundation models while maintaining computational savings?
- How effectively does DA-SIP transfer from simulation to physical robot deployment with real sensor noise and actuator dynamics?
- Can the difficulty-to-inference-configuration mapping be learned dynamically rather than manually specified?
- Why does Diffusion Policy significantly outperform SI Policy on the Tool Hang task?

## Limitations
- Classifier accuracy variability significantly impacts performance, with few-shot VLM achieving only 45.3% accuracy
- The phase-based configuration generalization assumption lacks external validation for tasks with unique computational requirements
- Real-world transfer remains unproven, with the paper acknowledging adaptive methods may struggle with unpredictable physical environments

## Confidence
- **Adaptive Compute Allocation Effectiveness**: High confidence - well-supported by simulation results across six tasks with clear baseline comparisons
- **Stochastic Interpolant Framework Flexibility**: Medium confidence - theoretical unification is sound but practical benefits need more extensive validation
- **Classifier-Generalization Across Tasks**: Low-Medium confidence - few-shot VLM accuracy (45.3%) is concerning despite theoretical appeal

## Next Checks
1. Evaluate DA-SIP performance using only the few-shot VLM classifier (45.3% accuracy) on all six tasks to determine if the 2.6-4.4× speedup is still achievable with the most adaptable but least accurate approach.

2. Apply the six-phase configuration mapping from the tested tasks to a new, structurally different manipulation task (e.g., precision insertion or assembly) to validate the phase-based generalization assumption.

3. Implement DA-SIP on a physical robot platform for one task (e.g., Block Push) to measure computation savings and success rates under real-world conditions including sensor noise, latency, and unexpected perturbations.