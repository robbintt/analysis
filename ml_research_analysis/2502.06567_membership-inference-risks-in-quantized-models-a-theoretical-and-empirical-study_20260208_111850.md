---
ver: rpa2
title: 'Membership Inference Risks in Quantized Models: A Theoretical and Empirical
  Study'
arxiv_id: '2502.06567'
source_url: https://arxiv.org/abs/2502.06567
tags:
- bits
- quantization
- quantized
- privacy
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how quantization affects membership inference
  attack (MIA) risks in machine learning models. The authors develop theoretical bounds
  showing that as training data size grows, the membership inference security (MIS)
  of quantized models depends on the loss distribution of quantized parameters.
---

# Membership Inference Risks in Quantized Models: A Theoretical and Empirical Study

## Quick Facts
- arXiv ID: 2502.06567
- Source URL: https://arxiv.org/abs/2502.06567
- Authors: Eric Aubinais; Philippe Formont; Pablo Piantanida; Elisabeth Gassiat
- Reference count: 40
- Primary result: Theoretical bounds show quantization affects membership inference privacy; proposed method reliably ranks quantizers' privacy while being computationally faster than baseline MIAs

## Executive Summary
This paper establishes theoretical bounds on membership inference security (MIS) for quantized machine learning models, showing that asymptotic privacy depends on the loss distribution of quantized parameters. The authors develop a practical method to estimate and compare privacy levels of different quantization techniques by sampling along training trajectories and focusing on models with lowest loss. Experiments on synthetic and real-world molecular datasets demonstrate that sparse quantization methods provide higher privacy but lower accuracy, while denser quantization preserves accuracy at privacy cost, with regression tasks showing the most severe trade-offs.

## Method Summary
The method estimates membership inference security by sampling quantized models along a training trajectory and computing per-sample losses on a validation set. For each checkpoint, the algorithm applies different quantizers, computes validation losses, and aggregates statistics across runs. The key metric r_Q is derived from the loss gap between best and second-best quantized models divided by the variance in loss differences. The approach focuses on low-loss quantizers, which empirically dominate the privacy estimation. This method is compared against baseline MIA evaluations that train discriminators to distinguish training vs test samples.

## Key Results
- Sparse quantization (1.58b 90%) provides highest privacy but lowest accuracy, especially problematic for regression tasks
- Proposed r_Q method achieves Spearman correlation > 0.8 with baseline MIS while requiring only ~20 runs vs ~150 for baseline
- Classification tasks tolerate quantization-induced accuracy loss better than regression tasks
- Low-loss quantizers along training trajectory are sufficient for accurate privacy estimation, eliminating need for exhaustive exploration

## Why This Works (Mechanism)

### Mechanism 1
Asymptotic privacy is determined by loss distribution of quantized parameters. For large training datasets, MIS converges to a value depending on r_Q = (δ²)/(2σ²), where δ is the loss gap between best and second-best quantized models, and σ² captures variance in loss differences. Higher r_Q implies better privacy because larger loss gaps make it harder to distinguish training samples. This relies on training dataset growing large while loss gaps satisfy δ₂ → 0 but √n·δ₂ → ∞.

### Mechanism 2
Low-loss quantizers along training trajectory dominate privacy estimation. Instead of exhaustive search through exponentially large quantized parameter space, the method samples quantized checkpoints during training and focuses on those with lowest validation loss. Empirically, the maximum variance term σ²_k that determines r_Q is achieved by low-loss quantizers, making full exploration unnecessary. This assumes training trajectory passes near quantized configurations that matter most for privacy estimation.

### Mechanism 3
Sparsity-based quantization improves privacy at accuracy cost, with task-dependent severity. Sparse quantization (e.g., 1.58b 90% setting 90% of weights to zero) reduces information capacity in parameters, making membership inference harder. Classification tasks tolerate this better because decision boundaries are robust to parameter perturbation. Regression requires precise weight values for continuous target estimation, creating catastrophic privacy-performance tradeoff. This assumes information reduction through quantization directly limits attack capability.

## Foundational Learning

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: This is the threat model. MIAs attempt to determine whether a specific data point was used in training by examining model parameters or outputs.
  - Quick check question: Given a trained model and a data point, can you explain why knowing whether the point was in training might leak sensitive information?

- Concept: Post-Training Quantization (PTQ)
  - Why needed here: The paper focuses on PTQ methods that reduce parameter precision after training, not during training (QAT). Understanding this distinction is critical for interpreting results.
  - Quick check question: What is the difference between quantizing weights after training versus incorporating quantization into the training loop?

- Concept: Total Variation Distance and MIS Metric
  - Why needed here: MIS = 1 - TV(P(θ̂_n, z₁), P_θ̂_n ⊗ P) measures privacy as the gap between joint and product distributions. This formalizes "security against all possible attacks."
  - Quick check question: Why does MIS range from 0 (no privacy) to 1 (full privacy), and what does the supremum over all attacks imply?

## Architecture Onboarding

- Component map: Training Loop -> Checkpoint Sampling -> Quantization (Q) -> Loss Computation -> r_Q Estimation

- Critical path:
  1. Train model for K epochs, saving checkpoints
  2. At each checkpoint, apply quantizer Q to get θ̄_k
  3. Compute validation losses L_k on held-out D_val
  4. Sort quantized models by mean loss m_k
  5. Compute variance of loss differences: σ²_k = Var(L_k - L_1) for k ≥ 2
  6. Calculate r_Q using formula in Algorithm 1 line 17
  7. Higher r_Q = better privacy ranking

- Design tradeoffs:
  - **Number of runs (k_run)**: More runs stabilize rankings but increase cost. Paper shows ~20 runs sufficient for r_Q vs ~150 for baseline MIS
  - **Validation set size**: Paper uses 40% of data for validation; smaller sets increase variance in r_Q estimates
  - **Epoch granularity**: Fewer checkpoints miss trajectory information; more checkpoints increase computation linearly
  - **Quantizer selection**: Must test multiple quantizers to establish relative rankings; no absolute privacy threshold provided

- Failure signatures:
  - **Ranking instability across runs**: Indicates insufficient samples or high variance in loss distributions
  - **Negative R² on regression tasks**: Aggressive quantization destroyed model utility
  - **σ² estimates dominated by high-loss models**: Violates assumption that low-loss quantizers dominate
  - **MIS baseline disagrees with r_Q ranking**: Spearman correlation < 0.8 indicates estimation issues

- First 3 experiments:
  1. Reproduce synthetic experiment with k_modes=8, σ=2: Train 20 single-layer classifiers on Gaussian mixture data, compute r_Q for Sign, 1.58b 50%, 2-bit, and 5-bit quantizers. Verify ranking: 1.58b 50% > Sign > 2-bit > 5-bit (least private).
  
  2. Ablation on validation set size: Take one molecular dataset, vary validation split from 20% to 50%, measure ranking stability (Spearman correlation across 5 runs). Expect 40% to be near stability plateau.
  
  3. Test break condition on regression task: Apply 1.58b 90% quantization to Lipophilicity regression, measure both r_Q and R². Verify that high privacy correlates with catastrophic accuracy loss (R² < 0).

## Open Questions the Paper Calls Out

- **Can Quantization-Aware Training (QAT) jointly optimize privacy (rn_Q) and task performance, achieving better privacy-utility trade-offs than Post-Training Quantization?**
  - Basis in paper: "due to computational constraints, we concentrated on Post-Training Quantization (PTQ) and did not examine Quantization-Aware Training (QAT), which presents a potential direction for future research."
  - Why unresolved: QAT requires training with quantization baked in, introducing different optimization dynamics that may alter the loss distribution of quantized parameters in ways not covered by the current theoretical framework.
  - What evidence would resolve it: Experiments comparing privacy-performance trade-offs between PTQ and QAT across multiple datasets and quantization levels.

- **How can quantization strategies be redesigned to preserve accuracy in regression tasks while maintaining membership inference security?**
  - Basis in paper: "regression tasks lack a viable privacy-performance trade-off: quantizers either degrade performance catastrophically or retain performance at the cost of privacy."
  - Why unresolved: The authors show that regression requires precise weight values for continuous targets, making it fundamentally more sensitive to precision loss than classification's decision boundaries.
  - What evidence would resolve it: Development of regression-specific quantization methods that achieve R² scores within 10% of original models while maintaining rn_Q comparable to sparse quantizers.

- **Can the theoretical MIS bounds be extended to predict privacy for individual trained models rather than just evaluating the training procedure?**
  - Basis in paper: "Since our analysis focuses on evaluating a training procedure rather than individual trained models, it does not directly predict the security of a specific trained model."
  - Why unresolved: The asymptotic analysis depends on population-level loss distributions, while individual models have specific parameter configurations that may deviate from expected behavior.
  - What evidence would resolve it: A framework that provides calibrated privacy certificates for a single trained model based on its observed training trajectory and final loss distribution.

## Limitations
- The method cannot predict absolute privacy levels for individual trained models—only relative rankings between quantizers
- Critical hyperparameters like validation set size and trajectory sampling frequency are chosen empirically without theoretical guidance
- Theoretical framework relies on asymptotic assumptions about loss distributions that may not hold in finite-data regimes

## Confidence
- **High Confidence**: The empirical observation that sparse quantization improves privacy at accuracy cost, and the relative ranking stability across multiple datasets and tasks
- **Medium Confidence**: The asymptotic theoretical bounds and their practical applicability to finite datasets; the assumption that low-loss quantizers dominate privacy estimation
- **Low Confidence**: The claim that this method is universally applicable across all quantization schemes and model architectures; the absence of absolute privacy thresholds

## Next Checks
1. Test the method's robustness to different validation set sizes (20%-50% splits) on one molecular dataset to determine stability plateaus
2. Apply the ranking method to additional quantization schemes (e.g., QAT vs PTQ, structured vs unstructured sparsity) to assess generalizability
3. Evaluate whether the low-loss quantizer assumption holds for non-standard training trajectories or alternative optimization algorithms