---
ver: rpa2
title: 'Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization
  for Multi-Agent LLMs'
arxiv_id: '2511.06134'
source_url: https://arxiv.org/abs/2511.06134
tags:
- arxiv
- multi-agent
- preprint
- zhang
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAESTRO, a multi-agent LLM collaboration
  framework that balances divergent exploration with convergent synthesis. The core
  innovation is separating these cognitive modes into parallel Execution Agents for
  diverse solution generation and a Central Agent for discriminative selection, guided
  by a novel Conditional Listwise Policy Optimization (CLPO) that disentangles decision
  and rationale signals.
---

# Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs

## Quick Facts
- arXiv ID: 2511.06134
- Source URL: https://arxiv.org/abs/2511.06134
- Authors: Wei Yang; Jiacheng Pang; Shixuan Li; Paul Bogdan; Stephen Tu; Jesse Thomason
- Reference count: 30
- Primary result: MAESTRO with CLPO consistently outperforms state-of-the-art multi-agent methods, achieving absolute accuracy gains averaging 6% and up to 10% on challenging tasks.

## Executive Summary
MAESTRO introduces a multi-agent LLM collaboration framework that separates divergent exploration from convergent synthesis. The framework uses parallel Execution Agents for diverse solution generation and a Central Agent for discriminative selection, guided by a novel Conditional Listwise Policy Optimization (CLPO) that disentangles decision and rationale signals. Experiments across mathematical reasoning, general reasoning, and code synthesis benchmarks demonstrate consistent performance improvements over state-of-the-art methods, with gains averaging 6% and reaching up to 10% on the most challenging tasks.

## Method Summary
MAESTRO employs N parallel Execution Agents (N=3 default) that each generate K candidates per round, creating a pooled candidate pool. A Central Agent selects from these candidates using discriminative comparison rather than direct generation. The CLPO loss function optimizes the Central Agent through two components: L_choice (advantage-weighted policy gradients applied only to decision tokens) and L_reason_rank (listwise ranking loss over rationale sequences). The framework uses LoRA fine-tuning on the Central Agent only, with epsilon-greedy exploration (ε=0.1) to prevent herding across rounds.

## Key Results
- MAESTRO with CLPO achieves absolute accuracy gains averaging 6% across benchmarks
- Coverage probability increases steadily from N=2 to N=4 agents, with saturation beyond 4 agents
- Central selection outperforms central generation structurally, showing q_t=0.8919 vs. generation's substantially lower rates
- CLPO's disentangled credit assignment shows sharp degradation when removing either L_choice or L_reason_rank

## Why This Works (Mechanism)

### Mechanism 1: Coverage Probability Through Parallel Divergent Exploration
Multiple parallel execution agents increase the probability that at least one correct solution exists in the candidate pool. N independent agents each sample K candidates, creating pooled coverage probability p_t. Epsilon-greedy exploration (ε=0.1) ensures broadcast-agnostic exploration mass, preventing over-conditioning on prior rounds. Coverage saturates when agents become redundant; gains decline beyond 4 agents due to distractor proliferation.

### Mechanism 2: Identification Probability Through Centralized Selection (Not Generation)
A central agent selecting from candidates outperforms a central agent generating solutions directly. Selection frames convergence as discriminative comparison, preserving informative differences between candidates. Generation forces absorption of noisy contexts, amplifying misleading patterns and self-consistent hallucinations. Selection degrades when candidate pool lacks reasoning traces (answer-only inputs drop GSM8K to 0.840).

### Mechanism 3: Credit Assignment Disentanglement via CLPO
Separating decision gradients (L_choice) from rationale ranking (L_reason_rank) improves convergence stability and final accuracy. L_choice applies advantage-weighted policy gradients only to decision tokens, masked from rationale interference. L_reason_rank applies listwise ranking loss over full rationale sequences, forcing correct reasoning to outrank distractors. KL divergence and entropy regularization prevent collapse. If rationales are systematically uninformative or adversarial, L_reason_rank amplifies noise rather than signal.

## Foundational Learning

- **Concept: Exploration-Exploitation Trade-off in Sequential Decision-Making**
  - Why needed here: MAESTRO operationalizes this trade-off structurally—divergence (exploration) via parallel agents, convergence (exploitation) via central selection.
  - Quick check question: Can you explain why epsilon-greedy prevents "herding" in multi-round collaboration?

- **Concept: Listwise Ranking Loss (ListNet)**
  - Why needed here: L_reason_rank uses listwise permutation-based loss over rationale scores; understanding why this differs from pairwise or pointwise ranking is essential for debugging CLPO.
  - Quick check question: Why would a pointwise loss fail to capture comparative rationale quality across candidates?

- **Concept: Policy Gradient with Baseline/Variance Reduction**
  - Why needed here: L_choice uses advantage A_k = r(c_k) - r̄; understanding baseline subtraction is necessary to diagnose unstable training.
  - Quick check question: What happens to gradient variance if you remove the baseline (average reward)?

## Architecture Onboarding

- **Component map:**
  Execution Agents (3) -> Candidate Pool (9 items) -> Central Agent -> CLPO Trainer -> Broadcast Buffer -> Execution Agents (next round)

- **Critical path:**
  1. Initialize: Execution agents sample 3 candidates each → pool C_t
  2. Central selection: Central agent evaluates C_t → outputs (reason, chosen_index, final_answer)
  3. Broadcast: b_t propagates to all agents for round t+1 conditioning
  4. Training step: Compute L_choice on decision tokens, L_reason_rank on rationales, update via CLPO

- **Design tradeoffs:**
  - Agent count (N) vs. samples per agent (K): Paper recommends prioritizing N over K; K>3 yields diminishing returns due to correlation
  - Rounds: 2-3 rounds optimal; excess rounds cause bias amplification and herding
  - Selection vs. generation: Selection is structurally superior but requires informative candidate reasoning traces

- **Failure signatures:**
  - Low coverage (p_t): Execution agents producing redundant/correlated outputs; check epsilon-greedy implementation
  - Low identification (q_t): Central agent swayed by plausible but incorrect reasoning; verify L_reason_rank is active
  - Training instability: Gradient explosion in L_choice; check token masking and advantage normalization

- **First 3 experiments:**
  1. **Baseline ablation**: Run MAESTRO with (a) answer-only, (b) reason-only, (c) both; compare identification rates to validate Mechanism 2
  2. **CLPO component ablation**: Remove L_choice, then L_reason_rank separately; measure GSM8K/AMC accuracy drop to validate Mechanism 3
  3. **Scaling sweep**: Vary N∈{2,3,4,5} and K∈{2,3,4,5}; plot coverage vs. accuracy to identify saturation points per Section 4.3

## Open Questions the Paper Calls Out

### Open Question 1
Can optimizing the policies of the Execution Agents, rather than keeping them fixed, further improve the framework's performance? The current methodology restricts training to the Central Agent via CLPO, leaving the diverse exploration capabilities of Execution Agents static. Experiments applying reinforcement learning or fine-tuning to Execution Agents, showing improved coverage (p_t) or identification rates (q_t), would resolve this.

### Open Question 2
How can unified policy objectives be designed to jointly optimize both exploration and synthesis phases? MAESTRO currently decouples these phases, optimizing the synthesis (Central Agent) while treating exploration (Execution Agents) as a separate generation process. A single loss function or training paradigm that simultaneously updates both exploration and synthesis policies to maximize end-to-end task success would resolve this.

### Open Question 3
How can the system overcome performance saturation caused by redundancy when scaling the agent population beyond four? The hyper-parameter analysis shows accuracy peaks at four agents and then slightly declines because "redundancy introduces distractors." Architectural modifications or noise-filtering mechanisms that allow accuracy to scale monotonically or remain stable with significantly more agents (e.g., >10) would resolve this.

## Limitations
- The framework assumes Central Agent's rationale generation remains sufficiently informative, but answer-only inputs show 4% performance drop
- LoRA adapter choice (rank 16, α=32) could significantly impact convergence without systematic validation
- Selection-vs-generation superiority claim lacks ablation showing persistence when candidates are limited to 1-2 per agent
- Fixed epsilon-greedy exploration (ε=0.1) lacks justification across different benchmark difficulties

## Confidence
- **High Confidence:** The parallel exploration mechanism increasing coverage probability - direct experimental evidence shows steady gains from N=2 to N=4 agents across multiple benchmarks
- **Medium Confidence:** The CLPO disentanglement claim - while ablation shows component importance, the specific mathematical form of the advantage weighting and its variance properties aren't fully validated
- **Low Confidence:** The selection-vs-generation superiority claim - only one direct comparison is provided, and the structural argument remains largely theoretical without deeper analysis of failure cases

## Next Checks
1. **Candidate Pool Sensitivity:** Systematically vary the number of candidates per agent (K∈{1,2,3,4,5}) while holding N=3 fixed to identify the point where correlation between candidates undermines coverage gains

2. **Rationale Quality Correlation:** Implement an intermediate validation step that measures the correlation between rationale quality scores (from L_reason_rank) and actual reasoning correctness, beyond the binary answer match used in the current reward function

3. **Exploration Rate Sweep:** Test ε∈{0.05, 0.1, 0.2, 0.3} to identify whether the fixed 0.1 value is optimal across different agent counts and benchmark difficulties, particularly for the hardest problems (AIME, AMC)