---
ver: rpa2
title: 'DocMEdit: Towards Document-Level Model Editing'
arxiv_id: '2505.19572'
source_url: https://arxiv.org/abs/2505.19572
tags:
- facts
- editing
- document
- methods
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes document-level model editing and introduces
  DocMEdit, a benchmark dataset with document-level inputs and outputs, multiple parallel
  facts, and non-trivial updates. Experiments show that existing methods struggle
  with this task due to long contexts and complex fact combinations, achieving low
  accuracy while causing significant side effects.
---

# DocMEdit: Towards Document-Level Model Editing

## Quick Facts
- **arXiv ID**: 2505.19572
- **Source URL**: https://arxiv.org/abs/2505.19572
- **Reference count**: 34
- **Primary result**: Existing methods struggle with document-level model editing, achieving low accuracy (43.7% at best) while causing significant side effects (17.3%-34.9%)

## Executive Summary
This paper introduces DocMEdit, a benchmark for document-level model editing that requires updating multiple facts within long documents while preserving document coherence. Unlike sentence-level editing, this task involves complex interactions between multiple facts and non-trivial document structures. The benchmark includes 5,000 instances with document-level inputs/outputs, multiple parallel facts, and non-trivial updates. Experiments reveal that current model editing methods perform poorly on this task due to challenges with long contexts and complex fact combinations, highlighting the need for new approaches to handle document-level editing effectively.

## Method Summary
The authors created DocMEdit by collecting 5,000 document pairs (before/after editing) from biographies and company descriptions. Human annotators designed edits that modify multiple facts simultaneously while maintaining document coherence. The benchmark evaluates model editing methods on their ability to update documents to reflect new facts while minimizing side effects on unchanged content. Five state-of-the-art editing methods were tested, including three specialized editing approaches and two adapted from sentence-level tasks. Performance was measured using BLEU scores for accuracy and side-effect metrics to quantify unintended changes.

## Key Results
- State-of-the-art methods achieve only 43.7% accuracy at best for document-level editing
- Side effects range from 17.3% to 34.9% across different methods
- Performance degrades significantly with longer contexts, more facts, and conflicting facts
- Document-level editing proves substantially harder than sentence-level editing

## Why This Works (Mechanism)
The benchmark works by isolating document-level editing challenges through carefully constructed test cases that require simultaneous updates of multiple facts. The mechanism reveals that current editing methods fail because they cannot effectively manage the complex dependencies and interactions between multiple facts in long documents. The side-effect measurements demonstrate how methods inadvertently alter unrelated content when attempting multi-fact updates.

## Foundational Learning
- **Document coherence maintenance**: Understanding how to preserve document structure while modifying content is essential for realistic editing tasks
- **Multi-fact dependency modeling**: Recognizing that facts in documents often interact and depend on each other requires careful handling during editing
- **Long-context processing**: The ability to maintain context across thousands of tokens while making precise edits is crucial for document-level tasks
- **Side-effect measurement**: Quantifying unintended changes helps evaluate the practical utility of editing methods
- **Fact conflict resolution**: Handling contradictory or competing facts during editing represents a key challenge

## Architecture Onboarding

**Component Map**: Document Input -> Fact Extraction -> Edit Generation -> Output Document -> Side-Effect Analysis -> Accuracy Metrics

**Critical Path**: The most important sequence is Document Input → Fact Extraction → Edit Generation → Output Document, as failures in any of these steps cascade through the entire pipeline.

**Design Tradeoffs**: The benchmark prioritizes realistic editing scenarios over synthetic simplicity, accepting lower scalability for higher ecological validity. This means fewer instances but more complex, real-world editing challenges.

**Failure Signatures**: Methods fail by either missing edits entirely (low accuracy) or over-editing (high side effects). The pattern shows that techniques good at precision struggle with completeness, while comprehensive approaches introduce more side effects.

**First Experiments**: 
1. Compare document-level vs sentence-level editing on the same fact set
2. Test performance degradation as context length increases
3. Evaluate how fact conflict density affects accuracy and side effects

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark limited to 5,000 instances from biographies and company descriptions
- Only evaluates generation phase, not training/inference phase performance
- May not generalize to all document editing scenarios
- Human annotation limits scalability and may introduce selection biases

## Confidence

**High**: Core observation that existing methods struggle with document-level editing, given systematic evaluation across multiple baselines

**Medium**: Analysis of contributing factors (context length, fact length, number of facts, fact conflicts) since correlations are demonstrated but mechanisms could be more complex

**Medium to Low**: Benchmark representativeness due to limited scale and specific domain focus

## Next Checks
1. Test the benchmark across a broader range of domains beyond biographies and company descriptions to assess generalizability
2. Evaluate the edited models' performance on downstream tasks to quantify real-world impact of side effects
3. Investigate hybrid approaches that combine sentence-level editing with document-level refinement to improve accuracy while reducing side effects