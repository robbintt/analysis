---
ver: rpa2
title: Evaluating the Process Modeling Abilities of Large Language Models -- Preliminary
  Foundations and Results
arxiv_id: '2503.13520'
source_url: https://arxiv.org/abs/2503.13520
tags:
- process
- modeling
- evaluation
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights that evaluating the process modeling abilities
  of Large Language Models (LLMs) is non-trivial, requiring consideration of quality,
  time, and cost, with trade-offs producing Pareto-optimal solutions. It proposes
  a standard evaluation scenario using BPMN models generated from English textual
  descriptions, with gold-standard models for comparison.
---

# Evaluating the Process Modeling Abilities of Large Language Models -- Preliminary Foundations and Results

## Quick Facts
- arXiv ID: 2503.13520
- Source URL: https://arxiv.org/abs/2503.13520
- Authors: Peter Fettke; Constantin Houy
- Reference count: 9
- Primary result: Evaluating LLM process modeling requires multi-criteria optimization (quality, time, cost) rather than single-metric ranking

## Executive Summary
This paper establishes a foundational framework for evaluating Large Language Models' abilities to generate Business Process Model and Notation (BPMN) diagrams from English textual descriptions. The framework operationalizes three independent dimensions—quality (graph-edit-distance, precision/recall), time (generation latency), and cost (token-based pricing)—and identifies Pareto-optimal solutions where no single variant strictly dominates another across all criteria. The authors introduce a standard evaluation scenario using controlled text-to-BPMN generation with gold-standard models for comparison, while highlighting critical challenges including data leakage risks and the need for generalizable evaluation approaches.

## Method Summary
The evaluation framework follows a controlled text-to-BPMN scenario where English process descriptions are transformed into BPMN models through LLM prompting. Quality is measured via graph-edit-distance (GED) between generated and gold-standard models, capturing structural similarity through minimum edit operations. Time is recorded as generation latency, while cost is calculated using token pricing calculators. The framework generates multiple model variants per input using different seeds/temperatures to explore the Pareto frontier across quality-cost-time trade-offs. The approach builds on prior work by incorporating cost data and validating the multi-criteria framework through controlled experiments on benchmark datasets.

## Key Results
- Multi-criteria evaluation reveals Pareto fronts rather than single optimal solutions, reflecting real-world budget and latency constraints
- GED serves as a proxy for human rework effort by counting minimum edit operations between generated and gold-standard BPMN models
- Data leakage poses a critical validity threat when benchmark datasets may have been included in foundation model pre-training
- The framework enables systematic comparison across models while identifying trade-offs between quality, cost, and time dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evaluating LLM process modeling performance requires multi-criteria optimization, not single-metric ranking.
- **Mechanism:** The framework operationalizes three independent dimensions—quality (graph-edit-distance, precision/recall), time (generation latency), and cost (token-based pricing)—and identifies Pareto-optimal solutions where no single variant strictly dominates another across all criteria. This reflects the reality that production deployments face budget and latency constraints alongside quality requirements.
- **Core assumption:** Quality, time, and cost are (at least partially) independent and measurable dimensions; decision-makers accept trade-offs rather than requiring a single "best" model.
- **Evidence anchors:**
  - [abstract] "not only the quality of a model should be taken into account, but also the costs and time needed for generation. Thus, an LLM does not generate one optimal solution, but a set of Pareto-optimal variants"
  - [section 3.2] "the variable costs for usage are typically measured in input tokens, output tokens, API calls, etc., and have to be paid in US-Dollars, Euros, or other currencies"
  - [corpus] Neighboring paper "Pitfalls in Evaluating Language Model Forecasters" similarly warns that evaluating LLMs presents unique challenges requiring multi-faceted assessment; corpus shows emerging consensus on multi-dimensional evaluation.
- **Break condition:** If quality and cost/time are perfectly correlated (e.g., higher quality always costs more), the Pareto front collapses to a trivial single-point solution; if token pricing becomes irrelevant (e.g., open local models with fixed hardware), cost dimension loses discriminative power.

### Mechanism 2
- **Claim:** Graph-edit-distance (GED) between generated and gold-standard BPMN models provides a proxy for human rework effort.
- **Mechanism:** BPMN diagrams are represented as graphs; GED counts the minimum edit operations (node/edge insertions, deletions, substitutions) required to transform the LLM output into the reference model. This operationalizes "semantic adequacy" by assuming that fewer edits implies less manual correction.
- **Core assumption:** GED correlates with actual human revision effort and captures semantically relevant differences; node/edge labels are meaningfully matchable across models.
- **Evidence anchors:**
  - [section 3.1] "the idea of a graph-edit-distance can be employed... the quality of a machine-generated solution is correlated to the amount of rework needed to revise the model accordingly"
  - [section 3.1] "all execution paths of the gold-standard should be included in the presented solution... execution paths of a particular solution which are not possible in the gold-standard should be penalized"
  - [corpus] No direct corpus validation of GED for BPMN evaluation; neighboring papers focus on other domains. Corpus evidence is weak on this specific metric.
- **Break condition:** If models differ in abstraction level (e.g., LLM correctly models at higher granularity), GED penalizes structurally valid alternatives; if label matching fails due to synonymy, GED inflates edit counts spuriously.

### Mechanism 3
- **Claim:** A controlled text-to-BPMN scenario enables systematic comparison but risks data leakage compromising validity.
- **Mechanism:** The standard evaluation scenario fixes three assumptions: (1) domain given as English text, (2) gold-standard BPMN solutions exist, (3) BPMN is the target notation. This controlled setup allows pairwise comparison across models but introduces leakage risk if benchmark datasets were included in foundation model pre-training.
- **Core assumption:** Public benchmark datasets are not memorized by evaluated LLMs; gold-standard models represent correct/complete conceptualizations of the textual description.
- **Evidence anchors:**
  - [section 2] "We introduce three assumptions to define the standard evaluation scenario"
  - [section 6] "if the well-known datasets are used for evaluation, the percentage of data leakage might be 100 percent... Without transparency, the comparison and evaluation of the abilities of LLM is almost uncontrollable"
  - [corpus] Corpus does not provide specific leakage mitigation techniques; neighboring papers acknowledge evaluation validity concerns but no direct solutions for BPMN.
- **Break condition:** If foundation models were trained on benchmark datasets, measured performance reflects retrieval/memorization rather than modeling capability; if gold-standards are themselves flawed or incomplete, comparisons become meaningless.

## Foundational Learning

- **Concept: Pareto optimality and multi-objective optimization**
  - **Why needed here:** The paper explicitly frames LLM evaluation as yielding Pareto fronts rather than single winners; understanding non-dominated solutions is essential to interpret results.
  - **Quick check question:** Given two models where Model A has higher quality but costs more than Model B, under what conditions is Model A Pareto-optimal?

- **Concept: BPMN (Business Process Model and Notation) fundamentals**
  - **Why needed here:** The evaluation scenario assumes BPMN as the target language; understanding activities, gateways, events, and execution semantics is required to assess graph-based quality measures.
  - **Quick check question:** What is the difference between an exclusive gateway and a parallel gateway in BPMN, and how would each affect the set of execution paths?

- **Concept: Graph-edit-distance (GED)**
  - **Why needed here:** GED is proposed as the primary quality metric; understanding edit operations on labeled graphs enables critical assessment of this measure's validity.
  - **Quick check question:** If two BPMN models describe the same process but one uses sub-processes and the other uses flat activities, would GED capture them as equivalent? Why or why not?

## Architecture Onboarding

- **Component map:** Text input → Prompt engineering → LLM generation → BPMN parsing/validation → GED computation → Multi-criteria aggregation → Pareto frontier extraction → Reporting
- **Critical path:** Text input → Prompt engineering → LLM generation → BPMN parsing/validation → GED computation → Multi-criteria aggregation → Pareto frontier extraction → Reporting
- **Design tradeoffs:**
  - **Automated vs. human-in-the-loop quality assessment:** Automated GED is scalable but may miss semantic/pragmatic quality dimensions; human expert review is thorough but expensive and subjective.
  - **Single-prompt vs. interactive/agentic modeling:** Single-prompt is reproducible and easier to benchmark; agentic approaches may yield higher quality but are harder to control and compare fairly.
  - **Public vs. held-out benchmarks:** Public benchmarks enable community comparison but risk data leakage; held-out datasets protect validity but limit reproducibility.
- **Failure signatures:**
  - **High GED despite correct semantics:** Suggests abstraction-level mismatch or label synonymy issues; investigate normalization procedures.
  - **Zero variance across temperature settings:** May indicate deterministic prompts or model caching; verify temperature > 0 and distinct seeds.
  - **Cost-quality correlation breaks down:** May indicate pricing tier boundaries or rate-limiting effects; inspect token distributions per run.
  - **Unexpectedly high performance on all metrics:** Suspect data leakage; audit benchmark against training data disclosures or use synthetic/held-out test cases.
- **First 3 experiments:**
  1. **Reproduce Pareto analysis on a subset:** Take 5 textual descriptions from an existing benchmark, generate BPMN models with 2-3 LLMs at multiple temperature settings, compute GED/time/cost, plot Pareto fronts to validate the multi-criteria framework.
  2. **Leakage probe with perturbed inputs:** Modify benchmark text descriptions (paraphrase, shuffle sentence order) and compare GED scores against unperturbed originals; significant degradation may indicate memorization rather than modeling capability.
  3. **Execution-path coverage validation:** For models where execution semantics can be computed, compare the set of execution paths in LLM outputs vs. gold-standards; assess whether GED correlates with path-level precision/recall or captures different failure modes.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** How can researchers effectively mitigate data leakage risks when proprietary LLMs may have been trained on established benchmark datasets?
  - **Basis in paper:** [explicit] The authors note in Section 6 that "if the well-known datasets are used for evaluation, the percentage of data leakage might be 100 percent" due to a lack of transparency in foundation models.
  - **Why unresolved:** Major foundation models are closed-source, making it impossible to verify if specific process model gold-standards were included in training data.
  - **Evidence would resolve it:** Development of evaluation datasets guaranteed to be absent from training windows (e.g., synthetic data or recent real-world processes) or the release of training data logs for auditing.

- **Open Question 2**
  - **Question:** Do proposed automated metrics (graph-edit-distance, precision/recall) correlate with human expert assessments of semantic quality and understandability?
  - **Basis in paper:** [explicit] Section 3.1 states that while measures like graph-edit-distance have "face validity," the actual "validity of these measures for evaluating process modeling abilitites of LLM is more or less unknown."
  - **Why unresolved:** Subjective expert evaluation is difficult to scale, yet it remains the true test of a model's utility, whereas automated metrics may miss semantic nuance.
  - **Evidence would resolve it:** Empirical studies comparing metric scores against rankings from human modeling experts across a diverse set of generated models.

- **Open Question 3**
  - **Question:** How can the evaluation framework be extended to assess "agentic" or interactive modeling scenarios where the LLM iterates with a user?
  - **Basis in paper:** [explicit] Section 6 identifies the need to evaluate "interactive modeling scenarios" (agentic process modeling) rather than just the standard single-prompt generation, noting this "cannot easily be controlled."
  - **Why unresolved:** The current standard scenario assumes a static text-to-model transformation, failing to capture the trade-offs in multi-turn interactions, negotiation, or tool usage.
  - **Evidence would resolve it:** New experimental designs that quantify cumulative cost/time and convergence quality in dynamic, multi-turn modeling sessions.

## Limitations
- Data leakage remains a critical validity threat when proprietary foundation models may have been trained on benchmark datasets, with no concrete mitigation strategies provided
- GED correlation with human rework effort lacks direct BPMN-specific validation in the corpus evidence
- Quality metrics focus on structural/graph-level differences without addressing semantic/pragmatic adequacy beyond execution-path coverage

## Confidence
- **High confidence:** The multi-criteria evaluation framework (quality, time, cost) is well-grounded in practical constraints and the Pareto optimality concept is mathematically sound.
- **Medium confidence:** The GED metric's correlation with rework effort is plausible but lacks direct BPMN-specific validation in the corpus.
- **Low confidence:** Data leakage mitigation strategies are acknowledged as critical but no concrete solutions are provided or validated.

## Next Checks
1. **Execute controlled reproduction:** Select 5-10 textual descriptions from an open benchmark (e.g., PET dataset), generate BPMN models with multiple LLMs at varied temperatures, compute GED/time/cost, and validate that Pareto fronts emerge as claimed.
2. **Probe for data leakage:** Modify benchmark text descriptions (paraphrase, shuffle sentences) and compare quality scores to original versions; significant performance drops would suggest memorization rather than modeling capability.
3. **Test GED validity:** Compare GED scores against execution-path coverage metrics on a subset of models; if GED correlates poorly with path-level precision/recall, investigate whether GED captures different failure modes or is an inappropriate proxy for human effort.