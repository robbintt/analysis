---
ver: rpa2
title: 'Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?'
arxiv_id: '2508.05464'
source_url: https://arxiv.org/abs/2508.05464
tags:
- evaluation
- capabilities
- benchmarks
- benchmark
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Bench-2-CoP, a systematic framework that\
  \ quantifies the alignment between AI evaluation benchmarks and the EU AI Act\u2019\
  s Code of Practice. Using a validated LLM-as-judge approach, it analyzes 194,955\
  \ questions across six major benchmarks against the Act\u2019s taxonomy of 13 capabilities\
  \ and 9 propensities."
---

# Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?

## Quick Facts
- **arXiv ID:** 2508.05464
- **Source URL:** https://arxiv.org/abs/2508.05464
- **Reference count:** 40
- **Primary result:** Existing public AI benchmarks are severely misaligned with EU AI Act requirements, with critical safety capabilities receiving virtually no evaluation coverage

## Executive Summary
This paper introduces Bench-2-CoP, a systematic framework that quantifies the alignment between AI evaluation benchmarks and the EU AI Act's Code of Practice. Using a validated LLM-as-judge approach, it analyzes 194,955 questions across six major benchmarks against the Act's taxonomy of 13 capabilities and 9 propensities. The results reveal a severe misalignment: current benchmarks overwhelmingly focus on a few behavioral propensities (hallucination 61.6%, bias 17.2%) while neglecting critical capabilities—particularly those related to loss-of-control risks (autonomy, self-replication, evasion of oversight receive zero coverage). Three systemic risks (cyber offense, CBRN, loss of control) are almost entirely absent from evaluation. This demonstrates that existing public benchmarks are insufficient, on their own, for regulatory compliance. The study calls for new evaluation frameworks targeting underrepresented capabilities and provides actionable insights for policymakers and developers to build next-generation safety assessments.

## Method Summary
The study employs an LLM-as-judge methodology to systematically map 194,955 benchmark questions from six major AI evaluation suites against the EU AI Act's taxonomy. The framework analyzes how well these questions cover 13 distinct AI capabilities and 9 behavioral propensities defined in the Act. Questions are processed through multiple LLM judges (GPT-4, Gemini, Claude) and validated against expert annotations to ensure reliability. The analysis identifies coverage gaps by comparing benchmark question distributions to the regulatory requirements, revealing that current evaluation practices focus disproportionately on certain propensities while completely ignoring critical capabilities related to autonomy and loss-of-control scenarios.

## Key Results
- 194,955 benchmark questions analyzed across six major evaluation suites
- Hallucination propensity receives 61.6% of coverage while critical capabilities like autonomy receive zero coverage
- Three systemic risks (cyber offense, CBRN, loss of control) are almost entirely absent from evaluation
- Current benchmarks provide insufficient coverage for regulatory compliance under EU AI Act

## Why This Works (Mechanism)
The LLM-as-judge approach provides scalable, systematic analysis of benchmark-question alignment with regulatory requirements. By leveraging multiple AI models and validation against expert annotations, the framework can process vast numbers of questions while maintaining accuracy. The method captures nuanced relationships between evaluation content and regulatory taxonomies, identifying both explicit and implicit coverage of capabilities and propensities. This automated yet validated approach enables comprehensive assessment that would be impractical through manual review alone, revealing systemic gaps in current evaluation practices.

## Foundational Learning
- **EU AI Act Taxonomy**: The regulatory framework defines 13 capabilities and 9 propensities that AI systems must demonstrate or avoid. Understanding this taxonomy is essential for mapping benchmarks to compliance requirements.
  - Why needed: Provides the regulatory baseline against which benchmarks are evaluated
  - Quick check: Can you list all 13 capabilities and 9 propensities?

- **LLM-as-judge methodology**: Using AI models to classify and evaluate benchmark content against regulatory taxonomies, validated through expert comparison
  - Why needed: Enables scalable analysis of thousands of questions that would be impractical manually
  - Quick check: Does the method include validation against human expert annotations?

- **Capability vs Propensity distinction**: Capabilities represent what AI systems can do (e.g., coding, reasoning), while propensities represent behavioral tendencies (e.g., bias, deception)
  - Why needed: Critical for understanding why certain evaluation gaps exist and how to address them
  - Quick check: Can you differentiate between a capability question and a propensity question?

## Architecture Onboarding
- **Component map**: EU AI Act Taxonomy -> Benchmark Question Corpus -> LLM Judge Pipeline -> Coverage Analysis -> Gap Identification
- **Critical path**: Taxonomy definition → Question extraction → LLM classification → Expert validation → Coverage analysis → Gap reporting
- **Design tradeoffs**: Automated LLM classification enables scalability but requires validation; comprehensive taxonomy ensures regulatory relevance but may miss emerging risks
- **Failure signatures**: Over-reliance on English benchmarks; binary classification oversimplifying nuanced coverage; missing proprietary or emerging evaluations
- **First experiments**: 1) Validate LLM alignment results with human expert review on stratified sample; 2) Test coverage analysis with additional regulatory frameworks; 3) Track benchmark evolution over time to measure gap reduction

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- LLM-as-judge methodology, while validated, remains an automated proxy for human expert judgment
- Analysis limited to English-language benchmarks and EU AI Act documentation
- Binary classification approach may oversimplify nuanced ways benchmarks address regulatory requirements
- Only captures publicly available benchmarks, potentially missing proprietary or emerging evaluations

## Confidence
- **High confidence**: Systematic identification of benchmark coverage gaps, particularly for autonomy, self-replication, and evasion capabilities
- **Medium confidence**: Quantitative alignment scores between benchmarks and EU AI Act taxonomy
- **Medium confidence**: Characterization of systemic risk underrepresentation given potential benchmark discovery limitations

## Next Checks
1. Conduct expert human review of a stratified sample of benchmark questions to validate the LLM-as-judge alignment results
2. Expand analysis to include non-English benchmarks and regulatory frameworks from other jurisdictions for comparative assessment
3. Perform longitudinal tracking of benchmark development to measure progress in addressing identified gaps over time