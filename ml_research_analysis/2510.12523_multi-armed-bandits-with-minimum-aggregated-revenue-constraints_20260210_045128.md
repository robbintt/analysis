---
ver: rpa2
title: Multi-Armed Bandits with Minimum Aggregated Revenue Constraints
arxiv_id: '2510.12523'
source_url: https://arxiv.org/abs/2510.12523
tags:
- constraints
- constraint
- regret
- optimal
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a multi-armed bandit problem with contextual
  information and per-arm minimum aggregated revenue constraints. The objective is
  to maximize cumulative reward while ensuring each arm meets a predefined minimum
  aggregated reward across contexts.
---

# Multi-Armed Bandits with Minimum Aggregated Revenue Constraints

## Quick Facts
- **arXiv ID**: 2510.12523
- **Source URL**: https://arxiv.org/abs/2510.12523
- **Reference count**: 40
- **Primary result**: OLP achieves poly-logarithmic regret with O(√T) constraint violation, while OPLP achieves O(√T) regret with poly-logarithmic constraint violation in contextual bandits with per-arm minimum aggregated revenue constraints.

## Executive Summary
This paper addresses a contextual multi-armed bandit problem where the learner must maximize cumulative reward while ensuring each arm meets a predefined minimum aggregated revenue across contexts. The authors introduce two novel algorithms: OLP (optimistic linear programming) and OPLP (optimistic-pessimistic linear programming). OLP achieves poly-logarithmic regret with O(√T) constraint violation, while OPLP achieves O(√T) regret with poly-logarithmic constraint violation. The paper establishes a lower bound demonstrating that the dependence on the time horizon in their results is optimal in general, revealing fundamental limitations of the free exploration principle leveraged in prior work. The authors show that in the contextual setting, the free exploration property no longer holds, and the exploration-exploitation trade-off is reinstated.

## Method Summary
The paper proposes two algorithms that use linear programming to solve the allocation problem at each time step. Both algorithms maintain estimates of reward means and construct confidence intervals for each arm-context pair. OLP uses optimistic estimates (UCB) for both the objective and constraints, while OPLP uses optimistic estimates for the objective and pessimistic estimates (LCB) for the constraints. The algorithms solve a linear program to determine the allocation weights for each context-arm pair, then sample an arm according to these weights. The key innovation is using LP structure to identify active constraints and optimal allocation patterns, rather than relying on standard index-based approaches.

## Key Results
- OLP achieves poly-logarithmic regret with O(√T) constraint violation under non-degeneracy assumptions
- OPLP achieves O(√T) regret with poly-logarithmic constraint violation by using pessimistic estimates for constraints
- The authors establish a lower bound showing that the dependence on the time horizon in their results is optimal in general
- The paper demonstrates that "free exploration" (where constraints naturally induce exploration) fails in the multi-context setting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Correctly identifying the set of "active" constraints (arms that exactly meet their revenue threshold) reduces the allocation problem to solving a Linear Program, yielding the optimal stationary policy.
- **Mechanism:** The algorithm maintains estimates of reward means and constructs an LP using these estimates. By observing which constraints are binding or forcing zero allocations in the LP solution, the learner deduces the optimal structure. If estimates are sufficiently accurate, the identified structure matches the oracle's optimal structure.
- **Core assumption:** The underlying optimization problem is strictly feasible and non-degenerate, ensuring a stable optimal stationary solution exists.
- **Evidence anchors:** [abstract] "optimistic and pessimistic estimation techniques with linear programming..."; [Section 3] "the optimal stationary allocation w* is the solution to LP(μ, μ)... characterized by identifying the optimal active set of constraints I*."

### Mechanism 2
- **Claim:** Using pessimistic estimates (LCB) for constraints while using optimistic estimates (UCB) for the objective creates a "safety margin" that logarithmically bounds constraint violation, albeit at the cost of √T regret.
- **Mechanism:** Algorithm OPLP solves LP(UCB, LCB). The use of LCB for constraint parameters implies that the algorithm enforces constraints based on worst-case revenue scenarios. If the problem is feasible under these pessimistic parameters, it is almost certainly feasible under true parameters, ensuring constraint satisfaction.
- **Core assumption:** A Slater constant γ* exists, quantifying the margin of feasibility required for the pessimistic phase to eventually become feasible.
- **Evidence anchors:** [Section 4] "OPLP proposes an asymmetric estimation strategy... optimistic estimate UCB(t) for the objective and a pessimistic estimate LCB(t) for the constraints"; [Section 5.1] "The pessimistic strategy ensures a more conservative treatment of constraints... leading to √T loss in performance."

### Mechanism 3
- **Claim:** The convergence rate is governed by a specific "sub-optimality gap" ρ(I), defined by the LP's geometry, rather than standard reward gaps.
- **Mechanism:** The paper defines ρ(I) combining a feasibility gap and a performance gap. Regret accumulates primarily while the confidence radius ε(t) is large enough to confuse the optimal set I* with a sub-optimal set I. Once ε(t) < ρ*, the correct structure is identified and regret growth slows or stops.
- **Core assumption:** The number of arms and contexts is finite.
- **Evidence anchors:** [Section 1.2] "...introducing a novel and more refined notion of the sub-optimality gap, which leverages the structure of the underlying linear program..."; [Section 3] "Definition 6 (Sub-optimality Gap)... ρ(I) plays a role analogous to the gap in classical MAB."

## Foundational Learning

- **Concept:** Linear Programming (LP) & Basic Duality
  - **Why needed here:** The core decision-making engine is a linear optimizer solving for allocation weights subject to revenue constraints. Understanding why a solution lies at a "vertex" (defined by active constraints) is necessary to follow the proof strategy.
  - **Quick check question:** Can you explain why adding a constraint to a Linear Program might change the solution location but never decreases the optimal objective value if the objective coefficients are positive?

- **Concept:** Concentration Inequalities (Hoeffding/Sub-Gaussian)
  - **Why needed here:** The algorithms rely on constructing confidence sets (UCB/LCB) that shrink as ~1/√n. The proofs rely on the probability of the true mean falling outside these bounds being small enough to sum to a constant over time.
  - **Quick check question:** If you have fewer samples n for a specific arm-context pair, does the confidence radius εk,c(t) increase or decrease, and how does that affect the "optimism" of the LP solution?

- **Concept:** Constraint Violation vs. Regret Trade-off
  - **Why needed here:** Unlike standard bandits which optimize a single metric (Regret), this system balances two competing metrics: Performance Regret (RT) and Constraint Violation (VT). The "Pareto frontier" concept is central to choosing between OLP and OPLP.
  - **Quick check question:** If an application requires strictly zero safety violations (hard constraints), would OPLP (with logarithmic violation) be sufficient, or would you need a different approach (like those cited in [6, 7])?

## Architecture Onboarding

- **Component map:** History Buffer -> Estimator -> LP Solver -> Sampler
- **Critical path:** Context Arrival → Update Confidence Intervals → Check LP Feasibility (OPLP only) → Solve LP → Sample Arm
- **Design tradeoffs:**
  - **OLP vs. OPLP:** OLP offers faster convergence (poly-log regret) but higher variance in constraint satisfaction (O(√T)). OPLP ensures stricter constraint adherence (log violation) but may degrade to OLP behavior if the feasibility margin is small.
  - **Primal vs. Primal-Dual:** The authors avoid primal-dual methods to exploit LP structure for better bounds, but this restricts the approach to linear constraints.
- **Failure signatures:**
  - **Linear Regret:** Check if the "Sub-optimality Gap" ρ* is effectively zero (degenerate problem instances).
  - **Infeasibility Loop (OPLP):** If the pessimistic LP is perpetually infeasible, OPLP degrades to OLP with added overhead. Check γ* estimation.
  - **Context Mismatch:** If contexts are ignored, performance degrades significantly (as shown in Appendix G vs non-contextual baselines).
- **First 3 experiments:**
  1. **Gap Sensitivity:** Replicate the synthetic setup (Sec G) varying λ values to transition the problem from "feasible with moderate cost" to "high cost." Plot RT and VT for OLP to verify poly-log dependence on ρ*.
  2. **OLP vs. OPLP Frontier:** Run both algorithms on the same instance (e.g., Table 8). Plot the cumulative Regret vs. Violation trajectory to visualize the trade-off (OLP should hug the y-axis, OPLP the x-axis).
  3. **Slater Constant Impact:** Artificially shrink the feasibility margin γ* and observe the degradation of OPLP's constraint violation bound.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the trade-off achieved by OLP and OPLP on the Pareto frontier of performance versus constraint violation optimal, or do there exist intermediate achievable rates?
- **Basis in paper:** [explicit] Section 5.2 states that while the sum R+V dependence on √T is confirmed by the lower bound, it "leaves open the question of whether the pair (R, V) is optimally positioned in the performance/constraint violation Pareto front."
- **Why unresolved:** The paper establishes a lower bound on the sum of regrets (R+V ≥ √T) and a separate lower bound on performance regret (R ≥ log T), but does not rule out the existence of algorithms that could achieve a better combined rate than the proposed O(log² T) vs O(√T) trade-off.
- **What evidence would resolve it:** A refined lower bound proving the strict curvature of the Pareto frontier or an algorithm achieving a trade-off strictly better than the OLP/OPLP pair.

### Open Question 2
- **Question:** Can the proposed linear programming framework be extended to settings with large or infinite arm and context spaces?
- **Basis in paper:** [explicit] Section 6 notes that extending the work to "vary large or infinite spaces would require additional structure on the reward function—such as Lipschitz continuity or parametric assumptions" and "represents a promising direction for future research."
- **Why unresolved:** The current analysis relies on finite sets K and C to compute empirical means and define the sub-optimality gap ρ*. Infinite spaces require functional estimation and may suffer from computational intractability in solving the LP at each step.
- **What evidence would resolve it:** Derivation of regret bounds for continuous spaces under specific smoothness assumptions or a computationally efficient approximation scheme for the allocation problem.

### Open Question 3
- **Question:** Under what conditions do aggregated revenue constraints induce "free exploration" in the multi-context setting?
- **Basis in paper:** [explicit] Section 6 and Appendix H discuss how the greedy policy works in the single-context case because "constraints enforce exploration," but this property fails in the multi-context setting where "constraints do not inherently induce exploration."
- **Why unresolved:** The paper provides a counterexample where greedy fails, but does not fully characterize the boundary conditions (e.g., specific relationships between λk and μk,c) under which simpler, greedy-like algorithms might regain sublinear regret guarantees.
- **What evidence would resolve it:** A theoretical characterization of the problem instances (e.g., based on the feasibility margin γ*) where the optimal allocation strictly bounds the exploration probabilities away from zero.

## Limitations
- The analysis relies heavily on the assumed Slater constant γ* and non-degeneracy of the LP solution; violations of these assumptions can break the convergence rate analysis.
- The "free exploration" failure in contextual settings is theoretically sound but depends critically on the finite context-arm pair assumption.
- The paper focuses on linear constraints and does not address how to extend the framework to non-linear or more complex constraint structures.

## Confidence
- **High Confidence:** The OLP regret bound O(log² T) and constraint violation O(√T) are well-established under the stated assumptions. The lower bound matching these rates (Thm 4) is rigorously proven.
- **Medium Confidence:** The OPLP logarithmic constraint violation O(log T) holds when the pessimistic LP is feasible, but the practical frequency of this condition and its dependence on γ* are less explicit.
- **Low Confidence:** The claim that "free exploration" fundamentally fails in contextual settings beyond the two-arm, two-context case is stated but the exact boundary conditions for when standard techniques break down could be more clearly delineated.

## Next Checks
1. **Degeneracy Test:** Run OLP on a modified Appendix G instance where multiple constraint sets yield identical objective values. Verify whether regret remains poly-logarithmic or degrades to √T.
2. **γ* Sensitivity:** Systematically vary the feasibility margin γ* in synthetic problems and measure the transition point where OPLP's constraint violation shifts from logarithmic to √T.
3. **Contextual Boundary:** Test the algorithm on K=2, |C|=2 vs K=2, |C|=3 instances to empirically verify where the "free exploration" property fails.