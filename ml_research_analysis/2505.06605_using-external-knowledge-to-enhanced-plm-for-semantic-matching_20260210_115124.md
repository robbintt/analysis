---
ver: rpa2
title: Using External knowledge to Enhanced PLM for Semantic Matching
arxiv_id: '2505.06605'
source_url: https://arxiv.org/abs/2505.06605
tags:
- knowledge
- semantic
- external
- performance
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving semantic matching
  in NLP by incorporating external knowledge into pre-trained language models (PLMs).
  The core method involves building a prior matrix based on lexical relations (synonyms,
  antonyms, hypernyms, hyponyms) from WordNet, and then integrating this knowledge
  into the PLM's attention mechanism through a novel adaptive fusion module.
---

# Using External knowledge to Enhanced PLM for Semantic Matching

## Quick Facts
- arXiv ID: 2505.06605
- Source URL: https://arxiv.org/abs/2505.06605
- Authors: Min Li; Chun Yuan
- Reference count: 35
- Primary result: Incorporates WordNet lexical relations into BERT attention mechanism, achieving 1.66% (BERT-base) and 2.31% (BERT-large) accuracy improvements on GLUE benchmark tasks

## Executive Summary
This paper addresses semantic matching in NLP by integrating external lexical knowledge from WordNet into pre-trained language models through a novel adaptive fusion module. The method constructs a prior matrix encoding synonymy, antonymy, hypernymy, and hyponymy relations, then calibrates attention scores using element-wise multiplication with this prior. An adaptive gating mechanism selectively fuses knowledge-aware and semantic signals while filtering potential noise. Experiments on 10 public datasets demonstrate consistent performance improvements over standard BERT models, with particular robustness to linguistic transformations and enhanced ability to capture fine-grained semantic differences.

## Method Summary
The approach builds a prior matrix from WordNet lexical relations and integrates it into BERT's attention mechanism through a knowledge-modulated attention path. Standard self-attention produces output Osem, while knowledge-modulated attention (with prior matrix) produces Oknw. These parallel outputs are fused via an adaptive module using cross-attention followed by fusion and filter gates. The filter gate learns to down-weight noisy knowledge signals, allowing the model to selectively incorporate beneficial external knowledge. Training uses Adadelta optimizer with unusual LR=0.5 and custom L2 regularization, switching to SGD after 30k steps without improvement.

## Key Results
- Achieves 1.66% accuracy improvement (BERT-base) and 2.31% (BERT-large) over standard BERT on GLUE benchmark
- Demonstrates enhanced robustness to linguistic transformations, particularly in SwapAnt test cases
- Shows improved ability to capture fine-grained semantic differences through selective knowledge incorporation

## Why This Works (Mechanism)

### Mechanism 1: Prior Matrix Construction from Lexical Relations
Encoding explicit lexical relationships (synonymy, antonymy, hypernymy, hyponymy) as a prior matrix provides structural signals absent from distributional pre-training. Word pairs from input sequences are mapped to knowledge vectors kij derived from WordNet relations, with an indicator function boosting attention scores when known relations exist. This promotes alignment of semantically related tokens across sentence pairs, leveraging WordNet's curated relations that capture semantic constraints missed by PLM co-occurrence statistics.

### Mechanism 2: Knowledge-Modulated Attention Calibration
Element-wise multiplication of attention scores with the prior matrix biases attention toward relationally-linked token pairs without replacing learned representations. Two parallel attention computations (standard and knowledge-modulated) yield outputs that can be selectively fused. The Hadamard product amplifies scores where Kprior indicates relations, creating a knowledge-informed alternative output that complements the standard attention path.

### Mechanism 3: Adaptive Gated Fusion with Noise Filtering
Multi-stage gating selectively incorporates knowledge signals while filtering irrelevant or conflicting priors. The three-stage process includes cross-attention mutual update, fusion gate balancing refined signals, and filter gate scaling final output based on relevance to original semantic signal. Low filter gate values indicate the model down-weights noisy fused signals, learning to suppress unhelpful priors through exposure to diverse training data.

## Foundational Learning

- **Concept: Scaled Dot-Product Attention**
  - Why needed here: The method modifies standard transformer attention; understanding Q, K, V computation is prerequisite to grasping where Kprior injects.
  - Quick check question: Can you compute softmax(QK^T/√dk)V for a 3×3 toy example?

- **Concept: Lexical Relations in WordNet**
  - Why needed here: The prior matrix encodes synonymy, antonymy, hypernymy, hyponymy; knowing these distinctions explains why certain pairs boost attention.
  - Quick check question: What relation holds between "wheat" and "grain"? Between "hot" and "cold"?

- **Concept: Gating Mechanisms (σ·x + (1-σ)·y)**
  - Why needed here: The adaptive fusion module uses multiple gates; understanding how sigmoid gates blend signals is essential.
  - Quick check question: If gfilter = 0.2, what fraction of the fused signal passes through?

## Architecture Onboarding

- **Component map:** Input Encoder (BERT) → contextualized hidden states HA, HB → Knowledge Lookup (WordNet) → knowledge vectors kij per token pair → Prior Matrix Builder → Kprior → Knowledge-Aware Self-Attention → parallel Osem and Oknw outputs → Adaptive Fusion Module → cross-attention → fusion gate → filter gate → yi → Task Head

- **Critical path:** WordNet lookup → Kprior construction → attention calibration (⊙ operation) → filter gate output. Errors in prior matrix dimensions or broadcasting will cascade through all downstream components.

- **Design tradeoffs:**
  - Simple indicator G(kij) = γI(kij) vs. learned MLP: simpler is faster but less expressive
  - Single γ hyperparameter vs. relation-type-specific weights: single is easier to tune but conflates relation strengths
  - Apply to all layers vs. specific layers: paper doesn't specify; likely applied at lower layers for lexical alignment

- **Failure signatures:**
  - Accuracy drops below vanilla BERT: check Kprior construction (dimensions, sparsity)
  - High variance across seeds: filter gate may not be learning; inspect gfilter distribution
  - Poor performance on antonym tests (SwapAnt): antonym relations may not be encoding correctly in kij

- **First 3 experiments:**
  1. Ablation on prior matrix: Run with empty Kprior (all zeros) to isolate adaptive fusion module's contribution. Expected: performance near vanilla BERT.
  2. Gate value analysis: Log gfilter distribution across validation set; check if low values correlate with paraphrase pairs (noise filtering working).
  3. Per-relation breakdown: Construct separate prior matrices for synonyms, antonyms, hypernyms; evaluate each independently to identify which relations drive gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may diminish substantially in domains with poor WordNet coverage (technical jargon, neologisms)
- Unclear specification of which transformer layers receive knowledge-modulated attention
- Ambiguous mapping between co-attention matrices and self-attention dimensions could introduce implementation errors

## Confidence
- **High Confidence:** Adaptive fusion module architecture and three-stage gating mechanism are well-specified and implementable
- **Medium Confidence:** Reported performance improvements are based on standard GLUE benchmarks, but unconventional optimizer choice and unclear layer integration create uncertainty
- **Low Confidence:** Claim that knowledge-modulated attention consistently improves semantic matching across all dataset types is not fully substantiated

## Next Checks
1. **Layer-Wise Performance Analysis:** Implement knowledge-modulated attention at different transformer layers and compare performance to reveal optimal integration points.
2. **Knowledge Coverage Impact Study:** Create synthetic test cases with varying WordNet coverage levels and measure performance degradation to quantify method sensitivity.
3. **Optimizer Sensitivity Analysis:** Replace Adadelta with standard AdamW while maintaining other hyperparameters to determine if optimizer choice drives results.