---
ver: rpa2
title: 'Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language
  Models'
arxiv_id: '2601.21794'
source_url: https://arxiv.org/abs/2601.21794
tags:
- knowledge
- unlearning
- forget
- retain
- weakening
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2601.21794
- Source URL: https://arxiv.org/abs/2601.21794
- Authors: Yejin Kim; Dongjun Hwang; Sungmin Cha; Junsuk Choe
- Reference count: 40
- Primary result: Training-free unlearning via progressive weakening of FFN value vectors achieves strong forget-retain tradeoff with minimal computational overhead

## Executive Summary
Knowledge Vector Weakening (KVW) is a training-free unlearning method for Large Vision-Language Models (LVLMs) that identifies and weakens knowledge vectors in FFN layers without gradient computation or retraining. The method uses a Forget Knowledge Accessor to selectively target knowledge vectors activated for forget content, applying progressive exponential weakening to suppress their contributions. KVW demonstrates competitive unlearning performance compared to fine-tuning methods while offering dramatically lower computational costs (20x faster, 25x less memory).

## Method Summary
KVW operates by treating FFN layers as key-value memory structures, where each row of the value matrix stores a knowledge vector. The method computes knowledge coefficients by averaging FFN activations at answer tokens for forget and retain datasets, then uses a log-ratio Forget Knowledge Accessor to identify vectors strongly associated with forget content. These vectors are progressively weakened using an exponential gate function, scaling them down based on their accessor values. The approach is training-free, requiring only forward passes and permanent weight modifications.

## Key Results
- Achieves strong forget-retain tradeoff comparable to fine-tuning methods
- Reduces computational cost by 20x in runtime and 25x in memory usage
- Maintains stable unlearning performance across a broad range of weakening parameters
- Effective for both vision and language unlearning tasks

## Why This Works (Mechanism)

### Mechanism 1: FFN as Key-Value Memory for Knowledge Storage
- Claim: Factual and conceptual knowledge in transformer-based LVLMs is substantially stored within Feed-Forward Network (FFN) layers, which can be interpreted as key-value memory structures.
- Mechanism: The FFN computation FFN(x) = f(xK^T)V treats the value matrix V as storing knowledge vectors (rows v_i), while the key matrix K determines which vectors are activated for a given input. The output is a weighted sum of knowledge vectors based on input-dependent coefficients.
- Core assumption: Knowledge targeted for unlearning is locally represented in specific value vectors rather than distributed globally across all parameters.
- Evidence anchors:
  - [abstract]: "KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions"
  - [section 4.1]: "Geva et al. (2021) demonstrate that the FFN can be interpreted as a key–value memory structure... we define each row vector v_i of V as a fundamental unit of knowledge, referred to as a knowledge vector"
  - [corpus]: Limited direct corpus support for FFN-as-memory specifically in LVLMs; related work on LVLM hallucinations suggests modality-specific representations exist but mechanism is not confirmed.
- Break condition: If target knowledge is distributed across attention layers, embedding matrices, or compositional representations rather than localized in FFN value vectors, this mechanism fails.

### Mechanism 2: Forget Knowledge Accessor for Selective Identification
- Claim: Knowledge vectors specific to forget content can be identified by contrasting their activation patterns on forget vs. retain datasets using a log-scale ratio.
- Mechanism: Compute knowledge coefficients C_f and C_r by averaging FFN activations at answer tokens for forget and retain sets. The Forget Knowledge Accessor A = max(0, log(C_f/C_r)) identifies vectors strongly activated for forget content but not retain content, filtering out general language understanding components.
- Core assumption: Forget-specific knowledge exhibits measurably different activation patterns than retain knowledge, and answer tokens capture the relevant knowledge access.
- Evidence anchors:
  - [section 4.2]: "To address this issue, we explicitly contrast the activations observed on the forget set with those on the retain set... A serves as a quantitative criterion for identifying the knowledge vectors to be targeted"
  - [section 6.1 ablation]: "disabling w/ retain leads to near zero forget accuracy but significantly harms retain performance, resulting in imbalanced forget–retain trade-off"
  - [corpus]: "Rethinking Post-Unlearning Behavior of LVLMs" (FMR 0.51) discusses unlearning aftermaths but doesn't validate this specific accessor mechanism.
- Break condition: If forget and retain knowledge share overlapping activation patterns (e.g., compositional knowledge, shared attributes), the log-ratio discriminator cannot cleanly separate them.

### Mechanism 3: Progressive Weakening via Exponential Gating
- Claim: Directly scaling down identified knowledge vectors proportionally to their accessor values suppresses forget-related generation pathways without requiring gradient computation or retraining.
- Mechanism: Apply gate function g(A) = exp(-γ·A) to compute scaling coefficients, then modify each knowledge vector as ṽ_i = g(A_i)·v_i. Vectors with high accessor values are weakened more; those with low values remain largely unchanged. This modifies the model permanently without backpropagation.
- Core assumption: Weakening vectors reduces their contribution to output generation, and the exponential gate provides stable, monotonic control over forgetting strength.
- Evidence anchors:
  - [section 4.2]: "By adopting an exponential form, the gate function adjusts the weakening strength in a continuous and monotonic manner... repeatedly applying this weakening process across the entire forget set progressively blocks the pathways"
  - [figure 3]: Shows KVW achieves substantially lower FLOPs, runtime, and memory than gradient-based methods
  - [section 6.3]: "effective unlearning outcomes... are achieved stably across a broad range of γ values"
  - [corpus]: "MLLMEraser" (FMR 0.53) proposes activation steering for test-time unlearning but uses different mechanism.
- Break condition: If knowledge vectors encode multiple overlapping concepts, weakening them may cause collateral damage to retain knowledge; if γ is too large, over-weakening degrades general capabilities.

## Foundational Learning

- Concept: **FFN/MLP structure in transformers**
  - Why needed here: The entire KVW method operates on FFN value matrices; understanding how FFN processes hidden states is prerequisite to identifying knowledge vectors.
  - Quick check question: Can you write out the FFN computation and explain what each matrix represents?

- Concept: **Machine unlearning (forget/retain tradeoff)**
  - Why needed here: KVW is evaluated on its ability to remove forget knowledge while preserving retain knowledge; understanding this tradeoff is essential for hyperparameter selection.
  - Quick check question: What happens if unlearning removes forget knowledge but also degrades retain performance?

- Concept: **LoRA limitations (intruder dimensions)**
  - Why needed here: The paper motivates KVW by showing LoRA-based unlearning is rank-sensitive and inefficient; understanding this helps justify the training-free approach.
  - Quick check question: Why does LoRA rank sensitivity matter for unlearning effectiveness?

## Architecture Onboarding

- Component map:
  Input (forget/retain data) → Forward Pass → Knowledge Coefficient Extraction (answer tokens only) → Forget Knowledge Accessor: A = max(0, log(Cf/Cr)) → Gate Function: g(A) = exp(-γ·A) → Knowledge Vector Update: ṽ_i = g(A_i)·v_i → Modified FFN layers (permanent weight change)

- Critical path:
  1. Precompute average C_r over entire retain set (one epoch forward pass)
  2. For each forget batch: forward pass → extract C_f at answer tokens → compute A → apply weakening to value matrices in layers [l_s, l_e]
  3. Progressive weakening across all forget batches

- Design tradeoffs:
  - γ (weakening strength): Small → incomplete forgetting; Large → retain degradation
  - Layer range [l_s, l_e]: Early layers integrate visual/language; late layers form output probabilities. Paper uses validation to select from bucketed ranges.
  - ans-only vs full tokens: Using all tokens weakens unrelated knowledge; answer-only targets specific knowledge access.

- Failure signatures:
  - Zero retain performance: γ too large or layer range too aggressive
  - High forget accuracy: γ too small or accessor not discriminating (Cf ≈ Cr)
  - Model collapse (zero ROUGE everywhere): MMU-style full fine-tuning on limited data (addressed via LoRA variant)
  - Hallucinated similar identities: Gradient-based methods may suppress exact name but leave knowledge pathway intact (Figure 4)

- First 3 experiments:
  1. **Validate knowledge coefficient extraction**: Run forward pass on small forget set, visualize C_f values; confirm high values correlate with answer tokens.
  2. **Ablate retain contrast**: Run KVW with and without C_r (w/ retain disabled); measure forget accuracy vs retain performance tradeoff on validation split.
  3. **Layer sensitivity sweep**: Fix γ, vary start/end layer across 3-4 ranges; plot forget-retain curve to identify stable operating region (Figure 6 pattern).

## Open Questions the Paper Calls Out
- How can unlearning methods account for interactions among knowledge vectors or compositional knowledge representations, rather than treating vectors as independent units?
- Can lightweight post-weakening adaptation effectively restore or improve the model’s overall representational capacity following knowledge removal?
- How does the permanent scaling of value vectors impact model stability when unlearning requests are applied sequentially to different data subsets?

## Limitations
- Does not explicitly account for interactions among knowledge vectors or compositional knowledge representations
- Permanent weight modifications may accumulate and degrade model stability with sequential unlearning requests
- Effectiveness depends on knowledge being localized in FFN value vectors rather than distributed representations

## Confidence
- High: KVW achieves stated computational efficiency improvements (20x runtime, 25x memory)
- Medium: Effectiveness depends on FFN-as-memory assumption which has limited direct corpus support
- Medium: Sequential unlearning stability is raised as concern but not empirically tested

## Next Checks
1. Validate that high C_f values correlate with answer tokens by visualizing activation patterns on sample forget data
2. Run ablation comparing KVW with/without retain contrast to quantify its impact on forget-retain tradeoff
3. Perform layer sensitivity analysis by varying [l_s, l_e] and plotting forget-retain curves to identify optimal ranges