---
ver: rpa2
title: Parameter-efficient Multi-Task and Multi-Domain Learning using Factorized Tensor
  Networks
arxiv_id: '2310.06124'
source_url: https://arxiv.org/abs/2310.06124
tags:
- network
- parameters
- learning
- low-rank
- wwith
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Factorized Tensor Networks (FTN), a parameter-efficient
  method for multi-task and multi-domain learning that adapts pre-trained networks
  to new tasks/domains by adding task-specific low-rank tensor factors to shared frozen
  weights. The approach represents low-rank updates as a sum of rank-1 tensors, significantly
  reducing parameters compared to fine-tuning while maintaining comparable accuracy.
---

# Parameter-efficient Multi-Task and Multi-Domain Learning using Factorized Tensor Networks

## Quick Facts
- **arXiv ID:** 2310.06124
- **Source URL:** https://arxiv.org/abs/2310.06124
- **Reference count:** 40
- **Primary result:** Introduces Factorized Tensor Networks (FTN) that achieve comparable accuracy to single-task methods while using only a fraction of additional parameters per task.

## Executive Summary
This paper presents Factorized Tensor Networks (FTN), a parameter-efficient method for multi-task and multi-domain learning. FTN adapts pre-trained networks to new tasks/domains by adding task-specific low-rank tensor factors to shared frozen weights, enabling adaptation without catastrophic forgetting. The approach represents weight updates as a sum of rank-1 tensors (CP decomposition), significantly reducing parameters compared to fine-tuning while maintaining comparable accuracy. Experiments across convolutional and transformer architectures demonstrate FTN's effectiveness in both multi-task learning (MTL) and multi-domain learning (MDL) scenarios.

## Method Summary
FTN adapts a pre-trained network to new tasks by learning low-rank tensor factors that are added to shared frozen weights. For each task, the method learns a set of rank-1 tensors that approximate the weight updates needed for adaptation. These factors are combined via Kronecker product and added to the original weights. The method also includes task-specific Batch Normalization parameters to handle domain shifts. By freezing the shared backbone, FTN prevents catastrophic forgetting while enabling incremental learning. The rank of the tensor factors can be selected based on task complexity, providing flexibility in the parameter-efficiency trade-off.

## Key Results
- FTN achieves similar performance to single-task/domain methods while using only a fraction of additional parameters per task
- Parameter efficiency outperforms methods like LoRA, KAdaptation, and TAPS across multiple datasets
- Supports flexible rank selection based on task complexity and enables incremental learning without catastrophic forgetting
- Works effectively across both convolutional and transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
Representing weight updates as a sum of rank-1 tensors allows the network to capture essential task-specific features with drastically fewer parameters than full matrices. Instead of learning a full dense update matrix $\Delta W$ for each layer, FTN decomposes the update into $R$ factors (vectors) and combines them via the Kronecker product. This constrains the solution space to low-rank tensors, assuming the required adaptation lies in this sub-space.

### Mechanism 2
Freezing the shared backbone weights prevents catastrophic forgetting while enabling incremental learning. By isolating task-specific parameters ($\Delta W^t$ and BN layers) and keeping the shared backbone $W_{shared}$ constant, gradient updates for a new task $t$ mathematically cannot overwrite the knowledge used for previous tasks $t-1$.

### Mechanism 3
Training task-specific Batch Normalization (BN) parameters alongside low-rank tensors is critical for handling domain shifts. BN layers learn scaling ($\Gamma$) and shifting ($\beta$) parameters. Optimizing these per task adjusts the activation statistics (mean/variance) to suit the new domain, effectively performing an affine transformation of features without modifying the convolutional filters.

## Foundational Learning

- **Tensor Decomposition (CP/Tucker)**
  - **Why needed here**: To understand how FTN compresses a 4D weight tensor (Conv) or 3D tensor (Attention) into small vectors.
  - **Quick check question**: Can you explain the difference between a rank-1 tensor (outer product of vectors) and a rank-R tensor (sum of rank-1 tensors)?

- **Catastrophic Forgetting**
  - **Why needed here**: To understand why the "shared frozen backbone" design is necessary for sequential multi-domain learning.
  - **Quick check question**: If you sequentially train a standard network on Task A then Task B, what happens to performance on Task A?

- **Low-Rank Adaptation (LoRA)**
  - **Why needed here**: To establish a baseline for parameter-efficient fine-tuning and understand how FTN extends matrix factorization to tensor factorization.
  - **Quick check question**: How does the number of parameters in a rank-R matrix (LoRA) compare to a rank-R tensor (FTN) for a $k \times C_{in} \times C_{out}$ convolution?

## Architecture Onboarding

- **Component map**: Input -> [Frozen Conv($X$) + $\Delta W_{FTN}(X)$] -> [Task-Specific BN] -> Activation -> Next Layer
- **Critical path**: Input $\to$ [Frozen Conv($X$) + $\Delta W_{FTN}(X)$] $\to$ [Task-Specific BN] $\to$ Activation $\to$ Next Layer
- **Design tradeoffs**:
  - **Rank ($R$) Selection**: Lower $R$ minimizes storage but may underfit complex domains (e.g., WikiArt vs. Flowers). The paper suggests tuning $R$ based on task complexity.
  - **Initialization**: Factors must be near-zero (Xavier/Gaussian $\mu=0$) to start close to the pre-trained state; large random initialization destroys transferability.
- **Failure signatures**:
  - **Minimal gain over "Feature-Extractor"**: Suggests rank $R$ is too low or BN layers are frozen.
  - **Instability**: Check learning rates; the paper uses different LRs for low-rank layers (Adam, 0.005) vs. BN/Heads (SGD, 0.008).
- **First 3 experiments**:
  1. **Ablation on Rank**: Run FTN with $R \in \{1, 5, 10, 20\}$ on a single domain to plot the accuracy vs. parameter curve (refer to Figure 2).
  2. **Component Isolation**: Compare "Frozen Backbone + Head" vs. "Frozen + BN only" vs. "Full FTN" to quantify the contribution of the tensor factors vs. BN.
  3. **Backbone Portability**: Implement the FTN module on a ResNet block (Convolution) and a ViT block (Transformer Attention) to verify the "plug-in" capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can adaptive rank selection per layer, rather than a fixed rank across the entire network, yield further parameter reductions or performance gains?
- **Basis in paper**: The authors state, "In our current approach, we used a fixed rank for each layer. Moving forward, we can explore adaptively selecting the rank for different layers, which may further reduce the number of parameters."
- **Why unresolved**: The current work uses a manually chosen, global rank for all layers. While the paper shows flexibility in choosing a rank based on task complexity, it does not investigate if different layers require different levels of expressivity.
- **What evidence would resolve it**: An experiment comparing a model with a global rank R to a model with a per-layer rank, reporting both final task performance and the total parameter count.

### Open Question 2
- **Question**: What are the practical latency implications of FTN's shared-backbone architecture in real-world, multi-task inference scenarios?
- **Basis in paper**: The authors note, "while our method requires a separate forward pass for each task due to the shared backbone, we could further explore branched or tree-structured models... to reduce latency."
- **Why unresolved**: The paper focuses on parameter efficiency and accuracy but does not analyze the inference latency bottleneck inherent to a shared backbone.
- **What evidence would resolve it**: A benchmark of inference throughput (e.g., images/sec) and latency (ms) for a multi-task setting, comparing the sequential processing of FTN against a model with branched, task-specific pathways.

### Open Question 3
- **Question**: How can negative transfer or task interference be systematically identified and mitigated when using FTN in a joint multi-task learning setup?
- **Basis in paper**: The authors state, "MDL/MTL models are often challenged by task interference or negative transfer learning when conflicting tasks are trained together. Future work can address this by investigating which tasks or domains should be learned jointly to mitigate such drawbacks."
- **Why unresolved**: The paper demonstrates FTN primarily in an incremental/sequential learning setting, which avoids this issue. Its performance in a joint optimization setup is only briefly explored in supplementary material.
- **What evidence would resolve it**: An analysis of FTN performance when trained jointly on a diverse set of tasks, identifying specific task pairs that cause performance degradation (negative transfer) and evaluating a method designed to alleviate it.

## Limitations
- Limited ablation studies on hyperparameter sensitivity, particularly rank selection across diverse tasks
- No comprehensive analysis on negative transfer scenarios where new tasks require contradictory feature transformations
- Effectiveness assumes task-specific adaptations can be captured by low-rank tensor factors, which may not hold for high-rank update requirements

## Confidence

- **High confidence**: Parameter efficiency claims (measured by parameter counts) and general effectiveness of low-rank tensor factorization for weight updates
- **Medium confidence**: Catastrophic forgetting prevention claims (logically sound but lack extensive sequential multi-task experiments) and BN adaptation contribution
- **Low confidence**: Generalizability of rank selection heuristics across completely different architectures and task types, and robustness to negative transfer scenarios

## Next Checks

1. Implement systematic rank sensitivity analysis across multiple tasks (not just one) to establish reliable heuristics for rank selection based on task complexity
2. Design experiments to test the method's behavior under negative transfer conditions where new tasks require contradictory feature transformations from the frozen backbone
3. Conduct sequential multi-task learning experiments with long task sequences (more than 3-4 tasks) to rigorously validate the catastrophic forgetting prevention claims