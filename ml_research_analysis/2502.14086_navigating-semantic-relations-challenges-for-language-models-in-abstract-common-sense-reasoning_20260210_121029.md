---
ver: rpa2
title: 'Navigating Semantic Relations: Challenges for Language Models in Abstract
  Common-Sense Reasoning'
arxiv_id: '2502.14086'
source_url: https://arxiv.org/abs/2502.14086
tags:
- relations
- reasoning
- prompting
- relation
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the abstract common-sense reasoning capabilities
  of large language models (LLMs) using the ConceptNet knowledge graph. The authors
  propose two prompting approaches: instruct prompting with named relations, where
  models predict semantic relationships based on provided definitions, and few-shot
  prompting with unnamed relations, where models identify relations using examples
  as guidance.'
---

# Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning

## Quick Facts
- **arXiv ID**: 2502.14086
- **Source URL**: https://arxiv.org/abs/2502.14086
- **Reference count**: 35
- **Key outcome**: LLMs show consistent ranking performance but substantial decline when restricted to single predictions; accuracy improves significantly when selecting from five relations rather than full set.

## Executive Summary
This paper evaluates abstract common-sense reasoning capabilities of large language models using the ConceptNet knowledge graph. The authors propose two prompting approaches: instruct prompting with named relations where models predict semantic relationships based on provided definitions, and few-shot prompting with unnamed relations where models identify relations using examples as guidance. Experiments with GPT-4o-mini reveal that while models maintain consistent ranking quality across variable output depths, precision degrades sharply when forced to single-prediction. In few-shot prompting, accuracy improves significantly when selecting from five relations rather than the full set, though notable bias toward certain relations persists.

## Method Summary
The authors evaluate GPT-4o-mini on ConceptNet edges using two prompting approaches. Instruct prompting provides relation definitions and asks models to rank multiple relations. Few-shot prompting uses synthetic examples without relation names, asking models to identify the correct relation from shuffled candidates. The evaluation uses NDCG for ranking tasks and Cohen's κ for few-shot accuracy, testing both full relation sets and restricted five-relation subsets.

## Key Results
- Ranking performance remains consistent across variable output depths but drops sharply when restricted to single predictions
- Few-shot accuracy improves significantly when selecting from five relations rather than full set (κ = 0.385 vs 0.094)
- Notable systematic bias toward certain relations (especially /r/IsA) persists across conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs maintain consistent ranking quality across variable output depths, but precision degrades sharply when forced to single-prediction.
- Mechanism: Ranking tasks allow the model to surface multiple plausible relation hypotheses simultaneously. When N > 1, the correct relation need only appear in the top-k; NDCG penalizes position but preserves credit. At N = 1, the model must commit to a single output without the safety of alternatives, exposing uncertainty in abstract semantic mapping.
- Core assumption: Models encode partial semantic information sufficient for relative ordering but insufficient for absolute disambiguation.
- Evidence anchors:
  - [abstract] "consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation"
  - [section 3] "negligible differences in the average NDCG score as the model was allowed to rank more or less... unless the model was restricted to responding with only one possible relation"
- Break condition: If single-prediction accuracy matched NDCG@k (k > 1) for k including the correct answer, the mechanism would not hold.

### Mechanism 2
- Claim: Constraining the candidate relation space improves few-shot abstract reasoning accuracy.
- Mechanism: When the model selects from all ~36 ConceptNet relations, decision boundaries blur, and superficial pattern-matching dominates. Reducing to five candidates (including the correct one) narrows the comparison space, making discriminative features more salient. The unnamed-relation setup forces the model to generalize from examples rather than retrieve memorized labels.
- Core assumption: The model's implicit semantic representations are sufficiently structured to discriminate among limited options but become overwhelmed by high-cardinality choice sets.
- Evidence anchors:
  - [abstract] "in few-shot prompting, accuracy improves significantly when selecting from five relations rather than the full set"
  - [section 3] "κ = 0.385 when the model was only provided with five possible relations, compared to... all possible relations (κ = 0.094)"
- Break condition: If performance degraded or stayed flat when reducing from full set to 5 relations, the mechanism would not hold.

### Mechanism 3
- Claim: Models exhibit systematic bias toward frequent or structurally salient relations (e.g., /r/IsA), especially under high choice-cardinality.
- Mechanism: Hierarchical relations like IsA are linguistically prevalent and often serve as default categorizations. Under uncertainty, the model defaults to high-prior relations. Bias attenuates when choice space shrinks because the contrast set forces finer-grained discrimination.
- Core assumption: Pretraining distribution skews model priors toward certain relation types, which dominate predictions when discriminative pressure is low.
- Evidence anchors:
  - [abstract] "notable bias toward certain relations persists"
  - [section 3] "/r/IsA was often the most confused with or mistaken for other categories... this widespread confusion seems to dissipate when the model is only presented with five possible relations"
- Break condition: If bias toward /r/IsA or similar relations remained equally strong under 5-relation restriction, the attenuation mechanism would not hold.

## Foundational Learning

- **Concept: Knowledge graphs and semantic relations**
  - Why needed here: ConceptNet edges encode typed relations (IsA, HasA, PartOf, UsedFor, etc.) between entities; understanding multigraph structure (parallel edges allowed) is prerequisite to interpreting why ranking and disambiguation are non-trivial.
  - Quick check question: Given nodes "car" and "vehicle," what relation type connects them? What if "car" and "wheel"?

- **Concept: Ranking evaluation metrics (NDCG, Cohen's κ)**
  - Why needed here: NDCG measures ranking quality with position-aware credit; Cohen's κ measures agreement beyond chance. Misunderstanding these leads to incorrect performance interpretation.
  - Quick check question: If a model ranks the correct relation at position 2 vs. position 5, how does NDCG differ? What does κ = 0.094 vs. 0.385 imply about improvement magnitude?

- **Concept: Few-shot vs. instruct prompting paradigms**
  - Why needed here: Instruct prompting provides explicit definitions; few-shot provides examples without names. The latter tests generalization and guards against memorization, which is central to the paper's abstract reasoning claims.
  - Quick check question: If you provide relation names in few-shot prompting, what validity threat does this introduce?

## Architecture Onboarding

- **Component map:**
  - Evaluation dataset (filtered ConceptNet edges) -> Prompt templates (instruct/few-shot) -> GPT-4o-mini API -> NDCG/κ computation -> Analysis

- **Critical path:**
  1. Sample and validate ConceptNet edges (filters: English, non-reflexive, unique)
  2. Construct prompt templates (instruct vs. few-shot; full-set vs. 5-relation)
  3. Query GPT-4o-mini via API with deterministic settings
  4. Compute NDCG@k and κ; analyze per-relation bias patterns

- **Design tradeoffs:**
  - Synthetic vs. human-authored few-shot examples: synthetic ensures coverage but may inherit model biases
  - Manual vs. automated 5-relation candidate selection: paper manually includes correct relation; RAG-based retrieval could automate but is untested
  - Single-model vs. multi-model evaluation: GPT-4o-mini only; generalization to Llama/o1 unknown (cost barrier)

- **Failure signatures:**
  - NDCG@k drops sharply only at k = 1 → indicates ranking buffer effect
  - κ ≈ 0 under full-set few-shot → indicates near-random selection; models overwhelmed
  - Systematic overprediction of /r/IsA across diverse ground-truth relations → indicates prior bias

- **First 3 experiments:**
  1. Replicate instruct prompting with NDCG@10, 5, 3, 1 on a held-out ConceptNet sample to confirm ranking buffer effect
  2. Run few-shot prompting with full-set vs. 5-relation conditions; compute κ and per-relation confusion matrices to validate choice-space reduction mechanism
  3. Introduce a retrieval-based candidate selector (e.g., RAG over relation descriptions) to automatically construct 5-relation sets; compare κ against manual selection to test scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs autonomously restrict choices to a smaller candidate set before ranking, using techniques like retrieval-augmented generation (RAG)?
- Basis in paper: [explicit] The authors state: "Further experimentation is needed to determine whether the model is capable of restricting the choices to fewer relations, prior to ranking, on its own using techniques inspired by (for example) retrieval augmented generation or RAG."
- Why unresolved: The study manually provided five relations to the model; the model's ability to self-restrict without explicit prompting remains untested.
- What evidence would resolve it: Experiments where the model must retrieve or narrow candidate relations autonomously before ranking, with performance compared to the manual restriction condition.

### Open Question 2
- Question: Do the findings generalize to other LLMs such as Llama or OpenAI's o1?
- Basis in paper: [explicit] The authors explicitly call for replication on other models: "An important line of future research is to replicate the experiment on models other than gpt-4o-mini; specifically models like Llama and OpenAI's o1."
- Why unresolved: Only gpt-4o-mini was tested due to cost and availability constraints at the time of writing.
- What evidence would resolve it: Applying the same experimental protocol to additional models and comparing NDCG scores, kappa scores, and relation-specific biases.

### Open Question 3
- Question: Would alternative prompting strategies yield different conclusions about abstract common-sense reasoning capabilities?
- Basis in paper: [explicit] The authors note: "replicating the results on other models, and also on other prompts, would provide a more rigorous basis for quantifying abstract common sense in LLMs."
- Why unresolved: Only two prompting approaches were tested (instruct and few-shot); sensitivity to prompt design is unknown.
- What evidence would resolve it: Systematic comparison across varied prompt formats, chain-of-thought prompting, or zero-shot settings using the same ConceptNet evaluation dataset.

### Open Question 4
- Question: What mechanisms underlie the systematic bias toward /r/IsA, and can this bias be mitigated?
- Basis in paper: [inferred] The authors report "/r/IsA was often the most confused with or mistaken for other categories" with 13 relations showing incorrect /r/IsA predictions, but do not explain the cause or propose mitigation.
- Why unresolved: The bias is quantified but its origins (e.g., training data skew, relation hierarchy confusions) are not investigated.
- What evidence would resolve it: Analysis of training corpus distributions, controlled experiments balancing relation frequencies, or debiasing interventions during prompting.

### Open Question 5
- Question: Does using gpt-4o-mini to generate synthetic few-shot examples introduce circular evaluation artifacts?
- Basis in paper: [inferred] The few-shot example dataset was "synthetically generated" using gpt-4o-mini itself, but the paper does not discuss whether this creates self-contamination or inflates performance estimates.
- Why unresolved: No comparison to human-authored examples or alternative generation sources was conducted.
- What evidence would resolve it: Re-running few-shot experiments with human-curated examples or examples from a different model, then comparing accuracy and kappa scores.

## Limitations
- Single-model evaluation: All experiments use only GPT-4o-mini; findings may not generalize across model families or scales
- Synthetic few-shot examples: Generated by GPT-4o-mini itself, potentially inheriting and amplifying model biases
- Limited dataset size: 100 samples per relation from ConceptNet provides statistical power but may not capture full semantic complexity

## Confidence
- **High confidence**: Ranking performance degradation when restricted to single prediction (supported by clear NDCG patterns)
- **Medium confidence**: Choice-space reduction improving accuracy (κ improvement robust but context-dependent)
- **Medium confidence**: Systematic bias toward /r/IsA (consistent pattern observed but corpus support limited)

## Next Checks
1. Replicate findings across multiple LLM architectures (Llama, Claude, o1) to test generalizability
2. Compare synthetic vs. human-authored few-shot examples to quantify generation bias
3. Implement automated relation candidate selection (RAG-based) and evaluate against manual selection