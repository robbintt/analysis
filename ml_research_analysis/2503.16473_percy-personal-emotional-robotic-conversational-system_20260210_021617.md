---
ver: rpa2
title: 'PERCY: Personal Emotional Robotic Conversational System'
arxiv_id: '2503.16473'
source_url: https://arxiv.org/abs/2503.16473
tags:
- percy
- emotion
- emotional
- gpt-4
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PERCY, a multimodal conversational system that
  integrates Large Language Models with real-time emotion recognition to enable socially
  aware human-robot interactions. PERCY combines a GPT-4 reasoning engine with visual
  emotion recognition using MobileNetV2 and textual sentiment analysis via NLTK, enabling
  open-domain dialogues that adapt to users' emotional states.
---

# PERCY: Personal Emotional Robotic Conversational System

## Quick Facts
- arXiv ID: 2503.16473
- Source URL: https://arxiv.org/abs/2503.16473
- Reference count: 40
- Achieves 92% emotion recognition accuracy with 1.7s end-to-end latency on ARI robot platform

## Executive Summary
PERCY integrates Large Language Models with real-time emotion recognition to enable socially aware human-robot interactions. The system combines GPT-4 reasoning with visual emotion recognition using MobileNetV2 and textual sentiment analysis via NLTK, enabling open-domain dialogues that adapt to users' emotional states. PERCY achieves strong performance across dialogue quality metrics while demonstrating superior personalization compared to baseline systems.

## Method Summary
PERCY is a multimodal conversational system that fuses visual emotion recognition (MobileNetV2 + SSD) with textual sentiment analysis (NLTK/VADER) to create emotional states that condition GPT-4 responses. The system processes speech through OpenAI Whisper, runs parallel emotion recognition pipelines, fuses the results, and generates responses incorporating user profiles and dialogue history. The ARI robot platform executes synchronized verbal and non-verbal outputs based on the generated responses and detected emotions.

## Key Results
- 92% emotion recognition accuracy with 1.7s end-to-end latency
- Automated evaluation: BERT 0.32, BLEU 0.51, MAUVE 0.98, Perplexity 24.53
- Human evaluation: PERCY 4.7/5 personalization vs GPT-4 3.7/5 and EmpGPT-3 3.5/5
- Comparable naturalness to GPT-4 (4.5/5 vs 4.6/5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal emotion fusion improves personalization over text-only approaches.
- Mechanism: Visual emotion recognition extracts facial affect while NLTK/VADER performs sentiment analysis on transcribed speech; these signals are fused into a unified emotional state that conditions GPT-4's response generation. The fused signal provides both immediate affective context and temporal consistency across turns.
- Core assumption: Visual and textual emotional cues are complementary rather than redundant, and their fusion provides signal beyond either modality alone.
- Evidence anchors: The paper claims combining textual sentiment analysis with visual emotional cues to accurately assess and respond to user emotions; HARMONI paper similarly finds multimodal personalization improves HRI.

### Mechanism 2
- Claim: Embedding emotional state directly into LLM prompts enables context-aware, personalized responses.
- Mechanism: The GPT-4 prompt incorporates user profile, fused emotion, and dialogue history. This "bidirectional affective grounding" means the LLM receives structured emotional context that modulates tone, content, and empathic framing.
- Core assumption: The prompt engineering effectively transfers emotional state to GPT-4's generation process.
- Evidence anchors: Human evaluation shows PERCY achieved 4.7/5 personalization vs. GPT-4 baseline at 3.7/5; "Conversations: Love Them, Hate Them, Steer Them" supports targeted activation over fine-tuning for emotional expression.

### Mechanism 3
- Claim: Parallel processing pipelines achieve sub-2-second latency for real-time robot interaction.
- Mechanism: PERCY overlaps speech-to-text processing with emotion recognition and LLM inference. The lightweight MobileNetV2 + SSD model runs on edge hardware (ARI robot), avoiding round-trip latency for vision.
- Core assumption: 1.7s latency is acceptable for natural HRI; prior work suggests LLM inference alone takes ~3s.
- Evidence anchors: The paper claims achieving 92% emotion recognition accuracy with 1.7s end-to-end latency; parallel processing pipelines overlap STT processing with LLM inference.

## Foundational Learning

- Concept: **Affective Computing & Emotion Recognition**
  - Why needed here: Understanding how MobileNetV2 + SSD extracts facial features and how VADER assigns sentiment scores is essential for debugging fusion failures and interpreting the 92% accuracy claim.
  - Quick check question: Can you explain why MobileNetV2 + SSD might outperform YOLOv8n on facial emotion detection (91.3% vs. 92.0%), and what "accuracy" means in a multi-class emotion taxonomy?

- Concept: **LLM Prompt Engineering for State Injection**
  - Why needed here: PERCY's personalization hinges on structuring prompts that convey user profiles, emotional states, and history without exceeding context windows or confusing the model.
  - Quick check question: If you had to inject a "sad" emotional state into a GPT-4 prompt, would you prepend it as a system message, append it to user input, or use another method? What tradeoffs exist?

- Concept: **ROS Middleware & Robot Action Packages**
  - Why needed here: The emotion control module maps detected emotions to ARI's predefined action package (facial expressions, gestures). Understanding ROS topics, message passing, and action libraries is required to extend or port this system.
  - Quick check question: How would you debug a situation where the emotion recognition module publishes "happy" but the robot displays "neutral" expressions?

## Architecture Onboarding

- Component map:
  1. **Speech Processing System**: OpenAI Whisper API (ASR, large-v2) → text; ARI built-in TTS for output
  2. **Emotion Recognition**: Intel RealSense D435i → MobileNetV2+SSD (visual) + NLTK/VADER (textual) → fused emotion state
  3. **GPT-4 Reasoning Engine**: Receives (Pu, Et, Ht−1) → generates response Rt via prompt-based conditioning
  4. **Emotion Control Module**: Maps Et to ARI action package (facial expressions, gestures)
  5. **ROS Framework**: Orchestrates message passing between components

- Critical path:
  User speech → Whisper ASR → [parallel: (1) NLTK sentiment analysis, (2) MobileNetV2 emotion detection] → emotion fusion → GPT-4 (with Pu + Ht−1 + Et) → response text → ARI TTS + action package → synchronized verbal + non-verbal output

- Design tradeoffs:
  1. **Latency vs. accuracy**: MobileNetV2 chosen over heavier models for edge deployment; 92% accuracy may be acceptable given real-time constraints.
  2. **Personalization vs. privacy**: User profiles Pu persist across sessions (implied), but paper does not detail data retention or consent mechanisms.
  3. **API dependency vs. control**: Whisper and GPT-4 are external APIs; latency and availability are outside system control.
  4. **Fluency vs. emotional alignment**: PERCY's perplexity (24.53) is higher than GPT-4 baseline (18.20), suggesting some fluency cost for emotional conditioning.

- Failure signatures:
  1. **Emotion-text contradiction**: User says "I'm fine" with sad facial expression—fusion may produce inconsistent state; no conflict resolution logic described.
  2. **Latency spikes**: If GPT-4 API exceeds ~1s response time, end-to-end latency will exceed acceptable thresholds for turn-taking.
  3. **Action package desync**: If emotion updates mid-utterance, robot may switch expressions abruptly; speech-triggered reset may not handle overlapping speech.
  4. **Profile drift**: Long-term personalization is claimed but not evaluated; user profiles may accumulate stale or incorrect preferences.

- First 3 experiments:
  1. **Ablate visual vs. textual emotion signals**: Run PERCY with visual-only and text-only emotion inputs; compare personalization scores to isolate fusion contribution.
  2. **Latency profiling under load**: Measure end-to-end latency with varying network conditions and concurrent users; identify bottleneck (ASR, emotion detection, or LLM inference).
  3. **Contradictory stimulus test**: Present users displaying incongruent facial and vocal emotions; evaluate system behavior and user perception of response appropriateness.

## Open Questions the Paper Calls Out
None

## Limitations
- Emotion fusion mechanism between visual and textual signals is underspecified, making it difficult to assess whether complementary information is being leveraged versus averaged.
- Human evaluation lacks statistical significance testing and control for ordering effects.
- Long-term personalization is claimed but not evaluated—profiles may accumulate stale preferences without adaptive forgetting.

## Confidence
- **High confidence**: Technical implementation details (MobileNetV2+SSD architecture, ROS integration, MERCI dataset usage)
- **Medium confidence**: Emotion recognition accuracy (92%) and latency (1.7s) claims, assuming stable network conditions
- **Medium confidence**: Human evaluation results, though lacking statistical rigor and potential confounds
- **Low confidence**: Long-term personalization claims and multimodal fusion efficacy due to missing ablations

## Next Checks
1. **Ablation of emotion fusion mechanism**: Run PERCY with visual-only, textual-only, and fused emotion inputs on the same conversation set; compare personalization scores to isolate fusion contribution.

2. **Latency sensitivity analysis**: Measure end-to-end latency across varying network conditions (simulated API delays from 100ms to 3s) and concurrent user loads; identify which component (ASR, emotion detection, or LLM inference) dominates latency.

3. **Contradictory emotion stimulus test**: Design experiments where users display incongruent facial and vocal emotions (e.g., "I'm fine" with sad expression); evaluate system response appropriateness and user perception of emotional alignment.