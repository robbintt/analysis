---
ver: rpa2
title: 'GPT Carry-On: Training Foundation Model for Customization Could Be Simple,
  Scalable and Affordable'
arxiv_id: '2504.07513'
source_url: https://arxiv.org/abs/2504.07513
tags:
- training
- carry-on
- layer
- layers
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPT Carry-On, a framework for customizing
  large language models (LLMs) using lightweight training on existing inference infrastructure.
  The key idea is to train additional transformer layers on top of the final-layer
  embedding of a pretrained LLM, rather than fine-tuning the base model itself.
---

# GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable

## Quick Facts
- arXiv ID: 2504.07513
- Source URL: https://arxiv.org/abs/2504.07513
- Authors: Jianqiao Wangni
- Reference count: 8
- Primary result: Lightweight training of additional transformer layers on top of pretrained LLM embeddings achieves efficient customization with minimal memory overhead

## Executive Summary
GPT Carry-On introduces a framework for customizing large language models using lightweight training on existing inference infrastructure. The approach trains additional transformer layers on top of the final-layer embedding of a pretrained LLM, allowing most computation to be offloaded to inference nodes while only lightweight carry-on modules are trained on training nodes. This architecture requires less than 1GB GPU memory for training a 100M parameter layer on a 30B LLM, addressing scalability and cost challenges in LLM customization while preserving the base model's general intelligence.

## Method Summary
The method involves training additional transformer layers (carry-on modules) on top of the final embedding layer of a pretrained LLM, rather than fine-tuning the base model itself. This design allows inference-heavy computation to be handled by inference nodes optimized for forward passes, while training nodes only need to handle the lightweight carry-on parameters. The approach supports mixing multiple specialized LLMs through shallow layer shortcuts and demonstrates faster convergence when continuing pretraining on models like Qwen and DeepSeek. Experimental validation shows significant performance improvements in math question solving using minimal training data (1000 chain-of-thought samples) and small parameter count (1MB).

## Key Results
- Training a 100M parameter layer on a 30B LLM requires less than 1GB GPU memory
- Faster loss convergence observed when continuing pretraining Qwen and DeepSeek models
- Significant performance improvements in math question solving with only 1000 training samples and 1MB of carry-on parameters

## Why This Works (Mechanism)
The approach works by leveraging the pretrained LLM's final embedding as a rich feature representation while training lightweight additional layers to adapt to specific tasks. This architecture maintains the base model's general capabilities while allowing specialized customization through the carry-on modules. The shallow layer shortcuts enable complementary information flow between different specialized models, creating a flexible system that can combine multiple domain-specific expertise without extensive retraining of the entire model.

## Foundational Learning
- **Transformer Architecture**: Understanding of self-attention mechanisms and layer stacking is essential for grasping how carry-on modules integrate with pretrained models
  - Why needed: Core to understanding how additional layers interact with existing model components
  - Quick check: Verify knowledge of transformer layer operations and parameter flow

- **Fine-tuning vs. Training New Layers**: Recognizing the difference between parameter-efficient adaptation and full model training
  - Why needed: Critical for understanding the memory and computational efficiency claims
  - Quick check: Compare memory requirements for fine-tuning vs. carry-on training

- **Inference vs. Training Infrastructure**: Understanding the separation of compute resources for different phases of model deployment
  - Why needed: Key to appreciating the scalability benefits of the approach
  - Quick check: Map typical resource allocation between inference and training nodes

## Architecture Onboarding

Component Map:
Inference Node -> Base LLM (frozen) -> Final Embedding Layer -> Carry-On Layers (trainable) -> Output

Critical Path:
The critical path runs through the base LLM's forward pass to generate embeddings, then through the carry-on layers during training. During inference, only the base LLM and carry-on layers execute, with gradients flowing only through the carry-on parameters.

Design Tradeoffs:
- Memory Efficiency vs. Expressiveness: Minimal carry-on parameters reduce memory usage but may limit task-specific adaptation capacity
- Base Model Preservation vs. Specialization: Avoiding base model fine-tuning maintains general capabilities but may limit deep integration of specialized knowledge
- Computational Distribution vs. Latency: Offloading computation to inference nodes improves scalability but may introduce additional latency

Failure Signatures:
- Degradation in base model capabilities over extended training
- Ineffective adaptation to target tasks despite carry-on training
- Memory overflow when scaling carry-on modules beyond tested sizes

First Experiments:
1. Train carry-on layers on a simple classification task using a small pretrained model to verify basic functionality
2. Measure memory usage and training speed compared to standard fine-tuning approaches
3. Test carry-on integration with multiple specialized base models to validate mixing capabilities

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Limited evaluation scope focusing primarily on math problems with small sample sizes
- No analysis of potential negative effects on base model capabilities over extended training
- Lack of comparison with established fine-tuning approaches beyond the specific scenarios tested

## Confidence

High confidence: The technical description of the carry-on architecture and memory/compute efficiency claims
Medium confidence: The experimental results showing faster convergence and performance improvements on math tasks
Low confidence: The generalizability of results across different task types and the long-term stability of the approach

## Next Checks
1. Test the carry-on approach on diverse NLP tasks (sentiment analysis, summarization, code generation) with varying dataset sizes to assess generalizability
2. Conduct long-term training experiments to evaluate stability and potential degradation of base model performance over time
3. Compare carry-on performance against standard fine-tuning and parameter-efficient methods (LoRA, prefix tuning) on the same tasks using identical computational budgets