---
ver: rpa2
title: Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment
arxiv_id: '2509.09327'
source_url: https://arxiv.org/abs/2509.09327
tags:
- pre-training
- surgical
- data
- performance
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of self-supervised pre-training
  on few-shot surgical skill assessment (SSA), a critical yet data-scarce task in
  surgical computer vision. We annotate a robotic surgery dataset with Objective Structured
  Assessment of Technical Skill (OSATS) scores and evaluate various pre-training sources
  across three few-shot settings (1-, 2-, and 5-shot).
---

# Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment

## Quick Facts
- **arXiv ID:** 2509.09327
- **Source URL:** https://arxiv.org/abs/2509.09327
- **Reference count:** 32
- **Primary result:** Small, domain-relevant pre-training datasets outperform large-scale, less aligned ones for few-shot surgical skill assessment, achieving 60.16%, 66.03%, and 73.65% accuracy in 1-, 2-, and 5-shot settings.

## Executive Summary
This study investigates self-supervised pre-training for few-shot surgical skill assessment (SSA), a critical task in surgical computer vision with limited labeled data. The authors annotate a robotic surgery dataset with OSATS scores and evaluate various pre-training sources across three few-shot settings. Through domain similarity quantification and systematic analysis of transfer learning effects, they demonstrate that small but domain-relevant datasets can outperform large-scale, less aligned ones. The research reveals that combining procedure-specific data with domain-relevant external datasets significantly boosts performance, while combining with dissimilar but larger sources leads to degradation.

## Method Summary
The study employs self-supervised video masked autoencoding (VideoMAEv2) with a ViT-Small encoder and 4-layer decoder, pre-training on various surgical and non-surgical datasets. They evaluate three few-shot settings (1-, 2-, and 5-shot) using episode-based evaluation with 100 random episodes per setting. Domain similarity is quantified using Earth Mover's Distance (EMD) between feature distributions. The approach combines linear evaluation (frozen features + linear classifier) and temporal evaluation (TCN head for temporal aggregation). They systematically analyze domain gap effects and the impact of combining procedure-specific data with external datasets.

## Key Results
- Small domain-relevant datasets (RALPN) outperformed large-scale but less aligned sources, achieving 60.16%, 66.03%, and 73.65% accuracy in 1-, 2-, and 5-shot settings
- Combining procedure-specific data with domain-relevant external datasets boosted performance by +1.22% accuracy and +2.28% F1-score
- Combining with dissimilar but larger-scale sources led to performance degradation
- Large-scale pre-training showed diminishing returns for linear evaluation but benefited temporal evaluation with more labeled examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-aligned pre-training data outperforms larger but less domain-relevant sources for few-shot surgical skill assessment.
- **Mechanism:** Self-supervised video masked autoencoding on domain-similar surgical videos learns representations containing procedure-relevant visual and temporal features that transfer more effectively to SSA classification than generic features from large-scale non-surgical or diverse surgical datasets.
- **Core assumption:** The Earth Mover's Distance between feature distributions meaningfully captures domain relevance for downstream SSA transfer.
- **Evidence anchors:**
  - RALPN consistently demonstrates strong performance across all shot settings despite its relatively small size
  - Related work on surgical foundation models similarly emphasizes domain-specific pre-training
- **Break condition:** If downstream SSA requires fine-grained motion patterns absent from the small domain-aligned dataset, larger diverse sources may become necessary.

### Mechanism 2
- **Claim:** Combining procedure-specific unlabeled data with external domain-aligned datasets improves few-shot SSA; combining with dissimilar large-scale sources degrades performance.
- **Mechanism:** Procedure-specific data provides exact visual context for the downstream task, while domain-aligned external data adds representation diversity without introducing irrelevant features. When combined with dissimilar sources, gradient conflicts or feature interference during pre-training may corrupt useful representations.
- **Core assumption:** The pre-training objective does not sufficiently filter irrelevant features from high-domain-gap data.
- **Evidence anchors:**
  - RALPN∪SAR-RARP50U showed +1.22% accuracy gain vs. RALPN alone
  - Something-Something-v2∪SAR-RARP50U showed -0.65% accuracy gain vs. Something-Something-v2 alone
- **Break condition:** If pre-training used contrastive objectives with explicit domain alignment regularization, combining dissimilar sources might not degrade performance.

### Mechanism 3
- **Claim:** Large-scale pre-training provides diminishing returns for linear evaluation but benefits temporal evaluation when more labeled examples are available.
- **Mechanism:** Linear classifiers rely directly on frozen encoder representations; domain-misaligned features lack discriminative power for SSA, hurting performance. Temporal models (TCN) can aggregate and reweight features over time, partially compensating for less relevant representations when sufficient labeled data guides the temporal aggregation learning.
- **Core assumption:** TCN can learn to extract signal from temporally coherent but spatially less discriminative features.
- **Evidence anchors:**
  - In the 5-shot setting, Something-Something-v2 achieves the highest accuracy
  - Large domain gap results in less effective representations which may lack the task-specific discriminative features necessary for effective linear classification
- **Break condition:** If downstream tasks require fine-grained spatial discrimination, temporal aggregation cannot fully compensate for poor spatial representations.

## Foundational Learning

- **Concept: Few-Shot Learning (FSL) Episode-Based Evaluation**
  - **Why needed here:** SSA evaluation uses k-shot episodes where k labeled samples per class train the model, with remaining samples for testing. Results are averaged over 100 random episodes to reduce variance.
  - **Quick check question:** Can you explain why episode-based evaluation is used instead of a single train/test split for few-shot tasks?

- **Concept: Self-Supervised Video Masked Autoencoding (VideoMAEv2)**
  - **Why needed here:** Pre-training uses VideoMAEv2, which masks spatiotemporal patches in 16-frame snippets and trains a ViT encoder-decoder to reconstruct them, learning temporal dynamics critical for skill assessment.
  - **Quick check question:** How does video masked autoencoding differ from image-based MAE, and why might temporal reconstruction benefit SSA?

- **Concept: Domain Gap Quantification via Earth Mover's Distance (EMD)**
  - **Why needed here:** The paper quantifies domain similarity using EMD between feature distributions extracted from pre-trained ViT-ImageNet, enabling systematic comparison of pre-training sources.
  - **Quick check question:** What does a lower EMD value between two datasets indicate about their relationship for transfer learning?

## Architecture Onboarding

- **Component map:** Pre-training: VideoMAEv2 (ViT-Small encoder + 4-layer decoder) → Feature Extraction: Frozen ViT-Small encoder → 16-frame clip features → Downstream: Option A: Linear classifier (frozen features) OR Option B: TCN head (temporal aggregation over clip features)

- **Critical path:**
  1. Select pre-training dataset with low EMD to downstream SSA task
  2. Pre-train VideoMAEv2 for 300 epochs (AdamW, lr=0.0006, batch=96)
  3. Extract frozen features from downstream videos
  4. Fine-tune linear classifier or TCN (30 epochs, lr=0.001, cosine schedule)
  5. Evaluate across 100 episodes per k-shot setting

- **Design tradeoffs:**
  - Small domain-aligned vs. large diverse pre-training: Small aligned better for low-shot; large diverse competitive for 5-shot temporal evaluation
  - Linear vs. temporal evaluation: Linear faster but requires better representations; TCN more expressive but needs more shots
  - Combined vs. single-source pre-training: Beneficial only when domain gap between sources is small

- **Failure signatures:**
  - EndoViT underperforms random initialization → excessive pre-training diversity may dilute task-specific features
  - Combined pre-training with dissimilar sources shows inconsistent or negative gains → domain gap introduces noise
  - Large F1-score variance across episodes → insufficient samples per class for stable evaluation

- **First 3 experiments:**
  1. **Baseline domain gap analysis:** Compute EMD between your pre-training candidates and downstream SSA dataset using ViT-ImageNet features; rank sources by EMD.
  2. **Single-source pre-training comparison:** Pre-train VideoMAEv2 on each source, evaluate linear and TCN heads across 1/2/5-shot settings; verify domain-aligned sources outperform larger diverse sources.
  3. **Combined dataset ablation:** Pre-train on procedure-specific + domain-aligned vs. procedure-specific + diverse source; confirm positive gains only for aligned combinations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the findings regarding domain alignment and pre-training source selection generalize to multi-center datasets with varying surgical environments?
- **Basis in paper:** The authors explicitly state that extending the analysis to additional datasets from different centers is necessary to validate the generalizability of their conclusions, which were limited to a single dataset due to computational demands.
- **Why unresolved:** The study relied on the SAR-RARP50 dataset from specific institutions, and the pre-training data sources may have institutional biases that are not accounted for.
- **What evidence would resolve it:** Replicating the few-shot SSA experiments on external robotic surgery datasets from geographically distinct centers to verify if domain-relevant data consistently outperforms large-scale generic data.

### Open Question 2
- **Question:** Can the proposed few-shot framework effectively scale to multi-class or regression-based skill assessment rather than binary classification?
- **Basis in paper:** The authors note that their formulation simplified SSA into a binary task because the narrow range of Global Rating Scores made fine-grained classification statistically unreliable with the current sample size.
- **Why unresolved:** It is unknown if the learned representations are discriminative enough to distinguish between intermediate skill levels or predict continuous scores in a low-data regime.
- **What evidence would resolve it:** Evaluating the pre-trained models on a dataset with a wider distribution of skill scores to assess performance on multi-class classification or regression tasks.

### Open Question 3
- **Question:** To what extent does the choice of SSL architecture (e.g., spatial vs. temporal reconstruction) interact with dataset diversity in pre-training?
- **Basis in paper:** The paper observes that EndoViT (trained on diverse data) underperforms, potentially due to "excessive diversity," but acknowledges this is not a direct comparison because EndoViT uses spatial-only Masked Autoencoders while the proposed method uses VideoMAEv2.
- **Why unresolved:** It remains unclear if the poor transfer performance of the diverse EndoViT model was caused by the dataset's domain gap or the model's inability to capture the temporal dynamics essential for skill assessment.
- **What evidence would resolve it:** Ablation studies comparing spatial-only versus video-based SSL methods pre-trained on the same diverse datasets to isolate the effect of temporal modeling from domain diversity.

## Limitations
- Evaluation limited to single surgical procedure (robotic-assisted radical prostatectomy) and specific OSATS score, limiting generalizability
- Does not explore alternative pre-training objectives beyond masked autoencoding
- Does not investigate transfer to more granular surgical subtasks or continuous skill assessment

## Confidence
- **High Confidence:** The superiority of small domain-aligned datasets over large but less relevant sources for few-shot SSA
- **Medium Confidence:** The claim that combining procedure-specific data with domain-aligned external datasets improves performance while combining with dissimilar sources degrades it
- **Medium Confidence:** The observation that temporal evaluation shows diminishing returns for large-scale pre-training in low-shot settings

## Next Checks
1. **Cross-procedure generalization:** Replicate the pre-training experiments on additional surgical procedures (laparoscopic cholecystectomy, cardiac surgery) to verify that domain alignment benefits generalize beyond robotic prostatectomy
2. **Alternative pre-training objectives:** Compare VideoMAEv2 against contrastive learning approaches (SimCLR, MoCo) for the same datasets to determine if masked autoencoding is optimal for SSA transfer
3. **Finer-grained assessment transfer:** Test whether pre-training benefits extend to more granular surgical metrics (instrument-tissue interaction, efficiency) rather than just OSATS aggregate scores