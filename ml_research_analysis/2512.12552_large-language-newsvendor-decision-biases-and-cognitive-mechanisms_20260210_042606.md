---
ver: rpa2
title: 'Large Language Newsvendor: Decision Biases and Cognitive Mechanisms'
arxiv_id: '2512.12552'
source_url: https://arxiv.org/abs/2512.12552
tags:
- decision
- biases
- cognitive
- demand
- newsvendor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how leading LLMs (GPT-4, GPT-4o, and LLaMA-8B)
  make decisions in the newsvendor problem, a canonical inventory management model.
  Through dynamic multi-round experiments across different demand distributions, the
  research reveals that LLMs consistently replicate and often amplify human cognitive
  biases like the "Too Low/Too High" ordering pattern, with GPT-4 deviating up to
  70% more than human benchmarks.
---

# Large Language Newsvendor: Decision Biases and Cognitive Mechanisms

## Quick Facts
- arXiv ID: 2512.12552
- Source URL: https://arxiv.org/abs/2512.12552
- Authors: Jifei Liu; Zhi Chen; Yuanguang Zhong
- Reference count: 10
- Primary result: LLMs consistently replicate and amplify human cognitive biases in inventory management decisions

## Executive Summary
This study investigates how leading LLMs (GPT-4, GPT-4o, and LLaMA-8B) make decisions in the newsvendor problem, a canonical inventory management model. Through dynamic multi-round experiments across different demand distributions, the research reveals that LLMs consistently replicate and often amplify human cognitive biases like the "Too Low/Too High" ordering pattern, with GPT-4 deviating up to 70% more than human benchmarks. A key finding is the "paradox of intelligence" - GPT-4, despite being the most sophisticated, exhibited the greatest deviations due to overanalysis, while efficiency-optimized GPT-4o achieved near-optimal performance. The study concludes that these biases stem from architectural constraints rather than knowledge gaps, as they persisted even when optimal formulas were provided.

## Method Summary
The researchers conducted dynamic multi-round experiments with three leading LLMs (GPT-4, GPT-4o, and LLaMA-8B) across different demand distributions in the newsvendor problem. They compared LLM decisions against human benchmarks and tested whether providing optimal formulas would reduce bias. The study examined ordering patterns, deviation magnitudes, and performance across model architectures to understand the cognitive mechanisms underlying AI decision-making in inventory management.

## Key Results
- LLMs consistently replicated and amplified human cognitive biases like the "Too Low/Too High" ordering pattern
- GPT-4 deviated up to 70% more than human benchmarks due to overanalysis
- GPT-4o achieved near-optimal performance as an efficiency-optimized model
- Biases persisted even when optimal formulas were provided, indicating architectural constraints

## Why This Works (Mechanism)
The study reveals that LLMs' decision biases stem from architectural constraints rather than knowledge gaps. When faced with the newsvendor problem, models rely on heuristic reasoning patterns that mirror human cognitive limitations. GPT-4's superior analytical capabilities paradoxically lead to greater deviations through overanalysis, while GPT-4o's efficiency optimization produces better results. The persistence of biases even with optimal formulas suggests that these are fundamental processing characteristics rather than information deficits.

## Foundational Learning
- Newsvendor problem fundamentals: Critical for understanding the inventory management context and optimal decision criteria
- Cognitive bias theory: Essential for interpreting AI decision patterns and comparing them to human behavior
- LLM architecture basics: Necessary to understand how model design influences decision-making processes
- Demand distribution analysis: Important for evaluating how different statistical patterns affect ordering decisions

Quick check: Verify understanding of newsvendor optimal order quantity formula
Quick check: Compare human vs. AI bias patterns in simple decision scenarios
Quick check: Map basic LLM architecture to observed decision behaviors

## Architecture Onboarding

Component map: User input -> Prompt processing -> Reasoning module -> Decision output -> Feedback loop

Critical path: The reasoning module is the critical path where biases manifest, as it processes the inventory problem using heuristic patterns rather than optimal calculations.

Design tradeoffs: Model sophistication vs. decision accuracy shows inverse relationship - more capable models like GPT-4 overanalyze, while efficiency-optimized GPT-4o performs better.

Failure signatures: "Too Low/Too High" ordering patterns, deviation from optimal solutions, persistence of biases despite formula provision.

First experiments:
1. Test different prompt structures with GPT-4 to isolate prompt-induced vs. architectural effects
2. Compare bias patterns across temperature settings to identify sensitivity thresholds
3. Evaluate single-round vs. multi-round decision consistency to understand learning limitations

## Open Questions the Paper Calls Out
None

## Limitations
- Medium confidence in the "paradox of intelligence" interpretation due to potential unmeasured factors
- Limited exploration of alternative explanations for bias persistence
- Generalizability to real-world inventory contexts may be limited
- Effectiveness of structured prompts may be context-dependent

## Confidence
High confidence in the core finding that LLMs systematically replicate and amplify human decision biases in inventory management tasks.
Medium confidence in the "paradox of intelligence" interpretation and the claim that biases stem from architectural constraints.
Medium confidence in the universal applicability of structured prompts to constrain heuristic tendencies.

## Next Checks
1. Replicate experiments across additional LLMs (Claude, Gemini) and inventory problem variants to test generalizability of bias patterns
2. Systematically vary prompt structure, temperature, and context length to isolate architectural vs. prompt-induced effects on bias magnitude
3. Conduct field studies with human-AI collaboration in actual inventory management scenarios to validate laboratory findings in operational settings