---
ver: rpa2
title: Optimised Feature Subset Selection via Simulated Annealing
arxiv_id: '2507.23568'
source_url: https://arxiv.org/abs/2507.23568
tags:
- feature
- algorithm
- selection
- value
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SA-FDR, a simulated annealing-based algorithm
  for $\ell0$-norm feature selection in classification tasks. It frames feature selection
  as a combinatorial optimisation problem, using the Fisher discriminant ratio (FDR)
  as a computationally efficient proxy for logistic regression model quality.
---

# Optimised Feature Subset Selection via Simulated Annealing

## Quick Facts
- **arXiv ID:** 2507.23568
- **Source URL:** https://arxiv.org/abs/2507.23568
- **Reference count:** 40
- **Primary result:** SA-FDR achieves sparser feature subsets than RFE/Lasso while maintaining high AUC on datasets up to 100K samples and 500 features

## Executive Summary
SA-FDR introduces a simulated annealing framework for ℓ₀-norm feature selection that performs global search over feature subsets using Fisher Discriminant Ratio (FDR) as a computationally efficient proxy. The method maintains multiple replicas with batched FDR computation to avoid overfitting and explores inter-feature dependencies missed by greedy methods. Experiments on nine datasets show SA-FDR consistently selects fewer features while achieving competitive or superior AUC compared to Recursive Feature Elimination and Lasso regularization.

## Method Summary
SA-FDR frames feature selection as a combinatorial optimization problem where the state space consists of all possible feature subsets of size k. The algorithm uses simulated annealing with 50 replicas, each maintaining a different subset, and employs Metropolis acceptance to probabilistically explore the search space. FDR serves as the objective function, computed using randomly sampled batch matrices to inject noise and prevent overfitting. After annealing converges, logistic regression is fitted to each replica's subset and the best performing subset is selected via cross-validation. The method scales to datasets with hundreds of thousands of samples and hundreds of features while maintaining computational tractability.

## Key Results
- SA-FDR consistently selects sparser feature subsets than RFE and Lasso while maintaining high predictive accuracy
- On high-dimensional datasets, SA-FDR achieved better or comparable AUC scores with fewer features
- The method maintains computational efficiency through FDR proxy, avoiding expensive cross-entropy optimization during search
- Multi-replica exploration with batch noise prevents convergence to identical local optima

## Why This Works (Mechanism)

### Mechanism 1: Global Search via Simulated Annealing
- Claim: Simulated annealing enables global search over feature subsets, escaping local optima that trap greedy methods
- Mechanism: The Metropolis acceptance rule probabilistically accepts worse feature subsets based on temperature T, allowing the search to escape local FDR maxima. As T decreases, the search converges toward high-FDR regions
- Core assumption: The temperature schedule is sufficiently slow to ensure convergence toward near-optimal subsets rather than arbitrary local minima
- Evidence anchors: Abstract states "uses simulated annealing to perform a global search over the space of feature subsets"; section 3.1 explains "allowing probabilistic transitions that occasionally accept worse solutions...avoids becoming trapped in local minima"

### Mechanism 2: FDR as Computational Proxy
- Claim: Fisher Discriminant Ratio serves as a computationally efficient proxy for logistic regression cross-entropy
- Mechanism: FDR quantifies class separability via the ratio of between-class to within-class scatter, avoiding iterative gradient descent. This reduces per-iteration cost from O(Nk) cross-entropy optimization to O(k²) matrix operations
- Core assumption: FDR correlates with cross-entropy for the target classification task. Figure 1 empirically shows this correlation, but it may not hold for non-linear class boundaries
- Evidence anchors: Abstract mentions "computationally efficient proxy"; section 2.2 states "correlates well with the cross entropy of a logistic regression (see Fig. 1)"

### Mechanism 3: Multi-Replica Batch Regularization
- Claim: Maintaining multiple replicas with batched FDR computation reduces overfitting and improves exploration
- Mechanism: R replicas explore different regions of feature space; using different random batches of data to compute SB and SW matrices at each step injects noise that prevents convergence to the same local optimum
- Core assumption: Batch-level variance in FDR estimates provides sufficient noise for regularization without destabilizing convergence
- Evidence anchors: Section 3.1 explains "to keep a baseline noise that avoids overfitting and helps produce replicas that have not converged to the same FDR local maximum"

## Foundational Learning

- **Concept:** Simulated Annealing and the Metropolis-Hastings Criterion
  - Why needed here: Core optimization algorithm; understanding acceptance probability and temperature schedules is essential for tuning
  - Quick check question: Given current FDR=2.0, proposed FDR=1.8, and β=0.5, what is the acceptance probability for the worse solution?

- **Concept:** Fisher Linear Discriminant and Generalized Rayleigh Quotient
  - Why needed here: Objective function; knowing how FDR captures class separability helps diagnose when it may fail
  - Quick check question: If within-class scatter matrix SW is near-singular, what happens to FDR computation and how might you regularize it?

- **Concept:** ℓ₀-norm Feature Selection vs. Embedded Regularization (Lasso/ℓ₁)
  - Why needed here: Frames the problem; understanding why ℓ₀ is NP-hard motivates the combinatorial approach versus convex relaxation
  - Quick check question: Why does Lasso (ℓ₁ regularization) tend to select more features than optimal ℓ₀ solutions for correlated feature groups?

## Architecture Onboarding

- **Component map:** Data → Batch Matrix Precomputation → SA Initialization → Annealing Loop → Cross-Entropy Refinement → Cross-Validation Wrapper
- **Critical path:** Temperature schedule → replica count → batch count → final selection criterion
- **Design tradeoffs:**
  - Replica count vs. runtime: Paper uses R=50; fewer replicas faster but may miss optimal regions
  - Sweeps per temperature (NS): NS=0.5 used; lower values risk incomplete equilibration per temperature
  - Final selection criterion: FDR vs. cross-entropy; paper selects by cross-entropy post-hoc, adding computational overhead but improving alignment with target metric
- **Failure signatures:**
  - All replicas converge to identical subsets: Increase replica count or batch noise
  - AUC degrades on validation set while FDR improves: FDR-proxy mismatch; consider alternative objectives
  - Runtime scales poorly with K: Precompute feature statistics; consider parallel replica evaluation
- **First 3 experiments:**
  1. Baseline reproduction: Run SA-FDR on UCI Heart dataset with paper hyperparameters (R=50, NS=0.5, ϵ=0.7); verify k*≈3-4 and AUC≈0.82
  2. Ablation on replica count: Compare R∈{10, 25, 50, 100} on Spambase dataset; plot k* and AUC vs. R to assess exploration-saturation point
  3. Objective function substitution: Replace FDR with direct cross-entropy (without batching) on Bankruptcy dataset; compare runtime and AUC to quantify proxy efficiency tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1:** How does SA-FDR perform when adapted for non-linear classifiers, such as neural networks, where the Fisher Discriminant Ratio may not be an adequate proxy for model accuracy? The authors state the algorithm is not restricted to logistic regression but theoretical correlation with non-linear model performance is unknown.

- **Open Question 2:** Can advanced simulated annealing techniques, such as parallel tempering, significantly improve the selection accuracy or convergence speed of SA-FDR? The conclusion invites "further refinements, such as improvements to the simulated annealing that have been proposed and used in fields such as statistical physics."

- **Open Question 3:** Does SA-FDR maintain computational tractability when scaling to ultra-high-dimensional datasets (K > 10,000) common in genomics? The introduction identifies genomics as a key application area, but experiments are limited to K ≤ 500 while runtime increases non-linearly with feature count.

## Limitations

- The batching strategy for FDR computation lacks precise specification, creating ambiguity in noise regularization strength
- Test-set-based standardization explicitly recommended by authors introduces potential data leakage
- Empirical validation of FDR as logistic regression quality proxy limited to specific datasets shown in Figure 1
- Computational complexity scaling for hundreds of thousands of samples not fully characterized

## Confidence

- **High:** Simulated annealing framework, Metropolis acceptance, general SA-FDR architecture
- **Medium:** FDR as proxy efficiency, multi-replica exploration benefits, overall AUC improvement claims
- **Low:** Exact impact of batch-noise regularization, temperature schedule optimization, reproducibility of k* selection across datasets

## Next Checks

1. Implement exact batch-size sweep (2-100) on Heart dataset to quantify noise regularization effects on FDR stability and AUC
2. Compare test-set vs. training-set standardization impact on AUC for all 9 datasets to quantify leakage effect
3. Benchmark SA-FDR runtime scaling on synthetic datasets with 100K-1M samples to characterize computational limits