---
ver: rpa2
title: Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken
  Language Understanding in SpeechLLMs
arxiv_id: '2508.17863'
source_url: https://arxiv.org/abs/2508.17863
tags:
- speech
- discrete
- tokens
- continuous
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive benchmark comparing
  self-supervised learning (SSL)-based discrete tokens and continuous features for
  spoken language understanding (SLU) in Speech Large Language Models (SpeechLLMs).
  The authors evaluate both paradigms across six SLU tasks using HuBERT-Large and
  WavLM-Large as SSL models, with Qwen1.5-0.5B and Llama3.1-8B as LLM decoders.
---

# Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs

## Quick Facts
- arXiv ID: 2508.17863
- Source URL: https://arxiv.org/abs/2508.17863
- Reference count: 8
- Primary result: Continuous features outperform discrete tokens on ASR, ST, and ER tasks, while discrete tokens excel at phoneme recognition

## Executive Summary
This paper presents the first comprehensive benchmark comparing self-supervised learning (SSL)-based discrete tokens and continuous features for spoken language understanding (SLU) in Speech Large Language Models (SpeechLLMs). The authors evaluate both paradigms across six SLU tasks using HuBERT-Large and WavLM-Large as SSL models, with Qwen1.5-0.5B and Llama3.1-8B as LLM decoders. Continuous features consistently outperform discrete tokens in most tasks, particularly for Automatic Speech Recognition (ASR), Speech Translation (ST), and Emotion Recognition (ER), with WavLM-Large continuous features achieving the best results. However, discrete tokens excel in Phoneme Recognition (PR), demonstrating their strength in capturing subword-level structure. The study also reveals distinct learning patterns between the two representations across SSL and LLM layers, with continuous features showing greater robustness in noisy conditions. Discrete tokens offer advantages in data and training efficiency, requiring 21-27% of the time needed for continuous features, but suffer from under-utilized vocabulary slots.

## Method Summary
The study compares discrete tokens (K-means clustering + BPE) versus continuous features (downsampled SSL embeddings) for SLU tasks in SpeechLLMs. Two SSL encoders (HuBERT-Large, WavLM-Large) generate speech features that are either quantized into discrete tokens (2000 centroids, 6000 vocab) or used directly as continuous features. Two LLM decoders (Qwen1.5-0.5B, Llama3.1-8B) process these representations via vocabulary extension or linear adapters. The benchmark evaluates six SLU tasks (ASR, PR, ST, KS, IC, ER) on multiple datasets (LibriSpeech, GigaSpeech, CHiME-4, etc.) using standard metrics (WER, PER, BLEU, accuracy). Training uses instruction tuning with task-specific prompts, AdamW optimization, and careful layer selection from SSL models.

## Key Results
- Continuous features achieve lower WER on ASR (4.91% vs 4.56% for HuBERT-Large) and ST tasks
- Discrete tokens outperform continuous features on Phoneme Recognition (7.02% PER vs 9.29%)
- Continuous features show superior emotion recognition accuracy (60-66% vs 37-40%)
- Discrete tokens require 21-27% of training time compared to continuous features
- Continuous features demonstrate better robustness to noisy conditions (8.68% vs 13.35% WER on CHiME-4)

## Why This Works (Mechanism)

### Mechanism 1
Continuous features preserve fine-grained acoustic and temporal information that quantization discards, enabling superior performance on tasks requiring rich signal detail. SSL embeddings (HuBERT/WavLM) retain continuous high-dimensional vectors (1024-dim × 25 fps). Discrete tokens compress these via K-means (k=2000) into 13-bit codes—a 99.9% bit-rate reduction. This compression preserves linguistic structure but sacrifices paralinguistic nuance (e.g., emotion cues). Continuous features maintain dense frame-level representations without sparse token utilization issues.

### Mechanism 2
Discrete tokens' categorical structure aligns naturally with phoneme-level classification, enabling superior subword-level task performance. K-means clustering on SSL features creates discrete codes that approximate phonetic categories. BPE subword modeling then merges frequent token subsequences. This categorical discretization maps speech directly to symbolic representations similar to text tokens the LLM already processes—reducing the modality gap for phoneme-level tasks specifically.

### Mechanism 3
Continuous features exhibit different layer-wise learning dynamics in both SSL encoders and LLM decoders, with emotion information concentrated in shallower SSL layers and text-speech alignment peaking deeper in LLMs. SSL layers encode different information hierarchically—shallower layers (4-6) capture paralinguistic emotion cues while deeper layers encode phonetic/semantic content. For LLM processing, continuous features show gradually increasing text-speech alignment through layer ~28, whereas discrete tokens peak at layer ~22 then decline—suggesting discrete tokens are processed more similarly to text earlier, then diverge.

## Foundational Learning

- **Self-Supervised Learning (SSL) speech representations**: Why needed? The entire comparison depends on understanding how models like HuBERT and WavLM encode speech into feature vectors suitable for both continuous and discrete use. Quick check: Can you explain why HuBERT learns through masked prediction of hidden units and how this creates representations suitable for both continuous and discrete use?

- **K-means discretization with BPE subword modeling**: Why needed? This is the core transformation pipeline converting continuous SSL features into discrete tokens; understanding the compression and vocabulary construction is essential for interpreting results. Quick check: Given a sequence of SSL frame features, can you trace how they become discrete tokens through K-means assignment, de-duplication, and BPE merging?

- **Speech-text modality alignment in LLMs**: Why needed? The paper assumes readers understand why aligning speech representations to LLM embedding space is necessary—via linear adapters for continuous or vocabulary extension for discrete. Quick check: Why does the paper use downsampling (rate=2) for continuous features but BPE compression for discrete tokens, and what tradeoff does each approach make?

## Architecture Onboarding

- **Component map**: Speech Input (16kHz) → SSL Encoder (HuBERT/WavLM) → [DISCRETE PATH: K-means (2000) → Deduplication → BPE (6000)] or [CONTINUOUS PATH: Downsample (rate=2) → Linear Adapter] → LLM Decoder (Qwen1.5-0.5B/Llama3.1-8B) → Task-specific output

- **Critical path**: SSL feature extraction quality determines both paths' upper bounds; K-means centroid count and BPE vocab size are coupled; downsampling rate >2 causes sharp WER degradation; discrete converges in 4-5 epochs vs 10-15 for continuous

- **Design tradeoffs**: Discrete tokens offer 21-27% training time, 99.9% bit-rate reduction, but suffer from ~20% under-utilized vocabulary slots and worse robustness on noisy data. Continuous features provide superior task performance and noise robustness but require 4-5× training time and more data bandwidth. Performance gap widens with larger LLMs.

- **Failure signatures**: Discrete tokens perform poorly on emotion recognition (~37-40% accuracy vs ~60-66% for continuous) due to quantization discarding paralinguistic cues; degraded performance on noisy ASR without additional training data; removing 10% under-trained tokens shows no WER change.

- **First 3 experiments**: 1) Baseline replication with HuBERT-Large + Qwen1.5-0.5B on LibriSpeech ASR to verify expected WER and convergence. 2) SSL layer ablation for emotion recognition across layers 4,8,12,16,20,24 to validate layer-wise patterns. 3) Robustness stress test: train on LibriSpeech 960h, evaluate zero-shot on CHiME-4, then add CHiME-4 data incrementally to measure discrete token improvement.

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced tokenizer designs overcome the failure of K-means-based discrete tokens to capture fine-grained paralinguistic cues required for Emotion Recognition (ER)? The study only evaluated semantic tokens derived from K-means clustering; it did not test acoustic tokenizers (e.g., RVQ-based) that may preserve the prosodic and spectral details necessary for ER.

### Open Question 2
How can the "wasted capacity" caused by under-trained discrete tokens be mitigated to improve utility efficiency? The paper identifies that ~20% of tokens account for only ~5% of occurrences, creating inefficiency, but does not propose methods to prune these tokens or balance their distribution effectively.

### Open Question 3
What specific architectural or training interventions are required to close the robustness gap between discrete tokens and continuous features in noisy environments? While discrete tokens improve with data augmentation, they still suffer from "suboptimal performance" in noisy settings compared to the "greater robustness" of continuous features.

### Open Question 4
Can a unified architecture effectively leverage the complementary strengths of discrete tokens (efficiency, phonetics) and continuous features (robustness, semantics)? The paper highlights these complementary strengths but leaves the potential for hybridization unexplored.

## Limitations

- Findings may not generalize to smaller LLM models or different architectural choices
- Fixed K-means centroids (k=2000) may be suboptimal for certain SSL encoders or tasks
- Discrete tokens require more task-specific data for noise robustness despite showing improvement

## Confidence

**High Confidence**: Continuous features outperform discrete tokens on ASR, ST, and ER tasks; discrete tokens excel at phoneme recognition; training efficiency: discrete tokens require 21-27% of continuous features' training time; bit-rate compression: discrete tokens achieve 99.9% reduction.

**Medium Confidence**: Layer-wise learning patterns differ between discrete and continuous representations; WavLM-Large continuous features achieve best overall performance; discrete tokens suffer from under-utilized vocabulary slots; continuous features show greater noise robustness.

**Low Confidence**: Exact mechanisms behind why discrete tokens fail on emotion recognition beyond "quantization loses paralinguistic cues"; whether layer-wise alignment patterns hold with different adapter architectures; optimal vocabulary size across different SSL encoders.

## Next Checks

1. **SSL Layer Ablation Study**: Extract features from multiple HuBERT and WavLM layers (not just final layers) for emotion recognition and phoneme tasks to validate whether observed layer-wise patterns are robust to layer selection.

2. **Vocabulary Size Optimization**: Systematically vary K-means centroids (k=1000, 2000, 4000, 8000) and BPE vocabulary sizes while measuring WER and vocabulary utilization to determine if under-utilization is due to suboptimal k=2000 or fundamental discretization limitations.

3. **Noise-Robust Training Protocol**: Train discrete tokens on increasingly noisy versions of LibriSpeech (additive noise at different SNRs) before evaluating on CHiME-4 to determine whether targeted noise-robust training can close the performance gap without requiring domain-matched data.