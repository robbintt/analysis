---
ver: rpa2
title: 'PT$^2$-LLM: Post-Training Ternarization for Large Language Models'
arxiv_id: '2510.03267'
source_url: https://arxiv.org/abs/2510.03267
tags:
- ternary
- quantization
- ternarization
- arxiv
- grid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PT\xB2-LLM introduces a post-training ternarization framework\
  \ for LLMs that addresses the underexplored challenges of training-free parameter\
  \ optimization and quantization difficulty posed by outliers and dispersed weights.\
  \ The method employs an Asymmetric Ternary Quantizer with two training-free refinement\
  \ stages: Iterative Ternary Fitting, which alternates between optimal ternary grid\
  \ construction and flexible rounding to minimize quantization error, and Activation-aware\
  \ Grid Alignment, which refines the ternary grid to better match full-precision\
  \ outputs."
---

# PT$^2$-LLM: Post-Training Ternarization for Large Language Models

## Quick Facts
- **arXiv ID:** 2510.03267
- **Source URL:** https://arxiv.org/abs/2510.03267
- **Reference count:** 8
- **Primary result:** Achieves competitive performance to 2-bit PTQ while using lower memory and accelerating end-to-end inference

## Executive Summary
PT$^2$-LLM introduces a post-training ternarization framework that addresses the underexplored challenges of training-free parameter optimization and quantization difficulty posed by outliers and dispersed weights. The method employs an Asymmetric Ternary Quantizer with two training-free refinement stages: Iterative Ternary Fitting, which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and Activation-aware Grid Alignment, which refines the ternary grid to better match full-precision outputs. Additionally, a Structural Similarity-based Reordering strategy is introduced to reorganize columns based on inter-column structural correlation, easing quantization and mitigating outlier effects. Experimental results demonstrate that PT$^2$-LLM achieves competitive performance against state-of-the-art 2-bit post-training quantization methods while using lower memory and accelerating both prefill and decoding for end-to-end speedup.

## Method Summary
PT$^2$-LLM is a post-training quantization (PTQ) method that compresses FP16 weights to ternary values {-1, 0, +1} with asymmetric scaling. The method uses 128 calibration samples from WikiText2 and operates on LLaMA (7B-65B), LLaMA-2, LLaMA-3, and Qwen3-base models. The approach consists of four main components: (1) Asymmetric Ternary Quantizer with row-wise offset μ, (2) Iterative Ternary Fitting (~10 iterations alternating optimal grid via closed-form solution and flexible rounding), (3) Activation-aware Grid Alignment using calibration activations to refine scaling parameters (freezing T to avoid overfitting), and (4) Structural Similarity-based Reordering with block size 128 for GPTQ-style sequential quantization.

## Key Results
- Achieves competitive perplexity to 2-bit PTQ methods on WikiText2 and C4 benchmarks
- Maintains zero-shot accuracy on 7 QA tasks (PiQA, ARC-e, ARC-c, HellaSwag, Winogrande, OBQA, BoolQ)
- Demonstrates lower memory usage compared to 2-bit quantization methods
- Provides end-to-end speedup by accelerating both prefill and decoding stages

## Why This Works (Mechanism)

### Mechanism 1: Iterative Ternary Fitting (ITF)
Alternating between optimal grid construction and flexible rounding reduces weight quantization error without training. Given a fixed ternary matrix T, solves closed-form equations for optimal scaling α and shift µ. Then updates T via element-wise rounding to minimize ||W - αT - µ||²_F. Repeats until T stabilizes. Core assumption: LLM weight rows have non-zero means; centering via µ better captures distribution bias than symmetric ternarization.

### Mechanism 2: Activation-aware Grid Alignment (AGA)
Refining the ternary grid to minimize output error (rather than just weight error) better preserves model behavior. After ITF converges, freezes T and re-optimizes (α, µ) using calibration data X to minimize ||WX - (αT + µ)X||²_F. Single-pass update prevents overfitting. Core assumption: Calibration data activations are representative of inference-time activations.

### Mechanism 3: Structural Similarity-based Reordering (SSR)
Clustering columns by structural similarity before blockwise quantization reduces variance and outlier impact. Computes cosine similarity between columns. During GPTQ-style sequential quantization, selects top-k most similar columns for next block. Permutation does not change matrix multiplication result. Core assumption: Similar columns have similar magnitude distributions; grouping them yields tighter per-block ranges.

## Foundational Learning

- **Concept: Post-training quantization (PTQ) vs. Quantization-aware training (QAT)**
  - Why needed here: PT²-LLM is a PTQ method; understanding this distinction explains why iterative fitting (not backpropagation) is used.
  - Quick check question: Why can't PTQ methods update weights via gradient descent?

- **Concept: Ternary representation {−1, 0, +1} with per-row scaling**
  - Why needed here: Core representation; enables 1.58-bit effective precision while eliminating most multiplications.
  - Quick check question: How many bits are needed to store a ternary value, and why is effective bitwidth 1.58?

- **Concept: Blockwise quantization with error compensation (GPTQ foundation)**
  - Why needed here: SSR operates within GPTQ's sequential block framework; understanding this clarifies why reordering is needed per-block.
  - Quick check question: Why does quantizing a large matrix in independent blocks require special handling?

## Architecture Onboarding

- **Component map:** SSR (preprocessing) -> ATQ Initialization -> ITF Loop -> AGA (final) -> Dequantization
- **Critical path:** ITF convergence → AGA alignment. Ew should decrease during ITF; Ex should drop sharply at AGA (monitor as sanity check).
- **Design tradeoffs:** Calibration size: 128 samples sufficient; 256 gives marginal gains. Block size: Fixed at 128 per paper; smaller blocks may improve granularity but increase overhead. Updating T during AGA: Intentionally skipped to avoid overfitting.
- **Failure signatures:** High perplexity (>100) on LLaMA-3-8B suggests insufficient ITF iterations or poor initialization. AGA increasing Ex indicates calibration data mismatch. SSR providing no gain suggests column structure lacks correlation.
- **First 3 experiments:**
  1. Reproduce ITF convergence: Run ATQ on LLaMA-2-7B, log Ew per iteration. Should stabilize in ~10 steps.
  2. Ablate AGA: Compare ITF-only vs ITF+AGA on WikiText2 perplexity. Expect ~4-5 point drop.
  3. Verify SSR effect: Plot block-wise variance before and after reordering on a single layer. Should show reduction.

## Open Questions the Paper Calls Out

### Open Question 1
Can the ternary matrix $T$ be refined during the Activation-aware Grid Alignment (AGA) stage without inducing overfitting to the calibration set? The current method resorts to freezing parameters to ensure robustness, leaving the optimization of the ternary structure under output-aware constraints as an unsolved problem. A regularization technique or optimization constraint that allows $T$ to update during AGA while maintaining or improving generalization would resolve this.

### Open Question 2
How effective is the Structural Similarity-based Reordering (SSR) strategy when applied to Mixture-of-Experts (MoE) architectures with sparse activation patterns? The introduction highlights the massive scale of MoE models like DeepSeek-R1 as a motivation for compression, yet the experimental evaluation is restricted to dense LLaMA and Qwen models. Benchmark results on MoE models showing SSR successfully reduces block-wise variance and retains accuracy comparable to dense model results would resolve this.

### Open Question 3
Can the Structural Similarity-based Reordering (SSR) be combined with rotation-based transformations to further mitigate outlier effects? SSR relies on column similarity, which might be altered or enhanced by random rotations that pre-process weights to be more Gaussian. Ablation studies testing PT$^2$-LLM with and without rotation transforms prior to SSR, reporting changes in perplexity and zero-shot accuracy, would resolve this.

## Limitations
- Reliance on representative calibration data for AGA effectiveness, with unclear criteria for calibration data quality
- SSR effectiveness dependent on column correlation structure, which may vary across different LLM architectures and layers
- Lack of detailed analysis of trade-off between block size and quantization quality across different model sizes

## Confidence
- **High Confidence:** Iterative Ternary Fitting effectively reduces weight quantization error; Activation-aware Grid Alignment improves output preservation
- **Medium Confidence:** Structural Similarity-based Reordering meaningfully reduces block-wise variance; 1.58-bit effective precision claim is mathematically sound but practical impact needs validation
- **Low Confidence:** Claims of "competitive performance" vs 2-bit PTQ difficult to verify without standardized benchmarks; lower memory usage requires careful implementation consideration

## Next Checks
1. **Calibration Data Sensitivity Analysis:** Systematically evaluate PT²-LLM performance using calibration data from different domains and sequence lengths to establish robustness to data distribution shifts.
2. **Cross-Architecture Generalization Test:** Apply PT²-LLM to transformer architectures beyond LLaMA family and measure SSR effectiveness across different weight matrix structures.
3. **Hardware-Aware Efficiency Validation:** Implement PT²-LLM on actual edge devices and measure end-to-end latency, memory bandwidth, and energy consumption during inference compared to FP16 and 2-bit PTQ implementations.