---
ver: rpa2
title: Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent
  Data Generation
arxiv_id: '2512.03445'
source_url: https://arxiv.org/abs/2512.03445
tags:
- knowledge
- disease
- medical
- image
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a vision-language pretraining (VLP) framework
  for medical image analysis that addresses two main challenges: noisy web-collected
  data and the difficulty of learning from long, unstructured medical texts. The proposed
  solution, O-MAKE, combines a Multi-Agent data GENeration (MAGEN) system with an
  Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining approach.'
---

# Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation

## Quick Facts
- arXiv ID: 2512.03445
- Source URL: https://arxiv.org/abs/2512.03445
- Reference count: 40
- One-line primary result: O-MAKE achieves state-of-the-art zero-shot performance on 8 dermatology datasets with 54.4% accuracy on disease classification and 45.3% on cross-modal retrieval.

## Executive Summary
This paper introduces O-MAKE, a vision-language pretraining framework for medical image analysis that addresses two key challenges: noisy web-collected data and long, unstructured medical texts. The framework combines a Multi-Agent data GENeration (MAGEN) system with an Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining approach. MAGEN improves data quality through foundation model-assisted captioning and retrieval-based verification, while O-MAKE decomposes long texts into distinct knowledge aspects for fine-grained alignment. The system achieves state-of-the-art zero-shot performance on dermatology tasks and releases a new augmented dataset, Derm1M-AgentAug.

## Method Summary
The method operates in two stages. MAGEN uses a foundation model (PanDerm v2) to predict top-5 diagnoses for each image, then employs a Captioning Agent (LLaVA-v1.5 with PanDerm V2 + Qwen3-14B) to generate descriptions. A Summary Agent (Qwen2.5-72B) distills knowledge into structured "Disease Cards," and a Verification Agent (Qwen2.5-VL-72B with RAG) refines captions by cross-referencing against these cards. O-MAKE pretraining then uses a ViT-B/16 encoder and GPT-2 decoder, decomposing text into Ontology Captions, Visual Concept Captions, and sentence-level Sub-Captions. The framework employs multi-knowledge image alignment with ontology-based soft labels to maintain semantic hierarchy during contrastive learning.

## Key Results
- Achieves 54.4% average accuracy on zero-shot disease classification across eight dermatology datasets
- Achieves 45.3% average Recall@10/50/100 on cross-modal retrieval tasks
- Demonstrates consistent improvement over baseline models across all evaluation datasets
- Releases Derm1M-AgentAug, an augmented dataset with over 400k high-quality skin-image-text pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data quality improves when generative hallucinations are constrained by external knowledge retrieval.
- **Mechanism**: MAGEN uses a Verification Agent to cross-reference generated captions against curated "Disease Cards" (structured summaries of 371 diseases) using RAG. The agent triangulates input image, preliminary caption, and retrieved clinical facts to rectify inconsistencies.
- **Core assumption**: Disease Cards contain accurate ground truth and the verification agent can reliably detect contradictions between visual evidence and text.
- **Evidence anchors**: Abstract mentions "retrieval-based verification pipeline"; Section III-B describes "triangulating information from the input image, the preliminary caption, and the retrieved Disease Cards"; KERAP supports agent-based verification workflows.
- **Break condition**: If knowledge base contains incorrect diagnostic criteria, verification agent will enforce wrong facts, propagating label noise.

### Mechanism 2
- **Claim**: Decomposing long medical texts into distinct semantic aspects improves alignment over truncation.
- **Mechanism**: O-MAKE splits raw text into Ontology Caption, Visual Concept Caption, and sentence-level Sub-Captions, then applies Multi-Knowledge Image Alignment using multi-positive contrastive loss to force visual embedding to align with all textual representations simultaneously.
- **Core assumption**: Semantic signal lost in standard truncation is recoverable via LLM-based extraction, and single image embedding can effectively serve as hub for multiple distinct textual modalities.
- **Evidence anchors**: Abstract states "decomposing them into distinct knowledge aspects... facilitates fine-grained alignment"; Section III-C describes "Multi-Knowledge Image Alignment... enforces... positive alignment with all textual embeddings"; MedFILIP highlights importance of fine-grained language-image pre-training.
- **Break condition**: If LLM extraction fails to isolate relevant aspects, contrastive loss will optimize image embedding toward noisy, conflicting targets.

### Mechanism 3
- **Claim**: Treating ontologically related diseases as "soft negatives" preserves semantic hierarchy during representation learning.
- **Mechanism**: O-MAKE uses Ontology-Based Soft Labels that calculate similarity score based on shared ancestors in disease tree and reduce penalty for confusing related diseases while maintaining strict separation for unrelated ones.
- **Core assumption**: Visual features of diseases correlate strongly with their position in taxonomic tree (ontologically close diseases look similar).
- **Evidence anchors**: Abstract mentions "explicitly modeling medical concept relationships through ontology-guided mechanisms"; Section III-C describes "assigning larger soft-label values to ontologically related diseases"; SLIP emphasizes utilizing relational structure in pretraining.
- **Break condition**: If ontology is purely taxonomic but not phenotypic, forcing visual embeddings to respect this hierarchy may reduce discrimination between visually distinct but biologically related conditions.

## Foundational Learning

- **Concept**: Contrastive Language-Image Pre-training (CLIP)
  - **Why needed here**: This is the base architecture upon which O-MAKE builds. You must understand how standard image-text contrastive loss works (one-to-one matching) to grasp why the paper introduces "multi-positive" and "soft-label" modifications.
  - **Quick check question**: How does the loss function change if an image has three valid positive text labels instead of one?

- **Concept**: Hierarchical Ontologies
  - **Why needed here**: The core innovation relies on quantifying "relatedness" between diseases. You need to understand how to compute similarity (e.g., Wu-Palmer similarity) based on tree distance to implement the soft-labeling logic.
  - **Quick check question**: If Disease A and B share a parent node, but Disease C is in a completely different branch, how should the training loss treat the distance between their embeddings differently?

- **Concept**: Retrieval-Augmented Generation (RAG)
  - **Why needed here**: MAGEN pipeline relies on Verification Agent that uses RAG. Understanding how "Summary Agent" distills knowledge into "Disease Cards" and how "Verification Agent" retrieves them is critical for debugging data generation failures.
  - **Quick check question**: What happens to verification process if retrieval step fails to find matching "Disease Card" for rare input diagnosis?

## Architecture Onboarding

- **Component map**: Input Image -> PanDerm (Top-5 Diagnosis Priors) + Caption Agent (LLaVA) -> Draft Caption + Summary Agent (Retrieval) -> Verification Agent (RAG Fact-Check) -> Cleaned Pair -> ViT Encoder. Text -> Splitter/LLM -> (Raw, Ontology, Concept, Sub-captions) -> Text Encoder. Losses: Ontology Weighting + Multi-Positive Contrastive Loss + Soft-Label Loss.

- **Critical path**: The Verification Agent is the gatekeeper of data quality. If it hallucinates or is too strict ("No definitive diagnosis"), pretraining dataset either becomes noisy or significantly smaller, directly impacting downstream zero-shot accuracy.

- **Design tradeoffs**:
  - *LLM Dependency vs. Automation*: System relies heavily on GPT-4o/Qwen for data synthesis, trading manual labeling cost for API inference costs and potential algorithmic biases.
  - *Granularity vs. Noise*: Splitting text into fine-grained sub-captions helps alignment but risks amplifying noise if source text contains irrelevant clinical chitchat. Ontology-Guided Weighting attempts to mitigate this.

- **Failure signatures**:
  - *Data Drop*: If Verification Agent rejects >50% of samples (flagging "No definitive diagnosis"), check Retrieval parameters or strictness of prompt.
  - *Collapse to Mode*: If model outputs generic skin descriptions for all inputs, Ontology-Guided Weighting may be down-weighting specific diagnostic terms too aggressively.
  - *Confusion Matrix*: If model confuses clinically distinct but ontologically close relatives, soft-label parameter Î² may be set too high, preventing model from learning discriminative boundaries.

- **First 3 experiments**:
  1. **MAGEN Ablation**: Train baseline VLP model on raw Derm1M vs. Derm1M-AgentAug (Table III). Verify that "Verified" caption component provides +1.3% bump over just "Captioning."
  2. **Soft-Label Validation**: Visualize embeddings (t-SNE) with and without soft-label loss (Fig. 4). Confirm that ontologically related diseases cluster closer together only when specific ontology loss is active.
  3. **Long-Text Stress Test**: Evaluate retrieval performance on SkinCAP dataset (avg 80 tokens). Compare standard CLIP (truncates) vs. O-MAKE (decomposes) to verify that decomposition mechanism actually recovers information lost to truncation.

## Open Questions the Paper Calls Out
- Adapting this approach to additional specialties and general medicine
- Exploring dynamic ontology construction for emerging diseases
- Improving zero-shot generalization to rare and novel diseases

## Limitations
- Data quality control bottleneck where verification agent's reliability is not independently validated
- Ontology structure ambiguity with missing details on tree construction and validation
- Heavy LLM dependency creating reproducibility and generalization concerns

## Confidence
- **Data Quality Improvement (MAGEN)**: Medium - mechanism is plausible but verification agent reliability unproven
- **Long-Text Alignment (O-MAKE Decomposition)**: Medium - decomposition approach novel but lacks ablation evidence for information recovery
- **Ontology-Based Soft Labels**: Low - concept sound but implementation details missing for verification
- **State-of-the-Art Performance**: High - reported accuracy and retrieval performance clearly stated and reproducible

## Next Checks
1. **MAGEN Verification Agent Ablation**: Train baseline VLP model on raw Derm1M and compare to model trained on Derm1M-AgentAug where verification agent is replaced with simple heuristic (e.g., keyword matching). This will isolate contribution of LLM-based verification and quantify its impact on data quality and downstream performance.

2. **Ontology Tree Structure Analysis**: Visualize disease embeddings with and without soft-label loss (t-SNE or UMAP) and color-code points by their ontology-based similarity scores. This will reveal whether ontologically related diseases cluster together only when soft-label loss is active, confirming mechanism's effect.

3. **Long-Text Dataset Stress Test**: Evaluate model on medical image-text dataset with extremely long captions (e.g., radiology reports) to test whether decomposition mechanism scales beyond dermatology. Compare standard CLIP (truncates) vs. O-MAKE (decomposes) to verify that method generalizes to other medical specialties with longer textual descriptions.