---
ver: rpa2
title: Compositional Reasoning with Transformers, RNNs, and Chain of Thought
arxiv_id: '2503.01544'
source_url: https://arxiv.org/abs/2503.01544
tags:
- node
- will
- each
- depth
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the expressive power of transformers, RNNs,
  and transformers with chain of thought on Compositional Reasoning Questions (CRQs),
  a class of hierarchical reasoning tasks. The authors prove that none of these architectures
  can solve CRQs without some hyperparameter (depth, dimension, or number of CoT tokens)
  growing with input size.
---

# Compositional Reasoning with Transformers, RNNs, and Chain of Thought

## Quick Facts
- arXiv ID: 2503.01544
- Source URL: https://arxiv.org/abs/2503.01544
- Authors: Gilad Yehudai; Noah Amsel; Joan Bruna
- Reference count: 40
- Key outcome: None of transformers, RNNs, or transformers with CoT can solve CRQs without hyperparameters growing with input size; each has distinct strengths with different tradeoffs.

## Executive Summary
This paper establishes fundamental limits on the expressive power of transformers, RNNs, and transformers with chain-of-thought for solving Compositional Reasoning Questions (CRQs). The authors prove that no single architecture can solve all CRQs without some hyperparameter (depth, dimension, or CoT tokens) growing with input size. Through explicit constructions, they show transformers need logarithmic depth, RNNs need logarithmic dimension (with good ordering), and transformers with CoT need linear tokens to solve CRQs. The key insight is that while CRQs are inherently hard, each architecture has distinct strengths and weaknesses in solving them, with no single architecture being strictly superior.

## Method Summary
The paper provides explicit weight constructions for three architectures solving CRQs on rooted trees where each node has a vector label. For deep transformers, they use L layers with positional encodings to traverse the tree level-by-level in parallel. For RNNs, they simulate a stack machine that processes nodes in memory-rank order to minimize memory requirements. For transformers with CoT, they use a 2-layer architecture that autoregressively generates one token per sub-question. All constructions use hardmax attention and finite precision (O(log n) bits) to prove that log-depth, O(log n) dimension, and O(n) CoT tokens are sufficient for solving CRQs.

## Key Results
- Transformers require logarithmic depth to solve CRQs, with constant-depth transformers provably insufficient (implies TC0 = NC1)
- RNNs need O(log n) hidden dimension with optimal input ordering, but require O(n) dimension with adversarial ordering
- Transformers with CoT need n tokens for n-node trees, improving on previous O(n²) approaches
- CRQs are NC¹-hard, establishing fundamental limits on all three architectures

## Why This Works (Mechanism)

### Mechanism 1: Deep Transformer Parallel Tree Traversal
- **Claim:** Transformers solve CRQs by parallel layer-by-layer tree traversal, with depth requirements scaling logarithmically with problem size.
- **Mechanism:** Each transformer layer simultaneously resolves all sub-questions at a given tree depth. Nodes at depth L−1 attend to their children, compute answers via argmax inner product, then transform into pseudo-leaves for the next layer. This propagates upward until the root is resolved.
- **Core assumption:** TC0 ≠ NC1; positional encodings capture tree structure with O(log n) bits.
- **Evidence anchors:** [abstract] "For transformers, our construction uses depth that is logarithmic in the problem size." [section 4.1, Theorem 4.1] Explicit construction with depth L−1 solving CRQs of depth L. [corpus] Related work confirms log-depth transformers can perform k-hop induction and graph algorithms.

### Mechanism 2: RNN Stack Machine Simulation
- **Claim:** RNNs solve CRQs by simulating a stack machine, with memory requirements determined by input ordering.
- **Mechanism:** Under memory-rank sort ordering, the RNN maintains a stack of intermediate results. When processing node v: (1) if its children aren't yet computed, push v; (2) if children are on stack top, pop them, compute argmax, push result. Stack depth ≤ memory-rank(T) ≤ log(n).
- **Core assumption:** Inputs arrive in memory-rank sorted order; ReLU networks can approximate inner products to sufficient precision.
- **Evidence anchors:** [section 5.3, Theorem 5.4] "There exists an RNN with 5 layers and hidden dimension O(d·log(n)) that solves any CRQ with a binary tree, if the nodes are ordered by Algorithm 1." [section 5.1, Theorem 5.2] Adversarial ordering forces Ω(n) hidden dimension.
- **Break condition:** Wrong input ordering forces linear hidden dimension.

### Mechanism 3: CoT Autoregressive Sub-question Generation
- **Claim:** Shallow transformers with chain-of-thought solve CRQs by autoregressively generating one CoT token per sub-question.
- **Mechanism:** A 2-layer transformer generates n tokens sequentially. First layer attends to children; second layer enforces reverse-BFS ordering so each new token attends to the next unresolved node. Each CoT token embeds one sub-question's answer.
- **Core assumption:** O(n) CoT tokens are acceptable; bit-precision is O(log n); reverse-BFS ordering is encoded in positional embeddings.
- **Evidence anchors:** [section 6, Theorem 6.1] "2-layer transformer...solves all CRQs with n nodes...generates n CoT tokens." [section 6] Comparison to Feng et al. (2024): their arithmetic solution needs O(n²) tokens vs. this paper's O(n).
- **Break condition:** O(log n) CoT tokens provably insufficient under TC0 ≠ NC1.

## Foundational Learning

- **Concept: NC¹-completeness and Boolean formula evaluation**
  - **Why needed here:** CRQs are proven NC¹-hard via reduction from Boolean Formula Evaluation. Understanding that NC¹ problems require either logarithmic depth or linear sequential steps is essential for interpreting the tradeoffs.
  - **Quick check question:** Why can't a constant-depth circuit evaluate arbitrary Boolean formulas?

- **Concept: Communication complexity lower bounds**
  - **Why needed here:** Theorem 5.2's proof reduces from set disjointness to show adversarial RNN ordering requires Ω(n) memory. Understanding how two-party communication protocols relate to sequential model memory is key.
  - **Quick check question:** If Alice processes first n inputs and Bob processes next n, what must the hidden state communicate?

- **Concept: Stack machines and post-order traversal**
  - **Why needed here:** The RNN construction simulates a stack-based tree evaluation. Memory-rank is the minimal stack depth under optimal child ordering (analogous to reverse Polish notation evaluation).
  - **Quick check question:** For a binary tree, what ordering minimizes maximum stack depth during post-order evaluation?

## Architecture Onboarding

- **Component map:** CRQ Input → Node embeddings (x_v, z_v, z_parent(v), depth) → Architecture choice → Answer vector at root

- **Critical path:**
  1. Encode tree structure in positional embeddings (z_v vectors with low pairwise inner products)
  2. Choose architecture based on constraints: parallel runtime (transformer), parameter count (CoT), or inference efficiency (RNN with good ordering)
  3. Ensure hyperparameters scale with problem size: depth ~ log(n), or hidden_dim ~ log(n), or CoT_tokens ~ n

- **Design tradeoffs:**
  | Architecture | Parameters | Runtime | Parallel Runtime | Constraint |
  |--------------|------------|---------|------------------|------------|
  | Deep Transformer | O(L log²n) | O(Ln² log n) | O(L) | Depth scales with tree |
  | RNN (good order) | O(log n) | O(n log n) | O(n) | Ordering-dependent |
  | RNN (bad order) | O(n) | O(n²) | O(n) | Linear memory |
  | CoT | O(log² n) | O(n² log² n) | O(n) | Sequential, many tokens |

- **Failure signatures:**
  - Transformer with constant depth fails on deep trees (returns wrong answers, not just degraded)
  - RNN with O(log n) hidden dim on adversarially-ordered input: stack overflow → corrupted state
  - CoT with o(n) tokens: incomplete tree traversal, root answer undefined

- **First 3 experiments:**
  1. **Verify depth scaling:** Train transformers with depths 2, 4, 8, 16 on balanced binary CRQs of sizes 16, 64, 256, 1024. Confirm accuracy drops when depth < log(n).
  2. **Test RNN ordering sensitivity:** Compare RNN performance on same CRQs with (a) memory-rank sorted order, (b) random order, (c) adversarial order from Theorem 5.2 construction. Measure hidden dimension required for >90% accuracy.
  3. **CoT token ablation:** Run shallow transformer with CoT, limiting tokens to k ∈ {n/4, n/2, n, 2n}. Verify n tokens necessary and sufficient for full accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can constant-depth transformers solve Compositional Reasoning Questions (CRQs) using o(n) chain-of-thought (CoT) tokens? The authors identify the gap between Ω(log n) (insufficient) and O(n) (sufficient) CoT tokens as open, asking if their construction using n tokens is optimal.

- **Open Question 2:** Can the theoretically constructed models be trained to solve CRQs using standard optimization methods? The paper proves transformers and RNNs can represent solutions but does not analyze if gradient-based optimization can actually find these solutions.

- **Open Question 3:** Do solutions learned by transformers for specific CRQ structures generalize to trees with different sizes, structures, and question regimes? The authors ask about generalization to other trees and regimes, noting that theoretical guarantees are for fixed input sizes and tree structures.

## Limitations

- **Softmax Approximation Gap:** Theoretical constructions use hardmax attention but practical deployment requires softmax. The paper claims extension with temperature tuning but doesn't specify the precise schedule or demonstrate it experimentally.

- **Sequential RNN Ordering Sensitivity:** The RNN construction shows dramatic performance variation based on input ordering (O(log n) vs O(n) hidden dimension). While memory-rank sorting is optimal, the paper doesn't evaluate how close practical sorting heuristics perform.

- **CoT Token Efficiency Gap:** The CoT construction requires n tokens for n-node trees, which is better than previous O(n²) approaches but still substantial. The paper identifies a gap between the log(n) lower bound and n token upper bound as an open question.

## Confidence

**High Confidence Claims:**
- CRQs are NC¹-hard, requiring either logarithmic depth or linear sequential steps
- Constant-depth transformers cannot solve all CRQs (Theorem 4.3)
- RNNs require Ω(n) hidden dimension for adversarially ordered inputs (Theorem 5.2)
- O(log n) CoT tokens cannot solve all CRQs (Theorem 6.2)

**Medium Confidence Claims:**
- Explicit constructions actually work as described (due to lack of empirical validation)
- The O(n) vs O(log n) memory-rank gap is tight for RNNs (no lower bounds provided)
- The log(n) vs n CoT token gap can be closed

**Low Confidence Claims:**
- Softmax extensions will perform as well as hardmax constructions
- Practical sorting heuristics will achieve near-memory-rank performance

## Next Checks

1. **Experimental Implementation:** Implement the deep transformer construction from Theorem 4.1 with depth L = ⌈log₂(n)⌉ and test on balanced binary CRQs of sizes 16, 64, 256, 1024. Measure accuracy drop when depth < log(n) and verify the logarithmic depth requirement empirically.

2. **RNN Ordering Sensitivity Analysis:** Compare RNN performance on CRQs with (a) memory-rank sorted order, (b) random order, and (c) adversarial order from Theorem 5.2. Measure the actual hidden dimension required for >90% accuracy and quantify how close practical orderings come to the theoretical memory-rank bound.

3. **CoT Token Efficiency Benchmark:** Run the shallow transformer with CoT construction, varying token count k ∈ {n/4, n/2, n, 2n} on CRQs with n ∈ {16, 64, 256, 1024}. Confirm n tokens are necessary and sufficient for full accuracy, and test whether any intermediate token counts provide partial solutions.