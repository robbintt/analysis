---
ver: rpa2
title: 'Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection'
arxiv_id: '2506.18145'
source_url: https://arxiv.org/abs/2506.18145
tags:
- mamba
- layers
- experts
- arxiv
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Routing Mamba (RoM), the first framework for
  integrating sparse mixture of experts with State Space Models by leveraging Mamba's
  projection layers as scalable expert components. Unlike naive MoE integration that
  degrades performance, RoM employs selective scaling and shared routing decisions
  across Conv, Gate, and Output projections, enabling effective sparse scaling of
  Mamba layers.
---

# Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection

## Quick Facts
- **arXiv ID**: 2506.18145
- **Source URL**: https://arxiv.org/abs/2506.18145
- **Reference count**: 40
- **One-line primary result**: 1.3B active-parameter Routing Mamba matches dense Mamba with 2.3× more active parameters

## Executive Summary
This work introduces Routing Mamba (RoM), the first framework for integrating sparse mixture of experts with State Space Models by leveraging Mamba's projection layers as scalable expert components. Unlike naive MoE integration that degrades performance, RoM employs selective scaling and shared routing decisions across Conv, Gate, and Output projections, enabling effective sparse scaling of Mamba layers. At 1.3B active parameters (10B total), RoM achieves language modeling performance equivalent to dense Mamba models requiring 2.3× more active parameters, with consistent perplexity across context lengths.

## Method Summary
RoM applies mixture-of-experts to Conv, Gate, and Output projections of Mamba with shared routing decisions across these layers. The approach selectively scales only the high-parameter projections while sharing the small SSM-specific projections (x Proj, dt Proj, Conv1D) across all experts. A single router selects Top-1 expert for each token, and this routing decision is reused across all three projection layers, enabling coherent expert specialization while reducing router learning burden.

## Key Results
- 1.3B active-parameter RoM matches dense Mamba-1.3B performance while using only 1.3B active parameters (vs 3B for dense)
- Consistent perplexity across context lengths (4K/8K/16K) with no degradation
- 23% FLOPS saving when applied to hybrid Samba models
- Scales effectively from 115M to 1.3B active parameters

## Why This Works (Mechanism)

### Mechanism 1: Shared Routing Enables Coherent Expert Specialization
Reusing a single routing decision across Conv, Gate, and Output projections yields better performance than independent routing per layer. A unified router selects one "expert pathway" (e.g., Expert 3's Conv, Gate, and Out projections together) for each token, allowing co-adaptation among projections.

### Mechanism 2: Selective Scaling Targets High-Impact Parameters
Applying MoE only to Conv, Gate, and Output projections while sharing x Proj, dt Proj, and Conv1D across experts is more effective than expertizing all components. Larger projections carry most representational capacity; smaller SSM-specific projections have few parameters and may not benefit from expertization.

### Mechanism 3: Naive Independent MoE Degrades Due to Fragmented Routing
Applying MoE independently to each Mamba projection layer fails to improve and often degrades performance. Independent routers per layer create conflicting expert assignments, fragmenting the routing logic and preventing coherent specialization across the Mamba block.

## Foundational Learning

- **Mamba Projection Layers and Their Roles**: Understanding Conv (input expansion), Gate (gating signal), and Output (final projection) and their interdependence is prerequisite to grasping why shared routing matters.
  - Quick check: Which Mamba components does RoM expertize, and which remain shared across experts?

- **Top-K Gating and Router Learning in MoE**: RoM uses Top-1 routing (activate 1 of 8 experts). The shared routing strategy simplifies the router's task by requiring one decision per token for all projections.
  - Quick check: How does shared routing reduce router learning burden compared to independent routing per layer?

- **State Space Model Recurrence and Discretization**: Mamba's SSM core processes sequential dependencies. Parameters like Δt (timescale) and state dimension ds affect how information is retained; these small components are shared, not expertized.
  - Quick check: Why might x Proj and dt Proj be poor candidates for expertization compared to Conv/Gate/Output?

## Architecture Onboarding

- **Component map**:
  - Router (W_r) computes Top-K expert indices
  - Conv Proj Experts (Win,i) - input expansion
  - Gate Proj Experts (Wg,i) - gating signal
  - Out Proj Experts (Wout,i) - final projection
  - Shared SSM Core (x Proj, dt Proj, Conv1D) - single copy
  - Gating Fusion - combines SSM output with gated expert outputs

- **Critical path**:
  1. Router computes P(X_t) = Softmax(X_t · W_r), selects top-K experts
  2. Shared routing indicator used for Conv Proj (H_t), Gate Proj (G), and Out Proj
  3. SSM processes H_t via shared x Proj, dt Proj, Conv1D, produces Y_t
  4. Final output O_t combines SSM output Y_t with gated expert outputs via R_i(X_t) weights

- **Design tradeoffs**:
  - Shared vs. Independent Routing: Shared simplifies learning but constrains per-layer flexibility
  - Selective vs. Full Expertization: Sharing small projections reduces parameters; expertizing them adds overhead with unclear benefit
  - Top-1 vs. Top-K: Paper uses Top-1 for efficiency; higher K may improve expressiveness at cost
  - Load Balance Loss: Paper finds it unnecessary; routers naturally balance, but this may not generalize to all settings

- **Failure signatures**:
  - Naive Independent MoE: PPL increases (e.g., 10.26 vs. 10.00 dense); fragmented routing
  - Expertizing All Components: No PPL gain, added complexity
  - Latency Overhead: Naive MoE increases latency without quality gain
  - Training Instability: Router collapse if not properly regularized

- **First 3 experiments**:
  1. Shared vs. Independent Routing Ablation: Implement RoM with shared routing and variant with independent routers per projection. Compare PPL on SlimPajama validation set at 4K/8K/16K lengths.
  2. Selective Scaling Validation: Compare RoM (Conv, Gate, Out) vs. RoM (Conv, Gate, dt, x, Out) on same dataset. Expect minimal or no PPL improvement from expertizing smaller projections.
  3. Scaling Curve Comparison: Train RoM and dense Mamba at 115M, 353M, 765M, 1.3B active params. Plot PPL vs. active params. Expect RoM to achieve dense-equivalent PPL with ~2× fewer active params.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can future SSM architectures be designed from the ground up to be natively "MoE-friendly" based on the parameter sharing insights from RoM?
- **Basis in paper:** Section 5.4 states that the finding regarding selective vs. holistic expertization "could shed light on the future architecture design for MoE-friendly SSMs."
- **Why unresolved:** The current study applies RoM to *existing* architectures and finds that optimal MoE application varies. It does not propose a new SSM block designed specifically to maximize the efficiency of this sparse scaling.

### Open Question 2
- **Question:** Can the RoM shared routing strategy effectively scale pure self-attention and other linear attention mechanisms without performance degradation?
- **Basis in paper:** The Conclusion lists limitations regarding "uncertainty around RoM's optimal configuration and its broader applicability to the rapidly evolving landscape of SSM variants, as well as broader self-attention and linear attention architectures."
- **Why unresolved:** The paper validates RoM on Samba (a hybrid) but notes that applying MoE to partial self-attention blocks "does not yield clear performance improvements." The generalizability of the shared routing mechanism to non-SSM architectures remains unconfirmed.

### Open Question 3
- **Question:** Does the avoidance of expert parallelism and capacity factors limit RoM's scalability to extremely large models (e.g., >100B parameters)?
- **Basis in paper:** Section 5.1 states, "We avoid expert parallelism to eliminate the need for a capacity factor or token dropping, ensuring more stable training."
- **Why unresolved:** Expert parallelism and token dropping are standard techniques for training massive MoE models. If RoM relies on avoiding them for stability, it may face significant engineering hurdles or latency overheads when scaling beyond the 10B total parameter size tested.

## Limitations
- Router generalization across diverse datasets and tasks requiring specialized state dynamics remains unclear
- Selective scaling justification lacks rigorous testing of expertizing smaller SSM components
- Training complexity and resource requirements (router collapse, instability) not fully explored
- Limited downstream task coverage beyond language modeling

## Confidence
- **High Confidence**: Shared routing strategy improves performance over naive independent MoE integration
- **Medium Confidence**: RoM achieves equivalent perplexity to dense Mamba with ~2.3× fewer active parameters (based on single dataset)
- **Low Confidence**: RoM generalizes effectively to other SSM architectures and hybrid FFN-MoE configurations (limited experiments)

## Next Checks
1. **Router Generalization Test**: Train RoM on diverse datasets (code, multilingual text, long-context sequences) and compare against dense Mamba and naive MoE to test shared routing robustness.

2. **Selective Scaling Ablation**: Expertize x Proj and dt Proj in RoM and evaluate on tasks requiring specialized state dynamics to determine if full expertization is beneficial in certain cases.

3. **Hybrid Architecture Scaling**: Apply RoM to hybrid models combining SSMs with FFN-MoE (e.g., Samba) and evaluate FLOPS savings and performance at different scales (1B, 10B, 100B active parameters) to validate consistent 23% FLOPS reduction.