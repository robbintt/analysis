---
ver: rpa2
title: 'PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative
  Recommendations'
arxiv_id: '2510.07784'
source_url: https://arxiv.org/abs/2510.07784
tags:
- training
- retrieval
- arxiv
- generative
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLUM adapts pre-trained language models for industrial-scale generative
  recommendations by integrating item tokenization using Semantic IDs, continued pre-training
  on domain-specific data, and fine-tuning for retrieval tasks. The framework addresses
  the challenge of bridging the domain gap between LLMs and recommendation systems
  by enriching models with user behavior and item corpus through continued pre-training.
---

# PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations

## Quick Facts
- **arXiv ID:** 2510.07784
- **Source URL:** https://arxiv.org/abs/2510.07784
- **Reference count:** 40
- **Primary result:** 2.6x larger effective vocabulary size with 47% relative Recall@10 improvement via continued pre-training and fine-tuning.

## Executive Summary
PLUM adapts pre-trained language models for industrial-scale generative recommendations by integrating item tokenization using Semantic IDs, continued pre-training on domain-specific data, and fine-tuning for retrieval tasks. The framework addresses the challenge of bridging the domain gap between LLMs and recommendation systems by enriching models with user behavior and item corpus through continued pre-training. Semantic IDs are enhanced with multi-modal content embeddings, hierarchical refinements, and co-occurrence contrastive regularization to improve representation quality. Generative retrieval is implemented by training the model to directly generate Semantic IDs of recommended items based on user context.

## Method Summary
The PLUM framework uses a three-stage pipeline: (1) SID-v2 tokenization via RQ-VAE with multi-resolution codebooks (2048/2^(level-1)), progressive masking, and co-occurrence contrastive loss; (2) Continued Pre-Training (CPT) expanding LLM vocabulary with SIDs—1M steps, batch 16, ~260B tokens, 50/50 mixture of user behavior and video metadata; (3) Supervised Fine-Tuning (SFT) on click labels using decoder-only MoE (110M–3B activated params). Warm-start from Gemini-1.5 MoE. Beam search at inference. Training on 1,024 v6e TPUs.

## Key Results
- 2.6x larger effective vocabulary size compared to traditional embedding tables
- 47% relative improvement in Recall@10 from CPT alignment (0.28 vs 0.19 without CPT)
- +0.80% engaged users and +4.96% panel CTR in live experiments vs production baseline
- <0.55x FLOPs to train vs LEM despite 100x more dense parameters due to faster convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic IDs (SIDs) enable LLMs to process recommendation items as language tokens, achieving better generalization than random ID embeddings.
- **Mechanism:** Multi-modal content embeddings are quantized into hierarchical token sequences via RQ-VAE with multi-resolution codebooks. Co-occurrence contrastive loss aligns SIDs with user behavior patterns. The hierarchical structure allows the model to learn compositional item representations rather than memorizing sparse ID-to-embedding mappings.
- **Core assumption:** Items with similar content and co-occurrence patterns should share SID prefixes, enabling transfer across items with limited interaction data.
- **Evidence anchors:** [abstract] "2.6x larger effective vocabulary size"; [Table 4] SID uniqueness improved from 94.0% to 96.7%, VID Recall@10 from 12.3% to 14.4% with SIDv2 enhancements.
- **Break condition:** If SID collision rates exceed ~5-10% or hierarchy degrades, the mechanism collapses into a hashing function with no generalization benefit.

### Mechanism 2
- **Claim:** Continued pre-training (CPT) on domain-specific data aligns pre-trained LLMs with recommendation semantics, accelerating fine-tuning convergence.
- **Mechanism:** LLM vocabulary is expanded with SID tokens. The model is trained on a 50/50 mixture of user behavior sequences and video metadata. This grounds SID tokens in the LLM's existing text embedding space. Total training: ~260B tokens over 1M steps.
- **Core assumption:** Pre-trained LLM's sequence modeling and world knowledge transfer to recommendation sequences.
- **Evidence anchors:** [Section 3.3, Table 5] CPT + LLM init achieves Recall@10 of 0.28 vs. 0.19 for random init without CPT—a 47% relative improvement.
- **Break condition:** If CPT data mixture overfits to historical patterns and fails to generalize to new items, or if catastrophic forgetting degrades text understanding.

### Mechanism 3
- **Claim:** Generative retrieval via autoregressive SID prediction bypasses embedding-based retrieval limitations and enables end-to-end optimization.
- **Mechanism:** A decoder-only model generates SID tokens autoregressively conditioned on user context. Beam search produces multiple candidate SIDs, which are mapped to actual videos. Loss is weighted by engagement rewards.
- **Core assumption:** The sequence generation objective captures user preferences more expressively than inner product similarity in dense retrieval.
- **Evidence anchors:** [Table 3] Live experiments show +0.80% engaged users and +4.96% panel CTR vs production baseline.
- **Break condition:** If hallucination rate exceeds ~10% or beam search produces low-diversity candidates, the retrieval quality degrades below embedding baselines.

## Foundational Learning

- **Concept: Residual Quantization VAE (RQ-VAE)**
  - **Why needed here:** Converts continuous embeddings into hierarchical discrete tokens. Without understanding residual quantization, you cannot diagnose SID collision issues or tune codebook resolution.
  - **Quick check question:** Can you explain why multi-resolution codebooks (2048/2^(level-1)) are more efficient than fixed-size codebooks for a billion-item corpus?

- **Concept: Transfer Learning with Vocabulary Expansion**
  - **Why needed here:** Adding SID tokens to a pre-trained LLM without breaking existing knowledge requires careful embedding initialization and mixed-data CPT.
  - **Quick check question:** If you initialize new SID embeddings randomly, what failure mode would you expect during CPT, and how would you detect it?

- **Concept: Autoregressive Decoding for Retrieval**
  - **Why needed here:** Generative retrieval fundamentally differs from classification-based retrieval. Beam search, hallucination, and SID-to-item mapping are unique challenges.
  - **Quick check question:** Why does beam search outperform random decoding in this setup, and what is the tradeoff in candidate diversity?

## Architecture Onboarding

- **Component map:** Multi-modal content → fused embedding → RQ-VAE quantization → SID tuple; Pre-trained LLM + expanded vocabulary → next-token prediction on user sequences + metadata corpus; CPT checkpoint → SFT with reward-weighted loss → beam search inference → SID-to-video mapping.

- **Critical path:** SID quality (uniqueness, hierarchy) → CPT alignment (SID-text grounding) → SFT reward engineering. If SID uniqueness is <95%, downstream retrieval degrades measurably. If CPT under-trains, SFT convergence slows 2-3x.

- **Design tradeoffs:**
  - SID vocabulary size vs. sequence length: More codebook levels increase uniqueness but require longer sequences, raising inference cost.
  - CPT data mixture: 50/50 split is heuristic; more user behavior may improve personalization but risks overfitting to historical biases.
  - Beam width: Higher beam search width improves recall but reduces diversity and increases latency.

- **Failure signatures:**
  - Low SID uniqueness (<90%): Codebook capacity insufficient for corpus size. Fix: increase codebook resolution or levels.
  - High hallucination (>10%): SFT insufficient or reward signal misaligned. Fix: increase SFT data quality, add constrained decoding.
  - Slow SFT convergence: CPT undertrained or learning rate mismatch. Fix: extend CPT, tune LR per model size.

- **First 3 experiments:**
  1. SID collision audit: Quantify uniqueness and collision distribution on held-out corpus. If >5% of items share SIDs, tune multi-resolution codebooks before proceeding.
  2. CPT ablation: Train retrieval model with/without CPT. Measure convergence speed and final recall. If gap <10%, CPT may not be necessary for your data scale.
  3. Beam search sweep: Test beam widths [1, 4, 8, 16] on retrieval recall and latency. Identify knee point where recall gains diminish.

## Open Questions the Paper Calls Out

- **Transfer to other tasks:** The paper explicitly lists "applying the PLUM framework to other tasks like ranking and personalized search" as a future research direction, leaving other recommendation stages unexplored.

- **MoE scaling limitations:** The authors note that the MoE-3B model did not outperform MoE-900M, likely due to "suboptimal hyperparameter setup" and smaller batch sizes necessitated by memory constraints, but did not perform a full hyperparameter search.

- **Candidate diversity strategies:** The conclusion identifies "developing new decoding strategies for candidate diversity" as a key area for future work, observing that beam search improves performance but reduces diversity.

- **Joint SID and natural language generation:** The paper lists "enabling seamless generation of SIDs and natural languages" as a future direction, as current fine-tuning optimizes specifically for SID generation.

## Limitations

- **Dataset scale assumptions:** The paper reports 260B CPT tokens but does not specify exact corpus size, making it difficult to determine if SID codebook resolution is optimal or merely sufficient.

- **Reward function opacity:** The SFT stage uses a "handcrafted reward" based on engagement signals, but the exact formulation is unspecified, introducing uncertainty about whether gains are attributable to model architecture versus reward engineering.

- **Hallucination handling gaps:** While hallucination is reported at <5% post-SFT, the paper does not specify beam search width, diversity constraints, or collision resolution for ambiguous SIDs.

## Confidence

**High confidence:** The CPT alignment benefit (47% relative Recall@10 improvement) and FLOPs efficiency claim (<0.55x training cost vs. large embedding models) are well-supported by ablation studies and scaling experiments.

**Medium confidence:** The live experiment engagement gains are credible given the controlled A/B test setup, but the exact counterfactual baseline and seasonal factors are not disclosed.

**Low confidence:** The claim that generative retrieval "fundamentally" outperforms embedding-based retrieval is not rigorously proven—no head-to-head comparison on the same backbone model without SID quantization is provided.

## Next Checks

1. **SID collision audit under expanded corpus:** Test the SID-v2 tokenizer on a held-out item set 2-3x larger than the reported corpus. Measure uniqueness and collision distribution. If uniqueness drops below 95%, tune codebook resolution or add additional hierarchy levels.

2. **CPT alignment ablation with controlled rewards:** Train two retrieval models: one with CPT (50/50 metadata/user mix) and one without, but both using identical SFT reward functions. Compare convergence speed and final Recall@10. If the CPT gap narrows to <10%, the claimed transfer learning benefit may be dataset-specific.

3. **Beam search diversity vs. recall tradeoff:** Sweep beam widths [1, 4, 8, 16] while measuring Recall@10, average candidate diversity (distinct SIDs), and inference latency. Identify the knee point where recall gains diminish to determine optimal production serving constraints.