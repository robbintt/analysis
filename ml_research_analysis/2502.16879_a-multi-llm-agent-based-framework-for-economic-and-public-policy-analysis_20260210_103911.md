---
ver: rpa2
title: A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis
arxiv_id: '2502.16879'
source_url: https://arxiv.org/abs/2502.16879
tags:
- economic
- llms
- policy
- income
- consumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Multi-LLM-Agent-Based (MLAB) framework
  that leverages different Large Language Models (LLMs) to represent heterogeneous
  economic agents. The authors evaluate five LLMs' capabilities in solving two-period
  consumption allocation problems, finding DeepSeek-V3 performs best with 90.63% accuracy
  in optimizing consumption choices.
---

# A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis

## Quick Facts
- arXiv ID: 2502.16879
- Source URL: https://arxiv.org/abs/2502.16879
- Reference count: 5
- Primary result: MLAB framework maps 5 LLMs to educational groups, finding tax sensitivity varies 2-3x across groups with DeepSeek-V3 performing best (90.63% accuracy)

## Executive Summary
This paper introduces a Multi-LLM-Agent-Based (MLAB) framework that leverages heterogeneous Large Language Models (LLMs) to represent different economic agents in policy analysis. The authors evaluate five LLMs' capabilities in solving two-period consumption allocation problems, finding DeepSeek-V3 achieves 90.63% accuracy while Llama-3.1-405B scores only 3.57%. When analyzing economic intuition without explicit utility functions, all models demonstrate human-like patterns including consumption smoothing and intertemporal substitution. The framework maps these LLMs to educational groups and income brackets, enabling more realistic policy simulations. A case study on interest income taxation reveals that highly educated groups show the strongest tax sensitivity while lower-educated groups demonstrate more stable, conservative responses.

## Method Summary
The authors evaluate five LLMs (DeepSeek-V3, GPT-4o, Gemini-1.5-pro, Claude-3.5-sonnet, Llama-3.1-405B) on two-period consumption-savings optimization problems using Chinese Family Panel Studies (CFPS) 2018 data. Each LLM completes 16 independent trials with both explicit utility functions and "gut feeling" prompts. The framework maps LLMs to educational groups based on their optimization performance, then simulates policy responses by running tax experiments (0-100%) across these agent groups. Results are aggregated using population weights from the CFPS data.

## Key Results
- DeepSeek-V3 achieves 90.63% accuracy on consumption optimization vs. Llama-3.1-405B's 3.57%, showing ~25x capability spread
- All 5 LLMs consistently demonstrate consumption smoothing (16/16 trials) and intertemporal substitution (5-12/16 trials) without explicit optimization
- Highly educated groups (mapped to top-performing LLMs) show 2-3x greater saving rate sensitivity to interest taxation than lower-educated groups
- Standard CRRA model (σ=2) produces flat ~28% saving rate; requires σ=0.5 to approximate MLAB heterogeneity magnitude

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different LLMs exhibit measurably distinct reasoning patterns that can proxy for cognitive heterogeneity across population segments
- Mechanism: LLM training on different corpora creates divergent problem-solving approaches. DeepSeek-V3's 90.63% accuracy vs. Llama's 3.57% reflects a ~25x capability spread that models agents with different "cognitive traits"
- Core assumption: LLM reasoning variability correlates meaningfully with human cognitive/educational heterogeneity
- Evidence anchors: Table 2 shows qualitative reasoning differences; SYMPHONY paper confirms heterogeneous LLM assemblies improve multi-agent planning
- Break condition: If LLM capability differences shrink through convergence, the heterogeneity signal weakens

### Mechanism 2
- Claim: LLMs demonstrate emergent economic intuition without explicit optimization formulas, exhibiting human-like behavioral patterns
- Mechanism: When prompted for "gut feeling" decisions without utility functions, all tested LLMs consistently show consumption smoothing and intertemporal substitution, suggesting training on economic text embeds behavioral economics principles
- Core assumption: Training corpora contain sufficient economic reasoning patterns that transfer to novel scenarios
- Evidence anchors: All models incorporate consumption smoothing in 16/16 trials; Chen et al. (2023) shows "emergence of economic rationality in LLMs"
- Break condition: If economic intuition is superficial mimicry rather than transfer, responses will fail on novel policy scenarios

### Mechanism 3
- Claim: Two-dimensional heterogeneity (economic parameters + cognitive capabilities) captures policy response patterns that representative-agent models miss
- Mechanism: By varying both prompts (income/wealth parameters) and LLM selection (cognitive proxy), the framework generates differentiated tax responses. Higher-educated groups show 2-3x greater saving rate sensitivity to interest taxation
- Core assumption: The LLM-to-education-group mapping reflects real cognitive-behavioral correlations
- Evidence anchors: Standard CRRA model (σ=2) produces flat ~28% saving rate; requires σ=0.5 to approximate MLAB heterogeneity magnitude
- Break condition: If LLM behaviors don't generalize to real population responses, policy insights are simulation artifacts

## Foundational Learning

- Concept: Two-period consumption-savings optimization (CRRA utility)
  - Why needed here: Core benchmark for evaluating LLM economic reasoning. Understanding Euler equation (c₂/c₁ = (β(1+r))^(1/σ)) is essential to interpret accuracy metrics
  - Quick check question: Given β=0.99, r=0.486, σ=2, what's the predicted consumption ratio c₂/c₁? (Answer: ~1.22)

- Concept: Agent-Based Modeling (ABM) with bounded rationality
  - Why needed here: MLAB extends traditional ABM by replacing fixed behavioral rules with LLM-generated decisions. Understanding ABM limitations clarifies the innovation
  - Quick check question: Why do traditional ABMs struggle to capture emergent heterogeneity? (Answer: Pre-specified rules can't adapt to novel scenarios)

- Concept: LLM temperature and decision variability
  - Why needed here: Temperature settings affect response consistency. Llama-3.1-405B has lowest temperature (0.2) but highest decision variance, revealing that model architecture matters more than sampling parameters
  - Quick check question: If all LLMs used temperature=0, would heterogeneity disappear? (Answer: Not necessarily—architectural differences persist)

## Architecture Onboarding

- Component map:
  CFPS Data -> Parameter Calibration -> Educational Group Profiles -> 5 LLM APIs -> Agent Instances -> Policy Scenarios -> Consumption Decisions -> Saving Rate Analysis -> Policy Insights

- Critical path:
  1. Calibrate income/wealth parameters per educational group (6-period -> 2-period transformation)
  2. Map LLMs to groups by capability ranking (DeepSeek-V3 -> highest education)
  3. Generate prompts with group-specific parameters + policy scenario
  4. Collect 16 independent trials per LLM (clear context between trials)
  5. Aggregate responses weighted by population shares

- Design tradeoffs:
  - LLM mapping granularity vs. cost: 5 LLMs = 5 agent types; more LLMs increase coverage but API costs scale linearly
  - Trial count vs. variance reduction: 16 trials balances statistical power against rate limits; paper shows SD varies 10x across models
  - Prompt specificity vs. autonomy: Explicit utility functions test computation; "gut feeling" prompts test intuition—both needed

- Failure signatures:
  - Budget constraint violations (>5% deviation): Indicates LLM failed arithmetic reasoning
  - Uniform responses across LLMs: Mapping collapsed, check API configuration
  - Extreme under-consumption (>30% below budget): Model risk aversion miscalibrated (Claude-3.5-sonnet pattern)
  - Non-monotonic aggregate responses to monotonic policy changes: Insufficient trials or high-variance models dominating

- First 3 experiments:
  1. Baseline replication: Run 16 trials per LLM on paper's exact parameters; verify DeepSeek-V3 >90% accuracy, Llama <5%
  2. Robustness test: Vary interest rate (20%, 48.6%, 80%); check if intertemporal substitution responses scale appropriately
  3. Policy extension: Add progressive taxation scenario; compare LLM-group responses to representative-agent prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the mapping between LLMs and educational/socioeconomic groups be made more scientifically rigorous and multidimensional?
- Basis in paper: Authors state: "the current assignment of LLMs to educational groups is not yet as scientifically rigorous or multidimensional as desired"
- Why unresolved: Current mapping relies on capability rankings alone; no validated framework exists for translating LLM behavioral patterns to human cognitive traits
- What evidence would resolve it: Empirical validation comparing LLM decision patterns to actual behavioral data from corresponding demographic groups

### Open Question 2
- Question: How would incorporating dynamic agent interactions—such as peer effects, social learning, and market interactions—alter the policy simulation outcomes?
- Basis in paper: Authors identify this as a key future direction: "developing dynamic agent interactions across income groups would enable more sophisticated modeling"
- Why unresolved: Current framework treats agents independently; interaction mechanisms require substantial methodological development
- What evidence would resolve it: Comparative simulations with and without interaction modules showing divergence in aggregate policy responses

### Open Question 3
- Question: Why does the MLAB framework require non-standard risk aversion calibration (σ=0.5) to approximate observed behavioral responses, rather than conventional σ=2?
- Basis in paper: Authors note that standard σ=2 produces flat saving profiles that "fail to capture the magnitude of variation observed in our MLAB results"
- Why unresolved: Unclear whether this reflects genuine behavioral heterogeneity, LLM artifacts, or limitations in the two-period model specification
- What evidence would resolve it: Multi-period extensions and empirical calibration against household panel data

### Open Question 4
- Question: Can bespoke LLM instances be automatically generated to emulate the cognitive styles of specific target populations with high fidelity?
- Basis in paper: Authors propose that future LLMs could "be provided with detailed data on the characteristics of a target group and subsequently generate bespoke code to create a tailored LLM"
- Why unresolved: Technical feasibility unproven; requires advances in LLM customization and behavioral data integration
- What evidence would resolve it: Successful generation of specialized agents whose decisions statistically match target group survey/experimental data

## Limitations
- The framework's validity depends critically on the assumption that LLM capability variance meaningfully proxies for human cognitive heterogeneity
- The model's behavioral predictions remain untested against real-world policy outcomes
- Current mapping from LLM performance to educational groups lacks empirical validation against actual behavioral data

## Confidence
- **High Confidence**: Technical implementation of two-period CRRA optimization and observed capability differences across LLMs
- **Medium Confidence**: Emergence of economic intuition patterns across LLMs and qualitative consistency of consumption smoothing responses
- **Low Confidence**: Validity of LLM-to-educational-group mapping as behavioral proxy and generalizability of policy insights to real populations

## Next Checks
1. **Empirical Validation**: Compare MLAB-predicted tax response patterns against actual household saving behavior data from China's tax policy changes
2. **Cross-Country Robustness**: Test whether the LLM mapping generalizes across different cultural contexts using alternative datasets
3. **Behavioral Edge Cases**: Design novel policy scenarios (e.g., unexpected inflation shocks, credit constraints) to test whether LLM economic intuition extends beyond standard optimization problems