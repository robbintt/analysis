---
ver: rpa2
title: Medical Interpretability and Knowledge Maps of Large Language Models
arxiv_id: '2510.11390'
source_url: https://arxiv.org/abs/2510.11390
tags:
- layer
- umap-2
- stage
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic interpretability analysis of
  five large language models (LLMs) across medical domains using four techniques:
  UMAP projections, gradient-based saliency, layer lesioning, and activation patching.
  The study builds "LLM maps" showing where knowledge about patient age, symptoms,
  diseases, drugs, and dosages is stored across model layers.'
---

# Medical Interpretability and Knowledge Maps of Large Language Models

## Quick Facts
- arXiv ID: 2510.11390
- Source URL: https://arxiv.org/abs/2510.11390
- Reference count: 40
- Key outcome: LLM maps show medical knowledge is stored in specific layer intervals, with most knowledge in first half of Llama3.3-70B layers

## Executive Summary
This paper presents a systematic interpretability analysis of five large language models across medical domains using four techniques: UMAP projections, gradient-based saliency, layer lesioning, and activation patching. The study builds "LLM maps" showing where knowledge about patient age, symptoms, diseases, drugs, and dosages is stored across model layers. For Llama3.3-70B, most medical knowledge is found in the first half of layers. Key findings include: age is encoded non-linearly with discontinuities at age 18, disease progression shows circular non-monotonic patterns, drugs cluster better by medical specialty than mechanism of action, and Gemma/MedGemma models show collapsed activations at intermediate layers that recover later. These results guide future work on fine-tuning, unlearning, or debiasing medical LLMs by identifying specific layers where interventions should be applied.

## Method Summary
The authors use four complementary interpretability techniques to map where medical knowledge is stored in LLMs. They extract intermediate activations from each layer, apply UMAP dimensionality reduction to visualize embeddings, compute cluster quality via Silhouette scores, and analyze age manifolds for non-linearities. Gradient-based saliency identifies influential weights per layer. Layer lesioning tests degradation by replacing layers with identity functions. Activation patching performs causal interventions between clean and corrupted prompt pairs. Results are combined to identify layer intervals where multiple methods agree, providing confidence in identified knowledge-encoding regions.

## Key Results
- Most medical knowledge in Llama3.3-70B is processed in the first half of layers (0-40 for age/symptoms, 0-5 or 27-37 for diseases, 15-45 for drugs)
- Age is encoded non-linearly with a discontinuity at age 18 (local R² drop in embedding space)
- Disease progression embeddings form circular patterns where late stages approach early stages
- Gemma/MedGemma models show activation collapse at intermediate layers that recovers later
- Drugs cluster better by medical specialty than by mechanism of action

## Why This Works (Mechanism)

### Mechanism 1: Layer-Localized Medical Knowledge Storage
Medical concepts are processed in specific layer intervals rather than uniformly distributed. For Llama3.3-70B, age/symptoms/diseases show peak interpretability signals in layers 0-10, while drugs cluster in layers 15-45. Four independent methods (UMAP Silhouette scores, weight saliency, lesioning degradation, activation patching) converge on overlapping layer intervals. High Silhouette scores and patching effects indicate where concept-relevant processing occurs.

### Mechanism 2: Non-Linear Manifold Encoding
Age and disease progression are encoded as non-linear manifolds with discontinuities/circularities at specific layers. UMAP projections reveal age manifolds bend and reverse direction; embedding distances spike at age 17→18 (R² drops locally). Disease progression embeddings form circular paths where late stages approach early stages.

### Mechanism 3: Multi-Method Triangulation Reduces False Positives
Agreement across UMAP, saliency, lesioning, and patching increases confidence in identified knowledge-encoding layers. Each method has different failure modes (UMAP: projection artifacts; saliency: gradient noise; lesioning: redundant circuits; patching: off-target effects). Convergent layer intervals suggest genuine localization.

## Foundational Learning

- **UMAP Dimensionality Reduction**
  - Why needed here: Core visualization and Silhouette score computation for detecting cluster structure in intermediate activations.
  - Quick check question: Can you explain why Silhouette scores require known labels and how the anisotropy metric differs from cluster separation?

- **Gradient-Based Saliency (∂Loss/∂w)**
  - Why needed here: Identifies which weights are most influential for medical concept processing per layer.
  - Quick check question: Why does averaging saliency across prompts reduce noise compared to single-prompt saliency?

- **Activation Patching (Clean/Corrupted/Patched Runs)**
  - Why needed here: Causal intervention to test whether specific layer activations are necessary/sufficient for correct medical outputs.
  - Quick check question: What does a normalized logit difference of 0.5 indicate about patching effectiveness?

## Architecture Onboarding

- **Component map:**
  Prompt templates (Table 3) → LLM forward pass → Hook intermediate activations → Four parallel analyses (UMAP, saliency, lesioning, patching) → Quantitative metrics per layer → LLM Map visualization (Figure 2)

- **Critical path:**
  1. Design prompts covering target medical concepts (age, symptoms, diseases, drugs, dosages)
  2. Register hooks at each transformer layer to extract attention/MLP activations
  3. Run UMAP on D=30 dimensions; compute Silhouette/anisotropy
  4. Compute per-layer weight saliency averaged across prompts
  5. Lesion each layer (identity replacement); score degradation with LLM-as-judge
  6. Run activation patching with clean/corrupted prompt pairs; compute normalized logit difference
  7. Identify layer intervals where ≥2 methods agree above 75th percentile

- **Design tradeoffs:**
  - UMAP D=30 vs D=2: Higher dimensions retain more structure but complicate visualization
  - Lesioning with identity vs zero: Identity preserves residual flow; zero may introduce artifacts
  - Silhouette vs clustering algorithms: Silhouette requires labels but avoids clustering hyperparameters

- **Failure signatures:**
  - Silhouette scores near -0.3 across all layers → no cluster structure (see Gemma/MedGemma in Table 2)
  - Activation collapse in UMAP (single dense cluster at intermediate layers) → representational bottleneck (Figure 7)
  - Lesioning degradation = 1/10 (no effect) for all layers → high redundancy or flawed lesioning implementation

- **First 3 experiments:**
  1. Replicate age manifold on a smaller model (e.g., Llama-8B) to test if discontinuity at 18 generalizes
  2. Test whether drug clustering by specialty vs mechanism differs in domain-specific (MedGemma) vs general models
  3. Apply lesioning to Gemma's collapsed layers (15-25) to determine if they are functionally redundant or critical for recovery

## Open Questions the Paper Calls Out

### Open Question 1
What architectural, training, or data factors cause the activation collapse observed at intermediate layers in Gemma3-27B and MedGemma-27B? The authors identified the phenomenon but did not isolate the root cause or determine if it wastes representational power.

### Open Question 2
What specific behavioral biases result from the non-linear age discontinuity found at age 18 in Llama3.3-70B? While the geometric discontinuity in the manifold is mapped, the functional impact on the model's clinical reasoning for pediatric vs. adult patients remains unknown.

### Open Question 3
Why do drug dosage representations cluster in a manner that is misaligned with the model's "alive/dead" classification logic? UMAP projections show clear separation of dosage embeddings that does not correspond to the toxicity outcome.

### Open Question 4
Can the derived "LLM Maps" be validated against external ground-truth knowledge structures? The study maps where knowledge is stored but lacks an objective benchmark to confirm that the mapped layers process the information correctly regarding medical reality.

## Limitations
- Lack of ground-truth data about how medical knowledge is actually organized in LLMs
- Findings based on proxy metrics that may capture correlated but not causative processing patterns
- Analysis focuses on specific medical concepts and may not generalize to other domains

## Confidence
- Layer-localized knowledge storage: Medium
- Non-linear manifold encoding: Medium
- Multi-method triangulation validity: High
- Drug clustering patterns: Low (based on small sample size)
- Age discontinuity at 18: Medium

## Next Checks
1. Test whether age discontinuity at 18 appears consistently across different LLM architectures to determine if this is an architectural artifact or genuine pattern.

2. Apply targeted lesioning to Gemma's collapsed layers (15-25) while monitoring downstream task performance to determine if these layers are functionally redundant or critical for later recovery.

3. Compare drug clustering patterns against established medical ontologies to validate whether LLM organization aligns with human medical knowledge structures.