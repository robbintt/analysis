---
ver: rpa2
title: 'ESNLIR: A Spanish Multi-Genre Dataset with Causal Relationships'
arxiv_id: '2503.08803'
source_url: https://arxiv.org/abs/2503.08803
tags:
- dataset
- examples
- test
- language
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESNLIR, a large-scale Spanish multi-genre
  dataset for Natural Language Inference (NLI) that includes causal relationships.
  The dataset was created using an automated method based on linking phrases to extract
  premise-hypothesis pairs from diverse text genres.
---

# ESNLIR: A Spanish Multi-Genre Dataset with Causal Relationships

## Quick Facts
- **arXiv ID:** 2503.08803
- **Source URL:** https://arxiv.org/abs/2503.08803
- **Reference count:** 8
- **Primary result:** ESNLIR is a large-scale Spanish NLI dataset with causal relationships, achieving 0.676 accuracy and 0.676 macro F1 with XLMRoBERTa.

## Executive Summary
This paper introduces ESNLIR, a large-scale Spanish multi-genre dataset for Natural Language Inference (NLI) that includes causal relationships. The dataset was created using an automated method based on linking phrases to extract premise-hypothesis pairs from diverse text genres. The authors evaluated the dataset using BERT-based models and found strong baseline performance, with XLMRoBERTa achieving an accuracy of 0.676 and macro F1 score of 0.676. The dataset demonstrates good generalization across genres and is robust to annotation artifacts. However, it shows lower performance on non-formal writing genres. Human validation confirmed the dataset's usefulness, with overall performance around 68%, indicating it is a challenging benchmark for evaluating NLU models.

## Method Summary
The authors created ESNLIR using an automated extraction method that identifies linking phrases (e.g., "por lo tanto" for reasoning, "sin embargo" for contrasting) to generate premise-hypothesis pairs from 34 corpora across 8 genres. They used Spacy's es_core_news_lg for POS filtering and created stratified train/val/test splits. The dataset was evaluated using fine-tuned BERT-based models (BERTIN and XLMRoBERTa) with early stopping on F1-score, batch size 64, and learning rate 2e-5. Evaluation included standard accuracy/F1 metrics, artifact detection through premise-only training, and stress tests for robustness to length mismatch, negation, overlap, and spelling perturbations.

## Key Results
- XLMRoBERTa achieved 0.676 accuracy and 0.676 macro F1 on the ESNLIR test set
- Clinical genre achieved 0.735 accuracy in zero-shot transfer, demonstrating strong cross-genre generalization
- Stress tests revealed significant sensitivity to superficial perturbations, particularly for contrasting class (accuracy drops from 0.664 to 0.580 with negation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linking phrases serve as reliable proxies for semantic relationship types between sentence pairs
- Mechanism: The extraction method maps specific Spanish linking phrases to labels, treating the phrase as a consequence of the underlying semantic relationship between adjacent sentences
- Core assumption: Writers use linking phrases consistently and correctly according to the semantic relationship they intend to express
- Evidence anchors: Table 2 lists linking phrases mapped to four classes; linking phrases show cause-effect relationships not captured by previous works
- Break condition: If authors misuse linking phrases, or if context required for disambiguation extends beyond adjacent sentences, label quality degrades

### Mechanism 2
- Claim: Multi-genre training data improves cross-domain generalization for NLI models
- Mechanism: Exposure to 8 distinct writing styles during training enables models to learn genre-independent semantic patterns that transfer to held-out corpora
- Core assumption: Semantic relationships manifest similarly across genres, and variation in surface form does not fundamentally alter inference patterns
- Evidence anchors: "enrichment of genres essentially contributes to the enrichment of the model's capability to generalize"; clinical genre achieves 0.735 accuracy despite zero training examples
- Break condition: If a target genre has fundamentally different semantic conventions, generalization from formal genres may fail

### Mechanism 3
- Claim: Neutral pairs are easier to classify due to differential extraction methodology creating distributional artifacts
- Mechanism: Neutral examples are constructed by pairing sentences from different paragraphs without linking phrases, while other classes use same-paragraph pairs with explicit connectors
- Core assumption: Models exploit these artifacts rather than learning true semantic independence
- Evidence anchors: "it is expected that the classifiers will have some bias towards this class"; Table 4 shows neutral class achieves highest accuracy (0.695) for XLMRoBERTa
- Break condition: If neutral pairs are constructed from same-paragraph sentences without linking phrases, this artifact advantage would diminish

## Foundational Learning

- **Natural Language Inference (NLI) / Recognizing Textual Entailment (RTE)**
  - Why needed here: This is the core task—determining whether a hypothesis can be inferred from a premise
  - Quick check question: Given "María compró pan" (premise) and "María compró comida" (hypothesis), what label applies and why?

- **BERT-based fine-tuning for classification**
  - Why needed here: All baseline models use transformer architectures with token-level embeddings pooled for sentence-pair classification
  - Quick check question: Why would a model trained on only premises (no hypothesis) perform near random (0.25) if the dataset lacks artifacts?

- **Annotation artifacts and stress testing in NLI**
  - Why needed here: The paper explicitly tests for artifacts and applies stress tests to probe model robustness
  - Quick check question: If adding "y falso no es verdadero" to hypotheses drops contrasting class accuracy from 0.664 to 0.580, what heuristic might the model have learned?

## Architecture Onboarding

- **Component map:** Corpora → linking phrase detection → sentence pair extraction → POS filtering (Spacy es_core_news_lg) → train/val/test split → pre-trained transformer (BERTIN/XLMRoBERTa) → fine-tuning with classification head → 4-class output → evaluation (metrics + artifact detection + stress tests + per-genre breakdown)

- **Critical path:** 1) Verify linking phrase extraction produces syntactically complete sentences (subject + predicate check) 2) Ensure no document leakage between splits (double stratification by article) 3) Fine-tune with early stopping on F1-score, batch size 64, learning rate 2e-5 4) Run full evaluation: standard metrics → artifact detection → stress tests → per-genre breakdown

- **Design tradeoffs:**
  - Scale vs. quality: 7.3M training examples enable scale but ~50% require human relabeling
  - Automation vs. annotation cost: Linking phrase method is free but introduces label noise; full human annotation would be expensive
  - Genre coverage vs. balance: Legal/clinical have few training examples; books dominate with 3M+ examples

- **Failure signatures:**
  - Reasoning/Entailment confusion: Check confusion matrix—these classes intermix most frequently
  - Non-formal genre collapse: Comments/talks show ~0.58-0.65 accuracy vs. 0.73+ for clinical/legal
  - Stress test sensitivity: Length mismatch and negation drops contrasting/entailment by 10-15 points

- **First 3 experiments:**
  1. Baseline reproduction: Fine-tune XLMRoBERTa on ESNLIR training split, evaluate on test—target accuracy ~0.676 ± 0.01
  2. Artifact probe: Train premise-only model and verify accuracy drops to ~0.35, confirming dataset lacks hypothesis-only shortcuts
  3. Genre ablation: Train on formal genres only, test on talks/comments to quantify informal text generalization gap

## Open Questions the Paper Calls Out

- **Open Question 1:** Why do Large Language Models (LLMs) like GPT-4o underperform compared to fine-tuned BERT-based models on the ESNLIR dataset? The paper conducted preliminary experiments showing underwhelming results but has not yet analyzed the specific failure modes or architectural reasons for the performance gap.

- **Open Question 2:** What is the primary cause of the label discrepancy between the automatic extraction method and human validation? The paper identifies the high mismatch rate (~50%) but does not determine which of three hypothesized causes (lack of domain knowledge, unclear instructions, or incorrect linking phrase usage) is responsible for the majority of errors.

- **Open Question 3:** How well does ESNLIR generalize to existing Spanish NLI resources? While the dataset was designed to improve generalization, the paper deferred cross-dataset performance metrics (specifically against the Spanish section of XNLI) for a separate article.

## Limitations

- The linking phrase-based extraction method introduces potential label noise, with ~50% of examples requiring human relabeling
- Lower performance on non-formal genres (comments: 0.647, talks: 0.597 F1) indicates limited cross-genre robustness
- Significant sensitivity to superficial perturbations in stress tests, particularly for the contrasting class

## Confidence

- **High confidence:** Dataset creation methodology using linking phrases is clearly described and reproducible; baseline model performance metrics are verifiable through provided code; stress test results are well-documented
- **Medium confidence:** Claims about multi-genre benefits for generalization; assertion that neutral pairs are easier due to distributional artifacts
- **Low confidence:** Overall usefulness claim based on 68% human validation accuracy from small 100-sentence subset; long-term dataset stability given high relabeling requirements

## Next Checks

1. **Artifact robustness validation:** Replicate premise-only model training to verify the ~0.35 accuracy baseline, then systematically test for other potential artifacts (entity overlap patterns, topical coherence scores between premise-hypothesis pairs)

2. **Cross-genre generalization stress test:** Train models exclusively on formal genres (legal, clinical, theses, news, articles) and evaluate on non-formal genres (comments, talks) to quantify the generalization gap beyond the reported zero-shot clinical transfer

3. **Human performance benchmarking:** Conduct human evaluation on a larger subset (500+ examples) spanning all genres to establish reliable human baseline metrics for comparison with model performance across the full difficulty spectrum