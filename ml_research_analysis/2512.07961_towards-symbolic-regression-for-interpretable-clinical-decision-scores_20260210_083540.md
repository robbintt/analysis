---
ver: rpa2
title: Towards symbolic regression for interpretable clinical decision scores
arxiv_id: '2512.07961'
source_url: https://arxiv.org/abs/2512.07961
tags:
- brush
- regression
- symbolic
- performance
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Brush is a symbolic regression algorithm that integrates decision-tree-like
  splitting operations with non-linear parameter optimization, addressing the challenge
  of modeling rule-based decision-making in healthcare. Unlike traditional symbolic
  regression methods that focus solely on continuous mathematical expressions, Brush
  introduces a split node operator that partitions data based on learned thresholds
  while maintaining compatibility with parameter optimization techniques like Levenberg-Marquardt.
---

# Towards symbolic regression for interpretable clinical decision scores

## Quick Facts
- arXiv ID: 2512.07961
- Source URL: https://arxiv.org/abs/2512.07961
- Reference count: 40
- Brush achieves Pareto-optimal performance on SRBench across 252 datasets, recovering over 50% of physics equations even with noise, and produces interpretable clinical scoring models with high accuracy

## Executive Summary
Brush is a symbolic regression algorithm that integrates decision-tree-like splitting operations with non-linear parameter optimization, addressing the challenge of modeling rule-based decision-making in healthcare. Unlike traditional symbolic regression methods that focus solely on continuous mathematical expressions, Brush introduces a split node operator that partitions data based on learned thresholds while maintaining compatibility with parameter optimization techniques like Levenberg-Marquardt. Applied to electronic health record data, Brush successfully recapitulates two widely used clinical scoring systems (CART and simplified MEWS) with high accuracy (AUPRC > 0.95) while producing interpretable models with 60-76 nodes.

## Method Summary
Brush is a symbolic regression algorithm that evolves mathematical expression trees using genetic programming with a novel split node operator. The split node partitions data based on learned thresholds while maintaining compatibility with parameter optimization techniques like Levenberg-Marquardt. The algorithm employs ϵ-lexicase selection and NSGA-II survival to balance model performance and complexity. Brush uses a three-step optimization heuristic for split nodes: first fitting the condition expression, then determining the threshold that minimizes target variance across partitions, and finally optimizing the remaining subtrees on their respective data subsets. The method was evaluated on SRBench benchmark datasets and applied to clinical scoring system prediction from MIMIC-IV EHR data.

## Key Results
- Brush achieves Pareto-optimal performance across 252 SRBench datasets, recovering over 50% of physics equations even with noise
- Clinical scoring models achieve high accuracy: AUPRC > 0.95 for both CART and simplified MEWS deterioration classification
- Models are highly interpretable with 60-76 nodes, significantly smaller than baseline approaches while maintaining or exceeding predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating decision-tree-like split nodes into symbolic regression enables the discovery of hybrid models that capture both continuous physiological relationships and discrete clinical rules.
- **Mechanism:** The algorithm introduces a specialized "split node" with three branches: a condition subtree, a true-branch subtree, and a false-branch subtree. Unlike standard decision trees, the condition can be a complex mathematical expression. Optimization follows a three-step heuristic: first fitting the condition expression, then determining the threshold τ that minimizes target variance across partitions, and finally optimizing the remaining subtrees on their respective data subsets.
- **Core assumption:** The heuristic of optimizing the condition and threshold separately from the branch subtrees effectively navigates the non-convex loss landscape introduced by the split's discontinuity.
- **Evidence anchors:**
  - [abstract] "...introduces a split node operator that partitions data based on learned thresholds while maintaining compatibility with parameter..."
  - [section 3] "...split is a predicate determining whether to evaluate the left or right branch... optimization is done in three steps..."
  - [corpus] Indirect support found in corpus neighbors (e.g., SHAPoint, Tsetlin Machines) emphasizing the clinical necessity of interpretable "rule-based" or "point-based" systems, though specific technical validation of Brush's split-node architecture is absent in the provided corpus.
- **Break condition:** If the variance-minimization heuristic for threshold selection consistently fails to create pure partitions in noisy data, the splits may add complexity without improving the signal-to-noise ratio, leading to overfitting.

### Mechanism 2
- **Claim:** Simultaneously optimizing for prediction error (MSE) and model complexity (linear complexity) produces Pareto-optimal models that are significantly smaller and more interpretable than standard machine learning baselines.
- **Mechanism:** Brush employs a multi-objective genetic programming loop. It uses ϵ-lexicase selection to maintain diverse, high-performing candidates during parent selection and NSGA-II (Non-dominated Sorting Genetic Algorithm II) for survival selection. This explicitly preserves a frontier of solutions ranging from simple/low-accuracy to complex/high-accuracy, rather than converging solely on the lowest error.
- **Core assumption:** Linear complexity (sum of node complexities) serves as an effective proxy for human interpretability in clinical settings.
- **Evidence anchors:**
  - [abstract] "...employs genetic programming with modern strategies... to balance model performance and complexity."
  - [section 3] "Brush uses epsilon-lexicase for selection, and non-dominated sorting (NSGA-II) for survival... proven to yield better convergence performance."
  - [corpus] Weak/missing direct technical validation in corpus; however, [section 6] shows Brush produces models 20x smaller than PS-Tree with comparable performance.
- **Break condition:** If the complexity penalty is not weighted correctly against the error metric, the algorithm may either trivially converge to a simple constant (underfitting) or produce excessively deep trees (bloat).

### Mechanism 3
- **Claim:** Applying non-linear gradient descent (Levenberg-Marquardt) to the constants within the expression tree accelerates convergence and accuracy compared to evolutionary constant tuning alone.
- **Mechanism:** While the tree structure is evolved via genetic operations (crossover, mutation), the free parameters (weights and constants) are treated as differentiable parameters. The algorithm computes the residual error H(θ) = f̂(X, θ) - y and iteratively updates θ using the Levenberg-Marquardt algorithm, decoupling structural search from parameter tuning.
- **Core assumption:** The optimization landscape remains sufficiently smooth for gradient descent to be effective, even when "split nodes" introduce conditional logic discontinuities (the paper argues the heuristic handles this).
- **Evidence anchors:**
  - [abstract] "...maintaining compatibility with parameter optimization techniques like Levenberg-Marquardt."
  - [section 3] "...optimization problem is done by an iterative process of gradient descent to minimize the mean squared error described by Levenberg-Marquardt..."
  - [corpus] Missing direct evidence in corpus regarding the specific efficacy of LM in this hybrid context.
- **Break condition:** If the initial random constants are too far from optimal, or if the split node logic creates pathological loss surfaces, the gradient-based optimization may fail to converge, rendering the structural evolution ineffective.

## Foundational Learning

- **Concept: Genetic Programming (GP) & Expression Trees**
  - **Why needed here:** Brush is fundamentally an evolutionary algorithm that evolves a population of mathematical expression trees. Understanding how trees represent functions (operators as internal nodes, variables/constants as leaves) is required to grasp how "split nodes" modify the search space.
  - **Quick check question:** How does a tree representation allow for the "split node" to conditionally execute only one of its sub-branches (True vs. False) during evaluation?

- **Concept: Multi-Objective Optimization (Pareto Front)**
  - **Why needed here:** The paper claims Brush is "Pareto-optimal" on SRBench. You must understand that the algorithm optimizes two conflicting objectives (Accuracy vs. Complexity) and returns a set of solutions where you cannot improve one metric without degrading the other.
  - **Quick check question:** Why is NSGA-II survival selection used instead of selecting only the single best individual based on accuracy?

- **Concept: Levenberg-Marquardt (LM) Algorithm**
  - **Why needed here:** The authors credit LM optimization as a key factor in recovering physics equations and clinical scores.
  - **Quick check question:** Why might a gradient-based method like LM struggle with a tree containing "split nodes" (which are discontinuous), and how does the paper's 3-step heuristic address this?

## Architecture Onboarding

- **Component map:** Data Structures -> Initialization -> Evaluator -> Selector -> Variator -> Survival -> Post-processor
- **Critical path:**
  1. Initialization → 2. Evaluation (Optimize params with LM) → 3. Selection (Lexicase) → 4. Variation (Crossover/Mutation) → 5. Survival (NSGA-II) → 6. Repeat until max generations
- **Design tradeoffs:**
  - Exactness vs. Interpretability: Adding split nodes allows for rule-based logic (high interpretability) but technically prevents exact equation recovery in physics benchmarks (Section 4 notes "split nodes... may prevent exact equation recovery")
  - Speed vs. Accuracy: Using LM optimization is computationally expensive per evaluation but reduces the number of generations required for convergence compared to pure evolutionary search
- **Failure signatures:**
  - "Bloat": Model size grows without accuracy gain (Monitor: check if linear complexity constraint in NSGA-II is active)
  - Nan/Inf in LM: Optimization diverges on pathological trees (Check: constraint on max tree depth and valid function domains, e.g., log of negative numbers)
  - Stuck at Local Optima: Population converges to a suboptimal Pareto front (Check: increase ϵ in Lexicase to maintain diversity)
- **First 3 experiments:**
  1. Physics Recovery (Ground Truth): Run Brush on a Feynman equation dataset (e.g., from SRBench) to verify if it recovers the correct equation and achieves R² > 0.999
  2. Clinical Score Regression: Train on the MIMIC-IV CART score data to see if the model size stays under 100 nodes while maintaining R² > 0.8
  3. Ablation on Splits: Run the "MEWS Deterioration" classification task with Split node disabled vs. enabled to replicate the performance drop (0.88 vs 0.95 AUPRC) observed in Section 6

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can systematic hyperparameter tuning significantly improve Brush's performance or reduce model size compared to the fixed configurations used in the study?
- Basis in paper: [explicit] "First, Brush was not fine tuned, which could lead to better results, but was out of the scope of this paper."
- Why unresolved: The authors utilized fixed hyperparameters to standardize experiments, leaving the potential gains from optimization unquantified.
- What evidence would resolve it: Benchmark results from Bayesian optimization or grid search on Brush's evolutionary parameters (e.g., population size, mutation rates).

### Open Question 2
- Question: Does deriving node complexity weights from domain-specific prior distributions yield better models than the current arbitrary linear complexity definition?
- Basis in paper: [explicit] "Brush minimizes linear complexity based on arbitrarily defined complexities for each node, which could be tweaked or derived from prior distributions observed in different fields."
- Why unresolved: The current study employed a generic complexity metric, deferring the exploration of domain-adaptive complexity functions.
- What evidence would resolve it: Comparative results using priors from physics vs. clinical domains showing improved accuracy or human-rated interpretability.

### Open Question 3
- Question: How does Brush compare in performance to recent deep learning-based symbolic regression methods under strictly controlled, fair conditions?
- Basis in paper: [inferred] The authors noted that raw results for recent methods were unavailable, limiting the comparison to a "rough estimate" and potentially unfair comparisons.
- Why unresolved: Lack of standardized raw data for competitors (E2E, uDSR) prevented statistically rigorous head-to-head comparisons.
- What evidence would resolve it: A re-evaluation of Brush against deep learning SR methods on identical hardware with fixed seeds and evaluation limits using the SRBench framework.

## Limitations

- The specific efficacy of LM optimization in the presence of split nodes lacks direct validation in the provided corpus
- Clinical applicability depends on assumptions about EHR data quality and feature selection that aren't fully explored
- The paper does not explore systematic hyperparameter tuning, leaving potential performance gains unquantified

## Confidence

- **High Confidence:** SRBench benchmark performance claims (R² > 0.999 for physics equations, Pareto-optimality), MEWS score recovery accuracy (AUPRC > 0.95)
- **Medium Confidence:** Clinical score interpretability claims (60-76 node models recapitulating CART/MEWS logic), model size comparisons vs. baselines
- **Low Confidence:** The specific efficacy of LM optimization in the presence of split nodes, and whether the 3-step heuristic reliably navigates the optimization landscape

## Next Checks

1. **Optimization Stability Test:** Run the 3-step split node optimization on synthetic datasets with known ground truth equations to measure how often the LM component successfully converges versus getting stuck in local minima.

2. **Interpretability Audit:** Have clinical experts evaluate whether the recovered 60-76 node models for CART/MEWS actually reflect the logical structure of the original scoring systems, not just statistical similarity.

3. **Baseline Sensitivity Analysis:** Replicate the SRBench results with and without the ϵ-lexicase selection to quantify how much the Pareto-optimality depends on this specific selection mechanism versus the split node architecture.