---
ver: rpa2
title: Local-to-Global Logical Explanations for Deep Vision Models
arxiv_id: '2601.13404'
source_url: https://arxiv.org/abs/2601.13404
tags:
- explanations
- explanation
- image
- class
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need for interpretable explanations of
  deep vision models by introducing a neurosymbolic framework that generates human-understandable
  concept-level explanations. The method produces local explanations for individual
  images and global explanations for entire classes using logical formulas in monotone
  disjunctive normal form (MDNF), where each clause represents minimally sufficient
  combinations of annotated objects or parts.
---

# Local-to-Global Logical Explanations for Deep Vision Models

## Quick Facts
- arXiv ID: 2601.13404
- Source URL: https://arxiv.org/abs/2601.13404
- Reference count: 27
- Produces human-understandable concept-level explanations for vision models via local-to-global logical compilation

## Executive Summary
This work addresses the need for interpretable explanations of deep vision models by introducing a neurosymbolic framework that generates human-understandable concept-level explanations. The method produces local explanations for individual images and global explanations for entire classes using logical formulas in monotone disjunctive normal form (MDNF), where each clause represents minimally sufficient combinations of annotated objects or parts. The approach was evaluated on ADE20K and Pascal-Parts datasets with VGG-19 and ViT-B architectures, achieving coverage of approximately 85% of validation decisions with only 20 clauses.

## Method Summary
The framework takes a black-box vision model and object annotations as input. It first identifies Minimally Sufficient Concept-based Explanations (MSCX) for individual images via beam search that finds minimal object sets sufficient for classification. Local MSCXs are then compiled into global logical formulas using greedy set cover algorithms. For multi-class problems, an ordered explanation list is generated that prioritizes rules to maximize coverage while minimizing classification errors. The approach maintains high fidelity while being significantly more interpretable than traditional saliency-based methods.

## Key Results
- Achieved coverage of approximately 85% of validation decisions with only 20 clauses
- Multi-class explanation list accuracy of 75.69% for ViT-B on ADE20K
- High fidelity scores (Fid+ < 0.1, Fid- > 0.9) demonstrating accurate model replication
- Explanations maintain interpretability while requiring no architectural modifications or retraining

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Sufficiency Search
The algorithm uses beam search to incrementally add objects to a blurred baseline image, querying the black-box model to check if confidence scores exceed sufficiency thresholds. The core assumption is model monotonicity - adding relevant objects should not decrease class confidence.

### Mechanism 2: Logical Compilation via Greedy Set Cover
Local sufficient explanations are aggregated into compact global logical formulas (Monotone DNF) using greedy set cover algorithms. The approach assumes object-based rationales transfer across images within a class.

### Mechanism 3: Ordered Explanation Lists for Multi-Class Disambiguation
An ordered list of rules resolves multi-class ambiguity by evaluating each MSCX based on its classification error across different classes, constructing a decision list that maximizes coverage while minimizing errors.

## Foundational Learning

- **Concept: Disjunctive Normal Form (DNF) & Monotonicity**
  - Why needed here: Global explanations are structured as Monotone DNF formulas (OR of ANDs, no NOTs)
  - Quick check: If formula is `(Bed) OR (Wall AND Sofa)`, does an image with Bed, Wall, and Sofa satisfy it? (Yes)

- **Concept: Model Fidelity vs. Ground Truth**
  - Why needed here: This is post-hoc interpretability explaining the model, not reality
  - Quick check: If model predicts "Hospital" due to "Tree" (spurious), should explanation include "Tree"? (Yes)

- **Concept: Beam Search**
  - Why needed here: Finding exact minimal object sets is combinatorially expensive
  - Quick check: Beam Width B=1 is "Greedy Best-First," B=Infinity is "Breadth-First." Why use small B (e.g., 3)? (Balance quality vs. computational cost)

## Architecture Onboarding

- **Component map:** Oracle (Black-box Model) <- Local Explainer (BeamAdd) <- Segmentation Engine <- Input Image
- **Critical path:** Quality of Segmentation Engine is bottleneck - mislabeled masks create nonsensical explanations
- **Design tradeoffs:** Blur vs. Inpainting (blur faster but creates artifacts); Beam Width B (lower faster, may miss minimal sets)
- **Failure signatures:** High Fidelity+/Low Coverage (explanations don't generalize); Empty MSCX (search fails); Trivial Explanations (common objects dominate)
- **First 3 experiments:**
  1. Sanity Check: Train on random labels - should fail to find consistent MSCXs
  2. Perturbation Sensitivity: Vary masking strategy to measure monotonicity assumption validity
  3. Faithfulness Stress Test: Remove MSCX objects - model confidence should drop significantly

## Open Questions the Paper Calls Out

- **Open Question 1:** Does replacing Gaussian blurring with tractable generative inpainting improve explanation fidelity by reducing out-of-distribution artifacts?
  - Basis: Authors state future work will "employ tractable probabilistic models to inpaint or erase objects... mitigating the out-of-distribution artifacts"
  - Why unresolved: Current blurring creates unnatural image statistics that may distort confidence scores
  - What evidence would resolve it: Comparison of fidelity scores when using inpainting versus current blurring

- **Open Question 2:** Can the local-to-global logical framework be extended to video data while maintaining temporal consistency across frames?
  - Basis: Conclusion lists "Temporal generalization" as future direction
  - Why unresolved: Current framework operates on static images; unknown if MDNF rules remain stable over time
  - What evidence would resolve it: Evaluation on video datasets measuring temporal consistency of MSCXs

- **Open Question 3:** How does violation of the score monotonicity assumption affect correctness of derived local explanations?
  - Basis: Approach based on observation that confidence scores typically grow monotonically with positive evidence
  - Why unresolved: If adding relevant object occasionally lowers confidence, beam search might prune valid explanations
  - What evidence would resolve it: Analysis of explanation accuracy on models designed to exhibit non-monotonic reasoning

## Limitations

- Perturbation-based sufficiency search assumes model monotonicity, which may fail for models relying on global context
- Approach depends heavily on quality of object annotations - errors propagate directly to explanations
- Beam search with small width (B=3) may miss minimally sufficient sets, returning supersets instead

## Confidence

- **High Confidence:** Logical compilation via greedy set cover is well-established; 85% coverage with ~20 clauses is measurable
- **Medium Confidence:** Beam search sufficiency mechanism depends on assumptions about model behavior that may not generalize
- **Medium Confidence:** Multi-class explanation list accuracy (75.69% on ADE20K) is valid for specific experimental setup but may degrade with more classes

## Next Checks

1. **Perturbation Sensitivity:** Systematically vary masking strategies (blur, gray, inpainting) and measure how often monotonicity assumption holds across different model architectures
2. **Annotation Quality Impact:** Compare explanations from human annotations versus automated segmenters to quantify dependency on segmentation quality
3. **Generalization Test:** Apply method to dataset with high intra-class visual diversity to test whether local MSCXs generalize to global coverage