---
ver: rpa2
title: 'Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs'
arxiv_id: '2511.15921'
source_url: https://arxiv.org/abs/2511.15921
tags:
- reasoning
- confidence
- entropy
- calibration
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This project develops a self-correcting framework for large language
  models (LLMs) that detects and mitigates hallucinations during multi-step reasoning.
  Rather than relying solely on final answer correctness, our approach leverages fine-grained
  uncertainty signals: self-assessed confidence alignment and token-level entropy
  spikes to detect unreliable and unfaithful reasoning in real time.'
---

# Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs

## Quick Facts
- arXiv ID: 2511.15921
- Source URL: https://arxiv.org/abs/2511.15921
- Reference count: 2
- Primary result: Framework improves LLM reasoning accuracy by 3 percentage points while reducing calibration error by 9% through self-correcting mechanisms

## Executive Summary
This project develops a self-correcting framework for large language models that detects and mitigates hallucinations during multi-step reasoning. The approach leverages fine-grained uncertainty signals including self-assessed confidence alignment and token-level entropy spikes to detect unreliable reasoning in real time. Using a composite reward function with reinforcement learning, the framework shapes the model's generation behavior to improve not just final answer correctness but the coherence and faithfulness of intermediate reasoning steps.

## Method Summary
The framework implements GRPO-style RL fine-tuning on Qwen3-0.6B using LoRA adapters to optimize a composite reward function. The reward combines confidence calibration (alignment between self-assessed confidence and correctness), entropy penalties (negative rewards for token-level entropy spikes indicating reasoning instability), and format validation. The model generates structured outputs with reasoning, answer, and confidence tags, with the reward engine computing symbolic correctness, calibration alignment, and entropy statistics to guide policy updates.

## Key Results
- Improved final answer accuracy by 3 percentage points
- Reduced Expected Calibration Error by over 9 percent
- Confidence alone yields +2.2 pp accuracy gain; adding entropy yields full +3.0 pp improvement, confirming complementary effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localized spikes in token-level entropy act as a proxy for reasoning instability and potential hallucinations during generation.
- **Mechanism:** The framework computes Shannon entropy over top-k token probabilities at each step, normalizes using z-scores to identify outliers, and penalizes these spikes in the reward function.
- **Core assumption:** High entropy specifically indicates unfaithful reasoning or hallucination risk rather than valid uncertainty.
- **Evidence anchors:** Abstract states "leverages... token-level entropy spikes to detect unreliable and unfaithful reasoning"; section 3 describes spikes as proxies for instability.

### Mechanism 2
- **Claim:** Penalizing miscalibration between self-assessed confidence and ground-truth correctness forces the model to develop "introspective" awareness.
- **Mechanism:** Model outputs scalar confidence score with answer; reward function calculates alignment between this score and binary correctness label, explicitly rewarding justified high confidence.
- **Core assumption:** Model can learn to accurately represent internal certainty via text-based confidence score without external verification.
- **Evidence anchors:** Results show "fine-tuning... reduced calibration error by over 9 percent and improved accuracy by 3 percentage points."

### Mechanism 3
- **Claim:** Composite reward function combining entropy penalties and calibration rewards creates synergistic effect more robust than single-signal optimization.
- **Mechanism:** GRPO updates model weights using total reward combining confidence, entropy (negative penalty), and format enforcement terms.
- **Core assumption:** LoRA adapters on query/value projections are sufficient to alter complex reasoning behaviors without catastrophic forgetting.
- **Evidence anchors:** Section 5.5 shows "Confidence alone yields a +2.2 pp accuracy gain; adding entropy yields the full +3.0 pp improvement."

## Foundational Learning

- **Concept:** **Expected Calibration Error (ECE)**
  - **Why needed here:** Paper explicitly optimizes for calibration; required to interpret "Calibration Error" and "Brier Score" results.
  - **Quick check question:** If a model has 80% confidence on a set of examples, what fraction of those examples must be correct for it to be perfectly calibrated?

- **Concept:** **Shannon Entropy**
  - **Why needed here:** Primary uncertainty signal; understand how entropy is calculated from probability distributions to grasp "entropy spikes" detection.
  - **Quick check question:** Does a flattened probability distribution (e.g., 50/50 split) result in higher or lower entropy than a peaked distribution (e.g., 99/1 split)?

- **Concept:** **Z-score Normalization**
  - **Why needed here:** Mechanism relies on "z-score filters" to detect spikes relative to local context rather than using static threshold.
  - **Quick check question:** If the mean entropy of a sequence is 0.4 with a std dev of 0.1, what is the z-score of a token with entropy 0.7?

## Architecture Onboarding

- **Component map:** Structured Prompt (System + User query) -> Qwen3-0.6B + LoRA Adapters -> Outputs (<think>, <answer>, <confidence>) -> Reward Engine (correctness, calibration, entropy) -> GRPO Optimizer

- **Critical path:** Extraction of symbolic answer and confidence score from raw text output. If regex parsing fails, reward function cannot compute calibration, breaking training loop.

- **Design tradeoffs:**
  - LoRA vs. Full Finetuning: Authors use LoRA for efficiency but acknowledge model still struggles with complex problems.
  - Top-k Entropy: Uses k=5 for efficiency; approximation of full vocabulary entropy potentially missing uncertainty in long tail.

- **Failure signatures:**
  - Notation Instability: "Notation-heavy problems occasionally caused instability in... entropy outputs."
  - Underconfidence: Model sometimes outputs correct answers with lower confidence than expected.
  - Regex Failure: If model hallucinates text outside tags, Format Reward (-1.0) dominates, potentially drowning out learning signal.

- **First 3 experiments:**
  1. **Reward Ablation:** Train three separate models (Entropy-only, Confidence-only, Full Composite) on same subset to verify +3.0 pp gain and synergy.
  2. **Threshold Sensitivity:** Vary z-score threshold (Ï„=1.5) to higher/lower values to observe impact on "Spike Rate" and reasoning stability.
  3. **Format Robustness:** Test model on data without strict system prompt to see if structured reasoning behavior persists.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the entropy and confidence-based reward mechanisms effectively generalize to open-ended generation tasks or non-mathematical domains? [explicit] Conclusion states "extending this method to open-ended generation tasks... remains challenging." Study restricted to MATH-500 dataset.

- **Open Question 2:** How can the framework distinguish between genuine reasoning instability and noise caused by notation-heavy problems? [explicit] Discussion notes "notation-heavy problems... caused instability in the model's entropy outputs." Current token-level analysis appears sensitive to specialized symbols.

- **Open Question 3:** Would introducing a minimal confidence floor or modifying the calibration penalty effectively mitigate the observed underconfidence on straightforward questions? [explicit] Discussion observes "underconfidence on some straightforward questions" and suggests modifying penalty or adding floor.

## Limitations

- **Model Identification Ambiguity:** References "Qwen3-0.6B" which is not publicly available as of publication date, creating fundamental ambiguity for reproduction.
- **Confidence Signal Validity Uncertainty:** Doesn't address whether model truly learns introspection or merely outputs calibrated-looking confidence scores that don't reflect genuine reasoning certainty.
- **Token Entropy Proxy Assumption:** Assumes high entropy specifically indicates hallucination rather than valid uncertainty, which may not hold across different domains.

## Confidence

**High Confidence:** Experimental results showing improved accuracy (+3.0 percentage points) and calibration metrics (reduced ECE by 9%) based on controlled experiments with clear before/after comparisons.

**Medium Confidence:** Mechanism claims about how entropy spikes specifically indicate hallucination risk and how confidence calibration creates introspective awareness are theoretically plausible but lack direct empirical validation beyond correlation.

**Low Confidence:** Claim that framework would generalize to complex problems beyond tested MATH-500 subset is not supported by results; authors explicitly note model "still struggled with more complex problems."

## Next Checks

1. **Mechanism Validation Test:** Design experiment isolating entropy spikes from other forms of uncertainty by creating controlled test cases where high entropy clearly indicates hallucination versus cases where it indicates valid ambiguity.

2. **Confidence Signal Integrity Test:** Implement blinded evaluation where human annotators assess whether model's confidence outputs reflect genuine reasoning certainty versus calibrated-looking but potentially superficial outputs.

3. **Generalization Stress Test:** Evaluate framework on reasoning tasks requiring significantly more complex multi-step reasoning than MATH-500, such as advanced mathematical proofs or scientific reasoning problems.