---
ver: rpa2
title: 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall'
arxiv_id: '2510.07896'
source_url: https://arxiv.org/abs/2510.07896
tags:
- layers
- knowledge
- neurons
- editing
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of knowledge editing in large language
  models for multi-hop factual recall, where existing methods fail when edits involve
  intermediate implicit subjects in reasoning chains. The core method idea, ACE (Attribution-Controlled
  Knowledge Editing), leverages neuron-level attribution to identify and edit critical
  query-value pathways that propagate information through implicit subjects during
  multi-hop reasoning.
---

# ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall

## Quick Facts
- arXiv ID: 2510.07896
- Source URL: https://arxiv.org/abs/2510.07896
- Reference count: 26
- Primary result: ACE outperforms state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B in multi-hop accuracy

## Executive Summary
This work addresses the problem of knowledge editing in large language models for multi-hop factual recall, where existing methods fail when edits involve intermediate implicit subjects in reasoning chains. The core method idea, ACE (Attribution-Controlled Knowledge Editing), leverages neuron-level attribution to identify and edit critical query-value pathways that propagate information through implicit subjects during multi-hop reasoning. The framework systematically identifies query layers that activate value neurons, then edits both query and value components using multi-hop prompts. The primary results show ACE outperforms state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B in multi-hop accuracy, demonstrating the necessity of editing both query and value layers for effective multi-hop knowledge editing.

## Method Summary
ACE is a two-stage framework for multi-hop knowledge editing. First, it identifies critical query and value FFN layers using importance scores: I(l) measures value importance via log-probability increase, while I_query(l) measures query importance via inner product with subkey. Second, it edits these layers using PMET-style optimization with preservation loss (λ||ŴK₀ - Wfc2K₀||²) and editing loss (||ŴKE - VE||²). The method performs sequential edits: value edits update factual content in deeper FFN layers, then query edits redirect activation pathways in middle-to-shallow FFN layers. Critical layers are architecture-dependent: GPT-J uses query {3–8}, value {26–28}; Qwen3-8B uses query {25–27}, value {28–32}. The framework uses multi-hop prompts with few-shot examples and chain-of-thought reasoning to guide the editing process.

## Key Results
- ACE achieves 9.44% higher multi-hop accuracy than state-of-the-art on GPT-J
- ACE achieves 37.46% higher multi-hop accuracy than state-of-the-art on Qwen3-8B
- Ablating top query neurons from fq16/fq18 causes 46.2%/61.9% capability decrease, confirming their critical role

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-hop factual recall relies on coordinated query-value (Q-V) neuron interactions where implicit subjects function as "query neurons" that sequentially activate corresponding "value neurons" across layers.
- **Mechanism:** During reasoning chains, query neurons in middle-to-shallow FFN layers activate value neurons in deeper layers through inner product scoring (I_query = v · fc1_k). This creates cascading activations that accumulate information toward the final answer.
- **Core assumption:** Information propagates through structured neural pathways rather than being retrieved in a single step; the first-order Taylor approximation of log-probability changes accurately captures neuron importance.
- **Evidence anchors:**
  - [abstract] "We discover that during multi-hop reasoning, implicit subjects function as 'query neurons', which sequentially activate corresponding 'value neurons' across transformer layers to accumulate information toward the final answer"
  - [section 4.2] "query neuron activation (blue curve) consistently precedes value neuron activation (red histogram) by 1-2 layers"; ablating top 100 query neurons from fq16/fq18 caused 46.2%/61.9% capability decrease
  - [corpus] Related work (Yu & Ananiadou, 2023) supports query-value interaction patterns, but corpus lacks direct validation of the sequential activation timing claim
- **Break condition:** If attention layers (not FFN) primarily drive implicit subject resolution, or if multi-hop recall operates via single-step retrieval without cascading activation, the Q-V pathway hypothesis fails.

### Mechanism 2
- **Claim:** LLMs store semantically analogous knowledge in structurally similar transformer components with consistent layer-wise localization patterns.
- **Mechanism:** Knowledge categories (e.g., Nationality, Capital, Sports Team) activate overlapping sets of attention and FFN layers across different subjects. MHSA stores general knowledge patterns; FFN layers extract domain-specific knowledge.
- **Core assumption:** The localization patterns observed on single-hop queries generalize to multi-hop reasoning chains.
- **Evidence anchors:**
  - [abstract] "LLMs store semantically analogous knowledge in structurally similar transformer components, with query and value neurons for specific knowledge types exhibiting consistent localization patterns"
  - [section 4.1, Table 1] Layers a27, a26, a7 rank top across all 8 semantic categories in attention; FFN shows semantic clustering (f24, f27, f16 for NN/CT/LG/CP/LS vs. f26, f27, f24 for AT/ST/CF)
  - [corpus] Limited corpus validation; Geva et al. (2020) supports FFN as key-value memories but doesn't address semantic clustering patterns
- **Break condition:** If localization patterns vary significantly across model scales or architectures beyond GPT-J and Qwen3-8B, or if semantic clustering is an artifact of the specific dataset (MQuAKE-3K subset), this mechanism weakens.

### Mechanism 3
- **Claim:** Effective multi-hop knowledge editing requires editing both query layers (to redirect activation pathways) and value layers (to update stored knowledge); editing only one fails to propagate changes through reasoning chains.
- **Mechanism:** ACE performs sequential edits: (1) identify critical query/value layers via importance scores, (2) edit value layers in deeper FFNs to update factual content, (3) edit query layers in middle-to-shallow FFNs to redirect activation toward updated value neurons.
- **Core assumption:** PMET-style linear weight updates can effectively modify both activation patterns (query) and semantic content (value) without interfering with preserved knowledge.
- **Evidence anchors:**
  - [abstract] "ACE outperforms state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B"
  - [section 6.3, Table 4] Skipping top 3 query layers: -16.51% performance; skipping top 2 value layers: -40.45% performance
  - [corpus] IFMET (Zhang et al., 2024b) improves multi-hop KE via deeper layer editing but lacks mechanistic explanation; corpus supports difficulty of multi-hop KE but not the specific query/value decomposition
- **Break condition:** If the performance gains derive primarily from increased edit capacity (more layers edited) rather than the query/value distinction, or if alternative architectures (e.g., hypernetworks) achieve similar results without this decomposition, the mechanism is insufficient.

## Foundational Learning

- **Concept: FFN as Key-Value Memories (Geva et al., 2020)**
  - Why needed here: ACE builds on the view that FFN layers store factual knowledge where fc1 rows act as keys and fc2 columns as values. Understanding this is prerequisite to grasping why ACE targets FFN specifically.
  - Quick check question: Can you explain why ablating an FFN value neuron affects token probability but ablating a query neuron affects which value neurons get activated?

- **Concept: Locate-Then-Edit Paradigm (ROME, MEMIT, PMET)**
  - Why needed here: ACE extends PMET; you must understand causal tracing, importance scoring, and closed-form weight optimization to follow the methodology.
  - Quick check question: What does the "locate" phase identify and what constraint does the "edit" phase impose on the weight update?

- **Concept: Multi-Hop Reasoning Chains and Implicit Subjects**
  - Why needed here: The core problem ACE addresses is editing intermediate facts in reasoning chains. You need to distinguish explicit subjects (query surface) from implicit subjects (intermediate entities) to understand why standard KE fails.
  - Quick check question: For "What country is Mark Trumbo's sport from?", identify the explicit subject, implicit subject(s), and the reasoning chain structure.

## Architecture Onboarding

- **Component map:**
  - Stage 1 (Identifying): Forward pass on multi-hop questions → compute importance scores (Eq. 9-10) at last token position → rank query/value layers → select top-k layers
  - Stage 2 (Editing): Split into value edit (deeper FFN layers, using multi-hop prompts) then query edit (middle-to-shallow FFN layers)
  - Backbone: PMET-style optimization with preservation loss (λ||ŴK₀ - Wfc2K₀||²) and editing loss (||ŴKE - VE||²)
  - Attribution metrics: Value importance via log-prob increase; query importance via inner product with subkey

- **Critical path:** Correctly identifying query layers is the bottleneck—if query layers are misidentified, value edits won't be properly activated during inference. For GPT-J: Rq = {3,4,5,6,7,8}, Rv = {26,27,28}. For Qwen3-8B: Rq = {25,26,27}, Rv = {28,29,30,31,32}.

- **Design tradeoffs:**
  - More edited layers → higher efficacy but risk of interfering with preserved knowledge (knowledge bottleneck, Table 6)
  - Query layers are architecture-dependent (fixed separation in GPT-J vs. dynamic alignment in Qwen3)
  - Assumption: Value neurons exhibit semantic interpretability when projected to vocabulary space—this enables targeted editing but may not hold across all architectures

- **Failure signatures:**
  - Skipping value edits → 40%+ accuracy drop (knowledge not stored)
  - Skipping query edits → 16%+ accuracy drop (new knowledge not activated during reasoning)
  - Random neuron ablation → ~9% drop vs. semantic neuron ablation → ~90% drop (Section 4.1)
  - Ablating 27 interpretable neurons → 3.2% accuracy vs. 27 high-importance non-interpretable neurons → 59.4% accuracy (Section 6.4)

- **First 3 experiments:**
  1. **Reproduce importance score distribution:** Run forward pass on MQuAKE-3K subset with GPT-J, compute I(l) and I_query(l) across all layers. Verify query activation precedes value activation by 1-2 layers.
  2. **Ablation validation:** Zero out top-100 query neurons from fq16/fq18. Confirm value neuron activation drops in f17/f18/f19 from (28,16,33) to (6,4,7).
  3. **Single-edit baseline comparison:** Apply ACE with only value layer editing (skip query) vs. full ACE on 100 multi-hop instances. Target ~26% gap to validate query contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the query-value activation mechanism discovered in multi-hop factual recall generalize to other reasoning paradigms such as logical deduction, mathematical reasoning, or code generation?
- **Basis in paper:** [inferred] The paper claims its "mechanistic understanding resolves long-standing questions about how information propagates in multi-hop scenarios" but only evaluates on factual recall chains from MQuAKE-3K, leaving other reasoning types unexplored.
- **Why unresolved:** The Q-V pathway hypothesis was developed and validated exclusively on factual knowledge retrieval; whether similar neuron-level coordination underlies other reasoning tasks remains unknown.
- **What evidence would resolve it:** Applying ACE's attribution analysis to benchmarks requiring non-factual reasoning (e.g., GSM8K step-by-step math, logical entailment datasets) and testing whether editing Q-V pairs improves multi-step performance.

### Open Question 2
- **Question:** How does ACE scale to models with 70B+ parameters, and does the query-value layer separation pattern remain consistent or fragment across larger architectures?
- **Basis in paper:** [inferred] Experiments are limited to GPT-J (6B) and Qwen3-8B; the paper notes architectural differences between these models (fixed vs. dynamic layer alignment) but does not address scaling trends.
- **Why unresolved:** Larger models have more layers and potentially distributed representations; the neuron-level attribution approach may face computational or interpretability challenges at scale.
- **What evidence would resolve it:** Evaluating ACE on LLaMA-70B or similar, reporting attribution stability, layer identification consistency, and multi-hop accuracy relative to compute cost.

### Open Question 3
- **Question:** Can the observation that "semantically convergent tokens" align with token entropy patterns in RL be leveraged to improve reinforcement learning training for reasoning?
- **Basis in paper:** [explicit] Section 6.4 states: "Furthermore, these semantically convergent tokens, and neurons which serve as shared neurons also are aligned in recent RL research concerning token entropy (Wang et al., 2025), represents a promising focus for future studies of critical tokens in RL."
- **Why unresolved:** The connection is noted as an observation but not experimentally validated; the causal relationship between interpretability, entropy, and RL training dynamics is untested.
- **What evidence would resolve it:** Experiments correlating neuron-level semantic convergence with token-level entropy during RL fine-tuning, and interventions targeting high-convergence tokens to test impact on reasoning performance.

### Open Question 4
- **Question:** What determines the domain-specific positioning of query and value layers in models like Qwen3, and can this positioning be predicted or optimized during training?
- **Basis in paper:** [explicit] Section 6.4 notes: "In Qwen3-8B, the absolute positions of these query and value layers are not statically fixed, they shift dynamically depending on the knowledge domain."
- **Why unresolved:** The paper documents this phenomenon but provides no theoretical account of why certain domains trigger different layer activations, limiting transferability of the method.
- **What evidence would resolve it:** Systematic analysis mapping knowledge domains to layer positions, potentially identifying input features (syntactic, semantic, or embedding-space) that predict activation patterns.

## Limitations

- **Localization Generalization:** The claim that knowledge types activate consistent layer patterns may not generalize beyond GPT-J and Qwen3-8B architectures; the corpus provides limited validation of this semantic clustering pattern.
- **Attribution Mechanism Specificity:** The sequential query-value activation mechanism lacks direct empirical validation and cross-architecture verification; the corpus doesn't provide independent confirmation of this specific pathway.
- **Query-Value Decomposition Necessity:** While both query and value edits are empirically necessary, the assumption that PMET-style linear updates can effectively modify both activation patterns and semantic content without interference is not independently validated.

## Confidence

- **High:** Multi-hop accuracy improvements (9.44% on GPT-J, 37.46% on Qwen3-8B) are well-supported by ablation studies showing both query and value edits are individually necessary.
- **Medium:** The Q-V pathway hypothesis is supported by temporal activation patterns and ablation effects, but lacks cross-architecture validation and mechanistic specificity.
- **Low:** Semantic clustering patterns and localization generalization across knowledge types are primarily observational claims without systematic external validation.

## Next Checks

1. **Cross-architecture replication:** Apply ACE to a different LLM family (e.g., LLaMA, Mistral) and verify whether the same query-value layer separation pattern holds, or if localization patterns vary significantly with architecture.

2. **Alternative pathway validation:** Test whether editing only attention layers (instead of FFN query layers) can achieve similar multi-hop performance, which would challenge the FFN-specific Q-V pathway hypothesis.

3. **Capacity vs. mechanism isolation:** Compare ACE performance against a baseline that edits the same number of layers randomly (without query-value decomposition) to determine whether gains derive from increased edit capacity or the specific query-value distinction.