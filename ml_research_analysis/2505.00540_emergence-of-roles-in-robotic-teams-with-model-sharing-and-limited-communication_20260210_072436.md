---
ver: rpa2
title: Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication
arxiv_id: '2505.00540'
source_url: https://arxiv.org/abs/2505.00540
tags:
- agents
- learning
- agent
- environment
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high computational and energy costs of
  multi-agent reinforcement learning (MARL) by proposing a strategy where a single
  agent learns via deep Q-learning and periodically shares its model with non-learning
  agents. This model sharing, combined with a reward function that encourages agents
  to collect resources away from allies and near adversaries, leads to the emergence
  of differentiated roles (e.g., explorers and disruptors) without explicit communication
  or role assignment.
---

# Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication

## Quick Facts
- **arXiv ID**: 2505.00540
- **Source URL**: https://arxiv.org/abs/2505.00540
- **Reference count**: 14
- **Primary result**: A single learning agent periodically shares its model with non-learning teammates, enabling role emergence and 80% reduction in training time versus MARL while maintaining competitive performance.

## Executive Summary
This paper introduces a resource-efficient approach to multi-agent reinforcement learning where one agent learns via DQN and periodically shares its model with non-learning teammates. The method combines centralized learning with distributed execution and incorporates a reward function that encourages agents to spread out from allies and approach adversaries. This leads to emergent role differentiation—explorers that avoid allies and disruptors that engage adversaries—without explicit role assignment or communication. The approach achieves competitive performance with 80% lower computational cost than traditional MARL, demonstrating effective specialization in resource collection tasks.

## Method Summary
The method uses a single "leader" agent learning via DQN with a CNN to process 3-channel spatial state observations (resources, allies, adversaries) within a visible radius. The leader trains with ε-greedy exploration, and its model is periodically shared with non-learning "ally" agents at lifetime boundaries, with parameter mutations added to preserve diversity. A reward function combines resource collection with terms encouraging distance from allies (explorer incentive) and proximity to adversaries (disruptor incentive). The system runs for 400 total episodes across multiple lifetimes, with evaluation conducted in exploitation mode to measure performance.

## Key Results
- Achieves resource collection performance comparable to MARL while requiring only 20% of the training time
- Successfully induces emergent role differentiation between explorer and disruptor agents without explicit role assignment
- Outperforms centralized DQN in resource collection while maintaining similar win rates against adversarial agents
- Demonstrates 80% reduction in computational overhead through single-agent learning approach

## Why This Works (Mechanism)

### Mechanism 1: Centralized Learning with Model Dissemination
Concentrating learning in a single agent while distributing learned policies reduces computational overhead while maintaining competitive task performance. One "leader" agent performs DQN learning; allies receive periodic model updates with evolutionary mutations, eliminating parallel gradient computation across agents.

### Mechanism 2: Implicit Role Differentiation via Multi-Objective Reward Shaping
A reward function combining ally-distance and adversary-proximity terms induces emergent behavioral specialization without explicit role assignment or communication. Ra (distance from allies) incentivizes exploration in uncongested zones; Re (proximity to adversaries) incentivizes disruption; optimization under mutation creates heterogeneous role emergence.

### Mechanism 3: Evolutionary Model Mutation for Behavioral Diversity
Injecting parameter noise during model transfer preserves diversity while leveraging learned policies. Noise tensors scaled by a hyperparameter perturb each weight during sharing, creating policy variants that explore different behavioral modes under the same reward structure.

## Foundational Learning

- **Deep Q-Networks (DQN)**: The leader agent uses a CNN-based DQN to process spatial state tensors and output Q-values for discrete movement actions. Why needed: Provides stable learning for the leader agent. Quick check: Can you explain why experience replay and target networks stabilize DQN training compared to online Q-learning?

- **Multi-Agent Reinforcement Learning (MARL) Fundamentals**: The paper positions its approach against MARL baselines; understanding independent learner dynamics clarifies the computational tradeoffs. Why needed: Essential for understanding the efficiency gains. Quick check: What happens to learning stability when multiple agents update policies simultaneously in a shared environment?

- **Reward Shaping and Emergent Behavior**: The core innovation is a reward function that induces roles; understanding how reward structure drives behavior is essential. Why needed: Critical for understanding role emergence. Quick check: How can multi-component reward functions lead to unintended behavioral equilibria?

## Architecture Onboarding

- **Component map**: Grid environment (100×100) -> 3-channel state tensor -> Leader DQN (CNN + FC) -> Q-values (4 actions) -> ε-greedy exploration -> Model sharing with mutation -> Allies (ε=0 exploitation)

- **Critical path**: 1. Lifetime 1: Leader learns alone, collects experiences, updates DQN. 2. End of lifetime: Mutate leader model → instantiate allies with perturbed copies. 3. Subsequent lifetimes: Leader continues learning; allies exploit. 4. Model sharing repeats each lifetime boundary. 5. Evaluation phase: All agents exploit, measure resource collection and win rate.

- **Design tradeoffs**: Model sharing frequency (optimal around 10-15 lifetimes for 400 episodes); mutation scale (balance exploration vs policy degradation); reward weights (balance disruptor vs explorer incentives); single-learner vs MARL (80% computation reduction but higher variance).

- **Failure signatures**: Allies exhibit identical trajectories (mutation too weak or reward dominated by single component); win rate stagnates below 50% (model sharing too infrequent or leader undertrained); high variance across evaluation episodes (insufficient lifetime count or unstable convergence).

- **First 3 experiments**: 1. Reproduce baseline comparison: Run single-learner vs MARL vs centralized DQN for 400 training episodes, measure average resource collection and win rate. 2. Reward ablation: Disable Ra or Re individually and quantify whether role differentiation collapses. 3. Model sharing frequency extension: Test intermediate configurations (e.g., 8 and 15 lifetimes) to validate performance peak.

## Open Questions the Paper Calls Out
- What are the specific sensitivities and boundary conditions of the reward function required to guarantee the reliable emergence of specialized roles?
- How does the single-agent learning mechanism perform when transferred to continuous environments with realistic robotic constraints?
- Is the learned strategy robust against adaptive, learning adversaries, or is it specialized for static opponents?

## Limitations
- The approach relies on periodic model sharing that may introduce latency in adapting to environmental changes
- Performance with learning adversaries remains unexplored, limiting understanding of robustness
- The method requires careful tuning of model sharing frequency and mutation parameters for optimal performance

## Confidence
- Method reproducibility: High - Core components (DQN, model sharing, reward shaping) are well-specified
- Results validity: Medium - Key hyperparameters and architectural details are unspecified
- Generalizability: Low - Limited to discrete grid environments with static adversaries

## Next Checks
1. Verify role differentiation by plotting reward component breakdown per agent; expect one agent with significantly higher Ra percentage (>70%) indicating explorer role
2. Test multiple lifetime configurations (5, 10, 20 lifetimes) to validate the claimed optimal exists around 10-15 lifetimes
3. Reproduce the 80% reduction in training time by measuring timesteps required for single-learner vs MARL baseline