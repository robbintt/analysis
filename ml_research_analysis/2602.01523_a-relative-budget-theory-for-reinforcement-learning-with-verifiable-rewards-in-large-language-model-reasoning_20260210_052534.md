---
ver: rpa2
title: A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards
  in Large Language Model Reasoning
arxiv_id: '2602.01523'
source_url: https://arxiv.org/abs/2602.01523
tags:
- budget
- relative
- learning
- reasoning
- regime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the sample complexity of reinforcement learning\
  \ with verifiable rewards (RLVR) for large language models, unifying compute constraints\
  \ and task difficulty into a relative budget framework. The key insight is that\
  \ the ratio of token budget H to expected time-to-solution \xB5x (relative budget\
  \ \u03BE=H/\xB5x) determines three distinct learning regimes: deficient (\u03BE\u2192\
  0, sample complexity explodes), balanced (\u03BE=\u0398(1), maximal sample efficiency),\
  \ and ample (\u03BE\u2192\u221E, diminishing returns)."
---

# A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2602.01523
- Source URL: https://arxiv.org/abs/2602.01523
- Reference count: 40
- Key outcome: Identifies relative budget ξ=H/E[T] as determining three learning regimes (deficient, balanced, ample) with distinct sample efficiency properties, validated empirically across models and tasks.

## Executive Summary
This paper analyzes the sample complexity of reinforcement learning with verifiable rewards (RLVR) for large language models by introducing a relative-budget framework that unifies compute constraints and task difficulty. The key insight is that the ratio of token budget H to expected time-to-solution µx (relative budget ξ=H/µx) determines three distinct learning regimes: deficient (ξ→0, sample complexity explodes), balanced (ξ=Θ(1), maximal sample efficiency), and ample (ξ→∞, diminishing returns). The authors prove that in the balanced regime, anti-concentration remains bounded away from zero, enabling efficient learning, while in the deficient regime, the probability of discovering high-reward trajectories vanishes. Empirically, they validate these predictions across models and tasks, finding that the optimal learning occurs at ξ∈[1.5,2.0], coinciding with peak reasoning performance.

## Method Summary
The paper proposes GRPO (Group Relative Policy Optimization) training on GSM8K and MATH-500 benchmarks with 4-bit quantization + LoRA optimization. The core method involves estimating the expected token count µx to correct solution for each problem using 100 solution traces from the base model, then computing the relative budget ξ=H/µx for different token budgets H. Models are trained at different ξ values (0.5, 1.0, 1.5, 2.0, 3.0) using static analysis where difficulty is estimated once from the base policy. The theoretical framework analyzes sample complexity through anti-concentration coefficients and trust-region policy optimization, with additional study of online RL dynamics under gamma distribution assumptions for time-to-solution.

## Key Results
- Three learning regimes identified by relative budget ξ: deficient (ξ→0) with prohibitive sample complexity, balanced (ξ=Θ(1)) with maximal efficiency, and ample (ξ→∞) with diminishing returns
- Anti-concentration coefficient c₀(κ) controls sample efficiency by determining probability of discovering high-reward trajectories
- Online RL causes ξ to grow linearly over iterations under gamma model, requiring quadratically increasing samples per iteration
- Optimal learning occurs at ξ∈[1.5,2.0], validated empirically across models and tasks

## Why This Works (Mechanism)

### Mechanism 1: Relative Budget Determines Regime-Dependent Sample Efficiency
The quantity ξ := H/E[T] predicts three distinct learning regimes with different sample efficiency characteristics. When ξ ≪ 1 (deficient), successful trajectories are rare because H is insufficient for the base policy to reach solutions—sample complexity diverges. When ξ ≈ 1 (balanced), both success and failure occur with probability Θ(1), maximizing reward variance while maintaining bounded anti-concentration. When ξ → ∞ (ample), variance decays as O(H/ξ) and gains per iteration diminish.

### Mechanism 2: Anti-concentration Coefficient Controls Informative Trajectory Discovery
RL sample efficiency is governed by the anti-concentration coefficient c₀(κ), which measures the probability of discovering trajectories with rewards significantly above the mean. Lemma 5.1 shows that cₓ(ε) equals P[T(τ) ≤ η(x,ε)·E[T(τ)]]—the probability of solving within a fraction η of expected time. Higher c₀ means high-reward trajectories are sampled more frequently, reducing the samples needed to learn.

### Mechanism 3: Online RL Induces Linear Relative Budget Growth with Quadratic Sample Cost
Under the gamma model for T(τ), online RL causes ξ to grow linearly over iterations while sample complexity per iteration grows quadratically. Each RL iteration reduces E[T(τ)], shifting tasks toward the ample regime. However, maintaining monotonic improvement requires exponentially increasing samples, creating a fundamental trade-off.

## Foundational Learning

- **Concept: χ²-divergence and trust regions in policy optimization**
  - Why needed here: The paper's sub-optimality analysis compares the learned policy to the best policy within a χ²-divergence ball around the base policy.
  - Quick check question: If πᵦ assigns probability 0.1 to some trajectory and the trust region radius κ = 0.5, what is the maximum probability a new policy can assign to that trajectory while staying in Πκ?

- **Concept: Anti-concentration in probability distributions**
  - Why needed here: The anti-concentration coefficient c₀(κ) captures whether a distribution has non-negligible mass in its upper tail.
  - Quick check question: For a Gaussian with mean μ and variance σ², what is P[X ≥ μ + 2σ]? Is this distribution "anti-concentrated"?

- **Concept: Finite-horizon MDPs for autoregressive generation**
  - Why needed here: The paper models LLM reasoning as an MDP where states are partial token sequences, actions are tokens, and H is the token budget.
  - Quick check question: In this MDP formulation, if H = 512 and the model generates a correct solution at token 200, what is the shaped return R(τ)?

## Architecture Onboarding

- **Component map:** Difficulty estimator -> Relative budget calculator -> Regime classifier -> Rollout allocator -> Online difficulty tracker

- **Critical path:** Estimating μₓ accurately is the bottleneck—requires substantial sampling from base policy. Paper used 100 solution traces per problem with temperature 0.7, top-p 0.9. In production, you may need to cache and incrementally update these estimates.

- **Design tradeoffs:**
  1. **H vs. coverage:** Increasing H moves more prompts from deficient→balanced→ample, but increases compute and may waste tokens on already-solved tasks
  2. **Static vs. dynamic filtering:** Static filtering is cheaper but ignores policy improvement; dynamic filtering is more accurate but adds 2-3x sampling overhead
  3. **Gamma model vs. empirical distribution:** Gamma model enables closed-form analysis but some models have bimodal distributions—empirical estimation is more robust but harder to analyze

- **Failure signatures:**
  1. **No learning signal:** Training loss flat, accuracy unchanged → likely ξ ≪ 1 for most prompts; check μₓ distribution
  2. **Diminishing returns:** Early epochs show gains, then plateau despite continued training → tasks shifted to ample regime
  3. **High variance in training metrics:** c₀ unstable across batches → prompt heterogeneity; filter to ξ ∈ [1.5, 2.0] band
  4. **Disagreement between theory and practice:** Peak efficiency at ξ ≠ [1.5, 2.0] → check reward shaping or verifier correctness

- **First 3 experiments:**
  1. **Calibrate μₓ estimates:** Sample 50-100 rollouts per prompt from your base model on a held-out set; fit gamma vs. empirical distribution; validate that ξ ≈ 1 corresponds to ~63% success rate
  2. **Ablate token budget H:** Train with H ∈ {0.5μ, 1.0μ, 1.5μ, 2.0μ, 3.0μ} where μ is median difficulty; plot final accuracy vs. ξ to identify your peak efficiency zone
  3. **Validate regime-dependent learning:** Log per-batch statistics: fraction of prompts in each regime, mean reward, reward variance, gradient norm. Confirm that balanced-regime batches show highest variance and largest gradients.

## Open Questions the Paper Calls Out

- **Can the relative-budget theory be extended to binary reward settings (terminal 1/0 outcome supervision) rather than shaped step-based rewards?**
  - Basis: Paper explicitly states this as a key open problem for characterizing standard reasoning pipelines.
  - Why unresolved: Current theory relies on shaped rewards for tractable variance analysis; binary rewards eliminate the gradient structure that enables analysis.

- **How should the theory be modified when time-to-solution distributions deviate from the gamma model, such as bimodal cases observed in contaminated models?**
  - Basis: Appendix A notes that Qwen3-4B-Instruct exhibits bimodal distributions requiring mixture model approaches.
  - Why unresolved: Gamma assumption enables closed-form analysis; mixture models complicate anti-concentration calculations.

- **Does the relative-budget framework generalize to non-mathematical reasoning domains with different solution structure characteristics?**
  - Basis: Experiments limited to GSM8K and MATH-500 with multi-step mathematical derivation.
  - Why unresolved: Other domains may have fundamentally different relationships between token budgets and solution discovery.

## Limitations

- The theoretical framework relies on distributional assumptions (vₓ = Θ(μ²ₓ), gamma distribution) that may not hold for all model behaviors, particularly those with bimodal or heavy-tailed distributions.
- Practical implementation challenges include the computational overhead of difficulty estimation and uncertainty about how optimal ξ values vary across model families and task domains.
- The paper focuses on sample efficiency within training distribution but does not address generalization to unseen problems or how relative-budget optimization affects reasoning generalization.

## Confidence

- **High Confidence:** The regime classification (deficient/balanced/ample) based on ξ = H/μₓ and qualitative predictions about sample efficiency are well-supported by both theory and empirical evidence.
- **Medium Confidence:** Specific quantitative predictions (optimal ξ ∈ [1.5, 2.0], linear growth of ξ in online RL) are supported by analysis and GSM8K/MATH-500 validation but require testing on other domains.
- **Low Confidence:** Practical implications for compute allocation and static versus dynamic difficulty estimation trade-offs lack comprehensive empirical validation.

## Next Checks

1. **Distributional Robustness Test:** Validate theory's predictions on task distributions violating gamma assumption (multi-modal, heavy-tailed). Compare theoretical predictions with empirical results on datasets with known distribution characteristics.

2. **Cross-Model Generalization:** Test whether optimal ξ range [1.5, 2.0] holds across different model families (small vs. large, different architectures). Identify systematic variations in relative-budget efficiency across model scales.

3. **Dynamic vs. Static Estimation Comparison:** Implement and compare static difficulty estimation (pre-compute μₓ) versus dynamic estimation (re-estimate during training). Quantify trade-off between estimation accuracy and computational overhead, and identify conditions where each approach is preferable.