---
ver: rpa2
title: Statistical mechanics of extensive-width Bayesian neural networks near interpolation
arxiv_id: '2505.24849'
source_url: https://arxiv.org/abs/2505.24849
tags:
- gaussian
- neural
- networks
- error
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work provides a theoretical framework for analysing the generalisation\
  \ performance of two-layer Bayesian neural networks in the extensive-width regime\
  \ near interpolation. The key method combines replica theory from statistical physics\
  \ with spherical/Harish-Chandra\u2013Itzykson\u2013Zuber integrals to derive tractable\
  \ predictions for the Bayes-optimal mean-square generalisation error."
---

# Statistical mechanics of extensive-width Bayesian neural networks near interpolation

## Quick Facts
- arXiv ID: 2505.24849
- Source URL: https://arxiv.org/abs/2505.24849
- Reference count: 40
- Key outcome: Provides theoretical framework for analysing generalisation performance of two-layer Bayesian neural networks in extensive-width regime near interpolation using replica theory and spherical integrals.

## Executive Summary
This paper presents a statistical mechanics analysis of two-layer Bayesian neural networks in the extensive-width regime where the number of hidden units scales proportionally to input dimension (k ∝ d) and the number of data points scales as n ∝ d². Using replica theory combined with spherical/Harish-Chandra–Itzykson–Zuber integrals, the authors derive tractable predictions for Bayes-optimal generalisation error and reveal rich phase transitions as data increases. The analysis shows that networks exhibit a universal learning phase at low sample rates where they learn non-linear combinations of teacher weights, followed by specialisation transitions where individual neurons align with the teacher. The work bridges theoretical physics methods with practical neural network learning phenomena.

## Method Summary
The authors employ replica symmetric (RS) theory from statistical physics to analyze Bayes-optimal learning in two-layer networks. The core approach involves deriving a replica free entropy using spherical/Harish-Chandra–Itzykson–Zuber integrals to handle the extensive-rank matrix structure. The theory requires decomposing activation functions into Hermite polynomials and solving saddle point equations for order parameters (weight overlaps and second-order statistics). Numerical validation uses Hamiltonian Monte Carlo and ADAM optimization, with code provided for reproducing theoretical curves. The framework is extended to handle generic activation functions beyond the quadratic case previously studied.

## Key Results
- Derives replica symmetric free entropy potential that predicts Bayes-optimal generalisation error as function of sample rate α and aspect ratio γ
- Identifies two distinct learning phases: universal phase (low α) where network learns tensor combinations, and specialisation phase (high α) where individual neurons align with teacher
- Shows specialisation occurs hierarchically, with neurons having larger readout weights learned first
- Demonstrates statistical-to-computational gap where optimal solution is hard to find with polynomial-time algorithms
- Extends GAMP-RIE algorithm to arbitrary activation functions, matching universal branch performance

## Why This Works (Mechanism)

### Mechanism 1: The Universal Manifold Learning Phase
When data is scarce (α < α_sp), the network converges to a "universal" solution where it learns the interaction matrix (non-linear combinations) of the teacher's weights rather than individual hidden neurons. In the regime where n = Θ(d²) but the sample rate α is low, the student cannot break the symmetry of the teacher's hidden neurons. Instead, it learns the second-order tensor W_0^T diag(v) W_0 effectively. This results in a generalisation error that is independent of the specific prior distribution of the inner weights (e.g., Gaussian vs. binary), provided they are centered with unit variance. This mechanism collapses when the sample rate α exceeds a critical threshold α_sp, triggering the specialization phase.

### Mechanism 2: Hierarchical Specialization via Readout Strength
Specialization (aligning student weights to teacher weights) occurs as a sequence of phase transitions ordered by the magnitude of the readout weights |v_j|. As data increases (α > α_sp), the network gains enough statistical power to distinguish individual teacher neurons. Neurons associated with larger readout amplitudes (stronger signal-to-noise ratio) are "synchronized" and specialized first. If the activation function is purely quadratic (σ(x)=x²) and weights are Gaussian, rotational invariance prevents specialization entirely.

### Mechanism 3: Statistical-to-Computational Gap
While the specialization phase is information-theoretically optimal, it is algorithmically "hard" to reach; efficient algorithms tend to get trapped in the sub-optimal universal branch. The energy landscape appears to contain a "glassy" phase associated with the universal solution. Polynomial-time algorithms like GAMP-RIE (Generalized Approximate Message Passing with Rotational Invariant Estimator) follow the universal branch even past the transition point α_sp, failing to find the specialized minimum without informative initialization.

## Foundational Learning

- **Concept: Extensive-Width Scaling (n = Θ(d²))**
  - Why needed here: Unlike the infinite-width limit (k → ∞) where networks behave as Gaussian Processes (NNGP) and lack feature learning, the extensive-width regime (k ∝ d) with quadratic sample scaling is the minimal setting where non-trivial feature learning and specialization emerge.
  - Quick check question: If we scaled n = Θ(d) instead of Θ(d²), would the model learn the order-2 tensor (Mechanism 1)? (Answer: No, it would only learn the linear term μ₁).

- **Concept: Hermite Expansion of Activations**
  - Why needed here: The theory relies on decomposing the activation function σ into orthogonal Hermite polynomials. The mechanism distinguishes between learning low-order statistics (μ₀, μ₁) versus higher-order interactions that drive specialization.
  - Quick check question: Why does a purely quadratic activation (μ₂ ≠ 0, μℓ>2=0) prevent specialization for Gaussian weights? (Answer: It leaves a large rotational invariance group intact).

- **Concept: Replica Symmetric (RS) Ansatz**
  - Why needed here: This is the mathematical engine used to compute the free entropy. It assumes that the overlap order parameters (measures of alignment) are "concentrated" or self-averaging in the high-dimensional limit.
  - Quick check question: What physical symmetry justifies the Replica Symmetric ansatz in this context? (Answer: The Nishimori identity, which holds due to the Bayes-optimal setting).

## Architecture Onboarding

- **Component map:** Inputs x ∈ ℝᵈ (Standard Gaussian) → Two-layer Fully Connected Networks with weights W ∈ ℝᵏˣᵈ and readout v ∈ ℝᵏ → Activation σ (Generic, analyzed via Hermite coefficients) → Theoretical Core: "Replica Free Entropy" potential (Eq. 6) that predicts the phase state based on order parameters (q₂, Q_W)

- **Critical path:**
  1. Define Scaling: Set aspect ratios γ = k/d and sample rate α = n/d²
  2. Solve Saddle Point: Numerically extremize the RS potential (Result 2.1) to find overlaps Q_W*(v) and q₂*
  3. Determine Phase: Check if Q_W*(v) = 0 (Universal) or Q_W*(v) > 0 (Specialized)
  4. Compute Error: Plug overlaps into the generalization error formula (Result 2.2)

- **Design tradeoffs:**
  - Analysis vs. Exactness: The theory is a "tractable approximation" (Result 2.1). It sacrifices rigorous exactness (acknowledged in text) for the ability to handle generic activations beyond the quadratic case.
  - Universal vs. Specialized: The "Universal" solution is algorithmically accessible (via GAMP-RIE) but statistically suboptimal. The "Specialized" solution is statistically optimal but computationally hard to locate.

- **Failure signatures:**
  - Glassy Stagnation: Training error drops to the "Universal" level but fails to reach the "Specialized" minimum despite high α.
  - Algorithmic Mismatch: GAMP-RIE tracks the universal theory curve perfectly but deviates from the Bayes-optimal (Specialized) MCMC results.

- **First 3 experiments:**
  1. Phase Diagram Verification: Run the provided code to plot generalization error ε_opt vs. sample rate α for binary vs. Gaussian inner weights. Verify that the curves coincide for low α (universal phase) and diverge at high α.
  2. Hardness Validation: Train a network using ADAM/HMC with random initialization for increasing dimensions d. Plot the time to reach specialization. Fit the curve to check if it is exponential (as suggested by Appendix I).
  3. Activation Sensitivity: Compare ReLU (μ₂ ≠ 0) vs. Tanh (μ₂ = 0) on a 4-point readout distribution. Confirm that Tanh skips the order-2 learning step, resulting in a flat error curve until the specialization jump (Page 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the specialisation phase fundamentally hard to reach for polynomial-time algorithms, and does GAMP-RIE achieve the best prediction performance achievable by a polynomial-time learner when n = Θ(d²)?
- Basis in paper: [explicit] "It would thus be interesting to settle whether GAMP-RIE has the best prediction performance achievable by a polynomial-time learner when n = Θ(d²) for such targets." The authors also note "a more systematic analysis on the computational hardness of the problem is an important step."
- Why unresolved: While empirical evidence suggests exponential training time for some cases (ADAM, HMC with homogeneous/Rademacher readouts), results for Gaussian readouts remain inconclusive; no rigorous hardness proof exists.
- What evidence would resolve it: Rigorous computational lower bounds showing no polynomial-time algorithm can beat universal branch performance before αsp, or conversely, a polynomial-time algorithm that reliably finds specialisation.

### Open Question 2
- Question: Can the theoretical framework be extended to deeper architectures while retaining tractability, and does the phase transition phenomenology persist with more layers?
- Basis in paper: [explicit] "The extension to deeper architectures is also possible, in the vein of (Cui et al., 2023; Pacelli et al., 2023) who analysed the over-parametrised proportional regime."
- Why unresolved: The current derivation relies on a specific combination of replica method and spherical integration tailored to two-layer networks; deeper networks introduce additional order parameters and coupling structures that may not admit closed-form treatment.
- What evidence would resolve it: A principled extension of the replica formalism to three or more layers with derived saddle-point equations that match numerical experiments.

### Open Question 3
- Question: Is the derived replica-symmetric free entropy asymptotically exact, or do finite-rank corrections or replica symmetry breaking effects become relevant?
- Basis in paper: [explicit] "We call these 'results' because, despite their excellent match with numerics, we do not expect these formulas to be exact: their derivation is based on an unconventional mix of spin glass techniques and spherical integrals, and require approximations in order to deal with the fact that the degrees of freedom to integrate are large matrices of extensive rank."
- Why unresolved: Standard rigorous proof techniques for replica formulas (adaptive interpolation, cavity method) do not directly apply to the extensive-rank matrix structure; the HCIZ integral suggests effective one-body problems are insufficient.
- What evidence would resolve it: Rigorous mathematical proof of replica formula validity, or identification of systematic deviations at larger system sizes beyond current finite-size simulations.

### Open Question 4
- Question: What is the precise connection between the specialisation phase transition identified here and the "Grokking" phenomenon observed in practical neural network training?
- Basis in paper: [explicit] "It would be interesting to connect the picture we have drawn here with Grokking, a sudden drop in generalisation error occurring during the training of neural nets close to interpolation."
- Why unresolved: Grokking is typically studied in the context of algorithmic datasets and gradient descent dynamics, while this work analyzes Bayes-optimal equilibrium properties; bridging these requires understanding both dynamical and statistical aspects.
- What evidence would resolve it: Demonstrating that the specialisation transition αsp corresponds to a regime where gradient descent dynamics exhibit delayed generalisation, or showing that the matrix denoising sub-problem explains grokking timescales.

## Limitations

- Replica Theory Approximations: The theoretical predictions rely on the replica symmetric (RS) ansatz and a saddle-point approximation, which the authors acknowledge is only an approximation to the rigorous result.
- Algorithmic Barriers: The statistical-to-computational gap is based on empirical observations rather than rigorous proof, with exponential scaling of training time remaining a conjecture.
- Activation Function Assumptions: The theory requires activation functions to be decomposed into Hermite polynomials, with certain activations (purely quadratic with Gaussian weights) being explicit exceptions.

## Confidence

- **High Confidence**: The existence of a universal phase at low sample rates where the model learns tensor combinations rather than individual neurons. This is directly observed in both theory and numerical experiments (Figures 1-3), and the mechanism is well-grounded in the extensive-width scaling (n=Θ(d²)).
- **Medium Confidence**: The hierarchical specialization sequence where neurons with larger readout weights are learned first. While supported by Figure 3 and theoretical derivation, this requires the specific structure of the readout distribution and may not generalize to all activation functions equally.
- **Medium Confidence**: The statistical-to-computational gap characterization. The empirical evidence from GAMP-RIE vs HMC is compelling, but the theoretical understanding of why this gap exists and its precise boundaries remains incomplete.

## Next Checks

1. **Rigorous Verification of Replica Stability**: For each reported phase diagram (Figures 1-3), compute the replicon eigenvalue to verify the RS ansatz is stable in the predicted phase regions, particularly near transition boundaries.

2. **Algorithmic Phase Diagram**: Systematically compare multiple optimization algorithms (SGD, ADAM, GAMP-RIE, HMC) across a grid of hyperparameters (α, γ, activation functions) to map which algorithms find which phases and characterize the computational hardness more precisely.

3. **Cross-Activation Generalization**: Test the theoretical predictions with activation functions that have significant Hermite coefficients beyond μ₂ (e.g., erf, cos) to verify the framework's claimed generality and understand how higher-order Hermite terms affect the specialization dynamics.