---
ver: rpa2
title: 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for
  Feature Implementation'
arxiv_id: '2503.06680'
source_url: https://arxiv.org/abs/2503.06680
tags:
- code
- other
- task
- development
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FEA-Bench is a new benchmark for evaluating repository-level code
  generation on feature implementation. It collects pull requests from 83 GitHub repositories,
  filtering for tasks that add new components and are paired with unit tests.
---

# FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation

## Quick Facts
- arXiv ID: 2503.06680
- Source URL: https://arxiv.org/abs/2503.06680
- Authors: Wei Li; Xin Zhang; Zhongxin Guo; Shaoguang Mao; Wen Luo; Guangyue Peng; Yangyu Huang; Houfeng Wang; Scarlett Li
- Reference count: 24
- Key outcome: New benchmark for repository-level code generation on feature implementation with 1,401 task instances; best model (DeepSeek-R1) solves only 9.92% of tasks

## Executive Summary
FEA-Bench is a new benchmark designed to evaluate large language models on repository-level code generation for feature implementation. Unlike existing benchmarks focused on bug fixes or standalone function synthesis, FEA-Bench targets the implementation of new features requiring both code completion (for new components) and code editing abilities (for integrating those components). The benchmark collects pull requests from 83 GitHub repositories, filtering for tasks that add new components and are paired with unit tests. Experiments show current models perform poorly, with the best model (DeepSeek-R1) solving only 9.92% of tasks, highlighting the difficulty of this problem.

## Method Summary
The benchmark constructs task instances by extracting pull requests from GitHub repositories, filtering for those that add new components and are classified as feature implementations using intent-based classification. Each task provides the PR description, related issues, signatures/docstrings of new components, and unit tests. Models generate code changes in natural format (before/after code snippets) or patch format (unified diffs), which are then applied to the repository and evaluated based on whether all unit tests pass. The evaluation uses Docker-based test execution adapted from SWE-bench, with context settings varying between Oracle (all gold patch files) and BM25 retrieval (27K tokens).

## Key Results
- Best-performing model (DeepSeek-R1) solves only 9.92% of tasks in the lite version
- Models struggle particularly with tasks involving multiple new functions, with performance dropping from 18.96% (1 function) to 5.47% (≥3 functions)
- Natural format output achieves significantly higher success rates (44-73% apply rate) compared to patch format (13-33%)
- Performance varies across repository categories, with database libraries showing higher success rates (22.65%) than cloud platforms (5.05%)

## Why This Works (Mechanism)

### Mechanism 1
Filtering pull requests by new component presence and intent classification yields feature implementation tasks distinct from bug-fix benchmarks. The pipeline extracts PRs with test file changes, parses Python files to identify newly added functions/classes via AST comparison, applies a 25% threshold for new component lines, then uses GPT-4o to classify intent as "new feature." Final verification runs pytest before and after applying the gold patch. Break condition: If PRs bundle unrelated refactoring with feature additions, task instances may conflate multiple objectives.

### Mechanism 2
Providing component signatures as partial guidance tests models' ability to integrate new code with existing repository structure. Each task supplies the PR description, related issues, and signatures/docstrings of new components—but not their implementations. Models must generate both new component bodies and coordinated edits to dependent code elsewhere. Break condition: If signatures leak implementation details (e.g., via overly specific docstrings), task difficulty decreases artificially.

### Mechanism 3
Natural-format edit output achieves higher success rates than patch-format due to formatting flexibility. Models generate before/after code snippets rather than strict diff syntax. Post-processing converts these to applicable patches. The looser format reduces syntactic errors that cause `git apply` failures. Break condition: If natural format outputs are ambiguous (multiple matching locations), conversion may fail silently.

## Foundational Learning

- Concept: Repository-level code generation vs. standalone synthesis
  - Why needed here: FEA-Bench explicitly targets multi-file, cross-component edits, unlike HumanEval/MBPP that evaluate isolated functions.
  - Quick check question: Can you explain why a model that scores 90% on HumanEval might score <10% on FEA-Bench?

- Concept: Pull request as task specification
  - Why needed here: Tasks are derived from real PRs, not synthetic descriptions, requiring models to parse natural language requirements from multiple sources (PR body, issue discussions, doc changes).
  - Quick check question: What information sources does FEA-Bench combine to specify a feature request?

- Concept: Execution-based evaluation
  - Why needed here: Correctness is determined by pytest outcomes, not string matching or model-based grading.
  - Quick check question: Why does FEA-Bench allow ImportError/AttributeError before applying the gold patch but SWE-bench doesn't?

## Architecture Onboarding

- Component map: Repository selection -> PR crawling -> filtering (rule + intent) -> test verification -> task instance construction
- Critical path: 1) Set up repository at base commit 2) Apply test patch 3) Run model inference with configured context 4) Convert model output to patch 5) Apply generated patch 6) Run pytest; task resolved iff all tests pass
- Design tradeoffs: Oracle (files in gold patch) vs. BM25 retrieval: Oracle provides ground-truth context but leaks information; BM25 is realistic but may miss relevant files (Table 3: 76% recall at 27K tokens). Detailed vs. Brief hints: Detailed includes non-Python file changes; Brief provides only signatures—performance varies by subset. Single-round generation: Cost-efficient but may underrepresent multi-turn agent capabilities.
- Failure signatures: Low apply rate (%Apply column): Indicates format adherence issues. High discrepancy between Oracle and BM25: Suggests retrieval bottleneck. Performance drop with more added functions (Figure 5): Signals complexity scaling failure.
- First 3 experiments: 1) Establish baseline on FEA-Bench lite using Qwen2.5-Coder-32B with Oracle/Detailed setting to validate pipeline. 2) Compare Natural vs. Patch output formats on a 50-instance subset to measure format sensitivity. 3) Test retrieval precision impact: manually curate "gold" file sets for 20 instances and compare against BM25 retrieval to isolate context quality effects.

## Open Questions the Paper Calls Out

### Open Question 1
How can LLMs more reliably generate code edits in formats that successfully apply to repositories? The paper shows low patch application success rates (12.92%-73.16%) and states this remains an open question. Evidence would come from edit generation approaches achieving >90% apply rates with maintained or improved resolved ratios.

### Open Question 2
Can improved retrieval precision outperform simply increasing context length for feature implementation tasks? The paper demonstrates that increasing context from 27K to 40K tokens decreased model performance despite improved recall, suggesting precision matters more than quantity. Resolution requires experiments with precision-focused retrieval methods compared against larger context windows.

### Open Question 3
Why do models struggle disproportionately with tasks requiring multiple new functions, and can this be mitigated? The paper documents a drop from 18.96% (1 function) to 5.47% (≥3 functions) but does not investigate whether the difficulty stems from planning, context management, or code coordination challenges. Resolution requires ablation studies isolating planning complexity vs. generation length, or experiments with decomposition-based approaches.

## Limitations

- The benchmark focuses exclusively on Python repositories, limiting generalizability to other programming languages and broader software engineering domains.
- Evaluation relies heavily on GPT-4o for intent classification and repository categorization, with validation beyond stated 89.6% accuracy not provided.
- The single-round generation approach may underestimate the capabilities of multi-turn agent-based systems that could better handle complex feature implementations.

## Confidence

**High Confidence**: The benchmark construction methodology (PR filtering pipeline, execution-based evaluation) is well-specified and reproducible. The performance gap between models on FEA-Bench versus other benchmarks (SWE-Bench, HumanEval) is clearly demonstrated.

**Medium Confidence**: The claim that FEA-Bench measures distinct capabilities from existing benchmarks is supported by performance differentials but would benefit from more systematic comparison of task characteristics. The effectiveness of natural format output versus patch format is demonstrated but the underlying reasons for the performance difference could be explored further.

**Low Confidence**: The generalizability of findings across different programming languages and software engineering domains is uncertain given the Python-only focus. The impact of intent classification errors on task difficulty and model evaluation fairness requires further investigation.

## Next Checks

1. **Classification Validation**: Manually verify a stratified sample of 50 task instances across different repository categories to assess the accuracy of GPT-4o intent classification and its impact on benchmark difficulty.

2. **Format Conversion Robustness**: Analyze the conversion process from natural format outputs to patches by examining cases where application fails, identifying whether failures stem from model outputs or the conversion algorithm.

3. **Cross-Language Generalization**: Construct a small-scale version of FEA-Bench for another language (e.g., Java or JavaScript) using the same methodology to test whether the observed performance patterns hold across programming ecosystems.