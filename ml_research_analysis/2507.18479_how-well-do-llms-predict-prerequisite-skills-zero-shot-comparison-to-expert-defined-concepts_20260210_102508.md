---
ver: rpa2
title: How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined
  Concepts
arxiv_id: '2507.18479'
source_url: https://arxiv.org/abs/2507.18479
tags:
- prerequisite
- llms
- skill
- zero-shot
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ESCO-PrereqSkill, a new benchmark dataset
  of 3,196 skills with expert-defined prerequisite links derived from the ESCO taxonomy.
  It investigates whether large language models (LLMs) can predict prerequisite skills
  in a zero-shot setting, using only skill descriptions and without fine-tuning.
---

# How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined Concepts

## Quick Facts
- arXiv ID: 2507.18479
- Source URL: https://arxiv.org/abs/2507.18479
- Authors: Ngoc Luyen Le; Marie-Hélène Abel
- Reference count: 30
- Primary result: 13 state-of-the-art LLMs achieved F1BERT scores above 0.82 on zero-shot prerequisite skill prediction using only skill descriptions

## Executive Summary
This paper introduces ESCO-PrereqSkill, a benchmark dataset of 3,196 skills with expert-defined prerequisite links from the ESCO taxonomy. The study evaluates whether large language models can predict prerequisite skills in a zero-shot setting using only skill descriptions, without fine-tuning. Thirteen state-of-the-art models were tested using standardized prompts and assessed through semantic similarity, BERTScore, and inference latency. Results show that top models like LLaMA4-Maverick, Claude-3-7-Sonnet, and Qwen2-72B achieved strong alignment with expert-defined prerequisites, demonstrating that LLMs can effectively infer prerequisite relationships from natural language alone. These findings support scalable, AI-driven educational tools for curriculum planning and personalized learning.

## Method Summary
The study evaluates 13 LLMs on zero-shot prerequisite skill prediction using the ESCO-PrereqSkill benchmark (3,196 skills with expert-defined prerequisites). Models receive standardized prompts containing skill name and description, responding with comma-separated prerequisite lists. Predictions are compared against ground truth using Sentence-BERT embeddings for semantic similarity and BERTScore for token-level evaluation. Inference latency is also measured. The experimental design tests whether LLMs can infer prerequisite relationships without task-specific fine-tuning, relying solely on implicit knowledge gained during pretraining.

## Key Results
- Top-performing models (LLaMA4-Maverick, Claude-3-7-Sonnet, Qwen2-72B) achieved F1BERT scores above 0.82
- Semantic similarity scores ranged from 0.60-0.71, indicating robust alignment beyond lexical matching
- Instruction-following capability was crucial, with models successfully producing parseable comma-separated outputs
- Gemini-2.0-Flash showed high F1 but lower semantic similarity (0.6049), suggesting surface-level lexical matching

## Why This Works (Mechanism)

### Mechanism 1: Implicit Hierarchical Knowledge Encoding
During pretraining on diverse web-scale data including instructional materials and educational discourse, models may implicitly learn hierarchical relationships among concepts. This enables inference of prerequisite dependencies without explicit supervision. However, this mechanism may fail when skills originate from domains poorly represented in pretraining corpora.

### Mechanism 2: Semantic Alignment via Contextual Embeddings
BERTScore and Sentence-BERT embeddings map predicted and reference skill sets into shared semantic space, capturing conceptual overlap even when surface forms differ. This allows models to match predicted prerequisites to expert-defined ground truth through semantic similarity rather than lexical exactness. The mechanism may yield false positives when prerequisite relationships depend on domain-specific jargon where semantic proximity doesn't imply pedagogical dependency.

### Mechanism 3: Instruction-Following for Structured Output Generation
Standardized prompting constrains model outputs to comparable, parseable prerequisite lists across diverse model families. A unified prompt template elicits comma-separated lists without explanatory text, enabling consistent post-processing and evaluation. This mechanism may fail with smaller or less instruction-tuned models that don't adhere to formatting constraints.

## Foundational Learning

- **Concept: Zero-Shot Learning**
  - Why needed here: The entire experimental design hinges on evaluating models without task-specific fine-tuning
  - Quick check question: Can you explain why zero-shot evaluation differs from few-shot or fine-tuned approaches, and what inferences it permits about model capabilities?

- **Concept: BERTScore and Semantic Similarity Metrics**
  - Why needed here: Performance claims rest on F1_BERT and Sim_sem scores
  - Quick check question: Given two skill lists ["Linear Algebra", "Calculus"] and ["Math foundations", "Differential calculus"], would you expect high BERTScore? Why or why not?

- **Concept: Prerequisite Relationship Modeling**
  - Why needed here: The target task assumes prerequisite relations exist and are objective
  - Quick check question: What distinguishes a prerequisite relationship from a related-but-not-required relationship in a skill taxonomy?

## Architecture Onboarding

- **Component map:**
  Input layer (skill name + description) -> Prompt constructor (four-part template) -> LLM inference engine (13 models) -> Output parser (normalization pipeline) -> Evaluation module (Sentence-BERT + BERTScore) -> Ground truth store (ESCO-PrereqSkill)

- **Critical path:**
  1. Skill selection from ESCO (must have description + known prerequisites)
  2. Prompt construction and dispatch to target LLM
  3. Response parsing into candidate prerequisite list
  4. Semantic comparison against ground truth via BERTScore/Sim_sem
  5. Latency logging for efficiency metrics

- **Design tradeoffs:**
  - Open vs. commercial models: API availability vs. reproducibility
  - Lexical vs. semantic evaluation: Exact match is stricter but penalizes valid paraphrases
  - Prompt specificity: More constrained prompts yield cleaner outputs but may suppress creative synonyms

- **Failure signatures:**
  - Hallucinated prerequisites: Model generates plausible-sounding skills absent from any taxonomy
  - Format violation: Model includes explanatory text; parsing fails or introduces noise
  - High lexical match, low semantic depth: Model surface-matches terminology without grasping pedagogical dependency

- **First 3 experiments:**
  1. Baseline replication: Run prompt template on 3-5 models over 100-skill subset; verify F1_BERT ranges match reported ~0.81-0.83
  2. Prompt sensitivity test: Vary prompt phrasing on same skill subset; quantify variance in predictions
  3. Domain shift probe: Evaluate on skills from underrepresented ESCO categories to test break conditions for implicit hierarchical knowledge encoding

## Open Questions the Paper Calls Out

- **Can few-shot prompting with examples from ESCO-PrereqSkill improve prerequisite prediction accuracy compared to the zero-shot baseline?**
  - Basis: The conclusion states this as a next step
  - Why unresolved: Only zero-shot performance was evaluated
  - What evidence would resolve it: Run same evaluation protocol with 1-shot, 3-shot, and 5-shot prompting using sampled ESCO-PrereqSkill examples

- **How robust are LLM prerequisite predictions to prompt variation and domain shifts beyond the ESCO taxonomy?**
  - Basis: Listed as a next step in the conclusion
  - Why unresolved: A single standardized prompt was used across all models
  - What evidence would resolve it: Test multiple prompt formulations and evaluate on additional taxonomies

- **Can LLMs identify valid prerequisite relationships absent from expert-curated taxonomies that experts would endorse upon review?**
  - Basis: Inferred from evaluation against existing expert ground truth only
  - Why unresolved: Current metrics penalize predictions not in ground truth regardless of pedagogical validity
  - What evidence would resolve it: Human expert review of LLM-only predictions

## Limitations

- Implicit knowledge encoding reliability depends on pretraining corpora containing sufficient educational structure
- Semantic embedding adequacy for prerequisite equivalence remains partially validated
- Format compliance across all model families wasn't systematically measured

## Confidence

- **High Confidence**: Semantic Alignment via Contextual Embeddings, Instruction-Following for Structured Output
- **Medium Confidence**: Implicit Hierarchical Knowledge Encoding
- **Low Confidence**: Scalability to emerging domains, Semantic embeddings capturing true prerequisite equivalence

## Next Checks

1. **Ablation Study on Pretraining Data**: Remove skills from domains poorly represented in common pretraining corpora and re-evaluate top models to test whether implicit hierarchical knowledge encoding breaks down in data-sparse domains.

2. **Human Validation of Semantic Matches**: Have domain experts rate whether predicted prerequisites are pedagogically valid for a random sample where semantic similarity is high but lexical overlap is low.

3. **Cross-Dataset Generalization**: Evaluate the same models on prerequisite prediction tasks from different educational taxonomies (e.g., Bloom's taxonomy, medical competency frameworks) to test whether models have learned general prerequisite reasoning or memorized ESCO-specific patterns.