---
ver: rpa2
title: 'Seeing isn''t Hearing: Benchmarking Vision Language Models at Interpreting
  Spectrograms'
arxiv_id: '2511.13225'
source_url: https://arxiv.org/abs/2511.13225
tags:
- linguistics
- computational
- language
- association
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks the ability of vision-language models to interpret
  spectrograms and waveforms of speech. The authors created a novel dataset of over
  4,000 English words spoken in isolation, paired with spectrogram and waveform visualizations.
---

# Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms

## Quick Facts
- **arXiv ID**: 2511.13225
- **Source URL**: https://arxiv.org/abs/2511.13225
- **Reference count**: 11
- **Primary result**: VLMs perform at chance level (25%) on spectrogram interpretation tasks, demonstrating fundamental limitations in visual-speech mapping

## Executive Summary
This paper benchmarks the ability of vision-language models to interpret spectrograms and waveforms of speech, creating a novel dataset of over 4,000 English words with paired visualizations. The study reveals that VLMs perform at chance level on a multiple-choice task requiring them to select correct phonemic or graphemic transcriptions from distractors. This failure persists even after finetuning and suggests VLMs lack the specific parametric knowledge needed to interpret speech visualizations, highlighting a fundamental gap in multimodal reasoning capabilities.

## Method Summary
The researchers created a dataset of 4,068 English words spoken in isolation, paired with spectrogram and waveform visualizations generated using Librosa with specific parameters (n_fft=128, hop_length=22, 70dB dynamic range). Models were tested on a multiple-choice task where they must select the correct transcription from four options, including distractors selected based on phonemic edit distance. Both zero-shot and finetuned models (Qwen2.5-VL, LLaVA 1.6, InternVL 3) were evaluated across four conditions (graphemic/phonemic × spectrogram/spectrogram+waveform). Finetuning used 5 epochs with specific hyperparameters on a single A100 GPU.

## Key Results
- VLMs achieved approximately 25% accuracy (chance level) on spectrogram interpretation tasks across all tested conditions
- No significant performance improvement was observed from including waveforms or from finetuning
- Human baseline achieved 75% accuracy while ASR systems reached 87.56%, demonstrating the task is solvable with appropriate processing
- Average phonemic edit distance of model selections (17.2) remained far from ground truth (0), indicating random rather than informed errors

## Why This Works (Mechanism)

### Mechanism 1: Parametric Knowledge Gap in Visual-Speech Mapping
VLMs fail at spectrogram interpretation because they lack domain-specific acoustic phonetic knowledge rather than insufficient visual information. The task requires mapping visual patterns to phonemic categories using specialized knowledge about articulatory phonetics not encoded in standard VLM pre-training.

### Mechanism 2: Finetuning Captures Statistical Bias, Not Visual Understanding
Performance gains from finetuning reflect learning of residual dataset biases rather than genuine spectrogram comprehension. Distractor selection based on phonemic edit distance creates statistical regularities that models can exploit without processing visual input meaningfully.

### Mechanism 3: Modality Transfer Failure from Natural Images to Scientific Visualizations
Standard VLM pre-training on natural images does not transfer to specialized scientific visualizations requiring expert interpretation schemas. The gap between ASR (87.56%) and VLM (25%) performance on identical speech content demonstrates modality-specific processing requirements.

## Foundational Learning

- **Acoustic Phonetics & Spectrogram Structure**
  - Why needed here: Spectrograms encode time, frequency, and energy density; vowels show characteristic formant patterns while consonants show distinct signatures
  - Quick check question: Given a spectrogram, can you identify where a vowel /a/ vs. /i/ would differ visually?

- **Phonemic vs. Graphemic Representation**
  - Why needed here: The task tests both IPA phonemic transcription and standard written words; phonemic options may better activate relevant acoustic knowledge
  - Quick check question: Why might "ʃip" (phonemic) be easier to match to a spectrogram than "ship" (graphemic)?

- **Phonemic Edit Distance (PED)**
  - Why needed here: Distractors are selected by PED from ground truth, creating a gradient of difficulty; understanding this metric helps interpret whether models make "informed errors"
  - Quick check question: If a model consistently selects options with low PED but incorrect answers, what does this suggest about its partial understanding?

## Architecture Onboarding

- **Component map**: Vision encoder -> Vision-language projector -> Language backbone -> MCQ head
- **Critical path**: Spectrogram generation (Librosa) -> Image tokenization -> Prompt construction -> Model scoring -> Accuracy/PED metrics
- **Design tradeoffs**: Synthetic vs. natural speech (controlled but less ecologically valid); single words vs. connected speech (avoids coarticulation but limits real-world applicability); 4-way MCQ vs. open generation (cleaner evaluation but may underestimate capabilities)
- **Failure signatures**: Accuracy clustering at 25% across all conditions; no significant difference with/without waveform; finetuned models perform similarly with/without image inputs; average PED slightly below random but far from ground truth
- **First 3 experiments**:
  1. Baseline replication: Test Qwen2.5-VL-7B on 100 held-out examples with spectrogram-only input, graphemic options
  2. In-context knowledge injection: Provide explicit phonetic cues in prompt about formant patterns
  3. Ablate visual input: Run finetuned model on test set with blank/masked images to detect bias exploitation

## Open Questions the Paper Calls Out

- **Can training VLMs to recognize individual phoneme characteristics from spectrograms improve performance on whole-word interpretation?**
  - Basis: Authors suggest future work may be best served by training models to recognize individual phoneme characteristics first
  - Evidence needed: Compare models trained on phoneme-level mapping before word-level training

- **Would providing explicit phonetic knowledge within the prompt improve VLM performance?**
  - Basis: Authors did not include explicit knowledge within prompts, leaving this for future work
  - Evidence needed: Ablation study comparing zero-shot performance with and without phonetic reasoning instructions

- **How would VLMs perform on spectrograms of natural human speech versus synthetic speech?**
  - Basis: Authors acknowledge synthetic speech is less variable than natural human speech
  - Evidence needed: Create parallel test set using human recordings and compare VLM accuracy

## Limitations
- The study cannot definitively distinguish whether VLM failures reflect fundamental limitations or simply the need for domain-specific pre-training data
- Finetuning experiments may not have reached optimal hyperparameters or sufficient training duration to demonstrate true learning capability
- The synthetic speech dataset may not capture the variability present in natural speech that could provide additional visual cues
- Distractor selection strategy may inadvertently create text-based patterns that models exploit without visual reasoning

## Confidence
- **High Confidence**: VLMs perform at chance level (~25%) on spectrogram interpretation tasks
- **Medium Confidence**: The failure reflects parametric knowledge gaps rather than visual information insufficiency
- **Medium Confidence**: Finetuning captures dataset bias rather than genuine visual understanding
- **Low Confidence**: VLMs cannot learn spectrogram interpretation through standard multimodal pre-training approaches

## Next Checks
1. **Expert Phoneticist Evaluation**: Have trained phoneticians transcribe the same spectrogram dataset to validate whether human performance varies systematically with difficulty metrics
2. **Incremental Visual Cue Ablation**: Systematically mask or simplify spectrogram features to identify which visual elements are most critical for human interpretation versus what VLMs can process
3. **Cross-Domain Transfer Learning**: Train VLMs on a broader range of scientific visualizations before fine-tuning on speech spectrograms to test whether limitations reflect general challenges with scientific visualization interpretation