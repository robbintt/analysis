---
ver: rpa2
title: Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians
arxiv_id: '2510.06263'
source_url: https://arxiv.org/abs/2510.06263
tags:
- summarization
- summary
- summaries
- patient
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a dual-stage, edge-based system for summarizing
  electronic health records (EHRs) in emergency medicine, addressing privacy concerns
  and the need for offline operation. The system splits tasks between two NVIDIA Jetson
  Orin Nano devices: one retrieves relevant EHR sections via semantic search, and
  the other generates a two-part summary (critical findings and context-specific narrative)
  using a small language model (SLM).'
---

# Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians

## Quick Facts
- **arXiv ID:** 2510.06263
- **Source URL:** https://arxiv.org/abs/2510.06263
- **Reference count:** 37
- **Primary result:** Dual-stage edge-based EHR summarization system achieves accurate, timely summaries using two Jetson Orin Nano devices and small language models.

## Executive Summary
This paper introduces a dual-stage, edge-based system for summarizing electronic health records (EHRs) in emergency medicine, addressing privacy concerns and the need for offline operation. The system splits tasks between two NVIDIA Jetson Orin Nano devices: one retrieves relevant EHR sections via semantic search, and the other generates a two-part summary (critical findings and context-specific narrative) using a small language model (SLM). It benchmarks six open-source SLMs under 7B parameters and uses an LLM-as-Judge framework to evaluate factual accuracy, completeness, and clarity, given the lack of gold-standard summaries. Results on MIMIC-IV and real EHRs show the system can produce accurate, useful summaries in under 30 seconds. The dual-device setup improves stability and allows larger models to run reliably.

## Method Summary
The proposed system consists of two main stages running on separate edge devices. First, the retrieval stage (Nano-R) identifies relevant EHR sections using semantic search and term-based matching, optimizing for context (e.g., "chest pain") with a weighted combination of search scores. Second, the summarization stage (Nano-S) generates a two-part summary: a list of critical findings and a context-specific narrative, using small language models (SLMs) under 7B parameters. The authors evaluate six SLMs (including Mistral, OpenChat) and prompting strategies (Zero-Shot, Chain-of-Thought). To address the absence of gold-standard summaries, they adopt an LLM-as-Judge framework, using GPT-4o and a 20B-parameter open-source model to assess factual accuracy, completeness, and clarity. The system is validated on MIMIC-IV and real EHR datasets, with performance measured in latency and quality metrics.

## Key Results
- The dual-stage edge architecture reliably produces accurate, context-specific summaries in under 30 seconds on Jetson Orin Nano hardware.
- OpenChat with Zero-Shot prompting achieved the highest factual accuracy among tested SLMs.
- Splitting tasks across two devices enabled the use of larger SLMs while maintaining system stability and latency targets.

## Why This Works (Mechanism)
The system's effectiveness stems from task-specific specialization: the retrieval module (Nano-R) focuses solely on extracting relevant EHR sections, optimizing the input for the summarization module (Nano-S), which generates concise, clinically relevant summaries. The two-device setup allows each component to run the most appropriate model for its task without resource contention. By leveraging semantic search and term matching, the system improves retrieval precision for critical clinical information, while the SLM summarization produces actionable summaries tailored to the clinical context. The LLM-as-Judge evaluation framework provides a scalable, automated means of assessing summary quality in the absence of gold standards.

## Foundational Learning

**Semantic Search:** Using vector embeddings to find relevant text based on meaning, not just keywords. *Why needed:* Enables retrieval of clinically relevant EHR sections even when exact terms aren't used. *Quick check:* Can the system retrieve "myocardial infarction" sections when searching for "chest pain"?

**Chain-of-Thought Prompting:** Guiding language models through intermediate reasoning steps. *Why needed:* Improves model reasoning and output quality for complex summarization tasks. *Quick check:* Does CoT prompting improve summary coherence compared to Zero-Shot?

**LLM-as-Judge:** Using large language models to automatically evaluate outputs for accuracy, completeness, and clarity. *Why needed:* Provides scalable evaluation when human gold standards are unavailable. *Quick check:* Are automated FA scores correlated with clinician judgments?

## Architecture Onboarding

**Component Map:** EHR Input -> Nano-R (Semantic Search + Term Matching) -> Context-Specific Retrieval -> Nano-S (SLM Summarization) -> Summary Output

**Critical Path:** EHR data → Nano-R retrieval → filtered EHR sections → Nano-S summarization → final summary. The bottleneck is typically Nano-S processing time, but splitting devices reduces contention.

**Design Tradeoffs:** Dual-device setup trades hardware cost and complexity for improved stability and model capacity. Using small SLMs under 7B parameters balances performance and edge inference feasibility. LLM-as-Judge replaces human evaluation for scalability but may introduce bias.

**Failure Signatures:** Retrieval failures (Nano-R) lead to incomplete or irrelevant summaries; summarization failures (Nano-S) result in hallucinations or missing critical information. Hardware resource exhaustion on either device causes latency spikes or crashes.

**First Experiments:**
1. Test retrieval accuracy on MIMIC-IV with varying context weights.
2. Benchmark summarization latency and factual accuracy across six SLMs.
3. Validate LLM-as-Judge scores against blinded clinician assessments on a subset of summaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do the automated LLM-as-Judge evaluations (FA, completeness, clarity) correlate with expert human evaluations by emergency physicians?
- Basis in paper: [explicit] The authors state: "We would validate our FA metric by comparing model-assigned scores to clinician adjudications and having clinicians check the FA directly on the source EHRs."
- Why unresolved: The current study relies entirely on automated evaluation frameworks (GPT-4o and GPT-oss-20b) due to the lack of gold-standard summaries, leaving the clinical validity of the proposed "Factual Accuracy" metric unconfirmed.
- What evidence would resolve it: A statistical correlation analysis comparing the automated FA scores against blind ratings from multiple board-certified emergency physicians for the same generated summaries.

### Open Question 2
- Question: Can fine-tuning SLMs specifically for the dual-device edge environment improve summary quality beyond the performance of current prompting strategies?
- Basis in paper: [explicit] The Future Work section notes: "On our dual-device system, we will work on fine-tuning SLMs for edge inference in order to improve the quality."
- Why unresolved: The current implementation relies on off-the-shelf open-source models (e.g., Mistral, OpenChat) optimized via prompting techniques (Zero-Shot, CoT) rather than domain-specific training.
- What evidence would resolve it: A comparative experiment benchmarking the summarization accuracy and latency of a specialized fine-tuned SLM against the current top-performing baselines (e.g., OpenChat + Zero-Shot) on the Jetson hardware.

### Open Question 3
- Question: How robust is the context-specific summarization capability when scaling to diverse chief complaints beyond the "chest pain" scenario tested?
- Basis in paper: [inferred] While the system is designed for "context-specific" queries, the methodology (Section IV-B) notes that for the real EHR dataset, "we used the complaint 'chest pain' as the context."
- Why unresolved: It is unclear if the retrieval module (Nano-R) and specific prompting weights hold up effectively for other high-stakes ED presentations (e.g., abdominal pain, stroke symptoms) which may require different semantic retrieval thresholds.
- What evidence would resolve it: Evaluation results showing consistent FA scores and latency across a representative distribution of common emergency department chief complaints.

## Limitations
- The system was tested only on MIMIC-IV and a small real-world sample; broader EHR diversity and different emergency department workflows remain untested.
- The LLM-as-judge evaluation, while pragmatic, introduces subjectivity and lacks validation against human clinician judgments, raising concerns about the reliability of quality metrics.
- Privacy and security implications of processing sensitive health data on edge devices are not addressed, nor is the system's behavior under realistic EHR variability, noisy data, or hardware failure scenarios.

## Confidence
- **High confidence** in the technical feasibility and performance of the dual-stage edge architecture for controlled test data.
- **Medium confidence** in the usability and factual accuracy of generated summaries, due to reliance on automated evaluation without clinician validation.
- **Low confidence** in real-world robustness, privacy safeguards, and scalability beyond the tested settings.

## Next Checks
1. Conduct a clinician-driven evaluation study to assess summary usefulness, accuracy, and impact on clinical decision-making in real emergency department settings.
2. Expand testing to diverse EHR sources, including non-MIMIC datasets, to evaluate generalizability and performance under varied data quality and formatting.
3. Perform a formal privacy and security audit of the edge-based system, including risk assessment and mitigation strategies for handling protected health information.