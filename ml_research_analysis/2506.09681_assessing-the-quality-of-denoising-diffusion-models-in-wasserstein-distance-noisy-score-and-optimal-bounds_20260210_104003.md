---
ver: rpa2
title: 'Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance:
  Noisy Score and Optimal Bounds'
arxiv_id: '2506.09681'
source_url: https://arxiv.org/abs/2506.09681
tags:
- score
- distribution
- noise
- lemma
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds

## Quick Facts
- **arXiv ID:** 2506.09681
- **Source URL:** https://arxiv.org/abs/2506.09681
- **Authors:** Vahan Arsenyan; Elen Vardanyan; Arnak Dalalyan
- **Reference count:** 40
- **Key outcome:** Provides tighter convergence bounds for DDPMs in Wasserstein-2 distance, showing linear dependence on discretization step size and analyzing robustness to score noise.

## Executive Summary
This paper provides a rigorous theoretical analysis of Denoising Diffusion Probabilistic Models (DDPMs) with respect to the Wasserstein-2 distance, focusing on two key aspects: the impact of noisy score estimators and optimal discretization schedules. The authors prove that score estimation error degrades sample quality linearly with the maximum discretization step size, but this degradation vanishes as the number of sampling steps increases. They also introduce a two-regime discretization schedule that achieves optimal convergence rates for a broad class of target distributions. Empirical validation on standard datasets demonstrates the practical relevance of their theoretical findings.

## Method Summary
The paper analyzes the standard forward (data to noise) and reverse (noise to data) diffusion processes. The reverse process uses a discretized numerical scheme (Algorithm 1) to generate samples from noise, with the score function estimated by a neural network. The key methodological contributions are: (1) proving convergence bounds for the reverse process when the score estimator is corrupted by additive noise, and (2) designing an optimal two-regime discretization schedule (Algorithm 2) that achieves dimension-free convergence rates. The analysis assumes the target distribution satisfies semi-log-concavity and bounded support conditions (Assumption 1).

## Key Results
- Score noise with zero mean degrades sample quality proportionally to the maximum step size, but this effect vanishes as the number of steps increases.
- A two-regime discretization schedule (arithmetic early, geometric late) achieves optimal convergence rates for semi-log-concave distributions.
- Recursive analysis of squared Wasserstein distance yields tighter bounds with linear dependence on discretization step size, improving on prior polynomial dependencies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The impact of additive noise variance in the score estimator diminishes as the number of sampling steps increases.
- **Mechanism:** The error bound scales with $\sqrt{h_{\text{max}}} \cdot \epsilon^v_{\text{score}}$. Since the maximum step size $h_{\text{max}}$ is inversely proportional to the number of steps $K$, the noise contribution vanishes as $K$ increases, provided the noise has zero mean.
- **Core assumption:** The noise is centered (unbiased) and has bounded variance; the step size satisfies stability conditions.
- **Evidence anchors:**
  - [abstract] "degradation scales only linearly with the largest discretization step size, not with the number of steps."
  - [Remark 3, Page 9] "The corresponding term... is $O(\sqrt{D} h_{\text{max}}^{1/2} \epsilon^v_{\text{score}})$, which scales as... and thus vanishes as $K$... grows large."
  - [corpus] Paper 70540 discusses sublinear iterations, supporting the idea that step counts and convergence rates are flexible.
- **Break condition:** If the noise has a non-zero bias, the error scales with $\epsilon^b_{\text{score}}$ and does not vanish with more steps.

### Mechanism 2
- **Claim:** Convergence rates are optimized by using a two-regime discretization schedule (arithmetic early, geometric late).
- **Mechanism:** The process uses an arithmetic grid when the drift is strongly contracting (early times) and switches to a geometric grid near the data distribution (late times) to handle changing Lipschitz constants of the score.
- **Core assumption:** The target distribution satisfies specific log-concavity or bounded support properties.
- **Evidence anchors:**
  - [Section 4, Page 8] "This scheme operates in two regimes: an arithmetic grid in the first half and a geometric grid in the second half."
  - [Algorithm 2, Page 8] Defines the specific schedule construction.
  - [corpus] Paper 25654 discusses dimension-free convergence, suggesting schedule choice heavily influences dimension scaling.
- **Break condition:** Using a uniform grid degrades the rate to $O(D/\epsilon^2)$ or worse, failing to achieve the optimal $\sqrt{D}/\epsilon$.

### Mechanism 3
- **Claim:** The Wasserstein-2 distance is bounded linearly by the discretization step size, improving on prior polynomial dependencies.
- **Mechanism:** By applying a recursive approach directly to the *squared* Wasserstein distance (rather than triangle inequalities on the distance itself), the analysis avoids "loose" correlation terms between steps, yielding tighter bounds.
- **Core assumption:** The drift coefficient (related to the score) is smooth and satisfies specific convexity properties derived from Assumption 1.
- **Evidence anchors:**
  - [Section 5, Page 11] "Applying the recursive approach to the squared Wasserstein distance yields significantly tighter control."
  - [Abstract] "Achieve faster convergence rates than previously known results."
  - [corpus] Paper 107238 analyzes Wasserstein convergence, corroborating the focus on tighter distance bounds.
- **Break condition:** If the score Hessian is unbounded (violating smoothness), the linear dependence on step size may break.

## Foundational Learning

- **Concept: Wasserstein-2 Distance**
  - **Why needed here:** It is the primary metric for this paper. Unlike Total Variation (TV), $W_2$ controls moments (mean/variance).
  - **Quick check question:** Why does the paper argue TV distance is insufficient for measuring generative quality? (Answer: TV does not guarantee moment closeness; see Section 3.4).

- **Concept: Score Function (Tweedie's Formula)**
  - **Why needed here:** The paper analyzes "noisy score" estimators. Understanding that the score is $\nabla \log p_t(x)$ is essential for Algorithm 1.
  - **Quick check question:** In the context of the paper, what does the "score" estimate? (Answer: The gradient of the log-density of the perturbed data distribution).

- **Concept: Semi-log-concavity**
  - **Why needed here:** This is Assumption 1. It defines the class of distributions for which the optimal bounds hold.
  - **Quick check question:** Does the paper require the target distribution to be strictly log-concave? (Answer: No, it covers a broad class including semi-log-concave distributions with bounded support).

## Architecture Onboarding

- **Component map:** Forward Process (Ornstein-Uhlenbeck SDE) -> Reverse Process (Algorithm 1) -> Scheduler (Algorithm 2) -> Score Network -> Perturbation Module
- **Critical path:** The update step in Algorithm 1: `Z_k+1 = (1+h_k)Z_k + 2h_k * tilde_s(...) + sqrt(2h_k) * xi`.
  - **Constraint:** $h_k$ (step size) must satisfy stability conditions relative to the noise variance.
- **Design tradeoffs:**
  - **Variance vs. Steps:** High score noise ($\epsilon^v$) requires smaller step sizes ($h_{\text{max}}$) or more steps ($K$) to maintain quality.
  - **Efficiency:** The two-regime scheduler is more complex than uniform stepping but theoretically necessary for optimal rates.
- **Failure signatures:**
  - **Sharp FID degradation:** Figure 2 shows a "cliff" where quality drops sharply beyond a noise threshold (e.g., $\sigma > 2$ for CIFAR). This indicates the noise variance has overwhelmed the score signal.
  - **Exponential explosion:** If the step size $h_k$ is too large relative to the score Lipschitz constant $L$, the recursive bounds fail and sampling diverges.
- **First 3 experiments:**
  1. **Noise Injection Reproduction:** Implement Algorithm 1 and add Gaussian noise $\zeta$ to the score at inference. Verify if FID remains stable for $\sigma < 1$ as per Figure 2.
  2. **Scheduler Ablation:** Compare the default two-regime scheduler (Alg 2) vs. a uniform scheduler. Measure $W_2$ distance on a Gaussian target to confirm the linear $h_{\text{max}}$ scaling.
  3. **Bias vs. Variance Sensitivity:** Inject constant bias vs. zero-mean noise into the score. Confirm that bias degrades performance proportionally, while variance degrades with $\sqrt{h_{\text{max}}}$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the exponential dependence of convergence rates on parameters (e.g., support diameter) intrinsic to semi-log-concave targets, or is it an artifact of the analysis?
- **Basis in paper:** [explicit] The conclusion explicitly asks whether this dependence is intrinsic or an artifact.
- **Why unresolved:** The derived upper bounds scale exponentially, but no matching lower bounds are provided to confirm this rate is unavoidable.
- **Evidence:** Proving matching lower bounds or developing an alternative analysis that removes the exponential factor.

### Open Question 2
- **Question:** Does extending this analysis to kinetic Langevin diffusion-based DDPMs improve the error bounds' dependence on the discretization step size?
- **Basis in paper:** [explicit] The conclusion lists the extension to kinetic Langevin diffusion as an open question regarding step size dependence.
- **Why unresolved:** The current theoretical guarantees are derived specifically for the standard (Ornstein-Uhlenbeck) DDPM process.
- **Evidence:** A convergence proof for kinetic diffusion models showing improved step size scaling compared to the standard process.

### Open Question 3
- **Question:** What explains the sharp degradation in sample quality (FID) beyond a critical noise threshold, which is not predicted by the linear theoretical bounds?
- **Basis in paper:** [inferred] Section 6 notes a "sharp degradation in quality beyond a certain noise threshold, a phenomenon not accounted for by our theoretical analysis."
- **Why unresolved:** The paper's bounds degrade linearly with noise, failing to capture the empirical phase transition or instability observed at high noise levels.
- **Evidence:** A theoretical analysis capturing non-linear stability thresholds or phase transitions in the diffusion dynamics under noise.

### Open Question 4
- **Question:** Can the uniform score error assumption ($L^\infty$ norm) be relaxed to an $L^2$-norm or weighted average?
- **Basis in paper:** [inferred] Section 5 notes the uniform requirement is "more difficult to replace" but believes it could be relaxed.
- **Why unresolved:** The current recursive proof technique requires uniform control over the error to manage the drift terms deterministically.
- **Evidence:** A proof technique that succeeds using only $L^2$-integrable score error bounds.

## Limitations

- The theoretical analysis assumes the target distribution satisfies semi-log-concavity and bounded support conditions, which may not hold for all real-world data distributions.
- The paper provides limited empirical validation across diverse real-world distributions beyond standard benchmarks.
- The analysis assumes access to an exact score function or one with bounded error, but real-world score networks may violate these smoothness conditions, especially near data boundaries.

## Confidence

- **High confidence** in Mechanism 1 (noise variance effects). The theoretical derivation is clear, and the vanishing noise effect with more steps is well-established for unbiased estimators.
- **Medium confidence** in Mechanism 2 (two-regime scheduler). The theory is sound for the assumed distribution class, but the practical necessity and gains over simpler schedules on real data need more empirical support.
- **Medium confidence** in Mechanism 3 (linear step-size dependence). The recursive approach is novel and mathematically rigorous, but its advantage over prior work is primarily theoretical for the analyzed class of distributions.

## Next Checks

1. **Distribution robustness test:** Validate the theoretical bounds on a broader set of distributions (e.g., multi-modal, heavy-tailed) beyond the standard Gaussian mixtures to stress-test Assumption 1.
2. **Schedule ablation on real data:** Systematically compare the two-regime scheduler against uniform and other adaptive schedules on complex datasets (e.g., LSUN) to quantify practical gains in FID/Wasserstein distance.
3. **Score smoothness verification:** Measure the empirical Lipschitz constants and Hessian norms of pretrained score networks on test data to confirm they satisfy the smoothness conditions required for the linear step-size bound.