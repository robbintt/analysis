---
ver: rpa2
title: Double Q-learning for Value-based Deep Reinforcement Learning, Revisited
arxiv_id: '2507.00275'
source_url: https://arxiv.org/abs/2507.00275
tags:
- double
- q-learning
- overestimation
- target
- ddql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Deep Double Q-learning (DDQL), a method that
  adapts Double Q-learning to deep reinforcement learning by training two Q-functions
  through reciprocal bootstrapping. DDQL reduces overestimation by de-correlating
  action-selection and action-evaluation in bootstrap targets, using two separate
  Q-functions that train on distinct minibatches.
---

# Double Q-learning for Value-based Deep Reinforcement Learning, Revisited

## Quick Facts
- arXiv ID: 2507.00275
- Source URL: https://arxiv.org/abs/2507.00275
- Reference count: 40
- Introduces DDQL, which reduces overestimation by de-correlating action-selection and action-evaluation through reciprocal bootstrapping

## Executive Summary
This paper introduces Deep Double Q-learning (DDQL), a method that adapts Double Q-learning to deep reinforcement learning by training two Q-functions through reciprocal bootstrapping. The authors propose two variants: double-head DDQL (DH-DDQL) with a shared network trunk and two output heads, and double-network DDQL (DN-DDQL) with two independent networks. Both variants consistently outperform Double DQN across 57 Atari 2600 games, with DH-DDQL showing slightly better aggregate performance. The method achieves these improvements without introducing additional hyperparameters beyond Double DQN.

## Method Summary
DDQL adapts Double Q-learning to deep reinforcement learning by training two Q-functions through reciprocal bootstrapping. The method de-correlates action-selection and action-evaluation in bootstrap targets by using two separate Q-functions that train on distinct minibatches. Two variants are studied: DH-DDQL uses a shared network trunk with two output heads, while DN-DDQL employs two completely independent networks. Both variants outperform Double DQN across the Atari benchmark, with DH-DDQL showing slightly better aggregate performance. The paper also examines dataset partitioning strategies and replay ratios, finding that DH-DDQL benefits from shared replay buffers while DN-DDQL requires separate buffers.

## Key Results
- Both DDQL variants (DH-DDQL and DN-DDQL) outperform Double DQN across 57 Atari 2600 games
- DH-DDQL shows slightly better aggregate performance than DN-DDQL
- DDQL variants exhibit less overestimation than Double DQN, with DN-DDQL often underestimating
- Method achieves improvements without introducing additional hyperparameters beyond Double DQN

## Why This Works (Mechanism)
DDQL works by de-correlating action-selection and action-evaluation in bootstrap targets. Traditional DQN suffers from overestimation bias because the same Q-network is used to select and evaluate actions. DDQL addresses this by maintaining two separate Q-functions that reciprocally bootstrap each other, effectively breaking the correlation between these two steps. This approach ensures that the action selection and action evaluation are performed by different networks, reducing the overestimation bias inherent in standard Q-learning.

## Foundational Learning

### Q-learning and Bellman Equation
- **Why needed**: Understanding the foundation of temporal difference learning and value function approximation
- **Quick check**: Can derive the Q-learning update rule from the Bellman optimality equation

### Double Q-learning
- **Why needed**: Understanding how to de-correlate action selection and evaluation
- **Quick check**: Can explain why using the same Q-values for both selection and evaluation causes overestimation

### Deep Q-networks (DQN)
- **Why needed**: Understanding the neural network architecture and experience replay used in deep RL
- **Quick check**: Can describe the role of target networks and replay buffers in stabilizing DQN training

## Architecture Onboarding

### Component Map
Input -> Shared/Network1/Network2 -> Q-values -> Loss computation -> Parameter update

### Critical Path
1. State observation from environment
2. Forward pass through Q-network(s) to generate Q-values
3. Compute target values using reciprocal bootstrapping
4. Calculate temporal difference loss
5. Backpropagate and update network parameters

### Design Tradeoffs
- **Shared trunk (DH-DDQL)**: Lower memory/computation but potential parameter sharing issues
- **Separate networks (DN-DDQL)**: No parameter sharing but higher memory/computation
- **Dataset partitioning**: Separate buffers prevent information leakage but reduce sample efficiency

### Failure Signatures
- Both networks converging to similar Q-values (defeating the purpose)
- One network consistently outperforming the other
- Overestimation or underestimation bias in learned Q-values

### First 3 Experiments
1. Compare DH-DDQL vs DN-DDQL on a simple gridworld environment
2. Measure overestimation bias in Q-values across different DDQL variants
3. Ablation study on dataset partitioning strategies

## Open Questions the Paper Calls Out
None

## Limitations

- Generalizability beyond the 57 Atari 2600 games tested remains uncertain
- The extent and practical implications of DN-DDQL's underestimation behavior lack quantitative analysis
- Optimal dataset partitioning strategies for different task types need further investigation

## Confidence

- **High confidence**: Architectural description of DDQL variants and reciprocal bootstrapping mechanism
- **Medium confidence**: Performance improvements across the Atari benchmark and reduction of overestimation
- **Low confidence**: Precise characterization of DN-DDQL's underestimation behavior and implications

## Next Checks

1. Evaluate DDQL variants on diverse reinforcement learning domains beyond Atari 2600, including continuous control tasks and simulated robotics environments, to assess cross-domain performance consistency.

2. Conduct detailed analysis measuring the magnitude and distribution of Q-value overestimation and underestimation across DDQL variants, including statistical tests comparing these distributions to Double DQN.

3. Systematically vary dataset partitioning strategies and replay buffer configurations to establish the sensitivity of DH-DDQL and DN-DDQL to these design choices, identifying optimal configurations for different task types.