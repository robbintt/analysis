---
ver: rpa2
title: Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents
arxiv_id: '2602.00929'
source_url: https://arxiv.org/abs/2602.00929
tags:
- learning
- abstractions
- agent
- state
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TheoryCoder-2 is a theory-based reinforcement learning agent that
  learns reusable abstractions from experience to enable efficient hierarchical planning
  in complex environments. It synthesizes high-level PDDL operators and Python world
  models using LLMs' in-context learning, then reuses these abstractions across tasks.
---

# Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents

## Quick Facts
- **arXiv ID**: 2602.00929
- **Source URL**: https://arxiv.org/abs/2602.00929
- **Reference count**: 21
- **Primary result**: TheoryCoder-2 achieves 0-2000 token costs (vs 8500-33000 for baselines) on complex levels, successfully solving tasks like Boss levels that baselines fail.

## Executive Summary
TheoryCoder-2 is a theory-based reinforcement learning agent that learns reusable abstractions from experience to enable efficient hierarchical planning in complex environments. It synthesizes high-level PDDL operators and Python world models using LLMs' in-context learning, then reuses these abstractions across tasks. Experiments on BabyAI, Minihack, and VGDL games show TheoryCoder-2 achieves 0-2000 token costs (vs 8500-33000 for baselines) on complex levels, successfully solving tasks like Boss levels that baselines fail. It demonstrates sample efficiency through abstraction transfer, requiring minimal human prompts compared to prior theory-based RL systems. The approach bridges symbolic planning with learned world models, achieving state-of-the-art performance on challenging environments.

## Method Summary
TheoryCoder-2 uses LLM in-context learning to synthesize reusable PDDL abstractions from raw state observations when guided by minimal, domain-agnostic few-shot examples. The LLM receives a raw state dictionary, a few toy PDDL examples, and a domain description, then outputs PDDL domain/problem files defining abstract operators and predicates at appropriate granularity. Hierarchical (bi-level) planning decomposes the search problem, with Fast Downward generating a high-level plan as a sequence of abstract operators (e.g., "unlock door"), while BFS maps each operator to primitive actions using the learned Python transition model. A predicate checker verifies grounding consistency. Abstraction transfer across environments within a curriculum yields sample efficiency gains, as learned operators require no re-synthesis after initial learning in simpler environments.

## Key Results
- TheoryCoder-2 achieves 0-2000 token costs on complex levels versus 8500-33000 for baselines
- Successfully solves Boss levels that baselines fail to complete
- Demonstrates abstraction transfer, with token consumption dropping dramatically on subsequent levels after initial learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM in-context learning can synthesize reusable PDDL abstractions from raw state observations when guided by minimal, domain-agnostic few-shot examples.
- **Mechanism**: The LLM receives a raw state dictionary, a few toy PDDL examples (e.g., "eat" with precondition "not eaten"), and a domain description. It outputs PDDL domain/problem files defining abstract operators and predicates at an appropriate granularityâ€”not too granular, not too coarse.
- **Core assumption**: LLMs possess sufficient prior knowledge of planning concepts to generalize from toy examples to novel domains.
- **Evidence anchors**: TheoryCoder-2 leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience.
- **Break condition**: If the environment requires abstractions that cannot be expressed in PDDL (continuous dynamics, probabilistic effects), or if the LLM lacks sufficient prior planning knowledge for the domain.

### Mechanism 2
- **Claim**: Hierarchical (bi-level) planning decomposes the search problem, reducing computational cost by planning abstractly first and grounding second.
- **Mechanism**: Fast Downward generates a high-level plan as a sequence of abstract operators (e.g., "unlock door"). BFS then maps each operator to primitive actions using the learned Python transition model. A predicate checker verifies grounding consistency.
- **Core assumption**: Abstract operators are composable and their preconditions/effects are correctly captured; the low-level world model is sufficiently accurate.
- **Evidence anchors**: The high-level planner generates a symbolic plan in terms of abstract operators. The low-level planner uses the learned transition function to map each operator to a sequence of primitive actions.
- **Break condition**: If predicate classifiers fail to correctly identify abstract states, or if the transition model has systematic errors that compound across the hierarchy.

### Mechanism 3
- **Claim**: Abstraction transfer across environments within a curriculum yields sample efficiency gains, as learned operators require no re-synthesis.
- **Mechanism**: After learning abstractions in simpler environments (e.g., "pickup" in BabyAI-Pickup), the agent reuses the PDDL domain file for harder environments (e.g., BabyAI Boss level), synthesizing only a new problem file.
- **Core assumption**: Environments share structure such that abstractions are genuinely reusable; the curriculum ordering presents learning problems in a tractable sequence.
- **Evidence anchors**: TheoryCoder-2 allocates high computation for the first and the second levels to learn the abstractions; but its token consumption dramatically drops on the Boss level (from about 8500 and 33000 to around 2000).
- **Break condition**: If environments are sufficiently dissimilar that abstractions do not transfer, or if the agent fails to recognize when new abstractions are needed.

## Foundational Learning

- **PDDL (Planning Domain Definition Language)**
  - Why needed here: Core representation for abstract operators, predicates, preconditions, and effects.
  - Quick check question: Can you write a simple PDDL domain file with one operator and its preconditions/effects?

- **Classical Search Algorithms (BFS, Fast Downward)**
  - Why needed here: High-level planner uses Fast Downward; low-level planner uses BFS to ground operators.
  - Quick check question: Explain why BFS guarantees shortest paths but A* with an admissible heuristic also does.

- **LLM In-Context Learning**
  - Why needed here: Understanding how few-shot examples in prompts guide synthesis without weight updates.
  - Quick check question: Why might in-context learning fail if examples are too domain-specific?

- **World Models (Transition Functions)**
  - Why needed here: The Python transition model predicts next states given current state and action.
  - Quick check question: What happens if the transition model incorrectly predicts a wall as passable?

## Architecture Onboarding

- **Component map**: LLM (GPT-4o) -> PDDL domain/problem files -> Fast Downward -> abstract operator sequence -> BFS planner -> primitive actions using transition model -> execution
- **Critical path**: 1. Observe raw state -> 2. LLM synthesizes PDDL files -> 3. Fast Downward generates high-level plan -> 4. For each operator: BFS grounds it using transition model -> 5. Execute actions -> 6. If predicate effects not satisfied, refine transition model or explore -> 7. Repeat
- **Design tradeoffs**: Token cost vs. abstraction quality: More compute on early levels yields better abstractions but higher initial cost; Curriculum design: Ordered by difficulty improves transfer; shuffled reduces transfer benefits; PDDL expressivity: Limited to discrete, deterministic domains; cannot handle continuous actions or stochastic transitions
- **Failure signatures**: Token costs remain high across environments -> abstractions not transferring correctly; Planner returns "unsolvable" -> PDDL predicates may be incorrectly specified or effects wrong; Low-level execution fails -> transition model has systematic errors; check predicate classifiers; State representation issues: Representing doors as "open door" vs. "closed door" strings vs. objects with attributes caused predicate classifier errors
- **First 3 experiments**: 1. Sanity check on simple navigation: Run TheoryCoder-2 on Labyrinth. Verify it learns "moveontop" operator and token cost matches Table 1 (~21K). Confirm Fast Downward plan terminates in <1 second. 2. Ablation of curriculum: Run TC-C (no curriculum) on BabyAI Boss level directly. Compare token cost to full TheoryCoder-2 (~45K vs ~2K). This isolates transfer contribution. 3. Predicate classifier stress test: Modify BabyAI state representation to use the problematic "open door" / "closed door" string format. Verify predicate classifiers fail, then switch to object-with-attributes representation and confirm recovery.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TheoryCoder-2's abstraction synthesis pipeline adapt to continuous state and action spaces?
- Basis in paper: The authors state that "extending beyond discrete domains to continuous ones introduces new challenges," such as predicting continuous trajectories.
- Why unresolved: The current architecture relies on discrete symbolic planners (Fast Downward) and BFS, which cannot directly handle continuous dynamics or operators.
- What evidence would resolve it: Successful application of TheoryCoder-2 to continuous control benchmarks (e.g., robotics simulation) using modified hybrid PDDL planners or continuous abstraction representations.

### Open Question 2
- Question: Can vision-language models (VLMs) reliably extract object-oriented states for TheoryCoder-2 in complex visual environments?
- Basis in paper: The authors note the system "assumes access to an object-oriented, text-based state representation" and suggest VLMs "may serve as perception modules" despite mixed prior results.
- Why unresolved: VLMs often struggle with spatial reasoning and hallucinations, which would likely propagate errors into the symbolic abstraction layer and break predicate classifiers.
- What evidence would resolve it: Demonstration of TheoryCoder-2 maintaining high success rates on visual benchmarks (e.g., Atari or 3D environments) when fed VLM-extracted states rather than ground-truth text states.

### Open Question 3
- Question: What mechanisms are required to support trial-and-error learning for PDDL operators when initial observations are insufficient?
- Basis in paper: The authors admit that "more challenging or less familiar environments will likely require additional mechanisms to support trial-and-error learning for the PDDL representations."
- Why unresolved: The current system relies heavily on LLM priors to synthesize abstractions zero-shot; if the initial observation is ambiguous, the agent lacks a robust recovery strategy to revise the PDDL domain.
- What evidence would resolve it: An extension where the agent successfully identifies and corrects flawed PDDL operators through active exploration in a domain where the initial LLM hypothesis is incorrect.

## Limitations

- The LLM's ability to generalize from toy PDDL examples to complex domains remains an empirical assumption that may not hold across all environment types.
- The curriculum design and abstraction transfer mechanism depend heavily on careful environment ordering, which may not generalize to arbitrary task distributions.
- Claims about sample efficiency compared to prior theory-based RL systems lack direct quantitative comparison.

## Confidence

**High Confidence**: The hierarchical planning decomposition mechanism (bi-level planning) is well-established and the paper provides clear evidence of computational efficiency gains. The token cost reductions on later levels (from 8500-33000 to ~2000) are directly measured and demonstrate clear transfer benefits.

**Medium Confidence**: The LLM's ability to synthesize correct PDDL abstractions from few-shot examples is supported by experimental results but depends on the underlying model's planning knowledge. The paper shows success across multiple domains but doesn't explore failure modes systematically or test the limits of what abstractions can be learned.

**Low Confidence**: Claims about sample efficiency compared to prior theory-based RL systems lack direct quantitative comparison. The paper states TheoryCoder-2 requires "minimal human prompts" but doesn't benchmark against specific prior systems on the same tasks to quantify this advantage.

## Next Checks

1. **Cross-domain generalization test**: Apply TheoryCoder-2 to a continuous control domain (e.g., MuJoCo) where PDDL cannot express actions. Measure whether the system gracefully degrades or fails entirely, establishing the boundaries of the approach's applicability.

2. **State representation ablation**: Systematically test different state representations for the same environment (e.g., door as string vs. object with boolean attribute vs. numerical encoding). Measure predicate classifier accuracy and planning success rates to quantify representation sensitivity.

3. **Curriculum disruption analysis**: Run experiments with shuffled curriculum orderings on BabyAI Boss levels. Compare token costs and success rates against the ordered curriculum to quantify how much performance depends on careful task sequencing versus the underlying abstraction learning mechanism.