---
ver: rpa2
title: 'Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM
  Inference: Algorithms and Numerical Analysis'
arxiv_id: '2503.01873'
source_url: https://arxiv.org/abs/2503.01873
tags:
- attention
- pasa
- matrix
- precision
- overflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the numerical instability of low-precision
  attention computation in large language models, specifically focusing on overflow
  issues in half-precision (FP16) calculations. The authors propose PASA (Pseudo-average
  Shifting Attention), an algorithm that introduces online pseudo-average shifting
  and global recovering techniques to eliminate overflow origins while maintaining
  computational efficiency.
---

# Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms and Numerical Analysis

## Quick Facts
- arXiv ID: 2503.01873
- Source URL: https://arxiv.org/abs/2503.01873
- Reference count: 40
- Primary result: PASA enables stable full FP16 attention computation by eliminating overflow through online pseudo-average shifting and global recovery

## Executive Summary
The paper addresses critical numerical instability in low-precision attention computation for large language models, specifically the overflow issues that arise when using half-precision (FP16) arithmetic. The authors identify that large biases in the sequence dimension and resonance mechanisms between query and key matrices in the head dimension are the primary sources of overflow. To solve this, they propose PASA (Pseudo-average Shifting Attention), an algorithm that introduces online pseudo-average shifting and global recovering techniques to eliminate overflow origins while maintaining computational efficiency. PASA enables full FP16 computation throughout the attention process by shifting local block biases close to zero and recovering global bias information online.

## Method Summary
PASA introduces a novel approach to low-precision attention computation by implementing online pseudo-average shifting and global recovery mechanisms. The algorithm works by first identifying and shifting local block biases close to zero to prevent overflow during intermediate computations. It then recovers the global bias information online, ensuring that the final results maintain accuracy comparable to high-precision attention. This two-stage process allows for full FP16 computation throughout the attention mechanism without sacrificing numerical stability. The method is validated on both randomly generated benchmarks and real large models including Qwen2-7B language models and Stable-Video-Diffusion multi-modal models, demonstrating effective overflow elimination while maintaining accuracy.

## Key Results
- PASA effectively eliminates overflow in low-precision attention computation across multiple model architectures
- The method maintains accuracy comparable to high-precision attention while using only FP16 precision
- Numerical analysis identifies large bias in sequence dimension and resonance mechanism between query and key matrices as primary overflow sources

## Why This Works (Mechanism)
PASA works by addressing the fundamental numerical instability issues in low-precision attention computation. The algorithm tackles two primary overflow sources: large biases in the sequence dimension that can cause intermediate values to exceed FP16 range, and resonance mechanisms between query and key matrices in the head dimension that amplify these biases. By implementing online pseudo-average shifting, PASA moves local block biases closer to zero during computation, preventing overflow at intermediate steps. The global recovery mechanism then reconstructs the correct global bias information, ensuring the final attention outputs remain accurate despite the shifting operations.

## Foundational Learning

**Floating Point Precision**: Understanding the numerical limits and representation of FP16 vs FP32 formats
- Why needed: Essential to grasp why overflow occurs in low-precision computation
- Quick check: Can you explain the dynamic range difference between FP16 and FP32?

**Attention Mechanism**: Core transformer operation involving query-key-value matrix multiplications
- Why needed: PASA specifically targets numerical stability in attention computation
- Quick check: Can you describe the three matrix multiplications in scaled dot-product attention?

**Resonance Effects**: Amplification phenomena when similar magnitude values interact repeatedly
- Why needed: Identifies how query-key interactions create overflow in head dimension
- Quick check: Can you explain how repeated matrix multiplications can amplify numerical errors?

## Architecture Onboarding

**Component Map**: Input tensors (Q, K, V) -> PASA Block Processing -> Shifted Computation -> Global Recovery -> Output attention weights

**Critical Path**: Query/Key matrix multiplication → Pseudo-average shifting → Value transformation → Output generation

**Design Tradeoffs**: PASA prioritizes numerical stability over minimal computational overhead, accepting slight additional operations to enable full FP16 computation versus falling back to higher precision

**Failure Signatures**: Overflow manifests as NaN or Inf values in attention weights; PASA failure would show either persistent overflow or significant accuracy degradation compared to baseline

**3 First Experiments**:
1. Verify overflow occurrence in baseline FP16 attention on synthetic Q/K matrices with large biases
2. Test PASA's shifting mechanism on controlled block sizes to observe bias reduction
3. Compare final attention outputs between PASA and FP32 baseline on standard benchmark datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to specific model families (Qwen2-7B, Stable-Video-Diffusion) requiring broader architecture testing
- Computational overhead of shifting and recovery mechanisms not extensively benchmarked against alternative approaches
- Claims of accuracy preservation need validation across diverse tasks beyond controlled benchmarks

## Confidence

**High Confidence**: Overflow source identification and elimination capability are well-supported by numerical analysis and empirical evidence

**Medium Confidence**: Accuracy claims are supported by benchmarks but require broader task and model diversity validation

**Medium Confidence**: Computational efficiency claims are plausible but lack comprehensive benchmarking against competing methods

## Next Checks

1. **Cross-Architecture Generalization**: Test PASA on additional LLM architectures (e.g., LLaMA, Mistral) and diverse downstream tasks (e.g., code generation, mathematical reasoning) to assess robustness beyond the initial experimental scope

2. **Performance Benchmarking**: Conduct head-to-head comparisons of PASA against alternative low-precision techniques (e.g., Block Floating Point, Dynamic Scaling) in terms of inference latency, memory footprint, and accuracy across multiple hardware platforms (GPU, CPU, specialized accelerators)

3. **Ablation Studies on Shifting Parameters**: Perform detailed ablation studies to determine the sensitivity of PASA's performance to its hyper-parameters (e.g., block size, shifting frequency) and identify optimal configurations for different model scales and sequence lengths