---
ver: rpa2
title: 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic
  Targets?'
arxiv_id: '2511.17792'
source_url: https://arxiv.org/abs/2511.17792
tags:
- wan2
- world
- planning
- path
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Target-Bench, the first benchmark for evaluating
  world models on mapless path planning toward semantic targets. The authors collect
  a dataset of 450 robot-collected video sequences covering 45 semantic categories
  with SLAM-based ground truth trajectories.
---

# Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?

## Quick Facts
- **arXiv ID**: 2511.17792
- **Source URL**: https://arxiv.org/abs/2511.17792
- **Reference count**: 39
- **Primary result**: Best off-the-shelf world model achieves only 0.299 overall score on mapless semantic path planning; fine-tuning on 325 scenarios improves score by 400% to 0.345

## Executive Summary
Target-Bench introduces the first benchmark for evaluating world models on mapless path planning toward semantic targets. The authors collect 450 robot-collected video sequences with SLAM-based ground truth trajectories across 45 semantic categories. Their evaluation pipeline extracts camera motion from generated videos and measures planning performance using five metrics. The benchmark reveals significant limitations in current world models, with the best off-the-shelf model achieving only 0.299 overall score. Domain-specific fine-tuning of a 5B-parameter model on 325 scenarios achieves 0.345 overall score—a 400%+ improvement over its base version and 15% higher than the best off-the-shelf model.

## Method Summary
The benchmark evaluates world models by generating future video sequences from initial frames and semantic target prompts, then extracting camera trajectories from these videos for comparison against SLAM ground truth. A world decoder (VGGT, SpaTracker, or ViPE) reconstructs camera poses from generated frames, with monocular methods using scale recovery based on ground truth displacement. Performance is measured across five metrics (ADE, FDE, MR, SE, AC) aggregated into a Weighted Overall score. Fine-tuning uses LoRA on 325 scenarios with frame augmentation, requiring 8× A800 80GB GPUs for training.

## Key Results
- Best off-the-shelf world model (Wan2.2-Flash) achieves only 0.299 overall score
- Fine-tuning a 5B-parameter model on 325 scenarios achieves 0.345 overall score (400%+ improvement over base)
- Shorter planning horizons (4s) yield 8-15% higher scores than longer horizons (8s)
- VGGT achieves highest reconstruction ceiling at 0.783 on ground truth videos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: World models can encode navigation plans implicitly through predicted video frames, which are then decoded into executable trajectories.
- Mechanism: Given an initial observation and a semantic target prompt, the world model generates a future video sequence depicting motion toward the goal. A "world decoder" extracts camera poses frame-by-frame to form a planned path in SE(3) space.
- Core assumption: The model's visual predictions contain geometrically consistent camera motion that correlates with physically realizable paths.
- Evidence anchors:
  - [abstract] "Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics."
  - [section 3.2.1] Describes VGGT, SpaTracker, and ViPE as reconstruction methods that extract camera trajectories from video frames.
  - [corpus] ImagineNav++ (arXiv 2512.17435) similarly uses VLMs for scene imagination in navigation, suggesting cross-validation of the vision-to-plan concept.
- Break condition: If generated videos lack temporal coherence or exhibit spatial warping, reconstruction fails and paths become unreliable.

### Mechanism 2
- Claim: Scale recovery from monocular predictions enables metric-accurate path comparison without depth sensors.
- Mechanism: Monocular reconstruction methods (VGGT, SpaTracker) estimate poses up to an unknown scale factor λ. The pipeline anchors predictions to ground truth by computing λ = d_real / d_pred using the first and k-th frame displacement, then uniformly rescales all translations.
- Core assumption: A single global scale factor suffices; scale does not vary significantly across the trajectory.
- Evidence anchors:
  - [section 3.2.1] "We restore metric consistency at the segment level by anchoring predictions to a single scalar scale factor λ derived from ground truth displacement."
  - [section 4.4] Ground truth video achieves 0.783 WO score with VGGT, validating that decoded trajectories align with SLAM ground truth when scale is properly recovered.
  - [corpus] No direct corpus evidence for this specific scale recovery technique.
- Break condition: If scale varies non-uniformly (e.g., due to varying depth along the path), single-factor rescaling introduces systematic errors.

### Mechanism 3
- Claim: Domain-specific fine-tuning on limited real-world robotic data dramatically outperforms larger pre-trained models for planning tasks.
- Mechanism: LoRA-based fine-tuning of a 5B-parameter Wan model on 325 scenarios (with frame augmentation) enables the model to learn robot-specific spatial reasoning patterns that generic video pre-training does not capture.
- Core assumption: World models possess latent spatial reasoning capabilities that require only targeted exposure to robotic navigation data to activate.
- Evidence anchors:
  - [abstract] "Fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score—an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model."
  - [section 4.3] "Wan2.2-5B-FT-DA outperforms the base model by more than 400% and achieves the best overall score."
  - [corpus] Map-World (arXiv 2511.20156) explores world models for autonomous driving planning, supporting domain adaptation as a general principle, though not quantifying fine-tuning gains.
- Break condition: If base model lacks sufficient spatial reasoning capacity, fine-tuning cannot inject capabilities that don't exist latently.

## Foundational Learning

- Concept: **SE(3) Transformations and Camera Extrinsics**
  - Why needed here: Understanding how 6-DOF poses (rotation + translation) represent camera trajectories is essential for interpreting the world decoder output and path evaluation metrics.
  - Quick check question: Can you explain why monocular visual SLAM estimates pose only up to an unknown scale factor?

- Concept: **SLAM (Simultaneous Localization and Mapping)**
  - Why needed here: The ground truth trajectories are generated via LiDAR-centric SLAM with multi-sensor fusion; understanding SLAM principles helps interpret benchmark validity and reconstruction error sources.
  - Quick check question: What role does loop closure play in reducing drift, and why might feed-forward pose estimation (VGGT) struggle without it?

- Concept: **World Models as Predictive Simulators**
  - Why needed here: The benchmark assumes world models can predict future frames conditioned on semantic goals; understanding this paradigm clarifies why visual fidelity alone is insufficient for planning.
  - Quick check question: If a world model generates visually realistic frames but violates physical consistency (e.g., impossible motion), which Target-Bench metrics would penalize this most?

## Architecture Onboarding

- Component map: Initial frame + prompt -> World Model -> Video generation -> World Decoder (VGGT/SpaTracker/ViPE) -> Scale recovery -> Metric computation -> WO score
- Critical path: Initial frame + prompt → World Model inference → Video generation → World Decoder pose extraction → Scale recovery → Metric computation → WO score
- Design tradeoffs:
  - **VGGT vs. SpaTracker vs. ViPE**: VGGT achieves highest WO (0.783 on GT) but requires scale recovery; ViPE provides metric-scale outputs directly but performs worse overall (0.249 on GT).
  - **Planning horizon**: Shorter horizons (4s) yield 8-15% higher WO scores than longer horizons (8s), trading planning depth for reliability.
  - **Fine-tuning data scale**: Only 325 scenarios needed for 400%+ improvement, but augmentation (shifting-frame sampling) provides additional 20% gain.
- Failure signatures:
  - **Miss Rate > 70%**: Indicates model generates frames that deviate >2m from ground truth at most timesteps (e.g., Veo 3.1 at 77.25%).
  - **SE < 0.1**: Model fails to reach semantic target endpoint even approximately (e.g., Wan2.1-Turbo at 0.000).
  - **Reconstruction ceiling at 0.783**: Even ground truth video doesn't achieve perfect score due to monocular scale recovery errors and motion blur in feed-forward pose estimation.
- First 3 experiments:
  1. **Baseline reconstruction validation**: Run VGGT, SpaTracker, and ViPE on ground truth videos to establish decoder-specific ceilings and confirm pipeline correctness (expect VGGT ≈ 0.78).
  2. **Off-the-shelf model sweep**: Evaluate all available world models (Sora 2, Veo 3.1, Wan variants) on the 125-scenario benchmark split using VGGT to identify best starting point for fine-tuning.
  3. **Minimal fine-tuning pilot**: Fine-tune Wan2.2-5B on 50 scenarios (subset of 325) without augmentation to validate that gains emerge quickly; target >200% improvement over base to confirm domain adaptation hypothesis before full training run.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would world models perform in closed-loop robot navigation with iterative re-planning, rather than single-shot inference?
- Basis in paper: [explicit] The authors state: "Future work could extend the current framework towards closed-loop re-planning on the robot."
- Why unresolved: Target-Bench evaluates one-time inference without re-planning, which doesn't reflect real-world navigation where robots must continuously adjust plans based on new observations.
- What evidence would resolve it: A modified benchmark evaluating WM performance in a receding-horizon control loop with repeated re-planning on a physical robot.

### Open Question 2
- Question: How can latent memory mechanisms in world models support complex, long-horizon navigation tasks?
- Basis in paper: [explicit] Future work should study "how latent memory supports complex receding-horizon navigation tasks," with reference to recent work showing WMs can explore new spaces while retaining information.
- Why unresolved: Current evaluation uses single video clips without testing memory retention across extended navigation episodes or previously unseen environments.
- What evidence would resolve it: Experiments testing WM navigation performance over multi-room trajectories requiring memory of earlier observations.

### Open Question 3
- Question: How accurate must world model predictions be to enable useful real-world robot planning?
- Basis in paper: [explicit] "A key question remains: How accurate must these predictions be to count as useful for planning?"
- Why unresolved: The best model achieves only 0.299 overall score, but the threshold for practical deployment remains unknown; ground truth achieves 0.783, leaving unclear whether improvement toward this ceiling is sufficient.
- What evidence would resolve it: Systematic evaluation of task success rates on physical robots using WM predictions at varying accuracy levels.

### Open Question 4
- Question: Does the finding that domain-specific fine-tuning outperforms larger pre-training generalize across different world model architectures?
- Basis in paper: [inferred] The 400%+ improvement from fine-tuning a 5B model on only 325 scenarios challenges conventional wisdom about scale, but this finding is based on a single model family.
- Why unresolved: Only Wan2.2-5B was fine-tuned; it remains unclear whether this pattern holds for other architectures or whether different models would benefit differently from domain adaptation.
- What evidence would resolve it: Fine-tuning experiments across multiple open-source world model architectures on the same dataset with controlled comparisons.

## Limitations

- Dataset availability and generalization remains uncertain since the 450 robot-collected sequences are not yet publicly available for independent validation
- The monocular reconstruction pipeline assumes a single global scale factor suffices, which may not hold for trajectories with varying depth profiles
- Heavy weighting of endpoint accuracy (65% for SE and AC) may not capture the full spectrum of planning quality needed for real-world deployment

## Confidence

- **High Confidence**: Off-the-shelf world models perform poorly on mapless planning (0.299 WO score); fine-tuning improvement claim (>400%) is well-supported by direct quantitative comparison
- **Medium Confidence**: The mechanism by which world models implicitly encode navigation plans through video generation is theoretically sound but not directly observed
- **Low Confidence**: The claim that only 325 scenarios are needed for significant improvement assumes base models have latent spatial reasoning capabilities

## Next Checks

1. **Scale Factor Validation**: Generate synthetic trajectories with known scale variations and test whether the single-factor recovery introduces measurable errors. Compare against multi-scale recovery approaches to establish the true impact on metric accuracy.

2. **Cross-Dataset Generalization**: Once the dataset becomes available, evaluate whether models fine-tuned on Target-Bench data transfer to other navigation benchmarks (e.g., Habitat, Gibson) or fail when environmental conditions change significantly.

3. **Qualitative Failure Analysis**: For the lowest-performing models (e.g., Veo 3.1 with 77.25% MR), conduct detailed frame-by-frame analysis to identify whether failures stem from visual artifacts, temporal inconsistency, or fundamental inability to reason about spatial goals.