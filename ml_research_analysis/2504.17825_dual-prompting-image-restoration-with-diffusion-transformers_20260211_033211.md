---
ver: rpa2
title: Dual Prompting Image Restoration with Diffusion Transformers
arxiv_id: '2504.17825'
source_url: https://arxiv.org/abs/2504.17825
tags:
- image
- restoration
- visual
- dual
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes DPIR, a novel DiT-based image restoration
  method that leverages a dual prompting strategy to extract valuable visual conditioning
  from both global and local perspectives, significantly enhancing restoration outcomes.
  The method consists of two branches: a low-quality image conditioning branch and
  a dual prompting control branch.'
---

# Dual Prompting Image Restoration with Diffusion Transformers

## Quick Facts
- arXiv ID: 2504.17825
- Source URL: https://arxiv.org/abs/2504.17825
- Authors: Dehong Kong; Fan Li; Zhixin Wang; Jiaqi Xu; Renjing Pei; Wenbo Li; WenQi Ren
- Reference count: 40
- Key outcome: Proposes DPIR, a DiT-based image restoration method leveraging dual prompting strategy, achieving superior performance across various tasks and metrics compared to state-of-the-art methods.

## Executive Summary
This paper introduces DPIR, a novel DiT-based image restoration method that leverages a dual prompting strategy to extract valuable visual conditioning from both global and local perspectives, significantly enhancing restoration outcomes. The method consists of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch efficiently incorporates image priors into the DiT, while the second branch extracts global-local visual prompts alongside textual prompts to form dual prompts, greatly enhancing the quality of restoration. Experimental results demonstrate that DPIR delivers superior image restoration performance compared to recent state-of-the-art methods, achieving impressive results across a broad range of image restoration tasks.

## Method Summary
DPIR modifies the Stable Diffusion 3 (SD3) DiT backbone with two key components: a lightweight conditioning branch and a dual prompting control branch. The lightweight conditioning branch uses a small convolutional network to extract image priors from the low-quality (LQ) latent, which are then adaptively aligned and added to the DiT's first block output. The dual prompting branch replaces CLIP text embeddings in SD3 with CLIP image embeddings extracted from both local patches and global context of the LQ image, combined with T5 text embeddings to form "dual prompts" that guide each DiT block via cross-attention. The method uses a global-local visual training strategy and trains with Conditional Flow Matching (Rectified Flow) on synthetic LQ/HQ pairs generated using the Real-ESRGAN degradation model.

## Key Results
- DPIR achieves the best or second-best scores of DISTS and no-reference metrics across all three datasets (PIRM, SPAQ, NTIRE).
- The method significantly improves restoration outcomes in terms of image aesthetics compared to state-of-the-art approaches.
- Ablation studies confirm the effectiveness of both the lightweight conditioning branch and the dual prompting strategy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual prompting strategy combining textual and visual cues improves image restoration quality over text-only conditioning.
- Mechanism: The dual prompting control branch extracts global-local visual features from the LQ image using CLIP image encoders. These visual tokens are concatenated with T5 text embeddings to form "dual prompts" (`c_dual`). These dual prompts are fed to each DiT block via cross-attention, providing richer, multi-perspective conditioning (global context + local appearance + semantics from text).
- Core assumption: The model assumes that textual descriptions alone cannot fully capture the rich visual characteristics of a degraded image and that visual features provide essential structural and texture guidance.
- Evidence anchors:
  - [abstract] "a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance."
  - [section 4.4] "we incorporate more visual control by replacing the CLIP text embeddings in DiT... combined with the text prompts from T5, result in the dual prompt cdual, which is applied to each block in DiT via cross-attention."
  - [corpus] Corpus contains "Dual-branch Prompting" in other domains (MMT), but specific evidence for this visual-textual dual prompting in DiT-based IR is weak/missing in neighbors.
- Break condition: If the visual features extracted from the LQ image are severely corrupted and carry no useful signal, replacing text embeddings with them might degrade performance. Ablation study shows "Visual Prompts Only" underperforms "Text Prompts Only", suggesting text provides crucial overall scene information.

### Mechanism 2
- Claim: A lightweight conditioning branch efficiently injects LQ image priors into the DiT backbone, avoiding the heavy computational cost of copying the backbone.
- Mechanism: Instead of a heavy ControlNet-style copy, this branch uses a small feature extraction module (`F_c`) with a few convolutional layers. It processes the LQ latent `z_LQ`. Its output is adaptively aligned using mean and variance statistics from the DiT's first block output and added to that output (`y_c = F_d0(z_t) + η(F_c(z_LQ))`).
- Core assumption: The model assumes that a lightweight convolutional feature extractor is sufficient to capture necessary image priors when combined with the dual prompting strategy and that feature alignment via normalization ensures stable training.
- Evidence anchors:
  - [abstract] "The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency."
  - [section 4.3] "Inspired by ControlNeXt... the LQ conditioning branch consists of... a lightweight feature extraction module with a few trainable convolutional layers... and an adaptive feature alignment module."
  - [corpus] Evidence for this specific lightweight mechanism in this context is weak/missing in the provided neighbors.
- Break condition: If the LQ image's degradation is very complex and subtle, a few convolutional layers might fail to extract meaningful priors, limiting the guidance signal's quality.

### Mechanism 3
- Claim: A global-local visual training strategy bridges the gap between local high-resolution patches and the global semantics required for restoration.
- Mechanism: During training, for a local patch to be restored (`x_local`), a surrounding global context patch (`x_global`) is also fed to a CLIP encoder. The resulting global visual tokens (`c_vis_global`) are concatenated with local visual tokens (`c_vis_local`). This combined representation is closer to the pre-trained T2I model's text embeddings (which capture global semantics), easing the adaptation.
- Core assumption: The model assumes that providing global context alongside local patches during training reduces learning difficulty and helps the model understand the scene, preventing it from generating incorrect textures.
- Evidence anchors:
  - [section 4.4.2] "global-local visual understanding training strategy to smoothly adapt pre-trained DiT from image synthesis to image restoration."
  - [section 5.3] Table 6 and text show "Global-Local" outperforms "Local" on metrics like CLIPIQA (0.7416 vs 0.6730).
  - [corpus] Evidence for this specific global-local visual training strategy in image restoration is weak/missing in the provided neighbors.
- Break condition: If the global context patch is irrelevant or misleading (e.g., contains mostly background with no relation to the object in the local patch), it may introduce noise rather than helpful context.

## Foundational Learning

- **Concept: Diffusion Transformers (DiTs) vs. U-Net Diffusion**
  - Why needed here: The paper argues U-Net backbones have limited capabilities and DiTs (specifically SD3) offer better scalability and long-range dependency modeling, which are crucial for high-quality restoration.
  - Quick check question: Why does the paper choose DiT over U-Net for the backbone?

- **Concept: Rectified Flow / Flow Matching**
  - Why needed here: The paper uses this formulation (as in SD3) instead of standard diffusion, training the network `v_theta` with a conditional flow matching objective (`L_CFM`).
  - Quick check question: What specific training objective function does DPIR optimize?

- **Concept: Visual & Textual Conditioning in Pre-trained T2I Models**
  - Why needed here: DPIR modifies how SD3 receives conditioning. It replaces CLIP text embeddings with CLIP image embeddings while keeping T5 text embeddings. Understanding the original role of these embeddings is key.
  - Quick check question: Which text encoder's embeddings are kept, and which are replaced by visual features in the dual prompting strategy?

## Architecture Onboarding

- **Component map:**
  - Inputs: Low-Quality Image (`x_LQ`), Text Prompt
  - Encoder: Degradation-Robust VAE Encoder (`E_dr`) produces `z_LQ`
  - Branch 1 (Lightweight Conditioning): Convolutional Feature Extractor (`F_c`) -> Adaptive Feature Alignment (`η`) -> Add to DiT Block 0 output
  - Branch 2 (Dual Prompting): `z_LQ` -> VAE Decoder -> Pixel-space LQ. Split into local (`x_local`) and global (`x_global`) patches
    - Patches -> CLIP Image Encoders -> `c_pool`, `c_vis_local`, `c_vis_global`
    - `c_vis_global` + `c_vis_local` + T5 Text Embeddings -> Dual Prompt (`c_dual`)
  - Backbone: DiT (e.g., SD3). Takes noisy latent `z_t`, timestep `t`, and `c_dual`. Conditioned by Branch 1 output
  - Outputs: `z_0` (HQ latent) -> VAE Decoder -> Restored Image

- **Critical path:**
  1. Input `x_LQ` through `E_dr` to get `z_LQ`
  2. `z_LQ` through lightweight branch to modify DiT Block 0
  3. `z_LQ` decoded to pixel space, cropped for local/global patches
  4. Patches through CLIP encoders + Text through T5 to form `c_dual`
  5. `c_dual` guides all DiT blocks via cross-attention
  6. Denoising loop produces final image

- **Design tradeoffs:**
  - **Efficiency vs. Detail:** Uses lightweight convolutions (Branch 1) instead of a full ControlNet copy for efficiency. Relies on dual prompting (Branch 2) to compensate for detail loss.
  - **Local vs. Global:** Global-local training adds data loading complexity but is crucial for semantic consistency.

- **Failure signatures:**
  - **Loss of Fidelity:** If Branch 2 is misconfigured (e.g., incorrect CLIP feature extraction), the model might generate plausible but hallucinated content.
  - **Training Instability:** If feature alignment (`η`) in Branch 1 is removed, adding lightweight features could destabilize training.
  - **Loss of Global Coherence:** If global-local training is disabled, the model may fail to restore correct semantics for high-res patches.

- **First 3 experiments:**
  1. **Dual Prompting Ablation:** Run restoration with (a) Text Prompts Only, (b) Visual Prompts Only, (c) Dual Prompts. Compare LPIPS/DISTS/CLIPIQA. Expect Dual > Text > Visual.
  2. **Global-Local Ablation:** Train two models, one with "Local" only and one with "Global-Local" visual tokens. Compare restoration quality. Expect Global-Local to be superior.
  3. **Degradation-Robust VAE Check:** Reconstruct LQ images using the fine-tuned `E_dr` and the standard SD3 VAE encoder. Check for smoothing/detail loss in the reconstruction.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How can the visual prompting strategy be optimized to function independently without reliance on textual prompts?
  - Basis in paper: [inferred] Table 5 shows that "Visual Prompts Only" performs significantly worse than "Text Prompts Only" (e.g., CLIPIQA 40.97 vs. 57.08), despite the paper's emphasis on visual cues.
  - Why unresolved: The authors attribute the failure to missing semantic "control signals," but do not explore mechanisms to inject these semantics into the visual tokens directly to remove the dependency on T5 text encoders.
  - What evidence would resolve it: Ablation studies modifying the visual token extraction (e.g., using richer CLIP layers or semantic aggregators) that match the performance of the dual prompting baseline.

- **Open Question 2**
  - Question: What are the computational latency and memory costs of DPIR compared to U-Net based restoration methods?
  - Basis in paper: [inferred] The introduction highlights DiT's "better quality with scalability," and Section 4.1 mentions a "lightweight module," but no efficiency metrics (FLOPs, inference time) are provided.
  - Why unresolved: DiT backbones (like SD3) are typically heavier than U-Nets; the efficiency of the "lightweight" conditioning branch does not account for the cost of the frozen DiT backbone.
  - What evidence would resolve it: Reporting GPU memory consumption and inference time per image compared against baselines like StableSR or SUPIR.

- **Open Question 3**
  - Question: How does the model perform on low-resolution inputs where the global context is identical to the local patch?
  - Basis in paper: [explicit] Section 4.4.2 states that for low-resolution inference, "DPIR can set $x_{global}$ to $x_{local}$," potentially limiting the global-local training utility.
  - Why unresolved: The global-local strategy relies on surrounding context to aid restoration; it is unclear if performance degrades when this contextual distinction is lost in low-res inputs.
  - What evidence would resolve it: A comparative evaluation on standard low-resolution test sets (e.g., DIV2K 256x256) analyzing the impact of forced identical global-local patches.

## Limitations

- The paper's dual prompting strategy, while showing strong results, relies on CLIP image encoders whose effectiveness may be compromised if the LQ image's degradation severely corrupts the visual features.
- The lightweight conditioning branch, though efficient, may not capture sufficiently rich image priors for very complex degradations, potentially limiting its guidance signal.
- The global-local visual training strategy adds complexity and computational cost, and its benefits depend on the relevance of the global context patch; irrelevant context could introduce noise.
- The specific architecture details of the lightweight conditioning module (exact layers, dimensions) are not fully specified, hindering exact replication.
- The training datasets used (a "self-collected" dataset of over 20 million images) are not public, making faithful reproduction challenging.

## Confidence

- **High**: The core methodology of using a DiT backbone with a lightweight conditioning branch and a dual prompting strategy is clearly described and the overall experimental design is sound.
- **Medium**: The specific implementation details of the lightweight conditioning branch and the exact training procedures (beyond the broad strokes provided) have some uncertainty.
- **Medium**: The claimed superiority over state-of-the-art methods is well-supported by quantitative metrics, but the specific contributions of each component (dual prompting, lightweight branch, global-local training) are best understood through the ablation studies.

## Next Checks

1. **Ablation Study on Dual Prompting**: Run the three variations of the dual prompting strategy (Text Only, Visual Only, Dual) on a held-out test set to verify the reported ranking of performance (Dual > Text > Visual) and to quantify the individual contributions.

2. **Ablation Study on Global-Local Training**: Train two models, one with only local visual prompts and one with the full global-local strategy, and compare their restoration quality on high-resolution patches to confirm the benefit of global context for semantic consistency.

3. **Degradation-Robust VAE Validation**: Use the fine-tuned `E_dr` and the standard SD3 VAE encoder to reconstruct a set of LQ images. Visually and quantitatively compare the reconstructions to check for smoothing or detail loss in the degradation-robust version, ensuring it preserves information for restoration.