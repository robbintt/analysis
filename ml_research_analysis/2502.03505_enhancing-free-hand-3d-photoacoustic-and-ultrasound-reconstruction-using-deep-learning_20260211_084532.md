---
ver: rpa2
title: Enhancing Free-hand 3D Photoacoustic and Ultrasound Reconstruction using Deep
  Learning
arxiv_id: '2502.03505'
source_url: https://arxiv.org/abs/2502.03505
tags:
- motion
- b-mode
- images
- ultrasound
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoGLo-Net, a deep learning model for reconstructing
  3D volumes from freehand 2D ultrasound and photoacoustic images without external
  tracking sensors. The model uses a ResNet-based encoder, global-local self-attention
  modules, and correlation volumes to accurately estimate motion between consecutive
  frames.
---

# Enhancing Free-hand 3D Photoacoustic and Ultrasound Reconstruction using Deep Learning

## Quick Facts
- **arXiv ID:** 2502.03505
- **Source URL:** https://arxiv.org/abs/2502.03505
- **Reference count:** 31
- **Primary result:** MoGLo-Net outperforms state-of-the-art methods for sensorless 3D reconstruction from freehand ultrasound, with lower accumulated errors and better trajectory reconstruction.

## Executive Summary
This paper introduces MoGLo-Net, a deep learning model for reconstructing 3D volumes from freehand 2D ultrasound and photoacoustic images without external tracking sensors. The model uses a ResNet-based encoder, global-local self-attention modules, and correlation volumes to accurately estimate motion between consecutive frames. A custom motion-based loss function improves estimation accuracy, especially for fast motions. Experiments on in-house and public datasets show that MoGLo-Net outperforms existing state-of-the-art methods, with lower accumulated errors and better trajectory reconstruction. The approach also extends to 3D visualization of vasculature using Doppler ultrasound and photoacoustic imaging.

## Method Summary
MoGLo-Net estimates 6-DoF relative motion between consecutive B-mode ultrasound frames for sensorless 3D volume reconstruction. The architecture processes pairs of shifted B-mode sequences through a ResNet encoder to generate feature maps, which are then used to compute patch-wise correlation volumes. Global-local self-attention modules weigh image regions based on their similarity to global features, and dual LSTM motion estimators process both local and global features to predict relative motion. The system is trained end-to-end using a custom motion-weighted MAE loss combined with correlation and triplet losses.

## Key Results
- MoGLo-Net outperforms existing state-of-the-art methods for 3D reconstruction from freehand ultrasound
- The approach demonstrates better trajectory reconstruction with lower accumulated errors (aAE) and final drift rates (FDR)
- The method successfully extends to 3D visualization of vasculature using Doppler ultrasound and photoacoustic imaging

## Why This Works (Mechanism)

### Mechanism 1: Correlation Volumes for Motion Estimation
Patch-wise correlation volumes between adjacent frames provide explicit motion-related information that improves both in-plane and out-of-plane motion estimation. The correlation operation extracts region-of-interest patches from feature maps of consecutive frames. One patch remains stationary while the other sweeps the RoI, computing correlations for all spatial pairings. The resulting 3D correlation volume encodes motion signatures: uniform values indicate stationary frames, shifted correlation peaks indicate lateral motion, and overall correlation decay indicates elevational (out-of-plane) motion.

### Mechanism 2: Global-Local Self-Attention for Region Selection
Global-local self-attention selectively weights image regions (fully-developed speckle areas for slow motion, high-echogenic tissues for fast motion) to improve motion estimation accuracy. The module derives local feature blocks and global features, then uses cosine similarity to generate attention weights—patches similar to global semantics receive higher weights. This adaptively emphasizes regions carrying reliable motion cues.

### Mechanism 3: Motion-Weighted Loss for Drift Reduction
Motion-weighted Mean Absolute Error (MMAE) loss reduces accumulated drift by over-weighting sparse fast-motion examples that are harder to estimate. Fast motions are both rarer (creating class imbalance) and harder to estimate (due to reduced frame correlation). MMAE weights each sample by motion magnitude, so high-magnitude motions contribute proportionally more to the loss gradient.

## Foundational Learning

### Concept: Ultrasound Speckle Decorrelation
- **Why needed here:** The entire sensorless reconstruction approach relies on speckle patterns persisting across frames and decorrelating predictably with out-of-plane motion.
- **Quick check question:** If you apply a speckle-reduction filter to B-mode images before feeding them to MoGLo-Net, would you expect performance to improve or degrade? Why?

### Concept: Self-Attention with Query-Key-Value
- **Why needed here:** The global-local attention module uses cosine similarity between local patches and global features as attention weights—a variant of attention mechanism.
- **Quick check question:** In the global-local attention module, what serves as the "query" and what serves as the "key" in the similarity computation?

### Concept: Accumulated Error in Sequential Prediction
- **Why needed here:** Motion estimation is fundamentally sequential—errors compound. The paper prioritizes accumulated metrics because clinical utility depends on end-to-end trajectory accuracy.
- **Quick check question:** If per-frame relative error (rAE) is 0.1mm but accumulated error (aAE) is 17mm after 900 frames, what does this tell you about the error distribution across the sequence?

## Architecture Onboarding

### Component map:
B-mode pair → Encoder1 → Correlation Volume + Encoder2 → [Encoder3 → Encoder4] → Global-Local Attention → LSTM Estimators → 6-DoF relative motion → Cumulative transform → 3D voxel placement

### Critical path:
B-mode pair → ResNet encoder → Correlation volume computation → Global-local attention → Dual LSTM motion estimators → 6-DoF relative motion estimation → Cumulative transform → 3D voxel placement

### Design tradeoffs:
- **Sequence length s:** Longer sequences give LSTM more temporal context but increase memory and inference latency
- **Patch size in correlation:** 4×4 patches balance spatial precision vs. correlation robustness
- **Dual-branch LSTM (global + local):** Adds parameters but allows specialized motion cues
- **Speckle preservation:** Ablation confirms raw/log-compressed B-mode outperforms denoised images

### Failure signatures:
- **High FDR with low rAE:** Indicates systematic bias accumulating over trajectory
- **Good in-plane but poor out-of-plane estimation:** Correlation volume may not be effectively utilized
- **Performance collapse on new datasets:** Overfitting to speckle patterns or scan protocols
- **Attention weights uniformly distributed:** Global-local attention not discriminative

### First 3 experiments:
1. **Reproduce ablation (Table 2) on your data:** Train MoGLo-Net variants without G, without C, without M individually
2. **Input modality stress test:** Compare standard B-mode, log-compressed only, IQ + B-mode concatenated, and speckle-reduced inputs
3. **Trajectory length scaling:** Evaluate FDR vs. scan length (e.g., 200, 400, 600, 800 frames)

## Open Questions the Paper Calls Out

### Open Question 1
Can MoGLo-Net maintain high reconstruction accuracy when applied to anatomical regions beyond the forearm, such as abdominal or thyroid structures? The current validation relies heavily on forearm data which may have specific speckle characteristics not present in other organs.

### Open Question 2
To what extent does integrating anatomical priors or explicit scan sequence ordering improve the estimation of arbitrary, complex motions? The current architecture does not utilize high-level semantic anatomical context or temporal ordering constraints.

### Open Question 3
How does significant pulsatile flow in larger vessels impact the quantitative stability of 3D vascular reconstruction in Photoacoustic and Doppler modes? The success of vascular visualization is demonstrated on stable targets, leaving the method's robustness against hemodynamic variations unproven.

## Limitations
- The paper does not specify critical hyperparameters including loss weights, LSTM sequence length, and exact ResNet architecture parameters
- Performance comparisons rely heavily on in-house datasets with potentially domain-specific speckle patterns
- Ablation studies are internally consistent but lack external validation on clinically diverse datasets

## Confidence

### High confidence:
- Core mechanism of using correlation volumes to encode motion signatures is well-supported by speckle physics

### Medium confidence:
- Global-local attention module's effectiveness is demonstrated through controlled ablations but lacks direct validation on diverse imaging scenarios

### Low confidence:
- MMAE loss formulation's superiority over alternatives is claimed but not experimentally validated against other imbalance-handling approaches

## Next Checks
1. **Domain transfer test:** Evaluate MoGLo-Net on an independent freehand ultrasound dataset with different tissue types and acquisition protocols
2. **Correlation volume ablation under controlled motion:** Systematically vary scan speeds and motion types to quantify correlation volume's contribution
3. **Attention visualization across motion regimes:** Generate and analyze attention maps for sequences with predominantly slow versus fast motion