---
ver: rpa2
title: 'Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs'
arxiv_id: '2504.00048'
source_url: https://arxiv.org/abs/2504.00048
tags:
- date
- year
- sysdate
- extract
- distill-c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Distill-C, a framework for enhancing small NL2SQL
  models through distilled customization with LLMs. It uses large teacher LLMs to
  generate synthetic training data tailored to specific customer use cases, then fine-tunes
  smaller open-source models on this data.
---

# Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs

## Quick Facts
- **arXiv ID:** 2504.00048
- **Source URL:** https://arxiv.org/abs/2504.00048
- **Reference count:** 12
- **Primary result:** 36% average execution accuracy improvement across three LLM families on NL2SQL benchmarks using distilled customization

## Executive Summary
Distill-C is a framework that enhances small NL2SQL models by generating synthetic training data tailored to customer-specific use cases through large teacher LLMs, then fine-tuning smaller open-source models on this data. The approach incorporates customization scenarios (AddRef, LearnPrior, FixIt) and a multi-step filtering pipeline to ensure data quality. Experiments demonstrate significant performance gains over baseline models while maintaining low computational costs.

## Method Summary
The framework generates synthetic (NL, SQL) pairs using large teacher LLMs guided by customer-specific customization information (instructions, reference examples, error cases). These pairs undergo multi-stage filtering (pattern-based, execution-based, LLM jury) before being used to fine-tune smaller student models via supervised fine-tuning. The method supports three customization scenarios: AddRef (example augmentation), LearnPrior (instructional learning), and FixIt (error-driven bootstrapping).

## Key Results
- 36% average improvement in execution accuracy across three LLM families on public benchmarks
- Additional 22.6% gain on three internal customer benchmarks
- Smaller student models (7-8B) match or outperform larger teacher models (70B) on specialized tasks
- Effective handling of SQL dialects and customer-specific business logic

## Why This Works (Mechanism)

### Mechanism 1: Customized Knowledge Distillation
Distilling synthetic data explicitly tailored to customer-specific rules allows smaller models to outperform generalist teachers on niche tasks. Customization Information (LearnPrior instructions and AddRef examples) guides teacher LLMs to generate SQL adhering to specific business logic, which students then learn to replicate.

### Mechanism 2: Multi-Stage Data Sanitization
Filtering synthetic data through execution validation and LLM-based juries creates high-fidelity training signals. Only executable SQL queries pass through pattern matching and execution filters, with LLM juries evaluating semantic relevance, preventing model collapse from noisy data.

### Mechanism 3: Error-Driven Bootstrapping (FixIt)
Using specific model failure cases as seeds for synthetic data generation directly targets competence gaps. Reference failures from baseline models become prompts for teachers to generate correct alternatives, creating targeted curriculum for students.

## Foundational Learning

- **Concept:** Knowledge Distillation (Teacher-Student)
  - **Why needed:** Uses large teacher models to supervise smaller students by mimicking output distributions
  - **Quick check:** Can you explain why a smaller model might outperform a larger teacher after distillation? (Answer: Specialization to distilled domain)

- **Concept:** Synthetic Data Generation
  - **Why needed:** Relies entirely on artificial (Question, SQL) pairs rather than human labels
  - **Quick check:** What is the primary risk of training on synthetic data from another model? (Answer: Model collapse/regression to the mean)

- **Concept:** SQL Dialects & Execution Accuracy
  - **Why needed:** Emphasizes OracleSQL vs SQLite compliance; execution accuracy better than token match
  - **Quick check:** Why is execution accuracy better than token match for NL2SQL? (Answer: Different SQL can produce same results)

## Architecture Onboarding

- **Component map:** Customer Data -> NL Synthesizer -> SQL Synthesizer -> Pattern Matcher -> SQL Executor -> LLM Jury -> Training Loop
- **Critical path:** NL Synthesizer is the strategic bottleneck; irrelevant questions waste downstream training
- **Design tradeoffs:** Decoupled synthesis (NL then SQL) is slower but yields higher diversity vs. joint synthesis
- **Failure signatures:** Model collapse (check bootstrapping diversity), syntax errors (tighten pattern filtering), semantic drift (calibrate jury thresholds)
- **First 3 experiments:**
  1. Run target 7B model on your schema without Distill-C for baseline error rate
  2. Feed 10 customer questions through SQL Synthesizer only, verify Execution Filter catches errors
  3. Train student on synthetic data only for 1 epoch, check if it learns specific Custom Instructions perfectly

## Open Questions the Paper Calls Out
- How to extend framework to preference alignment training (DPO/RLHF) rather than SFT only
- Whether customization scenarios transfer effectively to non-SQL domains like code generation
- Impact on performance if LLM-as-jury filtering introduces noise by approving erroneous data

## Limitations
- Heavy reliance on proprietary customer datasets limits independent verification
- Framework assumes teacher LLM access, creating deployment dependency
- Does not address computational overhead of multi-stage filtering pipeline in production

## Confidence
- **High Confidence:** 36% improvement on public benchmarks is well-supported by experimental design
- **Medium Confidence:** 22.6% gain on internal benchmarks is credible but cannot be independently verified
- **Low Confidence:** Claims about matching teacher performance assume perfect teacher compliance with customization

## Next Checks
1. Replicate core experiments using only Spider/BIRD datasets with public LLM APIs to verify 36% improvement independently
2. Systematically test whether teacher LLMs correctly follow customization instructions by sampling generated SQL
3. Measure end-to-end computational cost of filtering pipeline and compare against training time savings