---
ver: rpa2
title: 'Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach
  to Enhance the Seismic Foundation Model'
arxiv_id: '2511.13800'
source_url: https://arxiv.org/abs/2511.13800
tags:
- components
- low-frequency
- hilbert
- frequency
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of pretraining seismic foundation
  models to effectively capture both high- and low-frequency information in seismogram
  data. Existing Vision Transformer (ViT) models struggle with this due to sequential
  tokenization that disrupts intrinsic patterns.
---

# Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model

## Quick Facts
- **arXiv ID**: 2511.13800
- **Source URL**: https://arxiv.org/abs/2511.13800
- **Reference count**: 40
- **Primary result**: ADATG reduces MAE loss by up to 96.8% compared to standard ViT models for seismic foundation model pretraining

## Executive Summary
This work addresses the challenge of pretraining seismic foundation models to effectively capture both high- and low-frequency information in seismogram data. Existing Vision Transformer (ViT) models struggle with this due to sequential tokenization that disrupts intrinsic patterns. The authors propose ADATG, a novel adaptive two-grid foundation model training strategy that combines spectrum decomposition, hierarchical Hilbert encoding, and an adaptive training approach inspired by the frequency principle. The method decomposes seismograms into high- and low-frequency components, applies coarse-level Hilbert encoding to low frequencies and fine-level encoding to high frequencies, and uses an adaptive training strategy that initially emphasizes low-frequency features before shifting to high-frequency refinement. Experimental results show significant improvements over baseline methods, with the adaptive approach reducing MAE loss by up to 96.8% compared to standard ViT models. The study demonstrates that frequency-aware training strategies informed by seismic data characteristics can substantially enhance the performance of visual seismic foundation models.

## Method Summary
ADATG is a multimodal vision transformer-based seismic foundation model that addresses the challenge of capturing both high- and low-frequency information in seismogram data. The method uses discrete Fourier transform to decompose seismograms into frequency components, applies hierarchical Hilbert encoding at different scales (coarse for low frequencies, fine for high frequencies), and employs an adaptive training strategy that emphasizes low-frequency learning early in training before shifting focus to high-frequency refinement. The model is trained using masked auto-encoding with a time-varying loss function that balances contributions from both frequency bands according to a curriculum learning schedule. The approach is evaluated on USGS, SARIG, and SEG seismic datasets using standard reconstruction metrics including MAE, MSE, PSNR, SSIM, and MS-SSIM.

## Key Results
- ADATG achieves up to 96.8% reduction in MAE loss compared to standard ViT baselines
- Adaptive training strategy with low-to-high frequency curriculum improves convergence speed and final reconstruction quality
- Hierarchical Hilbert encoding preserves spatial locality better than raster-scan tokenization for seismic data
- Frequency decomposition with appropriate threshold selection (k₀=16) enables balanced energy distribution between frequency components

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Decomposed Tokenization with Scale-Appropriate Encoding
- Claim: Separating seismograms into frequency bands and applying appropriately-scaled Hilbert encodings preserves spatial locality better than uniform sequential tokenization.
- Mechanism: DFT decomposes input into high/low frequency components (Eq. 1-3). Low frequencies receive coarse-grid Hilbert encoding (order n₁), high frequencies receive fine-grid encoding (order n₂ > n₁). This matches the spatial scale of each frequency component to the encoding granularity.
- Core assumption: Assumption: High-frequency seismic features require finer spatial resolution in tokenization, while low-frequency features are adequately captured at coarser scales.
- Evidence anchors:
  - [abstract] "employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively"
  - [Section 3.4, Eq. 4] Defines E_low = H_{n1}(X_low), E_high = H_{n2}(X_high) with n1 < n2
  - [corpus] Weak direct validation; corpus neighbors address seismic processing but not this specific encoding strategy
- Break condition: If energy distribution between frequency bands is highly imbalanced (threshold k₀ selection fails), or if seismogram lacks clear frequency separation (homogeneous spectra), coarse/fine grid distinction provides no benefit.

### Mechanism 2: Frequency Principle-Aligned Curriculum Learning
- Claim: Training transformers with a curriculum that emphasizes low-frequency components first, then progressively shifts to high-frequency refinement, accelerates convergence and improves final reconstruction quality.
- Mechanism: Loss function L_adap = α_t * L_high + (1-α_t) * L_low, where α_t = α * t/T (Eq. 8). Early training: (1-α_t) dominates → focus on low frequencies. Later training: α_t increases → shift to high frequencies. This matches the empirically-observed "frequency principle" where neural networks naturally learn low frequencies first.
- Core assumption: Assumption: ViTs follow the same frequency principle established for MLPs/CNNs—low-frequency patterns are learned faster and should be prioritized early.
- Evidence anchors:
  - [abstract] "adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features"
  - [Section 4.5, Figure 5b] Compares Ada-Low-High vs Ada-High-Low; Low-High converges faster and achieves lower final loss
  - [corpus] No direct corpus validation of frequency principle in transformers
- Break condition: If training is too short for curriculum to complete its phase transition, or if the target task requires immediate high-frequency precision (no low-frequency scaffold is useful), curriculum may delay rather than accelerate learning.

### Mechanism 3: Hilbert Curve Spatial Preservation vs. Raster Scan
- Claim: Hilbert space-filling curve tokenization preserves 2D spatial relationships better than raster-scan (row-major) ordering, improving the transformer's ability to capture local seismic patterns.
- Mechanism: Standard ViT tokenizes patches left-to-right, top-to-bottom (Figure 4a), which separates spatially adjacent patches in the sequence. Hilbert curve visits patches in a fractal pattern that keeps spatially proximate patches closer in the token sequence (Figure 4b), improving locality for self-attention.
- Core assumption: Assumption: Seismogram patterns have strong local spatial correlation that benefits from preserved locality in token sequences.
- Evidence anchors:
  - [Section 3.3] "Hilbert encoding, the sequence of embeddings is constructed based on the Hilbert curve sequence; see Figure 4b"
  - [Section 4.2, Figure 5a] HE-ViT achieves faster initial convergence (loss 0.3104 at epoch 200 vs Base ViT)
  - [corpus] Weak; corpus mentions transformers for seismic but not Hilbert encoding specifically
- Break condition: If seismic patterns are globally distributed (no strong local correlation), or if attention mechanism already captures long-range dependencies sufficiently, Hilbert encoding provides marginal gains.

## Foundational Learning

- **Discrete Fourier Transform (DFT) for Spectral Decomposition**
  - Why needed here: Core preprocessing step; splits seismograms into high/low frequency components (Section 3.1, Eq. 1-3). Understanding DFT matrix formulation is essential to grasp how the decomposition works.
  - Quick check question: Can you explain how threshold k₀ determines which frequency components are classified as "high" vs "low"?

- **Space-Filling Curves (Hilbert Curve)**
  - Why needed here: Underlies the alternative tokenization scheme. The recursive construction H_{n+1} = [Rotate(H_n, π/2), H_n, H_n, Rotate(H_n, 3π/2)] determines patch ordering (Definition 3).
  - Quick check question: Why does a Hilbert curve preserve 2D spatial locality better than raster scan ordering?

- **Frequency Principle in Neural Networks**
  - Why needed here: Theoretical justification for adaptive curriculum training. Prior work [32] shows DNNs learn low frequencies faster; this paper extends observation to ViTs (Section 4.5).
  - Quick check question: What would happen if you trained with the reverse curriculum (high-frequency first)?

## Architecture Onboarding

- **Component map**: Input → DFT Spectral Decomposition (k₀ threshold) → High/Low frequency branches → Low-freq branch → Coarse Hilbert encoder (H_{n1}) → ViT Encoder (12 blocks) → Decoder (4 blocks) → Inverse Hilbert decoding → Reconstruction; High-freq branch follows similar path with fine Hilbert encoder (H_{n2})

- **Critical path**:
  1. Threshold selection (k₀ = 16 per Table 2 for balanced energy split)
  2. Hilbert order selection (n₁ < n₂; paper uses orders 3 and 4 as example)
  3. Adaptive schedule α_t = α * t/T over 1600 epochs
  4. Mask ratio 0.75 for MAE pretraining

- **Design tradeoffs**:
  - Lower k₀ → more energy in high-frequency branch → potentially noisier training
  - Higher k₀ → low-frequency dominates → may miss fine details
  - Larger n₂ - n₁ gap → more computation but better high-freq resolution
  - Faster α_t increase → quicker shift to high-freq but may underfit low-freq foundation

- **Failure signatures**:
  - MAE plateaus early without improvement: Check frequency split threshold or adaptive schedule
  - High-freq reconstruction is noisy/oversmoothed: Hilbert order n₂ may be too low
  - Low-freq reconstruction loses structure: Coarse grid n₁ may be too coarse
  - Reconstruction artifacts at epoch ~1300+: Possible overfitting (Figure 12 notes unwarranted details)

- **First 3 experiments**:
  1. **Baseline comparison**: Replicate Table 1 comparing SFM (Base ViT) vs HE-ViT vs ADATG-HH on same 256×256 seismogram slices; verify ~96% MAE reduction claim
  2. **Ablation on k₀ threshold**: Sweep k₀ ∈ {4, 8, 16, 32, 64} and measure reconstruction quality (PSNR, SSIM) to validate energy balance rationale from Table 2
  3. **Curriculum direction test**: Compare Ada-Low-High vs Ada-High-Low (Eq. 8 vs Eq. 9) to verify frequency principle holds for your data distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can ADATG be effectively extended to 3D seismogram data, and what architectural modifications are required?
- **Basis in paper**: [explicit] The conclusion states: "In the future, we would expand ADATG to pre-train a seismic foundation model with the 3D seismogram as input and explore the improvements in the downstream task including full waveform inversion."
- **Why unresolved**: The current method operates exclusively on 2D seismic slices (244×244 or 256×256 pixels). 3D seismograms introduce volumetric complexity requiring new Hilbert curve generalizations and potentially different frequency decomposition strategies.
- **What evidence would resolve it**: Implementation of a 3D Hilbert encoding scheme, experimental results on volumetric seismic datasets (e.g., USGS 3D surveys), and comparative metrics demonstrating performance retention or improvement on downstream tasks like full waveform inversion.

### Open Question 2
- **Question**: What is the optimal or adaptive method for selecting the spectral decomposition threshold $k_0$, beyond manual energy-balancing?
- **Basis in paper**: [inferred] Section 4.3 selects $k_0=16$ heuristically based on $\ell_2$-norm energy distribution between components. The paper notes "an even energy distribution between them is desirable" but provides no adaptive or data-driven selection mechanism.
- **Why unresolved**: Different seismic datasets may have varying frequency characteristics. A fixed threshold may not generalize across geological contexts or data acquisition parameters.
- **What evidence would resolve it**: A systematic ablation study with data-driven or learned threshold selection, validated across multiple seismic datasets from diverse geological settings, showing consistent or improved reconstruction metrics.

### Open Question 3
- **Question**: Does the Frequency Principle (low-to-high frequency learning) generalize to other transformer architectures and attention mechanisms?
- **Basis in paper**: [explicit] Section 4.5 states: "The observations presented in Figure 5b support the frequency principle for ViT blocks," but the experiments use a fixed ViT architecture. The principle's applicability to Swin Transformers, hierarchical attention, or convolution-enhanced transformers remains untested.
- **Why unresolved**: The paper demonstrates the principle for standard ViT with 12 transformer blocks. Different attention patterns (local, shifted window) may alter frequency learning dynamics.
- **What evidence would resolve it**: Controlled experiments applying the adaptive training strategy (Eq. 8) to alternative transformer architectures, measuring reconstruction quality and frequency-specific convergence behavior.

### Open Question 4
- **Question**: Can the adaptive training loss weight function (currently linear decay $\alpha_t = \alpha \cdot t/T$) be replaced with more sophisticated schedules for improved convergence?
- **Basis in paper**: [inferred] The adaptive strategy uses a simple linear decay. Appendix E notes potential overfitting in high-frequency reconstruction at epoch 1300 ("unwarranted details... may be a result of overfitting"), suggesting the transition schedule may require refinement.
- **Why unresolved**: Linear decay may not optimally match the actual frequency learning dynamics of transformers. Non-linear, exponential, or cosine-based schedules could better align with the model's intrinsic learning progression.
- **What evidence would resolve it**: Comparative experiments with alternative decay schedules (cosine, step-wise, exponential), analyzing both reconstruction metrics and temporal dynamics of frequency-specific losses across training epochs.

## Limitations

- **Implementation-specific constraints**: Key parameters including Hilbert curve orders (n₁, n₂) and adaptive loss coefficient (α) are not fully specified, making exact reproduction challenging
- **Methodological assumptions**: The extension of frequency principle from CNNs/MLPs to transformers is assumed but not empirically validated within this work
- **Data preprocessing details**: While reflection padding is mentioned, exact normalization scheme and Hilbert-inverse decoding implementation are not fully specified

## Confidence

- **96.8% MAE reduction claim**: Medium confidence - Based on Table 1 results, but dependent on precise implementation of all three mechanisms working in concert
- **Frequency principle alignment improves convergence**: Medium confidence - Supported by Figure 5b comparison of curriculum directions, but corpus lacks direct validation of this principle for transformers specifically
- **Hilbert encoding improves spatial locality**: Low confidence - Theoretical benefit shown, but quantitative comparison with raster scan tokenization is not provided in the main results

## Next Checks

1. **Ablation study on Hilbert encoding order**: Systematically vary n₁ and n₂ (e.g., n₁∈{2,3,4}, n₂∈{3,4,5}) while keeping adaptive training constant. Measure whether the 96.8% improvement persists when spatial locality preservation is removed.

2. **Curriculum direction validation**: Implement and train both Ada-Low-High and Ada-High-Low variants on the same data split. Quantify whether the Low-High approach consistently converges faster and achieves lower final loss across multiple random seeds.

3. **Frequency threshold sensitivity**: Sweep k₀ values (4, 8, 16, 32, 64) and measure reconstruction quality metrics (PSNR, SSIM). Verify that k₀=16 provides optimal energy balance as claimed, and determine if the improvement is robust to threshold selection.