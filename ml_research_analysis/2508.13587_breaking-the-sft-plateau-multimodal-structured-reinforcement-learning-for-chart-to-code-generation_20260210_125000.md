---
ver: rpa2
title: 'Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for
  Chart-to-Code Generation'
arxiv_id: '2508.13587'
source_url: https://arxiv.org/abs/2508.13587
tags:
- arxiv
- reward
- visual
- data
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the performance plateau problem in chart-to-code
  generation using supervised fine-tuning (SFT). The authors construct a large-scale
  dataset of 3 million chart-code pairs from real-world arXiv tables and systematically
  investigate SFT performance at different data scales.
---

# Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation

## Quick Facts
- arXiv ID: 2508.13587
- Source URL: https://arxiv.org/abs/2508.13587
- Reference count: 11
- 7B model rivals GPT-4o on chart-to-code generation with 96.5% execution rate

## Executive Summary
This paper addresses the performance plateau problem in chart-to-code generation using supervised fine-tuning (SFT). The authors construct a 3 million chart-code pair dataset from arXiv tables and systematically demonstrate that SFT performance saturates regardless of data scale. To break through this plateau, they propose Multimodal Structured Reinforcement Learning (MSRL), a two-stage curriculum that combines textual rewards for code detail accuracy with visual rewards for overall chart structure fidelity. The method significantly outperforms open-source baselines and proprietary models, achieving 96.5% execution rate and 83.8% high-level score on ChartMimic benchmark.

## Method Summary
The approach uses Qwen2.5-VL-7B-Instruct as base model, trained with LLaMA-Factory for SFT (1 epoch, lr=1e-5, batch=32) on 2.8M chart-code pairs from arXiv. For RL, they employ GRPO via MM-EUREKA framework with a two-stage curriculum: Stage 1 (22k samples) uses textual-only rewards (wt=1, wv=0, we=0.5), Stage 2 (11k samples) introduces hybrid rewards (wt=0.5, wv=0.5, we=0.5). Textual rewards combine execution success with rule-based accuracy scoring for data, chart types, layout, titles, and labels. Visual rewards use Qwen2.5-VL-72B-Instruct to score rendered chart similarity across six dimensions. Code normalization is applied before reward computation.

## Key Results
- 96.5% execution rate, 78.6% low-level score, 83.8% high-level score on ChartMimic
- 6.2% and 9.9% improvements over SFT alone on high-level and low-level scores
- Outperforms all open-source models and rivals proprietary GPT-4o
- Two-stage curriculum provides 1.4 percentage point improvement over single-stage hybrid

## Why This Works (Mechanism)

### Mechanism 1: SFT Token Uniformity Limitation
SFT plateaus because it assigns uniform importance to all tokens, failing to prioritize critical low-frequency information in plotting code. Plotting code contains boilerplate snippets (e.g., `plt.plot`) at high frequency while critical details (data values, styling parameters) appear infrequently. The negative log-likelihood objective treats both equally, leaving critical information under-optimized.

### Mechanism 2: Render-and-Compare Visual Feedback Loop
Visual rewards capture structural fidelity that textual rewards miss, enabling optimization of global chart appearance. Generated code is executed to render an image. An MLLM evaluator (Qwen2.5-VL-72B) compares rendered image against ground-truth across six dimensions (chart type, layout, text, data, style, clarity). This closes the perception-action loop between code and visual output.

### Mechanism 3: Two-Stage Curriculum Stabilization
Staged reward introduction prevents training instability when combining textual and visual signals. Stage 1 trains with textual rewards only (wt=1, wv=0, we=0.5), establishing code correctness. Stage 2 introduces visual rewards (wt=0.5, wv=0.5, we=0.5) for visual fidelity. Gradual transition prevents conflicting gradient signals.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: The RL backbone algorithm; samples G responses per input and computes group-normalized advantages to update policy. Quick check: Can you explain why GRPO normalizes advantages within each group rather than globally?

- **Reward Shaping for Structured Outputs**: Combining execution (binary), textual accuracy (weighted rule-based), and visual similarity (model-based) into a unified reward signal. Quick check: How would you determine appropriate weights (wt, wv, we) for a new code generation task?

- **Code Normalization for Reward Computation**: Generated code varies stylistically; normalization maps outputs to canonical form before extracting values for reward calculation. Quick check: What normalization steps would be needed for a JavaScript charting library vs. Python matplotlib?

## Architecture Onboarding

- **Component map**: Base model (Qwen2.5-VL-7B-Instruct) -> SFT trainer (LLaMA-Factory) -> RL framework (MM-EUREKA) -> Evaluator (Qwen2.5-VL-72B) -> Rendering environment

- **Critical path**: Data preparation → Filter 3M corpus to 2.8M SFT + 33k RL subsets → SFT training → RL Stage 1 → RL Stage 2 → Evaluation on ChartMimic/ReachQA

- **Design tradeoffs**: 7B vs. 72B evaluator: 72B provides more reliable visual scores but adds inference cost; RL corpus size (33k): Smaller than SFT (2.8M) to prevent overfitting and encourage exploration; Soft value matching tolerance (±5%): Balances reward sparsity with precision requirements

- **Failure signatures**: Code execution failures: Check normalization pipeline and data format filtering; Visual reward collapse: Evaluator may assign uniform scores; verify prompt and scoring range; Training instability in Stage 2: May indicate reward scale imbalance; check wt/wv/we ratios

- **First 3 experiments**: 1) SFT-only baseline: Train on 2.8M data without RL to confirm plateau exists at your compute scale; 2) Ablate visual reward: Run Stage 1 only to isolate textual reward contribution (expected: ~82 high-level per Table 5); 3) Single-stage hybrid: Train with wt=0.5, wv=0.5 from start to verify two-stage benefit (expected: degraded stability or lower final score)

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: Can the MSRL framework generalize effectively to other structured generation tasks involving dense visual inputs, such as webpage-to-HTML or UI-to-code generation?

- **Evaluator bias and reward hacking**: To what extent does the model-based visual evaluator (Qwen2.5-VL-72B) introduce bias or "reward hacking," where the policy optimizes for the evaluator's specific visual preferences rather than ground-truth accuracy?

- **Model capacity vs. data quality limits**: Is the observed SFT performance plateau a fundamental limitation of the model architecture (7B parameters) or a saturation of the synthetic dataset quality derived from arXiv tables?

## Limitations

- The approach is tightly coupled to Python matplotlib/seaborn ecosystem and may not generalize to other charting libraries
- The 33k RL corpus size is small relative to the 2.8M SFT corpus, raising questions about scalability to more diverse chart types
- Heavy reliance on a separate 72B parameter MLLM evaluator for visual rewards introduces reproducibility and computational concerns

## Confidence

- **SFT performance plateau mechanism**: Medium confidence. The empirical plateau is well-demonstrated, but the causal attribution to token uniformity is inferred rather than experimentally validated through targeted ablations.
- **MSRL performance superiority**: High confidence. The benchmark results show consistent improvements over all baselines, including the proprietary GPT-4o, and the ablation studies support the contribution of each component.
- **Two-stage curriculum necessity**: Medium confidence. While the staging shows clear numerical benefit, the paper doesn't explore whether simpler alternatives could achieve similar stability.

## Next Checks

1. **Token frequency ablation study**: Modify SFT training to use token-level loss weighting that emphasizes low-frequency plotting parameters versus high-frequency boilerplate. Compare performance to standard SFT to directly test whether token uniformity is the primary bottleneck.

2. **Single-stage RL with reward scheduling**: Replace the two-stage curriculum with a single-stage approach that uses a linear schedule to increase visual reward weight from 0 to 0.5 over the first 50% of training. Compare stability and final performance to validate whether staging is necessary versus other forms of reward transition.

3. **Cross-library generalization test**: Apply the trained model to JavaScript charting libraries (Chart.js, D3.js) by modifying the code normalization and reward extraction procedures. Measure execution rate and similarity scores to assess whether the approach generalizes beyond matplotlib/seaborn or is tightly coupled to Python plotting conventions.