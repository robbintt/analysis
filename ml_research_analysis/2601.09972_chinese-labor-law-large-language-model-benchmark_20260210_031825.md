---
ver: rpa2
title: Chinese Labor Law Large Language Model Benchmark
arxiv_id: '2601.09972'
source_url: https://arxiv.org/abs/2601.09972
tags:
- legal
- labor
- answer
- case
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a specialized large language model (LLM)
  for Chinese labor law, addressing the gap in domain-specific legal AI. The authors
  develop LabourLawLLM through supervised fine-tuning of Qwen2.5-7B using a curated
  labor law dataset and create LabourLawBench, a comprehensive benchmark covering
  12 task types across 12 case categories.
---

# Chinese Labor Law Large Language Model Benchmark

## Quick Facts
- arXiv ID: 2601.09972
- Source URL: https://arxiv.org/abs/2601.09972
- Reference count: 40
- LabourLawLLM achieves aggregate score of 0.68, outperforming general LLMs (0.61) and existing legal models (0.29)

## Executive Summary
This paper introduces LabourLawLLM, a specialized large language model for Chinese labor law, developed through supervised fine-tuning of Qwen2.5-7B on a curated dataset of 51,236 labor law instruction instances. The authors create LabourLawBench, a comprehensive benchmark spanning 12 task types across 12 case categories, to evaluate both their model and existing legal LLMs. Evaluation demonstrates that domain-specific fine-tuning substantially improves performance on labor law tasks, with LabourLawLLM achieving an aggregate score of 0.68 compared to 0.61 for general-purpose LLMs and 0.29 for existing legal-domain models.

## Method Summary
The authors develop LabourLawLLM through supervised fine-tuning of Qwen2.5-7B using LoRA parameter-efficient fine-tuning on 51,236 labor law instruction instances formatted as Instruction-Question-Answer triplets. Training employs 4× NVIDIA RTX 4090 GPUs, runs for 8 epochs with an effective batch size of 256, and takes approximately 22 days. The LabourLawBench benchmark evaluates models across 12 task types (T1-T12) spanning knowledge retrieval, classification, extraction, and reasoning, with task-specific metrics including ROUGE-L, accuracy, F1, Soft-F1, and GPT-o1 judging. The benchmark covers 12 case categories from social security to collective bargaining.

## Key Results
- LabourLawLLM achieves aggregate score of 0.68, outperforming general-purpose LLMs (0.61) and existing legal-domain models (0.29)
- Model demonstrates particularly strong performance on knowledge-centric tasks (T1-T4) with aggregate score of 0.79
- LabourLawLLM maintains low abstention rates across all case categories, suggesting domain training improves both accuracy and willingness to answer legally-framed queries

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific supervised fine-tuning on curated labor-law instructions substantially improves specialized legal task performance over general-purpose baselines. By exposing the model to 51,236 instruction-tuning instances covering statute recall, exam-style Q&A, case classification, and legal reasoning—each formatted with explicit role assignment and standardized output constraints—the model learns to map labor-law facts to legally correct responses while adhering to task-specific output formats.

### Mechanism 2
Unified instruction-question-answer formatting with strict output templates improves structured extraction and classification accuracy in legal contexts. Each task enforces constrained generation (e.g., "[Correct Answer]C<eoa>" for multiple-choice, "[Category]Wage Recovery Dispute<eoa>" for classification) that reduces parsing errors and aligns model outputs with automated evaluation scripts.

### Mechanism 3
Domain-specific training reduces refusal/abstention rates on legally-framed queries while maintaining or improving accuracy. General-purpose models often refuse legally-sensitive queries due to safety alignment; exposure to labor-law SFT data appears to shift the model's willingness-to-answer threshold for domain-specific prompts without inducing harmful outputs.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) with Instruction Tuning**
  - Why needed here: The entire approach relies on fine-tuning a general LLM (Qwen2.5-7B) on domain-specific instruction-response pairs to specialize it for labor-law tasks.
  - Quick check question: Can you explain how SFT differs from continued pretraining, and why instruction format matters for task alignment?

- **Concept: Legal Task Taxonomy (Classification, Extraction, Generation, Reasoning)**
  - Why needed here: The benchmark spans 12 diverse task types (T1–T12), each requiring different evaluation approaches (accuracy, F1, ROUGE-L, GPT-based judging).
  - Quick check question: What metric would you use for multi-label welfare compensation prediction, and why might F1 be preferable to accuracy?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The authors use LoRA rather than full-parameter fine-tuning, which affects training cost, memory requirements, and potential for catastrophic forgetting.
  - Quick check question: What are the tradeoffs between LoRA and full fine-tuning for domain specialization, particularly regarding knowledge retention vs. adaptation?

## Architecture Onboarding

- **Component map:** Qwen2.5-7B (base model) -> LoRA fine-tuning -> 51,236 labor-law instruction instances -> 12-task × 12-category evaluation
- **Critical path:** 1. Data curation → 2. Format standardization → 3. LoRA fine-tuning → 4. Task-specific evaluation → 5. Aggregate scoring across all 12 tasks
- **Design tradeoffs:** 7B scale balances computational efficiency with capacity; LoRA reduces training cost but may limit domain knowledge integration depth; strict output formats enable automated evaluation but may penalize valid non-canonical responses
- **Failure signatures:** Low ROUGE-L with high task understanding (T10); high abstention in baselines; mixed extraction performance (T7–T9)
- **First 3 experiments:**
  1. Reproduce LabourLawLLM training on Qwen2.5-7B using LoRA with the 51,236-example SFT dataset; validate aggregate score approaches 0.68
  2. Ablate by task group: Train separate models on knowledge-centric tasks (T1–T4) vs. extraction/reasoning tasks (T7–T12) to measure task-specific gains
  3. Evaluate format sensitivity: Compare strict output-format enforcement vs. free-form generation with post-hoc parsing to quantify format-induced penalties in T10

## Open Questions the Paper Calls Out

1. **Metric Design for Legal Text**: How can evaluation metrics be designed to balance statutory precision against explanatory thoroughness in legal text generation tasks? The authors note that for scenario-based statute prediction (T10), LabourLawLLM's concise citations are penalized by ROUGE-L, which rewards longer rationales.

2. **Cross-Domain Transfer**: To what extent does domain-specific fine-tuning on labor law transfer to other specialized legal subfields (e.g., tax, IP, criminal law)? The authors claim their methodology provides "a scalable approach for building specialized LLMs in other legal subfields" but only validate on labor law.

3. **Temporal Robustness**: How robust is LabourLawLLM to temporal changes in labor regulations and statutes? The authors list "assess temporal robustness as statutes evolve" as a priority, indicating the current model has not been evaluated under legal changes.

4. **Geographic Generalization**: Does the model trained on Jiangsu Province court decisions generalize to labor disputes from other Chinese jurisdictions with different local interpretations or procedural norms? All case data is sourced exclusively from Jiangsu Province.

## Limitations

- **Data Transparency**: Exact LoRA hyperparameters and optimizer schedules are not specified, creating moderate barriers to faithful reproduction
- **Format-Metric Mismatch**: ROUGE-L sensitivity in T10 scenario-based statute prediction suggests format-metric mismatches may penalize concise, legally accurate responses
- **Safety Evaluation Gap**: While demonstrating lower abstention rates, the paper does not audit for potential over-confidence or hallucination in domain-specific outputs

## Confidence

- **High Confidence**: Aggregate performance improvement claim (0.68 vs 0.61 general LLMs, 0.29 existing legal models) is well-supported by benchmark methodology
- **Medium Confidence**: Mechanism attributing gains to domain knowledge acquisition is plausible but not definitively proven without ablation studies
- **Low Confidence**: Claims about practical utility for real-world legal applications require external validation beyond controlled benchmark environment

## Next Checks

1. **Format Sensitivity Analysis**: Systematically vary output format strictness in T10 to quantify penalties imposed by ROUGE-L on concise legal citations versus longer paraphrases

2. **Safety-Aware Reliability Audit**: Implement adversarial testing on legally-sensitive prompts across all 12 case categories to measure hallucination rates and safety compliance

3. **Cross-Domain Transfer Evaluation**: Test LabourLawLLM on non-labor legal domains using the CLaw benchmark to determine whether domain specialization improves general legal reasoning or creates knowledge silos