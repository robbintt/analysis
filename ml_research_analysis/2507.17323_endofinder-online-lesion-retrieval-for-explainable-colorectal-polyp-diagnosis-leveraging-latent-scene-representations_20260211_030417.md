---
ver: rpa2
title: 'EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis
  Leveraging Latent Scene Representations'
arxiv_id: '2507.17323'
source_url: https://arxiv.org/abs/2507.17323
tags:
- scene
- polyp
- image
- retrieval
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EndoFinder, a retrieval-based framework for
  explainable colorectal polyp diagnosis that leverages multi-view scene representations.
  The key idea is to treat each polyp as a 3D "scene" and learn a unified latent representation
  by fusing multiple endoscopic views, enabling interpretable diagnosis through retrieval
  of similar historical cases.
---

# EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations

## Quick Facts
- arXiv ID: 2507.17323
- Source URL: https://arxiv.org/abs/2507.17323
- Reference count: 8
- Primary result: Retrieval-based explainable diagnosis outperforms existing methods with 4x speedup via hash-based indexing

## Executive Summary
EndoFinder introduces a retrieval-based framework for explainable colorectal polyp diagnosis by treating each polyp as a 3D "scene" and learning unified latent representations from multiple endoscopic views. The method leverages self-supervised learning (contrastive learning and reconstruction) with a Scene Representation Transformer to capture robust features without extensive labeled data. Evaluated on public and newly collected polyp datasets, EndoFinder achieves superior re-identification and pathology classification accuracy while providing interpretable diagnoses through retrieval of similar historical cases. The framework addresses the "black box" nature of deep learning models and offers a promising direction for more efficient, trustworthy AI-driven colonoscopy workflows.

## Method Summary
EndoFinder is a retrieval-based framework that learns robust latent scene representations from multi-view endoscopic images for explainable colorectal polyp diagnosis. The method consists of two main stages: (1) a self-supervised image encoder trained on 17,969 polyp images using contrastive learning and masked autoencoder reconstruction, with masking guided by segmentation masks to focus on lesion regions; and (2) a scene representation transformer that fuses features from multiple views into a unified representation using self-attention and cross-attention reconstruction. The resulting embeddings are discretized through hashing for efficient retrieval, enabling real-time diagnosis by finding the most similar historical cases and voting on their labels. The framework is evaluated on both public and newly collected polyp datasets, demonstrating superior performance in re-identification and pathology classification while maintaining interpretability.

## Key Results
- EndoFinder outperforms existing methods in both re-identification (mAP) and pathology classification accuracy on polyp datasets
- The framework achieves a 4x speedup in retrieval through hash-based indexing while maintaining interpretability
- The method successfully captures robust features without requiring large-scale annotated data through self-supervised pre-training

## Why This Works (Mechanism)

### Mechanism 1: Self-Supervised Pre-training with Guided Masking
The Polyp-aware Image Encoder uses a dual-objective strategy: contrastive learning (SimCLR-style) pulls augmented views of the same polyp closer in embedding space while pushing unrelated images apart, while a generative masked autoencoder (MAE) branch forces the model to reconstruct missing patches. Crucially, the masking is guided by segmentation masks to preserve the lesion region, ensuring the model focuses on diagnostically relevant tissue rather than background noise. This combination of global contrastive separation and local reconstruction of masked regions forces the model to encode high-level semantic structure and low-level texture simultaneously.

### Mechanism 2: Multi-View Scene Representation via Transformer Fusion
The Scene Representation Transformer aggregates feature embeddings from multiple endoscopic viewpoints by using a "Scene Token" that attends to all view embeddings via self-attention. This is regularized by a cross-attention reconstruction task, where the model must reconstruct a target view given the scene representation and visible tokens, implicitly learning geometric consistency. This mechanism assumes that polyps possess consistent 3D geometric structures that can be disentangled from specific 2D viewing angles, and fusing these views resolves ambiguities present in single frames.

### Mechanism 3: Retrieval-Based Diagnosis with Hashing
Instead of mapping images directly to a label (inductive), the system maps them to a binary hash code (transductive). A K-Nearest Neighbors (KNN) search using Hamming distance retrieves historical cases. The final diagnosis is a majority vote of the retrieved neighbors' labels. This effectively treats the database as the "knowledge base," making the decision process transparent by showing the "digital twins." The hashing layer enables 4x faster retrieval but causes a reported ~6.7% drop in mAP compared to floating-point search.

## Foundational Learning

- **Concept: Contrastive Learning (SimCLR/InfoNCE)**
  - **Why needed here:** This is the engine of the Image Encoder. You must understand how augmentations create "positive pairs" and how the loss function maximizes agreement between them while minimizing agreement with "negatives" to learn invariant features.
  - **Quick check question:** If two augmented views of the same polyp are fed into the network, should their cosine similarity increase or decrease according to the InfoNCE loss?

- **Concept: Attention Mechanisms & Transformers**
  - **Why needed here:** The Scene Encoder relies entirely on self-attention to fuse multiple views and cross-attention to reconstruct them. You need to grasp how Query, Key, and Value matrices allow the model to "search" for relevant information across different viewpoints.
  - **Quick check question:** In the Scene Representation Transformer, what specific token aggregates the information from all the different view embeddings?

- **Concept: Semantic Hashing**
  - **Why needed here:** To understand how the system achieves the reported 4x speedup. You need to know how sign-based quantization converts continuous vectors into binary codes for ultra-fast Hamming distance calculations.
  - **Quick check question:** Why is Hamming distance computationally cheaper to calculate than Euclidean distance in large databases?

## Architecture Onboarding

- **Component map:** Input multi-view images -> Image Encoder (ViT-L/16) -> Scene Encoder (Transformer) -> Hashing Layer -> Ball Tree Retrieval -> K-NN Decision

- **Critical path:**
  1. **Stage 1 (Pre-training):** Train Image Encoder on Polyp-18k using Contrastive + MAE loss
  2. **Stage 2 (Scene Training):** Freeze Image Encoder. Train Scene Encoder + Decoder on PolypScene-2k
  3. **Stage 3 (Inference):** Hash query -> Retrieve Neighbors -> Vote

- **Design tradeoffs:**
  - **Accuracy vs. Interpretability:** The system trades the potential higher abstraction of a direct classifier for the transparency of showing reference images
  - **Speed vs. Precision:** The hashing layer enables 4x faster retrieval but causes a reported ~6.7% drop in mAP compared to floating-point search
  - **CNN vs. ViT:** The paper notes the Scene Encoder works well with ViT features but decreased performance with CNN features, suggesting the fusion mechanism is architecture-sensitive

- **Failure signatures:**
  - **"Digital Twin" Failure:** Retrieving visually similar polyps that have different pathologies (violating the core assumption)
  - **Mask Leakage:** If segmentation masks are poor, the Image Encoder may reconstruct background noise, diluting the polyp features
  - **View Sparsity:** If fewer than 2-3 views are provided, the "Scene" representation degrades to near single-view performance

- **First 3 experiments:**
  1. **Image Encoder Validation:** Run single-view re-identification on Polyp-Twin to verify the self-supervised pre-training creates discriminative features before attempting multi-view fusion
  2. **Ablation on Views:** Using PolypScene-250, incrementally add query/reference views (1-view vs 2-view vs 4-view) to quantify the performance gain from the Scene Encoder vs. simple averaging
  3. **Hashing Threshold Analysis:** Test retrieval accuracy vs. speed on PolypScene-80 by varying the hash code length or distance threshold to find the optimal operating point for real-time use

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the polyp-aware image encoder be effectively trained using weakly-supervised or unsupervised methods to eliminate the dependency on manual polyp segmentation masks?
- **Basis in paper:** The Discussion section identifies the reliance on segmentation masks as a limitation because "segmentation errors could propagate," and explicitly calls for future work to explore "weakly-supervised or unsupervised methods to relax this requirement."
- **Why unresolved:** The current framework utilizes masks to guide the masking process in the reconstruction task, forcing the model to focus on lesion regions; it is untested whether robust features can be learned without this spatial guidance.
- **What evidence would resolve it:** Demonstration of comparable re-identification (mAP) and classification accuracy on the PolypScene-250/80 benchmarks using an encoder trained without pixel-level mask annotations.

### Open Question 2
- **Question:** How does the Scene Representation Transformer perform when the number of available input views is inconsistent or sparse, and can novel view synthesis effectively compensate for missing perspectives?
- **Basis in paper:** The authors state that a challenge is "the need for multiple views, which may not always be captured in routine practice," and suggest developing methods to "work with a variable number of views or even synthesize novel viewpoints."
- **Why unresolved:** The current evaluation assumes a relatively structured acquisition of views (e.g., four views per polyp in PolypScene-250), whereas real-world colonoscopy workflows are often unstructured and may provide fewer distinct angles.
- **What evidence would resolve it:** Experiments evaluating retrieval accuracy on the PolypScene datasets where input views are randomly dropped to simulate sparse data, combined with qualitative assessment of synthesized views filling the gaps.

### Open Question 3
- **Question:** Does the integration of non-visual clinical data (e.g., patient history or electronic health records) into the latent scene representation improve the discriminative power of the retrieval system?
- **Basis in paper:** The paper concludes by proposing that future research could "incorporate additional data modalities, such as patient history or electronic health records, to further refine diagnostic predictions."
- **Why unresolved:** The current EndoFinder framework relies exclusively on visual features from endoscopic images, potentially missing contextual clinical factors that influence diagnosis.
- **What evidence would resolve it:** A multi-modal implementation where patient metadata is fused with the visual scene embedding, showing a statistically significant improvement in pathology classification AUC over the visual-only baseline.

## Limitations

- The framework requires accurate polyp segmentation masks for guided masking, and segmentation errors could propagate through the learning pipeline
- The method depends on having multiple views of each polyp, which may not always be captured in routine clinical practice
- The current implementation relies exclusively on visual features and does not incorporate potentially valuable clinical context such as patient history or electronic health records

## Confidence

- **High confidence:** The self-supervised learning pipeline (contrastive + masked autoencoder) is well-grounded in established literature and the paper provides clear implementation details
- **Medium confidence:** The multi-view scene representation mechanism is plausible and methodologically sound, but lacks direct ablation studies in the provided results to conclusively prove its superiority over simpler fusion methods
- **Low confidence:** The "4x speedup" efficiency claim is based on the introduction of hashing but lacks a detailed ablation or benchmark in the main results section to isolate the speedup from other factors

## Next Checks

1. **Data Availability Check:** Verify the accessibility of Polyp-18k and PolypScene datasets (2k and 250 subsets) from the GitHub repository or through author contact before attempting full reproduction
2. **Single-View Baseline Validation:** Run the EndoFinder-I (Image Encoder only) on Polyp-Twin to confirm it achieves the reported mAP (~0.67) before proceeding to the multi-view EndoFinder-S, ensuring the self-supervised pre-training is functioning correctly
3. **View Ablation Study:** Conduct an experiment on PolypScene-250 incrementally adding query/reference views (1→2→4) to quantify the exact performance gain from the Scene Encoder compared to a simple average of single-view embeddings, validating the "scene representation" hypothesis