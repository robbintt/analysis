---
ver: rpa2
title: 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward
  Learning'
arxiv_id: '2508.18966'
source_url: https://arxiv.org/abs/2508.18966
tags:
- style
- generation
- subject-driven
- style-driven
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces USO, a unified framework for subject-driven
  and style-driven image generation. The core idea is to unify both tasks through
  cross-task co-disentanglement, leveraging the complementarity between them to achieve
  better content-style separation.
---

# USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning

## Quick Facts
- arXiv ID: 2508.18966
- Source URL: https://arxiv.org/abs/2508.18966
- Authors: Shaojin Wu; Mengqi Huang; Yufeng Cheng; Wenxu Wu; Jiahe Tian; Yiming Luo; Fei Ding; Qian He
- Reference count: 40
- One-line primary result: Achieves SOTA on both style similarity (CSD: 0.557) and subject consistency (DINO: 0.793, CLIP-I: 0.623) in unified subject-style generation

## Executive Summary
This paper introduces USO, a unified framework for subject-driven and style-driven image generation. The core idea is to unify both tasks through cross-task co-disentanglement, leveraging the complementarity between them to achieve better content-style separation. USO introduces a cross-task triplet curation framework, style-alignment training, content-style disentanglement training, and a style reward learning paradigm. The method is evaluated on a newly constructed USO-Bench benchmark, showing state-of-the-art performance on both subject consistency (DINO: 0.793, CLIP-I: 0.623) and style similarity (CSD: 0.557) tasks. The unified model also outperforms task-specific baselines on style-subject-driven generation. The approach demonstrates strong generalization and disentanglement capabilities across diverse scenarios.

## Method Summary
USO is a unified framework for subject-driven and style-driven image generation that leverages cross-task co-disentanglement. The method employs separate SigLIP and VAE encoders for style and content feature extraction, followed by a multi-scale hierarchical projector. Training occurs in two stages: Stage 1 (Style Alignment) freezes the backbone and trains the projector on style-image/target pairs; Stage 2 (Content-Style Disentanglement) freezes the projector and trains the DiT on triplets of content, style, and target images. Style Reward Learning (SRL) provides explicit optimization pressure for style fidelity by backpropagating style-similarity rewards through the diffusion sampling trajectory. The framework is evaluated on a newly constructed USO-Bench benchmark, showing state-of-the-art performance on both subject consistency and style similarity tasks.

## Key Results
- Achieves SOTA on USO-Bench: DINO 0.793, CLIP-I 0.623, CSD 0.557
- Unified model outperforms task-specific baselines on style-subject-driven generation
- Cross-task co-disentanglement improves content-style separation (CLIP-I: 0.623 vs 0.594 without disentangled encoders)
- Style Reward Learning significantly boosts style fidelity (CSD: 0.557 vs 0.413 without SRL)

## Why This Works (Mechanism)

### Mechanism 1
Jointly training style-driven and subject-driven tasks enables mutually-reinforcing disentanglement, where learning to include features for one task improves learning to exclude them for the complementary task. Cross-task co-disentanglement pairs tasks with opposing objectives—style-driven generation includes style while excluding subject appearance; subject-driven generation does the inverse. The shared model learns sharper feature boundaries through this contrastive pressure. Core assumption: Style and content features are sufficiently separable in the model's latent space for the contrastive signal to be meaningful. Evidence: [abstract] "unify both tasks through cross-task co-disentanglement, leveraging the complementarity between them to achieve better content-style separation"; [section 1, p.3-4] "learning to include certain features in one task inherently informs and enhances the process of learning to exclude those same features in a complementary task"; [corpus] Neighbor papers focus on single-task disentanglement; cross-task mutual reinforcement is not demonstrated elsewhere, suggesting this is a novel claim requiring independent validation. Break condition: If style and subject features share significant latent overlap (e.g., style heavily constrains subject geometry), contrastive signal weakens and both tasks degrade.

### Mechanism 2
Using separate encoders for style (SigLIP with hierarchical projector) and content (frozen VAE) explicitly disentangles feature extraction pathways, reducing content leakage during stylization. SigLIP provides semantic features for abstract style cues (high-level semantics for geometric deformation, low-level for brushstrokes), projected via hierarchical multi-scale projector. VAE provides spatial content tokens with UnoPE positional encoding. Separate encoders prevent the model from conflating which features belong to which conditioning stream. Core assumption: SigLIP embeddings capture style-relevant features more effectively than VAE latents for style conditioning. Evidence: [section 3.3.1, p.6] "we argue that style is a more abstract cue demanding richer semantic information. Therefore, we employ the semantic encoder SigLIP instead of the VAE"; [section 5.3, p.13, Table 3] Removing disentangled encoders (w/o DE) drops CLIP-I from 0.623 to 0.594 and CSD from 0.495 to 0.382; [corpus] Corpus does not directly validate SigLIP vs VAE for style encoding; this is a design choice supported by internal ablation only. Break condition: If style and content require correlated features (e.g., 3D cartoon style affects both texture and geometry), disentangled encoders may underfit compared to joint encoders.

### Mechanism 3
Style Reward Learning (SRL) provides explicit optimization pressure for style fidelity by backpropagating style-similarity rewards through the diffusion sampling trajectory. During training, SRL samples a timestep t, denoises to predict x0, computes reward loss as negative CSD score between predicted image and style reference, then backpropagates combined LPre + λLSRL. This guides the model to prioritize style-relevant features during generation. Core assumption: CSD scores reliably capture human-perceived style similarity and provide meaningful gradients. Evidence: [section 3.3.3, p.7-8] "we define the reward score as the style similarity between the reference style image and the generated stylized image, measured by either a VLM-based filter or the CSD model"; [section 5.1, p.13, Table 3] w/o SRL drops CSD from 0.495 to 0.413; CLIP-I and CLIP-T also decline; [corpus] Related work on RL for subject-driven generation (FMR=0.52 neighbor) shows similar reward-based approaches but does not validate cross-task reward transfer. Break condition: If reward model (CSD) is misaligned with target style dimensions, SRL optimizes wrong objective and may harm subject consistency.

## Foundational Learning

- Concept: Multi-modal DiT attention (MM-DiT)
  - Why needed here: USO builds on FLUX's MM-DiT architecture where text, style, and content tokens are concatenated in shared attention.
  - Quick check question: Can you explain how concatenating [zs, c, zt, zc] in a single attention operation enables cross-modal conditioning?

- Concept: Flow matching in diffusion models
  - Why needed here: USO uses flow-matching objective (LPre) as base loss, with velocity prediction vθ(xt, t).
  - Quick check question: What is the relationship between velocity prediction and the denoising trajectory in flow matching?

- Concept: Content-style disentanglement
  - Why needed here: The core hypothesis is that disentanglement is improved by cross-task training; understanding prior disentanglement approaches (CSGO, DEADiff) contextualizes this contribution.
  - Quick check question: Why does content leakage occur when using a single encoder for both style and content images?

## Architecture Onboarding

- Component map:
  - SigLIP encoder → Hierarchical Projector → style tokens zs (same positional indices as text)
  - VAE encoder → content tokens zc with UnoPE diagonal layout
  - MM-DiT blocks: receive z2 = Concatenate(zs, c, zt, zc)
  - Style Reward Model (CSD or VLM filter): provides LSRL gradient signal

- Critical path:
  1. Stage 1 (Style Alignment): Freeze DiT, train Hierarchical Projector on style-image/target pairs for ~23K steps
  2. Stage 2 (Content-Style Disentanglement): Freeze Projector, unfreeze DiT, train on triplets for ~21K steps
  3. SRL fine-tuning: Enable reward loss (λ=1) after step S in both stages

- Design tradeoffs:
  - Hierarchical projector vs single-layer resampler: Table 4 shows hierarchical achieves 0.402 CSD vs 0.336 for depth-1 resampler
  - LoRA rank 128: Enables efficient fine-tuning but may limit full model capacity
  - Separate encoders: Improves disentanglement but increases inference complexity

- Failure signatures:
  - Content leakage: Style image's subject appears in output → check if style encoder is receiving content images by mistake
  - Style weakness: Output appears photorealistic despite style reference → verify SAT completed and λLSRL enabled after step S
  - Identity loss: Subject features degraded → confirm VAE encoder is frozen and UnoPE positions are correct

- First 3 experiments:
  1. Reproduce SAT ablation (Table 3, w/o SAT): Train without Stage 1 style alignment, verify CSD drops to ~0.409 and CLIP-T degrades on subject-driven tasks.
  2. Test disentangled vs single encoder: Replace SigLIP+VAE with single VAE for both inputs, confirm content leakage in qualitative outputs (Figure 11 pattern).
  3. Validate SRL timing: Vary reward step S (16K vs 18K vs 20K) and plot CSD/CLIP-I curves to find optimal alignment point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cross-task co-disentanglement paradigm scale beyond two tasks to simultaneously unify subject-driven, style-driven, and additional reference-guided generation tasks (e.g., depth-controlled, pose-guided) within a single model?
- Basis in paper: [inferred] The paper unifies two complementary tasks (style and subject) through a specifically designed dual-encoder architecture and triplet curation. The method's design is inherently pairwise, and no experiments explore extending to three or more conditioning modalities.
- Why unresolved: The disentangled encoder architecture and triplet formulation are structured for binary task complementarity. Adding more tasks would require rethinking the data curation, training objectives, and potentially the encoder design.
- What evidence would resolve it: Experiments extending USO to a third conditioning modality (e.g., depth or pose), with appropriate triplet/quadruplet curation and unified benchmark evaluation showing mutual benefits across all three tasks.

### Open Question 2
- Question: How robust is USO's performance to errors or biases in the stylization and de-stylization expert models used for triplet curation?
- Basis in paper: [inferred] Section 3.2 describes constructing triplets using two expert models (stylized expert and de-stylized expert) trained on UNO. The quality of these experts directly determines training data quality, but no sensitivity analysis or error propagation study is provided.
- Why unresolved: The expert models may introduce systematic biases (e.g., incomplete de-stylization, style overfitting) that could propagate into USO's learned representations. The paper does not analyze how expert model imperfections affect final performance.
- What evidence would resolve it: Ablation studies varying expert model quality (e.g., using weaker or differently trained experts) and measuring downstream USO performance, or analysis of failure modes attributable to expert model errors.

### Open Question 3
- Question: Does the Style Reward Learning (SRL) paradigm introduce a fundamental trade-off between style fidelity and text controllability when prompts specify content that conflicts with style reference features?
- Basis in paper: [inferred] SRL optimizes style similarity via reward signals, while flow-matching preserves generative capability. Table 1 shows CLIP-T scores that are competitive but not dominant, suggesting potential trade-offs. No analysis examines edge cases where style and text prompts conflict.
- Why unresolved: The λ=1 setting after step S fully activates style reward, which may override text guidance in conflicting scenarios. The paper does not examine how SRL affects the balance between style adherence and prompt following.
- What evidence would resolve it: Controlled experiments with deliberately conflicting style-text pairs (e.g., style reference with warm colors paired with prompts specifying cold-colored scenes), measuring both CSD and CLIP-T across varying λ schedules.

### Open Question 4
- Question: How does the computational overhead of Style Reward Learning scale with image resolution and batch size compared to standard flow-matching training?
- Basis in paper: [inferred] Algorithm 1 shows SRL requires reverse-step updates (lines 5-8) and intermediate image decoding (line 10) for each training sample before gradient computation. No timing or computational analysis is provided in the implementation details or experiments.
- Why unresolved: The gradient-free inference phase adds iterations per sample, potentially multiplying training time. The practical deployability of SRL at scale remains unclear.
- What evidence would resolve it: Wall-clock training time comparisons between USO with and without SRL across different resolutions (768 vs. 1024) and batch sizes, with FLOP analysis of the SRL overhead component.

## Limitations

- Generalizability across domains: The method's performance on non-photorealistic styles and diverse subject domains (e.g., 3D rendered characters, medical imagery) remains untested.
- Reward model reliability: CSD as the primary reward signal is not validated against human perceptual studies.
- Computational overhead: Separate SigLIP and VAE encoders increase inference latency compared to unified encoders.
- Dataset construction bias: The 200k triplet dataset relies on stylized pairs from licensed datasets and SOTA models, introducing potential bias toward common styles and subjects.

## Confidence

- **High confidence**: The baseline performance metrics (DINO 0.793, CLIP-I 0.623, CSD 0.557) are directly supported by USO-Bench evaluations and Table 1 comparisons.
- **Medium confidence**: The claim that cross-task co-disentanglement improves feature separation is supported by internal ablations (Table 3) but lacks external validation against other multi-task training paradigms.
- **Low confidence**: The assertion that USO's unified model outperforms task-specific baselines on style-subject-driven generation requires independent reproduction.

## Next Checks

1. **Cross-domain generalization test**: Evaluate USO on a held-out dataset of 3D rendered characters and medical imagery. Compare subject consistency (DINO/CLIP-I) and style similarity (CSD) against a strong style-specific baseline like DeepDream. Expect CSD to remain stable but DINO to drop >0.1 if cross-task co-disentanglement degrades on non-photorealistic subjects.

2. **Human perceptual study**: Conduct a crowdsourced study (n=100 participants) comparing USO outputs against GroundTruth and baselines on style-subject-driven generation. Use paired comparisons with 5-point Likert scales for style fidelity and subject preservation. If human ratings diverge significantly from CSD scores (e.g., CSD=0.557 but human style rating=3.2/5), this indicates reward model misalignment.

3. **Inference overhead benchmarking**: Measure end-to-end inference latency for USO vs a unified encoder baseline on a 768x768 batch of 16 images. If USO adds >200ms per image due to separate SigLIP and VAE encoders, this could limit real-time applications despite performance gains.