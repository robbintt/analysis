---
ver: rpa2
title: Transferring Visual Explainability of Self-Explaining Models to Prediction-Only
  Models without Additional Training
arxiv_id: '2507.04380'
source_url: https://arxiv.org/abs/2507.04380
tags:
- explainability
- target
- classification
- explanation
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes transferring visual explainability from self-explaining
  models to prediction-only models without additional training, using task arithmetic.
  The core idea is to define an "explainability vector" as the difference between
  parameters of a self-explaining model and its prediction-only counterpart, then
  transfer this vector to prediction-only models in target domains.
---

# Transferring Visual Explainability of Self-Explaining Models to Prediction-Only Models without Additional Training

## Quick Facts
- arXiv ID: 2507.04380
- Source URL: https://arxiv.org/abs/2507.04380
- Authors: Yuya Yoshikawa; Ryotaro Shimizu; Takahiro Kawashima; Yuki Saito
- Reference count: 40
- Primary result: Transfers visual explainability from self-explaining models to prediction-only models without additional training using task arithmetic

## Executive Summary
This paper introduces a method to transfer visual explainability from self-explaining models to prediction-only models without requiring additional training on the target domain. The core innovation is defining an "explainability vector" as the parameter difference between self-explaining and prediction-only models trained on a source domain, then applying this vector to prediction-only models in target domains. Experiments demonstrate that this approach significantly improves explanation quality (E-RMSE reduced by ~9%) while maintaining high classification accuracy (within 4% of upper bound) across 10 diverse datasets. The method achieves superior computational efficiency by producing explanations in a single forward pass, outperforming Kernel SHAP with 300+ perturbations.

## Method Summary
The method operates by training two models on a source domain: one with explanation supervision (α < 1) and one without (α = 1). The difference between their parameters (τS⋆) forms an "explainability vector" that isolates explanation capability. This vector is then transferred to target-domain prediction-only models through parameter arithmetic: θ̃T_ft⋆ = θbase + λ₁τT_ft + λ₂τS⋆. The approach leverages Vision Transformers with fixed domain-specific heads constructed from CLIP text embeddings, enabling the explainability vector to be defined purely in the feature extractor parameters. The method requires ground-truth patch-level attributions from the source domain (computed via Kernel SHAP) but transfers this capability without additional training on target domains.

## Key Results
- Explainability vector transfer reduces E-RMSE by ~9% compared to baseline prediction-only models
- Maintains classification accuracy within 4% of upper-bound performance on target datasets
- Produces explanations in single forward pass, outperforming Kernel SHAP (300+ perturbations) on most metrics
- Successfully transfers from ImageNet+X to 10 diverse target datasets including Cars, DTD, GTSRB, MNIST, and CIFAR100
- Domain similarity correlates with transfer success (r=-0.32 correlation between cosine similarity of task vectors and E-RMSE)

## Why This Works (Mechanism)

### Mechanism 1: Explainability Vector as Isolated Capability Representation
The parameter difference between self-explaining and prediction-only models isolates explanation capability as a transferable vector in parameter space. By training two models on the same source domain with different supervision levels, the difference encodes the learned ability to map patch embeddings to attribution values independently of domain-specific classification knowledge.

### Mechanism 2: Task Analogy via Parameter Arithmetic
Adding the source-domain explainability vector to target-domain prediction parameters creates a self-explaining model for the target domain without additional training. The task analogy "prediction-only is to prediction-with-explanation in source domain as prediction-only is to prediction-with-explanation in target domain" enables cross-domain capability transfer.

### Mechanism 3: Shared Domain-Specific Head for Zero-Shot Attribution
The domain-specific head uses fixed text embeddings from a VLM (CLIP) to produce both classification logits and patch attributions. This shared architecture enables attribution capability transfer since the same projection applies to both [CLS] and patch tokens, with the latter yielding unnormalized attributions indicating semantic alignment.

## Foundational Learning

- **Task Arithmetic**: Mathematical foundation for parameter-space editing enabling capability transfer across domains. Quick check: Given task vectors for tasks A, B, C, how would you construct parameters for task D assuming A:B :: C:D?

- **Vision Transformer (ViT) Patch Embeddings**: ViT processes images as M=49 patches (32×32 for 224×224 images), producing M+1 feature vectors. Quick check: How does applying the same linear projection to [CLS] vs. patch tokens yield classification vs. attribution outputs?

- **Shapley Values / Patch Attribution**: Ground-truth supervision uses Kernel SHAP (500 perturbations/image) to compute patch contributions to classification. Quick check: Why does the proposed single-forward-pass method outperform Kernel SHAP with 300+ perturbations on most metrics?

## Architecture Onboarding

- **Component map**: Input Image -> ViT Feature Extractor fθ -> [CLS] + M Patch Tokens -> Domain-Specific Head gW (CLIP text embeddings) -> Classification Logits + Patch Attributions

- **Critical path**: 1) Train θS_ft (α=1) and θS_ft⋆ (α=0.8) on ImageNet+X; 2) Compute τS⋆ = θS_ft⋆ - θS_ft; 3) Train θT_ft on target domain; 4) Apply transfer: θ̃T_ft⋆ = θbase + λ₁(θT_ft - θbase) + λ₂τS⋆

- **Design tradeoffs**: λ₂ scaling balances explanation quality vs. accuracy (higher λ₂ improves explanation but risks accuracy drop); α balance controls classification vs. explanation during source training; source dataset choice affects transfer effectiveness

- **Failure signatures**: Large accuracy drop (>5%) indicates τS⋆ aligned with high-eigenvalue eigenvectors of HT; poor attribution quality suggests source-target domain mismatch; domain dissimilarity (e.g., MNIST→Cars) causes transfer failure

- **First 3 experiments**: 1) Verify explainability vector isolation by computing τS⋆ and confirming attribution capability transfer; 2) Cross-domain transfer sanity check from ImageNet+X to GTSRB, comparing E-RMSE and accuracy against baselines; 3) Domain similarity analysis by correlating cosine similarity of τS_ft and τT_ft with E-RMSE

## Open Questions the Paper Calls Out

- **Universal Explainability Vector**: Can a single vector trained on massive datasets (e.g., LAION) match or exceed ImageNet+X performance across all evaluated target datasets without tuning?

- **Architecture Adaptation**: Can the method be adapted for architectures without fixed VLM embeddings, such as standard ResNets or randomly initialized Transformers?

- **Negative Transfer Prediction**: How can accuracy degradation be automatically predicted and mitigated when applying explainability vectors to semantically distant target domains?

## Limitations
- Theoretical understanding gap: Conditions for successful cross-domain explainability transfer remain poorly characterized
- Dataset dependency: Method requires computationally expensive ground-truth explanations (Kernel SHAP with 500 perturbations) for source domain
- Parameter space assumptions: Success assumes explanation capability resides in disentangled subspace of parameter space

## Confidence
- **High Confidence**: Empirical results showing improved explanation quality (E-RMSE reduction ~9%) while maintaining classification accuracy within 4% of upper bound are robust across 10 diverse datasets
- **Medium Confidence**: Cross-domain transfer mechanism works well for similar domains but fails for dissimilar ones; domain similarity correlation is descriptive rather than predictive
- **Low Confidence**: Theoretical interpretation of explainability vector as "optimal perturbation" lacks rigorous derivation and clear connection to empirical results

## Next Checks
1. **Domain Similarity Predictor**: For new source-target pairs, compute cosine similarity between τS_ft and τT_ft and test whether it predicts E-RMSE improvement on held-out validation data
2. **Scaling Analysis**: Systematically vary λ₂ from 0.4 to 1.2 on multiple target datasets and plot the Pareto frontier of accuracy vs. explanation quality
3. **Alternative Attribution Methods**: Replace Kernel SHAP ground truth with other attribution methods (Grad-CAM, Integrated Gradients) and verify whether τS⋆ transfers equally well