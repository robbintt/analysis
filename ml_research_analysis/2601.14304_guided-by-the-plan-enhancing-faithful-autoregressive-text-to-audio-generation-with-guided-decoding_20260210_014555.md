---
ver: rpa2
title: 'Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation
  with Guided Decoding'
arxiv_id: '2601.14304'
source_url: https://arxiv.org/abs/2601.14304
tags:
- audio
- arxiv
- generation
- clap
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of faithful instruction-following
  in autoregressive (AR) text-to-audio generation, particularly when prompts describe
  multiple complex sound events. AR models struggle with semantic alignment despite
  their strength in temporal coherence.
---

# Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding
## Quick Facts
- arXiv ID: 2601.14304
- Source URL: https://arxiv.org/abs/2601.14304
- Reference count: 23
- Addresses faithful instruction-following in autoregressive text-to-audio generation with guided decoding

## Executive Summary
This paper addresses the challenge of faithful instruction-following in autoregressive text-to-audio generation, particularly for complex prompts describing multiple sound events. The authors introduce Plan-Critic, a lightweight auxiliary model that predicts final semantic alignment (measured by CLAP score) from partial audio sequences during generation. By using early-generation signals as implicit planning, Plan-Critic enables guided exploration that prunes low-fidelity trajectories and reallocates computation to high-potential paths, achieving significant improvements in instruction-following fidelity while maintaining computational efficiency.

## Method Summary
The approach centers on Plan-Critic, an auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict the final CLAP score from partial audio sequences. During autoregressive generation, Plan-Critic evaluates candidate prefixes early in the generation process, allowing the system to prune trajectories unlikely to produce faithful outputs and focus computational resources on promising "planning seeds." This guided exploration strategy leverages temporal coherence strengths of autoregressive models while addressing their semantic alignment weaknesses through early-fidelity prediction.

## Key Results
- Achieves up to 10-point improvement in CLAP score over autoregressive baseline
- Maintains computational parity with best-of-N decoding while improving faithfulness
- Establishes new state of the art in autoregressive text-to-audio generation

## Why This Works (Mechanism)
The method works by addressing a fundamental tension in autoregressive text-to-audio generation: while AR models excel at maintaining temporal coherence, they struggle with semantic alignment when prompts describe multiple complex sound events. Plan-Critic solves this by learning to predict final instruction-following quality from partial sequences, effectively creating an early-warning system for semantic drift. This allows the decoder to explore multiple candidate trajectories in parallel, evaluate their fidelity potential early, and prune those unlikely to succeed, thereby focusing computational resources where they matter most.

## Foundational Learning
**Generalized Advantage Estimation (GAE)**
- *Why needed*: Provides a principled way to train Plan-Critic to predict long-term fidelity from short-term signals
- *Quick check*: Verify that GAE reduces variance in fidelity predictions compared to simple supervised learning

**CLAP Score**
- *Why needed*: Standardized metric for measuring semantic alignment between generated audio and text prompts
- *Quick check*: Confirm CLAP correlates with human judgments of instruction-following quality

**Prefix-based Prediction**
- *Why needed*: Enables early detection of semantic alignment issues before full sequence generation
- *Quick check*: Test prediction accuracy at different prefix lengths to find optimal early-detection point

## Architecture Onboarding
**Component Map**
Text Prompt -> Autoregressive Generator -> Partial Audio Prefix -> Plan-Critic -> CLAP Score Prediction -> Guided Pruning/Exploration

**Critical Path**
During generation: Autoregressive generator produces partial sequences in parallel → Plan-Critic evaluates each prefix → Low-scoring candidates pruned → Resources allocated to high-scoring "planning seeds" → Final sequence selected

**Design Tradeoffs**
- *Faithfulness vs. diversity*: Pruning low-scoring trajectories may reduce audio diversity in favor of semantic alignment
- *Early prediction vs. accuracy*: Earlier predictions require less computation but may be less reliable
- *Computational overhead vs. quality gain*: Plan-Critic adds minimal overhead compared to generating multiple full sequences

**Failure Signatures**
- Over-pruning leading to repetitive or overly conservative audio patterns
- Plan-Critic consistently underestimating fidelity of complex multi-event prompts
- Degradation in temporal coherence as semantic alignment improves

**First Experiments**
1. Test Plan-Critic prediction accuracy across different prefix lengths (10%, 25%, 50% of sequence)
2. Compare guided exploration with simple best-of-N decoding across multiple prompt types
3. Measure diversity metrics (e.g., Fréchet Audio Distance) alongside CLAP scores to assess tradeoff impacts

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Narrow empirical scope with single autoregressive architecture and limited prompt set
- Potential bias toward simpler audio patterns through aggressive pruning
- Lack of ablation studies on GAE-inspired training's specific contribution
- Limited computational comparisons beyond best-of-N decoding strategy

## Confidence
**High confidence**: Plan-Critic architecture is technically sound with well-motivated GAE-inspired training
**Medium confidence**: Efficiency claims hold under best-of-N comparison but need broader verification
**Low confidence**: Generalization across diverse prompt types and scalability to complex audio sequences remain untested

## Next Checks
1. Test Plan-Critic on prompts with overlapping or hierarchically structured sound events beyond the current 7-prompt set
2. Compare Plan-Critic-guided decoding against non-autoregressive methods (RFM, MeanAudio) across multiple metrics
3. Perform ablation study removing GAE-inspired objective to quantify its contribution to performance gains