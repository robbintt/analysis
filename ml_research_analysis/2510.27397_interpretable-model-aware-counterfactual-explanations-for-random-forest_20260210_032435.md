---
ver: rpa2
title: Interpretable Model-Aware Counterfactual Explanations for Random Forest
arxiv_id: '2510.27397'
source_url: https://arxiv.org/abs/2510.27397
tags:
- counterfactual
- explanations
- random
- forest
- credit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of interpreting black-box machine\
  \ learning models, particularly in regulated industries like finance, where transparency\
  \ is essential. The authors propose a model-specific counterfactual explanation\
  \ framework for random forests that leverages the model\u2019s internal geometry\
  \ to generate interpretable, example-based explanations."
---

# Interpretable Model-Aware Counterfactual Explanations for Random Forest

## Quick Facts
- **arXiv ID**: 2510.27397
- **Source URL**: https://arxiv.org/abs/2510.27397
- **Reference count**: 40
- **Primary result**: Model-aware counterfactual explanations for Random Forests that are more interpretable and actionable than SHAP, validated on MNIST and German Credit datasets.

## Executive Summary
This paper addresses the challenge of interpreting black-box machine learning models, particularly in regulated industries like finance, where transparency is essential. The authors propose a model-specific counterfactual explanation framework for random forests that leverages the model's internal geometry to generate interpretable, example-based explanations. Instead of relying on feature attribution methods like SHAP, their approach uses RF-GAP distances to find plausible counterfactual cases and interprets them through decision partition crossings along a trajectory. This yields sparse, actionable explanations grounded in the model's learned structure.

Empirical evaluation on MNIST and German Credit datasets demonstrates that the method produces more interpretable and useful explanations compared to SHAP, with higher sparsity and better alignment with human reasoning. In the credit scoring task, counterfactual explanations based on partition crossings were more effective at identifying actionable feature changes that could flip predictions, underscoring the practical value of this approach in real-world financial decision-making.

## Method Summary
The method trains a Random Forest classifier and computes RF-GAP proximity distances between instances based on leaf co-occurrence patterns. For a given test instance, it performs a greedy hill-climbing search to find a trajectory of intermediate instances that increases the probability of the desired class. Along this trajectory, it tallies how many decision partitions (splits) are crossed for each feature, producing a signed tally that indicates which features to change and in what direction. The approach is evaluated against SHAP using sparsity and usefulness metrics on MNIST and German Credit datasets.

## Key Results
- Partition tally explanations achieved significantly higher sparsity (0.6253) compared to SHAP (0.3839) on MNIST
- Counterfactual trajectories using RF-GAP distances produced higher rates of valid class flips compared to Euclidean distance approaches
- In the German Credit task, partition crossing-based explanations were more effective at identifying actionable feature changes that could flip predictions

## Why This Works (Mechanism)

### Mechanism 1: RF-GAP Proximity for Model-Aware Similarity
Random Forest internal geometry provides a more faithful similarity metric for counterfactual retrieval than Euclidean distance. The method computes RF-GAP proximity by weighting the co-occurrence of instances in leaf nodes across the ensemble. This distance reflects the model's learned decision boundaries rather than raw feature magnitudes. Counterfactuals are retrieved via a constrained nearest-neighbor search using this distance. Core assumption: similarity relevant to a user corresponds to proximity within the Random Forest's partitioned feature space. Evidence: Section 3.2 defines $d^{GAP}_{i,j}$ based on weighted leaf co-occurrences; Section