---
ver: rpa2
title: Towards Harmless Multimodal Assistants with Blind Preference Optimization
arxiv_id: '2503.14189'
source_url: https://arxiv.org/abs/2503.14189
tags:
- safety
- mllms
- preference
- multimodal
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the safety alignment challenge for Multimodal
  Large Language Models (MLLMs), which have shown impressive capabilities but also
  pose significant safety risks. The authors identify two key observations: "modality
  co-defense," where MLLMs exhibit inherent safety through language-to-visual transfer,
  and "modality cheating," where text modality can mislead the model into ignoring
  visual information.'
---

# Towards Harmless Multimodal Assistants with Blind Preference Optimization

## Quick Facts
- arXiv ID: 2503.14189
- Source URL: https://arxiv.org/abs/2503.14189
- Reference count: 27
- Primary result: BPO improves LLaVA safety rate by 45.0% vs base, outperforming DPO's 33.8%

## Executive Summary
This paper addresses safety alignment challenges for Multimodal Large Language Models (MLLMs) by introducing Blind Preference Optimization (BPO). The authors identify two key phenomena: "modality co-defense" where text-trained safety transfers to visual modality, and "modality cheating" where text patterns override visual information. They construct MMSafe-PO, a high-quality safety preference dataset, and propose BPO which trains models to prefer responses from full multimodal input over text-only input. Experiments show BPO significantly outperforms standard DPO, reducing unsafe rates to 14.5% on MM-SafetyBench and 82.9% on HarmEval.

## Method Summary
The approach constructs MMSafe-PO from Anthropic-HH text data through entity recognition, image retrieval, instruction rephrasing, and human quality filtering. BPO extends DPO by adding blind responses (y_b) generated without visual input, creating dual preference pairs: (y_w, y_l) from dataset and (y_w, y_b) from on-the-fly generation. The combined loss optimizes the model to distinguish responses based on visual context versus text-only reasoning. Training uses LLaVA-1.5 with frozen visual encoder, AdamW optimizer (lr=1e-6), and DeepSpeed Stage 2 on 4×A6000 GPUs.

## Key Results
- BPO improves LLaVA's safety rate by 45.0% compared to base model
- Outperforms DPO's 33.8% safety improvement on MMSafe-PO test set
- Strong generalization: reduces unsafe rates to 14.5% on MM-SafetyBench and 82.9% on HarmEval
- Ablation confirms both (y_w, y_l) and (y_w, y_b) pairs are necessary (removing former drops safety from 0.89 to 0.61)

## Why This Works (Mechanism)

### Mechanism 1: Modality Co-Defense via Language-to-Visual Transfer
- Claim: MLLMs exhibit partial inherent safety on multimodal harmful queries without explicit multimodal safety training
- Mechanism: Safety behaviors learned in text-only alignment transfer to visual modality through shared embedding space created during vision-language alignment training
- Core assumption: Vision encoder and LLM share aligned semantic representations where harmful concepts map consistently across modalities
- Evidence anchors: Figure 2 shows MLLM correctly refusing harmful multimodal instruction about drug transport when shown actual drug image; quantitative analysis shows 20% improvement in safety rate for multimodal vs text-only instructions
- Break Condition: Co-defense fails when visual input contradicts textual safety cues (modality cheating), or when novel visual harmful content lacks textual correlates

### Mechanism 2: Modality Cheating from Language-Bias Dominance
- Claim: Text patterns can dominate MLLM behavior, causing models to ignore visual context and produce incorrect refusals or responses
- Mechanism: Safety-trained LLM backbones develop strong pattern-matching to harmful text templates; when these patterns appear, model defaults to text-only reasoning pathways, bypassing visual processing
- Core assumption: Visual features receive insufficient weighting in cross-attention layers during safety-critical decisions
- Evidence anchors: Figure 2 shows identical text with safe suitcase image causes incorrect refusal while same text with drug image was correctly refused; pass rate dropped 50% when unsafe images were replaced with safe alternatives while keeping text unchanged
- Break Condition: Cheating is amplified when text strongly matches learned harmful patterns; reduced when visual features have higher signal-to-noise ratio

### Mechanism 3: Blind Preference Optimization (BPO) for Visual Grounding
- Claim: Training MLLMs to prefer responses from full multimodal input over text-only input improves safety by strengthening visual attention
- Mechanism: BPO extends DPO by adding third response type—"blind" responses generated without images; loss function optimizes: L_BPO = -E[log σ(r(x, y_w) - r(x, y_l)) + log σ(r(x, y_w) - r(x, y_b))]
- Core assumption: Blind response y_b is consistently inferior to chosen response y_w specifically due to missing visual information
- Evidence anchors: BPO significantly improves LLaVA's safety rate by 45.0% compared to base model, outperforming DPO which achieved 33.8%; ablation shows removing (y_w, y_l) pairs drops safety rate from 0.89 to 0.61
- Break Condition: BPO effectiveness depends on y_b being meaningfully worse than y_w due to visual absence; if text is already sufficient, contrast signal weakens

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed: BPO builds directly on DPO's mathematical framework; understanding reward-parameterization trick is prerequisite to grasping how BPO extends it
  - Quick check: Can you explain why DPO eliminates the need for explicit reward model during training?

- Concept: Vision-Language Alignment in MLLMs
  - Why needed: Mechanisms of co-defense and cheating both stem from how vision encoders connect to LLMs through cross-attention or projection layers
  - Quick check: What is the role of vision-language connector (e.g., MLP, Q-Former) in enabling semantic transfer between modalities?

- Concept: Safety Alignment Taxonomy
  - Why needed: MMSafe-PO dataset uses hierarchical safety categories (Representation Harms, Malicious Use, etc.); evaluating safety requires understanding these distinctions
  - Quick check: How does "harmless" alignment objective differ from "helpful" and "honest" in RLHF contexts?

## Architecture Onboarding

- Component map:
```
Anthropic-HH text data → Entity Recognition → Image Matching → Instruction Rephrasing → Quality Filtering → MMSafe-PO
                          ↓                           ↓                   ↓                  ↓
                   Multimodal instruction → Generate y_w, y_l → Branch 1
                           ↓                           ↓
                   Text-only instruction → Generate y_b → Branch 2
                                           ↓
                                   BPO Loss: contrast y_w vs y_l AND y_w vs y_b
                                           ↓
                                   Update policy model π* (keep π_ref frozen)
```

- Critical path:
  1. Dataset quality: Human-ranked preference pairs are essential; synthetic ranking showed lower effectiveness in prior work
  2. Blind response generation: Must use reference model π_ref, not training model, to ensure consistent quality gap
  3. Dual loss terms: Both (y_w, y_l) and (y_w, y_b) are necessary; ablation shows 31% safety drop without original pairs

- Design tradeoffs:
  - Dataset source: Converting text-only Anthropic-HH preserves human feedback quality but may miss multimodal-specific harms not present in text data
  - Training efficiency: BPO requires generating y_b on-the-fly, adding ~50% compute overhead vs standard DPO
  - Generalization: BPO shows stronger out-of-domain results (14.5% unsafe rate on MM-SafetyBench vs DPO's 60.86%)—suggests on-policy sampling effect

- Failure signatures:
  - Low pairwise accuracy with high safety rate: May indicate model learned shallow refusal patterns rather than genuine visual reasoning
  - BPO underperforming DPO: Check that y_b is genuinely blind (image tensor zeroed/removed, not just masked)
  - Catastrophic forgetting of helpfulness: Over-optimization on safety can degrade general capabilities; monitor on standard VQA benchmarks

- First 3 experiments:
  1. Baseline replication: Train LLaVA-1.5 with standard DPO on MMSafe-PO; verify ~33% safety improvement matches paper before attempting BPO
  2. Blind response quality audit: Manually compare 50 samples of y_w vs y_b to confirm visual information meaningfully differentiates responses; if gap is small, entity recognition may be failing
  3. Cross-benchmark evaluation: After training, evaluate on both MMSafe-PO test set AND MM-SafetyBench; BPO's advantage should be larger on out-of-domain data (paper shows 14.5% vs 60.86% unsafe rate difference)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Blind Preference Optimization be effectively adapted to text-only LLMs by removing key words from instructions, analogous to blinding visual input in MLLMs?
- Basis in paper: [explicit] The limitations section states: "A natural question is whether BPO could also be effective for LLMs. Removing certain words from textual instructions might have a similar effect to blinding the MLLMs, and the corresponding evaluation is expected."
- Why unresolved: Authors designed BPO specifically for multimodal settings and did not experimentally validate whether same principle transfers to unimodal language models
- What evidence would resolve it: Experiments applying text-analogous BPO (e.g., masking named entities or key terms) to standard LLMs, comparing safety improvements against baseline DPO

### Open Question 2
- Question: What is the impact of BPO-based safety alignment on model helpfulness and general capability performance?
- Basis in paper: [inferred] Paper reports safety improvements (45.0% increase in safety rate) but does not evaluate whether this comes at cost of reduced helpfulness or task performance, common trade-off in safety alignment
- Why unresolved: Safety benchmarks alone cannot determine if model becomes overly-refusal or degraded on benign tasks
- What evidence would resolve it: Evaluation on standard MLLM benchmarks (e.g., VQAv2, TextVQA) measuring any capability degradation after BPO training

### Open Question 3
- Question: Does BPO generalize effectively across diverse MLLM architectures beyond LLaVA-1.5?
- Basis in paper: [inferred] While multiple MLLMs are evaluated as baselines, BPO experiments are conducted only on LLaVA-1.5, leaving unclear whether approach transfers to architectures with different vision encoders or alignment mechanisms
- Why unresolved: Modality cheating phenomenon may manifest differently across architectures, affecting BPO's relative effectiveness
- What evidence would resolve it: Applying BPO to at least 2-3 additional MLLM architectures (e.g., InternLM-XComposer, Qwen-VL) and comparing safety improvement rates

## Limitations
- Blind response quality: BPO's effectiveness depends on consistent quality gap between y_w and y_b, but generation parameters are underspecified
- Co-defense mechanism: While intuitively appealing, lacks direct mechanistic validation of how safety transfer occurs
- GPT-4V evaluation: Introduces potential subjectivity and dependency on API availability/consistency across experimental runs

## Confidence
- **High confidence**: BPO's mathematical formulation and ability to improve safety rates on MMSafe-PO test set (45.0% improvement vs base)
- **Medium confidence**: BPO's superior generalization to MM-SafetyBench (14.5% vs 60.86% unsafe rate) due to potential evaluation protocol variations
- **Medium confidence**: Modality cheating mechanism description, though empirically supported, lacks formal ablation studies isolating text bias from other factors

## Next Checks
1. **Blind response quality audit**: Manually examine 100 randomly sampled (y_w, y_b) pairs to verify that visual information meaningfully differentiates responses and that y_b generation parameters are properly calibrated
2. **Cross-model safety transfer**: Evaluate BPO-trained LLaVA-1.5 on another MLLM architecture (e.g., Qwen-VL-Plus) to test whether learned safety behaviors transfer across different vision-language connectors
3. **Temporal stability analysis**: Retrain BPO after 3 months using updated GPT-4V API to verify that safety rate improvements are consistent across different model versions and evaluation protocols