---
ver: rpa2
title: 'Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM
  as a Judge, and a Lightweight CTF Benchmark'
arxiv_id: '2508.05674'
source_url: https://arxiv.org/abs/2508.05674
tags:
- agent
- performance
- across
- challenges
- challenge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation framework for LLM-based
  offensive security agents, addressing the challenge of benchmarking agentic reasoning
  and tool use in cybersecurity contexts. The authors introduce CTFJudge, an LLM-based
  judge that evaluates agent trajectories across six cybersecurity competencies, providing
  granular scoring beyond simple pass/fail metrics.
---

# Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark

## Quick Facts
- arXiv ID: 2508.05674
- Source URL: https://arxiv.org/abs/2508.05674
- Reference count: 40
- Primary result: CTFJudge evaluation framework and CTFTiny benchmark reveal that high temperature (≈1.0) and top-p (≈1.0) sampling, plus 4096 max tokens, optimize LLM agent performance on offensive security tasks.

## Executive Summary
This paper addresses the challenge of benchmarking LLM-based offensive security agents by introducing CTFJudge, an LLM-based evaluator that provides granular competency scoring beyond simple pass/fail metrics, and CTFTiny, a curated 50-challenge benchmark enabling rapid experimentation. Through extensive hyperparameter analysis, the authors demonstrate that high sampling diversity (temperature and top-p ≈ 1.0) and mid-range max tokens (4096) significantly improve solve rates for strong models like Claude 4 Sonnet, which achieves 76% solve rate on CTFTiny. The framework reveals that failure patterns are concentrated in domain expertise gaps and exploit development rather than task delegation, providing actionable insights for improving agentic systems in cybersecurity contexts.

## Method Summary
The authors present a comprehensive evaluation framework for LLM-based offensive security agents using the D-CIPHER agent architecture on the CTFTiny benchmark (50 curated CTF challenges). They conduct systematic hyperparameter sweeps across temperature (0.0-1.0), top-p (0.25-1.0), and max tokens (2048-8192) to identify optimal configurations. The CTFJudge system uses Claude 3.7 Sonnet as an evaluator to compare agent trajectories against expert write-ups, producing the CTF Competency Index (CCI) across six cybersecurity competencies. The evaluation measures both pass@1 rates and qualitative competency scores, with failure modes categorized to identify specific bottlenecks in the agent pipeline.

## Key Results
- Claude 4 Sonnet achieves 76% solve rate on CTFTiny with optimal hyperparameters (T=1.0, Top-p=1.0, Max Tokens=4096)
- CTF Competency Index scores range from 77.5-84.5, revealing gaps between solve rates and reasoning quality
- High sampling diversity (T≈1.0, Top-p≈1.0) correlates with improved solve rates for strong models
- Mid-range max tokens (4096) provide optimal balance between reasoning depth and context maintenance
- Failure patterns concentrate in domain expertise gaps and exploit development rather than task delegation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher sampling diversity (temperature and top-p ≈ 1.0) correlates with improved solve rates for strong models in offensive security tasks.
- Mechanism: CTF challenges often require non-deterministic, multi-step reasoning. High temperature allows the model to explore diverse reasoning paths and potential exploit vectors rather than converging prematurely on a suboptimal strategy.
- Core assumption: The model has sufficient underlying capability to distinguish useful exploration from hallucination.
- Evidence anchors:
  - [Section 5.3] "Controlled randomness enhances exploratory reasoning... Claude’s performance remains stable across T ∈ [0.2, 0.8], then surges at T = 1.0."
  - [Abstract] "Temperature ≈ 1.0 and top-p ≈ 1.0 optimize solve rates for strong models like Claude 4 Sonnet."
  - [Corpus] Weak direct support; related work (D-CIPHER, CRAKEN) focuses on agent architecture rather than decoding parameter sensitivity.
- Break condition: If the model is weak or prone to hallucination, high diversity degrades performance into incoherence.

### Mechanism 2
- Claim: Mid-range maximum token limits (4096) provide an optimal balance between reasoning depth and context maintenance.
- Mechanism: Too few tokens truncate complex chain-of-thought required for exploitation; too many tokens induce "context saturation" or attention drift, causing the model to lose focus on the critical constraints.
- Core assumption: The agent's attention mechanism suffers when processing overly verbose or irrelevant context.
- Evidence anchors:
  - [Section 5.3] "The dip at 8192 suggests context saturation, where longer completions may overwhelm model’s attention or generate overly verbose."
  - [Figure 11] Shows a non-monotonic performance curve peaking at 4096 for Claude 4 Sonnet.
- Break condition: If models develop better attention mechanisms for long contexts, or if tasks require significantly longer reasoning traces than typical CTFs.

### Mechanism 3
- Claim: LLM-based judges evaluating trajectories against expert solutions provide actionable signal on *why* agents fail, distinct from binary pass/fail metrics.
- Mechanism: By decomposing trajectories into six competency dimensions (e.g., Reconnaissance vs. Exploitation), the judge identifies specific bottlenecks—such as domain knowledge gaps versus tool delegation failures.
- Core assumption: The judging LLM (e.g., Claude 3.7 Sonnet) can reliably interpret technical security logs and align them with expert logic.
- Evidence anchors:
  - [Section 5.2] "CCI exposes the gap between solve rates and true reasoning quality... pinpoints where the pipeline breaks."
  - [Abstract] "Reveals how closely agent solutions align with human-crafted gold standards."
  - [Corpus] Related work (InterCode, CyBench) relies on execution-based pass/fail; this paper introduces a qualitative trajectory judge.
- Break condition: If the agent's strategy is valid but orthogonal to the provided expert writeup, or if the judge lacks the specific domain knowledge to assess a novel technique.

## Foundational Learning

- Concept: **LLM Sampling Hyperparameters (Temperature & Top-p)**
  - Why needed here: The paper's core counter-intuitive finding is that "creative" (high temperature) settings outperform deterministic ones for security agents, which contradicts standard precision-focused usage.
  - Quick check question: Why would an agent solving a logic puzzle benefit from randomness?

- Concept: **Agentic Trajectory vs. Final Output**
  - Why needed here: The CTFJudge evaluates the *process* (the trajectory summary) rather than just the flag capture. Understanding that "how" matters more than "what" is central to the paper's contribution.
  - Quick check question: Can an agent fail to capture a flag but still demonstrate high competency in vulnerability analysis?

- Concept: **CTF Domains (Pwn, Rev, Crypto, Web, Forensics)**
  - Why needed here: The paper breaks down performance and failure modes by domain (e.g., agents struggle with binary interaction in 'misc'/'pwn' but excel at standardized 'web' tasks).
  - Quick check question: Why might an agent perform well on Cryptography but poorly on Binary Exploitation?

## Architecture Onboarding

- Component map:
  - CTFTiny (Benchmark) -> D-CIPHER (Agent) -> CTFJudge (Evaluator)

- Critical path:
  1. Select challenge from CTFTiny.
  2. Configure Agent LLM (Temp=1.0, Top-p=1.0, MaxTokens=4096 based on findings).
  3. Agent generates a solution trajectory (commands, reasoning).
  4. CTFJudge compares trajectory against the Gold Write-up.
  5. Output CCI score (0-1) and failure classification (e.g., "Domain Knowledge Gap").

- Design tradeoffs:
  - **Speed vs. Granularity:** CTFTiny allows rapid iteration (50 challenges vs. hundreds in full benchmarks) but may reduce statistical significance for niche categories.
  - **Exploration vs. Stability:** High temperature (1.0) increases solve rates for top models but risks instability in weaker models.
  - **Judge Reliability:** Using an LLM as a judge (Claude 3.7) scales evaluation but introduces model-specific biases compared to deterministic tests.

- Failure signatures:
  - **High Recon / Low Exploit:** Agent finds the vulnerability but fails to weaponize it (identified as "Exploit Development Failure").
  - **Low Efficiency:** Agent solves the challenge but uses brute-force or redundant loops (low CCI efficiency score).
  - **Context Drift:** Performance drops when Max Tokens > 4096 due to "context saturation."

- First 3 experiments:
  1. **Baseline Validation:** Run the default D-CIPHER agent on CTFTiny with the paper's recommended settings (T=1.0) to verify the 76% solve rate baseline.
  2. **Ablation on Sampling:** Sweep temperature (0.0 to 1.0) on a subset of 10 challenges to observe the non-linear performance curve described in Section 5.3.
  3. **Failure Mode Analysis:** Use CTFJudge to categorize failures on "Hard" challenges to determine if the bottleneck is Knowledge, Recon, or Tool Delegation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do optimal hyperparameter settings and evaluation outcomes generalize to alternative agent architectures, such as reinforcement-learning systems or knowledge-based retrieval agents?
- Basis in paper: [explicit] The authors note in Section 6 that their benchmarking focused on a single open-source agent architecture (D-CIPHER) and explicitly call for extending evaluations to fundamentally different designs.
- Why unresolved: It remains unclear if the finding that temperature $\approx 1.0$ and top-p $\approx 1.0$ optimize solve rates is specific to the D-CIPHER planning/execution structure or a universal property for all cybersecurity agents.
- What evidence would resolve it: Replicating the CTFTiny benchmark experiments using non-planner-based architectures (e.g., RL agents or RAG-based agents) and comparing their optimal hyperparameter curves and CCI scores.

### Open Question 2
- Question: Can dynamic reweighting of the CTF Competency Index (CCI) factors improve evaluation accuracy for emerging or atypical CTF tasks?
- Basis in paper: [explicit] Section 6 identifies the fixed, expert-curated weighting scheme as a limitation, suggesting the metric could benefit from tuning to accommodate new challenge types.
- Why unresolved: The current framework assumes equal importance across six competencies (e.g., reconnaissance vs. exploitation), which may not hold for novel attack vectors where one skill is disproportionately critical.
- What evidence would resolve it: Developing an adaptive calibration mechanism for CCI weights and validating its correlation with human expert assessments on a dataset of novel "zero-day" style CTF challenges.

### Open Question 3
- Question: To what extent does explicit chain-of-thought (CoT) prompting contribute to success and robustness compared to implicit reasoning in offensive security agents?
- Basis in paper: [explicit] Section 6 calls for rigorous targeted ablation studies isolating explicit CoT and implicit reasoning to clarify each approach's contribution.
- Why unresolved: While the paper analyzes failure patterns, it does not disentangle whether the successful trajectories relied on the explicit reasoning steps generated or the model's internal implicit capabilities.
- What evidence would resolve it: Ablation experiments running the agent with forced CoT prompts versus restricted CoT, measuring the delta in pass rates and the "Technical Accuracy" dimension of the CCI.

## Limitations

- The evaluation framework relies heavily on an LLM judge (Claude 3.7) to assess trajectory quality, introducing potential model-specific biases and subjectivity in competency scoring.
- The CTFTiny benchmark, while enabling rapid experimentation, may not fully represent the complexity and diversity of real-world CTF challenges.
- The hyperparameter optimizations were primarily tested on Claude 4 Sonnet and Claude 3.5 Sonnet, with limited validation across different model families.

## Confidence

- **High confidence:** The empirical finding that mid-range max tokens (4096) optimizes performance for the tested models, supported by clear performance curves showing context saturation effects.
- **Medium confidence:** The claim that high temperature (≈1.0) improves solve rates for strong models, as this relies on the judge's assessment and may be model-specific.
- **Medium confidence:** The CTF Competency Index as a meaningful measure of reasoning quality, given it depends on LLM-based evaluation which may not perfectly align with human expert assessment.

## Next Checks

1. **Cross-model validation:** Replicate the temperature and top-p optimization experiments using a different strong model family (e.g., GPT-4 or Gemini) to verify the generality of the "high diversity" finding beyond Claude models.

2. **Judge reliability test:** Conduct a blind evaluation where human experts assess a subset of agent trajectories to measure agreement with the LLM judge's CCI scores and identify systematic biases.

3. **Real-world applicability test:** Apply the optimized hyperparameters and evaluation framework to a small set of actual production-like vulnerability cases or bug bounty programs to assess transfer from controlled CTF environments.