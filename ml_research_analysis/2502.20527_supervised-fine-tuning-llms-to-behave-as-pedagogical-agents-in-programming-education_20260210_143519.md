---
ver: rpa2
title: Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in Programming
  Education
arxiv_id: '2502.20527'
source_url: https://arxiv.org/abs/2502.20527
tags:
- fine-tuning
- education
- code
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GuideLM is a fine-tuned LLM developed for programming education,
  designed to provide pedagogically sound assistance in the Debugging C Compiler (DCC).
  It addresses the issue of over-assistance by base LLMs, which often provide direct
  solutions to students.
---

# Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in Programming Education

## Quick Facts
- arXiv ID: 2502.20527
- Source URL: https://arxiv.org/abs/2502.20527
- Reference count: 40
- GuideLM fine-tuned on 528 CS1 forum Q&A pairs improves Socratic guidance by 8% and economy of words by 58% vs GPT-4o, at slight accuracy cost

## Executive Summary
GuideLM is a fine-tuned LLM designed to provide pedagogically sound assistance in programming education, specifically addressing the issue of over-assistance by base LLMs. Using supervised fine-tuning on a dataset of 528 high-quality student-question/teacher-answer pairs, the authors create GuideLM and GuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini respectively. Expert evaluation of 400 responses per model shows an 8% increase in Socratic guidance and a 58% improvement in economy of words compared to GPT-4o, though at the cost of slight accuracy reduction.

## Method Summary
The method involves curating a dataset of 528 student-question/teacher-answer pairs from a CS1 course forum (filtered from 13,000 raw entries), preprocessing with PII removal and grammar correction, and fine-tuning GPT-4o and GPT-4o-mini via OpenAI's API. The fine-tuning data is formatted as JSONL with a system prompt instructing the model to act as a tutor who does not provide direct solutions. The resulting models are evaluated by expert TAs on 400 responses each, using nine pedagogical properties including Socratic guidance and economy of words.

## Key Results
- 8% increase in Socratic guidance compared to GPT-4o
- 58% improvement in economy of words compared to GPT-4o
- Slight reduction in general accuracy (20-45% decrease in completeness, 9-20% decrease in conceptual accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised Fine-Tuning (SFT) shifts model behavior from "solution generation" to "pedagogical scaffolding" by reweighting response patterns based on expert-curated examples.
- **Mechanism:** The base model (GPT-4o) is conditioned on a specific dataset of 528 question-answer pairs where the desired behavior is "Socratic guidance" rather than direct answers. By minimizing loss on this specific distribution, the model learns to suppress the "direct solution" token sequences prevalent in base training in favor of "guided question" sequences found in the fine-tuning data.
- **Core assumption:** The foundational model's reasoning capabilities remain intact during fine-tuning, and the dataset quality is sufficient to overrule the base model's propensity for verbose problem-solving.
- **Evidence anchors:**
  - [abstract] "Results indicate that GuideLM... improve pedagogical performance, with an 8% increase in Socratic guidance."
  - [section 3.1] "Five senior CS1 teaching assistants were tasked with assessing... criteria: Not over-helpful... Demonstrative code blocks only."
  - [corpus] 'Narrowing the Gap' confirms that proprietary models often over-assist and specialized SFT is a viable alternative.
- **Break condition:** The mechanism likely fails if the fine-tuning dataset contains >~10% noise (incorrect solutions or non-Socratic answers), which would reinforce the exact behaviors (hallucination or over-assistance) the system aims to suppress.

### Mechanism 2
- **Claim:** Economy of words improves through "style transfer" enforced by grammar and formatting normalization.
- **Mechanism:** The authors applied an LLM-based enhancement step to correct grammar and format code in the training data. This preprocessing step standardizes the input distribution, ensuring the model learns from clean, concise text. The 58% reduction in verbosity suggests the model learned to mimic the high information-density of the corrected tutor answers rather than the "chatty" style typical of general-purpose LLMs.
- **Core assumption:** The "grammar corrector" prompt (GPT-4o) did not inadvertently change the semantic meaning or pedagogical intent of the tutor answers during the cleaning process.
- **Evidence anchors:**
  - [abstract] "58% improvement in economy of words compared to GPT-4o."
  - [section 3.3] "The following system prompt was given to the model... 'Format code snippets with correct spacing...'"
  - [corpus] Weak/missing explicit evidence in corpus regarding style transfer specifically for education; claims rely primarily on the paper text.
- **Break condition:** If the base model's RLHF alignment heavily penalizes brevity (interpreting it as rudeness), the SFT may struggle to maintain conciseness without losing the "tutor" persona.

### Mechanism 3
- **Claim:** Improving pedagogical alignment causes a measurable regression in general factual accuracy (the "alignment tax").
- **Mechanism:** By forcing the model to prioritize a specific response style (hints over solutions), the probability space for "correct code generation" is compressed. The model essentially unlearns some generic debugging capability to fit the constraints of the CS1 tutor persona.
- **Core assumption:** The trade-off is inherent to the SFT process on small datasets, rather than a result of overfitting on a specific error type.
- **Evidence anchors:**
  - [abstract] "This refinement comes at the cost of a slight reduction in general accuracy."
  - [section 5] "Both GuideLM models saw between an 8-20% decrease in conceptual accuracy... a 20-45% decrease in completeness."
  - [corpus] 'Narrowing the Gap' and 'Survey of LLM-Based Applications' both highlight the challenge of balancing automation (solving) with human oversight (teaching).
- **Break condition:** If reasoning capabilities degrade significantly, the model may provide "confident but wrong" pedagogical hints, which is arguably more damaging than over-assistance.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - **Why needed here:** The core technical contribution of the paper is using SFT to change behavior. You must understand that SFT updates model weights to minimize loss on a specific dataset, unlike RAG which injects context without weight changes.
  - **Quick check question:** How does SFT differ from few-shot prompting in terms of how the model "learns" the tutor persona?

- **Concept: Constructivism & Cognitive Load Theory**
  - **Why needed here:** These are the theoretical frameworks defining the "success" criteria. The paper explicitly optimizes for "Socratic guidance" (Constructivism) and "economy of words" (Cognitive Load). Without understanding these, the drop in "completeness" looks like a failure rather than a feature.
  - **Quick check question:** Why would a "complete explanation" (C6) be rated lower for a novice student according to Cognitive Load Theory?

- **Concept: Precision vs. Recall in Evaluation**
  - **Why needed here:** The paper shows a trade-off. The model gets better at the specific task (Precision in tone/pedagogy) but worse at general capability (Recall of facts/solutions).
  - **Quick check question:** In the context of GuideLM, is a "Socratic" response that is technically incomplete a False Positive or a True Positive for the intended educational task?

## Architecture Onboarding

- **Component map:** Forum Scrapper -> PII Filter (Regex) -> Manual Filter (Senior TAs) -> LLM Enhancer (Grammar/Format) -> OpenAI Fine-tuning API (Base: GPT-4o/4o-mini) -> Custom Weights (GuideLM) -> Debugging C Compiler (DCC) -> Error Context + Code -> GuideLM -> Student Feedback
- **Critical path:** The **Manual Filtering** step (Section 3.3). 13,000 pairs were reduced to 528 (4% yield). This high-bar filtering is the most likely point of failure; using lower-quality data would result in a model that is both inaccurate *and* poorly aligned.
- **Design tradeoffs:**
  - **Pedagogy vs. Accuracy:** The system explicitly trades ~20% factual accuracy for ~58% better conciseness and Socratic tone.
  - **Cost vs. Control:** SFT creates a specialized model but requires re-running the expensive training process when new base models (e.g., GPT-5) release, unlike prompt engineering which is cheaper but less effective here.
- **Failure signatures:**
  - **The "Lazy Tutor":** Model refuses to help or gives vague hints (over-optimization of "economy of words").
  - **The "Confident Hallucinator":** Model gives a Socratic hint based on an incorrect understanding of the C code (overfitting to training style, losing reasoning capability).
  - **Data Drift:** The model fails on modern C standards or library updates not present in the static training set.
- **First 3 experiments:**
  1. **A/B Test on "Overhelpfulness":** Run 50 distinct student errors through both Base GPT-4o and GuideLM. Measure the percentage of responses containing runnable code blocks. The target is a >30% reduction in runnable code in GuideLM.
  2. **Accuracy Regression Test:** Evaluate GuideLM on a standardized benchmark (e.g., HumanEval or a subset of C-exercises) to quantify the exact "alignment tax" mentioned in Section 5.
  3. **Context Sensitivity Check:** Input code with subtle logic errors (which compile) vs. syntax errors. Verify if the "Socratic" behavior holds for logic errors or if the model reverts to base behavior when compiler error messages are missing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the observed trade-off between pedagogical alignment and technical accuracy inherent to fine-tuning, or can it be mitigated?
- Basis in paper: [explicit] The authors state, "It is yet unclear if this performance cost can be mitigated with improved fine-tuning techniques... or if the performance trade-off is inherent."
- Why unresolved: While the paper demonstrates a gain in Socratic guidance (+8%) and economy of words (+58%), it concurrently reports a 20-45% decrease in completeness and a rise in inaccuracies. The mechanism causing this specific degradation during Supervised Fine-Tuning (SFT) is not identified.
- What evidence would resolve it: Comparative experiments using Reinforcement Learning from Human Feedback (RLHF) or larger datasets to determine if technical accuracy can be restored without losing the pedagogical style.

### Open Question 2
- Question: Does the increase in Socratic guidance lead to measurable improvements in student learning outcomes and knowledge retention?
- Basis in paper: [explicit] The paper notes, "We are particularly interested in measuring how the increased Socratic guidance affects student problem-solving capabilities and knowledge retention."
- Why unresolved: The current study relies on expert evaluation of response quality (rankings), rather than empirical data on student performance or long-term skill acquisition.
- What evidence would resolve it: Results from the mentioned A/B testing deployment, comparing assessment scores and problem-solving success rates between students using GuideLM versus base models.

### Open Question 3
- Question: Can course forum data from diverse academic disciplines form effective training datasets for general-purpose pedagogical models?
- Basis in paper: [explicit] The authors ask, "We will also investigate whether broad course forum data from diverse academic disciplines can form effective training datasets for producing general-purpose pedagogical models."
- Why unresolved: The current study is limited to CS1 (C programming). It is unknown if the "noisy" nature of forum data in other domains (e.g., humanities) would yield similar high-quality training pairs (only 21% of the scraped data was usable here).
- What evidence would resolve it: Replicating the data curation and fine-tuning pipeline on non-CS course forums and evaluating the resulting model's pedagogical performance.

## Limitations
- The primary contribution relies on a proprietary dataset of 528 student-question/teacher-answer pairs from a private CS1 course forum, which is essential for reproducing the results.
- The paper does not provide ablation studies to definitively prove each mechanism's contribution (e.g., the contribution of the grammar correction pass vs. the fine-tuning itself).
- The evaluation properties are somewhat subjective (e.g., "Socratic Guidance," "Economy of Words") and the paper does not specify inter-rater reliability scores or how disagreements were resolved.

## Confidence
- **Major uncertainty:** The paper's primary contribution rests on a dataset of 528 student-question/teacher-answer pairs from a private CS1 course forum. This proprietary dataset is essential for reproducing the results, and without it, any replication effort requires creating a proxy dataset with similar pedagogical constraints. The effectiveness of GuideLM is thus tightly coupled to the quality and representativeness of this specific dataset. **Confidence: Low** on the generalizability of the findings beyond this specific context.
- **Mechanism confidence:** The three mechanisms (SFT for pedagogical alignment, style transfer for brevity, and the alignment tax for accuracy) are plausible and supported by the evaluation results. However, the paper does not provide ablation studies to definitively prove each mechanism's contribution. **Confidence: Medium** on the stated mechanisms.
- **Evaluation limitations:** The expert evaluation (400 responses per model, 9 properties) is a strength, but the properties are somewhat subjective (e.g., "Socratic Guidance," "Economy of Words"). The paper does not specify inter-rater reliability scores or how disagreements were resolved, which introduces potential bias. Additionally, the "completeness" metric (C6) is penalized by the model's pedagogical design, so a low score here is a feature, not a bug, which could confuse readers. **Confidence: Medium** on the evaluation design's objectivity.

## Next Checks
1. **Dataset Verification:** Create a proxy dataset of 528 C programming questions and answers from public sources (e.g., StackOverflow) and apply the same filtering criteria. Fine-tune a base model and compare its performance to GuideLM on a held-out test set to validate the importance of dataset quality.

2. **Ablation Study on Style Transfer:** Fine-tune two models: one with the grammar correction pass on the training data, and one without. Compare the "Economy of Words" and "Socratic Guidance" scores to isolate the contribution of the preprocessing step.

3. **Generalization Test:** Evaluate GuideLM on a dataset of C programming errors from a different course or institution. Measure the drop in performance to quantify how much the model is overfitting to the specific training data.