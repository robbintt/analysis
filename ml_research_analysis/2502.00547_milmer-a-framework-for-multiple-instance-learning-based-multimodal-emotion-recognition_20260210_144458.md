---
ver: rpa2
title: 'Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion
  Recognition'
arxiv_id: '2502.00547'
source_url: https://arxiv.org/abs/2502.00547
tags:
- facial
- emotion
- fusion
- recognition
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Milmer, a transformer-based multimodal framework
  that integrates EEG signals and facial expressions for emotion recognition. The
  framework leverages multiple instance learning (MIL) to extract temporal information
  from facial expression videos, employs a fine-tuned Swin Transformer for feature
  extraction, and utilizes a cross-attention mechanism to balance modality representations.
---

# Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion Recognition

## Quick Facts
- **arXiv ID:** 2502.00547
- **Source URL:** https://arxiv.org/abs/2502.00547
- **Reference count:** 40
- **Primary result:** Achieves 96.72% accuracy on 4-class emotion classification using EEG + facial expressions with MIL-based temporal fusion

## Executive Summary
This study introduces Milmer, a transformer-based multimodal framework that integrates EEG signals and facial expressions for emotion recognition. The framework leverages multiple instance learning (MIL) to extract temporal information from facial expression videos, employs a fine-tuned Swin Transformer for feature extraction, and utilizes a cross-attention mechanism to balance modality representations. Experiments on the DEAP dataset demonstrate state-of-the-art performance with 96.72% accuracy in four-class emotion classification. Ablation studies confirm the effectiveness of each component, highlighting the significance of advanced feature extraction and fusion strategies in enhancing emotion recognition performance.

## Method Summary
Milmer processes 3-second EEG segments (32 channels, 128Hz) and 10 evenly-spaced facial frames from DEAP dataset. EEG signals undergo 1-50Hz bandpass filtering and Fast ICA artifact removal, then are upsampled via MLP to 768 points. Facial frames are processed through a fine-tuned 6-layer Swin Transformer, with MIL attention scoring selecting top-K instances. Cross-attention compresses visual tokens to 64 learnable queries, balancing representation with EEG tokens. The fused representation passes through a transformer encoder with modal and positional embeddings, using a CLS token for 4-class emotion classification (HAHV, HALV, LAHV, LALV). The model trains for 100 epochs with cosine learning rate decay.

## Key Results
- Achieves 96.72% accuracy and F1-score on 4-class DEAP emotion classification
- MIL mechanism alone improves accuracy from 89.91% (no integration) to 89.83% (MIL only), with full system reaching 96.72%
- Cross-attention token compression to 64 queries optimizes performance, with accuracy dropping to 93.84% (16 tokens) and 95.51% (147 tokens)
- Transformer fusion outperforms concatenation (89.88%) and DeepCCA (88.13%) baselines by over 6 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating video segments as bags of frames with attention-weighted selection improves temporal emotion representation over single-frame baselines.
- **Mechanism:** Multiple Instance Learning (MIL) with attention-based pooling assigns learned weights to each frame in a temporal window, selecting top-K frames by attention score before fusion. This captures transient emotional cues that single-frame methods miss.
- **Core assumption:** Emotional expressivity varies across frames; the most informative frames exist within the bag and can be identified via learned attention scores.
- **Evidence anchors:**
  - [abstract] "A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies."
  - [section 3.3.2] Formula for attention-weighted selection: T = {q | a_q is among the top K largest values}
  - [corpus] Limited direct corpus support for MIL in emotion recognition; Romeo et al. (2019) cited in paper uses MIL for physiological signals but not specifically for video-frame selection in this multimodal context.
- **Break condition:** If attention weights become uniform (no discriminative selection) or if K is set too low/high relative to informative frame density, the mechanism degrades to simple pooling or redundant representation.

### Mechanism 2
- **Claim:** Cross-attention compression of visual tokens balances modality contributions during transformer fusion, preventing visual dominance over EEG.
- **Mechanism:** Learnable query tokens (N) attend to Swin Transformer output tokens (M, where M >> N), compressing visual representation to match EEG token scale before multimodal fusion.
- **Core assumption:** EEG tokens carry complementary but lower-dimensional information that would be drowned out by uncompressed visual tokens in a shared attention mechanism.
- **Evidence anchors:**
  - [section 3.3.3] "The cross-attention mechanism compresses the M visual tokens into N learnable query tokens, where N ≪ M."
  - [section 4.3, Table 4] Cross-attention (96.72%) significantly outperforms no integration (89.91%) and MLP integration (88.83%).
  - [corpus] Cross-attention for multimodal fusion appears in MCN-CL and related work, though specific token-balancing applications to EEG-video fusion are less documented externally.
- **Break condition:** If query count N is set too high, visual dominance returns; if too low, critical facial features are lost. Table 4 shows performance degrades at N=16 (93.84%) and N=32 (95.05%) compared to optimal N=64 (96.72%).

### Mechanism 3
- **Claim:** Transformer-based fusion with modal type and position embeddings captures cross-modal dependencies better than concatenation or correlation-based fusion.
- **Mechanism:** EEG and facial tokens are concatenated with modal type embeddings (learned modality identifiers) and position embeddings, then processed through a transformer encoder with a CLS token for classification.
- **Core assumption:** Heterogeneous modalities (EEG temporal signals vs. facial spatial features) can be aligned in a shared embedding space where self-attention learns cross-modal relationships.
- **Evidence anchors:**
  - [section 3.4] "Modal Type Embedding: To differentiate between the two modalities, we introduce modal type embeddings... enabling the Transformer to process and fuse modality-specific features more effectively."
  - [section 4.4, Table 5] Transformer fusion achieves 96.72% vs. DeepCCA (88.13%) and Concatenation (89.88%).
  - [corpus] Cross-attention and transformer fusion for emotion recognition is supported by MCN-CL and audio-video transformer fusion work; specific EEG-facial transformer fusion is less externally validated.
- **Break condition:** If modal embeddings fail to differentiate modalities, or if position embeddings misalign temporal vs. spatial structure, the transformer may learn spurious correlations or fail to integrate complementary information.

## Foundational Learning

- **Concept: Multiple Instance Learning (MIL)**
  - **Why needed here:** The framework treats a 3-second video as a "bag" of unlabeled frames where only the bag label (emotion) is known. Understanding weak supervision and attention-based pooling is essential to grasp how top-K frame selection works.
  - **Quick check question:** Given a bag of 10 frames with attention scores [0.05, 0.12, 0.08, 0.15, 0.03, 0.18, 0.09, 0.11, 0.14, 0.05], which frames would be selected for K=3?

- **Concept: Cross-Attention Mechanism**
  - **Why needed here:** Used twice—once for MIL-based frame selection and once for visual token compression. Understanding Q/K/V computation and scaled dot-product attention is critical.
  - **Quick check question:** If query tokens have shape (N=64, D=256) and visual tokens have shape (M=49, D=256), what is the shape of the attention matrix before softmax?

- **Concept: EEG Preprocessing (Bandpass Filtering + ICA)**
  - **Why needed here:** Raw EEG contains artifacts (EOG, EMG, line noise). Understanding why 1-50 Hz filtering and ICA decomposition are applied informs expectations about signal quality and residual noise.
  - **Quick check question:** Why might ICA fail to remove artifacts if the recording session is short or if artifact sources are not statistically independent from neural signals?

## Architecture Onboarding

- **Component map:** EEG Preprocessing -> Swin Transformer -> AMIL Attention Scoring -> Cross-Attention Compression -> Transformer Fusion -> CLS Classification
- **Critical path:** Input: 3s EEG segment (384 samples @ 128 Hz) + 10 evenly-spaced video frames → Swin extracts M visual tokens per frame → AMIL selects K frames → Cross-attention compresses to N tokens → EEG and visual tokens fused in transformer → CLS token classification → Output: 4-class emotion (HAHV, HALV, LAHV, LALV)
- **Design tradeoffs:**
  - **K (selected frames):** Higher K captures more temporal info but increases computation; paper uses 10 frames with top-K selection but doesn't specify final K value explicitly—experiments suggest K≈5-10 is reasonable.
  - **N (query tokens):** Table 4 shows optimal N=64; higher values (147) reduce accuracy (95.51%), lower values (16) lose information (93.84%).
  - **Swin fine-tuning:** Improves feature quality but requires emotion-labeled pretraining data; the paper fine-tunes on emotion datasets but doesn't specify which.
- **Failure signatures:**
  - **Visual dominance:** If N is too high, accuracy drops (Table 4: 147 tokens → 95.51% vs. 64 → 96.72%)
  - **Subject-independent generalization gap:** Fig. 3 shows significant accuracy drop in subject-independent vs. subject-dependent settings—common to EEG-based systems due to inter-subject variability.
  - **MIL collapse:** If attention weights become near-uniform, the mechanism degenerates to average pooling; monitor attention distribution variance during training.
- **First 3 experiments:**
  1. **Baseline fusion comparison:** Run ablation with concatenation vs. DeepCCA vs. transformer fusion on DEAP-4 to reproduce Table 5; verify transformer fusion gain (target: >6% improvement).
  2. **Token count sweep:** Vary N from 16 to 196 on the cross-attention output to reproduce Table 4; identify optimal N for your compute budget.
  3. **Single-frame vs. MIL comparison:** Ablate MIL by using only the middle frame vs. top-K selection; target improvement from MIL should be ~3-4% based on Table 6 (MIL alone: 89.83% vs. FT+MIL+CA: 96.72%).

## Open Questions the Paper Calls Out

- **How can the framework's robustness to individual variability be enhanced to bridge the performance gap between subject-dependent and subject-independent settings?**
  - Basis in paper: [explicit] The authors note in Section 4.2 that "future work could focus on strategies to further enhance the model’s robustness to individual variability" due to the "significant performance gap" in subject-independent experiments.
  - Why unresolved: The current model shows a notable accuracy drop when moving from subject-dependent to subject-independent tasks, a common challenge in the field that this specific architecture does not yet fully overcome.
  - What evidence would resolve it: Successful application of domain adaptation or personalization techniques that significantly narrow the accuracy gap in cross-subject validation on the DEAP dataset.

- **Does the inclusion of additional physiological signals, such as ECG and GSR, improve the diversity and robustness of the multimodal fusion?**
  - Basis in paper: [explicit] Section 5 states, "integrating additional physiological signals, such as electrocardiogram (ECG) and galvanic skin response (GSR), could further enhance the diversity and robustness of multimodal learning frameworks."
  - Why unresolved: The current study limits the physiological modality to EEG, leaving the potential synergy of Milmer's transformer-based fusion with peripheral physiological signals unexplored.
  - What evidence would resolve it: Experimental results demonstrating improved classification accuracy or noise resilience when ECG and GSR streams are fused with the existing EEG and facial data.

## Limitations

- **Pre-training specifics:** The Swin Transformer's emotion dataset pre-training details remain unspecified, creating potential performance gaps in reproduction attempts
- **Hyperparameter sensitivity:** Critical values for learning rate, batch size, and optimizer configuration are absent from the paper
- **Subject-independent generalization:** The 96.72% accuracy is achieved in subject-dependent settings, with significantly lower performance expected when generalizing across subjects due to EEG's inherent inter-subject variability

## Confidence

- **High Confidence:** The MIL mechanism's theoretical framework and cross-attention token compression are well-supported by ablation results (Table 4) and established literature
- **Medium Confidence:** Transformer-based fusion with modal embeddings shows clear performance gains over concatenation and DeepCCA baselines (Table 5), but external validation on non-DEAP datasets is needed
- **Low Confidence:** The claim of state-of-the-art performance lacks benchmarking against recent transformer-based multimodal approaches in the emotion recognition literature

## Next Checks

1. **Cross-dataset generalization:** Test Milmer on a separate multimodal emotion dataset (e.g., SEED, DREAMER) to assess whether the 96.72% accuracy on DEAP represents learned task patterns versus dataset-specific overfitting
2. **Attention distribution analysis:** Monitor and report the variance of attention weights during MIL top-K selection across training epochs to verify the mechanism is performing discriminative selection rather than uniform pooling
3. **Component ablation on held-out subjects:** Perform systematic ablation of Swin fine-tuning, MIL, and cross-attention components specifically in subject-independent evaluation to identify which components are most vulnerable to inter-subject variability