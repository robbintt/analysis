---
ver: rpa2
title: LLM-Generated Feedback Supports Learning If Learners Choose to Use It
arxiv_id: '2506.17006'
source_url: https://arxiv.org/abs/2506.17006
tags:
- feedback
- learners
- learning
- llm-generated
- posttest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines whether on-demand LLM-generated explanatory
  feedback enhances learning in scenario-based tutor training. Using a randomized
  controlled design, 885 tutors completed seven lessons with three conditions: LLM
  feedback, declined LLM feedback, or no access.'
---

# LLM-Generated Feedback Supports Learning If Learners Choose to Use It

## Quick Facts
- arXiv ID: 2506.17006
- Source URL: https://arxiv.org/abs/2506.17006
- Reference count: 33
- Primary result: Learners with higher propensity to use LLM feedback scored significantly higher on posttests, with small to moderate effects (0.28-0.33 SD) in two of seven lessons after adjustment.

## Executive Summary
This study examines whether on-demand LLM-generated explanatory feedback enhances learning in scenario-based tutor training. Using a randomized controlled design, 885 tutors completed seven lessons with three conditions: LLM feedback, declined LLM feedback, or no access. Posttest performance served as the primary outcome. To address selection bias, propensity scoring was used to adjust for learners' likelihood of requesting feedback. Results showed that learners with higher predicted likelihood of engaging with LLM feedback scored significantly higher on posttests, with small to moderate effects (0.28-0.33 SD) in two of seven lessons after adjustment. LLM feedback did not significantly increase completion time, and learners overwhelmingly rated it as helpful. These findings suggest LLM feedback can support learning when learners choose to use it, offering a scalable addition to existing feedback systems.

## Method Summary
The study evaluated on-demand LLM-generated explanatory feedback in a scenario-based tutor training program with 885 tutors completing seven lessons. GPT-3.5-turbo generated feedback using few-shot prompting and schema-based classification, providing targeted explanations for incorrect responses. The primary outcome was posttest performance scored by GPT-4o, with propensity score adjustment using ElasticNet regression (ROC-AUC M=0.75) to address self-selection bias. Analysis employed linear mixed models with random intercepts for learner and lesson, plus principal stratification to compare treatment-on-treated versus intent-to-treat effects.

## Key Results
- Learners with higher propensity to engage with LLM feedback showed significant posttest gains (0.28-0.33 SD) in two lessons after adjustment
- LLM feedback did not increase completion time (difference of merely 9 seconds)
- Learners rated LLM feedback highly helpful (M=4.46/5) across all lessons
- Effect sizes varied substantially across lessons (-0.15 to 0.33 SD), with motivation skills showing stronger effects than equity lessons

## Why This Works (Mechanism)

### Mechanism 1: Active Help-Seeking Selection Effect
- Claim: LLM feedback effectiveness is moderated by learners' propensity to seek support, not merely by feedback availability.
- Mechanism: Learners who proactively request feedback may possess greater metacognitive awareness, intrinsic motivation, or pre-existing learning strategies that enable them to process and apply feedback more effectively.
- Core assumption: Help-seeking behavior reflects learner readiness to engage cognitively with feedback content.
- Evidence anchors:
  - [abstract]: "Learners with a higher predicted likelihood of engaging with LLM feedback scored significantly higher at posttest than those with lower propensity."
  - [section]: "The key lesson learned from the present study is that even if the LLM feedback was effective, its effectiveness hinges on the willingness of learners to request and engage with it." (p. 11)
  - [corpus]: Weak/missing direct support—corpus papers focus on feedback generation quality, not help-seeking moderation effects.
- Break condition: If feedback becomes mandatory rather than on-demand, the selection effect should attenuate or disappear.

### Mechanism 2: Explanatory Feedback Enhancement Over Corrective Baseline
- Claim: LLM-generated explanatory feedback adds learning value beyond existing corrective feedback systems.
- Mechanism: The LLM provides not merely correctness information but rephrased, research-aligned alternatives with minimal modifications—preserving learner intent while improving alignment with target skills.
- Core assumption: Learners can transfer insights from the explanatory feedback to novel posttest scenarios.
- Evidence anchors:
  - [abstract]: "LLM feedback can support learning when learners choose to use it, offering a scalable addition to existing feedback systems."
  - [section]: "Incorrect responses trigger the generation of targeted feedback that not only identifies weaknesses, but also provides a rephrased version generated by the LLM that adheres better to the research-aligned tutoring skill." (p. 4)
  - [corpus]: "A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI" describes generative AI scaling formative assessment, aligning with explanatory feedback potential.
- Break condition: If learners skip or superficially read long feedback, the explanatory mechanism fails to operate.

### Mechanism 3: Time-Neutral Efficiency via Fluency Gains
- Claim: LLM feedback improves posttest performance without increasing completion time.
- Mechanism: Feedback may produce fluency gains that offset reading time, or learners selectively process only relevant portions. Alternatively, learners who receive feedback may produce longer, more thoughtful posttest responses efficiently.
- Core assumption: Learners can integrate feedback without cognitive overload that would slow subsequent tasks.
- Evidence anchors:
  - [abstract]: "LLM feedback did not significantly increase completion time."
  - [section]: "Learners receiving feedback taking an average of 5.75 minutes... others took an average of 5.59 minutes, which is a difference of merely 9 seconds." (p. 10)
  - [corpus]: Weak/missing—no corpus papers directly address time-efficiency mechanisms for LLM feedback.
- Break condition: If feedback verbosity increases substantially or task complexity rises, time costs may emerge.

## Foundational Learning

- Concept: **Propensity Score Adjustment for Selection Bias**
  - Why needed here: On-demand feedback creates self-selection; without adjustment, treatment effects conflate learner characteristics with intervention effects.
  - Quick check question: Why would comparing only learners who chose feedback against those who didn't yield biased effect estimates?

- Concept: **Intent-to-Treat (ITT) vs. Treatment-on-the-Treated (TOT) Analysis**
  - Why needed here: The study distinguishes effect of being offered feedback (ITT) from effect of actually receiving it (TOT), critical for interpreting real-world intervention impact.
  - Quick check question: If 30% of offered learners never use feedback, which analysis preserves randomization integrity?

- Concept: **Explanatory vs. Corrective Feedback**
  - Why needed here: The intervention layers LLM explanatory feedback on top of existing corrective feedback; understanding this distinction is essential for mechanism interpretation.
  - Quick check question: What learning advantage does rephrasing + explanation provide over simply marking an answer incorrect?

## Architecture Onboarding

- Component map:
  - Learner submits open response → Schema classification (correct/incorrect) → If incorrect + requested → LLM generates targeted explanatory feedback with minimal rephrasing → Learner can retry (≤3 attempts), optionally rate feedback helpfulness → Posttest assessed via GPT-4o; propensity adjustment applied in analysis

- Critical path:
  1. Learner submits open response → Schema classification (correct/incorrect)
  2. If incorrect + requested → LLM generates targeted explanatory feedback with minimal rephrasing
  3. Learner can retry (≤3 attempts), optionally rate feedback helpfulness
  4. Posttest assessed via GPT-4o; propensity adjustment applied in analysis

- Design tradeoffs:
  - On-demand vs. mandatory: Preserves learner agency but introduces selection bias requiring statistical adjustment
  - GPT-3.5-turbo vs. newer models: Lower cost but 15.7% API failure rate may reduce trust
  - Optional feedback ratings: High response burden if mandatory, but current design lacks nuanced perception data

- Failure signatures:
  - API failures (15.7%) → learners assigned to treatment unable to receive feedback
  - Low uptake in specific lessons (e.g., "What Students Know": 25 TOT vs. 102 declined)
  - Potential copy/adapt behavior: learners who received feedback produced 6 words longer posttest responses on average, risking inflated scores

- First 3 experiments:
  1. Randomize mandatory vs. on-demand feedback within lessons to isolate selection effect from treatment effect.
  2. Instrument time spent viewing feedback to distinguish skimming from deep processing.
  3. Vary feedback length/conciseness to identify thresholds where time costs emerge without learning gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would requiring learners to engage with LLM feedback (rather than making it optional/on-demand) produce larger learning gains?
- Basis in paper: [explicit] The authors state: "It is possible, though subject to future research, that, if learners were required to request LLM feedback benefits would be larger."
- Why unresolved: The study design made feedback optional, introducing self-selection bias that propensity scoring only partially addressed.
- What evidence would resolve it: A randomized experiment comparing mandatory LLM feedback exposure versus on-demand access.

### Open Question 2
- Question: Why did lessons on motivation skills show significant learning effects while equity-related lessons did not?
- Basis in paper: [explicit] The authors note: "Future work will also explore why lessons on motivation showed stronger effects, while equity lessons did not—potentially due to content complexity or feedback alignment."
- Why unresolved: Effect sizes varied substantially across lessons (-0.15 to 0.33 SD) without clear explanatory factors.
- What evidence would resolve it: Systematic analysis comparing feedback quality, task complexity, and schema alignment across lesson types.

### Open Question 3
- Question: Do learners actually read and cognitively process LLM-generated feedback, or do they skip ahead without engagement?
- Basis in paper: [explicit] The authors state: "Future work will analyze log data for time spent reading LLM feedback versus skipping ahead."
- Why unresolved: No time-on-feedback metrics were collected; lack of completion time differences may suggest superficial processing.
- What evidence would resolve it: Fine-grained log analysis of scroll behavior, time on feedback pages, and eye-tracking or comprehension checks.

### Open Question 4
- Question: Do learners inappropriately copy or adapt LLM feedback into their subsequent responses, inflating posttest performance?
- Basis in paper: [inferred] The authors acknowledge: "it is also possible that some learners copied or adapted the LLM-generated feedback from Q1, which could have inflated their performance."
- Why unresolved: Learners receiving LLM feedback produced significantly longer posttest responses, which could indicate either deeper learning or copying behavior.
- What evidence would resolve it: Text similarity analysis between LLM feedback and learner posttest responses, controlling for response length.

## Limitations

- Reliance on self-selection into LLM feedback condition creates potential confounding that propensity score adjustment cannot fully eliminate
- Small number of posttest items (two open-response questions) limits generalizability to broader learning outcomes
- No instrumentation of actual time spent viewing feedback content, making it unclear whether learners engage deeply or superficially

## Confidence

- **High confidence:** The absence of completion time differences between conditions; the overall positive learner ratings of feedback helpfulness (M=4.46/5)
- **Medium confidence:** The specific lessons showing significant propensity-adjusted effects (Giving Effective Praise, Supporting a Growth Mindset); the mechanism that explanatory feedback adds value beyond existing corrective feedback
- **Low confidence:** The exact contribution of LLM feedback versus learner selection effects; whether observed gains persist beyond immediate posttest scenarios

## Next Checks

1. Conduct a within-subjects replication where the same learners receive both mandatory and on-demand feedback across different lessons to isolate selection effects from treatment effects.

2. Implement instrumentation to measure actual time spent viewing feedback content, distinguishing between superficial viewing and deep processing that might explain time-neutral learning gains.

3. Test feedback length variations (concise vs. detailed explanations) to identify optimal balance between information density and cognitive load that maintains time efficiency while maximizing learning transfer.