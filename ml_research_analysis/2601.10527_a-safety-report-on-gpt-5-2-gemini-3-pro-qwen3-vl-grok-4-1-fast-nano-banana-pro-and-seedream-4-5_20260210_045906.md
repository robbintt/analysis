---
ver: rpa2
title: A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana
  Pro, and Seedream 4.5
arxiv_id: '2601.10527'
source_url: https://arxiv.org/abs/2601.10527
tags:
- safety
- evaluation
- adversarial
- across
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This report evaluates the safety of six frontier models\u2014\
  GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5\u2014\
  across language, vision-language, and image generation settings. A unified protocol\
  \ combining benchmark, adversarial, multilingual, and compliance evaluations was\
  \ used to assess each model."
---

# A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5

## Quick Facts
- arXiv ID: 2601.10527
- Source URL: https://arxiv.org/abs/2601.10527
- Authors: Xingjun Ma; Yixu Wang; Hengyuan Xu; Yutao Wu; Yifan Ding; Yunhan Zhao; Zilong Wang; Jiabin Hua; Ming Wen; Jianan Liu; Ranjie Duan; Yifeng Gao; Yingshui Tan; Yunhao Chen; Hui Xue; Xin Wang; Wei Cheng; Jingjing Chen; Zuxuan Wu; Bo Li; Yu-Gang Jiang
- Reference count: 16
- Key outcome: Unified safety evaluation across language, vision-language, and image generation reveals trade-offs; GPT-5.2 leads overall with strong balanced performance while other models show clear modality-specific vulnerabilities and safety-robustness trade-offs.

## Executive Summary
This report evaluates the safety of six frontier models across three modalities using a unified protocol combining benchmark, adversarial, multilingual, and compliance assessments. GPT-5.2 demonstrates the most balanced safety profile across all dimensions, while other models exhibit clear trade-offs between safety robustness, multilingual generalization, and regulatory compliance. Across all models, worst-case adversarial safety rates remain below 6%, highlighting persistent jailbreak vulnerabilities despite recent safety advances.

## Method Summary
The evaluation uses five language benchmarks (ALERT, Flames, BBQ, SORRY-Bench, StrongREJECT) with 100 prompts each, 30 black-box jailbreak attacks across 10 strategy categories, four vision-language benchmarks (MemeSafetyBench, MIS, USB-SafeBench, SIUO), T2ISafety for image generation (315 prompts), and multilingual testing using PolyGuardPrompt and ML-Bench for 18+ languages. Qwen3Guard serves as the text safety judge while Grok 4 Fast evaluates image toxicity. The protocol measures Safe rate, Safe worst (defends against all attacks), Safe worst-3 (defends against top-3 attacks), refusal behavior, multilingual Micro F1, compliance rate, and toxicity scores.

## Key Results
- GPT-5.2 achieves the highest overall safety performance with strong benchmark and adversarial scores across all modalities
- Worst-case adversarial safety rates drop below 6% for all models, indicating persistent jailbreak vulnerabilities
- Grok 4.1 Fast shows dramatic multilingual safety collapse from 97% (English) to 3% (Chinese) under identical attacks
- Text-to-image models remain fragile under adversarial or semantically ambiguous prompts

## Why This Works (Mechanism)

### Mechanism 1
Unified cross-modal evaluation reveals safety trade-offs invisible to single-modality assessments. By applying the same evaluation dimensions across language-only, vision-language, and image generation modes, the protocol forces direct comparison of safety generalization. Models that appear safe in one modality can be systematically compared to their performance in another.

### Mechanism 2
Worst-case adversarial testing exposes a 6% safety ceiling that benchmark evaluations systematically overestimate. The protocol uses 30 black-box jailbreak attacks across 10 strategy categories, with Safe-worst metrics capturing the percentage of queries defended against all attacks simultaneously, revealing brittleness when any single attack succeeds.

### Mechanism 3
Safety profiling via multi-dimensional radar charts operationalizes trade-offs between refusal behavior, semantic reasoning, and compliance. Projecting results onto radar charts with axes for benchmark, adversarial, multilingual, and compliance scores reveals distinct archetypes: "Comprehensive Generalist," "Polarized Rule-Follower," and "Guardrail-Light."

## Foundational Learning

- **Jailbreak Attack Taxonomy (Template-based vs. Adaptive Multi-turn)**: Understanding why Safe-worst remains at 6% requires distinguishing attacks that exploit surface patterns from those leveraging iterative interaction. Quick check: Why do adaptive multi-turn attacks succeed where template-based attacks fail?

- **Cross-Lingual Safety Generalization**: Section 2.2.3 documents Grok 4.1 Fast's collapse from 97% (English) to 3% (Chinese) under identical attacks. Safety alignment is English-centric. Quick check: A model passes adversarial testing in English. What additional validation is needed before multilingual deployment?

- **Compositional Multimodal Risk**: Section 3.1.1 introduces SIUO benchmark for cross-modal misalignmentâ€”individually benign text and images that combine to produce unsafe outputs. Safety pipelines must reason jointly over modalities. Quick check: Your model passes image-only and text-only safety tests. What failure mode might still occur in deployment?

## Architecture Onboarding

- **Component map**: Benchmark Layer (ALERT, Flames, BBQ, SORRY-Bench, StrongREJECT; MemeSafetyBench, MIS, USB-SafeBench, SIUO; T2ISafety) -> Adversarial Layer (30-attack suite across 10 strategy categories) -> Judge Layer (Qwen3Guard for text, Grok 4 Fast for image) -> Aggregation Layer (Safety leaderboards, radar profiles, archetype classification)

- **Critical path**: Prompt/Query -> Attack Suite Application -> Model Response -> Judge Classification -> Metric Aggregation -> Profile Generation

- **Design tradeoffs**: Breadth vs. depth (100 prompts per benchmark vs. exhaustive coverage); Automated vs. manual judgment (Qwen3Guard consistency vs. human review for borderline cases); Safe-worst vs. Safe-3 (pessimistic real-world reflection vs. lenient masking of vulnerabilities)

- **Failure signatures**: Analytical operationalization (academic framing with actionable harmful content); Refusal drift (initial refusal followed by compliance under multi-turn pressure); Cross-lingual collapse (high English safety with near-zero safety in other languages)

- **First 3 experiments**:
  1. Reproduce Safe-worst vs. Safe-3 gap: Run the 30-attack suite on a target model; verify Safe-worst remains below 10% while Safe-3 exceeds 30%
  2. Profile archetype classification: Generate radar charts for two models; confirm they map to distinct archetypes (e.g., "Polarized Rule-Follower" vs. "Guardrail-Light")
  3. Cross-lingual vulnerability test: Apply CipherChat attack in English and Chinese; measure safety rate differential (target: >50% gap indicates alignment language bias)

## Open Questions the Paper Calls Out

### Open Question 1
How can safety mechanisms be designed to defend against adaptive, multi-turn jailbreak attacks that exploit sustained interaction and agentic control over dialogue? The paper states that "defenses that perform well against static prompts often fail to contain long-horizon, adaptive jailbreak strategies" and identifies this as "the primary remaining risk surface."

### Open Question 2
What alignment methods can achieve robust cross-lingual safety generalization, particularly for low-resource languages and culturally distinct contexts? The paper reports that "current safety alignment remains heavily English-centric, creating brittle defenses that shatter when the linguistic medium changes."

### Open Question 3
How can models be trained to recognize regulatory violations in contextually complex requests where compliance itself constitutes a violation? The paper notes models "fail to recognize that complying with a user's specific request (e.g., drafting a memo, designing a study) can itself constitute a regulatory violation."

### Open Question 4
Do safety evaluation results under static, controlled conditions predict real-world deployment behavior under distributional shift and adaptive user behavior? The limitations section states evaluations "cannot capture long-tail risks or emergent behaviors" and that "factors such as distributional shift, continuous model updates, user adaptation, and platform-specific safeguards lie outside the scope."

## Limitations

- Model access uncertainty for GPT-5.2, Gemini 3 Pro, Nano Banana Pro, and Seedream 4.5 (future/hypothetical models)
- ML-Bench multilingual dataset is privately constructed and not publicly available
- Qwen3Guard judge model access details are unclear, limiting reproducibility
- Safety evaluation results are highly time-sensitive due to rapid model updates and evolving attack strategies

## Confidence

**High Confidence**: Unified protocol provides comprehensive safety assessment framework; cross-modal evaluation reveals distinct safety trade-offs; worst-case adversarial safety rates below 6% across all models indicate persistent jailbreak vulnerabilities.

**Medium Confidence**: GPT-5.2 demonstrates "Comprehensive Generalist" safety profile with strong performance across all dimensions; multilingual safety generalization shows significant English-centric bias; text-to-image models remain fragile under adversarial or semantically ambiguous prompts.

**Low Confidence**: Specific safety rates for future model versions without access to those models; archetype classifications without standardized validation methodology; cross-lingual F1 scores without full dataset transparency.

## Next Checks

1. Reproduce Safe-worst vs. Safe-3 gap: Apply the 30-attack suite to a current model version and verify the 6% worst-case vs. ~40% Safe-3 gap persists.

2. Cross-Lingual Vulnerability Test: Run CipherChat attack in English and Chinese on a multilingual model; measure safety rate differential to confirm >50% gap indicating alignment language bias.

3. Judge Agreement Validation: Implement the Qwen3Guard evaluation pipeline with current judge model; run benchmark tests and verify classification consistency with reported safety rates to identify potential judge disagreement failure modes.