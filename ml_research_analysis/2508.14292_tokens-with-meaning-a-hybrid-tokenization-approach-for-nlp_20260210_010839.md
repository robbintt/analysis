---
ver: rpa2
title: 'Tokens with Meaning: A Hybrid Tokenization Approach for NLP'
arxiv_id: '2508.14292'
source_url: https://arxiv.org/abs/2508.14292
tags:
- tokenization
- morphological
- token
- linguistic
- turkish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid tokenizer that integrates rule-based
  morphological analysis with statistical subword segmentation to improve linguistic
  integrity in Turkish NLP. By incorporating phonological normalization, root-affix
  dictionaries, and BPE for out-of-vocabulary handling, the method preserves morpheme
  boundaries while reducing vocabulary redundancy.
---

# Tokens with Meaning: A Hybrid Tokenization Approach for NLP

## Quick Facts
- arXiv ID: 2508.14292
- Source URL: https://arxiv.org/abs/2508.14292
- Reference count: 32
- Primary result: Achieves 90.29% Turkish Token Percentage and 85.80% Pure Token Percentage on TR-MMLU benchmark, outperforming major tokenizers

## Executive Summary
This study introduces a hybrid tokenizer that integrates rule-based morphological analysis with statistical subword segmentation to improve linguistic integrity in Turkish NLP. By incorporating phonological normalization, root-affix dictionaries, and BPE for out-of-vocabulary handling, the method preserves morpheme boundaries while reducing vocabulary redundancy. The tokenizer assigns shared IDs to phonologically variant affixes and altered roots, uses special tokens for whitespace and case, and includes an UPPERCASE token to prevent vocabulary inflation. Evaluated on the TR-MMLU benchmark, it achieves the highest Turkish Token Percentage (90.29%) and Pure Token Percentage (85.80%) among all tested models, outperforming widely used tokenizers such as LLaMA, Gemma, and GPT. Qualitative analysis confirms more semantically coherent and interpretable tokenization. Although demonstrated on Turkish, the approach is language-independent and adaptable to other morphologically rich languages, offering a practical path toward more interpretable and effective multilingual NLP systems.

## Method Summary
The method employs a hierarchical pipeline that first attempts dictionary-based morphological segmentation, then falls back to BPE for out-of-vocabulary words. The encoder preprocesses text by mapping whitespace/newline/tab to special tokens and prepending `<uppercase>` for capitalized words. It then performs longest-match root lookup using a 22,000-entry root dictionary, applies iterative suffix matching with shared IDs for phonological allomorphs, and finally uses SentencePiece BPE (10K vocab) for remaining portions. Phonological normalization unifies vowel harmony variants and final devoicing forms, while decoding reverses these alternations using disambiguation rules. The approach assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and phonologically altered root forms (e.g., kitap vs. kitabı), reducing vocabulary redundancy while preserving morphological semantics.

## Key Results
- Achieves highest Turkish Token Percentage (90.29%) and Pure Token Percentage (85.80%) among tested models on TR-MMLU benchmark
- Outperforms major tokenizers including LLaMA, Gemma, GPT, Qwen, and Aya-Expanse
- Demonstrates more semantically coherent and interpretable tokenization through qualitative analysis
- Uses smaller vocabulary (32K) compared to Gemma (255K+) while maintaining superior linguistic integrity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phonological normalization reduces vocabulary redundancy while preserving morphological semantics.
- Mechanism: Surface allomorphs that share grammatical function (e.g., Turkish plural markers -ler/-lar, ablative markers -dAn/-tAn, or root alternations like kitap→kitabı) are mapped to shared token IDs rather than stored as separate vocabulary entries. This consolidation occurs during dictionary construction, where phonological rules identify variant forms belonging to the same morpheme.
- Core assumption: Phonological variation within morphemes carries primarily phonetic rather than semantic information; models can learn to process normalized forms without needing surface-level distinctions at the token level.
- Evidence anchors: [abstract] "assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and phonologically altered root forms (e.g., kitap vs. kitabı)"; [section 8] "affixes with identical grammatical functions, such as the plural markers '-lAr' or the ablative markers '-dAn,' are assigned a common identifier"; [corpus] MorphBPE (arXiv:2502.00894) uses similar morpheme-aware grouping.

### Mechanism 2
- Claim: Hierarchical morphological segmentation before statistical fallback produces linguistically coherent tokens.
- Mechanism: The encoder applies a strict priority order: (1) longest-match root detection from dictionary, (2) iterative suffix matching, (3) BPE segmentation for remaining portions. This ensures morpheme boundaries are respected whenever possible, with BPE serving only as safety net for OOV words rather than primary segmentation strategy.
- Core assumption: Dictionary-based morphological parsing captures the linguistically relevant segmentation more reliably than frequency-based merging, particularly for agglutinative languages where surface frequency does not correlate with morphological importance.
- Evidence anchors: [section 9] "The algorithm first searches for the longest matching root in the dictionary, prioritizing exact matches before considering phonological variants"; [figure 1] Decision flow explicitly shows root→suffix→BPE fallback hierarchy; [corpus] MorphTok (arXiv:2504.10335) and MoVoC (arXiv:2509.08812) employ similar morpheme-first approaches.

### Mechanism 3
- Claim: Explicit structural tokens preserve document-level information without vocabulary inflation.
- Mechanism: Rather than treating whitespace, case, and formatting as implicit or encoding them within word tokens, the framework assigns dedicated tokens (<uppercase>, <space>, newline, tab). This allows reconstruction of original text structure and prevents capitalization from creating duplicate vocabulary entries (e.g., "Word" vs "word" share the same root token, differentiated only by preceding <uppercase> marker).
- Core assumption: Explicit encoding of orthographic and structural features improves downstream task performance on documents where layout, case, and spacing carry meaning (structured parsing, code, formatted text).
- Evidence anchors: [abstract] "special tokens for whitespace and case, including an UPPERCASE marker to avoid vocabulary inflation from capitalization"; [section 9] "whitespace characters such as spaces, newlines, and tab spaces are explicitly encoded using dedicated tokens... words that begin with capital letters are marked with an uppercase token".

## Foundational Learning

- Concept: **Agglutinative Morphology**
  - Why needed here: The paper's entire motivation rests on the failure of frequency-based tokenization for languages where single words contain multiple concatenated morphemes (Turkish, Finnish, Hungarian). Without understanding that Turkish words like "anlayabildiklerimizden" decompose into 6+ morphemes, the tokenization challenge is unintelligible.
  - Quick check question: Given the Turkish word "evlerimizden" (from our houses), can you identify at least three morpheme boundaries?

- Concept: **Vowel Harmony and Consonant Alternation**
  - Why needed here: Phonological normalization (Mechanism 1) relies on recognizing that surface variants like -ler/-lar or -dAn/-tAn are phonologically conditioned allomorphs. Without grasping Turkish phonological rules, the shared-ID strategy appears arbitrary.
  - Quick check question: Why would a Turkish plural suffix have two surface forms (-ler, -lar), and what determines which appears?

- Concept: **BPE Algorithm**
  - Why needed here: The hybrid approach positions BPE as fallback; understanding what BPE does (iteratively merge most frequent character pairs) clarifies why it fails on morphologically complex words and how morphological pre-segmentation prevents those failures.
  - Quick check question: If BPE sees "kitaplar" and "kitaplık" frequently but "kitap" rarely, what subword tokens might it create, and why is this problematic?

## Architecture Onboarding

- Component map: Input Text -> Preprocessing Layer [case detection → <uppercase> token insertion, whitespace → explicit tokens] -> Morphological Dictionary Lookup [root dictionary (22K entries, hierarchical by length) → suffix dictionary (~230 entries)] -> BPE Fallback Layer [10K subword vocabulary from SentencePiece, trained on 8.52GB Turkish corpus] -> Token ID Assignment + Special Token Insertion -> Output: Token ID Sequence

- Critical path: Root dictionary quality and coverage. If root lookup fails or returns incorrect matches, suffix segmentation and BPE fallback receive corrupted input, propagating errors. The hierarchical longest-match strategy depends on accurate root identification as its first gate.

- Design tradeoffs:
  - **Vocabulary size vs. morphological coverage**: 32K vocabulary is smaller than Gemma (255K+) but requires careful curation; under-resourced languages may lack morphological dictionaries to populate this approach.
  - **Token count vs. interpretability**: Hybrid tokenizer produces more total tokens (707K) than aya-expanse (434K) on same corpus, but higher Pure % (85.8% vs. unstated, likely <40%) trades sequence length for linguistic coherence.
  - **Language-specificity vs. multilingual deployment**: Approach is described as language-independent but requires per-language morphological resources; zero-shot transfer to unresourced languages is not addressed.

- Failure signatures:
  - High BPE fallback rate indicates dictionary coverage gaps; monitor ratio of dictionary-derived vs. BPE-derived tokens during validation.
  - Poor reconstruction fidelity during decode suggests phonological normalization rules are over-aggressive or incomplete; test roundtrip encode-decode on held-out corpus.
  - Excessive unknown tokens in domain-specific text (medical, legal) signals vocabulary mismatch; evaluate on target domain early.

- First 3 experiments:
  1. **Reproduce TR-MMLU tokenization metrics** on the paper's benchmark (1.6M characters). Implement TR % and Pure % calculations exactly as defined in referenced [9], compare against baseline tokenizers (LLaMA-3, Gemma-2) to validate claimed 90.29% and 85.80% figures.
  2. **Ablate phonological normalization** by training two variants: one with shared allomorph IDs, one with surface-form distinct IDs. Measure vocabulary size reduction and downstream task performance delta (if language model training resources available) or proxy via token count/sequence length on standard corpus.
  3. **Port to second agglutinative language** (Finnish or Hungarian) using publicly available morphological resources. Test whether TR % and Pure % gains replicate, or whether language-specific tuning (phonological rules, dictionary coverage) dominates the mechanism. This tests the claimed language-independence.

## Open Questions the Paper Calls Out

- **Downstream Task Performance**: Does training a large language model (LLM) from scratch with this hybrid tokenizer yield statistically significant improvements in downstream task performance (e.g., MMLU accuracy, perplexity) compared to models trained with standard BPE? The paper evaluates tokenization metrics but does not validate this by training a model.

- **Language-Portability**: How effectively does the framework transfer to other morphologically rich or agglutinative languages (e.g., Finnish, Arabic) without requiring extensive manual re-engineering of the root-affix dictionaries? The current implementation relies on Turkish-specific dictionaries and phonological rules.

- **Semi-Supervised Refinement**: Can semi-supervised learning techniques be used to automatically refine or expand the morphological rules and dictionaries to handle domain-specific terminology or evolving language use? The methodology relies on static dictionaries that may lack coverage for domain-specific jargon.

- **Phonological Fidelity Trade-offs**: Does the aggressive normalization of phonologically variant affixes (mapping surface variants like *-ler* and *-lar* to shared IDs) negatively impact performance on tasks requiring sensitivity to phonological details, such as speech synthesis or dialect identification? The paper does not analyze this trade-off.

## Limitations

- **Resource Dependencies**: Effectiveness critically depends on availability and quality of morphological dictionaries, which must be constructed manually or through semi-supervised methods for under-resourced languages, introducing potential coverage gaps.

- **Phonological Rule Completeness**: The paper provides only examples of phonological normalization rules without specifying the complete rule inventory, creating uncertainty about over-normalization or under-normalization scenarios.

- **Downstream Task Validation**: Primary evaluation focuses on tokenization metrics (TR % and Pure %) rather than downstream task performance improvements, leaving unclear whether linguistic improvements translate to practical gains.

- **Reproducibility Barriers**: Key implementation details including exact morphological dictionaries, complete phonological normalization rules, SentencePiece hyperparameters, and compound word handling criteria are underspecified, limiting direct replication.

## Confidence

- **High Confidence**: The hybrid architecture (dictionary lookup + BPE fallback) is well-defined and implementable. The hierarchical segmentation strategy is clearly specified and theoretically sound for agglutinative morphology.

- **Medium Confidence**: The core claim that morphological awareness improves tokenization (higher TR % and Pure %) is supported by the TR-MMLU benchmark results, but limited to Turkish and lacks downstream task validation.

- **Low Confidence**: Claims about language-independence and practical improvements for other morphologically rich languages lack empirical validation beyond theoretical discussion and related work citations.

## Next Checks

1. **Downstream Task Performance**: Implement the tokenizer in a Turkish language model and evaluate on standard benchmarks (masked language modeling, text classification). Compare performance against models using baseline tokenizers to verify that linguistic improvements translate to practical gains.

2. **Language-Portability Study**: Apply the approach to a second agglutinative language (Finnish or Hungarian) with available morphological resources. Measure whether TR % and Pure % improvements replicate, and identify language-specific tuning requirements that affect performance.

3. **Robustness to Dictionary Gaps**: Systematically remove root entries from the dictionary and measure degradation in TR % and Pure % metrics. This quantifies the sensitivity to dictionary coverage and helps estimate performance for under-resourced languages where complete morphological lexicons are unavailable.