---
ver: rpa2
title: A Minimax Approach to Ad Hoc Teamwork
arxiv_id: '2502.02377'
source_url: https://arxiv.org/abs/2502.02377
tags:
- policy
- learning
- utility
- policies
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust ad hoc teamwork, where
  an agent must cooperate effectively with unknown teammates without prior coordination.
  The core contribution is adapting minimax-Bayes reinforcement learning to the multi-agent
  setting, where policies are trained against an adversarial prior over teammates
  rather than a fixed distribution.
---

# A Minimax Approach to Ad Hoc Teamwork

## Quick Facts
- **arXiv ID**: 2502.02377
- **Source URL**: https://arxiv.org/abs/2502.02377
- **Reference count**: 40
- **Primary result**: Minimax-Bayes RL approach achieves superior worst-case utility and regret in ad hoc teamwork compared to self-play, fictitious play, and population best response baselines.

## Executive Summary
This paper addresses the challenge of robust ad hoc teamwork, where an agent must cooperate effectively with unknown teammates without prior coordination. The core contribution is adapting minimax-Bayes reinforcement learning to the multi-agent setting, where policies are trained against an adversarial prior over teammates rather than a fixed distribution. This approach optimizes for worst-case performance guarantees rather than performance on a specific distribution. The method is evaluated on both a fully observable Iterated Prisoner's Dilemma and a partially observable collaborative cooking task (Overcooked) from the Melting Pot suite. Results show that policies trained using the minimax framework achieve superior worst-case utility and regret compared to baselines including self-play, fictitious play, and population best response.

## Method Summary
The paper proposes a maximin game formulation where the focal policy maximizes expected utility while an adversarial prior over training scenarios minimizes it. This is implemented via Gradient Descent-Ascent (GDA) with Proximal Policy Optimization (PPO) for the policy updates. The method trains against an adversarial prior that concentrates probability on scenarios where the current policy performs worst, forcing the policy to improve on its weakest points. Two objectives are explored: maximin utility (optimizing worst-case utility) and minimax regret (optimizing worst-case regret, which requires computing best-response utilities). The approach is evaluated on Iterated Prisoner's Dilemma and a collaborative cooking task, showing superior worst-case performance compared to baselines.

## Key Results
- On Collaborative Cooking, minimax-utility approach achieved 266.9±4.3 average utility and 225.3±11.5 worst-case utility on training scenarios, outperforming all other methods.
- On Iterated Prisoner's Dilemma, Minimax Regret (MR) achieved the lowest worst-case regret (R_max=4.35) compared to Population Best Response (PBR, R_max=5.63) and Maximin Utility (MU, R_max=9.00).
- The approach demonstrated faster learning curves and improved sample efficiency compared to baselines, showing a curriculum-like effect from the evolving adversarial distribution.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The minimax-Bayes formulation yields policies with improved worst-case utility guarantees on unseen partner distributions.
- **Mechanism**: The paper frames Ad Hoc Teamwork (AHT) as a maximin game where the focal policy maximizes expected utility while an adversarial prior over training scenarios minimizes it. Through Gradient Descent-Ascent, the prior concentrates probability on scenarios where the current policy performs worst, forcing the policy to improve specifically on its weakest points.
- **Core assumption**: The training background population is diverse enough that the set of induced scenarios forms an ε-net for the true distribution of partners.
- **Evidence anchors**: Abstract states the approach "optimizes policies against an adversarial prior over partners... improves worst-case performance guarantees." Results show MU achieves highest worst-case utility on test sets.
- **Break condition**: If the background population is narrow or unrepresentative, the learned policy may achieve optimal worst-case utility on training scenarios but perform poorly on novel, out-of-distribution teammates.

### Mechanism 2
- **Claim**: Optimizing minimax regret leads to lower worst-case regret than optimizing for utility or using a uniform training distribution.
- **Mechanism**: Instead of utility, the adversary maximizes Bayesian regret, where regret is the gap between optimal and achieved utility. This prevents the adversary from focusing on "degenerate" scenarios where all policies perform poorly.
- **Core assumption**: Best-response utilities can be accurately estimated for each training scenario.
- **Evidence anchors**: Section 8.1 results on IPD show MR achieves lowest worst-case regret (4.35) compared to PBR (5.63) and MU (9.00).
- **Break condition**: If best-response estimates are noisy or computed with insufficient samples, the regret signal becomes unreliable, misguiding the adversary's distribution update.

### Mechanism 3
- **Claim**: Gradient Descent-Ascent with delayed focal policy copies can approximate solutions for both maximin utility and minimax regret.
- **Mechanism**: The algorithm iteratively updates policy parameters and the scenario distribution. Using delayed parameters for partner copies treats them as quasi-fixed background, reducing non-stationarity.
- **Core assumption**: The bilinearity required for GDA convergence is not fundamentally broken by partial observability or the multi-agent nature of the focal team.
- **Evidence anchors**: Section 7 describes the adaptation of GDA and introduces delayed policies. Figure 4 shows MU and MR achieve better final performance and learn faster than PBR.
- **Break condition**: Improper tuning of learning rates or delay can cause instability, with the distribution collapsing onto a single scenario or oscillating wildly.

## Foundational Learning

- **Concept: Ad Hoc Teamwork (AHT)**
  - **Why needed here**: This is the core problem domain. Understanding AHT—collaborating with unknown partners without prior coordination—is essential to frame why robustness and worst-case performance are the primary objectives.
  - **Quick check question**: How does AHT differ from standard multi-agent reinforcement learning with a fixed team?

- **Concept: Minimax / Robust Optimization**
  - **Why needed here**: The paper's core contribution is formulating AHT as a maximin game. Familiarity with this concept is needed to understand the adversarial prior and the resulting worst-case guarantees.
  - **Quick check question**: In a two-player zero-sum game, what property does the value of the game represent for both players?

- **Concept: Policy Gradient Methods (PPO)**
  - **Why needed here**: The proposed algorithm uses Proximal Policy Optimization (PPO) as its policy gradient component. Understanding gradient estimation is critical for implementation and debugging.
  - **Quick check question**: What is the primary role of the clipping parameter in PPO's objective function?

## Architecture Onboarding

- **Component map**:
  Background Population Generator -> Scenario Sampler -> Policy Network -> Utility/Regret Estimator -> Adversarial Prior Distribution

- **Critical path**:
  1. Offline: Generate background population B_train and pre-compute best-response estimates if using minimax regret.
  2. Initialize: Start with uniform distribution β0 and random policy parameters θ0.
  3. Training Loop: Sample scenarios from βt, estimate utilities/regret, update β via gradient step and projection, update policy θ via PPO.

- **Design tradeoffs**:
  - **Utility vs. Regret**: Utility optimization is computationally simpler and achieved best worst-case utility. Regret is theoretically cleaner for avoiding degenerate scenarios but requires costly best-response computation.
  - **Fixed vs. Learned Distribution**: Fixed uniform distribution is a strong baseline for average utility but offers no worst-case guarantees. Learned distribution provides robustness at potential cost of average-case performance.
  - **Exact vs. Stochastic GDA**: Exact is possible for small, known environments. Stochastic sampling is necessary for deep RL but introduces noise into distribution updates.

- **Failure signatures**:
  - **Distribution Collapse**: Monitoring β shows >95% probability on a single scenario. Policy overfits to this one case and loses generalizability.
  - **Degenerate Learning**: Worst-case utility plateaus at a low value while average utility increases, indicating policy is ignoring hard scenarios.
  - **Instability**: Learning curves for utility/regret oscillate wildly without converging, typically due to mismatched learning rates between π and β.

- **First 3 experiments**:
  1. **Reproduce IPD Results**: Implement the tabular IPD experiment with exact gradients to verify the theoretical ordering: MU should have highest Umin, MR should have lowest Rmax.
  2. **Ablation on Distribution Update**: On Collaborative Cooking, run the same PPO training with fixed uniform β, learned adversarial β (MU), and learned β with forced exploration floor. Compare learning stability and final worst-case utility.
  3. **Out-of-Distribution Test**: Train on a small training population and test on a progressively wider set of ε-close partners to empirically validate the ε-net generalization bounds and observe where guarantees break.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the minimax utility approach be modified to maintain worst-case performance guarantees while preventing the policy from overfitting to a narrow subset of "worst-case" scenarios?
- **Basis in paper**: Section 8.1 notes that the worst-case distribution can force policies onto a narrow subset of scenarios (e.g., pure defection), leaving them unable to learn dynamics for cooperative scenarios.
- **Why unresolved**: The current formulation optimizes strictly for the adversarial prior, which may ignore game dynamics not present in the immediate worst-case scenarios.
- **What evidence would resolve it**: A modified algorithm that achieves similar worst-case utility (Umin) but significantly higher average utility (Uavg) compared to the baseline Maximin Utility method.

### Open Question 2
- **Question**: Can a curriculum learning framework based on partner distributions improve sample efficiency and asymptotic performance over the proposed GDA approach?
- **Basis in paper**: The conclusion states: "we see great potential in extending our approach to a curriculum learning framework based on partner distributions, which could dramatically improve sample efficiency, asymptotic performance, and AHT robustness."
- **Why unresolved**: The current study treats the optimization of the partner distribution as a static game solution rather than a dynamic curriculum that evolves from easy to hard scenarios.
- **What evidence would resolve it**: Empirical results showing that a curriculum-based distribution schedule converges faster or to a higher utility equilibrium than the uniform or adversarial distribution initialization.

### Open Question 3
- **Question**: How can the computational expense of the Minimax Regret objective be reduced for complex environments where best-response utilities are not readily available?
- **Basis in paper**: The conclusion notes that "utilising regret as an objective... proved computationally expensive when best-response utilities are not readily available." Section 8.2 also suggests lower performance might be due to approximate best responses.
- **Why unresolved**: Calculating exact regret requires computing the optimal policy (best response) for every scenario in the training set, which is often infeasible in deep RL.
- **What evidence would resolve it**: The development of an approximation method for regret that maintains the "Minimax Regret" robustness properties without requiring the full computational cost of calculating exact best responses for all background policies.

## Limitations
- The theoretical generalization guarantees (ε-net coverage) are strongly dependent on the background population being sufficiently diverse and representative of the true test distribution.
- The minimax regret approach requires computing best-response utilities U*(σ) for each scenario, which is computationally expensive and only feasible due to the small number of scenarios in the experiments.
- The paper does not extensively explore hyperparameter sensitivity, particularly around the distribution learning rate ηβ and the random scenario floor (5%).

## Confidence
- **High Confidence**: The empirical results showing MU and MR outperforming baselines on worst-case metrics (Umin, Rmax) are well-supported by the presented data across both test domains.
- **Medium Confidence**: The claim that the minimax framework provides provable worst-case performance guarantees relies on the ε-net assumption, which is theoretically justified but only partially validated empirically.
- **Medium Confidence**: The learning speed advantage of MU/MR over PBR is demonstrated, but the paper does not deeply investigate whether this constitutes a fundamental advantage or a function of specific hyperparameter choices.

## Next Checks
1. **Test ε-net assumption**: Systematically vary the diversity and size of the background population Btrain, then measure how performance degrades on progressively more out-of-distribution test partners to empirically validate the generalization bounds.
2. **Analyze distribution evolution**: Track and visualize the adversarial prior βt over training to confirm it concentrates on "hard" scenarios and does not collapse, providing insight into the learning dynamics.
3. **Ablate random floor**: Conduct an experiment comparing performance with different levels of forced random exploration in the distribution update (0%, 2%, 5%, 10%) to understand its impact on robustness versus convergence.