---
ver: rpa2
title: 'OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs'
arxiv_id: '2510.10689'
source_url: https://arxiv.org/abs/2510.10689
tags:
- reasoning
- wang
- video
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniVideoBench is a new benchmark designed to evaluate how well
  multimodal large language models (MLLMs) understand and reason using both audio
  and visual information together. Existing benchmarks often focus on short clips
  or only one modality, missing the challenge of combining audio and vision in a logically
  consistent way.
---

# OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs

## Quick Facts
- arXiv ID: 2510.10689
- Source URL: https://arxiv.org/abs/2510.10689
- Reference count: 15
- Key outcome: New benchmark reveals significant performance gaps in audio-visual understanding for current MLLMs

## Executive Summary
OmniVideoBench is a new benchmark designed to evaluate how well multimodal large language models (MLLMs) understand and reason using both audio and visual information together. Existing benchmarks often focus on short clips or only one modality, missing the challenge of combining audio and vision in a logically consistent way. OmniVideoBench addresses this by using 628 real-world videos (from seconds to 30 minutes long) and creating 1,000 high-quality question-answer pairs, each annotated with detailed step-by-step reasoning traces. The benchmark covers 13 types of reasoning tasks, including temporal reasoning, spatial localization, counting, causal inference, summarization, and more. Evaluations show that current MLLMs, including leading open-source models, perform significantly worse than humans and lag far behind closed-source models, especially on tasks requiring audio-visual integration or understanding music. This gap highlights the need for stronger, more generalizable multimodal reasoning capabilities. The dataset and results are publicly available to support further research.

## Method Summary
OmniVideoBench introduces a comprehensive evaluation framework for audio-visual understanding in MLLMs by collecting 628 real-world videos spanning various durations and content types. The benchmark creates 1,000 question-answer pairs with detailed reasoning traces, covering 13 distinct reasoning tasks from temporal reasoning to causal inference. The evaluation methodology involves human annotation of videos to generate high-quality questions that require both audio and visual understanding, with each question accompanied by step-by-step reasoning traces. The benchmark tests models across multiple reasoning categories and evaluates their ability to integrate multimodal information, with performance comparisons against human baselines and both open-source and closed-source MLLM systems.

## Key Results
- Current MLLMs significantly underperform humans on audio-visual reasoning tasks
- Leading open-source models lag far behind closed-source models, especially on audio-visual integration and music understanding
- Benchmark reveals specific weaknesses in multimodal reasoning capabilities requiring combined audio-visual comprehension
- Performance gaps highlight need for stronger, more generalizable audio-visual understanding in MLLMs

## Why This Works (Mechanism)
OmniVideoBench works by providing a comprehensive evaluation framework that captures the complexity of real-world audio-visual understanding through diverse video content and carefully designed question-answer pairs. The benchmark's strength lies in its combination of long-form videos (up to 30 minutes), multimodal reasoning requirements, and detailed annotation traces that expose the reasoning processes needed for successful comprehension. By including 13 distinct reasoning task types and requiring integration of both audio and visual modalities, the benchmark reveals specific weaknesses in current MLLMs that simpler, single-modality evaluations miss.

## Foundational Learning
- **Multimodal Integration** - why needed: MLLMs must combine information from multiple sensory channels; quick check: model can answer questions requiring both visual and audio information
- **Temporal Reasoning** - why needed: Understanding events across time is crucial for video comprehension; quick check: model can answer "what happened before/after" questions
- **Spatial Localization** - why needed: Identifying where objects or events occur in video space; quick check: model can pinpoint locations of objects or actions
- **Causal Inference** - why needed: Understanding cause-effect relationships between events; quick check: model can explain why events occurred
- **Audio-Visual Correlation** - why needed: Linking sounds to visual events for comprehensive understanding; quick check: model can match audio cues to visual occurrences
- **Long-Form Video Processing** - why needed: Real videos span various durations requiring sustained attention; quick check: model maintains coherence across extended video sequences

## Architecture Onboarding
- **Component Map**: Video input -> Visual encoder -> Audio encoder -> Multimodal fusion -> Reasoning module -> Answer generation
- **Critical Path**: Visual/audio feature extraction → Cross-modal attention fusion → Temporal reasoning → Final answer prediction
- **Design Tradeoffs**: Balance between processing long videos efficiently vs. maintaining comprehensive temporal context
- **Failure Signatures**: Poor performance on audio-visual integration tasks, inability to maintain coherence in long videos, struggles with music understanding
- **3 First Experiments**:
  1. Test model on single-modality questions vs. audio-visual integration questions to isolate multimodal weakness
  2. Evaluate performance on short clips vs. long videos to assess temporal reasoning capability
  3. Compare open-source vs. closed-source model performance on specific reasoning task categories

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset consists of only 628 videos, potentially limiting generalizability to diverse real-world scenarios
- Annotation process relies on human-generated content, introducing potential biases and subjective interpretations
- Benchmark focuses primarily on English-language content and Western cultural contexts, limiting global applicability
- Evaluation metrics may not capture all aspects of true audio-visual comprehension, particularly for emergent reasoning capabilities

## Confidence
**High Confidence** - Current MLLMs significantly underperform humans on audio-visual reasoning tasks is well-supported by benchmark results
**Medium Confidence** - Benchmark comprehensively covers 13 types of reasoning tasks, though distribution may not be perfectly balanced
**Low Confidence** - Claims about specific weaknesses in music understanding may be influenced by particular video selection and annotation choices

## Next Checks
1. Conduct cross-cultural validation by testing the benchmark with video content from diverse cultural contexts
2. Perform ablation studies to determine which specific aspects of the benchmark contribute most to performance gaps
3. Implement human evaluation studies to validate alignment between reasoning traces and actual cognitive processes