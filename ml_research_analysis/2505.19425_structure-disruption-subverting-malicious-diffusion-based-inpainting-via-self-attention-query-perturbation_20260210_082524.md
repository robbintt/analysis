---
ver: rpa2
title: 'Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via
  Self-Attention Query Perturbation'
arxiv_id: '2505.19425'
source_url: https://arxiv.org/abs/2505.19425
tags:
- image
- diffusion
- inpainting
- protection
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Structure Disruption Attack (SDA), a novel
  protection framework that prevents malicious diffusion-based inpainting by disrupting
  self-attention queries during initial denoising steps. The method targets the contour-focused
  nature of self-attention mechanisms to prevent coherent image generation.
---

# Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation

## Quick Facts
- arXiv ID: 2505.19425
- Source URL: https://arxiv.org/abs/2505.19425
- Authors: Yuhao He; Jinyu Tian; Haiwei Wu; Jianqing Li
- Reference count: 40
- Primary result: Introduces SDA framework that prevents malicious diffusion-based inpainting by disrupting self-attention queries during initial denoising steps

## Executive Summary
This paper presents Structure Disruption Attack (SDA), a novel protection framework designed to prevent malicious diffusion-based inpainting by targeting the self-attention mechanism during initial denoising steps. The approach specifically disrupts contour-focused self-attention queries to prevent coherent image generation while maintaining computational efficiency. The method demonstrates state-of-the-art protection performance across multiple quantitative metrics including VIF, SSIM, PSNR, FID, LPIPS, CLIP Score, and PIQE on face and instance datasets.

## Method Summary
SDA works by perturbing self-attention queries during the initial denoising steps of diffusion-based inpainting models. By disrupting the contour-focused nature of self-attention mechanisms, the framework prevents the model from capturing structural information and maintaining semantic alignment with text prompts. The attack is computationally efficient because it only needs to target the first few denoising steps, yet achieves strong robustness against various data augmentations, different model versions, and mask variations.

## Key Results
- Achieves state-of-the-art protection performance across multiple quantitative metrics (VIF, SSIM, PSNR, FID, LPIPS, CLIP Score, PIQE)
- Demonstrates strong robustness to data augmentations, different model versions, and mask variations
- Maintains computational efficiency by only attacking initial denoising steps

## Why This Works (Mechanism)
The method exploits the fact that diffusion-based inpainting models rely heavily on self-attention mechanisms to capture structural information and maintain semantic consistency during image generation. By perturbing the self-attention queries during the critical initial denoising steps, SDA disrupts the model's ability to establish coherent structural relationships, effectively preventing successful inpainting while minimizing computational overhead.

## Foundational Learning

1. **Self-Attention Mechanisms**: Used to capture long-range dependencies and structural relationships in images
   - Why needed: Essential for understanding how SDA disrupts the inpainting process
   - Quick check: Verify that perturbation affects attention weight distributions

2. **Diffusion-based Inpainting**: Progressive denoising process for image completion
   - Why needed: Core target of the protection framework
   - Quick check: Confirm that initial steps are most critical for structure formation

3. **Quantitative Metrics**: VIF, SSIM, PSNR, FID, LPIPS, CLIP Score, PIQE
   - Why needed: Used to evaluate protection effectiveness
   - Quick check: Ensure metrics align with perceptual quality degradation

4. **Computational Efficiency**: Minimizing attack overhead
   - Why needed: Critical for practical deployment of protection framework
   - Quick check: Verify that attacking only initial steps provides sufficient protection

5. **Robustness Analysis**: Performance across variations
   - Why needed: Validates real-world applicability of SDA
   - Quick check: Test against different model versions and augmentations

## Architecture Onboarding

Component Map: Input Image -> Mask Application -> Diffusion Model -> Self-Attention Layers -> SDA Perturbation -> Protected Output

Critical Path: The most critical path involves the initial denoising steps where self-attention queries are established. SDA targets these steps to maximize disruption while minimizing computational cost.

Design Tradeoffs: The framework balances protection strength against computational efficiency by limiting attacks to initial steps. This creates a tradeoff between attack effectiveness and processing overhead.

Failure Signatures: When SDA fails, models may still produce coherent structures or maintain semantic alignment with prompts, indicating insufficient perturbation of self-attention queries.

First Experiments:
1. Baseline test: Run standard inpainting without SDA to establish reference performance
2. Single-step perturbation test: Apply SDA to only the first denoising step
3. Multi-step perturbation test: Gradually increase the number of attacked steps to find optimal balance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations

- Generalizability to non-face/non-instance domains remains uncertain
- Computational efficiency analysis lacks comprehensive hardware configuration testing
- Limited exploration of defense mechanisms against adaptive attacks

## Confidence

- Protection performance metrics (VIF, SSIM, PSNR, etc.): High confidence
- Robustness to data augmentations and model variations: Medium confidence
- Computational efficiency: Medium confidence
- Defense against adaptive attacks: Low confidence

## Next Checks

1. Test SDA's effectiveness across diverse image domains (medical imaging, satellite imagery, artistic content) to assess domain generalizability

2. Conduct latency measurements on multiple hardware configurations to validate computational efficiency claims

3. Design and execute adaptive attack scenarios where malicious actors attempt to circumvent SDA through modified inpainting strategies