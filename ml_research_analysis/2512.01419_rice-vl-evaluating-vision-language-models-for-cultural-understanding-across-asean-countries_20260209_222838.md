---
ver: rpa2
title: 'Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across
  ASEAN Countries'
arxiv_id: '2512.01419'
source_url: https://arxiv.org/abs/2512.01419
tags:
- cultural
- visual
- across
- culturally
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RICE-VL is a benchmark designed to evaluate cultural understanding
  in vision-language models across 11 Southeast Asian countries, addressing the Western-centric
  bias in existing datasets. It includes 28,000 curated visual question-answer pairs
  and 1,000 image-bounding box pairs across 14 cultural categories, annotated by regionally
  informed experts over 720 hours.
---

# Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries

## Quick Facts
- arXiv ID: 2512.01419
- Source URL: https://arxiv.org/abs/2512.01419
- Reference count: 25
- Primary result: Benchmark shows significant performance gaps for VLMs on culturally-specific tasks, especially for low-resource Southeast Asian countries.

## Executive Summary
Rice-VL is a benchmark designed to evaluate cultural understanding in vision-language models across 11 Southeast Asian countries, addressing the Western-centric bias in existing datasets. It includes 28,000 curated visual question-answer pairs and 1,000 image-bounding box pairs across 14 cultural categories, annotated by regionally informed experts over 720 hours. The benchmark introduces SEA-LAVE, an evaluation metric assessing textual accuracy, cultural alignment, and country identification. Testing six open- and closed-source VLMs, results show significant performance gaps, with closed-source models outperforming others, especially in high-resource countries, while low-resource nations like Timor-Leste and Brunei see weaker results.

## Method Summary
Rice-VL evaluates VLMs on two tasks: CulturalVQA (True/False, Fill-in-the-Blank, Open-ended questions) and Cultural Visual Grounding (localizing cultural elements via bounding boxes) across 11 ASEAN countries. The dataset comprises 28,000 VQA pairs from 7,000 images and 1,000 image-bounding box pairs across 14 cultural categories. The SEA-LAVE metric scores VQA responses as (TU + CU + CI/2) / 3, where TU=Text Understanding, CU=Cultural Understanding, and CI=Country Identification. Evaluation uses two prompting conditions: "Global" and "Southeast Asian" context framing. Six VLMs were tested (GPT-4o, Claude-3-Opus, LLaMA 3.2, Ola, Ovis 2, Qwen-VL 2.5), with Qwen2.5-VL 7B serving as the judge model.

## Key Results
- Closed-source models (GPT-4o, Claude-3-Opus) outperform open-source models across all metrics and countries.
- High-resource countries (Singapore, Thailand) show higher performance than low-resource nations (Timor-Leste, Brunei).
- Visual grounding accuracy varies by cultural category, with abstract or symbolic themes posing greater challenges.
- SEA-specific prompting significantly improves performance, suggesting models have latent but poorly accessed cultural knowledge.

## Why This Works (Mechanism)

### Mechanism 1: Expert-Mediated Cultural Grounding
The benchmark effectively exposes model limitations because it relies on regionally-informed expert annotation rather than automated web scraping, which filters out superficial visual patterns that models might otherwise exploit. By utilizing 720 hours of annotation by trained cultural experts, the dataset enforces a distinction between visual correlation and cultural semantic validity, forcing models to require specific cultural priors rather than generic visual recognition.

### Mechanism 2: Contextual Priming for Latent Knowledge Retrieval
VLMs store cultural knowledge latently but fail to retrieve it without explicit regional prompting; the benchmark exploits this by testing "Global" vs. "SEA" prompting conditions. The prompting strategy ("This is a Southeast Asian setting") acts as a cognitive switch, shifting the model's probability mass from dominant Western priors to sparsely represented SEA regional priors. The performance delta between the two settings quantifies the accessibility of cultural knowledge versus its lack of existence.

### Mechanism 3: Decomposed Semantic-Cultural Evaluation (SEA-LAVE)
Traditional metrics conflate visual correctness with cultural relevance; SEA-LAVE isolates these dimensions to penalize "hallucinated cultural specificity." The metric decomposes the score into Text Understanding, Cultural Understanding, and Country Identification, preventing a model from receiving full credit for generic visual descriptions when specific cultural practices are required.

## Foundational Learning

- **Visual Grounding (Referring Expression Comprehension):** Required for interpreting Cultural Visual Grounding tasks where models output bounding boxes. *Quick check:* If a model correctly identifies a "religious statue" but the bounding box cuts off the base, does IoU penalize it?

- **Western-Centric Bias in Pre-training:** Core problem being diagnosed. Understanding that VLMs perform well on "high-resource" (Western) data is essential to interpreting low scores for Timor-Leste vs. Singapore. *Quick check:* Why might a model mistake a "traditional game" for a generic children's toy?

- **LLM-as-a-Judge:** SEA-LAVE metric relies on an LLM to grade the VLM's answers. *Quick check:* What are the risks of using a general-purpose VLM to judge the "cultural correctness" of another model?

## Architecture Onboarding

- **Component map:** Dataset Layer (7,000 images, 14 categories) -> Annotation Layer (human-in-the-loop for VQA and bounding boxes) -> Evaluation Layer (SEA-LAVE metric with Qwen2.5-VL judge) -> Model Layer (tested VLMs)

- **Critical path:** 1) Data Curation (bottleneck for low-resource countries) -> 2) Prompt Engineering (Global vs. SEA context) -> 3) Metric Calculation (inference + SEA-LAVE evaluation)

- **Design tradeoffs:** Scope vs. Depth (11 countries dilutes specificity), English-only limitation (simplifies evaluation but ignores native nuance), Model-as-Judge (trades human labor for speed, risks bias propagation)

- **Failure signatures:** "Tourist" Effect (high TU but zero CU), Visual Similarity Trap (confusing Kaya toast with generic toast), Prompt Sensitivity (large variance between Global and SEA scores)

- **First 3 experiments:** 1) Baseline Reproduction (GPT-4o vs. LLaMA 3.2 on Global vs. SEA prompt split), 2) Category Error Analysis (isolate Religious Practices and Key Figures), 3) Negative Ablation (feed non-SEA images with SEA prompt to test hallucination)

## Open Questions the Paper Calls Out
1. How does evaluating VLMs in native Southeast Asian languages versus English affect cultural reasoning accuracy and semantic nuance?
2. What specific training interventions can close the performance gap for severely underrepresented nations like Timor-Leste and Brunei?
3. How can VLMs be improved to accurately localize abstract or symbolic cultural elements that lack distinct visual salience?

## Limitations
- Benchmark is English-only, potentially overlooking culturally nuanced meanings in native languages
- Western-centric evaluation metric (SEA-LAVE using Qwen2.5-VL) may introduce bias
- Dataset construction faces scalability issues for low-resource countries, affecting statistical significance

## Confidence
**High Confidence:** Closed-source models outperform open-source models is well-supported empirically.
**Medium Confidence:** SEA-LAVE metric's effectiveness depends on Qwen2.5-VL as reliable judge; prompt sensitivity shows clear differences but interpretation requires validation.
**Low Confidence:** Claim that models have "latent knowledge" of SEA cultures is speculative; poor performance on abstract categories could reflect task design limitations.

## Next Checks
1. Run the same SEA-specific prompt on images from completely different cultural contexts to test whether models hallucinate SEA cultural markers.
2. Conduct human expert validation (5-10 cultural experts) to verify SEA-LAVE metric scores on a stratified sample of 100 responses.
3. Systematically test whether targeted fine-tuning on a small corpus of Timor-Leste/Brunei data can close the performance gap.