---
ver: rpa2
title: Discrete Optimization of Min-Max Violation and its Applications Across Computational
  Sciences
arxiv_id: '2508.13437'
source_url: https://arxiv.org/abs/2508.13437
tags:
- amvm
- solution
- discrete
- dmmv
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Discrete Min-Max Violation (DMMV) problem,
  which seeks to minimize the maximum constraint violation when variables are chosen
  from a discrete set. The authors prove DMMV is NP-hard and propose a GPU-accelerated
  heuristic called AMVM based on adaptive large neighborhood search.
---

# Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences

## Quick Facts
- arXiv ID: 2508.13437
- Source URL: https://arxiv.org/abs/2508.13437
- Reference count: 39
- Proposes GPU-accelerated AMVM heuristic achieving state-of-the-art results in 3-bit LLM quantization, discrete tomography, and FIR filter design

## Executive Summary
This paper introduces the Discrete Min-Max Violation (DMMV) problem, which seeks to minimize the maximum constraint violation when variables are chosen from a discrete set. The authors prove DMMV is NP-hard and propose a GPU-accelerated heuristic called AMVM based on adaptive large neighborhood search. AMVM exploits mathematical properties of DMMV to speed up solution search. Experimental results demonstrate its effectiveness across three domains: (1) 3-bit post-training quantization of LLMs, achieving 14% improvement in perplexity over existing methods; (2) discrete tomography, reducing reconstruction error by 16% under uniform noise with 6× GPU speedup; and (3) Finite Impulse Response (FIR) filter design, nearly achieving 50% ripple reduction compared to commercial solver Gurobi. The GPU implementation significantly accelerates convergence by enabling parallel evaluation of swap candidates and bulk residual updates. The open-source code aims to stimulate further research on DMMV and its applications.

## Method Summary
The authors propose AMVM (Adaptive Min-Max Violation), a GPU-accelerated heuristic for solving the NP-hard DMMV problem. The method uses Adaptive Large Neighborhood Search (ALNS) with destroy-repair cycles: destruction operators randomly or strategically remove variables, repair operators restore feasibility, and local search refines solutions through 1-Opt moves and swap filtering. The GPU implementation parallelizes swap evaluation and residual updates using batched linear algebra operations. Mathematical properties of the ℓ∞ violation constraints are exploited to filter candidates efficiently, checking only a subset of constraints with the largest residuals. The approach is validated across three domains: 3-bit LLM quantization using OPT-125M, discrete tomography reconstruction from 128×128 phantom images, and FIR filter design with 4-bit coefficients.

## Key Results
- 3-bit LLM quantization: 14% perplexity improvement over existing methods on OPT-125M model
- Discrete tomography: 16% reduction in reconstruction error under uniform noise with 6× GPU speedup
- FIR filter design: Nearly 50% ripple reduction compared to commercial solver Gurobi
- GPU acceleration enables 70 iterations vs 12 at 2-bit quantization (6× faster)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive Large Neighborhood Search (ALNS) can navigate the discrete solution space of DMMV instances more effectively than standard local search or continuous relaxation methods.
- **Mechanism:** The algorithm iteratively applies "destroy" operators to unassign variables (randomly or based on impact scores) and "repair" operators to restore feasibility. This allows the search to escape local optima by exploring larger neighborhoods in the solution space rather than moving strictly along gradients or performing 1-Opt swaps.
- **Core assumption:** The structure of DMMV allows effective exploration via partial solution destruction and reconstruction, and the adaptive operator selection correctly exploits problem structure.
- **Evidence anchors:**
  - [abstract]: "propose a GPU-accelerated heuristic... based on the adaptive large neighborhood search"
  - [section 3]: "Our proposed improvement heuristic... iteratively refines this solution through three phases. 1) Destruction... 2) Repair... 3) Local search"
  - [corpus]: Weak direct support for ALNS in DMMV; related papers focus on tree-based or gradient-based min-max methods (e.g., "EXOTIC" [arXiv:2508.12479]).
- **Break condition:** If the destroy operators remove too few variables, the search may get stuck; if they remove too many, the heuristic degrades into random restarts.

### Mechanism 2
- **Claim:** The computational cost of identifying improving swaps can be drastically reduced by exploiting the mathematical structure of $\ell_\infty$ violation constraints.
- **Mechanism:** Instead of exhaustively evaluating all $O(n^2)$ swaps, the method uses Proposition 1 and Corollaries 1 & 2 to filter candidates. It prioritizes checking rows in matrix $A$ with the largest residuals ($k_\epsilon$-subset) and uses bounds on constraint violations to prune non-improving moves before calculating full objective values.
- **Core assumption:** Improving moves are correlated with constraints that currently have the largest violations, and checking a small subset of constraints is sufficient to identify promising swaps.
- **Evidence anchors:**
  - [section 7 (Materials and Methods)]: "It is computationally expensive to verify the condition in Proposition 1 for all possible swaps directly. Therefore, our heuristic... uses following filters..."
  - [appendix b]: "Corollary 1 (Swap Candidate Selection)... A list of promising candidate swaps can be generated efficiently by checking... against only a small subset of rows."
  - [corpus]: No direct evidence for this specific swap-filtering mechanism in the provided corpus summaries.
- **Break condition:** If the "worst-case" violation is determined by constraints not in the top residual subset, the filter may discard improving swaps, leading to premature convergence.

### Mechanism 3
- **Claim:** GPU acceleration enables effective scaling by parallelizing the evaluation of swap candidates and bulk residual updates.
- **Mechanism:** The method maps the constraint matrix $A$, residual vector $s$, and search operators to dense tensors. This allows swap evaluation and residual updates to be performed via batched linear algebra and parallel reductions (e.g., computing $\ell_\infty$ norms across thousands of candidates simultaneously) rather than sequential loops.
- **Core assumption:** The overhead of data transfer and kernel launch is outweighed by the throughput of parallel tensor operations, and the problem fits within GPU memory constraints.
- **Evidence anchors:**
  - [section 7 (Materials and Methods)]: "Swap filtering constructs a three-dimensional difference tensor... extracting promising pairs without explicit loops."
  - [section 4 (Table 2)]: "The GPU executes 70 iterations versus 12 at 2-bit (6× faster)... The higher iteration throughput of the GPU directly translates into tighter worst-case errors."
  - [corpus]: No specific evidence for GPU-ALNS synergy in the provided corpus summaries.
- **Break condition:** Performance degrades if memory bandwidth is saturated or if the problem structure forces frequent synchronization (e.g., highly sparse data requiring CPU intervention).

## Foundational Learning

- **Concept: $\ell_\infty$ Norm (Chebyshev Approximation)**
  - **Why needed here:** The core objective of DMMV is to minimize the *maximum* absolute error ($\|Ax - b\|_\infty$), not the average error. This "worst-case" focus is distinct from standard Least Squares.
  - **Quick check question:** If you have errors [1.0, 1.0, 10.0], does minimizing the $\ell_\infty$ norm focus on the 10.0 or the average (4.0)?

- **Concept: Large Neighborhood Search (LNS)**
  - **Why needed here:** AMVM relies on "destroy" and "repair" cycles rather than just gradient descent or simple swaps. Understanding that LNS explores larger moves is key to grasping why it escapes local minima.
  - **Quick check question:** Does the algorithm proceed by changing one variable at a time, or by unfixing groups of variables and re-optimizing them?

- **Concept: Mixed-Integer Linear Programming (MILP)**
  - **Why needed here:** The paper formally defines DMMV as an MILP to prove hardness and uses the commercial solver Gurobi as a baseline. Understanding the discrete nature of variables ($x \in V^n$) is essential.
  - **Quick check question:** Why can't we just use standard convex optimization solvers for this problem?

## Architecture Onboarding

- **Component map:** Problem Data ($A, b, V$) -> AMVM Loop (CPU Orchestration) -> ALNS Core (GPU Kernels: Destroy/Repair, Local Search with Swap Filter) -> State (current solution $x$, residuals $s$, best-found solution $x^*$)

- **Critical path:** The **Swap Evaluation** kernel. This is the most frequently called component. Efficient implementation of the "screening test" (Corollary 2) and the batched residual update determines the system's throughput.

- **Design tradeoffs:**
  - **Quality vs. Speed:** Increasing the $k_\epsilon$-subset size (filters) improves the chance of finding good swaps but linearly increases compute time per iteration.
  - **Memory vs. Parallelism:** Storing the full constraint matrix $A$ on the GPU enables maximum parallelism but limits the maximum problem size compared to CPU-based sparse methods.

- **Failure signatures:**
  - **Stagnation:** Operator scores collapse to zero; the solution stops improving despite high residuals.
  - **OOM (Out of Memory):** Attempting to load very large matrices (e.g., massive tomography datasets) as dense tensors.
  - **Numerical Instability:** The $\ell_\infty$ objective is non-smooth; if step-sizes or update logic are slightly off, residuals may oscillate rather than converge.

- **First 3 experiments:**
  1. **Unit Test (Correctness):** Run AMVM on the small FIR filter example (N=12) from the paper. Verify the result matches the "optimal" Gurobi solution (Objective $\delta = 0.207$) to ensure the heuristic logic is implemented correctly.
  2. **Ablation (Mechanism):** Disable the swap filtering (Corollary 1) on a medium-sized problem (e.g., Discrete Tomography). Measure the drop in iterations-per-second to quantify the value of the mathematical optimization.
  3. **Scaling (GPU):** Run the quantization task (OPT-125M) while varying the number of parallel rows processed per GPU. Plot throughput vs. batch size to find the saturation point of the hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DMMV framework and AMVM heuristic be effectively extended to robust scheduling under failure scenarios and robust regret minimization?
- Basis in paper: [explicit] The authors state "Future research includes extending DMMV to new discrete-design domains such as robust scheduling under failure scenarios, and robust regret minimization."
- Why unresolved: The current work validates the approach only on quantization, tomography, and filter design; robust regret minimization involves different objective structures not yet tested.
- What evidence would resolve it: Successful application of the AMVM heuristic to regret minimization benchmarks with performance comparable to or better than specialized solvers.

### Open Question 2
- Question: How can AMVM be adapted for scalable multi-GPU or distributed environments to handle larger-scale or real-time DMMV instances?
- Basis in paper: [explicit] The authors suggest "develop a scalable multi-GPU and distributed versions of AMVM... (e.g., via CUDA-MPI/NCCL) to tackle larger-scale or real-time DMMV instances."
- Why unresolved: The current implementation is optimized for a single GPU, and it is unclear if the memory bandwidth or kernel fusion strategies scale efficiently across nodes.
- What evidence would resolve it: A distributed implementation demonstrating linear speedup and maintained solution quality on datasets exceeding single-GPU memory capacity.

### Open Question 3
- Question: Is it possible to derive theoretical approximation guarantees for the AMVM heuristic?
- Basis in paper: [explicit] The authors explicitly acknowledge the limitation that "AMVM offers promising empirical performance, it remains a heuristic without optimality guarantees."
- Why unresolved: Adaptive Large Neighborhood Search (ALNS) is a stochastic metaheuristic, making formal worst-case bounds difficult to establish compared to exact solvers like Gurobi.
- What evidence would resolve it: A formal proof establishing a constant-factor approximation bound for AMVM relative to the optimal MILP solution.

## Limitations
- The method's effectiveness depends heavily on the mathematical structure of DMMV instances, potentially degrading on problems with different constraint characteristics or variable domains
- GPU acceleration assumes dense matrix representations, limiting scalability to very large problems that require sparse representations
- The adaptive operator selection mechanism lacks theoretical guarantees for convergence to global optima

## Confidence
- **High Confidence:** The NP-hardness proof and basic ALNS framework are well-established in the literature
- **Medium Confidence:** The specific swap filtering mechanisms (Propositions and Corollaries) and their implementation details, as the paper provides theoretical justification but limited empirical ablation studies
- **Low Confidence:** The claim that GPU acceleration provides 6× speedup across all problem sizes, as the paper only demonstrates this for specific instances and problem sizes

## Next Checks
1. **Ablation Study:** Systematically disable the swap filtering mechanism (Corollary 1) and measure the impact on both solution quality and runtime to quantify the value of the mathematical optimization
2. **Scaling Analysis:** Test the method on increasingly large discrete tomography problems (e.g., 512×512 images) to determine the memory and performance limits of the dense GPU representation
3. **Generalization Test:** Apply AMVM to a different DMMV instance type (e.g., resource allocation with discrete budgets) to evaluate performance outside the three demonstrated domains