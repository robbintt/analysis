---
ver: rpa2
title: Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field
  Study
arxiv_id: '2508.20244'
source_url: https://arxiv.org/abs/2508.20244
tags:
- students
- reliance
- answer
- student
- quiz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how college students interact with ChatGPT-4
  during STEM quizzes, focusing on AI reliance patterns. Using a novel four-stage
  taxonomy, researchers analyzed 315 student-AI conversations to identify twelve reliance
  scenarios across competence, relevance, adoption, and answer correctness.
---

# Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study

## Quick Facts
- arXiv ID: 2508.20244
- Source URL: https://arxiv.org/abs/2508.20244
- Authors: Jiayu Zheng; Lingxin Hao; Kelun Lu; Ashi Garg; Mike Reese; Melo-Jean Yap; I-Jeng Wang; Xingyun Wu; Wenrui Huang; Jenna Hoffman; Ariane Kelly; My Le; Ryan Zhang; Yanyu Lin; Muhammad Faayez; Anqi Liu
- Reference count: 10
- Key outcome: Students showed low AI reliance in STEM quizzes; behavioral metrics (similarity, duration, complexity) better predicted adoption than content labels; negative experiences persisted across interactions.

## Executive Summary
This study examines college students' interactions with ChatGPT-4 during STEM quizzes, revealing patterns of under-reliance rather than over-reliance. Using a novel four-stage taxonomy, researchers analyzed 315 student-AI conversations to identify twelve reliance scenarios. Results show students exhibited overall low reliance on AI, with many failing to effectively use it for learning. Negative reliance patterns often persisted across interactions, indicating difficulty in shifting strategies after initial unsuccessful experiences. Behavioral metrics like prompt-question similarity, conversation duration, and complexity strongly predicted AI adoption.

## Method Summary
The study analyzed 315 anonymized student-ChatGPT conversation logs from five STEM subjects, including quiz questions, chat history, timestamps, and student final answers. Researchers developed a four-stage taxonomy (competence → relevance → adoption → correctness) to classify 12 reliance scenarios. A GPT-4o labeling pipeline with few-shot chain-of-thought prompting labeled conversations across three stages, validated through human-in-the-loop checks. Features included behavioral metrics (time, similarity scores, complexity), text embeddings (1,042-dim → 8 PCA components), and linguistic features. XGBoost and Random Forest classifiers predicted AI adoption, with SHAP analysis identifying key predictors.

## Key Results
- Students exhibited under-reliance on AI, with more "Inappropriate self-reliance" (14.6%) than "Inappropriate AI-reliance" cases
- Behavioral metrics (prompt-question similarity, conversation duration, complexity) predicted AI adoption more reliably than content taxonomy labels
- Negative reliance patterns persisted across interactions, with 48% of students only interacting once after unsuccessful initial experiences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral signals predict AI adoption more reliably than content taxonomy labels.
- Mechanism: Students who paraphrase quiz questions closely, engage in longer conversations, or produce complex prompts exhibit higher likelihood of following AI advice. SHAP analysis shows these continuous behavioral features dominate feature importance, while categorical labels contribute near zero.
- Core assumption: Behavioral traces reflect underlying cognitive states that drive reliance decisions.
- Evidence anchors: "Behavioral metrics like prompt-question similarity, conversation duration, and complexity strongly predicted AI adoption" [abstract]; "The SHAP feature-importance plot shows that the strongest predictors of adopting AI's advice are continuous measures of semantic and behavioral features" [results].

### Mechanism 2
- Claim: Negative initial experiences with AI create persistent maladaptive strategies.
- Mechanism: Students who experience "Failed application" or "Inappropriate self-reliance" on early interactions often disengage or repeat unsuccessful patterns. This suggests difficulty in strategy revision after negative outcomes.
- Core assumption: Students lack meta-cognitive skills or onboarding support to diagnose why AI guidance failed and adjust prompting behavior.
- Evidence anchors: "Negative reliance patterns often persisted across interactions, indicating difficulty in shifting strategies after initial unsuccessful experiences" [abstract]; "Many students only interacted with the AI once as their initial interaction did not lead to the correct answer" [results].

### Mechanism 3
- Claim: Students in this study exhibited under-reliance rather than over-reliance.
- Mechanism: In contexts where students trust human peers/tutors more than AI, and when AI familiarity is low, students default to self-reliance even when AI provides correct, relevant guidance.
- Core assumption: Trust is context-dependent and shaped by prior social learning structures and tool familiarity.
- Evidence anchors: "There are more 'Inappropriate self-reliance' (N = 46, 14.6%) cases... than 'Inappropriate reliance' cases" [results]; "In a college peer-led-team-learning (PLTL) program, students place greater trust in instructors, teaching assistants, and knowledgeable peers than in GenAI" [introduction].

## Foundational Learning

- Concept: **Reliance Taxonomy (4-stage)**
  - Why needed here: The paper introduces a novel framework (competence → relevance → adoption → correctness) to classify 12 reliance scenarios. Understanding this is essential to interpret results and apply the methodology.
  - Quick check question: Given reliance code 1-1-0-0, what scenario does this represent and why might it occur?

- Concept: **SHAP (Shapley Additive Explanations)**
  - Why needed here: The study uses SHAP to interpret XGBoost predictions, quantifying feature contributions to AI adoption. Required to understand which behavioral features drive outcomes.
  - Quick check question: If prompt-question similarity has high positive SHAP values for a prediction, what does this imply about that student's behavior?

- Concept: **Sequence Clustering with Edit Distance**
  - Why needed here: The paper clusters conversation trajectories using edit distance on question-type sequences to identify prototypical interaction patterns.
  - Quick check question: Why would substitution cost (2) be higher than insertion/deletion cost (1) when analyzing student-AI conversation sequences?

## Architecture Onboarding

- Component map:
  - Raw conversation logs → timestamp alignment → conversation-level aggregation
  - GPT-4o labeling pipeline (3 stages) with human validation (85%+ alignment threshold)
  - Feature extraction (behavioral + embeddings) → PCA reduction
  - Classification (follow vs. not-follow) with 80/20 train-test split
  - SHAP analysis for feature importance
  - Individual trajectory construction → pattern analysis

- Critical path:
  1. Raw conversation logs → timestamp alignment → conversation-level aggregation
  2. GPT-4o labeling pipeline (3 stages) with human validation (85%+ alignment threshold)
  3. Feature extraction (behavioral + embeddings) → PCA reduction
  4. Classification (follow vs. not-follow) with 80/20 train-test split
  5. SHAP analysis for feature importance
  6. Trajectory analysis for pattern identification

- Design tradeoffs:
  - GPT-4o labeling vs. human annotation: Scalability vs. accuracy; 85% alignment deemed acceptable but introduces label noise
  - Conversation-level vs. message-level analysis: Captures holistic patterns but may obscure turn-by-turn dynamics
  - 30-minute window: Authentic quiz context but limits observation of long-term reliance evolution

- Failure signatures:
  - Low similarity between student prompt and quiz question may indicate insufficient context for AI to help
  - High "Failed application" rate (20.9%) suggests students cannot translate correct AI guidance into correct answers within time constraints
  - Single-interaction disengagement (48%) signals negative first-experience entrenchment

- First 3 experiments:
  1. Replicate taxonomy labeling: Apply the 4-stage GPT-4o pipeline to a sample of conversations, compare your labels against human annotations to validate the 85% alignment threshold.
  2. Ablate feature sets: Train classifiers using behavioral features only vs. text embeddings only vs. combined. Verify that behavioral + text yields highest accuracy.
  3. Trajectory persistence test: For students with 2+ conversations, compute transition probabilities between reliance scenarios. Test whether negative outcomes have higher self-transition rates than positive outcomes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do student-AI reliance patterns evolve over extended learning periods rather than brief, single-session quiz interactions?
- Basis in paper: [explicit] The authors state: "our reliance analysis was constrained by the quiz context and a 30-minute interaction window... this limitation underscores the need to investigate reliance behaviors over extended interactions."
- Why unresolved: The study only captured brief, time-limited interactions; long-term reliance trajectory formation, strategy refinement, and habituation effects remain unknown.
- What evidence would resolve it: Longitudinal field studies tracking the same students across multiple weeks or months with varied task types.

### Open Question 2
- Question: What specific onboarding interventions most effectively improve students' appropriate reliance on AI tools?
- Basis in paper: [explicit] The authors emphasize "the need for enhanced onboarding processes to improve student's familiarity and effective use of AI tools" and "AI interfaces should be designed with reliance-calibration mechanisms."
- Why unresolved: The study identified low effective AI use but did not test any interventions; the optimal content, timing, and delivery of onboarding remain unspecified.
- What evidence would resolve it: Randomized controlled experiments comparing different onboarding curricula against control conditions.

### Open Question 3
- Question: How do cultural, institutional, and demographic factors moderate student-AI reliance patterns?
- Basis in paper: [explicit] The authors note: "our reliance data was originated exclusively from a private university in the United States, limiting the generalizability of our conclusions."
- Why unresolved: Single-institution sample with specific PLTL context cannot reveal how prior educational experiences, cultural attitudes toward AI, or institutional norms shape reliance.
- What evidence would resolve it: Multi-site comparative studies across diverse universities with demographic stratification.

### Open Question 4
- Question: Can predictive models accurately distinguish appropriate from inappropriate reliance in real-time to support adaptive interface interventions?
- Basis in paper: [explicit] The authors state: "our predictive models classified reliance broadly, without specifically distinguishing appropriate reliance from inappropriate or misguided reliance."
- Why unresolved: Current models predict binary adoption but conflate appropriate and inappropriate reliance scenarios; real-time detection remains untested.
- What evidence would resolve it: Development and validation of multi-class classifiers using the 12-scenario taxonomy, tested on held-out conversations with latency constraints.

## Limitations
- The 85% human-AI alignment threshold for the labeling pipeline may mask systematic disagreements on borderline cases
- Single-session observation window (30 minutes) cannot capture long-term reliance evolution or adaptation patterns
- The PLTL program context (peer-led learning) may produce atypical reliance patterns that don't generalize to traditional classroom settings

## Confidence
- **High confidence** in behavioral metrics predicting AI adoption (supported by robust SHAP analysis)
- **Medium confidence** in negative experience persistence claims (supported by qualitative observation but limited sample size)
- **Low confidence** in generalizability across educational contexts (PLTL-specific trust dynamics not replicated)

## Next Checks
1. Replicate the labeling pipeline with synthetic conversation examples to test sensitivity to few-shot prompt variations
2. Conduct cross-context validation by applying the taxonomy to student-AI interactions in traditional lecture-based courses
3. Implement a longitudinal study tracking the same students across multiple AI-assisted learning sessions to measure persistence dynamics