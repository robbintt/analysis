---
ver: rpa2
title: 'On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial
  Approach'
arxiv_id: '2502.02209'
source_url: https://arxiv.org/abs/2502.02209
tags:
- layers
- mamba
- arxiv
- layer
- polynomial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the expressivity of selective state-space
  layers (S6), the core component of Mamba architectures. Using multivariate polynomials,
  the authors prove that S6 layers surpass linear transformers in expressiveness,
  particularly for long sequences.
---

# On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial Approach

## Quick Facts
- **arXiv ID**: 2502.02209
- **Source URL**: https://arxiv.org/abs/2502.02209
- **Reference count**: 40
- **Primary result**: S6 layers are theoretically more expressive than linear transformers for long sequences

## Executive Summary
This paper establishes a rigorous theoretical foundation for the expressivity of selective state-space layers (S6), the core component of Mamba architectures. Using multivariate polynomial analysis, the authors prove that S6 layers can represent high-degree polynomials with a single layer, while linear attention requires a logarithmic number of layers. This theoretical superiority is demonstrated to hold even as sequence length increases, with the first length-agnostic generalization bound provided for S6 layers. The work bridges the gap between empirical success and theoretical understanding of Mamba models.

## Method Summary
The authors employ multivariate polynomial analysis as their primary theoretical framework, proving that S6 layers can express high-degree polynomials with a single layer while linear attention requires multiple layers. They develop a novel length-agnostic generalization bound specifically for S6 layers. To validate their theoretical claims, they conduct empirical experiments on synthetic datasets where they demonstrate that S6 layers can effectively learn high-degree polynomials that self-attention models struggle with, particularly for long sequences.

## Key Results
- S6 layers can represent high-degree multivariate polynomials with a single layer, while linear attention requires O(log n) layers
- First length-agnostic generalization bound established for S6 layers
- Empirical validation shows S6 outperforms self-attention on synthetic polynomial datasets for long sequences
- Theoretical expressivity advantage does not compromise generalization

## Why This Works (Mechanism)
The S6 layer's superiority stems from its ability to capture long-range dependencies through selective state updates, which allows it to represent complex multivariate polynomial functions efficiently. Unlike linear attention which scales poorly with polynomial degree, S6 maintains constant computational complexity while increasing expressivity. The selective gating mechanism enables the layer to focus computational resources on relevant sequence elements, making it particularly effective for high-degree polynomial approximations.

## Foundational Learning
- **Selective State-Space Models**: Why needed - forms the theoretical basis for Mamba architectures; Quick check - understand how state updates differ from standard RNNs
- **Multivariate Polynomial Analysis**: Why needed - provides the mathematical framework for comparing expressivity; Quick check - can decompose polynomial degree requirements for different architectures
- **Generalization Bounds**: Why needed - establishes theoretical limits on learning performance; Quick check - understand difference between length-dependent and length-agnostic bounds
- **Attention Mechanisms**: Why needed - provides context for comparing S6 to established methods; Quick check - can articulate limitations of linear attention
- **Sequence Length Scaling**: Why needed - critical for understanding performance on long-range tasks; Quick check - can explain why log(n) scaling matters

## Architecture Onboarding
- **Component Map**: Input -> Selective State-Space Layer -> Output (single layer achieves what linear attention needs log(n) layers for)
- **Critical Path**: Theoretical analysis of polynomial expressivity → Generalization bound derivation → Empirical validation on synthetic datasets
- **Design Tradeoffs**: Single S6 layer vs. multiple linear attention layers; theoretical expressivity vs. practical computational complexity
- **Failure Signatures**: Poor performance on high-degree polynomial fitting; inability to maintain expressivity as sequence length increases
- **First Experiments**: 1) Test S6 on synthetic polynomial datasets of varying degrees; 2) Compare generalization bounds across different sequence lengths; 3) Validate theoretical expressivity claims on real-world long-sequence tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Computational complexity of high-degree polynomial representations in practice is not addressed
- Generalization bound relies on specific data distribution assumptions that may not hold in real-world scenarios
- Empirical validation is limited to synthetic datasets, not real-world data
- Comparison with linear attention is limited to specific settings, unclear how results scale to very large models

## Confidence
- **High confidence**: Theoretical proofs demonstrating S6's superiority over linear attention in expressiveness
- **Medium confidence**: Generalization bound for S6 layers, given its reliance on specific assumptions
- **Low confidence**: Practical implications for real-world applications due to limited empirical validation

## Next Checks
1. Conduct experiments on real-world datasets to validate the practical performance of S6 layers compared to linear attention and self-attention models
2. Investigate the computational complexity of representing high-degree multivariate polynomials with S6 layers in practice, and compare it with the theoretical complexity
3. Extend the generalization bound analysis to more diverse data distributions and larger model sizes to assess its robustness and applicability