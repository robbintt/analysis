---
ver: rpa2
title: 'Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs
  for High- and Low-Resource Languages'
arxiv_id: '2504.18560'
source_url: https://arxiv.org/abs/2504.18560
tags:
- languages
- language
- bias
- translation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MLA-BiTe, a framework for automated multilingual
  bias evaluation in LLMs using translation and paraphrasing techniques. The framework
  was evaluated across four LLMs in six languages (English, Spanish, French, German,
  Catalan, Luxembourgish) using 7 bias categories (ageism, Lgbtiqphobia, politics,
  racism, religion, sexism, xenophobia).
---

# Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages

## Quick Facts
- arXiv ID: 2504.18560
- Source URL: https://arxiv.org/abs/2504.18560
- Reference count: 40
- Primary result: MLA-BiTe framework successfully augmented bias-testing templates across 6 languages using LLM translation/paraphrasing, with low-resource languages showing higher bias rates than high-resource languages

## Executive Summary
This study introduces MLA-BiTe, a framework for automated multilingual bias evaluation in LLMs using translation and paraphrasing techniques. The framework was evaluated across four LLMs in six languages (English, Spanish, French, German, Catalan, Luxembourgish) using 7 bias categories (ageism, Lgbtiqphobia, politics, racism, religion, sexism, xenophobia). LLM-based translation and paraphrasing effectively augmented bias-testing templates, with paraphrasing before translation yielding marginally better results. Aggregated results show GPT-4o and Claude 3.5 Sonnet had the lowest bias rates (~75% success), while Llama3 405B showed the highest bias rates (~49% success). Low-resource languages exhibited higher bias rates than high-resource languages, with Luxembourgish showing the highest discrimination rates overall.

## Method Summary
The MLA-BiTe framework applies translation and paraphrasing to English bias templates to create multilingual test suites. The pipeline translates templates using GPT-4o, then generates P paraphrases while preserving grammatical number and placeholders. The framework processes templates at the template level rather than individual prompts to ensure syntactic uniformity. Quality is assessed via cosine similarity (semantic preservation) and BLEU scores (diversity). The augmented templates are evaluated using the LangBiTe framework, which compares LLM responses to predefined oracles.

## Key Results
- GPT-4o and Claude 3.5 Sonnet achieved the lowest bias rates (~75% success) across all languages and categories
- Llama3 405B showed the highest bias rates (~49% success) with highest unprocessable responses (10.5% overall)
- Luxembourgish exhibited the highest discrimination rates overall, with lowest aggregated success rates (~61% mean)
- Low-resource languages (Luxembourgish, Catalan) showed higher bias rates than high-resource languages (English, Spanish, French, German)
- No consistent bias patterns emerged across linguistically related languages, indicating complex cross-linguistic bias transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based translation and paraphrasing can effectively augment bias-testing templates while preserving semantic integrity.
- Mechanism: In-Context Learning (ICL) capabilities allow LLMs to translate templates into target languages and generate semantically equivalent paraphrases. The framework uses explicit prompting constraints (e.g., "CRITICAL to maintain exact semantic meaning," preserve placeholders in `{{PLACEHOLDER}}` format) to enforce consistency. Cosine similarity scores (0.85–0.95 average) confirm semantic preservation across paraphrases.
- Core assumption: LLM translation quality for bias templates generalizes beyond the tested language pairs (English, Spanish, Catalan, French, German, Luxembourgish).
- Evidence anchors: [abstract] "MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings."; [section 4.3] Figure 1 shows GPT-4o achieving highest translation quality with cosine similarity >0.95 for most language pairs.; [corpus] "Mind the Gap... or Not?" (arXiv:2511.05162) cautions that translation errors can skew multilingual results—relevant to validity of this approach.
- Break condition: If target language has significantly different grammatical structure (e.g., non-Indo-European with complex noun class systems), translation quality and placeholder preservation may degrade.

### Mechanism 2
- Claim: Template-level augmentation (before placeholder instantiation) ensures syntactic uniformity and scalability.
- Mechanism: Translation and paraphrasing operate on templates containing placeholders like `{GENDER1}` rather than instantiated prompts. A single template with `p` placeholders can yield up to `n!/(p!(n-p)!)` test prompts. Processing at template level ensures all derived prompts share syntactic structure, improving comparability. Grammatical number is explicitly identified and enforced during paraphrasing to prevent errors like "Is a men better than a women."
- Core assumption: Placeholder syntax is language-agnostic and community terms can be slotted post-translation without grammatical conflicts.
- Evidence anchors: [section 3] "translating and paraphrasing at the individual prompt level would result in prompts derived from the same template being syntactically divergent."; [section 3.2] Algorithm 3 explicitly handles grammatical number identification to maintain consistency.; [corpus] No direct corpus evidence on template-level vs. prompt-level augmentation tradeoffs.
- Break condition: When target language requires grammatical agreement between placeholders and surrounding text (e.g., gendered adjectives in Romance languages), slotting English community terms may produce ungrammatical outputs.

### Mechanism 3
- Claim: Low-resource languages exhibit higher detected bias rates due to sparser training data and reduced cultural nuance coverage.
- Mechanism: Models trained on less data for languages like Luxembourgish or Catalan have weaker representations of culturally specific terms and bias-related nuances. This manifests as higher variability in bias detection (especially Politics, Racism categories) and lower overall "success" rates (non-biased responses). Luxembourgish showed lowest aggregated success rates (~61% mean) vs. English (~76%).
- Core assumption: Higher detected bias rates reflect actual model bias rather than evaluation artifact (e.g., translation quality affecting oracle matching).
- Evidence anchors: [section 5.2] "Luxembourgish stands out as the language with the highest discrimination rates overall."; [section 5.2] Figure 5 shows clear resource-level stratification: EN > ES > FR/DE > CA > LB.; [corpus] "Cross-Language Bias Examination in Large Language Models" (arXiv:2512.16029) also finds bias patterns differ across languages but doesn't establish causation.
- Break condition: If evaluation oracle keywords are poorly calibrated for low-resource language cultural contexts, detected "biases" may reflect measurement error rather than true model behavior.

## Foundational Learning

- Concept: **Bias Testing via Template-Oracle Matching**
  - Why needed here: MLA-BiTe inherits LangBiTe's evaluation paradigm where templates have expected "valid, non-biased" responses (oracles). Understanding this is essential to interpret "success rates."
  - Quick check question: Given template "Should {GROUP1} and {GROUP2} have equal rights?" with oracle expecting "Yes" responses—what would indicate bias?

- Concept: **Semantic Similarity Metrics (Cosine Similarity vs. BLEU)**
  - Why needed here: Framework quality assessment relies on cosine similarity for semantic alignment and BLEU for structural diversity. Higher cosine + lower BLEU = good paraphrase.
  - Quick check question: Why is lower BLEU score desirable for paraphrase evaluation while higher cosine similarity is preferred?

- Concept: **Grammatical Number Preservation in Paraphrasing**
  - Why needed here: The framework explicitly detects and enforces grammatical number (singular/plural) to prevent malformed outputs when placeholders are instantiated.
  - Quick check question: What goes wrong if paraphrasing changes "Are {GENDER1} better than {GENDER2}?" to singular form before placeholder instantiation?

## Architecture Onboarding

- Component map:
  - Translator -> Paraphraser -> Template Processor -> Downstream Evaluator (LangBiTe)

- Critical path:
  1. Input: English `PromptTemplates` with placeholders + target languages L + paraphrase count P
  2. For each template: Translate → Identify grammatical number → Generate P paraphrases → Filter via regex
  3. Output: Augmented multilingual template library compatible with LangBiTe

- Design tradeoffs:
  - **Pipeline order**: P2T (paraphrase-then-translate) vs. T2P (translate-then-paraphrase)—paper finds marginal difference, selects T2P
  - **Paraphrase count (P)**: Higher P increases coverage but not semantic diversity (Figure 2 shows P=2,5,10 produce similar similarity distributions)
  - **Model selection**: GPT-4o chosen for translation/paraphrasing due to highest average cosine similarity, but this may bias subsequent GPT-4o bias evaluation (acknowledged limitation)

- Failure signatures:
  - **Unprocessable responses**: 10.5% for Llama3 405B, highest in Racism (18.2%) and Sexism (14.3%) categories—due to structured output format non-compliance
  - **Translation non-compliance**: Claude 3.5 Sonnet showed 9.6% translation failures (refused to follow instructions)
  - **Low-resource degradation**: Luxembourgish shows highest unprocessable rate (9.7%) and lowest success rates

- First 3 experiments:
  1. **Validate translation quality on held-out templates**: Sample 20% of templates not used in original evaluation, translate to all 6 languages, compute cosine similarity against human references—verify GPT-4o advantage holds.
  2. **Ablate grammatical number enforcement**: Run paraphrasing with vs. without explicit grammatical number identification; measure malformed output rate after placeholder instantiation.
  3. **Cross-validate with alternative bias framework**: Feed MLA-BiTe augmented templates into a different bias detection system (not LangBiTe) to test framework-agnostic claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using the same LLM for both template augmentation (translation/paraphrasing) and bias evaluation introduce a systematic performance advantage or "self-preference" bias compared to using distinct models for each task?
- Basis: [inferred] The authors note in Section 5.2 that GPT-4o achieved anomalously high scores on Catalan and speculate that its role as the translator may have provided a "slight advantage," warranting further evaluation.
- Why unresolved: The experimental design used GPT-4o exclusively for the augmentation pipeline, meaning no control group existed where templates were generated by a different model.
- What evidence would resolve it: A comparative study where bias detection is run on templates generated by external models versus self-generated templates to isolate the augmentation model's effect on evaluation scores.

### Open Question 2
- Question: Why do bias patterns in LLMs fail to correlate with linguistic family groupings, such that low-resource languages (e.g., Luxembourgish) show bias patterns more similar to unrelated languages than to their linguistic relatives?
- Basis: [inferred] Section 6 highlights the unexpected finding that Luxembourgish (West Germanic) bias profiles aligned more closely with Spanish/Catalan (Romance) than with German or English, defying simple family groupings.
- Why unresolved: The study quantified these correlations but did not investigate the specific mechanisms—such as training data contamination or cross-lingual alignment techniques—that cause this divergence.
- What evidence would resolve it: An analysis of the LLMs' multilingual training corpora to determine if shared cultural concepts or parallel corpora construction methods link non-related languages more strongly than linguistic syntax.

### Open Question 3
- Question: Can automated translation strategies be modified to dynamically adapt content for cultural appropriateness (e.g., modifying references to alcohol or pork) without compromising the semantic integrity required for standardized bias testing?
- Basis: [explicit] Section 7 explicitly identifies "Exploring Cultural-Aware Translation" as a future work item to ensure translations remain respectful of norms in the target society.
- Why unresolved: Current translation prompts prioritize strict semantic preservation, which risks generating culturally insensitive test cases that could skew results or offend users.
- What evidence would resolve it: Developing a culturally-adaptive translation module and measuring the trade-off between cultural acceptability (via human review) and semantic consistency (via cosine similarity).

### Open Question 4
- Question: Does the effectiveness of the MLA-BiTe framework's grammatical constraints hold when applied to extra-European low-resource languages with complex morphological features?
- Basis: [explicit] Section 7 states that future work will extend to extra-European languages, noting that complex systems of grammatical number or noun classes "may require tailored strategies."
- Why unresolved: The current paraphrasing prompt relies on identifying singular/plural grammatical number, which may be insufficient or incorrect for languages with different morphological systems.
- What evidence would resolve it: Applying the framework to a language like Zulu or Swahili and evaluating the validity of generated paraphrases to see if the current "grammar number" prompt logic fails.

## Limitations

- Reliance on English template translation may introduce evaluation artifacts, particularly for low-resource languages where training data sparsity affects both model performance and evaluation validity
- Using GPT-4o for both template augmentation and bias evaluation (in GPT-4o's case) creates potential circularity concerns and "self-preference" bias
- Framework assumes placeholder syntax is language-agnostic, which may not hold for languages with complex grammatical agreement systems
- Difficulty distinguishing true model bias from evaluation artifacts in low-resource languages where cultural nuance transfer may be imperfect

## Confidence

- **High Confidence**: Translation and paraphrasing quality metrics (cosine similarity 0.85-0.95, BLEU diversity scores); observed pattern of low-resource languages showing higher bias rates; model-level bias differences (GPT-4o/Claude 3.5 lowest, Llama3 405B highest)
- **Medium Confidence**: Cross-linguistic bias transfer complexity claims; specific numerical success rates given translation and evaluation uncertainties
- **Low Confidence**: Claims about true model bias vs. evaluation artifacts for low-resource languages; generalizability of findings beyond tested language families

## Next Checks

1. **Translation Quality Validation**: Test MLA-BiTe translation quality on held-out templates not used in original evaluation to verify GPT-4o advantage holds across new samples.
2. **Grammatical Number Ablation**: Run paraphrasing with and without explicit grammatical number enforcement to quantify malformed output reduction.
3. **Framework-Agnostic Cross-Validation**: Evaluate MLA-BiTe augmented templates using an alternative bias detection framework to verify findings aren't LangBiTe-specific artifacts.