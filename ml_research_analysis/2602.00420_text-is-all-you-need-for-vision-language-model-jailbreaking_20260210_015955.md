---
ver: rpa2
title: Text is All You Need for Vision-Language Model Jailbreaking
arxiv_id: '2602.00420'
source_url: https://arxiv.org/abs/2602.00420
tags:
- text-dj
- what
- queries
- harmful
- distraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Text-DJ, a jailbreak attack that bypasses LVLM safety
  filters by exploiting OCR capabilities. Our method decomposes harmful queries into
  benign sub-queries, adds maximally irrelevant distraction queries, and presents
  them as images in a grid.
---

# Text is All You Need for Vision-Language Model Jailbreaking

## Quick Facts
- arXiv ID: 2602.00420
- Source URL: https://arxiv.org/abs/2602.00420
- Reference count: 40
- Key outcome: Text-DJ jailbreak attack bypasses LVLM safety filters by converting harmful text queries into image-based OCR inputs with distraction techniques

## Executive Summary
This paper introduces Text-DJ, a novel jailbreak attack that exploits the optical character recognition (OCR) capabilities of vision-language models (LVLMs) to bypass safety filters. The attack works by decomposing harmful queries into benign sub-queries, adding maximally irrelevant distraction queries, and presenting them as images in a grid format. By converting text-based prompts into visual inputs, Text-DJ evades traditional text-based safety filters while the distraction strategy prevents semantic linking of sub-queries. The method demonstrates high attack success rates against state-of-the-art LVLMs including Qwen3-VL, GPT 4.1 mini, and Gemini across multiple harmful categories.

## Method Summary
Text-DJ employs a three-stage approach to jailbreak LVLMs: first, harmful queries are decomposed into benign sub-queries that individually appear safe but collectively reconstruct the original harmful intent; second, maximally irrelevant distraction queries are added to prevent semantic linking of sub-queries; third, all text is rendered as images in a grid format, exploiting the LVLM's OCR capabilities to bypass text-based safety filters. The attack incorporates randomized colorization to further obscure the relationship between sub-queries. This cross-modal transformation converts textual jailbreak attempts into visual inputs that standard text-based safety filters cannot detect, while the distraction strategy ensures the LVLM cannot easily connect the fragmented information into harmful requests.

## Key Results
- High attack success rates across multiple harmful categories including unethical, illegal, and dangerous content
- Effective against state-of-the-art LVLMs including Qwen3-VL, GPT 4.1 mini, and Gemini
- Ablation studies confirm critical components: cross-modal transformation, semantic distraction strategy, and randomized colorization

## Why This Works (Mechanism)
Text-DJ exploits a fundamental vulnerability in LVLM safety alignment by leveraging the disconnect between text-based safety filters and multimodal input processing. Traditional safety filters scan for harmful patterns in text, but when that text is presented as an image, these filters become ineffective. The OCR capability of LVLMs allows them to read text from images, but this same capability creates a blind spot in safety alignment. By fragmenting harmful queries into benign sub-queries and adding irrelevant distractions, the attack prevents the model from semantically linking the components into a harmful request, even though each component is processed individually through OCR.

## Foundational Learning
- **OCR-based Text Extraction**: Why needed - To convert text-based jailbreak attempts into visual format that bypasses text filters; Quick check - Verify LVLM can accurately read text from various image formats and qualities
- **Semantic Distraction Strategy**: Why needed - To prevent the model from linking sub-queries into harmful requests; Quick check - Test whether distraction queries effectively prevent semantic association in controlled experiments
- **Cross-modal Attack Vectors**: Why needed - To exploit the gap between text-based and vision-based safety mechanisms; Quick check - Confirm that text presented as images bypasses traditional safety filters while remaining readable by LVLM OCR

## Architecture Onboarding
Component map: Harmful Query -> Decomposition -> Sub-query Generation -> Distraction Addition -> Image Grid Creation -> LVLM Input
Critical path: The attack succeeds when (decomposition prevents detection) AND (OCR accurately extracts text) AND (distractions prevent semantic linking)
Design tradeoffs: Balancing between sufficiently benign sub-queries to avoid detection while maintaining ability to reconstruct harmful intent
Failure signatures: Attack fails when sub-queries are too fragmented to reconstruct intent, distractions are semantically linked, or OCR fails to extract text accurately
First experiments: 1) Test OCR accuracy on various text presentations, 2) Verify distraction effectiveness in preventing semantic linking, 3) Validate attack success rates across different harmful categories

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness limited to controlled laboratory conditions with specific LVLM models
- Scalability to more complex harmful queries not fully explored
- Potential adaptive defenses and their effectiveness not characterized

## Confidence
- Attack methodology and basic effectiveness: High
- Ablation study results: Medium
- Generalizability across LVLMs and scenarios: Low

## Next Checks
1. Test Text-DJ against a broader range of LVLM models, including those with updated safety measures post-publication of this research
2. Evaluate the attack's effectiveness in real-world deployment scenarios, including different image formats, resolutions, and presentation contexts
3. Investigate potential adaptive defenses by simulating LVLM responses that could detect and mitigate fragmented multimodal adversarial inputs