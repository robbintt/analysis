---
ver: rpa2
title: 'BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese
  Medicine'
arxiv_id: '2510.17415'
source_url: https://arxiv.org/abs/2510.17415
tags:
- bencao
- reasoning
- language
- clinical
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BenCao, a ChatGPT-based multimodal assistant
  for Traditional Chinese Medicine (TCM) that addresses the challenge of applying
  large language models to TCM's holistic and multimodal diagnostic reasoning. BenCao
  is developed through natural language instruction tuning rather than parameter retraining,
  integrating over 1,000 classical and modern TCM texts, a scenario-based instruction
  framework, chain-of-thought simulation for interpretable reasoning, and expert feedback
  refinement from licensed TCM practitioners.
---

# BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2510.17415
- Source URL: https://arxiv.org/abs/2510.17415
- Reference count: 20
- Primary result: BenCao achieved superior accuracy compared to general-domain and TCM-domain models, particularly in diagnostics, herb recognition, and constitution classification.

## Executive Summary
BenCao is a ChatGPT-based multimodal assistant for Traditional Chinese Medicine (TCM) that addresses the challenge of applying large language models to TCM's holistic and multimodal diagnostic reasoning. The system is developed through natural language instruction tuning rather than parameter retraining, integrating over 1,000 classical and modern TCM texts, a scenario-based instruction framework, chain-of-thought simulation for interpretable reasoning, and expert feedback refinement from licensed TCM practitioners. BenCao connects to external APIs for tongue-image classification and multimodal database retrieval, enabling dynamic access to diagnostic resources. In evaluations, BenCao demonstrated superior accuracy compared to general-domain and TCM-domain models, particularly in diagnostics, herb recognition, and constitution classification. The model was deployed on the OpenAI GPTs Store, accessed by nearly 1,000 users globally, demonstrating its practical applicability and feasibility as a scalable pathway for real-world deployment of TCM-domain LLMs.

## Method Summary
BenCao was developed using natural language instruction tuning on the GPT-4 base model without modifying model weights. The approach integrates over 1,000 classical and modern TCM texts converted to clean text format, scenario-based prompts defining four interaction types (theory learning, mild discomforts, constitution/tongue diagnosis, seasonal wellness), chain-of-thought simulation with four sequential reasoning stages (symptom recognition, pattern differentiation, treatment principle reasoning, lifestyle recommendations), and information-seeking loops for clarification. The system incorporates human feedback refinement from three licensed TCM practitioners and connects to external APIs for tongue-image classification and multimodal database retrieval. The model enforces safety guardrails including mandatory disclaimers and prohibits prescription outputs.

## Key Results
- BenCao achieved 82.18% accuracy in herb recognition tasks, outperforming general-domain models.
- The system reached 63.42% accuracy in constitution classification, demonstrating strong domain-specific capabilities.
- In comprehensive evaluations across seven TCM disciplines, BenCao showed superior performance compared to GPT-4o, Gemini 2.5 Pro, Claude 3, Grok3, Qwen3, and Kimi 1.5 models.

## Why This Works (Mechanism)

### Mechanism 1
In-domain performance can be achieved via retrieval-augmented instruction tuning without modifying model weights. BenCao leverages a static base model (GPT-4) and injects domain expertise by uploading a curated corpus of 1,000+ TCM texts and binding them to scenario-based prompts. The model uses semantic search over these attached files to ground its responses in classical literature rather than relying solely on pre-trained parametric knowledge. The core assumption is that the base model possesses sufficient reasoning and retrieval capabilities to utilize the uploaded text context correctly and prioritize it over hallucinated internal knowledge.

### Mechanism 2
Simulating expert reasoning via structured Chain-of-Thought (CoT) and active inquiry improves diagnostic validity. The system enforces a four-stage reasoning path (Symptom Recognition → Pattern Differentiation → Treatment Principle → Recommendation) and grants the model "agency" to ask clarifying questions if information is insufficient. This mimics the TCM "Four Diagnostic Methods" and prevents premature conclusion. The core assumption is that the model can reliably determine "information sufficiency" and adhere to the stop/inquire logic without drifting into generic advice.

### Mechanism 3
Human feedback applied to natural language instructions aligns model behavior with professional norms better than prompt engineering alone. Licensed TCM practitioners review outputs and provide "critical" or "positive" feedback in natural language. This feedback is used to iteratively refine the system prompt and constraints, effectively "tuning" the model's behavior at the semantic level rather than the parameter level. The core assumption is that valid constraints and logic injected via the system prompt persist across diverse conversation contexts and do not degrade over long sessions.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) in Practice**
  - Why needed here: BenCao does not memorize 1,000 books; it retrieves from them. Understanding how to chunk, clean, and attach data so an LLM can "see" it is the core technical skill here.
  - Quick check question: If you upload a PDF to an LLM, does it "learn" the content forever, or must it retrieve it for every query? (Answer: It must retrieve it into the context window for every query.)

- **Concept: Chain-of-Thought (CoT) & Intermediate Reasoning**
  - Why needed here: The paper explicitly forces the model to "show its work" (syndrome differentiation). You need to understand that CoT is not just output formatting; it is a reasoning scaffold that improves accuracy.
  - Quick check question: Why would asking a model to "list symptoms before diagnosing" yield a different result than asking for a diagnosis directly?

- **Concept: Multimodal Integration via API Tools**
  - Why needed here: BenCao handles tongue images not by "seeing" them directly with the main LLM, but by calling an external classification API.
  - Quick check question: How does the text-based LLM understand the result of a visual analysis? (Answer: The API returns text/labels which the LLM then incorporates into its reasoning.)

## Architecture Onboarding

- **Component map**: Interface (OpenAI GPTs Store) -> Controller (System Prompt) -> Knowledge Store (1,000+ Cleaned/Chunked Text Files) -> Tools/Actions (External APIs) -> Refinement Loop (Expert Reviewers → Natural Language Feedback → Prompt Updates)

- **Critical path**: 
  1. Data Hygiene: Converting raw PDFs to clean text/Word docs is the most fragile step (OCR errors break retrieval).
  2. Instruction Design: Writing the "Scenario Instructions" so the model knows when to use which knowledge file.
  3. API Binding: Ensuring the tongue-analysis API outputs structured text the LLM can parse.

- **Design tradeoffs**: 
  - Speed vs. Control: Using GPTs infrastructure allows rapid deployment (no GPU cluster needed) but creates a "black box" dependency on the base model's reasoning quality.
  - Retrieval vs. Hallucination: The model is constrained to the 1,000 texts, reducing hallucinations but potentially missing modern research not in the corpus.

- **Failure signatures**: 
  - "I don't know": Model fails to retrieve relevant chunks from attached files (chunking strategy mismatch).
  - Premature Diagnosis: Model skips the "Inquiry" stage of CoT and gives generic advice.
  - Safety Override: User prompt injection convinces the model to ignore the "Reference Only" disclaimer.

- **First 3 experiments**:
  1. Retrieval Validation: Ask BenCao specific questions about a rare concept in one of the 1,000 texts and verify if it cites the correct source.
  2. CoT Stress Test: Provide vague symptoms and verify if the model enters the "information-seeking phase" (asking follow-up questions) or hallucinates a diagnosis.
  3. Multimodal Loop: Upload a synthetic tongue image and trace the execution flow to confirm the external API is called and its classification is used in the final text output.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating pulse signals and physiological data into BenCao improve diagnostic accuracy for complex TCM cases beyond the current multimodal capabilities? The paper states "the system's reliance on textual data and a few visual modalities also constrains its applicability to more complex diagnostic contexts, such as pulse analysis or integrative treatment planning." Pulse diagnosis requires tactile signal acquisition and interpretation, which are not currently supported by the text-and-image-based architecture.

### Open Question 2
Would combining expert feedback with reinforcement learning yield measurable improvements in TCM reasoning alignment compared to natural language instruction tuning alone? The paper notes "while human feedback refinement improved the model's behavioral stability and adherence to ethical guidelines, it cannot yet replace the rigor of reinforcement learning or comprehensive benchmark evaluation." The current study used only semantic-level optimization via expert feedback; reinforcement learning approaches were not implemented or compared.

### Open Question 3
What standardized benchmarks and evaluation protocols are required to ensure safety, transparency, and reproducibility in TCM-domain LLMs? The paper explicitly calls for "the creation of an open benchmark for TCM-domain LLMs with standardized evaluation protocols and ethical frameworks." Current evaluations use single-choice questions and limited classification tasks without unified TCM-specific safety or clinical-validity criteria.

### Open Question 4
How does BenCao's performance generalize across diverse linguistic and cultural contexts beyond the primarily Chinese-user deployment reported? The paper reports nearly 1,000 global users but does not analyze performance variations across languages, TCM practice traditions, or cultural health beliefs. TCM terminology and diagnostic concepts may not translate consistently, potentially affecting accuracy and user trust in non-Chinese contexts.

## Limitations
- The paper does not disclose exact prompt templates, knowledge file organization, or human feedback protocols, making faithful reproduction difficult.
- Model performance is benchmarked on undisclosed single-choice datasets and single-turn tasks, leaving multi-turn and practical application robustness unverified.
- The system depends on external API availability and file attachment limits of the GPTs platform, which are not fully specified.
- No quantitative evidence is provided for the model's adherence to the active inquiry loop or for the persistence of safety constraints across sessions.

## Confidence

- **High**: Retrieval-augmented instruction tuning can achieve in-domain performance without parameter retraining (supported by explicit architecture and documented results).
- **Medium**: Chain-of-thought simulation and active inquiry improve diagnostic validity (mechanism described, but multi-turn robustness unverified).
- **Medium**: Human feedback applied to instructions aligns model behavior with professional norms (protocol described, but lacks quantitative adherence metrics).

## Next Checks

1. **Retrieval Validation**: Test BenCao with specific questions about rare concepts in the 1,000 texts to verify accurate source citation and chunk mapping.
2. **CoT Stress Test**: Provide vague symptoms and confirm the model enters the information-seeking phase (asking follow-up questions) rather than generating premature diagnoses.
3. **Multimodal Loop Trace**: Upload a synthetic tongue image and trace execution to confirm the external API is called and its classification is incorporated into the final reasoning output.