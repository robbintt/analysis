---
ver: rpa2
title: 'STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game'
arxiv_id: '2505.03547'
source_url: https://arxiv.org/abs/2505.03547
tags:
- actions
- story
- action
- game
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STORY2GAME addresses the challenge of generating interactive fiction
  games from scratch using LLMs, where traditional approaches constrain stories to
  predefined actions. The system first generates a story with actions annotated with
  preconditions and effects, then builds a minimal text game engine to execute those
  actions.
---

# STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game

## Quick Facts
- arXiv ID: 2505.03547
- Source URL: https://arxiv.org/abs/2505.03547
- Authors: Eric Zhou; Shreyas Basavatia; Moontashir Siam; Zexin Chen; Mark O. Riedl
- Reference count: 31
- Primary result: Generates interactive fiction games from scratch using LLMs with dynamic action generation, achieving 80% compilation success and 60% semantic correctness for novel player actions.

## Executive Summary
STORY2GAME addresses the challenge of generating interactive fiction games from scratch using LLMs, where traditional approaches constrain stories to predefined actions. The system first generates a story with actions annotated with preconditions and effects, then builds a minimal text game engine to execute those actions. For player creativity, it dynamically generates new actions on-the-fly, inferring necessary objects, attributes, and dependencies from player input. Action generation succeeds 80% of the time compilation-wise, with 60% achieving semantic coherence matching human expectations. The approach enables emergent gameplay while keeping interactions grounded in the game world state, allowing players to explore stories beyond scripted paths.

## Method Summary
The system generates interactive fiction games through a pipeline: (1) Story generation using GPT-4o-mini with preconditions and effects annotation for each action, (2) World instantiation creating a graph of rooms, items, characters, and containers, (3) Action code compilation translating preconditions and effects into executable Python, and (4) Dynamic action generation handling player-initiated actions not in the original story by inferring necessary objects, attributes, and dependencies. The approach uses structured output formats and relies on LLM-generated semantic scaffolding to maintain game state consistency while enabling open-ended player agency.

## Key Results
- Dynamic action generation compilation success rate: ~80%
- Dynamic action semantic success rate: ~60%
- Story compilation success rate: ~97% per action
- Fully compiled stories rate: ~75%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated preconditions and effects serve as semantic scaffolding for executable action code generation.
- Mechanism: The story generator annotates each action with preconditions (what must be true: location, inventory, attributes, prior events) and effects (what changes: movement, attribute updates, object creation/deletion). These annotations are translated into code that checks and updates the game state graph.
- Core assumption: LLMs can reliably infer causally meaningful preconditions and effects from natural language action descriptions.
- Evidence anchors: [abstract] "The key to successful action generation is to use LLM-generated preconditions and effects of actions in the stories as guides for what aspects of the game state must be tracked and changed." [section III-A] Preconditions are split into fundamental (location/inventory), additional (custom attributes), and preceding events. [corpus] Weak corpus signal; related work (Starling, Word2World) focuses on story-to-game but not precondition-effect grounding.
- Break condition: If LLM misidentifies objects (e.g., "Key" vs. "Metallic Key"), compilation fails. This is the dominant failure mode per Table III.

### Mechanism 2
- Claim: Dynamic action generation enables open-ended player agency while maintaining grounded state consistency.
- Mechanism: When a player attempts an unrecognized action, the system prompts the LLM to infer preconditions, effects, required items, and preceding events. New objects are instantiated with random placement; new attributes are added with LLM-determined defaults; preceding events can trigger recursive action generation (depth-limited to 1).
- Core assumption: Commonsense reasoning in LLMs generalizes to novel action-context combinations within existing worlds.
- Evidence anchors: [abstract] "Dynamic action generation may require on-the-fly updates to the game engine's state representation and revision of previously generated actions." [section IV] New actions are restricted to operate on existing items to ensure grounding. [corpus] No direct corpus comparison for dynamic IF action generation; this appears novel.
- Break condition: Actions involving room properties (e.g., "illuminate forest") fail semantically because rooms cannot hold attributes in the current architecture (~60% semantic success).

### Mechanism 3
- Claim: Attribute propagation preserves world consistency after dynamic action creation.
- Mechanism: When a new action creates an attribute on an object, the system iterates through all existing actions involving that object and queries the LLM to determine if the attribute is relevant. If so, a precondition is appended to the existing action.
- Core assumption: LLMs can correctly judge attribute-action relevance across the action space.
- Evidence anchors: [section IV-D] "If the new action results in the creation of a new attribute for an object, we iterate through the other actions that involve said object." [section V-B] Fig. 4 shows new attributes are created in >50% of dynamic actions for both items and characters. [corpus] No corpus precedent for this propagation mechanism.
- Break condition: Over-constrained actions may make story completion impossible; the paper acknowledges this risk but defers automated detection to future work.

## Foundational Learning

- Concept: **Planning representations (preconditions/effects)**
  - Why needed here: The entire pipeline depends on translating natural language into structured state-change specifications that can be compiled to code.
  - Quick check question: Given "unlock chest with key," can you list three preconditions and two effects?

- Concept: **Graph-structured world state**
  - Why needed here: The game engine represents rooms, objects, and characters as nodes with attributes; actions manipulate this graph.
  - Quick check question: How would you represent a player carrying a key in a locked room as a graph?

- Concept: **Just-in-time compilation from LLM outputs**
  - Why needed here: Action code is not templated but generated per-story; structural consistency between LLM JSON and engine expectations is critical.
  - Quick check question: What happens if an LLM outputs "Metallic Key" where the engine expects "Key"?

## Architecture Onboarding

- Component map: Story Generator (GPT-4o-mini) → World Generator → Action Code Generator → Game Engine, with Dynamic Action Generator handling runtime player inputs

- Critical path:
  1. Story generation with precondition/effect annotation (III-A)
  2. World instantiation from story entities (III-B)
  3. Sequential action code compilation (III-C)
  4. Runtime: player input → match existing action OR trigger dynamic generation (IV)

- Design tradeoffs:
  - Room placement is random (no semantic spatial reasoning) for simplicity
  - Preceding event recursion is depth-1 limited to prevent cascades
  - Rooms cannot have attributes to avoid making them impassable
  - Dynamic actions must involve existing objects (no pure fabrication)

- Failure signatures:
  - Object misidentification (adjective variation): causes compilation failure
  - Actions on rooms (e.g., "illuminate"): compiles but semantically incorrect
  - Attribute over-constraint: may block story progress (not automatically detected)

- First 3 experiments:
  1. Run story compilation on 10 stories of varying length; log per-action success and object misidentification rate.
  2. Test 30 dynamic actions across 5 stories; manually verify semantic correctness against human expectations.
  3. Inject an action creating a new attribute (e.g., "break bucket"); trace propagation to existing "fill bucket" action.

## Open Questions the Paper Calls Out
None

## Limitations
- Object misidentification occurs in ~3-7% of actions when LLMs use inconsistent names for the same object (e.g., "Key" vs "Metallic Key")
- Semantic correctness of dynamic actions drops ~20% from compilation to semantic success, with actions on rooms failing systematically
- Attribute propagation may create over-constrained actions that block story completion, with no automated detection mechanism

## Confidence

**High Confidence** in the core concept that LLM-annotated preconditions and effects can guide action code generation. The mechanism is well-specified and the compilation success rates (80% for dynamic actions) provide quantitative support.

**Medium Confidence** in the semantic correctness of dynamically generated actions. While the paper claims 60% success, this self-assessment and the lack of independent validation reduce confidence in this metric.

**Low Confidence** in the scalability and robustness of the attribute propagation system. The mechanism is novel but untested for edge cases where attribute propagation might create impossible preconditions or contradictory game states.

## Next Checks

1. **Independent Semantic Validation**: Have three independent reviewers assess semantic correctness of 20 randomly selected dynamic actions across 3 different stories, using the same criteria described in the paper. Compare inter-rater agreement and calculate Cohen's kappa to establish reliability.

2. **Attribute Propagation Stress Test**: Create a test suite where new actions systematically create attributes that should and should not propagate to existing actions. Verify that the propagation mechanism correctly identifies relevant cases (precision) and doesn't miss relevant cases (recall), aiming for both metrics above 80%.

3. **Failure Mode Analysis**: Instrument the system to log all object misidentification incidents during story compilation. Analyze whether these failures cluster around specific object types, action descriptions, or story lengths, and test whether canonical ID usage in prompts reduces failure rates below 3%.