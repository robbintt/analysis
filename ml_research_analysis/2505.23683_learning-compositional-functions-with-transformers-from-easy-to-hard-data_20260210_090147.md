---
ver: rpa2
title: Learning Compositional Functions with Transformers from Easy-to-Hard Data
arxiv_id: '2505.23683'
source_url: https://arxiv.org/abs/2505.23683
tags:
- gradient
- hop2
- have
- layer
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the learnability of a k-fold composition task,
  which requires computing an interleaved composition of k input permutations and
  k hidden permutations, and can be expressed by a transformer with O(log k) layers.
  On the negative front, the authors prove a Statistical Query (SQ) lower bound showing
  that any SQ learner that makes only polynomially-many queries to an SQ oracle for
  the k-fold composition task distribution must have sample size exponential in k.
---

# Learning Compositional Functions with Transformers from Easy-to-Hard Data

## Quick Facts
- **arXiv ID**: 2505.23683
- **Source URL**: https://arxiv.org/abs/2505.23683
- **Reference count**: 40
- **Primary result**: Proves statistical query lower bound for k-fold composition task while showing polynomial-time learnability via curriculum learning with O(log k)-depth transformers

## Executive Summary
This paper studies the learnability of k-fold composition tasks, which require computing interleaved compositions of k input and hidden permutations. The authors establish both negative and positive results: they prove an exponential statistical query (SQ) lower bound for learning these functions from hard examples alone, while demonstrating that O(log k)-depth transformers can learn them efficiently using curriculum learning strategies that present data in increasing difficulty. The work highlights the fundamental role of data composition (easy-to-hard examples) in enabling transformers to learn complex compositional reasoning tasks, providing theoretical justification for curriculum learning approaches in training deep models.

## Method Summary
The method involves learning k-fold composition functions using O(log k)-depth transformers through curriculum learning. The training procedure uses an L-stage curriculum (where L = log₂k + 1) where each stage ℓ focuses on learning 2^(ℓ-1)-hop examples. The transformer architecture has fixed sparse value matrices W^(ℓ)_OV and trainable attention weights W^(ℓ)_KQ initialized at zero. Training proceeds by taking gradient steps on attention weights and readout layers sequentially for each curriculum stage, using sufficient samples (M ≥ Ω(k⁴N⁶)) to ensure accurate gradient estimates. The approach contrasts with training on only hard k-hop examples, which provably requires exponential sample complexity.

## Key Results
- Proves exponential statistical query lower bound for learning k-fold composition tasks using only hard examples
- Shows O(log k)-depth transformers can express k-fold composition functions with specific construction
- Demonstrates polynomial-time learnability via curriculum learning with runtime and sample complexity polynomial in k
- Establishes that data mixture (easy-to-hard examples) is necessary for efficient learning, while sufficient for polynomial-time convergence

## Why This Works (Mechanism)

### Mechanism 1: Parallel Doubling of Composition Depth
- **Claim:** An O(log k) depth transformer can express a k-fold composition task by recursively doubling the "hop" distance at each layer
- **Mechanism:** Layer ℓ computes 2^(ℓ-1)-hop by attending to previous layer's hop calculation, composing two 2^(ℓ-1)-hops to get 2ℓ-hop
- **Core assumption:** Model uses specific input embeddings storing position information for attention to retrieve correct next hop index
- **Evidence anchors:** [abstract]: "...can be expressed by a transformer with O(log k) layers"; [section 3 / Theorem 1]: proves expressivity via recursive construction; [corpus]: "Layer Specialization..." discusses compositional task specialization
- **Break condition:** If k is not power of 2 or embedding dimension insufficient to store intermediate hops, recursive construction fails

### Mechanism 2: Implicit Curriculum via Vanishing Gradients
- **Claim:** Training on mixture of easy and hard examples allows gradient descent to learn efficiently, while hard-only fails
- **Mechanism:** Gradient for deeper layer ℓ is effectively zero when previous layers at zero initialization; signal activates only after previous layer learns non-uniform attention scores
- **Core assumption:** Optimizer uses gradient descent with zero initialization (W_KQ = 0)
- **Evidence anchors:** [abstract]: "...efficiently learned... via two different curriculum learning strategies..."; [appendix D.1]: "When previous layer is not trained and stays zero, all later layers have zero gradient"; [corpus]: Weak direct evidence, related works focus on compositional generalization
- **Break condition:** If learning rate too high or batch size too small, noise destabilizes sequential locking before hard task gradients activate

### Mechanism 3: SQ Lower Bound on Hard-Only Data
- **Claim:** Any SQ learner requires exponential sample complexity to learn k-fold composition using only hard data
- **Mechanism:** Function class is "nearly orthogonal" with respect to input distribution; correlational queries cannot efficiently distinguish target from noise without intermediate supervision
- **Core assumption:** Learner queries function via statistical properties rather than memorizing specific samples
- **Evidence anchors:** [abstract]: "...prove a Statistical Query (SQ) lower bound... sample size exponential in k"; [theorem 2 / appendix B]: demonstrates near-orthogonality of function class
- **Break condition:** If learner has access to intermediate difficulty examples (k' < k), orthogonality barrier breaks and sample complexity becomes polynomial

## Foundational Learning
- **Concept: Statistical Query (SQ) Model**
  - **Why needed here:** Frames difficulty of learning as SQ lower bound, serving as theoretical proxy for gradient descent limitations on noisy/hard data
  - **Quick check question:** Can learning algorithm succeed using only aggregate statistics (mean gradients) of data, or does it need to memorize specific hard examples?

- **Concept: Curriculum Learning**
  - **Why needed here:** Core solution proposed to bridge gap between exponential lower bound (hard data) and polynomial upper bound (mixed data)
  - **Quick check question:** Does order or mixture of data difficulties (easy vs. hard) significantly alter convergence speed or final accuracy?

- **Concept: Compositional Reasoning (k-hop)**
  - **Why needed here:** Specific task involves iterating function k times, testing model's ability to handle depth and recurrence via parallel attention
  - **Quick check question:** Can model decompose long reasoning chain into parallelizable steps rather than single sequential chain?

## Architecture Onboarding
- **Component map:** Input Embedding -> Layer 1 (Find 1-hop) -> Layer 2 (Find 2-hop using 1-hop) -> ... -> Output
- **Critical path:** Input → Layer 1 (Find 1-hop) → Layer 2 (Find 2-hop using 1-hop) → ... → Output; gradient must flow back from final loss to guide attention pattern (W_KQ)
- **Design tradeoffs:**
  - **Fixed vs. Trainable W_OV:** Fix Value matrix to sparse/identity structure to simplify dynamics; note learning from scratch is hard due to vanishing gradient at zero initialization (Appendix E)
  - **Depth vs. Width:** Construction requires O(log k) depth but potentially large width (O(kN)) to store state of all hops
- **Failure signatures:**
  - **Training Collapse:** If trained on hard-only data, model appears to train (loss might jitter) but won't converge to correct solution, likely staying at random accuracy
  - **Layer Stagnation:** In curriculum training, if learning rate insufficient for ℓ-th layer to lock in before ℓ+1 stage begins, signal degrades
- **First 3 experiments:**
  1. **Verify Expressivity:** Implement constructed transformer weights (from Appendix A) to confirm zero loss on task
  2. **Hard vs. Mixed Training:** Train randomly initialized transformer on only k-fold data vs. mixture of 1..k-fold data; verify hard-only fails to converge (exponential samples) while mixed succeeds (polynomial samples)
  3. **Ablate Depth:** Train model with < log k layers; verify failure to represent function, confirming logarithmic depth necessity

## Open Questions the Paper Calls Out
- **Open Question 1:** How can transformers efficiently learn the value matrices (W_OV) in k-fold composition task? [explicit] Section 7 states population gradient vanishes at zero initialization, leaving precise study of learning dynamics to future work. Unresolved because main theorem relies on fixing these matrices to identity-like structures while noting "other ingredients" needed to learn them via gradient descent. Evidence: proof that non-zero initialization or specific data distributions allow gradient flow, or alternative architecture avoiding vanishing gradient issue.

- **Open Question 2:** Can learned model be distilled into smaller dimensional model, or can smaller model with trainable embedding map learn task? [explicit] Section 7 notes current construction requires embedding dimension poly(Nk), which appears suboptimal. Unresolved because theory proves expressivity with large embeddings but leaves open whether same compositional reasoning achievable in more parameter-efficient settings (e.g., Õ(mN)). Evidence: theoretical guarantee or empirical demonstration that task is learnable with smaller embedding dimensions or through model distillation techniques.

- **Open Question 3:** Do learning guarantees generalize to standard decoder-only transformers? [inferred] Section 1.1 highlights paper uses cross-attention, explicitly noting decoder-only lower bounds (Chen et al. [12]) do not apply to their setting. Unresolved because theoretical construction relies on cross-attention mechanisms, leaving applicability to predominant decoder-only LLM architectures unproven. Evidence: extending construction and convergence proof to decoder-only setting, or showing separation in learnability between two architectures.

## Limitations
- Theoretical analysis relies on highly specific transformer construction with fixed sparse value matrices and zero-initialized attention weights, unclear if standard training discovers these weights without exact curriculum
- SQ lower bound applies specifically to SQ model; connection to real-world gradient descent limitations not formally established
- Minimal experimental validation; no empirical results demonstrating standard transformers learn these compositional tasks under proposed curriculum

## Confidence
- **High confidence:** Transformer construction (Theorem 1) mathematically correct; SQ lower bound (Theorem 2) properly proven - pure mathematical results with clear proofs
- **Medium confidence:** Polynomial-time learnability result (Theorem 3) via curriculum learning correct but relies on strong assumptions about optimizer and specific architectural constraints that may not hold in practice
- **Low confidence:** Practical implications for standard transformer training uncertain; without empirical validation, unclear whether theoretical curriculum requirements can be relaxed or construction transfers to realistic settings

## Next Checks
1. **Expressivity verification:** Implement constructed transformer weights from Appendix A and empirically verify they achieve zero loss on k-fold composition tasks for various k and N values
2. **Curriculum vs. Hard-only training:** Train standard transformer (random initialization, Adam optimizer) on both hard-only data and mixed curriculum data, measuring convergence speed and final accuracy to test whether theoretical separation appears in practice
3. **Value matrix learning:** Test whether learning value matrices from scratch (rather than fixing them) succeeds under proposed curriculum, or whether zero-initialization gradient vanishing problem identified in Appendix E is practical barrier