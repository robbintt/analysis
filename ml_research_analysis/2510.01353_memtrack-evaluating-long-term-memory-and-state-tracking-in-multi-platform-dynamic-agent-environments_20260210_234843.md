---
ver: rpa2
title: 'MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform
  Dynamic Agent Environments'
arxiv_id: '2510.01353'
source_url: https://arxiv.org/abs/2510.01353
tags:
- memory
- arxiv
- agent
- ticket
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEMTRACK introduces a benchmark for evaluating long-term memory
  and state tracking in multi-platform agent environments. It simulates realistic
  enterprise workflows by integrating asynchronous events across Slack, Linear, and
  Git, with noisy, conflicting, and cross-referring information.
---

# MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments

## Quick Facts
- arXiv ID: 2510.01353
- Source URL: https://arxiv.org/abs/2510.01353
- Reference count: 40
- Key outcome: GPT-5 achieves only 60% Correctness on MEMTRACK, demonstrating significant challenges with cross-platform memory and state tracking in complex enterprise environments.

## Executive Summary
MEMTRACK introduces a benchmark for evaluating long-term memory and state tracking in agents operating across multi-platform environments like Slack, Linear, and Git. The dataset simulates realistic enterprise workflows through manually curated timelines with noisy, conflicting, and cross-referring information. Experiments show that state-of-the-art LLMs, including GPT-5, struggle with utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions, with GPT-5 achieving only 60% Correctness. The benchmark provides metrics for Correctness, Efficiency, and Redundancy, revealing that memory backends like MEM0 and ZEP fail to improve performance due to ineffective tool utilization.

## Method Summary
MEMTRACK evaluates agents in a containerized environment where events are injected into mock Slack, Linear, and Git servers. Agents interact through specific tool APIs to answer sequential questions about chronologically interleaved timelines. The benchmark uses LLM-as-judge (GPT-4o) to score Correctness, Efficiency (exponential decay of tool calls), and Redundancy (fraction of duplicate calls). The dataset consists of 47 manually curated instances with platform entropy and chronological heterophily metrics to quantify cross-platform complexity. Questions are introduced strictly sequentially to prevent preemptive solution planning and test actual memory retention.

## Key Results
- GPT-5 achieves only 60% Correctness on MEMTRACK, significantly below human performance
- Memory backends (MEM0, ZEP) fail to improve performance, with MEM0 actually increasing redundancy by 2.8%
- Follow-up questions show consistent performance drops (GPT-5: 0.601 → 0.571), indicating poor context maintenance
- High redundancy (>20%) is common across models, revealing inefficient tool utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributing information across multiple platforms with chronological interleaving forces agents to engage in cross-platform context switching, which tests memory acquisition and state tracking capabilities.
- Mechanism: Information fragmentation across Slack, Linear, and Git creates dependencies where no single platform contains complete answers. Agents must maintain and coordinate state across multiple information sources, accessing tools sequentially rather than processing consolidated context.
- Core assumption: Cross-platform information distribution increases cognitive/processing load proportionally to platform entropy and interleaving frequency.
- Evidence anchors:
  - [abstract]: "Each benchmark instance provides a chronologically platform-interleaved timeline, with noisy, conflicting, cross-referring information"
  - [section 3]: "The timeline events will be only accessible through tools set by the servers and are not available to agents as a whole. This necessitates multi-platform context switching, information retention and effective reasoning."
  - [corpus]: Related benchmarks (LoCoMo, LongMemEval) focus on single-thread conversational settings; MEMTRACK differentiates by multi-platform complexity, suggesting this is a novel stress test.
- Break condition: If agents can cache entire timelines upfront or if platform entropy is low (events clustered by platform), the mechanism would fail to create meaningful cross-platform reasoning pressure.

### Mechanism 2
- Claim: Including conflicting and noisy information across the timeline tests conflict resolution capability, requiring temporal reasoning to identify and resolve contradictions.
- Mechanism: Contradictory information (e.g., version A mentioned early, version B later) forces agents to track information provenance temporally, determining which updates supersede earlier statements rather than treating all retrieved information as equally valid.
- Core assumption: Agents default to treating retrieved information as authoritative unless explicitly tracking temporal precedence and conflict relationships.
- Evidence anchors:
  - [abstract]: "benchmark tests memory capabilities such as acquisition, selection and conflict resolution"
  - [section 5.1]: "resolving contradictions" identified as a key challenge; GPT-5 with memory components showed no significant improvement
  - [corpus]: Evo-Memory paper discusses self-evolving memory for test-time learning, suggesting conflict resolution is an active research area but mechanisms remain underexplored.
- Break condition: If conflicts are trivially resolvable (e.g., explicit timestamps always present) or if agents ignore temporal ordering, the mechanism would not effectively test conflict resolution.

### Mechanism 3
- Claim: Sequential question injection without lookahead forces agents to rely on actual memory retention rather than pre-planning, revealing retention decay on follow-up questions.
- Mechanism: Questions are introduced one at a time ("strictly sequentially"), preventing agents from batching information gathering or pre-computing answers. This tests whether agents retain information from earlier tool calls when answering later questions.
- Core assumption: Without sequential constraints, agents would optimize by gathering all needed information upfront, masking memory retention weaknesses.
- Evidence anchors:
  - [section 3]: "questions are introduced strictly sequentially to remove the possibility of preemptive solution planning"
  - [section 5.1, Table 4]: Follow-up questions show consistent performance drop (e.g., GPT-5+NOMEM: 0.601 → 0.571)
  - [corpus]: CloneMem and EverMemBench also emphasize multi-turn interactive memory, but MEMTRACK's sequential constraint is explicitly designed to prevent pre-planning.
- Break condition: If agents can maintain perfect working memory across all tool calls, or if follow-up questions are independent rather than building on earlier context, sequential injection would not reveal retention differences.

## Foundational Learning

- Concept: **Memory Acquisition vs. Utilization vs. Maintenance**
  - Why needed here: The paper explicitly structures evaluation around these three memory capabilities (Section 1, Section 2). Understanding this taxonomy is required to interpret why current memory backends (MEM0, ZEP) fail to improve performance—they may support acquisition but not effective utilization or conflict-aware maintenance.
  - Quick check question: Can you explain why adding MEM0 or ZEP memory backends did not improve GPT-5's performance on MEMTRACK, and which memory capability (acquisition, utilization, or maintenance) appears to be the bottleneck?

- Concept: **Platform Entropy and Chronological Heterophily**
  - Why needed here: The paper introduces specific metrics (Section 3, Table 1) to quantify cross-platform complexity. Platform entropy measures how evenly events are distributed across platforms; chronological heterophily measures platform switching frequency. These metrics are critical for understanding benchmark difficulty and ensuring ecological validity.
  - Quick check question: If a benchmark instance has platform entropy of 0.2 and chronological heterophily of 0.1, would you expect it to effectively test cross-platform memory? Why or why not?

- Concept: **Ecological Validity in Benchmark Design**
  - Why needed here: The paper emphasizes creating "ecologically valid scenarios grounded in real-world software development" (Section 3). Understanding this principle is required to evaluate whether MEMTRACK's complexity (noisy events, vague questions, cross-references) reflects realistic agent deployment conditions versus artificial benchmark artifacts.
  - Quick check question: The benchmark uses "strategic vagueness" in questions (e.g., "that caching situation we dealt with" instead of "ticket LIN-447"). How does this design choice affect the gap between benchmark performance and real-world agent utility?

## Architecture Onboarding

- Component map: Timeline Generator -> Containerized Environment -> Agent Tool Interface -> Sequential Question Injector -> Evaluator

- Critical path:
  1. Timeline curation (manual/agent-based/hybrid) → Event JSON + config YAML
  2. Environment setup: Docker containers + event injection into Slack/Linear/Git servers
  3. Agent execution: Tool calls across platforms to gather information
  4. Sequential question presentation: Agent answers each question before seeing the next
  5. Evaluation: Correctness (LLM-judge), Efficiency (tool call count), Redundancy (duplicate/subsumed calls)

- Design tradeoffs:
  - **Dataset scale (47 instances) vs. diversity**: Small dataset limits statistical power, but manual curation ensures ecological validity and complexity
  - **Brief phrase outputs vs. open-ended**: Reduces non-determinism and MCQ bias, but may not capture full reasoning chains
  - **Memory backend agnosticism vs. implementation specificity**: Allows testing any memory approach, but makes it harder to diagnose *why* specific backends fail
  - **Temperature=1 for all experiments**: Encourages exploration but increases variance (σ reported in Table 6)

- Failure signatures:
  - **High redundancy (>20%)**: Agent repeats tool calls (e.g., re-reading same ticket after interlude), indicating poor retention
  - **Performance drop on follow-up questions**: Suggests inability to maintain context across sequential queries
  - **Low tool call entropy**: Agent over-relies on one platform, failing cross-platform reasoning
  - **Memory backend increases redundancy**: Adding MEM0/ZEP causes more redundant calls without improving correctness (Table 3)

- First 3 experiments:
  1. **Baseline without memory**: Run GPT-5+NOMEM on all 47 instances to establish correctness ceiling (~60%) and identify which instance types (high entropy, high cross-references) are hardest
  2. **Memory backend comparison**: Run GPT-5+MEM0 and GPT-5+ZEP with identical seeds; analyze whether memory tools are *called* at all, and if calls correlate with correctness improvements on specific question types (e.g., temporal reasoning, conflict resolution)
  3. **Ablation on platform complexity**: Create synthetic variants of 5 instances by (a) reducing platform entropy (cluster events by platform), (b) removing conflicts, (c) removing cross-references; measure impact on correctness and redundancy to validate which complexity dimensions drive difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can memory architectures be redesigned to reduce the redundancy of tool calls and improve efficiency in multi-platform environments?
- Basis in paper: [explicit] In Section 5.3 (RQ2), the authors conclude that "memory equipped LLMs fail to call memory tools effectively" and that the use of memory components "consistently display[s] increased redundancy."
- Why unresolved: The experiments show that adding memory backends (MEM0, Zep) resulted in higher Redundancy scores compared to the NOMEM baseline, indicating that current models prefer repeatedly accessing external information over utilizing stored memories.
- What evidence would resolve it: A memory-augmented agent architecture that achieves a statistically significant reduction in Redundancy (fraction of redundant tool calls) and higher Efficiency compared to the NOMEM baseline on the MEMTRACK benchmark.

### Open Question 2
- Question: What specific modeling improvements are required to break the 60% correctness ceiling for long-horizon, cross-platform reasoning?
- Basis in paper: [explicit] Section 5.3 (RQ1) highlights that frontier models like GPT-5 exhibit "suboptimal performance" and that there is "significant room for improvement, particularly on improving large context reasoning."
- Why unresolved: The paper demonstrates that even the most capable models fail to reliably resolve conflicts or track state over long timelines, but it does not propose architectural or training solutions to bridge this performance gap.
- What evidence would resolve it: A model or agent framework that achieves a Correctness score significantly higher than 0.60 on the MEMTRACK dataset, specifically demonstrating superior performance on follow-up questions which currently show a drop in accuracy.

### Open Question 3
- Question: How does agent performance change when the evaluation task requires active intervention in the timeline rather than passive information retrieval?
- Basis in paper: [explicit] The Conclusion states, "In the future, MEMTRACK can be extended to... settings where the agent is... contextually act, such as creating Linear tickets... [to] drive the overall organization."
- Why unresolved: The current study explicitly focuses on "passive" answering to establish an "ecologically valid base," leaving the "harder class of settings" involving proactive workflow manipulation unexplored.
- What evidence would resolve it: Results from an extended MEMTRACK benchmark where agents must successfully execute actions (e.g., sending Slack messages, updating Linear tickets) to alter the environment state to solve tasks.

## Limitations

- **Dataset Scale**: Only 47 manually curated instances limit statistical power and may not capture full diversity of real-world workflows
- **Model Accessibility**: GPT-5 is not publicly available, making exact performance replication impossible
- **Memory Backend Implementation**: Insufficient details on MEM0/ZEP integration make it unclear whether failures are architectural or implementation-specific

## Confidence

- **High Confidence**: The benchmark design and evaluation framework are well-specified and reproducible
- **Medium Confidence**: The conclusion that current LLMs struggle with long-term memory and state tracking is well-supported
- **Low Confidence**: The claim that memory backends fundamentally cannot solve these problems is premature

## Next Checks

1. **Memory Backend Implementation Study**: Create detailed implementation log comparing MEM0 vs ZEP vs no-memory conditions, including exact tool call patterns and memory storage/retrieval behaviors
2. **Dataset Expansion and Diversity Analysis**: Generate 20 additional instances covering underrepresented scenario types and measure whether performance trends hold across broader distribution
3. **Alternative Model Comparison**: Run benchmark on multiple model families (Claude-3, Gemini-1.5-Pro, GPT-4) with consistent prompting to establish whether 60% Correctness ceiling is model-specific