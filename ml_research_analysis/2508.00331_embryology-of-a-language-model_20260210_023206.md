---
ver: rpa2
title: Embryology of a Language Model
arxiv_id: '2508.00331'
source_url: https://arxiv.org/abs/2508.00331
tags:
- tokens
- umap
- token
- spacing
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an "embryological" approach to studying language
  model development by visualizing susceptibility vectors using UMAP dimensionality
  reduction. The authors track how a low-dimensional representation of token-level
  susceptibilities evolves during training, revealing the emergence of internal structures
  analogous to anatomical features.
---

# Embryology of a Language Model

## Quick Facts
- **arXiv ID:** 2508.00331
- **Source URL:** https://arxiv.org/abs/2508.00331
- **Authors:** George Wang; Garrett Baker; Andrew Gordon; Daniel Murfet
- **Reference count:** 40
- **Primary result:** Introduces susceptibility-based UMAP visualization to reveal developmental "body plan" structures in language models

## Executive Summary
This paper introduces an "embryological" approach to studying language model development by visualizing susceptibility vectors using UMAP dimensionality reduction. The authors track how a low-dimensional representation of token-level susceptibilities evolves during training, revealing the emergence of internal structures analogous to anatomical features. The visualizations reveal a clear "body plan" consisting of an anterior-posterior axis reflecting global expression versus suppression, a dorsal-ventral stratification corresponding to the induction circuit, and a previously unknown "spacing fin" structure for counting space tokens.

## Method Summary
The authors train a 3M parameter, 2-layer attention-only transformer on subsets of the Pile dataset, computing susceptibility vectors for ~260k token sequences at four training checkpoints. They apply UMAP to the standardized 16-dimensional susceptibility data (one dimension per attention head) and color points by token pattern to track structural emergence. The approach reveals how different computational mechanisms (induction, spacing, etc.) develop distinct geometric signatures in the susceptibility manifold, with variance shifts along specific principal components indicating circuit formation.

## Key Results
- Reveals a "body plan" with anterior-posterior axis (expression vs. suppression), dorsal-ventral stratification (induction circuit), and "spacing fin" for space token counting
- Induction circuit emergence visible as thickening along dorsal-ventral axis, with PC2 variance increasing from 2.3% to 6.4% between steps 9000-17500
- Rainbow-colored stratification by token pattern (word start/end, induction, spacing) becomes increasingly organized during training
- Demonstrates susceptibility analysis can discover novel computational mechanisms beyond validation of existing circuits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token sequences cluster by computational function when projected into susceptibility space.
- **Mechanism:** The susceptibility vector η_w(xy) = (χ^1_xy, ..., χ^H_xy) captures how each attention head responds to a token-in-context. Tokens requiring similar computational processing exhibit similar susceptibility profiles across heads, which UMAP preserves as local neighborhoods.
- **Core assumption:** Coordinated susceptibility patterns across components indicate shared computational function (the "guilt-by-association" principle from genomics).
- **Evidence anchors:**
  - [abstract] "The visualizations reveal a clear 'body plan'... dorsal-ventral stratification corresponding to the induction circuit, and a previously unknown 'spacing fin' structure"
  - [Section 4.1] "This stratification results from patterns in the tokens inducing differentiated patterns of susceptibilities across the heads"
  - [corpus] Related work "Towards Spectroscopy: Susceptibility Clusters in Language Models" (arXiv:2601.12703) confirms susceptibility-based clustering is reproducible.
- **Break condition:** Different tokenizers may produce different stratification patterns; the spacing fin prominence is tokenizer-dependent (Section 6, Limitations).

### Mechanism 2
- **Claim:** The primary organizational axis reflects global expression versus suppression across all heads.
- **Mechanism:** PC1 captures average susceptibility across heads, explaining 95.19% of variance at end of training. Tokens broadly suppressed (positive susceptibility) cluster at the anterior; broadly expressed tokens (negative susceptibility) cluster at the posterior.
- **Core assumption:** Expression/suppression duality is fundamental to neural network computation, analogous to excitation/inhibition balance in biological networks.
- **Evidence anchors:**
  - [Section 4.1] "tokens are distributed along PC1 according to their average susceptibility across heads: tokens that are broadly suppressed are at the anterior end, tokens that are broadly expressed are at the posterior"
  - [Section 6] "the duality between expression and suppression may be fundamental to how computation in neural networks is organized"
  - [corpus] "Patterning: The Dual of Interpretability" (arXiv:2601.13548) provides theoretical grounding for susceptibility-based structure analysis.
- **Break condition:** May not hold for architectures where component roles are less differentiated (e.g., MLP-heavy models not tested).

### Mechanism 3
- **Claim:** Circuit formation produces detectable geometric changes in the susceptibility manifold.
- **Mechanism:** As the induction circuit forms during stages LM3-LM4, induction tokens develop differentiated susceptibility patterns, increasing variance along PC2. This manifests visually as "thickening" along the dorsal-ventral axis.
- **Core assumption:** Structural inference can detect circuits even without knowing the mechanism a priori.
- **Evidence anchors:**
  - [Section 4.2] "the percentage of total variance of the subset of induction tokens accounted for by PC2 increases from 2.3% to 6.4% between 9000 and 17500 steps"
  - [Section 4.2] "the emergence of the induction circuit is represented visually in the development of the UMAP by thickening along the dorsal-ventral axis"
  - [corpus] Limited external validation; corpus neighbors focus on theoretical foundations rather than empirical replication of this specific finding.
- **Break condition:** Small or diffuse circuits may not produce statistically detectable variance shifts; currently demonstrated only on 3M parameter model.

## Foundational Learning

- **Concept:** Per-token susceptibility (χ^C_xy)
  - **Why needed here:** This is the fundamental observable. It quantifies how component C's perturbations covary with changes in loss on token y in context x.
  - **Quick check question:** Given a token with negative susceptibility on head h, would increasing head h's weights tend to increase or decrease p(y|x, w)?

- **Concept:** Quenched posterior (p^β_n(w))
  - **Why needed here:** Susceptibilities are defined as expectations under this distribution, which concentrates on low-loss perturbations.
  - **Quick check question:** Why does the quenched posterior suppress perturbations that increase the population loss L(w)?

- **Concept:** UMAP preservation properties
  - **Why needed here:** The paper's visual conclusions depend on UMAP faithfully representing local neighborhoods. Global distances may be distorted.
  - **Quick check question:** If two tokens appear far apart in a UMAP plot, can you conclude they have different susceptibility profiles in the original 16D space?

## Architecture Onboarding

- **Component map:** 2-layer attention-only transformer (16 heads: 0:0–0:7, 1:0–1:7) -> SGLD susceptibility estimator -> UMAP visualization

- **Critical path:**
  1. Sample ~260k token sequences from training distribution
  2. For each checkpoint w(t), compute 16D susceptibility vector per token
  3. Standardize data matrix, apply UMAP
  4. Color points by token pattern; track evolution across checkpoints

- **Design tradeoffs:**
  - UMAP preserves local topology but not global distances; use PCA as sanity check for large-scale phenomena
  - n_neighbors=45 balances local detail vs. spurious clustering (verified robust from 15 to 1000)
  - Per-pattern aggregation trades individual-token resolution for clearer developmental trajectories

- **Failure signatures:**
  - Spurious clusters appearing only at low n_neighbors values (dismiss as artifact)
  - Structures not visible in PCA top-3 components may still be real (spacing fin visible in UMAP but not PCA 3D—see Appendix C.2)
  - Cross-seed differences in timing may reflect different stage boundaries, not different mechanisms

- **First 3 experiments:**
  1. **Reproduce the rainbow serpent on seed 1:** Compute susceptibilities at step 49900, generate UMAP, verify token-pattern stratification matches Figure 1.
  2. **Ablate the spacing fin:** Test whether removing spacing tokens from training data eliminates the fin, confirming it's not a tokenizer artifact.
  3. **Track a new circuit:** Apply the same methodology to a model with a known circuit (e.g., copy suppression) to validate that circuit formation consistently produces variance increases along a secondary axis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the complete mechanistic explanation for how the "spacing fin" structure operates to count space tokens?
- **Basis in paper:** [explicit] "At present we lack a complete mechanistic explanation of how this structure operates (analogous to the idealized model of the induction circuit) and this would be an interesting avenue for future work."
- **Why unresolved:** The authors discovered this structure through susceptibility visualization but have not identified which specific attention heads or circuit mechanisms implement spacing token counting.
- **What evidence would resolve it:** Targeted ablation experiments on heads with high spacing token susceptibility loadings; mechanistic analysis similar to that performed on induction heads.

### Open Question 2
- **Question:** Does susceptibility-based UMAP visualization scale to larger language models for discovering computational structure?
- **Basis in paper:** [explicit] "The primary limitation is the scale of our model: a 3M parameter, two-layer attention-only transformer. Whether the UMAP of susceptibility vectors is useful for discovering structure in larger language models remains an open question."
- **Why unresolved:** Computational costs of susceptibility estimation scale with model size, and it is unclear whether the clear stratification patterns persist in higher-dimensional spaces.
- **What evidence would resolve it:** Applying the same methodology to transformers with more layers, MLP components, and larger parameter counts; observing whether analogous "body plan" structures emerge.

### Open Question 3
- **Question:** What universal principles govern the consistent organizational structure observed across different random seeds?
- **Basis in paper:** [explicit] "It is an interesting question for the science of deep learning whether the universal principles of organization behind such empirical observations can be clarified."
- **Why unresolved:** While the authors observe remarkable consistency in UMAP structure across four seeds, the theoretical basis for why architecture and data distribution dictate this organization remains unclear.
- **What evidence would resolve it:** Systematic variation of architecture (layers, heads) and data distribution; theoretical analysis connecting inductive biases to susceptibility structure.

### Open Question 4
- **Question:** Are qualitative changes in susceptibility patterns during training associated with specific geometric changes in the loss landscape, as Thom's morphogenesis framework would predict?
- **Basis in paper:** [explicit] "We conjecture that the qualitative changes in the pattern of susceptibilities... are associated with specific changes in this geometry."
- **Why unresolved:** The connection between susceptibility dynamics and level set geometry of the loss function is hypothesized but not empirically validated.
- **What evidence would resolve it:** Computing local learning coefficient trajectories and analyzing loss landscape geometry at developmental stage boundaries; correlating with observed UMAP structural transitions.

## Limitations
- Scale: 3M parameter, two-layer attention-only transformer; scalability to larger models unknown
- Spacing fin: Tokenizer-dependent structure whose prominence varies with tokenization scheme
- Theoretical foundation: Limited external validation of specific empirical findings beyond theoretical groundwork

## Confidence

| Claim | Confidence |
|-------|------------|
| Susceptibility-based UMAP reveals developmental structures | High |
| Induction circuit emergence detectable via variance shifts | Medium |
| Spacing fin is a real computational structure | Medium |
| These patterns generalize to larger models | Low |

## Next Checks
1. Verify PC1 explains >95% variance and PC2 increases from ~2% to ~6% during LM3-LM4 transition
2. Confirm spacing tokens constitute appropriate proportion of dataset (check Figure 6)
3. Test UMAP robustness across n_neighbors range (15-1000) to ensure spacing fin is not an artifact