---
ver: rpa2
title: Is CLIP ideal? No. Can we fix it? Yes!
arxiv_id: '2503.08723'
source_url: https://arxiv.org/abs/2503.08723
tags:
- clip
- image
- text
- which
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies fundamental geometric limitations in CLIP\u2019\
  s latent space that prevent it from accurately representing complex visual-textual\
  \ interactions like attribute binding, spatial relationships, and negation. The\
  \ authors prove that no CLIP-like space can satisfy all necessary conditions for\
  \ ideal multimodal understanding simultaneously."
---

# Is CLIP ideal? No. Can we fix it? Yes!

## Quick Facts
- arXiv ID: 2503.08723
- Source URL: https://arxiv.org/abs/2503.08723
- Authors: Raphi Kang; Yue Song; Georgia Gkioxari; Pietro Perona
- Reference count: 40
- Primary result: Proves CLIP's joint embedding space cannot simultaneously handle content representation, attribute binding, spatial relationships, and negation, then proposes DCSMs to fix this limitation.

## Executive Summary
This paper identifies fundamental geometric limitations in CLIP's latent space that prevent it from accurately representing complex visual-textual interactions like attribute binding, spatial relationships, and negation. The authors prove that no CLIP-like space can satisfy all necessary conditions for ideal multimodal understanding simultaneously. To address this, they propose Dense Cosine Similarity Maps (DCSMs), which retain full token and patch embeddings and use a learned CNN to score image-text matches, improving performance on multiple benchmarks over state-of-the-art joint encoder models.

## Method Summary
The proposed method replaces CLIP's global embedding with a Dense Cosine Similarity Map (DCSM) that preserves the full topology of token-patch relationships. The approach computes a similarity matrix between all text tokens and image patches, then processes this map with a lightweight CNN to produce a match score. To handle functional words (spatial relations, negations), the method replaces these with fixed random vectors that act as synthetic labels, allowing the CNN to learn structural logic independent of CLIP's weak text embeddings for non-visual words.

## Key Results
- Proves CLIP's joint embedding space cannot simultaneously satisfy four fundamental conditions for multimodal understanding
- Achieves 62.6% accuracy on the WhatsUp spatial benchmark (vs 48.6% without Functional Rows)
- Improves attribute binding accuracy across multiple datasets (CLEVR-bind, NCD, VG-attribution)
- Demonstrates effectiveness on negation tasks (NegBench-coco, NB-voc)

## Why This Works (Mechanism)

### Mechanism 1
Standard cosine similarity on unit hypersphere embeddings forces composite concepts into linear superpositions, causing inevitable "binding collisions" where distinct scenes map to identical vectors. The paper demonstrates that to satisfy basic categorization, a composite image embedding must reside on the geodesic arc between its constituent concepts. Mathematically, this results in $i(x_a, y_b) \approx i(x_b, y_a)$; the model cannot distinguish "red car, blue ball" from "blue car, red ball" because the vector sum is order-invariant.

### Mechanism 2
Preserving the dense topology of token-patch similarities allows a lightweight scorer to recover spatial and binding information lost in global vector compression. Instead of collapsing image and text into single vectors, the DCSM retains the $N \times M$ matrix of similarities between all patches and tokens. This matrix preserves the "where" and "what" correlation, treating semantic alignment as a pattern recognition task rather than a distance metric.

### Mechanism 3
Replacing "functional words" (spatial relations, negations) with fixed random vectors acts as a synthetic label, allowing the scorer to learn structural logic independent of CLIP's weak text embeddings for non-visual words. Words like "left of" or "no" often lack strong visual correlates, resulting in noisy or uninformative CLIP text embeddings. By stripping these rows from the DCSM and replacing them with constant random vectors, the model forces the downstream CNN to learn a specific association between that unique vector and the spatial/negation pattern observed in the image patches.

## Foundational Learning

- **Unit Hypersphere Geometry & Linear Superposition**: The paper's core mathematical proof relies on the constraints of vector addition on a sphere. Understanding why $A + B \approx B + A$ on a sphere explains why CLIP cannot bind attributes.
  - Quick check: If you normalize the sum of two orthogonal vectors, does the resulting vector encode the *order* in which they were added?

- **Transformer "Register" Tokens vs. Dense Patches**: The paper hypothesizes that background patches store global info (like register tokens) while salient patches store local info. DCSM leverages this distinction.
  - Quick check: Why would a background patch in a ViT have a higher similarity to a text prompt than the patch containing the object described?

- **Contrastive Learning Objectives (Global vs. Local)**: To understand why standard CLIP training optimizes for global alignment at the expense of local binding fidelity.
  - Quick check: Does a standard contrastive loss penalize a model if it matches "red car" to an image of a "blue car and red hat"? (Hint: Does the global embedding match?)

## Architecture Onboarding

- **Component map**: Frozen CLIP Encoders (ViT-B/16) -> DCSM Generator -> Lookup Table -> Row Injector -> Lightweight CNN
- **Critical path**: The transformation of the text prompt into the DCSM via the Functional Row lookup is the most fragile step; if the tokenizer strips or splits a functional word, the FR injection fails, and the model defaults to standard noisy CLIP features.
- **Design tradeoffs**: Efficiency vs. Precision (DCSM requires storing/computing an $N \times M$ matrix per pair vs. standard CLIP's $O(1)$ dot product); Generalization (CNN might overfit to synthetic templates).
- **Failure signatures**: Identical Scores (if CNN outputs near-identical scores for "red circle blue square" vs "blue circle red square"); Negation Ignored (if "dog" and "no dog" produce similar scores).
- **First 3 experiments**:
  1. Sanity Check (Binding): Generate synthetic images of colored shapes. Plot the heatmap of the DCSM. Verify that the "Red" token row shows high activation specifically on the patch containing the red object.
  2. Ablation (Functional Rows): Train two modelsâ€”one with Functional Rows, one without standard CLIP embeddings for functional words. Evaluate on the *WhatsUp* spatial benchmark.
  3. Generalization Test: Train the CNN only on synthetic data (Objaverse) and test immediately on COCO validation pairs to ensure geometric reasoning transfers.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the DCSM approach be scaled effectively to more sophisticated base models and larger training datasets without diminishing returns? (Basis: Conclusion mentions scaling up pipeline with more sophisticated models and training data)
- **Open Question 2**: How can the DCSM framework improve its handling of natural language to overcome the bottleneck of long-winded prompts and complex syntax? (Basis: Notes long-winded prompts are still a bottleneck for the lightweight CNN model)
- **Open Question 3**: What other fundamental geometric restrictions exist in current Vision-Language Model (VLM) architectures beyond attribute binding, spatial relationships, and negation? (Basis: Authors conclude that further analyses to discover other restrictions will help guide future design choices)
- **Open Question 4**: Does the DCSM method introduce a trade-off that degrades performance on general, large-scale tasks despite gains in compositional reasoning? (Basis: Paper notes maintaining classification accuracy but Table 5 shows decrease in classification accuracy)

## Limitations
- Theoretical proof assumes simple linear aggregation of semantic components, which may not hold with complex attention mechanisms
- Effectiveness of Functional Rows depends on assumption that functional words produce unreliable CLIP embeddings without quantifying baseline degradation
- Limited ablation studies comparing Functional Rows with alternative encoding strategies for functional words

## Confidence
- **High Confidence**: Experimental results showing DCSM's superior performance on attribute binding, spatial reasoning, and negation benchmarks are well-supported with specific accuracy improvements over baselines.
- **Medium Confidence**: Theoretical proof of CLIP's fundamental geometric limitations is mathematically sound but relies on idealized assumptions about vector superposition that may not fully capture real CLIP behavior.
- **Low Confidence**: Claim that Functional Rows with random vectors are optimal solution for handling spatial and negation terms lacks extensive ablation studies comparing different encoding strategies.

## Next Checks
1. Quantify baseline degradation: Measure CLIP's exact performance drop on functional words by comparing models that include vs. exclude these terms in their embeddings.
2. Test alternative functional word encodings: Compare Functional Rows with other approaches such as learned embeddings for functional words, positional encodings, or prompt engineering.
3. Stress test generalization: Evaluate DCSM performance on out-of-distribution spatial and negation tasks not seen during training to assess whether the model learns genuine geometric reasoning or overfits to training templates.