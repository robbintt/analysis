---
ver: rpa2
title: 'Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic
  Understanding'
arxiv_id: '2509.19323'
source_url: https://arxiv.org/abs/2509.19323
tags:
- similarity
- metrics
- cosine
- semantic
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes and evaluates two new magnitude-aware similarity\
  \ metrics\u2014Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS)\u2014\
  as alternatives to cosine similarity for comparing sentence embeddings. These metrics\
  \ integrate vector magnitude and alignment more effectively than standard methods,\
  \ addressing limitations in anisotropic embedding spaces."
---

# Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding

## Quick Facts
- arXiv ID: 2509.19323
- Source URL: https://arxiv.org/abs/2509.19323
- Authors: V. S. Raghu Parupudi
- Reference count: 10
- Primary result: Two new magnitude-aware similarity metrics (OS, HTS) outperform cosine similarity on classification tasks with statistically significant MSE improvements

## Executive Summary
This paper introduces two novel similarity metrics—Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS)—designed to address limitations of cosine similarity in anisotropic embedding spaces. By integrating vector magnitude and alignment more effectively, these metrics significantly improve performance on classification-based NLP tasks like paraphrase detection and natural language inference. However, on fine-grained semantic textual similarity tasks, cosine similarity remains competitive, highlighting the task-dependent nature of metric performance.

## Method Summary
The study post-hoc evaluates four similarity metrics (Dot Product, Cosine, OS, HTS) on pre-trained sentence embeddings from four BERT-style models across eight NLP benchmarks. No fine-tuning occurs; embeddings are generated once per model-dataset pair. The proposed metrics—OS and HTS—incorporate vector magnitude while maintaining bounded outputs, aiming to better capture semantic relationships in anisotropic embedding spaces. Performance is measured via Mean Squared Error and Spearman correlation against normalized ground truth, with statistical significance assessed using Wilcoxon signed-rank tests.

## Key Results
- OS and HTS significantly outperformed cosine similarity on paraphrase and inference tasks with statistically significant MSE improvements
- On fine-grained semantic textual similarity tasks (SICK, STS-B), cosine similarity remained competitive
- Dot product showed extreme MSE values on certain models (e.g., paraphrase-mpnet-base-v2), demonstrating the robustness of OS and HTS
- Identical Spearman scores across all metrics when embeddings have uniform L2 norms

## Why This Works (Mechanism)

### Mechanism 1: Relational Normalization (Overlap Similarity)
OS normalizes by a term dependent on the relationship between vectors rather than their independent properties. The denominator `||x||² + ||y||² - |x·y| + ε` applies an inclusion-exclusion principle, measuring the "union" of vector energies. This makes normalization sensitive to pairwise alignment, potentially compensating for reduced angular discrimination in anisotropic spaces where all vectors point in similar directions.

### Mechanism 2: Non-Linear Saturation (Hyperbolic Tangent Similarity)
HTS applies tanh to a normalized dot product, bounding similarity to [-1, 1] while amplifying mid-range differences and compressing extremes. This S-shaped transformation may better reflect human similarity perception and separate decision boundaries in classification tasks, being less sensitive to outlier dimensions while maintaining bounded outputs.

### Mechanism 3: Magnitude as Semantic Signal
The paper posits that vector magnitude encodes task-relevant information (semantic specificity, sentence complexity) that cosine discards. OS and HTS retain magnitude information while maintaining bounded outputs, creating a middle ground between dot product's magnitude sensitivity and cosine's magnitude agnosticism. This appears beneficial for classification but potentially noisy for fine-grained semantic comparisons.

## Foundational Learning

- **Anisotropy in Embedding Spaces**
  - Why needed: Modern PLM embeddings occupy narrow cones, reducing angular discrimination and making cosine similarity less effective
  - Quick check: If all embeddings lie within a 10° cone, why would cosine similarity have reduced discriminative power?

- **L2 Normalization Trade-offs**
  - Why needed: Understanding why cosine discards magnitude and when that's harmful vs. beneficial is central to interpreting results
  - Quick check: What semantic information might be lost when normalizing all vectors to unit length?

- **Statistical Significance Testing (Wilcoxon Signed-Rank)**
  - Why needed: The paper's claims hinge on statistically significant improvements; understanding the test prevents over-interpreting noise
  - Quick check: Why use a non-parametric test (Wilcoxon) rather than a paired t-test for comparing MSE distributions?

## Architecture Onboarding

- **Component map:** Input sentence pair → Pre-trained encoder → Embeddings (x, y) → Four similarity metrics (Dot Product, Cosine, OS, HTS) → MSE and Spearman evaluation → Statistical significance testing

- **Critical path:** Embedding generation (once per model-dataset) → metric computation (four formulas, deterministic) → MSE and Spearman evaluation → Wilcoxon test + bootstrap CI

- **Design tradeoffs:**
  - OS: More stable for anisotropic spaces; assumes magnitude is signal. Can degrade when magnitude is noise
  - HTS: Bounded, non-linear; assumes mid-range discrimination matters. Same break condition as OS
  - Cosine: Safe default for fine-grained STS; discards potentially useful magnitude
  - Dot Product: Fast; unbounded and highly sensitive to norm variance (e.g., paraphrase-mpnet-base-v2 produced extreme MSE values)

- **Failure signatures:**
  - Identical Spearman ρ across all metrics → uniform norms, no ranking benefit from magnitude-aware metrics
  - OS/HTS MSE > Cosine on STS tasks → magnitude carrying noise for nuanced semantics
  - Dot Product MSE orders of magnitude higher → embedding model produces large/unstable norms (mitigate by normalizing embeddings first)

- **First 3 experiments:**
  1. **Sanity check:** On a single dataset (e.g., Quora), compute all four metrics using a pre-trained encoder. Verify OS and HTS produce lower MSE than cosine; confirm Spearman scores are identical
  2. **Ablation by norm variance:** Compute L2 norm statistics per dataset. Test whether datasets with higher norm variance show larger OS/HTS improvements
  3. **Cross-model generalization:** Replicate the paper's finding that paraphrase-mpnet-base-v2 yields extreme dot product MSE but stable OS/HTS, confirming the metrics' robustness to norm scaling

## Open Questions the Paper Calls Out

### Open Question 1
Can a formal measurement of embedding space anisotropy be used to predict which similarity metric (OS, HTS, or Cosine) will perform best for a specific model? The authors suggest this warrants deeper investigation but did not quantify anisotropy levels or correlate them with relative performance gains.

### Open Question 2
Do Overlap Similarity and HTS provide similar performance benefits when applied to embeddings generated by Large Language Models (LLMs) or non-English architectures? The empirical evaluation was restricted to four specific BERT-style sentence-transformer models.

### Open Question 3
Does vector magnitude encode noise or signal in the context of fine-grained compositional semantics? The authors observe their metrics failed to improve upon cosine similarity on STS tasks and hypothesize magnitude may represent noise, but this claim is not empirically validated.

## Limitations
- The boundary between magnitude as signal vs. noise is not rigorously defined, particularly for nuanced semantic tasks
- HTS's benefits may be nullified when embeddings have uniform norms, reducing it to a monotonic transform
- The paper's claims about magnitude being "noise" for STS tasks are inferred from negative results rather than tested through ablation studies

## Confidence
- **High confidence**: OS and HTS reduce MSE vs. cosine on classification tasks (paraphrase, NLI) across multiple models with statistical significance
- **Medium confidence**: The proposed mechanisms (relational normalization, tanh saturation) are plausible but lack direct empirical validation
- **Low confidence**: The claim that magnitude is "almost entirely noise" for STS tasks is asserted but not empirically verified

## Next Checks
1. **Magnitude-signal ablation**: Train a simple classifier to predict task labels from embedding norms alone on each dataset to validate whether norms carry signal for classification but not STS tasks
2. **Cross-dataset norm variance correlation**: Quantify L2 norm variance per dataset and correlate with the magnitude of OS/HTS improvements over cosine
3. **HTS saturation edge cases**: Generate synthetic embeddings with uniform norms and verify HTS produces identical rankings to cosine, then test whether HTS still offers benefits on datasets where norms are approximately uniform