---
ver: rpa2
title: Mixed Precision Training of Neural ODEs
arxiv_id: '2510.23498'
source_url: https://arxiv.org/abs/2510.23498
tags:
- precision
- training
- neural
- scaling
- mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a mixed-precision training framework for neural
  ordinary differential equations (Neural ODEs), addressing the computational and
  memory challenges in training continuous-time architectures. The key innovation
  combines explicit ODE solvers with custom backpropagation and dynamic adjoint scaling,
  using low-precision computations for network evaluations and storing intermediate
  states while maintaining stability through higher-precision accumulation and gradient
  computations.
---

# Mixed Precision Training of Neural ODEs

## Quick Facts
- **arXiv ID**: 2510.23498
- **Source URL**: https://arxiv.org/abs/2510.23498
- **Reference count**: 36
- **Primary result**: Mixed precision training achieves ~50% memory reduction and up to 2× speedup for Neural ODEs while maintaining accuracy.

## Executive Summary
This paper develops a mixed-precision training framework for neural ordinary differential equations (Neural ODEs) that addresses computational and memory challenges in continuous-time architectures. The key innovation combines explicit ODE solvers with custom backpropagation and dynamic adjoint scaling, using low-precision computations for network evaluations while maintaining stability through higher-precision accumulation and gradient computations. The framework achieves approximately 50% memory reduction and up to 2× speedup compared to single-precision training while maintaining comparable accuracy across three learning tasks.

## Method Summary
The framework uses discretization-then-optimization with fixed-step explicit solvers (RK4 or Euler) combined with custom backpropagation that stores intermediate states in low precision (float16/bfloat16) while accumulating gradients in float32. Dynamic adjoint scaling prevents underflow in the adjoint computation, which is critical for stable training. The authors provide theoretical analysis proving that relative errors remain at the order of low-precision unit roundoff and do not grow uncontrollably with time steps. The approach requires manual handling of operations blacklisted in PyTorch's autocast (like softmax and norms) by casting around them to maintain stability.

## Key Results
- Achieves ~50% memory reduction and up to 2× speedup compared to single-precision training
- Maintains comparable accuracy across three tasks: CNF on 2D datasets, OT-Flow on 63D data, and STL-10 image classification
- Dynamic adjoint scaling prevents underflow and maintains gradient stability in low-precision training
- Theoretical analysis proves bounded error accumulation at low-precision unit roundoff order

## Why This Works (Mechanism)
Mixed precision training works by leveraging the fact that neural ODE evaluations are computationally intensive but don't require full precision for stability, while gradient accumulation and adjoint computations need higher precision to avoid underflow. By storing states in low precision and using dynamic scaling for the adjoint computation, the framework maintains numerical stability while reducing memory bandwidth requirements. The discretization-then-optimization approach allows explicit control over precision at each computational stage, unlike continuous adjoint methods.

## Foundational Learning
- **Neural ODEs**: Continuous-depth networks where hidden states evolve according to ODEs, enabling adaptive computation and continuous-time modeling. Needed because they provide the foundation for the memory-efficient continuous-time architectures being optimized.
- **Mixed Precision Training**: Using different numerical precisions for different parts of the computation to balance speed, memory, and accuracy. Critical for reducing memory bandwidth while maintaining numerical stability.
- **Adjoint Method**: Efficient backpropagation technique for ODEs that computes gradients by solving a backward-in-time ODE, avoiding storage of intermediate states. Essential for training Neural ODEs without prohibitive memory costs.
- **Dynamic Scaling**: Adaptive scaling of numerical values during computation to prevent underflow/overflow in low-precision arithmetic. Necessary for maintaining gradient stability in float16/bfloat16 computations.
- **Discretize-then-Optimize**: Optimization approach where the ODE is first discretized, then the resulting discrete optimization problem is solved. Provides explicit control over numerical precision at each stage.

## Architecture Onboarding

**Component Map**: Network Evaluation -> Low Precision (float16/bfloat16) -> Dynamic Adjoint Scaling -> High Precision Accumulation (float32) -> Gradient Computation

**Critical Path**: The forward pass uses low-precision ODE solver evaluations, storing states in low precision. The backward pass uses dynamic adjoint scaling to prevent underflow, with gradient accumulation in high precision. Operations blacklisted in autocast (softmax, norms) require manual precision casting.

**Design Tradeoffs**: Uses fixed-step explicit solvers for simplicity and theoretical tractability, sacrificing the adaptive step control of continuous adjoint methods. Manual handling of low-precision operations adds implementation complexity but provides better control than automatic mixed precision.

**Failure Signatures**: Without dynamic scaling, float16 gradients underflow and diverge to 1.0 relative error. Blacklisted operations computed in low precision cause numerical instability. Memory savings may be reduced if states aren't properly stored in low precision.

**First Experiments**:
1. Install `rampde` and run minimal example replacing `torchdiffeq.odeint` with `rampde.odeint` in autocast context
2. Reproduce CNF on 2-spirals using RK4 with 128 steps, Adam optimizer, compare validation NLL (~2.67) and visualize samples
3. Test float16 with dynamic adjoint scaling vs no scaling to verify scaling prevents underflow (gradients should remain stable)

## Open Questions the Paper Calls Out

**Open Question 1**: Can the framework be extended to 8-bit quantization while maintaining training stability and comparable accuracy? The theoretical analysis indicates larger unit roundoff in 8-bit formats may violate error bounds that ensure stability.

**Open Question 2**: How can mixed precision training be adapted for neural stochastic differential equations (Neural SDEs) used in score-based generative models? The stochastic nature introduces additional numerical challenges beyond deterministic ODEs.

**Open Question 3**: Can mixed precision training be made compatible with optimize-then-discretize approaches using adaptive time integrators? Adaptive step size selection requires accurate error estimation that may be compromised by low-precision computations.

**Open Question 4**: Can implicit ODE solvers benefit from mixed precision training for neural ODEs with exploitable structure? Mixed precision methods exist for scientific computing implicit solvers but require custom backpropagation schemes not yet developed for the discretize-then-optimize paradigm.

## Limitations
- Theoretical analysis assumes ideal conditions and may not fully capture real-world numerical instability
- Implementation requires careful handling of low-precision operations, particularly around blacklisted operations like softmax and norms
- Relies on discretization-then-optimization with fixed-step explicit solvers, introducing theoretical gaps compared to continuous-time adjoint methods
- Manual intervention needed to ensure operations maintain sufficient precision, adding implementation complexity

## Confidence
- **High confidence**: Memory reduction claims (~50%) and runtime speedup (up to 2×) for fixed-step explicit solvers with mixed precision
- **Medium confidence**: Accuracy preservation across all three learning tasks, as exact hyperparameter details and initialization schemes are not fully specified
- **Medium confidence**: Theoretical stability analysis, as it assumes ideal conditions that may not fully capture practical numerical issues

## Next Checks
1. Verify dynamic adjoint scaling prevents underflow by comparing float16 training with and without scaling - gradients should remain stable with scaling but diverge without it
2. Test memory usage explicitly by measuring peak GPU memory with `torch.cuda.max_memory_allocated()` during CNF training on 2D datasets, comparing rampde vs torchdiffeq implementations
3. Reproduce the STL-10 classification task end-to-end, checking both accuracy (should match paper's ~80% final) and cross-entropy loss trajectories to verify stable training dynamics