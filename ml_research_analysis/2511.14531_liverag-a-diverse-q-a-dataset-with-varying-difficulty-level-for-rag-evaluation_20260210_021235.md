---
ver: rpa2
title: 'LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation'
arxiv_id: '2511.14531'
source_url: https://arxiv.org/abs/2511.14531
tags:
- question
- questions
- difficulty
- document
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiveRAG benchmark introduces a publicly available dataset of
  895 synthetic question-answer pairs for evaluating Retrieval-Augmented Generation
  (RAG) systems. Generated using the DataMorgana tool, questions cover diverse topics
  and varying difficulty levels, with each associated with difficulty and discriminability
  scores derived from an Item Response Theory (IRT) model trained on system performance
  data.
---

# LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation

## Quick Facts
- arXiv ID: 2511.14531
- Source URL: https://arxiv.org/abs/2511.14531
- Reference count: 40
- Introduces 895-question synthetic dataset for RAG evaluation with IRT-based difficulty scoring

## Executive Summary
LiveRAG introduces a publicly available dataset of 895 synthetic question-answer pairs designed for comprehensive evaluation of Retrieval-Augmented Generation (RAG) systems. The dataset covers diverse topics and varying difficulty levels, with each question associated with difficulty and discriminability scores derived from an Item Response Theory (IRT) model trained on system performance data. This benchmark includes ground-truth answers, supporting documents, and answer claims used for evaluation.

The dataset was generated using the DataMorgana tool to ensure diversity in query characteristics, with questions spanning simple single-document queries to complex multi-document reasoning tasks. Analysis shows the questions cover a wide difficulty range, with multi-document questions being notably harder than single-document ones. The IRT-based difficulty scores demonstrate strong correlation with system performance and generalize across different large language models (LLMs), making LiveRAG a robust tool for systematic RAG research and evaluation.

## Method Summary
The LiveRAG benchmark was created using the DataMorgana tool to generate synthetic questions from Wikipedia documents. Questions were generated in batches of 100 per topic, with topics selected to cover diverse domains including historical events, scientific concepts, and biographical information. Each question was associated with difficulty and discriminability scores computed using an IRT model trained on the performance of five baseline RAG systems. The evaluation methodology uses answer claims - structured representations of ground-truth answers - which are compared against system-generated responses using precision and recall metrics. The dataset includes both single-document and multi-document questions, with multi-document questions specifically designed to require information from multiple sources to answer correctly.

## Key Results
- Dataset contains 895 synthetic question-answer pairs spanning diverse topics and difficulty levels
- Multi-document questions show significantly higher difficulty than single-document questions (IRT difficulty scores 0.03 vs 0.33)
- IRT-based difficulty scores correlate strongly with system performance (Pearson correlation 0.82-0.91 across different LLMs)
- Linguistic diversity of LiveRAG exceeds that of popular QA benchmarks like NQ and TriviaQA

## Why This Works (Mechanism)
The LiveRAG benchmark works by providing a systematically varied and calibrated set of evaluation questions that can distinguish between different RAG system capabilities. The IRT-based scoring mechanism creates a psychometric model of question difficulty that correlates with actual system performance, allowing researchers to identify which aspects of RAG systems are most challenging. The synthetic generation process using DataMorgana ensures controlled diversity in query characteristics while maintaining realistic complexity levels that mirror real-world information needs.

## Foundational Learning

1. Item Response Theory (IRT) - Why needed: Provides a statistical framework to model the relationship between question difficulty and system performance, enabling difficulty calibration across different systems.
   Quick check: Verify that IRT scores correlate with actual system accuracy and generalize across different LLM architectures.

2. Retrieval-Augmented Generation (RAG) - Why needed: Understanding the two-stage process of retrieval followed by generation is crucial for interpreting system performance differences on various question types.
   Quick check: Confirm that retrieval quality (e.g., recall@5) correlates with generation quality on the same questions.

3. Answer Claims - Why needed: Structured representations of ground-truth answers enable consistent and reproducible evaluation across different system outputs.
   Quick check: Ensure that answer claims capture all relevant information needed to fully answer each question.

4. Question Difficulty Calibration - Why needed: Systematic difficulty variation allows for fine-grained evaluation of system capabilities and identification of performance bottlenecks.
   Quick check: Validate that difficulty scores predict relative system performance rankings.

5. Multi-document Reasoning - Why needed: Complex questions requiring information from multiple sources test the integration capabilities of RAG systems beyond simple retrieval.
   Quick check: Verify that multi-document questions indeed require information synthesis across sources.

## Architecture Onboarding

Component Map: Question Generation -> IRT Model Training -> Difficulty Scoring -> System Evaluation -> Performance Analysis

Critical Path: Synthetic questions are generated → Baseline systems evaluate questions → IRT model learns difficulty parameters → Questions are scored and categorized → Researchers use scored questions to evaluate new systems.

Design Tradeoffs: Synthetic vs. real questions (controlled diversity vs. ecological validity), single vs. multi-document questions (simplicity vs. complexity), Wikipedia domain vs. broader domains (clean data vs. generalizability).

Failure Signatures: Low discriminability scores indicate questions that don't differentiate between systems; high difficulty with low discriminability suggests questions that are too hard for all systems; poor correlation between IRT scores and actual performance indicates model misspecification.

First Experiments:
1. Evaluate baseline system performance on the dataset to verify the IRT model's calibration.
2. Test correlation between IRT difficulty scores and system accuracy across different LLMs.
3. Compare linguistic diversity metrics of LiveRAG against established QA benchmarks.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Synthetic nature of questions may not fully capture real-world query characteristics and information needs
- Focus on Wikipedia as knowledge source limits generalizability to domains with different document characteristics
- Evaluation methodology may not fully capture nuanced aspects of RAG quality such as hallucination detection

## Confidence
- High: Dataset diversity and difficulty calibration supported by quantitative analyses and cross-LLM validation
- Medium: Benchmark utility for real-world RAG evaluation given synthetic generation process and limited knowledge domain
- High: IRT-based difficulty scores' predictive power within tested system space
- Medium: IRT scores' generalization beyond the initial five baseline systems

## Next Checks
1. Test the dataset's discriminative power across a broader range of RAG systems, including those with different architectures and training approaches, to validate generalizability beyond the initial five systems.

2. Conduct user studies with human evaluators to assess whether the synthetic questions capture the complexity and diversity of real-world information needs compared to human-generated queries.

3. Evaluate system performance on the LiveRAG dataset against performance on real user query logs from deployed RAG systems to measure ecological validity and practical relevance.