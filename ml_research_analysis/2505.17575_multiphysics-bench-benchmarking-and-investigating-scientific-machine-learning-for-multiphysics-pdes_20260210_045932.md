---
ver: rpa2
title: 'Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning
  for Multiphysics PDEs'
arxiv_id: '2505.17575'
source_url: https://arxiv.org/abs/2505.17575
tags:
- multiphysics
- coupling
- physical
- field
- fluid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Multiphysics Bench, the first benchmark dataset
  for multiphysics partial differential equations (PDEs), featuring six representative
  coupled problems across electromagnetics, heat transfer, fluid flow, solid mechanics,
  acoustics, and mass transport. The dataset includes 10,000+ samples per problem
  with diverse coupling mechanisms and boundary conditions.
---

# Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs

## Quick Facts
- arXiv ID: 2505.17575
- Source URL: https://arxiv.org/abs/2505.17575
- Reference count: 40
- Key outcome: Introduces first benchmark dataset for multiphysics PDEs with 6 coupled problems, revealing that naively applying single-physics SciML methods yields suboptimal performance and that complete multiphysics priors significantly improve accuracy

## Executive Summary
This paper introduces Multiphysics Bench, the first comprehensive benchmark dataset for multiphysics partial differential equations, featuring six representative coupled problems spanning electromagnetics, heat transfer, fluid flow, solid mechanics, acoustics, and mass transport. The dataset includes over 10,000 samples per problem with diverse coupling mechanisms and boundary conditions. Through systematic evaluation of four representative scientific machine learning models (PINNs, DeepONet, FNO, and DiffusionPDE), the study reveals that current general-purpose approaches struggle with multiphysics problems, particularly in handling bidirectional coupling and transient dynamics. The research demonstrates that incorporating complete multiphysics priors significantly improves prediction accuracy compared to incomplete supervision and identifies several practical tricks for enhancing model robustness in coupled systems.

## Method Summary
The study benchmarks four representative scientific machine learning architectures on six multiphysics PDE problems: PINNs (physics-informed neural networks with residual blocks and PDE:data loss ratio of 1:1000), DeepONet (unstacked architecture with basis dimension p=256), FNO (Fourier neural operator with 12 Fourier modes), and DiffusionPDE (score matching with 10-step Euler inference). Each task uses 128×128 resolution fields with 10,000 training samples and 1,000 test samples (1,000/100 for Mass Transport-Fluid). The evaluation employs relative error as the primary metric alongside RMSE, max error, and frequency-domain band errors. Key implementation tricks include quantile normalization (95th percentile clipping) for long-tailed data and auto-balanced loss weighting using L2 norm scaling for multi-PDE residuals.

## Key Results
- Naive application of single-physics SciML methods (PINNs, DeepONet, FNO, DiffusionPDE) yields suboptimal performance on multiphysics problems
- Incorporating complete multiphysics priors significantly improves prediction accuracy compared to incomplete supervision
- Performance saturates around 10,000 samples across all architectures, revealing poor scaling behavior
- All methods struggle with transient multiphysics dynamics, particularly in the Mass Transport-Fluid coupling task
- FNO performs best on steady-state problems while DeepONet shows more consistent performance across tasks

## Why This Works (Mechanism)
The study works by creating a standardized evaluation framework that isolates the challenges of multiphysics coupling from other factors. By providing six diverse coupled PDE problems with consistent input/output formats and evaluation metrics, it enables fair comparison between different scientific machine learning approaches. The benchmark reveals that the core difficulty lies in representing bidirectional, nonlinear inter-field coupling rather than solving individual PDEs. The systematic ablation studies demonstrate that incomplete physical priors (training on single fields without coupling terms) lead to significant accuracy degradation, confirming that the coupling mechanism itself is the primary challenge rather than computational complexity or data requirements.

## Foundational Learning

- **Concept: Coupled Partial Differential Equations (PDEs)**
    - **Why needed here:** The entire paper is about solving systems of PDEs where the solution of one equation (e.g., heat) directly affects another (e.g., fluid flow) through coupled terms. This is the core problem definition.
    - **Quick check question:** Can you explain how the temperature field in the Thermo-Fluid Coupling scenario influences the velocity field?

- **Concept: Physics-Informed Neural Networks (PINNs)**
    - **Why needed here:** PINNs are one of the four baseline architectures evaluated. They learn by incorporating the PDE residual as a regularization term in the loss function, which is a key approach for SciML.
    - **Quick check question:** How does a PINN use the governing differential equation during its training process?

- **Concept: Neural Operators (e.g., FNO, DeepONet)**
    - **Why needed here:** These are the dominant data-driven architectures benchmarked. Unlike PINNs, they learn the mapping from initial/boundary conditions to the solution directly from data, representing a different paradigm in SciML.
    - **Quick check question:** What is the fundamental difference in what a DeepONet learns compared to a traditional finite element solver?

## Architecture Onboarding

- **Component Map:** Dataset -> 6 Multiphysics Scenarios (Electro-Thermal, Thermo-Fluid, Electro-Fluid, Magneto-Hydrodynamic, Acoustic–Structure, Mass Transport–Fluid) -> Four Baselines (PINNs, DeepONet, FNO, DiffusionPDE) -> Evaluation Framework (Relative Error, RMSE, Frequency-domain metrics)

- **Critical Path:**
    1. **Select a Multiphysics Scenario:** Start with a steady-state, bidirectional coupled problem (e.g., Electro-Thermal)
    2. **Implement a Baseline:** Choose DeepONet (often best performer in the paper) or FNO
    3. **Apply Key "Tricks":** Implement quantile normalization for data preprocessing and auto-balanced loss weighting if using a physics-informed approach
    4. **Evaluate:** Use the defined Relative Error metric across all output fields
    5. **Iterate:** Attempt improvements on failure modes (e.g., poor high-frequency feature preservation)

- **Design Tradeoffs:**
    - **Generalization vs. Specialization:** The paper evaluates general-purpose solvers. A tradeoff exists between using a general model (FNO, DeepONet) versus developing an architecture specialized for multiphysics coupling (a suggested future direction)
    - **Data Efficiency vs. Complexity:** The paper notes poor scaling laws; more data doesn't guarantee better performance. One must trade off dataset size investment against potential model capacity limitations
    - **Accuracy vs. Inference Speed:** DiffusionPDE can generate plausible distributions but is computationally expensive and has high pointwise errors. FNO is fast but can fail on steady-state problems

- **Failure Signatures:**
    - **PINNs:** Discontinuities near interfaces due to truncation errors from finite-order differential operators
    - **FNO:** Collapsed, non-diverse outputs, especially in steady-state scenarios with high spectral similarity across samples
    - **DiffusionPDE:** Substantial pointwise errors and failure on transient problems due to noise and error accumulation over diffusion steps
    - **General:** Performance saturation around 10,000 samples and inability to handle transient multiphysics dynamics

- **First 3 Experiments:**
    1. **Baseline Reproduction on Electro-Thermal:** Train a DeepONet or FNO on the Electro-Thermal dataset, using the paper's specified hyperparameters, to verify performance metrics and observe its handling of bidirectional coupling
    2. **Ablation on Physical Priors:** Using the Electro-Thermal setup, train a model using only the temperature field (incomplete priors) and compare its relative error on T against a model trained on the full coupled system. This validates the "complete physical priors" claim
    3. **Apply a "Bag of Tricks" Ablation:** Train a PINN or DiffusionPDE on the Mass Transport-Fluid (Elder) problem. Compare a baseline run against one using quantile normalization for input data to directly measure the impact of this specific stabilization technique

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can neural architectures be designed to explicitly represent and learn bidirectional, nonlinear inter-field coupling in multiphysics systems?
- **Basis in paper:** [explicit] The authors identify that "existing architectures inadequately represent cross-field interactions, restricting their ability to capture bidirectional and nonlinear couplings intrinsic to multiphysics systems" and call for "developing targeted improvements based on baseline limitations, including the incorporation of explicit cross-physics coupling mechanisms."
- **Why unresolved:** Current single-physics methods applied naively to multiphysics yield suboptimal performance; no architecture in the benchmark effectively captures strong inter-field coupling
- **What evidence would resolve it:** A model architecture with dedicated coupling modules that achieves significantly lower relative error on bidirectional coupling tasks (e.g., Thermo-Fluid, MHD) compared to baselines, with ablations showing the coupling mechanism's contribution

### Open Question 2
- **Question:** How can gradient inconsistencies arising from multiple coupled PDE loss terms be mitigated during multiphysics training?
- **Basis in paper:** [explicit] The authors report that "gradient inconsistencies arise in multiphysics training, as models like PINNs and DiffusionPDE generate conflicting gradient directions and magnitudes, leading to instability, field dominance shifts, and potential negative transfer."
- **Why unresolved:** Coupled PDE residuals often differ by orders of magnitude (e.g., up to 1:10⁻⁵ ratio in Thermo-Fluid), causing imbalanced gradients that hurt convergence
- **What evidence would resolve it:** A training strategy or loss weighting scheme that yields stable convergence curves across all coupled fields simultaneously, demonstrated on multiple multiphysics scenarios

### Open Question 3
- **Question:** Can multiphysics foundation models be developed that exhibit smooth scaling laws with respect to data, compute, and model size?
- **Basis in paper:** [explicit] The authors note that "prediction accuracy does not consistently and significantly improve with increasing data volume, revealing the absence of smooth scaling behavior" and highlight the need for "designing multiphysics foundation models with data, compute, and model size scaling laws."
- **Why unresolved:** Current SciML models saturate around 10,000 samples and show limited generalization across physical scales
- **What evidence would resolve it:** Empirical scaling curves showing continuous improvement in relative error as data/compute/model size increase, with predictable power-law relationships

### Open Question 4
- **Question:** Why do all evaluated methods degrade significantly on transient multiphysics systems compared to steady-state problems, and how can this be addressed?
- **Basis in paper:** [inferred] The authors observe that "all models exhibit degraded performance in transient Mass Transport–Fluid coupling tasks" and note "current network architectures appear insufficiently expressive for capturing time-dependent multiphysics dynamics."
- **Why unresolved:** Five steady-state tasks show acceptable performance, but the single transient task shows uniformly poor results across all methods
- **What evidence would resolve it:** A temporal modeling approach achieving comparable relative error on transient tasks as steady-state tasks, with analysis of what temporal features are currently being missed

## Limitations
- Only four general-purpose SciML architectures were evaluated; specialized multiphysics architectures could perform significantly better
- Poor scaling behavior observed (performance saturation around 10,000 samples) suggests fundamental limitations in current approaches
- No investigation of discretization schemes or mesh resolution impacts on model performance

## Confidence
- **High Confidence:** Dataset construction methodology, relative performance rankings of baselines, core finding that complete multiphysics priors improve accuracy
- **Medium Confidence:** Claims about poor scaling laws, observations about PINNs producing discontinuities near interfaces
- **Low Confidence:** Suggestion that specialized architectures would perform better (speculative without implementation)

## Next Checks
1. **Architecture Specialization Test:** Implement and evaluate a dedicated multiphysics architecture (such as the one referenced in the "When Network Architecture Meets Physics" paper) on the Electro-Thermal problem to empirically test whether specialized designs outperform the general-purpose baselines

2. **Data Scaling Experiment:** Conduct experiments training models on 50K and 100K samples for the Electro-Thermal problem to rigorously verify the claimed scaling limitations and identify the precise point of performance saturation

3. **Transient Problem Investigation:** Systematically analyze the failure modes of all four architectures on the transient Mass Transport-Fluid problem, examining whether the issues stem from temporal discretization, model capacity, or the coupling mechanism itself, and test whether incorporating recurrent structures improves performance