---
ver: rpa2
title: 'CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality'
arxiv_id: '2511.16191'
source_url: https://arxiv.org/abs/2511.16191
tags:
- causal
- graph
- rumor
- nodes
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CausalMamba integrates Mamba sequence modeling, GCN graph structure
  encoding, and differentiable causal discovery via NOTEARS to jointly detect rumors
  and uncover influence pathways in social media propagation chains. The model learns
  both content and structural dependencies while inferring causal graphs that identify
  influential nodes.
---

# CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality

## Quick Facts
- arXiv ID: 2511.16191
- Source URL: https://arxiv.org/abs/2511.16191
- Authors: Xiaotong Zhan; Xi Cheng
- Reference count: 3
- Primary result: 59.7% accuracy and 59.8% macro-F1 on Twitter15 rumor detection task

## Executive Summary
CausalMamba introduces a novel framework for detecting rumors and uncovering their propagation pathways in social media networks. The model integrates Mamba sequence modeling for temporal dynamics, GCN graph structure encoding for network topology, and differentiable causal discovery via NOTEARS to jointly identify rumors and infer influence pathways. Experiments demonstrate competitive performance against strong baselines while providing interpretable causal graphs that reveal influential nodes and potential intervention points in rumor cascades.

## Method Summary
The model combines three key components: Mamba sequence modeling captures temporal dependencies in user interactions and content evolution; GCN graph structure encoding learns topological features from the propagation network; and NOTEARS-based differentiable causal discovery jointly learns a causal graph while predicting rumor labels. This unified architecture enables simultaneous rumor detection and pathway interpretation, with learned causal graphs identifying influential nodes that can be targeted for intervention. The model processes both content and structural information to uncover how misinformation spreads through social networks.

## Key Results
- Achieved 59.7% accuracy and 59.8% macro-F1 on Twitter15 benchmark
- Competitive performance compared to BiLSTM-CNN and Transformer baselines
- Learned causal graphs enable intervention simulation by identifying influential nodes
- Removing top-ranked nodes disrupts graph connectivity and alters predicted outcomes

## Why This Works (Mechanism)
The model's effectiveness stems from integrating temporal modeling (Mamba) with graph structure (GCN) and causal inference (NOTEARS). Mamba captures sequential dependencies in how rumors evolve over time, while GCN encodes the network topology of information spread. The causal discovery component then learns directed relationships between nodes, revealing which users have the most influence in propagating misinformation. This multi-modal approach allows the model to understand both what is being shared and how it spreads through the network.

## Foundational Learning
- **Mamba sequence modeling**: Needed for capturing temporal dynamics in rumor propagation; quick check: verify attention-like behavior through selective state spaces
- **GCN graph encoding**: Essential for learning structural patterns in social networks; quick check: test graph pooling effectiveness on different network sizes
- **NOTEARS causal discovery**: Required for inferring influence pathways; quick check: validate acyclicity constraints on real-world networks
- **Differentiable causal learning**: Enables joint optimization of detection and pathway discovery; quick check: measure gradient flow between components
- **Intervention simulation**: Allows testing impact of node removal on network behavior; quick check: compare random vs. targeted removal strategies

## Architecture Onboarding

**Component Map**: Input Data -> Mamba Encoder -> GCN Encoder -> NOTEARS Causal Discovery -> Output Layer

**Critical Path**: The Mamba encoder processes temporal sequences first, followed by GCN encoding of graph structure, with NOTEARS discovering causal relationships. The causal graph then influences the final classification decision through attention mechanisms.

**Design Tradeoffs**: NOTEARS assumes linear relationships and acyclic graphs, which may not hold in complex social networks with feedback loops. This tradeoff enables differentiable learning but may limit causal accuracy.

**Failure Signatures**: Poor performance on networks with strong cyclical dependencies, reduced accuracy when temporal patterns are non-sequential, and unreliable causal graphs when network density is low.

**First Experiments**: 
1. Ablation study removing causal discovery to measure performance impact
2. Test on networks with known causal structures to validate graph accuracy
3. Compare Mamba vs. Transformer performance on same task

## Open Questions the Paper Calls Out
None

## Limitations
- NOTEARS assumptions of linearity and acyclicity may not hold in real social networks with feedback loops
- Performance is competitive but modest compared to strong baselines, with unclear contribution from causal component
- Intervention simulation shows theoretical potential but lacks real-world validation of effectiveness

## Confidence
- Causal graph interpretability: Medium
- Rumor detection accuracy: Medium  
- Intervention potential: Low

## Next Checks
1. Conduct ablation studies removing the causal discovery component to quantify its contribution to detection performance versus computational overhead
2. Validate causal graph assumptions by testing for feedback loops and non-linear relationships in real propagation networks using Granger causality or other non-linear methods
3. Design and execute a controlled intervention study using historical data to measure whether removing top-ranked nodes actually reduces rumor spread compared to random removal or no intervention