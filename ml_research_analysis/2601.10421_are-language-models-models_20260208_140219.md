---
ver: rpa2
title: Are Language Models Models?
arxiv_id: '2601.10421'
source_url: https://arxiv.org/abs/2601.10421
tags:
- language
- level
- computational
- cognitive
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critically evaluates the claim that large language models
  (LMs) can serve as cognitive model systems, analyzing them at each of Marr's three
  levels of analysis. At the implementation level, LMs lack biological plausibility
  and show little engagement with neurobiological advances.
---

# Are Language Models Models?

## Quick Facts
- arXiv ID: 2601.10421
- Source URL: https://arxiv.org/abs/2601.10421
- Authors: Philip Resnik
- Reference count: 0
- Key outcome: LMs cannot serve as cognitive model systems at any of Marr's three levels of analysis, though they can be valuable tools for operationalization.

## Executive Summary
This paper critically evaluates whether large language models can serve as cognitive model systems for human language processing. Through the lens of Marr's three levels of analysis (implementation, algorithmic/representational, and computational theory), the author argues that LMs fail as model systems at each level while potentially serving as valuable tools for operationalization. The paper distinguishes between LMs as model systems (requiring mechanistic correspondence) versus tools (useful for generating proxies without cognitive claims), arguing that overstating LMs as models feeds harmful hype about their cognitive plausibility.

## Method Summary
The paper employs theoretical argumentation and literature synthesis rather than empirical experimentation. It analyzes LMs-as-cognitive-models claims through Marr's framework, examining each level systematically. The method involves comparing LM characteristics against biological, computational, and theoretical requirements for cognitive modeling, using illustrative counterexamples from biology (sharks vs. dolphins) and computation (iterative vs. recursive Fibonacci) to challenge convergence arguments. No quantitative metrics or empirical data collection is employed.

## Key Results
- LMs lack implementation-level biological plausibility due to connectionist foundations abandoning biological correspondence
- Convergence arguments fail at algorithmic level because language decomposes into distinct subproblems with multiple valid computational solutions
- LMs can serve as valuable tools for operationalization without requiring cognitive plausibility claims

## Why This Works (Mechanism)

### Mechanism 1: Implementation-Level Biological Disconnect
LMs cannot serve as implementation-level cognitive models because connectionist approaches deliberately abandoned biological correspondence in favor of function optimization. Modern LM research shows little substantive engagement with neurobiological advances, lacking structured correspondences between model entities and neurobiological entities.

### Mechanism 2: Convergence Fails at Algorithmic-Representational Level
Functional similarity between LMs and humans does not imply mechanistic convergence. Language decomposes into distinct subproblems with multiple valid computational solutions. Biological counterexamples (sharks use electroreception; dolphins use echolocation) and computational ones (iterative vs. recursive Fibonacci) demonstrate that similar functions can arise from different mechanisms.

### Mechanism 3: Productive Tool Use Without Cognitive Plausibility Claims
LMs contribute to cognitive science as operationalization tools, not as model systems. They excel at generating plausible text continuations, enabling operationalizing constructs like surprisal and anticipation vs. responsivity, requiring only text generation capability without cognitive plausibility.

## Foundational Learning

- **Concept: Marr's Three Levels of Analysis**
  - Why needed here: The entire argument is structured around whether LMs qualify as models at the implementation (hardware), algorithmic/representational (processes and representations), and computational theory (what problem is being solved and why) levels.
  - Quick check question: Can you explain why a chess program and a human chess master might share a computational theory level while differing at the implementation level?

- **Concept: Model Systems vs. Tools in Science**
  - Why needed here: Resnik's core distinction is between "model systems" (with structured mechanistic correspondences) and "tools" (useful for operationalization without mechanistic claims)â€”confusing these feeds hype.
  - Quick check question: Why are mice selected as model organisms for cancer research rather than arbitrarily? What does this imply about selection criteria?

- **Concept: Convergence vs. Multiple Realizability**
  - Why needed here: The argument against algorithmic-level LMs-as-models hinges on whether solving the same problem implies similar mechanisms (convergence) or allows different mechanisms (multiple realizability).
  - Quick check question: If two systems achieve similar accuracy on language modeling, what additional evidence would you need to claim they use similar mechanisms?

## Architecture Onboarding

- **Component map:**
  Implementation level assessment -> Algorithmic/representational level assessment -> Computational theory level assessment -> Tool operationalization pathway

- **Critical path:**
  1. Define the cognitive construct to model (e.g., surprisal in sentence processing)
  2. Identify whether the goal requires mechanistic correspondence (model system) or functional approximation (tool)
  3. If tool: Validate proxy accuracy against human behavioral/neural data with explicit caveats
  4. If model system: Establish structured correspondences at target Marr level(s) before making cognitive claims

- **Design tradeoffs:**
  - Proxy validity vs. cognitive claim strength: Stronger cognitive claims require more mechanistic evidence; tool use requires less but offers weaker theoretical contribution
  - Generalization vs. biological fidelity: LMs generalize across tasks but lack biological constraints; brain-constrained models sacrifice some generalization for plausibility
  - Elegant unified theory vs. evolutionary tinkering: Computational-theory elegance trades off against capturing biological messiness

- **Failure signatures:**
  - Overclaiming cognitive plausibility without mechanistic evidence at any Marr level
  - Treating functional similarity as sufficient for model-system status
  - Ignoring that language decomposes into distinct subproblems with potentially different mechanisms
  - Extrapolating from "LMs do X well" to "LMs do X like humans"

- **First 3 experiments:**
  1. Corpus-based counterexample audit: Identify linguistic phenomena where LMs and humans achieve similar behavioral accuracy but show divergent error patterns
  2. Ablation-to-lesion mapping test: Compare effects of LM component ablations to human lesion/deficit patterns
  3. Tool-proxy validation pipeline: Quantify variance explained when using LM-derived vs. human-derived surprisal estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions (if any) should we expect genuine mechanistic convergence between LMs and human language processing?
- Basis in paper: The paper challenges the convergence argument by noting that language "decomposes into very distinct subproblems," and provides counterexamples from biology and computation.
- Why unresolved: The author critiques the "argument from amazingness" but does not specify what positive evidence would establish valid convergence claims for specific linguistic subproblems.
- What evidence would resolve it: Systematic comparative studies of LM and human processing mechanisms across distinct linguistic subproblems, with pre-registered criteria for mechanistic correspondence.

### Open Question 2
- Question: What methodological frameworks can properly leverage LMs as tools while avoiding overclaims about their cognitive plausibility?
- Basis in paper: The author provides examples of LMs used appropriately as tools but warns that "calling them cognitive models overstates the case and unnecessarily feeds LLM hype."
- Why unresolved: The paper critiques overclaiming without fully specifying best practices for appropriate tool use.
- What evidence would resolve it: Comparative studies demonstrating which LM-as-tool approaches yield genuine cognitive insights versus superficial correlations.

### Open Question 3
- Question: What criteria could establish LMs as valid "model systems" analogous to model organisms in biology?
- Basis in paper: The author notes that model-organism choices "are supported by existing, solidly established mechanistic evidence," implying LMs lack equivalent justification.
- Why unresolved: The gap is identified but no framework for establishing such criteria is proposed.
- What evidence would resolve it: Development of pre-established mechanistic criteria that LMs would need to satisfy for specific linguistic phenomena before being considered valid model systems.

## Limitations

- Conceptual analysis lacks empirical grounding at each Marr level
- Counterexamples from biology and computation are illustrative but not systematically tested against LM mechanisms
- The distinction between tools and model systems remains largely philosophical without quantitative validation

## Confidence

- **High confidence**: LMs lack implementation-level biological plausibility (supported by connectionist foundations explicitly abandoning biological correspondence)
- **Medium confidence**: Convergence arguments fail at algorithmic level (counterexamples are compelling but not systematically tested against LM mechanisms)
- **Medium confidence**: LMs serve as operationalization tools (supported by existing applications but not validated for proxy accuracy)

## Next Validation Checks

1. **Mechanism divergence audit**: Systematically compare LM and human error patterns on linguistically diverse phenomena to identify where functional similarity masks mechanistic differences
2. **Proxy accuracy validation**: Quantify how LM-derived surprisal estimates explain variance in human processing data versus human-derived surprisal estimates, with explicit error bounds
3. **Marr-level mapping**: Create structured criteria for evaluating whether LM components satisfy each Marr level's requirements for cognitive modeling, then apply to representative LM architectures