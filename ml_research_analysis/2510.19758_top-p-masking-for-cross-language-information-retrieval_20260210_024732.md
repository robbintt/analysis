---
ver: rpa2
title: Top-P Masking for Cross Language Information Retrieval
arxiv_id: '2510.19758'
source_url: https://arxiv.org/abs/2510.19758
tags:
- masking
- top-k
- terms
- language
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Cross Language Information Retrieval (CLIR)
  by proposing Top-P Dynamic Masking as an improvement over Top-K masking for generating
  sparse representations in multilingual embeddings. The core method involves using
  Top-P Dynamic Masking (similar to Nucleus Sampling) instead of Top-K masking during
  post-processing of BERT embeddings.
---

# Top-P Masking for Cross Language Information Retrieval

## Quick Facts
- arXiv ID: 2510.19758
- Source URL: https://arxiv.org/abs/2510.19758
- Authors: Joseph Casale; Andrew Silverschotz; Joseph DeSimone
- Reference count: 4
- Key outcome: Top-P Dynamic Masking consistently outperforms Top-K masking for sparse CLIR representations

## Executive Summary
This paper addresses Cross Language Information Retrieval (CLIR) by proposing Top-P Dynamic Masking as an improvement over Top-K masking for generating sparse representations in multilingual embeddings. The authors evaluate their method on the NeuCLIR dataset using BLADE-C architecture, comparing Top-K and Top-P masking across various sparsity levels. Results show that Top-P Dynamic Masking achieves similar or better Mean Average Precision (mAP) with improved query throughput compared to Top-K masking.

## Method Summary
The core method involves using Top-P Dynamic Masking (similar to Nucleus Sampling) instead of Top-K masking during post-processing of BERT embeddings. While Top-K selects a fixed number of top terms, Top-P selects terms until their cumulative importance reaches proportion p, allowing a dynamic number of terms per document/query. The authors evaluate their method on the NeuCLIR dataset (75,000 Mandarin documents) using BLADE-C architecture, comparing Top-K and Top-P masking across various sparsity levels (K values from 0.5% to 2% of vocabulary, P values from 0.25 to 0.99).

## Key Results
- Top-P Dynamic Masking consistently outperforms Top-K masking on the NeuCLIR dataset
- Top-P achieves similar or better mAP scores with improved query throughput
- Top-P selects more terms for documents but fewer for queries compared to Top-K, resulting in better overall performance

## Why This Works (Mechanism)
None

## Foundational Learning
- BERT embeddings: Dense representations from pre-trained transformer models that capture semantic information - needed to understand the base representations being sparsified
- Cross Language Information Retrieval (CLIR): Task of retrieving documents in one language using queries in another language - core problem being addressed
- Sparse representations: Document/query representations with most elements set to zero - the output format being optimized
- BLADE-C architecture: Specific CLIR architecture used for evaluation - the framework within which improvements are measured

## Architecture Onboarding

Component map: BERT -> Top-K/Top-P masking -> Sparse representation -> BLADE-C retrieval

Critical path: Input document/query -> BERT embedding generation -> Term importance scoring -> Top-P/K masking -> Sparse vector output -> Retrieval matching

Design tradeoffs: Top-K uses fixed sparsity (constant computational cost) vs Top-P uses adaptive sparsity (potentially variable computational cost but better information preservation)

Failure signatures: Performance degradation when p is too low (insufficient information) or too high (loss of sparsity benefits)

First experiments:
1. Replicate baseline Top-K masking results on NeuCLIR dataset
2. Implement Top-P masking with varying p values (0.25, 0.5, 0.75, 0.85, 0.99)
3. Compare mAP and query throughput between Top-K and Top-P configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (NeuCLIR with Mandarin documents) and BLADE-C architecture
- Limited theoretical explanation for why term selection pattern differences yield performance gains
- Narrow sparsity range tested (0.5% to 2% of vocabulary)

## Confidence
- Top-P Dynamic Masking consistently outperforms Top-K masking on NeuCLIR dataset - Medium Confidence
- Top-P Dynamic Masking improves query throughput while maintaining or improving mAP - Medium Confidence
- Term selection pattern differences explain performance gains - Low Confidence

## Next Checks
1. Test Top-P masking on additional CLIR datasets spanning multiple languages and domains to verify generalizability beyond Mandarin documents
2. Conduct detailed timing benchmarks comparing inference speed and memory usage of Top-K versus Top-P masking across different hardware configurations
3. Evaluate Top-P masking performance across a wider range of sparsity levels to identify optimal operating points and potential failure modes at extremes