---
ver: rpa2
title: 'SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation
  without Real Medical Data'
arxiv_id: '2504.08177'
source_url: https://arxiv.org/abs/2504.08177
tags:
- medical
- images
- data
- synthfm
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SynthFM addresses the challenge of medical image segmentation\
  \ by introducing a synthetic data generation framework that enables training foundation\
  \ models without real medical data. The method generates synthetic images mimicking\
  \ medical imaging characteristics like contrast, texture, and noise through two\
  \ modules: shape-aware (using B\xB4ezier curves for anatomical structures) and boundary-aware\
  \ (creating adjacent structures with shared boundaries)."
---

# SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data

## Quick Facts
- arXiv ID: 2504.08177
- Source URL: https://arxiv.org/abs/2504.08177
- Reference count: 0
- Key outcome: Zero-shot medical image segmentation model trained exclusively on synthetic data outperforms SAM, SAM 2, and UnSAM across CT, MRI, and Ultrasound modalities with DSC values up to 90.29%

## Executive Summary
SynthFM introduces a synthetic data generation framework that enables training foundation models for medical image segmentation without requiring any real medical data. The method generates synthetic images that capture key medical imaging characteristics like contrast, texture, and noise through shape-aware and boundary-aware modules. Trained on these synthetic images using SAM's pretrained encoder and a newly trained decoder, SynthFM achieves state-of-the-art zero-shot performance across 11 anatomical structures spanning CT, MRI, and Ultrasound modalities. The approach demonstrates that synthetic data can effectively approximate the statistical manifold of medical images, eliminating the dependency on costly annotated medical datasets while maintaining strong segmentation performance.

## Method Summary
SynthFM generates synthetic medical images using two modules: a shape-aware module that creates anatomical structures using Bézier curves with randomized control points and a noise library (Gaussian, Poisson, Perlin, speckle, Rician) with contrast modulation, and a boundary-aware module that creates adjacent structures with shared boundaries using SynthMorph-generated label maps with random erosion. The model uses SAM's pretrained ViT-H encoder (frozen) with a decoder trained from scratch on 100 epochs of synthetic data (10,000 images/epoch, batch size 1, learning rate 1×10⁻⁴). Training uses Dice loss and on-the-fly synthetic batch generation (1024×1024 resolution). The approach is evaluated zero-shot on CT, MRI, and Ultrasound datasets without any real medical data during training.

## Key Results
- SynthFM achieves DSC values up to 90.29% for CT kidney segmentation and 83.58% for US fetal head segmentation
- Outperforms zero-shot baselines (SAM, SAM 2, UnSAM) with statistically significant improvements (p < 0.001) across all prompt settings
- Demonstrates superior generalizability by outperforming supervised baseline MedSAM on unseen label-free microscopy data
- Ablation study confirms boundary-aware module is critical for ultrasound performance (DSC drops from 63.86 to 51.37 when removed)

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Manifold Approximation
SynthFM's analytically generated synthetic data approximates the statistical manifold of medical images sufficiently for zero-shot transfer. The shape-aware module generates diverse anatomical structures using Bézier curves with randomized control points, while a noise library and contrast modulation function simulate modality-specific intensity distributions. This exposes the model to the full range of texture/contrast variations found in CT, MRI, and ultrasound without requiring real patient data. The core assumption is that medical image segmentation performance depends primarily on learning contrast-invariant shape representations and noise-robust boundary detection, not on precise reproduction of organ-specific anatomy.

### Mechanism 2: Boundary-aware Training for Low-contrast Adjacent Structures
The boundary-aware module leverages SynthMorph to generate multiclass label maps, then applies random erosion to create artificial boundaries between adjacent regions. These are converted to binary masks where the erosion-created gap becomes background, training the model to discriminate structures with near-identical intensity profiles. This mechanism is particularly effective for modalities like ultrasound where organ boundaries are inherently ambiguous. The core assumption is that the difficulty in ultrasound/medical segmentation stems primarily from low inter-structure contrast rather than unique anatomical knowledge.

### Mechanism 3: Frozen Encoder with Decoder Retraining Preserves Visual Primitives
Keeping SAM's encoder frozen while training the decoder from scratch on synthetic data preserves transferable low-level visual features while adapting the mask prediction head to medical image characteristics. SAM's ViT-H encoder, pretrained on 1 billion masks from natural images, provides general edge/texture/shape detectors. The decoder, trained with Dice loss on 100 epochs of synthetic data, learns to map these features to medical segmentation masks without overfitting to synthetic artifacts. The core assumption is that natural image features (edges, textures, shapes) transfer to medical images; only the mask decoding strategy requires adaptation.

## Foundational Learning

- **Concept: Domain Shift / Distribution Mismatch**
  - Why needed here: The entire motivation for SynthFM is that SAM, trained on natural images, fails on medical images due to domain differences in contrast, noise, and texture. Understanding domain shift is essential to grasp why synthetic data generation is a viable solution.
  - Quick check question: Can you explain why a model trained on photographs of cats and cars might fail to segment a kidney in an ultrasound image, even though both are "images"?

- **Concept: Zero-shot / Foundation Model Transfer**
  - Why needed here: SynthFM's value proposition is achieving zero-shot medical segmentation without real medical data. The distinction between zero-shot (no task-specific training), supervised fine-tuning (MedSAM), and synthetic pretraining (SynthFM) is critical for interpreting the results.
  - Quick check question: What is the difference between MedSAM's supervised approach and SynthFM's synthetic-only approach in terms of data requirements and expected generalization?

- **Concept: Dice Loss and Segmentation Metrics**
  - Why needed here: All reported results use Dice Similarity Coefficient (DSC), and the model is trained with Dice loss. Understanding why Dice is preferred over pixel-wise cross-entropy for medical segmentation (class imbalance, region-based evaluation) is necessary to interpret performance claims.
  - Quick check question: Why might a model achieve 95% pixel accuracy but only 50% Dice on a small tumor segmentation task?

## Architecture Onboarding

- **Component map:**
  - Data Generation Pipeline: Shape-aware module (Bézier curves + noise library + contrast modulation) → Boundary-aware module (SynthMorph labels + erosion)
  - Model: SAM ViT-H encoder (frozen, ~636M params) → Prompt encoder → Mask decoder (trained from scratch)
  - Training Loop: On-the-fly synthetic batch generation (1024×1024) → Forward pass → Dice loss → Decoder-only backprop

- **Critical path:**
  1. Verify synthetic data visual quality (run shape-aware and boundary-aware generators independently)
  2. Confirm encoder weights load correctly from SAM checkpoint
  3. Train decoder with small synthetic subset (1 epoch) and check loss convergence
  4. Evaluate on held-out real medical data before full 100-epoch training

- **Design tradeoffs:**
  - Frozen vs. fine-tuned encoder: Paper freezes encoder; fine-tuning might improve performance but requires more compute and risks overfitting to synthetic artifacts
  - Shape-only vs. boundary-only vs. combined: Ablation shows combined is best, but shape-only is competitive for CT/MR; consider modality-specific deployments
  - Prompt strategy: (1,0) to (3,2) prompts all improve over baselines, but clinical workflows may limit prompt counts

- **Failure signatures:**
  - High training loss, low validation DSC: Synthetic data may not match target modality; check noise library coverage
  - Good CT/MR performance, poor ultrasound: Boundary module may be under-trained; increase boundary-aware sample ratio
  - Out-of-distribution failures (e.g., microscopy): If specific modality characteristics are missing from synthetic data, generalization will degrade

- **First 3 experiments:**
  1. Sanity check: Generate 100 synthetic samples, visualize alongside real CT/MR/US images from target dataset; manually verify texture/contrast similarity
  2. Ablation replication: Train shape-only and boundary-only variants on a single organ (e.g., liver CT); compare to Table 2 patterns
  3. Prompt sensitivity: Evaluate SynthFM with (1,0), (3,0), (1,2), (3,2) prompts on a small validation set; confirm monotonic improvement with more prompts as reported

## Open Questions the Paper Calls Out
- Can the SynthFM framework be effectively extended to 3D medical image segmentation? (Future work section)
- Does incorporating bounding box or mask prompts improve SynthFM's performance compared to the tested point-click prompts? (Future work section)
- Does pre-training the image encoder on SynthFM's synthetic data improve segmentation accuracy compared to freezing the SAM encoder? (Inferred from section 3.2 methodology)

## Limitations
- Ultrasound performance remains notably lower (DSC ~63%) compared to CT/MR, suggesting potential gaps in synthetic boundary simulation
- Method's success depends on SAM's pretrained encoder, meaning it still relies on real data for the encoder weights
- Core assumption that medical image characteristics are fully captured by shape diversity and noise libraries requires validation across diverse pathological cases

## Confidence
- **High Confidence:** Methodology for synthetic data generation and superiority over SAM/SAM2 baselines are well-supported by experimental results and statistical significance (p < 0.001)
- **Medium Confidence:** Claim of outperforming MedSAM on microscopy data is compelling but based on a single downstream task, requiring more diverse validation
- **Low Confidence:** Assertion that no real medical data is needed long-term, given that SynthFM still depends on SAM's real-data pretraining for its encoder weights

## Next Checks
1. Generate synthetic CT, MRI, and ultrasound samples and conduct blinded radiologist review to assess whether synthetic images capture modality-specific artifacts and tissue appearances
2. Train SynthFM with encoder fine-tuning on synthetic data to quantify the contribution of frozen SAM features versus domain-specific adaptation
3. Introduce synthetic pathologies (tumors, lesions) into the generation pipeline and evaluate whether performance degrades or maintains when applied to real pathological cases