---
ver: rpa2
title: Context-Informed Grounding Supervision
arxiv_id: '2506.15480'
source_url: https://arxiv.org/abs/2506.15480
tags:
- context
- performance
- ctx-llm
- training
- noctx-llm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how training language models with context-augmented
  instruction tuning (CTX-LLM) versus context-free instruction tuning (NOCTX-LLM)
  affects their behavior and downstream performance. The authors find that CTX-LLM
  models show stronger grounding to external context, leading to improved performance
  on information-seeking tasks when context is available, and reduced hallucination
  in vision-language tasks.
---

# Context-Informed Grounding Supervision

## Quick Facts
- arXiv ID: 2506.15480
- Source URL: https://arxiv.org/abs/2506.15480
- Reference count: 26
- Key outcome: Training with context-augmented data (CTX-LLM) improves grounding and reduces hallucination when context is available, but reduces parametric knowledge use, requiring routing strategies for optimal deployment.

## Executive Summary
This paper investigates how training language models with context-augmented instruction tuning (CTX-LLM) versus context-free instruction tuning (NOCTX-LLM) affects model behavior and downstream performance. The authors find that CTX-LLM models show stronger grounding to external context, leading to improved performance on information-seeking tasks when context is available, and reduced hallucination in vision-language tasks. However, CTX-LLM models perform worse on knowledge-intensive tasks without context due to reduced reliance on parametric knowledge. The paper also demonstrates that using CTX-LLM as the backbone for vision-language models reduces hallucination and maintains factual consistency across long responses. For practical deployment, the authors show that routing inputs between separate CTX-LLM and NOCTX-LLM models based on context availability outperforms training a single mixed model, providing a practical way to preserve both grounding ability and parametric knowledge use.

## Method Summary
The authors train two versions of 7B LLMs: CTX-LLM with context-augmented instruction tuning where relevant context is prepended to response tokens, and NOCTX-LLM with context-free instruction tuning. During training, loss is computed only on response tokens (not context tokens). They evaluate on information-seeking tasks with and without context, and examine vision-language model performance by using these backbones in standard VLM alignment pipelines. They also compare mixed training approaches and input routing strategies.

## Key Results
- CTX-LLM achieves 5.5% higher average performance than NOCTX-LLM on context-available information-seeking tasks (GPT-4o evaluation)
- NOCTX-LLM outperforms CTX-LLM on knowledge-intensive tasks without context due to better parametric knowledge retention
- Routing inputs between separate CTX-LLM and NOCTX-LLM models based on context availability outperforms single mixed models (56.7 vs 55.6 avg score)
- CTX-VLM backbone reduces hallucination across four vision-language benchmarks compared to NOCTX-VLM

## Why This Works (Mechanism)

### Mechanism 1: Attention Redistribution Toward External Context
- Claim: Training with context-augmented data implicitly shifts attention from self-generated tokens to provided context tokens.
- Mechanism: During training, relevant context is prepended to response tokens, and loss is computed only on response tokens (not context). This setup conditions the model to treat context as a primary information source for generation rather than background noise.
- Core assumption: The model learns to attend more strongly to context because it consistently contains information needed to produce correct responses during training.
- Evidence anchors:
  - [section] Figure 1 shows CTX-LLM attends most strongly to context 51.1% of the time vs. 23.6% for NOCTX-LLM, while NOCTX-LLM attends to generated response 60.6% vs. 31.7% for CTX-LLM.
  - [section 3.2] "CTX-LLM assigns greater attention to the given context (blue), whereas NOCTX-LLM tends to focus more on its own previously generated responses (orange)."
  - [corpus] Weak direct corpus support; related work (GRACE, ConSens) addresses grounding but not this specific attention mechanism.
- Break condition: If context is unreliable or irrelevant during training, the model may learn incorrect attention patterns or fail to trust context appropriately.

### Mechanism 2: Parametric Knowledge Deprioritization
- Claim: Context-augmented training reduces encoding of new knowledge into parameters, shifting reliance toward external context at inference.
- Mechanism: When context is always available during training, the model does not need to memorize information in its parameters to minimize loss. Instead, it learns retrieval and usage patterns for external evidence.
- Core assumption: Models optimize for training loss efficiently; if context provides answers, parametric encoding is unnecessary.
- Evidence anchors:
  - [section 3.3] "NOCTX models tend to memorize training knowledge more strongly... whereas CTX models retain more of their original knowledge but memorize counterfactual knowledge less strongly."
  - [section 5] "Increasing the rate of context-augmented training data generally improves performance when context is available at inference, while causing slight degradation when no context is provided."
  - [corpus] LUMINA discusses imbalance between hallucinations and context usage in RAG, but does not directly address training-time parametric deprioritization.
- Break condition: At inference time without context, CTX-LLM underperforms because parametric knowledge was deprioritized during training.

### Mechanism 3: Cross-Modal Grounding Transfer
- Claim: Grounding behavior learned through text-based context training transfers to visual grounding in vision-language models.
- Mechanism: CTX-LLM backbone learns a general "attend to provided evidence" behavior. When this backbone is used in a VLM, the same attention pattern applies to visual tokens as external evidence.
- Core assumption: Visual tokens in VLMs are processed similarly to text context tokens; the grounding behavior is modality-agnostic.
- Evidence anchors:
  - [abstract] "In the vision-language domain, replacing a vision-language model's LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks."
  - [section 4.1] "CTX-VLM consistently outperforms NOCTX-VLM, achieving higher accuracy and reduced hallucination."
  - [section 4.2] CTX-VLM maintains stable accuracy across response positions; NOCTX-VLM degrades at later positions (Figure 5).
  - [corpus] GC-KBVQA and MÂ³KG-RAG address multimodal knowledge grounding but do not examine backbone transfer specifically.
- Break condition: If visual alignment training significantly overwrites backbone behavior, transfer may not occur.

## Foundational Learning

- Concept: **Autoregressive Language Modeling with Conditional Context**
  - Why needed here: Understanding how loss is computed only on response tokens while context is prepended requires grasp of autoregressive training and masking.
  - Quick check question: Given input `[context; response]`, which tokens contribute to the loss function?

- Concept: **Parametric vs. Contextual Knowledge**
  - Why needed here: The core tradeoff is between knowledge stored in model weights versus knowledge provided at inference time.
  - Quick check question: If a model trained with context encounters a question without context at inference, what knowledge source must it use?

- Concept: **Attention Analysis for Grounding**
  - Why needed here: The paper uses attention patterns as evidence for grounding behavior; understanding attention weights is essential for mechanism validation.
  - Quick check question: What does it indicate if a model's attention during generation is focused on its own previously generated tokens rather than input context?

## Architecture Onboarding

- Component map:
  - Training data: Context-augmented (CTX) vs. context-free (NOCTX) instruction-response pairs
  - Loss computation: Cross-entropy on response tokens only; context tokens masked (not included in loss)
  - Inference routing (optional): Two models with heuristic routing based on context availability
  - VLM backbone: LLM trained with CTX or NOCTX used in two-stage vision-language alignment (pretraining + finetuning)

- Critical path:
  1. Prepare instruction-tuning dataset with context prepended to response sentences
  2. Train LLM with loss over response tokens only (Section 2.1 equation)
  3. Evaluate on information-seeking tasks with and without context
  4. For VLM: use trained LLM as backbone, apply standard vision-language alignment

- Design tradeoffs:
  - **CTX-LLM vs. NOCTX-LLM**: CTX improves grounding (+5.5% avg on context-available tasks) but degrades on knowledge-intensive tasks without context
  - **Mixed training (50/50)**: Balanced single model but neither capability is maximized
  - **Routing setup**: Best overall performance (56.7 vs. 55.6 for mixed) but requires maintaining two models
  - **LoRA alternative**: Similar trends with lower computational cost (Appendix D.2)

- Failure signatures:
  - CTX-LLM underperforms on tasks requiring parametric knowledge (Figure 2: lower performance without context)
  - NOCTX-LLM hallucinates more in VLMs (Table 2: higher CHAIR scores, lower F1)
  - CTX-LLM susceptible to misleading context when many documents are retrieved (Appendix B.1: performance gap narrows with more contexts)

- First 3 experiments:
  1. **Ablate loss masking**: Train with loss on context tokens vs. loss only on response tokens; expect grounding degradation when context is in loss (Table 5 shows 61.5 vs. 55.9 avg for output-side context placement)
  2. **Vary context ratio in mixed training**: Train with 0%, 25%, 50%, 75%, 100% context-augmented data; expect 50% to be most balanced (Figure 6)
  3. **Transfer to VLM**: Train CTX-LLM and NOCTX-LLM backbones, then apply identical vision-language alignment; evaluate on hallucination benchmarks (POPE, AMBER, ImageInWords) to confirm cross-modal transfer

## Open Questions the Paper Calls Out
- Can a trainable router outperform the simple heuristic of routing based on context availability? The current routing mechanism depends on a simple heuristic (whether context is provided), which does not account for whether the context is actually useful or necessary for a specific query.
- How does Context-Informed Grounding Supervision (CINGS) affect model behavior when external context is noisy, unreliable, or adversarial? The current findings are based on the assumption of relevant context; it is unknown if the induced grounding behavior makes models more susceptible to misleading or incorrect evidence.
- Do the observed shifts in knowledge reliance and grounding persist in models significantly larger than 7B parameters? LLM behaviors often change qualitatively with scale; it is unclear if the trade-off between parametric knowledge and grounding remains consistent in larger architectures.

## Limitations
- Experiments limited to 7B models due to computational constraints, leaving scaling behavior undefined
- Assumes context is reliable and relevant; does not explore scenarios with unreliable, noisy, or partially irrelevant context
- Does not address potential security implications of reduced parametric knowledge retention

## Confidence
- **High**: CTX-LLM shows stronger attention to context and better grounding when context is available (Figure 1, Table 1)
- **High**: Routing between CTX-LLM and NOCTX-LLM outperforms mixed training (Table 3, 56.7 vs 55.6)
- **Medium**: CTX-LLM backbone reduces VLM hallucination (Table 2, CHAIR scores), though some metrics show smaller improvements
- **Medium**: Attention mechanism explanations are plausible but not directly verified through ablation studies
- **Low**: Limited to 7B models; findings may not scale to larger models

## Next Checks
1. Verify the attention redistribution claim by examining attention patterns on a held-out validation set for both CTX-LLM and NOCTX-LLM
2. Replicate the routing experiment with a larger range of context availability thresholds to optimize the heuristic
3. Test the VLM backbone transfer claim on a different vision-language alignment pipeline (e.g., Q-former based) to confirm generality