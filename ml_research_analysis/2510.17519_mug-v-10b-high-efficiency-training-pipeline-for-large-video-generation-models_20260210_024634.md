---
ver: rpa2
title: 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models'
arxiv_id: '2510.17519'
source_url: https://arxiv.org/abs/2510.17519
tags:
- video
- arxiv
- generation
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of training large-scale video
  generation models, which are particularly resource-intensive due to cross-modal
  text-video alignment, long sequences, and complex spatiotemporal dependencies. To
  overcome these issues, the authors present a training framework that optimizes four
  pillars: data processing, model architecture, training strategy, and infrastructure.'
---

# MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models

## Quick Facts
- arXiv ID: 2510.17519
- Source URL: https://arxiv.org/abs/2510.17519
- Reference count: 40
- First public release of large-scale video generation training code leveraging Megatron-Core for high efficiency and near-linear multi-node scaling

## Executive Summary
This work presents MUG-V 10B, a training framework addressing the computational challenges of large-scale video generation models through four optimized pillars: data processing, model architecture, training strategy, and infrastructure. The framework introduces a scalable video processing pipeline, high-ratio VideoVAE compression achieving ~2048x compression, a training-stable 10B-parameter Diffusion Transformer, a multi-stage training strategy, and efficient Megatron-Core-based infrastructure. The resulting model matches state-of-the-art video generators overall and surpasses leading open-source baselines on e-commerce-oriented video generation tasks in human evaluations. The complete stack, including model weights, training code, and inference pipelines, is open-sourced.

## Method Summary
The MUG-V framework optimizes video generation training through a scalable pipeline. Videos are processed via scene detection, VLM captioning, and quality filtering, then compressed using an 8×8×8 VideoVAE with minimal encoding. A 10B Diffusion Transformer is trained using a curriculum starting from images and 360p videos, expanding from a 2B proxy via weight tiling with perturbation. The model uses Megatron-Core with hybrid parallelism (TP+SP+PP+DP) to achieve near-linear scaling across 500 H100 GPUs. Post-training applies annealed SFT with post-EMA followed by iterative KTO/DPO preference optimization using human-verified annotations.

## Key Results
- Achieves near-linear scaling across 500 H100 GPUs with Megatron-Core infrastructure
- 10B DiT matches state-of-the-art video generators overall on VBench benchmarks
- Surpasses leading open-source baselines on e-commerce-oriented video generation tasks in human evaluations
- Open-sources complete training stack including model weights, code, and inference pipelines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Minimal encoding in Video VAE eliminates information-density imbalance from causal convolutions while maintaining 8×8×8 compression quality.
- **Mechanism**: Each latent token derives solely from its 8-frame chunk via tensor reshaping, ensuring uniform information density. The decoder uses wider temporal windows for reconstruction without this constraint.
- **Core assumption**: Video VAE's primary role is compression/reconstruction, not generation, making temporal context mixing at encoder unnecessary.
- **Evidence anchors**: Abstract mentions ~2048x compression; section 3.1.2 explains minimal encoding rationale; corpus lacks direct validation of this specific approach.
- **Break condition**: If reconstruction quality degrades significantly on highly dynamic scenes where 8-frame chunks lack sufficient context.

### Mechanism 2
- **Claim**: Parameter expansion via hidden-size equivariant expansion with perturbation enables efficient 2B→10B transfer.
- **Mechanism**: Train 2B proxy with same depth (56 blocks) but smaller hidden dimension (1728), then expand to 10B by tiling weights and adding random perturbations to avoid gradient duplication.
- **Core assumption**: Hyperparameters effective for small models at same depth transfer to expanded models when initialized functionally equivalently.
- **Evidence anchors**: Abstract mentions training-stable 10B DiT; section 4.2.1 describes 4× parameter increase via expansion; corpus lacks specific validation of this technique.
- **Break condition**: If expanded models underperform compared to training from scratch or if perturbations prove insufficient to break gradient symmetry.

### Mechanism 3
- **Claim**: Near-linear multi-node scaling achieved through hybrid parallelism addressing video's long-sequence memory burden without activation recomputation.
- **Mechanism**: TP within nodes combined with SP for long activations, PP across nodes using point-to-point communication, and DP for batch size. Activations are sharded across TP groups via SP to avoid recomputation.
- **Core assumption**: Communication overhead of distributing activations across TP groups is lower than recomputation overhead during backprop.
- **Evidence anchors**: Abstract states "near-linear scaling" on 500 H100 GPUs; section 5.1 describes sharding activations across TP group via SP; corpus lacks direct validation of this specific parallelism strategy.
- **Break condition**: If inter-node communication becomes bottleneck at larger scales or for longer sequences.

## Foundational Learning

- **Concept: Latent Diffusion with Flow Matching**
  - **Why needed here**: MUG-V operates in compressed latent space; understanding VAE latent interface with diffusion/flow objectives is essential for debugging reconstruction vs. generation failures.
  - **Quick check question**: Can you explain why flow matching objectives are used instead of standard DDPM denoising, and what the velocity field represents?

- **Concept: Megatron-Core Parallelism Primitives**
  - **Why needed here**: Training infrastructure relies on composing TP, SP, PP, and DP; understanding memory/compute tradeoffs is crucial for diagnosing scaling bottlenecks.
  - **Quick check question**: What is the difference between tensor parallelism and sequence parallelism in terms of what gets sharded, and when would you prefer one over the other?

- **Concept: Preference Optimization for Diffusion (DPO/KTO)**
  - **Why needed here**: Post-training uses human-labeled preferences to reduce physical implausibilities; understanding how these differ from standard likelihood maximization is crucial for alignment stage.
  - **Quick check question**: Why does the paper use KTO for error-free generation and DPO for motion quality?

## Architecture Onboarding

- **Component map**: PySceneDetect + CSS for splitting → VLM captioning (Qwen2-VL-7B distilled from 72B) → Quality filters (sharpness, aesthetic, motion amplitude, LLM filter) → Video VAE (8×8×8 downsampling, 24 channels) → DiT Backbone (56 blocks, full attention, 3D RoPE) → Training Infrastructure (Megatron-Core: TP+SP+PP+DP)

- **Critical path**: 1) Train 2B proxy model on images + 360p video (validates hyperparameters) 2) Expand to 10B via weight tiling + perturbation 3) Curriculum: Stage 1 (image + 360p, 2s) → Stage 2 (360p, 5s) → Stage 3 (720p, 5s, 12M clips) 4) Post-training: Annealed SFT with post-EMA → Preference optimization (KTO + DPO iteratively)

- **Design tradeoffs**:
  - **Compression vs. fidelity**: 2048× compression enables full attention but risks fine-detail loss; mitigated by C=24 channels and adaptive reconstruction weighting
  - **Full attention vs. efficiency**: Full attention provides global coherence; feasible only because of aggressive compression; spatio-temporal separated attention would be cheaper but weaker
  - **Curriculum ordering**: Low-res first maximizes sample throughput (10× more samples in Stages 1-2); high-res last refines details but is expensive

- **Failure signatures**:
  - Reconstruction artifacts in fast motion: Likely Video VAE insufficient temporal context or adaptive weighting not triggered
  - Training instability at 10B scale: Likely normalization issues (check QK norm, cross-attention norm) or learning rate not properly transferred
  - Physical implausibilities persisting: Preference optimization may need more iterations or better annotation coverage; SFT regularizer may be too strong
  - Scaling efficiency drops: Check for pipeline bubbles (imbalanced stage execution) or communication overhead from SP on long sequences

- **First 3 experiments**:
  1. Validate Video VAE reconstruction: Run 8×8×8 VAE on held-out clips with varying motion amplitude; confirm adaptive weighting activates and compare PSNR/SSIM against baselines in Table 2.
  2. Small-scale parallelism benchmark: On 8 GPUs, measure throughput with TP-only vs. TP+SP for 2B model on 5s 720p clips; confirm memory savings justify any throughput loss.
  3. Post-training iteration ablation: After Stage 3 pretraining, run one iteration of KTO-only, DPO-only, and KTO+DPO interleaved; compare VBench metrics and human evaluation pass rates to validate multi-stage preference strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off strategy between supervised fine-tuning (SFT) and reinforcement learning (RL) preference optimization in video generation models?
- Basis in paper: Section 4.2.3 states "striking an effective trade-off between supervised fine-tuning (SFT) and RL remains a challenging open problem."
- Why unresolved: The paper employs both approaches heuristically—retaining SFT as a regularizer during preference optimization—without establishing principled guidelines for balancing their contributions or scheduling.
- What evidence would resolve it: Systematic ablation studies comparing different SFT/RL ratios and training schedules, with convergence analysis and quality metrics, would establish optimal trade-offs.

### Open Question 2
- Question: How can video generation models achieve robust fine-grained appearance fidelity (material and texture preservation) that is insensitive to VAE compression artifacts?
- Basis in paper: Appendix C states "fine-grained appearance fidelity, such as material and texture preservation, still lags, with sensitivity to VAE compression and DiT noise initialization leading to subtle but consequential degradations."
- Why unresolved: Despite strong reconstruction metrics (Table 2), the 8×8×8 VideoVAE compression introduces sensitivity that degrades fine texture details critical for e-commerce applications.
- What evidence would resolve it: Comparative evaluations across VAE compression ratios, bottleneck channel dimensions, and quantitative texture fidelity metrics would identify effective solutions.

### Open Question 3
- Question: Can automated video reward models replace human annotators for preference optimization while handling the multiplicity of evaluation axes?
- Basis in paper: Section 4.2.3 notes RL application "remains challenging due to (i) the limited capacity of current video evaluation (reward) models and (ii) the multiplicity of optimization axes, such as appearance, motion, temporal coherence."
- Why unresolved: The paper relied on human-annotated preferences because automated reward models cannot adequately capture all quality dimensions simultaneously.
- What evidence would resolve it: Development of multi-dimensional reward models achieving strong correlation with human evaluations across motion smoothness, physical plausibility, and aesthetics would demonstrate viability.

### Open Question 4
- Question: What architectural innovations are needed to scale video generation to longer durations and higher resolutions while maintaining long-range temporal consistency?
- Basis in paper: Appendix C states "scaling to longer durations and higher resolutions demands algorithms and systems that cope with long-sequence training, inference efficiency, and long-range temporal consistency."
- Why unresolved: Current training is limited to 5-second clips at 720p; full attention becomes computationally prohibitive for longer sequences, while separated attention may sacrifice global coherence.
- What evidence would resolve it: Successful training at 10+ second clips and 1080p+ resolution with maintained VBench scores and temporal consistency metrics would confirm scalability.

## Limitations
- Critical hyperparameters including exact learning rates, batch sizes, and loss weighting schedules are omitted, making reproduction speculative
- Dataset composition ambiguity with unspecified public dataset names and image-to-video ratios in Stage 1
- Parallelism configuration details missing exact degrees for TP/PP/DP/SP and micro-batch sizes for 500 H100 deployment

## Confidence

**High confidence**: Core architectural innovations (8×8×8 VideoVAE compression, minimal encoding principle, 10B DiT with full attention) are well-specified and theoretically sound; video compression ratio and reconstruction quality claims are directly measurable.

**Medium confidence**: Curriculum learning strategy and 2B→10B expansion via HyperCloning are described with sufficient detail for replication, though exact hyperparameter transfer effectiveness remains uncertain without base rates.

**Low confidence**: Post-training preference optimization pipeline (KTO/DPO interleaving, SFT regularization strength) lacks specificity in iteration counts and preference pair distributions, making it difficult to assess claimed improvements in physical plausibility.

## Next Checks

1. **Video VAE reconstruction validation**: Implement 8×8×8 minimal encoding VAE with adaptive spatiotemporal weighting on held-out video set; measure PSNR/SSIM against baseline VAEs and verify adaptive weighting activates for high-motion regions as claimed.

2. **2B→10B expansion fidelity**: Train 2B DiT proxy to completion, then apply HyperCloning expansion with perturbation; compare validation loss trajectories and qualitative outputs against both proxy and 10B model trained from scratch to quantify initialization benefits.

3. **Preference optimization ablation**: After completing Stage 3 pretraining, conduct controlled experiments varying KTO/DPO iteration counts and SFT regularization strength; measure changes in VBench scores and human evaluation pass rates to isolate contribution of each post-training component.