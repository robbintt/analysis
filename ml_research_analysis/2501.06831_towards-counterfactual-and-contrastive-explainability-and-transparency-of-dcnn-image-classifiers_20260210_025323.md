---
ver: rpa2
title: Towards Counterfactual and Contrastive Explainability and Transparency of DCNN
  Image Classifiers
arxiv_id: '2501.06831'
source_url: https://arxiv.org/abs/2501.06831
tags:
- filters
- class
- image
- explanations
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for generating counterfactual
  and contrastive explanations for deep convolutional neural networks (DCNNs). The
  method identifies the most important filters in the top convolution layer of a pre-trained
  DCNN that separate the model's decision between classifying an image to the original
  inferred class or some other specified alter class.
---

# Towards Counterfactual and Contrastive Explainability and Transparency of DCNN Image Classifiers

## Quick Facts
- **arXiv ID**: 2501.06831
- **Source URL**: https://arxiv.org/abs/2501.06831
- **Reference count**: 14
- **Primary result**: Proposes method to generate counterfactual and contrastive explanations for DCNNs by identifying minimum correct (MC) and minimum incorrect (MI) filters in top convolution layer

## Executive Summary
This paper introduces a novel approach for generating counterfactual and contrastive explanations for deep convolutional neural networks (DCNNs) by identifying critical filters in the top convolution layer that separate model decisions between original and target classes. The method provides contrastive explanations by identifying the minimum set of filters necessary to maintain the predicted class, while counterfactual explanations are provided by specifying minimal changes needed in these filters to achieve a different prediction. The approach is evaluated on the Caltech-UCSD Birds (CUB) 2011 dataset using a VGG-16 backbone, demonstrating that the generated explanations are both meaningful and interpretable, thereby increasing model transparency and trustworthiness.

## Method Summary
The method modifies a pre-trained VGG-16 network by replacing dense layers with Global Average Pooling (GAP) followed by dropout and softmax for 200 classes. The model is trained on CUB-2011 using transfer learning (50 epochs) followed by fine-tuning (150 epochs). Two specialized CFE models are then trained: an MC CFE model with sigmoid activation and thresholded-ReLU to identify filters maintaining correct predictions, and an MI CFE model with ReLU activation to identify filters that would change predictions to a target class. Both models use L1 regularization to promote sparsity, with the MC model additionally incorporating logits loss to ensure selected filters are critical for the target class.

## Key Results
- Proposed method achieves 69.5% test accuracy on CUB-2011 dataset with VGG-16 backbone
- MC filter identification shows high impact on recall when disabled (e.g., 93.3% → 30% for "Red-winged blackbird")
- User study demonstrates high explanation satisfaction for understandability, usefulness, and confidence
- Quantitative comparison shows competitive performance against GradCAM and SCOUT baselines

## Why This Works (Mechanism)
The approach works by leveraging the fact that deep convolutional networks learn hierarchical representations where top-layer filters capture high-level semantic concepts. By identifying the minimal set of filters that are both necessary and sufficient for a particular classification decision, the method can explain why the model made its prediction (contrastive) and what minimal changes would lead to a different prediction (counterfactual). The L1 regularization combined with thresholded activations ensures sparse, interpretable filter sets, while the logits loss term ensures selected filters are genuinely important for the classification decision rather than merely correlated.

## Foundational Learning
- **Global Average Pooling (GAP)**: Reduces spatial dimensions while preserving channel-wise feature importance; needed to connect convolutional features to classification layer in a differentiable manner
- **Transfer learning with fine-tuning**: Enables effective use of pre-trained ImageNet weights on domain-specific CUB-2011 dataset; check by verifying baseline accuracy improvement
- **L1 regularization for sparsity**: Promotes selection of minimal filter sets by penalizing non-zero filter activations; check by comparing filter counts across different λ values
- **Thresholded-ReLU activation**: Creates binary mask of active/inactive filters; needed to distinguish between contributing and non-contributing filters
- **Logits loss integration**: Ensures selected filters have high impact on classification scores; check by comparing filter importance scores with/without this term
- **Receptive field visualization**: Helps interpret what visual features each filter responds to; needed for qualitative validation of filter importance

## Architecture Onboarding
- **Component map**: Input Image → VGG-16 (conv layers) → GAP → Dropout(0.5) → Softmax(200) → Prediction
- **Critical path**: GAP output → MC/MI CFE model (dense → activation → loss) → filter importance scores
- **Design tradeoffs**: Using top-layer filters provides high-level semantic explanations but may miss lower-level feature interactions; using only L1 regularization vs. adding logits loss affects filter selection quality
- **Failure signatures**: MC model predicts too many/few active filters (λ tuning issue), MI filters don't change predictions when added (training issue), explanations don't align with human perception (interpretation issue)
- **3 first experiments**: 1) Train baseline VGG-16 on CUB-2011 and verify ~69.5% accuracy, 2) Train MC CFE model for one class and check filter count matches expected range, 3) Apply MI filters to test image and verify prediction changes to target class

## Open Questions the Paper Calls Out
- How do counterfactual and contrastive explanations differ when identifying filters in intermediate or lower convolutional layers compared to the top layer?
- How can clear and robust evaluation metrics be developed to validate counterfactual explanations beyond existing synthetic ground truths?
- To what extent can the identified "minimum correct" and "minimum incorrect" filters be utilized for real-time adversarial attack detection and model debugging?

## Limitations
- Method currently restricted to top convolution layer, missing potential insights from lower-level feature interactions
- Evaluation relies on synthetic ground truths (part annotations) that may not capture all distinguishing features
- Limited exploration of real-world applications like adversarial detection and model debugging

## Confidence
- Model architecture and training procedure: **High**
- MC/MI CFE model implementations: **Medium**
- Quantitative results reproducibility: **Low-Medium**

## Next Checks
1. Verify filter count consistency: Train MC CFE model with λ₁=1, 2, 4 and confirm filter counts match Table 3 values (~21, ~15, ~11 filters respectively)
2. Validate logits loss importance: Train MC model with and without logits loss term; compare filter importance scores and recall impact when disabling filters
3. Confirm MI filter effectiveness: Add MI filter values to GAP features for test images; verify predicted class changes to target alter class with sufficient confidence margin