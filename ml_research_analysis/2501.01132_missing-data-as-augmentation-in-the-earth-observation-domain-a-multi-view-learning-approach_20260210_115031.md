---
ver: rpa2
title: 'Missing Data as Augmentation in the Earth Observation Domain: A Multi-View
  Learning Approach'
arxiv_id: '2501.01132'
source_url: https://arxiv.org/abs/2501.01132
tags:
- missing
- views
- data
- optical
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-view learning (MVL) method that treats
  missing data as augmentation (MAug) for Earth Observation (EO) applications. The
  core idea is to simulate all combinations of missing views during training (CoM)
  and use dynamic merge functions at feature level that ignore missing views, instead
  of imputing them.
---

# Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach

## Quick Facts
- arXiv ID: 2501.01132
- Source URL: https://arxiv.org/abs/2501.01132
- Authors: Francisco Mena; Diego Arenas; Andreas Dengel
- Reference count: 40
- Primary result: Multi-view learning method that treats missing data as augmentation, improving robustness and performance across four EO datasets.

## Executive Summary
This paper introduces a novel multi-view learning approach for Earth Observation that treats missing data as a form of data augmentation rather than a problem to be solved through imputation. The core innovation is simulating all combinations of missing views during training (CoM) and using dynamic merge functions that ignore missing views at the feature level. The method is validated across four EO datasets with both temporal and static views, showing improved model robustness under moderate missingness and enhanced predictive performance when all views are present, compared to state-of-the-art imputation-based methods.

## Method Summary
The approach uses view-dedicated encoders (1D CNNs for temporal data, MLPs for static data) followed by dynamic fusion modules that can handle variable numbers of input views. During training, the model is exposed to all non-empty subsets of available views through combinatorial simulation, forcing it to learn robust features that remain discriminative even when key modalities are missing. The fusion layer implements several merge functions (average, gated, cross-attention, memory) that operate on the set of present feature vectors without requiring fixed input dimensions. This eliminates the bias introduced by zero-imputation while maintaining computational efficiency through cached encoder forward-passes.

## Key Results
- Improved model robustness under moderate missingness (one top view missing) across all four EO datasets
- Enhanced predictive performance in full-view scenarios for classification tasks compared to imputation-based baselines
- FCoM-av (Average) method showed the best overall performance with lowest computational overhead
- Significant performance degradation in extreme missingness (one view available) for regression tasks, particularly with single-view scenarios

## Why This Works (Mechanism)

### Mechanism 1
Simulating all combinations of missing views (CoM) during training acts as a data augmentation strategy, regularizing the model to handle inference-time absences gracefully. By exposing the model to the power set of available views, the network learns feature representations that remain discriminative even when key modalities are dropped, preventing overfitting to the presence of specific sensors.

### Mechanism 2
Dynamic merge functions allow the model to ignore missing views structurally, preventing the bias introduced by zero-imputation found in fixed-size fusion. Instead of filling missing inputs with zeros, dynamic functions operate on the set of present feature vectors, ensuring the fused representation reflects only available information and maintains permutation-invariance.

### Mechanism 3
Training with feature-level CoM improves full-view performance by encouraging the learning of redundant cross-view correlations. By forcing the model to predict correctly with subsets of views, it must learn robust, shared semantic features across modalities that reinforce each other when all views are present at inference.

## Foundational Learning

- **Feature-level Fusion vs. Input-level Fusion**: Understanding why feature-level extraction and fusion is critical for handling missing views, as input-level concatenation fails with zero-imputation. Quick check: Can your fusion layer handle a variable number of input tensors without throwing a shape error?

- **Multi-View Learning (MVL)**: Understanding that "views" are distinct data sources requiring dedicated encoders before merging in heterogeneous sensor applications. Quick check: Do you know why we might use a 1D CNN for temporal views and an MLP for static views in the same architecture?

- **Data Augmentation as Regularization**: Viewing masking/dropout as a training strategy rather than just a preprocessing step. Quick check: How does the dropout layer function similarly to the "Sensor Dropout" technique mentioned in the paper?

## Architecture Onboarding

- **Component map**: View Encoders (1D CNN/MLP) -> Normalization Layer -> Dynamic Fusion Module (FCoM-av/ga/cr/me) -> Prediction Head

- **Critical path**: The Dynamic Fusion Module is the bottleneck. You must implement merge functions that accept a list of vectors and output a fixed-size vector regardless of input count. Do not use `torch.cat` on raw inputs if inputs might be missing.

- **Design tradeoffs**: FCoM-av (Average) is fastest with lowest compute but assumes equal view importance. FCoM-ga/ga (Gated/Cross-Attention) learns view importance but increases training time by ~2x. CoM is exhaustive but computationally expensive compared to random sampling methods.

- **Failure signatures**: Performance collapse in extreme missingness (negative R2 in regression), high variance in FCoM-cr if not tuned properly, and slow training due to CoM's exponential subset generation.

- **First 3 experiments**: 
  1. Implement FCoM-av vs. concatenation with zero-imputation on 30% random missing views
  2. Train FCoM-av on full dataset and systematically drop optical view in 10% increments to plot F1 degradation
  3. Compare FCoM-av vs. FCoM-ga on LFMC regression task to determine if data-driven weighting is necessary

## Open Questions the Paper Calls Out

- Can decision-level fusion strategies effectively mitigate performance degradation in extreme missingness scenarios compared to the feature-level fusion used in this study? The paper suggests future work should consider weighing predictions with missing data at the decision level.

- Is the Combinations of Missing views (CoM) technique computationally feasible for Earth Observation applications involving a large number of distinct views? The exponential growth of combinations may cause memory or time bottlenecks in complex multi-sensor setups.

- Does training with CoM maintain robustness if the training dataset itself contains naturally missing views, rather than the full-view training assumption used in the experiments? Real-world EO scenarios often have incomplete training data.

## Limitations

- The CoM approach requires combinatorial simulation of all missing combinations, becoming computationally intractable if training data itself contains significant missingness
- The permutation-invariance assumption may not hold for all fusion architectures, particularly those relying on sequential processing without proper position encoding
- While classification results support the approach, regression tasks show significant performance degradation in extreme missingness scenarios, particularly with single-view availability

## Confidence

- **CoM as augmentation mechanism**: High - experimental results consistently show improved robustness under missingness scenarios
- **Dynamic merge functions**: Medium - theoretical justification is sound but edge cases and comparison to sophisticated imputation methods need exploration
- **Full-view performance enhancement**: Medium-Low - supported by classification but contradicted in regression tasks where baselines sometimes outperform

## Next Checks

1. Test CoM performance when training data contains 20-40% missing views rather than assuming full-view training to validate generalizability beyond controlled experimental conditions.

2. Implement a controlled experiment comparing CoM against view-importance-weighted imputation methods to quantify the relative benefits of augmentation versus intelligent imputation.

3. Conduct a sensitivity analysis on the permutation-invariance assumption by systematically varying view order during training and measuring the impact on FCoM-cr performance.