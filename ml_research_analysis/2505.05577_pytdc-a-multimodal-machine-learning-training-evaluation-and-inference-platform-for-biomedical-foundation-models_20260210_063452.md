---
ver: rpa2
title: 'PyTDC: A multimodal machine learning training, evaluation, and inference platform
  for biomedical foundation models'
arxiv_id: '2505.05577'
source_url: https://arxiv.org/abs/2505.05577
tags:
- data
- learning
- pytdc
- cell
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PyTDC addresses the lack of unified platforms for multimodal biomedical
  AI by introducing an API-first system that integrates heterogeneous, continuously
  updated single-cell and therapeutic data sources. It provides standardized benchmarking,
  model retrieval, and inference endpoints for context-aware foundation models.
---

# PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models

## Quick Facts
- **arXiv ID:** 2505.05577
- **Source URL:** https://arxiv.org/abs/2505.05577
- **Reference count:** 40
- **Primary result:** PyTDC unifies multimodal biomedical AI training, evaluation, and inference with standardized benchmarking, context-specific metrics, and foundation model integration

## Executive Summary
PyTDC addresses the fragmentation in multimodal biomedical AI by introducing an API-first platform that integrates heterogeneous single-cell and therapeutic data sources. The system provides standardized benchmarking, model retrieval, and inference endpoints for context-aware foundation models across modalities like gene expression, perturbations, and protein interactions. A case study on single-cell drug-target nomination reveals that domain-specific and state-of-the-art graph models underperform context-aware approaches, while highlighting generalization limitations.

## Method Summary
PyTDC implements a model-view-controller architecture with a domain-specific language for multimodal data integration and transformation. The platform includes a unified model server for retrieving and deploying foundation models from repositories like HuggingFace and CELLxGENE, with standardized tokenization and inference endpoints. Evaluation uses context-specific metrics that weight performance on biologically relevant cell type subsets rather than pooled datasets. The system supports training, evaluation, and fine-tuning workflows with reproducible pipelines and hardware-aware deployment options.

## Key Results
- Domain-specific and state-of-the-art graph models (GATv2, Node2Vec, label propagation) underperform on single-cell drug-target nomination compared to context-aware PINNACLE
- Context-specific metrics reveal performance gaps masked by traditional evaluation, with PINNACLE achieving 0.917 AP@5 on top-20 cell types versus GATv2's 0.334
- PINNACLE excels in context-aware evaluation but fails to generalize to unseen cell types, highlighting the need for improved foundation models
- Model server reduces workflow code from hundreds of lines to under 30 while maintaining functionality across biomedical modalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** API-first architecture unifies heterogeneous, continuously updated biomedical data sources.
- **Mechanism:** The system implements a model-view-controller design pattern to create data views from multiple datasets, using a domain-specific language to define transformations and mappings without manual integration.
- **Core assumption:** Heterogeneous biomedical data can be meaningfully integrated through standardized abstraction layers.
- **Evidence anchors:**
  - [abstract] "PyTDC introduces an API-first system that integrates heterogeneous, continuously updated single-cell and therapeutic data sources."
  - [section 3.1] "This microservice architecture is implemented using the model-view-controller design pattern... to enable multimodal data views under a domain-specific-language."
  - [corpus] Weak external validation; corpus shows related multimodal integration work but no direct evaluation of this specific architecture.
- **Break condition:** If data schemas become so divergent that no unified view abstraction can meaningfully represent them, the API-first approach would fail to reduce integration complexity.

### Mechanism 2
- **Claim:** Context-specific metrics reveal model performance gaps that aggregate metrics miss.
- **Mechanism:** By computing performance on critical biological subsets (e.g., top-K cell types) rather than pooled datasets, the evaluation framework surfaces whether models capture biologically meaningful patterns or just statistical artifacts.
- **Core assumption:** Performance on specific biological contexts predicts real-world therapeutic utility.
- **Evidence anchors:**
  - [abstract] "PyTDC introduces context-specific metrics and facilitates development of multimodal, context-aware foundation models."
  - [section 4.2] Defines weighted AUROC for top-K cell types: "AUROC_TopK = Σ(AUROC_c × |D_c|) / Σ|D_c|"
  - [section 4.3, Table 2] PINNACLE achieves 0.917 AP@5 on top-20 cell types for RA, while GATv2 achieves only 0.334, revealing the gap masked by context-free evaluation.
  - [corpus] No corpus papers directly validate context-specific evaluation; this appears novel to PyTDC.
- **Break condition:** If biological contexts are incorrectly specified (wrong cell types, inappropriate granularity), context-specific metrics could mislead rather than illuminate.

### Mechanism 3
- **Claim:** Model server standardizes inference and fine-tuning across diverse foundation models.
- **Mechanism:** A unified retrieval API abstracts model loading, tokenization, and inference across distributed repositories (HuggingFace, CELLxGENE, TDC storage), reducing workflow code from hundreds of lines to under 30.
- **Core assumption:** Foundation model architectures are sufficiently similar that a common interface can abstract their differences without losing critical functionality.
- **Evidence anchors:**
  - [abstract] "It provides standardized benchmarking, model retrieval, and inference endpoints for context-aware foundation models."
  - [section 3.2] "PyTDC presents open source model retrieval and deployment software that streamlines AI inferencing... across biomedical modalities."
  - [figure 3, page 6] Code example shows integration of perturbation dataset with Geneformer inference in ~15 lines.
  - [corpus] Platform for Representation and Integration of multimodal Molecular Embeddings (FMR=0.53) addresses similar integration challenges, suggesting this is a recognized need.
- **Break condition:** If model architectures diverge significantly (e.g., requiring fundamentally different tokenization schemes or inference paradigms), the unified interface becomes leaky or unusable.

## Foundational Learning

- **Concept: Single-cell transcriptomics and cellular heterogeneity**
  - **Why needed here:** The entire platform centers on single-cell resolution data; without understanding that cells of the same "type" can have different gene expression profiles based on context, the motivation for context-aware models is opaque.
  - **Quick check question:** Can you explain why drug targets might vary in relevance across different cell types in the same tissue?

- **Concept: Foundation models and transfer learning in biology**
  - **Why needed here:** PyTDC explicitly aims to support foundation model development; understanding pre-training → fine-tuning workflows is essential to grasp why model servers and standardized evaluation matter.
  - **Quick check question:** What is the difference between training a model from scratch versus fine-tuning a pre-trained foundation model on a therapeutic task?

- **Concept: Graph neural networks and protein-protein interaction networks**
  - **Why needed here:** The case study compares GAT, Node2Vec, label propagation, and PINNACLE on PPI networks; without graph ML basics, the performance comparisons are uninterpretable.
  - **Quick check question:** Why might a protein's position in a PPI network predict whether it's a good drug target?

## Architecture Onboarding

- **Component map:** Data Layer (TDC DataLoader + CellXGene API + PrimeKG knowledge graph) -> View Layer (data processing, split generators, multimodal views via DSL) -> Controller Layer (DSL for data configurations) -> Model Server (unified retrieval from HuggingFace, CELLxGENE, TDC storage) -> Benchmarking Layer (task-specific evaluators with context-specific metrics)

- **Critical path:**
  1. Define task (e.g., `scDTN` for drug-target nomination)
  2. Retrieve dataset via DataLoader with appropriate filters
  3. Load foundation model from model server with correct tokenizer
  4. Extract embeddings from model server inference
  5. Fine-tune or evaluate using benchmark group evaluator with context-specific metrics

- **Design tradeoffs:**
  - **Abstraction vs. flexibility:** The DSL simplifies complex workflows but may constrain novel data transformations not anticipated by the framework.
  - **Standardization vs. domain specificity:** Context-specific metrics improve biological relevance but increase evaluation complexity and may not align with established benchmarks.
  - **Accessibility vs. correctness:** The model server reduces code burden but may hide important preprocessing choices; users must understand what tokenization and inference choices are being made implicitly.

- **Failure signatures:**
  - **Data integration failures:** Mismatched gene identifiers between single-cell atlas and foundation model vocabulary (e.g., Ensembl IDs vs. gene symbols).
  - **Context specification errors:** Selecting wrong or overly granular cell types for context-specific metrics, leading to noisy or misleading performance estimates.
  - **Model inference failures:** Segmentation faults or memory errors when loading large foundation models without appropriate hardware (noted in scGPT documentation).
  - **Overfitting to context-specific metrics:** Models that perform well on top-K cell types but fail to generalize to unseen cell types (as observed with PINNACLE).

- **First 3 experiments:**
  1. **Reproduce the RA drug-target nomination baseline:** Load the `opentargets_dti` dataset, train a GATv2 model on the context-free PPI network, and compare your AUROC and AP@5 scores to those in Table 2. This validates your data loading and evaluation pipeline.
  2. **Extract and visualize Geneformer embeddings for a small single-cell dataset:** Use the CellXGene API to retrieve ~1000 cells from a single tissue, tokenize with GeneformerTokenizer, extract embeddings, and visualize with UMAP colored by cell type. This tests the model server integration and helps you understand what the foundation model captures.
  3. **Implement a simple context-specific metric:** For a binary classification task of your choice, compute both standard AUROC and a "top-K class" weighted AUROC. Compare how model rankings change when using the context-specific metric. This builds intuition for why the paper introduces these metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can context-aware models be designed to generalize to unseen cell types in single-cell drug-target nomination?
- **Basis in paper:** [explicit] The abstract and Section 4.3 state that while the context-aware model (PINNACLE) excels on specific metrics, it "is unable to generalize to unseen cell types."
- **Why unresolved:** Current models rely on pre-defined subgraph views and specific contexts encountered during training, failing to transfer learned biological insights to novel cellular environments.
- **What evidence would resolve it:** A model that maintains high AUROC and Average Precision on a test set comprising cell types completely excluded from the training data.

### Open Question 2
- **Question:** What architectures are required to integrate additional biological modalities (e.g., protein structure, sequences) into context-aware foundation models for therapeutics?
- **Basis in paper:** [explicit] The abstract and Conclusion highlight the "inability of (Li et al., 2024) to... incorporate additional modalities" as a limitation of current state-of-the-art approaches.
- **Why unresolved:** Current geometric deep learning methods are engineered for specific inputs (e.g., cell-type specific PPI networks) and cannot readily ingest disparate data types like sequences or 3D structures alongside interaction networks.
- **What evidence would resolve it:** A unified model that jointly processes interaction networks and sequence data, demonstrating superior performance over unimodal baselines on the single-cell drug-target nomination benchmark.

### Open Question 3
- **Question:** Which heuristic for generating negative samples yields the most reliable evaluation for TCR-epitope binding interaction prediction?
- **Basis in paper:** [explicit] Section A.3.1 notes that current benchmarks show a "lack of a reasonable heuristic for generating negative samples," leading to unsatisfactory or near-random performance in realistic experimental setups.
- **Why unresolved:** Model performance varies drastically (e.g., near-perfect vs. near-random AUROC) depending on whether negative samples are generated via random shuffling, experimental negatives, or external TCR pairing.
- **What evidence would resolve it:** Identification of a sampling strategy where model rankings remain consistent and robust (AUROC > 0.7) across "hard" splits involving unseen peptides.

## Limitations
- Performance comparisons limited to a single task (drug-target nomination) may not generalize to other biomedical applications
- Context-specific metrics lack external validation against clinical outcomes or established therapeutic benchmarks
- DSL's abstraction may constrain novel data transformations not anticipated by the framework
- Claims about impact on foundation model development remain aspirational without evidence of sustained adoption

## Confidence

- **High confidence:** API-first architecture and model server implementation are technically sound and directly verifiable through provided code examples
- **Medium confidence:** Performance gaps revealed by context-specific metrics are demonstrated convincingly for the drug-target nomination task, but broader generalizability remains unproven
- **Low confidence:** Claims about PyTDC's impact on foundation model development in therapeutics are aspirational rather than evidence-based

## Next Checks

1. **Benchmark reproducibility across tasks:** Replicate the drug-target nomination results, then apply the same evaluation pipeline to a different biomedical task (e.g., protein function prediction) to test framework generalizability.

2. **Clinical relevance validation:** Compare context-specific metric performance with downstream validation on clinical trial data or established therapeutic databases to assess real-world utility.

3. **Integration stress test:** Attempt to integrate a novel, high-dimensional biomedical dataset (e.g., spatial transcriptomics) using PyTDC's DSL to identify limitations in handling emerging data types.