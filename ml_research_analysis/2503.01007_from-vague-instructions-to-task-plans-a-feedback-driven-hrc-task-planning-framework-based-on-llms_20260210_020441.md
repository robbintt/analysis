---
ver: rpa2
title: 'From Vague Instructions to Task Plans: A Feedback-Driven HRC Task Planning
  Framework based on LLMs'
arxiv_id: '2503.01007'
source_url: https://arxiv.org/abs/2503.01007
tags:
- task
- planning
- code
- execution
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IteraPlan, a feedback-driven human-robot collaboration
  (HRC) task planning framework that uses large language models (LLMs) to handle vague,
  high-level human instructions. Unlike prior approaches requiring detailed structured
  prompts or explicit task descriptions, IteraPlan uses a single concise prompt to
  decompose ambiguous natural language into executable task plans.
---

# From Vague Instructions to Task Plans: A Feedback-Driven HRC Task Planning Framework based on LLMs

## Quick Facts
- arXiv ID: 2503.01007
- Source URL: https://arxiv.org/abs/2503.01007
- Reference count: 29
- Primary result: Achieves 1.00 optimal code generation and successful execution rates across most tasks after iterative refinements in AI2-THOR simulation

## Executive Summary
This paper introduces IteraPlan, a feedback-driven framework that enables robots to interpret vague human instructions and generate executable task plans using large language models (LLMs). The framework handles ambiguous natural language by decomposing tasks into structured subtasks through a two-stage LLM process, then iteratively refines code based on execution errors and human feedback. Evaluated across kitchen, living room, bedroom, and bathroom environments in AI2-THOR simulation, IteraPlan demonstrates high success rates with minimal prompt engineering, achieving optimal code generation at 1.00 and successful execution at 1.00 for most tasks after 2-5 refinements.

## Method Summary
IteraPlan processes vague human instructions through a pipeline that begins with two-stage task decomposition using LLMs. The first stage generates an initial task description from the vague input, while the second stage filters relevant objects and refines the plan based on environmental context. Code generation translates the structured subtasks into Python scripts using a skill library of atomic actions. The framework incorporates human feedback at multiple stages and uses iterative error-based refinement when execution fails in simulation. Task allocation assigns skills to human or robot agents using rule-based heuristics, with an ablation study showing rule-based allocation (CASR 1.00) outperforms LLM-based allocation (CASR 0.76).

## Key Results
- Optimal code generation (OCR) reaches 1.00 with 2-3 refinements using GPT-4o across all environments
- Successful execution (SER) reaches 1.00 across most tasks after iterative refinements, with Kitchen tasks requiring 4-5 refinements due to complexity
- Rule-based task allocation achieves 100% Correctly Allocated Skills Rate (CASR) versus 76% for LLM-based allocation

## Why This Works (Mechanism)

### Mechanism 1
Two-stage task decomposition improves feasibility while reducing prompt complexity. Stage 1 generates an initial task description from vague input using basic environment context. Stage 2 filters to relevant objects/locations before refinement, preventing information overload while ensuring environmental compatibility. This works because LLMs can self-correct when given focused, task-relevant context rather than exhaustive scene descriptions.

### Mechanism 2
Iterative error-based code refinement closes the simulation-to-execution gap. Execution failures generate structured error messages from AI2-THOR, which are parsed into prompts that guide the LLM to modify code sequences, select alternative skills, or reorder operations until success. This works because error messages contain sufficient signal for LLMs to infer correct corrective actions without explicit debugging rules.

### Mechanism 3
Rule-based affordance heuristics outperform LLM-based allocation for skill-to-agent assignment. Pre-computed affordance values for atomic actions aggregate into skill-level affordances via heuristics. The LLM only performs syntax modification for agent assignment, not affordance reasoning. This works because human-robot capability differences are sufficiently captured by discrete affordance values that rule-based logic can process deterministically.

## Foundational Learning

- **Skill composition from atomic actions**: Tasks map to executable code through a skill library where each skill is a sequence of atomic actions (e.g., SliceObject = GoTo + Pickup + Slice + Throw). Understanding this hierarchy is essential for debugging generated plans. Quick check: Given the atomic actions in Table I, what skill sequence would "put an apple in a cabinet" require?

- **Affordance-based capability modeling**: Task allocation depends on mapping atomic actions to agent capabilities via affordance values. This determines whether human or robot executes each skill. Quick check: If a robot cannot reach high shelves but a human can, how would this constraint manifest in affordance values for "PutObject" skills targeting upper cabinets?

- **LLM prompt refinement loops**: The framework relies on iterative prompt updates—Stage 2 filtering, human feedback during code review, and error-driven refinement. Recognizing when and how prompts change is critical for system debugging. Quick check: What is the difference between Stage 2 refinement prompts and error-driven refinement prompts in terms of information source?

## Architecture Onboarding

- **Component map**: Vague instruction → Stage 1 decomposition → Stage 2 refinement → Code generation → Human approval loop → Simulation execution → Error-driven refinement loop → Affordance computation → Agent assignment → Final execution

- **Critical path**: Vague instruction → Stage 1 decomposition → Stage 2 refinement → Code generation → Human approval loop → Simulation execution → Error-driven refinement loop → Affordance computation → Agent assignment → Final execution

- **Design tradeoffs**:
  - Rule-based vs LLM-based allocation: Rule-based achieves 1.00 CASR but requires manual affordance engineering; LLM-based (0.76 CASR) offers flexibility but introduces allocation errors
  - Single concise prompt vs detailed structured prompts: Generalizes across tasks/environments with minimal engineering but relies on LLM's zero-shot reasoning
  - Human-in-loop vs autonomous refinement: Human feedback ensures preference alignment but adds latency; full autonomy risks misaligned plans

- **Failure signatures**:
  - Non-existent skill references: Generated code calls skills not in Table II → immediate execution failure
  - Object hallucination: Code references objects not in current floorplan → AI2-THOR returns object not found error
  - Allocation failure: LLM-based allocation assigns skill to incapable agent → OER drops to 0.62, 15/40 experiments fail or run suboptimally
  - Zero-shot stochasticity: GPT-3.5 OCR at 0 refinements shows 0.32±0.25 (high variance) vs GPT-4o's 0.32±0.25; stabilizes at 2+ refinements

- **First 3 experiments**:
  1. Kitchen "I feel hungry" task with GPT-4o: Run full pipeline across 5 kitchen floorplans. Measure OCR and SER at each refinement count (0-5). Expected: OCR reaches 1.00 by 2-3 refinements; SER reaches 1.00 by 3-5 refinements due to kitchen complexity.
  2. Ablation: LLM-based vs rule-based allocation: Take 10 generated scripts from Component 3. Run both allocation methods. Compare CASR and OER. Expected: Rule-based achieves 1.00 on both; LLM-based shows ~0.76 CASR with failures in Kitchen/Bathroom tasks.
  3. Stochasticity test with single prompt: Run "tidy up living room" task 10 times with GPT-3.5 and GPT-4o using identical single-prompt setup. Measure variance in OCR/SER at 0-2 refinements. Expected: High variance at 0-1 refinements stabilizing at 2+; GPT-4o more consistent than GPT-3.5.

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs be improved to perform affordance-based task allocation at levels comparable to rule-based methods? The ablation study shows GPT-4o achieves only 76% CASR versus 100% for rule-based allocation, and the authors explicitly state "its performance in reasoning over affordances for task and skill allocation remains mediocre." This remains unresolved as the paper does not propose methods to improve LLM-based affordance reasoning.

### Open Question 2
How does IteraPlan perform in real-world physical robot deployments beyond AI2-THOR simulation? All experiments are conducted in simulation with "a perfect agent" assumption. The paper mentions real-world compatibility motivations but provides no physical robot evaluation, leaving the simulation-to-reality gap untested.

### Open Question 3
What is the relationship between task complexity and the number of refinements required, particularly explaining why Kitchen tasks demand significantly more iterations? Table IV shows Kitchen tasks require 4-5 refinements to reach 1.00 SER across all models, while other environments reach 1.00 SER at 2-3 refinements. The paper notes "This is due to tasks being more complex in the Kitchen" without detailed analysis.

### Open Question 4
How does the framework's performance and user burden scale when human feedback availability is limited or delayed? The framework requires human feedback at multiple stages, but practical HRC scenarios may involve distracted or unavailable users. No experiments test feedback latency, reduced feedback frequency, or autonomous recovery when feedback is unavailable.

## Limitations
- Performance in real-world settings remains unverified as all experiments were conducted in controlled AI2-THOR simulation
- Framework still requires significant manual engineering for critical components like affordance computation despite achieving full LLM autonomy goals
- Framework's scalability to complex, multi-step tasks involving numerous objects and agents is not thoroughly evaluated

## Confidence

**High Confidence**: Iterative refinement mechanism effectiveness is well-supported by experimental data showing OCR improvement from 0.32 to 1.00 with 2-3 refinements across multiple tasks and environments. Advantage of rule-based task allocation over LLM-based allocation (CASR 1.00 vs 0.76) is clearly demonstrated through direct comparison.

**Medium Confidence**: Framework's generalizability across different environments is supported by consistent performance metrics, though evaluation only covered 5 floorplans per environment type. Single concise prompt sufficiency relies on GPT-4o's capabilities, but GPT-3.5 requires more refinements, suggesting approach may not generalize equally well across LLM variants.

**Low Confidence**: Framework's scalability to complex, multi-step tasks is not thoroughly evaluated. Paper does not address computational overhead of iterative refinement loops or real-time constraints in practical HRC scenarios. Human feedback mechanism's effectiveness depends on user expertise, but this dependency is not quantified.

## Next Checks

1. **Real-world transfer validation**: Implement IteraPlan on a physical robot system (e.g., Fetch or similar) in a real kitchen environment. Compare success rates against AI2-THOR performance to quantify simulation-to-reality gaps, particularly focusing on object recognition accuracy and physical interaction reliability.

2. **Long-horizon task evaluation**: Design and test multi-stage tasks requiring 10+ sequential skills (e.g., "prepare a full meal from raw ingredients") across all four environment types. Measure success rates, execution time, and refinement counts to assess scalability and identify failure modes in complex task sequences.

3. **User expertise impact study**: Conduct a controlled experiment with participants of varying technical backgrounds (novice to expert) providing feedback during the code review phase. Measure the correlation between user expertise and final success rates to quantify the framework's dependency on human feedback quality and identify opportunities for automated preference learning.