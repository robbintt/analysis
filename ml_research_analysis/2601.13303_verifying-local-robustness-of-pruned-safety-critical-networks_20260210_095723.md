---
ver: rpa2
title: Verifying Local Robustness of Pruned Safety-Critical Networks
arxiv_id: '2601.13303'
source_url: https://arxiv.org/abs/2601.13303
tags:
- pruned
- pruning
- neural
- networks
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how neural network pruning affects formal\
  \ local robustness verification in safety-critical domains. Using ResNet4 models\
  \ and the \u03B1,\u03B2-CROWN verifier, we evaluate pruning ratios from 10% to 80%\
  \ on MNIST and NASA JPL Mars Frost datasets."
---

# Verifying Local Robustness of Pruned Safety-Critical Networks

## Quick Facts
- arXiv ID: 2601.13303
- Source URL: https://arxiv.org/abs/2601.13303
- Reference count: 17
- Primary result: Optimal pruning ratios for verified L∞ robustness vary significantly between datasets (40% MNIST vs 70-80% JPL)

## Executive Summary
This paper investigates how neural network pruning affects formal local robustness verification in safety-critical domains. Using ResNet4 models and the α,β-CROWN verifier, the authors evaluate pruning ratios from 10% to 80% on MNIST and NASA JPL Mars Frost datasets. Results show a non-linear relationship: light pruning (40%) improves verification on MNIST, while heavy pruning (70%-90%) is optimal for JPL. This suggests that reduced connectivity simplifies the search space for formal solvers, with optimal ratios varying significantly between datasets. The findings demonstrate that pruned networks can outperform unpruned baselines in proven L∞ robustness properties, offering critical insights for deploying formally verified, efficient DNNs in high-stakes environments.

## Method Summary
The authors train ResNet4 models on MNIST and JPL Mars Frost datasets to fixed accuracy thresholds (99.4% and 84% respectively). They apply global unstructured L1 magnitude pruning to convolutional layers at ratios from 10% to 80%, followed by finetuning to restore accuracy. Verification is performed using α,β-CROWN with 5-minute timeout per input, testing 100 inputs per model across multiple L∞ epsilon bounds. Results are aggregated across 10 random seeds per configuration.

## Key Results
- Light pruning (40%) improves verified robustness properties on MNIST compared to unpruned baseline
- Heavy pruning (70-80%) is optimal for JPL Mars Frost dataset verification
- Verified property counts vary non-linearly with pruning ratio, with different optimal points for each dataset
- Accuracy-preserving finetuning maintains comparable performance across pruning ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reduced network connectivity simplifies the formal verification search space, enabling more properties to be proven within computational limits.
- Mechanism: Magnitude-based pruning removes weights with smallest absolute values, reducing the number of active decision boundaries the verifier must explore. Fewer non-zero weights means fewer branching conditions in the constraint-solving process, narrowing the feasible region that α,β-CROWN must search.
- Core assumption: The weights pruned (smallest magnitude) contribute minimally to decision boundaries relevant to local robustness; this is plausible but not proven in the paper.
- Evidence anchors:
  - [abstract] "This suggests that reduced connectivity simplifies the search space for formal solvers"
  - [section 1.1] "pruning them would have minimal impact on the model"
  - [corpus] Related work (Lipschitz Constant paper) suggests high compression can increase adversarial susceptibility, indicating pruning effects are not uniformly positive—mechanism may depend on pruning criterion.

### Mechanism 2
- Claim: Optimal pruning ratio for verifiability varies non-linearly with dataset characteristics, not model size or accuracy alone.
- Mechanism: Different datasets impose different verification complexity profiles. MNIST (simple, grayscale digits) maintains verifiability under light pruning but degrades under heavy pruning. JPL Mars Frost (complex terrain imagery, smaller epsilon bounds) shows monotonically improving verifiability up to 70-80% pruning. Assumption: Dataset-specific feature complexity determines how much redundancy can be removed before decision boundaries become jagged or undefined.
- Evidence anchors:
  - [section 3.2] "We find the result surprising because the optimal pruning ratios across the datasets differ significantly."
  - [section 3] "optimal pruning ratio is 40-50% for MNIST and 70-80% for JPL"
  - [corpus] No directly comparable cross-dataset pruning verification studies found; this finding is novel but underexplored.

### Mechanism 3
- Claim: Accuracy-preserving finetuning after pruning can maintain or improve verifiability even when substantial weights are removed.
- Mechanism: The paper trains to fixed accuracy thresholds (99.4% MNIST, 84% JPL) before comparison, controlling for accuracy as a confound. Finetuning redistributes representational capacity across remaining weights, potentially creating smoother decision regions that are easier to verify.
- Evidence anchors:
  - [section 2.2] "We stopped training once the test set accuracy reached a threshold... enabling fairer comparisons"
  - [table 1] All pruning ratios achieve comparable accuracy within standard deviation
  - [corpus] S2AP paper notes adversarial pruning requires finetuning to preserve robustness; mechanism aligns but is not directly confirmed.

## Foundational Learning

- Concept: **L∞ Local Robustness Verification**
  - Why needed here: Understanding that verification proves "no adversarial example exists within ε-ball" is essential to interpret the results. This is a formal guarantee, not empirical robustness.
  - Quick check question: Given an input x and ε=0.01, what does a successful verification guarantee about all inputs within the L∞ ball around x?

- Concept: **Magnitude-Based (L1) Pruning**
  - Why needed here: The paper uses global unstructured pruning of smallest-absolute-value weights. Understanding this is different from structured pruning (removing entire channels) or gradient-based pruning is critical for replication.
  - Quick check question: Why might removing the smallest 40% of weights by magnitude preserve accuracy better than removing 40% randomly?

- Concept: **α,β-CROWN Verifier**
  - Why needed here: This is a bound-propagation-based verifier using branch-and-bound. Knowing it won VNN-COMP 2021-2025 establishes credibility, but understanding its computational limits (5-minute timeout per input in this study) is essential for interpreting "properties verified" counts.
  - Quick check question: What happens to the count of verified properties if the timeout is reduced from 5 minutes to 30 seconds?

## Architecture Onboarding

- Component map: ResNet4 -> Train to accuracy threshold -> Magnitude pruning (10%-80%) -> Finetune to accuracy threshold -> α,β-CROWN verification -> Aggregate verified properties
- Critical path: 1. Train baseline models to target accuracy (99.4% MNIST, 84% JPL) 2. Apply pruning at specified ratio to convolutional layers 3. Finetune to recover accuracy 4. Run verification on 100 inputs per model at all ε values 5. Aggregate verified counts across 10 seeds per configuration
- Design tradeoffs:
  - Only convolutional layers pruned (residual/FC more sensitive per [section 1.1] citing Li et al.)
  - Fixed accuracy threshold ensures fair comparison but may hide accuracy-verifiability Pareto frontier
  - 5-minute timeout per input balances thoroughness with experimental feasibility; production settings may need longer
- Failure signatures:
  - Heavy pruning on MNIST (70-80%) drops verified properties from 81.6 to 51.2 at ε=0.006
  - High variance in results (±22.3 baseline, ±28.9 at ε=0.007 for MNIST) indicates seed sensitivity
  - If finetuning fails to reach accuracy threshold, that configuration is invalid for comparison
- First 3 experiments:
  1. **Baseline replication**: Train 3 ResNet4 models on MNIST with seeds 10, 20, 30; verify at ε=0.006; confirm verified count ~81±22
  2. **Pruning sweep**: Apply 40% and 70% pruning to baseline models; finetune; verify; confirm 40% improves (to ~84) while 70% degrades (to ~66)
  3. **Dataset transfer test**: Train ResNet4 on a new binary classification dataset (e.g., CIFAR-2 subset); run pruning sweep to determine if optimal ratio clusters with MNIST or JPL pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do optimal pruning ratios for local robustness verification differ significantly between datasets (e.g., MNIST vs. JPL Mars Frost)?
- Basis in paper: [explicit] Page 6 states, "We believe this warrants an investigation into why different datasets yield different optimal pruning ratios."
- Why unresolved: The paper identifies the divergence (40-50% for MNIST vs. 70-80% for JPL) but does not provide a theoretical or empirical explanation for what dataset characteristics drive this difference.
- What evidence would resolve it: A comparative analysis linking dataset properties (e.g., dimensionality, feature complexity) to changes in the network's decision boundary geometry during pruning.

### Open Question 2
- Question: How do different pruning criteria (e.g., gradient-based vs. magnitude) and granularities (structured vs. unstructured) impact certified local robustness?
- Basis in paper: [explicit] Page 6 notes that other settings "might have different impacts across tasks and, if true, calls for explanations of these findings."
- Why unresolved: The methodology was restricted to global unstructured L1 magnitude pruning; other criteria were not tested.
- What evidence would resolve it: Comparative experiments measuring verified robustness on the same benchmarks using diverse pruning algorithms.

### Open Question 3
- Question: Does the relationship between pruning and improved verifiability persist in other safety-critical domains such as high-frequency trading or medical AI?
- Basis in paper: [explicit] Page 6 lists these as targets: "We will investigate this in areas such as high-frequency trading, FDA-approved AI medical devices..."
- Why unresolved: The conclusions are drawn exclusively from image recognition tasks (MNIST and Mars Frost).
- What evidence would resolve it: Application of the described verification-pruning pipeline to time-series or medical diagnostic datasets.

## Limitations

- Study uses only two datasets with very different complexity profiles, limiting generalizability of optimal pruning ratios
- ResNet4 architecture specification is incomplete, particularly regarding channel widths and residual block structure
- Verification results depend heavily on the 5-minute timeout per input in α,β-CROWN, which may not reflect production verification requirements

## Confidence

- **High confidence**: The finding that light pruning (40%) improves verification on MNIST while heavy pruning (70%-80%) is optimal for JPL. The experimental methodology is sound, and the results are internally consistent.
- **Medium confidence**: The mechanism explanation that reduced connectivity simplifies the formal verification search space. While plausible, the paper doesn't directly measure search complexity or prove that pruned weights minimally impact decision boundaries.
- **Low confidence**: Extrapolating optimal pruning ratios to other datasets or safety-critical domains. With only two datasets tested, this claim requires additional validation.

## Next Checks

1. **Dataset generalization test**: Apply the same pruning and verification pipeline to a third dataset with intermediate complexity (e.g., CIFAR-10 subset) to determine if optimal pruning ratios follow the MNIST or JPL pattern, or reveal a new regime.

2. **Architecture sensitivity analysis**: Vary ResNet4 architecture parameters (channel widths, residual structure) while maintaining the same pruning methodology to determine if verification improvements are architecture-dependent.

3. **Timeout sensitivity study**: Re-run verification with varied timeout limits (30 seconds, 2 minutes, 10 minutes) on a subset of models to quantify how computational budget affects verified property counts across different pruning ratios.