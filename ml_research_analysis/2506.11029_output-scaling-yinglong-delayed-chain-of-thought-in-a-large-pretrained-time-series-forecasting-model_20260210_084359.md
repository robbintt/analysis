---
ver: rpa2
title: 'Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time
  Series Forecasting Model'
arxiv_id: '2506.11029'
source_url: https://arxiv.org/abs/2506.11029
tags:
- forecasting
- time
- series
- crps
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint forecasting framework that unifies
  direct and recursive time series prediction methods. The key innovation is delayed
  chain-of-thought (DCoT) reasoning, enabled by a non-causal, bidirectional transformer
  trained with masked token recovery.
---

# Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model

## Quick Facts
- arXiv ID: 2506.11029
- Source URL: https://arxiv.org/abs/2506.11029
- Reference count: 33
- Key outcome: Joint forecasting framework using delayed chain-of-thought reasoning; longer forecasts yield higher accuracy, validated on GIFT-Eval, ETT, and Weather datasets with MASE/CRPS improvements.

## Executive Summary
This paper introduces YingLong, a non-causal, bidirectional transformer for time series forecasting that unifies direct and recursive prediction methods. The key innovation is delayed chain-of-thought (DCoT) reasoning, where extending the prediction horizon beyond the target window improves accuracy. The model is trained via masked token recovery and validated across diverse datasets, achieving state-of-the-art performance. A multi-input ensemble strategy further enhances robustness, and models ranging from 6M to 300M parameters are released.

## Method Summary
YingLong is an encoder-only U-Net style Transformer that processes time series as patches and predicts future values via masked token recovery. It uses bidirectional attention (breaking causality) to allow future "reasoning" tokens to refine earlier predictions. The model is trained on ~78B time points and evaluated on ETT, Weather, and GIFT-Eval datasets. Key innovations include DCoT for output scaling and a multi-input ensemble to reduce variance.

## Key Results
- Output scaling effect: Longer forecasts (up to 4096 tokens) significantly improve accuracy due to delayed chain-of-thought reasoning.
- MASE improvements: 14% rank improvement over prior methods on GIFT-Eval.
- CRPS improvements: 44% rank improvement over prior methods on GIFT-Eval.
- SOTA performance: Achieves over 60% best performance across benchmarks.

## Why This Works (Mechanism)

### Mechanism 1: Output Scaling via Delayed Chain-of-Thought (DCoT)
- **Claim:** Extending the prediction horizon improves the accuracy of target predictions.
- **Mechanism:** Bidirectional attention allows future placeholder tokens to attend to historical data and the target, refining the target representation via information flow back from DCoT tokens.
- **Core assumption:** Time series dynamics benefit from a "global" view where future context acts as a computational buffer for refining signal processing.
- **Evidence anchors:** [abstract] "longer outputs significantly enhance model accuracy due to delayed chain-of-thought reasoning"; [Section 5.1] "positioning CoT after the target is a delayed CoT (DCoT)... tokens on the right can influence those on the left."
- **Break condition:** If attention is restricted to a causal mask, this mechanism fails.

### Mechanism 2: Joint Forecasting as Mask Recovery
- **Claim:** Formulating forecasting as masked token recovery preserves inter-horizon dependencies and reduces error accumulation.
- **Mechanism:** The model treats the forecast horizon as masked patches, allowing direct mutual conditioning between predictions at different steps.
- **Core assumption:** Time series forecasting is framed more effectively as a "language understanding" task (filling in blanks) rather than a "language generation" task (predicting next word).
- **Evidence anchors:** [abstract] "YingLong is a non-causal, bidirectional attention encoder-only transformer trained through masked token recovery..."
- **Break condition:** If horizons are predicted independently or strictly sequentially, the mutual conditioning benefit is lost.

### Mechanism 3: Variance Reduction via Multi-Input Ensembling
- **Claim:** Aggregating forecasts from varying input lookback windows stabilizes predictions against horizon-specific biases.
- **Mechanism:** Averaging outputs from different input lengths (and reversed "mirror" inputs) cancels out horizon-specific variance.
- **Core assumption:** No single lookback window is optimal for all frequencies or horizons within a dataset.
- **Evidence anchors:** [abstract] "...tackling output variance with a multi-input ensemble."
- **Break condition:** If the time series lacks stationarity or distinct frequency components, the ensemble offers diminishing returns.

## Foundational Learning

- **Concept: Bidirectional vs. Causal Attention**
  - **Why needed here:** Standard LLMs use causal masks; YingLong explicitly breaks this to allow future tokens to influence past targets.
  - **Quick check question:** Can a token at position $t$ attend to a token at position $t+5$ in the YingLong architecture? (Answer: Yes).

- **Concept: Patching in Time Series**
  - **Why needed here:** The model processes time series not point-by-point, but by grouping points into "patches" (tokens), similar to vision transformers.
  - **Quick check question:** Does the model predict the next floating-point number directly, or a vector representing a segment of time? (Answer: A vector/patch).

- **Concept: Quantile Regression for Probabilistic Forecasting**
  - **Why needed here:** The model outputs a distribution via quantile predictions (e.g., 10th, 50th, 90th percentiles), not a single "best guess" value.
  - **Quick check question:** Does the loss function optimize for the average error (MSE) or the likelihood across a distribution of outcomes? (Answer: Weighted quantile loss).

## Architecture Onboarding

- **Component map:** Input Layer (Patches time series) -> Encoder (U-Net style Transformer with token merging and skip connections) -> Output Layer (Parallel projection heads for multiple quantiles) -> Inference Wrapper (Handles DCoT extension and Multi-Input Ensemble).
- **Critical path:** The *Inference Configuration* is most critical; manually extend output sequence length (e.g., to 4096) even if the target forecast is shorter (e.g., 96) to trigger the DCoT effect.
- **Design tradeoffs:**
  - Vanilla vs. U-Transformer: DCoT works on both, but U-Net yields marginal gains compared to massive gains from DCoT.
  - Compute vs. Accuracy: Longer DCoT sequences require more memory and compute (quadratic attention scaling) for a single forward pass, traded for higher accuracy without retraining.
- **Failure signatures:**
  1. Causal Leakage: Standard HuggingFace causal mask drops performance; must be bidirectional.
  2. Trend Drift: Without DCoT, model may capture seasonality but drift on long-term trend errors.
  3. Ensemble Overhead: Without efficient batching, ensemble strategy linearly increases latency.
- **First 3 experiments:**
  1. DCoT Ablation: Run inference with output lengths of 96, 512, and 4096; plot MSE/MAE drop as output length increases.
  2. Baseline Comparison: Compare 300M model against PatchTST or TimesNet on GIFT-Eval using MASE/CRPS metrics.
  3. Structural Ablation: Disable U-Net skip connections to measure performance gap and decide if architectural complexity is justified.

## Open Questions the Paper Calls Out
- How can the latent reasoning process of DCoT be interpreted or visualized to match the explicitness of textual Chain of Thought in LLMs?
- Does increasing the pretraining dataset size beyond 78 billion time points yield proportional performance gains?
- To what extent does DCoT performance depend on the presence of periodic behavior or low-frequency structures in the target data?

## Limitations
- The theoretical justification for why DCoT works remains under-specified; the paper demonstrates the effect but does not fully explain the information flow dynamics.
- The ensemble strategy adds computational overhead without a clear analytical framework for when it will be most beneficial.
- The claim of being the first foundation model for time series forecasting is difficult to evaluate objectively due to lack of clear definition in the field.

## Confidence
- **High Confidence:** The output scaling effect (DCoT mechanism) and its empirical demonstration across multiple datasets.
- **Medium Confidence:** The claim that this approach unifies direct and recursive forecasting paradigms.
- **Low Confidence:** The assertion that this is the first foundation model for time series forecasting.

## Next Checks
1. **DCoT Mechanism Dissection:** Systematically vary output sequence length (e.g., 96, 512, 1024, 2048, 4096) on a held-out test set and plot accuracy metrics to identify the point of diminishing returns.
2. **Ensemble Strategy Optimization:** Implement a learned weighting scheme for ensemble components and compare against simple averaging to determine if added complexity is justified.
3. **Theoretical Analysis of Bidirectional Attention:** Conduct an ablation study where the attention mask is gradually relaxed from causal to fully bidirectional to quantify the contribution of future token influence in the DCoT mechanism.