---
ver: rpa2
title: Suboptimal Shapley Value Explanations
arxiv_id: '2502.12209'
source_url: https://arxiv.org/abs/2502.12209
tags:
- feature
- baselines
- shapley
- sampling
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the suboptimality of random and conditional
  baselines in Shapley value-based feature attribution for deep neural networks. It
  identifies that problematic baselines create asymmetric feature interactions with
  significant directional bias toward model outputs, leading to misleading explanations.
---

# Suboptimal Shapley Value Explanations

## Quick Facts
- arXiv ID: 2502.12209
- Source URL: https://arxiv.org/abs/2502.12209
- Reference count: 35
- Primary result: Random and conditional baselines introduce asymmetric feature interaction bias in Shapley explanations; uncertainty-based reweighting improves faithfulness

## Executive Summary
This paper identifies a fundamental problem with standard Shapley value baselines in NLP: random and conditional replacements can create asymmetric feature interactions that bias feature importance rankings. The paper proposes that the optimal baseline should satisfy p(y|x'i) = p(y), which is generalized to maximizing label space entropy to avoid explicit probability estimation. A simple uncertainty-based reweighting mechanism is introduced to accelerate computation. Experiments on five NLP tasks with BERT and RoBERTa show improved faithfulness metrics compared to standard baselines, though explanations still show limited overlap with human intuition.

## Method Summary
The paper addresses biased feature attributions in Shapley explanations by introducing uncertainty-based reweighting. Instead of standard random or conditional sampling, each replacement feature x'i is weighted by its normalized predictive entropy Ĥ(x'i) = H(x'i)/log₂(L), where L is the label space size. This reweighting approximates the condition p(y|x'i) = p(y) without requiring explicit probability estimation. The mechanism is applied to both random and conditional sampling strategies, with random sampling plus reweighting (random-uw) often outperforming conditional sampling. The approach maintains the same sampling size (1000) but improves faithfulness metrics (LOR, SF, CM) across BERT and RoBERTa models on five NLP tasks.

## Key Results
- Uncertainty-based reweighting (random-uw) consistently outperforms standard random and conditional baselines in faithfulness metrics
- LOR, SF, and CM metrics show improvement across all five tasks (SST-2, SNLI, SNIPS, Yelp-2, 20Newsgroup)
- Baseline-explanation correlations with humans remain low (0.071-0.231) despite improved faithfulness
- GPT-4 explanations show significantly higher human correlation (0.710-0.856) than any model-based explanation

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Interaction Bias
Standard baselines introduce directional bias because replacement features (x'i) can interact with context (xS) more strongly than original features. When x'i has higher positive influence on output y than original xi, marginal contribution φS(i) becomes artificially low. The optimal baseline should be uninformative regarding y (p(y|x'i) = p(y)), minimizing interaction with context.

### Mechanism 2: Uncertainty-Based Reweighting
Samples are weighted by predictive entropy to minimize influence of problematic replacements with specific label information. High entropy replacements (uncertain predictions) are weighted higher, while low entropy replacements (confident predictions) are down-weighted. This approximates label-independence without explicit density estimation.

### Mechanism 3: Generalization of Uninformativeness
Avoiding marginal label distribution estimation p(y) enables practical computation of optimal baseline properties. Local uninformativeness is enforced by maximizing entropy of output probability vector, rather than requiring global density estimation.

## Foundational Learning

- **Concept: Shapley Value Baselines (Missingness)**
  - Why needed here: Understanding how to represent "missing" features is crucial since Shapley values simulate absence by replacement
  - Quick check question: If you replace "bad" with "excellent" in a positive review, does Shapley value reflect "bad" importance or "excellent" influence?

- **Concept: Entropy as Uncertainty**
  - Why needed here: The solution relies on entropy H(Y|x) as proxy for "neutrality"
  - Quick check question: For 5-class classifier, does [0.2, 0.2, 0.2, 0.2, 0.2] represent better "neutral" baseline than [0.9, 0.025, 0.025, 0.025, 0.025]?

- **Concept: Faithfulness Metrics (LOR, Sufficiency, Comprehensiveness)**
  - Why needed here: Paper evaluates success by "faithfulness"—how well explanation reflects model's internal reasoning
  - Quick check question: If removing top 3 important words changes model prediction significantly, is explanation "faithful" under Comprehensiveness metric?

## Architecture Onboarding

- **Component map:** Input x -> Sampler (Random/Conditional) -> Uncertainty Scaler -> Value Estimator -> Aggregator
- **Critical path:** Implementation hinges on Eq. 22 - modify standard sampling loop to calculate entropy of perturbed input's prediction, normalize it, and use as coefficient for sample's contribution
- **Design tradeoffs:** Random sampling is faster with OOD samples (high variance); conditional sampling is slower but creates coherent sentences (lower variance, potentially higher bias). Random+reweighting often superior to conditional.
- **Failure signatures:** Low overlap with human explanations (0.1-0.2 correlation); OOD noise from random sampler may underweight valid baselines if model is unfamiliar
- **First 3 experiments:**
  1. Replicate "plot is fun but confusing" example to verify "confusing" rises in importance with Ĥ-weighting
  2. Run with small sampling sizes (m=10, 50, 100) to test if reweighting stabilizes rankings faster
  3. Implement LOR and SF metrics on BERT-base SST-2; compare random-uw vs. random for improved faithfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the disparity between model inference and human understanding be formally bridged to improve Shapley explanation utility?
- Basis: Conclusion states analysis "highlight[s] the disparity between model inference and human understanding"
- Why unresolved: Paper demonstrates gap through low overlap rates but doesn't propose alignment mechanism
- What evidence would resolve it: Training objective minimizing rank correlation difference between model and human attributions

### Open Question 2
- Question: Does uncertainty-based reweighting generalize to non-NLP domains like computer vision or tabular data?
- Basis: Experimental scope strictly limited to NLP tasks and Transformer architectures
- Why unresolved: Text features are discrete tokens, whereas images/tables have different structural dependencies affecting uncertainty calculation
- What evidence would resolve it: Application to CNNs on ImageNet or tree-based models on tabular benchmarks

### Open Question 3
- Question: Is entropy maximization robust as uninformativeness proxy in highly imbalanced datasets?
- Basis: Method avoids p(y) estimation by maximizing entropy to approximate p(y|x'i) = p(y)
- Why unresolved: High entropy forces uniform distribution assumption, which may be misleading when true p(y) is highly skewed
- What evidence would resolve it: Comparative analysis on synthetic imbalanced datasets using entropy proxy vs. explicit p(y) estimation

## Limitations
- Low correlation between model explanations and human understanding persists despite improved faithfulness metrics
- Entropy-based uninformativeness assumption may fail in severely imbalanced datasets where uniform distribution is informative
- OOD noise from random sampling may introduce artifacts if model encounters unfamiliar representations

## Confidence

- **High confidence:** Theoretical formulation of asymmetric interaction bias and empirical evaluation framework
- **Medium confidence:** Generalizability of entropy-based baseline selection across different model architectures and data distributions
- **Medium confidence:** Claims about disparity between model inference and human understanding based on correlation metrics

## Next Checks

1. Test entropy-based reweighting across diverse model architectures (CNNs, LSTMs) to verify generalization beyond transformers
2. Evaluate performance on datasets with severe class imbalance to assess robustness when high entropy predictions may be misleading
3. Compare against alternative baseline selection strategies that explicitly model feature-label correlations rather than relying solely on predictive entropy