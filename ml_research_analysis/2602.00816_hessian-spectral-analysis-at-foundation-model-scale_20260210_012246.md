---
ver: rpa2
title: Hessian Spectral Analysis at Foundation Model Scale
arxiv_id: '2602.00816'
source_url: https://arxiv.org/abs/2602.00816
tags:
- hessian
- lanczos
- vector
- curvature
- fsdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable framework for computing exact Hessian
  vector products under Fully Sharded Data Parallelism (FSDP), enabling second-order
  analysis at foundation model scale. The core method uses shard-local finite-difference
  Hessian vector products that operate directly on FSDP-sharded parameters without
  requiring full parameter gathers.
---

# Hessian Spectral Analysis at Foundation Model Scale

## Quick Facts
- arXiv ID: 2602.00816
- Source URL: https://arxiv.org/abs/2602.00816
- Authors: Diego Granziol; Khurshid Juarev
- Reference count: 28
- Primary result: First Hessian spectral density estimates at foundation model scale using scalable shard-local finite-difference HVPs under FSDP

## Executive Summary
This paper presents the first scalable framework for computing exact Hessian spectral densities at foundation model scale. By developing shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, the method avoids expensive parameter gathers while maintaining autograd compatibility. The approach enables stochastic Lanczos quadrature on models up to 100 billion parameters, revealing that widely used block-diagonal curvature approximations can fail catastrophically with order-one relative error and near-zero directional alignment in mid-scale transformers.

## Method Summary
The core innovation is a shard-preserving central finite-difference estimator for Hessian vector products that operates directly on FSDP-sharded parameters without global parameter gathers. The method applies perturbations locally to DTensor shards, performs two gradient passes over the same dataloader slice, and assembles the finite-difference HVP using only shard-local operations plus scalar all-reduces. This is combined with stochastic Lanczos quadrature where Lanczos vectors are stored in bfloat16 and scalar operations in float32. The spectral density is extracted from the tridiagonal matrix eigendecomposition, with optional reorthogonalization to control ghost eigenvalues.

## Key Results
- First Hessian spectral density estimation on models up to 100B parameters
- Block-diagonal curvature approximations show order-one relative error (0.94) and cosine similarity near zero (0.31) in mid-scale transformers
- Near-linear scaling across nodes with modest constant-factor overhead relative to first-order gradients
- Global curvature statistics stabilize with scale, showing more robust spectral structure in larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finite-difference Hessian vector products can be computed without global parameter gathers under FSDP by applying perturbations locally on each shard.
- Mechanism: The central finite-difference estimator Hv ≈ (∇L(θ+εv) − ∇L(θ−εv))/(2ε) operates on DTensor shards in-place. Two gradient passes over the same dataloader slice yield shard-local pieces of Hv, which are then scaled by (2ε)⁻¹. The only additional collective is a scalar all-reduce for normalization.
- Core assumption: The finite-difference bias remains acceptable at the chosen ε, and gradient computations remain stable under perturbation.
- Evidence anchors:
  - [abstract] "Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism...avoids costly parameter gathers while maintaining autograd compatibility."
  - [section 3.1] Algorithm 1 details the full procedure: perturb local shards, accumulate gradients, restore parameters—all without full gathers.
  - [corpus] HessFormer (Granziol et al., 2025) achieves similar HVP computation but is limited to single-node; this work extends to multi-node FSDP with 4× speedup (Table 5).
- Break condition: If ε is too small, floating-point noise dominates; if too large, truncation bias dominates. Theorem 3.1 quantifies the optimal ε* ∝ (ε_mach·||∇L||/||D³f||)^(1/3).

### Mechanism 2
- Claim: Stochastic Lanczos Quadrature (SLQ) with shard-preserving HVPs produces faithful spectral density estimates at 100B+ parameter scale.
- Mechanism: Lanczos builds a Krylov subspace via recurrence β_{k+1}q_{k+1} = Hq_k − α_k q_k − β_k q_{k-1}. Each iteration requires one HVP. After m steps, the tridiagonal T_m's Ritz values approximate eigenvalues, and spectral density is recovered via quadrature weights.
- Core assumption: Finite-difference errors are sufficiently unbiased and independent across Lanczos iterations that Krylov stability is preserved.
- Evidence anchors:
  - [abstract] "Hessian spectral density estimation was performed on models up to 100B parameters."
  - [section 4] Theorem 4.1 bounds the Ritz value perturbation under finite-difference HvPs: |λ̃_i − λ_i| ≲ ||H||² σ_f^{1/2} ||D⁴f||^{1/2} √(mη̄_P).
  - [corpus] No direct corpus validation for 100B-scale SLQ; related work (Tang et al. 2025) reports power-law spectra but only for sub-10B models.
- Break condition: Loss of orthogonality in finite precision creates "ghost" eigenvalues. Reorthogonalization (sliding window of size r) adds (2+r) scalar all-reduces per iteration.

### Mechanism 3
- Claim: Block-diagonal Hessian approximations exhibit order-one relative error and poor directional alignment in mid-scale LLMs, invalidating a core assumption of K-FAC, GPTQ, and influence-function approximations.
- Mechanism: Compare full HVP (h_full = Hv) to masked HVP (h_masked^{(b)} = Hv^{(b)}) where v^{(b)} zeros all components outside block b. Relative error ||(Hv)^{(b)} − (Hv^{(b)})^{(b)}|| / ||(Hv)^{(b)}|| quantifies cross-block coupling.
- Core assumption: Random probe vectors v ~ N(0,I) are representative of typical optimization directions.
- Evidence anchors:
  - [abstract] "Direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment."
  - [section 6.1] Table 6 shows relative error 0.941 ± 0.059 and cosine similarity 0.311 ± 0.120 for Qwen-0.6B.
  - [section 6.2] Table 7 shows relative error 1.214 ± 0.150 and cosine similarity 0.00443 ± 0.127 for DeepSeek-1.3B coder (fine-tuning regime).
  - [corpus] No corpus contradiction; ASDL/K-FAC systems (PipeFisher, KAISA) target approximations by design (Table 1).
- Break condition: Strong cross-layer coupling in curvature. The final transformer block shows higher cosine similarity (reduced downstream mixing), but most blocks show near-zero alignment.

## Foundational Learning

- Concept: **Hessian Vector Products (HVPs)**
  - Why needed here: The entire method builds on Hv as a primitive. Without understanding that Hv = ∇_θ(⟨∇_θ L, v⟩) can be computed in O(1) backprops (Pearlmutter 1994), the finite-difference approach won't make sense.
  - Quick check question: Can you explain why computing Hv via finite differences requires two gradient passes but never forms the full Hessian?

- Concept: **Lanczos Algorithm and Krylov Subspaces**
  - Why needed here: SLQ is the downstream consumer of HVPs for spectral density. Understanding the three-term recurrence and ghost eigenvalue phenomenon is essential for debugging numerical issues.
  - Quick check question: After m Lanczos iterations, what matrix is used to approximate H's spectrum, and why does loss of orthogonality matter?

- Concept: **FSDP (Fully Sharded Data Parallelism)**
  - Why needed here: The method's value proposition is avoiding parameter gathers under FSDP. Understanding ZeRO-3's all-gather/reduce-scatter pattern is necessary to see why naive HVPs are prohibitively expensive.
  - Quick check question: Under FSDP ZeRO-3, when are parameters gathered, and how does the shard-local HVP avoid changing this communication pattern?

## Architecture Onboarding

- Component map: Shard-local perturbation -> Dual gradient passes -> Finite-difference assembly -> Lanczos loop -> Spectral density extraction
- Critical path: The HVP primitive (Algorithm 1) dominates runtime. Each HVP costs T_HvP ≈ 2T_grad + T_vec where T_vec = O(P_loc γ) is shard-local vector ops. For SLQ with m steps and s probes, total is ≈ 2sm T_grad.
- Design tradeoffs:
  - **Precision**: bfloat16 Lanczos vectors reduce memory but require fp32 scalar ops. Paper shows minimal spectral deviation for ε ∈ [10⁻⁴, 10⁻³].
  - **Reorthogonalization**: Window size r adds (2+r) scalar all-reduces/iteration. Essential for spectral outliers, but increases latency.
  - **Subsampling**: 30× dataset increase doesn't meaningfully alter spectrum (Figure 7). Small calibration sets suffice for curvature-based quantization.
- Failure signatures:
  - **ε too small** (e.g., 10⁻⁵ in bf16): Spectrum resembles random matrix, no outlier separation (Figure 3).
  - **ε too large** (e.g., 10⁻²): Same random-matrix degradation (Figure 4).
  - **No reorthogonalization + many iterations**: Ghost eigenvalues with RMS splitting ∝ √k after Ritz convergence.
  - **FSDP on small models**: ZeRO-3 is provably slower than standard DP when model fits in memory (Appendix B: ~35% overhead).
- First 3 experiments:
  1. **Validate HVP correctness on small model**: Compute Hv via finite-difference and Pearlmutter's exact method on a single-GPU model. Verify ||Hv_fd − Hv_exact|| / ||Hv_exact|| matches Theorem 3.1 predictions for varying ε.
  2. **Spectral density sanity check**: Run SLQ on Pythia-160M with known spectral structure. Confirm ε ∈ [10⁻⁴, 10⁻³] produces clean outlier separation (replicate Figure 5/6). Test bf16 vs fp32 storage.
  3. **Block-diagonal error replication**: On Qwen-0.6B, compute full HVP and block-masked HVP for random probes. Verify relative error ≈ 0.9 and cosine similarity ≈ 0.3 (Table 6). Identify which layers have strongest/weakest cross-coupling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What curvature approximation structures can faithfully capture cross-layer coupling in large transformers while remaining computationally tractable?
- Basis in paper: [explicit] The paper demonstrates that block-diagonal assumptions incur "order-one relative error and poor directional alignment even in mid-scale LLMs," with cosine similarity near zero across most blocks (Table 7), indicating strong cross-layer coupling that current approximations discard.
- Why unresolved: The paper exposes the failure mode but does not propose or evaluate alternative structural approximations that could preserve accuracy while remaining scalable.
- What evidence would resolve it: Systematic comparison of alternative low-rank, banded, or hierarchical curvature approximations against exact HVPs across multiple model scales and tasks, measuring both approximation error and downstream impact on optimization/quantization.

### Open Question 2
- Question: Why do global curvature statistics stabilize with model scale when trained on the same dataset?
- Basis in paper: [explicit] The paper reports that "larger models trained on the same dataset are more stable in metrics of global curvature compared to their smaller counterparts" (Section 7.1, Figure 8), described as emergent regularity not previously reported.
- Why unresolved: The paper documents the empirical phenomenon but does not provide theoretical explanation for why spectral structure becomes more robust to finite-difference step size and subsampling variations at larger scales.
- What evidence would resolve it: Controlled experiments varying model width/depth independently, combined with theoretical analysis linking overparameterization to spectral concentration properties.

### Open Question 3
- Question: How does finite-difference local averaging bias affect downstream applications such as influence functions and second-order optimization?
- Basis in paper: [inferred] Theorem 3.3 shows finite-difference HVPs compute weighted averages over perturbations rather than pointwise curvature. While numerically validated for spectral estimation, the downstream consequences for CG solvers, influence analysis, and optimization remain uncharacterized.
- Why unresolved: The paper focuses on spectral density estimation validity but does not quantify how the smoothing bias propagates to inverse-HVP applications or curvature-conditioned training.
- What evidence would resolve it: End-to-end comparison of finite-difference versus exact HVPs (where computable) on tasks like influence attribution accuracy and optimizer convergence rates.

## Limitations
- Method scalability constraints: Runtime overhead remains significant (20-40× single forward-backward cost) despite avoiding parameter gathers
- Theoretical error characterization gaps: Error bounds validated on transformer LLMs but not established for other architectures
- Spectral interpretation challenges: Practical significance of spectral structure for optimization and generalization not fully established

## Confidence
- High confidence: Core computational framework and runtime scaling results are technically sound and well-validated
- Medium confidence: 100B+ parameter spectral density results are pioneering but have limited external validation
- Low confidence: Practical utility of exact Hessian analysis for foundation model development remains largely unproven

## Next Checks
- **Validation Check 1**: Apply Hessian spectral analysis to vision transformers, multimodal models, and non-LLM architectures to verify generalization beyond language models
- **Validation Check 2**: Implement second-order optimizer using exact Hessian for mid-scale models and compare convergence against first-order methods and approximations
- **Validation Check 3**: Use exact Hessian to compute influence functions and compare against block-diagonal approximations to measure impact on influence analysis quality