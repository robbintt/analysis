---
ver: rpa2
title: An exploration for higher efficiency in multi objective optimisation with reinforcement
  learning
arxiv_id: '2512.10208'
source_url: https://arxiv.org/abs/2512.10208
tags:
- operator
- learning
- selection
- search
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates improving efficiency in multi-objective
  optimisation through reinforcement learning (RL) based adaptive operator selection.
  The core challenge addressed is selecting optimal operators from a pool during search
  to avoid local optima and improve convergence speed.
---

# An exploration for higher efficiency in multi objective optimisation with reinforcement learning

## Quick Facts
- arXiv ID: 2512.10208
- Source URL: https://arxiv.org/abs/2512.10208
- Reference count: 33
- Primary result: RL-embedded ABC algorithm outperforms random and probability matching baselines on SUKP

## Executive Summary
This paper investigates improving efficiency in multi-objective optimisation through reinforcement learning (RL) based adaptive operator selection. The core challenge addressed is selecting optimal operators from a pool during search to avoid local optima and improve convergence speed. The method uses RL to dynamically map problem states to operators based on accumulated rewards, enabling adaptive selection during search. For multi-objective problems, the approach extends single-objective RL credit assignment to handle multiple objectives via union-based operator ranking. Experiments on Set Union Knapsack Problem (SUKP) show the RL-embedded ABC algorithm (RLABC) significantly outperforms random and probability matching baselines. The work is ongoing, with cross-problem and multi-objective generalisation as future directions.

## Method Summary
The paper proposes a reinforcement learning approach for adaptive operator selection in combinatorial optimization. It frames operator selection as a Markov Decision Process where Q-learning maps problem states to operators based on accumulated rewards. For multi-objective problems, the method extends single-objective credit assignment to handle multiple objectives through union-based operator ranking. The approach uses Hard-C-Means clustering to store and update Q-values, representing successful operators with state clusters. The system incorporates binarification to enable potential cross-problem transfer learning by translating different problem types into a common binary space.

## Key Results
- RL-embedded ABC algorithm (RLABC) outperforms baseline approaches (Random, Probability Matching) on SUKP instances
- Transfer learning across SUKP instances reduces computational time when reusing RL experience
- Multi-objective credit assignment framework proposed but not experimentally validated

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Credit Assignment via Q-Learning
The system models operator selection as a Markov Decision Process with Q-learning maintaining a value function $Q(x, o_i)$ representing expected utility of applying operator $o_i$ to state $x$. The update rule incorporates reward and estimated value of subsequent state, using Hard-C-Means clustering to approximate Q-values by associating successful operators with state clusters $c_i$.

### Mechanism 2: Generalization via Binarification
Problem states are transformed into binary space to enable learned experiences to transfer across structurally different problem instances. This aligns state space across different domains, allowing the RL agent to recognize similar search circumstances regardless of underlying problem type.

### Mechanism 3: Multi-Objective Credit Aggregation (Proposed/Theoretical)
The system maintains a set $Q_j$ for each objective $j$ and proposes aggregating these via weighted sum or Pareto front analysis. This allows operators to be credited for improving one objective even if they degrade another.

## Foundational Learning

- **Concept: Adaptive Operator Selection (AOS)**
  - Why needed here: The paper builds on AOS as the foundational problem of choosing the best heuristic operator during search
  - Quick check question: Can you explain why "Probability Matching" might fail in a dynamic search landscape compared to a state-aware method?

- **Concept: Markov Decision Processes (MDPs) in Optimization**
  - Why needed here: The search process is framed as an MDP where "state" is the current solution and "action" is the operator choice
  - Quick check question: In the context of this paper, what constitutes the "State" ($S$) and the "Action" ($A$) in the MDP tuple?

- **Concept: Transfer Learning**
  - Why needed here: The core novelty is generalizing experience by reusing pre-trained Q-tables or cluster models
  - Quick check question: What is the risk of "negative transfer" when applying experiences learned on a SUKP instance to a Job Shop Scheduling instance?

## Architecture Onboarding

- **Component map:** Base Optimizer (ABC) -> State Encoder (Binarifier) -> RL Agent (Q-learning with clustering) -> Operator Pool -> Evaluator
- **Critical path:** The State Encoder $\to$ Credit Update Loop. If the binary state representation does not effectively correlate with operator success, the cluster centers ($c_i$) will not converge, and the agent cannot guide the search.
- **Design tradeoffs:**
  - Weighted Sum vs. Pareto (Eq. 7): The paper suggests weighted sum for computational efficiency but may fail to find diverse Pareto front
  - Hard-C-Means vs. Deep Q-Networks: Uses clustering model for simplicity and interpretability, potentially limiting scalability to high-dimensional states
- **Failure signatures:**
  - Credit Oscillation: Rewards fluctuate wildly without convergence, suggesting learning rate is too high or state representation is unstable
  - Single-Objective Collapse: In multi-objective mode, if one objective dominates weighted sum, algorithm reverts to single-objective optimization
  - Negative Transfer: Performance drops significantly when reusing experience compared to learning from scratch
- **First 3 experiments:**
  1. Baseline Validation (Single-Obj): Reproduce SUKP experiments comparing Random, Probability Matching, and RLABC to verify credit assignment logic
  2. Transfer Learning Validity: Test RLABC-TL variant to verify previous experience reduces convergence time on similar SUKP instance
  3. Multi-Objective Stub Implementation: Implement proposed Eq. 7 with weighted sum on bi-objective SUKP to check credit vector $Q_j$ updates correctly

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed "binarification" of state spaces successfully facilitate cross-problem transfer learning, allowing experiences gained on one problem type (e.g., TSP) to improve search efficiency on a structurally different problem type (e.g., Job Shop Scheduling)? The paper states that while transfer learning across instances has been achieved, "cross-problem transfer learning... remain unsolved" and "cross-problem type utilisation is outstanding."

### Open Question 2
What is the most effective method for aggregating multi-objective credit scores (e.g., weighted sum vs. Pareto front) to select operators without diminishing the impact of individual objectives? The authors note that using a weighted sum "may end up with some reductions in the impact of each objective" and state that implementation approaches have "many open questions outstanding."

### Open Question 3
Does the proposed multi-objective reinforcement learning formulation empirically improve operator selection efficiency compared to baseline adaptive methods? The paper explicitly states that "Multi-objective optimisation cases have not been experimented yet, therefore no results will be demonstrated in this regard."

## Limitations

- No explicit performance metrics provided in the text
- Limited experimental scope restricted to Set Union Knapsack Problem (SUKP)
- Multi-objective extension remains theoretical with no experimental validation
- Missing hyperparameter details (learning rate, discount factor, colony size) prevent independent verification

## Confidence

- Single-objective RL mechanism: Medium
- Transfer learning claims: Low (unproven binary state alignment assumption)
- Multi-objective extension: Low (no experimental validation)

## Next Checks

1. Implement and benchmark the RLABC algorithm on SUKP instances, comparing against Random and Probability Matching baselines with statistical significance testing
2. Validate the transfer learning mechanism by testing RLABC-TL across similar SUKP instances to measure convergence speed gains versus learning from scratch
3. Prototype the multi-objective credit aggregation (Eq. 7) on a bi-objective SUKP to verify correct credit vector updates before scaling to full benchmarks