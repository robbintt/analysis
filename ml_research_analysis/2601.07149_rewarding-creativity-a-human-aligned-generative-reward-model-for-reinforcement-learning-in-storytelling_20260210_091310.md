---
ver: rpa2
title: 'Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement
  Learning in Storytelling'
arxiv_id: '2601.07149'
source_url: https://arxiv.org/abs/2601.07149
tags:
- story
- reward
- genrm
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RLCS, a reinforcement learning framework
  for creative storytelling that addresses two key challenges: designing reliable
  reward signals for subjective storytelling quality and mitigating training instability.
  The authors develop a Generative Reward Model (GenRM) that provides multi-dimensional
  analysis and explicit reasoning about story preferences through supervised fine-tuning
  on reasoning chains and GRPO-based refinement.'
---

# Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling

## Quick Facts
- arXiv ID: 2601.07149
- Source URL: https://arxiv.org/abs/2601.07149
- Reference count: 40
- RLCS achieves 68% alignment with human creativity judgments and outperforms Gemini-2.5-Pro in story quality

## Executive Summary
This paper introduces RLCS, a reinforcement learning framework for creative storytelling that addresses two key challenges: designing reliable reward signals for subjective storytelling quality and mitigating training instability. The authors develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences through supervised fine-tuning on reasoning chains and GRPO-based refinement. They also introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions. Experiments show that GenRM achieves 68% alignment with human creativity judgments and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality.

## Method Summary
RLCS combines a generative reward model with entropy-based reward shaping for creative storytelling. The GenRM is trained in two stages: first through supervised fine-tuning on chain-of-thought reasoning distilled from Gemini-2.5-Pro annotations of human story pairs, then refined with GRPO on preference data. The reward shaping strategy uses token-level entropy to classify samples into four categories (confident/uncertain × correct/incorrect) and assigns differential weights to prioritize informative samples. The story policy is trained using GRPO with group-relative advantages, eliminating the need for a separate value model. The framework is evaluated on story generation tasks using Qwen-72B as the policy model.

## Key Results
- GenRM achieves 68% alignment with human creativity judgments, substantially outperforming discriminative baselines
- RLCS with entropy shaping achieves 58.9% win rate over uniform weighting variant, with smoother training curves
- RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative reward models with explicit chain-of-thought reasoning outperform discriminative scalar-based reward models for subjective creative tasks.
- Mechanism: GenRM generates articulated natural language analysis evaluating stories across multiple dimensions (plot coherence, character development, creative originality) before producing a preference judgment, providing richer gradient signal for policy optimization than single scalar scores.
- Core assumption: Explicit reasoning captures nuances of creative quality that scalar rewards compress away.
- Evidence anchors: [abstract] "GenRM achieves 68% alignment with human creativity judgments"; [section 5.2] GenRM 32B (68.3%) substantially outperforms discriminative Bradley-Terry model (54.1%), and even 7B model (64.5%) surpasses the discriminative baseline.

### Mechanism 2
- Claim: Entropy-based reward shaping focusing on confident errors and uncertain correct predictions improves training stability over uniform sample weighting.
- Mechanism: The model computes token-level entropy H for each generated output, classifies samples into four categories using median entropy threshold, and assigns differential weights (1.5 for confident errors and uncertain correct, 0.5 for confident correct, 1.0 for uncertain errors) to concentrate learning on informative samples.
- Core assumption: Confident errors reveal systematic misconceptions; uncertain correct predictions indicate emerging capabilities; confident correct patterns are already mastered.
- Evidence anchors: [abstract] "dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns"; [section 5.4] RLCS with entropy shaping achieves 58.9% win rate over uniform weighting variant.

### Mechanism 3
- Claim: Two-stage training (SFT cold-start followed by GRPO refinement) produces better reward models than either stage alone.
- Mechanism: SFT on high-quality CoT demonstrations establishes basic task-following and reasoning generation; GRPO on expanded preference data refines alignment with human preferences through reinforcement learning with binary correctness rewards.
- Core assumption: Cold-start capabilities from SFT provide stable initialization for subsequent RL refinement.
- Evidence anchors: [section 5.2] GRPO refinement provides consistent 4.9%-6.3% improvement over SFT-only across all model scales; [corpus] Consistent with broader RLHF literature showing staged training improves stability.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO eliminates the need for a separate value model by computing advantages relative to a group of sampled responses, critical when reward signals are sparse and noisy in creative domains.
  - Quick check question: Can you explain how GRPO computes the advantage Ai,t differently from PPO?

- Concept: **Position Bias in LLM Judgments**
  - Why needed here: LLM judges exhibit systematic bias toward stories presented first; the paper addresses this by evaluating both (s1, s2) and (s2, s1) orderings and requiring consistency.
  - Quick check question: Why does swapping story order and requiring agreement improve data quality?

- Concept: **Bradley-Terry Preference Model**
  - Why needed here: Traditional discriminative reward models use BT formulation to convert pairwise preferences to scalar rewards; understanding this baseline clarifies why generative reasoning offers advantages.
  - Quick check question: What information does a BT model discard that GenRM preserves?

## Architecture Onboarding

- Component map: Human Annotations (4,000 pairs) → CoT Distillation (Gemini-2.5-Pro) → Consistency Filtering → SFT Dataset (~1,400 samples) → GenRM SFT Cold-Start → Multi-Model Consensus Data → GRPO Refinement → Trained GenRM → Entropy-Based Reward Shaping → Story Policy GRPO Training

- Critical path: CoT distillation quality → consistency filtering stringency → SFT cold-start stability → GenRM accuracy → policy training reward quality.

- Design tradeoffs:
  - Higher filtering stringency reduces data quantity but improves SFT quality
  - Larger rollout groups (G) improve advantage estimation but increase compute
  - Entropy weighting improves stability but requires tuning thresholds per task

- Failure signatures:
  - Low agreement between swapped-order predictions indicates position bias contamination
  - Flat or decreasing reward curves during GRPO suggest over-regularization or reward hacking
  - High variance in entropy-weighted rewards may indicate threshold instability

- First 3 experiments:
  1. Replicate GenRM SFT-only baseline on a subset to validate cold-start quality before GRPO.
  2. Ablate entropy weighting (use uniform weights) to isolate its contribution to training stability.
  3. Test GenRM on held-out expert-annotated pairs early to detect overfitting to training preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RLCS framework effectively generalize to other creative domains like poetry or screenplay writing?
- Basis in paper: [explicit] The authors state in the Limitations section that "its applicability to other creative domains such as poetry or screenplay writing requires further investigation."
- Why unresolved: The current reasoning dimensions (e.g., plot coherence) are storytelling-specific; different domains require distinct structural and aesthetic evaluation criteria.
- What evidence would resolve it: Successful application and evaluation of the GenRM architecture on poetry or screenplay generation tasks.

### Open Question 2
- Question: Is the entropy-based reward shaping strategy robust enough to function without task-specific hyperparameter tuning?
- Basis in paper: [explicit] The paper notes the strategy "may require task-specific tuning when adapted to different creative generation scenarios."
- Why unresolved: The specific weight multipliers (e.g., 1.5 for confident errors) may be heuristics optimal only for the current story generation distribution.
- What evidence would resolve it: Ablation studies on new tasks showing whether default parameters maintain stability or require modification.

### Open Question 3
- Question: To what extent does the GenRM capture universal creativity versus the specific biases of its annotators?
- Basis in paper: [explicit] The authors acknowledge the model "may not fully capture all aesthetic perspectives or cultural contexts" beyond the specific annotators.
- Why unresolved: Alignment with a specific group of professional screenwriters does not ensure the model generalizes to diverse cultural or stylistic preferences.
- What evidence would resolve it: Cross-demographic evaluation of the model's judgments against diverse human preference groups.

## Limitations
- Position-bias filtering threshold sensitivity is unclear, with 10% of human annotations discarded as ambiguous but no ablation of filtering stringency provided.
- Entropy weighting calibration requires task-specific tuning, with specific weight multipliers acknowledged as potentially suboptimal for domains beyond creative writing.
- Generalization beyond creative writing domains is unproven, with reasoning-based rewards and entropy shaping not validated on factual or technical domains.

## Confidence

- **High confidence**: GenRM achieves 68% alignment with human creativity judgments; RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality; two-stage training (SFT cold-start followed by GRPO refinement) produces better reward models than either stage alone.
- **Medium confidence**: Entropy-based reward shaping focusing on confident errors and uncertain correct predictions improves training stability over uniform sample weighting; GRPO with group relative advantages eliminates need for separate value model in sparse reward settings.
- **Low confidence**: Explicit chain-of-thought reasoning captures nuances of creative quality that scalar rewards compress away; confident errors reveal systematic misconceptions while uncertain correct predictions indicate emerging capabilities.

## Next Checks

1. **Cross-domain robustness test**: Apply RLCS framework to a non-story domain (e.g., code generation or dialogue) with minimal architectural changes to assess whether reasoning-based rewards and entropy shaping generalize beyond creative writing.

2. **Threshold sensitivity analysis**: Systematically vary the position-bias filtering threshold (0.7 to 0.95) and entropy weighting parameters to quantify their impact on GenRM accuracy and training stability across multiple seeds.

3. **Human preference verification**: Conduct a user study where human judges evaluate whether RLCS-generated stories are preferred over Gemini-2.5-Pro outputs in blinded pairwise comparisons, measuring both quality and creativity dimensions separately.