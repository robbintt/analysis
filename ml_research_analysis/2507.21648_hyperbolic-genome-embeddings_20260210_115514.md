---
ver: rpa2
title: Hyperbolic Genome Embeddings
arxiv_id: '2507.21648'
source_url: https://arxiv.org/abs/2507.21648
tags:
- hyperbolic
- sequence
- datasets
- sequences
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces hyperbolic convolutional neural networks (HCNNs)
  to improve genomic sequence representation by leveraging the evolutionary hierarchical
  structure of DNA. Unlike traditional Euclidean models, HCNNs embed sequences on
  hyperbolic manifolds, enabling better capture of phylogenetic and regulatory relationships.
---

# Hyperbolic Genome Embeddings

## Quick Facts
- **arXiv ID:** 2507.21648
- **Source URL:** https://arxiv.org/abs/2507.21648
- **Reference count:** 40
- **One-line primary result:** HCNNs outperform standard CNNs on 37 of 42 benchmark datasets while using far fewer parameters than DNA language models.

## Executive Summary
This paper introduces hyperbolic convolutional neural networks (HCNNs) for genomic sequence representation, leveraging the evolutionary hierarchical structure inherent in DNA. By embedding sequences on hyperbolic manifolds rather than Euclidean space, HCNNs better capture phylogenetic and regulatory relationships. The method demonstrates superior performance across 42 benchmark datasets, including seven Genome Understanding Evaluation (GUE) datasets, while requiring orders of magnitude fewer parameters than state-of-the-art DNA language models. A novel Transposable Elements Benchmark is introduced, and synthetic experiments reveal HCNNs excel at distinguishing evolutionary signal from noise.

## Method Summary
The method embeds one-hot encoded DNA sequences onto hyperbolic manifolds using the Lorentz (hyperboloid) model, where the exponential map projects data into a curved space that naturally represents hierarchical relationships. The architecture consists of Lorentz convolution layers, Lorentz batch normalization, and a Lorentz multinomial logistic regression head. Training uses RiemannianAdam optimization to handle gradient updates on the manifold, with curvature K as a learnable parameter. Two variants are explored: HCNN-S (single curvature) and HCNN-M (multiple curvatures per block). The approach is evaluated on 42 benchmark datasets including Transposable Elements Benchmark (TEB), Genome Understanding Evaluation (GUE), and Genomic Benchmarks (GB).

## Key Results
- HCNNs outperform standard CNNs on 37 of 42 benchmark datasets
- Surpasses state-of-the-art DNA language models on seven of 28 GUE datasets despite using far fewer parameters
- Introduced Transposable Elements Benchmark to test evolutionary active region classification
- Synthetic experiments show HCNNs excel at distinguishing evolutionary signal from random noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If genomic sequences contain latent hierarchical structures (e.g., evolutionary trees), hyperbolic manifolds provide a more efficient embedding space than Euclidean space.
- **Mechanism:** The negative curvature of hyperbolic space allows for the low-distortion embedding of tree-like data where volume grows exponentially. By mapping DNA sequences to the Lorentz model (a hyperboloid), the model implicitly captures phylogenetic relationships without explicit tree mapping.
- **Core assumption:** Genomic datasets possess an innate "hyperbolicity" or tree-like structure that is relevant to the learning task.
- **Evidence anchors:**
  - [abstract] "enabling better capture of phylogenetic and regulatory relationships."
  - [section 5.2] "25 out of 43 sequence datasets have significantly lower δ values than the baseline... supporting the hypothesis that genomic sequence data may possess an innate hyperbolicity."
  - [corpus] Weak support; related papers focus on standard Euclidean genomic language models or benchmarking libraries, lacking specific validation of hyperbolic geometry in this context.
- **Break condition:** Performance degrades on tasks where the underlying data structure is not tree-like (e.g., purely combinatorial interactions) or where hierarchy is absent.

### Mechanism 2
- **Claim:** Hyperbolic inductive biases appear particularly effective at separating sequences generated by evolutionary processes from random background noise.
- **Mechanism:** The geometry aligns with the data-generating process (evolutionary trees), allowing the model to distinguish "signal" (tree-generated) from "noise" (random) more effectively than Euclidean models which lack this structural bias.
- **Core assumption:** The classification task involves distinguishing functional/evolutionary sequences from non-functional background sequences.
- **Evidence anchors:**
  - [abstract] "experiments with synthetic data reveal that HCNNs excel when distinguishing evolutionary signal from noise."
  - [section 5.1] "Table 1 suggests that this bias is particularly beneficial in Scenario C, where it aids in uncovering the underlying phylogenetic tree structure in the presence of noise."
  - [corpus] No direct corpus validation for this specific signal-to-noise mechanism in genomics.
- **Break condition:** Tasks requiring fine-grained intra-tree differentiation (Scenario A), where the signal is not defined by the presence of a tree structure relative to noise.

### Mechanism 3
- **Claim:** Hyperbolic geometry enables high expressivity with fewer parameters, potentially allowing lightweight models to compete with much larger Euclidean foundation models.
- **Mechanism:** The inherent capacity of hyperbolic space to model complex hierarchies reduces the need for massive width/depth to approximate these relationships, enabling compact models.
- **Core assumption:** The hierarchical features are the dominant complexity in the data.
- **Evidence anchors:**
  - [abstract] "surpasses state-of-the-art... while using orders of magnitude fewer parameters and avoiding pretraining."
  - [section 1] "HCNNs outperform DNA language models (LMs) in seven of the 28 GUE datasets... despite having fewer parameters."
  - [corpus] Contextual support exists in corpus papers noting the resource intensity of current Genomic Foundation Models (GFMs), suggesting a need for efficiency.
- **Break condition:** When tasks require vast general knowledge not captured by hierarchical structure alone, where large-scale pretraining in Euclidean space dominates.

## Foundational Learning

- **Concept: Lorentz Model of Hyperbolic Geometry**
  - **Why needed here:** This paper uses the Lorentz (hyperboloid) model, not the Poincaré ball. You must understand operations on the forward sheet of a hyperboloid in Minkowski space to implement the layers.
  - **Quick check question:** How does the Lorentzian inner product differ from the standard Euclidean dot product, and how does it constrain points to the hyperboloid?

- **Concept: Inductive Bias**
  - **Why needed here:** The core argument is that the model's geometry is a prior (bias) matching the data's evolutionary structure. Understanding this helps diagnose why HCNNs fail on non-hierarchical tasks (like the Covid variant task).
  - **Quick check question:** Why would a standard CNN struggle to represent a perfect binary tree in low dimensions compared to a hyperbolic neural network?

- **Concept: δ-Hyperbolicity**
  - **Why needed here:** The paper uses this metric to empirically validate that genomic data is actually tree-like. You need this to verify if a new dataset is a good candidate for an HCNN.
  - **Quick check question:** Does a low δ-hyperbolicity value indicate a dataset is more tree-like or less tree-like?

## Architecture Onboarding

- **Component map:** Input (one-hot DNA) -> Projection (Exponential map to Lorentz manifold) -> Encoder (stacked LConv + LBN + Activation) -> Head (Flatten -> Lorentz MLR)

- **Critical path:**
  - **Optimization:** You cannot use standard Adam; you must use **RiemannianAdam** (or RSGD) to handle gradient updates on the manifold.
  - **Curvature ($K$):** This is a learnable parameter. Monitor if $K$ diverges, as it defines the "shape" of your space.
  - **Mapping:** For HCNN-M (multi-curvature), ensure numerical stability in the exponential/logarithmic maps when transferring points between manifolds of different $K$.

- **Design tradeoffs:**
  - **HCNN-S vs. HCNN-M:** HCNN-S (Single $K$) is more stable and easier to debug. HCNN-M (Multiple $K$) offers representational flexibility but introduces instability via manifold mapping operations.
  - **Dimensionality:** Hyperbolic embeddings are often effective in lower dimensions; increasing channel width may yield diminishing returns compared to Euclidean CNNs.

- **Failure signatures:**
  - **Performance collapse on Scenario A tasks:** If the task is distinguishing variants of a single lineage (like Covid variants), the HCNN performs worse than Euclidean CNNs because the distinction is clade-level, not signal-vs-noise.
  - **NaN gradients:** Likely due to instability in `exp`/`log` maps or operations pushing points off the manifold during HCNN-M training.

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Generate a "Tree vs. Random" dataset (Scenario C). Verify HCNN separates classes while CNN struggles.
  2. **Benchmark Alignment:** Run HCNN-S on the **Transposable Elements Benchmark (TEB)** to verify the hypothesis that evolutionarily active regions benefit most from hyperbolic geometry.
  3. **Hyperbolicity Analysis:** Compute $\delta$-hyperbolicity on your target dataset's embeddings. If $\delta$ is not significantly lower than a Euclidean baseline, an HCNN is likely the wrong choice.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the statistical behavior of $\delta$-distributions be formalized to account for "emergent hyperbolicity" and the influence of high dimensionality?
- **Basis in paper:** [Explicit] The authors state that "critical challenges persist: formalizing the behavior of $\delta$-distributions statistically... and formalizing its relationship with curvature and dimensionality" to clarify applicability.
- **Why unresolved:** Current methods using single-point estimates ($\delta_{worst}$, $\delta_{avg}$) fail to capture the complexity of high-dimensional spaces, where "emergent hyperbolicity" artificially lowers $\delta$ values, leading to miscalibrated measurements.
- **What evidence would resolve it:** A theoretical framework or robust metric that accurately quantifies hyperbolicity in high-dimensional embeddings, validated against baseline manifolds of matching dimensionality.

### Open Question 2
- **Question:** To what extent is hyperbolic representation learning applicable to tasks requiring fine-grained intra-tree differentiation versus noise filtering?
- **Basis in paper:** [Inferred] The authors note that while HCNNs excel at Scenario C (tree identification/filtering noise), they perform poorly on the COVID task, which resembles Scenario A (intra-tree differentiation), suggesting the inductive bias may not support all hierarchical tasks equally.
- **Why unresolved:** The paper demonstrates a performance gap between evolutionary signal detection and specific variant classification tasks but does not fully isolate the geometric reasons for this failure.
- **What evidence would resolve it:** Ablation studies on synthetic datasets that systematically vary the signal-to-noise ratio and the depth of hierarchical separation required for classification.

### Open Question 3
- **Question:** Can hyperbolic CNNs be effectively integrated with pretraining strategies to outperform current state-of-the-art DNA language models?
- **Basis in paper:** [Explicit] The conclusion suggests that HCNNs are "lightweight, modular models with the scalability to produce competitive DNA LMs, offering additional performance gains through pretraining."
- **Why unresolved:** The experiments primarily benchmark scratch-trained HCNNs against massive pretrained Euclidean models; the potential of a *pretrained* hyperbolic model remains unexplored.
- **What evidence would resolve it:** Benchmarking a pretrained HCNN (trained on the human genome or 1000 Genomes) against models like Nucleotide Transformer and DNABERT-2 on the GUE benchmark.

## Limitations

- **Architectural specification gaps:** The exact architectural details for the "small" baseline CNN are not numerically specified, making precise reproduction challenging.
- **Domain generalizability:** While the paper demonstrates strong results on genomic sequence classification, the generalizability of hyperbolic benefits to other genomic domains (e.g., protein sequences, non-coding RNA) remains partially untested.
- **Pretraining potential unexplored:** The paper benchmarks scratch-trained HCNNs against large pretrained Euclidean models, leaving the potential of pretrained hyperbolic models unexplored.

## Confidence

- **High Confidence:** The core claim that hyperbolic geometry improves genomic sequence representation is strongly supported by the extensive benchmark results (37/42 datasets showing improvement) and the empirical validation of δ-hyperbolicity in genomic data.
- **Medium Confidence:** The mechanism explaining why hyperbolic geometry excels at separating evolutionary signal from noise is plausible but requires more direct experimental validation beyond the synthetic data experiments.
- **Medium Confidence:** The claim about parameter efficiency compared to large DNA language models is supported by results on GUE datasets, but broader comparisons across different model scales would strengthen this claim.

## Next Checks

1. **Replicate the Synthetic Signal-Noise Experiment:** Generate a new synthetic dataset with controlled tree-like structure and noise levels to verify the HCNN's ability to distinguish evolutionary signal from background noise across different signal-to-noise ratios.

2. **Extended Hyperbolicity Analysis:** Compute δ-hyperbolicity on additional genomic datasets not covered in the paper (e.g., protein sequences, non-coding RNA) to determine if the observed tree-like structure is specific to certain genomic domains.

3. **Ablation Study on Manifold Curvature:** Systematically vary the initial curvature K and observe its impact on performance across different benchmark types to better understand how curvature affects learning in different genomic contexts.