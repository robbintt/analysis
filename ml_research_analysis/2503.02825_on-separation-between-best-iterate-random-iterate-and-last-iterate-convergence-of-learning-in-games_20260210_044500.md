---
ver: rpa2
title: On Separation Between Best-Iterate, Random-Iterate, and Last-Iterate Convergence
  of Learning in Games
arxiv_id: '2503.02825'
source_url: https://arxiv.org/abs/2503.02825
tags:
- convergence
- have
- omwu
- dualitygap
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes convergence properties of Optimistic Multiplicative
  Weights Update (OMWU) in two-player zero-sum games. The key problem is understanding
  whether OMWU achieves uniform random-iterate or best-iterate convergence despite
  recent results showing it lacks uniform last-iterate convergence.
---

# On Separation Between Best-Iterate, Random-Iterate, and Last-Iterate Convergence of Learning in Games

## Quick Facts
- **arXiv ID**: 2503.02825
- **Source URL**: https://arxiv.org/abs/2503.02825
- **Reference count**: 40
- **Primary result**: OMWU achieves O(T^{-1/6}) best-iterate convergence in 2×2 games but only Ω(1/log T) random-iterate convergence, establishing fundamental separations between convergence modes.

## Executive Summary
This paper analyzes convergence properties of Optimistic Multiplicative Weights Update (OMWU) in two-player zero-sum games, revealing that best-iterate, random-iterate, and last-iterate convergence are fundamentally distinct. The authors prove that OMWU does not achieve polynomial uniform random-iterate convergence, with a lower bound of Ω(1/log T) even for 2×2 games with fully-mixed Nash equilibria. However, they show OMWU achieves O(T^{-1/6}) uniform best-iterate convergence for 2×2 games, demonstrating a separation between convergence modes previously unknown. The work challenges conventional wisdom that these convergence modes are essentially equivalent.

## Method Summary
The paper employs a novel two-phase analysis approach. The global phase establishes O(T^{-1/4}δ^{-1/2}) universal random-iterate convergence through connections to dynamic regret and interval regret, valid for general d₁×d₂ zero-sum games. The initial phase shows OMWU achieves fast convergence to an iterate with duality gap O(δ) in early iterations, with all initial iterates having best-iterate convergence rate of O(log²t/t). For the negative result, the authors construct a parametric family of 2×2 games Aδ and analyze OMWU's trajectory to identify persistent "bad" blocks of high-duality-gap iterates, deriving the Ω(1/log T) lower bound.

## Key Results
- OMWU does not achieve polynomial uniform random-iterate convergence, with Ω(1/log T) lower bound even for 2×2 games with fully-mixed Nash equilibria
- OMWU achieves O(T^{-1/6}) uniform best-iterate convergence for 2×2 games with fully-mixed Nash equilibrium
- These results demonstrate a separation between best-iterate and random-iterate convergence that was previously unknown

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The uniform random-iterate convergence rate of OMWU in 2×2 zero-sum games is bounded below by Ω(1/log T), precluding polynomial convergence.
- Mechanism: The proof constructs a parametric family of 2×2 games Aδ. On this instance, OMWU exhibits a sustained block of Θ(1/δ) iterations, each with a constant duality gap (Ω(1)). For entropy regularizers, the time to reach this "bad" block is T = O(log(1/δ)/δ). Translating this to the random-iterate measure yields the Ω(1/log T) lower bound.
- Core assumption: The regularizer must satisfy unbiasedness (Fη,R(0)=0.5) and rational limiting behavior (Assumption 2).
- Evidence anchors: [abstract] "OMWU does not achieve polynomial uniform random-iterate convergence, with a lower bound of Ω(1/log T) even for 2×2 games with fully-mixed Nash equilibria"; [section 3.1] "...for some T = Ω(1/δ) iterations, there will be a block of Θ(1/δ) iterations each with a constant duality gap"
- Break condition: The bound fails if a non-entropy regularizer significantly alters the stability property fR(δ).

### Mechanism 2
- Claim: OMWU achieves a universal O(T⁻¹/⁴δ⁻¹/²) random-iterate convergence rate in general d₁×d₂ zero-sum games with a fully mixed equilibrium (with minimum probability δ).
- Mechanism: The analysis connects random-iterate convergence (average duality gap) to social dynamic regret. It leverages two properties: (1) bounded interval regret O(1/δ) for any interval I, and (2) sublinear loss variation O(√|I|). These imply a sublinear dynamic regret bound O(T³/⁴δ⁻¹/²) by partitioning T rounds into an optimal number of intervals.
- Core assumption: The Nash equilibrium is fully mixed, ensuring iterates stay away from the simplex boundary, which is critical for the interval regret bound.
- Evidence anchors: [abstract] "Global phase: establishes O(T⁻¹/⁴δ⁻¹/²) universal random-iterate convergence using connections to dynamic regret and interval regret"; [section 4.2, Theorem 4] "...the social dynamic regret is bounded by O( (d1 + d2) log(d1d2) / (η · T3/4δ-1/2) )."
- Break condition: The bound degrades if the equilibrium is not fully mixed (δ → 0).

### Mechanism 3
- Claim: For 2×2 zero-sum games with a fully mixed equilibrium, OMWU achieves a uniform O(T⁻¹/⁶) best-iterate convergence rate.
- Mechanism: A novel two-phase analysis. The **Initial Phase** shows that within the first T₁ iterations, OMWU reaches an iterate with duality gap O(δ) while maintaining a fast O(log²t/t) best-iterate rate. The **Global Phase** provides the universal O(T⁻¹/⁴δ⁻¹/²) bound. The best-iterate convergence is the minimum of these two, which optimizes to O(T⁻¹/⁶), independent of δ.
- Core assumption: The game is restricted to the 2×2 case, and the Nash equilibrium satisfies certain structural conditions (Assumption 1) which are made WLOG for analysis.
- Evidence anchors: [abstract] "OMWU achieves an O(T⁻¹/⁶) uniform best-iterate convergence rate for 2×2 games with fully-mixed Nash equilibria"; [section 4.1] "...combining the analysis in the initial phase and the global phase as follows: min{δ, O(T⁻¹/⁴δ⁻¹/²)} ≤ O(T⁻¹/⁶)."
- Break condition: The O(T⁻¹/⁶) rate is not guaranteed for games larger than 2×2, as the initial phase analysis exploits specific 2×2 structure.

## Foundational Learning

- **Concept: Duality Gap**
  - Why needed here: This is the core metric used to measure proximity to Nash equilibrium in all three convergence modes. A zero duality gap is equivalent to being at a Nash equilibrium.
  - Quick check question: What is the duality gap of a strategy profile (x, y), and what does it imply if the gap is zero?

- **Concept: Dynamic Regret & Interval Regret**
  - Why needed here: The positive result (Mechanism 2) links random-iterate convergence to the minimization of dynamic and interval regret. Understanding this connection is crucial for the proof.
  - Quick check question: How does dynamic regret differ from static (external) regret, and how is it related to the sum of duality gaps?

- **Concept: OMWU Algorithm (OFTRL with Entropy Regularizer)**
  - Why needed here: The paper analyzes OMWU specifically. Understanding its update rule and the role of the entropy regularizer is essential to grasp why it behaves differently from algorithms like OGDA.
  - Quick check question: What regularizer does OMWU use, and what is its closed-form update rule for the probability of an action?

## Architecture Onboarding

- **Component map**: Hard Instance Construction (Aδ) -> Trajectory Analysis -> Lower Bound Proof / Two-Phase Analysis (Initial Phase -> Global Phase) -> Best-Iterate Convergence Rate
- **Critical path**: The negative result relies on understanding how the entropy regularizer's properties (quantified by fR(δ)) control the timing and length of the bad-iterate block. The positive result follows from the two-phase proof; engineers should first grasp the dynamic regret connection (Theorem 4) and then the structural analysis of the 2×2 case in the initial phase (Theorem 5).
- **Design tradeoffs**: The paper reveals a critical tradeoff in **regularizer selection**. Entropy (OMWU) leads to poor uniform random-iterate convergence in simple 2×2 games, while squared Euclidean norm yields an even worse Ω(1) lower bound. The choice of regularizer profoundly impacts the type and rate of convergence achievable.
- **Failure signatures**: Key failures include **recurrence or chaotic behavior** in learning dynamics (which iterate convergence avoids) and the **lack of uniform convergence** even when universal convergence exists, as bounds dependent on condition number δ become weak for ill-conditioned problems.
- **First 3 experiments**:
  1. **Replicate Figure 1**: Implement OMWU (and other OFTRL variants) on the 2×2 game Aδ for a small δ (e.g., 10⁻²) and plot the average social dynamic regret to visualize different convergence behaviors.
  2. **Empirical Rate Extraction**: Run OMWU on various 2×2 games with fully mixed equilibria and estimate the empirical best-iterate convergence rate to test the O(T⁻¹/⁶) theoretical bound.
  3. **Test Regularizer Sensitivity**: Implement OFTRL with different regularizers (entropy, squared Euclidean, Tsallis entropy) on Aδ and observe how the random-iterate convergence rate changes as predicted by Theorem 2 and Lemma 8.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does OMWU achieve polynomial uniform best-iterate convergence in general $d_1 \times d_2$ games? The authors state, "Extending our positive result beyond the 2-by-2 case is an interesting future direction." Why unresolved: The proof of the $O(T^{-1/6})$ rate relies heavily on the specific structure of $2 \times 2$ games. What evidence would resolve it: A proof extending the uniform best-iterate rate to arbitrary dimensions or a lower bound showing impossibility in higher dimensions.

- **Open Question 2**: Can the proposed two-phase analysis be applied to establish best-iterate convergence for other regularizers? The authors note their "novel two-phase approach... could be of independent interest" and suggest the "techniques... could be valuable for studying other algorithms beyond OMWU." Why unresolved: The technique was developed specifically to sidestep the lack of random-iterate convergence in OMWU (entropy regularizer); its applicability to log-barrier or Tsallis entropy remains unverified. What evidence would resolve it: Deriving uniform best-iterate convergence rates for OFTRL with alternative regularizers using the two-phase method.

- **Open Question 3**: Is the $O(T^{-1/6})$ best-iterate convergence rate tight for OMWU in $2 \times 2$ games? The paper provides an upper bound of $O(T^{-1/6})$ and a random-iterate lower bound, but explicitly lacks a lower bound for best-iterate convergence. Why unresolved: It is unclear if the $T^{-1/6}$ dependence is an artifact of the proof technique (balancing the two phases) or a fundamental limit of the algorithm. What evidence would resolve it: A lower bound proof demonstrating that no polynomial rate faster than $\Omega(T^{-1/6})$ is possible, or an improved upper bound.

## Limitations

- The negative result (Ω(1/log T) lower bound) is established only for 2×2 games with entropy regularizers
- The positive result (O(T^{-1/6}) best-iterate rate) is proven specifically for 2×2 games and does not extend to larger games
- The analysis relies heavily on the assumption of fully-mixed Nash equilibria, with rates degrading significantly for near-pure strategy equilibria

## Confidence

- **High**: The Ω(1/log T) lower bound for random-iterate convergence is rigorously proven with explicit construction of the hard instance Aδ
- **Medium**: The O(T^{-1/6}) best-iterate convergence rate for 2×2 games follows from a novel two-phase analysis that combines initial and global phase results
- **Medium**: The connection between random-iterate convergence and dynamic regret is theoretically sound but relies on assumptions about interval regret that require further empirical validation

## Next Checks

1. **Empirical verification of separation**: Implement OMWU on a suite of 2×2 games and systematically measure the ratio between best-iterate and random-iterate convergence rates to empirically confirm the theoretical separation
2. **Regularizer sensitivity analysis**: Test OMWU with alternative regularizers (Tsallis entropy, squared Euclidean) on the hard instance Aδ to verify whether the Ω(1/log T) lower bound persists across different regularizer choices
3. **Scaling behavior in larger games**: Extend the analysis to 3×3 and 4×4 games to investigate whether the O(T^{-1/6}) best-iterate rate for 2×2 games scales predictably with game dimension