---
ver: rpa2
title: 'Generating Natural-Language Surgical Feedback: From Structured Representation
  to Domain-Grounded Evaluation'
arxiv_id: '2511.15159'
source_url: https://arxiv.org/abs/2511.15159
tags:
- feedback
- surgical
- video
- action
- tissue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a structure-aware pipeline for generating natural-language
  surgical feedback grounded in clinically meaningful representations. The approach
  first induces an Instrument-Action-Tissue (IAT) ontology from real trainer-to-trainee
  feedback transcripts, then trains a multimodal video-to-IAT predictor that fuses
  video frames, fine-grained temporal instrument motion, and clinical context.
---

# Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation

## Quick Facts
- arXiv ID: 2511.15159
- Source URL: https://arxiv.org/abs/2511.15159
- Reference count: 40
- Primary result: IAT-grounded feedback generation improves clinician-aligned fidelity scores (2.44 vs 2.17) with doubled admissible outputs (21%→42%)

## Executive Summary
This paper introduces a novel approach to surgical feedback generation by grounding natural language output in a structured Instrument-Action-Tissue (IAT) ontology derived from real surgical trainer feedback. The system combines multimodal video analysis with fine-grained temporal instrument motion tracking and clinical context to predict IAT triplets, which then condition GPT-4o to produce trainer-style feedback. The evaluation demonstrates that this structured approach significantly improves the fidelity of generated feedback compared to video-only methods, with clinician ratings showing a 12.4% improvement and the proportion of acceptable feedback doubling.

## Method Summary
The approach consists of two main components: an IAT ontology induction from surgical feedback transcripts, and a multimodal video-to-IAT predictor that fuses video frames, temporal instrument motion, and clinical context. The IAT triplets serve as structured conditioning for GPT-4o to generate natural-language feedback. The system was evaluated on the JIGSAWS dataset using AUC metrics for IAT recognition and clinician-aligned fidelity scores for the generated feedback quality.

## Key Results
- IAT recognition AUCs improved with temporal tracking and context: Instrument (0.67→0.74), Action (0.60→0.63), Tissue (0.74→0.79)
- IAT-conditioned feedback achieved higher clinician-aligned fidelity score (2.44 vs 2.17 for video-only)
- Share of admissible outputs (score≥3) doubled from 21% to 42%
- Grounding in explicit IAT structure provides clinician-verifiable rationales

## Why This Works (Mechanism)
The system works by first establishing a domain-specific ontology (IAT) that captures the essential elements of surgical interactions. This structured representation provides a semantically meaningful intermediate representation that bridges the gap between raw video data and clinically relevant feedback. By conditioning language generation on these structured triplets rather than raw video alone, the system produces more focused, accurate, and clinically meaningful feedback that aligns better with how human trainers communicate.

## Foundational Learning

**IAT Ontology** - Instrument-Action-Tissue triplet representation of surgical interactions
- Why needed: Provides structured, clinically meaningful representation of surgical events
- Quick check: Verify coverage of common surgical procedures and completeness of triplet combinations

**Multimodal Fusion** - Integration of visual, motion, and contextual features
- Why needed: Captures different aspects of surgical activity for comprehensive understanding
- Quick check: Test individual modality performance and compare to fused performance

**Conditional Language Generation** - GPT-4o generation conditioned on structured IAT triplets
- Why needed: Ensures generated feedback remains grounded in clinically relevant facts
- Quick check: Compare generated outputs with and without IAT conditioning

## Architecture Onboarding

**Component Map**: Video Frames + Motion Tracking + Clinical Context -> IAT Predictor -> GPT-4o -> Natural Language Feedback

**Critical Path**: Multimodal feature extraction → IAT triplet prediction → Language generation conditioning → Feedback output

**Design Tradeoffs**: Structured IAT representation provides clinical grounding but limits expressiveness; multimodal fusion improves accuracy but increases computational complexity; using GPT-4o enables natural language but reduces interpretability of generation process.

**Failure Signatures**: Incorrect IAT predictions lead to irrelevant or misleading feedback; poor multimodal fusion results in missing critical surgical context; over-reliance on clinical context may produce generic feedback lacking procedural specificity.

**First Experiments**:
1. Test IAT predictor performance with different combinations of input modalities (video only, motion only, context only)
2. Compare generated feedback quality when conditioning on correct vs. predicted IAT triplets
3. Evaluate feedback fidelity across different surgical procedures in the JIGSAWS dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on subjective clinician ratings rather than objective task performance metrics
- IAT ontology covers only subset of surgical interactions and may not generalize across all procedures
- Uses simulated surgical environments (JIGSAWS) rather than real operating room data
- 12.4% improvement in fidelity score represents modest absolute gain

## Confidence

**High Confidence**: Technical implementation of multimodal IAT prediction pipeline and improvement in IAT recognition metrics (AUC increases: Instrument 0.67→0.74, Action 0.60→0.63, Tissue 0.74→0.79)

**Medium Confidence**: Claim that IAT-grounded generation improves clinician-aligned fidelity scores (2.44 vs 2.17), though subjective evaluation introduces uncertainty

**Medium Confidence**: Assertion that grounding improves "clinician-verifiable rationales" supported by qualitative feedback but needs systematic validation

## Next Checks

1. Conduct longitudinal study measuring actual learning outcomes in trainees using IAT-grounded versus video-only feedback, tracking performance improvements over multiple training sessions.

2. Expand IAT ontology coverage by collecting and analyzing feedback from multiple surgical specialties and complex procedures, then retraining IAT predictor on expanded dataset.

3. Implement ablation study comparing full multimodal IAT generation system against variants using only visual features, only motion tracking, or only clinical context to quantify individual contributions.