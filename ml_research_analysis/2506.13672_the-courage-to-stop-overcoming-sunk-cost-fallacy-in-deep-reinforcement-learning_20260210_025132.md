---
ver: rpa2
title: 'The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning'
arxiv_id: '2506.13672'
source_url: https://arxiv.org/abs/2506.13672
tags:
- learning
- least
- deep
- reinforcement
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of inefficiency in deep reinforcement
  learning caused by agents continuing to interact with the environment in low-quality,
  familiar trajectories, which wastes samples and contaminates replay buffers. The
  authors introduce LEAST, a method that enables early termination of episodes based
  on Q-value and gradient statistics, allowing agents to recognize when to stop unproductive
  interactions.
---

# The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.13672
- Source URL: https://arxiv.org/abs/2506.13672
- Reference count: 40
- Primary result: Introduces LEAST, a method for early episode termination based on Q-value and gradient statistics, improving sample efficiency across multiple RL algorithms

## Executive Summary
This paper addresses inefficiency in deep reinforcement learning caused by agents continuing to interact with the environment in low-quality, familiar trajectories, which wastes samples and contaminates replay buffers. The authors introduce LEAST, a method that enables early termination of episodes based on Q-value and gradient statistics, allowing agents to recognize when to stop unproductive interactions. By comparing current Q-values and learning gradients to historical buffers, LEAST dynamically determines when to terminate episodes, improving sample efficiency. Experiments on MuJoCo and DeepMind Control Suite benchmarks show that LEAST significantly improves learning efficiency across multiple RL algorithms, including TD3, SAC, REDQ, and DrQv2, with faster convergence and better final performance.

## Method Summary
LEAST maintains two buffers (BQ for Q-values, BG for gradient magnitudes) from recent episodes. At each step, it computes step-specific thresholds using median statistics from these buffers and stops episodes when current Q-values fall below the threshold, optionally modulated by gradient magnitude. The method includes entropy-based buffer resizing and adaptive noise scheduling. LEAST is integrated as a wrapper around off-policy actor-critic algorithms like TD3 and SAC, requiring minimal modifications to the baseline training loop.

## Key Results
- LEAST significantly improves sample efficiency across TD3, SAC, REDQ, and DrQv2 on MuJoCo and DeepMind Control Suite benchmarks
- The method reduces replay buffer pollution by eliminating low-quality transitions characterized by low Q-values and low losses
- LEAST maintains network plasticity longer than vanilla baselines, as measured by slower decline in Fraction of Active Units (FAU)
- The Q+gradient threshold variant outperforms Q-only threshold variants

## Why This Works (Mechanism)

### Mechanism 1
Early episode termination based on Q-value thresholds improves sample efficiency by redirecting interaction budget away from low-quality trajectories. LEAST maintains a two-dimensional buffer BQ storing Q-values from the K most recent episodes (up to L steps each). At each intra-episode step i, it computes a threshold ϵi = Median(BQ[:, i]) and stops when the current Q̂i falls below this threshold. Step-specific thresholds handle the natural Q-value scale differences across episode positions. Core assumption: Q-values reliably distinguish trajectory quality during online interaction; median historical Q-values approximate "acceptable" performance at each step.

### Mechanism 2
Gradient magnitude serves as a learning potential proxy, modulating stopping thresholds to preserve exploration of novel but currently low-value states. A parallel gradient buffer BG stores TD loss magnitudes (used as gradient proxies). The dynamic weight ωi = Median(BG[:, i]) / Gi adjusts the threshold: when current gradient Gi exceeds historical median (ωi < 1), the threshold lowers, encouraging continued interaction for learning-rich trajectories. Core assumption: TD loss magnitude correlates with learning potential and state novelty; larger losses indicate underexplored regions worth persisting in.

### Mechanism 3
Reducing low-quality transitions in the replay buffer mitigates buffer pollution and preserves network plasticity for long-horizon learning. By stopping early, LEAST prevents accumulation of low-Q, low-loss transitions (uninformative samples). Figure 3 shows LEAST reduces the "white region" (low Q, low loss) in buffer distribution. Appendix C.1 demonstrates slower FAU decline, suggesting maintained plasticity. Core assumption: Replay buffer composition directly affects optimization quality; low-quality transitions contribute to overfitting and plasticity loss.

## Foundational Learning

- **Off-policy actor-critic architecture (TD3/SAC)**: Why needed: LEAST operates as a wrapper around actor-critic methods, requiring understanding of how critics estimate Q-values and how replay buffers function. Quick check: Can you explain how TD3 uses clipped double Q-learning to mitigate overestimation bias?
- **Temporal difference (TD) learning and loss computation**: Why needed: LEAST uses TD loss as the gradient proxy (BG buffer); understanding how this loss reflects learning signal is essential. Quick check: What does a high TD error suggest about a transition's value for learning?
- **Median vs. mean robustness to outliers**: Why needed: LEAST explicitly uses median thresholds (not means) to handle noisy Q-value distributions; Section 4.3 validates this design. Quick check: Why would median outperform mean when Q-values contain outliers from unstable early policies?

## Architecture Onboarding

- **Component map**:
  - Environment -> Agent -> Q-value computation -> BQ buffer update
  - Environment -> Agent -> TD loss computation -> BG buffer update
  - BQ/BG buffers -> Threshold computer -> Stopping decision module -> Environment reset
  - Stopping frequency -> Noise scheduler -> Exploration noise adjustment

- **Critical path**:
  1. During rollout, at each step compute Q̂(s, a) and TD loss
  2. Update BQ and BG with current values
  3. If t ≥ t_start, compute threshold and weight; check stopping condition
  4. If triggered, terminate episode and reset environment; optionally increase noise

- **Design tradeoffs**:
  - **Start time (t_start)**: Too early → unstable thresholds from insufficient history; too late → misses early efficiency gains. Paper suggests 10–20% of training for MuJoCo, 5–15% for visual RL.
  - **Buffer size K**: 250 episodes recommended (Table 5); too small → poor statistics, too large → over-smoothing
  - **Weight ω scaling**: Paper finds [0.3, 0.6] robust; SAC more sensitive than TD3

- **Failure signatures**:
  - Excessive early stopping (episodes terminating immediately) → t_start too early or thresholds too aggressive
  - No performance gain over baseline → LEAST not activating (check t_start) or Q-values miscalibrated
  - High variance across seeds → adjust entropy scaling parameter γ (Table 3 suggests 0.05–0.10)

- **First 3 experiments**:
  1. Reproduce Figure 2 (PointMaze) with vanilla SAC vs. SAC+LEAST to validate stopping behavior; visualize episode lengths and buffer composition.
  2. Ablate individual components (Q-only threshold, Q+gradient, Q+gradient+dynamic buffer) on a single MuJoCo task to match Figure 4.
  3. Sweep t_start (0.25M, 0.5M, 0.75M, 1M) on Ant to identify task-specific optimal start time (Table 4).

## Open Questions the Paper Calls Out

- **Adaptive evaluation refinement**: How can the adaptive evaluation mechanism in LEAST be refined to mitigate the observed increase in performance variance across different seeds? The authors note in Section 4.1 that "LEAST may increase the score variance of the algorithms," identifying this as a direction for future work to enhance training stability.

- **Additional stopping indicators**: Can additional indicators be incorporated into the stopping threshold to specifically filter out low-quality transitions characterized by high loss but low Q-values? Section 6 states, "the proportion of samples with high loss but low Q-value (blue region) remains largely unchanged. Future work could investigate refining the sampling threshold..."

- **Noise-based diversification module**: Can a noise-based diversification module effectively prevent agents from immediately re-entering suboptimal trajectories after an early reset? The authors mention in Section 6 that "After reset, agents may re-enter previously suboptimal trajectories," suggesting the introduction of a noise-based module as a solution.

- **Theoretical plasticity correlation**: What is the theoretical correlation between adaptive early stopping and the preservation of network plasticity? Appendix C.1 notes the empirical finding that LEAST slows the decay of Fraction of Active Units (FAU), concluding, "In the future, we will deeply analyze the theoretical correlation between plasticity and adaptive stopping."

## Limitations
- The increase in performance variance across different seeds may limit practical deployment
- The gradient-as-learning-potential assumption lacks theoretical grounding and may fail when TD loss is dominated by approximation error
- The entropy-based buffer resizing mechanism introduces hyperparameters that require task-specific tuning
- The conceptual framing of "sunk cost fallacy" is not rigorously tested as a cognitive bias mechanism

## Confidence
- **High**: The empirical demonstration that LEAST reduces replay buffer pollution and maintains network plasticity (Figure 3, Appendix C.1)
- **Medium**: The improvement in sample efficiency across TD3, SAC, REDQ, and DrQv2; results are consistent but algorithm-specific sensitivity to ω and t_start requires tuning
- **Low**: The claim that LEAST addresses "sunk cost fallacy" as a cognitive bias; this is a conceptual framing rather than a rigorously tested mechanism

## Next Checks
1. Test LEAST on sparse-reward tasks (e.g., Montezuma's Revenge) to verify robustness when Q-values are unreliable
2. Conduct ablation studies isolating the gradient-modulation component to validate its contribution beyond Q-thresholding alone
3. Evaluate LEAST's sensitivity to buffer size K and entropy scaling γ across multiple random seeds to quantify hyperparameter stability