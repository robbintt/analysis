---
ver: rpa2
title: 'A Multi-Labeled Dataset for Indonesian Discourse: Examining Toxicity, Polarization,
  and Demographics Information'
arxiv_id: '2503.00417'
source_url: https://arxiv.org/abs/2503.00417
tags:
- polarization
- toxicity
- detection
- toxic
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multi-labeled Indonesian dataset for toxicity,\
  \ polarization, and demographic annotation. The dataset contains 28,477 texts annotated\
  \ by 29 diverse annotators, achieving Gwet\u2019s AC1 scores of 0.61 for toxicity\
  \ and 0.37 for polarization."
---

# A Multi-Labeled Dataset for Indonesian Discourse: Examining Toxicity, Polarization, and Demographics Information

## Quick Facts
- **arXiv ID:** 2503.00417
- **Source URL:** https://arxiv.org/abs/2503.00417
- **Reference count:** 40
- **Primary result:** IndoBERTweet achieves 0.791 macro F1 for toxicity and 0.731 for polarization; demographic features improve performance to 0.806/0.750 macro F1

## Executive Summary
This paper introduces IndoToxic2024, a multi-labeled Indonesian dataset for toxicity, polarization, and demographic annotation. The dataset contains 28,477 texts annotated by 29 diverse annotators, achieving Gwet's AC1 scores of 0.61 for toxicity and 0.37 for polarization. Experiments with BERT-based models and LLMs show that IndoBERTweet outperforms LLMs, achieving 0.791 macro F1 for toxicity and 0.731 for polarization. Incorporating polarization as a feature improves toxicity classification (0.828 macro F1), and vice versa (0.757 macro F1). Demographic information further enhances performance, especially for IndoBERTweet, with the best setup (ethnicity, domicile, religion) achieving 0.806 macro F1 for toxicity and 0.750 for polarization. Notably, 48% of toxic texts during the 2024 Indonesian election were polarizing, highlighting the dataset's relevance for studying toxic polarization in political discourse.

## Method Summary
The study uses the IndoToxic2024 dataset (28,477 texts) from social media and news sources, with annotations from 29 diverse annotators. Texts were sampled to achieve a 1:3 positive:negative class ratio using seed=42. Labels were aggregated via majority vote, excluding texts with perfect disagreement. The primary method involves fine-tuning IndoBERTweet using stratified 5-fold cross-validation (seed=42) for binary toxicity and polarization classification. Cross-task features are incorporated as averaged annotation values [0,1], while demographic information is appended via [SEP] token for IndoBERTweet or prompt engineering for GPT-4o-mini. The "exploded" dataset approach duplicates texts for each annotator when incorporating demographic features.

## Key Results
- IndoBERTweet achieves 0.791 macro F1 for toxicity and 0.731 for polarization detection
- Cross-task features improve toxicity classification to 0.828 macro F1 and polarization to 0.757 macro F1
- Demographic features (ethnicity, domicile, religion) boost IndoBERTweet to 0.806 macro F1 for toxicity and 0.750 for polarization
- 48% of toxic texts during the 2024 Indonesian election were also polarizing

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Feature Correlation
Toxicity and polarization share semantic correlation in political discourse. By providing the model with one label's continuous value as a feature for the other task, the model conditions its prediction on the contextual "heat" of the text, narrowing the hypothesis space. This works because annotators' definitions of polarization overlap with linguistic markers of toxicity. Break condition: If toxicity and polarization were orthogonal, feature addition would act as noise.

### Mechanism 2: Demographic-Aware Modeling
Incorporating annotator demographics transforms the task from "objective" detection to "perspective-aware" modeling. By exploding the dataset and concatenating demographic embeddings, the model learns to predict how specific demographic profiles would interpret text, resolving ambiguities that plague demographic-blind models. This works because toxicity and polarization are subjective perceptions influenced by identity. Break condition: If the annotator pool is too small or unrepresentative, the model may overfit to specific annotator idiosyncrasies.

### Mechanism 3: Domain-Specific BERT vs General LLMs
Specialized pre-training (IndoBERTweet) outperforms general-purpose LLMs for nuanced, subjective classification in low-resource languages. While LLMs possess vast general knowledge, they lack alignment with specific annotation schemas and struggle with local linguistic context. Fine-tuning domain-specific BERT allows granular adaptation to specific decision boundaries. Break condition: If the LLM is provided with highly effective few-shot examples or specialized prompt engineering not covered in the paper.

## Foundational Learning

- **Concept: Gwet's AC1 vs. Cohen's Kappa**
  - **Why needed here:** The paper relies on Gwet's AC1 (0.61 toxicity, 0.37 polarization) to validate data quality. Standard metrics like Kappa paradoxically drop when class imbalance is high or agreement is very high/very low; AC1 is robust to these.
  - **Quick check question:** Why does the paper report 0.37 for polarization as a "satisfactory" score when it intuitively seems low? (Hint: Consider the inherent subjectivity of polarization vs. toxicity).

- **Concept: Dataset "Explosion" for Contextual Modeling**
  - **Why needed here:** To utilize demographics, the authors convert a 1-to-N mapping (one text, N annotators) into an N-row dataset.
  - **Quick check question:** How does exploding the dataset change the loss function compared to majority-vote aggregation?

- **Concept: Multi-Label Correlation (Toxicity vs. Polarization)**
  - **Why needed here:** The paper posits that these are distinct but correlated phenomena.
  - **Quick check question:** Why can't we simply treat "toxic polarization" as a single class?

## Architecture Onboarding

- **Component map:** Text -> [SEP] Token -> Feature String (Demographic info or Average Label Value) -> Classification Head
- **Critical path:**
  1. **Preprocessing:** Filter spam/short texts
  2. **Annotation/Aggregation:** Decide between Majority Vote (AGG) or Any (ANY) depending on tolerance for noise vs. recall
  3. **Feature Engineering:** Construct input string by concatenating "Informasi Demografis: [Ethnicity], [Religion]..." before [SEP] token
  4. **Training:** Fine-tune IndoBERTweet (Stratified 5-Fold Cross-Validation)

- **Design tradeoffs:**
  - **LLM vs. BERT:** LLMs (GPT-4o) are faster to deploy (zero-shot) but fail to capture polarization nuances. BERT requires GPU fine-tuning but yields ~20%+ higher F1
  - **Single-Coder vs. Multi-Coder Data:** Training on single-coder data yields high precision (conservative); multi-coder data yields high recall (broad-net)
  - **Demographic Inclusion:** Adds complexity and requires "exploding" the dataset, but appears to be the only way to break the performance ceiling on polarization tasks

- **Failure signatures:**
  - **LLM Collapse:** Model outputs "Not Toxic" for everything (observed with Aya23-8B)
  - **BERT Blindness:** Model performs at random (ROC AUC 0.5) if trained on "exploded" data *without* demographic tags (it sees duplicate texts with conflicting labels and averages them to zero)

- **First 3 experiments:**
  1. **Baseline Calibration:** Train IndoBERTweet on text-only data using Majority Vote (AGG) to establish the lower bound
  2. **Feature Ablation (Cross-task):** Retrain the Toxicity model, appending the *Polarization* average value to the input (Goal: Achieve >0.82 F1)
  3. **Demographic Integration:** "Explode" the dataset, append Annotator Demographics (Ethnicity, Domicile, Religion), and verify if the model learns to distinguish conflicting labels based on demographic context

## Open Questions the Paper Calls Out

### Open Question 1
Why do large language models (LLMs) like GPT-4o-mini fail to utilize demographic and cross-task featural information in Indonesian discourse contexts? The authors note that GPT-4o-mini ignores added demographic and polarization features, suggesting the model "selectively prioritizes certain features over others, a behavior that warrants further investigation." This remains unresolved as the paper doesn't determine if it stems from lack of specific cultural training data or architectural inability to process persona-based constraints in low-resource languages.

### Open Question 2
Should polarization annotation protocols use "ANY" aggregation (labeling as polarized if any annotator flags it) rather than standard majority voting? The authors state that "further analysis on whether polarization detection should adhere to the same strict dataset creation protocols as toxicity detection should be done," noting that polarization is highly subjective and majority voting may be counterproductive. This remains unresolved as the "Wisdom of the Crowd" experiments show the "ANY" criterion works surprisingly well with limited data, but the paper doesn't establish a definitive standard for balancing noise reduction against capturing diverse perspectives.

### Open Question 3
Can robust automated predictors be developed to generate cross-task features without degrading performance? The authors conclude that "creating a predictor through simple methods may not be adequate and is a potential area for future work" after finding that simple predictors degraded classification performance compared to human labels. This remains unresolved as it's unknown if machine-generated proxies can achieve similar synergy or if the noise introduced by automated predictors negates the benefits.

## Limitations

- Modest dataset size (28,477 texts with 29 annotators) relative to multi-label task complexity, particularly for polarization detection with low inter-annotator agreement (AC1=0.37)
- Aggregated annotations without exploring individual annotator calibration, making it unclear whether demographic improvements reflect systematic patterns or overfitting
- Dataset composition (only 16% toxic texts after sampling) and zero-shot GPT-4o-mini usage without prompt tuning limit generalizability of LLM comparisons

## Confidence

- **High confidence:** IndoBERTweet significantly outperforms GPT-4o-mini for toxicity and polarization detection (0.791 vs. 0.593 F1 for toxicity; 0.731 vs. 0.548 F1 for polarization)
- **Medium confidence:** Incorporating polarization as a feature improves toxicity classification (0.828 vs. 0.791 F1) and vice versa
- **Medium confidence:** Demographic information significantly enhances IndoBERTweet performance (0.806 vs. 0.791 F1 for toxicity; 0.750 vs. 0.731 F1 for polarization)
- **Low confidence:** The finding that 48% of toxic texts during the 2024 election were polarizing represents a temporal observation rather than systematic analysis

## Next Checks

1. **Annotator Calibration Analysis:** Conduct systematic analysis of individual annotator reliability by examining intra-annotator consistency across multiple texts and comparing agreement patterns between different demographic groups to determine whether demographic features capture systematic biases or annotator-specific tendencies.

2. **Dataset Scaling Experiment:** Test whether observed performance improvements from demographic features scale with dataset size by training models on progressively larger subsets of the exploded dataset (25%, 50%, 100%) to identify whether current results reflect genuine signal or overfitting to the limited annotator pool.

3. **Temporal Generalization Test:** Validate whether the toxicity-polarization correlation (48% overlap) holds across different time periods and political contexts by applying trained models to texts from pre-election periods and non-political domains to assess temporal and topical generalizability.