---
ver: rpa2
title: Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing
arxiv_id: '2501.14457'
source_url: https://arxiv.org/abs/2501.14457
tags:
- neurons
- gender
- bias
- arxiv
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the CommonWords dataset to systematically evaluate
  gender bias in large language models (LLMs). The dataset contains 500 words across
  five categories (traits, actions, professions, hobbies, and colors), designed to
  avoid overlap with prior datasets.
---

# Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing

## Quick Facts
- arXiv ID: 2501.14457
- Source URL: https://arxiv.org/abs/2501.14457
- Reference count: 18
- This paper introduces a systematic evaluation dataset and interpretable neuron editing method that effectively reduces gender bias in LLMs while preserving model capabilities.

## Executive Summary
This paper addresses gender bias in large language models by proposing the CommonWords dataset and an interpretable neuron editing method. The dataset contains 500 words across five categories designed to systematically evaluate gender bias without overlap with prior datasets. Through neuron-level analysis, the authors identify two distinct neuron circuits responsible for gender bias: "gender neurons" activated by stereotypical words and "general neurons" activated by the <start> token. They demonstrate that editing just two general neurons can significantly degrade model performance due to hierarchical interactions. Based on these insights, they propose a method combining logit-based and causal-based strategies to selectively target biased neurons while preserving model capabilities, achieving superior bias mitigation compared to existing approaches.

## Method Summary
The authors propose a three-step neuron editing pipeline: (1) identify top 150 neurons (50 FFN value, 50 attention value, 50 FFN query) using logit-based importance scores that measure log probability increase for biased tokens; (2) exclude neurons located at the <start> position to avoid general capability degradation; (3) apply causal-based masking to select the final 50 neurons that effectively reduce bias metrics without harming common task performance. The method combines coefficient analysis of individual neurons with position-aware filtering and causal validation to ensure selectivity and effectiveness.

## Key Results
- CommonWords dataset reveals pervasive gender bias across five tested LLMs
- Editing just two general neurons causes catastrophic performance degradation (51.9%→7.5% on arithmetic, 63.5%→31.5% on RACE)
- The interpretable neuron editing method achieves higher ICAT scores (61.6 vs 59.1) on StereoSet
- The method produces lower entropy differences (0.81 vs 0.95) on WinoGender while maintaining performance on common tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gender bias is stored in identifiable "gender neurons" activated by stereotypical words, with coefficient signs determining prediction direction.
- Mechanism: Stereotypical words activate FFN query neurons in shallow layers (L11, L14). These propagate through attention neurons (notably L18H7) to deeper FFN value neurons (L20+). Coefficient sign reversals between cases flip probability distributions toward "woman" or "man" tokens.
- Core assumption: Neuron coefficients directly and causally influence token probabilities in predictable ways.
- Evidence anchors:
  - [abstract]: "identifies specific neuron circuits, including 'gender neurons' and 'general neurons,' responsible for this behavior"
  - [section 4.2]: Table 2 shows gender neurons with interpretable top/last tokens (e.g., `ffnL11N17` top: [herself, woman, actress], last: [himself, male, boy])
  - [corpus]: Related work (KnowBias, arXiv:2601.21864) supports neuron-level bias mitigation but uses a suppressive paradigm rather than selective editing
- Break condition: If gender bias is distributed across thousands of neurons rather than concentrated in identifiable circuits, targeted editing becomes infeasible.

### Mechanism 2
- Claim: "General neurons" activated by the `<start>` token support broad model capabilities; editing them catastrophically degrades performance via hierarchical neuron interactions.
- Mechanism: Two neurons in L2 (`ffnL2N7003`, `ffnL2N4090`) have exceptionally high query scores at the `<start>` position. Masking them reverses coefficient signs of deeper arithmetic neurons, collapsing accuracy from 51.9%→7.5% on arithmetic and 63.5%→31.5% on reading comprehension.
- Core assumption: Shallow-layer neurons exert disproportionate influence on deeper neurons through coefficient cascades.
- Evidence anchors:
  - [abstract]: "editing just two general neurons can significantly degrade performance on common tasks due to hierarchical neuron interactions"
  - [section 4.2]: Table 3 documents coefficient sign reversals in "3+5=" case after masking general neurons
  - [corpus]: Locate-then-Merge (arXiv:2505.16703) similarly finds neuron-level parameter changes mitigate catastrophic forgetting in multimodal LLMs
- Break condition: If early-layer general neurons are task-specific rather than general-purpose, the hierarchical interaction claim weakens.

### Mechanism 3
- Claim: Combining logit-based identification with causal-based filtering and position-aware exclusion yields effective bias mitigation while preserving capabilities.
- Mechanism: Three-step process: (1) identify top 150 neurons via logit-based importance scores; (2) exclude neurons at `<start>` position (general neurons); (3) apply causal-based masking to select final 50 neurons that reduce bias metrics without harming common tasks.
- Core assumption: Neurons contributing to bias but not at `<start>` position are safer to edit than general neurons.
- Evidence anchors:
  - [abstract]: "combines logit-based and causal-based strategies to selectively target biased neurons"
  - [section 5.1]: "we calculate the important positions for each neuron and exclude those located at the <start> position"
  - [corpus]: ACE (arXiv:2510.07896) uses attribution-controlled editing for multi-hop recall, suggesting causal analysis complements logit methods
- Break condition: If position-based filtering excludes critical bias neurons or retains harmful ones, the method's selectivity degrades.

## Foundational Learning

- Concept: **Residual Stream Decomposition**
  - Why needed here: The paper analyzes how contributions from individual neurons accumulate through the residual stream to influence final predictions.
  - Quick check question: Can you explain why `h_i^l = h_i^{l-1} + A_i^l + F_i^l` allows decomposing a model's output into neuron-level contributions?

- Concept: **FFN Neurons as Key-Value Memories**
  - Why needed here: The paper defines neurons as (subkey, subvalue) pairs where subkeys determine activation and subvalues contribute to vocabulary distributions.
  - Quick check question: Given equation `F_i^l = Σ m_{i,k}^l · fc2_k^l`, what does the coefficient `m` represent and how does its sign affect token probabilities?

- Concept: **Logit Lens / Unembedding Projection**
  - Why needed here: The paper projects neuron subvalues into vocabulary space via `D_v^l = softmax(E_u · v^l)` to interpret what tokens each neuron promotes or suppresses.
  - Quick check question: If a neuron's top tokens are [woman, actress, lady] and last tokens are [man, boy, male], what happens to "woman" probability when this neuron's coefficient is negative?

## Architecture Onboarding

- Component map:
  Input tokens → Embeddings → [L1-L32 Transformer Layers] → Each layer: MHSA + FFN → Output: Unembedding → Token probabilities

- Critical path:
  1. Identify bias-contributing neurons via log probability increase (Eq. 8-9)
  2. Trace information flow: query → attention → value neurons
  3. Filter by position (exclude `<start>`-located neurons)
  4. Validate via causal masking before editing

- Design tradeoffs:
  - **Selectivity vs. coverage**: Selecting fewer neurons (K=10) yields higher shared frequency (60%) but may miss case-specific bias; K=50+ captures more but includes noise
  - **Logit-based vs. causal-based**: Logit-based finds neurons storing bias; causal-based finds neurons necessary for bias expression—both needed
  - **Edit magnitude**: Paper uses neuron masking (zeroing); stronger edits risk broader capability degradation

- Failure signatures:
  - Arithmetic accuracy drops from ~50% to <10% → likely edited general neurons
  - RACE accuracy drops from ~63% to ~30% → hierarchical disruption from shallow edits
  - ICAT score doesn't improve → biased neurons not correctly identified

- First 3 experiments:
  1. **Reproduce neuron identification**: Run logit-based importance scoring on 100 CommonWords sentences; verify top neurons align with paper's Table 2
  2. **Ablate general neurons**: Mask `ffnL2N7003` and `ffnL2N4090`; confirm arithmetic collapse per Table 3
  3. **Position filtering validation**: Compare editing all top-50 neurons vs. excluding `<start>`-position neurons on StereoSet ICAT and PIQA accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The neuron identification mechanism relies on coefficient analysis without establishing causal proof of necessity versus correlation
- The hierarchical interaction claim between shallow and deep neurons remains observational without additional causal validation
- The CommonWords dataset may not capture all forms of gender bias, particularly intersectional or contextual biases

## Confidence

- **High Confidence**: The neuron-level analysis methodology (logit-based importance scoring, coefficient sign interpretation) is technically sound and reproducible. The observation that editing general neurons degrades performance is empirically validated through controlled experiments.
- **Medium Confidence**: The hierarchical neuron interaction mechanism is plausible but requires additional causal validation. The position-based filtering strategy shows empirical benefits but the theoretical justification could be stronger.
- **Low Confidence**: The claim that this approach generalizes to all forms of social bias or that the CommonWords dataset comprehensively captures gender bias in real-world scenarios.

## Next Checks

1. **Causal Validation of Neuron Necessity**: Implement ablation studies where identified gender neurons are sequentially masked in random order, measuring whether bias reduction correlates with masking these specific neurons versus random selection. Use causal tracing techniques to verify these neurons are actually necessary for bias expression, not just correlated.

2. **Generalization to Other Bias Types**: Apply the neuron identification and editing pipeline to racial bias using a modified CommonWords-style dataset with racially stereotypical terms. Compare success rates and identify whether the same hierarchical mechanisms apply or if different neuron circuits are involved.

3. **Long Sequence and Contextual Bias Testing**: Evaluate the edited models on longer narrative contexts (multiple paragraphs) and tasks requiring temporal or situational understanding of gender roles. Test whether bias mitigation holds when gender implications are implicit rather than explicit in vocabulary, using datasets like Winogender in extended contexts.