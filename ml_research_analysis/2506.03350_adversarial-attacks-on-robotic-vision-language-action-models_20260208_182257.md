---
ver: rpa2
title: Adversarial Attacks on Robotic Vision Language Action Models
arxiv_id: '2506.03350'
source_url: https://arxiv.org/abs/2506.03350
tags:
- arxiv
- action
- attacks
- which
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study of adversarial attacks on vision-language-action
  (VLA) models, which are a critical component of modern robotics systems. The authors
  demonstrate that by optimizing textual prompts, an attacker can gain complete control
  authority over a VLA-controlled robot, causing it to perform any targeted action
  in its action space.
---

# Adversarial Attacks on Robotic Vision Language Action Models

## Quick Facts
- arXiv ID: 2506.03350
- Source URL: https://arxiv.org/abs/2506.03350
- Reference count: 40
- Authors demonstrate 90%+ success rates in controlling VLA-controlled robots through textual prompt optimization

## Executive Summary
This paper presents the first systematic study of adversarial attacks on Vision-Language-Action (VLA) models, demonstrating that attackers can achieve complete control authority over robotic systems by optimizing textual prompts. The authors adapt large language model jailbreaking techniques, specifically the Greedy Coordinate Gradient (GCG) algorithm, to manipulate VLAs into executing targeted low-level control actions. Experiments show these attacks succeed at over 90% rates across multiple fine-tuned OpenVLA variants and can persist across multiple rollout steps. The attacks transfer from simulation to real-world environments, highlighting the critical need for VLA-specific defenses as these systems become more widely deployed.

## Method Summary
The paper adapts GCG jailbreaking to VLA models by treating robot actions as autoregressive tokens. The attack optimizes a suffix of textual tokens to minimize the negative log-likelihood of target actions, using gradients computed with respect to token distributions. Single-step attacks optimize over one image embedding, while persistence attacks aggregate loss across multiple diverse image embeddings to force generalization. The threat model assumes white-box access to VLA weights and gradients. The evaluation uses OpenVLA models (base and four fine-tuned variants) on tasks from the LIBERO dataset, testing 1792 one-hot target actions across simulation and real-world environments.

## Key Results
- GCG-based attacks achieve >90% success rate on single-step tasks across multiple OpenVLA fine-tuned variants
- Optimized adversarial suffixes can persist across multiple rollout steps, showing up to 28× improvement over nominal behavior
- No equivalent "refusal" mechanisms exist in VLAs compared to LLMs, enabling complete control authority
- Attacks transfer from simulation (SIMPLER) to real-world (HYDRA) environments

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Token Optimization for Action Elicitation
The attack treats the VLA as a function mapping text and image to actions, then uses GCG to iteratively replace tokens that minimize the loss of target action tokens. This gradient-based optimization allows attackers to override visual grounding and force specific robot movements by driving the model to predict desired discrete action tokens.

### Mechanism 2: Multi-Step Persistence via Image-Agnostic Optimization
By aggregating loss over multiple diverse image embeddings, the attack forces the adversarial suffix to generalize across time steps. This penalizes token choices that rely on specific visual features, pushing optimization toward a "universal" suffix that dominates the vision encoder's contribution and causes the robot to persist in targeted actions as visual context changes.

### Mechanism 3: Control Authority via Lack of Action Alignment
VLAs lack the "refusal" mechanisms common in LLMs (like RLHF to refuse harmful requests), allowing attackers to achieve "complete control authority" rather than just semantic jailbreaks. Without learned gradients repelling the model from outputting dangerous actions, the optimization landscape remains smooth and highly reachable for attackers.

## Foundational Learning

- **Concept: Autoregressive Action Tokenization**
  - Why needed: The attack relies on treating robot actions as language tokens, understanding that VLAs predict action sequences token-by-token like text
  - Quick check: Does the VLA output continuous torque values directly, or does it output discrete tokens that map to actions?

- **Concept: Greedy Coordinate Gradient (GCG)**
  - Why needed: This is the core algorithm used, a white-box attack that uses gradients to decide which token to swap without differentiating through discrete token selection
  - Quick check: In GCG, do we backpropagate through the token indices or the logits before sampling to find the best replacement token?

- **Concept: Sim-to-Real Transfer in Robotics**
  - Why needed: The paper claims attacks transfer from simulation to real-world, understanding this helps evaluate practical threat severity
  - Quick check: Why might an adversarial suffix optimized in simulation fail when applied to a physical robot in the real world?

## Architecture Onboarding

- **Component map**: Text Encoder (LLM Tokenizer) -> Vision Encoder (ViT) -> Transformer Backbone (Llama-based) -> Action Detokenizer (Maps output tokens -> Continuous 7-DoF Action)
- **Critical path**: Text Prompt -> Token Embeddings -> Joint Vision-Language Attention -> Action Token Logits. The attack injects noise into the Text Prompt stage to bias the Action Token Logits.
- **Design tradeoffs**:
  - White-box vs. Black-Box: Demonstrates white-box attacks; real-world threats may be black-box (transfer attacks), which are currently less effective but possible
  - Efficiency: Attack optimization takes 3-10 minutes on H100s - too slow for real-time reaction but fast for offline crafting of persistent attacks
- **Failure signatures**:
  - Diffusion-based VLAs: GCG token attack fails if output head is diffusion model (actions not generated autoregressively as tokens)
  - Normalization Clamping: If target actions outside training distribution's normalization statistics, attack may fail or cluster around nearest valid value
- **First 3 experiments**:
  1. Replicate Single-Step Attack: Run RoboGCG on OpenVLA-Base with target action of [0, 0, 0, 0, 0, 0, 0] to verify ≥90% success rate
  2. Test Persistence: Optimize suffix using 3 random images from "pick coke can" task and measure if robot continues target action for 10+ steps
  3. Evaluate Perplexity Defense: Implement text-only perplexity filter on input prompt and confirm if it blocks adversarial suffix without rejecting nominal instruction

## Open Questions the Paper Calls Out

- Can effective defenses for VLAs be developed that do not corrupt nominal task performance? The authors tested perplexity filtering and smoothing but found existing defenses either fail or degrade nominal performance significantly.
- Do adversarial attacks transfer effectively to diffusion-based VLA architectures? The GCG attack relies on autoregressive token prediction, but diffusion models denoise continuous action representations through an iterative process.
- Can vision-based adversarial perturbations achieve comparable control authority over VLAs? The paper only evaluates textual attacks and anticipates extending to vision-based attacks in future work.
- Why does task-specific fine-tuning increase VLA vulnerability to adversarial attacks? Fine-tuned models show 40-60% higher attack success rates, but the mechanistic relationship between fine-tuning and vulnerability remains unexplained.

## Limitations

- Limited action space evaluation: Tests only 1792 one-hot target actions across 7 dimensions rather than comprehensive exploration of all possible actions
- Simulation-to-real transfer validity: Claims attacks transfer but provides limited quantitative comparison between simulation and real-world environments
- Defense validation gap: Suggests perplexity-based filtering as defense but only validates through ablation experiments without comprehensive evaluation

## Confidence

**High Confidence** (supported by extensive experimental evidence):
- GCG-based attacks can achieve >90% success rate on single-step tasks across multiple OpenVLA fine-tuned variants
- Attacks can persist across multiple rollout steps when optimized with multi-image loss aggregation
- No equivalent "refusal" mechanisms exist in VLAs compared to LLMs, enabling complete control authority

**Medium Confidence** (supported by experimental evidence but with caveats):
- Transferability across different robotic environments (simulation to real-world)
- Effectiveness of perplexity-based filtering as a defense mechanism
- Clustering of failures in specific action space regions due to normalization

**Low Confidence** (primarily theoretical or limited evidence):
- Threat model applicability to black-box scenarios
- Long-term persistence under significant visual distribution shift
- Effectiveness of proposed defenses against adaptive adversaries

## Next Checks

1. **Multi-Environment Transfer Validation**: Systematically compare attack success rates across simulation environments (LIBERO, SIMPLER) and the real-world HYDRA setting using identical attack parameters to quantify degradation and establish realistic threat boundaries.

2. **Continuous Action Space Attack Testing**: Evaluate the attack against continuous action targets rather than one-hot vectors to test whether optimization can elicit specific non-grid-aligned actions that are more representative of real-world control scenarios.

3. **Adaptive Defense Evaluation**: Implement and evaluate a dynamic defense mechanism that adapts to adversarial suffixes in real-time, testing whether a moving window perplexity filter or action space constraint system can block attacks while maintaining nominal task performance.