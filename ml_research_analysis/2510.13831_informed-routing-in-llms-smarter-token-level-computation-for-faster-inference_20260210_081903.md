---
ver: rpa2
title: 'Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference'
arxiv_id: '2510.13831'
source_url: https://arxiv.org/abs/2510.13831
tags:
- routing
- sparsity
- training
- token
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces informed routing to address the limitations
  of greedy routing in dynamic token-level computation allocation for large language
  models (LLMs). While existing methods make binary execute-or-skip decisions based
  on immediate token importance, informed routing replaces this with execute-or-approximate
  decisions using a lightweight feature forecaster (LFF) that estimates unit outputs
  before routing.
---

# Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference

## Quick Facts
- arXiv ID: 2510.13831
- Source URL: https://arxiv.org/abs/2510.13831
- Authors: Chao Han; Yijuan Liang; Zihao Xuan; Daokuan Wu; Wei Zhang; Xiaoyu Shen
- Reference count: 12
- Primary result: Informed routing reduces training time by over 50% while matching or surpassing SkipGPT performance across multiple sparsity levels.

## Executive Summary
This paper introduces informed routing to address limitations in greedy routing for dynamic token-level computation allocation in large language models. While existing methods make binary execute-or-skip decisions based on immediate token importance, informed routing uses a lightweight feature forecaster (LFF) to estimate unit outputs before routing, enabling execute-or-approximate decisions. This approach assesses token recoverability rather than just immediate impact, preserving model fidelity while reducing computation. Experiments on Llama models (3B and 8B) demonstrate that informed routing achieves better efficiency-performance trade-offs than strong baselines, reducing training time by over 50% across multiple sparsity levels.

## Method Summary
The method employs a three-stage optimization pipeline: (1) LFF initialization, where lightweight feature forecasters are trained on pre-extracted features from a frozen LLM using L1 and cosine similarity losses; (2) router training, where a Gumbel-Softmax-based router learns to select between executing the original unit or using the LFF approximation while enforcing target sparsity; and (3) optional LoRA fine-tuning to recover any lost precision. The LFF uses a bottleneck architecture (d→r→d) to approximate residual transformations, particularly effective for self-attention modules. The router is trained to assess token recoverability, allowing it to bypass full computation when approximations are sufficient while preserving execution for complex transformations.

## Key Results
- Informed routing matches or surpasses SkipGPT performance across multiple sparsity levels (25%, 40%, 70%) on Llama-3B and 8B models
- Reduces training time by over 50% compared to standard approaches
- Self-attention modules are highly "predictable," with LFF selecting 71.4% of tokens from attention modules vs 58.0% in baselines at 25% sparsity
- LoRA fine-tuning stage effectively recovers any performance loss from approximation, enabling high sparsity without fidelity degradation

## Why This Works (Mechanism)

### Mechanism 1: Recoverability-Based Gating
The router replaces binary skip decisions with "execute-or-approximate" decisions by assessing whether a token's transformation can be accurately predicted by a Lightweight Feature Forecaster (LFF). This reduces irreversible information loss associated with zeroing out features, as decisions are based on token recoverability rather than immediate impact.

### Mechanism 2: Local Linear Approximation of Self-Attention
Self-attention modules exhibit properties that allow their outputs to be mimicked by significantly smaller linear networks. The LFF uses a bottleneck architecture to approximate the residual transformation of self-attention, enabling aggressive computation reduction in these sub-components while preserving full FFN computation for complex nonlinearities.

### Mechanism 3: Decoupled Gradient Isolation
Isolating LFF training from router training stabilizes convergence and minimizes training overhead. LFFs are trained locally on pre-extracted features with the LLM frozen, allowing massive parallelization and preventing the router from learning to rely on post-hoc recovery to fix its errors.

## Foundational Learning

- **Concept:** Dynamic Computation Allocation (DCA)
  - **Why needed here:** You must understand the baseline (e.g., Mixture-of-Depths, SkipGPT) to appreciate why "greedy" routing is identified as a bottleneck.
  - **Quick check question:** How does a standard DCA router differ from a static pruning mask?

- **Concept:** Gumbel-Softmax Estimator
  - **Why needed here:** The router makes discrete decisions but requires backpropagation. Understanding how to differentiate through discrete samples is critical for debugging router convergence.
  - **Quick check question:** How does the Gumbel-Softmax allow gradients to flow through a discrete routing decision during training?

- **Concept:** Feature Distillation/Approximation
  - **Why needed here:** The core novelty is the LFF, which acts as a teacher-student distiller but for intra-layer features. You need to understand loss functions like Cosine Similarity and L1 loss used to align the LFF with the original unit.
  - **Quick check question:** Why might Cosine Similarity be preferred over MSE for aligning feature direction in LLMs?

## Architecture Onboarding

- **Component map:** Token embeddings X → Norm(X) → Router (d→⌊d/4⌋-200→2) → Mux → Output
- **Critical path:**
  1. LFF Init: Train LFFs locally on frozen features (must converge quickly, <5 mins)
  2. Router Train: Train router to select path while enforcing sparsity constraint
  3. LoRA (Optional): Fine-tune residual projections to recover any lost precision

- **Design tradeoffs:**
  - LFF Capacity vs. Speed: A larger LFF approximates better but negates inference speedups; rank 100 is optimal for Llama-8B
  - Router Size: Router hidden dimension reduced by 200 from standard d/4 to accommodate LFF parameters without increasing total parameter count

- **Failure signatures:**
  - Collapse to Identity: Router selects LFF for all tokens but LFF outputs zeros, causing PPL to spike
  - Attention Starvation: At extreme sparsity (70%), LFF cannot maintain reasoning capability
  - KV Cache Mismatch: Masking skipped tokens in KV cache can drop PPL performance significantly

- **First 3 experiments:**
  1. Unit Sanity Check: Train LFF for a single attention layer and visualize cosine similarity distribution between original and approximated outputs
  2. Sparsity vs. PPL: Run Stage 2 at 25% and 40% sparsity, comparing PPL against SkipGPT-Router baseline
  3. Ablation on LFF Rank: Sweep LFF intermediate dimensions (10, 50, 100, 500) to find saturation point

## Open Questions the Paper Calls Out

- **Open Question 1:** Can more sophisticated forecaster architectures or hybrid strategies extend efficiency beyond extreme sparsity limits (>70%) where simple linear LFFs fail?
- **Open Question 2:** What are the actual wall-clock latency improvements during inference on hardware accelerators compared to theoretical FLOP reductions?
- **Open Question 3:** Does the observed "linear simplicity" of self-attention modules generalize to non-standard architectures like Mixture-of-Experts (MoE) or State Space Models (SSMs)?

## Limitations
- Approximation quality generalization to non-attention modules and complex architectures remains unclear
- Training time savings claims assume sufficient GPU memory for parallel feature extraction
- Sparsity enforcement mechanism details are not fully specified, affecting reproducibility

## Confidence

**High Confidence:**
- The informed routing paradigm is technically sound and represents a meaningful extension
- The three-stage optimization pipeline is well-defined and experimentally validated
- The claim that self-attention modules are "predictable" is supported by empirical evidence

**Medium Confidence:**
- Overall performance improvements over SkipGPT are robust across sparsity levels
- LoRA fine-tuning provides consistent quality recovery across conditions
- Computational savings claims are representative of typical implementation scenarios

**Low Confidence:**
- Generalization to architectures beyond standard transformer attention mechanisms
- Performance stability under extreme sparsity levels (70%) across diverse tasks
- Long-term behavior of routing policy during extended inference or on different data distributions

## Next Checks

1. **LFF Generalization Test:** Apply LFF architecture to a non-attention module (e.g., FFN) in Llama-8B and measure approximation quality, comparing L1 and cosine similarity losses against attention module results.

2. **KV Cache Reduction Validation:** Implement KV cache reduction optimization and measure trade-off between memory savings and perplexity degradation, systematically varying skip ratio threshold.

3. **Long-Range Context Performance:** Evaluate informed routing on tasks requiring long-range dependencies (e.g., LRA tasks, document-level QA) to test whether approximation quality degrades with complex attention patterns.