---
ver: rpa2
title: Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish
  View
arxiv_id: '2511.06722'
source_url: https://arxiv.org/abs/2511.06722
tags:
- grpo
- arxiv
- reasoning
- visual
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of effective data sampling
  for multimodal post-training, specifically focusing on how to identify and utilize
  samples of varying difficulty levels to improve both perception and reasoning capabilities
  in Multimodal Large Language Models (MLLMs). The authors propose two novel difficulty-aware
  sampling strategies: Progressive Image Semantic Masking (PISM), which quantifies
  sample hardness through systematic image degradation, and Cross-Modality Attention
  Balance (CMAB), which assesses cross-modal interaction complexity via attention
  distribution analysis.'
---

# Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View

## Quick Facts
- arXiv ID: 2511.06722
- Source URL: https://arxiv.org/abs/2511.06722
- Reference count: 8
- Primary result: GRPO-only training on medium+hard difficulty samples consistently outperforms SFT+GRPO pipelines in multimodal post-training.

## Executive Summary
This paper addresses the challenge of effective data sampling for multimodal post-training by proposing two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) and Cross-Modality Attention Balance (CMAB). The authors demonstrate that GRPO-only training on difficulty-stratified samples (medium and hard) consistently outperforms conventional SFT+GRPO pipelines, achieving superior results on both perception and reasoning tasks. The key finding is that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy.

## Method Summary
The method employs two complementary difficulty assessment strategies. PISM progressively masks image pixels and identifies failure thresholds to classify samples by visual sensitivity, while CMAB analyzes cross-modal attention distribution to assess reasoning complexity. Using these metrics, samples are stratified into easy, medium, and hard categories. The authors then train using either GRPO-only on mid+hard samples or hybrid SFT+GRPO pipelines, evaluating performance across six benchmark datasets. The training leverages reinforcement learning via Group Relative Policy Optimization (GRPO) with reward functions designed for multimodal tasks.

## Key Results
- GRPO-only training on medium+hard samples achieved 68.300 on MathVista, significantly outperforming full-set GRPO (53.400) and all SFT+GRPO variants.
- For visual perception tasks like OCRBench, GRPO-only achieved 77.800, surpassing all SFT+GRPO configurations.
- PISM excels in perception-heavy tasks while CMAB dominates reasoning tasks, suggesting task-specific selection of difficulty metrics.
- The approach demonstrates that data quality through strategic sample selection matters more than quantity or inclusion of supervised fine-tuning stages.

## Why This Works (Mechanism)

### Mechanism 1: Visual Sensitivity Quantification via Progressive Masking
Sample difficulty correlates with model fragility to controlled visual information loss. PISM progressively masks image pixels and identifies the failure threshold where accuracy drops below a specified threshold. Samples failing at low masking ratios require tight visual-textual coupling, while those surviving high masking rely primarily on textual cues.

### Mechanism 2: Cross-Modal Attention Balance as Cognitive Demand Proxy
Balanced attention between image and text tokens during generation signals higher reasoning complexity. CMAB computes per-token attention ratios and aggregates them to yield a measure of multimodal integration. Values in specific ranges indicate balanced multimodal integration (hard samples), while extreme values indicate single-modality dominance (easy samples).

### Mechanism 3: GRPO-Only Outperforms SFT+GRPO on Stratified Data
Reinforcement learning on medium+hard samples alone outperforms hybrid SFT→GRPO pipelines. SFT imposes template patterns constraining exploration, while GRPO-only preserves policy diversity with reward signals that directly reinforce correct reasoning paths. Difficulty stratification ensures GRPO focuses on informative samples, avoiding gradient dilution from easy/unsolved data.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Core RL algorithm; estimates advantage via group-level statistics rather than learned critic
  - Quick check: Given a prompt and N sampled responses with rewards r₁...r_N, how does GRPO compute the advantage for response i? (Answer: A_i = (r_i − mean(r)) / std(r))

- **Concept: Cross-Attention in Multimodal Transformers**
  - Why needed: CMAB analyzes how generated tokens attend to image vs. text tokens
  - Quick check: In a transformer layer generating token y_t, which matrices contain attention weights over image tokens vs. text tokens? (Answer: A(l,t) ∈ R^(1×(Limg+Ltxt)); split at Limg boundary)

- **Concept: Difficulty Stratification via Threshold-Based Classification**
  - Why needed: Both PISM and CMAB partition samples via thresholds
  - Quick check: If you shift λ_hard from 0.4 to 0.3, does the "hard" sample count increase or decrease? (Answer: Decrease—fewer samples cross the stricter threshold)

## Architecture Onboarding

- **Component map:** Difficulty Assessment Module (PISM + CMAB) -> Data Stratification Layer -> Training Orchestrator (GRPO-only or SFT+GRPO) -> Evaluation Framework
- **Critical path:** 1) Run baseline model to identify unsolved samples, 2) Apply PISM/CMAB to classify remaining samples, 3) Select mid+hard subset for GRPO-only training, 4) Evaluate on target benchmarks
- **Design tradeoffs:** PISM excels at perception tasks; CMAB excels at reasoning tasks. Stricter thresholds yield smaller, harder subsets—faster training but risk coverage gaps.
- **Failure signatures:** All samples classified as "easy" indicates threshold misconfiguration; GRPO divergence suggests reward sparsity; CMAB attention ratios near 0 or ∞ indicate extraction bugs.
- **First 3 experiments:** 1) Reproduce PISM stratification on held-out perception dataset, 2) Ablate threshold sensitivity by varying λ_hard, 3) Cross-validate CMAB on reasoning-heavy benchmark

## Open Questions the Paper Calls Out

### Open Question 1
Would the GRPO-only superiority over SFT+GRPO persist across different MLLM architectures and scales beyond Qwen2.5VL-7B?

### Open Question 2
How sensitive are PISM and CMAB difficulty classifications to the specific threshold values chosen (τ=0.1, λ_hard=0.4, λ_easy=0.7, ρ boundaries)?

### Open Question 3
Can a unified difficulty metric combining PISM (visual sensitivity) and CMAB (cross-modal attention balance) outperform task-specific usage?

### Open Question 4
How should "unsolved samples" (those the base model answers incorrectly) be optimally incorporated into post-training?

## Limitations
- Core difficulty metrics rely on specific architectural assumptions about attention extraction and visual degradation sensitivity that may not generalize across model families
- The paper does not provide ablation studies on threshold sensitivity or reward function design
- Comparison against SFT+GRPO baselines assumes equal computational budgets and identical training protocols

## Confidence

- **High Confidence:** GRPO-only on stratified data consistently outperforms full-set GRPO (68.300 vs 53.400 on MathVista)
- **Medium Confidence:** PISM and CMAB difficulty stratification effectively identifies informative samples for post-training
- **Low Confidence:** Attention-based difficulty assessment (CMAB) generalizes across different MLLM architectures

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically vary λ_hard and λ_easy thresholds for PISM to quantify performance stability across the mid+hard subset.
2. **Attention Pattern Validation:** Verify that CMAB's attention ratio ρ̄ correlates with human-annotated difficulty labels or response complexity metrics on a held-out reasoning dataset.
3. **Reward Function Transparency:** Implement and test alternative reward formulations for GRPO to assess sensitivity to reward design choices in the mid+hard training paradigm.