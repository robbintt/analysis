---
ver: rpa2
title: A Study of Rule Omission in Raven's Progressive Matrices
arxiv_id: '2510.03127'
source_url: https://arxiv.org/abs/2510.03127
tags:
- reasoning
- rules
- accuracy
- performance
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how well modern AI models generalize abstract\
  \ reasoning when trained on incomplete rule sets using Raven\u2019s Progressive\
  \ Matrices. The core idea is to remove one or two of the five structural rules (Number,\
  \ Position, Type, Size, Color) during training and test models on those omitted\
  \ rules."
---

# A Study of Rule Omission in Raven's Progressive Matrices

## Quick Facts
- arXiv ID: 2510.03127
- Source URL: https://arxiv.org/abs/2510.03127
- Authors: Binze Li
- Reference count: 13
- Primary result: AI models trained on incomplete rule sets show poor generalization to omitted rules, with transformer accuracy dropping from ~92% to ~31% when two rules are withheld.

## Executive Summary
This study investigates how well modern AI models generalize abstract reasoning when trained on incomplete rule sets using Raven's Progressive Matrices. The core idea is to remove one or two of the five structural rules (Number, Position, Type, Size, Color) during training and test models on those omitted rules. Both vision-based models (CoPINet, Dual-Contrast Network) and a sequence-to-sequence transformer are evaluated on the Impartial-RAVEN dataset. Results show that while transformers perform well on trained rules, their accuracy drops sharply when tested on novel rules—e.g., token accuracy falls from ~92% to ~31% when two rules are omitted. Vision models exhibit similar but weaker performance overall. The gap between token-level and full-answer accuracy highlights fundamental limitations in current architectures. These findings suggest that AI models rely more on memorization than genuine analogical reasoning, pointing to the need for architectures integrating symbolic reasoning and stronger inductive biases.

## Method Summary
The study uses Impartial-RAVEN dataset with five attributes governed by four rule types. Two training conditions are created: Remove 1 Rule (excluding all Progression instances) and Remove 2 Rules (excluding Progression and Arithmetic). Seven spatial configurations are tested. Models include a sequence-to-sequence transformer processing text representations and vision models (CoPINet, Dual-Contrast Network). Evaluation uses both token-level accuracy and full-answer accuracy on "Same Rule" (seen rules) vs "Different Rule" (omitted rules) test sets.

## Key Results
- Transformer token accuracy: ~92% on trained rules vs ~31% on omitted rules
- Vision models show similar generalization gaps but overall lower performance
- Token-level accuracy often exceeds 90% while full-answer accuracy drops below 50%
- Sequence models outperform vision models on held-out rules (47.00% vs 30.29% for Remove 1 Different)
- Configuration effects show Center > 2x2/3x3 performance patterns

## Why This Works (Mechanism)

### Mechanism 1
Removing specific rules during training exposes whether models learn transferable reasoning patterns or dataset-specific statistical correlations. By training on subsets of rules and testing exclusively on held-out rules, the experiment isolates each model's capacity for out-of-distribution generalization.

### Mechanism 2
The gap between token-level accuracy and full-answer accuracy reveals "fragile correctness"—models solve most of the problem but fail on critical differentiating features. High token accuracy on partial features should translate to proportionally high answer selection if reasoning is systematic.

### Mechanism 3
Sequence-based transformers outperform vision-based models on held-out rules because structured tokenization provides stronger inductive biases for relational reasoning. The text representation explicitly encodes attributes as discrete tokens, making relational patterns more accessible to attention mechanisms.

## Foundational Learning

- **Raven's Progressive Matrices structure**: Understanding the 3×3 matrix format, five attributes, and four rule types is essential for interpreting experimental conditions. Quick check: Can you explain why "Progression" and "Arithmetic" were chosen as the rules to omit during training?

- **Token-level vs. sequence-level evaluation metrics**: The paper's central finding depends on understanding why 90%+ token accuracy can coexist with <50% answer accuracy. Quick check: If a model predicts 9 out of 10 tokens correctly in an RPM answer, what is the minimum and maximum possible full-answer accuracy?

- **In-distribution vs. out-of-distribution generalization**: The "Same Rule" vs. "Different Rule" conditions operationalize this distinction. Interpreting results requires understanding why performance gaps matter for claims about reasoning vs. memorization. Quick check: In this paper's framing, would testing on a new dataset configuration using only Constant and Distribute Three rules count as OOD generalization for a model trained with Remove 2 Rules?

## Architecture Onboarding

- **Component map**: Input processing (images → CNN/ViT vs text → tokenization) → Encoder (CNN/ViT backbones vs multi-head self-attention) → Decoder/prediction (contrastive scoring vs token generation) → Training objective (contrastive loss vs cross-entropy)

- **Critical path**: Load I-RAVEN dataset → Apply rule-removal filters → Convert images to text for transformer → Train each model on filtered training set → Evaluate on Same/Different test sets → Compute token-level and full-answer accuracy

- **Design tradeoffs**: Text vs image input (explicit structure vs spatial layout), Token-level vs answer-level supervision (denser gradients vs true evaluation metric), Rule removal severity (harder OOD test vs reduced training diversity)

- **Failure signatures**: High token accuracy + low answer accuracy (brittle pattern matching), Large Same/Different gap (memorization), Vision models underperforming transformers (perceptual bottleneck), Performance variance across configurations (spatial complexity sensitivity)

- **First 3 experiments**: 1) Baseline replication on full I-RAVEN to establish ceiling performance, 2) Ablation on tokenization granularity to test discrete attribute encoding effects, 3) Cross-configuration transfer to distinguish spatial reasoning difficulty from training coverage

## Open Questions the Paper Calls Out

1. **Can intermediate representations effectively bridge the performance gap between sequence-based and vision-based models in abstract reasoning tasks?** The paper observes that sequence models outperform vision models but doesn't propose methods for unifying these modalities to leverage both strengths.

2. **How can model architectures be refined to close the gap between high token-level accuracy and low full-answer accuracy?** The study quantifies this discrepancy but doesn't identify architectural changes that would ensure partial correctness leads to correct holistic reasoning.

3. **What specific inductive biases are required to facilitate extrapolation to novel rules without explicit training?** The results show current models rely on pattern memorization rather than genuine reasoning, but the specific structural biases needed remain undefined.

## Limitations

- Rule removal implementation lacks precise specification of how rules are identified and filtered
- Configuration generalization claims don't establish whether differences reflect genuine architectural limitations or dataset-specific correlations
- Critical model architecture hyperparameters are not fully specified

## Confidence

**High confidence** in: The fundamental observation that token-level accuracy significantly exceeds answer-level accuracy across all models and conditions.

**Medium confidence** in: The conclusion that current architectures exhibit "fundamental limitations" in abstract reasoning.

**Low confidence** in: The specific claim that sequence-to-sequence transformers demonstrate superior reasoning capabilities compared to vision models.

## Next Checks

1. **Rule identification validation**: Implement and test multiple methods for identifying rule types in I-RAVEN samples to verify experimental manipulation produces intended training distribution.

2. **Token criticality analysis**: Conduct ablation studies isolating which token positions most frequently determine correct/incorrect answers to validate "fragile correctness" mechanism.

3. **Architecture hyperparameter sweep**: Systematically vary transformer depth, attention heads, and vision model configurations to determine whether performance differences persist across broader architectural space.