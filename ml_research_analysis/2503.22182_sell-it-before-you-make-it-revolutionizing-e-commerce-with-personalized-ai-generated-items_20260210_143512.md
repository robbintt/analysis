---
ver: rpa2
title: 'Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated
  Items'
arxiv_id: '2503.22182'
source_url: https://arxiv.org/abs/2503.22182
tags:
- preference
- personalized
- user
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items

## Quick Facts
- arXiv ID: 2503.22182
- Source URL: https://arxiv.org/abs/2503.22182
- Authors: Jianghao Lin; Peng Du; Jiaqi Liu; Weite Li; Yong Yu; Weinan Zhang; Yang Cao
- Reference count: 40
- Primary result: PerFusion outperforms baselines on personalized preference estimation (MAP/GAUC) and image generation quality (Aesthetic, CLIP, PerFusionRM) for e-commerce applications.

## Executive Summary
PerFusion introduces a personalized text-to-image generation framework for e-commerce that enables merchants to visualize custom product designs before manufacturing. The system combines user preference modeling with group-level preference optimization and a personalized diffusion model to generate images aligned with merchant-specific styles and price points. Evaluated on two real-world datasets, PerFusion demonstrates superior performance in both predicting user preferences and generating aesthetically pleasing, style-matched images while maintaining reasonable inference speed for production deployment.

## Method Summary
PerFusion addresses personalized text-to-image generation for e-commerce through two key innovations: (1) a feature-crossing-based personalized plug-in that injects user preferences into pretrained vision-language models for ranking tasks, and (2) a personalized diffusion model with ControlNet-style adaptive networks that condition generation on user embeddings. The framework employs group-level preference optimization using Plackett-Luce ranking instead of pairwise comparisons, better capturing how users evaluate multiple candidates simultaneously. Training proceeds in two stages: first optimizing PerFusionRM for preference estimation, then freezing it as a reward model for training the personalized generation model PerFusion.

## Key Results
- PerFusionRM achieves 2.27% MAP improvement and 3.64% GAUC improvement over PickScore baseline on Industrial dataset
- PerFusion generates images with 93.26% Aesthetic score and 96.10% PerFusionRM score on Industrial dataset
- Inference time increases from 1.49s to 3.03s per sample, with xFormers reducing memory from 9GB to 5GB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User profile features can be injected into pretrained vision-language models to predict personalized image preferences.
- Mechanism: A feature-crossing network transforms sparse user features into dense preference embeddings via multi-layer cross-interactions, which are fed through adaptive networks that output scale/shift parameters injected into each CLIP Transformer layer via element-wise addition to attention and FFN hidden states.
- Core assumption: User features encode stable, learnable visual preferences that generalize across sessions.
- Evidence anchors: [abstract] "feature-crossing-based personalized plug-in"; [Section 4.2] Equations 2-5 detail the crossing network and adaptive injection; "the last linear layer of the adaptive network is initialized with zero parameters"
- Break condition: If user features are noisy or preferences shift rapidly, the crossing network may overfit to spurious correlations—evidenced if PerFusionRM GAUC degrades on cold-start users.

### Mechanism 2
- Claim: Group-level preference optimization captures comparative decision-making more effectively than pairwise methods.
- Mechanism: Replaces Bradley-Terry (pairwise) with Plackett-Luce (ranking) model. For a candidate set split into positives P and negatives N, the objective maximizes the log probability of selecting positives given the full group context.
- Core assumption: Users evaluate candidates comparatively within the presented group, and positive samples are conditionally independent given the negative set.
- Evidence anchors: [abstract] "group-level preference optimization objective"; [Section 4.3.2 & Appendix A] Full derivation from Plackett-Luce; Appendix A.3 shows reduction to standard Diffusion DPO when |P|=|N|=1
- Break condition: If users make decisions independently per image or if group size varies wildly without normalization, the Plackett-Luce assumption may introduce bias.

### Mechanism 3
- Claim: Conditioning diffusion models on personalized embeddings via ControlNet-style parallel branches enables user-specific generation.
- Mechanism: User embedding u is reshaped to latent spatial dimensions, added to zₜ via 1×1 zero-conv, then passed through trainable copies of the 12 encoder blocks + middle block. Outputs are zero-convolved and added to decoder/middle blocks of the frozen U-Net.
- Core assumption: User preferences manifest as spatially-structured modifications to the denoising trajectory.
- Evidence anchors: [Section 4.3.1] "create trainable copies of the 12 encoding blocks and 1 middle block"; [Figure 4] Architecture diagram shows parallel branch with zero convolutions
- Break condition: If user preferences require semantic rather than spatial modifications, the spatial injection path may be insufficient—would manifest as high CLIP Score but low PerFusionRM for style-agnostic users.

## Foundational Learning

- **Latent Diffusion Models (Stable Diffusion)**:
  - Why needed here: PerFusion builds directly on SD's U-Net architecture; understanding the encoder-middle-decoder structure and how conditioning enters via cross-attention is essential for grasping where personalized adaptive networks attach.
  - Quick check question: Can you explain why zero-convolution initialization ensures the model starts identical to the frozen backbone?

- **Direct Preference Optimization (DPO) for Diffusion**:
  - Why needed here: The group-level objective generalizes Diffusion DPO; understanding how DPO avoids explicit reward modeling by reparameterizing through likelihood ratios is prerequisite to the Plackett-Luce extension.
  - Quick check question: What does the β hyperparameter control in DPO, and how does increasing it affect optimization?

- **Feature Crossing Networks (DCN-style)**:
  - Why needed here: The personalized plug-in uses explicit feature crossing to capture co-occurrence patterns in sparse user features; this differs from pure MLP embedding.
  - Quick check question: Why might feature crossing outperform simple concatenation + MLP for sparse categorical user features like shop ID and style?

## Architecture Onboarding

- **Component map**: User features F → embedding layer → feature crossing network → user representation u → adaptive networks → injected into CLIP/SD layers via element-wise addition

- **Critical path**: Collect user interaction logs → Train PerFusionRM first to validate personalized signal exists → Freeze trained PerFusionRM as reward model for generation training → Train PerFusion with group-level objective; zero-conv ensures stable initialization

- **Design tradeoffs**:
  - **Duplication vs. sharing plug-ins**: Ablation shows sharing text/vision plug-ins degrades GAUC—knowledge conflicts between modalities
  - **Training cost**: Table 4 shows PerFusion is ~2× slower per batch than Diffusion-DPO (6.29s vs 3.05s) due to adaptive network overhead
  - **Memory**: 9GB FP16 vs 5GB for SD1.5; production uses xFormers to mitigate

- **Failure signatures**:
  - PerFusionRM GAUC close to baseline → personalized signal weak; check feature coverage or crossing network depth
  - High Aesthetic/CLIP Score but low PerFusionRM → model captures general quality but not user-specific preferences
  - Return rate increases post-deployment → generated images misrepresent manufacturable products; check human expert scores (craft design at 82% of human baseline is weakest dimension)

- **First 3 experiments**:
  1. **Validate personalized signal**: Train PerFusionRM on held-out users; if MAP improvement <2% vs. PickScore, user features lack predictive power—revisit feature engineering before generation training
  2. **Ablate group size**: Train with N=2 (pairwise) vs. N=5 (group); expect larger N to show greater PerFusionRM improvement if comparative behavior is real
  3. **Cold-start test**: Evaluate PerFusion on users with <10 historical records; expect degraded performance, defining the operational boundary for deployment

## Open Questions the Paper Calls Out

- **Open Question 1**: How can personalization frameworks extend from merchant-level design preferences to end-customer purchasing preferences?
  - Basis: The conclusion states, "Future work could explore more comprehensive personalization from merchants to end customers."
  - Why unresolved: Current PerFusion optimizes for merchant features rather than end consumer taste, creating a gap in the full e-commerce personalization loop.
  - What evidence would resolve it: Deployment results showing a model trained on end-user interaction data achieving higher conversion rates than the merchant-optimized baseline.

- **Open Question 2**: How can the fidelity of "craft design" in AI-generated items be improved to match human-designed standards?
  - Basis: Table 6 shows "Craft Design" scores lowest (82.28% relative to human) compared to other aspects like "Color Coordination" (97.77%) and "Design Aesthetics" (95.85%).
  - Why unresolved: Current diffusion models struggle with structural garment details relative to surface-level aesthetics.
  - What evidence would resolve it: A new model variant achieving "Craft Design" scores statistically indistinguishable from human-designed items in expert evaluations.

- **Open Question 3**: Can the computational overhead of the personalized adaptive network be reduced for real-time large-scale deployment?
  - Basis: Section 5.5 notes that PerFusion increases inference time to ~3.03s/sample from ~1.49s due to the personalized adaptive network.
  - Why unresolved: Additional network parameters significantly increase latency, which may hinder scalability despite business value.
  - What evidence would resolve it: A distilled or quantized model version that retains PerFusionRM scores while reducing inference time to near-baseline levels.

## Limitations

- The framework requires substantial training data per user (minimum 10 group-level records) limiting effectiveness for new or infrequent users
- Computational overhead from personalized adaptive networks increases inference time by 2× and memory usage by 80% compared to baseline models
- "Craft Design" quality remains 18% below human-designed standards, suggesting structural details are not well captured by current diffusion approaches

## Confidence

- **Personalized plug-in mechanism**: Medium confidence—theoretically sound but dataset-dependent with marginal gains on PickaPic
- **Group-level preference optimization**: High confidence in formulation but Low confidence in behavioral validity—no corpus evidence users make comparative choices across five candidates
- **ControlNet-style personalization**: Medium confidence—spatial injection mechanism is architecturally clear but unproven for semantic preference modifications

## Next Checks

1. **Cold-start validation**: Deploy PerFusionRM on users with <10 historical records to empirically define the operational boundary where personalization degrades

2. **Group size sensitivity**: Systematically train PerFusion with N=2 (pairwise) vs. N=5 (group) on Industrial dataset to determine if group-level objective adds value

3. **Feature coverage audit**: Cross-reference user feature availability across Industrial and PickaPic datasets to identify minimum feature set required for meaningful PerFusionRM improvement