---
ver: rpa2
title: 'The Model''s Language Matters: A Comparative Privacy Analysis of LLMs'
arxiv_id: '2510.08813'
source_url: https://arxiv.org/abs/2510.08813
tags:
- privacy
- memorization
- language
- english
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how linguistic structure influences privacy
  leakage in LLMs across four languages (English, Spanish, French, Italian) using
  three attack vectors: extraction, counterfactual memorization, and membership inference.
  Results show that privacy vulnerability scales with linguistic redundancy and tokenization
  granularity: Italian exhibits the strongest leakage (up to 13,000 unique extractions
  with 25-word prompts), while English shows higher membership separability.'
---

# The Model's Language Matters: A Comparative Privacy Analysis of LLMs

## Quick Facts
- arXiv ID: 2510.08813
- Source URL: https://arxiv.org/abs/2510.08813
- Reference count: 40
- Primary result: Privacy vulnerability in LLMs scales with linguistic redundancy and tokenization granularity, with Italian showing strongest leakage and French/Spanish displaying greater resilience due to morphological complexity.

## Executive Summary
This study investigates how linguistic structure influences privacy leakage in LLMs across four languages (English, Spanish, French, Italian) using three attack vectors: extraction, counterfactual memorization, and membership inference. Results show that privacy vulnerability scales with linguistic redundancy and tokenization granularity: Italian exhibits the strongest leakage (up to 13,000 unique extractions with 25-word prompts), while English shows higher membership separability. French and Spanish display greater resilience due to higher morphological complexity. Quantitatively, English achieved 59% membership inference accuracy while French only 50%, demonstrating that language structure is a significant determinant of privacy risk in LLM deployments.

## Method Summary
The study uses the HiTZ Multilingual Medical Corpus (10k sentences selected from 3M+), splitting data 80/20 for training/testing. Monolingual models were fine-tuned per language: encoder models (BERT-family) for membership inference and counterfactual memorization tasks, and decoder models (GPT-family) for extraction tasks. Nine length-binned classification tasks were used for training. Ten independently seeded models per language were trained for counterfactual analysis. Shadow models with XGBoost classifiers were used for membership inference across epochs 1-30, testing extraction with prompts of 5, 12, 25, and 37 words.

## Key Results
- Italian showed the strongest extraction vulnerability with up to 13,000 unique extractions using 25-word prompts
- English achieved 59% membership inference accuracy while French only 50%
- French and Spanish displayed greater resilience due to higher morphological complexity
- Privacy vulnerability scales with linguistic redundancy and tokenization granularity

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Redundancy and Tokenization Granularity
High redundancy and longer word lengths create denser, repetitive token patterns during subword tokenization. These repetitive patterns are more easily "greedy-decoded" by generative models, allowing attackers to extract verbatim training data using shorter prompts. This mechanism assumes subword tokenization fragmentation correlates directly with the model's ability to overfit to specific surface patterns rather than abstract semantics.

### Mechanism 2: Morphological Complexity as Natural Obfuscation
Morphologically rich languages distribute semantic information across many inflectional variants, creating a "natural obfuscation" effect where the model learns underlying lemmas rather than specific surface forms. This reduces the loss disparity between training (member) and non-training (non-member) samples, lowering membership inference accuracy.

### Mechanism 3: Syntactic Entropy and Capitalization Effects
Languages with high syntactic entropy (diverse word orders) and frequent capitalization possess unique structural fingerprints. Encoder models overfit to these distinct syntactic patterns and entity positions, creating sharper confidence gaps between data seen during training versus unseen data.

## Foundational Learning

**Concept: Membership Inference Attacks (MIA)**
- Why needed: Core evaluation metric for privacy leakage in encoder experiments
- Quick check: If a model generalizes perfectly, what would the MIA accuracy be? (Answer: ~50%, indistinguishable from random guessing)

**Concept: Morphological Typology (Inflectional vs. Analytic)**
- Why needed: The paper relies on the distinction between languages that use many word forms vs. those that use helper words as the causal factor for privacy resilience
- Quick check: Why would a model struggle to overfit a specific sentence in a language with 50 forms per verb compared to a language with 1 form?

**Concept: Subword Tokenization (BPE/WordPiece)**
- Why needed: The mechanism of "tokenization granularity" depends on how different languages split into tokens
- Quick check: Does a longer average word length always result in more tokens per sentence in a subword tokenizer, or fewer?

## Architecture Onboarding

**Component map:** HiTZ Corpus -> Language-specific pre-processing -> Linguistic profiling (M, R, S metrics) -> Monolingual model fine-tuning -> Attack execution (Extraction/MIA)

**Critical path:** 1) Corpus ingestion and pre-processing 2) Profile linguistic metrics to predict vulnerability profile 3) Fine-tune monolingual models 4) Execute attacks

**Design tradeoffs:**
- Monolingual vs. Multilingual Models: Monolingual models isolate language effects but create cross-lingual privacy transfer risks
- Prompt Length vs. Extraction Yield: Increasing prompt length increases extraction generally, but Italian shows a sharper incline than others

**Failure signatures:**
- The "Saturation" Effect: If extraction counts saturate or decline with larger prompts, the model is generalizing rather than regurgitating
- High Overlap: If the "in" and "out" confidence distributions significantly overlap, the defense is working; distinct clusters indicate failure of generalization

**First 3 experiments:**
1. Reproduce Italian vs. French Extraction: Fine-tune GPT-2 variants on small Italian and French medical corpora, verify Italian yields >2x unique extractions with 5-word prompts
2. Morphological Perturbation Test: Artificially "inflect" English words to increase morphological complexity, retrain and verify if MIA accuracy drops
3. Cross-Entropy Thresholding: For Spanish BERT model, plot loss distribution of training vs. test data, confirm tighter overlap than English model

## Open Questions the Paper Calls Out

**Open Question 1:** Does cross-lingual transfer in unified multilingual models alter the relationship between specific linguistic structures and privacy leakage observed in monolingual fine-tuning?

**Open Question 2:** Do the protective effects of high morphological complexity against extraction attacks persist when scaling training data from thousands to millions of examples?

**Open Question 3:** Can differential privacy mechanisms be optimized by dynamically adjusting noise levels based on the linguistic redundancy or morphological complexity of the specific language being processed?

## Limitations

- Linguistic Metric Validation Gap: Metrics derived from training corpus not directly validated against privacy outcomes
- Domain-Specific Confounds: Medical text's repetitive structures may artificially inflate linguistic redundancy scores
- Tokenization Granularity Assumptions: Paper does not control for tokenizer-specific fragmentation patterns

## Confidence

**High Confidence:** Membership inference attack results (English 59% vs French 50%) and extraction vulnerability ranking (Italian > English > Spanish > French)
**Medium Confidence:** Claim that morphological complexity reduces privacy vulnerability
**Low Confidence:** Linguistic profiling framework as predictive tool for privacy vulnerability across arbitrary languages

## Next Checks

**Validation 1:** Replicate extraction attack experiments using general-purpose multilingual corpus (Wikipedia/OSCAR) rather than medical text to test generalizability

**Validation 2:** Conduct ablation studies controlling for tokenization effects using language-specific tokenizers or manual word-level segmentation

**Validation 3:** Train regression model using proposed linguistic metrics (M, R, S) to predict extraction counts or MIA accuracy on held-out language (Portuguese/German) to test predictive power