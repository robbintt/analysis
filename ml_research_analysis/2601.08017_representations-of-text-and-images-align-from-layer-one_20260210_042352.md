---
ver: rpa2
title: Representations of Text and Images Align From Layer One
arxiv_id: '2601.08017'
source_url: https://arxiv.org/abs/2601.08017
tags:
- images
- image
- layer
- text
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to investigate when representations
  of concepts in text and images align in vision-language models. Instead of searching
  datasets, the authors synthesize images from scratch by optimizing them to match
  the representation of a textual concept at a specific layer.
---

# Representations of Text and Images Align From Layer One

## Quick Facts
- arXiv ID: 2601.08017
- Source URL: https://arxiv.org/abs/2601.08017
- Reference count: 27
- Key outcome: Cross-modal alignment emerges from layer 1 in VLMs without explicit training

## Executive Summary
This paper introduces a novel method to investigate when representations of concepts in text and images align in vision-language models. Instead of searching datasets, the authors synthesize images from scratch by optimizing them to match the representation of a textual concept at a specific layer. They then test whether an independent model recognizes the synthesized image as depicting the concept. Applied to Gemma 3 4B across seven layers and over 100 concepts, the method reveals that representations of text and images meaningfully align from layer 1 for many concepts including animals, seasons, activities, and emotions—contrary to prior findings that alignment only emerges in mid-to-late layers.

## Method Summary
The authors extract concept vectors from text at specific VLM layers by averaging activations at concept token positions and subtracting a language baseline. They then synthesize images using Direct Ascent Synthesis, optimizing multi-resolution perturbation components to maximize cosine similarity between image representations and text concept vectors. Image representations are computed via attention-weighted aggregation of centered patch activations. The synthesized images are validated using an independent model (GPT-5) with both lenient (category hints) and stringent (open-ended) recognition protocols.

## Key Results
- Recognition rates exceed 50% for multiple concept categories in early layers (1-5)
- Alignment emerges from layer 1 despite the model never being explicitly trained for cross-modal alignment
- Three categories (famous people, nationalities, LLM tasks) produce near-zero recognizable images across all layers
- Gemma shows middle-layer collapse (layers 15-20) not observed in InternVL architecture

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Synthesis via Cosine Similarity Maximization
- Claim: Optimizing images to maximize cosine similarity with text-derived concept vectors produces visually meaningful outputs that transfer to independent models
- Mechanism: Start from neutral gray, parameterize perturbation as multi-resolution components, apply augmentations (random shifts, noise), and use gradient ascent to align patch-aggregated image representations with centered text concept vectors at target layers
- Core assumption: Linear representation hypothesis—concepts are encoded as directions in activation space, so cosine similarity meaningfully captures semantic alignment
- Break condition: If cosine similarity cannot discriminate between matching/mismatching pairs (as in Gemma's middle layers 15–22 where both concepts align more with noise than real images), synthesis fails

### Mechanism 2: Early Cross-Modal Alignment Emerges Without Explicit Training
- Claim: Vision-language models develop cross-modal representational alignment from layer 1 as an emergent property of end-to-end training, despite frozen vision encoders and no explicit alignment objectives
- Mechanism: During VLM training, the language model learns to process visual tokens alongside text for next-token prediction; shared representations form because both modalities must contribute to the same output space, causing semantic directions to align
- Break condition: If explicit cross-modal loss terms or different adapter architectures are required, spontaneous early alignment may not occur

### Mechanism 3: Recognition Transfer Validates Constructive Alignment
- Claim: If an independent model (GPT-5) recognizes synthesized images as depicting target concepts, this provides constructive evidence that textual representations contained recoverable visual information
- Break condition: If synthesized images transfer but lack human recognizability, evidence becomes weaker; if evaluation model shares training data or architecture artifacts with target model, transfer may be spurious

## Foundational Learning

- **Concept: Linear Representation Hypothesis**
  - Why needed here: The entire synthesis method assumes concepts are directions in activation space that cosine similarity can capture; understanding this justifies the optimization objective
  - Quick check question: Given two word embeddings, what would you expect their cosine similarity to measure vs. what it actually measures?

- **Concept: Multi-Scale Optimization for Adversarial Robustness**
  - Why needed here: The Direct Ascent Synthesis method uses multi-resolution components specifically to avoid adversarial patterns that exploit model quirks without semantic content
  - Quick check question: Why would optimizing pixels directly produce different results than optimizing multi-resolution components that get upscaled and summed?

- **Concept: Constructive Evidence vs. Proving Absence**
  - Why needed here: The paper establishes a lower bound—if synthesis fails, it could be optimization failure, not absence of alignment; understanding this asymmetry prevents over-interpreting null results
  - Quick check question: If layer 10 produces no recognizable images for concept X, what can and cannot you conclude about alignment at that layer?

## Architecture Onboarding

- **Component map**: Vision encoder (frozen) -> Linear projector (optional) -> Language model transformer layers (1-34) -> Text concept extraction -> Image representation aggregation -> Synthesis engine -> Augmentation -> Independent model evaluation

- **Critical path**:
  1. Extract `rep_ℓ(w)` by averaging activations at concept token positions, subtract language baseline
  2. Initialize multi-resolution perturbation layers (start at 8×8, step by 20 up to 448×448)
  3. Forward: upscale and sum components, apply tanh, add to gray base
  4. Augment image (shift + noise)
  5. Pass through VLM, extract patch activations at layer ℓ
  6. Aggregate patches via attention-weighted sum (semantic similarity to target + spatial prior)
  7. Compute loss: negative cosine similarity between aggregated image rep and text concept vector
  8. Backprop through multi-resolution components only
  9. After 600 steps, evaluate with independent model

- **Design tradeoffs**:
  - Temperature τ (0.005 vs 0.5): Low values focus attention on most relevant patches but may miss distributed features; paper uses layer-specific values
  - Spatial prior σ schedule (2→16): Early training focuses attention centrally for coherent objects; later training allows distributed patterns
  - Category hints in evaluation: Lenient protocol reveals more alignment but introduces context; stringent protocol is cleaner but masks genuine alignment
  - Learning rates differ by layer (0.15 for middle, 0.04 for early/late): Empirical tuning; middle layers require more aggressive optimization to overcome collapse

- **Failure signatures**:
  - Middle-layer collapse (layers 15-20 in Gemma): Recognition drops to near-zero; cosine similarity discriminates poorly between matching/mismatching pairs; white noise outperforms real images
  - Text-in-image artifacts: Synthesis produces embedded text describing concept rather than visual depiction
  - Transfer without human recognizability: Independent model recognizes image but humans don't
  - Optimization divergence: Loss fails to decrease

- **First 3 experiments**:
  1. Replicate apple/orange cosine similarity analysis on your target VLM across all layers. Plot similarity between text concept vectors and matching/mismatching images plus noise controls.
  2. Single-concept synthesis sanity check: Run synthesis for "dog" at layer 1, 10, and 25 with default hyperparameters. Evaluate qualitatively and quantitatively.
  3. Aggregation method ablation: Compare attention-weighted aggregation vs. simple mean aggregation for animal concepts.

## Open Questions the Paper Calls Out

- **Open Question 1**: What causes the middle-layer collapse in Gemma 3 4B, where image recognisability drops sharply in layers 15–20, and does this phenomenon generalize across different VLM architectures?

- **Open Question 2**: How can the synthesis method be improved to produce reliably human-recognisable images across all concept categories, not just model-recognisable ones?

- **Open Question 3**: Why do famous people, nationalities, and LLM tasks produce near-zero recognisable images across all layers—do these concepts lack visual grounding, or does the method fail to recover it?

## Limitations

- The method cannot distinguish between absence of alignment and optimization failure, establishing only a lower bound on alignment
- Middle-layer collapse observed in Gemma but not InternVL suggests findings may be architecture-specific rather than universal VLM behavior
- Synthesized images often achieve model recognition without human recognizability, limiting interpretability applications

## Confidence

- **High Confidence**: Empirical observation that synthesized images achieve >50% recognition rates for many concepts in early layers
- **Medium Confidence**: Interpretation that early alignment is an emergent property of end-to-end training rather than explicit design
- **Low Confidence**: Claim that this phenomenon is universal across VLMs, given contrasting behavior in InternVL

## Next Checks

1. **Architecture-Specific Validation**: Test the synthesis method on multiple VLMs (InternVL, CLIP, Qwen2-VL) to determine whether Gemma's middle-layer collapse is a general phenomenon or architecture-specific quirk.

2. **Human Recognition Validation**: Evaluate synthesized images using human annotators alongside model recognition to assess whether high model recognition rates correspond to human-perceivable concepts.

3. **Training Objective Manipulation**: Train VLMs with and without explicit cross-modal alignment objectives to test whether early alignment requires implicit grounding or can emerge through other mechanisms.