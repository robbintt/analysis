---
ver: rpa2
title: 'LOTION: Smoothing the Optimization Landscape for Quantized Training'
arxiv_id: '2510.08757'
source_url: https://arxiv.org/abs/2510.08757
tags:
- loss
- lotion
- quantized
- quantization
- rounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LOTION smooths quantized training by replacing the discontinuous
  quantized loss with its expectation under unbiased randomized rounding. This creates
  a continuous, almost-everywhere differentiable surrogate that preserves global minima
  of the original quantized problem.
---

# LOTION: Smoothing the Optimization Landscape for Quantized Training

## Quick Facts
- **arXiv ID**: 2510.08757
- **Source URL**: https://arxiv.org/abs/2510.08757
- **Reference count**: 37
- **Primary result**: LOTION smooths quantized training by replacing the discontinuous quantized loss with its expectation under unbiased randomized rounding, creating a continuous, almost-everywhere differentiable surrogate that preserves global minima and improves INT4 validation loss over QAT and PTQ baselines.

## Executive Summary
LOTION addresses the discontinuity problem in quantization-aware training (QAT) by smoothing the quantized loss surface through expectation under unbiased randomized rounding noise. This creates a continuous, almost-everywhere differentiable objective that preserves all global minima of the original quantized problem. The smoothed loss can be interpreted as a curvature-aware regularizer dependent on the Gauss-Newton matrix and quantization noise variance. Experiments on synthetic linear regression and 150M/300M-parameter language models demonstrate that LOTION consistently outperforms standard QAT and post-training quantization (PTQ) baselines, particularly at low bit-widths like INT4, while being applicable to both INT4 and FP4 quantization formats.

## Method Summary
LOTION replaces the discontinuous quantized loss with its expectation under unbiased randomized rounding noise, creating a continuous surrogate objective. The method computes a curvature-aware regularizer based on the Gauss-Newton diagonal (approximated by empirical Fisher) and quantization noise variance, which depends on per-parameter scaling. During training, LOTION adds this regularizer to the task loss with a hyperparameter λ controlling regularization strength. The approach maintains full-precision forward passes while simulating quantization, and importantly preserves all global minima of the original quantized problem. Experiments show consistent improvements over QAT and PTQ baselines on both synthetic and large language model tasks.

## Key Results
- On 150M-parameter models, LOTION achieves 3.276 INT4 validation cross-entropy versus 3.315 for QAT and 3.864 for PTQ
- Results persist at 300M scale and extend to modern FP4 quantization
- LOTION requires no additional parameters or tuning beyond the regularization coefficient λ
- The method outperforms previous noise-based approaches like NIPQ without introducing extra complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized rounding creates a differentiable surrogate from a piecewise-constant quantized loss
- Mechanism: LOTION optimizes the expected loss E_ε~RR(w)[L(w+ε)] where ε is zero-mean randomized rounding noise. This expectation is continuous and differentiable almost everywhere because it smooths over quantization bin boundaries
- Core assumption: The loss function L is continuous with respect to L2 norm
- Evidence anchors: Abstract states expectation yields "continuous, almost-everywhere differentiable objective"; Section 3 proves continuity; Lemma 1 shows E_q~f(w)[L(q)] is continuous

### Mechanism 2
- Claim: The smoothed loss implicitly adds a curvature-aware regularizer proportional to the Gauss-Newton diagonal and quantization noise variance
- Mechanism: Second-order Taylor expansion of L(w+ε) with E[ε]=0 yields L_smooth(w) ≈ L(w) + ½tr(H·Σ_ε). For neural networks, the full Hessian H is approximated by the positive-semidefinite Gauss-Newton matrix G, giving the diagonal form: Σ_reg = ½ Σ_i g_ii · s²_B · ∆_i(1-∆_i)
- Core assumption: Gauss-Newton approximates curvature well; higher-order O(‖ε‖³) terms are negligible
- Evidence anchors: Abstract mentions "curvature-aware regularizer dependent on Gauss-Newton component"; Section 3.2 derives closed form for quadratic; Section 3.3 shows diagonal GN form

### Mechanism 3
- Claim: The smoothed loss preserves all global minima of the original quantized problem
- Mechanism: By definition, when w∈Q (already quantized), RR(w) has probability 1 on w, ensuring min_w E_q~RR(w)[L(q)] = min_w L(cast(w))
- Core assumption: Randomized rounding satisfies all three axioms (unbiased, continuous, exact on Q)
- Evidence anchors: Abstract states "preserves all global minima"; Section 3.1 Definition 1 axiom 3; Lemma 2 proves min_w E[L(q)] = min_w L(cast(w))

## Foundational Learning

- **Straight-Through Estimator (STE) limitations**
  - Why needed here: LOTION is motivated by STE's lack of convergence guarantees and gradient instability at low bit-widths
  - Quick check question: Why does treating a quantizer as identity in backprop provide no theoretical guarantees?

- **Gauss-Newton matrix decomposition**
  - Why needed here: LOTION uses GN (not full Hessian) to avoid negative curvature in the regularizer
  - Quick check question: What are the two components of ∇²ℓ and why is the GN component always positive semi-definite?

- **Fine-grained shared-scale quantization**
  - Why needed here: The variance term σ²_i = s²_B · ∆_i(1-∆_i) depends on block-wise scaling
  - Quick check question: How does the scale s_B relate to the quantization bin width?

## Architecture Onboarding

- **Component map**: Full-precision forward pass -> Compute per-tensor scale s_B -> Calculate variance σ²_i = s²_B · ∆_i(1-∆_i) -> Accumulate empirical Fisher diagonal -> Add regularization term (λ/2) Σ_i g_ii · σ²_i -> Total loss

- **Critical path**: 1) Compute per-tensor absmax scale s_B, 2) For each weight compute ∆_i = frac(w_i/s_B), 3) Compute σ²_i = s²_B · ∆_i(1-∆_i), 4) Accumulate g_ii via empirical Fisher, 5) Add regularization term to loss

- **Design tradeoffs**: 
  - Empirical Fisher vs. true GN diagonal: Cheaper but potentially less accurate curvature estimate
  - Not differentiating through Fisher: Faster but ignores regularizer's gradient contribution
  - Hyperparameter λ: Controls regularization strength; paper tests [3000, 10000, 30000, 100000]

- **Failure signatures**: 
  - Loss plateaus higher than QAT: λ may be too large, over-regularizing
  - Jagged validation curves: λ too small, insufficient smoothing
  - Poor INT4 vs INT8 gap: Check scale computation handles small values correctly in FP4/int4

- **First 3 experiments**:
  1. **Synthetic linear regression sanity check**: Replicate Section 4.1 with power-law covariance; verify LOTION validation loss < PTQ < QAT at INT4
  2. **Ablation on λ**: Train 150M model with λ∈{0, 3000, 10000, 30000}; plot quantized validation loss curves to find sweet spot
  3. **Format transfer test**: Apply same λ to INT4 and FP4 on 150M; verify gains persist (per Section 4.3.3)

## Open Questions the Paper Calls Out

- **Can LOTION be extended to activation quantization while preserving theoretical guarantees and empirical gains?**
  - Basis: "Finally, another promising direction for future research is extending LOTION to activation quantization to further improve compute and memory efficiency."
  - Why unresolved: Current framework only addresses weight quantization; activations introduce additional complexity in quantization noise distribution during forward pass
  - What evidence would resolve it: Theoretical derivation showing how randomized rounding applies to activations, plus empirical results on models with quantized activations

- **Do alternative noise distributions exist that yield a fully smooth (everywhere differentiable) loss surface while still preserving global minima of the original quantized problem?**
  - Basis: "Using other noise distributions for obtaining a smooth loss surface while preserving the global minima property is an interesting research direction."
  - Why unresolved: Randomized rounding leaves undefined derivatives at quantization bin boundaries; smoothed loss is only "almost-everywhere differentiable"
  - What evidence would resolve it: Identification of noise distribution with theoretical proofs of complete differentiability and preservation of global minima

## Limitations

- Relies on approximations (empirical Fisher for Gauss-Newton, per-tensor scaling) that may limit regularizer effectiveness
- Requires manual tuning of regularization coefficient λ across different bit-widths and model scales
- Experimental validation only covers models up to 300M parameters, leaving scalability to billion-parameter models uncertain

## Confidence

- **High confidence**: Theoretical framework for smoothing quantized loss via expectation and preservation of global minima are well-supported by proofs
- **Medium confidence**: Curvature-aware regularization interpretation relies on empirical Fisher approximation, whose impact on real-world training is less certain
- **Medium confidence**: Experimental results show consistent gains, but exact tuning process for λ and block granularity are not fully specified

## Next Checks

1. **Ablation on Empirical Fisher Approximation**: Compare LOTION regularizer using true Gauss-Newton vs. empirical Fisher; measure impact on INT4 validation loss and training stability

2. **Scale Granularity Sensitivity**: Test LOTION with per-tensor vs. per-channel vs. per-weight block scales; quantify how block choice affects regularization magnitude and final quantized validation loss

3. **Out-of-Distribution Model Scaling**: Apply LOTION to 1B+ parameter transformer; check if gains persist or if new scaling issues (vanishing gradients, over-regularization) emerge