---
ver: rpa2
title: 'Olica: Efficient Structured Pruning of Large Language Models without Retraining'
arxiv_id: '2506.08436'
source_url: https://arxiv.org/abs/2506.08436
tags:
- pruning
- olica
- language
- layer
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Olica, a structured pruning framework for\
  \ Large Language Models (LLMs) that eliminates the need for retraining. The method\
  \ leverages Principal Component Analysis (PCA) to compress the Multi-Head Attention\
  \ (MHA) layer by treating the matrix products WqW\u22A4k and WvW\u22A4o as unified\
  \ entities."
---

# Olica: Efficient Structured Pruning of Large Language Models without Retraining

## Quick Facts
- **arXiv ID**: 2506.08436
- **Source URL**: https://arxiv.org/abs/2506.08436
- **Reference count**: 40
- **Primary result**: Achieves 33% sparsity on LLaMA-7B with 19.83 perplexity and 61.21% average accuracy without retraining

## Executive Summary
Olica presents a novel structured pruning framework for Large Language Models that eliminates the need for costly retraining while maintaining high performance. The method leverages Principal Component Analysis (PCA) to compress Multi-Head Attention (MHA) layers by treating key-query and value-output matrix products as unified entities, achieving significant parameter reduction. A linear calibration strategy using ridge regression-derived low-rank matrices mitigates error accumulation in Feed-Forward Network (FFN) layers. Extensive experiments demonstrate Olica's superiority across multiple benchmarks including WikiText2 and downstream tasks, with substantial reductions in data requirements, GPU memory usage, and runtime compared to existing pruning techniques.

## Method Summary
Olica employs a two-pronged approach to structured pruning of LLMs without retraining. For MHA layers, it uses PCA to compress the concatenated matrix products Wq*Wk^T and Wv*Wo^T, preserving essential attention patterns while reducing parameters. The FFN layer compression utilizes a linear calibration strategy where low-rank matrices derived from ridge regression solutions compensate for reconstruction errors. This approach avoids the need for fine-tuning while maintaining model quality. The method specifically targets structured pruning, removing entire neurons or attention heads rather than individual weights, resulting in hardware-friendly sparse matrices that accelerate inference.

## Key Results
- Achieves 33% sparsity on LLaMA-7B with perplexity of 19.83 and 61.21% average accuracy
- Outperforms state-of-the-art pruning techniques on WikiText2 and downstream tasks
- Reduces data requirements, GPU memory usage, and runtime compared to retraining-based methods

## Why This Works (Mechanism)
Olica works by exploiting the redundancy in LLM attention mechanisms and feed-forward networks through principled linear algebra techniques. The PCA-based compression identifies and preserves the most informative principal components in attention weight matrices, maintaining the essential relationships between tokens while eliminating redundancy. The linear calibration strategy compensates for reconstruction errors in FFN layers by learning optimal low-rank correction matrices that minimize the difference between original and pruned representations. This dual approach addresses the two most parameter-intensive components of transformers while preserving their functional capacity.

## Foundational Learning
- **Principal Component Analysis (PCA)**: Dimensionality reduction technique that identifies orthogonal components explaining maximum variance - needed to compress attention weight matrices while preserving essential information
- **Ridge Regression**: Regularized linear regression that prevents overfitting - used to derive optimal low-rank matrices for error compensation in FFN layers
- **Structured Pruning**: Removal of entire neurons, channels, or attention heads rather than individual weights - enables hardware-efficient inference acceleration
- **Multi-Head Attention (MHA) Matrix Products**: The four matrices (Wq, Wk, Wv, Wo) and their interactions - central to understanding how attention mechanisms can be compressed
- **Feed-Forward Network (FFN) Architecture**: Two linear transformations with ReLU activation - understanding error propagation is crucial for calibration strategy

## Architecture Onboarding

**Component Map**: Input -> MHA (Wq/Wk/Wv compression) -> FFN (Linear Calibration) -> Output

**Critical Path**: The attention mechanism and FFN layers are the primary targets as they contain the majority of parameters in transformer models

**Design Tradeoffs**: Structured vs unstructured pruning (speed vs parameter efficiency), PCA compression vs fine-tuning (accuracy vs computational cost), linear vs nonlinear calibration (simplicity vs model fidelity)

**Failure Signatures**: Performance degradation at high sparsity levels (>33%), reduced accuracy on tasks requiring fine-grained attention patterns, increased perplexity on long-range dependencies

**First Experiments**: 1) Validate MHA compression on small attention layers with synthetic data, 2) Test linear calibration on single FFN block with controlled error injection, 3) Evaluate end-to-end performance on LLaMA-7B with 10% incremental sparsity increases

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on MHA and FFN layers leaves other architectural components potentially unoptimized
- Effectiveness may diminish for extreme sparsity levels beyond the tested 33% threshold
- Linear calibration approach might not capture complex error patterns in highly pruned networks
- Reliance on specific model sizes and datasets limits generalizability to other architectures and domains

## Confidence
- **High**: Novel PCA-based MHA compression approach, mathematical formulation, method's logical foundation in linear algebra principles
- **Medium**: Linear calibration strategy for FFN layers, comparative performance claims, efficiency gains (memory and runtime)

## Next Checks
1. Test Olica on diverse model architectures (e.g., OPT, GPT-Neo) and varying model sizes to assess scalability and robustness
2. Evaluate performance degradation at higher sparsity levels (50% and above) to determine practical limits
3. Compare against state-of-the-art unstructured pruning methods in terms of final model quality and efficiency metrics