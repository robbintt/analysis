---
ver: rpa2
title: 'Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing'
arxiv_id: '2505.20899'
source_url: https://arxiv.org/abs/2505.20899
tags:
- speech
- translation
- speed
- source
- duration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dub-S2ST, a textless speech-to-speech translation
  system designed for seamless dubbing. The method employs a discrete diffusion-based
  speech-to-unit translation model with explicit duration control, trained using a
  unit-based speed adaptation strategy that aligns target speaking speed with the
  source.
---

# Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing

## Quick Facts
- arXiv ID: 2505.20899
- Source URL: https://arxiv.org/abs/2505.20899
- Authors: Jeongsoo Choi; Jaehun Kim; Joon Son Chung
- Reference count: 8
- Primary result: Achieves state-of-the-art duration compliance (100%), competitive translation quality (BLEU 24.16), and superior speaker preservation (SIM 0.266) on CVSS-C dataset

## Executive Summary
This paper introduces Dub-S2ST, a textless speech-to-speech translation system designed for seamless dubbing that preserves source duration, speaker identity, and speaking speed. The method employs a discrete diffusion-based speech-to-unit translation model with explicit duration control and a unit-based speed adaptation strategy. Speech is first converted to discrete units via mHuBERT, and the translation decoder generates units conditioned on source speech features with fixed output length matching the source. A conditional flow matching-based unit-to-speech synthesizer reconstructs the translated speech while preserving the source speaker's identity. Experiments on the CVSS-C dataset demonstrate state-of-the-art performance across all three key dubbing metrics.

## Method Summary
Dub-S2ST processes speech through a three-stage pipeline: first converting source audio to discrete units using mHuBERT+k-means, then translating these units via a discrete diffusion decoder with explicit duration control, and finally reconstructing the translated speech using a conditional flow matching synthesizer. The translation decoder receives a fully masked unit sequence with length predetermined to match the source speech, enabling 100% duration compliance. Speaking speed is normalized across languages through unit-based speed adaptation, which adjusts target unit repetitions based on the ratio of deduplicated unit counts. The unit-to-speech synthesizer is conditioned on translated units, source speaker embeddings, and source mel-spectrograms to preserve speaker identity while transferring prosodic information.

## Key Results
- Achieves perfect duration compliance (100%) while maintaining competitive translation quality (BLEU 24.16)
- Superior speaker identity preservation (SIM 0.266) compared to baselines (0.036-0.145)
- Effective unit-based speed adaptation shows correlation of 0.606 between unit speed and syllable speed
- Ablation studies confirm effectiveness of each component in improving dubbing quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit duration control via fixed-length masked sequence initialization enables 100% duration compliance.
- **Mechanism:** The discrete diffusion decoder receives a fully masked unit sequence with length predetermined to match source speech. During iterative denoising, the model generates translated units that fill exactly this length, rather than predicting length autoregressively. This decouples translation quality from duration constraints.
- **Core assumption:** The semantic content can be compressed or expanded to fit arbitrary durations without catastrophic quality degradation.
- **Evidence anchors:** [abstract] "discrete diffusion-based speech-to-unit translation model with explicit duration control, enabling time-aligned translation"; [Section 3.2] "during inference, it receives a fully masked sequence whose length matches that of the source speech units, thereby generating translated speech aligned in length with the source"; [Table 5] Demonstrates relative character counts scale with duration ratio (0.851 at 0.8x, 1.133 at 1.2x), confirming semantic adaptation rather than forced compression.
- **Break condition:** If target language requires significantly more tokens than source for equivalent meaning (high morphological complexity mismatch), the model may produce semantic truncation or filler artifacts.

### Mechanism 2
- **Claim:** Unit-based speed adaptation aligns speaking pace by normalizing unit repetition rates across languages.
- **Mechanism:** Speaking speed is estimated as ratio r = L̂/L (deduplicated units / original units). Target sequences are adjusted by applying the speed ratio r_src/r_tgt, modifying unit repetitions rather than signal-level stretching. Language-specific average speeds normalize cross-linguistic rate differences.
- **Core assumption:** The deduplicated-to-original unit ratio correlates with perceived speaking speed and this relationship transfers across languages.
- **Evidence anchors:** [Section 3.1] "We hypothesize that the reduced sequence L̂ captures a distinct set of pronunciations, and the ratio against original length L serves as an implicit estimate of speaking speed"; [Table 4] Source unit speed correlates 0.606 with syllable speed; target syllable speed after adaptation correlates 0.519 vs 0.235 without; [Figure 2] Visual confirmation that English syllable speed distribution shifts to match French after adaptation.
- **Break condition:** If source and target languages have fundamentally different unit-to-syllable mappings (e.g., tonal languages), the speed estimate may misalign with perceptual rate.

### Mechanism 3
- **Claim:** Source-conditioned flow matching preserves speaker identity by learning to reconstruct target units in source speaker's voice.
- **Mechanism:** The unit-to-speech synthesizer uses Optimal Transport Conditional Flow Matching, conditioned on: (1) translated unit embeddings, (2) source speaker embedding from pretrained verification model, (3) source mel-spectrogram. The mel-spectrogram provides prosodic in-context learning; speaker embedding provides timbre; units provide linguistic content.
- **Core assumption:** Speaker identity is sufficiently captured by embeddings from verification models and can be disentangled from linguistic content.
- **Evidence anchors:** [abstract] "conditional flow matching-based unit-to-speech synthesizer reconstructs the translated speech while preserving the source speaker's identity"; [Section 3.3] "These features are concatenated channel-wise to enable in-context learning: the sampled prior is transformed into a mel-spectrogram conditioned on both speaker identity and prosodic information"; [Table 1] SIM score 0.266 vs 0.036-0.145 for baselines.
- **Break condition:** If source speaker has voice characteristics poorly represented in the pretrained TTS initialization data, zero-shot identity transfer may fail or produce artifacts.

## Foundational Learning

- **Concept: Discrete Speech Units (mHuBERT + k-means)**
  - **Why needed here:** Understanding how continuous speech becomes discrete tokens is foundational to the entire architecture. The model operates on units, not waveforms.
  - **Quick check question:** Can you explain why deduplicating units helps synthesis quality but the paper *retains* repetitions for duration control?

- **Concept: Discrete Diffusion (Masked Language Modeling for Sequences)**
  - **Why needed here:** The speech-to-unit translator uses discrete diffusion, not continuous. Understanding mask schedules and partial prediction is essential for debugging generation quality.
  - **Quick check question:** Why does computing loss only on masked units (Table 7) outperform computing loss on all units?

- **Concept: Conditional Flow Matching (OT-CFM)**
  - **Why needed here:** The vocoder uses flow matching, not diffusion. The optimal transport path differs from standard diffusion training.
  - **Quick check question:** What is the difference between the velocity field in OT-CFM (Eq. 4) and a standard score-based diffusion model?

## Architecture Onboarding

- **Component map:**
  Source Audio → VAD → mHuBERT → k-means → Source Units → [Conformer Encoder] → h_src → [DiT Decoder] ← t_dec, mask schedule → Target Units (fixed length = source) → [U-Net CFM Synthesizer] ← Source Audio → Speaker Embedding ← Source Audio → Mel-spectrogram → Generated Mel → HiFi-GAN → Audio

- **Critical path:** Speed adaptation preprocessing → correct unit repetition rate → encoder quality → decoder mask schedule → unit-to-speech conditioning fidelity

- **Design tradeoffs:**
  - NFE (Table 6): 64 NFE balances latency vs BLEU; fewer steps degrade rapidly
  - Linear vs Cosine schedule: Linear works better for this task
  - Loss computation: Masked-only training prevents model from "cheating" on easy predictions
  - Vocoder initialization: Fine-tuning from CosyVoice outperforms zero-shot vocoders

- **Failure signatures:**
  - **Duration mismatch:** Check mask schedule and length initialization; if using non-zero initial mask, verify ratio
  - **Speaker identity loss:** Verify speaker embedding extraction model matches training; check mel-spectrogram padding alignment
  - **Speed drift:** Inspect unit-based speed ratio computation; verify language normalization constants
  - **Translation quality drop with strict duration:** May indicate semantic compression failure; examine relative character counts

- **First 3 experiments:**
  1. **Reproduce ablation on NFE:** Train model, evaluate at NFE ∈ {1, 4, 16, 64, 256}. Confirm BLEU trajectory matches Table 6 before proceeding.
  2. **Validate speed adaptation correlation:** Compute unit-speed to syllable-speed correlation on held-out data. Target: ρ > 0.5 with adaptation.
  3. **Duration-translation tradeoff sweep:** Evaluate translation quality (BLEU, BLASER) at duration ratios {0.8, 0.9, 1.0, 1.1, 1.2}. Confirm semantic flexibility per Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Dub-S2ST framework maintain translation quality and duration compliance when applied to noisy, "in-the-wild" speech data?
- Basis: [explicit] The Limitations section states the model is trained on speech from controlled environments and may require larger, more diverse datasets for in-the-wild scenarios.
- Why unresolved: The experiments were conducted exclusively on the CVSS-C dataset, which consists of clean, synthetic speech generated by a TTS system.
- What evidence would resolve it: Evaluating performance on noisy, real-world datasets (e.g., VoxPopuli or multimedia extracts) with background noise and reverberation.

### Open Question 2
- Question: Is the unit-based speed adaptation heuristic robust across language pairs with significantly different phonotactic structures or information densities?
- Basis: [inferred] The experiments are restricted to the French-to-English pair, and the method relies on the assumption that unit reduction ratios correlate directly with speaking speed.
- Why unresolved: Languages with different syllable structures (e.g., agglutinative vs. analytic) may exhibit different relationships between discrete unit counts and actual speaking duration.
- What evidence would resolve it: Testing the speed adaptation strategy on a diverse set of language pairs (e.g., English-to-Japanese or Spanish-to-German).

### Open Question 3
- Question: Does the conditioning on source mel-spectrograms effectively transfer fine-grained prosodic cues, such as emotion and emphasis, beyond just duration and speed?
- Basis: [inferred] The unit-to-speech synthesizer uses source mel-spectrograms as a condition to capture prosody, but the evaluation metrics (BLEU, SIM, DNSMOS, Duration/Speed Compliance) do not explicitly measure emotional or prosodic fidelity.
- Why unresolved: While the model preserves global temporal features, it is unclear if local prosodic variations (pitch contours, stress patterns) are successfully transferred to the target speech.
- What evidence would resolve it: Conducting human or automated evaluations specifically targeting emotional congruence and prosodic similarity between source and generated speech.

## Limitations
- Architecture specificity: Performance heavily depends on pretrained components (mHuBERT, CosyVoice) whose behavior on unseen speakers remains uncertain
- Speed adaptation validity: The unit-to-syllable speed correlation is derived from limited corpus evidence and untested beyond French-English
- Duration-quality tradeoff: Perfect duration compliance comes at cost of semantic compression/expansion, with unclear robustness boundaries

## Confidence
- **High Confidence:** Explicit duration control via fixed-length sequence generation, speaker identity preservation through source-conditional synthesis, discrete diffusion for speech-to-unit translation
- **Medium Confidence:** Unit-based speed adaptation effectiveness and specific design choices (NFE=64, linear mask schedule, masked-only loss computation)
- **Low Confidence:** Generalization to unseen speakers and languages with different phonological structures than training corpus

## Next Checks
1. **Speaker Generalization Test:** Evaluate SIM scores on speakers with atypical voice characteristics (high pitch, strong accents, non-standard recording conditions) from CVSS-C test set. Compare against reported 0.266 to assess robustness boundaries.

2. **Cross-Language Speed Adaptation Validation:** Extend unit-speed to syllable-speed correlation analysis to additional language pairs beyond French-English. Target ρ > 0.5 with adaptation across at least 3 diverse language pairs to validate general applicability.

3. **Duration-Semantic Tradeoff Analysis:** Systematically evaluate translation quality (ASR-BLEU and BLASER) across duration ratios {0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4} to identify semantic preservation threshold where quality degradation becomes unacceptable.