---
ver: rpa2
title: Influence Guided Sampling for Domain Adaptation of Text Retrievers
arxiv_id: '2601.21759'
source_url: https://arxiv.org/abs/2601.21759
tags:
- sampling
- training
- influence
- data
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Inf-DDS, an influence-guided reinforcement
  learning framework for adaptive data sampling in text retriever training. The method
  replaces noisy gradient-based rewards with influence scores computed via online
  proxy models, enabling more stable and interpretable sampling trajectories.
---

# Influence Guided Sampling for Domain Adaptation of Text Retrievers

## Quick Facts
- arXiv ID: 2601.21759
- Source URL: https://arxiv.org/abs/2601.21759
- Reference count: 40
- Primary result: 5.03 absolute NDCG@10 improvement on multilingual bge-m3 model and 0.94 absolute NDCG@10 improvement for all-MiniLM-L6-v2

## Executive Summary
This paper proposes Inf-DDS, an influence-guided reinforcement learning framework for adaptive data sampling in text retriever training. The method replaces noisy gradient-based rewards with influence scores computed via online proxy models, enabling more stable and interpretable sampling trajectories. Inf-DDS achieves significant performance improvements while being 1.5x-4x cheaper in GPU compute compared to gradient-based methods.

## Method Summary
Inf-DDS learns adaptive sampling weights across multiple training datasets to maximize retriever performance on target development sets via influence-guided reinforcement learning. The method initializes a sampling distribution over datasets, then iteratively creates online proxy models by taking gradient steps on each dataset, computes influence scores by measuring performance changes on dev batches, and updates both the model parameters via weighted Reptile and the sampler via policy gradient. The approach uses subsampling to maintain tractability and demonstrates superior domain adaptation across BEIR, MLDR, and Sentence-Transformers datasets.

## Key Results
- 5.03 absolute NDCG@10 improvement when training multilingual bge-m3 model
- 0.94 absolute NDCG@10 improvement for all-MiniLM-L6-v2
- 1.5x-4x cheaper in GPU compute compared to gradient-based methods
- Superior domain adaptation and efficient handling of multilingual and heterogeneous training data

## Why This Works (Mechanism)

### Mechanism 1: Stable Influence-Based Rewards
Influence scores provide more stable reward signals than gradient-based rewards for adaptive data sampling. The method computes actual performance changes on dev batches rather than gradient alignment, reducing variance in the reward signal. This works because influence scores directly measure performance impact rather than gradient alignment, though the approach assumes dev set representativeness.

### Mechanism 2: Weighted Reptile Updates
Weighted Reptile updates enable efficient gradient reuse across datasets, reducing memory overhead while maintaining performance gains. The method forms a weighted parameter update that reuses gradient computations from influence estimation to also update model parameters, requiring only one copy of gradients and optimizer states. This works under the assumption that weighted combinations of proxy model parameters approximate optimal update directions.

### Mechanism 3: Subsampling for Tractability
Subsampling datasets per iteration enables tractable computation while preserving adaptive sampling benefits. Rather than computing influence for all datasets every iteration, the method samples a subset and restricts policy updates to this subset. This works under the assumption that bias introduced by subsampling averages out over training iterations.

## Foundational Learning

- **Concept: Influence Functions** - Why needed: Inf-DDS builds on influence estimation to quantify how training examples affect model predictions. Quick check: Given a training batch that reduces loss but increases dev set NDCG@10, would gradient-based reward assign it positive or negative value? How about influence-based reward?

- **Concept: Bi-level Optimization** - Why needed: The method jointly optimizes model and sampler parameters where each depends on the other. Quick check: In Inf-DDS, does the scorer receive gradients from model loss or from influence reward? Which optimizer updates which?

- **Concept: Policy Gradient / REINFORCE** - Why needed: The sampler update uses standard REINFORCE form. Quick check: Why must reward be multiplied by ∇_ψ log P_D rather than directly by ∇_ψ P_D? What would happen if we used the latter?

## Architecture Onboarding

- **Component map:** [Training Loop] -> [Dataset Sampler ψ] -> sample D^i_train -> [Inner Loop: l steps] -> θ^i_{t+1} (proxy) -> [Influence Estimator] -> I_i -> [Weighted Reptile] -> θ_{t+1} -> [Policy Update] -> ψ

- **Critical path:** Initialize ψ with temperature-scaled distribution; sample dataset; create proxy model via l inner steps; compute influence on dev batch; aggregate influences; update ψ via policy gradient; apply weighted Reptile update to θ; repeat every k steps after warmup.

- **Design tradeoffs:** l (inner steps): 3-5 optimal; k (subsample size): k=8 for M=13 languages; τ (temperature): controls sampling entropy; Reptile vs. no Reptile: minimal performance difference, keep for memory efficiency.

- **Failure signatures:** Oscillating sampling weights (gradient-based methods show this); no improvement over static (check dev set quality); GPU OOM during influence computation (verify single copy of gradients); performance degrades on held-out domains (model may overfit to dev domains).

- **First 3 experiments:** 1) Sanity check on BEIR-7: Train roberta-base with uniform vs. Inf-DDS optimizing on FEVER dev. Target: >2 NDCG@10 improvement. 2) Ablate inner steps l: On FEVER+FiQA, test l∈{1, 3, 5, 10, 20}. Expect peak at l=3-5. 3) Test initialization sensitivity: Start from τ=0.3, 1, 5, ∞ on BEIR-5 joint optimization. Expect variance in final scores.

## Open Questions the Paper Calls Out
1. Can heuristic or automated methods be developed to select initial sampling weights that ensure Inf-DDS converges to optimal performance without manual temperature tuning?
2. Do more sophisticated optimization strategies for the scorer parameters improve convergence guarantees or final performance compared to the current REINFORCE-based approach?
3. Does the computational efficiency of Inf-DDS hold for significantly larger model architectures where training online proxy models is prohibitively expensive?

## Limitations
- Influence estimates assume dev set representativeness, which may not hold for small or unrepresentative dev sets
- Temperature initialization sensitivity requires manual tuning across different dataset configurations
- Computational efficiency gains may not scale to billion-parameter models where online proxy training becomes expensive

## Confidence
- **High confidence:** Influence-based rewards provide more stable sampling trajectories than gradient-based rewards
- **Medium confidence:** Weighted Reptile updates reduce memory overhead while maintaining performance
- **Medium confidence:** Subsampling preserves adaptive benefits while reducing compute

## Next Checks
1. Dev set representativeness test: Train with dev sets that mismatch target domains to quantify influence estimate degradation
2. Influence variance analysis: Systematically vary dev batch size and measure influence score variance across iterations
3. Temperature sensitivity validation: Extend τ testing beyond current values to identify optimal temperature ranges for different dataset sizes