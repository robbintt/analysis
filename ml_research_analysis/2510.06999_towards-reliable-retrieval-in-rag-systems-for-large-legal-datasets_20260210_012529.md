---
ver: rpa2
title: Towards Reliable Retrieval in RAG Systems for Large Legal Datasets
arxiv_id: '2510.06999'
source_url: https://arxiv.org/abs/2510.06999
tags:
- legal
- retrieval
- document
- information
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses a critical failure mode in Retrieval-Augmented
  Generation (RAG) systems called Document-Level Retrieval Mismatch (DRM), where retrieval
  systems select information from incorrect source documents due to high lexical and
  structural similarity in legal documents. To mitigate DRM, the authors propose Summary-Augmented
  Chunking (SAC), a simple and computationally efficient technique that prepends document-level
  synthetic summaries to each text chunk, injecting crucial global context lost during
  standard chunking.
---

# Towards Reliable Retrieval in RAG Systems for Large Legal Datasets

## Quick Facts
- arXiv ID: 2510.06999
- Source URL: https://arxiv.org/abs/2510.06999
- Reference count: 20
- Primary result: Summary-Augmented Chunking (SAC) reduces Document-Level Retrieval Mismatch (DRM) by up to 50% in legal RAG systems

## Executive Summary
This study addresses a critical failure mode in Retrieval-Augmented Generation (RAG) systems called Document-Level Retrieval Mismatch (DRM), where retrieval systems select information from incorrect source documents due to high lexical and structural similarity in legal documents. To mitigate DRM, the authors propose Summary-Augmented Chunking (SAC), a simple and computationally efficient technique that prepends document-level synthetic summaries to each text chunk, injecting crucial global context lost during standard chunking. Experiments on the LegalBench-RAG benchmark demonstrate that SAC significantly reduces DRM (by up to 50%) and improves text-level retrieval precision and recall. Surprisingly, generic summarization strategies outperform expert-guided legal domain knowledge approaches. SAC provides a practical, scalable solution for enhancing retrieval reliability in large-scale legal document databases, bringing us closer to reliable AI systems for legal applications.

## Method Summary
The method involves generating 150-character document summaries using gpt-4o-mini with a generic prompt, then applying recursive character splitting at 500 characters without overlap. Each chunk has the summary prepended before embedding with thenlper/gte-large and indexing in FAISS with cosine similarity. The approach is evaluated on LegalBench-RAG benchmark datasets (CUAD contracts, MAUD merger agreements, ContractNLI NDAs, PrivacyQA privacy policies) using DRM percentage, character-level precision, and character-level recall metrics.

## Key Results
- SAC reduces DRM by up to 50% compared to baseline chunking methods
- Generic summarization strategies outperform expert-guided legal domain knowledge approaches
- SAC improves both text-level retrieval precision and recall on LegalBench-RAG benchmark
- Document-level summaries effectively disambiguate between structurally similar legal documents

## Why This Works (Mechanism)

### Mechanism 1: Global Context Injection via "Document Fingerprints"
Prepending a document-level summary to local chunks mitigates Document-Level Retrieval Mismatch (DRM) by restoring global context lost during chunking. Standard chunking isolates text segments, causing them to lose their association with the parent document. In legal datasets, where clauses are often lexically similar across documents, isolated chunks look identical to the retriever. By injecting a "fingerprint" (summary) into every chunk, the retrieval vector is shifted to represent both the local content and the global document identity, disambiguating similar clauses from different contracts.

### Mechanism 2: Semantic Disambiguation Favors Generic Entities
Generic summarization prompts outperform expert-guided prompts because they produce "semantically broader" summaries that align better with general query intent, whereas expert prompts create dense "keyword stuffing" that confuses the embedding space. Expert prompts attempt to force specific legal variables (e.g., "Governing Law," "Remedies") into the summary. If these variables are standardized across documents, they fail to distinguish them. Generic prompts naturally gravitate towards unique entities (Party Names, specific subject matter) which are the true differentiators in a dense vector space.

### Mechanism 3: Dense Retrieval Bias Reduction
Summary-Augmented Chunking (SAC) effectively acts as a soft filter for dense retrievers, reducing the "false positive" scores of chunks that share boilerplate text but belong to the wrong document. In a standard vector space, a chunk containing "This agreement constitutes the entire understanding" from Document A has a nearly identical vector to the same clause in Document B. SAC shifts the vector of Document A's chunk closer to the "Document A" region of the embedding space. This increases the cosine distance between the query (which often implies a specific document context) and the irrelevant chunk.

## Foundational Learning

- **Concept: Document-Level Retrieval Mismatch (DRM)**
  - Why needed here: This is the specific failure mode the paper defines. You must understand that in legal RAG, "correct clause, wrong contract" is a distinct error from "wrong clause."
  - Quick check question: If a retriever finds the correct termination clause but pulls it from Company B's contract instead of Company A's, has DRM occurred?

- **Concept: The Chunking Trade-off (Locality vs. Context)**
  - Why needed here: The paper targets the pre-retrieval stage. Understanding that smaller chunks improve precision (less noise) but lose context (fewer clues about the parent document) is essential to grasping why SAC works.
  - Quick check question: Why does splitting a contract into 100-character chunks make it harder to distinguish between two nearly identical Non-Disclosure Agreements?

- **Concept: Semantic vs. Keyword Search in Legal Domains**
  - Why needed here: The paper experimentally rejects hybrid search (BM25) in favor of pure dense semantic search (SAC + embeddings). You need to know *why* keyword matching fails on boilerplate (identical keywords) to appreciate the semantic solution.
  - Quick check question: Why would BM25 (keyword search) struggle to find a specific clause in a database of identical contracts?

## Architecture Onboarding

- **Component map:** Document Loader -> LLM Summarizer (Generic Prompt) -> Recursive Character Splitter (500 chars) -> Prepend Operator -> Embedding Model (thenlper/gte-large) -> Vector Store (FAISS) -> Dense Retriever (Cosine Similarity) -> Top-K Selection

- **Critical path:** The quality of the **Generic Summary** is the single point of failure. If the summary prompt is poorly designed (e.g., "List all legal topics"), you lose the disambiguation power. The summary must be concise (~150 chars) and entity-focused.

- **Design tradeoffs:**
  - Dense vs. Hybrid: The paper shows adding BM25 improves DRM slightly but hurts Precision/Recall. Stick to dense retrieval for text-level accuracy.
  - Generic vs. Expert Prompts: Do not over-engineer prompts with legal domain knowledge. The paper demonstrates simple generic prompts yield better retrieval.
  - Chunk Size: 500 characters is the sweet spot; smaller (200) lowers recall; larger (800) lowers precision.

- **Failure signatures:**
  - High DRM (>50%): Your summaries are likely too similar or missing distinct entities (Party Names, Dates). Check the summarization prompt.
  - Low Precision/High Recall: Your chunks are too large or summaries are too dominant, pulling in irrelevant sections of the correct document.
  - "Boilerplate Retrieval": The system retrieves the correct document but consistently pulls the header/intro instead of the content. This indicates the summary is "outshouting" the chunk content in the embedding space.

- **First 3 experiments:**
  1. Baseline Establishment: Run a standard Recursive Chunking (500 chars) pipeline on the ContractNLI dataset. Measure DRM. You should see high failure rates (>90%) as cited in the paper.
  2. SAC Integration: Implement the Generic Summary prompt (150 chars) using `gpt-4o-mini`. Prepend this to every chunk in a document. Re-run retrieval. Verify the DRM drops by roughly 50%.
  3. Ablation on Summary Length: Test 150 vs. 300 character summaries. The paper suggests 300 characters may introduce noise or "shadow" the chunk content. Verify if 150 chars maintains the optimal balance between context and precision.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does Summary-Augmented Chunking (SAC) improve the factual accuracy of the final generated answer in end-to-end RAG applications?
  - Basis in paper: The authors state that future work will "investigate how the DRM metric and SAC impact downstream generation through end-to-end benchmarking."
  - Why unresolved: The study deliberately isolated the retrieval component to quantify Document-Level Retrieval Mismatch (DRM) and text-level precision/recall, without measuring the quality of the final LLM output.
  - What evidence would resolve it: Evaluating SAC on a generative legal QA benchmark (e.g., Australian Legal QA) to measure hallucination rates and factual consistency of the final answer.

- **Open Question 2:** How does SAC compare in performance and efficiency to other context-preserving strategies like Late Chunking or RAPTOR?
  - Basis in paper: The authors note it would be "valuable to benchmark SAC against other context-preserving chunking strategies, such as Late Chunking... and RAPTOR."
  - Why unresolved: The paper focused on a lightweight, modular intervention (SAC) rather than comparing it against more architecturally complex or computationally intensive hierarchical methods.
  - What evidence would resolve it: A head-to-head comparison of DRM and retrieval precision/recall between SAC and Late Chunking/RAPTOR on LegalBench-RAG, including metrics on computational overhead.

- **Open Question 3:** Can more capacious embedding models leverage the density of expert-guided summaries to outperform generic summarization?
  - Basis in paper: The authors hypothesize that expert summaries may fail because "informationally dense and structured language... may pose challenges for smaller embedding models," suggesting "future experiments with stronger, more capacious embedding models are needed."
  - Why unresolved: The observed counter-intuitive result (generic > expert) might be an artifact of the specific embedding model used (thenlper/gte-large) rather than a fundamental limitation of expert-guided context.
  - What evidence would resolve it: Re-evaluating the Expert-Guided SAC approach using high-dimensional or legal-domain-specific embedding models to see if the performance gap narrows or reverses.

## Limitations
- Evaluation is constrained to a specific legal corpus with high structural similarity, raising questions about generalizability to other domains
- Reliance on gpt-4o-mini for summary generation introduces potential cost and scalability concerns for production deployment
- Ablation studies are limited to a small set of chunk sizes and summary lengths, leaving the optimal configuration space incompletely explored

## Confidence
- High confidence: Existence of DRM as a failure mode and effectiveness of SAC in reducing it
- Medium confidence: Superiority of generic summaries over expert-guided approaches
- Medium confidence: The specific mechanism (global context injection) explaining SAC's success

## Next Checks
1. Cross-domain validation: Apply SAC to non-legal domains with repetitive boilerplate text (e.g., medical forms, financial reports) to verify the generalization of DRM mitigation.
2. Cost-performance tradeoff analysis: Measure the latency and token costs of SAC preprocessing against retrieval quality improvements across different document volumes to establish practical scalability limits.
3. Summary content ablation: Systemively vary summary content (entities vs. structure vs. topics) and length to identify the minimal effective summary that maintains DRM reduction while optimizing embedding space utilization.