---
ver: rpa2
title: 'Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection'
arxiv_id: '2508.13365'
source_url: https://arxiv.org/abs/2508.13365
tags:
- reasoning
- performance
- idiomaticity
- which
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether reasoning models improve idiomaticity
  detection compared to standard LLMs. It evaluates DeepSeek-R1 distilled models (1.5B
  to 70B parameters) across four idiomaticity datasets (FLUTE, SemEval, MAGPIE, DICE)
  using chain-of-thought reasoning.
---

# Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection

## Quick Facts
- arXiv ID: 2508.13365
- Source URL: https://arxiv.org/abs/2508.13365
- Authors: Dylan Phelps; Rodrigo Wilkens; Edward Gow-Smith; Thomas Pickard; Maggie Mi; Aline Villavicencio
- Reference count: 8
- Primary result: Chain-of-thought reasoning improves idiomaticity detection only in models above ~14B parameters, with mixed effects in smaller models

## Executive Summary
This paper investigates whether reasoning models improve idiomaticity detection compared to standard LLMs. Evaluating DeepSeek-R1 distilled models (1.5B to 70B parameters) across four idiomaticity datasets reveals that the effect of reasoning is smaller and more varied than expected. While larger models (14B, 32B, 70B) show modest improvements, smaller models (1.5B, 7B) initially perform worse with CoT reasoning, though reasoning tuning can partially recover capability. Manual analysis shows that larger models consistently produce accurate idiomatic definitions while smaller models struggle with understanding, with reasoning quality being the primary factor determining correctness.

## Method Summary
The paper evaluates Qwen2.5 base and DeepSeek-R1 distilled models (1.5B to 70B parameters) on four idiomaticity datasets using chain-of-thought reasoning. Models are tested on FLUTE, SemEval, MAGPIE, and DICE datasets, with performance measured by macro F1 score across 5 random seeds. The evaluation uses vLLM library with Q6_K_M quantization on a single A100 80GB GPU. The study also tests using definitions generated by larger models as prompts for smaller models to investigate knowledge distillation potential.

## Key Results
- Chain-of-thought reasoning shows mixed effects: smaller models (1.5B, 7B) perform worse initially, while larger models (14B, 32B, 70B) show modest improvements
- The 32B model achieves the best overall performance across datasets
- Definition quality strongly correlates with model size: larger models produce accurate idiomatic definitions while smaller models struggle
- Longer CoTs do not consistently achieve better accuracy
- Using definitions from larger models as prompts improves 1.5B/7B performance on FLUTE by +0.069 macro F1, but shows no effect on DICE

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Reasoning Benefit
Chain-of-thought reasoning improves idiomaticity detection only above a critical model scale (~14B+ parameters), while harming performance in smaller models. Larger models possess sufficient parametric knowledge to generate accurate idiomatic definitions during CoT, creating a valid reasoning chain. Smaller models lack this knowledge, producing incorrect definitions that propagate errors through the reasoning process, degrading final predictions.

### Mechanism 2: Definition Knowledge as Detection Prerequisite
Accurate idiomatic definition generation is a necessary (but not sufficient) condition for successful idiomaticity detection. Detection requires distinguishing idiomatic from literal usage, which first requires knowing what the idiomatic meaning is. Models that cannot produce definitions cannot establish the comparison baseline, leaving classification near-random.

### Mechanism 3: Context Disambiguation as Persistent Bottleneck
Even models with strong definition knowledge fail when contextual disambiguation requires nuanced reasoning about usage context. DICE dataset controls for surface-form cues, forcing models to attend to surrounding context. Definition injection helps FLUTE (which involves paraphrase comparison) but not DICE, indicating context-reasoning is a distinct capability not solved by knowledge alone.

## Foundational Learning

- **Chain-of-Thought Reasoning**: Understanding that CoT creates intermediate reasoning tokens that become context for final prediction is essential. Quick check: Can you explain why generating reasoning before an answer could either help or hurt depending on what the model "knows"?

- **Knowledge Distillation**: The paper tests using 32B-generated definitions to improve 1.5B/7B performance—a form of cross-model knowledge transfer. Quick check: What's the difference between distillation via fine-tuning vs. in-context provision of teacher outputs?

- **Scale-Dependent Capabilities**: The central finding is that reasoning benefits emerge only at 14B+ parameters. Understanding that capabilities don't scale linearly with parameter count is critical. Quick check: Why might a smaller model's CoT actively harm rather than simply not help?

## Architecture Onboarding

- **Component map**: Input (sentence + target expression) → [Reasoning Model: CoT Generation] → Definition extraction (understanding) → Context-reasoning (disambiguation) → Classification head (idiomatic/literal)

- **Critical path**: Prompt engineering → CoT generation and extraction → Label parsing (GPT-4o fallback) → Evaluation: Macro F1 across 5 random seeds

- **Design tradeoffs**: Math-tuning intermediate step traded general capability for reasoning format familiarity; definition injection showed dataset-specific effects; quantization enabled single-A100 inference but may affect smaller models differently

- **Failure signatures**: Smaller models: "No definition" or "Inaccurate definition" in CoT → near-random classification; All models: Nonsensical reasoning chains even with correct definition → context-attention failure; Dataset noise: ~40-47% of "incorrect" large-model predictions were judged valid by humans

- **First 3 experiments**:
  1. Pinpoint the scale threshold: Test 10B and 12B models to determine where reasoning benefits emerge
  2. Definition-only baseline: Provide gold-standard human definitions to all model sizes to isolate knowledge gap vs. reasoning gap
  3. Context-focused fine-tuning: Fine-tune smaller models specifically on context-disambiguation examples to test targeted training effectiveness

## Open Questions the Paper Calls Out

- Can reasoning frameworks specifically designed for figurative language processing significantly outperform general-purpose CoT reasoning on idiomaticity detection?
- Does fine-tuning smaller models on definitions generated by larger models provide greater performance gains than in-context definition prompting?
- Why does providing explicit definitions improve small model performance on FLUTE but not on DICE?
- Do the findings about reasoning's limited impact on idiomaticity detection generalize to other reasoning model families beyond DeepSeek-R1 distillations?

## Limitations

- Dataset label reliability issues with 40-47% of "incorrect" predictions on DICE and MAGPIE judged valid by human annotators
- Unspecified CoT generation hyperparameters (temperature, top_p, max_tokens) affecting reproducibility
- Inconsistent effectiveness of definition injection across datasets, suggesting potential overfitting to paraphrase-style tasks

## Confidence

**High confidence**: The scale-dependent threshold finding (reasoning benefits emerge only at 14B+ parameters) is robust across multiple model families and datasets.

**Medium confidence**: The claim that reasoning quality (not just understanding) determines correctness is supported by analysis but could benefit from more systematic measurement.

**Low confidence**: The distillation approach's effectiveness is questionable given the DICE null result, and the paper doesn't explore whether the FLUTE improvement represents genuine capability transfer or dataset-specific artifacts.

## Next Checks

1. Pinpoint the scale threshold: Systematically test 10B and 12B models to determine the exact parameter count where reasoning benefits emerge.

2. Isolate knowledge vs. reasoning gaps: Provide gold-standard human definitions to all model sizes and measure the delta in performance.

3. Targeted context-reasoning fine-tuning: Fine-tune smaller models specifically on DICE-style examples that require nuanced context disambiguation.