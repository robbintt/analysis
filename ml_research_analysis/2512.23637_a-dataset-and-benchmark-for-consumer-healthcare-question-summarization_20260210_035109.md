---
ver: rpa2
title: A Dataset and Benchmark for Consumer Healthcare Question Summarization
arxiv_id: '2512.23637'
source_url: https://arxiv.org/abs/2512.23637
tags:
- question
- questions
- summary
- original
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHQ-Summ, a dataset for consumer healthcare
  question summarization. The dataset contains 1,507 domain-expert annotated question-summary
  pairs derived from the Yahoo!
---

# A Dataset and Benchmark for Consumer Healthcare Question Summarization

## Quick Facts
- arXiv ID: 2512.23637
- Source URL: https://arxiv.org/abs/2512.23637
- Authors: Abhishek Basu; Deepak Gupta; Dina Demner-Fushman; Shweta Yadav
- Reference count: 40
- Key outcome: Introduces CHQ-Summ dataset with 1,507 expert-annotated consumer healthcare question-summary pairs, achieving best results with DeepSeek-7B on reference-based and reference-free metrics

## Executive Summary
This paper introduces CHQ-Summ, a novel dataset for consumer healthcare question summarization consisting of 1,507 domain-expert annotated question-summary pairs derived from Yahoo! Answers L6 corpus. The dataset advances the field by including annotations about question focus and question type, enabling more nuanced evaluation of summarization models in healthcare contexts. The authors establish benchmark results using both fine-tuned encoder-decoder models (BART, PEGASUS, ProphetNet, T5) and instruction-tuned large language models (Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Gemma-7B-IT, DeepSeek-7B-Chat).

## Method Summary
The CHQ-Summ dataset was constructed by extracting consumer healthcare questions from the Yahoo! Answers L6 corpus and having domain experts create corresponding summary pairs. Each question-summary pair includes additional annotations for question focus and question type. The benchmark evaluation employed multiple state-of-the-art summarization models including fine-tuned encoder-decoder architectures (BART, PEGASUS, ProphetNet, T5) and instruction-tuned large language models (Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Gemma-7B-IT, DeepSeek-7B-Chat). Evaluation used both reference-based metrics (ROUGE-LSum, METEOR, BERTScore-F1) and reference-free metrics (Semantic Coherence, Entailment consistency), along with a healthcare answer retrieval task using nDCG@4.

## Key Results
- DeepSeek-7B achieved the highest top-1 average score across evaluation metrics
- The CHQ-Summ dataset showed DeepSeek-7B also achieved the best nDCG@4 performance on healthcare answer retrieval
- Instruction-tuned LLMs generally outperformed fine-tuned encoder-decoder models on the benchmark

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning

- **Consumer healthcare question domain**: Understanding the unique characteristics and challenges of healthcare questions from non-expert users is essential for developing effective summarization models that capture medical relevance while maintaining accessibility.

- **Question focus and type annotations**: These additional annotations provide structured metadata that can guide summarization models to produce more targeted and appropriate summaries for different healthcare query types.

- **Reference-based vs reference-free evaluation metrics**: Different evaluation approaches capture different aspects of summarization quality - reference-based metrics measure similarity to human references while reference-free metrics assess intrinsic qualities like coherence and entailment.

- **Encoder-decoder vs instruction-tuned architectures**: Fine-tuned encoder-decoder models are optimized for sequence-to-sequence tasks, while instruction-tuned LLMs leverage broader language understanding capabilities through pre-training on diverse instruction-response pairs.

- **Healthcare answer retrieval evaluation**: Using nDCG@4 on answer retrieval tasks provides indirect validation of summarization quality by testing whether summaries effectively represent the information needs expressed in questions.

## Architecture Onboarding

**Component Map**: Consumer healthcare questions -> Summarization models (encoder-decoder and instruction-tuned LLMs) -> Generated summaries -> Evaluation metrics (ROUGE-LSum, METEOR, BERTScore-F1, Semantic Coherence, Entailment consistency, nDCG@4)

**Critical Path**: Question input → Model generation → Summary output → Metric computation → Performance ranking

**Design Tradeoffs**: Fine-tuned encoder-decoder models offer task-specific optimization but may lack broader contextual understanding, while instruction-tuned LLMs provide better generalization but require careful prompt engineering for optimal performance.

**Failure Signatures**: Poor performance may manifest as summaries that miss key medical information, introduce irrelevant content, or fail to capture the specific focus of the healthcare question, particularly for complex or ambiguous queries.

**First 3 Experiments to Run**:
1. Compare summary quality between models on questions with different focus annotations to assess if models adapt to different healthcare query types
2. Evaluate model performance on questions requiring domain-specific medical knowledge versus general health information
3. Test the impact of including question focus and type annotations in the model input prompt on summary quality

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The dataset size of 1,507 pairs is modest compared to general summarization benchmarks
- All examples come from a single source (Yahoo! Answers L6), potentially limiting generalizability
- Expert annotation process details and inter-annotator agreement metrics are not fully reported

## Confidence

**High confidence**: Dataset creation methodology and basic statistics are clearly described and reproducible; inclusion of question focus and type annotations represents genuine contribution.

**Medium confidence**: Benchmark results showing DeepSeek-7B as top-performing model are plausible but limited evaluation scope prevents stronger claims.

**Low confidence**: Claims about dataset effectiveness for benchmarking and impact on downstream tasks are not fully supported by presented methodology.

## Next Checks

1. Conduct human evaluation studies with healthcare professionals to assess faithfulness, coherence, and medical accuracy of model-generated summaries.

2. Expand evaluation to include additional datasets from different sources (medical forums, patient communities) to test generalizability across varied consumer health question styles.

3. Perform ablation studies on annotation schema to determine utility of question focus and type annotations in improving summarization quality and downstream task performance.