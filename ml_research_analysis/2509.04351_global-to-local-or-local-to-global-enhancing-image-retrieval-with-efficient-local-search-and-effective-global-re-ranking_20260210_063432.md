---
ver: rpa2
title: Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient
  Local Search and Effective Global Re-ranking
arxiv_id: '2509.04351'
source_url: https://arxiv.org/abs/2509.04351
tags:
- re-ranking
- retrieval
- local
- global
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a local-to-global (L2G) image retrieval paradigm
  that flips the conventional global-to-local approach. The authors propose using
  efficient local feature search (via CANN) for initial retrieval and then re-ranking
  the top candidates using global features derived on-the-fly from local feature similarities
  through multidimensional scaling (MDS).
---

# Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking

## Quick Facts
- arXiv ID: 2509.04351
- Source URL: https://arxiv.org/abs/2509.04351
- Reference count: 39
- One-line primary result: Achieves 92.9% mAP on ROxf and 90.5% mAP on ROxf+1M distractors using a novel local-to-global image retrieval approach.

## Executive Summary
This paper introduces a local-to-global (L2G) image retrieval paradigm that flips the conventional global-to-local approach. The authors propose using efficient local feature search (via CANN) for initial retrieval and then re-ranking the top candidates using global features derived on-the-fly from local feature similarities through multidimensional scaling (MDS). This approach addresses the limitations of both global-only search (which struggles with partial matches) and local-only re-ranking (which lacks information sharing across shortlisted images). By converting pairwise local feature dissimilarities into a Euclidean embedding space via MDS, the method creates query-specific global features that respect localized similarities. On the Revisited Oxford and Paris datasets, this L2G approach achieves state-of-the-art performance, improving over previous methods by 2-3% in mean average precision. The system shows particular strength in large-scale settings with 1M distractors, demonstrating the effectiveness of combining localized search with effective re-ranking based on MDS-derived global features.

## Method Summary
The L2G framework operates in two stages: first, it uses CANN (Constrained Approximate Nearest Neighbors) with Chamfer similarity to efficiently search local features (FIRE) and retrieve top-k candidates. Then, it constructs a sparse dissimilarity matrix from pairwise local similarities and applies SMACOF multidimensional scaling to create query-specific global embeddings that respect the local geometry. Finally, these MDS embeddings are fused with pre-trained global features (SuperGlobal) via weighted averaging and re-ranked using the SuperGlobal re-ranking algorithm. The system is evaluated on Revisited Oxford (ROxf), Revisited Paris (RPar), and their 1M distractor variants, achieving state-of-the-art mAP scores of 92.9% (ROxf), 97.1% (RPar), 90.5% (ROxf+1M), and 92.1% (RPar+1M).

## Key Results
- Achieves 92.9% mAP on ROxf and 90.5% mAP on ROxf+1M distractors, surpassing previous state-of-the-art methods.
- Particularly strong performance on large-scale 1M distractor sets, demonstrating scalability of the L2G approach.
- Ablation studies confirm that both MDS embeddings and global feature fusion are critical for optimal performance.
- Outperforms AMES and BiLSTM on ROxf+1M, establishing new benchmarks for large-scale image retrieval.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Initiating retrieval with local features (rather than global aggregates) improves recall for partial matches, provided the search is computationally tractable.
- **Mechanism**: The system uses Constrained Approximate Nearest Neighbors (CANN) to search local descriptors directly. Unlike global features which average out localized signals, CANN identifies images sharing specific local patches, mitigating the "summary" effect of global aggregation.
- **Core assumption**: The efficiency of CANN allows local search to scale to 1M+ distractor sets without the latency traditionally associated with local matching.
- **Evidence anchors**:
  - [abstract]: "...enabling detailed retrieval at large scale, to find partial matches which are often missed by global feature search."
  - [section 3]: "CANN employs a novel nearest neighbor search strategy that efficiently finds the best matches... using only local features."
  - [corpus]: Corpus evidence is weak for this specific image retrieval inversion; related papers discuss global-to-local attention in Graph Transformers, which is architecturally distinct.
- **Break condition**: If the dataset consists primarily of full-image semantic matches rather than object-level/partial matches, the computational overhead of local search may not justify marginal recall gains over global descriptors.

### Mechanism 2
- **Claim**: Creating query-specific global embeddings on-the-fly via Multidimensional Scaling (MDS) allows the re-ranker to operate in a metric space that respects local similarities.
- **Mechanism**: MDS converts the sparse matrix of pairwise dissimilarities (derived from the top-k local search results) into a dense Euclidean embedding. This "re-ranking-only" global feature geometrically encodes the relationships between the query and candidates, allowing standard distance metrics to replace complex non-metric comparisons.
- **Core assumption**: The sparse distance matrix (filled with '1' for unknown relations) sufficiently preserves the local structure required for SMACOF to converge to a useful embedding.
- **Evidence anchors**:
  - [abstract]: "...leverage multidimensional scaling techniques to create embeddings which respect the local similarities obtained during search."
  - [section 3.2]: "We employ SMACOF... [which] allows us to... Handle non-metric dissimilarities [and] complete the dissimilarity matrix."
  - [corpus]: N/A (No direct corpus validation for MDS in this specific retrieval context).
- **Break condition**: If the sparsity of known distances is too high (i.e., neighbors have few shared connections), the MDS embedding may distort the manifold, degrading re-ranking accuracy.

### Mechanism 3
- **Claim**: Fusing MDS-induced embeddings with pre-trained global features (e.g., SuperGlobal) combines geometric consistency with semantic context.
- **Mechanism**: The system computes a weighted average of the MDS embedding (geometry-heavy) and the SuperGlobal feature (semantic-heavy). This ensemble corrects cases where local geometry is ambiguous but semantic context is strong, or vice versa.
- **Core assumption**: The optimal weight (tuned at w=0.19 for SuperGlobal) transfers across datasets (ROxf vs RPar) without requiring per-dataset re-tuning.
- **Evidence anchors**:
  - [section 4.1]: "...employ a weighted average between the MDS embeddings and SuperGlobal global features."
  - [section 4.3 (ablation)]: "Without merging final features with SuperGlobal... hurts performance, confirming the synergy."
  - [corpus]: N/A
- **Break condition**: If domain shift is significant (e.g., from landmarks to abstract art), the fixed semantic prior from the pre-trained global feature may conflict with the purely geometric MDS embedding.

## Foundational Learning

- **Concept: Chamfer Similarity**
  - **Why needed here**: The paper relies on CANN, which uses Chamfer similarity (asymmetric) rather than standard Euclidean distance. Understanding this is required to grasp why the initial local search is efficient but produces non-metric distances.
  - **Quick check question**: How does Chamfer similarity approximate the match quality of two sets of local descriptors differently than a RANSAC-based geometric verification?

- **Concept: Multidimensional Scaling (MDS) & SMACOF**
  - **Why needed here**: The core innovation is using MDS to "fix" the non-metric distances from the first stage. You must understand stress functions to debug why the re-ranking might fail on sparse data.
  - **Quick check question**: In the context of this paper, does MDS reduce dimensionality for compression, or does it construct an embedding space to satisfy pairwise distance constraints?

- **Concept: Sparse Distance Matrices**
  - **Why needed here**: The system cannot compute all pairwise distances (O(N^2)). It relies on a sparse matrix where missing entries are set to a maximum distance. Understanding this approximation is key to evaluating system robustness.
  - **Quick check question**: What is the default value assigned to missing pairwise distances in the MDS input matrix, and how might this affect the "repulsion" between unrelated images in the embedding?

## Architecture Onboarding

- **Component map**: Feature Extractor (FIRE local + SuperGlobal global) -> CANN Index (Local) + Global Index -> Query -> CANN Retrieval -> Top-k candidates + Sparse Dissimilarity Matrix -> SMACOF MDS -> (k+1) Embeddings -> Concatenate/Fuse MDS Embeddings + Global Features -> Re-ranker -> Final Ranking

- **Critical path**: The **SMACOF MDS computation**. This iterative step converts the raw local rankings into a usable embedding. If convergence (epsilon) is too strict, latency spikes; if too loose, ranking quality drops.

- **Design tradeoffs**:
  - **Latency vs. Context**: The system takes ~0.7s (0.2s search + 0.5s MDS/rerank). This is significantly slower than pure global search but offers higher accuracy.
  - **Memory vs. Accuracy**: Requires storing local features (~21kB/image), which is ~2x higher than compressed global methods like AMES.

- **Failure signatures**:
  - **High Latency**: If `k` (top ranked for MDS) is set too high, the O(k^2) complexity of MDS dominates.
  - **Poor Generalization**: If query images have no close local neighbors in the database, the sparse distance matrix is mostly "1s", leading to a degenerate MDS embedding.
  - **Metric Mismatch**: Attempting to substitute a different local similarity metric without adjusting the MDS stress function weights may cause non-convergence.

- **First 3 experiments**:
  1. **Sparse Matrix Robustness**: Vary the `k` parameter (number of candidates for MDS) on a subset of ROxf to find the knee curve where MDS latency outweighs mAP gains.
  2. **Distance Completion Ablation**: Replace the "missing distance = 1" heuristic with "missing distance = mean distance" to validate the paper's design choice for sparse matrix completion.
  3. **Feature Isolation**: Run the pipeline using *only* MDS embeddings (w=1.0) vs *only* SuperGlobal (w=0.0) on the "Hard" subset of ROxf to quantify the specific contribution of the geometric MDS signal vs the semantic global signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the MDS-based re-ranking framework maintain its efficiency and accuracy advantages when applied to complex, learned similarity measures (e.g., AMES) rather than the hand-crafted Chamfer similarity used in the experiments?
- **Basis in paper**: [explicit] The authors explicitly state in the Future Work section that this approach "opens exciting possibilities for using it with diverse similarity measures, including learned ones like those in AMES."
- **Why unresolved**: The current implementation relies on FIRE features and Chamfer similarity; learned metrics often have different properties and computational requirements that may affect the MDS embedding quality.
- **What evidence would resolve it**: Integrating a learned similarity metric like AMES into the L2G pipeline and benchmarking the retrieval accuracy and latency against the current FIRE-based results.

### Open Question 2
- **Question**: Does the optimal weighting between MDS embeddings and global features transfer effectively to non-landmark domains (e.g., product or medical image retrieval) without extensive re-tuning?
- **Basis in paper**: [inferred] The paper notes that all hyperparameters, specifically the weight $w$ between feature types, were "tuned on a small sample (1000 images) of Oxford only."
- **Why unresolved**: Tuning on a single, specific dataset (Oxford buildings) raises concerns that the sensitivity of the MDS embedding might require different fusion weights for datasets with varying texture or geometry distributions.
- **What evidence would resolve it**: Evaluating the L2G pipeline on diverse datasets (e.g., MET or products) using the current Oxford-tuned hyperparameters to test for performance degradation.

### Open Question 3
- **Question**: Can the concept of constructing embeddings from sparse pairwise distances be extended to build a complete indexing system for generic, non-metric similarity search?
- **Basis in paper**: [explicit] The authors suggest that "embedding from pairwise distances requires only a constant number of pairs suggests a novel and efficient approach to building indexing and query systems for any generic similarity."
- **Why unresolved**: The paper demonstrates this only for the re-ranking stage (using $k$ candidates); it does not show if this principle holds or is efficient when applied to the entire database indexing process.
- **What evidence would resolve it**: A theoretical analysis or system implementation demonstrating that on-the-fly MDS embeddings can scale to index millions of items for generic similarity search efficiently.

## Limitations

- **Reproducibility barrier**: Performance claims rely heavily on proprietary feature extractors (FIRE local, SuperGlobal) whose exact implementations are not detailed.
- **Computational cost**: MDS computation on 700 candidates per query (~0.5s) may be prohibitive for real-time applications, with no systematic exploration of latency-accuracy tradeoffs.
- **Assumption validation**: The MDS embedding approach assumes that pairwise local dissimilarities can be meaningfully embedded into a Euclidean space, but this is not rigorously validated for the specific landmark retrieval task.

## Confidence

- **High confidence**: The core mechanism of using CANN for efficient local search is well-established and directly supported by the cited work [1]. The improvement over baseline global methods on ROxf/RPar datasets is measurable and consistent across experiments.
- **Medium confidence**: The MDS embedding approach is novel for this application, and while the mathematical framework is sound, the paper provides limited empirical validation of whether the embedding truly captures the intended local similarity structure. The specific hyperparameter choices (w=0.19, k=700) appear effective but lack systematic justification.
- **Low confidence**: Claims about superiority over concurrent works like AMES and BiLSTM are difficult to verify without access to the exact feature extraction protocols and re-ranking implementations used by those methods.

## Next Checks

1. **Embedding quality validation**: Generate t-SNE visualizations of MDS embeddings for random query-image pairs to visually verify that the geometric distances reflect semantic similarity. Quantify whether stress function convergence correlates with mAP improvements.

2. **Scalability stress test**: Benchmark the complete L2G pipeline on 10K vs 1M distractor sets to measure whether the claimed O(kÂ²) MDS complexity becomes a bottleneck, and whether accuracy degrades gracefully with scale.

3. **Component ablation on HARD subset**: Run the pipeline with MDS-only (w=1.0) and SuperGlobal-only (w=0.0) on the HARD protocol of ROxf to quantify the marginal contribution of the geometric MDS signal versus the semantic global feature, particularly for the most challenging queries.