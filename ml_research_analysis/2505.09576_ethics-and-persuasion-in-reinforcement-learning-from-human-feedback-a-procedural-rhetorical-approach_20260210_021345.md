---
ver: rpa2
title: 'Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural
  Rhetorical Approach'
arxiv_id: '2505.09576'
source_url: https://arxiv.org/abs/2505.09576
tags:
- human
- language
- rlhf
- llms
- rhetoric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a rhetorical analysis of Reinforcement Learning
  from Human Feedback (RLHF), a training technique used to enhance large language
  models (LLMs) like ChatGPT and Claude. Using Ian Bogost's concept of procedural
  rhetoric, the authors shift focus from content analysis to examining the persuasive
  mechanisms embedded in RLHF-enhanced LLMs.
---

# Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach

## Quick Facts
- arXiv ID: 2505.09576
- Source URL: https://arxiv.org/abs/2505.09576
- Reference count: 40
- Authors: Shannon Lodoen; Alexi Orchard
- Primary result: This paper analyzes how RLHF trains language models to reinforce hegemonic language use, perpetuate biases, decontextualize learning, and impact interpersonal relations through procedural rhetoric.

## Executive Summary
This paper presents a rhetorical analysis of Reinforcement Learning from Human Feedback (RLHF), a training technique used to enhance large language models (LLMs) like ChatGPT and Claude. Using Ian Bogost's concept of procedural rhetoric, the authors shift focus from content analysis to examining the persuasive mechanisms embedded in RLHF-enhanced LLMs. They analyze three key procedures: determining language conventions, information seeking practices, and relationship formation. The analysis reveals how these procedures reinforce hegemonic language use, perpetuate biases, decontextualize learning, and impact interpersonal relations. The paper highlights ethical concerns around transparency, trust, and the hidden assumptions in AI-generated text, while calling for greater attention to the social and cultural implications of RLHF-enhanced AI technologies.

## Method Summary
The paper employs rhetorical analysis through Ian Bogost's framework of procedural rhetoric to examine how RLHF training procedures shape LLM outputs. Rather than analyzing the content of AI responses, the authors investigate the underlying mechanisms and rules that govern how these systems operate. They identify three key procedural areas: language conventions, information seeking practices, and relationship formation, analyzing how each reinforces particular values and behaviors. The analysis draws on existing literature about RLHF training methodologies, annotator demographics, and user interaction patterns with LLMs.

## Key Results
- RLHF training embeds hegemonic language norms and values through human annotator feedback that encodes specific preferences into reward models
- Dialogic AI interfaces shift users from source evaluation to answer acceptance, potentially degrading critical information literacy
- AI companion interactions establish procedural expectations for relationships that may transfer to human-human interactions, with features like Replika's "Relationship Bond" mirroring LLM training procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF embeds persuasive procedures through human annotator feedback that encodes specific values and language norms into model outputs.
- Mechanism: Human annotators evaluate model responses using criteria (Helpful, Honest, Harmless), and their judgments—shaped by demographic backgrounds, instructions, and inherent biases—become encoded into the reward model that guides future outputs.
- Core assumption: Annotator preferences are not universally representative; the paper notes annotators are predominantly from specific demographics (Filipino/Bangladeshi nationals for OpenAI, white ethnic backgrounds for Anthropic).
- Evidence anchors:
  - [abstract] "the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more 'human-like'"
  - [section IV.A] "The black-boxed nature of the training models and human involvement prevent users from understanding or even considering why and how the responses being provided have been selected"
  - [corpus] Related papers on persuasion in LLMs exist but primarily focus on output content rather than training procedures; direct corpus support for procedural rhetoric analysis is limited.
- Break condition: If annotator selection becomes sufficiently diverse and instruction bias is systematically mitigated, the hegemonic reinforcement effect may weaken—though the paper suggests this remains an open challenge.

### Mechanism 2
- Claim: Dialogic information-seeking interfaces shift user behavior from source evaluation to answer acceptance, potentially obscuring reliability assessment.
- Mechanism: Unlike search engines that present multiple sources for user evaluation, LLMs synthesize information into natural language responses that appear authoritative, reducing user practice in source verification.
- Core assumption: Users have a "natural propensity to take the LLM's responses at face value" (citing Kim et al., 2024).
- Evidence anchors:
  - [section IV.B] "the onus is on the LLM (and its programmers) to moderate and mitigate users' expectations of trustworthiness"
  - [section IV.B] "people tend to agree with the AI system when its responses are provided"
  - [corpus] "Towards Strategic Persuasion with Language Models" corroborates that LLMs demonstrate persuasive capabilities comparable to humans, though corpus papers focus more on output persuasion than interface procedure.
- Break condition: If LLMs consistently express appropriate uncertainty (hedging) and cite verifiable sources, user over-reliance may decrease.

### Mechanism 3
- Claim: AI companion interactions establish new procedural expectations for relationships that may transfer to human-human interactions.
- Mechanism: Social chatbots operate without human limitations (fatigue, emotional variation, boundary-setting), creating interaction patterns where user preferences are always accommodated—patterns that may reshape expectations for human relationships.
- Core assumption: Procedures learned in AI interactions transfer to human social cognition; the paper notes this warrants further research.
- Evidence anchors:
  - [section IV.C] "procedures surrounding interpersonal interactions espoused by AI chatbots and companions could have negative impacts to human-to-human relationships"
  - [section IV.C] "Replika released a feature... 'Relationship Bond'... this process actually mirrors the process of LLM training (rewarding certain responses)"
  - [corpus] Corpus papers on persuasion do not directly address interpersonal relationship procedures; this remains an underexplored area.
- Break condition: If users maintain clear categorical distinctions between AI and human relationships, transfer effects may be limited.

## Foundational Learning

- Concept: **Procedural Rhetoric** (Bogost)
  - Why needed here: The paper's entire analytical framework depends on understanding how rule-based systems make implicit arguments through their operation, not just their outputs.
  - Quick check question: Can you explain how a traffic light makes a procedural argument about time and priority, distinct from any text or symbols it displays?

- Concept: **RLHF Three-Phase Pipeline**
  - Why needed here: Understanding data collection → reward modeling → policy optimization is necessary to trace where biases enter and how they propagate.
  - Quick check question: In which phase does human judgment become automated at scale, and what risk does this introduce?

- Concept: **Annotator Selection Bias**
  - Why needed here: The paper's critique of normative language reinforcement depends on recognizing that annotator demographics and instruction design systematically shape model outputs.
  - Quick check question: If annotators are selected for high agreement rates, what type of perspective diversity might be unintentionally filtered out?

## Architecture Onboarding

- Component map:
  Input Layer (User prompts) -> Training Pipeline (Human annotators → Feedback data → Reward model → Policy optimization) -> Evaluation Metrics (Helpful / Honest / Harmless) -> Output Layer (Natural language responses)

- Critical path:
  1. Annotator instruction design → shapes feedback quality
  2. Reward model training → encodes annotator preferences
  3. Policy optimization → deploys encoded values at scale
  4. User interface → mediates information-seeking and relationship procedures

- Design tradeoffs:
  - Annotator agreement vs. perspective diversity (76% agreement for OpenAI may reduce bias but also reduce representation)
  - Helpfulness vs. Honesty (completing tasks vs. expressing uncertainty)
  - Conversational engagement vs. source transparency (dialogic satisfaction vs. verifiability)

- Failure signatures:
  - Instruction bias: Annotators influenced by dataset creator instructions, leading to "overestimation of model's performance" (Parmar et al.)
  - Low annotator agreement: Anthropic observed only 63% agreement, indicating divergent value judgments
  - User over-reliance: Agreement with AI responses increases when certainty is expressed, regardless of accuracy

- First 3 experiments:
  1. **Annotator diversity audit**: Map demographic and cultural distribution of annotators; measure correlation between annotator characteristics and preferred response styles.
  2. **Uncertainty expression A/B test**: Compare user trust and agreement rates between high-certainty and appropriately-hedged responses on factual questions.
  3. **Source citation interface test**: Measure whether providing inline citations changes user behavior compared to synthesized natural language answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the procedural "scripts" of AI companions impact human-to-human empathy and relational expectations?
- Basis in paper: [explicit] The authors explicitly ask if features like Replika's "Relationship Bond," which quantifies interactions, might "shift how (and if) users feel empathy" or alter how users relate to different perspectives.
- Why unresolved: The paper establishes the theoretical link between procedural rhetoric and relationship scripts but notes that empirical data on the long-term sociological impacts is currently lacking.
- What evidence would resolve it: Longitudinal studies comparing the social behaviors and empathy levels of frequent AI companion users versus control groups.

### Open Question 2
- Question: Does the shift from search-and-recall to AI-mediated "conversational search" degrade users' ability to identify reliable sources?
- Basis in paper: [explicit] Section IV.B highlights the concern that "users may struggle more with identifying reliable sources, interpreting texts, and thinking critically" due to the dialogic method that obscures source reliability.
- Why unresolved: While the procedural difference is identified, the paper calls for future "discipline-specific generative AI training" rather than presenting empirical results on skill retention.
- What evidence would resolve it: Comparative user studies assessing source verification skills between those using traditional search engines and those using LLM-based conversational search.

### Open Question 3
- Question: How can RLHF methodologies move from "universal framings" to "situated value alignment"?
- Basis in paper: [explicit] The authors cite Arzberger et al. to argue that current models rely on universal assumptions and that future research must engage the "situated knowledge" of annotators at design-time and users at use-time.
- Why unresolved: Current RLHF processes generally aggregate preferences into a single reward model, obscuring cultural nuance and reinforcing hegemonic language use.
- What evidence would resolve it: The development of new RLHF frameworks that successfully weigh and retain diverse, context-specific annotator values without converging to a homogenous mean.

## Limitations
- The analysis is primarily theoretical, drawing on existing literature about RLHF rather than conducting new empirical studies on user behavior or annotator selection
- The paper does not provide concrete quantitative data on the extent of bias in RLHF training or specific measures of how procedural rhetoric affects user trust and behavior
- While the framework identifies important ethical concerns, it does not offer specific technical solutions for mitigating the identified issues in RLHF training pipelines

## Confidence
- The paper's theoretical framework using procedural rhetoric: High
- Claims about annotator bias and demographic limitations: High
- Concerns about user over-reliance on LLM outputs: Medium
- Assertions about long-term impact on human relationships: Low

## Next Checks
1. Verify the demographic composition of annotators across major RLHF implementations (OpenAI, Anthropic, etc.) and assess correlation with model output preferences
2. Design and conduct user studies comparing source verification skills between traditional search engine users and LLM conversational interface users
3. Develop a framework for measuring "procedural persuasion" in AI interfaces and test its predictive validity for user behavior and trust metrics