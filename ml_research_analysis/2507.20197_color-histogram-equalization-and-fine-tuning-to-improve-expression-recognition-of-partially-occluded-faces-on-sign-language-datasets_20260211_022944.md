---
ver: rpa2
title: Color histogram equalization and fine-tuning to improve expression recognition
  of (partially occluded) faces on sign language datasets
arxiv_id: '2507.20197'
source_url: https://arxiv.org/abs/2507.20197
tags:
- dataset
- face
- mean
- color
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated how well computer vision methods can classify
  facial expressions on a sign language dataset. The authors introduced a preprocessing
  technique using histogram equalization and fine-tuning to improve recognition performance.
---

# Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets

## Quick Facts
- arXiv ID: 2507.20197
- Source URL: https://arxiv.org/abs/2507.20197
- Reference count: 27
- 83.8% mean sensitivity with 0.042 variance on sign language dataset

## Executive Summary
This study introduces a preprocessing technique combining color histogram equalization and progressive fine-tuning to improve facial expression recognition (FER) on sign language datasets. The method achieves 83.8% mean sensitivity on the FePh dataset, with region-specific models recognizing emotions from partial faces at near-full-face performance. Notably, lower-face recognition (79.6%) exceeded upper-face recognition (77.9%), surpassing human-level performance for upper-face emotion classification.

## Method Summary
The approach uses MobileNetV2 pretrained on ImageNet, fine-tuned in two stages: first on AffectNet (273k images), then on FePh (2,547 samples) with progressive layer unfreezing. A novel preprocessing pipeline includes MTCNN face detection, square cropping, 110% zoom-out, histogram equalization (applied before rotation to avoid black corner artifacts), and rotation for eye alignment. The method also trains separate models for full-face, upper-half, and lower-half face recognition to evaluate occlusion robustness.

## Key Results
- 83.8% mean sensitivity with 0.042 variance across 8 emotion classes on FePh dataset
- Upper-face recognition (77.9%) exceeded human performance (68.34% hearing, 60.17% deaf)
- Lower-face recognition (79.6%) outperformed upper-face recognition despite being less studied
- Method demonstrates potential for emotion recognition with masks/VR helmets through partial-face analysis

## Why This Works (Mechanism)

### Mechanism 1: Color Histogram Equalization for Domain Adaptation
Histogram equalization redistributes pixel intensities to maximize contrast and standardize color distributions across datasets with different color profiles. This enhances detection of shadows from facial muscle activation while freeing the model from hard-coded values dependent on specific training sets.

### Mechanism 2: Two-Stage Progressive Fine-Tuning
Sequential fine-tuning (AffectNet → FePh) with phased layer unfreezing bridges domain gaps better than direct transfer. The approach first adapts the model to general FER on 273k AffectNet images, then fine-tunes on the specific characteristics of sign language expressions in the smaller FePh dataset (2,547 samples).

### Mechanism 3: Partial Face Recognition via Occlusion-Robust Feature Learning
CNNs learn region-specific features through separate training on upper and lower face regions. The 4.2-5.9% gap between partial and full-face recognition suggests substantial information redundancy, with different emotions having distinguishable patterns in both face regions.

## Foundational Learning

- **Histogram Equalization**
  - Why needed: Core preprocessing innovation for cross-domain color normalization
  - Quick check: Given pixel intensities clustered in [100, 150], what range will histogram equalization map them to?

- **Transfer Learning with Layer-wise Fine-tuning**
  - Why needed: Entire training strategy depends on proper layer freezing/unfreezing
  - Quick check: Why might fine-tuning only the final layer for 3 epochs before unfreezing all layers work better than immediate full fine-tuning?

- **Sensitivity vs. Accuracy in Imbalanced Datasets**
  - Why needed: Paper emphasizes mean sensitivity (83.8%) and low variance (0.042) due to severe class imbalance
  - Quick check: On a dataset with 90% "happy" samples, what sensitivity does a model that always predicts "happy" achieve for "sad"?

## Architecture Onboarding

- Component map: Input Image → Face Detection (MTCNN) → Crop & Square → Zoom Out (110%) → Histogram Equalization (OpenCV) → Rotation (eye alignment) → MobileNetV2 backbone → Custom Classifier (8 classes)
- Critical path: Histogram equalization must occur before rotation to prevent black corners from corrupting color distribution
- Design tradeoffs:
  - Batch size 16 vs. 32: Smaller batches with longer training achieved 83.8% vs. 75.9% with batch 32
  - Mean subtraction vs. histogram equalization: Dataset-specific vs. dataset-agnostic color normalization
  - Single model vs. region-specific models: Trading training cost for cleaner evaluation of region-specific performance
- Failure signatures:
  - Color profile mismatch: Performance drop when applying FePh-trained model to new datasets without histogram equalization
  - Resolution mismatch: FePh face crops are ~50×50 pixels; high-resolution images may require downsampling
  - Contempt class: Always check first due to 0% sensitivity in baseline
- First 3 experiments:
  1. Ablation test: Disable histogram equalization and measure sensitivity drop
  2. Cross-dataset validation: Test AffectNet+histogram model directly on FePh without fine-tuning
  3. Occlusion pattern test: Compare zero-masking vs. Gaussian noise vs. mean-filling for partial-face performance

## Open Questions the Paper Calls Out

- Does the neural network's attention shift from eye shapes to wrinkle formation when fine-tuned for FePh? Needs further investigation using explainable AI methods like GradCAM saliency maps.
- Does the histogram equalization and fine-tuning pipeline improve recognition on other datasets with specific recording conditions and low color variance? Future work should investigate applicability to television studio environments.
- Is the improved performance on FePh due to learning actual signer-specific emotion expressions or overfitting to the dataset's low resolution and specific image artifacts? The authors note the model likely adapts to image quality rather than grasping different ways of expressing emotions.

## Limitations

- Small dataset size (2,547 samples) raises concerns about overfitting and real-world robustness
- Artificial half-face occlusions limit claims about mask/VR helmet applications without testing on actual masked faces
- Unclear learning rate schedules and lack of explicit data augmentation strategies create reproducibility challenges

## Confidence

- **High confidence:** 83.8% mean sensitivity with 0.042 variance demonstrates effectiveness on tested dataset; upper-face recognition exceeding human performance is well-supported
- **Medium confidence:** Cross-domain generalization claims through histogram equalization are supported by 24.3%→83.8% improvement but lack isolating ablation studies
- **Low confidence:** Mask/VR helmet application claims extend beyond tested artificial occlusions without empirical validation on actual masked faces

## Next Checks

1. Conduct ablation study comparing models with and without histogram equalization on FePh to isolate its contribution
2. Test model on actual masked face datasets (e.g., MaskedFER) rather than artificial half-face occlusions
3. Evaluate model robustness to varying lighting conditions and image qualities outside controlled FePh environment