---
ver: rpa2
title: 'ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts'
arxiv_id: '2511.23442'
source_url: https://arxiv.org/abs/2511.23442
tags:
- stitching
- astro
- trajectory
- offline
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASTRO is a data augmentation framework for offline RL that generates
  distributionally novel and dynamics-consistent trajectories through adaptive stitching.
  It addresses the limitations of existing methods by using a Temporal Distance Representation
  (TDR) for distinct, reachable target selection and a dynamics-guided stitch planner
  with rollout deviation feedback for feasible completion.
---

# ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts

## Quick Facts
- arXiv ID: 2511.23442
- Source URL: https://arxiv.org/abs/2511.23442
- Reference count: 40
- Primary result: Achieves 32.7% (+9.68) average performance improvement on OGBench suite

## Executive Summary
ASTRO introduces an adaptive trajectory stitching framework for offline reinforcement learning that addresses the challenge of generating novel, high-quality trajectories from existing datasets. The framework leverages a Temporal Distance Representation (TDR) to identify distinct and reachable target states, combined with a dynamics-guided stitch planner that incorporates rollout deviation feedback for feasible trajectory completion. By decoupling planning from dynamics modeling and introducing iterative refinement, ASTRO significantly improves upon existing trajectory stitching methods, demonstrating substantial performance gains on both challenging offline RL benchmarks and standard datasets.

## Method Summary
ASTRO presents a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories through adaptive stitching. The core innovation lies in using Temporal Distance Representation (TDR) for target selection and a dynamics-guided stitch planner with rollout deviation feedback. The framework addresses limitations of existing methods by decoupling planning from dynamics modeling and incorporating iterative refinement. ASTRO improves trajectory stitching by selecting distinct and reachable target states, then using a dynamics-guided planner with rollout deviation feedback to ensure feasible completion. The approach is evaluated across multiple offline RL algorithms, showing significant performance improvements on challenging benchmarks.

## Key Results
- Achieves 32.7% (+9.68) average performance improvement on OGBench suite
- Demonstrates consistent improvements across multiple offline RL algorithms
- Shows significant gains over existing trajectory stitching methods

## Why This Works (Mechanism)
The framework works by first identifying suitable target states using Temporal Distance Representation, which captures temporal relationships between states to ensure both distinctiveness and reachability. The dynamics-guided stitch planner then generates trajectories between current and target states while monitoring for rollout deviations that would indicate infeasibility. By decoupling the planning process from the underlying dynamics model and incorporating iterative refinement, the system can generate trajectories that are both novel (distributionally different from training data) and feasible (consistent with environment dynamics).

## Foundational Learning
- Temporal Distance Representation (TDR): A method for encoding temporal relationships between states to assess reachability and distinctiveness. Why needed: To identify target states that are both novel and achievable from current states. Quick check: Verify TDR can distinguish between reachable and unreachable states in controlled test environments.
- Dynamics-guided planning: A planning approach that incorporates knowledge of environment dynamics during trajectory generation. Why needed: To ensure generated trajectories are physically feasible and consistent with environment behavior. Quick check: Validate that planned trajectories respect known constraints and avoid infeasible regions.
- Rollout deviation feedback: A mechanism for monitoring trajectory generation and detecting when planned paths deviate from expected dynamics. Why needed: To identify and correct trajectories that become infeasible during execution. Quick check: Measure detection accuracy of rollout deviations in simulated environments.

## Architecture Onboarding

Component Map: State Representation -> TDR Target Selection -> Dynamics-Guided Planning -> Rollout Monitoring -> Iterative Refinement -> Augmented Trajectory

Critical Path: The core execution flow involves TDR encoding of states, target selection based on temporal distance metrics, dynamics-guided trajectory planning between current and target states, continuous monitoring for rollout deviations, and iterative refinement until a valid augmented trajectory is produced.

Design Tradeoffs: The framework trades computational overhead from iterative refinement and rollout monitoring against improved trajectory quality and feasibility. The decoupling of planning from dynamics modeling allows for more flexible and robust trajectory generation but requires careful coordination between components.

Failure Signatures: Common failure modes include selecting unreachable target states despite TDR guidance, rollout deviations that cannot be corrected through refinement, and computational bottlenecks in the iterative refinement process. These manifest as poor-quality augmented trajectories or excessive computation times.

First Experiments:
1. Validate TDR's ability to distinguish reachable vs unreachable states in simple grid-world environments
2. Test dynamics-guided planning with known constraints in deterministic environments
3. Evaluate rollout deviation detection accuracy in environments with varying levels of stochasticity

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on quality of base dataset and underlying dynamics model
- Computational overhead from iterative refinement may limit real-time applications
- Effectiveness in highly stochastic environments or with sparse reward structures remains unexplored

## Confidence

High confidence:
- Core methodology (TDR representation, dynamics-guided planning, rollout deviation feedback) is technically sound
- Well-grounded in existing literature on trajectory optimization and data augmentation

Medium confidence:
- Performance improvements on OGBench and D4RL are compelling
- Results depend on specific implementation details and hyperparameter choices

Low confidence:
- Long-term stability in continuous, high-dimensional state spaces with complex dynamics
- Scalability to real-world applications with significant computational constraints

## Next Checks
1. Conduct ablation studies to quantify individual contributions of TDR, dynamics-guided planning, and rollout deviation feedback
2. Test robustness across diverse reward structures, including sparse and delayed reward scenarios
3. Analyze computational overhead and wall-clock time requirements compared to baseline methods