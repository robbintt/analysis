---
ver: rpa2
title: Detecting and Understanding Hateful Contents in Memes Through Captioning and
  Visual Question-Answering
arxiv_id: '2504.16723'
source_url: https://arxiv.org/abs/2504.16723
tags:
- hateful
- memes
- content
- multimodal
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal framework for detecting hateful
  content in memes by integrating OCR, captioning, retrieval-augmented generation,
  and visual question answering. The system processes both textual and visual components
  to uncover subtle and implicit hate signals that evade traditional unimodal detectors.
---

# Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering

## Quick Facts
- **arXiv ID**: 2504.16723
- **Source URL**: https://arxiv.org/abs/2504.16723
- **Reference count**: 35
- **Key outcome**: RAG (sub_label + VQA) method achieves 73.50% accuracy and 78.35% AUC-ROC on Facebook Hateful Memes dataset

## Executive Summary
This paper presents a multimodal framework for detecting hateful content in memes by integrating OCR, captioning, retrieval-augmented generation, and visual question answering. The system processes both textual and visual components to uncover subtle and implicit hate signals that evade traditional unimodal detectors. A key innovation is the use of sub-label classification (race, religion, etc.) combined with iterative VQA-based reasoning to improve retrieval precision and detection accuracy. Evaluated on the Facebook Hateful Memes dataset, the proposed approach significantly outperforms unimodal and simpler multimodal baselines while narrowing the gap to human-level performance.

## Method Summary
The proposed framework employs a multi-stage approach to detect hateful content in memes. First, OCR extracts textual information from images, followed by machine captioning to describe visual elements. A retrieval-augmented generation system queries a knowledge base using sub-label classifications (race, religion, etc.) and VQA-based questions to gather contextual information. The extracted text, generated captions, and retrieved context are then fused to make the final hate detection decision. The iterative VQA component allows the system to ask clarifying questions about the meme's content, progressively refining its understanding of potentially hateful elements. This multimodal approach addresses the challenge of memes that combine benign images with hateful text or vice versa.

## Key Results
- RAG (sub_label + VQA) method achieves 73.50% accuracy and 78.35% AUC-ROC on Facebook Hateful Memes dataset
- Outperforms unimodal text-only (65.80% accuracy) and image-only (59.90% accuracy) baselines
- Shows significant improvement over simpler multimodal baselines while approaching human-level performance (85.50% accuracy)

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to capture both explicit and implicit hate signals through multimodal fusion. By combining OCR-extracted text with machine-generated captions, the system addresses the dual nature of hateful memes where either visual or textual components alone may appear benign. The retrieval-augmented generation component enriches the understanding by incorporating external knowledge relevant to the identified sub-labels, while the iterative VQA allows for context-specific questioning that can uncover subtle hate indicators missed by static feature extraction. This layered approach mimics human reasoning by progressively building context before making classification decisions.

## Foundational Learning
**OCR (Optical Character Recognition)**
- Why needed: Memes often contain hateful text overlaid on seemingly innocuous images; extracting this text is essential for understanding the complete message
- Quick check: Verify OCR accuracy on diverse meme fonts, colors, and backgrounds

**Visual Question Answering (VQA)**
- Why needed: Allows the system to ask context-specific questions about meme content, uncovering implicit hate signals that static analysis might miss
- Quick check: Test VQA component's ability to generate relevant clarifying questions for ambiguous meme elements

**Retrieval-Augmented Generation (RAG)**
- Why needed: Enriches the detection process by incorporating external knowledge relevant to identified sub-labels, improving context understanding
- Quick check: Evaluate retrieval precision for different sub-label categories (race, religion, etc.)

## Architecture Onboarding
**Component map**: OCR -> Captioning -> Sub-label Classification -> RAG Query -> VQA Iteration -> Context Fusion -> Hate Detection

**Critical path**: The most computationally intensive step is the iterative VQA process, which can significantly impact real-time performance. The fusion of multimodal information also represents a critical decision point where classification accuracy is determined.

**Design tradeoffs**: The framework prioritizes detection accuracy over computational efficiency, accepting higher resource requirements for improved performance. The multi-stage approach trades latency for nuanced understanding, making it less suitable for real-time moderation but more effective for thorough content analysis.

**Failure signatures**: 
- False negatives occur when implicit hate relies on cultural context not captured in the knowledge base
- False positives arise when benign text-image combinations trigger hate-related sub-labels
- OCR failures on stylized text can eliminate crucial hate indicators
- VQA may generate irrelevant questions that dilute contextual understanding

**First experiments**:
1. Run ablation tests removing each component (OCR, captioning, RAG, VQA) to quantify individual contributions
2. Test framework on memes with deliberately obscured or stylized text to measure OCR robustness
3. Evaluate performance on memes targeting different hate categories to identify bias or weakness patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance gap remains between system (73.50% accuracy) and human-level performance (85.50% accuracy)
- Computational overhead challenges real-time deployment feasibility
- Limited discussion of false positive/negative handling for content moderation applications
- OCR component may struggle with handwritten or highly stylized meme text

## Confidence
**High confidence**: The framework demonstrably outperforms traditional unimodal detectors and simpler multimodal baselines on the Facebook Hateful Memes dataset.

**Medium confidence**: The claim of robustness in identifying nuanced, context-dependent hate is supported by performance metrics but lacks detailed edge case analysis.

**Low confidence**: Scalability and real-time deployment potential are acknowledged as challenges but lack specific performance metrics or resource utilization data.

## Next Checks
1. Conduct ablation studies to quantify individual contributions of OCR, captioning, retrieval-augmented generation, and VQA components, identifying potential bottlenecks or redundant elements.

2. Test the framework on diverse meme datasets from different platforms and cultural contexts to assess generalization capabilities and identify potential biases in the detection system.

3. Implement and benchmark the system's performance under real-time constraints, measuring latency, throughput, and resource utilization to determine practical deployment feasibility.