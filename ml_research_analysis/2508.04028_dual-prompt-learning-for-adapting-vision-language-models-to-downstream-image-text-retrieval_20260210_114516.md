---
ver: rpa2
title: Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text
  Retrieval
arxiv_id: '2508.04028'
source_url: https://arxiv.org/abs/2508.04028
tags:
- learning
- retrieval
- prompt
- image
- dcar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting pre-trained vision-language
  models (VLMs) to downstream image-text retrieval tasks, specifically focusing on
  fine-grained attribute recognition and subtle subcategory discrimination. The authors
  propose DCAR, a dual-prompt learning framework that dynamically adjusts prompt vectors
  from both semantic and visual dimensions.
---

# Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval

## Quick Facts
- arXiv ID: 2508.04028
- Source URL: https://arxiv.org/abs/2508.04028
- Authors: Yifan Wang; Tao Wang; Chenwei Tang; Caiyang Yu; Zhengqing Zang; Mengmi Zhang; Shudong Huang; Jiancheng Lv
- Reference count: 40
- Primary result: DCAR improves Recall@1 by 10.61% and 6.63% over CLIP in 16-shot fine-grained retrieval

## Executive Summary
This paper introduces DCAR, a dual-prompt learning framework designed to adapt pre-trained vision-language models (VLMs) for fine-grained image-text retrieval tasks. The method addresses challenges in recognizing subtle attributes and distinguishing between similar subcategories by dynamically adjusting prompt vectors from both semantic and visual dimensions. The authors construct a new benchmark dataset, FDRD, featuring over 1,500 fine categories and 230,000 image-caption pairs with detailed attribute annotations to evaluate their approach.

## Method Summary
DCAR employs a dual-prompt learning strategy that jointly optimizes attribute and category features through mutual information-based token re-weighting and category-aware negative sample augmentation. The framework dynamically adjusts prompt vectors from both semantic and visual dimensions to improve fine-grained recognition capabilities. The method is evaluated on a newly constructed Fine-class Described Retrieval Dataset (FDRD), which contains detailed attribute annotations across 1,500+ categories and 230,000 image-caption pairs.

## Key Results
- DCAR achieves state-of-the-art performance on FDRD dataset
- Improves average Recall@1 by 10.61% for image-to-text retrieval
- Improves average Recall@1 by 6.63% for text-to-image retrieval
- Results demonstrated in 16-shot learning setting

## Why This Works (Mechanism)
DCAR works by simultaneously optimizing attribute and category features through a dual-prompt mechanism. The semantic dimension captures high-level category information while the visual dimension focuses on fine-grained attribute details. Mutual information maximization between these dimensions ensures that both attribute and category features are well-aligned, while category-aware negative sampling helps the model distinguish between similar subcategories.

## Foundational Learning
- Vision-Language Models (VLMs): Pre-trained models that learn joint representations of images and text
  - Why needed: Provides foundation for image-text retrieval tasks
  - Quick check: Verify model understands basic image-text relationships
- Fine-grained recognition: Ability to distinguish subtle differences between similar categories
  - Why needed: Essential for attribute-level retrieval tasks
  - Quick check: Model can differentiate between similar subcategories
- Prompt engineering: Using learned prompts to guide model behavior
  - Why needed: Enables adaptation of pre-trained models to specific tasks
  - Quick check: Prompts improve task-specific performance
- Mutual information maximization: Maximizing shared information between modalities
  - Why needed: Ensures semantic alignment between visual and textual features
  - Quick check: Visual and text embeddings remain correlated
- Negative sampling: Selecting challenging negative examples during training
  - Why needed: Improves model's ability to distinguish similar classes
  - Quick check: Model performance improves with harder negative samples

## Architecture Onboarding
Component map: Image encoder -> Visual prompt adapter -> Mutual information module <- Text encoder <- Semantic prompt adapter
Critical path: Image/Text input → Dual prompt adjustment → Feature extraction → Mutual information optimization → Retrieval output
Design tradeoffs: The dual-prompt approach increases model complexity but provides better fine-grained discrimination compared to single-prompt methods
Failure signatures: Poor performance on attribute recognition indicates inadequate visual prompt adaptation; failure to distinguish similar categories suggests insufficient semantic prompt tuning
First experiments:
1. Test basic image-text matching performance on FDRD without dual prompts
2. Evaluate attribute recognition accuracy separately from category classification
3. Compare performance with different numbers of negative samples in category-aware augmentation

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on a newly constructed FDRD dataset, raising questions about generalizability to other fine-grained retrieval scenarios
- Limited baseline comparisons make it difficult to definitively claim state-of-the-art performance
- The dual-prompt mechanism introduces computational complexity that may affect practical deployment

## Confidence
- High confidence in the methodology description and experimental setup
- Medium confidence in the generalizability of results to other fine-grained retrieval scenarios
- Medium confidence in the claimed state-of-the-art performance due to limited baseline comparisons

## Next Checks
1. Evaluate DCAR on established fine-grained retrieval benchmarks (e.g., CUB-200-2011, Cars196) to assess cross-dataset generalization
2. Conduct comprehensive ablation studies to quantify the individual contributions of semantic vs. visual prompt adjustments and negative sample augmentation
3. Test computational efficiency and memory requirements of DCAR compared to baseline methods to evaluate practical deployment feasibility