---
ver: rpa2
title: 'Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini,
  and RankLLM'
arxiv_id: '2509.02558'
source_url: https://arxiv.org/abs/2509.02558
tags:
- retrieval
- bm25
- bright
- query
- anserini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents reproducible baselines for the BRIGHT benchmark,
  which evaluates retrieval systems on reasoning-intensive queries. The authors establish
  solid retrieval baselines using BM25, BGE-large-en-v1.5, and SPLADE-v3 integrated
  into Anserini, Pyserini, and RankLLM toolkits.
---

# Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini, and RankLLM

## Quick Facts
- **arXiv ID:** 2509.02558
- **Source URL:** https://arxiv.org/abs/2509.02558
- **Reference count:** 40
- **Primary result:** Query-side BM25 improves nDCG@10 for medium-length reasoning queries; fusion + reranking achieves nDCG@10 of 0.241-0.274

## Executive Summary
This paper establishes reproducible baselines for the BRIGHT benchmark, which evaluates retrieval systems on reasoning-intensive queries. The authors implement three first-stage retrievers (BM25, BGE-large-en-v1.5, SPLADE-v3) with query-side BM25 for long queries, pairwise Reciprocal Rank Fusion (RRF) for combination, and LLM-based listwise reranking using RankLLM. Their findings show that query-side BM25 outperforms bag-of-words for medium-length queries (16-256 tokens), RRF consistently improves retrieval effectiveness, and LLM reranking provides substantial gains (nDCG@10 increases from 0.162-0.172 to 0.241-0.274).

## Method Summary
The authors implement reproducible baselines using Anserini/Pyserini for first-stage retrieval and RankLLM for reranking. They apply query-side BM25 (applying BM25 saturation to query tokens rather than raw frequencies) for long reasoning queries, pair BGE-large-en-v1.5 (dense) and SPLADE-v3 (learned sparse) retrievers, and fuse results using pairwise RRF. Top-100 candidates are reranked with LLM-based models (Qwen3-8b, gpt-oss-20b) using 16k context windows. The system is evaluated on BRIGHT's 12 tasks across StackExchange, Coding, and Theorem categories using nDCG@10 and Recall@100.

## Key Results
- Query-side BM25 (BM25QueryGenerator) outperforms bag-of-words for queries of 16-256 tokens, improving nDCG@10 by up to 0.011
- Pairwise RRF fusion consistently improves retrieval effectiveness across all tasks compared to individual retrievers
- LLM-based reranking raises nDCG@10 from 0.162-0.172 to 0.241-0.274, with gpt-oss-20b achieving the highest scores
- SPLADE-v3 performs best on Coding tasks, BGE on Theorem tasks, and BM25 provides general coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Query-side BM25 improves retrieval for long reasoning queries by applying BM25 saturation and length normalization to query tokens rather than using raw frequency counts.
- **Mechanism:** Standard bag-of-words weights query terms by raw frequency, causing repeated terms in long prompts to dominate the score. Query-side BM25 applies the BM25 saturation function and length normalization to the query vector itself, down-weighting repeated terms that would otherwise skew similarity scores.
- **Core assumption:** Long queries contain redundant or low-discriminatory terms that require dampening, similar to how document length normalization handles verbose documents.
- **Evidence anchors:** Abstract states query-side BM25 "applies BM25 on the query rather than using the standard bag-of-words approach." Section 4.1 shows increasing frequency and magnitude differences as query length grows, recommending BM25 weighting for 16-256 token queries.
- **Break condition:** For short queries (<16 tokens), computational overhead may not justify marginal or negative performance difference.

### Mechanism 2
- **Claim:** Pairwise RRF fusion of lexical and semantic retrievers creates more robust first-stage candidates than any single retriever.
- **Mechanism:** BM25, BGE, and SPLADE exhibit uncorrelated error modes. BM25 fails on vocabulary mismatch; dense models may hallucinate semantic connections. RRF aggregates ranked lists based on rank positions, allowing high-ranking documents from one system to rescue misses from another.
- **Core assumption:** Retrievers possess complementary strengths such that relevant documents are likely captured by at least one modality.
- **Evidence anchors:** Section 4.2 states "Fusion consistently improves effectiveness, as all scores in the RRF section exceed their counterparts in the individual section." Table 3 shows RRF combinations consistently outperforming individual columns.
- **Break condition:** If retrievers share systematic bias, fusion amplifies rather than corrects errors.

### Mechanism 3
- **Claim:** LLM-based listwise reranking resolves complex relevance judgments that vector similarity misses.
- **Mechanism:** The reranker ingests long query and top-100 candidates, using reasoning capacity to compare documents against nuanced prompt constraints rather than relying solely on keyword overlap or embedding proximity.
- **Core assumption:** LLM's reasoning capability generalizes to target domain without fine-tuning, and 16k context window prevents truncation of critical query instructions.
- **Evidence anchors:** Section 3.2 extends context to 16k tokens, with reranking raising nDCG@10 from 0.162-0.172 to 0.241-0.274. Reranking runtime averages 2 hours 20 minutes for gpt-oss-20b.
- **Break condition:** If first-stage retrieval fails to surface relevant document in top-100, reranker cannot recover it.

## Foundational Learning

- **Concept:** **BM25 Saturation & Length Normalization**
  - **Why needed here:** Understanding why raw term frequency fails in long queries is critical to grasping query-side BM25.
  - **Quick check question:** Why does a term appearing 5 times in a 10-word query merit different handling than a term appearing 5 times in a 500-word document?

- **Concept:** **Sparse vs. Dense Representations**
  - **Why needed here:** The paper fuses BM25 (lexical/sparse), SPLADE (learned sparse), and BGE (dense). Knowing the distinction explains why fusion works.
  - **Quick check question:** If a user searches for "bank" (finance) but the document contains "bank" (river), which retrieval type (dense or lexical) is more likely to make the mistake, and which might correct it?

- **Concept:** **Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** This is the merging strategy used to combine three retrieval pipelines.
  - **Quick check question:** Does RRF require scores of different retrieval systems to be normalized/scaled before merging? (Check: No, it uses ranks).

## Architecture Onboarding

- **Component map:** Anserini/Pyserini (indexing + first-stage retrieval) -> Pairwise RRF (fusion) -> RankLLM (reranking) -> Evaluation

- **Critical path:**
  1. **Corpus Prep:** Clean BRIGHT data to remove duplicates and zero-token chunks (Table 5)
  2. **First-Stage Retrieval:** Run query-side BM25, SPLADE-v3, and BGE-large-en-v1.5 in parallel
  3. **Fusion:** Apply pairwise RRF to generate top-100 candidate list
  4. **Reranking:** Feed top-100 list + query to RankLLM (context window >= 16k)

- **Design tradeoffs:**
  - **Quantized vs. Accurate Norms:** Lucene's default (quantized) is faster but slightly less precise; accurate norms match original BRIGHT paper but differ marginally from Anserini defaults (~0.002 nDCG)
  - **Query-side vs. BoW BM25:** Query-side better for long queries but adds computation to query encoding step
  - **Retriever Choice:** SPLADE-v3 wins in Coding tasks; BGE wins in Theorem tasks; BM25 provides general coverage

- **Failure signatures:**
  - **Low nDCG in Theorem tasks:** Identified as hardest category (avg < 0.18 even after reranking)
  - **Context Overflow:** RankLLM default (4k tokens) fails on BRIGHT queries; must be extended to 16k
  - **Missing Gold IDs:** Evaluation reliability degrades if duplicate documents are removed but relevance labels (qrels) are not adjusted

- **First 3 experiments:**
  1. **Reproduce BM25 Delta:** Run BoW vs. query-side BM25 on subset to quantify "Query Length vs. nDCG" curve (Fig 1)
  2. **Ablate Fusion:** Compare BM25-only, BGE-only, and RRF results to verify error complementarity on target domain
  3. **Sanity Check Reranker:** Run RankLLM on top-100 fused results with 16k context window to verify reported lift (nDCG improvement)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does query-side BM25 provide consistent improvements over bag-of-words on traditional IR benchmarks (e.g., BEIR) with shorter queries, or is the benefit specific to reasoning-intensive, long-query datasets like BRIGHT? The paper states future work should examine behavior on traditional benchmarks like BEIR to validate robustness and explore correlation with query length or structure.

- **Open Question 2:** What is the precise functional relationship between query length (or structure) and magnitude of improvement from query-side BM25 over bag-of-words? Figure 1 shows non-monotonic behavior where gains peak at medium query lengths then taper off, but the underlying mechanism and optimal threshold remain unclear.

- **Open Question 3:** To what extent do duplicate documents and missing gold IDs in BRIGHT corpora underestimate effectiveness of stronger retrieval and reranking systems? Section 4.3 finds adjusted qrels change nDCG@10 by up to 0.010, and authors "expect even larger discrepancies for stronger retrievers and rerankers."

## Limitations

- **Model Access:** gpt-oss-20b model is referenced but not clearly identified in open-source ecosystem, limiting reproducibility
- **Parameter Specification:** RRF k parameter (standard value 60) not explicitly stated, altering fusion effectiveness if different value used
- **Tokenizer Discrepancies:** Anserini/Pyserini use different tokenization than original BRIGHT implementation (GPT-2), causing minor score deviations (~0.002 nDCG@10)
- **Evaluation Data Quality:** Original BRIGHT corpus contains duplicate documents and missing gold IDs, though authors provide adjusted qrels

## Confidence

- **High Confidence:** Query-side BM25 improves nDCG@10 for medium-length queries (16-256 tokens) vs. bag-of-words. Directly supported by quantitative comparisons in Section 4.1.
- **High Confidence:** Pairwise RRF fusion consistently improves retrieval effectiveness across tasks. Table 3 and Section 4.2 provide clear evidence.
- **High Confidence:** LLM-based reranking with RankLLM provides substantial gains (nDCG@10 +0.079 to +0.112). 16k context extension is critical and well-documented.
- **Medium Confidence:** Relative ranking of retrievers (SPLADE-v3 > BGE > BM25) varies by task domain. While stated, effect sizes and domain-specificity could benefit from additional ablation studies.

## Next Checks

1. **Validate Query-side BM25 Implementation:** Run controlled experiment comparing BoW vs. query-side BM25 on BRIGHT subset, plotting nDCG@10 vs. query length to confirm inflection point around 16-256 tokens.

2. **Test Fusion Robustness:** Perform ablation studies on RRF by removing one retriever at a time (BM25-only, BGE-only, SPLADE-only) to quantify error-correction benefit across all 12 BRIGHT tasks.

3. **Verify Reranker Context Extension:** Confirm extending RankLLM's context window to 16k is necessary by comparing nDCG@10 with default 4k setting on long-query subset, ensuring no truncation occurs.