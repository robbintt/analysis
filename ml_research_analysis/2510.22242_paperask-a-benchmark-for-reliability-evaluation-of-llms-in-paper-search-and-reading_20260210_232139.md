---
ver: rpa2
title: 'PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and
  Reading'
arxiv_id: '2510.22242'
source_url: https://arxiv.org/abs/2510.22242
tags:
- llms
- search
- papers
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PaperAsk, a benchmark for evaluating the\
  \ reliability of large language models (LLMs) in scholarly tasks. The authors systematically\
  \ assess LLMs on four key research tasks\u2014citation retrieval, content extraction,\
  \ paper discovery, and claim verification\u2014through controlled experiments using\
  \ web interfaces."
---

# PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading
## Quick Facts
- arXiv ID: 2510.22242
- Source URL: https://arxiv.org/abs/2510.22242
- Authors: Yutao Wu; Xiao Liu; Yunhao Feng; Jiale Ding; Xingjun Ma
- Reference count: 35
- Key outcome: Introduces PaperAsk benchmark revealing LLMs fail 48-98% on multi-reference citation retrieval, 72-91% on content extraction, and achieve F1<0.32 on paper discovery.

## Executive Summary
PaperAsk introduces a comprehensive benchmark for evaluating large language models (LLMs) in scholarly tasks, focusing on reliability across four key research activities: citation retrieval, content extraction, paper discovery, and claim verification. Through controlled experiments using web interfaces, the study reveals significant reliability failures across tasks, with citation retrieval failing in 48-98% of multi-reference queries, content extraction failing in 72-91% of cases, and paper discovery achieving F1 scores below 0.32. Human analysis attributes these failures to uncontrolled context expansion and models prioritizing semantically relevant text over task instructions. The authors develop lightweight reliability classifiers to identify unreliable outputs with 96% accuracy, providing a practical solution for real-world deployment.

## Method Summary
The benchmark evaluates LLMs on four research tasks: citation retrieval (extracting references from provided papers), content extraction (finding specific information in papers), paper discovery (finding relevant papers given a topic), and claim verification (verifying claims against paper content). Experiments use web interfaces (ChatGPT, Gemini) and API comparisons, with human analysis of failure modes and development of reliability classifiers trained on annotated outputs.

## Key Results
- Citation retrieval fails in 48-98% of multi-reference queries due to context pollution from web search expansion
- Content extraction fails in 72-91% of cases, with models extracting from wrong sections or sources
- Paper discovery achieves F1 scores below 0.32, indicating poor precision-recall balance
- API-based retrieval with full text shows 1-3% failure vs 12-18% for web interfaces, revealing deployment architecture bottlenecks
- Reliability classifiers achieve 96% accuracy in identifying unreliable outputs across tasks

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Uncontrolled context expansion causes retrieval-to-generation pipeline failures in web-based LLM interfaces.
- Mechanism: When users specify paper URLs or titles, the integrated web search retrieves multiple additional sources beyond requested ones, polluting context with conflicting information that overwhelms the model's ability to isolate target content.
- Core assumption: Retrieval breadth is prioritized over precision in deployed web interfaces to maintain responsiveness.
- Evidence anchors:
  - [abstract]: "Human analysis attributes these failures to the uncontrolled expansion of retrieved context"
  - [section]: "Web search expands beyond provided URLs, often retrieving multiple times the requested number of sources. This excessive retrieval creates context pollution" (Section 4.7, Obs.❻)
  - [corpus]: Weak corpus support—neighbor papers focus on citation classification rather than retrieval architecture failures

### Mechanism 2
- Claim: Models prioritize semantic similarity over structural task instructions, extracting content from wrong sources/sections.
- Mechanism: LLMs cannot reliably distinguish task instructions from retrieved content; they assemble answers by cross-referencing multiple sources based on semantic relevance, returning text that "looks right" rather than adhering to structural constraints (e.g., returning abstract instead of introduction).
- Core assumption: RLHF optimization favors response completeness over strict constraint adherence.
- Evidence anchors:
  - [abstract]: "Models prioritizing semantically relevant text over task instructions"
  - [section]: "LLMs cannot reliably distinguish task instructions from retrieved content, treating any semantically relevant text as valid information regardless of source" (Section 4.5)
  - [corpus]: HySemRAG (neighbor) addresses multi-layered RAG retrieval but doesn't examine this specific semantic-priority failure

### Mechanism 3
- Claim: Deployment architecture (not model capability) is the primary bottleneck—API with full text achieves 1-3% failure vs. 12-18% for web interfaces.
- Mechanism: Web interfaces use shallow retrieval (snippets) with limited context access to optimize latency; models follow sequential reading patterns (abstract first, full content only if needed) that cause premature conclusion when key information appears later in documents.
- Core assumption: Commercial deployments trade accuracy for speed through shallow retrieval budgets.
- Evidence anchors:
  - [abstract]: Not directly stated
  - [section]: "Web-based (black-box) vs API (%)... GPT-5: 12% web vs 1% API... a 9-17 percentage point improvement" (Table 8); "Constrained retrieval pressures LLMs toward premature conclusions using approximate answers to save computational overhead" (Section 4.7)
  - [corpus]: No direct corpus evidence on deployment architecture constraints

## Foundational Learning
- Concept: **Retrieval-Augmented Generation (RAG) with black-box search**
  - Why needed here: PaperAsk evaluates LLMs where retrieval is opaque—the model calls integrated web search but users cannot observe what sources were retrieved or how they were ranked.
  - Quick check question: Can you explain why API-based retrieval (where you control the fetch function) differs fundamentally from black-box web search in terms of debugging and reliability?

- Concept: **Semantic similarity vs. structural fidelity**
  - Why needed here: The core failure mode is models returning semantically plausible text (e.g., abstracts when asked for introductions) rather than structurally correct extractions.
  - Quick check question: If a model returns text that "answers the question" but from the wrong document section, is this a retrieval failure or a reasoning failure?

- Concept: **RLHF optimization and completion pressure**
  - Why needed here: The paper attributes fabrication (Gemini) and over-conservative refusal (GPT-5) to the same root cause—models optimizing for perceived helpfulness/completeness over constraint adherence.
  - Quick check question: Why might a model trained to "be helpful" produce fabricated citations rather than admitting inability?

## Architecture Onboarding
- Component map: User Query → Web Interface → [Black-box Search + Scraping] → Retrieved Context (expanded beyond request) → LLM Reasoning → Response
- Critical path: The failure originates in the search expansion step (retrieves 3-5× more sources than requested) → context pollution → model either refuses (GPT-5) or fabricates (Gemini) to maintain response completeness.
- Design tradeoffs:
  - Responsiveness vs. accuracy: Shallow snippet retrieval is faster but provides incomplete context; full-text retrieval is accurate but 3-5× slower.
  - Completeness vs. constraint adherence: Models optimized to "complete the task" will fabricate rather than return partial results.
  - Black-box convenience vs. observability: Web interfaces hide retrieval mechanics; API/agent frameworks expose tool calls for debugging.
- Failure signatures:
  - Citation retrieval (n=10): GPT-5 returns incomplete (66% refusal), Gemini returns fabricated arXiv IDs (35% point to wrong papers).
  - Content extraction: Models return abstract text when asked for introduction sections (72-91% failure).
  - Claim verification with URLs: Web interface fails 12-18%, API with full text fails 1-3%—the gap is diagnostic of deployment constraints.
- First 3 experiments:
  1. Single-paper baseline: Test citation retrieval with n=1 vs. n=10 to quantify failure rate delta (expected: 2-9% → 48-98% per paper).
  2. Search-disabled reasoning: For ChatGPT, disable built-in search and compare accuracy/latency (expected: 3-5× slower, higher accuracy).
  3. Reliability classifier validation: Train Llama-3.1-8B-Instruct on PaperAsk annotations (80/20 split), verify 96% accuracy on held-out test set before deployment as post-processing filter.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across domains: Failure rates may vary significantly across disciplines with different citation densities, writing styles, and accessibility patterns.
- Deployment configuration unknowns: Observed failure patterns could be influenced by undocumented configuration choices in commercial web interfaces.
- Human annotation subjectivity: 96% accuracy claim depends on potentially variable human judgments of "reliability" across annotators and cultures.

## Confidence
- High confidence: The deployment architecture bottleneck (API vs. web interface performance gap) and core failure mechanisms of context expansion and semantic prioritization are well-supported by controlled experiments.
- Medium confidence: The reliability classifier's 96% accuracy is robust for evaluated models but may not generalize to models with different failure patterns.
- Low confidence: Exact quantitative failure rates (48-98% for citation retrieval) may be specific to the particular query distribution and paper selection in the benchmark.

## Next Checks
1. Cross-domain replication: Run PaperAsk tasks across humanities, social sciences, and interdisciplinary domains to verify whether failure rate distributions remain consistent or reveal domain-specific vulnerabilities.
2. Architecture ablation study: Systematically disable components of the web interface pipeline (search expansion, snippet retrieval, sequential reading) to isolate which architectural choices contribute most to reliability failures.
3. Temporal stability analysis: Re-run the benchmark quarterly over a year to measure how reliability rates evolve as models and interfaces update, distinguishing between transient and persistent failure modes.