---
ver: rpa2
title: 'Code Review Comprehension: Reviewing Strategies Seen Through Code Comprehension
  Theories'
arxiv_id: '2503.21455'
source_url: https://arxiv.org/abs/2503.21455
tags:
- code
- review
- reviewers
- comprehension
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated how experienced developers comprehend and
  review code changes, focusing on the cognitive processes underlying code review.
  Ten expert reviewers were observed while performing 25 real-world code reviews,
  with data collected through think-aloud protocols and interviews.
---

# Code Review Comprehension: Reviewing Strategies Seen Through Code Comprehension Theories

## Quick Facts
- arXiv ID: 2503.21455
- Source URL: https://arxiv.org/abs/2503.21455
- Reference count: 40
- Primary result: Expert reviewers use opportunistic strategies based on change complexity, building three-layered mental models to evaluate code

## Executive Summary
This study investigates how experienced developers comprehend and review code changes through qualitative observation of ten expert reviewers performing 25 real-world code reviews. The research extends Letovsky's code comprehension model to create the Code Review Comprehension Model, revealing that code review relies on opportunistic strategies rather than fixed procedures. Reviewers typically begin with context-building, followed by code inspection involving reading, testing, and discussion management, ultimately constructing mental models by comparing expectations and ideals against actual implementations.

The findings demonstrate that experienced reviewers scope their reviews based on complexity, using chunking and difficulty-based reading strategies for complex changes while relying heavily on their knowledge base and mental models to evaluate code effectively. The study provides empirical evidence that code review is fundamentally a comprehension process, with reviewers opportunistically selecting strategies based on change size and complexity rather than following rigid review protocols.

## Method Summary
The study employed qualitative research methods including think-aloud protocols and semi-structured interviews with ten expert reviewers (recommended by peers or with 10+ years experience) performing 25 real-world code reviews. Data collection involved video recording review sessions and follow-up interviews to clarify decision-making sequences. Thematic analysis using NVivo 14 was conducted based on a coding schema extending Letovsky's code comprehension model, incorporating Piaget's cognitive development and Self-discrepancy theory. The research achieved "code saturation" to validate the Code Review Comprehension Model.

## Key Results
- Experienced reviewers scope their reviews based on complexity, using chunking and difficulty-based reading strategies for complex changes
- Reviewers construct three-layered mental models (specification, implementation, annotation) and compare them against expected and ideal alternatives
- Code review relies on opportunistic strategies rather than fixed procedures, with strategy selection driven by change size and cognitive load management

## Why This Works (Mechanism)

### Mechanism 1
Reviewers opportunistically select strategies based on change complexity, not fixed procedures. For small changes (≤6 files, ~176 LOC), reviewers use linear reading. For larger changes, they switch to difficulty-based reading (easy-first or core-first) and chunking (by commit, file, test, or functional area) to manage cognitive load. This works because cognitive capacity is limited per Piaget's model, making scoping necessary for complex changes.

Evidence: [abstract] "experienced reviewers scope their reviews based on complexity, use chunking and difficulty-based reading strategies for complex changes"; [section IV-B] Figure 4(b) shows linear reading for small reviews, with chunking/difficulty-based strategies emerging for larger changes. Break condition: If reviewers lack prior codebase familiarity or time pressure forces premature termination, strategy selection degrades to shallow/superficial reviews.

### Mechanism 2
Reviewers construct three-layered mental models (specification, implementation, annotation) and compare them against expected and ideal alternatives. Context-building populates the specification layer (goals, acceptance criteria). Code inspection fills the implementation layer (changed code, behavior). The annotation layer maps goals to implementation. Reviewers evaluate by comparing actual implementation against pre-formed expected models and ideal solutions.

Evidence: [abstract] "reviewers construct a mental model of the change as an extension of their understanding of the overall software system and contrast mental representations of expected and ideal solutions against the actual implementation"; [section IV-C3] "Reviewers need to understand the review goal... As they construct the specifications of the mental model of the PR, they also set explicit expectations." Break condition: If PR documentation is missing or poor (no title, description, or linked issues), the specification layer remains incomplete, forcing bottom-up reconstruction.

### Mechanism 3
Knowledge base accelerates review by supplying pre-existing mental models of the system and programming patterns. Reviewers with codebase familiarity and team knowledge can skip extensive context-building, recognize standard patterns quickly, and provide targeted feedback. Pre-alignment with authors further reduces comprehension burden.

Evidence: [abstract] "experienced reviewers...rely heavily on their knowledge base and mental models to evaluate code effectively"; [section IV-C2] "The more familiar a reviewer is with the overall context of the PR, the fewer resources they need to understand the PR." Break condition: If the reviewer faces unfamiliar code domains or architectural patterns outside their expertise, efficiency gains disappear regardless of general experience level.

## Foundational Learning

- **Letovsky's Code Comprehension Model**: Why needed: The CRCM extends this model; understanding the original (knowledge base + information sources → mental model) is prerequisite to grasping the extension. Quick check: Can you name the three components of Letovsky's model and how they interact?

- **Mental Model Layers (Specification, Implementation, Annotation)**: Why needed: Reviewers build three-layered models; tool design should support each layer distinctly. Quick check: What type of information would populate the specification layer versus the annotation layer?

- **Opportunistic Strategy Selection**: Why needed: Reviews are not linear; understanding when and why reviewers switch strategies (complexity thresholds) is essential for process/tool design. Quick check: At approximately what change size did reviewers in the study switch from linear reading to chunking/difficulty-based strategies?

## Architecture Onboarding

- **Component map**: Context-Building Phase → Gathers PR metadata, issues, discussions; populates specification layer → Code Inspection Phase → Combines code reading (linear/difficulty-based/chunking), testing, discussion management → Decision Phase → Synthesizes understanding into verdict (accept/comment/request changes) → Knowledge Base → Long-term memory: codebase patterns, domain knowledge, team practices → Mental Model → Working memory: current PR understanding across three layers

- **Critical path**: Context-building → Strategy selection (size/complexity assessment) → Code inspection with chosen strategy → Model comparison (expected/ideal vs. actual) → Decision

- **Design tradeoffs**:
  - Chunking vs. Overview: Breaking PR into units reduces cognitive load but risks losing global coherence (reviewers compensate with final linear pass)
  - Expected vs. Ideal comparison: Expected models accelerate review; ideal models improve feedback quality but require more cognitive effort
  - Scope depth vs. coverage: Focused reviews may miss integration issues; full reviews may exhaust reviewer capacity

- **Failure signatures**:
  - Shallow reviews triggered by: missing PR documentation, excessive size without chunking support, reviewer fatigue
  - Incomplete annotation layer: when traceability from goals to implementation is broken (poor naming, missing links)
  - Strategy collapse: reviewers revert to linear reading on complex changes when tools don't support chunking

- **First 3 experiments**:
  1. Chunking support test: Add UI features that visualize change distribution (core changes vs. peripheral) and allow commit/functional-area-based review ordering. Measure whether reviewers use non-linear strategies more frequently.
  2. Context quality audit: Instrument PRs to track what information sources reviewers access (description, issues, linked docs). Correlate with review outcome quality and time.
  3. Mental model probe: After reviews, ask reviewers to describe the PR's specification (goals) and annotation (goal-to-code mapping). Assess completeness against ground truth to identify layer-specific gaps.

## Open Questions the Paper Calls Out

- **How can code review tools be designed to visually identify "meaningful code chunks" or indicate "change distribution" to support the difficulty-based and chunking strategies observed?**
  - Basis: Section V.B states the finding is an "opportunity to investigate how tools can provide support to identify and visually separate meaningful code chunks, indicate the change distribution... and support Git hygiene."
  - Why unresolved: Current tools primarily offer linear file ordering, forcing reviewers to manually segment complex changes to manage cognitive load.
  - Evidence: A user study measuring defect detection rate and cognitive load using a prototype tool that highlights code chunks versus a standard linear diff tool.

- **To what extent can Large Language Models (LLMs) support reviewers in forming mental models, specifically by generating missing context or annotating implementation details?**
  - Basis: Section V.B suggests future research "investigate whether and how AI and LLMs have the potential to support many of the aforementioned improvements, e.g., by... generating missing context and documentation."
  - Why unresolved: The study observed reviewers manually gathering context; the efficacy of AI in automating this specific cognitive step remains unverified.
  - Evidence: An experiment where reviewers use an LLM-assisted tool to build mental models, measuring review speed and the accuracy of the mental model formed.

- **How do the opportunistic strategies (e.g., chunking, difficulty-based reading) observed in expert reviewers differ from those used by novice reviewers?**
  - Basis: Section III.E states, "The study participants are experienced reviewers... Therefore, this data does not represent the strategies of novice reviewers."
  - Why unresolved: The study sample was restricted to "experienced reviewers" with over ten years of experience, leaving the behavior of novices unknown.
  - Evidence: A comparative observational study using think-aloud protocols with novice developers to analyze their scoping and reading strategies using the Code Review Comprehension Model.

## Limitations
- The study sample (10 experts, 25 reviews) may not capture novice reviewer behaviors or non-OSS contexts
- Think-aloud protocols may alter natural review behavior despite attempts to minimize Hawthorne effects
- Code review practices evolve rapidly; findings may not hold as tools and workflows change

## Confidence
- **High confidence**: Core finding that experienced reviewers use opportunistic strategies based on change complexity
- **Medium confidence**: The three-layered mental model framework extends beyond the specific codebase studied
- **Low confidence**: The universality of difficulty-based reading strategies across different programming domains

## Next Checks
1. Cross-domain replication: Test the CRCM framework with reviewers from different programming ecosystems (e.g., web development, embedded systems)
2. Novice reviewer comparison: Contrast strategy selection patterns between expert and novice reviewers to identify learning curves
3. Tool impact study: Measure how modern code review tools (AI assistants, automated testing integration) affect the opportunistic strategy selection described in the model