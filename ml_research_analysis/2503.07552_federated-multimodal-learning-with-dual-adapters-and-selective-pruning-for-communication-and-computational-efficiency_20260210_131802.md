---
ver: rpa2
title: Federated Multimodal Learning with Dual Adapters and Selective Pruning for
  Communication and Computational Efficiency
arxiv_id: '2503.07552'
source_url: https://arxiv.org/abs/2503.07552
tags:
- local
- lora
- adapter
- global
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEDDLP, a federated learning framework that
  combines dual adapters with selective pruning to improve efficiency and personalization
  in multimodal learning. The method uses a larger local adapter for client-specific
  personalization and a smaller global adapter for knowledge sharing, along with a
  pruning mechanism to reduce communication overhead.
---

# Federated Multimodal Learning with Dual Adapters and Selective Pruning for Communication and Computational Efficiency

## Quick Facts
- **arXiv ID:** 2503.07552
- **Source URL:** https://arxiv.org/abs/2503.07552
- **Reference count:** 40
- **Primary result:** Introduces FEDDLP, a federated learning framework combining dual adapters with selective pruning for efficient multimodal learning on heterogeneous data

## Executive Summary
This paper introduces FEDDLP, a federated learning framework that combines dual adapters with selective pruning to improve efficiency and personalization in multimodal learning. The method uses a larger local adapter for client-specific personalization and a smaller global adapter for knowledge sharing, along with a pruning mechanism to reduce communication overhead. FEDDLP is evaluated on vision and language tasks using the CLIP model across heterogeneous client data distributions. Results show that FEDDLP outperforms existing approaches, achieving higher test accuracy, lower performance variance among clients, and improved worst-case performance while significantly reducing communication and computation costs.

## Method Summary
FEDDLP addresses federated learning with heterogeneous data by introducing a dual-adapter architecture. Each client maintains a large local LoRA adapter (rank 4) for personalization and a small global LoRA adapter (rank 2) for knowledge sharing. The framework incorporates SoRA-style selective pruning on the local adapter to reduce communication overhead and prevent overfitting. During training, local and global adapters are updated alternately with bi-directional knowledge distillation, and only the global adapter parameters are aggregated at the server. The method is evaluated on vision-language tasks using CLIP across multiple datasets with Dirichlet-sampled non-IID partitions.

## Key Results
- Achieved up to 97.51% accuracy on the Flowers dataset with reduced communication overhead
- Reduced communication overhead by up to 84% compared to FedDAT while maintaining higher accuracy
- Demonstrated lower performance variance among clients and improved worst-case performance compared to baselines
- Outperformed existing approaches on both vision and language tasks across heterogeneous client data distributions

## Why This Works (Mechanism)

### Mechanism 1: Structural Decoupling of Personalization and Generalization
- **Claim:** Separating the learning objective into two distinct adapters—a large local adapter for personalization and a small global adapter for generalization—mitigates the conflict between fitting heterogeneous local data and maintaining a consistent global representation.
- **Mechanism:** The framework assigns a "Local LoRA" (rank 4) that remains private to the client, capturing fine-grained, client-specific features. Simultaneously, a "Global LoRA" (rank 2) is trained to be shared. By keeping these structurally separate rather than combining them (as in FedDAT), the system prevents the averaging of personalized parameters during server aggregation, which the authors identify as a source of performance degradation under high heterogeneity.
- **Core assumption:** Client-specific knowledge is high-capacity and specific, whereas generalizable knowledge is low-capacity and consistent across the network.
- **Evidence anchors:**
  - [abstract]: "method utilizes a larger local adapter for client-specific personalization and a smaller global adapter to facilitate efficient knowledge sharing"
  - [section VIII]: Ablation results show the separate FEDDLP approach achieves 97.51% accuracy on Flowers (Image Encoder, $\beta=0.01$) vs. 93.80% for the "Combined" approach.
  - [corpus]: Neighbor papers (e.g., FedUMM, pFedMMA) similarly explore decoupling or adapter specialization in FL, suggesting this is a recognized strategy for handling multimodal or heterogeneous data, though specific dual-adapter implementations vary.
- **Break condition:** If data distributions across clients are largely Identically and Independently Distributed (IID), the overhead of maintaining two separate adapters may outweigh the marginal gains over a single global adapter.

### Mechanism 2: Sparsity-Induced Regularization via Selective Pruning
- **Claim:** Applying structured pruning to the local adapter (using SoRA) acts as a regularizer that prevents overfitting to local noise while reducing computational load.
- **Mechanism:** Instead of training a dense Local LoRA, the method applies a gating mechanism with a threshold $\xi$. It zeroes out singular values in the adapter matrices that fall below a learned importance score. The paper notes that vanilla LoRA tends to overfit and degrade in later rounds on non-IID data, whereas the pruned SoRA maintains higher accuracy, suggesting the sparsity mechanism forces the model to focus only on robust, critical features.
- **Core assumption:** Parameters with low importance scores (magnitude/gradients) contribute primarily to noise or overfitting in a personalized setting.
- **Evidence anchors:**
  - [section III]: Figure 2 demonstrates that vanilla LoRA suffers performance degradation after ~40 rounds, while SoRA (pruned) maintains higher Top-1 accuracy.
  - [abstract]: "incorporate a pruning mechanism to reduce communication overhead by selectively removing less impactful parameters"
  - [corpus]: Evidence regarding pruning as a regularizer in FL is limited in the immediate neighbors, though "FLoRIST" mentions efficient fine-tuning via Singular Value Thresholding, which aligns with the efficiency goal but not explicitly the regularization claim.
- **Break condition:** If the pruning threshold $\xi$ is too aggressive, the local adapter may lose the capacity to capture essential rare features unique to a specific client (catastrophic forgetting of minority classes).

### Mechanism 3: Asymmetric Bi-Directional Knowledge Distillation
- **Claim:** Transferring knowledge between the global and local adapters via KL divergence stabilizes training by anchoring the personalized model to the global consensus while injecting local insights into the global model.
- **Mechanism:** The local adapter is trained with Cross-Entropy (CE) loss plus KD loss from the Global adapter (weighted by $\alpha$). The Global adapter is trained with CE loss plus KD loss from the Local adapter. Crucially, the authors freeze the Global adapter when updating the Local one (and vice versa) to prevent mode collapse. This forces the Global adapter to learn a "compressed" representation of local nuances suitable for aggregation.
- **Core assumption:** The logits of a specialized local model contain extractable "generalizable" signal that can be distilled into a smaller, shared adapter.
- **Evidence anchors:**
  - [section V.C]: Describes the bi-directional KD where the local adapter minimizes $D_{KL}(\text{logits}_{local} || \text{logits}_{global})$ and vice versa.
  - [section V.H]: States that unlike FedDAT, FEDDLP separates these to ensure "personalized local training."
  - [corpus]: "FedDAT" is referenced as the origin of the dual-adapter distillation concept; other neighbor papers do not explicitly challenge this bi-directional flow.
- **Break condition:** If the capacity gap between the Local (Rank 4) and Global (Rank 2) adapters is too large, the distillation may fail as the teacher model (e.g., Local) is too complex for the student (Global) to mimic effectively.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** LoRA allows the large CLIP model to be fine-tuned by updating only tiny adapter matrices ($A$ and $B$). Without this, transmitting full model gradients in Federated Learning would be computationally prohibitive.
  - **Quick check question:** Can you explain why we freeze the main model weights ($W$) and only update the low-rank decomposition $BA$?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** This is the baseline aggregation algorithm. The paper modifies this by only aggregating the "Global LoRA" weights, ignoring the pruned local weights. Understanding standard FedAvg is necessary to see how the dual-adapter approach deviates to solve the "non-IID" problem.
  - **Quick check question:** How does the server aggregate updates in standard FedAvg, and which specific parameters does FEDDLP choose *not* to aggregate?

- **Concept: The Dirichlet Distribution ($\alpha / \beta$) for Non-IID Data**
  - **Why needed here:** The paper evaluates performance using Dirichlet distributions ($\beta=0.1, 0.01$) to simulate data heterogeneity. You must understand that a lower $\beta$ indicates higher skew (more non-IID) to interpret the results in Tables I and II correctly.
  - **Quick check question:** In the context of this paper, does a $\beta$ of 0.01 imply that client data distributions are similar or highly skewed?

## Architecture Onboarding

- **Component map:**
  - Frozen Backbone: CLIP ViT-B/32 (Image Encoder) or Text Encoder
  - Trainable Module A (Local): LoRA adapter (Rank 4) + SoRA Gate (Pruning mechanism)
  - Trainable Module B (Global): LoRA adapter (Rank 2)
  - Loss Functions: Cross-Entropy (Task performance) + KL Divergence (Distillation between A and B)

- **Critical path:**
  1. Initialization: Clients download Global LoRA from Server
  2. Local Training: Alternating updates. Update Local LoRA (Global frozen) using $L_{local}$. Update Global LoRA (Local frozen) using $L_{global}$. Apply pruning mask to Local LoRA
  3. Communication: Upload *only* the Global LoRA parameters (Rank 2)
  4. Aggregation: Server averages all received Global LoRAs

- **Design tradeoffs:**
  - Accuracy vs. Communication: The method transmits very little data (Rank 2 adapter) but requires local compute for two adapters
  - Hyperparameter $\alpha$: Controls how much the local adapter listens to global knowledge. If $\alpha$ is too high, personalization drops (it behaves like a global model); if too low, it overfits locally and loses generalization

- **Failure signatures:**
  - Diverging Loss: If Local and Global adapters are trained simultaneously without freezing the counterpart, the loss may oscillate or diverge (Mode Collapse)
  - Stagnant Accuracy: If the pruning threshold $\xi$ is too high, the model may underfit, resulting in accuracy comparable to the "Local Only" baseline

- **First 3 experiments:**
  1. Pruning Sensitivity: Replicate Figure 2 (SoRA vs. LoRA) on a single client to validate that the pruning mechanism actually prevents overfitting before introducing federation
  2. Aggregation Ablation: Run FEDDLP vs. "Combined Adapter" (Tables VII/VIII) on the Flowers dataset to verify the specific contribution of the dual-adapter architecture over a single unified adapter
  3. Communication Profile: Measure the total MB transmitted to reach target accuracy (replicating Table V) to confirm the claimed 40-80% reduction in overhead

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does FEDDLP perform in asynchronous federated settings where clients have varying connectivity and computational speeds?
- **Basis in paper:** [explicit] The conclusion states future work will explore "asynchronous settings."
- **Why unresolved:** The current framework relies on synchronous aggregation (FedAvg), assuming consistent client availability, which is often unrealistic in large-scale deployments.
- **What evidence would resolve it:** Evaluation of FEDDLP's convergence speed and accuracy stability in a simulated asynchronous environment with stragglers and delayed updates.

### Open Question 2
- **Question:** Can the framework be effectively extended to simultaneous multi-modal fine-tuning where both vision and language encoders are adapted together?
- **Basis in paper:** [explicit] The conclusion proposes extending FEDDLP to "multi-modal tasks." [inferred] Current experiments apply adapters to text or image encoders separately rather than jointly.
- **Why unresolved:** It is unclear if pruning dual adapters simultaneously on both modalities introduces interference or aggregation conflicts that degrade the personalization-generalization balance.
- **What evidence would resolve it:** Experiments demonstrating convergence and resource trade-offs when dual adapters are applied to both encoders concurrently on complex vision-language tasks.

### Open Question 3
- **Question:** Do advanced pruning techniques beyond SoRA further optimize the trade-off between local personalization and communication efficiency?
- **Basis in paper:** [explicit] The authors suggest exploring "advanced pruning techniques" to further optimize the framework.
- **Why unresolved:** The study is limited to SoRA (Sparse Low-rank Adaptation), leaving the potential benefits of other sparsity methods untested.
- **What evidence would resolve it:** A comparative analysis using alternative pruning metrics (e.g., gradient-based or magnitude-based) within the dual-adapter framework to measure accuracy retention versus communication reduction.

## Limitations
- The framework's performance under asynchronous federated settings remains unexplored
- Extension to simultaneous multi-modal fine-tuning with both vision and language encoders is not yet tested
- The study is limited to SoRA pruning, leaving potential benefits of other sparsity methods untested

## Confidence
- **Method Description:** High - Detailed architecture and training procedure provided
- **Results Validity:** Medium - Claims supported by extensive experiments but some hyperparameter details missing
- **Reproducibility:** Medium - Core implementation details specified but some hyperparameters not reported

## Next Checks
1. Replicate pruning sensitivity analysis (Figure 2) to validate SoRA prevents overfitting before federation
2. Run aggregation ablation comparing FEDDLP vs. "Combined Adapter" on Flowers dataset to verify dual-adapter contribution
3. Measure communication overhead to confirm claimed 40-80% reduction compared to FedDAT baseline