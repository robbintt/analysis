---
ver: rpa2
title: 'CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based
  Language Models'
arxiv_id: '2509.26136'
source_url: https://arxiv.org/abs/2509.26136
tags:
- generative
- performance
- zero-shot
- clinical
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CliniBench, the first benchmark for comparing
  encoder-based classifiers and generative LLMs on discharge diagnosis prediction
  from admission notes in MIMIC-IV. It evaluates 12 generative LLMs and 3 encoder-based
  classifiers, demonstrating that encoder-based classifiers consistently outperform
  generative models.
---

# CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models

## Quick Facts
- **arXiv ID:** 2509.26136
- **Source URL:** https://arxiv.org/abs/2509.26136
- **Reference count:** 40
- **Primary result:** Encoder-based classifiers consistently outperform generative LLMs in discharge diagnosis prediction from admission notes

## Executive Summary
This study introduces CliniBench, the first benchmark for comparing encoder-based classifiers and generative LLMs on discharge diagnosis prediction from admission notes in MIMIC-IV. It evaluates 12 generative LLMs and 3 encoder-based classifiers, demonstrating that encoder-based classifiers consistently outperform generative models. The study assesses retrieval augmentation and chain-of-thought prompting for in-context learning, finding that these strategies notably improve generative LLM performance. While generative models offer flexibility through zero-shot inference and dynamic label spaces, encoder-based models remain superior for diagnosis prediction accuracy.

## Method Summary
The benchmark evaluates both encoder-based classifiers (BioMedBERT, GatorTronS, M2-BERT) and generative LLMs (including Llama, Qwen2, and GPT variants) on MIMIC-IV discharge diagnosis prediction. Encoder models are fine-tuned with task-specific weights, while generative models use in-context learning with zero-shot, few-shot, and chain-of-thought prompting strategies. Retrieval augmentation is implemented using BM25 and BioMedBERT-based methods to provide demonstrations. The evaluation uses top-20 fixed-size metrics including Macro F1, MAP, and main diagnosis accuracy. A critical component is the text-to-class mapper that converts LLM-generated text descriptions back to ICD codes using NeuML-PubMedBERT embeddings.

## Key Results
- Encoder-based classifiers consistently outperform generative LLMs in diagnosis prediction accuracy
- Retrieval augmentation provides notable performance improvements for generative LLMs
- Chain-of-thought prompting degrades performance in this task
- Generative models offer flexibility through zero-shot inference and dynamic label spaces but remain less accurate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuned encoder-based classifiers outperform generative LLMs in diagnosis prediction because they optimize internal representations specifically for the static label distribution of the dataset.
- **Mechanism:** Encoders are trained via backpropagation to minimize loss on the exact set of ICD codes found in the training split, forcing them to learn specific feature-to-code correlations. Generative models rely on pre-trained knowledge and in-context learning without weight updates, limiting their ability to adapt to the specific "unbalanced data" and terminology of the target hospital system.
- **Core assumption:** The performance gap implies that general medical knowledge in LLMs is insufficient without task-specific weight adaptation or highly specific demonstrations.
- **Evidence anchors:**
  - [Abstract] "encoder-based classifiers consistently outperform generative models."
  - [Section 5.1] "Encoder-based architectures consistently outperform... with the exception of the M2 BERT model."
  - [Corpus] "Memorize and Rank" paper suggests LLMs struggle with the "inherent scarcity of patient data," supporting the need for adaptation.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) improves generative model performance by grounding predictions in similar historical cases (demonstrations), effectively bridging the knowledge gap without fine-tuning.
- **Mechanism:** The system retrieves admission notes with similar semantics or outcomes (using BM25 or fine-tuned encoders) and injects them into the LLM prompt. This allows the LLM to mimic the diagnostic patterns of the retrieved "gold" examples (ICL), significantly boosting recall and precision compared to zero-shot inference.
- **Core assumption:** The retriever successfully identifies cases with shared diagnostic pathways, and the LLM possesses the capability to infer the mapping from the retrieved note to its diagnosis.
- **Evidence anchors:**
  - [Abstract] "retrieval augmentation strategies... provide notable performance improvements for generative LLMs."
  - [Section 5.3] "providing generative LLMs with even a single example enhances their performance."
  - [Corpus] DiaLLMs and RiskAgent papers emphasize EHR grounding or data integration, aligning with the finding that external data improves clinical AI.

### Mechanism 3
- **Claim:** Chain-of-Thought (CoT) prompting degrades performance in this task because it forces models to generate reasoning steps that may distract from the direct associative mapping required for diagnosis prediction.
- **Mechanism:** CoT prompts the model to "think step-by-step." The authors suggest this mimics differential diagnosis but introduces noise. In practice, it leads to "low variance outputs" (repetitive generic diagnoses) or unrelated text generation, reducing the precision of the final ICD code mapping.
- **Core assumption:** Diagnosis prediction from admission notes is primarily an information extraction and associative classification task rather than a symbolic reasoning task.
- **Evidence anchors:**
  - [Section 5.1] "CoT induced reasoning could distract the model from the actual task."
  - [Section 5.4] "50% of CoT predictions [have low variance]... suggesting this issue exists across scenarios."
  - [Section 2.2] Cites Sprague et al. (2024) noting CoT benefits are limited to math/symbolic reasoning.

## Foundational Learning

- **Concept: Extreme Multi-Label Classification (XMLC)**
  - **Why needed here:** The task involves assigning multiple ICD codes (up to 18 avg) from a massive set (thousands) to a single note. Standard binary or multi-class metrics fail here.
  - **Quick check question:** How does the model handle the "Tail" (rare codes) vs. "Head" (frequent codes) distribution, and does it predict the correct number of codes per note?

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** The core comparison of the paper. One must understand that ICL adapts via prompts (temporary, flexible), while fine-tuning adapts via weights (permanent, rigid), explaining the trade-off between the accuracy of encoders and the flexibility of LLMs.
  - **Quick check question:** If a new disease emerges (e.g., a novel virus), which approach (Fine-tuned Encoder vs. Zero-Shot LLM) likely adapts faster without retraining?

- **Concept: Guided Decoding**
  - **Why needed here:** Generative models often hallucinate or output invalid formats. Guided decoding forces the LLM to output valid JSON, a prerequisite for the "Text-to-Class Mapping" pipeline used to evaluate LLMs.
  - **Quick check question:** How does the system ensure the LLM outputs exactly 20 diagnoses in a machine-readable format?

## Architecture Onboarding

- **Component map:** MIMIC-IV Admission Notes -> Filter to Admission Sections -> Encoder Fine-tuning OR Retriever -> LLM Prompting -> Guided Decoding -> Text-to-Class Mapper -> ICD Code Output
- **Critical path:** The **Text-to-Class Mapper** is critical for LLMs. Since LLMs generate text descriptions (e.g., "Acute sinusitis") rather than codes (e.g., "J01"), a secondary model (NeuML-PubMedBERT) maps these back to the formal label space. Failure here breaks evaluation.
- **Design tradeoffs:**
  - **Fixed vs. Dynamic Labels:** Encoders offer high accuracy but fixed labels; LLMs offer zero-shot flexibility but lower accuracy and higher inference cost (approx. 1000x more PFLOPs for Llama 70B vs BioMedBERT).
  - **Retrieval Strategy:** Term-based (BM25) is robust; Latent-outcome (BioMedBERT fine-tuned) is slightly better; Random is detrimental.
- **Failure signatures:**
  - **Low Variance Outputs:** LLMs (especially instruct models in few-shot) may generate repetitive text (e.g., "Cancer", "Lung Cancer") which collapses to few unique codes after deduplication.
  - **Unrelated Text:** Non-instruct models may output procedures or copy text rather than diagnoses.
- **First 3 experiments:**
  1. **Baseline Comparison:** Run BioMedBERT (fine-tuned) vs. Qwen2-72B-Instruct (Zero-Shot) to establish the performance gap (Encoder F1 ~25% vs LLM F1 ~13% on HOSP).
  2. **Retrieval Ablation:** Test the Qwen2 model with Random vs. BM25 vs. Gold Heuristic demonstrations to quantify the impact of retrieval quality (Gold Heuristic boosts Macro F1 significantly).
  3. **CoT Stress Test:** Compare Zero-Shot vs. Zero-Shot+CoT on a subset of notes to verify the reported degradation or stagnation in performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating multimodal clinical data (e.g., laboratory results, imaging) into retrieval systems bridge the performance gap between current latent similarity methods and the ideal "gold heuristic" for few-shot learning?
- **Basis:** [explicit] The authors state in Section 6 that "future work should augment retrieval with additional clinical information beyond admission notes, such as laboratory results or imaging data."
- **Why unresolved:** Current retrieval strategies rely solely on text-based admission notes, limiting the ability to match patients based on deeper clinical similarity.
- **What evidence would resolve it:** Experiments comparing text-only retrieval against multimodal retrieval strategies on the CliniBench dataset, specifically measuring the reduction in the performance gap to the gold heuristic upper bound.

### Open Question 2
- **Question:** Can domain-specific adaptation using clinical guidelines or differential diagnosis datasets enable Chain-of-Thought (CoT) prompting to improve, rather than degrade, diagnostic prediction accuracy?
- **Basis:** [explicit] Section 6 suggests "adapting models using clinical guidelines and datasets that capture cliniciansâ€™ differential diagnosis processes" to enable effective CoT.
- **Why unresolved:** The study found that CoT prompting often reduced performance (Section 5.1), likely because general LLMs lack domain-specific reasoning structures.
- **What evidence would resolve it:** Fine-tuning LLMs on datasets specifically designed for clinical reasoning (e.g., differential diagnosis logs) and re-evaluating the impact of CoT prompts on CliniBench metrics.

### Open Question 3
- **Question:** Do LLM-based self-verification mechanisms effectively mitigate the "low-variance" and "unrelated generated text" errors observed in generative diagnosis prediction?
- **Basis:** [explicit] Section 6 proposes adding "LLM-based verification mechanisms (Gero et al., 2023) to filter predictions and reduce duplicates" to address the specific error classes identified in the error analysis.
- **Why unresolved:** The error analysis (Section 5.4) highlights that generative models frequently produce repetitive or irrelevant outputs, but the study did not test automated filtering solutions.
- **What evidence would resolve it:** Implementation of a secondary LLM verification step to filter the primary model's outputs, followed by a comparison of error rates (specifically low-variance and unrelated text) against the baseline.

## Limitations

- **Incomplete specification:** The text-to-class mapping process details (threshold, top-k selection) are not fully specified, potentially impacting reproducibility.
- **Single dataset:** Focus on MIMIC-IV from one healthcare system may limit generalizability across different clinical environments.
- **Static evaluation:** The benchmark doesn't address temporal dynamics or model performance with novel diseases not represented in training data.

## Confidence

- **High Confidence:** Encoder-based classifiers consistently outperform generative LLMs for diagnosis prediction accuracy (supported by multiple experiments across different model sizes and configurations).
- **Medium Confidence:** Retrieval augmentation significantly improves generative LLM performance (clear improvements shown but optimal strategies need further validation).
- **Medium Confidence:** Chain-of-thought prompting degrades performance in this task (consistent evidence but mechanism is somewhat speculative).

## Next Checks

1. **Mapping Process Validation:** Implement and validate the exact text-to-class mapping procedure using the NeuML-PubMedBERT embeddings. Test different similarity thresholds and top-k selection strategies to determine their impact on final performance metrics.

2. **Cross-Hospital Generalization Test:** Apply the benchmark to a different hospital system's data (e.g., eICU or another MIMIC version) to validate whether encoder superiority holds across different clinical environments and coding practices.

3. **Dynamic Label Space Evaluation:** Create a synthetic test scenario where 10% of ICD codes are removed and 10% new codes are introduced. Compare how encoder-based classifiers (requiring retraining) versus generative LLMs (potentially zero-shot) adapt to this changing label space, measuring both adaptation speed and accuracy.