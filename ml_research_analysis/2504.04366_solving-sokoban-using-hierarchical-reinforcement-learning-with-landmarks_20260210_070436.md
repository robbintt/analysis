---
ver: rpa2
title: Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks
arxiv_id: '2504.04366'
source_url: https://arxiv.org/abs/2504.04366
tags:
- planning
- policy
- sokoban
- training
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HalfWeg, a novel hierarchical reinforcement
  learning framework that performs top-down recursive planning via learned subgoals,
  successfully applied to the complex combinatorial puzzle game Sokoban. The method
  constructs a six-level policy hierarchy, where each higher-level policy generates
  subgoals for the level below, all learned end-to-end from scratch without domain
  knowledge.
---

# Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks

## Quick Facts
- arXiv ID: 2504.04366
- Source URL: https://arxiv.org/abs/2504.04366
- Reference count: 9
- Novel hierarchical RL framework achieving 90.2% success rate on Boxoban test puzzles

## Executive Summary
This paper introduces HalfWeg, a novel hierarchical reinforcement learning framework that performs top-down recursive planning via learned subgoals, successfully applied to the complex combinatorial puzzle game Sokoban. The method constructs a six-level policy hierarchy, where each higher-level policy generates subgoals for the level below, all learned end-to-end from scratch without domain knowledge. Results show that the agent can generate long action sequences from a single high-level call, achieving 90.2% success rate on Boxoban test puzzles when combined with search, with an average solution length of 214.7 steps.

## Method Summary
The method redefines Sokoban as a generalized planning problem where policies navigate between arbitrary states rather than just solving puzzles. A six-level hierarchy (PL0-PL5) is trained end-to-end using dense rewards based on state-to-state distances. Each policy level predicts landmark states that are used to decompose the problem into smaller subgoals. The framework uses two lightweight neural networks: MA (Model Actions) predicts action sequences, and MS (Model State) predicts landmark states. During planning, a two-leg search samples candidate landmarks from replay buffer and selects the best path based on distance metrics.

## Key Results
- 90.2% success rate on Boxoban test puzzles when combined with search
- 79.4% success rate without search (PL5 with 100 searches)
- Average solution length of 214.7 steps
- Training curve shows successful learning of hierarchical planning behavior

## Why This Works (Mechanism)

### Mechanism 1: Exponential Search Space Expansion via Recursive Decomposition
- Claim: The 6-level hierarchy enables exponentially larger plan spaces with linear model complexity.
- Mechanism: Each level PL_i generates plans of length 2^i × d. Adding one level doubles compute but squares the searchable plan space: A^(2^(R+1)×d) = (A^(2^R×d))².
- Core assumption: The learned subgoals at each level meaningfully partition the solution space rather than creating dead-end branches.
- Evidence anchors: [abstract], [section 3.3], [corpus] shows related work on recursive decomposition.

### Mechanism 2: Policy-Guided Two-Leg Search as Learned Heuristic
- Claim: The search procedure uses learned policies to sample promising subgoals rather than random exploration.
- Mechanism: For each planning problem, sample NDSS (=100) candidate landmark states from replay buffer, evaluate two-leg paths through each, select best by distance metric.
- Core assumption: The replay buffer contains sufficiently diverse states, including states near solutions.
- Evidence anchors: [section 4.2], [table 1], [corpus] supports landmark-based planning heuristics.

### Mechanism 3: Dense Reward via Generalized Planning Reformulation
- Claim: Converting sparse Sokoban rewards to state-to-state distance metrics enables gradient-based learning.
- Mechanism: Instead of reward=1 only on puzzle completion, reward = -distance(v, v') where v' is reached state.
- Core assumption: The distance metric correlates with actual puzzle-solving progress.
- Evidence anchors: [section 3.1], [section 4.3], [corpus] has weak direct support for this technique.

## Foundational Learning

- Concept: **Goal-Conditioned Reinforcement Learning**
  - Why needed here: The entire framework depends on policies that can navigate from any state u to any target state v, not just solve puzzles.
  - Quick check question: Can you explain why training on random state-to-state transitions helps solve the original puzzle?

- Concept: **Temporal Abstraction in HRL**
  - Why needed here: Each hierarchy level operates at different time scales—PL0 outputs d actions, PL5 outputs 32d actions in one call.
  - Quick check question: What is the relationship between hierarchy depth R and maximum plan length?

- Concept: **Experience Replay Buffer as Training Data Source**
  - Why needed here: Subgoals are sampled from previously visited states; without diverse exploration, the agent cannot learn to reach novel configurations.
  - Quick check question: Why does the b=1 direction flag (moving away from goals) matter for long-term training?

## Architecture Onboarding

- Component map:
  - Input (u, v, b) → MS at level R predicts landmark w → PL_(R-1) generates action sequence to w → emulator yields state ŵ → PL_(R-1) from ŵ to v → concatenate sequences

- Critical path: Input (u, v, b) → MS predicts landmark → PL_(R-1) generates sequence to landmark → emulator executes → PL_(R-1) from new state to v → concatenate

- Design tradeoffs:
  - Deeper hierarchies increase plan length exponentially but require more accurate subgoal prediction
  - Larger models improve performance but weren't tested on Boxoban due to compute constraints
  - d=4 balances action sequence granularity; larger d may reduce subgoal quality

- Failure signatures:
  - Low solve rate with single forward pass but high rate with search → subgoal prediction MS is weak
  - Training plateau early → model capacity insufficient
  - Good training performance, poor test performance → overfitting

- First 3 experiments:
  1. Train PL_0 only on 6×6 Sokoban to verify short-horizon navigation
  2. Compare training with/without refinement examples (u, u_d, b=0)
  3. Compare R=2, 3, 4, 5 on validation puzzles to find optimal depth

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's recursive structure may amplify errors if subgoals are poor or unreachable
- Dense reward reformulation's effectiveness lacks theoretical grounding and comparison to alternatives
- Critical implementation details for reproduction are unspecified (optimizer settings, exact distance metrics, self-play ratios)

## Confidence
**High confidence** in empirical results: The reported success rates (90.2% with search, 79.4% without) on Boxoban test puzzles are well-supported by ablation studies and hierarchy depth analysis.

**Medium confidence** in theoretical mechanism claims: While exponential scaling is mathematically valid, limited empirical validation that learned subgoals partition solution spaces effectively rather than creating dead-end branches.

**Low confidence** in dense reward reformulation's general applicability: The claim that state-to-state distances enable gradient-based learning is supported by results but lacks rigorous analysis of when this transformation works or fails.

## Next Checks
1. **Subgoal Quality Analysis**: Measure how often predicted landmarks are reachable and lead toward solutions. Visualize subgoal sequences to verify progressive puzzle simplification.

2. **Failure Propagation Experiment**: Systematically introduce noise into subgoal predictions at different hierarchy levels and measure the impact on final solution success.

3. **Alternative Reward Comparison**: Implement baseline using standard sparse rewards with reward shaping and compare learning curves and final performance to the state-distance approach.