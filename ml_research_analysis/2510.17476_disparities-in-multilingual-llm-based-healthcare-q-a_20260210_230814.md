---
ver: rpa2
title: Disparities in Multilingual LLM-Based Healthcare Q&A
arxiv_id: '2510.17476'
source_url: https://arxiv.org/abs/2510.17476
tags:
- wikipedia
- english
- pages
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MultiWikiHealthCare, a multilingual healthcare
  dataset derived from Wikipedia across English, German, Italian, Turkish, and Chinese.
  By analyzing cross-lingual factual alignment and leveraging Retrieval-Augmented
  Generation (RAG), the research reveals substantial disparities in both Wikipedia
  coverage and LLM responses.
---

# Disparities in Multilingual LLM-Based Healthcare Q&A

## Quick Facts
- arXiv ID: 2510.17476
- Source URL: https://arxiv.org/abs/2510.17476
- Reference count: 24
- Primary result: Multilingual LLMs default to English Wikipedia knowledge even when responding to non-English prompts.

## Executive Summary
This study introduces MultiWikiHealthCare, a multilingual healthcare dataset derived from Wikipedia across English, German, Italian, Turkish, and Chinese. By analyzing cross-lingual factual alignment and leveraging Retrieval-Augmented Generation (RAG), the research reveals substantial disparities in both Wikipedia coverage and LLM responses. Across models, answers align more closely with English Wikipedia, even for non-English prompts. However, providing contextual excerpts from non-English Wikipedia shifts factual alignment toward culturally relevant knowledge. This highlights practical pathways for building more equitable, multilingual AI systems in healthcare.

## Method Summary
The study constructed MultiWikiHealthCare by scraping Wikipedia pages for 918 unique health-related entities, then extracting and aligning facts across five languages using InfoGap with GPT-4o-mini. They filtered for relevant entities using Llama 3.3-70B, classified relevance with fine-tuned transformers (XLM-RoBERTa for non-English, except Chinese uses monolingual BERT), and generated questions via GPT-4o-mini. Answers were generated by three LLMs (Llama 3.3-70B, Qwen3-Next-80B-A3B-Instruct, Aya Expanse-32B) with temperature=1 and max_tokens=4096. Factual alignment was evaluated using AlignScore against both English and source-language Wikipedia, with RAG experiments using BM25-Sparse retriever with top-10 PubMed articles.

## Key Results
- LLMs default to English Wikipedia knowledge even when responding to non-English prompts, showing English-centrism across all three tested models
- Providing non-English Wikipedia contextual excerpts at inference time shifts factual alignment toward culturally relevant knowledge (76-83% alignment vs 17-22% baseline for Llama)
- Cross-lingual Wikipedia coverage disparities are substantial: Chinese Wikipedia shows only 6.98% aligned facts with English, while English Wikipedia contains statistically more sections, paragraphs, and facts than all other editions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing non-English contextual excerpts at inference time shifts LLM factual alignment toward locally relevant knowledge.
- Mechanism: Context injection anchors generation to source-language evidence rather than defaulting to English-centric pretraining knowledge. When the prompt includes translated Wikipedia excerpts, models ground responses in the provided context, overriding implicit English biases.
- Core assumption: The model's attention mechanism prioritizes provided context over parametric knowledge when context is explicitly relevant to the query.
- Evidence anchors:
  - [abstract] "Providing contextual excerpts from non-English Wikipedia at inference time effectively shifts factual alignment toward culturally relevant knowledge."
  - [section 4.3, Table 9] "Wiki" method shows 76-83% alignment with non-English references across languages (vs. 17-22% baseline) for Llama.
  - [corpus] Related work "Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?" confirms cross-lingual inconsistencies persist without intervention.
- Break condition: Noisy or irrelevant context (as observed in RAG experiments with PubMed) can degrade alignment; high-quality, semantically matched excerpts are required.

### Mechanism 2
- Claim: LLMs default to English-centric factual knowledge even when responding to non-English prompts.
- Mechanism: Pretraining corpora are dominated by English content (Wikipedia, web text), creating an implicit prior that privileges English knowledge representations. When factual knowledge differs across languages (e.g., treatment guidelines, regional sources), models retrieve English-parametric knowledge by default.
- Core assumption: Factual knowledge is stored language-specifically in model weights, with English having higher coverage and retrieval probability.
- Evidence anchors:
  - [abstract] "Across models, answers align more closely with English Wikipedia, even for non-English prompts."
  - [section 4.2, Table 8] All models show higher AlignScore with English references than source-language references for non-English queries.
  - [corpus] "ECLeKTic" benchmark addresses cross-lingual knowledge transfer but doesn't solve the default-to-English problem.
- Break condition: Models with explicit multilingual pretraining objectives (e.g., Aya) may show reduced English-centrism, though still present.

### Mechanism 3
- Claim: Cross-lingual Wikipedia disparities in coverage, citations, and facts propagate to LLM response quality disparities.
- Mechanism: Wikipedia is a pretraining source for many LLMs; when source-language Wikipedia has fewer facts, different citations (e.g., German cites national sources like rki.de), or missing content, models lack culturally-specific knowledge in those languages.
- Core assumption: Pretraining data quality and coverage directly influence downstream factual accuracy and cultural relevance.
- Evidence anchors:
  - [section 4.1, Table 6] Chinese Wikipedia shows only 6.98% aligned facts with English; Turkish shows 33.79%.
  - [section 4.1, Table 7] English Wikipedia contains statistically more sections, paragraphs, and facts than all other editions (paired t-test significant).
  - [corpus] "Factual Inconsistencies in Multilingual Wikipedia Tables" confirms factual inconsistencies across language editions.
- Break condition: Models may learn from non-Wikipedia sources that compensate for Wikipedia gaps, but the study doesn't measure this.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper uses RAG as the intervention mechanism to shift factual alignment toward non-English sources.
  - Quick check question: Can you explain why RAG might fail when retrieved context is noisy or semantically distant from the query?

- **Concept: Factual Alignment vs. Hallucination Detection**
  - Why needed here: The paper distinguishes alignment (consistency with a reference) from hallucination (fabricated content); AlignScore measures the former.
  - Quick check question: If an LLM response is factually correct but contradicts the provided Wikipedia excerpt, is that an alignment failure or hallucination?

- **Concept: Cross-Lingual Knowledge Transfer**
  - Why needed here: Understanding whether knowledge learned in English can be accessed in other languages is central to the paper's disparity analysis.
  - Quick check question: Why might a model know a fact in English but fail to retrieve it when queried in Turkish?

## Architecture Onboarding

- **Component map:** Wikipedia scraping -> InfoGap fact extraction -> fine-tuned transformer relevance classification -> GPT-4o-mini question generation -> LLM inference -> AlignScore evaluation
- **Critical path:**
  1. Start with aligned cross-lingual facts from InfoGap (GPT-4o-mini + hubness correction)
  2. Filter for relevance using fine-tuned transformers (XLM-RoBERTa for non-English, except Chinese uses monolingual BERT)
  3. Generate questions via GPT-4o-mini, validate with GPT-5 judge
  4. Query target LLMs with/without context; score alignment with AlignScore
- **Design tradeoffs:**
  - Cost vs. scale: GPT-4o-mini used instead of GPT-4o for fact extraction (10x cheaper); fine-tuned transformers replace LLMs for relevance filtering
  - Alignment metric limitations: AlignScore is English-centric, requiring translation of all non-English content (potential artifacts)
  - RAG vs. direct context: Direct Wikipedia excerpts outperform PubMed RAG for cultural alignment (noisy retrieval degrades performance)
- **Failure signatures:**
  - Low AlignScore with source-language Wikipedia despite context injection → context not semantically matched or model ignoring context
  - High variance across languages in alignment → pretraining data disparity not fully mitigated
  - Aya fails to generate responses for some RAG prompts → prompt length exceeds model capacity
- **First 3 experiments:**
  1. **Baseline alignment measurement:** Query all three models in each target language without context; score against both English and source-language Wikipedia to quantify English-centrism.
  2. **Context injection ablation:** Provide translated Wikipedia excerpts vs. no context; measure alignment shift (expect 40-60 point increase per Table 9).
  3. **RAG retrieval quality test:** Compare BM25-Sparse retrieval with semantic embedding retrieval (e.g., multilingual E5) to determine if noisy context is the cause of RAG underperformance.

## Open Questions the Paper Calls Out

- **Cross-lingual disparities with clinical materials:** The authors state they plan to "extend sources beyond Wikipedia to clinical and public health materials" to validate whether findings hold with professional medical guidelines.
- **Advanced retrieval methods:** Section 4.3 notes that RAG results often contained "noisy context," and the authors explicitly plan to "explore more advanced information retrieval and RAG methods to improve contextual relevance" in future work.
- **Translation-based evaluation artifacts:** The Ethics Statement lists the reliance on English-centric AlignScore and translation as a limitation that "may introduce artifacts," while the Future Work proposes adopting "multilingual factuality metrics with native-speaker review."

## Limitations

- The English-centric AlignScore metric may systematically underestimate alignment quality in non-English Wikipedia due to translation artifacts
- Reliance on Wikipedia as both pretraining source and reference introduces potential circularity in evaluation
- RAG experiments show promising but inconsistent results, with PubMed retrieval performing poorly compared to direct Wikipedia context injection

## Confidence

- **High Confidence:** The core finding that LLMs default to English Wikipedia knowledge even when responding to non-English prompts is robustly supported by consistent patterns across three different models and multiple evaluation methods.
- **Medium Confidence:** The effectiveness of context injection at inference time is demonstrated but requires high-quality, semantically matched excerpts.
- **Low Confidence:** The practical implications for healthcare equity depend heavily on whether Wikipedia coverage gaps reflect actual knowledge gaps versus editorial preferences.

## Next Checks

1. **Retrieval Quality Validation:** Re-run RAG experiments using state-of-the-art multilingual dense retrievers (e.g., E5) versus BM25 to isolate whether poor performance stems from retrieval method or the RAG approach itself.

2. **Metric Robustness Test:** Evaluate a subset of responses using native speaker human judges for both factuality and cultural relevance, comparing results against AlignScore to quantify translation-induced measurement bias.

3. **Domain Generalization:** Test whether the English-centrism pattern holds for non-Wikipedia medical knowledge sources (e.g., WHO guidelines, clinical databases) to determine if findings reflect Wikipedia-specific issues or broader LLM behavior.