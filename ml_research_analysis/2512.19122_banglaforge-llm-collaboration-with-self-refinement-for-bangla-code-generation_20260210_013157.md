---
ver: rpa2
title: 'BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation'
arxiv_id: '2512.19122'
source_url: https://arxiv.org/abs/2512.19122
tags:
- bangla
- code
- test
- generation
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BanglaForge, a framework for generating Python
  code from Bangla natural language descriptions. It addresses the low-resource challenge
  in Bangla code generation by combining retrieval-augmented prompting, bilingual
  translation with a controlled glossary, iterative self-refinement using execution
  feedback, and a dual-model coder-reviewer pipeline.
---

# BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation

## Quick Facts
- **arXiv ID**: 2512.19122
- **Source URL**: https://arxiv.org/abs/2512.19122
- **Reference count**: 40
- **Primary result**: Achieves 84% Pass@1 accuracy on BLP-2025 Bangla code generation benchmark

## Executive Summary
BanglaForge introduces a novel framework for generating Python code from Bangla natural language descriptions, addressing the low-resource challenge in Bangla code generation. The system combines retrieval-augmented prompting, bilingual translation with controlled glossaries, iterative self-refinement using execution feedback, and a dual-model coder-reviewer pipeline. By retrieving relevant examples, generating initial code with the Coder LLM, and refining it with the Reviewer LLM until all test cases pass, BanglaForge demonstrates state-of-the-art performance on the BLP-2025 benchmark while being open-sourced for further research.

## Method Summary
BanglaForge addresses Bangla code generation through a multi-stage pipeline that leverages both retrieval and iterative refinement. The system first retrieves relevant code examples using a dense retriever trained on synthetic data, then generates initial code using a Coder LLM with bilingual translation and controlled glossaries. The generated code undergoes iterative self-refinement through a dual-LLM architecture where a Reviewer LLM provides feedback based on execution results, with the process repeating until all test cases pass or a maximum iteration limit is reached. This approach combines the strengths of example-based learning, bilingual support, and automated refinement to achieve high accuracy on Bangla code generation tasks.

## Key Results
- Achieves 84% Pass@1 accuracy on the BLP-2025 Bangla Code Generation benchmark
- Outperforms existing methods on low-resource Bangla code generation tasks
- Demonstrates effectiveness of retrieval-augmented prompting combined with self-refinement
- Shows that dual-LLM coder-reviewer architecture improves code quality through iterative refinement

## Why This Works (Mechanism)
The framework's effectiveness stems from addressing the fundamental challenges of low-resource language code generation through multiple complementary mechanisms. Retrieval-augmented prompting provides relevant examples that compensate for limited training data in Bangla, while bilingual translation with controlled glossaries ensures accurate semantic preservation between languages. The iterative self-refinement process allows the system to learn from execution feedback and progressively improve code quality, while the dual-LLM architecture separates concerns between code generation and quality assessment, enabling more focused optimization of each component.

## Foundational Learning

**Retrieval-augmented generation (RAG)**: Combines information retrieval with LLM generation to provide context-specific examples, needed to overcome limited training data in low-resource languages; quick check: verify retriever returns relevant examples for diverse Bangla prompts.

**Bilingual code generation**: Generates code from non-English natural language descriptions, needed because most programming resources are English-centric; quick check: test translation accuracy with domain-specific Bangla terminology.

**Iterative self-refinement**: Repeatedly improves generated code based on execution feedback, needed to handle complex programming tasks requiring multiple attempts; quick check: measure convergence rate across different problem difficulties.

**Dual-LLM architecture**: Separates code generation from code review tasks, needed to enable specialized optimization and reduce hallucination risks; quick check: compare performance against single-LLM baselines.

**Execution-based validation**: Uses test case results to guide refinement, needed to ensure generated code actually works as intended; quick check: verify test coverage across different code complexity levels.

## Architecture Onboarding

**Component map**: User Query -> Retrieval Engine -> Translation Module -> Coder LLM -> Execution Environment -> Reviewer LLM -> Refined Code (loop until pass or max iterations)

**Critical path**: The core execution loop involves retrieving examples, generating initial code, executing against test cases, receiving reviewer feedback, and refining until all tests pass or iteration limit reached.

**Design tradeoffs**: The dual-LLM approach provides specialized optimization but increases computational cost; the iterative refinement ensures quality but may face diminishing returns on very complex problems; the retrieval component adds context but depends on the quality of the example corpus.

**Failure signatures**: Common failure modes include retrieval returning irrelevant examples, translation errors causing semantic drift, reviewer feedback becoming stuck in local minima, and execution timeouts on computationally intensive problems.

**First experiments**:
1. Test retrieval relevance by measuring example similarity scores for diverse Bangla prompts
2. Evaluate bilingual translation accuracy using human-annotated code-translation pairs
3. Benchmark refinement convergence by measuring test pass rates across iteration counts

## Open Questions the Paper Calls Out
None

## Limitations
- Claims of state-of-the-art performance (84% Pass@1) are difficult to independently verify due to limited benchmark details
- Dual-LLM architecture introduces significant computational overhead not thoroughly analyzed
- Reliance on execution feedback assumes robust testing framework availability, limiting generalizability
- Controlled glossary approach may not scale well to domain-specific terminology or evolving programming features

## Confidence
- **High Confidence**: Retrieval-augmented prompting methodology and implementation details are well-documented and reproducible
- **Medium Confidence**: Performance metrics require independent validation on BLP-2025 benchmark to confirm generalizability
- **Medium Confidence**: Dual-LLM effectiveness demonstrated, but computational cost-benefit analysis is incomplete

## Next Checks
1. Conduct ablation studies removing individual components (retrieval, self-refinement, dual-LLM) to quantify their contributions to the 84% Pass@1 score
2. Test BanglaForge on additional Bangla code generation benchmarks beyond BLP-2025 to assess generalizability across different domains and difficulty levels
3. Measure and compare computational efficiency (time, cost) of BanglaForge against single-model approaches to understand practical trade-offs of dual-LLM architecture