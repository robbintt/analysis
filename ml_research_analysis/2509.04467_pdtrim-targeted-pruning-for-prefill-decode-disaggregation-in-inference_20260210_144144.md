---
ver: rpa2
title: 'PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference'
arxiv_id: '2509.04467'
source_url: https://arxiv.org/abs/2509.04467
tags:
- pruning
- block
- arxiv
- blocks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PDTrim, a pruning method specifically designed
  for large language models (LLMs) that leverages the characteristics of prefill-decode
  (PD) disaggregation in practical deployment. The core method constructs pruning
  and distillation sets to perform iterative block removal, enabling more precise
  pruning of blocks.
---

# PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference

## Quick Facts
- **arXiv ID:** 2509.04467
- **Source URL:** https://arxiv.org/abs/2509.04467
- **Reference count:** 40
- **Primary result:** PDTrim achieves improved performance and faster inference compared to prior methods under the same settings for both PD disaggregation and PD unified settings.

## Executive Summary
PDTrim introduces a novel pruning method specifically designed for large language models deployed with prefill-decode disaggregation. The approach constructs pruning and distillation sets based on block redundancy metrics, then iteratively optimizes block removal while accounting for the asymmetric error propagation between prefill and decode stages. Extensive experiments demonstrate that PDTrim consistently outperforms existing pruning methods across multiple model sizes and benchmarks, with particular benefits when leveraging stage-specific pruning strategies.

## Method Summary
PDTrim operates through a three-phase approach: (1) redundancy analysis using cosine similarity of block input/output hidden states to identify potentially removable blocks, (2) iterative simulated annealing search to optimize the combination of blocks to remove, and (3) stage-specific pruning that accounts for the different sensitivities of prefill and decode stages. The method distinguishes between blocks to be completely removed (pruning set) and consecutive blocks to be merged through distillation (distillation set), with a threshold-based decision mechanism to determine which blocks should be removed from prefill, decode, or both stages.

## Key Results
- PDTrim consistently achieves strong performance across 13 benchmarks including MMLU, CMMLU, PIQA, and others
- The method provides improved accuracy compared to prior pruning approaches while achieving similar or better inference latency
- PDDisaggregation (asymmetric prefill-decode pruning) outperforms unified pruning in both accuracy and efficiency
- PDTrim can be extended to non-block pruning methods while maintaining performance advantages

## Why This Works (Mechanism)

### Mechanism 1: Redundancy-Aware Block Allocation
The method partitions transformer blocks into distinct "pruning" and "distillation" sets based on input-output similarity. Blocks with high cosine similarity between input and output hidden states are marked for direct removal, while consecutive blocks with high mutual similarity are marked for merging. This minimizes information loss by only removing blocks that contribute minimal transformation to the input.

### Mechanism 2: Iterative Optimization via Perturbation Search
An iterative search strategy using probabilistic acceptance criteria finds superior block removal configurations compared to one-shot greedy selection. The algorithm accepts suboptimal swaps with probability $P_{s'} = e^{-\Delta f / T}$ to escape local minima, gradually cooling the search space to converge on optimal pruning configurations.

### Mechanism 3: Error Propagation Asymmetry in Prefill-Decode
Different pruning schemes are applied to prefill and decode stages to minimize error accumulation in the KV Cache mechanism. Pruning errors in the prefill stage contaminate the KV Cache for all tokens and accumulate during decode steps. By retaining more blocks in the prefill stage and pruning aggressively only in the decode stage, the method limits error propagation.

## Foundational Learning

- **PD (Prefill-Decode) Disaggregation**
  - Why needed: This is the deployment context PDTrim targets, where prefill processes the prompt in parallel (compute-heavy) while decode generates tokens sequentially (memory-bandwidth heavy)
  - Quick check: Why does pruning the prefill stage affect the accuracy of all subsequent generated tokens?

- **KV Cache Mechanics**
  - Why needed: Mechanism 3 relies on the fact that prefill generates the initial KV Cache
  - Quick check: Does the KV Cache generated in the prefill stage change during the decode stage?

- **Knowledge Distillation vs. Pruning**
  - Why needed: The method distinguishes between "removing" a block (pruning) and "merging" two blocks into one (distillation)
  - Quick check: If two consecutive blocks are distilled into one, does the parameter count drop by 50% for those layers, or is the architecture depth just reduced by 1?

## Architecture Onboarding

- **Component map:** Calibration Module -> Set Constructor -> Search Engine -> Stage Adapter
- **Critical path:** The determination of the **Distillation Threshold ($d_T$)** is the most sensitive hyperparameter. If set too high, you miss distillation opportunities; if too low, you aggressively merge blocks that should remain distinct, degrading accuracy.
- **Design tradeoffs:** Speed vs. Search Cost (iterative search takes time but yields better accuracy than fast greedy methods); Complexity vs. Compatibility (asymmetric pruning requires a serving system capable of loading different weights for prefill and decode nodes)
- **Failure signatures:** Catastrophic Recall Loss (if prefill blocks are over-pruned, causing grammatically correct but hallucinated content); Stalling in Search (if temperature decay $\alpha$ is too fast, the search freezes on a suboptimal solution early)
- **First 3 experiments:** 1) Baseline Similarity Check: Visualize cosine similarity of blocks to verify redundancy exists; 2) Unified vs. Disaggregated Pruning: Run PDTrim on single model vs. disaggregated setup to quantify asymmetric gain; 3) Hyperparameter Sweep on $d_T$: Test distillation thresholds to find tipping point where merging starts to hurt perplexity

## Open Questions the Paper Calls Out

- **Can PDTrim maintain its efficiency and accuracy benefits when applied to models with 70 billion parameters or more?** The computational cost of the iterative block removal process and distillation sets may scale poorly or yield different redundancy patterns in much larger architectures.

- **Can the heterogeneous pruning strategy be effectively adapted for Mixture-of-Experts (MoE) architectures?** Block redundancy metrics based on cosine similarity may not accurately reflect the importance of specific experts during the prefill versus decode stages.

- **How does PDTrim perform on tasks requiring extremely long context windows?** The theoretical error analysis suggests error is bounded by $||K||_F$ (Key matrix norm), which grows with sequence length $N$, potentially amplifying prefill pruning errors in long contexts.

## Limitations
- The method has not been evaluated on models with 70 billion parameters or more
- The distillation process details are insufficiently specified regarding the architectural changes when merging blocks
- The stage-specific pruning threshold of 3% improvement is heuristic and may not generalize across models with different prefill-decode characteristics

## Confidence

**High Confidence:** The experimental results showing PDTrim's performance advantage over baselines are well-supported by quantitative metrics (accuracy, perplexity, latency). The theoretical error analysis connecting prefill-stage pruning to KV cache contamination is mathematically sound.

**Medium Confidence:** The iterative search mechanism's superiority over greedy methods is demonstrated via ablation, but the specific temperature schedule appears tuned for LLaMA3.1-8B without cross-model validation.

**Low Confidence:** The distillation process details are insufficiently specifiedâ€”the paper states blocks are "merged" but doesn't clarify whether parameters are averaged, concatenated, or architecturally restructured.

## Next Checks

1. **Redundancy Metric Validation:** Run PDTrim on a model with known uniform layer importance to verify the cosine similarity metric correctly identifies truly redundant layers versus those with distinct functional roles.

2. **Distillation Process Specification:** Implement and test multiple distillation strategies (hidden-state matching, attention distillation, logits distillation) when merging blocks to determine which best preserves accuracy while achieving compression targets.

3. **Stage Sensitivity Calibration:** Systematically vary the 3% threshold for stage-specific pruning across multiple models to establish whether this heuristic value is optimal or if model-dependent thresholds would yield better performance.