---
ver: rpa2
title: 'IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination'
arxiv_id: '2602.01769'
source_url: https://arxiv.org/abs/2602.01769
tags:
- preference
- iris
- implicit
- hallucination
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IRIS introduces a hallucination mitigation framework that leverages
  continuous implicit rewards in the model's native log-probability space to capture
  internal modal competition, eliminating the need for external evaluators. By sifting
  self-generated preference pairs using Rectified Visual Guidance (RVG), IRIS prioritizes
  responses that are genuinely grounded in visual evidence over those supported by
  language priors.
---

# IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination

## Quick Facts
- **arXiv ID**: 2602.01769
- **Source URL**: https://arxiv.org/abs/2602.01769
- **Reference count**: 40
- **Primary result**: Achieves CHAIR scores of 2.4 on AMBER and 8.66 on Object HalBench using only 5.7k samples, reducing multimodal hallucination through implicit reward-guided sifting

## Executive Summary
IRIS introduces a hallucination mitigation framework that leverages continuous implicit rewards in the model's native log-probability space to capture internal modal competition, eliminating the need for external evaluators. By sifting self-generated preference pairs using Rectified Visual Guidance (RVG), IRIS prioritizes responses that are genuinely grounded in visual evidence over those supported by language priors. Extensive experiments show that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, reducing CHAIR scores to 2.4 on AMBER and 8.66 on Object HalBench, while maintaining general capability. This demonstrates IRIS as an efficient, principled paradigm for mitigating multimodal hallucinations through on-policy preference optimization.

## Method Summary
IRIS operates through iterative self-alignment using continuous implicit rewards derived from the model's native log-probability distributions. The framework begins with SFT warm-up on 5.7k samples from RLHF-V, then performs on-policy preference optimization using self-generated candidate pairs. Each round samples K candidates from the current policy, scores them using RVG (which penalizes responses favored by text-only conditions), selects preference pairs, and optimizes a composite DPO objective with CTP, CVP, and anchor terms. The implicit reward formulation r* = β log π(y|v,x) / πref(y|v,x) preserves fine-grained preference information that discrete external evaluators would collapse.

## Key Results
- Achieves CHAIR score of 2.4 on AMBER benchmark, significantly outperforming baseline models
- Reduces CHAIR score to 8.66 on Object HalBench, demonstrating strong object hallucination mitigation
- Maintains general conversational capability with minimal degradation (LLaVA-Bench accuracy remains >50%)
- Requires only 5.7k samples for effective training, making it highly data-efficient
- Shows superior performance compared to models using external GPT-4V evaluators for preference scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous implicit rewards in native log-probability space preserve fine-grained preference differences that discrete external evaluator scores collapse.
- Mechanism: The DPO implicit reward r*(v, x, y) = β log π*(y|v, x) / πref(y|v, x) provides a dense, continuous signal. Unlike GPT-4V scores that assign identical discrete labels to semantically distinct responses (Fig. 1a), this log-ratio captures the internal competition between visual evidence and language priors at the token level.
- Core assumption: The model's internal probability distribution reflects meaningful grounding quality after SFT calibration.
- Evidence anchors:
  - [abstract] "leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition"
  - [Section 1] "discrete external rewards assign identical scores to semantically distinct responses, collapsing fine-grained preference differences"
  - [corpus] Related work on implicit rewards (SeRA, DICE) supports self-evaluation potential, but corpus lacks direct validation of continuous vs. discrete signal superiority.
- Break condition: If the base model is poorly calibrated (no SFT warm-up), implicit rewards may not correlate with grounding quality.

### Mechanism 2
- Claim: Rectified Visual Guidance (RVG) separates visual grounding from language priors by penalizing responses that score higher under text-only conditions.
- Mechanism: RVG computes S(v, x, y) = r_image − γ·max(0, r_text − r_image). When r_text > r_image, the response is more supported by language priors than visual evidence, triggering a penalty. This suppresses candidates that hallucinate by relying on statistical patterns from pretraining.
- Core assumption: r_text > r_image reliably indicates language-prior-driven hallucination.
- Evidence anchors:
  - [Section 4.3] "The penalty term is activated only when r_text > r_image, which corresponds to cases where the model assigns a higher relative likelihood to a response in the absence of visual evidence"
  - [Table 4] RVG outperforms r_image and r_text alone across all metrics (CHAIR 8.66 vs. 14.1 at R2)
  - [corpus] No direct corpus validation; this is a novel formulation in this paper.
- Break condition: If γ is set too high, the model may over-penalize legitimate responses that happen to align with common knowledge.

### Mechanism 3
- Claim: On-policy self-generated preference pairs avoid distribution shift and learnability gaps that plague off-policy external feedback.
- Mechanism: By sampling candidates from π_θ^(r-1) and computing log-ratios against the preceding policy, preference pairs stay within the KL-feasible region. External off-policy pairs often lie outside this region, causing vanishing gradients (Fig. 1c).
- Core assumption: Self-generated pairs have a positive expected quality gap (E[s*(y_w) − s*(y_l)] ≥ δ).
- Evidence anchors:
  - [Section 1] "When preferred responses originate from disjoint off-policy distributions, they receive vanishing probability under the reference policy"
  - [Table 3] Self-generation with SFT warm-up achieves CHAIR 2.4 vs. 3.7 without self-generation at R2
  - [corpus] Tajwar et al. (2024) and Guo et al. (2024) support on-policy benefits for DPO, but lack multimodal specifics.
- Break condition: If sampling diversity is too low (K too small), preference pairs may have insufficient contrast for learning.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO) implicit reward formulation**
  - Why needed here: IRIS repurposes the DPO log-likelihood ratio as a scoring signal, not just an optimization objective. Without understanding this mapping (r* ↔ π*), the RVG scoring mechanism is opaque.
  - Quick check question: Can you explain why β log π(y|s) / πref(y|s) functions as an implicit reward rather than just a loss term?

- Concept: **KL-divergence constraints and learnability gaps**
  - Why needed here: The paper's core critique of external feedback is that off-policy pairs violate reverse KL constraints, causing zero-gradient updates. Understanding this explains why self-generation is necessary.
  - Quick check question: Why does a preference pair from a different distribution produce vanishing gradients under DPO?

- Concept: **Modal competition in multimodal generation**
  - Why needed here: Hallucinations arise from language priors dominating visual evidence. The r_image vs. r_text comparison relies on this theoretical framing.
  - Quick check question: What does it mean when r_text > r_image for a candidate response?

## Architecture Onboarding

- Component map:
  ```
  π_base → SFT warm-up → π_θ^0
                           ↓
           ┌───────────────────────────────────┐
           │  For each round r = 1..R:         │
           │  1. Sample K candidates from π_θ^(r-1)   │
           │  2. Compute r_image, r_text using π_θ^(r-2) as reference  │
           │  3. Apply RVG scoring (Eq. 10)     │
           │  4. Select y_w = argmax S, y_l = argmin S │
           │  5. Filter + anchor to y_sft if needed   │
           │  6. Construct negative image ̃v via diffusion │
           │  7. Optimize L_total = L_ctp + λL_cvp + L_anchor │
           └───────────────────────────────────┘
                           ↓
                        π_θ^R (final)
  ```

- Critical path:
  1. SFT warm-up (5.7k samples from RLHF-V) — calibrates implicit rewards
  2. On-policy sampling with K=5 — generates candidates
  3. RVG scoring with γ=0.7 — sifts preference pairs
  4. Composite DPO optimization with β=0.1, λ=1.0 — updates policy

- Design tradeoffs:
  - K (sampling repeats): Higher K improves pair contrast but increases cost. K=5 is stable; K=3–10 all work (Table 6).
  - γ (rectification strength): Controls penalty for prior-driven responses. Optimal at 0.7; too high over-constrains (Table 13).
  - Rounds: R=2 optimal; R=3 shows degradation in general capability (Tables 10, 11).

- Failure signatures:
  - No SFT warm-up: CHAIR 3.8 vs. 2.4 at R2 (Table 3)
  - Missing L_anchor: General capability drops below base model (Table 9)
  - γ too high (>1.5): Hallucination metrics worsen (Table 13)
  - K too low with no filtering: Noisy preference pairs, slower convergence

- First 3 experiments:
  1. **Baseline sanity check**: Run IRIS on LLaVA-1.5-7B with SFT warm-up only (no preference rounds). Verify implicit reward correlation with hallucination metrics on 100 samples.
  2. **Ablation on L_cvp**: Train R1 with L_ctp only vs. L_ctp + L_cvp. Compare CHAIR scores to confirm visual preference term provides the primary grounding signal (Table 2).
  3. **Hyperparameter sweep**: Run R1–R2 with γ ∈ {0.3, 0.5, 0.7, 1.0} on a 1k subset. Validate that γ=0.7 is robust before full-scale training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the IRIS framework be adapted to effectively mitigate compositional hallucinations (e.g., incorrect counting or spatial relations) with the same efficacy as object-level hallucinations?
- Basis in paper: [explicit] Section 5.2 observes that improvements on MMHal-Bench are smaller because the benchmark emphasizes "compositional visual reasoning (e.g., counting and relations)," whereas the proposed RVG scoring primarily optimizes for "reducing hallucinated objects and attributes."
- Why unresolved: The current rectification strategy penalizes text-only priors to improve object grounding, but it is unclear if this implicit reward signal inherently captures complex relational semantics.
- What evidence would resolve it: Targeted evaluation on relation-heavy benchmarks (e.g., VL-Checklist) or an ablation study isolating IRIS performance on counting versus object existence tasks.

### Open Question 2
- Question: What determines the optimal stopping criteria for iterative self-alignment to prevent the observed degradation in general conversational capability?
- Basis in paper: [inferred] Tables 10 and 11 show a decline in LLaVA-Bench (in-the-Wild) accuracy at Round 3 (dropping to 53.2% for 7B) after peaking at Round 2, indicating potential over-optimization or "reward hacking" of the implicit signal.
- Why unresolved: The paper selects Round 2 as the final model empirically but does not define a theoretical limit or automated metric to detect when self-generated preference optimization begins to hurt general utility.
- What evidence would resolve it: Analysis of the correlation between the implicit reward magnitude and general capability scores to define a threshold for early stopping.

### Open Question 3
- Question: Does the reliance on implicit rewards for sifting generalize to architectures with different vision-language connectors or pre-training objectives?
- Basis in paper: [inferred] The experimental scope is restricted to the LLaVA-1.5 family (7B/13B) using CLIP ViT-L/14, leaving the universality of the reward calibration untested.
- Why unresolved: The validity of the implicit reward as a proxy for visual grounding may be sensitive to the specific alignment properties of the vision encoder and LLM connection.
- What evidence would resolve it: Applying the IRIS pipeline to models with distinct architectures (e.g., Qwen-VL or InternVL) to verify if the RVG sifting mechanism remains robust without external feedback.

## Limitations
- The framework's effectiveness depends on SFT warm-up quality, with poor calibration leading to unreliable implicit rewards
- The RVG mechanism's assumption that r_text > r_image indicates hallucination lacks direct controlled validation
- The optimal stopping criteria for iterative self-alignment remain empirical rather than theoretically derived
- Generalization to architectures beyond LLaVA-1.5 with different vision-language connectors remains untested

## Confidence

- **High Confidence**: The overall framework architecture and training pipeline are clearly specified and reproducible. The empirical results showing CHAIR score improvements (2.4 on AMBER, 8.66 on Object HalBench) are well-documented with appropriate baselines.
- **Medium Confidence**: The mechanism claims about continuous implicit rewards preserving preference information and RVG's effectiveness in separating visual grounding from language priors are supported by ablation studies but lack deeper theoretical validation or controlled isolation experiments.
- **Low Confidence**: The claim that on-policy pairs avoid distribution shift issues is theoretically justified but not empirically validated against carefully controlled off-policy comparisons. The assumption that r_text > r_image reliably indicates hallucination is supported by results but not directly tested.

## Next Checks

1. **RVG Mechanism Isolation**: Run controlled experiments comparing RVG (r_image - γ·max(0, r_text - r_image)) against simple difference scoring (r_image - r_text) and individual reward terms. Measure which component drives the CHAIR improvements and whether the penalty term consistently activates on known hallucination cases.

2. **Distribution Shift Validation**: Compare IRIS performance against an off-policy variant where preference pairs are sampled from a fixed external dataset (e.g., AMBER) rather than self-generated. Measure gradient magnitudes and convergence patterns to empirically validate the distribution shift claim.

3. **Negative Image Impact**: Run ablation studies with different negative image generation strategies: (a) no negative image, (b) random noise, (c) DDPM t=100, (d) DDPM t=500. Measure impact on CHAIR scores and preference pair quality to isolate the visual context term's contribution.