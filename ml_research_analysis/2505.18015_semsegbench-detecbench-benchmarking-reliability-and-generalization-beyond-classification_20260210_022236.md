---
ver: rpa2
title: 'SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond
  Classification'
arxiv_id: '2505.18015'
source_url: https://arxiv.org/abs/2505.18015
tags:
- miou
- attacks
- methods
- segmentation
- corruptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SEMSEGBENCH and DETEC BENCH, the first comprehensive
  benchmarking tools for evaluating reliability and generalization of semantic segmentation
  and object detection models beyond classification. The authors benchmark 76 segmentation
  models across four datasets and 61 object detectors across two datasets, evaluating
  their performance under diverse adversarial attacks and common corruptions.
---

# SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification

## Quick Facts
- arXiv ID: 2505.18015
- Source URL: https://arxiv.org/abs/2505.18015
- Authors: Shashank Agnihotri; David Schader; Jonas Jakubassa; Nico Sharei; Simon Kral; Mehmet Ege Kaçar; Ruben Weber; Margret Keuper
- Reference count: 40
- Key outcome: Introduces SEMSEGBENCH and DETEC BENCH, comprehensive tools for benchmarking reliability and generalization of semantic segmentation and object detection models using 76 segmentation models and 61 object detectors across multiple datasets, revealing no correlation between i.i.d. performance and robustness.

## Executive Summary
This paper presents the first comprehensive benchmarking tools for evaluating reliability and generalization of semantic segmentation and object detection models beyond standard classification metrics. The authors benchmark 76 segmentation models across four datasets and 61 object detectors across two datasets, evaluating their performance under diverse adversarial attacks and common corruptions. Their analysis reveals systematic weaknesses in state-of-the-art models, showing no strong correlation between i.i.d. performance and reliability or generalization ability. Notably, transformer-based backbones demonstrate better OOD robustness than CNN-based ones. The benchmarks provide 6,139 evaluations with multiple pre-logged metrics, enabling immediate further analysis without re-computation.

## Method Summary
The methodology uses the SEMSEGBENCH and DETEC BENCH frameworks built on top of `mmsegmentation` and `mmdetection` repositories. The approach evaluates pre-trained models using adversarial attacks (PGD, CosPGD) with $\ell_\infty$ ($\epsilon=8/255$) and $\ell_2$ ($\epsilon=64$) bounds across 20 iterations, and common corruptions including 15 2D and 8 3D variants at severity levels 1-5. Two novel metrics are introduced: Reliability Measure (ReM) capturing worst-case performance under attack, and Generalization Ability Measure (GAM) capturing worst-case performance under corruption. The evaluation process is designed to be computationally efficient while providing comprehensive robustness assessments across diverse model architectures.

## Key Results
- No correlation exists between i.i.d. performance and adversarial reliability, but strong correlation exists between performance and generalization for object detection
- Transformer-based backbones show significantly better OOD robustness than CNN-based backbones across both tasks
- Synthetic image corruptions strongly correlate with real-world distribution shifts, serving as reliable computational proxies
- The benchmarks provide 6,139 evaluations with pre-logged metrics enabling immediate analysis without re-computation

## Why This Works (Mechanism)

### Mechanism 1: Synthetic corruptions as real-world shift proxies
The paper posits that synthetic image corruptions can serve as computationally efficient proxies for real-world distribution shifts. The mechanism assumes that statistical properties of synthetic noise, blur, and weather patterns adequately simulate real-world sensor noise and environmental changes. This is validated by observing very strong positive correlation in performance between synthetic corruptions and the ACDC real-world dataset. The mechanism fails if models are architected to be robust to synthetic noise in ways that don't transfer to real-world artifacts.

### Mechanism 2: Transformer architectural advantage for OOD generalization
Vision Transformers process images as sequences of patches with global self-attention, creating feature representations more invariant to local, texture-heavy perturbations from common corruptions. This architectural inductive bias leads to better generalization compared to CNNs. The mechanism assumes this advantage is inherent to transformer architecture rather than pre-training data or parameter counts. It breaks if future adversarial attacks exploit unique vulnerabilities of self-attention mechanisms.

### Mechanism 3: Decoupling of i.i.d. performance from robustness
The optimization landscape for maximizing accuracy on fixed test sets encourages learning shortcuts and dataset-specific features that don't build robust representations needed for adversarial attacks or unseen distributions. This results in lack of correlation between i.i.d. accuracy and robustness. The mechanism assumes improving one objective doesn't accidentally improve another without explicit intervention. It breaks if training paradigms simultaneously optimize for both accuracy and robustness.

## Foundational Learning

- **Concept: Distribution Shift (i.i.d. vs. OOD)** - Central to the benchmark design; distinguishes between standard test data and out-of-distribution data (corruptions, real-world shifts). Quick check: Testing on sunny city images after training on sunny city images is i.i.d., while testing with synthetic fog is OOD.

- **Concept: Adversarial Robustness & L_p Norms** - Critical for understanding reliability measurements; different norms (ℓ∞ vs ℓ₂) represent different attack strategies. Quick check: ℓ∞-norm attacks (small perturbations to many pixels) are harsher tests than ℓ₂-norm attacks (larger perturbations to fewer pixels).

- **Concept: Benchmark Metrics (ReM and GAM)** - Novel metrics introduced to simplify analysis; ReM captures worst-case performance under attack while GAM captures worst-case performance under corruption. Quick check: A model with ℓ∞-ReM8 of 0.01 and GAM3 of 35.4 has near-zero robustness to adversarial attacks but moderate robustness to severe corruptions.

## Architecture Onboarding

- **Component map:** Models (architectures + backbones) -> Threat Models (attacks + corruptions) -> Metrics (standard task metrics + robustness metrics) -> Analysis Code (correlation scripts + plotting tools)

- **Critical path:** Evaluating a new model involves selecting a model-dataset pair, choosing threat model (attack for reliability or corruption for generalization), running evaluation to get metrics, computing ReM/GAM from logged data, and comparing against the 6,139 evaluation database to determine relative standing.

- **Design tradeoffs:** Compute vs. thoroughness (full evaluation is expensive, prioritize worst-case proxies), generic vs. specialized attacks (use generic attacks for consistency across architectures, though task-specific attacks may be more effective).

- **Failure signatures:** Catastrophic reliability failure (high i.i.d. performance but ReM near zero indicates no robustness), poor generalization (high i.i.d. performance but very low GAM indicates dataset-specific features).

- **First 3 experiments:** 1) Establish baseline with i.i.d. evaluation on standard model (e.g., Mask2Former with Swin-Tiny) to get baseline mIoU, 2) Measure generalization with OOD evaluation using 2DCommonCorruption at severity=3 and calculate GAM, 3) Measure reliability with adversarial attack evaluation using CosPGD under L2 norm constraint and calculate ReM.

## Open Questions the Paper Calls Out

- How do object detection-specific adversarial attacks alter reliability rankings compared to generic attacks currently used? The paper acknowledges that choice of general-purpose adversarial attacks may overlook task-specific vulnerabilities and lists exploring object detection-specific attacks as key future work.

- Can the benchmarking tools effectively evaluate the stability and convergence of adversarial training methods during the training process? The authors state they aim to support benchmarking for adversarial training methods directly within the frameworks as future work.

- How to make adversarial training more efficient for object detection? While not explicitly called out as an open question, the paper notes that adversarial training can improve reliability but is computationally expensive and primarily studied for classification.

## Limitations

- Computational constraints necessitate worst-case proxy metrics (ReM, GAM) that may not fully capture performance across all attacks and corruptions
- Analysis of model architectures is limited by relatively small sample size of available transformer-based models
- Strong correlation between synthetic and real-world distribution shifts requires further validation across diverse real-world datasets

## Confidence

**High confidence:** The systematic lack of correlation between i.i.d. performance and adversarial reliability is well-supported by extensive benchmark data (6,139 evaluations) and aligns with established literature on robustness.

**Medium confidence:** The claim that synthetic corruptions serve as reliable proxies for real-world shifts is based on ACDC dataset validation but needs broader real-world testing.

**Medium confidence:** The architectural analysis showing transformer advantages for OOD generalization is robust though limited by model availability.

## Next Checks

1. Validate synthetic-to-real correlation by testing the correlation between 2D Common Corruptions and real-world shifts on additional datasets beyond ACDC (e.g., BDD100K, WildDash) to assess generalizability.

2. Expand transformer analysis by including more transformer-based segmentation models in the benchmark to strengthen statistical conclusions about architectural advantages for OOD robustness.

3. Test task-specific attacks by evaluating task-specific adversarial attacks (e.g., SegPGD) on models where they are applicable to determine if they reveal different robustness patterns compared to generic attacks.