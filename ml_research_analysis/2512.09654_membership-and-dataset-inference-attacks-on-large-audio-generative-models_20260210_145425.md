---
ver: rpa2
title: Membership and Dataset Inference Attacks on Large Audio Generative Models
arxiv_id: '2512.09654'
source_url: https://arxiv.org/abs/2512.09654
tags:
- audio
- training
- membership
- inference
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Membership inference alone is ineffective at scale for large audio
  generative models, as per-sample membership signals are too weak when models are
  trained on large, diverse datasets. Dataset inference, which aggregates membership
  evidence across multiple samples, is more effective, successfully identifying training
  set participation with as few as 20-300 samples depending on the model.
---

# Membership and Dataset Inference Attacks on Large Audio Generative Models

## Quick Facts
- arXiv ID: 2512.09654
- Source URL: https://arxiv.org/abs/2512.09654
- Reference count: 40
- Key outcome: Dataset inference successfully identifies training set participation with 20-300 samples, while single-sample membership inference fails at scale for large audio models

## Executive Summary
This paper investigates membership inference attacks on large-scale audio generative models, revealing that while per-sample membership inference becomes ineffective as models scale up, dataset-level inference can still identify training set participation. The study systematically evaluates both diffusion and autoregressive architectures, demonstrating that statistical aggregation of weak per-sample signals enables detection of training data usage with far fewer samples than required by traditional approaches. These findings have significant implications for copyright protection and dataset accountability in audio generation.

## Method Summary
The authors evaluate membership inference attacks on three large audio generative models (TANGO, AudioLDM2, AudioGen, FIGARO) using both diffusion and autoregressive architectures. They implement per-sample MIA using architecture-specific features (loss/likelihood for ARMs, denoising dynamics for DMs) and dataset inference that aggregates these signals across multiple samples using Welch's t-test. The evaluation measures AUC scores for single-sample attacks and determines minimum sample counts needed for DI to achieve statistical significance at α = 0.01.

## Key Results
- Per-sample membership inference fails at scale: AUC scores approach 50% (random guessing) for large models like AudioGen (52.85%) and FIGARO (50.28%)
- Dataset inference succeeds: Requires only 20-300 samples to achieve statistical significance, depending on model size
- Architecture matters: TANGO (smaller training set) shows stronger per-sample signals (AUC ~70%) than larger models
- Detection efficiency varies: Sample requirements range from 20 for TANGO to 900 for AudioGen

## Why This Works (Mechanism)

### Mechanism 1: Per-Sample Signal Dilution at Scale
As training dataset size and diversity increase, model overfitting to individual samples decreases. The loss gap between training samples and unseen samples narrows, causing AUC scores to approach 50% (random guessing baseline). Smaller models (e.g., TANGO with 46k clips) may still show detectable signals (AUC ~70%) due to greater per-sample memorization.

### Mechanism 2: Statistical Aggregation Amplifies Weak Signals
Dataset inference aggregates weak per-sample membership evidence across multiple samples into a dataset-level statistic. By extracting MIA features for each sample and applying Welch's t-test comparing suspected members against known non-members, the aggregated evidence reaches significance (α = 0.01) even when individual signals are indistinguishable from noise.

### Mechanism 3: Architecture-Specific Leakage Patterns
Different generative architectures expose membership through distinct signal types—token probabilities for autoregressive models, denoising dynamics for diffusion models. Both exploit that training data fits the learned distribution more tightly, with the most discriminative signals typically appearing at intermediate timesteps for diffusion models.

## Foundational Learning

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: Core building block for understanding how individual training samples can be detected through loss/likelihood analysis
  - Quick check question: Given a model's loss on sample x is lower than threshold γ, does this prove x was in training? (Answer: No—provides probabilistic evidence only; false positives occur.)

- Concept: Diffusion vs. Autoregressive Generation
  - Why needed here: Determines which attack features are applicable; token-based attacks don't transfer directly to diffusion models
  - Quick check question: Which architecture operates on discrete tokens vs. continuous latent denoising? (Answer: ARMs use discrete tokens; DMs use continuous latent diffusion.)

- Concept: Statistical Hypothesis Testing (Welch's t-test)
  - Why needed here: DI relies on statistical significance rather than binary per-sample decisions
  - Quick check question: What does rejecting H₀ at α = 0.01 mean in the DI context? (Answer: <1% probability the observed score difference occurred by chance if P wasn't in training.)

## Architecture Onboarding

- Component map: Data Preparation -> Feature Extraction -> Score Computation -> Statistical Testing

- Critical path:
  1. Obtain model with documented train/test splits (essential for creating valid U sets)
  2. Select architecture-appropriate MIA feature suite
  3. Extract features on both P and U sets
  4. Train scoring model on disjoint control splits (DMs) or compute normalized sum (ARMs)
  5. Apply t-test; report minimum P required for significance

- Design tradeoffs:
  - More samples in P → Higher detection power but requires more suspected data
  - More MIA features → Better coverage but increased computational cost
  - Strict α threshold → Fewer false positives but may miss weak leakage

- Failure signatures:
  - AUC ≈ 50% across all attacks → Model too large/well-regularized for single-sample MIA
  - DI requires >1000 samples → Weak aggregation signal; may need better features
  - P and U distribution mismatch → Invalid test; ensure same source distribution

- First 3 experiments:
  1. Replicate threshold-based loss attack on TANGO vs. AudioLDM2 to observe scale effect on AUC
  2. Run DI with varying |P| sizes (20, 50, 100, 300) to measure sample efficiency curve
  3. Test cross-architecture feature transfer (e.g., apply ARM features to DM) to confirm architecture specificity

## Open Questions the Paper Calls Out

### Open Question 1
Can dataset inference methods be improved to require fewer than 300 samples, making them practical for individual artists who typically possess limited works? Current DI requires 300-900 samples for most models studied, exceeding what typical individual creators can provide.

### Open Question 2
What factors explain the dramatic variance in DI sample requirements across models (20 for TANGO vs. 900 for AudioGen)? The paper notes TANGO's smaller training set may increase overfitting but does not systematically investigate the causes.

### Open Question 3
How robust is dataset inference when the suspect and non-member sets violate the i.i.d. assumption due to distributional mismatch? Real-world auditing scenarios may involve artists whose work distribution differs from available reference data.

## Limitations

- Black-box access assumption: The attacks assume white-box access to model internals (logits, intermediate denoising states), which may not reflect realistic scenarios where only generated outputs are available
- Architecture scope: The study focuses on specific model architectures and may not generalize to other generative approaches like GANs or flow-based models
- U set construction: Creating valid non-member sets from held-out test splits assumes these truly represent non-members, which can be challenging in practice

## Confidence

**High Confidence:** The finding that per-sample MIA fails at scale (AUC near 50%) is well-supported by empirical results across multiple models. The statistical framework for DI (Welch's t-test) is standard and appropriately applied.

**Medium Confidence:** The minimum sample thresholds for DI (20-900 samples) are empirically derived but may vary with different datasets, model architectures, or feature engineering approaches.

**Low Confidence:** The assertion that DI could enable "dataset accountability" lacks exploration of real-world constraints like partial dataset knowledge, adversarial countermeasures, or computational burden.

## Next Checks

1. Test whether dataset inference remains effective when only model outputs (generated audio samples) are accessible, not internal model states
2. Validate whether DI trained on one dataset's statistics can detect training set membership when applied to models trained on different but related datasets
3. Assess how defensive techniques like differential privacy training, data augmentation, or model regularization affect both per-sample MIA and dataset inference effectiveness