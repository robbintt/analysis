---
ver: rpa2
title: Explaining Machine Learning Predictive Models through Conditional Expectation
  Methods
arxiv_id: '2601.07313'
source_url: https://arxiv.org/abs/2601.07313
tags:
- feature
- features
- observation
- dataset
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the interpretability challenges of complex
  machine learning models by introducing Multivariate Conditional Expectation (MUCE),
  a model-agnostic local explainability technique. MUCE extends Individual Conditional
  Expectation (ICE) by exploring multivariate feature interactions in the neighborhood
  of a given observation at inference time.
---

# Explaining Machine Learning Predictive Models through Conditional Expectation Methods

## Quick Facts
- arXiv ID: 2601.07313
- Source URL: https://arxiv.org/abs/2601.07313
- Reference count: 38
- Primary result: Introduces MUCE, a model-agnostic local explainability technique that extends ICE to capture multivariate feature interactions with stability and uncertainty indices for prediction confidence assessment

## Executive Summary
This work addresses the interpretability challenges of complex machine learning models by introducing Multivariate Conditional Expectation (MUCE), a model-agnostic local explainability technique. MUCE extends Individual Conditional Expectation (ICE) by exploring multivariate feature interactions in the neighborhood of a given observation at inference time. The method generates graphical explanations showing local prediction evolution while two quantitative indices—stability and uncertainty—assess model reliability. Uncertainty is further decomposed into uncertainty+ and uncertainty- to capture asymmetric effects.

## Method Summary
MUCE is a model-agnostic local explainability technique that builds on Individual Conditional Expectation (ICE) by exploring multivariate feature interactions at inference time. During training, the method stores minimum and maximum values per feature to enable single-observation ICE curve generation without requiring the full training dataset. The core MUCE algorithm uses a greedy hill-climbing approach to iteratively explore perturbed observations, tracing upper and lower bounds of prediction variation when features change jointly. Two quantitative indices—stability and uncertainty—are computed to assess prediction reliability, with uncertainty further decomposed into directional components (uncertainty+/uncertainty-) to capture asymmetric sensitivity.

## Key Results
- MUCE effectively captures complex local model behavior that ICE alone cannot detect, particularly for multivariate feature interactions
- Stability index correlates with distance to decision boundaries, with values near 0 indicating prediction instability
- Uncertainty indices provide meaningful insights into prediction confidence, with decomposition revealing asymmetric sensitivity to feature perturbations
- Method successfully identifies prediction instability near class boundaries and detects feature interactions in both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modified ICE enables single-observation explainability at inference time without requiring access to the full training dataset
- Mechanism: During training, store min/max bounds per feature and construct a configurable grid. At inference, substitute feature values one-at-a-time while holding others constant, generating a single ICE curve for any new observation
- Core assumption: Feature value ranges observed during training reasonably bound the space of plausible values at inference
- Evidence anchors: Abstract mentions MUCE extends ICE for multivariate exploration at inference time; Section 2.2 states minimum/maximum values are stored during training

### Mechanism 2
- Claim: MUCE's greedy hill-climbing exploration captures multivariate feature interactions that univariate ICE cannot detect
- Mechanism: Starting from an observation, iteratively perturb features (excluding the target feature) by ±ε within a stability range. At each iteration, select observations producing max/min predictions and continue exploration
- Core assumption: Local model behavior is sufficiently smooth that greedy selection finds representative extremes
- Evidence anchors: Abstract emphasizes multivariate grid exploration; Section 2.3 describes t₁=5 initial repetitions to extend influence region; Section 3.3 demonstrates capturing multidimensional interactions

### Mechanism 3
- Claim: Stability and uncertainty indices quantify local prediction confidence and reveal asymmetric sensitivity to feature perturbations
- Mechanism: Stability = 1 − (max−min ICE predictions) within stability range. Uncertainty = mean gap between MUCE max/min curves across iterations. Decomposing into uncertainty+/uncertainty- captures directional asymmetry
- Core assumption: Posterior probabilities are bounded [0,1]; stability range meaningfully reflects realistic measurement variation
- Evidence anchors: Abstract mentions stability and uncertainty indices for model reliability; Section 2.5 formalizes calculations; Section 3.2 demonstrates stability index signaling instability

## Foundational Learning

- **Concept: Individual Conditional Expectation (ICE) plots**
  - Why needed here: MUCE builds directly on ICE; understanding how ICE varies one feature while holding others constant is prerequisite to grasping why multivariate extension matters
  - Quick check question: Given a 3-feature model, if you vary F1 while freezing F2 and F3, what does the resulting curve represent?

- **Concept: Hill-climbing / greedy search**
  - Why needed here: MUCE uses greedy selection at each iteration to trace max/min predictions; understanding trade-offs (speed vs. global optimality) clarifies method limitations
  - Quick check question: Why might a greedy approach fail to find the true global maximum of a function with multiple local optima?

- **Concept: Stability ranges and perturbation magnitude (δ, ε)**
  - Why needed here: The method's local validity hinges on defining meaningful neighborhoods; δ too narrow = optimistic confidence, δ too wide = excessive uncertainty
  - Quick check question: If δ_i is set to 5% of feature range but the true sensor measurement error is 15%, what happens to the reliability of the stability index?

## Architecture Onboarding

- **Component map:**
  FitICE(X, n) -> ComputeICE(G, x, F_i, model) -> GenerateCandidates(x, C, ε) -> PerformMuceSearch(...) -> Stability/Uncertainty index calculators

- **Critical path:**
  1. During training: call FitICE once per model to store bounds
  2. At inference: for each observation to explain, compute ICE curves for each feature of interest
  3. Run MUCE exploration per feature to capture multivariate interactions
  4. Calculate stability and uncertainty indices from ICE/MUCE outputs
  5. Generate visualizations (curves for continuous, bars for categorical/binary)

- **Design tradeoffs:**
  - Greedy search (fast, may miss global extremes) vs. exhaustive grid search (accurate, exponential cost)
  - Larger stability ranges capture more model behavior but may include unrealistic observations
  - More iterations (N) extend exploration distance but increase computation and risk leaving local neighborhood
  - Grid granularity (n) improves curve precision but linearly increases calls to model.predict()

- **Failure signatures:**
  - Stability ≈ 1.0 but uncertainty ≈ 0.5+ → single-feature stable but joint perturbations cause large swings (interaction effects present)
  - uncertainty+ >> uncertainty− (or vice versa) → asymmetric boundary proximity; model sensitive in one direction only
  - MUCE max/min curves converge to same values → either model is locally flat or greedy search stuck in plateau
  - Categorical features with high cardinality → k-NN distance computation becomes expensive and may leak information

- **First 3 experiments:**
  1. Replicate synthetic 2D cross-shaped dataset experiment: train XGBoost, select observations TP0–TP3, verify stability indices correlate with distance to decision boundary
  2. Ablation on stability range: vary δ from 1% to 20% of feature range for observations near boundaries; plot how stability/uncertainty indices change to calibrate appropriate δ for your domain
  3. Compare MUCE uncertainty estimates against ground-truth boundary distance on synthetic 3D ellipsoid dataset: correlate uncertainty values with actual Euclidean distance to ellipsoid surface

## Open Questions the Paper Calls Out

- **How can the stability and uncertainty indices be extended to regression tasks, survival analysis, and risk probability assessment where prediction ranges are not bounded between 0 and 1?**
  - Basis: The paper explicitly states these indices cannot be directly used in regression tasks, survival analysis, or risk probability assessment due to unbounded prediction ranges

- **How does MUCE compare to established local explainability methods like LIME and SHAP in terms of explanation fidelity, computational cost, and user trust?**
  - Basis: The paper identifies this as a prospective line of research for comparative analysis

- **How can the greedy exploration strategy in MUCE be improved to avoid convergence to local maxima or minima during the search process?**
  - Basis: The paper acknowledges that greedy approaches may reach local maximum or minimum, limiting search effectiveness

## Limitations
- Greedy hill-climbing may converge to local optima rather than global extremes, particularly in highly non-convex decision boundaries
- Reliance on stored min/max bounds assumes feature distributions remain stable between training and inference
- No quantitative comparison with established methods (LIME, SHAP) on identical datasets is provided to validate MUCE's effectiveness

## Confidence

- **High Confidence**: Modified ICE implementation for single-observation explainability at inference time
- **Medium Confidence**: Stability and uncertainty indices provide meaningful local confidence measures with limited empirical validation
- **Low Confidence**: MUCE's ability to capture all relevant multivariate interactions through greedy search

## Next Checks
1. **Ablation on stability range**: Systematically vary δ from 1% to 20% of feature range for observations at varying distances from decision boundaries. Plot stability/uncertainty index behavior to identify optimal δ calibration for your domain.
2. **Comparison with ground truth**: On synthetic 3D ellipsoidal dataset, correlate MUCE uncertainty estimates with actual Euclidean distance to class boundary. Validate whether high uncertainty reliably indicates boundary proximity.
3. **Sensitivity to model complexity**: Train XGBoost models with varying max_depth and n_estimators. Evaluate whether MUCE consistently captures interactions across model complexities, or if performance degrades for deeper trees with more complex decision boundaries.