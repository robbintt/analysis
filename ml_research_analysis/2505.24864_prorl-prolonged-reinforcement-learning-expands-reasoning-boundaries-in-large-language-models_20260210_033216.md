---
ver: rpa2
title: 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large
  Language Models'
arxiv_id: '2505.24864'
source_url: https://arxiv.org/abs/2505.24864
tags:
- reasoning
- pass
- final
- smoothed
- deepseek-r1-distill-qwen-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether reinforcement learning can genuinely
  expand a language model's reasoning boundaries or simply amplify existing capabilities.
  The authors introduce ProRL, a prolonged RL training methodology incorporating KL
  divergence penalties, reference policy resets, and diverse task exposure to maintain
  training stability over extended periods.
---

# ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models

## Quick Facts
- **arXiv ID:** 2505.24864
- **Source URL:** https://arxiv.org/abs/2505.24864
- **Reference count:** 40
- **Primary result:** 1.5B model trained with ProRL achieves +14.7% on math, +13.9% on coding, +54.8% on logic puzzles, +25.1% on STEM, and +18.1% on instruction following benchmarks over base model

## Executive Summary
This work investigates whether reinforcement learning can genuinely expand a language model's reasoning boundaries or simply amplify existing capabilities. The authors introduce ProRL, a prolonged RL training methodology incorporating KL divergence penalties, reference policy resets, and diverse task exposure to maintain training stability over extended periods. Their 1.5B model, Nemotron-Research-Reasoning-Qwen-1.5B, trained via ProRL, achieves substantial improvements over the base model: +14.7% on math, +13.9% on coding, +54.8% on logic puzzles, +25.1% on STEM, and +18.1% on instruction following benchmarks. Most significantly, ProRL enables performance gains on tasks where the base model fails entirely, demonstrating expansion of reasoning boundaries. The improvements correlate with initial base model competence and training duration, with extended training producing novel reasoning trajectories and sustained gains on challenging tasks. The results challenge prior assumptions about RL limitations and establish that prolonged training with appropriate techniques can meaningfully expand reasoning capabilities beyond pretraining distributions.

## Method Summary
ProRL employs GRPO with DAPO enhancements, using decoupled clipping bounds (ε_low=0.2, ε_high=0.4), dynamic sampling filtering for accuracy=1 or 0, KL divergence penalty, and reference policy resets. The training uses rollout temperature=1.2, n=16 samples per prompt, and context windows of 8096→16k tokens. Training runs sequentially with resets at validation degradation, using batch size=256, mini-batch=64, AdamW with LR=2×10⁻⁶ across 4×8 H100-80GB nodes (~16k GPU hours). The approach trains on 136K examples across five domains: math, code, STEM, logical puzzles, and instruction following, with verifiable rewards and extensive hyperparameter tuning for stability.

## Key Results
- Nemotron-Research-Reasoning-Qwen-1.5B achieves 91.7% on MATH benchmark (vs 75.9% base)
- Model reaches 100% on boxnet (previously 0% base) at k=128, demonstrating boundary expansion
- Sustained pass@k improvements across extended training periods, with gains correlating to base model competence
- Three regimes observed: Diminish (math), Plateau (GPQA), Sustained (codecontests) based on task characteristics

## Why This Works (Mechanism)

### Mechanism 1
KL divergence regularization combined with periodic reference policy resets enables stable prolonged training by preventing entropy collapse while allowing continued policy divergence. The KL penalty maintains output diversity and prevents premature mode collapse. When the KL term begins dominating the loss (causing diminishing updates), hard-resetting π_ref to a recent πθ snapshot reinitializes optimizer states and allows further improvement while preserving regularization benefits. Starting from a well-initialized checkpoint (already capable of CoT outputs) means retaining KL penalty is beneficial, contrary to works that remove it for untrained base models.

### Mechanism 2
Decoupled clipping bounds (ε_low ≠ ε_high) with "clip-higher" promotes exploration of previously unlikely tokens, enabling discovery of novel reasoning paths absent from the base model's distribution. Setting ε_high = 0.4 > ε_low = 0.2 allows larger probability increases for low-probability tokens during policy updates. This asymmetric clipping uplifts unlikely outputs, maintaining diversity and enabling exploration beyond the initial policy's modes. The base model's distribution contains underexplored regions that standard symmetric clipping would suppress.

### Mechanism 3
RL expands reasoning boundaries most effectively on tasks where the base model initially struggles, with gains correlating to training duration and base model competence. For tasks with low initial pass@k, the policy has more unexplored high-reward regions. Extended training allows gradual population of these regions. For high-competence tasks, RL primarily sharpens existing modes rather than discovering new strategies. Verifiable reward signals must exist and task difficulty distribution must allow meaningful gradient signal (not all accuracy=0 or accuracy=1).

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** ProRL builds on GRPO, which replaces PPO's value model with group-based baseline estimation. Understanding how advantages are computed from group rewards is essential for debugging training dynamics.
  - **Quick check question:** Given 16 sampled responses with rewards [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], what is the advantage for a response with reward 1?

- **Concept: Entropy Collapse in Policy Gradient Methods**
  - **Why needed here:** ProRL's core motivation is mitigating entropy collapse during extended training. Without understanding why peaked distributions harm exploration, the KL regularization and clip-higher rationale is opaque.
  - **Quick check question:** Why does low policy entropy reduce the effective learning signal in GRPO specifically (hint: consider how group advantages are estimated)?

- **Concept: pass@k as Reasoning Boundary Metric**
  - **Why needed here:** The paper's central claim uses pass@k curves to distinguish "amplifying existing patterns" (pass@1↑, pass@k plateau/decline) from "expanding boundaries" (pass@k continues improving with k).
  - **Quick check question:** If a model's pass@1 = 0.6 and pass@128 = 0.62, what does this suggest about its reasoning diversity compared to pass@1 = 0.4, pass@128 = 0.9?

## Architecture Onboarding

- **Component map:** Prompt → Rollout Generator (n=16, temp=1.2, max_len=8k) → Reward Verifier (domain-specific) → GRPO Advantage Computation (group baseline) → Policy Update with KL penalty (β·D_KL(π_θ || π_ref)) → [Periodic] Reference Reset (π_ref ← π_θ, optimizer reset)

- **Critical path:** Rollout generation → Reward computation → Advantage normalization → KL-regularized policy update. The reward verifier is domain-specific and must handle timeout/format errors gracefully (reward=0 for failures).

- **Design tradeoffs:**
  - High temperature (1.2) maintains exploration but increases compute and may produce low-quality samples
  - KL penalty prevents collapse but limits policy divergence; resets trade stability for exploration range
  - 8k context limits overthinking but may truncate valid long reasoning; 16k in final stage helps hard tasks

- **Failure signatures:**
  - Entropy declining monotonically despite interventions → KL coefficient too low or reset frequency too rare
  - Response length exploding with repetition → add termination penalty (as in Runs 4-5)
  - Validation accuracy=0 for extended period → task may be OOD or reward signal broken; check verifier
  - pass@1 improving but pass@16/128 declining → distribution narrowing; increase ε_high or temperature

- **First 3 experiments:**
  1. **Baseline replication:** Train GRPO without KL penalty or resets on a single domain (math) for 500 steps. Monitor entropy and pass@1/pass@16. Expect early entropy collapse to confirm problem exists.
  2. **Ablation: KL-only vs. KL+resets:** Compare training with KL penalty but no resets vs. full ProRL. Measure maximum stable training duration and final pass@k curves on a held-out validation set.
  3. **Task competence correlation:** Select 3 tasks with low, medium, and high base model pass@128. Train ProRL on each separately and verify the negative correlation claim (weaker base → larger gains). Use this to prioritize future task acquisition.

## Open Questions the Paper Calls Out

### Open Question 1
Does the ProRL methodology scale efficiently to models significantly larger than 1.5B parameters? The study only validates the method on a 1.5B parameter model, and the interaction between prolonged RL steps and much larger parameter spaces remains untested. Replicating the ProRL training recipe on 7B and 70B models while monitoring training stability and performance gains per compute unit would resolve this.

### Open Question 2
What is the optimal scheduling strategy for reference policy resets to maximize performance stability? The paper relies on reactive monitoring for resets rather than defining a theoretically grounded or automated optimal schedule. A comparative study of reset triggers (e.g., fixed intervals vs. KL threshold vs. validation loss plateau) measuring final pass@k and training efficiency would resolve this.

### Open Question 3
Does ProRL-induced generalization transfer to domains structurally different from the five training domains? While the model generalized to OOD tasks like "boxnet," these tasks may share latent features with the training distribution, leaving broader generalization unproven. Evaluating the ProRL-trained model on diverse OOD benchmarks outside the current scope, such as multimodal reasoning or causal inference tasks, would resolve this.

## Limitations
- KL coefficient tuning: The paper specifies KL divergence regularization as a key mechanism but does not report the β coefficient value used in training, which significantly affects the balance between policy stability and exploration.
- Limited scope of boundary expansion claims: The boundary expansion demonstration relies heavily on performance gains on tasks where the base model fails completely, but doesn't thoroughly investigate whether these gains represent genuine novel reasoning strategies or simply improved sampling/verification.
- Correlation findings without statistical validation: The claim that reasoning improvement correlates negatively with base model competence is supported by Figure 4 but lacks statistical significance testing and could be influenced by task difficulty calibration or reward signal quality.

## Confidence
**High confidence:** The core empirical results showing ProRL's performance improvements over the base model on established benchmarks (MATH, Codeforces, GPQA, etc.) are well-documented with multiple pass@k metrics. The methodology for measuring reasoning boundary expansion through pass@k curves is clearly specified and reproducible.

**Medium confidence:** The mechanism explanations for why ProRL works (KL regularization preventing collapse, clip-higher enabling exploration, task competence correlation) are logically consistent with the results but rely on indirect evidence. The paper provides behavioral observations rather than mechanistic proof.

**Low confidence:** The boundary expansion claims for out-of-distribution tasks (boxnet, redial) represent the most significant theoretical contributions but have the weakest empirical support. With only one data point per task and no ablation studies showing what aspects of ProRL enable these gains, these findings require independent verification.

## Next Checks
1. **KL coefficient ablation study:** Train ProRL with three different β values (0.01, 0.1, 1.0) on the same task sequence and compare: a) maximum stable training duration before validation degradation, b) final pass@k curves, and c) entropy trajectories.

2. **Reset frequency controlled experiment:** Implement a version of ProRL with fixed reset intervals (every 100, 200, 500 steps) versus adaptive resets based on validation performance. Compare the total training duration achievable and the quality of final policy distributions.

3. **Novelty analysis of reasoning trajectories:** For tasks where ProRL shows boundary expansion (particularly boxnet), extract and analyze the actual reasoning paths generated. Compare token-level diversity, strategy diversity, and solution patterns against the base model and against known solution templates.