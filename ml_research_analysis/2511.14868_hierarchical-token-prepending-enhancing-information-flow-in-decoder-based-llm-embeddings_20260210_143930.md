---
ver: rpa2
title: 'Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based
  LLM Embeddings'
arxiv_id: '2511.14868'
source_url: https://arxiv.org/abs/2511.14868
tags:
- token
- mean
- embedding
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models suffer from restricted backward information\
  \ flow due to their causal attention mechanism, which degrades embedding quality\u2014\
  especially for long documents. Hierarchical Token Prepending (HTP) mitigates this\
  \ by partitioning text into semantic blocks, prepending block-level summary tokens\
  \ to subsequent blocks for multi-path backward flow, and replacing last-token pooling\
  \ with mean-pooling to prevent over-squashing."
---

# Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings

## Quick Facts
- arXiv ID: 2511.14868
- Source URL: https://arxiv.org/abs/2511.14868
- Reference count: 40
- Large language models suffer from restricted backward information flow due to their causal attention mechanism, which degrades embedding quality—especially for long documents.

## Executive Summary
Decoder-only LLMs struggle with embedding generation because their causal attention mechanism restricts backward information flow, leading to poor representations for long documents. Hierarchical Token Prepending (HTP) addresses this by partitioning text into semantic blocks, prepending block-level summary tokens to subsequent blocks for multi-path backward flow, and replacing last-token pooling with mean-pooling to prevent over-squashing. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, with particular improvements in long-context settings, and can enhance both zero-shot and finetuned models.

## Method Summary
HTP is a training-free method that enhances text embeddings from decoder-only LLMs by improving backward information flow. The method partitions input text into semantic blocks (typically sentences), prepends block-level summary tokens to subsequent blocks, and uses mean-pooling instead of last-token pooling. Between transformer layers, HTP dynamically rewires hidden states to copy final token representations to summary token positions, creating hierarchical backward dependencies. This allows tokens to attend to summaries of all subsequent blocks through the causal attention mechanism, while mean-pooling distributes representational importance across all tokens to mitigate over-squashing.

## Key Results
- HTP achieves 62.84 average NDCG@10 on BEIR retrieval datasets vs 61.46 for vanilla embeddings (1.38 point improvement)
- On LongEmbed benchmarks, HTP scores 58.70 average NDCG@10 vs 56.65 for baselines (2.05 point improvement)
- HTP outperforms all baselines on 11/11 retrieval datasets and 24/30 general embedding benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical summary tokens enable multi-path backward information flow without over-compression. Input text is partitioned into semantic blocks (typically sentences). Each block's final token—often sentence-ending punctuation that naturally aggregates preceding information—is copied to a local summary token (`<PST>m`). These local summaries are then propagated to global summary tokens (`<B-PST>m`) at the sequence start, allowing any token to attend to summaries of all subsequent blocks through the causal attention mechanism. Sentence boundaries represent natural semantic units, and final tokens within sentences effectively capture local context.

### Mechanism 2
Mean-pooling readout mitigates over-squashing by distributing representational importance across all tokens rather than compressing into a single position. Theoretical analysis shows last-token embedding sensitivity depends on a single attention path entry, which can diminish rapidly with depth. Mean-pooling aggregates the entire column, making it topologically depth-agnostic and more robust to information attenuation. The mixing matrix A captures aggregate attention+residual flow across layers with row-stochastic properties preserved through products.

### Mechanism 3
Rewiring operations between layers establish backward dependencies without architecture changes. Two rewiring functions execute between layers: (1) `flocal` copies each block's final hidden state to its `<PST>` placeholder, (2) `fglobal` copies `<PST>` states to `<B-PST>` tokens. This happens dynamically before self-attention at each layer, allowing the causal mask to naturally permit attention from earlier tokens to these prepended summaries. Placeholder token embeddings can be randomly initialized for layer 1 without destabilizing representations.

## Foundational Learning

- **Causal attention masking in decoder-only Transformers**: HTP is designed specifically to work within the constraints of causal attention—it does not modify the attention mechanism itself but exploits its properties through strategic token placement. Understanding that tokens can only attend to positions ≤ their own index is essential for grasping why prepending summaries works. Given tokens at positions [1, 2, 3, 4, 5] with standard causal masking, which positions can token 3 attend to?

- **Over-squashing in sequential models**: The paper's theoretical contribution hinges on sensitivity analysis showing how information from early tokens gets compressed/attenuated when forced through narrow bottlenecks (single last token vs. distributed mean). This is the problem HTP's readout choice addresses. In a 24-layer causal transformer, would you expect the gradient signal from a token at position 5 to reach position 500's representation more strongly via last-token or mean-pooling readout, and why?

- **Semantic segmentation vs. fixed-length partitioning**: HTP's ablation demonstrates that sentence-based partitioning outperforms fixed token intervals, revealing that the method relies on linguistic structure rather than arbitrary chunking. This informs practical implementation decisions. Why might a 50-token sentence yield a better summary token than the first 50 tokens of a 200-token paragraph spanning multiple topics?

## Architecture Onboarding

- **Component map:**
Input Text → [Spacy Segmentation] → Sentence Blocks → [Token Augmentation] → Add <B-PST>1...M at front + <PST>m before each block → For each layer ℓ > 1: → flocal: Copy block-end positions → <PST> positions → fglobal: Copy <PST> positions → <B-PST> positions → Standard transformer attention (causal mask unchanged) → [Early Exit at Layer L'] → Mean-pool across all positions → Output Embedding

- **Critical path:**
1. Correct sentence boundary detection (Spacy parser dependency)
2. Placeholder position calculation must account for added tokens when mapping end(Sm) → <PST>m
3. Early exit layer selection (model-specific: Mistral uses third-to-last, Gemma2 uses second-to-last)

- **Design tradeoffs:**
- **K (sentences per block):** Low K preserves detail but creates more tokens; high K improves coherence for long docs but may elide fine-grained information. Paper recommends K=1 as default but shows K≈8-12 better for long documents.
- **Memory overhead:** ~10-20% increase over vanilla (1.12× for Mistral, 1.18× for Gemma2) vs. 4× for Echo Embedding
- **Task suitability:** Excels at retrieval/clustering, underperforms on STS—choose based on use case

- **Failure signatures:**
- STS tasks with short sentences: HTP scores 10-15 points lower than PromptEOL methods
- Very long documents with K=1: Excessive `<B-PST>` tokens cause OOD behavior
- Incorrect layer configuration: Prepending at wrong layers disrupts learned representations

- **First 3 experiments:**
1. **Sanity check:** Run HTP on NFCorpus (short docs) and SummScreenFD (long docs) with K=1; verify NFCorpus improves modestly while SummScreenFD shows larger gains, matching Figure 6 patterns.
2. **Ablation:** Compare sentence-based vs. fixed-token partitioning on SciFact; expect ~2-4 NDCG@10 improvement with sentence boundaries.
3. **Integration test:** Apply HTP to a finetuned model (e.g., NV-Embed-v2 checkpoint) on FiQA2018; verify ~1-2 point improvement persists, confirming orthogonality to training-based gains.

## Open Questions the Paper Calls Out

- **Interaction with diverse model architectures and training paradigms**: The authors explicitly state that "a broader investigation is needed to fully understand its interaction with diverse model architectures and training paradigms." The study primarily validates HTP on a limited set of decoder-only models (Mistral, Gemma, Qwen) and a single finetuned model (NV-Embed-v2). Empirical evaluation across a wider variety of model families and training objectives would resolve this.

- **Mechanistic roles of backward dependency and token prepending**: The paper notes that "A deeper look into the mechanisms of backward dependency and token prepending is also warranted." While the paper provides theoretical bounds on over-squashing, it does not deeply analyze the internal attention dynamics or representational shifts caused by the hierarchy. Mechanistic interpretability studies comparing standard token prepending to the hierarchical approach would help.

- **Adaptive strategy for optimal block size (K)**: Section 5.4 demonstrates that the optimal value for K varies significantly between short and long documents, suggesting that a static K is suboptimal. The paper uses a fixed value (K=1) or manual tuning, yet concludes that the optimal strategy is "dependent on document length and task granularity." A dynamic algorithm that adjusts the block partition size based on input length or semantic density would resolve this.

## Limitations

- Performance gains vary significantly with document length; very long documents may require adaptive K strategies to avoid excessive `<B-PST>` tokens causing OOD behavior
- HTP underperforms on semantic textual similarity tasks (53.64 vs 68.10 for PromptEOL methods), limiting its applicability to mixed-use embedding systems
- Different models require different layer configurations for optimal performance, suggesting sensitivity to model architecture and training history that may not generalize

## Confidence

**High confidence**: The core mechanism of hierarchical token prepending and mean-pooling readout is well-supported by both theoretical analysis and extensive empirical validation across 11 retrieval datasets. The performance improvements on BEIR (1.38 average NDCG@10 gain) and LongEmbed (2.05 average NDCG@10 gain) are consistent and statistically significant.

**Medium confidence**: The theoretical justification for mean-pooling as a solution to over-squashing is compelling, but relies on assumptions about the mixing matrix A's properties that aren't fully validated empirically. The paper shows mean-pooling outperforms last-token pooling in practice, but doesn't directly measure the information flow bottlenecks the theory predicts.

**Low confidence**: The optimal K selection strategy for different document lengths lacks a principled approach. While the paper demonstrates that K=1 works well for typical BEIR documents and larger K helps for very long documents, it doesn't provide a method for automatically determining K based on input characteristics or explain the theoretical basis for why specific K values work better at different lengths.

## Next Checks

1. **Sequence length boundary testing**: Systematically evaluate HTP performance on documents of increasing length (100, 500, 1000, 2000+ tokens) using both K=1 and adaptive K strategies. Measure whether performance degradation correlates with the number of `<B-PST>` tokens as predicted, and test whether the proposed K scaling (8-12 for long docs) actually recovers performance.

2. **Mixed-task embedding pipeline**: Design an experiment that routes short texts (potential STS inputs) through a different embedding pathway than long documents (retrieval inputs). Compare end-to-end performance on a benchmark containing both retrieval and STS tasks against pure HTP or pure PromptEOL approaches to quantify the practical impact of HTP's STS limitations.

3. **Layer configuration transferability**: Apply the exact layer configurations from Mistral-7B and Gemma2 to a different decoder-only model (e.g., Llama-3 or Qwen2) without additional tuning. Measure performance degradation to assess how sensitive HTP is to model-specific architectural differences, and test whether the paper's layer selection methodology can be replicated with limited computational resources.