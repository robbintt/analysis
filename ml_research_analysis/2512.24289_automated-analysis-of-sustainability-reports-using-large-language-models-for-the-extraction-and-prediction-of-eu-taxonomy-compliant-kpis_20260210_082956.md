---
ver: rpa2
title: 'Automated Analysis of Sustainability Reports: Using Large Language Models
  for the Extraction and Prediction of EU Taxonomy-Compliant KPIs'
arxiv_id: '2512.24289'
source_url: https://arxiv.org/abs/2512.24289
tags:
- company
- report
- activity
- activities
- taxonomy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automating EU Taxonomy compliance
  reporting, which requires identifying economic activities and extracting quantitative
  KPIs from corporate reports. The authors introduce a novel, structured dataset of
  190 companies with ground-truth economic activities and KPIs, then systematically
  evaluate large language models (LLMs) on four tasks: multiclass activity prediction,
  binary activity classification, KPI regression, and multi-step agentic workflows.'
---

# Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs

## Quick Facts
- **arXiv ID:** 2512.24289
- **Source URL:** https://arxiv.org/abs/2512.24289
- **Authors:** Jonathan Schmoll; Adam Jatowt
- **Reference count:** 40
- **Primary result:** LLMs perform moderately well on qualitative activity classification but fail comprehensively at quantitative KPI regression.

## Executive Summary
This paper introduces a novel benchmark dataset for automating EU Taxonomy compliance reporting, focusing on extracting economic activities and quantitative KPIs from corporate sustainability reports. The authors systematically evaluate large language models across four tasks: multiclass activity prediction, binary activity classification, KPI regression, and agentic workflows. Results reveal a paradox where concise metadata outperforms full reports for activity classification, and zero-shot models fail completely at KPI regression with negative R² values. The study concludes that while LLMs cannot fully automate compliance reporting, they can serve as assistive tools for human experts.

## Method Summary
The authors created a ground-truth dataset of 190 companies by extracting taxonomy activities and KPIs from annual reports, then using an LLM extraction pipeline with self-verification and human review. They evaluated multiple commercial LLMs (Gemini, Llama, Mistral, Gemma) on zero-shot inference tasks using structured metadata, full reports, and taxonomy sections. The evaluation included multi-label classification metrics (F1, precision, recall), regression metrics (R², MAE), and calibration analysis (ECE). Agentic workflows were tested using sequential and parallel architectures to improve precision.

## Key Results
- Zero-shot LLMs achieve F1-scores up to 0.311 for multiclass activity classification but only 0.3285 with multi-step agentic workflows.
- Concise metadata outperforms full annual reports for classification (F1: 0.2493 vs 0.2264).
- KPI regression fails completely with negative R² values across all predicted KPIs (e.g., -0.2106 for Turnover Eligible %).
- Model confidence scores are poorly calibrated with ECE values up to 0.684, making them unreliable for decision-making.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concise metadata outperforms full document context for activity classification in zero-shot settings.
- **Mechanism:** Full annual reports introduce noise that dilutes task-relevant signals; LLMs struggle to isolate taxonomy-relevant operations from narrative text. Structured metadata (sector, revenue, employees) constrains the hypothesis space, reducing false positives from tangential mentions.
- **Core assumption:** The taxonomy mapping task depends more on high-level business semantics than granular operational details.
- **Evidence anchors:**
  - [abstract] "We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports"
  - [section 6.4] "The average F1-score when using the full, unstructured annual report was 0.2264, which was lower than using concise, structured company metadata (0.2493)"
  - [corpus] Climate Finance Bench and ESGBench similarly note that retrieval-augmented approaches on full reports require careful chunking to avoid noise
- **Break condition:** When companies have highly diversified operations spanning multiple sectors, metadata alone cannot disambiguate which taxonomy activities apply.

### Mechanism 2
- **Claim:** Multi-step agentic workflows (generate-then-verify) modestly improve classification precision over single-step inference.
- **Mechanism:** A high-recall first pass extracts candidate activities from full context; a verification agent with access to summarized context filters false positives. This separates the discovery and validation subtasks, allowing each step to optimize for different objectives.
- **Core assumption:** Information loss during summarization is acceptable for verification but harmful for candidate generation.
- **Evidence anchors:**
  - [abstract] "a multi-step agentic framework modestly enhancing precision"
  - [section 6.3] "The verification step significantly improved precision (from 0.211 to 0.3436), resulting in a final F1-score of 0.3285"
  - [corpus] ESGReveal uses RAG for structured extraction; weak corpus signal on agentic architectures specifically
- **Break condition:** When the summarization step removes critical evidence, the verification agent lacks context to confirm true positives, degrading recall.

### Mechanism 3
- **Claim:** Zero-shot LLMs cannot reliably perform quantitative KPI regression from unstructured reports.
- **Mechanism:** KPI extraction requires precise numerical grounding—locating specific figures, understanding their relationship to eligibility criteria, and aggregating correctly. Zero-shot models lack the domain-specific alignment between text spans and regulatory calculations, defaulting to generic distributions rather than matching ground-truth's multi-modal, skewed patterns.
- **Core assumption:** KPI values are explicitly stated in reports but require regulatory interpretation to map to taxonomy definitions.
- **Evidence anchors:**
  - [abstract] "the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting"
  - [section 6.2] "R² values were consistently negative across all predicted KPIs (e.g., −0.2106 for Turnover Eligible %), indicating that the models' predictions were less accurate than a baseline model simply predicting the mean"
  - [corpus] Limited direct corpus evidence on KPI regression specifically; related work (ESGReveal) focuses on disclosure analysis rather than numerical prediction
- **Break condition:** Fine-tuning or few-shot examples with explicit numerical extraction patterns may close the gap—this study deliberately excludes fine-tuning to establish baseline.

## Foundational Learning

- **Concept: Multi-label text classification with hierarchical label spaces**
  - Why needed here: EU Taxonomy contains 100+ activities; companies typically report 6+ activities. Standard accuracy metrics are misleading; F1, precision, and recall must be computed per-label and macro-averaged.
  - Quick check question: If a model predicts 3 activities for a company with 5 ground-truth activities (2 correct, 1 wrong), what are the precision and recall for that instance?

- **Concept: Calibration and Expected Calibration Error (ECE)**
  - Why needed here: The paper shows high-performing models are often the most poorly calibrated. Using confidence scores for downstream decisions (e.g., which predictions require human review) requires understanding miscalibration.
  - Quick check question: If a model assigns 0.9 confidence to 100 predictions but only 45 are correct, what is the calibration error for that confidence bin?

- **Concept: Zero-shot vs. few-shot vs. fine-tuning tradeoffs**
  - Why needed here: This study establishes zero-shot baselines; real deployments will need to decide where to invest—prompting engineering, RAG, few-shot examples, or full fine-tuning.
  - Quick check question: What is the minimal annotation investment needed to move from zero-shot to few-shot evaluation for the KPI regression task?

## Architecture Onboarding

- **Component map:** PDF → Markdown (Marker) → rule-based taxonomy section isolation → LLM extraction → self-verification → human review → inference layer (Vertex AI/HPC) → agentic orchestration (Google ADK) → evaluation (multi-label F1, regression R²/MAE, calibration ECE)

- **Critical path:** Activity classification pipeline is the most mature; KPI regression is a known failure mode. Start with the parallel agentic workflow (Figure 2, right) as your reference implementation.

- **Design tradeoffs:**
  - Recall vs. precision: Gemini Flash 2.5 favors recall (0.493); Llama favors precision (0.348). Choose based on workflow stage (discovery vs. verification).
  - Context depth vs. noise: Full reports degrade performance; use metadata + taxonomy section only for classification.
  - Agentic complexity: Sequential workflows underperform due to information loss; parallel workflows add cost but improve F1 by ~0.017.

- **Failure signatures:**
  - Negative R² on KPI regression → model is not grounding predictions in text; do not deploy for quantitative extraction.
  - High ECE (>0.6) with high recall → model is overconfident on false positives; do not use confidence scores for routing decisions.
  - F1 degradation with full report vs. metadata → context window contains noise; implement chunking or section isolation.

- **First 3 experiments:**
  1. **Reproduce the parallel agentic workflow** on a 10-company subset: verify that the generate-then-verify pattern improves precision over single-step inference.
  2. **Test few-shot prompting for KPI regression:** Add 3-5 annotated examples per KPI type; measure whether R² improves from negative to positive.
  3. **Evaluate chunking strategies:** Compare full-report, metadata-only, and taxonomy-section-only contexts on a sector-stratified sample; confirm the "paradox of context" finding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain-specific fine-tuning effectively bridge the performance gap between qualitative classification and quantitative KPI regression that persists in zero-shot prompting?
- **Basis in paper:** [explicit] The authors conclude in the Discussion and Limitations sections that "the path toward robust automation will require specialized, fine-tuned models," explicitly noting that their exclusion of fine-tuning means their results represent only a "baseline for general-purpose models."
- **Why unresolved:** The study establishes that zero-shot models fail comprehensively at regression (negative R²), but it does not determine if this failure is inherent to the task complexity or simply a result of using general-purpose weights.
- **What evidence would resolve it:** A follow-up study utilizing the released dataset to fine-tune smaller models (e.g., Llama or Gemma) specifically for the regression task, measuring if R² values can be raised to positive, usable levels.

### Open Question 2
- **Question:** What specific information retrieval strategies can mitigate the "paradox of context" where concise metadata outperforms full reports?
- **Basis in paper:** [explicit] The authors identify a "paradox of context" where providing the full annual report degrades performance compared to concise metadata, suggesting that "extensive, noisy text... impedes zero-shot inference."
- **Why unresolved:** While the paper establishes that full-text noise is detrimental, it does not test intermediate solutions (such as Retrieval-Augmented Generation or targeted chunking) to determine the optimal level of context granularity.
- **What evidence would resolve it:** Experiments evaluating RAG-based approaches that dynamically retrieve relevant sections versus using static summaries or full texts, to see if retrieval can preserve the detail of full reports while maintaining the precision of metadata.

### Open Question 3
- **Question:** Can advanced calibration techniques make LLM self-reported confidence scores reliable enough for high-stakes compliance auditing?
- **Basis in paper:** [explicit] The authors find that confidence scores are "poorly calibrated" (ECE up to 0.684) and conclude they are "not dependable" for this task.
- **Why unresolved:** It is unclear if the miscalibration is a fixable artifact of the prompting strategy or a fundamental consequence of the "information gap" where the model acts as an external auditor lacking internal company data.
- **What evidence would resolve it:** Testing verbalized calibration methods (e.g., "Chain-of-Thought" confidence reasoning) or specialized classification heads to determine if the high Expected Calibration Error (ECE) can be reduced to a level suitable for professional risk assessment.

### Open Question 4
- **Question:** Does the observed "moderate success" in activity classification persist across a geographically and linguistically diverse dataset?
- **Basis in paper:** [inferred] The authors list the geographical concentration of the dataset (63% German, 20% Austrian) as a primary limitation, implicitly raising the question of generalizability to other regulatory environments or languages.
- **Why unresolved:** The models may be leveraging regional patterns or language specificities present in the Germanic-centric dataset to achieve F1-scores of ~0.31, which might not translate to global reporting standards.
- **What evidence would resolve it:** Validating the best-performing models (e.g., Gemini Flash 2.5) on a new dataset of corporate reports from non-European or distinct legal jurisdictions to test for performance degradation.

## Limitations

- The dataset covers 190 companies across 20 sectors but may not capture edge cases where companies have complex, multi-sector operations spanning several taxonomy activities.
- The study establishes zero-shot baselines but does not explore fine-tuning or few-shot learning, leaving open the question of whether targeted model adaptation could overcome the quantitative regression failures.
- While calibration metrics show poor model confidence calibration, the practical implications for human-AI collaboration workflows remain underexplored.

## Confidence

- **High Confidence:** Activity classification performance findings (F1-scores up to 0.311) and the superiority of concise metadata over full reports are well-supported by systematic evaluation across multiple models and contexts.
- **Medium Confidence:** The agentic workflow improvements (precision increase from 0.211 to 0.3436) are promising but may be sensitive to specific implementation choices in the verification agent.
- **Low Confidence:** The complete failure of zero-shot KPI regression is established, but the extent to which fine-tuning or few-shot learning could remedy this remains untested.

## Next Checks

1. Test whether few-shot prompting with 3-5 annotated examples per KPI type can transform negative R² values to positive, establishing the minimum annotation investment needed for quantitative extraction.
2. Evaluate the metadata advantage across a stratified sample of highly diversified companies to identify where the "paradox of context" breaks down.
3. Implement a human-in-the-loop workflow using model confidence scores for routing decisions, measuring whether confidence-based filtering improves overall accuracy despite poor calibration.