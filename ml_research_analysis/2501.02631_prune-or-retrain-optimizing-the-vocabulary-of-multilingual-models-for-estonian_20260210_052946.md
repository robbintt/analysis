---
ver: rpa2
title: 'Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian'
arxiv_id: '2501.02631'
source_url: https://arxiv.org/abs/2501.02631
tags:
- vocabulary
- training
- tokenizer
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated two approaches for optimizing the vocabulary
  of multilingual models for Estonian: retraining a tokenizer from scratch and pruning
  unused tokens. Both methods aimed to improve computational efficiency and potentially
  enhance performance on downstream tasks.'
---

# Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian
## Quick Facts
- arXiv ID: 2501.02631
- Source URL: https://arxiv.org/abs/2501.02631
- Reference count: 8
- Retraining tokenizer with 32K vocabulary reduced model size by 60% and tokens per word by 20%

## Executive Summary
This study evaluates two approaches for optimizing the vocabulary of multilingual models for Estonian: retraining a tokenizer from scratch and pruning unused tokens. Both methods aimed to improve computational efficiency and potentially enhance performance on downstream tasks. The researchers found that while retraining the tokenizer with a smaller 32K vocabulary significantly reduced model size and token count per word, it substantially degraded Named Entity Recognition (NER) performance. In contrast, pruning the vocabulary to retain 67% of tokens achieved a 23% reduction in model size with no negative impact on NER performance. The study suggests that vocabulary pruning is an effective method for reducing model size without compromising downstream task performance, while retraining tokenizers requires more extensive training to achieve comparable results.

## Method Summary
The study compared two approaches for vocabulary optimization: retraining a tokenizer from scratch using the BPE algorithm with a 32K vocabulary size, and pruning unused tokens from an existing vocabulary while retaining the top 67% of most frequent tokens. Both methods were evaluated on their impact on model size and downstream Named Entity Recognition performance. The researchers also applied LoRA fine-tuning to the pruned model to assess whether continued training could improve masked language modeling accuracy. The NLLB-200 model served as the base multilingual model for all experiments, with all optimizations targeting Estonian language performance specifically.

## Key Results
- Retraining tokenizer with 32K vocabulary reduced model size by 60% and tokens per word by 20%
- Pruning vocabulary to 67% of tokens reduced model size by 23% with no negative impact on NER performance
- LoRA fine-tuning improved masked language modeling accuracy but did not benefit NER results in the pruned model

## Why This Works (Mechanism)
Vocabulary optimization works by reducing the number of tokens the model must process, thereby decreasing computational requirements and model size. Pruning removes unused or infrequent tokens that contribute little to model performance, while retaining those that capture essential linguistic patterns. This selective reduction maintains semantic coverage while eliminating redundancy. The mechanism leverages the observation that many tokens in large vocabularies are rarely or never used in practice, particularly for specific languages or downstream tasks.

## Foundational Learning
- **BPE (Byte Pair Encoding)**: A tokenization algorithm that iteratively merges frequent character pairs to create subword units. Needed for understanding how tokenizers build vocabularies from text data. Quick check: Verify token frequency distribution before and after optimization.
- **Vocabulary pruning**: The process of removing unused or infrequent tokens from a model's vocabulary. Essential for understanding how to reduce model size without retraining. Quick check: Count token occurrences in training corpus to identify candidates for pruning.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that adds low-rank matrices to existing model weights. Important for understanding how to continue training optimized models. Quick check: Monitor training loss curves during LoRA fine-tuning to ensure convergence.

## Architecture Onboarding
- **Component map**: Tokenization (BPE) -> Vocabulary optimization (pruning/retraining) -> Model fine-tuning (LoRA) -> Downstream task evaluation (NER)
- **Critical path**: Tokenization affects vocabulary size, which impacts model parameters and computational efficiency, ultimately influencing downstream task performance
- **Design tradeoffs**: Smaller vocabularies reduce model size and computational cost but may lose linguistic coverage; pruning is less disruptive than retraining but may retain some redundancy
- **Failure signatures**: Retraining leads to degraded NER performance despite improved MLM accuracy; LoRA fine-tuning improves MLM but not NER
- **First experiments**: 1) Measure token frequency distribution in training data, 2) Compare model size before/after pruning, 3) Run baseline NER performance on original vs optimized models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on Named Entity Recognition, limiting understanding of effects on other downstream tasks
- The retraining approach shows significant NER degradation despite improved MLM accuracy, suggesting potential optimization issues
- LoRA fine-tuning methodology improved MLM accuracy but failed to translate gains to NER performance
- Results are based on a specific Estonian-specific multilingual model, limiting generalizability

## Confidence
- High confidence: Vocabulary pruning methodology and its demonstrated effectiveness in reducing model size without performance degradation on NER tasks
- Medium confidence: The claim that retraining with smaller vocabulary requires more extensive training to achieve comparable results
- Low confidence: The effectiveness of LoRA fine-tuning in translating MLM improvements to downstream NER performance

## Next Checks
1. Evaluate the pruned and retrained vocabularies on additional downstream tasks beyond NER to assess broader applicability
2. Conduct systematic ablation studies on LoRA fine-tuning parameters, including learning rates and training epochs
3. Replicate the experiments using alternative multilingual models to assess generalizability across different architectural families