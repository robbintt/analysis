---
ver: rpa2
title: 'WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless
  Communications'
arxiv_id: '2505.14354'
source_url: https://arxiv.org/abs/2505.14354
tags:
- wireless
- llms
- tasks
- mathematical
- wirelessmathbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications

## Quick Facts
- arXiv ID: 2505.14354
- Source URL: https://arxiv.org/abs/2505.14354
- Authors: Xin Li; Mengbing Liu; Li Wei; Jiancheng An; Mérouane Debbah; Chau Yuen
- Reference count: 40
- Primary result: Evaluates 16 LLMs on 587 wireless communications mathematical modeling tasks, showing reasoning models outperform non-reasoning models (38.05% vs 30% average accuracy) but struggle with multi-step symbolic derivations

## Executive Summary
WirelessMathBench is a benchmark dataset of 587 mathematical modeling tasks extracted from 40 peer-reviewed wireless communications papers. The benchmark evaluates LLMs across three progressively difficult task types: multiple-choice questions (MCQ), progressive masking fill-in-the-blank (3 levels), and full equation completion (FEC). The evaluation reveals significant performance gaps between reasoning and non-reasoning models, with reasoning models achieving 38.05% average accuracy versus 30% for non-reasoning models. However, all models struggle with complex multi-step derivations, showing error rates as high as 87.5% on Level 3 masking tasks and 92.17% on full equation completion.

## Method Summary
The benchmark extracts system models from LaTeX-formatted papers using automated tools (arxiv-latex-cleaner, latexpand, de-macro) followed by LLM template extraction and expert validation. Tasks span MIMO, RIS, NOMA, ISAC, UAV, Satellite, and SIM systems, covering beamforming, channel estimation, trajectory design, and resource allocation. Evaluation uses zero-shot inference with unified prompt templates across 16 models. MCQs use direct answer matching while fill-in-the-blank and FEC tasks employ GPT-4o for symbolic equivalence evaluation. The benchmark deliberately excludes training splits to reflect real-world zero-shot scenarios.

## Key Results
- DeepSeek-R1 achieves highest MCQ score at 76% but drops to 6.25% in Level 3 masking and 6.96% in full equation completion
- Reasoning models (DeepSeek-R1: 38.05%, OpenAI-o1: 34.55%) significantly outperform non-reasoning models (~30% average accuracy)
- Performance degrades predictably: MCQ (76%) → Level 1 masking (60%) → Level 3 masking (12.5%) → FEC (7.83%)
- Domain-specific fine-tuning on telecom data (LLaMA-Tele) yields only marginal improvement (12.07% vs 13.91% baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degrades predictably as symbolic reconstruction requirements increase because models lack robust multi-step derivation capabilities.
- Mechanism: Tasks progress from recognition (MCQ: 76% for DeepSeek-R1) to partial reconstruction (Level 1 masking: 60%, Level 3: 12.5%) to full derivation (FEC: 7.83%). Each step requires maintaining symbolic consistency across more interdependent variables without visual or contextual scaffolding.
- Core assumption: The degradation reflects genuine reasoning limitations rather than prompt design artifacts.
- Evidence anchors:
  - [abstract] "performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs"
  - [section 4.2] "DeepSeek-V3 achieves the highest MCQ score at 78.40%, drop to 6.25% in Level 3 masking filling, and 6.96% in FEC"
  - [corpus] WirelessMathLM (arxiv:2509.23219) corroborates that specialized technical mathematics causes "catastrophic" failures in state-of-the-art models
- Break condition: If models trained with explicit symbolic derivation objectives (e.g., theorem provers) show flatter degradation curves, this suggests the limitation is training-data rather than architectural.

### Mechanism 2
- Claim: Reasoning-oriented models outperform general-purpose LLMs because explicit chain-of-thought mechanisms decompose multi-step matrix operations into trackable sub-problems.
- Mechanism: Models like DeepSeek-R1 and OpenAI-o1 achieve 38.05% and 34.55% average accuracy versus ~30% for non-reasoning models. The paper attributes this to "decompos[ing] complex mathematical operations into sequential sub-steps—a capability critical for non-trivial matrix manipulations and dimensionality constraints."
- Core assumption: The advantage stems from architectural reasoning mechanisms, not merely larger scale or better general training data.
- Evidence anchors:
  - [section 4.2] "models with explicit reasoning mechanisms excel at tasks requiring symbolic consistency across multiple operations, systematically tracking variables through transformations"
  - [section F.3] CoT experiments show limited improvement for non-reasoning models, suggesting "ability to reason effectively about wireless communications problems cannot be induced solely through prompting"
  - [corpus] Insufficient direct evidence in corpus for architectural mechanism; this remains inference from performance gaps
- Break condition: If parameter-matched reasoning vs. non-reasoning model comparisons show no significant difference, the mechanism may be scale-related.

### Mechanism 3
- Claim: Domain-specific fine-tuning on telecom corpora yields limited gains because protocol-focused training data doesn't capture the symbolic manipulation and physical constraint satisfaction required for mathematical modeling.
- Mechanism: LLaMA-3-8B-Tele achieves only 12.07% average accuracy versus 13.91% for base LLaMA-3-8B—a marginal improvement. The paper notes "telecom fine-tuning data predominantly consists of wireless protocols, whereas the problems in WirelessMathBench require handling long contexts and performing high-level mathematical reasoning."
- Core assumption: The gap reflects data-content mismatch rather than insufficient fine-tuning scale.
- Evidence anchors:
  - [section 4.2] "fine-tuning general-purpose models like LLaMA to telecom-specific data yields only limited benefits"
  - [corpus] TelecomGPT (cited in paper but not in corpus neighbors) is mentioned as extending to "higher-level tasks like wireless-specific code generation and formula completion" but evaluated differently
  - [corpus] Direct corpus evidence is weak; this mechanism relies primarily on paper-internal data
- Break condition: If fine-tuning on derivation-heavy corpora (e.g., LaTeX arXiv math sections) shows significant improvement, the limitation is data selection rather than task nature.

## Foundational Learning

- **MIMO and RIS Channel Models**
  - Why needed here: 31 of 40 papers cover MIMO, RIS, or related multi-antenna systems; tasks require understanding channel gain expressions, phase-shift matrices, and spatial correlation.
  - Quick check question: Given a BS with M antennas and an RIS with N elements, what are the dimensions of the cascaded channel matrix H_BR × Θ × H_RU?

- **Physical and Dimensional Constraints**
  - Why needed here: The benchmark explicitly requires adherence to physical feasibility (path loss exponents, power limits) and dimensional consistency—a major error source (29% symbol misinterpretation).
  - Quick check question: If a received signal equation has terms with units of power (W), power spectral density (W/Hz), and voltage (V), which term breaks dimensional consistency?

- **LaTeX Mathematical Notation Conventions**
  - Why needed here: All equations use LaTeX; models must correctly interpret conjugates (H^H vs H), subscripts, and matrix operations (diag(), trace()).
  - Quick check question: In wireless notation, does H^H represent the Hermitian transpose or element-wise conjugation?

## Architecture Onboarding

- **Component map:**
  - arXiv papers → LaTeX extraction → system model extraction via LLM template → expert validation → 587 questions → 3 task types (MCQ, progressive masking, FEC) → zero-shot evaluation across 16 models

- **Critical path:**
  1. Paper selection → LaTeX processing (arxiv-latex-cleaner, latexpand, de-macro)
  2. System model extraction with expert reformulation (contamination mitigation)
  3. Question generation at 3 difficulty tiers per equation
  4. Zero-shot evaluation with unified prompts across all models

- **Design tradeoffs:**
  - Zero-shot vs. fine-tuning: Paper uses zero-shot for real-world applicability; future work may add training splits
  - Automated vs. expert validation: Hybrid approach balances scale with quality but may miss intermediate reasoning errors
  - Symbolic equivalence vs. semantic similarity: Evaluation checks final answer correctness, not derivation process

- **Failure signatures:**
  - Partial fill mismatch (31%): Correctly fills one placeholder but merges or misplaces others
  - Symbol misinterpretation (29%): Substitutes H_BR for H_BR^H or omits conjugation
  - Incorrect derivation (24%): Early errors propagate through multi-step expressions
  - Irrelevant system mixing (11%): Injects NOMA interference into RIS-MIMO scenarios

- **First 3 experiments:**
  1. Reproduce MCQ vs. masking level degradation on 3 models (e.g., GPT-4o, DeepSeek-R1, LLaMA-70B) to validate difficulty scaling
  2. Test whether explicit dimensional constraint prompts (e.g., "verify units before answering") reduce symbol misinterpretation errors
  3. Compare reasoning model performance with and without CoT prompting on Level 2-3 masking to isolate architectural vs. prompting effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain-specific fine-tuning or retrieval-augmented generation significantly improve LLM performance on wireless communications mathematical modeling tasks compared to zero-shot baselines?
- **Basis in paper:** [explicit] The limitations section states "all tests were done in a zero-shot setting. While this reflects real-world use, it does not explore whether fine-tuning or retrieval-based methods could improve results. Future versions of WirelessMathBench may include training splits to support domain adaptation and wireless-specific fine-tuning."
- **Why unresolved:** The benchmark currently lacks training splits, and no experiments evaluated fine-tuning approaches against the zero-shot baseline.
- **What evidence would resolve it:** Systematic comparison of domain-adapted models (via fine-tuning on wireless corpora or retrieval augmentation) against zero-shot performance on WirelessMathBench tasks.

### Open Question 2
- **Question:** How can intermediate reasoning steps be validated in wireless mathematical derivations, beyond checking final symbolic equivalence?
- **Basis in paper:** [explicit] The limitations section notes: "our automated evaluation checks the final symbolic equivalence and dimensionality plausibility but may miss incorrect reasoning at intermediate steps."
- **Why unresolved:** Current evaluation uses GPT-4o to assess final answer correctness but does not verify the validity of derivation paths, allowing models to reach correct answers through flawed reasoning.
- **What evidence would resolve it:** Development of step-by-step verification protocols or formal methods that track symbolic consistency through multi-stage derivations, validated on annotated reasoning chains from domain experts.

### Open Question 3
- **Question:** What architectural or training improvements beyond chain-of-thought prompting are needed to enable robust symbolic consistency across multi-variable wireless equations?
- **Basis in paper:** [inferred] The error analysis shows "Partial Fill Mismatch" (31%) and "Symbol Misinterpretation" (29%) as dominant failure modes. Appendix F demonstrates that CoT prompting yielded "only marginal average accuracy improvements" and sometimes decreased performance on complex tasks, suggesting prompting alone is insufficient.
- **Why unresolved:** Models struggle to maintain symbolic coherence when reconstructing heavily masked equations with interdependent variables, indicating fundamental limitations in current architectures.
- **What evidence would resolve it:** Novel training objectives or model architectures explicitly designed for multi-step symbolic manipulation, evaluated on progressive masking tasks at Level 3 difficulty.

### Open Question 4
- **Question:** How does LLM performance generalize to multimodal wireless communications tasks involving antenna diagrams, simulation plots, and RF measurements?
- **Basis in paper:** [explicit] The limitations section acknowledges: "it mainly covers text-based problems (e.g., symbolic derivations), missing other key data types like antenna diagrams, simulation plots, and Radio frequency (RF) measurements measurements, which are crucial for real-world wireless tasks."
- **Why unresolved:** WirelessMathBench currently only evaluates text-based mathematical reasoning, omitting visual-spatial reasoning essential for practical engineering.
- **What evidence would resolve it:** Extension of the benchmark to include multimodal tasks and systematic evaluation of vision-language models on diagram interpretation and simulation analysis tasks.

## Limitations
- Zero-shot evaluation only prevents assessment of fine-tuning benefits for domain adaptation
- GPT-4o evaluator for symbolic equivalence lacks transparency in scoring criteria and prompts
- No disclosure of source papers or specific questions limits independent verification
- Evaluation focuses on final answer correctness without validating intermediate reasoning steps

## Confidence
- **High confidence**: Performance degradation patterns across task types (MCQ → progressive masking → FEC) and general observation that reasoning models outperform non-reasoning models on symbolic reconstruction tasks
- **Medium confidence**: Architectural mechanism explanation for reasoning model superiority; attribution to chain-of-thought mechanisms versus scale or training data differences remains inferential
- **Low confidence**: Claim about domain-specific fine-tuning limitations; based on single comparison without exploring alternative fine-tuning strategies

## Next Checks
1. Evaluate evaluator bias: Run a controlled test where 10% of tasks are scored by both GPT-4o and human experts to quantify agreement rates and identify systematic evaluator patterns in symbolic equivalence assessment.

2. Test prompting effects: Compare reasoning model performance with and without explicit dimensional constraint and unit-checking prompts on Level 2-3 masking tasks to determine if prompting can reduce the 29% symbol misinterpretation error rate.

3. Assess scale effects: Compare parameter-matched reasoning versus non-reasoning model pairs (e.g., DeepSeek-R1 vs. DeepSeek-V3, OpenAI-o1 vs. GPT-4o) to isolate whether performance differences stem from reasoning architecture or model scale.