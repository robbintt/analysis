---
ver: rpa2
title: Estimating the Self-Consistency of LLMs
arxiv_id: '2509.19489'
source_url: https://arxiv.org/abs/2509.19489
tags:
- self-consistency
- bound
- error
- estimator
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of estimating the self-consistency
  of large language models (LLMs), where self-consistency measures how often repeated
  responses to the same prompt disagree with the majority label. The key method involves
  using a plug-in estimator based on repeated LLM calls per prompt and aggregating
  across sampled prompts under a fixed compute budget.
---

# Estimating the Self-Consistency of LLMs

## Quick Facts
- **arXiv ID**: 2509.19489
- **Source URL**: https://arxiv.org/abs/2509.19489
- **Reference count**: 4
- **Primary result**: Optimal compute budget allocation is m ∝ n ∝ √B for estimating LLM self-consistency

## Executive Summary
This paper addresses the problem of estimating self-consistency in large language models (LLMs), where self-consistency measures how often repeated responses to the same prompt disagree with the majority label. The work provides a theoretical analysis of how to optimally allocate a fixed compute budget between sampling diverse prompts and making repeated calls per prompt. The key finding is that the error-minimizing allocation follows a square-root scaling law, balancing the tradeoff between prompt coverage and reliable per-prompt estimates.

## Method Summary
The method involves a plug-in estimator that converts response counts from repeated LLM calls into self-consistency scores. For each prompt, the LLM is called n times to obtain binary responses, and the per-prompt self-consistency estimate is computed as the minimum of the positive-rate and negative-rate. These estimates are then averaged across m sampled prompts. The total compute budget B = mn constrains the allocation between m (number of prompts) and n (number of repeated calls per prompt). The paper derives a mean-squared error bound that decomposes into three terms representing variance from prompt sampling, bias from the estimator, and variance from repeated calls.

## Key Results
- The optimal allocation of compute budget B is to use approximately √B prompts and √B repeated calls per prompt
- The mean-squared error bound decomposes into three components: variance from prompt sampling (1/8m), estimator bias (1/πn), and per-prompt variance (1/2nm)
- The bias term dominates when n is small and decreases as √(1/n)
- The third variance term becomes negligible as B grows, leaving only the first two terms to balance

## Why This Works (Mechanism)

### Mechanism 1: Plug-in Estimator Converts Response Counts to Self-Consistency Scores
The minimum of positive-rate and negative-rate across n repeated calls estimates the probability that a new sample disagrees with the majority label. For each prompt x, count positive responses k out of n calls, compute p̂(x) = k/n, then estimate self-consistency error as Ê(x) = min{k/n, 1−k/n}. This captures disagreement frequency when the model is uncertain (p(x) ≈ 0.5), and approaches zero when confident (p(x) ≈ 0 or 1). The core assumption is that responses to repeated calls are i.i.d. draws from a fixed distribution with parameter p(x).

### Mechanism 2: MSE Bound Decomposition Reveals Three Competing Error Sources
The total estimation error decomposes into prompt-sampling variance (1/8m), estimator bias (1/πn), and per-prompt variance (1/2nm), with the third term becoming negligible as B grows. The bound E[(E − Ê)²] ≤ 1/(8m) + 1/(πn) + 1/(2nm) isolates: (1) uncertainty from finite prompt samples, (2) systematic underestimation from the discrete min{k, n−k}/n estimator, and (3) noise in individual Ê(x) estimates. The analysis uses Stirling bounds to show bias ≤ √(1/(2πn)).

### Mechanism 3: Optimal Budget Split Balances Two Dominant Terms
Under total budget B = mn, minimizing the upper bound yields m* ∝ √B and n* ∝ √B, with exact values m* = √(πB/8), n* = √(8B/π). Since the third term (1/2nm = 1/2B) becomes negligible relative to 1/m and 1/n as B grows, the optimization reduces to balancing the first two terms under the constraint mn = B. Setting m = n = √B equalizes their scaling.

## Foundational Learning

- **Bias-variance tradeoff in estimation**: Why needed here - The error decomposition separates bias (systematic underestimation from discrete k/n) from variance (noise from finite sampling). Understanding this is essential to see why increasing n reduces bias but doesn't help prompt coverage. Quick check: If you double n while holding m fixed, which error terms decrease and which stay the same?

- **Constrained optimization under budget**: Why needed here - The √B split emerges from minimizing a multi-term objective subject to mn = B. This is classic Lagrange multiplier territory. Quick check: Given B = 100, what are the rounded integer values of m* and n* per the paper's formulas?

- **Binomial distribution properties**: Why needed here - The bias derivation relies on properties of binomial distributions with p = 0.5, including symmetry and Stirling approximation for binomial coefficients. Quick check: Why is the bias largest when p(x) = 0.5?

## Architecture Onboarding

- **Component map**: Prompt sampler -> Repeated caller -> Per-prompt estimator -> Aggregator -> Budget allocator
- **Critical path**:
  1. Determine compute budget B based on latency/cost constraints
  2. Compute optimal m*, n* (round to integers)
  3. Sample m* prompts from target distribution
  4. For each prompt, make n* calls with appropriate sampling settings (temperature > 0)
  5. Aggregate per-prompt estimates into final Ê

- **Design tradeoffs**:
  - Larger m (more prompts) → better coverage of task distribution, higher per-prompt noise
  - Larger n (more repeats) → lower per-prompt bias and variance, less diverse prompt coverage
  - Temperature setting: Too low → correlated responses violating i.i.d.; too high → incoherent outputs
  - Assumption: Uniform per-call cost; if costs vary by prompt length, budget constraint becomes non-trivial

- **Failure signatures**:
  - Near-zero estimates with high variance: n too small, or model near-deterministic (p ≈ 0 or 1 for most prompts)
  - High variance across runs: m too small relative to task diversity
  - Systematic underestimation: Bias term dominates; increase n
  - Correlated responses: Temperature too low or caching enabled; check independence by comparing response diversity

- **First 3 experiments**:
  1. Validate the √B split: Fix B = 100, compare MSE of (m=10, n=10) vs. (m=5, n=20) vs. (m=20, n=5) over multiple runs on a held-out test set. Expect the balanced split to achieve lowest MSE.
  2. Test i.i.d. assumption: For a fixed prompt, run n = 100 calls at different temperatures (0.3, 0.7, 1.0). Compute empirical autocorrelation between consecutive responses. If correlation > 0.1, i.i.d. is violated.
  3. Measure actual bias: For prompts where you can compute true p(x) via very large n (e.g., n=1000), compare Ê(x) at n=10 vs. n=100 vs. ground truth. Verify bias decreases as √(1/n).

## Open Questions the Paper Calls Out

- **Can high-probability bounds be derived for the self-consistency estimator using concentration inequalities?** The paper suggests deriving "high-probability bounds by combining Bernstein (for $\tilde{E}$) with binomial concentration for $\hat{E}(x)$ and a union bound." This would characterize the probability of error exceeding a threshold, not just average error.

- **Does the $m, n \propto \sqrt{B}$ allocation strategy hold for multi-class classification tasks?** The paper proposes an extension to "multi-class settings by replacing $E(x) = \min\{p, 1-p\}$ with $E(x) = 1 - \max_c p_c(x)$" but doesn't develop the theory for this case.

- **How does intraclass correlation between repeated LLM calls impact the optimal compute allocation?** The authors suggest modeling "correlated LLM calls via an intraclass correlation $\rho$" to account for departures from i.i.d. sampling, noting this would "inflate the variance term."

## Limitations
- Theoretical analysis relies on i.i.d. assumption for repeated LLM calls, which may not hold in practice due to caching or shared randomness
- Binary classification framing excludes many practical use cases and the multi-class extension is not developed
- MSE bound is presented as an upper bound without empirical validation of its tightness
- No experimental validation - all claims are theoretical
- Assumes uniform per-call costs, which may not reflect real API pricing structures

## Confidence

**High confidence**: The basic plug-in estimator mechanism is straightforward and mathematically sound. The bias derivation using Stirling bounds is standard statistical theory. The optimization math leading to the √B split is correct given the stated constraints.

**Medium confidence**: The MSE decomposition itself is mathematically valid, but whether the three-term bound accurately characterizes real-world error is uncertain. The claim that the third term becomes negligible may not hold for small B or high-variance tasks.

**Low confidence**: The assumption that the optimal √B split transfers to different per-call costs, different prompt distributions, or multi-class settings. No empirical evidence supports the practical utility of these theoretical results.

## Next Checks

1. **Empirical MSE validation**: For B = 400, compare the MSE of the theoretically optimal split (m=13, n=31) against several alternative splits (m=5,n=80; m=20,n=20; m=40,n=10) on a real binary classification task. Compute actual MSE vs. theoretical bound predictions.

2. **i.i.d. assumption verification**: For 10 fixed prompts, generate n=100 responses each at temperatures 0.3, 0.7, 1.0. Compute autocorrelation between consecutive responses and response diversity (unique outputs/total outputs). Determine temperature threshold where i.i.d. breaks down.

3. **Bias verification with ground truth**: For prompts where you can obtain high-confidence estimates (n=1000), compare Ê(x) at n=10, n=20, n=50, n=100 against ground truth. Plot bias vs 1/√n to verify the theoretical bias scaling.