---
ver: rpa2
title: 'Toward Trustworthy Difficulty Assessments: Large Language Models as Judges
  in Programming and Synthetic Tasks'
arxiv_id: '2511.18597'
source_url: https://arxiv.org/abs/2511.18597
tags:
- problems
- hard
- difficulty
- gpt-4o
- programming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether GPT-4o can serve as a trustworthy
  difficulty judge for competitive programming problems. The authors compare GPT-4o
  (used as a black-box natural-language difficulty assessor) against a LightGBM ensemble
  that leverages explicit numeric and textual features.
---

# Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks

## Quick Facts
- arXiv ID: 2511.18597
- Source URL: https://arxiv.org/abs/2511.18597
- Reference count: 11
- GPT-4o achieves 37.75% accuracy vs LightGBM's 86% for programming difficulty classification

## Executive Summary
This paper evaluates whether GPT-4o can serve as a trustworthy difficulty judge for competitive programming problems. The authors compare GPT-4o (used as a black-box natural-language difficulty assessor) against a LightGBM ensemble that leverages explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, GPT-4o achieves only 37.75% accuracy versus LightGBM's 86%. Detailed analysis shows GPT-4o systematically underestimates difficulty, labeling 83% of real Hard problems as Easy. In a synthetic experiment, GPT-4o-generated Hard problems are nearly all labeled Medium, revealing internal inconsistency. SHAP-based interpretability highlights that numeric constraints (e.g., input sizes, acceptance rates) are critical for separating Hard problems, which GPT-4o overlooks.

## Method Summary
The study compares GPT-4o (text-only judge) against a LightGBM ensemble for predicting LeetCode difficulty (Easy/Medium/Hard). Inputs include 1,825 LeetCode problems with ground-truth labels, textual descriptions, and numeric metadata (input size, acceptance rate, time/space complexity). GPT-4o receives descriptions only (numeric metadata removed), while LightGBM receives TF-IDF vectors plus numeric features. The objective metrics are accuracy, macro-precision, macro-recall, and macro F1-score. LightGBM is trained with grid search optimizing macro F1, while GPT-4o performs single-pass labeling. The study also generates synthetic Hard problems from 21 seed problems, then re-classifies them with GPT-4o to test internal consistency.

## Key Results
- GPT-4o achieves 37.75% accuracy versus LightGBM's 86% on 3-class difficulty prediction
- Among 385 real Hard problems, GPT-4o labels 321 (83.38%) as Easy
- Of 385 synthetic Hard problems, GPT-4o labels 384 (99.74%) as Medium

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Numeric constraint features (input size limits, acceptance rates) are causally dominant for distinguishing Hard difficulty in programming problems.
- Mechanism: Tree-based ensembles learn explicit threshold rules on numeric metadata (e.g., input size > 10^5, acceptance rate < 25%), which strongly separate Hard from easier classes. SHAP analysis reveals these features have the highest mean absolute attribution values for Hard predictions.
- Core assumption: Difficulty in competitive programming is primarily constrained by algorithmic complexity requirements tied to input scale and empirical solve rates, not just textual semantics.
- Evidence anchors:
  - [abstract] "SHAP-based interpretability, show that numeric constraints—such as input size limits and acceptance rates—play a crucial role in separating Hard problems from easier ones."
  - [Section 4.4] "Larger input size limits consistently push predictions toward the Hard class. Lower acceptance rates...are strongly associated with Hard labels."

### Mechanism 2
- Claim: LLMs used as text-only judges systematically underweight or fail to reconstruct implicit numeric constraints from natural language descriptions.
- Mechanism: GPT-4o receives problem statements with numeric metadata removed or paraphrased. Without explicit numeric tokens, the model relies on semantic similarity to training examples, which fails to capture constraint-driven difficulty boundaries. This produces a strong downgrade bias (83% of Hard → Easy).
- Core assumption: LLMs do not reliably infer constraint severity from textual descriptions alone; they require explicit numeric tokens to calibrate difficulty.
- Evidence anchors:
  - [abstract] "GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories."
  - [Section 4.2] "Among the 385 real Hard problems...GPT-4o outputs: 321 problems (83.38%) as Easy."

### Mechanism 3
- Claim: GPT-4o's internal difficulty boundary is inconsistent between generation and judgment modes.
- Mechanism: When instructed to generate Hard problems, GPT-4o produces content labeled with large constraints. However, when re-prompted to classify these same problems, it labels 99.74% as Medium. This suggests the model's generative "Hard" schema does not match its discriminative "Hard" boundary.
- Core assumption: Generation and judgment in LLMs use different implicit representations; the model cannot self-consistently apply its own difficulty criteria.
- Evidence anchors:
  - [abstract] "Surprisingly, when GPT-4o generated synthetic Hard problems, it labeled almost all of them as Medium, revealing an inconsistency in its internal notion of difficulty."
  - [Section 4.5] "384 problems (99.74%) are labeled as Medium...This behavior contradicts its pattern on real Hard problems, which are mostly labeled as Easy."

## Foundational Learning

- Concept: **Gradient Boosted Decision Trees (LightGBM)**
  - Why needed here: Provides the interpretable baseline that outperforms GPT-4o. Understanding how tree ensembles partition feature space is essential to grasp why numeric constraints dominate.
  - Quick check question: Can you explain why LightGBM might learn threshold rules on "acceptance rate" that correlate with difficulty?

- Concept: **SHAP (SHapley Additive exPlanations)**
  - Why needed here: The paper uses SHAP to demonstrate which features drive Hard classifications. Without understanding Shapley values, you cannot interpret the claimed mechanism.
  - Quick check question: If a feature has a high mean absolute SHAP value, what does that imply about its contribution to model predictions?

- Concept: **LLM-as-Judge Paradigm**
  - Why needed here: The paper positions itself within this emerging field. You need to understand the promise (cheap, flexible evaluation) and known risks (bias, calibration failures).
  - Quick check question: What are two failure modes reported in the literature when using LLMs as automatic evaluators?

## Architecture Onboarding

- Component map:
  - Data Layer -> Feature Extraction -> LightGBM Ensemble -> SHAP Explainer
  - Data Layer -> GPT-4o Judge -> Label Analysis
  - Data Layer -> Synthetic Generation -> GPT-4o Re-classification

- Critical path:
  1. Curate dataset -> extract textual + numeric features
  2. Train LightGBM on training split -> tune hyperparameters on validation
  3. Compute SHAP values -> identify constraint features as Hard predictors
  4. Prompt GPT-4o on held-out test set -> log label distribution and confusion
  5. Generate synthetic Hard problems -> re-prompt GPT-4o to self-classify
  6. Compare consistency: LightGBM (stable) vs GPT-4o (inconsistent)

- Design tradeoffs:
  - **Interpretability vs flexibility**: LightGBM is transparent but requires explicit feature engineering; GPT-4o is flexible but opaque.
  - **Prompt isolation vs realism**: Removing numeric metadata isolates LLM's textual reasoning but may underrepresent real-world usage where constraints are visible.
  - **Single-pass vs self-consistency**: Paper uses single-prompt labeling; more sophisticated sampling (e.g., majority vote over multiple outputs) might improve LLM performance but was not tested.

- Failure signatures:
  - GPT-4o accuracy <40% on 3-class difficulty prediction
  - Hard → Easy downgrade rate >80%
  - Synthetic Hard → Medium classification rate >99%
  - SHAP shows numeric features dominate; GPT-4o lacks access to these

- First 3 experiments:
  1. **Constraint-aware prompting**: Re-run GPT-4o with explicit constraint tables included in the prompt; measure accuracy improvement.
  2. **Hybrid model**: Train LightGBM on LLM-derived embeddings (e.g., sentence transformers) plus numeric features; compare to text-only baseline.
  3. **Self-consistency sampling**: For each problem, prompt GPT-4o 5 times with temperature >0; take majority vote and measure whether variance correlates with misclassification.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can "constraint-aware" prompting strategies that explicitly foreground numeric details enable LLMs to reliably internalize and utilize these cues for accurate difficulty assessment?
  - Basis in paper: [explicit] The authors state, "We plan to explore prompts that explicitly foreground numeric details... to test whether GPT-4o can more reliably internalize and use them."
  - Why unresolved: The current study isolated the LLM by removing explicit numeric metadata, and the model failed to infer constraints from text alone.
  - What evidence would resolve it: A follow-up study showing significantly higher accuracy when numeric constraints are provided in structured tables within the prompt.

- **Open Question 2**: Can a hybrid architecture that combines LLM-derived textual embeddings with interpretable numeric features outperform pure LLM or purely feature-engineered baselines?
  - Basis in paper: [explicit] The authors propose to "combine LLM-derived embeddings of problem text and solution code with interpretable numeric features" in future work.
  - Why unresolved: The paper only compared a pure LLM judge against a separate gradient-boosted model, without integrating the two approaches.
  - What evidence would resolve it: Implementation of a multimodal model that fuses semantic embeddings with constraint data, achieving higher F1-scores than the standalone LightGBM ensemble.

- **Open Question 3**: What specific benchmark suites and agreement metrics are necessary to effectively meta-evaluate LLM-based judges specifically for programming difficulty assessment?
  - Basis in paper: [explicit] The authors suggest designing "benchmark suites and agreement metrics specifically tailored to difficulty assessment, including human studies on perceived difficulty."
  - Why unresolved: Existing meta-evaluation benchmarks (e.g., JUDGE-BENCH) are general, whereas difficulty assessment requires specific calibration against human perceptions of complexity.
  - What evidence would resolve it: The creation and validation of a difficulty-specific benchmark where LLM judgments show high correlation with human expert ratings across diverse problem types.

## Limitations
- Removing numeric metadata from prompts may not reflect real-world usage where constraints are visible
- Single-pass labeling without self-consistency sampling may underestimate LLM capability
- Small sample size (21 seeds) for synthetic problem generation may limit generalizability of internal inconsistency findings

## Confidence
- High confidence in LightGBM's superior performance (86% accuracy) and the critical role of numeric features (SHAP analysis is reproducible and interpretable)
- Medium confidence in the magnitude of GPT-4o's downgrade bias (37.75% accuracy, 83% Hard→Easy), as results depend heavily on prompt design and may improve with constraint-aware prompts
- Low confidence in the generality of generation-judgment inconsistency (99.7% Medium) due to small synthetic sample size and unknown seed problem selection

## Next Checks
1. **Constraint-aware prompting**: Re-run GPT-4o with explicit constraint tables in prompts and measure accuracy improvement
2. **Hybrid model evaluation**: Train LightGBM on LLM-derived embeddings plus numeric features to test whether semantic embeddings capture any additional signal
3. **Self-consistency sampling**: For each problem, prompt GPT-4o 5 times with temperature >0 and compute majority vote accuracy and variance