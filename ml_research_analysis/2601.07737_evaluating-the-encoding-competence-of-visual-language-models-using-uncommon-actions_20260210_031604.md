---
ver: rpa2
title: Evaluating the encoding competence of visual language models using uncommon
  actions
arxiv_id: '2601.07737'
source_url: https://arxiv.org/abs/2601.07737
tags:
- visual
- language
- semantic
- image
- uncommon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UAIT (Uncommon-sense Action Image-Text dataset),
  a novel benchmark designed to evaluate the semantic understanding ability of visual
  language models (VLMs) in counter-common sense action scenes. Unlike existing datasets
  focusing on common visual scenes, UAIT challenges models with grammatically reasonable
  but semantically counter-common sense image-text pairs.
---

# Evaluating the encoding competence of visual language models using uncommon actions

## Quick Facts
- arXiv ID: 2601.07737
- Source URL: https://arxiv.org/abs/2601.07737
- Authors: Chen Ling; Nai Ding
- Reference count: 21
- Primary result: Novel UAIT benchmark exposes VLM weaknesses in semantic role reasoning, with models performing significantly below human accuracy on counter-common-sense actions

## Executive Summary
This paper introduces UAIT (Uncommon-sense Action Image-Text dataset), a benchmark designed to evaluate semantic understanding in visual language models (VLMs) through counter-intuitive action scenes. Unlike existing datasets focusing on common visual scenes, UAIT challenges models with grammatically correct but semantically counter-common sense image-text pairs. The dataset is constructed using a semi-automated pipeline involving large language models, few-shot prompt engineering, and text-to-image generation. Experiments show that all evaluated VLMs perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. The study highlights key weaknesses in current VLMs and provides diagnostic tools for developing robust models with real visual semantic reasoning capabilities.

## Method Summary
The UAIT dataset is constructed through a multi-stage pipeline: filtering VerbNet for non-interchangeable verbs, generating common-sense sentences with Qwen2-LLM, swapping agent-patient roles to create uncommon-sense variants, and generating corresponding images using Stable Diffusion 3.5. The dataset consists of 400 image-text pairs with binary-choice questions testing fine-grained reasoning. Evaluation includes both pretrained VLMs (Qwen2-VL, LLaVA, LLaMA3.2-Vision) and contrastive models (CLIP, RWKV-CLIP), with human baseline established at 96% accuracy. Fine-tuning experiments use LoRA adaptation on LLaVA-1.5 with AdamW optimizer, learning rate 1e-4, and gradient accumulation steps of 8.

## Key Results
- All evaluated VLMs perform significantly below human baseline (96%) on uncommon-sense action recognition
- CLIP-style contrastive models achieve near-random accuracy, indicating inability to capture semantic role relationships
- LoRA fine-tuning improves LLaVA-1.5 accuracy to 0.79 but still falls short of human performance
- Models show strong statistical bias, preferring frequent agent-patient combinations regardless of visual evidence
- Performance varies significantly across verb categories, with directional and tool-use verbs posing greater challenges

## Why This Works (Mechanism)

### Mechanism 1: Semantic Role Reversal Exposes Statistical Bias
- Claim: Swapping agent-patient roles in grammatically correct sentences reveals whether models rely on co-occurrence statistics versus true semantic role understanding.
- Mechanism: The UAIT dataset constructs image-text pairs where the action is visually clear but semantically counter-intuitive (e.g., "a rabbit drags a tiger"). Models trained on common-sense co-occurrence patterns will incorrectly select the statistically frequent option even when it contradicts the visual evidence.
- Core assumption: Models have internalized statistical priors from training data where certain agent-patient combinations appear more frequently than their reversed counterparts.
- Evidence anchors:
  - [abstract]: "UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs... tasks require models to go beyond superficial pattern recognition."
  - [section 4.2]: "common statistical preferences in the training process ('tiger drags rabbit' appears more frequently than 'rabbit drags tiger') may cause the model to misjudge during actual reasoning."
  - [corpus]: Related work MASS (arXiv:2501.11469) addresses "language bias, where models predominantly rely on language priors and neglect to adequately consider the visual content," supporting the statistical bias mechanism.

### Mechanism 2: Targeted Fine-Tuning Enables Directional Adaptation
- Claim: LoRA fine-tuning on uncommon-sense examples improves semantic role discrimination, even for lightweight models, without requiring full retraining.
- Mechanism: Fine-tuning on counter-intuitive examples creates a "domain shift" that forces the model to attend to structural semantic relationships rather than relying on cached statistical associations. The low-rank adaptation preserves most pretrained knowledge while adjusting attention to role-specific features.
- Core assumption: The pretrained visual encoder has sufficient representational capacity to distinguish fine-grained action semantics; the bottleneck is in the alignment layer or decision head.
- Evidence anchors:
  - [abstract]: "Even lightweight models improved after fine-tuning, but still lagged behind human performance."
  - [section 4.2]: "after LoRA fine-tuning, the accuracy of LLaVA1.5 has been significantly improved to 0.79... demonstrates that even models with smaller parameter scales can significantly enhance their ability through customized fine-tuning."
  - [corpus]: No direct corpus evidence on LoRA for uncommon-sense reasoning; related work focuses on pretraining rather than fine-tuning for semantic roles.

### Mechanism 3: Contrastive Embeddings Lack Structural Role Encoding
- Claim: CLIP-style contrastive models fail at agent-patient discrimination because their embedding space captures entity co-occurrence but not relational structure.
- Mechanism: Contrastive learning optimizes for global image-text similarity, aligning "policeman" and "suspect" embeddings regardless of who is acting upon whom. The model detects presence but not directional action semantics.
- Core assumption: The embedding space lacks explicit encoding of predicate-argument structure; similarity is computed over bag-of-concepts representations.
- Evidence anchors:
  - [section 4.3]: "When the subject and object in a sentence are exchanged... the semantic meaning has changed fundamentally. Because the model relies too much on the statistical similarity between embedding vectors, it is unable to capture this subtle but crucial logical relationship."
  - [section 4.3]: "They often only focus on whether certain key elements exist in the image, but cannot accurately parse the dynamic and logical relationships between these elements."
  - [corpus]: DGTRS-CLIP (arXiv:2503.19311) notes CLIP models "struggle to process [long captions] effectively because of limited text-encoding capacity," indirectly supporting structural encoding limitations.

## Foundational Learning

- **Semantic Roles (Agent-Patient-Instrument)**
  - Why needed here: UAIT's core design principle is semantic non-interchangeability—verbs where swapping agent and patient fundamentally changes meaning. Understanding VerbNet's role classification is essential to replicating the verb filtering pipeline.
  - Quick check question: For the verb "follow," if you swap "A dog follows a cat" to "A cat follows a dog," does the meaning change? (Yes—the direction of motion relative to the follower is reversed.)

- **Contrastive Learning vs. Generative VLMs**
  - Why needed here: The paper evaluates both paradigms and finds different failure modes. CLIP fails at structural semantics; generative VLMs fail at overcoming statistical priors.
  - Quick check question: Given an image of "a suspect follows a policeman," would CLIP's similarity score distinguish between that caption and "a policeman follows a suspect"? (Likely no—both contain the same entities with similar co-occurrence statistics.)

- **Statistical Bias in Vision-Language Pretraining**
  - Why needed here: The paper's central claim is that VLMs rely on co-occurrence frequency rather than visual evidence when making semantic judgments under uncommon conditions.
  - Quick check question: Why might a model trained on web-scraped image-text pairs incorrectly describe "a rabbit drags a tiger" as "a tiger drags a rabbit"? (The latter appears far more frequently in training data, creating a prior that overwhelms visual evidence.)

## Architecture Onboarding

- **Component map:**
  VerbNet -> Verb Filter -> Text Generator (Qwen2) -> Image Generator (SD3.5) -> VQA Constructor -> Evaluation Suite

- **Critical path:**
  1. Verb selection (determines which semantic relationships can be tested)
  2. Text generation quality (uncommon-sense must be grammatically correct but semantically counter-intuitive)
  3. Image generation fidelity (action must be visually unambiguous—agent/patient roles clear from pose, direction, contact)
  4. Option design (A/B choices must be perfectly symmetric except for role swap)

- **Design tradeoffs:**
  - Synthetic vs. real images: Synthetic avoids copyright and ensures novelty (not in pretraining data), but may have artifacts that cue the model
  - Binary choice vs. open-ended: Binary enables clean accuracy comparison but doesn't test generative understanding
  - Suppressing CoT: Eliminates confounding from reasoning chain quality, but hides potentially diagnostic failure modes

- **Failure signatures:**
  - Random-guess accuracy (~50%): Model has no semantic role understanding, relying entirely on entity detection
  - Common-sense bias (>70% selection of frequent option regardless of image): Statistical prior dominates visual evidence
  - Asymmetric error rates (better at human/animal agents than object agents): Verb selection or visual encoding bottleneck

- **First 3 experiments:**
  1. **Baseline probe**: Evaluate pretrained CLIP, LLaVA, Qwen2-VL on full UAIT dataset to establish failure mode distribution (statistical bias vs. role confusion).
  2. **Fine-tuning ablation**: Train LLaVA-1.5 with LoRA on varying proportions of UAIT (10%, 30%, 70%) to measure data efficiency of directional adaptation.
  3. **Verb class analysis**: Break down accuracy by verb category (directional vs. body-contact vs. tool-use) to identify which semantic structures are hardest for current architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training on semantic role reversals in action scenes generalize to improved physical plausibility reasoning in other contexts?
- Basis in paper: [explicit] Section 5.3 proposes introducing "physical counter-intuitive scenarios," while Section 4.4 notes that current fine-tuning cannot "fundamentally solve the problem of insufficient generalization ability."
- Why unresolved: The authors demonstrate that LoRA fine-tuning improves performance on UAIT but shows limited transfer to general compositional benchmarks like Winoground (Table 6).
- What evidence would resolve it: A training regime that improves scores on UAIT while simultaneously yielding statistically significant gains on independent physical reasoning benchmarks.

### Open Question 2
- Question: What architectural modifications are required for contrastive learning models to distinguish agent-patient relationships independent of statistical co-occurrence?
- Basis in paper: [inferred] Section 4.3 states that contrastive models fail because they rely on "statistical similarity between embedding vectors" and lack "sufficient detailed semantic alignment capabilities."
- Why unresolved: The paper identifies the failure mode (near-random accuracy for CLIP) but only evaluates existing off-the-shelf architectures without proposing or testing a structural solution.
- What evidence would resolve it: A modified contrastive architecture that achieves significantly above-random accuracy on the UAIT dataset by explicitly modeling semantic role logic.

### Open Question 3
- Question: How does the definition of "uncommon-sense" and model performance on the UAIT benchmark vary across different languages and cultural contexts?
- Basis in paper: [explicit] Section 5.3 explicitly lists "Integrating multilingual context" as further work, noting that "the definition of common sense and abnormality itself has cultural differences."
- Why unresolved: The current study restricts evaluation to English scenarios, leaving the universality of the observed semantic reasoning deficits untested.
- What evidence would resolve it: A multilingual version of the UAIT dataset showing whether the performance gap between models and humans persists uniformly across diverse linguistic and cultural backgrounds.

## Limitations

- The dataset construction relies heavily on synthetic image generation, which may introduce subtle artifacts that could serve as unintended cues for models
- The paper does not specify LoRA hyperparameters (rank, alpha, target modules) that could significantly impact fine-tuning results
- The evaluation focuses primarily on binary-choice accuracy without exploring the reasoning process or error patterns in depth

## Confidence

- **High confidence**: The core finding that VLMs perform significantly worse than humans on uncommon-sense action recognition (established through direct comparison with human benchmark of 96%)
- **Medium confidence**: The effectiveness of lightweight fine-tuning for directional adaptation (based on single model experiment with LLaVA-1.5)
- **Medium confidence**: The claim about statistical bias dominance in model failure modes (inferred from performance patterns but not directly tested)

## Next Checks

1. **Error Analysis by Verb Category**: Break down model errors by verb type (directional, body-contact, tool-use) to identify which semantic structures pose the greatest challenges for current architectures
2. **Cross-Domain Transfer**: Evaluate whether fine-tuning on UAIT improves performance on other uncommon-sense benchmarks or real-world applications requiring semantic role understanding
3. **Human-Generated vs. Synthetic Image Comparison**: Create a subset of UAIT using human-drawn images or real photographs to isolate whether generation artifacts contribute to model failure modes